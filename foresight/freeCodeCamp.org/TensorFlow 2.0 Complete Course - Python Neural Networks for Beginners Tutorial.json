{"text": " Hello, everybody, and welcome to an absolutely massive TensorFlow slash machine learning slash artificial intelligence course. Now, please stick with me for this short introduction, as I am going to give you a lot of important information regarding the course concept, the resources for the course and what you can expect after going through this. Now, first, I will tell you who this course is aimed for. So this course is aimed for people that are beginners in machine learning and artificial intelligence, or maybe have a little bit of understanding that are trying to get better, but do have a basic fundamental knowledge of programming and Python. So this is not a course you're going to take if you haven't done any programming before, or if you don't know any Python syntax in general. It's going to be highly advised that you understand the basic syntax behind Python, as I'm not going to be explaining that throughout this course. Now, in terms of your instructor for this course, that is going to be me. My name is Tim. Some of you may know me as Tech with Tim from my YouTube channel, where I teach all kinds of different programming topics. And I've actually been working with Free Code Camp and posted some of my series on their channel as well. Now, let's get into the course breakdown and talk about exactly what you're going to learn and what you can expect from this course. So as this course is geared towards beginners and people just getting started in the machine learning and AI world, we're going to start by breaking down exactly what machine learning and artificial intelligence is. So talking about what the differences are between them, the different types of machine learning, reinforcement learning, for example, versus neural networks versus simple machine learning. We're going to go through all those different differences. And then we're going to get into a general introduction of TensorFlow. Now, for those of you that don't know, TensorFlow is a module developed and maintained by Google, which can be used within Python to do a ton of different scientific computing, machine learning and artificial intelligence applications. We're going to be working with that through the entire tutorial series. And after we do that general introduction to TensorFlow, we're going to get into our core learning algorithms. Now, these are the learning algorithms that you need to know before we can get further into machine learning. They build a really strong foundation. They're pretty easy to understand and implement, and they're extremely powerful. After we do that, we're going to get into neural networks, discuss all the different things that go into how neural networks work, how we can use them and then do a bunch of different examples. And then we're going to get into some more complex aspects of machine learning and artificial intelligence and get to convolutional neural networks, which can do things like image recognition and detection. And then we're going to get into recurrent neural networks, which are going to do things like natural language processing, chatbots, text processing, all those different kinds of things. And finally ended off with reinforcement learning. Now, in terms of resources for this course, there are a ton. And what we're going to be doing to make this really easy for you and for me is doing everything through Google Collaboratory. Now, if you haven't heard of Google Collaboratory, essentially it's a collaborative coding environment that runs an iPython notebook in the cloud on a Google machine where you can do all of your machine learning for free. So you don't need to install any packages. You don't need to use PIP. You don't need to get your environment set up. All you need to do is open a new Google Collaboratory window and you can start writing code. And that's what we're going to be doing in this series. If you look in the description right now, you will see links to all of the notebooks that I use throughout this guide. So if there's anything that you want to be cleared up, if you want the code for yourself, if you want just text based descriptions of the things that I'm saying, you can click those links and gain access to them. So with that being said, I'm very excited to get started. I hope you guys are as well. And let's go ahead and get into the content. So in this first section, I'm going to spend a few minutes discussing the difference between artificial intelligence, neural networks and machine learning. Now, the reason we need to go into this is because we're going to be covering all of these topics throughout this course. So it's vital that you guys understand what these actually mean. And you can kind of differentiate between them. So that's what we're going to focus on now. Now, quick disclaimer here, just so everyone's aware, I'm using something called Windows, Inc. This just default comes with Windows. I have a drawing tabled down here. And this is what I'm going to be using for some of the explanatory parts where there's no real coding, just to kind of illustrate some concepts and topics to you. Now, I have very horrible handwriting. I'm not artistic whatsoever. Programming is definitely more of my thing than drawing and doing diagrams and stuff. But I'm going to try my best. And this is just the way that I find I can convey information the best to you guys. So anyways, let's get started and discuss the first topic here, which is artificial intelligence. Now, artificial intelligence is a huge hype nowadays. And it's funny because a lot of people actually don't know what this means, or they try to tell people that what they've created is not artificial intelligence, when in reality, it actually is. Now, the kind of formal definition of AI, and I'm just going to read it off of my slide here to make sure that I'm not messing this up, is the effort to automate intellectual tasks normally performed by humans. Now, that's a fairly big definition, right? What is considered an intellectual task? And, you know, really, that doesn't help us too much. So what I'm going to do is bring us back to when AI was created, first created to kind of explain to you how AI has evolved and what it really started out being. So back in 1950, there was kind of the question being asked by scientists and researchers, can computers think, can we get them to figure things out? Can we get away from just hard coding? And, you know, having like, can we get a computer to think and it do its own thing? So that was kind of the question that was asked. And that's when the term artificial intelligence was kind of coined and created. Now, back then, AI was simply a predefined set of rules. So if you're thinking about an AI for maybe like tic-tac-toe or an AI for chess, all they would have had back then is predefined rules that humans had come up with and typed into the computer in code, and the computer would simply execute those set of rules and follow those instructions. So there was no deep learning, machine learning, crazy algorithms happening. It was simply if you wanted the computer to do something, you would have to tell it beforehand, say you're in this position and this happens, do this. And that's what AI was. And very good AI was simply just a very good set of rules or a ton of different rules that humans had implemented into some program. You can have AI programs that are stretching, you know, half a million lines of code, just with tons and tons and tons of different rules that have been created for that AI. So just be aware that AI does not necessarily mean anything crazy, complex or super complicated. But essentially, if you're trying to simulate some intellectual task, like playing a game that a human would do with a computer, that is considered AI. So even a very basic artificial intelligence for a tic-tac-toe game where it plays against you, that is still considered AI. And if we think of something like Pac-Man, right, where we have, you know, our little ghost, and this will be my rough sketch of a ghost, and we have our Pac-Man guy who will just be this. Well, would we consider this ghost AI? What it does is it attempts to find and kind of simulate how it would get to Pac-Man, right? And the way this works is just using a very basic path finding algorithm. This is nothing to do with deep learning or machine learning or anything crazy. But this is still considered artificial intelligence. The computer is figuring out how it can kind of play and do something by following an algorithm. So we don't necessarily need to have anything crazy, stupid, complex to be considered AI, it simply needs to just be simulating some intellectual human behavior. That's kind of the definition of artificial intelligence. Now, obviously today, AI has evolved into a much more complex field where we now have machine learning and deep learning and all these other techniques, which is what we're going to talk about now. So what I want to start by doing is just drawing a circle here. And I want to label this circle and say AI like that. So this is going to define AI because everything I'm going to put inside of here is considered artificial intelligence. So now let's get into machine learning. So what I'm going to do is draw another circle inside of here. And we're going to label this circle ML for machine learning. Now notice I put this inside of the artificial intelligence circle. This is because machine learning is a part of artificial intelligence. Now, what is machine learning? Well, what we talked about previously was kind of the idea that AI used to just be a predefined set of rules, right? Where what would happen is we would feed some data, we would go through the rules by and then analyze the data with the rules. And then we'd spit out some output, which would be, you know, what we're going to do. So in the classic example of chess, say we're in check, well, we pass that board information to the computer, it looks at its sets of rules, it determines we're in check, and then it moves us somewhere else. Now, what is machine learning in contrast to that? Well, machine learning is kind of the first field that's actually figuring out the rules for us. So rather than us hard coding the rules into the computer, what machine learning attempts to do is take the data and take what the output should be and figure out the rules for us. So you'll often hear that, you know, machine learning requires a lot of data and you need ton of examples and, you know, input data to really train a good model. Well, the reason for that is because the way that machine learning works is it generates the rules for us. We give it some input data, we give it what the output data should be. And then it looks at that information and figures out what rules can we generate so that when we look at new data, we can have the best possible output for that. Now, that's also why a lot of the times machine learning models do not have a hundred percent accuracy, which means that they may not necessarily get the correct answer every single time. And our goal when we create machine learning models is to raise our accuracy as high as possible, which means it's going to make the fewest mistakes possible. Because just like a human, you know, our machine learning models, which are trying to simulate, you know, human behavior can make mistakes. But to summarize that, essentially, machine learning, the difference between that and kind of, you know, algorithms and basic artificial intelligence is the fact that rather get that rather than us, the programmer giving it the rules, it figures out the rules for us. And we might not necessarily know explicitly what those rules are when we look at machine learning and create machine learning models. But we know that we're giving some input data, we're giving the expected output data, and then it looks at all of that information, does some algorithms, which we'll talk about later on that, and figures out the rules for us so that later when we give it some input data, and we don't know the output data, it can use those rules that it's figured out from our examples and all that training data that we gave it to generate some output. Okay, so that's machine learning. Now we've covered AI and machine learning. And now it's time to cover neural networks or deep learning. Now this circle gets to go right inside of the machine learning right here. I'm just going to label this one NN, which stands for neural networks. Now neural networks get a big hype. They're usually what the first, you know, when you get into machine learning, you want to learn neural networks, you're kind of like neural networks are cool, they're capable of a lot. But let's discuss what these really are. So the easiest way to define a neural network is it is a form of machine learning that uses a layered representation of data. Now we're not going to really understand this completely right now. But as we get further in that should start to make more sense as a definition. But what I need to kind of illustrate to you is that in the previous example, where we just talked about machine learning, essentially what we had is we had some input bubbles, which I'm going to define as these. We had some set of rules that is going to be in between here. And then we had some output. And what would happen is we feed this input to this set of rules. Something happens in here. And then we get some output. And then that is what, you know, our program does. So that's what we get from the model. We pretty much just have two layers. We have kind of the input layer, the output layer. And the rules are kind of just what connects those two layers together. Now in neural networks and what we call deep learning, we have more than two layers. Now I'm just trying to erase all this quickly so I can show you that. So let's say, and I'll draw this one another color, because why not? If we're talking about neural networks, what we might have, and this will vary. And I'll talk about this in a second, is the fact that we have an input layer, which will be our first layer of data. We could have some layers in between this layer that are all connected together. And then we could have some output layer. So essentially, what happens is our data is going to be transformed through different layers, and different things are going to happen. There's going to be different connections between these layers. And then eventually we'll reach an output. Now it's very difficult to explain neural networks without going completely in depth. So I'll cover a few more notes that I have here. Essentially, in neural networks, we just have multiple layers. That's kind of the way to think of them. And as we see machine learning, you guys should start to understand this more. But just understand that we're dealing with multiple layers. And a lot of people actually call this a multi stage information extraction process. Now, I did not come up with that term. I think that's from a book or something. But essentially what ends up happening is we have our data at this first layer, which is that input information, which we're going to be passing to the model that we're going to do something with. It then goes to another layer where it will be transformed. It will change into something else using a predefined kind of set of rules and weights that we'll talk about later. Then it will pass through all of these different layers where different kind of features of the data, which again, we'll discuss in a second, will be extracted, will be figured out, will be found until eventually we reach an output layer where we can kind of combine everything we've discovered about the data into some kind of output that's meaningful to our program. So that's kind of the best that I can do to explain neural networks without going on to a deeper level. I understand that a lot of you probably don't understand what they are right now. And that's totally fine. But just know that there are layered representation of data. We have multiple layers of information, whereas in standard machine learning, we only have, you know, one or two layers and an artificial intelligence. In general, we don't necessarily have to have like a predefined set of layers. OK, so that is pretty much it for neural networks. There's one last thing I will say about them is that they're actually not modeled after the brain. So a lot of people seem to think that neural networks are modeled after the brain and the fact that you have neurons firing in your brain, and that can relate to neural networks. Now, there is a biological inspiration for the name neural networks in the way that they work from, you know, human biology, but it is not necessarily modeled about the way that our brain works. And in fact, we actually don't really know how a lot of the things in our brain operate and work. So it would be impossible for us to say that neural networks are modeled after the brain, because we actually don't know how information is kind of happens and occurs and transfers through our brain, or at least we don't know enough to be able to say this is exactly what it is a neural network. So anyways, that was kind of the last point there. OK, so now we need to talk about data. Now, data is the most important part of machine learning and artificial intelligence neural networks as well. And it's very important that we understand how important data is and what the different kind of parts of it are, because they're going to be referenced a lot in any of the resources that we're using. Now, what I want to do is just create an example here where I'm going to make a data set that is about students final grades in like a school system. So essentially, we're going to make this a very easy example where all we're going to have for this data set is we're going to have information about students. So we're going to have their midterm one grade, their midterm two grade, and then we're going to have their final grade. So I'm just going to say midterm one. And again, excuse my handwriting here. It's not the easiest thing to write with this drawing tablet. And then I'll just do final. So this is going to be our data set. And we'll actually see some similar data sets to this as we go through and do some examples later on. So for student one, which we'll just put some students here, we're going to have their midterm one grade, maybe that's a 70, their midterm two grade, maybe that was an 80. And then let's say their final was like their final term grade, not just the mark on the final exam. Let's give them a 77. Now, for midterm one, we can give someone a 60. Maybe we give them a 90. And then we determined that the final grade on their exam was let's say an 84. And then we could do something with maybe a lower grade here. So 40, 50, and then maybe they got a 38 or something in the final grade. Now, obviously, we could have some other information here that we're omitting. Like maybe there was some exam, some assignments, whatever, some other things they did that contributed to their grade. But the problem that I want to consider here is the fact that given our midterm one grade and our midterm two grade and our final grade, how can I use this information to predict any one of these three columns? So if I were given a student's midterm one grade, and I were given a student's final grade, how could I predict their midterm two grade? So this is where we're going to talk about features and labels. Now, whatever information we have, that is the input information, which is the information we will always have that we need to give to the model to get some output is what we call our features. So in the example where we're trying to predict midterm two, and let's just do this and highlight this in red. So we understand what we would have as our features, our input information are going to be midterm one and final, because this is the information we are going to use to predict something. It is the input, it is what we need to give the model. And if we're training a model to look at midterm one and final grade, whenever we want to make a new prediction, we need to have that information to do so. Now, what's highlighted in red. So this midterm two here is what we would call the label or the output. Now, the label is simply what we are trying to look for or predict. So when we talk about features versus labels, features is our input information, the information that we have that we need to use to make a prediction. And our label is that output information that is just representing, you know, what we're looking for. So when we feed our features to a model, it will give to us a label. And that is kind of the point that we need to understand. So that was the basic here. And now I'm just going to talk a little bit more about data, because we will get into this more as we continue going and about the importance of it. So the reason why data is so important is this is kind of the key thing that we use to create models. So whenever we're doing AI and machine learning, we need data pretty much. Unless you're doing a very specific type of machine learning and artificial intelligence, which we'll talk about later. Now, for most of these models, we need tons of different data. We need tons of different examples. And that's because we know how machine learning works now, which is essentially we're trying to come up with rules for a data set. We have some input information. We have some output information or some features and some labels. We can give that to a model and tell it to start training. And what it will do is come up with rules such that we can just give some features to the model in the future. And then it should be able to give us a pretty good estimate of what the output should be. So when we're training, we have a set of training data. And that is data where we have all of the features and all of the labels. So we have all of this information. Then when we're going to test the model or use the model later on, we would not have this midterm to information. We wouldn't pass this to the model. We would just pass our features, which is midterm one and final. And then we would get the output of midterm two. So I hope that makes sense. That just means data is extremely important. If we're feeding incorrect data or data that we shouldn't be using to the model, that could definitely result in a lot of mistakes. And if we have incorrect output information or incorrect input information, that is going to cause a lot of mistakes as well, because that is essentially what the model is using to learn and to kind of develop and figure out what it's going to do with new input information. So that means that is enough of data. Now let's talk about the different types of machine learning. OK, so now that we've discussed the difference between artificial intelligence, machine learning and neural networks, we have a kind of decent idea about what data is in the difference between features and labels. It's time to talk about the different types of machine learning specifically, which are unsupervised learning, supervised learning and reinforcement learning. Now, these are just the different types of learning, the different types of figuring things out. Now, different kind of algorithms fit into these different categories from within artificial intelligence, within machine learning and within neural networks. So the first one we're going to talk about is supervised learning, which is kind of what we've already discussed. So I'll just write supervised up here again, excuse the handwriting. So supervised learning. Now, what is this? Well, supervised learning is kind of everything we've already learned, which is we have some features. So we'll write our features like this, right? We have some features and those features correspond to some label or potentially labels. Sometimes we might predict more than one information. So when we have this information, we have the features and we have the labels. What we do is we pass this information to some machine learning model. It figures out the rules for us. And then later on, all we need is the features and it will give us some labels using those rules. But essentially, what supervised learning is, is when we have both of this information. The reason it's called supervised is because what ends up happening when we train our machine learning model is we pass the input information. It makes some arbitrary prediction using the rules it already knows. And then it compares that prediction that it made to what the actual prediction is, which is this label. So we supervise the model and we say, OK, so you predicted that the color was red, but really the color of whatever we passed in should have been blue. So we need to tweak you just a little bit so that you get a little bit better and you move in the correct direction. And that's kind of the way that this works. For example, say we're predicting, you know, students final grade. Well, if we predict that the final grade is 76, but the actual grade is 77, we were pretty close, but we're not quite there. So we supervise the model and we say, hey, we're going to tweak you just a little bit, move you in the correct direction. And hopefully we get you to 77. And that is kind of the way to explain this, right? You have the features, you have the labels. When you pass the features, the model has some rules that it's already built. It makes a prediction. And then it compares that prediction to the label and then re tweaks the model and continues doing this with thousands upon thousands upon thousands of pieces of data, until eventually it gets so good that we can stop training it. And that is what supervised learning is. It's the most common type of learning. It's definitely the most applicable in a lot of instances. And most machine learning algorithms that are actually used use a form of supervised machine learning. A lot of people seem to think that this is, you know, a less complicated, less advanced way of doing things that is definitely not true. All of the different methods I'm going to tell you have different advantages and disadvantages. And this has a massive advantage when you have a ton of information and you have the output of that information as well. But sometimes we don't have the luxury of doing that. And that's where we talk about unsupervised learning. So hopefully that made sense for supervised learning. Tried my best to explain that. And now let's go into or sorry for supervised learning. Now let's go into unsupervised learning. So if we know the definition of supervised learning, we should hopefully be able to come up with a definition of unsupervised learning, which is when we only have features. So given a bunch of features like this and absolutely no labels, no output for these features, what we want to do is have the model come up with those labels for us. Now, this is kind of weird. You're kind of like, wait, how does that work? Why would we even want to do that? Well, let's take this for an example. We have some access, some axes of data. Okay, and we have like a two dimensional data point. So I'm just going to call this, let's say X and let's say Y. Okay. And I'm going to just put a bunch of dots on the screen that kind of represents like maybe a scatter plot of some of our different data. And I'm just going to put some dots specifically closer to other ones. Just so you guys kind of get the point of what we're trying to do here. So let's do that. Okay. So let's say I have this data set, this here is what we're working with. And we have these features. The features in this instance are going to be X and Y, right? So X and Y are my features. Now, we don't have any output specifically for these data points. What we actually want to do is we want to create some kind of model that can cluster these data points, which means figure out kind of, you know, unique groups of data and say, okay, so you're in group one, you're in group two, you're in group three, and you're in group four. We may not necessarily know how many groups we have, although sometimes we do. But what we want to do is just group them and kind of say, okay, we want to figure out which ones are similar and we want to combine those together. So hopefully what we would do with an unsupervised machine learning model is pass all of these features and then have the model create kind of these groupings. So like maybe this is a group, maybe this is a group, maybe this is a group, if we were having four groupings, and maybe if we had two groupings, we might get groupings that look something like this, right? And then when we pass a new data point in, that could, we could figure out what group that was a part of by determining, you know, which one it is closer to. Now, this is kind of a rough example. It's hard to again, explain all of these without going very in depth into the specific algorithms, but unsupervised machine learning or just learning in general is when you don't have some output information, you actually want the model to figure out the output for you. You don't really care how it gets there, you just want it to get there. And again, a good example is clustering data points, and we'll talk about some specific applications of when we might even want to use that later on, just understand you have the features, you don't have the labels, and you get the unsupervised model to kind of figure it out for you. Okay, so now our last type, which is very different than the two types I just explained is called reinforcement learning. Now personally, reinforcement learning, and I don't even know if I want to spell this because I feel like I'm going to mess it up. Reinforcement learning is the coolest type of machine learning, in my opinion. And this is when you actually don't have any data, you have what you call an agent, an environment and a reward. I'm going to explain this very briefly with a very, very, very simple example because it's hard to get too far. So let's say we have a very basic game, you know, maybe we made this game ourselves, and essentially, the objective of the game is to get to the flag. Okay, that's all it is. We have some ground, you can move left or right, and we want to get to this flag. Well, we want to train some artificial intelligence, some machine learning model that can figure out how to do this. So what we do is we call this our agent. We call this entire thing. So this whole thing here, the environment. So I guess I could write that here. So n by our meant think I spelled that correctly. And then we have something called a reward. And a reward is essentially what the agent gets when it does something correctly. So let's say the agent takes one step over this way. So let's say he's a new position is here. I just don't want to keep drawing him. So I'm just going to use a dot. Well, he got closer to the flag. So what I'm actually going to do is give him a plus two reward. So let's say he moves again closer to the flag. Maybe I give him now plus one this time he got even closer. And as he gets closer, I give him more and more reward. Now what happens if he moves backwards? So let's erase this and let's say that at some point in time, rather than moving closer to the threat, the flag, he moves backwards. Well, he might get a negative reward. Now, essentially, what the objective of this agent is to do is to maximize its reward. So if you give it a negative reward for moving backwards, it's going to remember that. And it's going to say, OK, at this position here, where I was standing, when I moved backwards, I got a negative reward. So if I get to this position again, I don't want to go backwards anymore. I want to go forwards because that should give me a positive reward. And the whole point of this is we have this agent that starts off with absolutely no idea, no kind of, you know, knowledge of the environment. And what it does is it starts exploring. And it's a mixture of randomly exploring and exploring using kind of some of the things that's figured out so far to try to maximize its reward. So eventually, when the agent gets to the flag, it will have the most the highest possible reward that it can have. And then next time that we plug this agent into the environment, it will know how to get to the flag immediately because it's kind of figured that out. It's determined that in all of these different positions, if I move here, this is the best place to move. So if I get in this position, move there. Now, this is, again, hard to explain without more detailed examples and going more mathematically and all of that. But essentially, just understand we have the agent, which is kind of what the thing is that's moving around in our environment. We have this environment, which is just what the agent can move around in. And then we have a reward. And the reward is what we need to figure out as the programmer, a way to reward the agent correctly so that it gets to the objective in the best possible way. But the agent simply maximizes that reward. So it just figures out where I need to go to maximize that reward. It starts at the beginning, kind of randomly exploring the environment because it doesn't know any of the rewards it gets at any of the positions. And then as it explores some more different areas, it kind of figures out the rules and the way that the environment works and then will determine how to reach the objective, which is whatever it is that it is. This is a very simple example. You could train a reinforcement model to do this and, you know, like half a second, right? But there is way more advanced examples and there's been examples of reinforcement learning, like of AI is pretty much figuring out how to play games together. How to it's it's actually pretty cool. Some of the stuff that reinforcement learning is doing. And it's a really awesome kind of advancement in the field, because it means we don't need all this data anymore. We can just get this to kind of figure out how to do things for us and explore the environment and learn on its own. Now, this can take a really long time. This can take a very short amount of time, really depends on the environment. But a real application of this is training AIs to play games, as you might be able to tell by kind of what I was explaining here. And yeah, so that is kind of the fundamental differences between supervised, unsupervised and reinforcement learning. We're going to cover all three of these topics throughout this course. And it's really interesting to see some of the applications we can actually do with this. So with that being said, I'm going to kind of end what I'm going to call module one, which is just a general overview of the different topics, some definitions and getting a fundamental knowledge. And in the next one, what we're going to be talking about is what TensorFlow is. We're going to get into coding a little bit and we're going to discuss some different aspects of TensorFlow and things we need to know to be able to move forward and do some more advanced things. So now in module two of this course, what we're going to be doing is getting a general introduction to TensorFlow, understanding what a tensor is, understanding shapes and data representation, and then how TensorFlow actually works on a bit of a lower level. This is very important because you can definitely go through and learn how to do machine learning without kind of gaining this information and knowledge. But it makes it a lot more difficult to tweak your models and really understand what's going on if you don't, you know, have that fundamental lower level knowledge of how TensorFlow actually works and operates. So that's exactly what we're going to cover here. Now, for those of you that don't know what TensorFlow is, essentially, this is an open source machine learning library. It's one of the largest ones in the world. It's one of the most well known and it's maintained and supported by Google. Now, TensorFlow essentially allows us to do and create machine learning models and neural networks and all of that without having to have a very complex math background. Now, as we get further in and we start discussing more in detail, how neural networks work in machine learning algorithms actually function, you'll realize there's a lot of math that goes into this. Now, it starts off being very kind of fundamental, like basic calculus and basic linear algebra, and then it gets much more advanced into things like gradient descent and some more regression techniques and classification. And essentially, you know, a lot of us don't know that and we don't really need to know that. So long as we have a basic understanding of it, then we can use the tools that TensorFlow provides for us to create models. And that's exactly what TensorFlow does. Now, what I'm in right now is what I call Google Collaboratory. I'm going to talk about this more in depth in a second. But what I've done for this whole course is I've transcribed very detailed everything that I'm going to be covering through each module. So this is kind of the transcription of module one, which is the introduction to TensorFlow. You can see it's not crazy long, but I wanted to do this so that any of you can follow along with kind of the text base and kind of my lecture notes. I almost want to call them as I go through the different content. So in the description, there will be links to all of these different notebooks. This is in something called Google Collaboratory, which again, we're going to discuss in a second. But you can see here that I have a bunch of text and it gets down to some different coding aspects. And what I'm going to be doing to make sure that I stay on track is simply following along through this, I might deviate slightly, I might go into some other examples. This will be kind of everything that I'm going to be covering through each module. So again, to follow along, click the link in the description. All right. So what can we do with TensorFlow? Well, these are some of the different things I've listed them here. So I don't forget we can do image classification, data clustering, regression, reinforcement learning, natural language processing, and pretty much anything that you can imagine with machine learning. Essentially, what TensorFlow does is gives us a library of tools that allow us to omit having to do these very complicated math operations. It just does them for us. Now, there is a bit that we need to know about them, but nothing too complex. Now, let's talk about how TensorFlow actually works. So TensorFlow has two main components that we need to understand to figure out how operations and math are actually performed. Now, we have something called graphs and sessions. Now, the way that TensorFlow works is it creates a graph of partial computations. Now, I know this is going to sound a little bit complicated. Some of you guys just try to kind of forget about the complex vocabulary and follow along. But essentially, what we do when we write code in TensorFlow is we create a graph. So if I were to create some variable, that variable gets added to the graph. And maybe that variable is the sum or the summation of two other variables. What the graph will define now is say, you know, we have variable one, which is equal to the sum of variable two and variable three. But what we need to understand is that it doesn't actually evaluate that. It simply states that that is the computation that we've defined. So it's almost like writing down an equation without actually performing any math. We kind of just, you know, have that equation there. We know that this is the value, but we haven't evaluated it. So we don't know that the value is like seven per se. We just know that it's the sum of, you know, vector one and vector two. Or it's the sum of this or it's the cross product or the dot product. We just define all of the different partial computations because we haven't evaluated those computation yet. And that is what is stored in the graph. Now, the reason it's called a graph is because different computations can be related to each other. For example, if I want to figure out the value of vector one, but vector one is equal to the value of vector three plus vector four, I need to determine the value of vector three and vector four before I can do that computation. So they're kind of linked together. And I hope that makes a little bit of sense. Now, what is a session? Well, session is essentially a way to execute part or the entire graph. So when we start a session, what we do is we start executing different aspects of the graph. So we start at the lowest level of the graph where nothing is dependent on anything else. We have maybe constant values or something like that. And then we move our way through the graph and start doing all of the different partial computations that we've defined. Now, I hope that this isn't too confusing. I know this is kind of a lot of lingo. You guys will understand this as we go through. And again, you can read through some of these components here that I have in collaboratory, if I'm kind of skipping through anything, or you don't truly understand. But that is the way that graphs and sessions work. We won't go too in depth with them. We do need to understand that that is the way TensorFlow works. And there's some times where we can't use a specific value in our code yet because we haven't evaluated the graph. We haven't created a session and gotten the values yet. Which we might need to do before we can actually, you know, use some specific value. So that's just something to consider. All right, so now we're actually going to get into coding, importing and installing TensorFlow. Now, this is where I'm going to introduce you to Google Collaboratory and explain how you guys can follow along without having to install anything on your computer. And it doesn't matter if you have like a really crappy computer or even if you're on like an iPhone per se, you can actually do this, which is amazing. So all you need to do is Google, Google Collaboratory and create a new notebook. Now, what Google Collaboratory is, is essentially a free Jupyter notebook in the cloud for you. The way this works is you can open up this notebook. You can see this is called I pi NB. I yeah, what is that? I pi NB, which I think just stands for I Python notebook. And what you can do in here is actually write code and write text as well. So this in here is what it's called, you know, Google Collaboratory Notebook. And essentially why it's called a notebook is because not only can you put code, but you can also put notes, which is what I've done here with these specific titles. So you can actually use Markdown inside of this. So if I open up one of these, you can see that I've used Markdown text to actually kind of create these sections. And yeah, that is kind of how Collaboratory works. But what you can do in Collaboratory is forget about having to install all of these modules, they're already installed for you. So what you're actually going to do when you open a Collaboratory window is Google is going to automatically connect you to one of their servers or one of their machines that has all of this stuff done and set up for you. And you can start writing code and executing it off their machine and seeing the result. So for example, if I want to print hello like this, and I'll zoom in a little bit so you guys can read this. All I do is I create a new code block, which I can do by clicking code. Like that, I can delete one like that as well. And I hit run. Now notice, give it a second, it does take longer than typically on your own machine, and we get hello popping up here. So the great thing about Collaboratory is the fact that we can have multiple code blocks and we can run them in whatever sequence we want. So to create another code block, you can just, you know, do another code block from up here or by just by looking down here, you get code and you get text and I can run this in whatever order I want. So I could do like print. Yes, for example, I could run yes, and we'll see the output of yes. And then I could print hello one more time and notice that it's showing me the number on this left hand side here on which these kind of code blocks were run. Now, all of these code blocks can kind of access each other. So for example, I do define funk and we'll just take some parameter H. And all we'll do is just print H. Well, if I create another code block down here, so let's go code, I can call funk with say, hello, make sure I run this block first, so we define the function. Now I'll run funk and notice we get the output hello, so we can access all of the variables, all the functions, anything we've defined in other code blocks from code blocks that are below it or code blocks that have executed after it. Now, another thing that's great about collaboratory is the fact that we can import pretty much any module we can imagine, and we don't need to install it. So I'm not actually going to be going through how to install TensorFlow completely. There is a little bit on how to install TensorFlow on your local machine inside of this notebook, which I'll refer you to. But essentially, if you know how to use pip, it's pretty straightforward, you can pip install TensorFlow, or pip install TensorFlow GPU, if you have a compatible GPU, which you can check from the link that's in this notebook. Now, if I want to import something, what I can do is literally just write the import. So I can say import numpy like this. And usually numpy is a module that you need to install. But we don't need to do that here. It's already installed on the machine. So again, we hook up to those Google servers, we can use their hardware to perform machine learning. And this is awesome. This is amazing. And it gives you performance benefits when you're running on like a lower kind of crappier machine, right? So we can have a look at the RAM in the disk space of our computer, we can see we have 12 gigs of RAM, we're dealing with 107 gigabytes of data on our disk space. And we can obviously, you know, look at that if we want, we can connect to our local runtime, which I believe connects to your local machine. But I'm not going to go through all of that. I just want to show you guys some basic components of collaboratory. Now, some other things that are important to understand is this runtime tab, which you might see me use. So restart runtime essentially clears all of your output, and just restarts whatever's happened. Because the great thing with collaboratory is since I can run specific code blocks, I don't need to execute the entire thing of code every time I want to run something. If I've just made a minor change in one code block, I can just run that code. Sorry, I can just run that code block. I don't need to run everything before it or even everything after it, right? But sometimes you want to restart everything and just rerun everything. So to do that, you click restart runtime, that's just going to clear everything you have. And then restart and run all will restart the runtime as well as run every single block of code you have in sequential order in which it shows up in the thing. So I recommend you guys open up one of these windows. You can obviously follow along with this notebook if you want. But if you want to type it out on your own and kind of mess with it, open up a notebook, save it. It's very easy. And these are again, extremely similar to Jupiter notebooks, Jupiter notebooks, they're pretty much the same. Okay, so that is kind of the Google Collaboratory aspect how to use that. Let's get into importing TensorFlow. Now this is going to be kind of specific to Google Collaboratory. So you can see here, these are kind of the steps we need to follow to import TensorFlow. So since we're working in Google Collaboratory, they have multiple versions of TensorFlow, they have the original version of TensorFlow, which is 1.0, and the 2.0 version. Now to define the fact that we want to use TensorFlow 2.0, just because we're in this notebook, we need to write this line of code at the very beginning of all of our notebooks. So percent TensorFlow underscore version 2.x. Now this is simply just saying we need to use TensorFlow 2.x. So whatever version that is, and this is only required in a notebook, if you're doing this on your local machine in a text editor, you're not going to need to write this. Now once we do that, we typically import TensorFlow as an alias name of TF. Now to do that, we simply import the TensorFlow module, and then we write as TF. If you're on your local machine, again, you're going to need to install TensorFlow first to make sure that you're able to do this. But since we're in Collaboratory, we don't need to do that. Now, since we've defined the fact we're using version 2.x, when we print the TensorFlow version, we can see here that it says version two, which is exactly what we're looking for. And then it says TensorFlow 2.1.0. So make sure that you print your version, you're using version 2.0, because there is a lot of what I'm using in this series that is kind of, if you're in TensorFlow 1.0, it's not going to work. So it's new in TensorFlow 2.0, or it's been refactored and the names have been changed. Okay, so now that we've done that, we've imported TensorFlow, we've got this here, and I'm actually going to go to my fresh notebook and just do this. So we'll just copy these lines over just so we have some fresh code, and I don't have all this text that we have to deal with. So let's do this TensorFlow, let's import TensorFlow as TF, and then we can print the TF dot version and have a look at that. So version. Okay, so let's run our code here, we can see TensorFlow is already loaded. Oh, it says 1.0. So if you get this error, it's actually good, I ran into this where TensorFlow has already been loaded. All you need to do is just restart your runtime. So I'm going to restart and run all just click Yes. And now we should see that we get that version 2.0. Once this starts running, give it a second TensorFlow 2.0 selected, we're going to import that module. And there we go, we have version 2.0. Okay, so now it's time to talk about tensors. Now, what is a tensor? Now, tensor just immediately seems kind of like a complicated name, you're like, All right, tensor, like this is confusing. But what is it? Well, obviously this is going to be a primary aspect of TensorFlow, considering the name similarities. And essentially, all it is is a vector generalized to higher dimensions. Now, what is a vector? Well, if you've ever done any linear algebra or even some basic kind of vector calculus, you should hopefully know what that is. But essentially, it is kind of a data point is kind of the way that I like to describe it. And the reason we call it a vector is because it doesn't necessarily have a certain coordinate. So like if you're talking about a two dimensional data point, you have, you know, maybe an x and a y value, or like an x one value and an x two value. Now a vector can have any amount of dimensions in it, it could have one dimension, which simply means it's just one number, could have two dimensions, which means we're having two numbers. So like an x and a y value, if we're thinking about a two dimensional graph, we'd have three dimensions, if we're thinking about a three dimensional graph, so that would be three data points, we could have four dimensions, if we're talking about sometimes some image data and some video data, five dimensions, and we can keep going, going, going with vectors. So essentially, what a tensor is, and I'll just read this formal definition to make sure I haven't butchered anything that's from the actual TensorFlow website. A tensor is a generalization of vectors and matrices to potentially higher dimensions, internally TensorFlow represents tensors as n dimensional arrays of base data types. Now we'll understand what that means in a second, but hopefully that makes sense. Now, since tensors are so important to TensorFlow, they're kind of the main object that we're going to be working with, manipulating and viewing. And it's the main object that's passed around through our program. Now, what we can see here is each tensor represents a partially defined computation that will eventually produce a value. So just like we talked about in the graphs and sessions, what we're going to do is when we create our program, we're going to be creating a bunch of tensors and TensorFlow is going to be creating them as well. And those are going to store partially defined computations in the graph. Later, when we actually build the graph and have the session running, we will run different parts of the graph, which means we'll execute different tensors, and be able to get different results from our tensors. Now each tensor has what we call a data type and a shape, and that's we're going to get into now. So a data type is simply what kind of information is stored in the tensor. Now it's very rare that we see any data types different than numbers, although there is the data type of strings and a few others as well. But I haven't included all of them here because they're not that important. But some examples we can see our float 32 in 32 string and others. Now the shape is simply the representation of the tensor in terms of what dimension it is. And we'll get some examples because I don't want to explain the shape until we can see some examples to really dial in. But here are some examples of how we would create different tensors. So what you can do is you can simply do TF dot variable. And then you can do the value and the data type that your tensor is. So in this case, we've created a string tensor which stores one string. And it is TF dot strings, we define the data type second, we have a number tensor which stores some integer value. And then that is up type TF int 16. And we have a floating point tensor, which stores a simple floating point. Now these tensors have a shape of I believe it's going to be one, which simply means they are a scalar. Now a scalar value and you might hear me say this a lot simply means just one value. That's all it means. When we talk about like vector values, that typically means more than one value. And we talk about matrices, we're having different it just it goes up but scalar simply means one number. So yeah, that is what we get for the different data types and creating tensors, we're not really going to do this very much in our program. But just for some examples here, that's how we do it. So we've imported them. So I can actually run these. And I mean, we're not going to really get any output by running this code because well, there's nothing to see. But now we're going to talk about the rank slash degree of tensors. So another word for rank is agree. So these are interchangeably. And again, this simply means the the number of dimensions involved in the tensor. So when we create a tensor of rank zero, which is what we've done up here, we call that a scalar. Now the reason this has rank zero is because it's simply one thing, we don't have any dimensions to this, there's like zero dimensionality of that. It was even a word, it's just one value. Whereas here, we have an array. Now when we have an array or a list, we immediately have at least rank one. Now the reason for that is because this array can store more than one value in one dimension, right? So I can do something like test, I could do okay, I could do Tim, which is my name, and we can run this and we're not going to get any output obviously here. But this is what we would call a rank one tensor, because it is simply one list, one array, which means one dimension. And again, you know, that's also like a vector. Now this, what we're looking at here is a rank to tensor. The reason this is a rank to tensor is because we have a list inside of a list, or in this case, multiple lists inside of a list. So the way that you can actually determine the rank of a tensor is the deepest level of a nested list, at least in Python with our representation, that's what that is. So here we can see we have a list inside of a list, and then another list inside of this upper list. So this would give us rank two. And this is what we typically call a matrices. And this again, is going to be of TF dot strings. So that's the data type for this tensor variable. So all of these that we've created are tensors, they have a data type, and they have some rank and some shape, and we're going to talk about the shape in a second. So to determine the rank of a tensor, we can simply use the method TF dot rank. So notice when I run this, we get the shape which is blank of rank to tensor. That's fine. And then we get num pi two, which simply means that this is of rank two. Now if I go for that rank one tensor, and I print this out. So let's have a look at it, we get num pi one here, which is telling us that this is simply of rank one. Now if I want to use one of these ones up here and see what it is, so let's try it, we can do numbers. So TF dot ring numbers. So we'll print that here. And we get num pi zero, because that's rank zero, right? So we'll go back to what we had, which was ranked to tensor. But again, those are kind of the examples we want to look at. Okay, so shapes of a tensor. So this is a little bit different now. What a shape simply tells us is how many items we have in each dimension. So in this case, when we're looking at rank two, tensor dot shape, so we have dot shape here, that's an attribute of all of our tensors, we get two two. Now let's look up here. What we have is Whoa, look at this two, and two. So we have two elements in the first dimension, right, and then two elements in the second dimension. That's pretty much what this is telling us. Now let's look at the rank for the shape of rank one tensor, we get three. So because we only have a rank one, notice we only get one number. Whereas when we had rank two, we got two numbers, and it told us how many elements were in each of these lists, right? So if I go and I add another one here, like that, and we have a look now at the shape. Oops, I got to run this first. So that's something can convert non square to tensor. Ah, sorry, so I need to have a uniform amount of elements in each one here, I can't just do what I did there. So add a third element here. Now what we can do is run this shouldn't get any issues. Let's have a look at the shape and notice we get now two three. So we have two lists, and each of those lists have three elements inside of them. So that's how the shape works. Now I could go ahead and add another list in here if I wanted to and I could say like, okay, okay, okay, so let's run this hopefully no errors. Looks like we're good. Now let's look at the shape again. And now we get a shape of three, three, because we have three interior lists. And in each of those lists, we have three elements. And that is pretty much how that works. Now again, we could go even further here and we could put another list inside of here that would give us a rank three tensor. And we'd have to do that inside of all of these lists. And then what that would give us now would be three numbers representing how many elements we have in each of those different dimensions. Okay, so changing shape. Alright, so this is what we need to do a lot of times when we're dealing with tensors and tensor flow. So essentially, there is many different shapes that can represent the same number of elements. So up here, we have three elements in a rank one tensor. And then here we have nine elements in a rank two tensor. Now there's ways that we can reshape this data so that we have the same amount of elements, but in a different shape. For example, I could flatten this, right, take all of these elements and throw them into a rank one tensor that simply is a length of nine elements. So how do we do that? Well, let me just run this code for us here and have a look at this. So what we've done is we've created tensor one, that is TF dot ones, what this stands for is we're going to create a tensor that simply is populated completely with ones of this shape. So shape one, two, three, which means, you know, that's the shape we're going to get. So let's print this out and look at tensor one, just so I can better illustrate this. So tensor one, look at the shape that we have one, two, three, right? So we have one interior list, which we're looking at here. And then we have two lists inside of that list. And then each of those lists, we have three elements. So that's the shape we just defined. Now we have six elements inside of here. So there must be a way that we can reshape this data to have six elements, but in a different shape. In fact, what we can do is reshape this into a two, three, one shape, where we're going to have two lists, right? We're going to have three inside of those. And then inside of each of those, we're going to have one element. So let's have a look at that one. So let's have a look at tensor two. Actually, what am I doing? We print all we can print all of them here. So let's just print them and have a look at them. So when we look at tensor one, we saw this was a shape. And now we look at this tensor two. And we can see that we have two lists, right? Inside of each of those lists, we have three lists. And inside of each of those lists, we have one element. Now, finally, our tensor three is a shape of three negative one. Well, what is negative one? When we put negative one here, what this does is infer what this number actually needs to be. So if we define an initial shape of three, what this does is say, Okay, we're going to have three lists. That's our first level. And then we need to figure out based on how many elements we have in this reshape, which is the method we're using, which I didn't even talk about, which we'll go into a second, what this next dimension should be. Now, obviously, this is going to need to be three. So three three, right, because we're going to have three lists inside of each of those lists we need to have. Or actually, is that correct? Let's see if that's even the shape, three, two, my bad. So this actually needs to change to three, two, I don't know why I wrote three, three there. But you get the point, right? So what this does is we have three lists, we have six elements, this number obviously needs to be two, because well, three times two is going to give us six. And that is essentially how you can determine how many elements are actually in a tensor by just looking at its shape. Now, this is the reshape method, where all we need to do is call tf dot reshape, give the tensor and give the shape we want to change it to. So long as that's a valid shape. And when we multiply all of the numbers in here, it's equal to the number of elements in this tensor that will reshape it for us and give us that new shaped data. This is very useful. We'll use this actually a lot as we go through TensorFlow. So make sure you're kind of familiar with how that works. All right. So now we're moving on to types of tensors. So there is a bunch of different types of tensors that we can use. So far, the only one we've looked at is variable. So we've created tf dot variables and kind of just hard coded our own tensors. We're not really going to do that very much. But just for that example. So we have these different types, we have constant placeholder sparse tensor variable. And there's actually a few other ones as well. Now, we're not going to really talk about these two that much, although constant and variable are important to understand the difference between. So we can read this says with the exception of variable, all of these tensors are immutable, meaning their value may not change during execution. So essentially, all of these when we create a tensor mean we have some constant value, which means that whatever we've defined here, it's not going to change. Whereas the variable tensor could change. So that's just something to keep in mind when we use variable, that's because we think we might need to change the value of that tensor later on. Whereas if we're using a constant value tensor, we cannot change it. So that's just something to keep in mind, we can obviously copy it, but we can't change it. Okay, so evaluating tensors, we're almost at the end of the section, I know, and then we'll get into some more kind of deeper code. So there will be some times for this guide, we need to evaluate a tense, of course, so what we need to do to evaluate a tensor is create a session. Now, this isn't really like, we're not going to do this that much. But I just figured I'd mention it to make sure that you guys are aware of what I'm doing. If I start kind of typing this later on. Essentially, sometimes we have some tensor object. And throughout our code, we actually need to evaluate it to be able to do something else. So to do that, all we need to do is literally just use this kind of default template, a block of code. Well, we say with TF dot session, as some kind of session doesn't really matter what we put here, then we can just do whatever the tensor name is dot eval. And calling that will actually have TensorFlow just figure out what it needs to do to find the value of this tensor, it will evaluate it, and then it will allow us to actually use that value. So I put this in here, you guys can obviously read through this if you want to understand some more in depth on how that works. And the source for this is straight from the TensorFlow website. A lot of this is straight up copied from there. And I've just kind of added my own spin to it and made it a little bit easier to understand. Okay, so we've done all that. So let's just go in here and do a few examples of reshaping just to make sure that everyone's kind of on the same page. And then we'll move on to actually talking about some simple learning algorithms. So I want to create a tensor that we can kind of mess with in reshape. So what I'm going to do is just say t equals and we'll say TF dot ones. Now what TF dot ones does is just create again, all of the values to be ones that we're going to have and whatever shape. Now we can also do zeros and zeros is just going to give us a bunch of zeros. And let's create some like crazy shape and just visualize this. Let's see like a five by five by five. So obviously, if we want to figure out how many elements are going to be in here, we need to multiply this value. So I believe this is going to be 625 because that should be five to the power of four. So five times five times five times five. And let's actually print T and have a look at that and see what this is. So we run this now. And you can see this is the output we're getting. So obviously, this is a pretty crazy looking tensor, but you get the point, right? And it tells us the shape is 55555. Now watch what happens when I reshape this tensor. So if I want to take all of these elements and flatten them out, what I could do is simply say, we'll say T equals TF dot reshape like that. And we'll reshape the tensor T to just the shape 625. Now if we do this and we run here, oops, I got a print T at the bottom after we've done that if I could spell the print statement correctly, you can see that now we just get this massive list that just has 625 zeros. And again, if we wanted to reshape this to something like 125, and maybe we weren't that good at math and couldn't figure out that this last value should be five, we could put a negative one, this would mean that TensorFlow would infer now what the shape needs to be. And now when we look at it, we can see that we're what we're going to get is well, just simply five kind of sets of these, I don't know, matrices, whatever you want to call them in our shape is 125 five. So that is essentially how that works. So that's how we reshape. That's how we kind of deal with tensors create variables, how that works in terms of sessions and graphs. And hopefully with that, that gives you enough of an understanding of tensors of shapes of ranks of value so that when we move into the next part of the tutorial, where we're actually writing code, and I promise we're going to be writing some more advanced code, you'll understand how that works. So with that being said, let's get into the next section. So welcome to module three of this course. Now what we're going to be doing in this module is learning the core machine learning algorithms that come with TensorFlow. Now these algorithms are not specific to TensorFlow, but they are used within there and we'll use some tools from TensorFlow to kind of implement them. But essentially, these are the building blocks before moving on to things like neural networks and more advanced machine learning techniques. You really need to understand how these work because they're kind of used in a lot of different techniques and combined together. And one of them but to show you is actually very powerful if you use it in the right way. A lot of what machine learning actually is in a lot of machine learning algorithms and implementations and businesses and applications and stuff like that, actually just use pretty basic models, because these models are capable of actually doing, you know, very powerful things. When you're not dealing with anything that's crazy complicated, you just need some basic machine learning, some basic classification, you can use these kind of fundamental core learning algorithms. Now the first one we're going to go through is a linear regression, but we will cover classification, clustering and hidden Markov models. And those are kind of going to give us a good spread of the different core algorithms. Now there is a ton, ton, like thousands of different machine learning algorithms. These are kind of the main categories that you'll cover. But within these categories, there is more specific algorithms that you can get into. I just feel like I need to mention that because I know a lot of you will have maybe seen some different ways of doing things in this course might show you, you know, a different perspective on that. So let me just quickly talk about how I'm going to go through this. It's very similar to before I have this notebook, as I've kind of talked about, there is a link in the description, I would recommend that you guys hit that and follow along with what I'm doing and read through the notebook, but I will just be going through the notebook. And then occasionally what I will actually do, oops, I need to open this up here is go to this kind of untitled tab I have here and write some code in here. Because most of what I'm going to do is just copy code over into here so we can see it all in kind of one block. And then we'll be good to go. And the last note before we really get into it, and I'm sorry I'm talking a lot, but it is important to make you guys aware of this, you're going to see that we use a lot of complicated syntax throughout this kind of series and the rest of the course in general. I just want to make it extremely clear that you should not have to memorize or even feel obligated to memorize any of the syntax that you see, everything that you see here, I personally don't even have memorized is a lot of what's in here that I can't just come up with on the top of my head. When we're dealing with kind of a library and modules so big that like TensorFlow, it's hard to memorize all those different components. So just make sure you understand what's happening, but you don't need to memorize it. If you're ever going to need to use any of these tools, you're going to look them up, you're going to see what it is you're going to be like, okay, I've used this before, you're going to understand it, and then you can go ahead and you know, copy that code in and use it in whatever way you need to, you don't need to memorize anything that we do. All right, so let's go ahead and get started with linear regression. So what is linear regression? What's one of those basic forms of machine learning? And essentially, what we try to do is have a linear correspondence between data points. So I'm just going to scroll down here, do a good example. So what I've done is use map plot live just to plot a little graph here. So we can see this one right here. And essentially, this is kind of our data set. This is what we'll call your data set. What we want to do is use linear regression to come up with a model that can give us some good predictions for our data points. So in this instance, maybe what we want to do is given some x value for a data point, we want to predict the y value. Now, in this case, we can see there is kind of some correspondence linearly for these data points. Now, what that means is we can draw something called a line of best fit through these data points that can kind of accurately classify them, if that makes any sense. So I'm going to scroll down here and look at what our line of best fit for this data set actually is, you can see this blue line, a pretty much, I mean, it is the perfect line of best fit for this data set. And using this line, we can actually predict future values in our data set. So essentially, linear regression is used when you have data points that correlate in kind of a linear fashion. Now, this is a very basic example, because we're doing this in two dimensions with x and y. But oftentimes, what you'll have is you'll have data points that have, you know, eight or nine kind of input values. So that gives us, you know, a nine dimensional kind of data set. What we'll do is predict one of the different values. So in the instance where we were talking about students before, maybe we have a student, what is it midterm grade, and their second midterm grade, and then we want to predict their final grade, what we can do is use linear regression to do that, where our kind of input values are going to be the two midterm grades and the output value is going to be that final grade that we're looking to predict. So if we were to plot that, we would plot that on a three dimensional graph, and we would draw a three dimensional line that would represent the line of best fit for that data set. Now, for any of you that don't know what line of best fit stands for, it says line, or this is just the definition I got from this website here, line of best fit refers to a line through a scatter plot of data points that best expresses the relationship between those points. So exactly what I've kind of been trying to explain, when we have data that correlates linearly, and I always butcher that word, what we can do is draw a line through it, and then we can use that line to predict new data points, because if that line is good, it's a good line of best fit for the data set, then hopefully we would assume that we can just, you know, pick some point, find where it would be on that line, and that'll be kind of our predicted value. So I'm going to go into an example now where I start drawing and going into a little bit of math. So we understand how this works on a deeper level. But that should give you a surface level understanding. So actually, I'll leave this up because I was messing with this beforehand. This is kind of a data set that I've drawn on here. So we have our x, and we have our y, and we have our line of best fit. Now, what I want to do is I want to use this line of best fit to predict a new data point. So all these red data points are ones that we've trained our model with their information that we gave to the model so that it could create this line of best fit. Because essentially, all linear regression really does is look at all of these data points and create a line of best fit for them. That's all it does. It's pretty, I don't know the word for it. It's pretty easy to actually do this. This algorithm is not that complicated. It's not that advanced. And that's why we start with it here, because it just makes sense to explain. So I hope that a lot of you would know in two dimensions, a line can be defined as follows. So with the equation y equals mx plus b. Now b stands for the y intercept, which means somewhere on this line. So essentially, where the line starts. So in this instance, our b value is going to be right here. So this is going to be b, because that is the y intercept. So we could say that that's like maybe, you know, we go on, we'll do this, we'll say this is like 123, we might say b is something like 0.4, right? So I could just pencil that into 0.4. And then what is mx and y? Well, x and y stand for the coordinates of this data point. So this would have, you know, some x, y value. In this case, we might call it, you know, something like, what do you want to say to 2.7, that might be the value of this data point. So that's our x and y. And then our m stands for the slope, which is probably the most important part. Now slope simply defines the steepness of this line of best fit that we've done here. Now the way we calculate slope is using rise over run. Now rise over run essentially just means how much we went up versus how much we went across. So if you want to calculate the slope of a line, what you can actually do is just draw a triangle. So a right angle triangle anywhere on the line. So just pick two data points. And what you can do is calculate this distance, and this distance. And then you can simply divide the distance up by the distance across. And that gives you the slope. I'm not going to go too far into slope because I feel like you guys probably understand what that is. But let's just pick some values for this line. And I want to actually show you some real examples of math and how we're going to do this. So let's say that our linear regression algorithm, you know, comes up with this line, I'm not going to discuss really how it does that, although it just pretty much looks at all these data points, and finds a line that you know, goes, it splits these data points evenly. So essentially, you want to be as close to every data point as possible. And you want to have as many data points, you want to have like the same amount of data points on the left side and the right side of the line. So in this example, we have, you know, a data point on the left, a data point on the left, we have what two that are pretty much on the line. And then we have two that are on the right. So this is a pretty good line of best fit, because all of the points are very close to the line. And they split them evenly. So that's kind of how you come up with a line of best fit. So let's say that the equation for this line is something like y equals, let's just give it 1.5 and x plus and let's say that value is just 0.5 to make it easy. So this is going to be the equation of our line. Now notice that x and y don't have a value, that's because we need to give the value to come up with one of the other ones. So what we can do is we can say if we have either the y value or we have the x value of some point, and we want to figure out, you know, where it is on the line, what we can do is just feed one in, do a calculation, and that will actually give us the other value. So in this instance, let's say that you know, I'm trying to predict something and I'm given the that the fact that x equals two, I know that x equals two, and I want to figure out what y would be if x equals two. Well, I can use this line to do so. So what I would do is I'm going to say y equals 1.5 times two plus 0.5. Now, all of you quick math majors out there give me the value of 3.5, which means that if x was at two, then I would have my data point as a prediction here on this line. And I would say, okay, so if you're telling me x is two, my prediction is that y is going to be equal to 3.5, because given the line of best fit for this data set, that's where this point will lie on that line. So I hope that makes sense. You can actually do this the reverse way as well. So if I'm just given some y values to say, I know that, you know, my y value is at like 2.7 or something, I can plug that in, just rearrange the numbers in this equation and then solve for x. Now, obviously, this is a very basic example, because we're just doing all of this in two dimensions. But you can do this in higher dimensions as well. So actually, most times, what's going to end up happening is you're going to have, you know, like eight or nine input variables. And then you're going to have one output variable that you're predicting. Now, so long as our data points are correlated linearly in three dimensions, we can still do this. So I'm going to attempt to show you this actually, in three dimensions, just to hopefully clear some things up, because it is important to kind of get a grasp and perspective of the different dimensions. So let's say we have a bunch of data points that are kind of like this, now I'm trying my best to kind of draw them in some linear fashion using like all the dimensions here. But it is hard because drawing in three dimensions on a two dimensional screen is not easy. Okay, so let's say this is kind of like what our data points look like. Now, I would say that these correlate linearly, like pretty, pretty well, they kind of go up in one fashion, and we don't know the scale of this. So this is probably fun. So the line of best fit for this data set, and I'll just put my kind of thickness up might be something like this, right? Now notice that this line is in three dimensions, right? This is going to cross our, I guess this is our x, y, and z axes. So we have a three dimensional line. Now the equation for this line is a little bit more complicated. I'm not going to talk about exactly what it is. But essentially what we do is we make this line, and then we say, Okay, what value do I want to predict? Do I want to predict y, x or z? Now, so long as I have two values, so two values, I can always predict the other one. So if I have, you know, the x, y of the data point, that will give me the z. And if I have the z, y, that will give me the x. So so long as you have, you know, all of the data points, except one, you can always find what that point is, based on the fact that, you know, we have this line, and we're using that to predict. So I think I'm going to leave it at that for the explanation. I hope that makes sense. Again, just understand that we use linear regression when our data points are correlated linearly. Now some good examples of linear regression were, you know, that kind of student predicting the grade kind of thing, you would assume that if someone has, you know, a low grade, then they would finish with a lower grade, and you would assume if they have a higher grade, they would finish with a higher grade. Now you could also do something like predicting, you know, future life expectancy. Now this is kind of a darker example. But essentially, what you could think of here is if someone is older, they're expected to live, you know, like not as long. Or you could look at health conditions, if someone is in critical illness condition, they have a critical illness, then chances are their life expectancy is lower. So that's an example of something that is correlated linearly. Essentially, something goes up, and something goes down, or something goes up, the other thing goes up. That's kind of what you need to think of when you think of a linear correlation. Now the magnitude of that correlation, so you know, how much does one go up versus how much one goes down is exactly what our algorithm figures out for us, we just need to know to pick linear regression when we think things are going to be correlated in that sense. Okay, so that is enough of the explanation of linear regression. Now we're going to get into actually coding and creating a model. But we first need to talk about the data set that we're going to use in the example we're going to kind of illustrate linear regression with. Okay, so I'm here and I'm back in the notebook. Now these are the imports we need to start with to actually start programming and getting some stuff done. Now the first thing we need to do is actually install SK learn. Now even if you're in a notebook, you actually need to do this because for some reason it doesn't come by default with the notebook. So to do this, we just did an exclamation point, pip install hyphen q SK learn. Now if you're going to be working on your own machine, again, you can use pip to install this. And I'm assuming that you know to use pip if you're going to be going along in that direction. Now as before, since we're in the notebook, we need to define we're going to use TensorFlow version to point x. So to do that, we're going to just, you know, do that up here with the percent sign. And then we have all these imports, which we're going to be using throughout here. So from future import, absolutely import division, print function, Unicode literals, and then obviously the big one. So NumPy, pandas, map plot lib, we're gonna be using I Python, we're gonna be using TensorFlow. And yeah, so I'm actually just gonna explain what some of these modules are, because I feel like some of you may actually not know. NumPy is essentially a very optimized version of arrays in Python. So what this allows us to do is lots of kind of multi dimensional calculations. So essentially if you have a multi dimensional array, which we've talked about before, right when we had, you know, those crazy shapes like 5555, NumPy allows us to represent data in that form, and then very quickly manipulate and perform operations on it. So we can do things like cross product, dot product, matrix addition, matrix subtraction, element wise addition, subtraction, you know, vector operations, that's what this does for us. It's pretty complex, but we're going to be using it a fair amount. Pandas. Now what pandas does is it's kind of a data analytics tool, I almost want to say, I don't know the formal definition of what pandas is. But it allows us to very easily manipulate data. So you know, load in data sets, view data sets, cut off specific columns or cut out rows from our data sets, visualize the data sets. That's what pandas does for us. Now map plot lib is actually a visualization of kind of graphs and charts. So we'll use that a little bit lower when I actually graph some different aspects of our data set. The IPython display, this is just specific for this notebook, it's just to clear the output, there's nothing crazy with that. And then obviously, we know what TensorFlow is, this crazy import for TensorFlow here. So compact v to feature column as FC, we'll talk about later, but we need something called a feature column when we create a linear regression algorithm or model and TensorFlow. So we're going to use that. Okay. So now that we've gone through all that, we need to start talking about the data set that we're going to use for linear regression. And for this example, because what we're going to do is, you know, actually create this model, and start using it to predict values. So the data set that we're going to use, actually, I need to read this, because I forget exactly what the name of it is, is the Titanic data set, that's what it is. So essentially, what this does is aim to predict who's going to survive or the likelihood that someone will survive, being on the Titanic, given a bunch of information. So what we need to do is load in this data set. Now, I know this seems like a bunch of gibberish, but this is how we need to load it. So we're going to use pandas. So PD dot read CSV from this URL. So what this is going to do is take this CSV file, which stands for comma, separated values. And we can actually look at this if we want, I think so I said, it said control click, let's see if this pops up. So let's actually download this. And let's open this up ourselves and have a look at what it is in Excel. So I'm going to bring this up here. You can see that link. And this is what our data set is. So we have our columns, which just stand for, you know, what is it the different attributes in our data set of the different features and labels of our data set, we have survived. So this is what we're actually going to be aiming to predict. So we're going to call this our label, right, or our output information. So here, a zero stands for the fact that someone did not survive. And one stands for the fact that someone did survive. Now, just thinking about it on your own for a second, and looking at some of the categories we have up here, can you think about why linear regression would be a good algorithm for something like this? Well, for example, if someone is a female, we can kind of assume that they're going to have a higher chance of surviving on the Titanic, just because of, you know, the kind of the way that our culture works, you know, saving women and children first, right? And if we look through this data set, we'll see that when we see females, it's pretty rare that they don't survive. Although as I go through, there is quite a few that didn't survive. But if we look at it compared to males, you know, there's definitely strong correlation that being a female results in a stronger survival rate. Now, if we look at age, right, can we think of how age might affect this? Well, I would assume if someone's way younger, they probably have a higher chance of surviving, because they would be, you know, prioritized in terms of lifeboats or whatever it was. I don't know much about the Titanic. So I can't talk about that specifically. But I'm just trying to go through the categories and explain to you why we picked this algorithm. Now, number of siblings, that one might not be as, you know, influential, in my opinion, parched. I don't actually remember what parched stands for. I think it is like what parched, I don't know exactly what this column stands for. So unfortunately, I can't tell you guys that one. But we'll talk about some more of the second fair. Again, not exactly sure what fair stands for. I'm going to look on the TensorFlow website after this and get back to you guys. And we have a class. So class is what class they were on the boat, right? So first class, second class, third class. So you might think someone that's in a higher class might have a higher chance of surviving. We have decks, this is what deck they were on when it crashed. So unknown is pretty common. And then we have all these other decks, you know, if someone got hit, if someone was standing on the deck that had the initial impact, we might assume that they would have a lower chance of survival. embark to is where they were going. And then are they alone? Yes or no. And this one, you know, this is interesting, we're going to see does this make an effect? If someone is alone, is that a higher chance of survival? Is that a lower chance of survival? So this is kind of interesting. And this is what I want you guys to think about is that when we have information and data like this, we don't necessarily know what correlations there might be. But we can kind of assume there's some linear thing that we're looking for some kind of pattern, right? Whereas if something is true, then you know, maybe it's more likely someone will survive. Whereas like, if they're not alone, maybe it's less likely. And maybe there's no correlation whatsoever. But that's where we're going to find out as we do this model. So let me look actually, on the TensorFlow website and see if I can remember what parched and I guess what fair was. So let's go up to the top here. Again, a lot of this stuff is just straight up copied from the TensorFlow website. I've just added my own stuff to it. You can see like, I just copied all this, we're just bringing it in there. Let's see what it says about the different columns, if it gives us any exact explanations. Okay, so I couldn't find what parts were fair stands for. For some reason, it's not on the TensorFlow website, either. I couldn't really find any information about it. If you guys know, you know, leave a comment down below, but it's not that important, we just want to use this data to do a test. So what I've done here, if I've loaded in my data set, and notice that I've loaded a training data set in a testing data set. Now we'll talk about this more later. This is important, I have two different data sets, one to train the model with, and one to test the model with. Now kind of the basic reason we would do this is because when we test our model for accuracy to see how well it's doing, it doesn't make sense to test it on data, it's already seen, it needs to see fresh data, so we can make sure there's no bias, and it hasn't simply just memorize the data, you know, that we had. Now what I'm doing here, at the bottom with this y train in this y eval, is I'm essentially popping a column off of this data set. So if I print out the data set here, and I'm actually I'm going to show you a cool trick with pandas that we can use to look at this. So I can say D F train dot head. So if I look at this, by just looking at the head, and we'll print this out, oh, I might need to import some stuff above. We'll see if this works or not. Yeah, so I need to just do these imports. So let's install. And let's do these imports. I'll wait for the surrounding. Okay, so I've just selected TensorFlow 2.0. We're just importing this now should be done in one second. And now what we'll do is we'll print out the the data frame here. So essentially what this does is load this into a pandas data frame. This is a specific type of object. Now we're not going to go into this specifically, but a data frame allows us to view a lot of different aspects about the data and kind of store it in a nice form, as opposed to just loading it in and storing it in like a list or a NumPy array, which we might do if we didn't know how to use pandas. This is a really nice way to do it read CSV, load it into a data frame object, which actually means we can reference specific columns and specific rows in the data frame. So let's run this and just have a look at it. Yeah, I got need to print dftrain.head. So let's do that. And there we go. So this is what our data frame head looks like. Now head what that does is show us the first five entries in our data set, as well as show us a lot of the different columns that are in it. Now since we have more than you know, we have a few different columns, it's not showing us all of them, it's just giving us the dot dot dot. But we can see this is what the data frame looks like. And this is kind of the representation internally. So we have entry zero, survived zero, survived one, we have male, female, all that. Now notice that this has the survived column. Okay. Because what I'm going to do is I'm going to print the data frame head again. So dftrain.head after we run these two lines. Now what this line does is takes this entire survived column, so all these zeros and ones, and removes it from this data frame, so the head data frame, and stores it in the variable y train. The reason we need to do that is because we need to separate the data, we're going to be classifying from the data that is kind of our input information or our initial data set, right? So since we're looking for the survived information, we're going to put that in its own, you know, kind of variable store here. Now we'll do the same thing for the evaluation data set, which is DF evaluation or testing data. And notice that here this was trained on CSV, and this one was eval.csv. Now these have the exact same form, they look the like completely identical, it's just that you know, some entries, we've just kind of arbitrarily split them. So we're going to have a lot of entries in this training set, and we'll have a few in the testing set that we'll just use to do an evaluation on the model later on. So we pop them off by doing this pop removes and returns this column. So if I print out why train, which are actually let's look at this one first, just to show you how it's been removed, we can see that we have the survived column here, we popped and now the survived column is removed from that data set. So that's just important to understand. Now we can print out some other stuff too. So we can look at the why train and see what that is. Just to make sure we really understand this data. So let's look at why train. And you can see that we have 626 or 627 entries and address, you know, zeros or ones representing whether someone survived or whether they did not. Now the corresponding indexes in this kind of list or data frame correspond to the indexes in the testing and training data frame. What I mean by that is, you know, entry zero in this specific data frame corresponds to entry zero in our why train variable. So if someone survived, you know, at entry zero, it would say one here, right? Or in this case, entry zero did not survive. Now, I hope that's clear. I hope I'm not confusing you with that. But I just want to show one more example to make sure. So we'll say D F train zero, I'm going to print that and I'm going to print why train at index zero. Oops, if I didn't mess up my brackets, and we'll have a look at it. Okay, so I've just looked up the documentation because I totally forgot that I couldn't do that. If I want to find one specific row in my data frame, what I can do is print dot loc. So I do my data frame and then dot loc and then whatever index I want. So in this case, I'm locating row zero, which is this. And then on the why train, I'm doing the same thing, I'm locating row zero. Now what I had before, right, if I did D F train, and I put square brackets inside here, what I can actually do is reference a specific column. So if I wanted to look at, you know, say the column for age, right, so we have a column for age, what I can do is do D F train age. And then I can print this out like this. And it gives me all of the different age values. So that's kind of how we use a data frame, we'll see that as we go further on. Now let's go back to the other example I had, because I just erased it, where I want to show you the row zero in the data frame that's training, and then in the why train, you know, output, whatever that is. So the survival. So you can see here that this is what we get from printing D F train loc zero. So row zero, this is all the information. And then here, this corresponds to the fact that they did not survive at row zero, because it's simply just the output is value zero. Now I know this is weird, it's saying like name, zero, d type, object, zero, don't worry about that. It's just because it's trying to print it with some information. But essentially, this just means this person who was male 22 and had one sibling did not survive. Okay, so let's get out of this. Now we can close this and let's go to Oh, we've pretty much already done what I've just have down here. But we can look at the data frame head. This is a little bit of a nicer output when we just have D F train dot head, we can see that we get kind of a nice outputted little graph. We've already looked at this information. So we know kind of some of the attributes of the data set. Now we want to describe the data set sometimes. What describe does is just give us some overall information. So let's have a look at it here. We can see that we have 627 entries, the mean of age is 29, the standard deviation is you know, 12 point whatever. And then we get the same information about all of these other different attributes. So for example, it gives us you know, the mean fair, the minimum fair, and just some statistics, because understand this great, if you don't doesn't really matter. The important thing to look at typically is just how many entries we have is sometimes we need that information. And sometimes the mean can be helpful as well, because you can kind of get an average of like what the average value is in the data set. So if there's any bias later on, you can figure that out. But it's not crazy important. Okay, so let's have a look at the shape. So just like NumPy arrays and tensors have a shape attribute, so do data frames. So we want to look at the shape, you know, we can just print out D F train dot shape, we get 627 by nine, which essentially means we have 627 rows, and nine columns or nine attributes. So yeah, that's what it says here, you know, 627 entries, nine features, we can interchange attributes and features. And we can look at the head information for why so we can see that here, which we've already looked at before. And that gives us the name, which was survived. Okay, so now what we can actually do is make some kind of graphs about this data. Now I've just stolen this code, you know, straight up from the TensorFlow website, I wouldn't expect you guys to do any of this, you know, like output any of these values. What we're going to do is create a few histograms and some plots just to look at kind of some correlations in the data. So that when we start creating this model, we have some intuition on what we might expect. So let's look at age. So this gives us a histogram of the age. So we can see that there's about 25 people that are kind of between zero and five. There is you know, maybe like five people that are in between five and 10. And then the most amount of people are kind of in between their 20s and 30s. So in the mid 20s, this is good information to know, because that's going to introduce a little bit of bias into kind of our linear correlation graph, right? So just understanding, you know, that we have like a large subset, there's some outliers here, like there's one person that's 80, right over here, a few people that are 70, some important things to kind of understand before we move on to the algorithm. So let's look at the sex values now. So this is how many female and how many male, we can see that there's many more males than there is females. We can have a look at the class. So we can see if they're in first, second or third class, most people are in third, then followed by first and then second. And then lastly, we can look at what is this that we're doing? Oh, the percentage survival by sex. So we can see how likely a specific person or a specific sex is to survive just by plotting this. So we can see that males have about a 20% survival rate, whereas females are all the way up to about 78%. So that's important to understand that kind of confirms that what we were looking at before in the data set when we were exploring it. And you don't need to do this every time that you're looking at a data set, but it is good to kind of get some intuition about it. So this is what we've learned so far, majority passengers are in their 20s or 30s, then majority passengers are male, they're in third class, and females have a much higher chance of survival, kind of already knew that. Alright, so training and testing data sets. Now we already kind of went through this all skim through it quickly. Essentially, what we did above is load in two different data sets. The first data set was that training data set which had the shape of 627 by nine. What I'm actually going to do is create a code block here, and just have a look at what was this Df eval dot shape to show you how many entries we have in here. So here in our testing data set, you can see we have significantly less at 264 entries, or rows, whatever you want to call them. So that's how many things we have to actually test our model. So what we do is we use that training data to create the model, and then the testing data to evaluate it and make sure that it's working properly. So these things are important whenever we're doing machine learning models, we typically have testing and training data. And yeah, that is pretty much it. Now I'm just going to take one second to copy over a lot of this code into the kind of other notebook I have, just so we can see all of it at once. And then I'll be back and we'll get into actually making the model. Okay, so I've copied in some code here. I know this seems like a lot of kind of gibberish right now, but I'm going to break down line by line what all this is doing and why we have this here. But we first need to discuss something called feature columns and the difference between categorical and numeric data. So categorical data is actually fairly common. Now when we're looking at our data set, and actually I can open I don't have it open in Excel anymore, but let's open this from my downloads. So let's go downloads. Where is this train? Okay, awesome. So we have this Excel data sheet here. And we can see what a categorical data or what categorical data is is something that's not numeric. So for example, unknown, C first, third, city, and why right so anything that has different categories, there's going to be like a specific set of different categories there could be. So for example, for age, kind of the set of values we get out for age, well, this is numeric. So that's different. But for categorical, we can have male or we can have female. And I suppose we could have other but in this data set, we just have male and we just have female. For class, we can have first, second, third. For deck, we can have unknown CA, I'm sure through all the letters of the alphabet, but that is still considered categorical. Now, what do we do with categorical data? Well, we always need to transform this data into numbers somehow. So what we actually end up doing is we encode this data using an integer value. So for the example of male and female, what we might say, and this is what we're going to do in a second is that female is represented by zero and male is represented by one. We do this because although it's interesting to know what the actual class is, the model doesn't care, right, female and male, it doesn't make a difference to it. It just needs to know that those values are the different are different or those values are the same. So rather than using strings and trying to find some way to pass that in and do math with that, we need to turn those into integers, we turn those into zeros and ones, right? Now for class, right, so first, second, third, you know, you guys can probably assume what we're going to encode this with, we're going to encode it with zero, one, two. Now, again, this doesn't necessarily need to be in order. So third could be represented by one and first could be represented by two, right? It doesn't need to be in order, it doesn't matter. So long as every third has the same number, every first has the same number and every second has the same number. And then same thing with that same thing with embark and same thing with alone. Now we could have an instance where you know, we've encoded every single one of these values with a different value. So in the, you know, rare occasion where there's one category that's categorical, and every single value in that category is different, then we will have, you know, 627 in this instance, different encoding labels that are going to be numbers, that's fine, we can do that. And actually, we don't really need to do that, because you're going to see how TensorFlow can handle that for us. So that is categorical data, numeric columns are pretty straightforward. They're anything that just have integer or float values already. So in this case, age and fair. And yeah, so that's what we've done. We've just defined our categorical columns here, and our numeric columns here. This is important because we're going to loop through them, which we're doing here to create something called feature columns, feature columns are nothing special. They're just what we need to feed to our linear estimator or linear model to actually make predictions. So kind of our steps here that we've gone through so far is import, load the data set, explore the data set, make sure we understand it, create our categorical columns and our numeric columns. So I've just hard coded these in right like sex, parts, class, deck, alone, all these ones. And then same thing with the numeric columns. And then for a linear estimator, we need to create these as feature columns using some kind of advanced syntax, which we're going to look at here. So we create a blank list, which is our feature columns, which will just store our different feature columns. We loop through each feature name in the categorical columns. And what we do is we define a vocabulary, which is equal to the data frame at that feature name. So first, we would start with sex, then we go and siblings, then we go parts, then we go class. And we get all of the different unique values. So that's actually what this does dot unique, gets a list of all unique values from the feature call. And I can print this out. She'll print this in a different line, we'll just take this value and have a look at actually what this is, right? So if I run, I guess we'll have to run all these in order. And then we'll create a new code block while we wait for that to happen. Let's see if we can get this installing fast enough. Run, run, run. Okay, now we go to df train. And we can see this is what this looks like. So these are all the different unique values that we had in that specific feature name. Now that feature name was what categorical columns? Oh, what I do feature name up, sorry, that's going to be the unique one. Let's just put rather than feature name. Let's put sex, right? And let's have a look at what this is. So we can see that the two unique values are male and female. Now I actually want to do what is it embark town? And I want to see what this one is. So how many different values we have. So we'll copy that in. And we can see we have Southampton cannot pronounce that and then the other cities and unknown. And that is kind of how we get the unique value. So that's what that method is doing there. Let's actually delete this code block because we don't need anymore. Alright, so that's what we do. And then what we do down here is we say feature columns dot append. So just add to this list, the TensorFlow feature column dot categorical column with vocabulary list. Now I know this is a mouthful, but this is kind of something again, you're just going to look up when you need to use it, right? So understand you need to make feature columns for linear regression, you don't really need to completely understand how, but you just need to know that that's something you need to do. And then you can look up the syntax and understand. So this is what this does. This is actually going to create for us a column, it's going to be in the form of a like NumPy array kind of that has the feature name. So whatever one we've looped through, and then all the different vocabulary associated with it. Now we need this because we just need to create this column so that we can create our model using those different columns, if that makes any sense. So our linear model needs to have, you know, all the different columns we're going to use, it needs to know all of the different entries that could be in that column, and needs to know whether this is a categorical column or a numeric column. In previous examples, what we might have done is actually change the data set manually, so encoded it manually. TensorFlow just can do this for us now in TensorFlow 2.0. So we'll just use that too. Okay, so that's what we did with these feature columns. Now for the numeric columns, a little bit different. It's actually easier. All we need to do is give the feature name and whatever the data type is and create a column with that. So notice we don't, we can omit this unique value, because we know when it's numeric, that you know, there could be an infinite amount of values. And then I've just printed out the feature columns, you can see what this looks like. So vocabulary list, categorical column gives us the number of siblings. And then the vocabulary list is these are all the different encoding values that is created. And then same thing, you know, we can go down here, parts, these are different encodings. So they're not necessarily in order is like what I was talking about before. Let's go do a numeric one. What do we have here? Yeah, so for a numeric column, just as is the key, that's the shape we're expecting. And this is the data type. So that is pretty much it. We're actually loading these in. So now it's almost time to create the model. So what we're going to do to create the model now is talk about first the training process and training some kind of, you know, machine learning model. Okay, so the training process. Now the training process of our model is actually fairly simple, at least for a linear model. Now, the way that we train the model is we feed it information, right? So we feed it that those data points from our data set. But how do we do that? Right? Like how do we feed that to the model? Do we just give it all at once? Well, in our case, we only have 627 rows, which isn't really that much data, like we can fit that in RAM in our computer, right? But what if we're training a crazy machine learning model, and we have, you know, 25 terabytes of data that we need to pass it, we can't load that into RAM, at least I don't know any RAM that's that large. So we need to find a way that we can kind of load it in what's called batches. So the way that we actually load this model is we load it in batches. Now we don't need to understand really kind of how this process works and how batching kind of occurs. What we do is give 32 entries at once to the model. Now the reason we don't just feed one at a time is because that's a lot slower, we can load, you know, a small batch size of 32, that can increase our speed dramatically. And that's kind of a lower level understanding. So I'm not going to go too far into that. Now that we understand, we kind of load it in batches, right? So we don't load it entirely all at once, we just load a specific set of kind of elements as we go. What we have is called epochs. Now what are epochs? Well, epochs are essentially how many times the model is going to see the same data. So we might be the case, right? And when we pass the data to our model, the first time, it's pretty bad, like it looks at the model creates our line of best fit, but it's not great, it's not working perfectly. So we need to use something called an epoch, which means we're just going to feed the model, feed the data again, but in a different order. So we do this multiple times, so that the model will look at a data, look at the data in a different way, then kind of a different form, and see the same data a few different times and pick up on patterns. Because the first time it sees a new data point, it's probably not going to have a good idea how to make a prediction for that. So if we can feed it more and more and more, then you know, we can get a better prediction. Now, this is where we talk about something called overfitting, though, sometimes we can see the data too much, we can pass too much data to our model, to the point where it just straight up memorizes those data points. And it's, it's really good at classifying for those data points. But when we pass it some new data points, like our testing data, for example, it's horrible at kind of, you know, classifying those. So what we do to kind of prevent this from happening, is we just make sure that we start with like a lower amount of epochs, and then we can work our way up and kind of incrementally change that if we need to, you know, go higher, right, we need more epochs. So yeah, so that's kind of it for epochs. Now, I will say that this training process kind of applies to all the different, what is it, machine learning models that we're going to look at, we have epochs, we have batches, we have a batch size, and now we have something called an input function. Now, this is pretty complicated. This is the code for the input function. I don't like that we need to do this. But it's necessary. So essentially, what an input function is is the way that we define how our data is going to be broke into epochs and into batches to feed to our model. Now, these, you probably aren't ever going to really need to code like from scratch by yourself. But this is the one I've just stolen from the TensorFlow website, pretty much like everything else that's in the series. And what this does is it takes our data and encodes it in a tf dot data dot data set object. Now, this is because our model needs this specific object to be able to work, it needs to see a data set object to be able to use that data to create the model. So what we need to do is take this pandas data frame, we need to turn it into that object. And the way we do that is with the input function. So we can see that what this is doing here. So this is make input function, we actually have a function defined inside of another function. I know this is kind of complicated for some of you guys. But and what I'm actually gonna do, sorry, I'm gonna just copy this into the other page, because I think it's easier to explain without all the text around. So let's create a new code block. Let's paste this in. And let's have a look at what this does. So actually, let me just tab down. Okay, so make input function, we have our parameters data data frame, which is our pandas data frame, our label data frame, which stands for those labels. So that y train or that eval y eval, right, we have a number of epochs, which is how many epochs we're going to do, we set the default 10 shuffle, which means are we going to shuffle our data and mix it up before we pass it to the model, and batch size, which is how many elements are we going to give to the data to the model? Well, it's training at once. Now, what this does is we have an input function defined inside of this function. And we say data set equals tensor frame dot data dot data set from tensor slices, dict data frame, a label data frame. Now, what this does, and we can read the comment, I mean, create a tf dot data dot data set object with the data and its label. Now, I can't explain to you like how this works on a lower level. But essentially, we pass a dictionary representation of our data frame, which is whatever we passed in here. And then we pass the label data frame, which is going to be, you know, all those y values. And we create this object. And that's what this line of code does. So tf data dot data set from tensor slices, which is just what you're going to use. I mean, we can read this documentation, create a data set whose elements are slices of the given tensors, the given tensors are sliced along their first dimension, this operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the data set dimension. So I mean, you guys can look at that, like read through the documentation, if you want. But essentially, what it does is create the data set object for us. Now, if shuffle DS equals DS dot shuffle 1000, what this does is just shuffle the data set, you don't really need to understand more than that. And then what we do is we say data set equals data set dot batch, the batch size, which is going to be 32, and then repeat for the number of epochs. So what this is going to do is essentially take our data set and split it into the number of, I don't want to, what do I want to call it? Like blocks that are going to be passed to our model. So we can do this by knowing the batch size, it obviously knows how many elements because that's the data set object itself, and then repeat number of epochs. So this can figure out, you know, how many one how many blocks do I need to split it into to feed it to my model. Now return data set, simply from this function here, we'll return that data set object. And then on the outside return, we actually return this function. So what this out exterior function does, and I'm really just trying to break this down. So you guys understand is make an input function, it literally makes a function and returns the function object to wherever we call it from. So that's how that works. Now we have a train input function and an eval input function. And what we need to do to create these, it's just use this function that we've defined above. So we say make input function, df train, y train, so our data frame for training and our data frame for the labels of that. So we can see the comment, you know, here, we will call the input function, right? And then eval train, so it's going to be the same thing, except for the evaluation, we don't need to shuffle the data because we're not training it, we only need one epoch. Because again, we're just training it. And we'll pass the evaluation data set and the evaluation value from Y. Okay, so that's it for making the input function. Now I know this is complicated, but that's the way we have to do it. And unfortunately, if you don't understand after that, there's not much more I can do, you might just have to read through some of the documentation. Alright, creating the model, we are finally here, I know this has been a while, but I need to get through everything. So linear estimate, so we're going to copy this, and I'm just going to put it in here and we'll talk about what this does. So linear underscore EST equals tf dot estimator dot linear classifier, and we're giving it the feature columns that we created up here. So this work was not for nothing, we have this feature column, which defines, you know, what is in every single, like what should we expect for our input data, we pass that to a linear classifier object from the estimator module from TensorFlow. And then that creates the model for us. Now, this again is syntax that you don't need to memorize, you just need to understand how it works. What we're doing is creating an estimator, all of these kind of core learning algorithms use what's called estimators, which are just basic implementations of algorithms and TensorFlow. And again, pass the feature columns. That's how that works. Alright, so now let's go to training the model. Okay, so I'm just going to copy this again, I know you guys think I'm just copying the code back and forth, but I'm not going to memorize the syntax, I just want to explain to you how all this works. And again, you guys will have all this code, you can mess with it, play with it, and learn on your own that way. So to train is really easy. All we need to do, I say linear EST dot train, and then just give that input function. So that input function that we created up here, right, which was returned from make input function, like this train input function here is actually equal to a function, it's equal to a function object itself. If I were to call train underscore input function like this, this would actually call this function. That's how this works in Python. It's a little bit of a complicated syntax, but that's how it works. We pass that function here. And then this will use the function to grab all of the input that we need and train the model. Now the result is going to be rather than trained, we're going to evaluate right and notice that we didn't store this one in a variable, but we're storing the result in a variable so that we can look at it. Now clear output is just from what we import above just going to clear the console output, because there will be some output while we're training. And then we can present print the accuracy of this model. So let's actually run this and see how this works. This will take a second. So I'll be back once this is done. Okay, so we're back, and we've got a 73.8% accuracy. So essentially what we've done right is we've trained the model, you might have seen a bunch of output while you were doing this on your screen. And then we printed out the accuracy after evaluating the model. This accuracy isn't very good. But for our first shot, this okay, we're going to talk about how to improve this in a second. Okay, so we've evaluated the data set, we stored that in result. I want to actually look at what result is, because obviously you can see we've referenced the accuracy part, like you know, as if this was a Python dictionary. So let's run this one more time. Oh, this is going to take a second again. So okay, so we printed out result here, and we can see that we have actually a bunch of different values. So we have accuracy, accuracy baseline, AUC, and all these different kinds of statistical values. Now, these aren't really going to mean much to you guys, but I just want to show you that we do have those statistics and to access any specific one, this is really just a dictionary object. So we can just reference the key that we want, which is what we did with accuracy. Now, notice our accuracy actually changed here. We went to 76. The reason for this is like I said, you know, our data is getting shuffled, it's getting put in a different order. And based on the order in which we see data, our model will, you know, make different predictions and be trained differently. So if we had, you know, another epoch, right, if I change epochs to say 11 or 15, our accuracy will change. Now it might go up, it might go down. That's something we have to play with as you know, our machine, a machine learning developer, right? That's what your goal is, is to get the most accurate model. Okay, so now it's time to actually use the model to make predictions. So up until this point, we've just been doing a lot of work to understand how to create the model, you know, what the model is, how we make an input function, training, testing data, I know a lot, a lot, a lot of stuff. Now to actually use this model and like make accurate predictions with it is somewhat difficult, but I'm going to show you how. So essentially, TensorFlow models are built to make predictions on a lot of things at once. They're not great at making predictions on like one piece of data, you just want like one passenger to make a prediction for, they're much better at working in like large batches of data. Now you can definitely do it with one, but I'm just going to show you how we can make a prediction for every single point that's in that evaluation data set. So right now we looked at the accuracy, and the way we determine the accuracy was by essentially comparing the results that the predictions gave from our model versus what the actual results were for every single one of those passengers. And that's how we came up with an accuracy of 76%. Now if we want to actually check and get predictions from the model and see what those actual predictions are, what we can do is use a method called dot predict. So what I'm going to do is I'm going to say, I guess, results like this equals, and in this case, we're going to do the model name, which is linear EST dot predict. And then inside here, what we're going to pass is that input function we use for the evaluation. So just like, you know, we need to pass an input function to actually train the model, we also need to pass an input function to make a prediction. Now this input function could be a little bit different, we can modify this a bit if we wanted to, but to keep things simple, we use the same one for now. So what I'm going to do is just use this eval input function. So the one we've already created where we did, you know, one epoch, we don't need to shuffle because it's just the evaluation set. So inside here, we're going to eval input function. Now what we need to do though is convert this to a list, just because we're going to loop through it. And I'm actually going to print out this value so we can see what it is before we go to the next step. So let's run this and have a look at what we get. Okay, so we get logistics array, we can see all these different values. So we have, you know, this array with this value, we have probabilities, this value. And this is kind of what we're getting. So we're getting logistic, all classes, like there's all this random stuff. What you hopefully should notice, and I know I'm just like whizzing through is that we have a dictionary that represents the predictions. And I'll see if I can find the end of the dictionary here for every single, what is it prediction? So since we've passed, you know, 267 input data from this, you know, eval input function, what was returned to us is a list of all of these different dictionaries that represent each prediction. So what we need to do is look at each dictionary so that we can determine what the actual prediction was. So what I'm going to do is actually just present to result. I'm wondering to result zero, because this is a list. So that should mean we can index it. So we can actually look at one prediction. Okay, so this is the dictionary of one prediction. So I know this seems like a lot. But this is what we have. This is our prediction. So logistics, we get some array, we have logistic in here in this dictionary, and then we have probabilities. So what I actually want is probability. Now, since what we ended up having was a prediction of two classes, right, either zero or one, we're predicting either someone survived, or they didn't survive, or what their percentage should be. We can see that the percentage of survival here is actually 96%. And the percentage that it thinks that it won't survive is, you know, 3.3%. So if we want to access this, what we need to do is click do result at some index. So whatever, you know, one we want. So we're gonna say result. And then here, we're going to put probabilities. So I'm just going to print that like that. And then we can see the probabilities. So let's run this. And now we see our probabilities are 96 and 33. Now, if we want the probability of survival, so I think I actually might have messed this up, I'm pretty sure the survival probability is actually the last one, whereas like the non survival is the first one, because zero means you didn't survive and one means you did survive. So that's my bad, I messed that up. So I actually want their chance of survival, I'll index one. So if I index one, you see we get 3.3%. But if I wanted their chance of survival, not surviving, I would index zero. And that makes sense because zero is you know, what we're looking like zero represents they didn't survive, whereas one represents they did survive. So that's kind of how we do that. So that's how we get them. Now, if we wanted to loop through all of these, we could we could loop through every dictionary, we could print every single probability of each person. We can also look at that person's stats and then look at their probability. So let's see the probability of surviving is in this case, you know, 3%, or whatever it was 3.3%. But let's look at the person that we were actually predicting them and see if that makes sense. So if I go eval, or what was it df eval dot loc, zero, we print that and then we print the result. What we can see is that for the person who was male and 35 that had no siblings, their fair was this, they're in third class, we don't know what deck they were on and they were alone. They have a 3.3% chance of survive. Now, if we change this, we could go like to to let's have a look at this second person and see what their chance of survival is. Okay, so they have a higher percent chance of 38% chance they're female, they're a little bit older. So that might be a reason why their survival rates a bit lower. I mean, we can keep doing this and look through and see what it is, right? If we want to get the actual value, like if this person survived, or if they didn't survive, and what I can do is I can print df eval. Actually, it's not going to be eval, it's going to be y underscore eval. Yeah. And that's going to be loc three. Now this will give us if they survived or not. So actually, in this case, that person did survive, but we're only predicting a 32%. So you can see that that's, you know, represented in the fact that we only have about a 76% accuracy, because this model is not perfect. And in this instance, it was pretty bad. It's saying they have a 32% chance of surviving, but they actually did survive. So maybe that should be higher, right? So we could change this number, go for four, I'm just messing around and showing you guys, you know, how we use this. So in this one, you know, same thing, this person survived, although what is it, they only were given a 14% chance of survival. So anyways, that is how that works. This is how we actually make predictions and look at the predictions, you understand that now what's happening is I've converted this to a list just because this is actually a generator object, which means it's meant to just be looped through rather than just look at it with a list, but that's fine, we'll use a list. And then we can just print out, you know, result at whatever index probabilities, and then one to represent their chance of survival. Okay, so that has been it for linear regression. Now let's get into classification. And now we are on to classification. So essentially, classification is differentiating between, you know, data points and separating them into classes. So rather than predicting a numeric value, which we did with regression earlier, so linear regression, and you know, the percentage survival chance, which is a numeric value, we actually want to predict classes. So what we're going to end up doing is predicting the probability that a specific data point or a specific entry or whatever we're going to call it is within all of the different classes it could be. So for the example here, we're going to use flowers. So it's called the iris. I think it's the iris flower data set or something like that. And we're going to use some different properties of flowers to predict what species of flower it is. So that's the difference between classification and regression. Now I'm not going to talk about the specific algorithm we're going to use here for classification, because there's just so many different ones you can use. But yeah, I mean, if you really care about how they work on a lower mathematical level, I'm not going to be explaining that because it doesn't make sense to explain it for one algorithm when there's like hundreds and they all work a little bit differently. So you guys can kind of look that up. And I'll tell you some resources and where you can find that. I'm also going to go faster through this example, just because I've already covered kind of a lot of the fundamental stuff in linear regression. So hopefully we should get this one done a little bit quicker and move on to the next kind of aspects in this series. Alright, so first steps, load tensor flow, import tensor flow, we've done that already, data set, we need to talk about this. So the data that we're using is that iris flowers data set like I talked about. And this specific data set separates flowers into three different species. So we have these different species. This is the information we have. So septal length width, petal length, petal width, we're going to use that information obviously to make the predictions. So given this information, you know, in our final model, can it tell us which one of these flowers it's most likely to be? Okay, so what we're going to do now is define the CSV column names and species. So the column names are just going to define what we're going to have in our data set is like the headers for the columns, species obviously is just the species and we'll throw them there. Alright, so now we're going to load in our data sets. So this is going to be different every time you're kind of working with models, depending on where you're getting your data from. In our example, we're going to get it from Keras, which is kind of in sub module of TensorFlow. It has a lot of useful data sets and tools that we'll be using throughout the series. But keras.utils.get file, again, don't really focus on this, just understand what this is going to do is save this file onto our computer as iris training dot CSV, grab it from this link. And then what we're going to do down here is load the train and test and again, notice this training and this is testing into two separate data frames. So here we're going to use the names of the columns as the CSV column names, we're going to use the path as whatever we loaded here, header equals zero, which just means row zero is the header. Alright, so now we will move down and we'll have a look at our data set. So like we've done before, oh, I've got to run this code first. CSV column names. Okay, so we've just we're just running things in the wrong order here, apparently. Okay, so let's look at the head. So we can see this is kind of what our data frame looks like. And notice that our species here are actually defined numerically. So rather than before, when we had to do that thing where you know, we made those feature columns, and we converted the categorical data into numeric data with those kind of weird TensorFlow tools. This is actually already encoded for us. Now zero stands for SITOSA. And then one and two obviously stand for these ones respectively. And that's how that works. Now these I believe are in centimeters, the septal length, petal length, petal width, that's not super important. But sometimes you do want to know that information. Okay, so now we're going to pop up those columns for the species like we did before and separate that into train white test, why and then have a look at the head again. So let's do that and run this notice that is gone. Again, we've talked about how that works. And then these, if we want to have a look at them, and actually, let's do this, by just having a new block, let's say train underscore y dot, what is it dot head? If I could spell head correctly. Okay, so we run head, and we can see this is what it looks like nothing special. That's what we're getting. Alright, so let's delete that. Let's look at the shape of our training data. I mean, we can probably guess what it is already, right? We're going to have shape four, because we have four features. And then how many entries do we have? Well, I'm sure this will tell us so 120 entries in shape four. Awesome. That's our shape. Okay, input function. So we're moving fast here already, we're getting into a lot of the coding. So what I'm actually going to do is again, copy this over into a separate document, and I'll be back in a second with all that. Okay, so input function time, we already know what the input function does because we used it previously. Now this input function is a little bit different than before, just because we're kind of changing things slightly. So here, we don't actually have any, what do you call it, we don't have any epochs, and our batch size is different. So what we've done here is rather than actually, you know, defining like make input function, we just have input function like this. And what we're going to do is a little bit different when we pass this input function, I'll kind of show you it's a little bit more complicated. But you can see that we've cleaned this up a little bit. So exactly we're doing what we did do before, we're converting this data, which is our features, which we're passing in here into a data set. And then we're passing those labels as well. And then if we're training, so if training is true, what we're going to do is say data set is equal to the data set dot shuffle. So we're going to shuffle that information, and then repeat that. And that is all we really need to do. We can do data set dot batch at the batch size 256 return that. And we're good to go. So this is our input function. Again, these are kind of complicated. You kind of have to just get experience seeing a bunch of different ones to understand how to actually make one on your own. For now on, don't worry about it too much. You can pretty much just copy the input functions you've created before, and modify them very slightly if you're going to be doing your own models. But by the end of this, you should have a good idea of how these input functions work. We will have seen like four or five different ones. And then, you know, we can kind of mess with them and tweak them as we go on, but don't focus on it too much. Okay, so input function, this is our input function, I'm not really going to go in too much more detail with that. And now our feature columns. So this is again, pretty straightforward for the feature columns. All we need to do for this is since our all numeric feature columns is rather than having two for loops where we were separating the numeric and categorical feature columns before, we can just loop through all of the keys in our training data set. And then we can append to my feature columns blank list, the feature column, the numeric column, and the key is equal to whatever key we've looped through here. Now I'm going to show you what this means in case anyone's confused. Again, you can see when I print my feature columns, we get key equals septal length, we get our shape, and we get all of that other nice information. So let's copy this into the other one and have a look at our output after this. Okay, so my feature columns for key and train dot keys. So notice train is here, train dot keys. What that does is actually give us all the columns. So this was a really quick and easy way to kind of loop through all the different columns. Although I could have looped through CSV column names and just remove the species column to do that. But again, we don't really need to. So for key and train dot keys, my feature columns dot append tf feature column, numeric column, key equals key, this was just going to create those feature columns, we don't need to do that vocabulary thing and that dot unique because again, these are all already encoded for us. Okay, awesome. So that was the next step. So let's go back here, building the model. Okay, so this is where we need to talk a bit more in depth of what we're actually going to build. So the model for this is a classification model. Now there is like hundreds of different classification models we can use that are pre made in TensorFlow. And so far, what we've done with that linear classifier is that's a pre made model that we kind of just feed a little bit of information to and it just works for us. Now here we have two kind of main choices that we can use for this kind of classification task that are pre built in TensorFlow, we have a DNN classifier, which stands for a deep neural network, which we've talked about very vaguely, very briefly. And we have a linear classifier. Now a linear classifier works very similarly to linear regression, except it does classification, rather than regression. So we get actually numeric value, or we get, sorry, you know, the labels like probability of being a specific label, rather than a numeric value. But in this instance, we're actually going to go with deep neural network. Now that's simply because TensorFlow on their website, like this is all of this is kind of building off of TensorFlow website, just all the code is very similar. And I've just added my own spin and explain things very in depth. They've recommended using that deep neural network for this is a better kind of choice. But typically when you're creating machine learning apps, you'll mess around with different models and kind of tweak them. And you'll notice that it's not that difficult to change models, because most of the work comes from loading and kind of pre processing our data. Okay, so what we need to do is build a deep neural network with two hidden later, two hidden layers with 30 nodes and 10 hidden nodes each. Now I'm going to draw out the architecture of this neural network in just one second. But I want to show you what we've done here. So we said classifier equals tf dot estimator. So this estimator module just stores a bunch of pre made models from TensorFlow. So in this case, DNN classifier is one of those. What we need to do is pass our feature columns just like we did to our linear classifier. And now we need to define the hidden units. Now hidden units is essentially us a building the architecture of the neural network. So like you saw before, we had an input layer, we had some like middle layers called our hidden layers in a neural network. And then we had our output layer. I'm going to explain neural networks in the next module. So this will all kind of click and make sense. For now, we've arbitrarily decided 30 nodes in the first hidden layer, 10 in the second, and the number of classes is going to be three. Now that's something that we need to decide. We know there's three classes for the flowers. So that's what we've defined. Okay, so let's copy this in. Go back to the other page here. And that is now our model. And now it is time to talk about how we can actually train the model, which is coming down here. Okay, so I'm going to copy this, I'm going to paste it over here and let's just dig through this because this is a bit more of a complicated piece of code than we usually used to work with. I'm also going to remove these comments just to clean things up in here. So we've defined the classifier, which is a deep neural network classifier, we have our feature columns hidden units classes. Now to train the classifier. So we have this input function here. This input function is different than the one we created previously. Remember when we had previously was like make input, whatever function, I won't continue typing in the inside it to find another function. And it actually returned that function from this function. I know, complicated. If you're not a Python kind of pro, I don't expect that to make perfect sense. But here, we just have a function, right? We do not returning a function from another function, it's just one function. So when we want to use this to train our model, what we do is create something called a lambda. Now a lambda is an anonymous function that can be defined in one line. When you write lambda, what that means is essentially this is a function. So this is a function. And whatever's after the colon is what this function does. Now this is a one line function. So like, if I create a lambda here, right, and I say lambda, print, hi, and I said, x equals lambda, and I called x like that, this works, this is a valid line of syntax. Actually, I want to make sure that I'm not just like messing with you. And I say that and then this is actually correct. Okay, so sorry, I just accidentally trained the model. So I just commented that out. You can see we're printing high, right? At the bottom of the screen, I know it's kind of small, but does say hi. That's how this works. Okay, so this is a cool thing. If you haven't seen this in Python before, that's what a lambda does allows you to define a function in one line. Now the thing that's great about this is that we can say, like, you know, x equals lambda, and here put another function, which is exactly what will be done with this print function. And that means when we call x, it will, you know, execute this function, which will just execute the other function. So it's kind of like a chain where you call x, x is a function. And inside that function, it does another function, right? It just like calling a function from inside a function. So what is lambda doing here? Well, since we need the actual function object, what we do is we define a function that returns to us a function. So this actually just like it calls this function, when you put this here. Now there's no I can't it's it's very difficult to explain this if you don't really understand the concept of lambdas, and you don't understand the input functions. But just know we're doing this because of the fact that we didn't embed another function and return the function object. If we had done that, if we had done that, you know, input function that we had created before where we had the interior function, then we wouldn't need to do this because what would happen is we would return the input function, right, like that, which means when we passed it into here, it could just call that directly. It didn't need to have a lambda. Whereas here, though, since we need to just put a lambda, we need to define what this is and then and then this works. That's just there's no other way to really explain this. So yeah, what we do is we create this input function. So we pass we have train, we have train y and we have training equals true. And then we do steps equals 5000. So this is similar to an epoch, except this is just defining a set amount of steps we're going to go through. So rather than saying like we'll go through the data set 10 times, we're just going to say we'll go through the data set until we fit 5000 numbers, like 5000 things that have been looked at. So that's what this does with that train. Let's run this and just look at the training output from our model, it gives us some like, things here, we can kind of see how this is working. Notice that if I can stop here for a second, it tells us the current step, it tells us the loss, the lowest, the lower this number, the better. And then it tells us global steps per second. So how many steps we're completing per second. Now at the end here, we get final step loss of 39, which is pretty high, which means this is pretty bad. But that's fine. This is kind of just our first test at training in neural network. So this is just giving us output while it's training to kind of save what's happening. Now, in our case, we don't really care because this is a very small model. When you're training models that are massive and take terabytes of data, you kind of care about the progress of them. So that's when you would use kind of that output, right? And you would actually look at that. Okay, so now that we've trained the model, let's actually do an evaluation on the model. So we're just going to say classifier dot evaluate. And what we're going to do is a very similar thing to what we've done here is just pass this input function, right, like here with a lambda once again, and reason we add the lambda when we don't have this like, double function going on, like a nested function, we need the lambda. And then in here, what we do is rather than passing train and train y, we're going to pass test, I believe. And I think it's I just called it test y. Okay, and then for training, obviously, this is false. So we can just set that false like that. I'm just going to look at the other screen and make sure I didn't mess this up. Because again, I don't remember the syntax. Yes, a cluster classifier dot evaluate test test y looks good to me. We'll take this print statement just so we get a nice output for our accuracy. Okay, so let's look at this. Again, we're going to have to wait for this to train. But I will show you a way that we don't need to wait for this to train every time in one second. And I'll be right back. Okay, so what I'm actually going to do, and I've just kind of pause like the execution of this code is throw this in the next block under, because the nice thing about Google Collaboratory is that I can run this block of code, right, I can train all this stuff, which is what I'll run now while we're talking just so it happens. And then I can have another code block kind of below it, which I have here. And it doesn't matter. I don't need to rerun that block every time I change something here. So if I change something in any lower blocks, I don't need to change the upper block, which means I don't need to wait for this to train every time I want to do an evaluation on it. Anyways, so we've done this, we got test, we got test why I just need to change this instead of eval result. Actually, I need to say eval underscore result equals classifier dot evaluate so that we can actually store this somewhere and get the answer. And now we'll print this and notice this happens much, much faster. We get a test accuracy of 80%. So if I were to retrain the model, chances are this accuracy would change again, because of the order in which we're seeing different flowers. But this is pretty decent considering we don't have that much test data. And we don't really know what we're doing, right? We're kind of just messing around and experimenting for right now. So to get 80% is pretty good. Okay, so actually, what am I doing? We need to go back now and do predictions. So how am I going to predict this for specific flowers? So let's go back to our core learning algorithms. And let's go to predictions. Now, I've written a script already, just to save a bit of time that allows us to do a prediction on any given flower. So what I'm going to do is create a new block down here, code block and copy this function in. Now we're going to digest this and kind of go through this on our own to make sure this makes sense. But what this little script does is allow the user to type in some numbers. So the septal length width, and I guess petal length and width, and then it will spit out to you what the predicted class of that flower is. So we could do a prediction on every single one of our data points like we did previously. And we already know how to do that. I showed you that with linear regression. But here I just wanted to do it on one entry. So what do we do? So I start by creating a input function, it's very basic, we have batch size 256. All we do is we give some features, and we created data set from those features that's a dict and then dot batch and the batch size. So what this is doing is notice we don't give any y value, right? We don't give any labels. The reason we do we don't do that is because when we're making a prediction, we don't know the label, right? Like we actually want that the model to give us the answer. So here I wrote down the features, I created a predict dictionary, just because I'm going to add things to it. And then I just prompted here with a print statement, please type numeric values as prompted. So for feature and feature, valid equals true, well valid, valid equals input feature colon. So this just means what we're going to do is for each feature, we're going to wait to get some valid response. Once we get some valid response, what we're going to do is add that to our dictionary. So we're going to say predict feature. So whatever that feature was, so septal length, septal width, petal length or pep, petal width is equal to a list that has in this instance, whatever that value was. Now the reason we need to do this is because again, the predict method from TensorFlow works on predicting for multiple things, not just one value. So even if we only have one value we want to predict for it, we need to put it inside of a list because it's expecting the fact that we will probably have more than one value in which we would have multiple values in the list, right, each representing a different row or a new flower to make a prediction for. Okay, now we say predictions equals classifier dot predict. And then in this case, we have input function lambda input function predict, which is this input function up here. And then we say for prediction dictionaries, because remember, every prediction comes back as a dictionary in predictions, we'll say the class ID is equal to whatever the class IDs of the prediction dictionary at zero. And these are simply what I don't know exactly how to explain this. We'll look at in a second, and I'll go through that. And then we have the probability is equal to the prediction dictionary probabilities of class ID. Okay, then we're going to say print prediction is we're going to do this weird format thing, I just stole this from TensorFlow. And it's going to be the species at the class ID, and then 100 times probability, which will give us actual integer value, we're going to digest this, but let's run this right now and have a look. So please type numeric values as prompted septal length, let's type like 2.4, septal width 2.6, petal width, let's just say that's like 6.5. And yeah, petal width like 6.3. Okay, so then it calls this and it says prediction is virginica, I guess that's the the class we're going with. And it says that's an 83 or 86.3% chance that that is the prediction. So yeah, that is how that works. So that's what this does. I wanted to give a little script, I wrote most of this, I mean, I stole some of this from TensorFlow. But just to show you how we actually predict on one value. So let's look at these prediction dictionary, because I just want to show you what one of them actually is. So I'm going to say print, pred underscore dict. And then this will allow me to actually walk through what class IDs are probabilities are and how I've kind of done this. So let's run this up to length. Okay, let's just go like 1.4, 2.3. I don't know what these values are going to end up being. And we get prediction is same one with 77.1%, which makes sense, because these values are similar kind of in difference to what I did before. Okay, so this is the dictionary. So let's look for what we were looking for. So probabilities notice we get three probabilities, one for each of the different classes. So we can actually say what, you know, the percentages for every single one of the predictions. Then what we have is class IDs. Now class IDs, what this does is tell us what class ID, it predicts is actually the flower, right? So here it says two, which means that this probability at 77%. That's that index two in this array, right? So that's why this value is two. So it's saying that that class is to it thinks it's class two, like that's whatever was encoded in our system is two. And that's how that works. So that's how I know, which one to print out is because this tells me it's class two. And I know for making this list all the way back up here, if I can get rid of this output. Where is it? When I say species, that number two is virginica, or I guess that's how you say it. So that is what the classification is. So that's what the prediction is. So that's how I do that. And that's how that works. Okay, so I think that is pretty much it for actually classification. So it was pretty basic. I'm going to go and see if there's anything else that I did for classification in here. Okay, so here, I just put some examples. So here's some example input and expected classes. So you guys could try to do these if you want. So for example, this one, septal length, septal width. So for 5.1, 3.3, 1.7 and 0.5, the output should be Satosa. For 5.9, 3.0, 4.2, 1.5, it should be this one. And then obviously this for this, just so you guys can mess with them if you want. But that's pretty much it for classification. And now on to clustering. Okay, so now we're moving on to clustering. Now clustering is the first unsupervised learning algorithm that we're going to see in this series. And it's very powerful. Now clustering only works for a very specific set of problems. And you use clustering when you have a bunch of input information or features, you don't have any labels or open information. Essentially, what clustering does is finds clusters of like data points and tells you the location of those clusters. So you give a bunch of training data, you can pick how many clusters you want to find. So maybe we're going to be classifying digits right handwritten digits using k means clustering. In that instance, we would have 10 different clusters for the digits zero through nine. And you pass all this information and the algorithm actually finds those clusters in the data set for you. We're going to walk through an example that'll make sense. But I just want to quickly explain the basic algorithm behind k means essentially the set of steps. These are going to walk you through them and with a visual example. So we're going to start by randomly picking k points to place k centroids. Now a centroid stands for where our current cluster is kind of defined. And we'll see it in a second. The next step is we're going to assign all of the data points to the centroids by distance. So actually, now that I'm talking about this, I think it just makes more sense to get right into the example, because if I keep talking about this, you guys are probably just going to be confused, although I might come back to this just to reference those points. Okay, so let's create a little graph like this in two dimensions for our basic example. And let's make some data points here. So I'm just going to make them all read. And you're going to notice that I'm going to make this kind of easier for ourselves by putting them in like their own unique little groups, right? So actually, we'll add one up here. Then we can add some down here and down here. Now the algorithm starts for k means clustering. And you guys understand how this works as we continue by randomly picking k centroids. Now I'm going to denote a centroid by a little filled in triangle like this. And essentially what these are is where these different clusters currently exist. So we start by randomly picking k, which is what we've defined. So like me in this instance, that we're going to say k equals three, k centroid, wherever. So maybe we put one, you know, somewhere like here, you know, I might not bother filling these in because they're going to take a while. Maybe we pull in here, maybe we end up putting one over here. Now, I've kind of put them close to where clusters are, but these are going to be completely random. Now what happens next is each group, or each data point is assigned to a cluster by distance. So essentially, what we do is for every single data point that we have, we find what's known as the Euclidean distance, or it actually could be a different distance, you'd use like Manhattan distance, if you guys know what that is, to all of the centroids. So let's say we're looking at this data point here, what we do is find the distance to all of these different centroids. And we assign this data point to the closest centroid. So the closest one by distance. Now in this instance is looking like it's going to be a bit of a tie between this centroid and this centroid. But I'm going to give it to the one on the left. So what we do is we're going to say this is now a part of this centroid. So if I'm calling this like, let's just say this is centroid one, this is centroid two, and this is centroid three, then this now is going to be a part of centroid one, because it's closest to centroid one. And we can go through and we do this for every single data point. So obviously, we know all of these are going to be our ones, right? And we know these are going to be our two. So two, two, two. And then these are obviously going to be our three. Now I'm actually just going to add a few other data points, because I want to make this a little bit more sophisticated, almost, if that makes any sense. So add those data points here, we've been add one here, and that will give these labels. So these ones are close. So I'm going to say this one's one, I'm going to say this one's two, I know it's not closest to it. But just because I want to do that for now, we'll say two for that. And we'll say three here. Okay, so now that we've done that, we've labeled all these points, what we do is we now move these centroids that we've defined into the middle of all of their data points. So what I do is I essentially find it's called center of mass, the center of mass between all of the data points that are labeled the same. So in this case, these will be all the ones that are labeled the same. And I take this centroid, which I'm going to have to erase, get rid of it here. And I put it right in the middle. So let's go back to blue. And let's say the middle of these data points ends up being somewhere around here. So we put it in here. And this is what we call center of mass. And this again, it'd be centroid two. So let's just erase this. And there we go. Now we do the same thing with the other centroid. So let's remove these ones, to remove these ones. So for three, I'm saying it's probably going to be somewhere in here. And then for one, our center of mass is probably going to be located somewhere about here. Now what I do is I repeat the process that I just did. And I reassign all the points now to the closest centroid. So all these points are labeled one to all that, you know, we can kind of remove their labels. And this is just going to be great. Me trying to erase the labels, I shouldn't have wrote them on top. But essentially, what we do is we're just going to be like reassigning them. So I'm going to say, okay, so this is two, and we just do the same thing as before, find the closest distance. So we'll say, you know, these can stay in the same cluster, maybe this one actually here gets changed to one now, because it's closest to centroid one. And we just reassigned all these points. And maybe, you know, this one now, if it was two before, let's say like this one's one, and we just reassigned them. Now we repeat this process of finding the closest or assigning all the points that are closest centroid, moving the centroid into the center of mass, and we keep doing this until eventually we reach a point where none of these points are changing which centroid they're a part of. So eventually we reach a point where I'm just going to erase this and draw like a new graph because it'll be a little bit cleaner. But what we have is, you know, like a bunch of data points. So we have some over here, some over here, maybe we'll just put some here, and maybe we'll do like a K equals four example for this one. And we have all these centroids and I'll just draw these centroids with blue again, that are directly in the middle of all of their data points, they're like as in the middle as they can get, none of our data points have moved. And we call this now our cluster. So now we have these clusters, we have these centroids, right, we know where they are. And what we do is when we have a new data point that we want to make a prediction for or figure out what cluster it's a part of, what we do is we will plot that data point. So let's say it's this new data point here, we find the distance to all of the clusters that exist. And then we assign it to the closest one. So obviously it would be assigned to that one. And we can do this for any data point, right? So even if I put a data point all the way over here, well, it's closest cluster is this, so it gets assigned to this cluster. And my output will be whatever this label of this cluster is. And that's essentially how this works, you're just clustering data points, figuring out which ones are similar. And this is a pretty basic algorithm, I mean, you draw your little triangle, you find the distance from every point of the triangle, or to all of the triangles, actually. And then what you do is just simply assign those values to that centroid, you move that centroid to the center of mass, and you repeat this process constantly, until eventually you get to a point where none of your data points are moving. That means you found the best clusters that you can, essentially. Now, the only thing with this is you do need to know how many clusters you want for k means clustering, because k is a variable that you need to define. Although there is some algorithms that can actually determine the best amount of clusters for a specific data set. But that's a little bit beyond what we're going to be focused on focusing on right now. So that is pretty much clustering. There's not really much more to talk about it, especially because we can't really code anything for it now. So we're going to move on to hidden Markov models. Now hidden Markov models are way different than what we've seen so far, we've been using kind of algorithms that rely on data. So like k means clustering, we gave a lot of data, and we know clustered all those data points found those centroids, use those centroids to find where new data points should be. Same thing with linear regression and classification. Whereas hidden Markov models, we actually deal with probability distributions. Now, the example we're going to go into here, and it's kind of I have to do a lot of examples for this because it's a very abstract concept is a basic weather model. So what we actually want to do is predict the weather on any given day, given the probability of different events occurring. So let's say we know, you know, maybe in like a simulated environment or something like that, this might be an application, that we have some specific things about our environment, we know if it's sunny, there's an 80% chance that the next day, it's going to be sunny again, and a 20% chance that it's going to rain. Maybe we know some information about sunny days and about cold days. And we also know some information about the average temperature on those days. Using this information, we can create a hidden Markov model that will allow us to make a prediction for the weather in future days, given kind of that probability that we've discovered. Now you might be like, Well, how do we know this? Like how do I know this probability? A lot of the times you actually do know the probability of certain events occurring or certain things happening, which makes these models really good. But there's some times where what you actually do is you have a huge data set, and you calculate the probability of things occurring based on that data set. So we're not going to do that part because that's just kind of going a little bit too far. And the whole point of this is just to introduce you to some different models. But in this example, what we will do is use some predefined probability distributions. So let me just read out the exact definition of a hidden Markov model will start going more in depth. So the hidden Markov model is a finite set of states, each of which is associated with a generally multi dimensional probability distribution, transitions among the states are governed by a set of probabilities called transition probabilities. So in a hidden Markov model, we have a bunch of states. Now in the example that I was just talking about with this weather model, the states we would have is hot day and cold day. Now, these are what we call hidden, because never do we actually access or look at these states, while we interact with the model. In fact, what we look at is something called observations. Now at each state, we have an observation, I'll give you an example of an observation. If it is hot outside, Tim has an 80% chance of being happy. If it is cold outside, Tim has a 20% chance of being happy. That is an observation. So at that state, we can observe the probability of something happening during that state is x, right, or is y or whatever it is. So we don't actually care about the states in particular, we care about the observations we get from that state. Now in our example, what we're actually going to do is we're going to look at the weather as an observation for the state. So for example, on a sunny day, the weather has, you know, the probability of being between five and 15 degrees Celsius, with an average temperature of 11 degrees. That's like, that's a probability we can use. Now I know this is slightly abstract, but I just want to talk about the data we're going to work with here. I'm going to draw out a little example, go through it and then we'll actually get into the code. So let's start by discussing the type of data we're going to use. So typically in previous ones, right, we use like hundreds, if not like thousands of entries or rows or data points for our models to train. For this, we don't need any of that. In fact, all we need is just constant values for probability and or what is it transition distributions and observation distributions. Now what I'm going to do is go in here and talk about states observations and transitions. So we have a certain amount of states. Now we will define how many states we have, we don't really care what that state is. So we could have states, for example, like warm, cold, high, low, red, green, blue, you have as many states as we want, we could have one state to be honest, although that would be kind of strange to have that. And these are called hidden because we don't directly observe. Now observations. So each state has a particular outcome or observation associated with it based on a probability distribution. So it could be the fact that during a hot day, it is 100% true that Tim is happy. Although, in a hot day, we could observe that 80% of the time Tim is happy. And 20% of the time, he is sad, right? Those are observations we make about each state. And each state will have their different observations and different probabilities of those observations occurring. So if we were just going to have like an outcome for the state, that means it's always the same, there's no probability that something happens. And in that case, that's just called an outcome, because the probability of the event occurring will be 100%. Okay, then we have transitions. So each state will have a probability defining the likelihood of transitioning to a different state. So for example, if we have a hot day, there will be a percentage chance the next day will be a cold day. And if we have a cold day, there will be a percentage chance that the next day is either a hot day or a cold day. So we're going to go through like the exact what we have for our specific model below. But just understand there's a probability that we could transition into a different state. And from each state, we can transition into every other state or a defined set of states given a certain probability. So I know it's a mouthful, I know it's a lot. But let's go into a basic drawing example, because I just want to illustrate like graphically a little bit kind of how this works. In case these are ideas are a little bit too abstract for any of you. Okay, I'm just pulling out the drawing tablet, just one second here, and let's do this basic weather model. So what I'm going to do is just simply draw two states. Actually, let's do it with some colors because why not? So we're going to use yellow. And this is going to be our hot day. Okay, this is going to be our sun. And then I'm just going to make a cloud. We'll just do like a gray cloud. This will be my cloud. And we'll just say it's going to be raining over here. Okay, so these are my two states. Now, in each state, there's a probability of transitioning to the other state. So for example, in a hot day, we have a let's say 20% chance of transitioning to a cold day. And we have a 80% chance of transitioning to another hot day, like the next day, right? Now in a cold day, we have let's say a 30% chance of transitioning to a hot day. And we have in this case, what is that going to be a 70% chance of transitioning to another cold day. Now, on each of these days, we have a list of observations. So these are what we call states, right? So this could be s one, and this could be s two, it doesn't really matter, like if we name them or anything, we just we have two states, that's what we know. We know the transition probability, that's what we've just defined. Now we want the observation probability or distribution for that. So essentially, on a hot day, our observation is going to be that the temperature could be between 15 and 25 degrees Celsius with an average temperature of let's say 20. So we can say observation, right? So say observation, and we'll say that the mean so the average temperature is going to be 20. And then the distribution for that will be like the minimum value is going to be 15. And the max is going to be 25. So this is what we call actually like a standard deviation. I'm not really going to explain exactly what standard deviation is, although you can kind of think of it as something like this. So essentially, there's a mean, which is the middle point, the most common event that could occur. And at different levels of standard deviation, which is going into statistics, which I don't really want to mention that much, because I'm definitely non expert. We have a probability of hitting different temperatures as we move to the left and right of this value. So on this curve somewhere, we have 15. And on this curve to the right somewhere, we have 25. Now, we're just defining the fact that this is where we're going to kind of end our curve. So we're going to say that like the probability is in between these numbers, it's going to be in between 15 and 25 with an average of 20. And then our model will kind of figure out some things to do with that. That's as far as I really want to go in standard deviation. And I'm sure that's like a really horrible explanation. That's kind of the best I'm going to give you guys for right now. Okay, so that's our observation here. Now our observation over here is going to be similar. So we're going to say mean on a cold day, temperature is going to be five degrees. We'll say the minimum temperature maybe is going to be something like negative five and the max could be something like 15 or like, yeah, we can say 15. So we'll have some distribution, not just what we want to understand, right? And this is kind of a strange distribution because we're dealing with what is it standard deviation, although we can just deal with like straight percentage observations. So for example, you know, there's a 20% chance that Tim is happy, or there's an 80% chance that he is sad, like those are probabilities that we can have as our observation probabilities in the model. Okay, so there's a lot of lingo. There's a lot going on, we're going to get into like a concrete example now. So hopefully this should make more sense. But again, just understand states, transitions, observations, we don't actually ever look at the states, we just have to know how many we have, and the transition probability and observation probability in each of them. Okay, so what I want to say now, though, is what do we even do with this model? So once I make this right, once I make this hidden markup model, what's the point of it? Well, the point of it is to predict future events based on past events. So we know that probability distribution. And I want to predict the weather for the next week, I can use that model to do that, because I can say, well, if the current day today is warm, then what is the likelihood that the next day tomorrow is going to be cold, right? And that's what we're kind of doing with this model, we're making predictions for the future based on probability of past events occur. Okay, so imports and so let's just run this already loaded import tensorflow. And notice that here I've imported tensorflow probability is TFP. This is because this is a separate module from TensorFlow that deals with probability. Now, we also need tensorflow two. But for this hidden markup model, we're going to use the tensorflow probability module, not a huge deal. Okay, so weather model. So this is just going to define what our model actually is so the different parts of it. So this is taken directly from the documentation of tensorflow. You guys can see you know, where I have all this information from like I've sourced all of it. But essentially what the model we're going to try to create is that cold days are encoded by zero and hot days are encoded by one. The first day in our sequence has an 80% chance of being cold. So whatever day we're starting out at has an 80% chance of being cold, which would mean 20% chance of being warm. A cold day has a 30% chance of being followed by hot day. And a hot day has a 20% chance of being followed by a cold day, which would mean you know, 70% cold to cold and 80% hot to hot. On each day, the temperature is normally distributed with mean and standard deviation zero and five on a cold day and mean and standard deviation 15 and 10 on a hot day. Now what that means standard deviation is essentially I mean, we can read this thing here is that on a hot day, the average temperature is 15 that's mean and ranges from five to 25 because the standard deviation is 10 of that, which just means 10 on each side kind of the min max value. Again, I'm not in statistics. So please don't quote me on any definitions of standard deviation. I just trying to explain it enough so that you guys can understand what we're doing. Okay, so what we're going to do to model this and I'm just kind of going through this fairly quickly because it's pretty easy to really do this is I'm going to load the TensorFlow probability distributions kind of module and just save that as TFD. And I'm just going to do that so I don't need to write TFP dot distributions dot all of this, I can just kind of shortcut it. So you'll notice I'm referencing TFD here, which just stands for TFP dot distributions and TFP is TensorFlow probability. Okay, so my initial distribution is TensorFlow probability distributions dot categorical. This is probability of 80% and 20%. Now this refers to point two. So let's look at point two. The first day in our sequence has an 80% chance of being cold. So we're saying that that's essentially what this is the initial distribution of being cold is 80%. And then 20% after categorical is just a way that we can do this distribution. Okay, so transition distribution. What is it TensorFlow probability categorical, the probability is 70% and 30% and 20% 80%. Now notice that since we have two states, we've defined two probabilities. Notice since we have two states, we have defined two probabilities, the probability of landing on each of these states at the very beginning of our sequence. This is the transition probability referred to points three and four above. So this is what we have here. So cold is 30% chance 20% chance for a hot day. And that's what we've defined. So we say this is going to be cold day state one, we have 70% chance of being cold day again, we have 30% chance of going hot day and then you know, reverse here. Okay, so observation distribution. Now this one is a little bit different, but essentially we do tfd dot normal. Now I don't know, I'm not going to explain exactly what all this is, but when you're doing standard deviation, you're going to do it like this, where you're going to say, look, which stands for your average or your mean, right? So that was our average temperature is going to be zero on a hot day, 15 on a cold day. The standard deviation on the cold days five, which means we range from five, or negative five to five degrees. And on a hot day, it's 10. So that is going to be we go range from five to 25 degrees, and our average temperature is 15. Now the reason we've added dot here is because these just need to be float values. So rather than inserting integers here and having potentially type errors later on, we just have floats. Okay, so the low argument represents the mean and the scales of standard deviation. Yeah, exactly what we just defined there. Alright, so let's run this, I think we actually already did. And now we can create our model. So to create the models pretty easy. I mean, all we do is say model equals TensorFlow distribution dot hidden Markov model, give it the initial distribution, which is equal to initial distribution, transition distribution, observation, distribution and steps. Now what is steps? Well, steps is how many days we want to predict for. So the number of steps is how many times we're going to step through this probability cycle, and run the model essentially. Now remember, what we want to do is we want to predict the average temperature on each day, right? Like that's what the goal of our example is is to predict the average temperature. So given this information, using these observations and using these transitions, what we'll do is predict that. So I'm going to run this model. What is the issue here? tensor is on hash of tensor is okay, give me one sec, I'll have a look here, though I haven't had this issue before. Okay, so after a painful amount of searching on stack overflow and Google and actually just reading through more documentation on TensorFlow, I have determined the issue. So remember the error was we were getting on actually this line here, I think I can see what the output is. Oh, this okay, well, this is a different error. But anyways, there was an error at this line. Essentially, what was happening is we have a mismatch between the two versions here. So the most recent version of TensorFlow is not compatible with the older version of TensorFlow probability, at least in the sense that the things that we're trying to do with it. So I just need to make sure that I installed the most recent version of TensorFlow probability. So what you need to do if this is in your notebook, and this should actually work fine for you guys, because this will be updated by the time you get there. But in case you run into the issue, I'll, you know, deal with it. But essentially, we're going to select version 2.x of TensorFlow, you're going to run this install commands, you're going to install TensorFlow probability, just run this command. Then after you run this command, you're going to need to restart your run times, go to run time, and then restart run time. And then you can just continue on with the script, select TensorFlow 2.x again, do your imports, and then you know, we'll test if this is actually going to work for us here, run our distributions, create the model without any issues this time, notice no red text, and then run this final line, which will give you the output. Now, this is what I wanted to talk about here that we didn't quite get to because we were having some bugs. But this is how we can actually kind of run our model and see the output. So what you can do is do model dot mean, so you say mean equals model dot mean. And what this is going to do is essentially just calculate the probability is going to essentially take that from the model. Now, when we have model dot mean, this is what we call, you know, a partially defined tensor. So remember our tensors were like partially defined computations. Well, that's what model dot mean actually is. That's what this method is. So if we want to get the value of that, what we actually need to do is create a new session in TensorFlow, run this part of the graph, which we're going to get by doing mean dot numpy, and then we can print that out. So I know this might seem a little bit confusing, but essentially to run a session in the new version of TensorFlow, so 2.x, or 2.1 or whatever it is, you're going to type with TF dot compact dot v one dot session as sesh. And then I mean, this doesn't really matter what you have here, but whatever you want. And then what I'm doing is just printing mean dot numpy. So to actually get the value from this here, this variable, I call dot numpy. And then what it does is print out this array that gives me the expected temperatures on each day. So we have, you know, three, six, essentially 7.5, 8.25. And you can see these are the temperatures, based on the fact that we start with an initial probability of starting on a cold day. So we kind of get that here, right, we're starting at three degrees. That's what it's determined, we're going to start at. And then we have all of these other temperatures is predicting for the next days. Now notice if we recreate this model, so just rerun the distributions, rerun them and go model dot mean again, this stays the same, right? Well, because our probabilities are the same, this model is going to do the calculation the exact same, there's not really any training that goes into this. So we get, you know, very similar, if not the exact same values, I can't remember if these are identical, but that's what it looks like to me. I mean, we can run this again, see, we get the same one, and we'll create the model one more time. And let me just check these values here to make sure I'm not lying to you as yes, they are the exact same. Okay, so let's start messing with a few probabilities and see what we can do to this temperature and see what changes we can cause. So if I do 0.5 here, and I do 0.5 for the categorical probability, remember this refers to points three and four above. So it's a cold day has a 30% chance of being followed by hot day and then a hot day has a 20% chance of being followed by cold day. So what I've just done now is change the probability to be 50% so that a cold day now has a 50% chance of being followed by hot day and a 50% chance of being followed by cold day. And let's recreate this model. Let's rerun this and let's see if we get a difference. But we do notice this, the temperature now has been a is going a little bit higher. Now notice that we get the same starting temperature because that's just the average based on this probability that we have here. But if we wanted to potentially start, you know, hotter, we could reverse these numbers, we go 0.2 0.8. Let's rerun all of this. And now look at this what our temperatures are, we start at 12. And then we actually drop our temperature down to 10. So that's how this hidden Markov model works. Now this is nice, because you can just tweak the probabilities. This happens pretty well instantly. And we can have a look at our output very nicely. So obviously, this is representing the temperature on our like the first day, this would be the second day, third day, fourth day, fifth, sixth, seventh, and obviously, like the more days you go on, the least accurate, this is probably going to be because it just runs off probability. And if you're going to try to predict, you know, a year in advance, and you're using the weather that you have from I guess the previous year, you're probably not going to get a very accurate prediction. But anyways, these are hidden Markov models. They're not like extremely useful. There's some situations where you might want to use something like this. So that's why we're implementing them in kind of in this course and showing you how they work. It's also another feature of TensorFlow that a lot of people don't talk about or see. And you know, personally, I hadn't really heard of hidden Markov models until I started developing this course. So anyways, that has been it for this module. Now I hope that this kind of gave you guys a little bit of an idea of how we can actually implement some of these machine learning algorithms, a little bit of idea of how to work with data, how we can feed that to a model, the importance between testing and training data. And then obviously, linear regression is one we focused a lot on. So I hope you guys are very comfortable with that algorithm. And then what was the last the second one we did, I got to go up to remember exactly the sequence we had here. So classification, that one was important as well. So I hope you guys really understood that clustering, we didn't go too far into that. But again, this is an interesting algorithm. And if you need to do some kind of clustering, you now know of one algorithm to do that called K means clustering, and you understand how that works. And now you know, hidden Markov models. So in the next module, we're going to start covering neural networks, we now have the knowledge we need to really dive in there and start doing some cool stuff. And then in the future modules, we're going to do deep computer vision, I believe we're going to do chatbots with recurrent neural networks, and then some form of reinforcement learning at the end. So with that being said, let's go to the next module. Hello, everybody, and welcome to module four. Now in this module of this course, we're going to be talking about neural networks, discussing how neural networks work, a little bit of the math behind them, talking about gradient descent and back propagation, and how information actually flows through the neural network, and then getting into an example where we use a neural network to classify articles of clothing. So I know that was a lot, but that's what we're going to be covering here. Now neural networks are complex. There's kind of a lot of components that go into them. And I'm going to apologize right now, because it's very difficult to explain it all at once. What I'm going to be trying to do is kind of piece things together and explain them in blocks. And then at the end, you know, kind of combine everything together. Now I will say in case any of you didn't watch the beginning of this course, I do have very horrible handwriting, but this is the easiest way to explain things to you guys. So bear with me, you know, I'm sure you'll be able to understand what I'm saying, but it might just be painful to read some of it. All right, so let's get into it right away and start discussing what neural networks are and how they work. Well, the whole point of a neural network is to provide, you know, classification or predictions for us. So we have some input information, we feed it to the neural network, and then we want it to give us some output. So if we think of the neural network as this black box, we have all this input, right, we give all this data to the neural network, maybe we're talking about an image, maybe we're talking about just some random data points, maybe we're talking about a data set, and then we get some meaningful output. This is what we're looking at. So if we're just looking at a neural network from kind of the outside, we think of it as this magical black box, we give some input, it gives us some output. And I mean, we could call this black box just some function, right, where it's a function of the input maps it to some output. And that's exactly what a neural network does. It takes input and maps that input to some output, just like any other function, right, just like if you had a straight line like this, this is a function, you know, this is your line, you know, whatever it is, you're going to say y equals like four x, maybe that's your line, you give some input x, and it gives you some value y, this is a mapping of your input to your output. Alright, so now that we have that down, what is a neural network made up of? Well, a neural network is made up of layers. And remember, we talked about the layered representation of data when we talked about neural networks. So I'm going to draw a very basic neural network, we're going to start with the input layer. Now the input layer is always the first layer in our neural network. And it is what is going to accept our raw data. Now what I mean by raw data is whatever data we like want to give to the network, whatever we want to classify whatever our input information is, that's what this layer is going to receive in the neural network. So we can say, you know, these arrows represent our input, and they come to our first input layer. So this means, for example, if you had an image, and this image, and I'll just draw like one like this, let's say this our image, and it has all these different pixels, right, all these different pixels in the image, and you want to make a classification on this image. Well, maybe it has a width and a height and a classic width and height example is 28 by 28. If you had 28 by 28 pixels, and you want to make a classification on this image, how many input neurons you think you would need in your neural network to do this? Well, this is kind of, you know, a tough question if you don't know a lot about neural networks. If you're predicting for the image, if you're going to be looking at the entire image to make a prediction, you're going to need every single one of those pixels, which is 28 times 28 pixels, which I believe is something like 784. I could be wrong on that number, but I believe that's what it is. So you would need 784 input input neurons. Now, that's totally fine. That might seem like a big number, but we deal with massive numbers when it comes to computers. So this really isn't that many. But that's an example of, you know, how you would use a neural network input layer to represent an image, you would have 784 input neurons, and you would pass one pixel to every single one of those neurons. Now, if we're doing an example where maybe we just have one piece of input information, maybe it's literally just one number. Well, then all we need is one input nerve. If we have an example where we have four pieces of information, we would need four input neurons, right? Now, this can get a little bit more complicated. But that's the basis that I want you to understand is, you know, the pieces of input you're going to have regardless of what they are, you need one input neuron for each piece of that information, unless you're going to be reshaping or putting that information in different form. Okay, so let's just actually skip ahead and go to now our output layer. So this is going to be our output. Now, what is our output layer? Well, our output layer is going to have as many neurons. And again, the neurons are just representing like a node in the layer as output pieces that we want. Now, let's say we're doing a classification for images, right? And maybe there's two classes that we could represent. Well, there's a few different ways we could design our output layer. What we could do is say, okay, we're going to use one output neuron. This output neuron is going to give us some value. We want this value to be between zero and one. And we'll say that's inclusive. Now, what we can do now if we're predicting two classes say, Okay, so if my output neuron is going to give me some value, if that value is closer to zero, then that's going to be class zero. If this value is closer to one, it's going to be class one, right? And that would mean when we have our training data, right, and we talked about training and testing data, we'd give our input and our output would need to be the value zero or one, because it's either the correct class, which is zero, right, or the correct class, which is one. So like our what am I saying, our labels for our training data set would be zero and one. And then this value on our output neuron will be guaranteed to be between zero and one, based on something that I'm going to talk about a little bit later. That's one way to approach it, right? We have a single value, we look at that value. And based on what that value is, we can determine, you know, what class we predicted, not work sometimes. But in other instances, when we're doing classification, what makes more sense is to have as many output neurons as classes you're looking to predict for. So let's say we're going to have, you know, like five classes that we're predicting for maybe these three pieces of input information are enough to make that prediction. Well, we'd actually have five output neurons. And each of these neurons would have a value between zero and one. And the combination, so the sum of every single one of these values would be equal to one. Now, can you think of what this means? If every single one of these neurons is a value between zero and one, and their sum is one, what does this look like to you? Well, to me, this looks like a probability distribution. And essentially, what's going to happen is we're going to make predictions for how strongly we think each input information is each class. So if we think that it's like class one, maybe we'll just label these like this, then what we would do is say, okay, this is going to be 0.9 representing 90%. Maybe this is like 0.001, maybe this is 0.05, 0.003, you get the point, it's going to add up to one, and this is a probability distribution for our output layer. So that's a way to do it as well. And then obviously, if we're doing some kind of regression task, we can just have one neuron and that will just predict some value. And we'll define, you know, what we want that value to be. Okay, so that's my example for my output. Now let's erase this and let's actually just go back to one output neuron, because that's what I want to use for this example. Now, we have something in between these layers, because obviously, you know, we can't just go from input to output with nothing else. What we have here is called a hidden layer. Now, in neural networks, we can have many different hidden layers, we can have, you know, hidden layers that are connecting to other hidden layers, and like we could have hundreds, thousands, if we wanted to, for this basic example, we'll use one. And I'll write this as hidden. So now we have our three layers. Now, why is this called hidden? The reason this is called hidden is because we don't observe it when we're using the neural network, we pass information to the input layer, we get information from the output layer, we don't know what happens in this hidden layer or in these hidden layers. Now, how are these layers connected to each other? How do we get from this input layer to the hidden layer to the output layer and get some meaningful output? Well, every single layer is connected to another layer with something called weights. Now, we can have different kind of architectures of connections, which means I could have something like this one connects to this, this connects to this, this connects to this, and that could be like my connection kind of architecture, right? We could have another one where this one goes here. And you know, maybe this one goes here. And actually, after I've drawn this line, now we get what we're going to be talking about a lot, which is called a densely connected neural network. Now a densely connected neural network or a densely connected layer, essentially means that is connected to every node from the previous layer. So in this case, you can see every single node in the input layer is connected to every single node in the output layer or in the hidden layer, my bad. And these connections are what we call weights. Now these weights are actually what the neural network is going to change and optimize to determine the mapping from our input to our output. Because again, remember, that's what we're trying to do. We have some kind of function, we give some input, it gives us some output. How do we get that input and output? Well, by modifying these weights, it's a little bit more complex, but this is the starting. So these lines that I've drawn are really just numbers. And every single one of these lines is some numeric value. Typically, these numeric values are between zero and one, but they can be large, they can be negative. It really depends on what kind of network you're doing and how you've designed it. Now, let's just write some random numbers, we have like 0.1, this could be like 0.7, you get the point, right? We just have numbers for every single one of these lines. And these are what we call the trainable parameters that our neural network will actually tweak and change as we train to get the best possible result. So we have these connections. Now our hidden layer is connected to our output layer as well. This is again, another densely connected layer, because every layer or every nor neuron from the previous layer is connected to every neuron from the next layer, you would like to determine how many connections you have, what you can do is say there's three neurons here, there's two neurons here, three times two equals six connections. That's how that works from layers. And then obviously, you can just multiply all of the neurons together as you go through and determine what that's going to be. Okay, so that is how we connect these layers, we have these weights. So let's just write a w on here. So we remember that those are weights. Now, we also have something called biases. So let's add a bias here, I'm going to label this B. Now biases are a little bit different than these nodes we have regularly. There's only one bias, and a bias exists in the previous layer to the layer that it affects. So in this case, what we actually have is a bias that connects to each neuron in the next layer from this layer, right? So it's still densely connected. But it's just a little bit different. Now notice that this bias doesn't have an arrow beside it because this doesn't take any input information. This is another trainable parameter for the network. And this bias is just some constant numeric value that we're going to connect to the hidden layer. So we can do a few things with it. Now these weights always have a value of one. We're going to talk about why they have a value of one in a second. But just know that whenever a bias is connected to another layer or to another neuron, its weight is typically one. Okay, so we have that connected, we have our bias, and that actually means we have a bias here as well. And this bias connects to this. Notice that our biases do not connect with each other. The reason for this, again, is they're just some constant value, and they're just something we're kind of adding into the network is another trainable parameter that we can use. Now let's talk about how we actually pass information through the network and why we even use these weights and biases of what they do. So let's say we have, I can't really think of a good example, so we're just going to do some arbitrary stuff. Let's say we have like data points, right? X, Y, Z, and all of these data points have some mapped value, right? There's some value that we're looking for for them, or there's some class we're trying to put them in, maybe we're clustering them between like, red dots and blue dots. So let's do that. Let's say an XYZ is either a part of the red class, or the blue class, let's just do that. So what we want this output neuron to give us is red or blue. So what I'm going to do is say since it's just one class, we'll get this output neuron in between the range is your own one, we'll say, okay, if it's closer to zero, that's red, if it's closer to one, that's blue. And that's what we'll do for this network. And for this example, now our input neurons are going to obviously be X, Y, and Z. So let's pick some data point. And let's say we have, you know, the value two, two, two, that's our data point. And we want to predict whether it's red or blue. How do we pass it through? Well, what we need to do is determine how we can, you know, find the value of this hidden layer node, we already know the value of these input node, but now we need to go to the next layer using these connections and find what the value of these nodes are. Well, the way we determine these values is I'm going to say, and I've just said n one, just to represent like this is a node, like this is node one, maybe this one should be node two, is equal to what we call the weighted sum of all of the previous nodes that are connected to it. If that makes any sense to you guys. So a weighted sum is something like this. So I'm just going to write the equation, I'll explain it, I'm going to say n one is equal to the sum of, let's not say n equals zero, let's say, I equals zero to n of, in this case, we're going to say w i times x i plus b. Now, I know this equation looks really mathy and complicated, it's really not what this symbol and this equation here means is take the weighted sum of all the neurons that are connected to this neuron. So in this case, we have a neuron x neuron y and neuron z connected to n one. So when we take the weighted sum, or we calculate this, what this is really equal to is the weight at neuron x, we can say w x times the value at neuron x, which in this case, is just equal to two, right, plus whatever the weight is at neuron y. So in this case, this is w y. And then times two, and then you get the point where we have w z and I'm trying on the edge of my drawing tablet to write this times two. Now, obviously, these weights have some numeric value. Now, when we start our neural network, these weights are just completely random. They don't make any sense or just some random values that we can use. As the neural network gets better, these weights are updated and changed to make more sense in our network. So right now, we'll just leave them as w x, w y, w z. But no, these are some numeric values. So this returns to a sum value, right, some value, let's just call this value v. And that's what this is equal to. So v. Then what we do is we add the bias. Now remember, the bias was connected with a weight of one, which means if we take the weighted sum of the bias, right, all we're doing is adding whatever that biases value was. So if this bias value was 100, then what we do is we add 100. Now, I've just written the plus B to explicitly state the fact that we're adding the bias, although it could really be considered as a part of the summation equation, because it's another connection to the neural. Now, let's just talk about what this symbol means for anyone that's confused about that. Essentially, this stands for sum, I stands for an index, and n stands for what index will go up to now n means how many neurons we had in the previous layer. And then what we're doing here is saying wi xi. So we're going to say weight zero x zero plus weight one x one plus weight two x two, it's almost like a for loop where we're just adding them all together. And then we add the B. And I hope that makes enough sense so that we understand that. So that is our weighted sum and our bias. So essentially, what we do is we go through and we calculate these values. So this gets some value, maybe this value is like 0.3, maybe this value seven, whatever it is, and we do the same thing now at our output neuron. So we take the weighted sum of this value times its weight. And then we take the weighted sum, so this value times its weight, plus the bias, this is given some value here. And then we can look at that value and determine what the output of our neural network is. So that is pretty much how that works in terms of the weighted sums, the weights and the biases. Now, let's talk about the kind of the training process and another thing called an activation function. So I've lied to a little bit because I've said I'm just going to start erasing some stuff. So we have a little bit more room on here. So I've lied to you and I've said that this is completely how this works. Well, we're missing one key feature that I want to talk about, which is called an activation function. Now remember how we want this value to be in between zero and one right at our output layer. Well, right now, we can't really guarantee that that's going to happen. I mean, especially for starting with random weights and random biases in our neural network, we're passing this information through, we could get to this, you know, point here, we could have like 700 as our value. That's kind of crazy to me, right? We have this huge value, how do we look at 700 and determine whether this is red or whether this is blue? Well, we can use something called an activation function. Now, I'm going to go back to my slides here, whatever you want to call this this notebook, just to talk about what an activation function is. And you guys can see here, you can follow along, I have all the equations kind of written out here as well. So let's go to activation function, which is right here. Okay. So these are some examples of an activation function. And I just want you to look at what they do. So this first one is called rectified linear unit. Now notice that essentially what this activation function does is take any values that are less than zero and just make them zero. So any x values that are, you know, in the negative, it just makes their y zero. And then any values that are positive, it's just equal to whatever their positive value is. So if it's 10, it's 10. This allows us to just pretty much eliminate any negative numbers, right? That's kind of what rectified linear unit does. Now 10 h or hyperbolic tangent. What does this do? This actually squishes our values between negative one and one. So it takes whatever values we have, and the more positive they are, the closer to one they are, the more negative they are, the closer to negative one they are. So when we see why this might be useful, right for a neural network, and then last one is sigmoid, what this does is squish our values between zero and one, a lot of people call it like the squishifier function. Because all it does is take any extremely negative numbers and put them closer to zero and any extremely positive numbers and put them close to one, any values in between, you're going to get some number that's kind of in between that, based on the equation one over one plus e to the negative z. And this is theta z, I guess, is equal to that. Okay, so that's how that works. Those are some activation functions. Now I hope that's not too much math for you. But let's talk about how we use them, right? So essentially, what we do is at each of our neurons, we're going to have an activation function that is applied to the output of that neuron. So we take this this weighted sum plus the bias, and then we apply an activation function to it before we send that value to the next neuron. So in this case, n one isn't actually just equal to this, what n one is equal to is n one is equal to f, which stands for activation function of this equation, right? So we say I equals zero, w i x i plus B. And that's what n one's value is equal to when it comes to this output neuron. So each of these have an activation function on them. And two has the same activation function as n one. And we can define what activation function we want to apply at each neuron. Now at our output neuron, the activation function is very important, because we need to determine what we want our value to look like. Do we want it between negative one on one? Do we want it between zero and one? Or do we want it to be some massively large number? Do we want it between zero and positive infinity? What do we want? Right? So what we do is we pick some activation function for our output neuron. And based on what I said, where we want our values between zero and one, I'm going to be picking the sigmoid function. So sigmoid, recall, squishes our values between zero and one. So what we'll do here is we'll take n one, right? So n one times whatever the weight is there. So weight zero, plus n two times weight one, plus a bias, and apply sigmoid. And then this will give us some value between zero and one, then we can look at that value and we can determine what the output of this network is. So that's great. And that makes sense. Why we would use that on the output neuron, right? So we can squish our value in between some kind of value. So we can actually look at it and determine, you know, what to do with it, rather than just having these crazy, and I want to see if I can make this eraser any bigger. Ah, that's much better. Okay. So there we go. Let's just erase some of this. And now let's talk about why we would use the activation function on like an intermediate layer like this. Well, the whole point of an activation function is to introduce complexity into our neural network. So essentially, you know, we just have these basic weights and these biases. And this is kind of just, you know, like a complex function at this point, we have a bunch of weights, we have a bunch of biases. And those are the only things that we're training. And the only things that we're changing to make our network better. Now, what an activation function can do is, for example, take a bunch of points that are on the same like plane, right? So let's just say, these are in some plane. If we can apply an activation function of these, where we introduce a higher dimensionality, so an activation function like sigmoid that is like a higher dimension function, we can hopefully spread these points out and move them up or down off the plane in a hopes of extracting kind of some different features. Now, it's hard to explain this until we get into the training process of the neural network. But I'm hoping this is maybe giving you a little bit of idea, if we can introduce a complex activation function into this kind of process, then it allows us to make some more complex predictions, we can pick up on some different patterns. If I can see that, you know, when sigmoid or rectify linear unit is applied to this output, it moves my point up or it moves it down or moves it in like whatever direction and n dimensional space, then I can determine specific patterns I couldn't determine in the previous dimension. That's just like if we're looking at something in two dimensions, if I can move that into three dimensions, I immediately see more detail, there's more things that I can look at, right? And I'll try to do a good example of why we might use it like this. So let's say we have a square, right, like this, right? And I ask you, I'm like, tell me some information about the square. Well, what you can tell me immediately is you can tell me the width, you can tell me the height, and I guess you could tell me the color, right? You can tell me it has one face, you can tell me it has four vertexes, you can tell me a fair amount about the square, you can tell me its area. Now what happens as soon as I extend the square and I make it into a cube? Well, now you can immediately tell me a lot more information, you can tell me, you know, the height, or I guess the depth with height depth, yeah, whatever you want to call it there. You can tell me how many faces it has, you can tell me what color each of the faces are, you can tell me how many vertexes you can tell me if this cube or the square, this rectangle is uniform or not, and you can pick up on a lot more information. So that's kind of I mean, this is a very over simplification of what this actually does. But this is kind of the concept, right, is that if we are in two dimensions, if we can somehow move our data points into a higher dimension by applying some function to them, then what we can do is get more information and extract more information about the data points, which will lead to better predictions. Okay, so now that we've talked about all this, it's time to talk about how neural networks train. And I think you guys are ready for this. This is a little bit more complicated. But again, it's not that crazy. Alright, so we talked about these weights and biases. And these weights and biases are what our network will come up with and determine to, you know, like make the network better. So essentially, what we're going to do now is talk about something called a loss function. So as our network starts, right, the way that we train it, just like we've trained other networks, or other machine learning models is we give it some information, we give it what the expected output is. And then we just see what the expected output or what the output was from the network, compare it to the expected output and modify it like that. So essentially, what we start with is we say, okay, 222, we say this class is red, which I forget what I labeled that was as but let's just say, like that was a zero, okay. So this class is zero. So I want this network to give me a zero for the point 222. Now, this network starts with completely random weights and completely random biases. So chances are, when we get to this output here, we're not going to get zero, maybe we get some value after applying the sigmoid function, that's like 0.7. Well, this is pretty far away from red. But how far away is it? Well, this is where we use something called a loss function. Now, what a loss function does is calculate how far away our output was from our expected output. So if our expected output is zero, and our output was 0.7, the loss function is going to give us some value that represents like how bad or how good this network was. Now, if it tells us this network was really bad, it gives us like a really high loss, then that tells us that we need to tweak the weights and biases more and move the network in a different direction. We're starting to get into gradient descent. But let's understand the loss function first. So it's going to say, if it was really bad, let's move it more, let's change the weights more drastically, let's change the biases more drastically. Whereas, if it was really good, it'll be like, Okay, so that one was actually decent, you know, you only need to tweak a little bit, and you only need to move this, this and this. So that's good. And that's the point of this loss function, it just calculates some value, the higher the value, the worse our network was a few examples of loss function. Let's go down here, because I think I had a few optimizer loss here, mean squared error, mean absolute error and hinge loss. Now mean absolute error, you know, let's actually just look one up here. So mean, absolute error, and have a look at what this is. So images, let's pick something. This is mean absolute error. This is the equation for mean absolute error. Okay, so the summation of the absolute value of yi minus lambda of xi over n. Now, this is kind of complicated. I'm not going to go into it too much. I was expecting I was hoping I was going to get like a better example for mean squared error. Okay, so these are the three loss functions here. So mean squared error, mean absolute error, hinge loss, obviously, there's a ton more that we could use. I'm not going to talk about which how each of these work specifically, I mean, you can look them up pretty easily. And also, so you know, these are also referenced as cost functions, so cost or loss, you might just hear these, these terms kind of interchanged cost and loss essentially mean the same thing, you want your network to cost the least, you want your network to have the least amount of loss. Okay, so now that we have talked about the loss function, we need to talk about how we actually update these weights and biases. Now, actually, let's go back to here, because I think I had some notes on it. This is what we call gradient descent. So essentially, the parameters for our network are weights and biases. And by changing these weights and biases, we will, you know, either make the network better or make the network worse, the loss function will determine if the network is getting better, if it's getting worse, and then we can determine how we're going to move the network to change that. So this is now gradient descent, where the math gets a little bit more complicated. So this is an example of what your neural network function might look like. Now, as you have higher dimensional math, you have, you know, a lot more dimensions, a lot more space to explore when it comes to creating different parameters and creating different biases and activation functions and all of that. So as we apply our activation functions, we're kind of spreading our network into higher dimensions, which just makes things much more complicated. Now, essentially, what we're trying to do with the neural network is optimize this loss function. This loss function is telling us how good it is or how bad it is. So if we can get this loss function as low as possible, then that means we should technically have the best neural network. So this is our kind of loss functions, like mapping or whatever, what we're looking for is something called a global minimum, we're looking for the minimum point where we get the least possible loss from our neural network. So if we start where these red circles are, right, and I've just stole this image off Google images, what we're trying to do is move downwards into this global global minimum. And this is with a process of called gradient descent. So we calculate this loss, and we use an algorithm called gradient descent, which tells us what direction we need to move our function to determine or to get to this global minimum. So it essentially looks where we are. It says this was the loss. And it says, okay, I'm going to calculate what's called a gradient, which is literally just a steepness or a direction. And we're going to move in that direction. And then the algorithm called brought back propagation, we'll go backwards through the network and update the weights and biases so that we move in that direction. Now, I think this is as far as I really want to go, because I know this is getting more complicated already, then some of you guys probably can handle and that I can probably explain. But that's kind of the basic principle. We'll go back to the drawing board and we'll do a very quick recap before we get into some of the other stuff, neural networks, input, output hidden layers connected with weights, there's biases that connect to each layer. These biases can be thought of as y intercepts, they'll simply move completely up or move completely down that entire, you know, activation function, right, we're shifting things left or right, because this will allow us to get a better prediction and have another parameter that we can train and add a little bit of complexity to our neural network model. Now, the way that information is passed through these layers is we take the weighted sum at a neuron of all of the connected neurons to it, we then add this bias neuron, and we apply some activation function that's going to put this, you know, these values in between two set values. So for example, when we talk about sigmoid, that's going to squish our values between zero and one, when we talk about hyperbolic tangent, that's going to squish our values between negative one and one. And when we talk about rectifier linear unit, that's going to squish our values between zero and positive infinity. So we apply those activation functions, and then we continue the process. So n one gets its value and two gets its value. And then finally, we make our way to our output layer, we might have passed through some other hidden layers before that. And then we do the same thing, we take the weighted sum, we add the bias, we apply an activation function, we look at the output, and we determine whether we know we are a class y or we are class z or whether this is the value we're looking for. And and that's how it works. Now we're at the training process, right? So we're doing this now, that's kind of how this worked when we were making a prediction. So when we're training, essentially, what happens is we just make predictions, we compare those predictions to whatever these expected value should be using this loss function. Then we calculate what's called a gradient, a gradient is the direction we need to move to minimize this loss function. And this is where the advanced math happens and why I'm kind of skimming over this aspect. And then we use an algorithm called back propagation, where we step backwards through the network, and update the weights and biases, according to the gradient that we calculated. Now that is pretty much how this works. So you know, the more info we have, likely unless we're overfitting, but you know, if we have a lot of data, if we can keep feeding the network, it starts off being really horrible, having no idea what's going on. And then as more and more information comes in, it updates these weights and biases gets better and better sees more examples. And after you know, a certain amount of epochs or certain amount of pieces of information, our network is making better and better predictions and having a lower and lower loss. And the way we will calculate how well our network is doing is by passing it, you know, our validation data set, where it can say, okay, so we got an 85% accuracy on this data set, we're doing okay, you know, let's tweak this, let's tweak that, let's do this. So the loss function, the lower this is the better, also known as the cost function. And that is kind of neural networks in a nutshell. Now I know this wasn't really in a nutshell, because it was 30 minutes long. But that is, you know, as much of an explanation as I can really give you without going too far into the mathematics behind everything. And again, remember, the activation function is to move us up in dimensionality. The bias is another layer of complexity and a trainable parameter for our network allows us to shift this kind of activation function left, right up, down. And yeah, that is how that works. Okay, so now we have an optimizer. This is kind of the last thing on how neural networks work. optimizer is literally just the algorithm that does the gradient descent and back propagation for us. So I mean, you guys can read through some of them here, we'll be using probably the atom optimizer for most of our examples, although there's, you know, lots of different ones that we can pick from. Now this optimization technique, again, is just a different algorithm. There's some of them are faster, some of them are slower, some of them work a little bit differently. And we're not really going to get into picking optimizers in this course, because that's more of an advanced machine learning technique. All right, so enough explaining, enough math, enough drawings, enough talking. Now it is time to create our first official neural network. Now these are the imports we're going to need. So import TensorFlow is TF from TensorFlow import Keras again, so this does actually come with TensorFlow. I forget if I said you need to install that before. My apologies and then import numpy as NP, import map plot live dot pi plot as PLT. Alright, so I'm going to do actually similar thing to what I did before where I'm kind of just going to copy some of this code into another notebook, just to make sure that we can look at everything at the end, and then kind of step through the code step by step rather than all of the text kind of happening here. Alright, so the data set, and the problem we are going to consider for our first neural network is the fashion MNIST data set. Now the fashion MNIST data set contains 60,000 images for training and 10,000 images for validating and testing 70,000 images. And it is essentially pixel data of clothing articles. So what we're going to do to load in this data set from Keras, this actually built into Keras, it's meant as like a beginner, like testing training data set, we're going to say fashion underscore MNIST equals Keras dot data sets dot fashion MNIST. Now this will get the data set object, and then we can load that object by doing fashion MNIST dot load data. Now by doing this by having the tuples train images train labels, test images test labels equals this, this will automatically split our data into the sets that we need. So we need the training, and we need the testing. And again, we've talked about all of that. So I'm going to kind of skim through that. And now we have it in all of these kind of tuples here. Alright, so let's have a look at this data set to see what we're working with. Okay, so let's run some of this code, let's get this import going, if it doesn't take forever. Okay, let's get the data sets. Yeah, this will take a second to download for you guys, if you don't already have it cached. And then we'll go train images dot shape. And let's look at what one of the images looks like. Or sorry, what our data set looks like. So we have 60,000 images that are 28 by 28. Now what that means is we have 28 pixels or 28 rows of 28 pixels, right? So that's kind of what our, you know, information is. So we're going to have in total 784 pixels, which I've denoted here. So let's have a look at one pixel. So to reference one pixel, this is what I what I'm doing, this comes in as a, actually, I'm not sure what type of data frame this is. But let's have a look at it. So let's say type of train underscore images, because I want to see that. So that's an numpy array. So to reference the different indexes in this is similar to pandas, we're just going to do zero comma 23 comma 23, which stands for, you know, image zero 23, and then 23. And this gives us one pixel. So row 23 column 23, which will be that. Okay, so let's run this. And let's see this value is 194. Okay, so that's kind of interesting. That's what one pixel looks like. So let's look at what multiple pixels look like. So we'll print train underscore images. And okay, so we get all these zeros, let's print train images, zero, colon, that should work for us. And we're getting all these zeros. Okay, so that's the border of the picture. That's okay, I can't show you what I wanted to show you. Anyways, one pixel. And I wanted to have you guys guess it is simply just represented by a number between zero and 255. Now what this stands for is the grayscale value of this pixel. So we're dealing with grayscale images, although we can deal with, you know, 3d, 4d, 5d images as well, or not 5d images, but we can deal with images that have like RGB values. First, so for example, we could have a number between zero 255, another number between zero and 255 and another number between zero and 255 for every single pixel, right? Whereas this one is just one simple static value. Okay, so it says that here, a pixel values between zero and 255, zero being black and 255 being white. So essentially, you know, if it's 255, that means that this is white, if it's zero, that means that it is black. Alright, so let's have a look at the first 10 training labels. So that was our training images. Now what are the training labels? Okay, so we have an array, and we get values from zero to nine. Now this is because we have 10 different classes that we could have for our dataset. So there's 10 different articles of clothing that are represented. I don't know what all of them are, although they are right here. So t shirt, trouser, pullover, dress, coat, sandals, shirt, sneaker, bag, ankle boot. Okay, so let's run this class names, just so that we have that saved. And now what I'm going to do is use matplotlib to show you what one of the images looks like. So in this case, this is a shirt. I know this is printing out kind of weird, but I'm just showing the image. I know it's like different colors, but that's because if we don't define that we're drawing a grayscale, it's going to do this. But anyways, that is what we get for the shirt. So let's go to another image and let's have a look at what this one is. I actually don't know what that is. So we'll skip that maybe that's a what is it t shirt or top. This I guess is going to be like a dress. Yeah, so we do have dressed there. Let's go for have a look at this. And some of these are like hard to even make out when I'm looking at them myself. And then I guess this will be like a hoodie or something. I'm trying to get one of the sandals to show you guys a few different ones. There we go. So that is a sandal or a sneaker. Okay, so that is kind of how we do that and how we look at the different images. So if you wanted to draw it out, all you do is just make a figure, you just show the image, do the color bar, which is just giving you this, then you're going to say, I don't want to grid and then you can just show the image, right? Because if you don't have this line here, and you show with the grid, oh, it's actually not showing the grid that's interesting. Although I thought it was going to show me those pixelated grid. So I guess you don't need that line. Alright, so data pre processing. Alright, so this is an important step in neural networks. And a lot of times when we have our data, we have it in these like random forms, or we're missing data, or there's information we don't know, or that we haven't seen. And typically what we need to do is pre process it. Now, what I'm going to do here is squish all my values between zero and one. Typically, it's a good idea to get all of your input values in a neural network in between, like that range in between, I would say negative one and one is what you're trying to do, you're trying to make your numbers as small as possible to feed to the neural network. The reason for this is your neural network starts out with random weights and biases that are in between the range zero and one, unless you change that value. So if you have massive input information and tiny weights, then you're kind of having a bit of a mismatch, and you're going to make it much more difficult for your network to actually classify your information, because it's going to have to work harder to update those weights and biases to reduce how large those values are going to be, if that makes any sense. So it usually is a good idea to pre process these and make them in between the value of zero and one. Now, since we know that we're just going to have pixel values that are in the range of 255, we can just divide by 255, and that will automatically scale it down for us. Although it is extremely important that we do this to not only the training images, but the testing images as well. If you just pre process your training images, and then you pass in, you know, new data that's not pre processed, that's going to be a huge issue. You need to make sure that your data comes in the same form. And that means when we're using the model to to make predictions, whatever, you know, I guess it pixel data we have, we need to pre process in the same way that we pre processed our other data. Okay, so let's pre process that so train images and test images. And I'm just going to actually steal some of this stuff here, and throw it in my other one before we get too far. So let's get this data sets. And let's throw it in here, just so we can come back and reference all this together. Let's go class names. We don't actually need the figures, a few things I can skip, we do need this pre processing step. Like that, if I could go over here. And then what else do we need, we're going to need this model. Okay, so let's actually just copy the model into this and just make it a little bit cleaner. We can have a look at it. So new code block model. Okay, so model, creating our model. Now creating our models actually really easy. I'm hoping what you guys have realized so far is that data is usually the hardest part of machine learning and neural networks, getting your data in the right form, the right shape and you know, pre processed correctly, building the models usually pretty easy because we have tools like TensorFlow and Keras that can do it for us. So we're going to say model equals Keras dot sequential. Now sequential simply stands for the most basic form of neural network, which we've talked about so far, which is just information going from the left side to the right side, passing through the layers, sequentially, right, called sequential, we have not talked about recurrent or convolutional neural networks yet. Now what we're going to do here is go Keras dot layers dot flat. So sorry, inside here, we're going to define the layers that we want in our neural network. This first layer is our input layer. And what flatten does is allows us to take in a shape of 28 by 28, which we've defined here, and flatten all of the pixels into 784 pixels. So we take this 28 by 28 kind of matrix like structure, and just flatten it out. And Keras will do that for us, we don't actually need to take our you know, matrix data and transform before passing. So we've done that. Next, we have Keras dot layers dot dense 128 activation equals rectify linear unit. So this is our first hidden layer, layer two, right, that's what I've denoted here. And this is a dense layer. Now dense, again, means that all of the, what is it, the neurons in the previous layer are connected to every neuron in this layer. So we have 828 neurons here. How do we pick that number? We don't know, we kind of just came up with it. Usually, it's a good idea that you're going to do this as like a little bit smaller than what your input layer is, although sometimes it's going to be bigger, you know, sometimes it's going to be half the size, it really depends on the problem, I can't really give you a straight answer for that. And then our activation function will define as rectify linear unit. Now we could pick a bunch of different activation functions, there's time we can pick sigmoid, we could pick 10 h, which is hyperbolic tangent, doesn't really matter. And then we're going to define our last layer, which is our output layer, which is a dense layer of 10 output neurons with the activation of softmax. Okay, so can we think of why we would have picked 10 here, right? I'll give you guys a second to think about it, based on the fact that our output layer, you know, is supposed to have as many neurons as classes we're going to predict for. So that is exactly what we have 10. If we look, we have 10 classes here. So we're going to have 10 output neurons in our output layer. And again, we're going to have this probability distribution. And the way we do that is using the activation function softmax. So softmax will make sure that all of the values of our neurons add up to one, and that they're between zero and one. So that is our, our model, we've created the model now. So let's actually run this. See, are we going to get any errors here? Is this going to run? And then we'll run the model, and then we'll go on to the next step, which actually going to be training and testing the model. Okay, so let's create the model now, shouldn't get any issues, and we're good. And now let's move on to the next step. I'm forgetting what it is, though, which is training the model. Oh, sorry, compiling the model. Okay, so compiling the model. So we've built now what we call the architecture of our neural network, right, we've defined the amount of neurons in each layer, we've defined the activation function, and we define the type of layer and the type of connections. The next thing we need to pick is the optimizer, the loss and the metrics we're going to be looking at. So the optimizer we're going to use is Adam. This is again, just the algorithm that performs the gradient descent, you don't really need to look at these too much, you can read up on some different activation functions or sorry, optimizers, if you want to kind of see the difference between them, but that's not crazy, we're going to pick a loss. So in this case, sparse categorical cross entropy, again, not going to go into depth about that, you guys can look that up if you want to see how it works. And then metrics. So what we're looking for the output that we want to see from the network, which is accuracy. Now from, you know, kind of right now with our current knowledge, we're just going to stick with this as what we're going to compile our neural networks with, we can pick different values if we want. And these are what we call, what is it hyper parameter tuning. So the parameters that are inside here, so like the weights and the biases are things that we can't manually change. But these are things that we can change, right, the optimizer, the loss, the metrics, the activation function, we can change that. So these are called hyper parameters, same thing with the number of neurons in each layer. So hyper parameter tuning is a process of changing all of these values and looking at how models perform with different hyper parameters change. So I'm not really going to talk about that too much. But that is something to note, because you'll probably hear that, you know, this hyper parameter kind of idea. Okay, so we've compiled the model now using this, which just means we've picked all the different things that we need to use for it. And now on to training the model. So I'm just going to copy this in. Again, remember this, these parts are pretty syntaxually heavy, but fairly easy to actually do. So we're going to fit the model. So fit just means we're fitting it to the training data. It's another word for training, essentially. So we're going to pass it the training images, the training labels, and notice how much easier it is to pass this. Now, we don't need to do this input function, we don't need to do all of that, because Keras can handle it for us. And we define our epochs as 10 epochs is another hyper parameter that you could tune and change if you wanted to. All right, so that will actually fit our model. So what I'm going to do is put this in another code block. So I don't need to keep retraining this. So we'll go like that. And let's actually look at this training process. So we've run the model, this should compile. And now let's fit it and let's see what we actually end up getting. Alright, so epoch one, and we can see that we're getting a loss and we're getting accuracy printing out on the side here. Now, this is going to take a second, like this is going to take a few minutes, as opposed to our other models that we made are not a few minutes, but you know, a few seconds, when you have 60,000 images, and you have a network that's comprised of 784 neurons, 128 neurons, and then 10 neurons, you have a lot of weights and biases and a lot of math that needs to go on. So this will take a few seconds to run. Now, if you're on a much faster computer, you'll probably be faster than this. But this is why I like Google Collaboratory, because you know, this isn't using any of my computer's resources to train. It's using this. And we can see, like the RAM and the disk. How do I look at this? In this network? Oh, is it going to let me look at this now? Okay, I don't know why it's not letting me click this, but usually you can have a look at it. And now we've trained and we've fit the model. So we can see that we have an accuracy of 91%. But the thing is, this is the accuracy on or testing or our training data. So now if we want to find what the true accuracy is, what we need to do is actually test it on our testing data. So I'm going to steal this line of code here. This is how we test our model. Pretty straightforward. I'll just close this. So let's go into code block. So we have test loss test accuracy is model dot evaluate test images test labels verbose equals one. Now what is verbose? I was hoping it was going to give me the thing so I could just read it to you guys. But verbose essentially is just are we looking at output or not? So like how much information are we seeing as this model evaluates? It's like how much is printing out to the console? That's what that means. And yes, this will just split up kind of the metrics that are returned to this into test loss and test accuracy so we can have a look at it. Now you will notice when I run this, that the accuracy will likely be lower on this than it was on our model. So actually, the accuracy we had from this was about 91. And now we're only getting 88.5. So this is an example of something we call overfitting. Our model seemed like it was doing really well on the testing data or sorry, the training data. But that's because it was seeing that data so often, right with 10 epochs, it started to just kind of memorize that data and get good at seeing that data. Whereas now when we pass it new data that it's never seen before, it's only 88.5% accurate, which means we overfit our model. And it's not as good at generalizing for other data sets, which is usually the goal, right? When we create a model, we want the highest accuracy possible, but we want the highest accuracy possible on new data. So we need to make sure our model generalizes properly. Now in this instance, you know, like, it's, it's hard to figure out how do we do that because we don't know that much about neural networks. But this is the idea of overfitting and of hyper parameter tuning, right? So if we can start changing some of this architecture, and we can change maybe the optimizer, the loss function, maybe we go epochs eight, let's see if this does any better, right? So let's now fit the model with eight epochs, we'll have a look at what this accuracy is. And then we'll test it and see if we get a higher accuracy on the testing data set. And this is kind of the idea of that hyper parameter tuning, right? Well, we just look at each epoch, or not each epoch, we look at each parameter, we tweak them a little bit. And usually we'll like write some code that automates this for us. But that's the idea is we want to get the most generalized accuracy that we can. So I'll wait for this to train. We're actually almost done. So I won't even bother cutting the video. And then we'll run at this evaluation. And we'll see now if we got a better accuracy. Now I'm getting a little bit scared because the accuracy is getting very high here. And sometimes, you know, like you want the accuracy to be high on your training data. But when it gets to a point where it's very high, you're in a situation where it's likely that you've overfit. So let's look at this now. And let's see what we get. So 88.4. So we actually dropped down a little bit. And it seemed like those epochs didn't make a big difference. So maybe if I train it on one epoch, let's have an idea and see what this does. You know, make your prediction, you think we're going to be better, do you think we're going to be worse? It's only seen the training data one time. Let's run this. And let's see 89.34. So in this situation, less epochs was actually better. So that's something to consider. You know, a lot of people I see just go like 100 epochs and just think their model is going to be great. That's actually not good to do. A lot of the times you're going to have a worse model because what's going to end up happening is it's going to be seeing the same information so much tweaking so specifically to that information that it's seen that when you show it new data, it can't actually, you know, classify and generalize on that. All right. So let's go back and let's see what else we're doing now with this. Okay, so now that we've done that, we need to make predictions. So to make predictions is actually pretty easy. So I'm actually just going to copy this line in, we'll go into a new code block down here. So all you have to do is say model that predict, and then you're going to give it an array of images that you want to predict on. So in this case, if we look at test images shape, so actually let's make a new code block and let's go here. So let's say test underscore images dot shape. All right, give me a second. So we have 10,000 by 28 by 28. So this is an array of 10,000 entries of images. Now, if I just wanted to predict on one image, what I could do is say test images, zero and then put that inside of an array. The reason I need to do that is because the data that this model is used to see in is an array of images to make a prediction on, that's what this predict method needs. And it's much better at making predictions on many things at once than just one specific item. So if you are predicting one item only, you do need to put it in an array, because it's used to seeing that form. So we could do this. I'm, I mean, I'm just going to leave it. So we're just going to predict on every single one of the test images, because then we can have a look at a cool function I've kind of made. So let's actually do this predictions equals model dot predict test images. I mean, let's print predictions. And look at actually what it is. Where is my autocomplete? There it is. Okay. So let's have a look. Is this some object? Whoa, okay. So this is a raise of arrays that looks like we have some like really tiny numbers in them. So what this is, is essentially every single, you know, prediction or every single image has a list that represents the prediction for it, just like we've done with kind of the linear models and stuff like that. So if I want to see the prediction for test image zero, I would say prediction zero, right? Let's print this out. And this is the array that we're getting. These this is the probability distribution that was calculated on our output layer for, you know, these, what is it for that image? So if we want to figure out what class we actually think that this is predicting for, we can use a cool function from NumPy called arg max, which essentially is just going to take the index, this is going to return to us the index of the maximum value in this list. So let's say that it was I'm looking for the least negative, which I believe is this, so this should be nine, this should return to us nine, because this is the index of the highest value in this list. Unless I'm just wrong when I'm looking at the negatives here. So nine, that's what we got. Okay, so now if we want to see what the actual classes, while we have our class names up here, so we know class nine is actually ankle boot. So let's see if this is actually an ankle boot. So I'm just going to do class underscore names, I think that's what I called it, like this, so that should print out what it thinks it is. Yeah, class underscore names. But now let's actually show the image of this prediction. So to do that, I'm just going to steal some code from here because I don't remember all the syntax off the top of my head. So this is what it looks like. So let's steal this figure. Let's show this and let's see if it actually looks like an ankle boot. So to do that, we're going to say test underscore images zero, because obviously image zero corresponds to predict prediction zero. And that will show this and see what we get. Okay, so ankle boot, and we'll be looking at the image is actually an ankle boot. And we can do this for any of the images that we want, right? So if I do prediction one, prediction one, now let's have a look pull over kind of looks like a pull over to me. I mean, I don't know if it actually is, but that's what it looks like. You do to to have a look here. Okay, trouser. Yep, looks like trousers to me. And we can see that that is how we get the predictions from our model, we use model dot predict. Alright, so let's move down here now to the next thing that we did. Alright, so we've already done that. So verifying predictions. Okay, so this is actually a cool kind of script that I wrote, I'll zoom out a little bit so we can read it. What this does is let us use our model to actually make. And I've stolen some of this from TensorFlow to make predictions on any entry that we want. So what it's going to do is ask us to type in some number, we're going to type in that number, it's going to find that image in the test data set, it's going to make your prediction on that from the model, and then show us what it actually is versus what it was predicted being. Now, I just need to actually run. Actually, let's just steal this code and bring it in the other one, because I've already trained the model there. So we don't have to wait again. So let's go f 11, f 11, let's go to a new code block, and run that. So let's run this script. Have a look down here. So pick a number, we'll pick some number, let's go 45. And then what it's going to do is say expected sneaker, guess sneaker, and actually show us the image that's there. So we can see this is what you know, our pixel kind of data looks like. And this is what the expected was, and this is what the guess was from the neural network. Now we can do the same thing if we run it again, pick a number 34. Let's see here, expected bag, guess bag. So that's kind of showing you how we can actually use this model. So anyways, that has been it for this kind of module on neural networks. Now I did this in about an hour, I'm hoping I explained a good amount that you guys understand now how neural networks work. In the next module, we're going to move on to convolutional neural networks, which again should help, you know, kind of get your understanding of neural networks up as well as learn how we can do deep computer vision, object recognition and detection using convolutional neural networks. So that being said, let's get into the next module. Hello, everyone, and welcome to the next module in this TensorFlow course. So what we're going to be doing here is talking about deep computer vision, which is very exciting, very cool. This has been used for all kinds of things you ever seen the self driving cars, for example, Tesla, they actually use a TensorFlow deep learning model, obviously very complicated, more than I can really explain here to do a lot of their computer vision for self driving, we've used computer vision in the medicine field, computer vision is actually used in sports a lot for things like goal line technology and even detecting images and players on the field doing analysis, there's lots of cool things are doing with it nowadays. And for our purposes, what we're going to be doing is using this for to perform classification, although it can be used for object detection and recognition, as well as facial detection and recognition as well. So all kinds of applications, in my opinion, one of the cooler things in deep learning that we're doing right now. And let's go ahead and talk about what we're actually going to be focusing on here. So we're going to start by discussing what a convolutional neural network is, which is essentially the way that we do deep learning, we're going to learn about image data. So what's the difference between image data and other regular data? We're going to talk about convolutional layers and pooling layers and how stacks of those work together as what we call a convolutional base for our convolutional neural network. We're going to talk about CNN architectures and get into actually using pre trained models that have been developed by companies such as Google and TensorFlow themselves to perform classification tasks for us. So that is pretty much the breakdown of what we're about to learn. There's quite a bit in this module, it's probably the more difficult one or the most difficult one we've been doing so far. So if you do get lost at any point, or you don't understand some of it, don't feel bad, this stuff is very difficult. And I would obviously recommend reading through some of the descriptions I have here in this notebook, which again, you can find from the link in the description or looking up some things that maybe I don't go into enough enough depth about in your own time, as I can't really spend, you know, 10, 11 hours explaining a convolutional neural network. So let's now talk about image data, which is the first thing we need to understand. So in our previous examples, what we did with when we had a neural network is we had two dimensional data, right, we had a width and height when we were trying to classify some kind of images using a dense neural network. And well, that's what we use two dimensions. Well, with an image, we actually have three dimensions. And what makes up those dimensions? Well, we have a height, and we have a width, and then we have something called a color channels. Now, it's very important to understand this, because we're going to see this a lot as we get into convolutional networks, that the same image is really represented by three specific layers, right? We have the first layer, which tells us all of the red values of the pixels, the second layer, which tells us all the green values, and the third layer, which tells us all the blue values. So in this case, those are the covered channels. And we're going to be talking about channels in depth quite a bit in this series. So just understand that although you think of an image as a two dimensional kind of thing, and our computer, it's really represented by three dimensions where these channels are telling us the color of each pixel. Because remember, in red, green, blue, you have three values for each pixel, which means that you're going to need three layers to represent that pixel, right? So this is what we can kind of think of it as a stack of layers. And in this case, a stack of pixels, right, or stack of colors really telling us the value for each pixel. So if we were to draw this to the screen, we would get the blue, green and red values of each pixel, determine the color of it, and then draw the two dimensional image right based on the width and the height. Okay, so now we're going to talk about a convolutional neural network and the difference between that in a dense neural network. So in our previous examples, when we use the dense neural network to do some kind of image classification, like that fashion, and this data set, what it essentially did was look at the entire image at once and determined based on finding features in specific areas of the image, what that image was, right? Maybe it found an edge here, a line here, maybe it found a shape, maybe it found a horizontal diagonal line. The important thing to understand, though, is that when it found these patterns and learned the patterns that made up specific shapes, it learned them in specific areas. It knew that if we're in between, for example, looking at this cat image, we're going to classify this as a cat, if an eye exists on, you know, the left side of the screen where the eyes are here, then that's a cat. It doesn't necessarily know that if we flipped this cat, we did a horizontal flip of this cat, and the eyes were over here, that that is a pattern that makes up a cat. So the idea is that the dense network looks at things globally, it looks at the entire image and learns patterns in specific areas. That's why we need things to be centered, we need things to be very similar when we use a dense neural network to actually perform image classification, because it cannot learn local patterns, and apply those to different areas of the image. So for example, some patterns we might look for, when we're looking at an image like a cat here would be something like this, right, we would hope that maybe we could find a few ears, we could find the eyes, the nose, and you know, the paws here. And those features would tell us that this makes up a cat. Now with a dense neural network, it would find these features, it would learn them, learn these patterns, we only learn them in this specific area where they're boxed off, which means if I horizontally flip this image, right, and I go like that, then it's not going to know that that's a cat, because it learned that pattern in a specific area, it'll need to relearn that pattern in the other area. Now a convolutional neural network, on the other hand, learns local patterns. So rather than learning that the ear exists in, you know, this specific location, it just learns that this is what an ear looks like, and it can find that anywhere in the image. And we'll talk about how we do that as we get to the explanation. But the whole point is that our convolutional neural network will scan through our entire image, it will pick up features and find features in the image. And then based on the features that exist in that image will pass that actually to a dense neural network or a dense classifier, it will look at the presence of these features and determine, you know, the combination of these presences of features that make up specific classes or make up specific objects. So that's kind of the point. I hope that makes sense. The main thing to remember is that dense neural networks work on a global scale, meaning they learn global patterns, which are specific and are found in specific areas. Whereas convolutional neural networks or convolutional layers will find patterns that exist anywhere in the image, because they know what the pattern looks like, not that it just exists in a specific area. Alright, so how they work, right? So let's see when a neural network, regular neural network looks at this dog image, this is a good example, I should have been using this before, it will find that there's two eyes that exist here, right? And we'll say, okay, so I found that these eyes make up a dog. This is its training image, for example, and it's like, okay, so this pattern makes up the dog, the IR is in this location. Now, what happens when we do this? And we flip the image to the other side. Well, our neural network starts looking for these eyes, right on the left side of the image where it found them previously and where it was trained on. It obviously doesn't find them there. And so it says that our image isn't a dog, although it clearly is a dog, it's just a dog that's orientated differently. In fact, it's just flipped horizontally, right? Or actually, I guess I would say vertically, flip vertically. So since it doesn't find the eyes in this location, and it can only look at patterns that it's learned in specific locations, it knows that this, or it's going to say this isn't a dog, even though it is. Whereas our convolutional layer will find the eyes regardless of where they are in the image, and still tell us that this is a dog, because even though the dogs moved over, it knows what an eye looks like, so it can find the eye anywhere in the image. So that's kind of the point of the convolutional neural network and the convolutional layer. And what the convolutional layer does is look at our image and essentially feedback to us, what we call an output feature map that tells us about the presence of specific features, or what we're going to call filters in our image. So that is kind of the way that works. Now, essentially, the thing we have to remember is that our dense neural networks output just a bunch of numeric values. Whereas what our convolutional layers are actually going to be doing is outputting what we call a feature map. Now I'm going to scroll down here to show you this example. What we're actually going to do is run what we call a filter over our image, we're going to sample the image at all these different areas. And then we're going to create what we call an output feature map that quantifies the presence of the filters pattern at different locations. And we'll run many, many, many different filters over our image at a time. So we have all these different feature maps telling us about the presence of all these different features. So one convolutional layer, we'll start by doing that with very small, simple filters such as straight lines like this. And then other convolutional layers on top of that, right, because it's going to return a map that looks something like this out of the layer, we'll take this map in now, the one that was created from the previous layer, and say, okay, what this map is representing to me, for example, the presence of these diagonal lines, let me try to look for curves, right, or let me try to look for edges. So it will look at the presence of the features from the previous convolutional layer, and then say, okay, well, if I have all these lines combined together, that makes up an edge, and it will look for that, right? And that's kind of the way that a convolutional neural network works and why we stack these different layers. Now, we also use something called pooling, and there's a few other things that we're going to get into. But that is the basics, I'm going to go into a drawing example and show you exactly how that works. But hopefully this makes a little bit of sense that the convolutional layer returns a feature map that quantifies the presence of a filter at a specific location. And this filter, the advantage of it is that we slide it across the entire image. So if this filter or this feature is presence anywhere in the image, we will know about it rather than in our dense network, where it had to learn that pattern in a specific global location. Okay, so let's get on the drawing tablet and do a few examples. All right, so I'm here on my drawing tablet, and we're going to explain exactly how a convolutional layer works, and how the network kind of works together. So this is an image I've drawn on the left side of our screen here. I know this is very basic, you know, this is just an X, right? This is what our images, we're just going to assume this is grayscale, we're going to avoid doing anything with color channels the second just because they're not that important. But just understand that what I'm going to show you does apply to color channels as well and to multiple kind of layers and depth. And then if we can understand it on a simple level, we should be able to understand it more thoroughly. So what we want essentially is our convolutional layer to give us some output. It's meaningful about this image. So we're going to assume this is the first convolutional layer. And what it needs to do essentially is return to us some feature map that tells us about the presence of specific what we call filters in this image. So each convolutional layer has a few properties to it. The first one is going to be the input size. So what can we expect? Wow, what is that that was as the as the input size, how many filters are we going to have so filters like this? And what's the sample size of our filters? That's what we need to know for each of our convolutional neural networks. So essentially, what is a filter? Well, a filter is just some pattern of pixels. And we saw them before, we'll do a pretty basic one here, as the filter we're going to look for, which will look something like this. This will be the first filter we're going to look for just to illustrate how this works. But the idea is that at each convolutional layer, we look for many different filters. And in fact, the number we're typically looking for is actually about times 32 filters. Sometimes we have 64 filters as well. And sometimes even 128. So we can do as many filters as we want, as few filters as we want. But the filters are what is going to be trained. So this filter is actually what is going to be found by the neural network. It's what's going to change. It's, you know, this is essentially what we're looking for. This is what's created in the program. And that's kind of like the trainable parameter of a convolutional neural network is the filter. So the amount of filters and what they are will change as the program goes on, as we're learning more and figuring out what features that make up, you know, a specific image. So I'm going to get rid of this stuff right now, just so we can draw and do a basic example. But I want to show you how we look for a filter in the image. So we have filters, right, they'll come up with them, they're gonna start completely random, but they'll change as we go on. So let's say the filter we're looking for is that one I drew before, I'm just going to redraw it at the top here a little bit smaller. And we'll just say it's a diagonal line, right? But another filter we could look for might be something like, you know, a straight line, just like that all across, we could have a horizontal line. And in fact, we'll have 32 of them. And when we're doing just, you know, three by three grids of filters, well, there's not that many, you know, combinations, we're going to do at least grayscale wise. So what we'll do is we'll define the sample size, which is how big our filter is going to be three by three, which we know right now, which means that what we're going to do is we're going to look at three by three spots in our image, and look at the pixels, and try to find how closely these filters match with the pixels we're looking at on each sample. So what this is going to do, this convolutional layer is going to output us what we call a feature map, which can be a little bit smaller than the original image. And you'll see why in a second, but that tells us about the presence of specific features in areas of the image. So since we're looking for two filters here, actually, we'll do two filters, which means that we're actually going to have a depth to feature map being returned to us, right? Because for two filters, that means we need two maps, quantifying the presence of both of those filters. So for this green box that we're looking on at the left side here, we'll look for this first filter here. And what do we get? Well, the way we actually do this, the way we look at this filter, is we take the cross product, or actually not the cross product, the dot product, sorry, between this little green box and this filter, right, because they're both pixels, they're both actually numeric values down at the bottom. So what we do is we take that dot product, which essentially means we're element wise, adding, or what is it element wise, multiplying all of these pixels by each other. So if this pixel values is zero, right, because it's white, or it could be the other way around, we could say white is one, black is zero, it doesn't really matter, right? If this is a zero, and this is a one, these are obviously very different. And when we do the dot product of those two, so we multiply them together, then in our output feature, we would have a zero, right, that's kind of the way it works. So we do this dot product of this entire thing. If you don't know the dot product is I'm not really going to go into that. But we do the dot product, and that gives us some value essentially telling us how similar these two blocks are. So how similar this sample is that we're taking of the image and the filter that we're looking for. They're very similar, we're going to likely put a one or something telling us, you know, they're very close together. They're not similar at all, we're going to put a zero. So in this case, for our first filter, we're probably going to have a value because this middle pixel is the same as something like 0.12, right? But the all the other values are different. So it's not going to be very similar whatsoever. So then what we're going to do now is we'll look at the actually second filter, which is this horizontal line. And in fact, we're going to get a very similar output response here, probably something like, you know, 0.12, that's going to go in the top left. And again, these are both maps representing each filter, right? So now we'll move our green box over one, like this. So just shift that over one. And now we'll start looking at the next section. And in fact, I'm going to see if I can erase this just to make it a little bit cleaner here. Get rid of the green, there we go. Okay, so we'll move this box over like this. And now we'll start looking at this one, we'll do the exact same thing we did again before. So we're going to say, alright, how similar are these? Well, they're not similar at all. So we're going to get a zero for that first filter. How similar the other ones? Oh, actually, they're like a little bit similar. There's a lot of white that's kind of in the same space, like, you know, stuff like that. So we'll say maybe this is like 0.7, right? I'm just randomly picking these numbers. They are going to be much different than what I'm putting in here. But I'm just trying to get you to understand what's kind of happening, right? And this is completely random, the way I'm making the numbers to just make sure you understand that because this is not exactly what it would look like. Okay, so then we're going to move the box over one more time. Let's just erase this to keep this clean. This will be the last time we do this for the purpose of this example. And now what we're going to have is wow, we have a perfect match for the first filter. So we put one, the other ones like ads kind of similar as a few things that are different. So maybe this gets like a 0.4 or something, right? Whatever they are, we end up getting some value. So we'll fill in all these values, let's just put some arbitrary values here for now, just so we can do something with the example 0.7, 0, 0.12, 0.42, 0.3, 0.9, 0.1, again, completely random, 0.4, 0.6. Alright, so this is now what we've gotten our response map from looking at two filters on our original image of five by five. Now notice that the size of these is three by three. And obviously, the reason for that is because in a five by five image, when we're taking three by three samples, well, we can only take nine three by three samples, because when we go down a row, right, we're going to move down one, and we're going to do the same thing we did before of this, these three by three samples. And if we add the amount of times we can do that, well, we just get three by three, which is not. So this now is kind of telling us of the presence of features in this original image map. Now the thing is, though, we're going to do this 64 times, right, for 64 filters or 32 filters or the amount of filters that we have. So we're going to have a lot of layers, like a ton of different layers, which means that we're going to be constantly expanding as we go through the convolutional layers, the depth of this, this kind of output feature map. And that means that there's a lot of computations that need to be done. And essentially, that means that this can be very slow. So now we need to talk about an operation called pooling. So I'll backtrack a little bit, but we will talk about pooling in a second. What's going to happen, right, is when we have all these layers that are generated, so this is called the output feature map, right, from this original image. What we're going to do is the next convolutional layer in the network is now going to do the process we just talked about, except on this output feature map, which means that since this one was picking up things like lines and edges, right, the next convolutional layer, we'll pick up combinations of lines and edges and maybe find what a curve is, right, we'll slowly work our way up from very, very small amount of pixels, defining more and more, almost I want to say abstract, different features that exist in the image. And this is what really allows us to do some amazing things with a convolutional neural network, when we have a ton of different layers stacking up on each other, we can pick out all the small little edges, which are pretty easy to find. And with all these combinations of layers working together, we can even find things like, say, eyes, right, or feet, or heads or face, right, we can find very complicated structures, because we slowly work our way up starting by solving very easy problem, which are like finding lines, and then finding combinations of lines, combination of edges, shapes and very abstract things. That's how this convolutional network works. So we've done that now, it's now time to talk about pooling. And we'll also talk about pat actually, we'll go padding first before we go pooling, I just, it doesn't really matter what order we talk about this in. But I just think padding makes sense based on the way we're going right now. So sometimes we want to make sure that the output feature map from our original image here is the same dimensions or same size as this, right? So this was five by five, obviously. And this is three by three. So if we want this to be five by five, as an output, what we need to do is add something called padding to our original image. So padding is essentially just adding an extra row and column on each side of our image here. So that when we, and we just fill in all these pixels in like kind of the padded pixels here, I just blank random pixels, they don't mean anything. Essentially, why we do that is so that when we do our three by three sample size here like this, we can take a three by three sample where every single pixel is in the center of that sample. Because right now, this pixel is not in the center. This pixel can never be in the center, this pixel can never be in the center, only, you know, a few pixels get to be in the center. And what this allows us to do is generate an output map that is the same size as our original input, and allows us to look at features that are maybe right on the edges of images that we might not have been able to see before. Now, this isn't super important when you go to like very large images, but it just something to consider you can add padding, we may do this as we get through our examples. And there's also something called stride, which I want to talk about as well. So what a stride is is essentially how much we move this sample box, every time that we're about to move it, right? So before, like, so let's say we're doing example with padding here, right, we are first sample, we would take here. And again, these pixels are just added, we added them in to make this work better for us. You would assume that the next time we move the box, we're going to move it one pixel over, that's called a stride of one, we can do that. But we also can employ stride of two, which means we'll move over by two. Obviously, the larger your stride, the smaller your output feature map is going to be. So you might want to add more padding. Well, you don't want to add too much padding, but it's just something to consider. And we will use a stride in different instances. Okay, so that's great. That hopefully makes sense. Let's erase this. Now we don't need this anymore. We talked about padding, we talked about the stride. And now we're going to talk about a pooling operation, which is very important. So kind of the idea is that we're going to have a ton of layers, right, for all these filters. And we're just going to have a lot of numbers, a lot of computations. And there must be some way to make these a little bit simpler, a little bit easier to use. Well, yes, that's true. And there is a way to do that. And that's called pooling. So there's three different types of pooling. Well, it is more but the basic ones are min, max, and average. And essentially, a pooling operation is just taking specific values from a sample of the output feature map. So once we generate this output feature map, what we do to reduce its dimensionality and just make it a little bit easier to work with is what we sample typically two by two areas of this output feature map. And just take either the min, max or average value of all the values inside of here and map these, we're going to go back this way to a new feature map that's twice, like one times the size essentially, or not, what am I saying, two times smaller than this original map. And it's kind of hard with three, like three by three, to really show you this. But essentially, what's going to end up happening is we're going to have something like this. So we're going to take the sample here, we're going to say, okay, what are we doing min, max or average pooling, if we're doing min pooling, we're going to take the smallest value, which means we'll take zero. If we're doing max pooling, we'll take the maximum value, which means we'll take 0.3. If we're doing average, we're probably going to get an average value of close to what 0.2, maybe. So let's say 0.2, we'll go there. That's how we do that with pooling. Again, just to make this feature map smaller. So we'll do that for both of the filters. But let's just say this is 0.2. Let's say this here that I'm blocking off is, I don't know, what is this going to be 0.6? It's hard to do this average with four numbers, let's say this one down here is going to be 0. I don't know, let's just do two one or something. And then this last one here, we okay, we got some bigger values, maybe this will be like 0.4. Okay, so that's one, the one down here will have some values of its own, we'll just do squiggles to represent that it has something. And we've effectively done a max pooling operation on this, we've reduced the size of it by about half. And that is kind of how that works. Now, typically what we do is we use a two by two pooling or like sample size like that, with a stride of two, which actually means that we would straw it like this. But since we're not going to do padding on this layer right now, we'll just do a stride of one. And this is how we pull it. Now the different kinds of pooling are used for different kind of things. The reason we would use a max pooling operation is to pretty much tell us about the maximum presence of a feature in that kind of local area. We really only care if the feature exists, where if it doesn't exist, an average pooling is not very often used, although in this case, we did use an average pooling. But you know, it's just different kinds of pooling and average tells you about the average presence of the feature in that area. Max tells you about is that feature present in that area at all and men tells you does it not exist. If it doesn't exist, right, we're just going to have a zero if there's even one zero in that area. So that's the point of pooling. That's the point of convolutional layers. I think I'm done with the white boarding for now, we're actually going to start getting into a little bit of code and talking about creating our own convolutional networks, which hopefully will make this a lot more clear. So let's go ahead and get into that. All right, so now it is time to create our first convolutional neural network. Now we're going to be using Keras to do this. And we're also going to be using the CI FAR image data set that contains 60,000 images of 10 different classes of everyday objects. Now these images are 32 by 32, which essentially means they are blurs, and they are colorful. Now, I just want to emphasize as we get into this, that the reason I'm not typing all of these lines out and I just have them in here already is because this is likely what you guys will be using or doing when you actually make your own models. Chances are that you are not going to sit unless you're a pro at TensorFlow, and I am not even there yet either with my knowledge of it, and have all of the lines memorized and not have to go reference the syntax. So the point is here, so long as you can understand why this works and what these lines are doing, you're going to be fine, you don't need to memorize them and I have not memorized them. And I don't I look up for the documentation, I copy and paste what I need, I alter them, I write a little bit of my own code. But that's kind of what you're going to end up doing. So that's what I'm doing here. So this is the image data set. We have truck, horse, ship, airplane, you know, just some everyday, regular objects, there is 60,000 images, as we said, and 6000 images of each class. So we don't have too many images of just one specific class. So we'll start by importing our modules. So TensorFlow, we're going to import TensorFlow dot Keras, we're going to use the data set built into Keras for this. So that's the CI FR image data set, which you can actually look at just by clicking at this, it'll bring you and give the information about the data set. Although we don't need that right now, because I already know the information about it. And now we're just going to load our images in. So again, this stuff, the way this works is you're gonna say data sets dot CI FR 10 dot load data. Now this loads it in as like a very strange TensorFlow object, that's like a data set object. So this is different from what we've used before, where some of our objects have actually been like in NumPy arrays, where we can look at them better. This is not going to be in that. So just something to keep in mind here. And we're going to normalize this data into train images and test images, but just dividing both of them by 255. Now, again, we're doing that because we want to make sure that our values are between zero and one, because that's just a lot better to work with in our neural networks, rather than large integer values, just causes, you know, some things to mess up sometimes. Now class names, we're just going to find a list here. So we have all the class names so that zero represents airplane one auto avail so far in tilt truck, run that block of code here, we'll download this data set, although I don't think it takes that long to do that. So okay, so wait, I guess, yeah, I guess that's good. I think we're okay there. And now let's just have a look at actually some of the images here by running this script. So we can see this is a truck can change the image index to be two, we can see this is another truck. Let's go to say six, we get a bird. And you can see these are really blurry. But that's fine. For this example, we're just trying to get something that works all right. Okay, so that's a horse, you know, you get the point. All right. So now CNN architecture. So essentially, we've already talked about how a convolutional neural network works, we haven't talked about the architecture and how we actually make one. Essentially, what we do is we stack a bunch of convolutional layers and max pooling, min pooling or average pooling layers together in something like this, right. So after each convolutional layer, we have a max pooling layer, some kind of pooling layer typically to reduce the dimensionality, although you don't need that, you could just go straight into three convolutional layers. And on our first layer, what we do is we define the amount of filters just like here, we define the sample size. So how big are those filters and activation function, which essentially means after we apply that, what is it that cross not cross product dot product operation that we talked about, we'll apply rectifier linear unit to that and then put that in the output feature map. Again, we've talked about activations functions before. So I won't go too far into depth with them. And then we define the input shape, which essentially means what can we expect in this first layer? Well, 32 by 32 by three, these ones, we don't need to do that, because they're going to figure out what that is based on the input from the previous layer. Alright, so these are just a breakdown of the layers. The convolution or the max pooling layers here, two by two, essentially means that what we're going to do is we're going to have a two by two sample size with actually a stride of two. Again, the whole point of this is to actually divide or, you know, shrink it by a factor of two, how large each of these layers are. Alright, so now let's have a summary. It's already printed out here. We can see that we have, oh, wait, is this correct? mobile net v two, I don't think that's correct. That's because I haven't run this one. My apologies on that guys, this is from something later in the tutorial, we can see that we have calm 2d as our first layer. This is the output shape of that layer. Notice that it is not 32 by 32 by 32. It is 30 by 30 by 32, because when we do that sampling without padding, right, that's what we're going to get. We're going to get two pixels less, because the amount of samples we can take. Alright, next, we have the max pooling 2d layer. So this now says the output shape is 15 by 15 by 32, which means we've shrunk this shape by a factor of two, we do a convolution on this, which means that now we get 1313 and we're doing 64, because we're going to take 64 filters this time. And then max pooling again, we go six by six by 64, because we're going to divide this again by factor of two. Notice that it just rounded, right? And then calm 2d. So another layer here, we get four by four by 64, again, because of the way we take those values. So this is what we've defined so far. But this is not the end of our convolutional neural network. In fact, this doesn't really mean much to us, right? This just tells us about the presence of specific features, as we've gone through this convolution base, which is what this is called the stack of convolution and max pooling layers. So what we actually need to do is now pass this information into some kind of dense layer classifier, which is actually going to take this pixel data that we've kind of calculated and found. So the almost extraction of features that exist in the image, and tell us which combination of these features map to either, you know, what one of these 10 classes are. So that's kind of the point you do this convolution base, which extracts all of the features out of your image. And then you use the dense network to say, Okay, well, if these combination of features exist, then that means this image is this, otherwise, it's this and that and so on. So that's what we're doing here. Alright, so let's say adding the dense layers. So to add the dense layers pretty easy model dot add is just how we add them, right? So we're going to flatten all of those pixels was which essentially means take the four by four by 64, and just put those all into a straight line, like we've done before. So just one dimensional, then we're going to have a 64 neuron dense layer that connects all of those things to it with an activation function of rectifier linear unit, then our output layer of a dense layer with 10 neurons, obviously 10, because that's the amount of classes we have for this problem. So let's run this here, we'll add those layers, let's look at a summary and see how things have changed now. So we go from four by four by 64 to 2024. Notice that that is precisely the calculation of four times four times 64. That's how we get that number here. Then we have a dense layer and another dense layer. And this is our output layer. Finally, this is what we're getting is we're going to get 10 neurons out. So essentially, just a list of values. And that's how we can determine which class is predicted. So this up to here is the convolutional base, this is what we call the classifier, and they work together to essentially extract the features, and then look at the features and predict the actual object or whatever it is the class. Alright, so that's how that works. Now it's time to train again, we'll go through this quickly. I believe I've already trained this this takes a long time to train. So I'm actually going to reduce the epochs here to just be for I'd recommend you guys train this on higher. So like 10, if you're going to do it, it does take a while. So for our purposes, and for my time, we'll leave a little bit shorter right now, but you should be getting about 70% accuracy. And you can see I've trained this previously, if you train it on 10 epochs, but I'm just going to train up to four, we get our 6768%. And that should be fine. So we'll be back once this is trained, then we'll talk about how some of this works. Okay, so the model is finally finished training, we did about four epochs, you can see we got an accuracy about 67% on the evaluation data. To quickly go over this stuff. optimizers, Adam talked about that before. loss function is sparse categorical cross entropy. That one, I mean, you can read this if you want computes the cross entropy loss between the labels and predictions. And I'm not going to go into that. But these kind of things are things that you can look up if you really understand why they work. For most problems, you can just if you want to figure out what, you know, loss function or optimizer to use, just use the basics, like use Adam, use a categorical cross entropy, using a classification task, you want to do something like this, there's just you can go up and look kind of all of the different loss functions, and it'll tell you when to use which one and you can kind of mess with them and tweak them if you want. Now history equals model dot fit. This is just so we can access some of the statistics from this model dot fit. Obviously, it's just training the data to this test images, test labels and train images and train labels where this is the validation data suite. So evaluating the model, we want to evaluate the model, we can evaluate it now on the test images and test labels, we're obviously going to get the same thing because the valuation is test images and test labels. So we should get the same accuracy as 6735, which we do right here. Alright, so there we go, we get about 70%. If you guys train this on 10 epochs, you should get close to 70. I'm a little bit lower just because I didn't want to go that high. And that is now the model. I mean, we could use this if we want, we could use predict, we could pass in some image, and we could see the prediction for it. I'm not going to do that just because we've already talked about that enough. And I want to get into some of the cooler stuff when we're working with smaller data sets. So the basic idea here is this is actually a pretty small data set, right? We use about 60,000 images. And if you think about the amount of different patterns we need to pick up to classify, you know, things like horses versus trucks, that's a pretty difficult task to do, which means that we need a lot of data. And in fact, some of the best convolutional networks that are out there are trained on millions of pieces of, you know, sample information or data. So obviously, we don't have that kind of data. So how can we work with, you know, a few images, maybe like a few 1000 images, and still get a decent model. Well, the thing is, you can't unless we use some of the techniques that I have to show you. So working with small data sets. So just like I mentioned, it's difficult to create a very good convolutional neural network from scratch, if you're using a small amount of data, that is why we can actually employ these techniques, the first one data augmentation, but also using pre trained models to kind of accomplish what we need to do. And that's what we're going to be talking about now in the second part of the tutorial, we're going to create another convolutional neural network. So just to clarify, this is created, we've made the model up here already. This is all we need to do to do it. This is the architecture. And this was just to get you familiar with the idea. So data augmentation. So this is basically the idea, if you have one image, we can turn that image into several different images, and train and pass all those images to our, our model. So essentially, if we can rotate the image, if we can flip it, if we can stretch it, compress it, you know, shift it, zoom it, whatever it is, and pass that to our model, it should be better at generalizing, because we'll see the same image, but modified and augmented multiple times, which means that we can turn a data set say of 10,000 images into 40,000 images, by doing four augmentations on every single image. Now, obviously, you still want a lot of unique images, but this technique can help a lot and is used quite a bit, because that allows our kind of model to be able to pick up images that maybe are orientated differently or zoomed in a bit or stretch something different, right, just better at generalizing, which is the whole point. So I'm not going to go through this in too depth, too much depth, but this is essentially a script that does data augmentation for you. We're gonna use this image data generator from the Keras dot preprocessing dot image module, we're going to create an image data generator object. Now essentially, what this allows us to do is specify some parameters on how we want to modify our image. In this case, we have the rotation range, some shifts, shear, zoom horizontal flip and the mode. Now I'm not going to go into how this works, you can look at the documentation if you'd like. But essentially, this will just allow us to augment our images. Now what I'm going to do is pick one arbitrary image from the test image data set, just our test image, I guess, group of photos, whatever you want to call it. I'm going to convert that to an image array, which essentially takes it from the weird data set object that it kind of is and turns it into a NumPy array. Then we're going to reshape this. So that's in the form one comma, which essentially means one, and then this will figure out what the rest of the shape should be. Oh, sorry, one and then plus the image shape, which is whatever this shape is. So we'll reshape that. And then what we're going to do is we're going to say for batch in data flow gen dot flow. Talk about how that works in a second. Essentially, this is just going to augment the image for us and actually save it onto our drive. So in this instance, what's going to happen is this data gen dot flow is going to take the image which we've created here, right? And we formatted it correctly by doing these two steps, which you need to do beforehand, it's going to save this image as test dot jpeg. And this will be the prefix, which means there'll be some information after. And it will do this as many times until we break. So essentially, given an image, it will do test one, test two, test three, test four, test five, with random augmentations using this, until eventually we decided to break out of this. Now what I'm doing is just showing the image by doing this and batch zero is just showing us the you know, that first image in there. And that's kind of how this works. So you can mess with the script and figure out a way to use it. But I would recommend if you want to do data augmentation, just look into image data generator. This is something that I just want to show you so you're aware of and I'll just run it so you can see exactly how this works. So essentially, given an image of a truck, what it will do is augmented in these different ways. You can see kind of the shifts, the translations, the rotations, all of that. And we'll do actually a different image here to see what one looks like. Let's just do image say 20. See if we get something different. So in this case, I believe this is maybe like a deer or rabbit or a dog or something. I don't really know exactly what it is because it's so blurry. But you can see that's kind of the shifts we're getting. And it makes sense because you want to have images in different areas so that we have a better generalization. All right, let's close that. Okay, so now we're going to talk about using or sorry, what is it pre trained models? Okay, so we talked about data augmentation. That's a great technique if you want to increase the size of your data set. But what if even after that, we still don't have enough images in our data set? Well, what we can do is use something called a pre trained model. Now companies like Google and you know, TensorFlow, which is owned by Google, make their own amazing convolutional neural networks that are completely open source that we can use. So what we're going to do is actually use part of a convolutional neural network that they've trained already on, I believe 1.4 million images. And we're just going to use part of that model as kind of the base of our models that we have a really good starting point. And all we need to do is what's called fine tune, the last few layers of that network, so that they work a little bit better for our purposes. So what we're going to do essentially say, All right, we have this model that Google's trained, they've trained it on 1.4 million images, it's capable of classifying, let's say 1000 different classes, which is actually the example we'll look at later. So obviously, the beginning of that model is what's picking up on the smaller edges, and you know, kind of the very general things that appear in all of our images. So if we can use the base of that model, so kind of the beginning of it, that does a really good job picking up on edges and general things that will apply to any images. Then what we can do is just change the top layers of that model a tiny bit or add our own layers to it to classify for the problem that we want. And that should be a very effective way to use this pre trained model. We're saying we're going to use the beginning part that's really good at kind of the generalization step, then we'll pass it into our own layers that we'll do whatever we need to do specifically for our problem. That's what's like the fine tuning step. And then we should have a model that works pretty well. And in fact, that's what we're going to do in this example now. So that's kind of the point of what I'm talking about here is using part of a model that already exists, that's very good at generalizing, and it's been trained on so many different images. And then we'll pass our own training data in, we won't modify the beginning aspect of our neural network, because it already works really well, we'll just modify the last few layers that are really good at classifying, for example, just cats and dogs, which is exactly the example we're actually going to do here. So I hope that makes sense as we get through this should be cleared up a little bit. But using a pretrained model is now the section we're getting into. So this is based on this documentation, as always, I'm referencing everything. So you guys can go see that if you'd like, and do our imports like this, we're going to load a data set that actually takes a second to load the data set, I believe, oh, maybe not. And essentially, the problem we're doing is trying to classify dogs versus cats with a fair degree of accuracy. In fact, we'd like to get above 90%. So this is the data set we're loading in from TensorFlow data sets as TFDS. This is kind of a weird way to load it in again, stuff like this, you just have to reference the documentation, I can explain it to you, but it's not really going to help when the next example is going to be a different way of loading the data, right? So so long as you know how to get the data in the correct form, you can get it into some kind of NumPy array, you can split it into training, testing and validation data, you should be okay. And if you're using a TensorFlow data set, it should tell you in the documentation how to load it in properly. So we'll load it in here, we're training 80% train will go 10% for what is it raw validation and 10% for the testing data. So we've loaded that. And now what we're doing here is just we're going to look at a few images. So this actually creates a function, I know this is a weird thing, this is pretty unique to this example, that allows us to call this function with some integer, essentially, and get what the actual string representation of that is to the label for it. And what I'm doing here is just taking two images from our raw training data set, and just displaying them. And you can see that's where we're getting here dog and dog. If I go ahead and take five, we'll see, these are what our images look like. Right? So here's example of a dog, we have a cat, right? And so on so forth, you kind of you get the you get the point there. Now, notice, though, that these images are different dimensions. In fact, none of these images other than these two actually are the same dimension at all. Oh, actually, I don't think these ones are either. So obviously, there's a step that we need to do, which is we need to scale all these images to be the same size. So to do that, what we're going to do is write a little function like this, and essentially, we'll return an image that is reshaped. So I guess that is reshaped to the image size, which I'm going to set at 160 by 160. Now, we can make this bigger if we want. But the problem sometimes is if you make an image that is bigger than like you want to make your image bigger than most of your data set examples, and that means you're going to be really stretching a lot of the examples out and you're losing a lot of detail. So it's much better to make the image size smaller rather than bigger. You might say, well, if you make it smaller, you're going to lose detail too. But it's just it's better to compress it smaller than it is to go really big, even just when it comes to the amount of training time and how complex networks going to be. So that's something to consider. You can mess around with those when you're making your own networks. But again, smaller is typically better in my opinion, you don't want to go too small, but something that's like, you know, half the size of what an average image would be. Alright, so we're going to go format example. So we're going to just take an image and a label. And what this will do is return to us just the reshaped image and label. So in this case, we're going to cast, which means convert every single pixel in our image to be a float 32 value, because it could be integers, we're then going to divide that by 127.5, which taken is exactly half of 255. And then subtract one, then we're going to resize this image to be the image size. So sorry, the image will be resized to the image size of 160 by 160 and we'll return the new image and the label. So now we can apply this function to all of our images using map, if you don't know what map is, essentially, it takes every single example in in this case, going to be raw train and applies the function to it, which will mean that it will convert raw train into images that are all resized to 160 by 160. And we'll do the same thing for validation and test. So run that no issue there. Now let's have a look at our images and see what we get. And there we are. Now I've just messed up the color because I didn't add a CMAP thing, which I think I needed. Where was the CMAP? Anyways, you know what, that's fine for now. This is what our images look like. This is the resize. Now we get all images 160 by 160. And we are good to go. Alright, so now let's have a look at the shape of an original image versus our new image. So I mean, this was just to prove that essentially our original shapes were like 262 409 by some random values, and they're all reshaped now to 160 160 by three, three obviously is the color channel of the images. Alright, so picking a pre trained model. So this is the next step, this is probably one of the harder steps is picking a model that you would actually like to use the base up. Now we're going to use one called mobile net v two, which is actually from Google, it's built into TensorFlow itself. That's why I've picked it. And all we're going to do is set this. So essentially, we're going to say the base model in our code is equal to TF dot keras dot applications dot mobile net v two, which is just telling us the architecture of the model that we want. And we'll have a look at it down below here. In just a second, we'll define the input shape, which is important, because this can take any input shape that we want. So we'll change it to 160 160 by three, which we've defined up here, include top, very important means do we include the classifier that comes with this network already or not. Now in our case, we're going to be retraining parts of this network so that it works specifically for dogs and cats, and not for 1000 different classes, which is what this model was actually aimed to do is train a 1.4 million images for 1000 different classes of everyday objects. So we're going to not include the top, which means I don't include the classifier for these 1000 classes. And we're going to load the weights from what's called image net, which is just a specific save of the weights. So this is the architecture. And this is kind of the data that we're filling in for that architecture. So the weights, and we'll load that in which we have here. So base model, now let's look at it. So let's have a summary. You can see this is a pretty crazy model. I mean, we would never be expected to create something like this by ourselves. This is, you know, teams of data scientists, PhD students, engineers, would I write the experts in the field that have created a network like this. So that's why we're going to use it because it works so effectively for the generalization at the beginning, which is what we want. And then we can take those features that this takes out. So in five by five by 1280, which is what I want us to focus on the output of this actual network here. So really, you can see this last layer, we're going to take this and using this information, pass that to some more convolutional layers and actually our own classifier, I believe, and use that to predict versus dogs versus cats. So at this point, the base model will simply output a shape 32 by five by five by 1280. That's the tensor that we're going to get out of this, that's the shape, you can watch how this kind of works as you go through it. And yes, all right. So we can just have a look at this here. This what I wanted to do essentially was just look at what the actual shape was going to be. So 32 five by five by 1280, just because this gives us none until it knows what the input is. And now it's time to talk about freezing the base. So essentially, the point is, we want to use this as the base of our network, which means we don't want to change it. If we just put this network in right now is the base to our neural network. Well, what's going to happen is, it's going to start retraining all these weights and biases. And in fact, it's going to train 2.257 million more weights and biases, when in fact, we don't want to change these because these have already been defined, they've been set. And we know that they work well for the problem already, right, they worked well for classifying 1000 classes. Why are we going to touch this now? And if we were going to touch this, what's the point of even using this base, right, we don't want to train this, we want to leave it the same. So to do that, we're just going to freeze it. Now, freezing is a pretty, I mean, it just essentially means turning the trainable attribute of a layer off or of the model off. So what we do is you just say base model dot trainable equals false, which essentially means that we are no longer going to be training any aspect of that, I want to say model, although we'll just call it the base layer for now, or the base model. So now if we look at the summary, we can see when we scroll down to the bottom, if we get there any day soon, that now the trainable parameters is zero instead of 2.257 million, which it was before. And now it's time to add our own classifier on top of this. So essentially, we've got a pretty good network, right, five by five by 1280s, our last output. And what we want to do now is take that. And we want to use it to classify either cat or either dog, right? So what we're going to do is add a global average layer, which essentially is going to take the entire average of every single so 1280 different layers that are five by five, and put that into a 1d tensor, which is kind of flattening that for us. So we do that global average pooling. And then we're just going to add the prediction layer, which essentially is going to just be one dense node. And since we're only classifying two different classes, right, dogs and cats, we only need one, then we're going to add all these models together. So the base model, and I guess layers, the global average layer that we define there, and then the prediction layer to create our final model. So let's do this global average layer, prediction layer model, give that a second to kind of run there. Now when we look at the summary, we can see we have mobile net v2, which is actually a model, but that is our base layer. And that's fine, because the output shape is that then global average pooling, which again, just takes this flattens it out does the average for us. And then finally, our dense layer, which is going to simply have one neuron, which is going to be our output. Now notice that we have 2.25 and nine million parameters in total, and only 1281 of them are trainable. That's because we have 1280 connections from this layer to this layer, which means 1280 weights and one bias. So that is what we're doing. This is what we have created now, this base, the majority of the network has been done for us. And we just add our own little classifier on top of this. And now we're going to feed some training samples and data to this. Remember, we're not training this base layer whatsoever. So the only thing that needs to be learned is the weights and biases on these two layers here. Once we have that, we should have a decent model ready to go. So let's actually train this now. I'm going to compile this here, I'm picking a learning rate that's very slow, what essentially what the learning rate means is how much am I allowed to modify the weights and biases of this network, which is what I've done, just made that very low, because we don't want to make any major changes if we don't have to, because we're already using a base model that exists, right? So we'll set the learning rate, I'm not going to talk about what this does specifically, you can look that up if you'd like to. And then the loss function will use binary cross entropy, just because we're using two classes, if you're using more than more than two classes, you would just have cross entropy, or some other type of cross entropy. And then what we're going to do is actually evaluate the model right now, before we even train it. So I've compiled it, I've just set what we'll end up using. But I want to evaluate the model currently, without training it whatsoever, on our validation data or validation batches, and see what it actually looks like, what it actually, you know, what we're getting right now, with the current base model being the way it is, and not having changed the weights and biases that completely random from the global average pooling in the dense layer. So let's evaluate. Let's see what we get as an accuracy. Okay, so we can actually see that with the random weights and biases for those last layer that we added, we're getting an accuracy of 56%, which pretty much means that it's guessing, right? It's, you know, 50% is only two classes. So if we got anything lower than 50, like 50 should have been our guess, which is what we're getting. So now what we're going to do. And actually, I've trained this already, I think so I might not have to do it again, is train this model on all of our images. So all of our images and cats and cats and dogs that we've loaded in before, which will allow us now to modify these weights and biases of this layer. So hopefully it can determine what features need to be present for a dog to be a dog and for a cat to be a cat, right? And then it can make a pretty good prediction. In fact, I'm not going to train this in front of us right now, because it actually takes close to an hour to train just because there is a lot of images that it needs to look at and a lot of calculations that need to happen. But when you do end up training this, you end up getting an accuracy of a close to 92 or 93%, which is pretty good, considering the fact that all we did was use an original layer, like base layer that classified up to 1000 different images, so very general, and applied that just to cats and dogs by adding our dense layer classifier on top. So you can see this was kind of the accuracy I had from training this previously, I don't want to train again, because it takes so long. But I did want to show that you can save a model and load a model by doing this syntax. So essentially, on your model object, you can call model dot save, save it as whatever name you like dot h5, which is just a format for saving models and Keras is specific to Keras, not TensorFlow. And then you can load the model by doing this. So this is useful because after you train this for an hour, obviously, you don't want to retrain this if you don't have to to actually use it to make predictions. So you can just load the model. Now, I'm not going to go into using the model specifically, you guys can look up the documentation to do that. We're at the point now where I've showed you so much syntax on predicting and how we actually use the models. But the basic idea would be to do model dot predict, right? And then you can see that it's even giving me the input here. So model dot predict, give it some x batch size or both, right, because it will predict on multiple things. And that will spit back to you a class, which then you can figure out, okay, this is a cat, or this is a dog, you're going to pass this obviously the same input information we have before, which is 160 by 160 by three. And that will make the prediction for you. So that's kind of the thing there. I was getting an OS error just because I hadn't saved this previously. But that's how you save and load models, which I think is important when you're doing very large models. So when you fit this, feel free to change the epochs to be something slower if you'd like, again, right, this takes a long time to actually end up running. But you can see that the accuracy increases pretty well, exponentially, exponentially from when we didn't even have that classifier on it. Now, the last thing that I want to talk about is object detection, I'm just going to load up a page, we're not going to do any examples, I'm just going to give you a brief introduction, because we're kind of running out of time for this module, because you can use TensorFlow to do object detection and recognition, which is kind of cool. So let's get into that now. Okay, so right now, I'm on a GitHub page that's built by TensorFlow here, I'm going to leave that link in the notebook where it said object detection, so you guys can look at that. But essentially, there is an API for TensorFlow that does object detection for you. And in fact, it works very well, and even gives you confidence scores. So you can see this is what you'll actually end up getting if you end up using this API. Now, unfortunately, we don't have time to go through this because this will take a good amount of time to talk about the setup and how to actually use this project properly. But if you go through this documentation, you should be able to figure it out. And now you guys are familiar with TensorFlow, and you understand some of the concepts here. This runs a very different model than what we've discussed before. Unfortunately, again, we don't have time to get into it. But just something I wanted to make clear is that you can do something like this with TensorFlow. And I will leave that resource so that if you'd like to check this out, you can use it. There's also a great module in Python called facial recognition, it's not a part of TensorFlow. But it does use some kind of convolutional neural network to do facial detection and recognition, which is pretty cool as well. So I'll put that link in here. But for that, for now, that's going to be our, what is it convolutional neural network kind of module. So I hope that says cleared some things up on how deep vision works and how convolutional neural networks work. I know I haven't gone into crazy examples of what I've shown you some different techniques that hopefully you'll go look up kind of on your own and really dive into. Because now you have that base kind of domain knowledge where you're going to be able to follow along with the tutorial and understand exactly what to do. And if you want to create your own model, so long as you can get enough sufficient training data, you can load that training data into your computer, put that in a NumPy array. Then what you can do is create a model like we've just done using even something like the mobile nets, what was it v two that we talked about previously, if I could even get up and need to close this output. Oh my gosh, this is this is massive output here. Where is this begin to pre train model? Yeah, mobile net v two, you can use the base of that, and then add your own classifier on do a similar thing to what I've done with that dense neuron and that global average layer. And hopefully you should get a decent result from that. So this is just showing you what you can do. Obviously, you can pick a different base layer, depending on what kind of problem you're trying to solve. So anyways, that has been convolutional neural networks. I hope you enjoyed that module. Now we're on to recurrent neural networks, which is actually going to be pretty interesting. So I'll see you in that module. Hello, everyone, and welcome to the next module in this course, which is covering natural language processing with recurrent neural networks. Now what we're going to be doing in this module here is first of all, first off discussing what natural language processing is, which I guess I'll start with here. Essentially, for those of you that don't know, natural language processing or NLP for short, is the field or discipline in computing or machine learning that deals with trying to understand natural or human languages. Now, the reason we call them natural is because these are not computer languages or programming languages, per se. And actually, computers are quite bad at understanding textual information and human languages. And that's why we've come up with this entire discipline focused on how they can do that. So we're going to do that using something called recurrent neural networks. But some examples of natural language processing would be something like spell check, autocomplete voice assistance, translation between languages, there's all different kinds of things, chatbots, but essentially anything that deals with textual data. So you like paragraphs, sentences, even words, that is probably going to be classified under natural language processing in terms of doing some kind of machine learning stuff with it. Now, we are going to be talking about a different kind of neural network in this series called recurrent neural networks. Now, these are very good at classifying and understanding textual data. And that's why we'll be using them. But they are fairly complex. And there's a lot of stuff that goes into them. Now, in the interest of time, and just not knowing a lot of your math background, I'm not going to be getting into the exact details of how this works on a lower level, like I did when I explained kind of our, I guess, fundamental learning algorithms, which are a bit easier to grasp, and even just regular neural networks in general, we're going to be kind of skipping over that and really focusing on why this works the way it does, rather than how and when you should use this. And then maybe understanding a few of the different kinds of layers that have to do with recurrent neural networks. But again, we're not going to get into the math. If you'd like to learn about that, there will be some sources at the bottom of the guide. And you can also just look up recurrent neural networks, and you'll find lots of resources that explain all of the fancy math that goes on behind them. Now, the exact applications and kind of things will be working towards here is sentiment analysis. That's the first kind of task or thing we're going to do. We're actually going to use movie reviews and try to determine whether these movie reviews are positive or negative by performing sentiment analysis on them. Now, if you're unfamiliar with sentiment analysis, we'll talk about it more later, but essentially means trying to determine how positive or negative a sentence or piece of text is, which you can see why that would be useful for movie reviews. Next, we're going to do character slash text generation. So essentially, we're going to use a natural language processing model, I guess, if you want to call it that, to generate the next character in a sequence of text for us. And we're going to use that model a bunch of times to actually generate an entire play. Now, I know this seems a little bit ridiculous compared to some of the trivial examples we've done before. This will be quite a bit more code than anything we've really looked at yet. But this is very cool, because we're going to actually going to train a model to learn how to write a play. That's literally what it's going to do. It's going to read through a play, I believe it's Romeo and Juliet. And then we're going to give it a little prompt when we're actually using the model and say, okay, this is the first part of the play, write the rest of it. And then it will actually go and write the rest of the characters in the play. And we'll see that we can get something that's pretty good using the techniques that we'll talk about. So the first thing that I want to do is talk about data. So I'm going to hop onto my drawing tablet here. And we're going to compare the difference between textual data and numeric data, like we've seen before, and why we're going to have to employ some pretty complex and different steps to turn something like this, you know, a block of text into some meaningful information that our neural networks actually going to be actually going to be able to understand and process. So let's go ahead and get over to that. Okay, so now we're going to get into the problem of how we can turn some textual data into numeric data that we can feed to our neural network. Now, this is a pretty interesting problem. And we'll kind of go through as we start going through it, you should see why this is interesting and why there's a lot of difficulties with the different methods that we pick. But the first method that I want to talk about is something called bag of words, in terms of how we can kind of encode and pre process text into integers. Now, obviously, I'm not the first person to come up with this bag of words is a very famous, almost I want to say algorithm or method of converting textual data to numeric data, although it is pretty flawed and only really works for simple tasks. And we're going to understand why in a second. So we're going to call this bag of words. Essentially, what bag of words says is what we're going to do is we're going to look at our entire training data set, right, because we're going to be turning our training data set into a form the network can understand. And we're going to create a dictionary lookup of the vocabulary. Now, what I mean by that is we're going to say that every single unique word in our data set is the vocabulary, right? That's the amount of words that the model is expected to understand, because we're, you know, going to show all those words to the model. And we're going to say that every single one of these words. So every single one of these words in the vocabulary is going to be placed in a dictionary. And beside that, we're going to have some integer that represents it. So for example, maybe the vocabulary of our data set is the words, you know, I, a, maybe Tim, maybe day, me, right, we're gonna have a bunch of arbitrary words, I'll just put dot dot dot to show that this kind of goes to the length of the vocabulary. And every single one of these words will be placed in a dictionary, which we're going to just going to call kind of our lookup table or word index table. And we're going to have a number that represents every single one of them. So you can imagine that in very large data sets, we're going to have, you know, tens of thousands of hundreds of thousands, sometimes even maybe millions of different words, and they're all going to be encoded by different integers. Now, the reason we call this bag of words is because what we're actually going to do when we look at a sentence is we're only going to keep track of the words that are present and the frequency of those words. And in fact, what we'll do well is we'll create what we call a bag. And whenever we see a word appears, we'll simply add its number into the bag. So if I have a sentence like, you know, I am Tim day, day, I'm just going to do like a random sentence like that. Then what we're going to do is every time we see a word, we're going to take its number and throw it into the bag. So we're going to say, all right, I, that's zero, and that's one, Tim, that's two, day, that's three, that's three again. And notice that what's happening here is we're losing the ordering of these words, but we're just keeping track of the frequency. Now, there's lots of different ways to kind of format how we want to do bag of words. But this is the basic idea, I'm not going to go too far in because we're not actually really going to use this technique. But essentially, you lose the ordering in which words appear, but you just keep track of the frequency and what words appear. So this could be very useful when you're looking, you know, you're doing very simple tasks where the presence of a certain word will really influence the kind of type of sentence that it is, or the meaning that you're going to get from it. But when we're looking at more complex input, where, you know, different words have different meanings, depending on where they are in a sentence, this is a pretty flawed way to encode this data. Now, I won't go much further into this. This is not the exact way the bag of words works. But I just wanted to show you kind of an idea here, which is we just encode every single unique word by an integer. And then we don't even really care about where these words are. We just throw them into a bag and we say, All right, you know, this is our bag right here that I'm doing the arrow to, we'll just throw in three as many times as you know, the word day appears, we'll throw in one as many times as the word, I guess, am appears, and so on and so forth. And then what will happen is we'll feed this bag to our neural network in some form, depending on the network that we're using. And it will just look at and say, Okay, so I have all these different numbers, that means these words are present and try to do something with it. Now, I'm going to show you a few examples of where this kind of breaks down, but just understand that this is how this works. This is the first technique called bag of words, which again, we will not be using. So what happens when we have a sentence where the same word conveys a very different meaning, right? And I'm actually, I think I have an example on the slides here that I'll go into. Yes, select this. Okay, so for our bag of words technique, which we can kind of see here, maybe we'll go through it. Let's consider the two sentences where are they here? I thought the movie was going to be bad, but it was actually amazing. And I thought the movie was going to be amazing, but it was actually bad, right? So consider these two sentences. Now, I know you guys already know what I'm going to get at. But essentially, these sentences use the exact same words. In fact, they use the exact same number of words, the exact same words in total. And well, they have a very different meaning. With our bag of words technique, we're actually going to encode these two sentences using the exact same representation. Because remember, all we do is we care about the frequency and what words appear, but we don't care about where they appear. So we end up losing that meaning from the sentence, because the sentence I thought the movie was going to be bad, but it was actually amazing is encoded and represented by the same thing as this sentence is. So that, you know, obviously is an issue that's a flaw. And that's one of the reasons why bag of words is not very good to use, because we lose the context of the words within the sentence, we just pick up the frequency and the fact that these words exist. So that's the first technique that's called bag of words. I've actually written a little function here that does this for us. This is not really the exact way that we would write a bag of words function. But you kind of get the idea that when I have a text, this is a test to see if this test will work is test a I just did a bunch of random stuff. So we can see what I'm doing is printing out the bag, which I get from this function. And you guys can look at this if you kind of want to see how this works. And essentially, what it tells us is the word one appears two times. Yes, the word two appears three times the word three appears three times word four appears three times five ones, six, one, seven ones. So on, that's the information we get from our bag, right from that encoding. And then if we look up here, this is our vocabulary. So this stands for one is is two is three so on. And you can kind of get the idea from that. So that is how we would use bag of words, right? If we did an encoding kind of like this, that's what that does. And that's one way of encoding it. Now, I'm going to go back and we'll talk about another method here as well, actually, a few more methods before we get into anything further. All right, so I'm sure a lot of you were looking at the previous example I did. And you saw the fact that what I did was completely remove the idea of kind of sequence or ordering of words, right? And what I did was just throw everything in a bag. And I said, All right, we're just going to keep track of the fact that we have, you know, three A's, or we have four, those or seven, Tim's right. And we're going to just going to lose the fact that, you know, words come after one each other, we're going to lose their ordering in the sentence. And that's how we're going to encode it. And I'm sure a lot of you were saying, Well, why don't we just not lose the ordering of those words? We'll just encode every single word with an integer and just leave it in its space where it would have been in the original string. Okay, good idea. So what you're telling me to do is something like this, you know, Tim is here will be our sentence. Let's say we encode the word Tim with zero is as one, here's two. And then that means our translation goes zero, one, two. And that means, right, if we have a translation, say, like two, one, zero, even though these use the exact same number of words and exact same representation for all these words. Well, this is a different sentence. And our model should be able to tell that because these words come in a different order. And to you, good point, if you made that point, but I'm going to discuss where this falls apart as well. And why we're not going to use this method. So although this does solve the problem I talked about previously, where we're going to kind of lose out on the context of a word, there's still a lot of issues with this and they come, especially when you're dealing with very large vocabularies. Now, let's take an example where we actually have a vocabulary of say 100,000 words. And we know that that means we're going to have to have 100,000 unique mappings from words to integers. So let's say our mappings are something like this, one maps to the string happy, the word happy, right, to maps to sad. And let's say that the string 100,000 or the number 100,000 maps to the word, I don't know, let's say good. Now, we know as humans, by kind of just thinking about, let's consider the fact that we're going to try to classify sentences as a positive or negative. So sentiment analysis, that the words happy and good in that regard, you know, sentiment analysis are probably pretty similar words, right? And then if we were going to group these words, we'd probably put them in a similar group, we'd classify them as similar words, we get probably interchange them in a sentence, and it wouldn't change the meaning a whole ton. I mean, it might, but it might not as well. And that we could say these are kind of similar. But our model or our encoder, right, whatever we're doing to translate our text into integers here, has decided that 100,000 is going to represent good, and one is going to represent happy. And well, there's an issue with that, because that means when we pass in something like one, or 100,000 to our model, it's going to have a very difficult time determining the fact that one and 100,000, although they're 99,999, kind of units apart, are actually very similar words. And that's the issue we get into when we do something like this is that the numbers we decide to pick to represent each word are very important. And we don't really have a way of being able to look at words group them and saying, okay, well, we need to put all of the happy words and the range like zero to 100, all of the like adjectives in this range, we don't really have a way to do that. And this gets even harder for our model when we have these arbitrary mappings, right? And then we have something like two in between where two is very close to one, right? Yet these words are complete opposites. In fact, I'd say they're probably polar opposites, our model trying to learn that the difference between one and two is actually way larger than the difference between one and 100,000 is going to be very difficult. And say it's even able to do that, as soon as we throw in the mapping 900, right, the 99,900, we put that as bad. Well, now it gets even more difficult, because it's now like, okay, what the range is this big, then that means these words are actually very similar. But then you throw another word in here like this, and it messes up the entire system. So that's kind of what I want to show is that that's where this breaks apart on these large vocabularies. And that's why I'm going to introduce us now to another concept called word embeddings. Now, what word embeddings does is essentially try to find a way to represent words that are similar using very similar numbers. And in fact, what a word embedding is actually going to do, I'll talk about this more in detail as we go on, is classify or translate every single one of our words into a vector. And that vector is going to have some, you know, n amount of dimensions. Usually, we're going to use something like 64, maybe 128 dimensions for each vector. And every single component of that vector will kind of tell us what group it belongs to or how similar it is to other words. So let me give you an idea of what I mean. So we're going to create something called the word embeddings. Now, don't ask why it's called embeddings, I don't know the exact reason, but I believe it's to have has to do something with the fact that they're vectors. And let's just say we have a 3d plane like this. And we've already kind of looked at what vectors are before. So I'll skip over explaining them. And what we're going to do is take some word. So let's say we have the word good. And instead of picking some integer to represent it, we're going to pick some vector, which means we're going to draw some vector in this 3d space. Actually, let's make this a different color. Let's make this vector say red, like this. And this vector represents this word good. And in this case, we'll say we have x one, x two, x three is our dimensions, which means that every single word in our data set will be represented by three coordinates. So one vector with three different dimensions, where we have x one x two and x three. And our hope is that by using this word embeddings layer, and we'll talk about how it accomplishes this in a second, is that we can have vectors that represent very similar words being very similar, which means that you know, if we have the vector good here, we would hope the vector happy from our previous example, right, will be a vector that points in a similar direction to it. That is kind of a similar looking thing where the angle between these two vectors, right, and maybe I'll draw it here so we can see is small so that we know that these words are similar. And then we would hope that if we had a word that was much different, maybe say like the word bad, that that would point in a different direction, the vector that represents it. And that that would tell our model, because the angle between these two vectors is so big, that these are very different words, right? Now, in theory, does the embedding work layer work like this, you know, not always, but this is what it's trying to do is essentially pick some representation in a vector form for each word. And then these vectors, we hope if they're similar words, they're going to be pointing in a very similar direction. And that's kind of the best explanation of a word embeddings layer I can give you. Now, how do we do this, though, how do we actually, you know, go from word to vector, and have that be meaningful? Well, this is actually what we call a layer. So word embeddings is actually a layer, and it's something we're going to add to our model. And that means that this actually learns the embeddings for our words. And the way it does that is by trying to kind of pick out context in the sentence and determine based on where a word is in a sentence, kind of what it means, and then encodes it doing that. Now, I know that's kind of a rough explanation to give to you guys, I don't want to go too far into word embeddings in terms of the math, because I don't want to get, you know, waste our time or get too complicated if we don't need to. But just understand that our word embeddings are actually trained, and that the model actually learns these word embeddings as it goes. And we hope that by the time it's looked at enough training data, it's determined really good ways to represent all of our different words, so that they make sense to our model in the further layers. And we can use pre trained word embedding layers, if we'd like, just like we use that pre trained convolutional base in the previous section. And we might actually end up doing that. Actually, probably not in this tutorial, but it is something to consider that you can do that. So that's how word embeddings work. This is how we encode textual data. And this is why it's so important that we kind of consider the way that we pass information to our neural network, because it makes a huge difference. Okay, so now that we've talked about kind of the form that we need to get our data in before we can pass it further in the neural network, right, before it can get past that embedding layer, before it can get put in, put into any dense neurons, before we can even really do any math with it, we need to turn it into numbers, right, our textual data. So now that we know that it's time to talk about recurrent neural networks. Now recurrent neural networks are the type of networks we use when we process textual data, typically, you don't always have to use these, but they are just the best for natural language processing. And that's why they're kind of their own class, right? Now, the fundamental difference between a recurrence neural network and something like a dense neural network, or a convolutional neural network, is the fact that it contains an internal loop. Now, what this really means is that the recurrent neural network does not process our entire data at once. So it doesn't process the entire training example, or the entire input to the model at once, what it does is processes it at different time steps, and maintains what we call an internal memory, and kind of an internal state, so that when it looks at a new input, it will remember what it's seen previously and treat that input based on kind of the context or the understanding it's already developed. Now, I understand that this doesn't make any sense right now. But with a dense neural network, or the neural networks we looked at so far, we call those something called feed forward neural networks. What that means is we give all of our data to it at once, and we pass that data from left to right, or I guess for you guys from left to right. So we give all of the information, you know, we would pass those through the convolutional layer to start, maybe we have passed them through dense neurons, but they get given all of the info. And then that information gets translated through the network to the very end, again, from left to right. Whereas here, with recurrent neural networks, we actually have a loop, which means that we don't feed the entire textual data at once, we actually feed one word at a time, it processes that word, generate some output based on that word, and uses the internal memory state that it's keeping track of to do that as part of the calculation. So essentially, the reason we do this is because just like humans, when we, you know, look at text, we don't just take a photo of this text and process it all at once, we read it left to right, word to word. And based on the words that we've already read, we start to slowly develop an understanding of what we're reading, right? If I just read the word now, that doesn't mean much to me, if I just read the word in code, that doesn't mean much. Whereas if I read the entire sentence, now that we've learned a little bit about how we can encode text, I start to develop an understanding about what this next word means, based on the previous words before it, right? And that's kind of the point here is that this is what a recurrent neural network is going to do for us. It's going to read one word at a time, and slowly start building up its understanding of what the entire textual data means. And this works in kind of a more complicated sense than that will draw it out a little bit. But this is kind of what would happen if we on, I guess, unraveled a recurrent layer, because recurrent neural network, yes, it has a loop in it. But really the recurrent aspect of a neural network is the layer that implements this recurrent functionality with a loop. Essentially, what we can see here is that if we're saying x is our input and h is our output, x t is going to be our input at time t, whereas h t is going to be our output at time t. If we had a text of say length four, so four words, like we've encoded them into integers now at this point, the first input at time zero will be the first word into our network, right, or the first word that this layer is going to see. And the output at that time is going to be our current understanding of the entire text after looking at just that one word. Next, what we're going to do is process input one, which will be the next word in the sentence. But we're going to use the output from the previous kind of computation or the previous iteration to do this. So we're going to process this word in combination with what we've already seen, and then have a new output, which hopefully should now give us an understanding of what those two words mean. Next, we'll go to the third word. And so forth, and slowly start building our understanding of what the entire textual data means by building it up one by one. The reason we don't pass the entire sequence at once is because it's very, very difficult to just kind of look at this huge blob of integers and figure out what the entire thing means. If we can do it one by one and understand the meaning of specific words based on the words that have came before it and start learning those patterns, that's going to be a lot easier for a neural network to deal with than just passing it all at once, looking at it and trying to get some output. And that's why we have these recurrent layers, there's a few different types of them. And I'm going to go through them, and then we'll talk a little bit more in depth of how they work. So the first one is called long short term memory. And actually, in fact, before we get into this, let's, let's talk about just a first like a simple layer so that we kind of have a reference point before going here. Okay, so this is kind of the example I want to use here to illustrate however current neural network works and a more teaching style rather than what I was doing before. So essentially, the way that this works is that this whole thing that I'm drawing here, right, all of this circle stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing you kind of how this works in a series of steps. So rather than passing all the information at once, we're going to pass it as a sequence, which means that we're going to have all these different words and we're going to pass them one at a time to the kind of to the layer, right, to this recurrent layer. So we're going to start from this left side over here. So this right, you know, start over here at time step zero, that's what zero means. So time step is just, you know, the order. In this case, this is the first word. So let's say we have the sentence Hi, I am Tim, right, we've broken these down into vectors, they've been turned into their numbers, I'm just writing them here so we can kind of see what I mean in like a natural language. And they are the input to this recurrent layer. So all of our different words, right, that's how many kind of little cells we're going to draw here is how many words we have in this sequence that we're talking about. So in this case, we have four, right, four words. So that's why I've drawn four cells to illustrate that. Now what we do is that time step zero, the internal state of this layer is nothing, there's no previous output, we haven't seen anything yet, which means that this first kind of cell, which is what I'm looking at right here, what I'm drawing in this first cell is only going to look and consider this first word and kind of make some prediction about it and do something with it. We're going to pass high to this cell, some math's going to go on in here. And then what it's going to do is it's going to output some value, which, you know, tells us something about the word high, right, some numeric value, we're not going to talk about what that is, but it's going to do is going to be some output. Now, what happens is after this cell has finished processing this, so right, so this one's done, this has completed h zero, the outputs there, we'll do a check mark to say that that's done, it's finished processing, this output gets fed into actually the same thing again, we're kind of just keeping track of it. And now what we do is we process the next input, which is I, and we use the output from the previous cell to process this and understand what it means. So now, technically, we should have some output from the previous cell. So from whatever high was, right, we do some analysis on the word I, we kind of combine these things together. And that's the output of this cell is our understanding of not only the current input, but the previous input with the current input. So we're slowly kind of building up our understanding of what this word I means, based on the words we saw before. And that's the point I'm trying to get at is that this network uses what it's seen previously to understand the next thing that it sees, it's building a context is trying to understand not only the word, but what the word means, you know, in relation to what's come before it. So that's what's happening here. So then this output here, right, we get some output, we finish this, we get some output h one, h one is passed into here. And now we have the understanding of what high and I means, and we add am like that, we do some kind of computations, we build an understanding of what this sentence is. And then we get the output h two, that passes to h three. And now finally, we have this final output h three, which is going to understand hopefully, what this entire thing means. Now, this is good, this works fairly well. And this is called a simple RNN layer, which means that all we do is we take the output from the previous cell or the previous iteration, because really, all of these cells is just an iteration almost in a for loop, right, based on all the different words in our sequence. And we slowly start building to that understanding as we go through the entire sequence. Now, the only issue with this is that as we have a very long sequence, so sequences of length, say 100 or 150, the beginning of those sequences starts to kind of get lost. As we go through this, because remember, all we're doing, right, is the output from h two is really a combination of the output from h zero and h one, and then there's a new word that we've looked at. And h three is now a combination of everything before it, and this new word. So it becomes increasingly difficult for our model to actually build a really good understanding of the text in general, when the sequence gets long, because it's hard for it to remember what it's seen at the very beginning, because that is now so insignificant, there's been so many outputs tacked on to that, that it's hard for it to go back and see that if that makes any sense. Okay, so what I'm going to do now is try to explain the next layer we're going to look at, which is called LSTM. So the previous layer we just looked at the recurrent layer was called a simple RNN layer. So simple recurrent neural network layer, whatever you want to call it, right, simple recurrent layer. Now we're going to talk about the layer, which is LSTM, which stands for long, short term memory. Now, long and short are hyphenated together. But essentially, what we're doing, and it just gets a little bit more complex, but I won't go into the math, is we add another component that keeps track of the internal state. So right now, the only thing that we were tracking as kind of our internal state as the memory for this model was the previous output. So whatever the previous output was. So for example, at time zero here, there was no previous output. So there was nothing being kept in this model. But at time one, the output from this cell right here was what we were storing. And then at cell two, the only thing we were storing was the output at time one, right? And we've lost now the output from time zero. What we're adding in long, short term memory is an ability to access the output from any previous state at any point in the future when we want it. Now, what this means is that rather than just keeping track of the previous output, we'll add all of the outputs that we've seen so far into what I'm going to call my little kind of conveyor belt, it's going to run at the top up here. I know it's kind of hard to see, but it's just what I'm highlighting. It's almost just like a lookup table that can tell us the output at any previous cell that we want. So we can kind of add things to this conveyor belt, we can pull things off, we can look at them. And this just adds a little bit of complexity to the model. It allows us to not just remember the last state, but look anywhere at any point in time, which can be useful. Now, I don't want to go into much more depth about exactly how this works. But essentially, you know, just think about the idea that as the sequence gets very long, it's pretty easy to forget the things we saw at the beginning. So if we can keep track of some of the things we've seen at the beginning, and some of the things in between on this little conveyor belt, and we can access them whenever we want, then that's going to make this probably a much more useful layer, right? We could look at the first sentence and the last sentence of a big piece of text at any point that we want and say, okay, you know, this tells us X about the meaning of this text, right? So that's what this LSTM does. Again, I don't want to go too far. We've already spent a lot of time kind of covering, you know, recurrent layers and how all this works. Anyways, if you do want to look it up, some great mathematical definitions, again, I will source everything at the bottom of this document so you can go there. But again, that's LSTM, long short term memory, that's what we're going to use for some of our examples, although simple RNN does work fairly well for shorter length sequences. And again, remember, we're treating our text as a sequence now, where we're going to feed each word into the recurrent layer, and it's going to slowly start to develop an understanding as it reads through each word, right and processes that. Okay, so now we are on to our first example, where we're going to be performing sentiment analysis on movie reviews to determine whether they are positive reviews or negative reviews. Now, we already know what sentiment means. That's essentially what I just described. So picking up, you know, whether a block of text is considered positive or negative. And for this example, we're going to be using the movie review data sets. Now, as per usual, this is based off of this TensorFlow tutorial slash guide. I found this one kind of confusing to follow in the TensorFlow website, but obviously you can follow along with that if you don't prefer that version over mine. But anyways, we're going to be talking about the movie review data set. So this data set is straight from Keras, and it contains 25,000 reviews, which are already pre processed and labeled. Now, what that means for us is that every single word is actually already encoded by an integer. And in fact, they've done kind of a clever encoding system where what they've done is said, if a character is encoded by say integer zero, that represents how common that word is in the entire data set. So if an integer was encoded, but are not interested, a word was encoded by integer three, that would mean that it is the third most common word in the data set. And in this specific data set, we have a vocabulary size of 88,584 unique words, which means that something that was classified as this. So 88,584 would be the least common word in the data set. So something to keep in mind, we're going to load in the data set and do our imports just by hitting run here. And as I've mentioned previously, you know, I'm not going to be typing this stuff out. It's just it's kind of a waste of time. I don't have all the syntax memorized. I would never expect you guys to memorize this either. But what I will do is obviously walk through the code step by step, and make sure you understand why it is that we have what we have here. Okay, so what we've done is to find the vocabulary size, the max length of a review, and the batch size. Now what we've done is just loaded in our data set by defining the vocabulary size. So this is just the words it will include. So in this case, all of them, then we have trained data, trained labels, test data, test labels. And we can look at a review and see what it looks like by doing something like this. So this is an example of our first review, we can see kind of the different encodings for all of these words. And this is what it looks like, they're already in integer form. Now, just something to note here is that the length of our reviews are not unique. So if I do the length of trained data, I guess I wouldn't say unique, but I mean, they're just all different. So the length of trained data is zero is different than the length of trained data one, right? So that's something to consider as we go through this and something we're actually going to have to handle. Okay, so more pre processing. So this is what I was talking about. If you have a look at our loaded interviews, we'll notice there are different lengths, this is an issue, we cannot pass different length data into our neural network, which is true. Therefore, we must make each review the same length. Okay, so what we're going to do for now is we're actually going to pad our sequences. Now what that means is we're going to follow this kind of step that I've talked about here. So if the review is greater than 250 words, we will trim off extra words, if the review is less than 250 words, we'll add the necessary amount of this should actually be zeros in here, let's fix this of zeros to make it equal to 250. So what that means is we're essentially going to add some kind of padding to our review. So in this case, I believe we're actually going to pad to the left side, which means that say we have a review of length, you know, 200, we're going to add 50, just kind of blank words, which will represent with the index zero to the left side of the review to make it the necessary length. So that's, that's good, we'll do that. So if we look at train data and test data, what this does is we're just going to use something from Keras, which we've imported above. So we're saying from Keras dot pre processing import sequence, again, we're treating our text data as a sequence, as we've talked about, we're going to say sequence dot pad sequences, train data, and then we define the length that we want to pad it to. So that's what this will do. It will perform these steps that we've already talked about. And again, we're just going to assign test data and train data to, you know, whatever this does for us, we can pass the entire thing, it'll pad all of them for us at once. Okay, so let's run that. And then let's just have a look at say train data one now, because remember, this was like 189, right? So if we look at train data, so train underscore data one, like that, we can see that as an array with a bunch of zeros before, because that is the padding that we've employed to make it the correct length. Okay, so that's padding, that's something that we're probably going to have to do most of the time, when we feed something to our neural networks. All right, so the next step is actually to create the model. Now this model is pretty straightforward. We have an embedding layer and LSTM in a dense layer here. So the reason we've done dense with the activation function of sigmoid at the end is because we're trying to pretty much predict the sentiment of this, right? Which means that if we have the sentiment between zero and one, then if a number is greater than 0.5, we could classify that as a positive review. And if it's less than 0.5 or equal, you know, whatever you want to set the bounds at, then we could say that's a negative review. So sigmoid, as we probably might recall, squishes our values between zero and one. So whatever the value is at the end of the network will be between zero and one, which means that, you know, we can make the accurate prediction. Now here, the reason we have the embedding layer, like, well, we've already pre processed our review is even though we've pre processed this with these integers, and they are a bit more meaningful than just our random lookup table that we've talked about before, we still want to pass that to an embedding layer, which is going to find a way more meaningful representation for those numbers than just their integer values already. So it's going to create those vectors for us. And this 32 is denoting the fact that we're going to make the output of every single one of our embeddings or vectors that are created 32 dimensions, which means that when we pass them to the LSTM layer, we need to tell the LSTM layer, it's going to have 32 dimensions for every single word, which is what we're doing. And this will implement that long short term memory process we talked about before, and output the final output to TF dot cares dot layers dot dense, which will tell us, you know, that's what this is, right? It'll make the prediction. So that's what this model is. We can see, give us a second to run here, the model summary, which is already printed out, we can look at the fact that the embedding layer actually has the most amount of parameters, because essentially, it's trying to figure out, you know, all these different numbers, how can we convert that into a tensor of 32 dimensions, which is not that easy to do. And this is going to be the major aspect that's being trained. And then we have our LSTM layer, we can see the parameters there. And our final dense layer, which is eight getting 33 parameters, that's because the output from every single one of these dimensions 32 plus a bias node, right, that we need. So that's what we'll get there. You can see model dot summary. We get the sequential model. Okay, so training. Alright, so now it's time to compile and train the model, you can see I've already trained mine. What I'm going to say here is if you want to speed up your training, because this will actually take a second, and we'll talk about why we pick these things in a minute is go to runtime, change runtime type, and add a hardware accelerator of GPU. What this will allow you to do is utilize a GPU while you're training, which should speed up your training by about 10 to 20 times. So I probably should have mentioned that beforehand. But you can do that. And please do for these examples. So model dot compile. Alright, so we're compiling our model, we're picking the loss function as binary cross entropy. The reason we're picking this is because this is going to essentially tell us how far away we are from the correct probability, right, because we have two different things we could be predicting. So you know, either zero or one, so positive or negative. So this will give us a correct loss for that kind of problem that we've talked about before. The optimizer, we're going to use rms prop. Again, I'm not going to discuss all the different optimizers, you can look them up if you care that much about what they do. And we're going to use metrics as ACC. One thing I will say is the optimizer is not crazy important. For this one, you could use Adam if you wanted to, and it would still work fine. My usual go to is just use the atom optimizer unless you think there's a better one to use. But anyways, that's something to mention. Okay, so finally, we will fit the model, we've looked at the syntax a lot before. So model that fit, we'll give the training data, the training labels, the epochs, and we'll do a validation split of 20%. So that's what 0.2 stands for, which means that what we're going to be doing is using 20% of the training data to actually evaluate and validate the model as we go through. And we can see that after training, which I've already done, and you guys are welcome to obviously do on your own computer, we kind of stall at an evaluation accuracy of about 88%. Whereas the model actually gets overfit to about 97 98%. So what this is telling us essentially is that we don't have enough training data, and that after we've even done just one epoch, we're pretty much stuck on the same validation accuracy, and that there's something that needs to change in the model to make it better. But for now, that's fine, we'll leave it the way that it is. Okay, so now we can look at the results. I've already did the results here, just to again, speed up some time, but we'll do the evaluation on our test data and test labels to get a more accurate kind of result here. And that tells us we have an accuracy of about 85.5%, which you know, isn't great, but it's decent considering that we didn't really write that much code to get to the point that we're at right now. Okay, so that's what we're getting. The model has been trained. Again, it's not too complicated. And now we're on to making predictions. So the idea is that now we've trained our model, and we want to actually use it to make a prediction on some kind of movie review. So since our data was pre processed, when we gave it to the model, that means we actually need to process anything we want to make a prediction on in the exact same way, we need to use the same lookup table, we need to encode it, you know, precisely the same. Otherwise, when we give it to the model, it's going to think that the words are different, and it's not going to make an accurate prediction. So what I've done here is I've made a function that will encode any text into what do you call the proper pre processed kind of integers, right, just like our training data was pre processed. That's what this function is going to do for us is pre processed some line of text. So what I've done is actually gotten the lookup table. So essentially, the mappings from IBM, I be IMDB, I could read that properly. From that data set that we loaded earlier. So let me go see if I can find where I defined IMDB, you can see up here. So keras dot data sets import IMDB, just like we loaded it in, we can also actually get all of the word indexes or that map, we can actually print this out if we want to look at what it is after. But anyways, we have that mapping, which means that all we need to do is keras dot preprocessing dot text dot text, two word sequence, what this means is give in some text convert all of that text into what we call tokens, which are just the individual words themselves. And then what we're going to do is just use a kind of for loop inside of here that says word index at word, if word in word index, L zero for word in tokens. Now what this means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary of 88,000 words, then what we'll do is replace its location in the list with that specific word, or with that specific integer that represents it, otherwise we'll put zero just to stand for, you know, we don't know what this character is. And then what we'll do is return sequence dot pad sequences, and we'll pad this token sequence, and just return actually the first index here. The reason we're doing that is because this pad sequences works on a list of sequences, so multiple sequences. So we need to put this inside a list, which means that this is going to return to us a list of lists. So we just obviously want the first entry, because we only want, you know, that one sequence that we padded. So that's how this works. Sorry, that's a bit of a mouthful to explain, but you guys can run through and print the stuff out if you want to see how all of it works specifically. But yeah, so we can run this cell and have a look at what this actually does for us on some sample text. So that maybe was just amazing. So amazing, we can see we get the output that we were kind of expecting. So integer encoded words down here, and then a bunch of zeros just for all the padding. Now, while we're at it, I decided why not we why don't we make a decode function so that if we have any movie review like this, that's in the integer form, we can decode that into the text value. So the way we're going to do that is start by reversing the word index that we just created. Now the reason for that is the word index we looked at, which is this right, goes from word to integer. But we actually now want to go from integer to word so that we can actually translate a sentence, right? So what I've done is made this decode integers function, we've set the padding key as zero, which means that if we see zero, that's really just means you know, nothing's there. We're going to create a text string, which we're going to add to. And I'm just gonna say for num in integers, integers is our input, which will be a list that looks something like this or an array, whatever you want to call it, we're gonna say if number does not equal pad. So essentially, if the number is not zero, right, it's not padding, then what we'll do is add the lookup of reverse word index num. So whatever that number is, into this new string plus a space, and then just return text colon negative one, which means return everything except the last space that we would have added. And then if I print the decode integers, we can see that this encoded thing that we have before, which looks like this gets encoded by the string that movie was just amazing. So maybe sorry, not encoded, decoded, because this was the encoded form. So that's how that works. Okay, so now it's time to actually make a prediction. So I've written a function here that will make a prediction on some piece of text as the movie review for us. And I'll just walk us through quickly how this works. And then I'll show us the actual output from our model, you know, making predictions like this. So what we say is we'll take some parameter text, which will be our movie review. And we're going to encode that text using the encode text function we've created above. So just this one right here that essentially takes our sequence of, you know, words, we get the pre processing, so turn that into a sequence, remove all the spaces, whatnot, you know, get the words, then we turn those into the integers, we have that we return that. So here we have our proper pre processed text. Then what we do is we create a blank NumPy array that is just a bunch of zeros that's in the form one to 50 or in that shape. Now the reason I'm putting in that in that shape is because the shape that our model expects is something 250, which means some number of entries, and then 250 integers representing each word, right? Because that's the length of movie review is what we've told the model is the length 250. So that's the length of the review. Then what we do is we put pred zero. So that's what's up here, equals the encoded text. So we just essentially insert our one entry into this, this array we've created. Then what we do is say modeled up predict on that array, and just return and print the result zero. Now, that's pretty much all there is to it. I mean, that's how it works. The reason we're doing result zero is because again, model is optimized to predict on multiple things, which means like I would have to do, you know, list of encoded text, which is kind of what I've done by just doing this prediction lines here, which means it's going to return to me an array of arrays. So if I want the first prediction, I need to index zero, because that will give me the prediction for our first and only entry. Alright, so I hope that makes sense. Now we have a positive review I've written and a negative review, and we're just going to compare the analysis on both of them. So that movie was so awesome. I really loved it and would watch it again, because it was amazingly great. And then that movie sucked, I hated it and wouldn't watch it again, was one of the worst things I've ever watched. So let's look at this now. And we can see the first one gets predicted at 72% positive, whereas the other one is 23% positive. So essentially what that means is that, you know, if the lower the number, the more negative we're predicting it is, the higher the number, the more positive we're predicting it is. If we wanted to not just print out this value, and instead what we wanted to do was print out, you know, positive or negative, we could just make a little if statement that says if this number is greater than 0.5, say positive, otherwise say not say negative, right? And I just want to show you that changing these reviews ever so slightly actually makes a big difference. So if I remove the word awesome, so that movie was so and then I run this, you can see that Oh, wow, this actually increases and goes up to 84%. So the presence of certain words in certain locations actually makes a big difference. And especially when we have a shorter length review, right, if we have a longer length review, it won't make that big of a difference. But even the removal of a few words here. And let's see, so the removing the word awesome changed it by almost like 10%. Right. Now if I move, so let's see if that makes a bigger difference. It makes a very little difference because it's learned, at least the model, right, that the word so doesn't really make a huge impact into the type of review. Whereas if I remove the word I, let's see if that makes a big impact, probably not right now, it goes back up to 84. So that's cool. And that's something to play with is removing certain words and seeing how much impact those actually carry. And even if I just add the word great, like would great to watch it again, just in the middle of the sentence, doesn't have to make any sense. Let's look at this here. Oh, boom, we increase like a little bit, right. And let's say if I add this movie, you really suck. Let's see if that makes a difference. No, that just reduces it like a tiny bit. So something cool, something to play with. Anyways, now let's move on to the next example. So now we're on to our last and final example, which is going to be creating a recurrent neural network play generator. Now, this is going to be the first kind of neural network we've done, that's actually going to be creating something for us. But essentially, what we're going to do is make a model that's capable of predicting the next character in a sequence. So we're going to give it some sequence as an input. And what it's going to do is just simply predict the most likely next character. Now, there's quite a bit that's going to go into this. But the way we're going to use this to predict a play is we're going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then we're going to have it. So that we'll ask the model will give it some starting prompt, some string to start with. And that'll be the first thing we pass to it, it will predict to us what the most likely next character for that sequence is. And we'll take the output from the model and feed it as the input again to the model and keep predicting sequence of characters. So keep predicting the next character from the previous output as many times as we want to generate an entire play. So we're going to have this neural network that's capable of predicting one letter at a time actually end up generating an entire play for us by running it multiple times on the previous output from the last iteration. Now, that's kind of the problem. That's what we're trying to solve. So let's go ahead and get into it and talk about what's involved in doing this. So the first thing we're going to do obviously is our imports. So from Keras dot preprocessing import sequence, import Keras, we need TensorFlow NumPy and OS. So we'll load that in. And now what we're going to do is download the file. So the data set for Romeo and Juliet, which we can get by using this line here. So Keras has this utils thing, which will allow us to get a file, save it as whatever we want. In this case, we're going to save it as Shakespeare dot txt. And we're going to get that from this link. Now, I believe this is just some like shared drive that we have access to from Keras. So we'll load that in here. And then this will simply give us the path on this machine. Because remember, this is Google Collaboratory to this text file. Now, if you want, you can actually load in your own text data. So we don't necessarily need to use the Shakespeare play, we could use anything we want. In fact, an example that I'll show later is using the B movie script. But the way you do that is run this block of code here. And you'll see that it pops up this thing for choose files, just choose a file from your local computer. And then what that will do is just save this on Google Collaboratory. And then that will allow you to actually use that. So make sure that's a text file that you're loading in there. But regardless, that should work. And then from there, you'll be good to go. So if you, you know, you don't need to do that, you can just run this block of code here, if you want to load in the Shakespeare txt, but otherwise, you can load in your own file. Now, after we do that, what we want to do is actually open this file. So remember, that was just saving the path to it. So we'll open that file in RB mode, which is read bytes mode, I believe. And then we're going to say dot read, so we're going to read that in as an entire string, we're going to decode that into utf a format. And then we're just printing the length of the text or the amount of characters in the text. So if we do that, we can see we have the length of the text is 1.1 million characters, approximately. And then we can have a look at the first 250 characters by doing this. So we can see that this is kind of what the plate looks like, we have whoever's speaking, colon, then some line, whoever's speaking, colon, some line, and there's all these break lines. So backslash ends, which are telling us, you know, go to the next line, right? So it's going to be important because we're going to hope that our neural network will be able to predict things like break lines and spaces, and even this kind of format as we teach it more and get further in. But now it's time to talk about encoding. So obviously, all of this text is in text form, it's not pre processed for us, which means we need to pre process it and encode it as integers before we can move forward. Now, fortunately, for us, this problem is actually a little bit easier than the problem we discussed earlier with encoding words, because what we're going to do is simply encode each character in the text with an integer. Now, you can imagine why this makes this easier, because there really is a finite set of characters, whereas there's kind of indefinite or, you know, I guess, infinite amount of words that could be created. So we're not really going to run into the problem where, you know, two words are encoded with such different or two characters are encoded with such different integers, that it makes it difficult for the model to understand. Because I mean, and we can look at what the value of vocab is here, we're only going to have so many characters in the text. And for characters, it just doesn't matter as much, because you know, an R isn't like super meaningful compared to an A. So we can kind of encode in a simple format, which is what we're going to do. So essentially, we need to figure out how many unique characters are in our vocabulary. So to do that, we're going to say vocab equals sorted, set text, this will sort all of the unique characters in the text. And then what we're going to do is create a mapping from unique characters to indices indices. So essentially, we're going to say UI, for IU in a new, a numerator vocabulary, what this will do is give us essentially zero, whatever the string is, one, whatever the string is, two, whatever the string is for every single letter or character in our vocabulary, which will allow us to create this mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array, so that we can just use the index at which a letter appears as the reverse mapping. So going from index to letter, rather than lettered index, which is what this one's doing here. Next, I've just written a function that takes some text and converts that to an int or the int representation for it, just to make a little bit easier for us as we get later on in the tutorial. So we're just going to say NP dot array of in this case, and we're just going to convert every single character in our text into its integer representation by just referencing that character and putting that in a list here, and then obviously converting that to NumPy array. So then if we wanted to have a look at how this works, we can say text as int equals text to int text. So remember text is that entire loaded file that we had above here. So we're just going to convert that to its integer representation entirely using this function. And now we can look at how this works down here. So we can see that the text for citizen, which is the first 13 letters is encoded by 1847 5657 581. And obviously each character has its own encoding, and you can go through and kind of figure out what they are based on the ones that are repeated, right? So that is how that works. Now I figured while we were at it, we might as well write a function that goes the other way. So into text. Reason I'm trying to convert this to a NumPy array first is just because we're going to be passing in different objects potentially in here. So if it's not already a NumPy array, it needs to be a NumPy array, which is kind of what this is doing. Otherwise, we're just going to pass on that we don't need to convert it to NumPy array, if it already is one, we can just join all of the characters from this list into here. So that's essentially what this is doing for us. It's just joining into text. And then we can see if we go into text, text is in colon 13, that translates that back to us for citizen, I mean, you can look more into this function if you want, but it's not that complicated. Okay, so now that we have all this text encoded as integers, what we need to do is create some training examples. It's not really feasible to just pass the entire you know, 1.1 million characters to our model at once for training, we need to split that up into something that's meaningful. So what we're actually going to be doing is creating training examples where we have the first, where the training input, right, so the input value is going to be some sequence of some length, we'll pick the sequence length, in this case, we're actually going to pick 100. And then the output or the expected output, so I guess like the label for that training example is going to be the exact same sequence shifted right by one character. So essentially, I put a good example here, our input will be something like hell, right? Now our output will be E L L O. So what it's going to do is predict this last character, essentially. And these are what our training examples are going to look like. So the entire beginning sequence, and then the output sequence should be that beginning sequence minus the first letter, but tack on what the last letter should be. So that this way, we can look at some input sequence and then predict that output sequence that you know, plus a character, right? Okay, so that's how that works. So now we're going to do is define a sequence length of 100. We're going to say the amount of examples per epoch is going to be the length of the text divided by the sequence length plus one. The reason we're doing this is because for every training example, we need to create a sequence input that's 100 characters long, and we need to create a sequence output that's 100 characters long, which means that we need to have 101 characters that we use for every training example, right? Hopefully that would make sense. So what this next line here is going to do is convert our entire string data set into characters. And it's actually going to allow us to have a stream of characters, which means that it's going to essentially contain, you know, 1.1 million characters inside of this TF dot data set object from tensor slices. That's what that's doing. Next, so let's run this and make sure this works. All right, what we're going to do is say sequences is equal to char data set dot batch sequence length is the length of each batch. So in this case, 101 and then drop remainder means let's say that we have, you know, 105 characters in our text, well, since we need sequences of length 101, we'll just drop the last four characters of our text, because we can't even put those into a batch. So that's what this is doing for us is going to take our entire character data set here that we've created and batch it into length of 101, and then just drop the remainder. So that's what we're going to do here. So sequences does now split input target. What this is going to do essentially is just create those training examples that we needed. So taking this, these sequences of 101 length and converting them into the input and target text, and I'll show you how they work in a second, we can do this convert the sequences to that by just mapping them to this function. So that's what this function does. So if we say sequences dot map, and we put this function here, that means every single sequence will have this operation applied to it. And that will be stored inside this data set object. Or I guess you'd say object, but we'll also just say that's it's going to be, you know, the variable, right? So if we want to look at an example of how this works, we can kind of see. So it just says example, the input will be first citizen, before we proceed any further here, me speak, all speak, speak, first citizen, you and the output notice the first character is gone, starts at I. And the last character is actually just a space here. Whereas here, it didn't have a space, or you can see there's no space. Here, there is a space. That's kind of what I'm trying to highlight for you. The next example, we get our all resolved rather to die rather than famine, whatever it goes to here, right? And then you can see here, we omit that a and the next letter is actually a K, right? That's added in there. So that's how that works. Okay, so next, we need to make training batches. So we're going to say the batch size equals 64. The vocabulary size is the length of the vocabulary, which if you remember all the way back up to the top of the code, was the set or the sorted set of the text, which essentially told us how many unique characters are in there. The embedding dimension is 256. The RNN units is 1024. And the buffered size is 10,000. What we're going to do now is create a data set that shuffled, we're going to switch around all these sequences, so they don't get shown in the proper order, which we actually don't want. And then we're going to batch them by the batch size. So if we haven't kind of gone over what batching and all this does before, I mean, you can read these comments, this is straight from the TensorFlow documentation, what we want to do is feed our model 64 batches of data at a time. So what we're going to do is shuffle all of the data, batch it into that size, and then again, drop the remainder, if there's not enough batches, which is what we'll do. We're going to define the embedding dimension, which is essentially how big we want every single vector to represent our words are in the embedding layer. And then the RNN units, I won't really discuss what that is right now. But that's essentially how many it's hard to really just, I'm just going to omit describing at for right now, because I don't want to butcher an explanation. It's not that important. Anyways, okay, so now we're going to go down to building the model. So we've kind of set these parameters up here. Remember what those are, we've batched and we've shuffled the data set. And again, that's how this works. You can print it out if you want to see what a batch actually looks like. But essentially, it's just 64 entries of those sequences, right? So 64 different training examples is what a batch that is. All right. So now we go down here, we're going to say build model, we're actually making a function that's going to return to us a built model. The reason for this is because right now, we're going to pass the model batches of size 64 for training, right? But what we're going to do later is save this model. And then we're going to patch pass it batches of one pieces of, you know, training whatever data, so that it can actually make a prediction on just one piece of data. Because for right now, what it's going to do is takes a batch size of 64, it's going to take 64 training examples, and return to us 64 outputs. That's what this model is going to be built to do the way we build it now to start. But later on, we're going to rebuild the model using the same parameters that we've saved and trained for the model, but change it to just be a batch size of one, so that that way we can get one prediction for one input sequence, right? So that's why I'm creating this build model function. Now in here, it's going to have the vocabulary sizes, first argument, the embedding dimension, which remember was 256 as a second argument, but also these are the parameters up here, right? And then we're going to find the batch size as you know, batch size, none, what this none means is we don't know how long the sequences are going to be in each batch. All we know is that we're going to have 64 entries in each batch. And then of those 64 entries, so training examples, right, we don't know how long each one will be. Although in our case, we're going to use ones that are length 100. But when we actually use the model to make predictions, we don't know how long the sequence is going to be that we input so we leave this none. Next, we'll make an LSTM layer, which is a long short term memory RNN units, which is 1024, which again, I don't really want to explain, but you can look up if you want return sequences means return the intermediate stage at every step. The reason we're doing this is because we want to look at what the model seeing at the intermediate steps and not just the final stage. So if you leave this as false, and you don't set this to true, what happens is this LSTM just returns one output that tells us what the model kind of found at the very last time step. But we actually want the output at every single time step for this specific model. And that's why we're setting this true, stateful, not going to talk about that one right now, that's something you can look up if you want. And then recurrent initializer is just what these values are going to start at in the LSTM. We're just picking this because this is what TensorFlow has kind of said is a good default to pick. I won't go into more depth about that again, things that you can look up more if you want. Finally, we have a dense layer, which is going to contain the amount of vocabulary size nodes. The reason we're doing this is because we want the final layer to have the amount of nodes in it equal to the amount of characters in the vocabulary. This way, every single one of those nodes can represent a probability distribution that that character comes next. So all of those nodes value some sum together should give us the value of one. And that's going to allow us to look at that last layer as a predictive layer, where it's telling us the probability that these characters come next, and we've discussed how that's worked previously with other neural networks. So let's run this now. Name embedding dim is not defined, which I mean, believes I have not ran this yet. So now we run that, and we should be good. So if we look at the model summary, we can see we have our initial embedding layer, we have our LSTM, and then we have our dense layer at the end. Now notice 64 is the batch size, right? That's the initial shape. None is the length of the sequence, which we don't know. And then this is going to be just the output dimension or sorry, this is the amount of values in the vector, right? So we're going to start with 256. We'll just do 1,024 units in the LSTM and then 65 stands for the amount of nodes, because that is the length of the vocabulary. Alright, so combined, that's how many trainable parameters we get. You can see each of them for each layer. And now it's time to move on to the next section. Okay, so now we're moving on to the next step of the tutorial, which is creating a loss function to compile our model with. Now, I'll talk about why we need to do this in a second, but I first want to explore the output shape of our model. So remember, the input to our model is something that is of length 64, because we're going to have batches of 64 training examples, right? So every time we feed our model, we're going to give it 64 training examples. Now what those training examples are, are sequences of length 100. That's what I want you to remember. We're passing 64 entries that are all of length 100 into the model as its training data, right? But sometimes, and when we make predictions with the model later on, we'll be passing it just one entry that is of some variable length, right? And that's why we've created this build model function, so that we can build this model using the parameters that we've saved later on, once we train the model, and it can expect a different input shape, right? Because when we're training it, it's going to be given a different shape, and we're actually testing with it. Now what I want to do is explore the output of this model, though, at the current point in time. So we've created a model that accepts a batch of 64 training examples that are length 100. So let's just look at what the output is from the final layer. Give this a second to run. We get 64, 165. And that represents the batch size, the sequence length, and the vocabulary size. Now the reason for this is we have to remember that when we create a dense layer as our last layer that has 65 nodes, every prediction is going to contain 65 numbers. And that's going to be the probability of every one of those characters occurring, right? That's what that does at the last one for us. So obviously, our last dimension is going to be 65 for the vocabulary size. This is a sequence length, and that's a batch, I just want to make sure this is really clear before we keep going. Otherwise, it's going to get very confusing very quickly. So what I want to do now is actually look at the length of the example batch predictions, and just print them out and look at what they actually are. So example batch predictions is what happens when I use my model on some random input example, actually, well, the first one from my data set with when it's not trained. So I can actually use my model before it's trained with random weights and random biases and parameters, by simply using model, and then I can put the little brackets like this and just pass in some example that I want to get a prediction for. So that's what I'm going to do, I'm going to give it the first batch, and it can even it shows me the shape of this batch 64 100, I'm going to pass that to the model, and it's going to give us a prediction for that. And in fact, it's actually going to give us a prediction for every single element in the batch, right, every single training example in the batch, it's going to give us a prediction for. So let's look at what those predictions are. So this is what we get, we get a length 64 tensor, right? And then inside of here, we get a list inside of a list or an array inside of an array with all of these different predictions. So we'll stop there for this, like explaining this aspect here. But you can see we're getting 64 different predictions because there's 64 elements in the batch. Now, let's look at one prediction. So let's look at the very first prediction for say the first element in the batch, right? So let's do that here. And we see now that we get a length 100 tensor. And that this is what it looks like, there's still another layer inside. And in fact, we can see that there's another nested layer here, right, another nested array inside of this array. So the reason for this is because at every single time step, which means the length of the sequence, right, because remember, our recurrent neural network is going to feed one at a time every word in the sequence. In this case, our sequences are like the 100 at every time step, we're actually saving that output as a, as a prediction, right? And we're passing that back. So we can see that for one batch, one training, sorry, not one batch, one training example, we get 100 outputs. And these outputs are in some shape, we'll talk about what those are in a second. So that's something to remember that for every single training example, we get whatever the length of that training example was outputs, because that's the way that this model works. And then finally, we look at the prediction at just the very first time step. So this is 100 different time steps. So let's look at the first time step and see what that prediction is. And we can see that now we get a tensor of length 65. And this is telling us the probability of every single character occurring next at the first time step. So that's what I wanted to walk through is showing you what's actually outputted from the model, the current way that it works. And that's why we need to actually make our own loss function to be able to determine how, you know, good our models performing, when it outputs something ridiculous that looks like this, because there is no just built in loss function and tensor flow that can look at a three dimensional nested array of probabilities over, you know, the vocabulary size, and tell us how different the two things are. So we need to make our own loss function. So if we want to determine the predicted character from this array, so we'll go there now. What we can do is get the categorical, what's this called, we can sample the categorical distribution. And that will tell us the predicted character. So what I mean is, let's just look at this. And then we'll explain this. So since our model works on random weights and biases right now, we haven't trained yet. This is actually all of the predicted characters that it had. So at every time step, at the first time step, it predicted h, then it predicted hyphen, then h, then g, then you, and so on so forth, you get the point, right? So what we're doing to get this value is we're going to sample the prediction. So at this, this is just the first time step, actually, we're sampled the prediction. Actually, no, sorry, we're sampling every time stamp, my bad there. We're going to say sampled indices equals NP dot reshapes, we're just reshaping this just changing the shape of it. We're going to say predicted characters equals int to text sampled indices. So it's, I really, it's hard to explain all this if you guys don't have a statistics kind of background a little bit to talk about why we're sampling and not just taking the argument max value of like this array, because you would think that what we'll do is just take the one that has the highest probability out of here, and that will be the index of the next predicted character. There's some issues with doing that for the loss function. Just because if we do that, then what that means is we're going to kind of get stuck in an infinite loop almost where we just keep accepting the biggest character. So what we'll do is pick a character based on this probability distribution, kind of, yeah, again, it's hard. It's called sampling the distribution. You can look that up if you don't know what that means, but sampling is just like trying to pick a character based on a probability distribution. It doesn't guarantee that the character with the highest probability is going to be picked. It just uses those probabilities to pick it. I hope that makes sense. I know that was like a really rambly definition, but that's the best I can do. So here, we reshape the array and convert all the integers to numbers to see the actual characters. So that's what these two lines are doing here. And then I'm just showing the predicted characters by showing you this. And you know, the character here is what was predicted at time step zero to be the next character and so on. Okay, so now we can create a loss function that actually handles this for us. So this is the loss function that we have. Keras has like a built in one that we can utilize, which is what we're doing. But what this is going to do is take all of the labels and all of the probability distributions, which is what this is, logits, I'm not going to talk about that really. And we'll compute a loss on those. So how different or how similar those two things are. Remember the goal of our algorithm in the neural network is to reduce the loss, right? Okay, so next, we're going to compile the model, which we'll do here. So we're going to compile the model with the atom optimizer and the loss function as loss, which we defined here. And now we're going to set up some checkpoints. I'm not going to talk about how these work, you can kind of just read through this if you want. And then we're going to train the model. Remember to start your GPU hardware accelerator under runtime, change runtime type GPU, because if you do not, then this is going to be very slow. But once you do that, you can train the model. I've already trained it. But if we go through this training, we can see it's going to say train for 172 steps. It's going to take about, you know, 30 seconds per epoch, probably maybe a little bit less than that. And the more epochs you run this for, the better it will get. This is a different we're not likely going to overfit here. So we can run this for like, say 100 epochs, if we wanted to. For our case, let's actually start by just training this on, let's say two epochs, just to see how it does. And then we'll train it on like 1020, 4050 and compare the results. But you'll notice the more epochs, the better it's going to get. But just like for our case, we'll start with two, and then we'll work our way up. So while that trains, we'll actually explain the next aspect of this without running the code. So essentially, what we need to do, after we've trained the model, we've initially the weights and biases, if we need to rebuild it using a new batch size of one. So remember, the initial batch size was 64, which means that we'd have to pass it 64 inputs or sequences for it to work properly. But now what I've done is I'm going to rebuild the model and change it to a batch size of one, so that we can just pass it some sequence of whatever length we want. And it will work. So if we run this, we've rebuilt the model with batch size one, that's the only thing we've changed. And now what I can do is load the weights by saying model dot load weights tf dot train dot latest checkpoint checkpoint directory, and then build the model using the tensor shape one none. I know sounds strange. This is how we do this rebuild the model. One none is just saying expect the input one and then none means we don't know what the next dimension length will be. But here, checkpoint directory is just we've defined where on our computer, we're going to save these TensorFlow checkpoints. This is just saying this is the was it the prefix we're going to save the checkpoint with. So we're going to do the checkpoint directory. And then checkpoint epoch where epoch will stand for obviously, whatever epoch we're on. So we'll save checkpoint here, we'll save a checkpoint at epoch one, a checkpoint at epoch two, to get the latest checkpoint, we do this. And then if we wanted to load any intermediate checkpoint, say like checkpoint 10, which is what I've defined here, we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading by saying tf dot train dot load checkpoint, whereas this one just gets the most recent. So we'll get the most recent, which should be checkpoint two for me. And then what we're going to do is generate the text. So this function, I'll dig into it in a second, but I just want to run and show you how this works, because I feel like we've done a lot of work for not very many results right now. And I'm just going to type in the string Romeo, and just show you that when I do this, we give it a second. And it will actually generate an output sequence like this. So we have Romeo us give this is the beginning of our sequence that says lady Capulet food, Marathon father, gnomes come to those shall, right? So it's like pseudo English, most of it are like kind of proper words. But again, this is because we trained it on just two epochs. So I'll talk about how we build this in a second. But if you wanted a better output for this part, then you would train this on more epochs. So now let's talk about how I actually generated that output. So we rebuilt the model to accept a batch size of one, which means that I can pass it a sequence of any length. And in fact, what I start by doing is passing the sequence that I've typed in here, which was Romeo, then what that does is we run this function generate text, I just stole this from TensorFlow's website, like I've stolen almost all of this code. And then we say the number of characters to generate is 800, the input evaluation, which is now like we need to pre process this text again, so that this works properly, we could use my little function, or we can just write this line of code here, which does with the function that I wrote does for us. So char to IDX S for S and start string, start string is what we typed in in that case, Romeo, then what we're going to do is expand the dimensions. So essentially turn just a list like this that has all these numbers, nine, eight, seven into a double list like this, or just a nested list, because that's what it's expecting as the input one batch, one entry. Then what we do is we're going to say the string that we want to store, because we want to print this out at the end, right, we'll put in this text generated list, temperature equals 1.0. What this will allow us to do is if we change this value to be higher, I mean, you can read the comment here, right, low temperature results in more predictable text, higher temperature results in more surprising text. So this is just a parameter to mess with if you want, you don't necessarily need it. And I would like, I've just left mine at one for now, we're going to start by resetting the states of the model. This is because when we rebuild the model, it's going to have stored the last state that it remembered when it was training. So we need to clear that before we pass new input text to it. And we say for I and range num generate, which means however many characters we want to generate, which is 800 here, what we're going to do is say predictions equals model input a vowel, that's going to start as the start string that's encoded, right. And then what we're going to do is say predictions equals TF dot squeeze prediction zero. What this does is take our predictions, which is going to be in a nested list, and just removes that exterior dimension. So we just have the predictions that we want, we don't have that extra dimension that we need to index again. And then we're going to say using a categorical distribution to predict the character returned by the model, that's what it writes here. We'll divide by the temperature, if it's one, that's not going to do anything. We'll say predicted ID equals we'll sample whatever the output was from the model, which is what this is doing. And then we're going to take that output. So the predicted ID, and we are going to add that to the input evaluation. And then what we're going to say is text generated dot append, and we're going to convert the text that are integers now, back into a string, and return all of this. Now I know this seems like a lot. Again, this is just given to us by TensorFlow to, you know, create this aspect, you can read through the comments yourself, if you want to understand it more. But I think that was a decent explanation of what this is doing. So yeah, that is how we can generate, you know, sequences using recurrent neural network. Now what I'm going to do is go to my other window here where I've actually typed all of the code, just in full and do a quick summary of everything that we've done, just because there was a lot that went on. And then from there, I'm actually going to train this on a B movie script and show you kind of how that works in comparison to the Romeo and Juliet. Okay, so what I'm in now is just the exact same notebook we have before, but I've just pretty much copied all the text in here. Or it's the exact same code we had before. So we just don't have all that other text in between. So I can kind of do a short summary of what we did, as well as show you how this worked when I trained it on the B movie script. So I did mention I was going to show you that I'm not lying, I will show you can see I've got B movie dot txt loaded in here. And in fact, actually, I'm going to show you this script first to show you what it looks like. So this is what the B movie script looks like. You can see it just like a long, you know, script of text, I just downloaded this for free off the internet. And it's actually not as long as the Romeo and Juliet play. So we're not going to get as good of results from our model. But it should hopefully be okay. So we just start and I'm just going to do a brief summary. And then I'll show you the results from the B movie script, just so that people that are confused, maybe have something that wraps it up here. We're doing our imports. I don't think I need to explain that this part up here is just loading in your file. Again, I don't think I need to explain that. Then we're actually going to read the file. So open it from our directory, decode it into utf eight, we're going to create a vocabulary and encode all of the text that's inside of this file. Then what we're going to do is turn all of that text up into you know, the encoded version, we're writing a function here that goes the other way around. So from int to text, not from text to int, we're going to define the sequence length that we want to train with, which will be sequence length of 100. You can decrease this value if you want, you go 50, go 20, it doesn't really matter. It's up to you. It just that's going to determine how many training examples you're going to have right is the sequence length. Next, what we're going to do is create a character data set from tensor slices from text as int. What this is going to do is just convert our entire text that's now an integer array into a bunch of slices of characters. And so that's what this is doing here. So or not slices, what am I saying, you're just going to convert, like, split that entire array into just characters, like that's pretty much what it's doing. And then what we're going to say sequences equals char data set dot batch, which now is going to take all those characters and batch them in lengths of 101. What we're going to do then is split all of that into the training examples. So like this, right, he ll and then yellow, we're going to map this function to sequences, which means we're going to apply this to every single sequence and store that in data set. Then we're going to find the parameters for our initial network. We're going to shuffle the data set and batch that into now 64 training examples. Then we're going to make the function that builds the model, which I've already discussed, we're going to actually build the model starting with a batch size of 64. We're going to create our loss function, compile the model, set our checkpoints for saving, and then train the model and make sure that we say checkpoint callback as the checkpoint callback for the model, which means it's going to save every epoch, the weights that the model had computed at that epoch. So after we do that, then our models train so we've trained the model, you can see I train this on 50 epochs for the B movie script. And then what we're going to do is build the model now with a batch size of one. So we can pass one example to it and get a prediction, we're going to load the most recent weights into our model from the checkpoint directory that we defined above. And then what we're going to do is build the model and tell it to expect the shape one, none as its initial input. Now none just means we don't know what that value is going to be, but we know we're going to have one entry. Alright, so now we have this generate text method, or function here, which I've already kind of went through how that works. And then we can see, if I type in input string, so we type, you know, input string, let's say, of hello, and hit enter, we'll watch and we can see that the B movie, you know, trained model comes up with its output here. Now, unfortunately, the B movie script does not work as well as Romeo and Juliet. That's just because Romeo and Juliet is a much longer piece of text. It's much better it's format a lot nicer and a lot more predictable. But yeah, you kind of get the idea here and it's kind of cool to see how this performs on different data. So I would highly recommend that you guys find some training data that you could give this other than just the Romeo and Juliet or maybe even try another play or something and see what you can get out of it. Also, quick side note, to make your model better, increase the amount of epochs here. Ideally, you want this loss to be as low as possible, you can see mine was still actually moving down at epoch 50. You will reach a point where the amount of epochs won't make a difference. Although, with models like this, the more epochs typically the better, because it's difficult for it to kind of overfit, because all you want it to do really is just kind of learn how the language works and then be able to replicate that to you almost, right? So that's kind of the idea here. And with that being said, I'm going to say that this section is probably done. Now, I know this was a long, probably confusing section for a lot of you. But this is, you know, what happens when you start getting into some more complex things in machine learning, it's very difficult to kind of grasp and understand all these concepts in an hour of me just explaining them. What I try to do in these videos is introduce you to the syntax show you how to get a working, you know, kind of prototype and hopefully give you enough knowledge to the fact where if you're confused by something that I said, you can go and you can look that up and you can figure out kind of the more important details for yourself, because I really just I can't go into all, you know, the extremes in these videos. So anyways, that has been this section. I hope you guys enjoyed doing this. I thought this was pretty cool. And in the next section, we're going to be talking about reinforcement learning. Hello, everyone, and welcome to the next module in this course on reinforcement learning. So what we're going to be doing in this module is talking about another technique in machine learning called reinforcement learning. Now, if you remember at the very beginning of this course, which I know for you guys is probably at like six hours ago at this point, we did briefly discuss what reinforcement learning was. Now I'll go through a recap here just to make sure everyone's clear on it. But essentially, reinforcement learning is kind of the strategy in machine learning where rather than feeding a ton of data and a ton of examples to our model, we let the model or in this case, we're going to call it agent actually come up with these examples itself. And we do this by letting an agent explore an environment. Now, essentially, the concept here is just like humans, the way that we learn to do something say like play a game is by actually doing it, we get put in the environment, we try to do it. And then, you know, we'll make mistakes, we'll encounter different things, we'll see what goes correctly. And based on those experiences, we learn and we figure out the correct things to do a very basic example is, you know, say we play a game. And when we go left, we fell off a cliff or something, right? Next time we play that game, and we get to that point, we're probably not going to go left, because we're going to remember the fact that that was bad, and hence learned from our mistakes. So that's kind of the idea here with reinforcement learning. I'm going to go through exactly how this works and give some better examples and some math behind one of the implementations we're going to use. But I just want to make this clear that there's a lot of different types of reinforcement learning. In this example, we're just going to be talking about something called q learning. And I'm going to keep this module shorter compared to the other ones. Because this field of AI machine learning is pretty complex and can get pretty difficult pretty quickly. So it's something that's maybe a more advanced topic for some of you guys. Alright, so anyways, now we need to define some terminology before I can even start really explaining the technique we're going to use and how this works. So we have something called an environment, agent, state action and reward. And I'm hoping that some of you guys will remember this from the very beginning. But environment is essentially what we're trying to solve or what we're trying to do. So in reinforcement learning, we have this notion of an agent. And the agent is what's going to explore the environment. So if we're thinking about reinforcement learning, when it comes to say training an AI to play a game, well, in that instance, say we're talking about Mario, the agent would be Mario as that is the thing that's moving around and exploring our environment. And the environment would be the level in which we're playing in. So you know, in another example, maybe in the example we're going to use below, we're actually going to be kind of in almost a maze. So the environment is going to be the maze. And the agent is going to be the character or the entity or whatever you want to call it, that's exploring that maze. So it's pretty, it's usually pretty intuitive to come up with what the environment and the agent are, although in some more complex examples, it might not always be clear. But just understand that reinforcement learning deals with an agent, something exploring an environment and a very common application of reinforcement learning is in training AI is on how to play games. And it's actually very interesting what they've been able to do in that field recently. Okay, so we have environments and agent, hopefully that makes sense. The next thing to talk about is state. So essentially, the state is where you are in the environment. So obviously, inside of the environment, we can have many different states. And a state could also be associated with the, you know, agent itself. So we're going to say the agent is in a specific state, whenever it is in some part of the environment. Now, in the case of our game, the state that an agent would be in would be their position in the level, say if they're at, you know, x y coordinates, like 1020, they would be at state or in state 1020. That's kind of how we think about states. Now, obviously, state could be applied in some different instances as well. We're playing say, maybe a turn based game. You know, actually, that's not really a great example. I'm trying to think of something where the state wouldn't necessarily be a position, maybe if you're playing a game where you have like health or something like that. And part of the state might be the health of the character. This can get complicated, depending on what you're trying to do. But just understand the notion that for most of our example, state is simply going to be in location, although it really is just kind of telling us information about where the agent is, and its status in the environment. So next, we have this notion of an action. So in reinforcement learning, our agent is exploring the environment, it's trying to figure out the best way or how to accomplish some kind of goal in the environment. And the way that it interacts with the environment is with something called actions. Now, actions could be say, moving the left arrow key, right, moving to the left in the environment, moving to the right, it could be something like jumping in an action can actually be not doing something at all. So when we say, you know, agent performed action, that could really mean that the action and that maybe time step was that they didn't do something, right, that they didn't do anything that was their action. So that's kind of the idea of action. In the example of our Mario one, which I keep going back to an action would be something like jumping. And typically actions will change the state of our entity or our agent, although they might not necessarily do that. In fact, we will observe with a lot of the different actions that we could actually be in the same state after performing that action. Alright, so now we're on to the last part, which is actually the most important to understand. And this is reward. So reward is actually what our agent is trying to maximize while it is in the environment. So the goal of reinforcement learning is to have this agent navigate this environment, go through a bunch of the different states of it and determine which actions maximize the reward at every given state. So essentially, the goal of our agent is to maximize a reward. But what is a reward? Well, after every action that's taken, the agent will receive a reward. Now this reward is something that us as the programmer need to come up with. The reason we need to do this is because we need to tell the agent when it's performing well and when it's performing poorly. And just like we had like a loss function in neural networks, when we're using those before, this is almost like our loss function, you know, the higher this number is, the more reward the agent gets, the better, the lower the reward, you know, it's not as good, it's not doing as well. So that's how we kind of monitor and assess performance for our agents is by determining the almost average amount of reward that they're able to achieve. And their goal is really to, you know, it's almost an optimization problem where they're trying to maximize this reward. So what we're going to do in reinforcement learning is have this agent exploring the environment, going through these different states and performing these different actions, trying to maximize its reward. And obviously, if we're trying to get the agent to say finish a level or, you know, complete the game, then the maximum maximum reward will be achieved once it's completed the level or completed the game. And if it does things that we don't like, say like dying or like jumping in the wrong spot, we could give it a negative reward to try to influence it to not do that. And our goal, you know, when we train these agents is for them to get the most reward. And we hope that they're going to learn the optimal route through a level or through some environment that will maximize that reward for them. Okay, so now I'm going to talk about a technique called Q learning, which is actually just an algorithm that we're going to use to implement this idea of reinforcement learning. We're not going to get into anything too crazy in this last module, because this is meant to be more of an introduction into the kind of field of reinforcement learning than anything else. But Q learning is the most basic way to implement reinforcement learning, at least that I have discovered. And essentially, what Q learning is, and I don't actually really know why they call it Q, although I should probably know that is creating some kind of table or matrix likes data structure, that's going to contain as the, what is it, I guess the rows, every single state, and as the columns, every single action that could be taken in all of those different states. So for an example here, and we'll do one on kind of the whiteboard later on, if we can get there. But here, we can see that this is kind of my Q table. And what I'm saying is that we have a one, a two, a three, a four, as all of the possible actions that could be performed in any given state. And we have three states denoted by the fact that we have three rows. And the numbers in this, this table with this Q, what do they call it, Q matrix Q table, whatever you want to call it, the numbers that are present here, represent what the predicted reward will be, given that we take an action, whatever this action is in this state. So I'm not sure if this is making sense to you guys, but essentially, if we're saying that row zero is state zero, action two, a two, this value tells us what reward we should expect to get. If we take this action while we're in this state, that's what that is trying to tell us. That's what that means. Same thing here in, you know, state two, we can see that the optimal action to take would be action two, because that has the highest reward for this state. And that's what this table is that we're going to try to generate with this technique called Q learning, a table that can tell us given any state, what the predicted reward will be for any action that we take. And we're going to generate this table by exploring the environment many different times, and updating these values according to what we kind of see or what the agent sees in the environment and the rewards that it receives for any given action in any given state. And we'll talk about how we're going to update that later. But this is the basic premise. So that is kind of Q learning, we're going to hop on the whiteboard now, and we'll do a more in depth example, but then we're going to talk about how we actually learned this Q table that I just discussed. Okay, so I've drawn a pretty basic example right now that I'm going to try to use to illustrate the idea of Q learning and talk about some problems with it and how we can kind of combat those as we learn more about how Q learning works. But the idea here is that we currently have three states and why, what is happening? Why was that happening up at the top? I don't know. Anyways, the idea is we have three states as one s two and s three. And at each state we have two possible actions that can be taken, we can either stay in this state or we can move. Now, what I've done is kind of just written some integers here that represent the reward that we're going to get or that the agent is going to get such that it takes that action in a given state. So if we take the action here in s one, right of moving, then we will receive a reward of one because that's what we've written here is the reward that we get for moving. Whereas if we stay, we'll get a reward of three, you know, same concept here, if we stay, we get two, if we move, we get one, and I think you understand the point. So the goal of our agent to remember is to maximize its reward in the environment. And what we're going to call the environment is this right here, the environment is essentially defines the number of states, the number of actions, and you know, the way that the agent can interact with these states and these actions. So in this case, the agent can interact with the states by taking actions that change its state, right? So that's where we're getting out with this. Now, what I want to do is show you how we use this queue table, or learn this queue table to come up with kind of the almost, you know, the model, like the machine learning model that we're going to use. So essentially, what we would want to have here is we want to have a kind of pattern in this table that allows our agent to receive the maximum reward. So in this case, we're going to say that our agent will start at state s one. And obviously, whenever we're doing this reinforcement learning, we need to have some kind of start state that the agent will start in this could be a random state, it could change, but it doesn't just start in some state. So in this case, we're going to say it starts at s one. Now, when we're in s one, the agent has two things that it can do. It can stay in the current state and receive a reward of three, or it can move and receive a reward of one, right? If we get to s two, in this state, what can we do? We can stay, which means we receive a reward of two, or we can move, which means we get a reward of one. And same thing for s three, we can stay, we get a reward of four, and we can move, we get a reward of one. Now, right now, if we had just ran this one time and have the agent stay in each state, like start in each unique state, this is what the queue table we would get would look like. Because after looking at this, just one time starting in each state, what the agent would be able to, or I guess, two times, because it would have to try each action. Let's say we had the agent start in each state twice. So it started an s one twice, it started s two twice, and it started an s three twice. And every time it started there, it tried one of the different actions. So when it started in s one, it tried moving once, and then it tried staying once, we would have a queue table that looks like this. Because what would happen is we would update values in our queue table to represent the reward we received when we took that action from that state. So we can see here that when we're in state s one, and we decide to stay, what we did is we wrote a three inside of the stay column, because that is how much reward we received when we moved, right? Same thing for state two, when we moved for state two or have I guess, sorry, stayed when we stayed in state two, we received a reward of two, same thing for four. Now, this is okay, right? This tells us kind of, you know, the optimal move to make in any state to receive the maximum reward. But what if we introduce the idea that, you know, our agent, we want it to receive the maximum total reward possible, right? So if it's in state one, ideally, we'd like it to move to state two, and then move to states three, and then just stay in state three, because it will receive the most amount of reward. Well, with the current table that we've developed, if we just follow this, and we look at the table, we say, okay, if we want to use this Q learning table now to, you know, move an agent around our level, what we'll do is we'll say, okay, what state is it in? If it's in state two, we'll do stay because that's the highest reward that we have in this table. If that's the approach we use, then we could see that if our, you know, agent start in state one or state two, it would stay in what we call a local minima, because it's not able to kind of realize from this state that it can move any further and receive a much greater reward, right? And that's kind of the concept we're going to talk about as we implement and, you know, discuss further how Q learning works. But hopefully this gives you a little bit of insight into what we do with this table. Essentially, when we're updating these table values is when we're exploring this environment. So when we explore this environment, and we start in a state, when we take an action to another state, we observe the reward that we got from going there, and we observe the state that we change to, right? So we observe the fact that in state one, when we go to state two, we receive the reward of one. And what we do is we take that observation and we use it to update this Q table. And the goal is that at the end of all of these observations, and there could be millions of them, that we have a Q table that tells us the optimal action to take in any single state. So we're actually hard coding, this kind of mapping that essentially just tells us given any state, all you have to do is look up in this table, look at all of the actions that could be taken, and just take the maximum action or the reward that's supposed to give, I guess, the action that's supposed to give the maximum reward. And if we were to follow that on this, we could see we get stuck in the local minima, which is why we're going to introduce a lot of other concepts. So our reinforcement learning model in Q learning, we have to implement the concept of being able to explore the environment, not based on previous experiences, right? Because if we just tell our model, okay, what we're going to do is we're going to start in all these different states, we're going to start in the start state and just start navigating around. If we update our model immediately, or update our Q table immediately and put this three here for stay, we can almost guarantee that since this three is here, when our model is training, right, if it's using this Q table to determine what state to move to next, when it's training and determining what to do, it's just always going to stay, which means we'll never get a chance to even see what we could have gotten to at S three. So we need to kind of introduce some concept of taking random actions, and being able to explore the environment more freely before starting to look at these Q values, and use that for the training. So I'm actually going to go back to my slides now to make sure I don't get lost, because I think I was starting to ramble a little bit there. So we're going to now talk about learning the Q table. So essentially, I showed you how we use that Q table, which is given some state, we just look that state up in the Q table, and then determine what the maximum reward we could get by taking, you know, some action is and then take that action. And that's how we would use the Q table later on when we're actually using the model. But when we're learning the Q table, that's not necessarily what we want to do. We don't want to explore the environment by just taking the maximum reward we've seen so far and just always going that direction, we need to make sure that we're exploring in a different way and learning the correct values for the Q table. So essentially, our agent learns by exploring the environment and observing the outcome slash reward from each action it takes in a given state, which we've already said. But how does it know what action to take in each state when it's learning? That's the question I need to answer for you now. Well, there's two ways of doing this. Our agent can essentially, you know, use the current Q table to find the best action, which is kind of what I just discussed. So taking looking at the Q table, looking at the state and just taking the highest reward, or it can randomly pick a valid action. And our goal is going to be when we create this Q learning algorithm to have a really great balance of these two, where sometimes we use the Q table to find the best action, and sometimes we take a random action. So that is one thing. But now I'm just going to talk about this formula for how we actually update Q values. So obviously, what's going to end up happening in our Q learning is we're going to have an agent that's going to be in the learning stage, exploring the environment and having all these actions and all these rewards and all these observations happening. And it's going to be moving around the environment by following one of these two kind of principles, randomly picking a valid action or using the current Q table to find the best action. When it gets into a net, a new state, and it, you know, moves from state to state, it's going to keep updating this Q table, telling it, you know, this is what I've learned about the environment, I think this is a better move, we're going to update this value. But how does it do that in a way that's going to make sense? Because we can't just put, you know, the maximum value we got from moving, otherwise, we're going to run into that issue, which I just talked about, where we get stuck in that local maxima, right? I'm not sure if I called it minimum before, but anyways, it's local maxima, where we see this high reward, but that's preventing us if we keep taking that action from reaching a potentially high reward in a different state. So the formula that we actually use to update the Q table is this. So Q state action equals Q state action, and a state action is just referencing first the rows for the state and then the action as the column, plus alpha times, and then this is all in brackets, right? Reward plus, I believe this is gamma times max Q of new states minus Q state action. So what the heck does this mean? What are these constants? What is all this? We're going to talk about the constants in a minute. But I want to, yeah, I want to explain this formula actually. So let's, okay, I guess we'll go through the constants, it's hard to go through a complicated math formula. So a stands for the learning rate, and gamma stands for the discount factor. So alpha learning rate, gamma discount factor. Now, what is the learning rate? Well, this is a little blurb on what this is. But essentially, the learning rate ensures that we don't update our Q table too much on every observation. So before, right, when I was showing you like this, if we can go back to my windows ink, why is this not working? I guess I'm just not patient enough. Before when I was showing you all I did when I took an action was I looked at the reward that I got from taking that action. And I just put that in my Q table, right? Now, obviously, that is not an optimal approach to do this, because that means that in the instance where we hit state one, well, I'm not going to be able to get to this reward of four, because I'm going to throw that, you know, three in here, and I'm just going to keep taking that action. We need to, you know, hopefully make this move action actually have a higher value than stay. So that next time we're in state one, we consider the fact that we could move to state two, and then move to state three to optimize our reward. So how do we do that? Well, the learning rate is one thing that helps us kind of accomplish this behavior. Essentially, what is telling us, and this is usually a decimal value, right, is how much we're allowed to update every single Q value by on every single action or every single observation. So if we just use the approach before, then we're only going to need to observe, given the amount of states and the amount of actions, and we'll be able to completely fill in the Q table. So in our case, if we had like three states and three actions, we could, you know, nine iterations, we'd be able to fill the entire Q table. The learning rate means that it's going to just update a little bit slower and essentially change the value in the Q table very slightly. So you can see that what we're doing is taking the current value of the Q table. So whatever is already there. And then what we're going to do is add some value here. And this value that we add is either going to be positive or negative, essentially telling us, you know, whether we should take this new action or whether we shouldn't take this new action. Now, the way that this kind of value is calculated, right, is obviously our alpha is multiplied this by this, but we have the reward plus, in this case, gamma, which is just going to actually be the discount factor. And I'll talk about how that works in a second of the maximum of the new state we moved into. Now, what this means is find the maximum reward that we could receive in the new state by taking any action and multiply that by what we call the discount factor. With this part of the formulas trying to do is exactly what I've kind of been talking about. Try to look forward and say, okay, so I know if I take this action in this state, I receive this amount of reward. But I need to factor in the reward I could receive in the next state, so that I can determine the best place to move to. That's kind of what this max and this gamma are trying to do for us. So this discount factor, whatever you want to call it. It's trying to factor in a little bit about what we could get from the next state into this equation so that hopefully our kind of agent can learn a little bit more about the transition states. So states that maybe are actions that maybe don't give us an immediate reward, but lead to a larger reward in the future. That's what this Y and max are trying to do. Then what we do is we subtract from this, the state and action. This is just to make sure that we're adding what the difference was in, you know, what we get from this versus what the current value is, and not like multiplying these values crazily. I mean, you can look into more of the math here and plug in like some values later, and you'll see how this kind of works. But this is the basic formula. And I feel like I explain that in depth enough. Okay, so now that we've done that, and we've updated this, we've learned kind of how we update the cells and how this works. I could go back to the whiteboard and draw it out. But I feel like that makes enough sense. We're going to look at what the next state is, we're going to factor that into our calculation, we have this learning rate, which tells us essentially how much we can update each cell value by. And we have this, what do you call it here discount factor, which essentially tries to kind of define the balance between finding really good rewards in our current state, and finding the rewards in the future state. So the higher this value is, the more we're going to look towards the future, the lower it is, the more we're going to focus completely on our current reward, right? And obviously, that makes sense, because we're going to add the maximum value. And if we're multiplying that by a lower number, that means we're going to consider that less than if that was greater. Awesome. Okay. So now that we've kind of understand that I want to move on to a Q learning example. And what we're going to do for this example is actually use something called the open AI gym. I just need to throw my drawing tablet away right there so that we can get started. But open AI gym is actually a really interesting kind of module. I don't even actually, I don't even really know the way to describe it almost tool. There's actually developed by open AI, you know, coincidentally by the name, which is founded by Elon Musk and someone else. So he's actually, you know, made this kind of, I don't really don't know the word to describe it. I almost want to say tool that allows programmers to work with these really cool gym environments and train reinforcement learning models. So you'll see how this works in a second, but essentially, there's a ton of graphical environments that have very easy interfaces to use. So like moving characters around them, that you're allowed to experiment with completely for free as a programmer to try to, you know, make some cool reinforcement learning models. That's what open AI gym is. And you can look at it. I mean, we'll click on it here actually to see what it is. You can see gym, there's all these different Atari environments, and it's just a way to kind of train reinforcement learning models. All right. So now we're going to start by just importing gym. If you're in Collaboratory, there's nothing you need to do here. If you're in your own thing, you're going to have to pip install gym. And then what we're going to do is make this frozen lake v zero gym. So essentially, what this does is just set up the environment that we're going to use. Now, I'll talk more about what this environment is later, but I want to talk about how gym works, because we are going to be using this throughout the thing. So the open AI gym is meant for reinforcement learning. And essentially what it has is an observation space and an action space for every environment. Now the observation space is what we call our environment, right? And that will tell us the amount of states that exist in this environment. Now, in our case, we're going to be using kind of like a maze like thing, which I'll show you in a second. So you'll understand why we get the values we do. Action space tells us how many actions we can take when we do the dot n, at any given state. So if we print this out, we get 16 and four, representing the observation space. In other words, the number of states is 16. And the amount of actions we can take in every single state is four. Now in this case, these actions are going to be left down up and right. But yes, now env dot reset. So essentially, we have some commands that allow us to move around the environment, which are actually down here. If we want to reset the environment and start back in the beginning state, then we do env dot reset, you can see this actually returns to us the starting state, which obviously is going to be zero. Now we also have the ability to take a random action, or select a random action from the action space. So what this line does right here is say of the action space, so of all the commands that are there, or all the actions we could take, pick a random one and return that. So if you do that, actually, let's just print action and see what this is. You see we get zero to right, it just gives us a random action that is valid from the action space. All right. Next, what we have is this env dot step in action. Now what this does is take whatever action we have, which in this case is three, and perform that in the environment. So tell our agent to take this action in the environment and return to us a bunch of information. So the first thing is the observation, which essentially means what state do we move into next? So I could call this new underserved state reward is what reward did we receive by taking that action? So this will be some value right in our in this case, the reward is either one or zero. But that's not that important to understand. And then we have a bool of done, which tells us did we lose the game or did we win the game? Yes or no. So true. So if this is true, what this means is we need to reset the environment because our agent either lost or won and is no longer in a valid state in the environment. Info gives us a little bit of information. It's not showing me anything here. We're not going to use info throughout this, but figured I'd let you know that now in VDOT render, I'll actually render this for you and show you renders a graphical user interface that shows you the environment. Now, if you use this while you're training, so you actually watch the agent do the training, which is what you can do with this, it slows it down drastically, like probably by, you know, 10 or 20 times, because it actually needs to draw the stuff on the screen. But you know, you can use it if you want. So this is what our frozen lake example looks like. You can see that the highlighted square is where our agent is. And in this case, we have four different blocks. We have SFH and G. So S stands for start F stands for frozen, because this is a frozen lake. And the goal is to navigate to the goal without falling in one of the holes, which is represented by H. And this here tells us the action that we just took. Now, I guess the starting action is up because that's zero, I believe. But yes, so if we run this a bunch of times, we'll see this updating. Unfortunately, this doesn't work very well in Google Collaboratory, the the GUIs. But if you did this in your own command line, and you like did some different steps and rounded it all out, you would see this working properly. Okay, so now we're on to talking about the frozen lake environment, which is kind of what I just did. So now we're just going to move to the example where we actually implement Q learning to essentially solve the problem. How can we train an AI to navigate this environment and get to the start to the goal? How can we do that? Well, we're going to use Q learning. So let's start. So the first thing we need to do is import gym, import numpy, and then create some constants here. So we'll do that. We're going to say the amount of states is equal to the line I showed you before. So env dot observation, space dot n, actions is equal to env dot action space n. And then we're going to say Q is equal to NP dot zeros, states and actions. So something I guess I forgot to mention is when we initialize the Q table, we just initialize all blank values or zero values, because obviously, at the beginning of our learning, our model or agent doesn't know anything about the environment yet. So we just leave those all blank, which means we're going to more likely be taking random actions at the beginning of our training, trying to explore the environment space more. And then as we get further on and learn more about the environment, those actions will likely be more calculated based on the Q table values. So we print this out, we can see this is the array that we get, we've had to be build a 16 by four, I guess not array, well, I guess this technically is an array, we'll call it matrix 16 by four. So every single row represents a state, and every single column represents an action that could be taken in that state. Alright, so we're going to find some constants here, which we talked about before. So we have the gamma, the learning rate, the max amount of steps and the number of episodes. So the number of episodes is actually, how many episodes do you want to train your agent on? So how many times do you want it to run around and explore the environment? That's what episode stands for. Max steps essentially says, Okay, so if we're in the environment, and we're kind of navigating and moving around, and we haven't died yet, how many steps are we going to let the agent take before we cut it off? Because what could happen is we could just bounce in between two different states indefinitely. So we need to make sure we have a max steps so that at some point, if the agent is just doing the same thing, we can, you know, end that or if it's like going in circles, we can end that and start again with different, you know, Q values. Alright, so episodes, yeah, we already talked about that learning rate, we know what that is gamma, we know what that is mess with these values as we go through and you'll see the difference it makes in our training. I've actually included a graph down below. So we'll talk about that kind of show us the outcome of our training. But learning rate, the higher this is, the faster I believe that it learns. Yes, so a high learning rate means that each update will introduce larger change to the current state. So yeah, so that makes sense based on the equation as well. Just wanted to make sure that I wasn't going crazy there. So let's run this constant block to make sure. And now we're going to talk about picking an action. So remember how I said, and I actually wrote them down here, there's essentially two things we can do at every, what do we call it, step, right? We can randomly pick a valid action, or we can use the current Q table to find the best action. So how do we actually implement that into our open AI gym? Well, I just wanted to write a little code block here to show you the exact code that will do this for us. So we're going to introduce this new concept or this new, I can almost call it constant, called epsilon. And I think epsilon, I think I spelt this wrong, ep salon. Yeah, that should be how you spell it. So we're going to start the epsilon value essentially tells us the percentage chance that we're going to pick a random action. So here, we're going to use a 90% epsilon, which essentially means that every time we take an action, there's going to be a 90% chance that it's random and 10% chance that we look at the Q table to make that action. Now, we'll reduce this epsilon value as we train, so that our model will start being able to explore, you know, as much as it possibly can in the environment by just taking random actions. And then after we have enough observations, and we've explored the environment enough, we'll start to slowly decrease the epsilon, so that it hopefully finds a more optimal route for things to do. Now, the way we do this is we save NP dot random dot uniform zero one, which essentially means pick a random value between zero and one is less than epsilon and epsilon like that. I think I'm going to have to change some other stuff, but we'll see, then action equals ENV dot action space dot sample. So take a random action. That's what this means store what that action is in here. Otherwise, we're going to take the argument max of the state row in the Q table. So what this means is find the maximum value in the Q table and tell us what row it's in. So that way we know what action to take. So if we're in row, I guess, not sorry, not row column for in column one, you know, that's maximum value, take action one, that's what this is saying. So using the Q table to pick the best action. Alright, so we don't need to run this because this is just going to be which I just wrote that to show you. Now, how do we update the Q values? Well, this is just following the equation that I showed above. So this is the line of code that does this, I just want to write it out so you guys could see exactly what each line is doing and kind of explore it for yourself. But essentially, you get the point, you know, you have your learning rate, reward, gamma, take the max, so NP dot max does the same thing as a max function in Python. This is going to take the max value, not the argument max from the next state, right, the new state that we moved into. And then subtracting obviously Q state action. Alright, so putting it all together. So now we're actually going to show how we can train and create this Q table and then use that Q table. So this is the pretty much all this code that I have, we've already actually written at least this block here, that's why I put it in its own block. So just all the constants, I've included this render constant to tell us whether we want to draw the environment or not. In this case, I'm going to leave it false, but you can make it true if you want. Episodes, I've left at 1500 for this, if you want to make your model better, typically you train it on more episodes, but that's up to you. And now we're going to get into the big chunk of code, which I'm going to talk about. So what this is going to do, we're going to have a rewards list, which is actually just going to store all the rewards we see, just so I can graph that later for you guys. Then we're going to say for episode in range episodes. So this is just telling us, you know, for every episode, let's do the steps I'm about to do. So maximum amount of episodes, which is our training length, essentially, we're going to reset the state, obviously, which makes sense. So state equals in V dot reset, which will give us the starting state. We're going to say for underscore in range, max steps, which means, okay, we're going to do, you know, we're going to explore the environment up to maximum steps, we do have a done here, which will actually break the loop if we've reached the goal, which we'll talk about further. So the first thing we're going to do is say, if render, you know, render the environment, that's pretty straightforward. Otherwise, let's take an action. So for each time step, we need to take an action. So epsilon, I think is spelled correctly here. Yeah, believe that's right. So I'm going to say action equals in V dot action space, this is already the code we've looked at. And then what we're going to say is next state reward done underscore equals in V dot step action, we've put an underscore here, because we don't really care about this info value. So I'm not going to store it, but we do care about what the next state will be the reward from that action. And if we were done or not. So we take that action, that's what does this EMB dot step. And then what we do is say Q state action, we just update the Q value using the formula that we've talked about. So this is the formula, you can look at it more in depth if you want. But based on whatever the reward is, you know, that's how we're going to update those Q values. And after a lot of training, we should have some decent Q values in there. Alright, so then we set the current state to be the next state. So that when we run this time step again, now our agent is in the next state, and can start exploring the environment again, in this current, you know, iteration, almost, if that makes sense. So then we say if done, so essentially, if the agent died, or if they lost or whatever it was, we're going to append whatever reward they got from their last step into the rewards up here. And it's worthy of noting that the way the rewards work here is you get one reward, if you move to a valid block, and you get zero reward, if you die. So every time we move to a valid spot, we get one, otherwise we get zero. I'm pretty sure that's the way it works at least. But that's something that's important to know. So then what we're going to do is reduce the epsilon if we die, but just a fraction of an amount, you know, 0.001, just so we slowly start decreasing the epsilon moving in the correct direction. And then we're going to break because we've reached the goals, print the Q table, and then print the average reward. Now this takes a second to train, like, you know, a few seconds, really. That one is pretty fast, because I've set this at was it 1500. But if you want, you can set this at say 10,000, wait another, you know, few minutes or whatever, and then see how much better you can do. So we can see that after that, I received an average reward of 0.28886667. This is actually what the Q table values look like. So all these decimal values after all these updates, I just decided to print them out. And I just want to show you the average reward so that we can compare that to what we can get from testing or this graph. So now I'm just going to graph this. And we're going to see this is what the graph so you don't have to really understand this code if you don't want to. But this is just graphing the average reward over 100 steps from the beginning to the end. So essentially, I've been, I've calculated the average of every 100 episodes, and then just graph this on here. We can see that we start off very poorly in terms of reward, because the epsilon value is quite high, which means that we're taking, you know, random actions pretty much all the time. So if we're taking a bunch of random actions, obviously, chances are, we're probably going to die a lot, we're probably going to get rewards of zeros quite frequently. And then after we get to about 600 episodes, you can see that six actually represents 600, because this is in hundreds, we start to slowly increase. And then actually, we go on a crazy increase here, when we start to take values more frequently. So the epsilon is increasing, right. And then after we get here, we kind of level off. And this does show a slight decline. But I guarantee you if we ran this for, you know, like 15,000, it would just go up and down and bob up and down. And that's just because even though we have increased the epsilon, there is still a chance that we take a random action and you know, gets your reward. So that is pretty much it for this Q learning example. You know, I mean, that's pretty straightforward to use the Q table. If you actually wanted to say, you know, watch the agent move around the thing, I'm going to leave that to you guys, because if you can follow what I've just done in here and understand this, it's actually quite easy to use the Q table. And I think as like a final, almost like, you know, trust in you guys, you can figure out how to do that. The hint is essentially do exactly what I've done in here, except don't update the Q table values, just use the Q table values already. And that's, you know, pretty much all there is to Q learning. So this has been the reinforcement learning module for this TensorFlow course, which actually is the last module in this series. Now, I hope you guys have enjoyed up until this point, just an emphasis again, this was really just an introduction to reinforcement learning. This technique and this problem itself is not very interesting and not, you know, the best way to do things is not the most powerful. It's just to get you thinking about how reinforcement learning works. And potentially, if you'd like to look into that more, there's a ton of different resources and, you know, things you can look at in terms of reinforcement learning. So that being said, that has been this module. And now we're going to move into the conclusion, we'll talk about some next steps and some more things that you guys can look at to improve your machine learning skills. So finally, after about seven hours of course content, we have reached the conclusion of this course. Now what I'm going to do in this last brief short section is just explain to you where you can go for some next steps and some further learning with TensorFlow and machine learning artificial intelligence in general. Now what I'm going to be recommending to you guys is that we look at the TensorFlow website, because they have some amazing guides and resources on here. And in fact, a lot of the examples that we used in our notebooks were based off of or exactly the same as the original TensorFlow guide. And that's because the code that they have is just very good. They're very good and easy to understand examples. And in terms of learning, I find that these guides are great for people that want to get in quickly, see the examples and then go and do some research on their own time and understand why they work. So if you're looking for some further steps, at this point in time, you have gained a very general and broad knowledge of machine learning and AI, you have some basic skills in a lot of the different areas. And hopefully this has introduced you to a bunch of different concepts and the possibilities of what you are able to do using modules like TensorFlow. Now what I'm going to suggest to all of you is that if you find a specific area of machine learning AI that you are very interested in, that you would dial in on that area and focus most of your time into learning that, that is because when you get to a point in machine learning and AI, where you really get specific and pick one kind of strain or one kind of area, it gets very interesting very quickly. And you can devote most of your time to getting as deep as possible and not specific topic. And that's something that's really cool. And most people that are experts in AI or machine learning field typically have one area of specialization. Now, if you're someone who doesn't care to specialize an area or you just want to play around and see some different things, the TensorFlow website is great to really get kind of a general introduction to a lot of different areas and be able to kind of use this code tweak it a little bit on your own, and implement it into your own projects. And in fact, the next kind of steps and resources I'm going to be showing you here, and involve simply going to the TensorFlow website, going to the tutorial page, this is very easy to find, I don't even need to link it, you can just search TensorFlow, and you'll find this online. And looking at some more advanced topics that we haven't covered. So we've covered a few of the topics and tutorials that are here, I've just kind of modified their version, and thrown out in the notebook and explained it in wars and video content. But if you'd like to move on to say a next step or something very cool, something I would recommend is doing the deep dream in the generic generative neural network section on the TensorFlow website, being able to make something like this, I think is very cool. And this is an example where you can tweak this a ton by yourself and get some really cool results. So some things like this are definitely next steps, there's tons and tons of guides and tutorials on this website, they make it very easy for anyone to get started. And with these guides, what I will say is typically what will end up happening is they just give you the code and brief explanations of why things work. You should really be researching and looking up some more, you know, deep level explanations of why some of these things work as you go through, if you want to have a firm and great understanding of why the model performs the way that it does. So with that being said, I believe I'm going to wrap up the course now. I know you guys can imagine how much work I put into this. So please do leave a like, subscribe to the channel, leave a content, show your support. This I believe is the largest open source machine learning course in the world that deals completely with TensorFlow and Python. And I hope that this gave you a lot of knowledge. So please do give me your feedback down below in the comments. With that being said, again, I hope you enjoyed. And I hopefully I will see you again in another tutorial guide or series.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.32, "text": " Hello, everybody, and welcome to an absolutely massive TensorFlow", "tokens": [50364, 2425, 11, 2201, 11, 293, 2928, 281, 364, 3122, 5994, 37624, 50580], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 1, "seek": 0, "start": 4.32, "end": 7.8, "text": " slash machine learning slash artificial intelligence course.", "tokens": [50580, 17330, 3479, 2539, 17330, 11677, 7599, 1164, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 2, "seek": 0, "start": 8.0, "end": 11.0, "text": " Now, please stick with me for this short introduction, as I am going to give you a", "tokens": [50764, 823, 11, 1767, 2897, 365, 385, 337, 341, 2099, 9339, 11, 382, 286, 669, 516, 281, 976, 291, 257, 50914], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 3, "seek": 0, "start": 11.0, "end": 15.040000000000001, "text": " lot of important information regarding the course concept, the resources for the", "tokens": [50914, 688, 295, 1021, 1589, 8595, 264, 1164, 3410, 11, 264, 3593, 337, 264, 51116], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 4, "seek": 0, "start": 15.040000000000001, "end": 18.12, "text": " course and what you can expect after going through this.", "tokens": [51116, 1164, 293, 437, 291, 393, 2066, 934, 516, 807, 341, 13, 51270], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 5, "seek": 0, "start": 18.44, "end": 21.84, "text": " Now, first, I will tell you who this course is aimed for.", "tokens": [51286, 823, 11, 700, 11, 286, 486, 980, 291, 567, 341, 1164, 307, 20540, 337, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 6, "seek": 0, "start": 22.080000000000002, "end": 25.44, "text": " So this course is aimed for people that are beginners in machine learning", "tokens": [51468, 407, 341, 1164, 307, 20540, 337, 561, 300, 366, 26992, 294, 3479, 2539, 51636], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 7, "seek": 0, "start": 25.44, "end": 28.44, "text": " and artificial intelligence, or maybe have a little bit of understanding", "tokens": [51636, 293, 11677, 7599, 11, 420, 1310, 362, 257, 707, 857, 295, 3701, 51786], "temperature": 0.0, "avg_logprob": -0.1567679246266683, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.14576829969882965}, {"id": 8, "seek": 2844, "start": 28.44, "end": 32.2, "text": " that are trying to get better, but do have a basic fundamental knowledge", "tokens": [50364, 300, 366, 1382, 281, 483, 1101, 11, 457, 360, 362, 257, 3875, 8088, 3601, 50552], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 9, "seek": 2844, "start": 32.2, "end": 34.04, "text": " of programming and Python.", "tokens": [50552, 295, 9410, 293, 15329, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 10, "seek": 2844, "start": 34.32, "end": 36.88, "text": " So this is not a course you're going to take if you haven't done any", "tokens": [50658, 407, 341, 307, 406, 257, 1164, 291, 434, 516, 281, 747, 498, 291, 2378, 380, 1096, 604, 50786], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 11, "seek": 2844, "start": 36.88, "end": 40.28, "text": " programming before, or if you don't know any Python syntax in general.", "tokens": [50786, 9410, 949, 11, 420, 498, 291, 500, 380, 458, 604, 15329, 28431, 294, 2674, 13, 50956], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 12, "seek": 2844, "start": 40.64, "end": 43.84, "text": " It's going to be highly advised that you understand the basic syntax", "tokens": [50974, 467, 311, 516, 281, 312, 5405, 26269, 300, 291, 1223, 264, 3875, 28431, 51134], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 13, "seek": 2844, "start": 43.84, "end": 46.92, "text": " behind Python, as I'm not going to be explaining that throughout this course.", "tokens": [51134, 2261, 15329, 11, 382, 286, 478, 406, 516, 281, 312, 13468, 300, 3710, 341, 1164, 13, 51288], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 14, "seek": 2844, "start": 47.28, "end": 50.32, "text": " Now, in terms of your instructor for this course, that is going to be me.", "tokens": [51306, 823, 11, 294, 2115, 295, 428, 18499, 337, 341, 1164, 11, 300, 307, 516, 281, 312, 385, 13, 51458], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 15, "seek": 2844, "start": 50.36, "end": 51.28, "text": " My name is Tim.", "tokens": [51460, 1222, 1315, 307, 7172, 13, 51506], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 16, "seek": 2844, "start": 51.28, "end": 54.96, "text": " Some of you may know me as Tech with Tim from my YouTube channel, where I teach", "tokens": [51506, 2188, 295, 291, 815, 458, 385, 382, 13795, 365, 7172, 490, 452, 3088, 2269, 11, 689, 286, 2924, 51690], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 17, "seek": 2844, "start": 54.96, "end": 56.88, "text": " all kinds of different programming topics.", "tokens": [51690, 439, 3685, 295, 819, 9410, 8378, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10192642211914063, "compression_ratio": 1.7934131736526946, "no_speech_prob": 0.03672860935330391}, {"id": 18, "seek": 5688, "start": 57.0, "end": 59.72, "text": " And I've actually been working with Free Code Camp and posted some of my", "tokens": [50370, 400, 286, 600, 767, 668, 1364, 365, 11551, 15549, 9189, 293, 9437, 512, 295, 452, 50506], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 19, "seek": 5688, "start": 59.72, "end": 61.480000000000004, "text": " series on their channel as well.", "tokens": [50506, 2638, 322, 641, 2269, 382, 731, 13, 50594], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 20, "seek": 5688, "start": 61.88, "end": 64.72, "text": " Now, let's get into the course breakdown and talk about exactly what you're", "tokens": [50614, 823, 11, 718, 311, 483, 666, 264, 1164, 18188, 293, 751, 466, 2293, 437, 291, 434, 50756], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 21, "seek": 5688, "start": 64.72, "end": 66.88, "text": " going to learn and what you can expect from this course.", "tokens": [50756, 516, 281, 1466, 293, 437, 291, 393, 2066, 490, 341, 1164, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 22, "seek": 5688, "start": 67.12, "end": 69.92, "text": " So as this course is geared towards beginners and people just getting", "tokens": [50876, 407, 382, 341, 1164, 307, 35924, 3030, 26992, 293, 561, 445, 1242, 51016], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 23, "seek": 5688, "start": 69.92, "end": 73.04, "text": " started in the machine learning and AI world, we're going to start by", "tokens": [51016, 1409, 294, 264, 3479, 2539, 293, 7318, 1002, 11, 321, 434, 516, 281, 722, 538, 51172], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 24, "seek": 5688, "start": 73.04, "end": 76.6, "text": " breaking down exactly what machine learning and artificial intelligence is.", "tokens": [51172, 7697, 760, 2293, 437, 3479, 2539, 293, 11677, 7599, 307, 13, 51350], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 25, "seek": 5688, "start": 76.72, "end": 80.04, "text": " So talking about what the differences are between them, the different types", "tokens": [51356, 407, 1417, 466, 437, 264, 7300, 366, 1296, 552, 11, 264, 819, 3467, 51522], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 26, "seek": 5688, "start": 80.04, "end": 83.76, "text": " of machine learning, reinforcement learning, for example, versus neural", "tokens": [51522, 295, 3479, 2539, 11, 29280, 2539, 11, 337, 1365, 11, 5717, 18161, 51708], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 27, "seek": 5688, "start": 83.76, "end": 86.4, "text": " networks versus simple machine learning.", "tokens": [51708, 9590, 5717, 2199, 3479, 2539, 13, 51840], "temperature": 0.0, "avg_logprob": -0.11155701359958513, "compression_ratio": 1.8856304985337244, "no_speech_prob": 0.040820349007844925}, {"id": 28, "seek": 8640, "start": 86.64, "end": 88.48, "text": " We're going to go through all those different differences.", "tokens": [50376, 492, 434, 516, 281, 352, 807, 439, 729, 819, 7300, 13, 50468], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 29, "seek": 8640, "start": 88.68, "end": 91.72, "text": " And then we're going to get into a general introduction of TensorFlow.", "tokens": [50478, 400, 550, 321, 434, 516, 281, 483, 666, 257, 2674, 9339, 295, 37624, 13, 50630], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 30, "seek": 8640, "start": 92.28, "end": 94.80000000000001, "text": " Now, for those of you that don't know, TensorFlow is a module", "tokens": [50658, 823, 11, 337, 729, 295, 291, 300, 500, 380, 458, 11, 37624, 307, 257, 10088, 50784], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 31, "seek": 8640, "start": 94.80000000000001, "end": 98.44000000000001, "text": " developed and maintained by Google, which can be used within Python to do", "tokens": [50784, 4743, 293, 17578, 538, 3329, 11, 597, 393, 312, 1143, 1951, 15329, 281, 360, 50966], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 32, "seek": 8640, "start": 98.44000000000001, "end": 101.4, "text": " a ton of different scientific computing, machine learning and", "tokens": [50966, 257, 2952, 295, 819, 8134, 15866, 11, 3479, 2539, 293, 51114], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 33, "seek": 8640, "start": 101.4, "end": 103.16000000000001, "text": " artificial intelligence applications.", "tokens": [51114, 11677, 7599, 5821, 13, 51202], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 34, "seek": 8640, "start": 103.16000000000001, "end": 105.84, "text": " We're going to be working with that through the entire tutorial series.", "tokens": [51202, 492, 434, 516, 281, 312, 1364, 365, 300, 807, 264, 2302, 7073, 2638, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 35, "seek": 8640, "start": 106.08000000000001, "end": 108.80000000000001, "text": " And after we do that general introduction to TensorFlow, we're going", "tokens": [51348, 400, 934, 321, 360, 300, 2674, 9339, 281, 37624, 11, 321, 434, 516, 51484], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 36, "seek": 8640, "start": 108.80000000000001, "end": 111.04, "text": " to get into our core learning algorithms.", "tokens": [51484, 281, 483, 666, 527, 4965, 2539, 14642, 13, 51596], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 37, "seek": 8640, "start": 111.36000000000001, "end": 114.12, "text": " Now, these are the learning algorithms that you need to know before we can get", "tokens": [51612, 823, 11, 613, 366, 264, 2539, 14642, 300, 291, 643, 281, 458, 949, 321, 393, 483, 51750], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 38, "seek": 8640, "start": 114.12, "end": 115.64000000000001, "text": " further into machine learning.", "tokens": [51750, 3052, 666, 3479, 2539, 13, 51826], "temperature": 0.0, "avg_logprob": -0.11023716024450354, "compression_ratio": 2.01840490797546, "no_speech_prob": 0.001867259736172855}, {"id": 39, "seek": 11564, "start": 115.88, "end": 117.68, "text": " They build a really strong foundation.", "tokens": [50376, 814, 1322, 257, 534, 2068, 7030, 13, 50466], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 40, "seek": 11564, "start": 117.84, "end": 121.28, "text": " They're pretty easy to understand and implement, and they're extremely powerful.", "tokens": [50474, 814, 434, 1238, 1858, 281, 1223, 293, 4445, 11, 293, 436, 434, 4664, 4005, 13, 50646], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 41, "seek": 11564, "start": 121.68, "end": 124.76, "text": " After we do that, we're going to get into neural networks, discuss all the", "tokens": [50666, 2381, 321, 360, 300, 11, 321, 434, 516, 281, 483, 666, 18161, 9590, 11, 2248, 439, 264, 50820], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 42, "seek": 11564, "start": 124.76, "end": 127.92, "text": " different things that go into how neural networks work, how we can use them", "tokens": [50820, 819, 721, 300, 352, 666, 577, 18161, 9590, 589, 11, 577, 321, 393, 764, 552, 50978], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 43, "seek": 11564, "start": 127.92, "end": 129.48, "text": " and then do a bunch of different examples.", "tokens": [50978, 293, 550, 360, 257, 3840, 295, 819, 5110, 13, 51056], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 44, "seek": 11564, "start": 129.76, "end": 132.56, "text": " And then we're going to get into some more complex aspects of machine", "tokens": [51070, 400, 550, 321, 434, 516, 281, 483, 666, 512, 544, 3997, 7270, 295, 3479, 51210], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 45, "seek": 11564, "start": 132.56, "end": 135.84, "text": " learning and artificial intelligence and get to convolutional neural networks,", "tokens": [51210, 2539, 293, 11677, 7599, 293, 483, 281, 45216, 304, 18161, 9590, 11, 51374], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 46, "seek": 11564, "start": 135.84, "end": 138.6, "text": " which can do things like image recognition and detection.", "tokens": [51374, 597, 393, 360, 721, 411, 3256, 11150, 293, 17784, 13, 51512], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 47, "seek": 11564, "start": 138.8, "end": 140.96, "text": " And then we're going to get into recurrent neural networks, which are", "tokens": [51522, 400, 550, 321, 434, 516, 281, 483, 666, 18680, 1753, 18161, 9590, 11, 597, 366, 51630], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 48, "seek": 11564, "start": 140.96, "end": 145.0, "text": " going to do things like natural language processing, chatbots, text", "tokens": [51630, 516, 281, 360, 721, 411, 3303, 2856, 9007, 11, 5081, 65, 1971, 11, 2487, 51832], "temperature": 0.0, "avg_logprob": -0.10365938186645508, "compression_ratio": 2.0371517027863777, "no_speech_prob": 0.018532440066337585}, {"id": 49, "seek": 14500, "start": 145.04, "end": 147.08, "text": " processing, all those different kinds of things.", "tokens": [50366, 9007, 11, 439, 729, 819, 3685, 295, 721, 13, 50468], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 50, "seek": 14500, "start": 147.28, "end": 149.76, "text": " And finally ended off with reinforcement learning.", "tokens": [50478, 400, 2721, 4590, 766, 365, 29280, 2539, 13, 50602], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 51, "seek": 14500, "start": 150.04, "end": 153.32, "text": " Now, in terms of resources for this course, there are a ton.", "tokens": [50616, 823, 11, 294, 2115, 295, 3593, 337, 341, 1164, 11, 456, 366, 257, 2952, 13, 50780], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 52, "seek": 14500, "start": 153.52, "end": 156.72, "text": " And what we're going to be doing to make this really easy for you and for me", "tokens": [50790, 400, 437, 321, 434, 516, 281, 312, 884, 281, 652, 341, 534, 1858, 337, 291, 293, 337, 385, 50950], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 53, "seek": 14500, "start": 156.76, "end": 159.12, "text": " is doing everything through Google Collaboratory.", "tokens": [50952, 307, 884, 1203, 807, 3329, 44483, 4745, 13, 51070], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 54, "seek": 14500, "start": 159.28, "end": 162.2, "text": " Now, if you haven't heard of Google Collaboratory, essentially it's a", "tokens": [51078, 823, 11, 498, 291, 2378, 380, 2198, 295, 3329, 44483, 4745, 11, 4476, 309, 311, 257, 51224], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 55, "seek": 14500, "start": 162.2, "end": 166.72, "text": " collaborative coding environment that runs an iPython notebook in the cloud", "tokens": [51224, 16555, 17720, 2823, 300, 6676, 364, 5180, 88, 11943, 21060, 294, 264, 4588, 51450], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 56, "seek": 14500, "start": 166.96, "end": 170.56, "text": " on a Google machine where you can do all of your machine learning for free.", "tokens": [51462, 322, 257, 3329, 3479, 689, 291, 393, 360, 439, 295, 428, 3479, 2539, 337, 1737, 13, 51642], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 57, "seek": 14500, "start": 170.8, "end": 172.6, "text": " So you don't need to install any packages.", "tokens": [51654, 407, 291, 500, 380, 643, 281, 3625, 604, 17401, 13, 51744], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 58, "seek": 14500, "start": 172.6, "end": 173.76, "text": " You don't need to use PIP.", "tokens": [51744, 509, 500, 380, 643, 281, 764, 430, 9139, 13, 51802], "temperature": 0.0, "avg_logprob": -0.13620330227745903, "compression_ratio": 1.7815384615384615, "no_speech_prob": 0.0027145801577717066}, {"id": 59, "seek": 17376, "start": 173.95999999999998, "end": 175.56, "text": " You don't need to get your environment set up.", "tokens": [50374, 509, 500, 380, 643, 281, 483, 428, 2823, 992, 493, 13, 50454], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 60, "seek": 17376, "start": 175.64, "end": 178.84, "text": " All you need to do is open a new Google Collaboratory window and you can", "tokens": [50458, 1057, 291, 643, 281, 360, 307, 1269, 257, 777, 3329, 44483, 4745, 4910, 293, 291, 393, 50618], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 61, "seek": 17376, "start": 178.84, "end": 179.92, "text": " start writing code.", "tokens": [50618, 722, 3579, 3089, 13, 50672], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 62, "seek": 17376, "start": 180.12, "end": 181.72, "text": " And that's what we're going to be doing in this series.", "tokens": [50682, 400, 300, 311, 437, 321, 434, 516, 281, 312, 884, 294, 341, 2638, 13, 50762], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 63, "seek": 17376, "start": 181.92, "end": 184.88, "text": " If you look in the description right now, you will see links to all of the", "tokens": [50772, 759, 291, 574, 294, 264, 3855, 558, 586, 11, 291, 486, 536, 6123, 281, 439, 295, 264, 50920], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 64, "seek": 17376, "start": 184.88, "end": 187.07999999999998, "text": " notebooks that I use throughout this guide.", "tokens": [50920, 43782, 300, 286, 764, 3710, 341, 5934, 13, 51030], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 65, "seek": 17376, "start": 187.2, "end": 190.12, "text": " So if there's anything that you want to be cleared up, if you want the code", "tokens": [51036, 407, 498, 456, 311, 1340, 300, 291, 528, 281, 312, 19725, 493, 11, 498, 291, 528, 264, 3089, 51182], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 66, "seek": 17376, "start": 190.12, "end": 193.32, "text": " for yourself, if you want just text based descriptions of the things that I'm", "tokens": [51182, 337, 1803, 11, 498, 291, 528, 445, 2487, 2361, 24406, 295, 264, 721, 300, 286, 478, 51342], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 67, "seek": 17376, "start": 193.32, "end": 195.95999999999998, "text": " saying, you can click those links and gain access to them.", "tokens": [51342, 1566, 11, 291, 393, 2052, 729, 6123, 293, 6052, 2105, 281, 552, 13, 51474], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 68, "seek": 17376, "start": 196.16, "end": 198.44, "text": " So with that being said, I'm very excited to get started.", "tokens": [51484, 407, 365, 300, 885, 848, 11, 286, 478, 588, 2919, 281, 483, 1409, 13, 51598], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 69, "seek": 17376, "start": 198.44, "end": 200.0, "text": " I hope you guys are as well.", "tokens": [51598, 286, 1454, 291, 1074, 366, 382, 731, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 70, "seek": 17376, "start": 200.2, "end": 202.79999999999998, "text": " And let's go ahead and get into the content.", "tokens": [51686, 400, 718, 311, 352, 2286, 293, 483, 666, 264, 2701, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11035575866699218, "compression_ratio": 1.8005464480874316, "no_speech_prob": 0.0055518909357488155}, {"id": 71, "seek": 20376, "start": 204.64, "end": 209.76, "text": " So in this first section, I'm going to spend a few minutes discussing the", "tokens": [50408, 407, 294, 341, 700, 3541, 11, 286, 478, 516, 281, 3496, 257, 1326, 2077, 10850, 264, 50664], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 72, "seek": 20376, "start": 209.76, "end": 214.28, "text": " difference between artificial intelligence, neural networks and machine learning.", "tokens": [50664, 2649, 1296, 11677, 7599, 11, 18161, 9590, 293, 3479, 2539, 13, 50890], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 73, "seek": 20376, "start": 214.56, "end": 216.84, "text": " Now, the reason we need to go into this is because we're going to be covering", "tokens": [50904, 823, 11, 264, 1778, 321, 643, 281, 352, 666, 341, 307, 570, 321, 434, 516, 281, 312, 10322, 51018], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 74, "seek": 20376, "start": 216.84, "end": 218.56, "text": " all of these topics throughout this course.", "tokens": [51018, 439, 295, 613, 8378, 3710, 341, 1164, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 75, "seek": 20376, "start": 218.76, "end": 221.76, "text": " So it's vital that you guys understand what these actually mean.", "tokens": [51114, 407, 309, 311, 11707, 300, 291, 1074, 1223, 437, 613, 767, 914, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 76, "seek": 20376, "start": 221.76, "end": 223.72, "text": " And you can kind of differentiate between them.", "tokens": [51264, 400, 291, 393, 733, 295, 23203, 1296, 552, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 77, "seek": 20376, "start": 223.72, "end": 225.23999999999998, "text": " So that's what we're going to focus on now.", "tokens": [51362, 407, 300, 311, 437, 321, 434, 516, 281, 1879, 322, 586, 13, 51438], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 78, "seek": 20376, "start": 225.51999999999998, "end": 228.23999999999998, "text": " Now, quick disclaimer here, just so everyone's aware, I'm using something", "tokens": [51452, 823, 11, 1702, 40896, 510, 11, 445, 370, 1518, 311, 3650, 11, 286, 478, 1228, 746, 51588], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 79, "seek": 20376, "start": 228.23999999999998, "end": 229.44, "text": " called Windows, Inc.", "tokens": [51588, 1219, 8591, 11, 7779, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 80, "seek": 20376, "start": 229.48, "end": 231.2, "text": " This just default comes with Windows.", "tokens": [51650, 639, 445, 7576, 1487, 365, 8591, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 81, "seek": 20376, "start": 231.39999999999998, "end": 232.88, "text": " I have a drawing tabled down here.", "tokens": [51746, 286, 362, 257, 6316, 4421, 1493, 760, 510, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1731310277371793, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0013248056638985872}, {"id": 82, "seek": 23288, "start": 232.88, "end": 235.84, "text": " And this is what I'm going to be using for some of the explanatory parts where", "tokens": [50364, 400, 341, 307, 437, 286, 478, 516, 281, 312, 1228, 337, 512, 295, 264, 9045, 4745, 3166, 689, 50512], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 83, "seek": 23288, "start": 235.84, "end": 240.0, "text": " there's no real coding, just to kind of illustrate some concepts and topics to you.", "tokens": [50512, 456, 311, 572, 957, 17720, 11, 445, 281, 733, 295, 23221, 512, 10392, 293, 8378, 281, 291, 13, 50720], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 84, "seek": 23288, "start": 240.2, "end": 242.2, "text": " Now, I have very horrible handwriting.", "tokens": [50730, 823, 11, 286, 362, 588, 9263, 39179, 13, 50830], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 85, "seek": 23288, "start": 242.24, "end": 243.92, "text": " I'm not artistic whatsoever.", "tokens": [50832, 286, 478, 406, 17090, 17076, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 86, "seek": 23288, "start": 243.92, "end": 249.51999999999998, "text": " Programming is definitely more of my thing than drawing and doing diagrams and stuff.", "tokens": [50916, 8338, 2810, 307, 2138, 544, 295, 452, 551, 813, 6316, 293, 884, 36709, 293, 1507, 13, 51196], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 87, "seek": 23288, "start": 249.76, "end": 250.76, "text": " But I'm going to try my best.", "tokens": [51208, 583, 286, 478, 516, 281, 853, 452, 1151, 13, 51258], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 88, "seek": 23288, "start": 250.76, "end": 254.35999999999999, "text": " And this is just the way that I find I can convey information the best to you guys.", "tokens": [51258, 400, 341, 307, 445, 264, 636, 300, 286, 915, 286, 393, 16965, 1589, 264, 1151, 281, 291, 1074, 13, 51438], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 89, "seek": 23288, "start": 254.8, "end": 259.28, "text": " So anyways, let's get started and discuss the first topic here, which is artificial intelligence.", "tokens": [51460, 407, 13448, 11, 718, 311, 483, 1409, 293, 2248, 264, 700, 4829, 510, 11, 597, 307, 11677, 7599, 13, 51684], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 90, "seek": 23288, "start": 259.64, "end": 262.15999999999997, "text": " Now, artificial intelligence is a huge hype nowadays.", "tokens": [51702, 823, 11, 11677, 7599, 307, 257, 2603, 24144, 13434, 13, 51828], "temperature": 0.0, "avg_logprob": -0.11525739656461702, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.003944691736251116}, {"id": 91, "seek": 26216, "start": 262.44, "end": 265.64000000000004, "text": " And it's funny because a lot of people actually don't know what this means, or", "tokens": [50378, 400, 309, 311, 4074, 570, 257, 688, 295, 561, 767, 500, 380, 458, 437, 341, 1355, 11, 420, 50538], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 92, "seek": 26216, "start": 265.64000000000004, "end": 269.36, "text": " they try to tell people that what they've created is not artificial intelligence,", "tokens": [50538, 436, 853, 281, 980, 561, 300, 437, 436, 600, 2942, 307, 406, 11677, 7599, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 93, "seek": 26216, "start": 269.6, "end": 271.32000000000005, "text": " when in reality, it actually is.", "tokens": [50736, 562, 294, 4103, 11, 309, 767, 307, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 94, "seek": 26216, "start": 271.92, "end": 275.48, "text": " Now, the kind of formal definition of AI, and I'm just going to read it off", "tokens": [50852, 823, 11, 264, 733, 295, 9860, 7123, 295, 7318, 11, 293, 286, 478, 445, 516, 281, 1401, 309, 766, 51030], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 95, "seek": 26216, "start": 275.48, "end": 278.72, "text": " of my slide here to make sure that I'm not messing this up, is the effort", "tokens": [51030, 295, 452, 4137, 510, 281, 652, 988, 300, 286, 478, 406, 23258, 341, 493, 11, 307, 264, 4630, 51192], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 96, "seek": 26216, "start": 278.72, "end": 282.40000000000003, "text": " to automate intellectual tasks normally performed by humans.", "tokens": [51192, 281, 31605, 12576, 9608, 5646, 10332, 538, 6255, 13, 51376], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 97, "seek": 26216, "start": 282.76000000000005, "end": 285.12, "text": " Now, that's a fairly big definition, right?", "tokens": [51394, 823, 11, 300, 311, 257, 6457, 955, 7123, 11, 558, 30, 51512], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 98, "seek": 26216, "start": 285.16, "end": 287.56, "text": " What is considered an intellectual task?", "tokens": [51514, 708, 307, 4888, 364, 12576, 5633, 30, 51634], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 99, "seek": 26216, "start": 287.6, "end": 290.76000000000005, "text": " And, you know, really, that doesn't help us too much.", "tokens": [51636, 400, 11, 291, 458, 11, 534, 11, 300, 1177, 380, 854, 505, 886, 709, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1267488752092634, "compression_ratio": 1.7129337539432177, "no_speech_prob": 0.004467559512704611}, {"id": 100, "seek": 29076, "start": 290.76, "end": 294.12, "text": " So what I'm going to do is bring us back to when AI was created, first created", "tokens": [50364, 407, 437, 286, 478, 516, 281, 360, 307, 1565, 505, 646, 281, 562, 7318, 390, 2942, 11, 700, 2942, 50532], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 101, "seek": 29076, "start": 294.28, "end": 298.2, "text": " to kind of explain to you how AI has evolved and what it really started out being.", "tokens": [50540, 281, 733, 295, 2903, 281, 291, 577, 7318, 575, 14178, 293, 437, 309, 534, 1409, 484, 885, 13, 50736], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 102, "seek": 29076, "start": 298.59999999999997, "end": 302.64, "text": " So back in 1950, there was kind of the question being asked by scientists", "tokens": [50756, 407, 646, 294, 18141, 11, 456, 390, 733, 295, 264, 1168, 885, 2351, 538, 7708, 50958], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 103, "seek": 29076, "start": 302.64, "end": 306.8, "text": " and researchers, can computers think, can we get them to figure things out?", "tokens": [50958, 293, 10309, 11, 393, 10807, 519, 11, 393, 321, 483, 552, 281, 2573, 721, 484, 30, 51166], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 104, "seek": 29076, "start": 306.8, "end": 308.76, "text": " Can we get away from just hard coding?", "tokens": [51166, 1664, 321, 483, 1314, 490, 445, 1152, 17720, 30, 51264], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 105, "seek": 29076, "start": 309.0, "end": 312.8, "text": " And, you know, having like, can we get a computer to think and it do its own thing?", "tokens": [51276, 400, 11, 291, 458, 11, 1419, 411, 11, 393, 321, 483, 257, 3820, 281, 519, 293, 309, 360, 1080, 1065, 551, 30, 51466], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 106, "seek": 29076, "start": 313.44, "end": 315.0, "text": " So that was kind of the question that was asked.", "tokens": [51498, 407, 300, 390, 733, 295, 264, 1168, 300, 390, 2351, 13, 51576], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 107, "seek": 29076, "start": 315.2, "end": 319.0, "text": " And that's when the term artificial intelligence was kind of coined and created.", "tokens": [51586, 400, 300, 311, 562, 264, 1433, 11677, 7599, 390, 733, 295, 45222, 293, 2942, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11685251356004835, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.002631395123898983}, {"id": 108, "seek": 31900, "start": 319.32, "end": 323.4, "text": " Now, back then, AI was simply a predefined set of rules.", "tokens": [50380, 823, 11, 646, 550, 11, 7318, 390, 2935, 257, 659, 37716, 992, 295, 4474, 13, 50584], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 109, "seek": 31900, "start": 323.44, "end": 327.72, "text": " So if you're thinking about an AI for maybe like tic-tac-toe or an AI for chess,", "tokens": [50586, 407, 498, 291, 434, 1953, 466, 364, 7318, 337, 1310, 411, 256, 299, 12, 83, 326, 12, 1353, 68, 420, 364, 7318, 337, 24122, 11, 50800], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 110, "seek": 31900, "start": 327.92, "end": 332.04, "text": " all they would have had back then is predefined rules that humans had come up", "tokens": [50810, 439, 436, 576, 362, 632, 646, 550, 307, 659, 37716, 4474, 300, 6255, 632, 808, 493, 51016], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 111, "seek": 31900, "start": 332.04, "end": 335.92, "text": " with and typed into the computer in code, and the computer would simply execute", "tokens": [51016, 365, 293, 33941, 666, 264, 3820, 294, 3089, 11, 293, 264, 3820, 576, 2935, 14483, 51210], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 112, "seek": 31900, "start": 335.92, "end": 338.2, "text": " those set of rules and follow those instructions.", "tokens": [51210, 729, 992, 295, 4474, 293, 1524, 729, 9415, 13, 51324], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 113, "seek": 31900, "start": 338.44, "end": 341.96, "text": " So there was no deep learning, machine learning, crazy algorithms happening.", "tokens": [51336, 407, 456, 390, 572, 2452, 2539, 11, 3479, 2539, 11, 3219, 14642, 2737, 13, 51512], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 114, "seek": 31900, "start": 342.04, "end": 345.48, "text": " It was simply if you wanted the computer to do something, you would have to tell", "tokens": [51516, 467, 390, 2935, 498, 291, 1415, 264, 3820, 281, 360, 746, 11, 291, 576, 362, 281, 980, 51688], "temperature": 0.0, "avg_logprob": -0.13233390567809578, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.008060726337134838}, {"id": 115, "seek": 34548, "start": 345.48, "end": 349.28000000000003, "text": " it beforehand, say you're in this position and this happens, do this.", "tokens": [50364, 309, 22893, 11, 584, 291, 434, 294, 341, 2535, 293, 341, 2314, 11, 360, 341, 13, 50554], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 116, "seek": 34548, "start": 349.32, "end": 350.64000000000004, "text": " And that's what AI was.", "tokens": [50556, 400, 300, 311, 437, 7318, 390, 13, 50622], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 117, "seek": 34548, "start": 350.88, "end": 355.40000000000003, "text": " And very good AI was simply just a very good set of rules or a ton of different", "tokens": [50634, 400, 588, 665, 7318, 390, 2935, 445, 257, 588, 665, 992, 295, 4474, 420, 257, 2952, 295, 819, 50860], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 118, "seek": 34548, "start": 355.40000000000003, "end": 358.20000000000005, "text": " rules that humans had implemented into some program.", "tokens": [50860, 4474, 300, 6255, 632, 12270, 666, 512, 1461, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 119, "seek": 34548, "start": 358.44, "end": 361.8, "text": " You can have AI programs that are stretching, you know, half a million lines", "tokens": [51012, 509, 393, 362, 7318, 4268, 300, 366, 19632, 11, 291, 458, 11, 1922, 257, 2459, 3876, 51180], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 120, "seek": 34548, "start": 361.8, "end": 365.40000000000003, "text": " of code, just with tons and tons and tons of different rules that have been", "tokens": [51180, 295, 3089, 11, 445, 365, 9131, 293, 9131, 293, 9131, 295, 819, 4474, 300, 362, 668, 51360], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 121, "seek": 34548, "start": 365.40000000000003, "end": 366.96000000000004, "text": " created for that AI.", "tokens": [51360, 2942, 337, 300, 7318, 13, 51438], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 122, "seek": 34548, "start": 367.6, "end": 372.36, "text": " So just be aware that AI does not necessarily mean anything crazy, complex", "tokens": [51470, 407, 445, 312, 3650, 300, 7318, 775, 406, 4725, 914, 1340, 3219, 11, 3997, 51708], "temperature": 0.0, "avg_logprob": -0.11598056504706375, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.02228310890495777}, {"id": 123, "seek": 37236, "start": 372.36, "end": 374.08000000000004, "text": " or super complicated.", "tokens": [50364, 420, 1687, 6179, 13, 50450], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 124, "seek": 37236, "start": 374.08000000000004, "end": 377.76, "text": " But essentially, if you're trying to simulate some intellectual task, like", "tokens": [50450, 583, 4476, 11, 498, 291, 434, 1382, 281, 27817, 512, 12576, 5633, 11, 411, 50634], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 125, "seek": 37236, "start": 377.76, "end": 381.68, "text": " playing a game that a human would do with a computer, that is considered AI.", "tokens": [50634, 2433, 257, 1216, 300, 257, 1952, 576, 360, 365, 257, 3820, 11, 300, 307, 4888, 7318, 13, 50830], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 126, "seek": 37236, "start": 381.84000000000003, "end": 385.68, "text": " So even a very basic artificial intelligence for a tic-tac-toe game", "tokens": [50838, 407, 754, 257, 588, 3875, 11677, 7599, 337, 257, 256, 299, 12, 83, 326, 12, 1353, 68, 1216, 51030], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 127, "seek": 37236, "start": 385.68, "end": 388.36, "text": " where it plays against you, that is still considered AI.", "tokens": [51030, 689, 309, 5749, 1970, 291, 11, 300, 307, 920, 4888, 7318, 13, 51164], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 128, "seek": 37236, "start": 388.40000000000003, "end": 391.36, "text": " And if we think of something like Pac-Man, right, where we have, you know,", "tokens": [51166, 400, 498, 321, 519, 295, 746, 411, 10702, 12, 6652, 11, 558, 11, 689, 321, 362, 11, 291, 458, 11, 51314], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 129, "seek": 37236, "start": 391.36, "end": 395.04, "text": " our little ghost, and this will be my rough sketch of a ghost, and we have our", "tokens": [51314, 527, 707, 8359, 11, 293, 341, 486, 312, 452, 5903, 12325, 295, 257, 8359, 11, 293, 321, 362, 527, 51498], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 130, "seek": 37236, "start": 395.04, "end": 396.72, "text": " Pac-Man guy who will just be this.", "tokens": [51498, 10702, 12, 6652, 2146, 567, 486, 445, 312, 341, 13, 51582], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 131, "seek": 37236, "start": 397.08000000000004, "end": 399.72, "text": " Well, would we consider this ghost AI?", "tokens": [51600, 1042, 11, 576, 321, 1949, 341, 8359, 7318, 30, 51732], "temperature": 0.0, "avg_logprob": -0.118044966584319, "compression_ratio": 1.713355048859935, "no_speech_prob": 0.022283829748630524}, {"id": 132, "seek": 39972, "start": 400.32000000000005, "end": 404.36, "text": " What it does is it attempts to find and kind of simulate how it would get", "tokens": [50394, 708, 309, 775, 307, 309, 15257, 281, 915, 293, 733, 295, 27817, 577, 309, 576, 483, 50596], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 133, "seek": 39972, "start": 404.36, "end": 405.56, "text": " to Pac-Man, right?", "tokens": [50596, 281, 10702, 12, 6652, 11, 558, 30, 50656], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 134, "seek": 39972, "start": 405.76000000000005, "end": 409.12, "text": " And the way this works is just using a very basic path finding algorithm.", "tokens": [50666, 400, 264, 636, 341, 1985, 307, 445, 1228, 257, 588, 3875, 3100, 5006, 9284, 13, 50834], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 135, "seek": 39972, "start": 409.28000000000003, "end": 412.52000000000004, "text": " This is nothing to do with deep learning or machine learning or anything crazy.", "tokens": [50842, 639, 307, 1825, 281, 360, 365, 2452, 2539, 420, 3479, 2539, 420, 1340, 3219, 13, 51004], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 136, "seek": 39972, "start": 412.64000000000004, "end": 415.04, "text": " But this is still considered artificial intelligence.", "tokens": [51010, 583, 341, 307, 920, 4888, 11677, 7599, 13, 51130], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 137, "seek": 39972, "start": 415.16, "end": 418.84000000000003, "text": " The computer is figuring out how it can kind of play and do something", "tokens": [51136, 440, 3820, 307, 15213, 484, 577, 309, 393, 733, 295, 862, 293, 360, 746, 51320], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 138, "seek": 39972, "start": 418.84000000000003, "end": 420.36, "text": " by following an algorithm.", "tokens": [51320, 538, 3480, 364, 9284, 13, 51396], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 139, "seek": 39972, "start": 420.36, "end": 424.20000000000005, "text": " So we don't necessarily need to have anything crazy, stupid, complex to be", "tokens": [51396, 407, 321, 500, 380, 4725, 643, 281, 362, 1340, 3219, 11, 6631, 11, 3997, 281, 312, 51588], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 140, "seek": 39972, "start": 424.20000000000005, "end": 428.72, "text": " considered AI, it simply needs to just be simulating some intellectual human", "tokens": [51588, 4888, 7318, 11, 309, 2935, 2203, 281, 445, 312, 1034, 12162, 512, 12576, 1952, 51814], "temperature": 0.0, "avg_logprob": -0.13514328002929688, "compression_ratio": 1.7318611987381702, "no_speech_prob": 0.0028892867267131805}, {"id": 141, "seek": 42872, "start": 428.72, "end": 432.20000000000005, "text": " behavior. That's kind of the definition of artificial intelligence.", "tokens": [50364, 5223, 13, 663, 311, 733, 295, 264, 7123, 295, 11677, 7599, 13, 50538], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 142, "seek": 42872, "start": 432.72, "end": 436.64000000000004, "text": " Now, obviously today, AI has evolved into a much more complex field where we", "tokens": [50564, 823, 11, 2745, 965, 11, 7318, 575, 14178, 666, 257, 709, 544, 3997, 2519, 689, 321, 50760], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 143, "seek": 42872, "start": 436.64000000000004, "end": 439.28000000000003, "text": " now have machine learning and deep learning and all these other techniques,", "tokens": [50760, 586, 362, 3479, 2539, 293, 2452, 2539, 293, 439, 613, 661, 7512, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 144, "seek": 42872, "start": 439.52000000000004, "end": 440.92, "text": " which is what we're going to talk about now.", "tokens": [50904, 597, 307, 437, 321, 434, 516, 281, 751, 466, 586, 13, 50974], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 145, "seek": 42872, "start": 441.20000000000005, "end": 444.0, "text": " So what I want to start by doing is just drawing a circle here.", "tokens": [50988, 407, 437, 286, 528, 281, 722, 538, 884, 307, 445, 6316, 257, 6329, 510, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 146, "seek": 42872, "start": 444.48, "end": 448.24, "text": " And I want to label this circle and say AI like that.", "tokens": [51152, 400, 286, 528, 281, 7645, 341, 6329, 293, 584, 7318, 411, 300, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 147, "seek": 42872, "start": 448.48, "end": 451.44000000000005, "text": " So this is going to define AI because everything I'm going to put inside of", "tokens": [51352, 407, 341, 307, 516, 281, 6964, 7318, 570, 1203, 286, 478, 516, 281, 829, 1854, 295, 51500], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 148, "seek": 42872, "start": 451.44000000000005, "end": 453.6, "text": " here is considered artificial intelligence.", "tokens": [51500, 510, 307, 4888, 11677, 7599, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 149, "seek": 42872, "start": 454.16, "end": 456.28000000000003, "text": " So now let's get into machine learning.", "tokens": [51636, 407, 586, 718, 311, 483, 666, 3479, 2539, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1251004779611835, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.005729681812226772}, {"id": 150, "seek": 45628, "start": 456.76, "end": 458.91999999999996, "text": " So what I'm going to do is draw another circle inside of here.", "tokens": [50388, 407, 437, 286, 478, 516, 281, 360, 307, 2642, 1071, 6329, 1854, 295, 510, 13, 50496], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 151, "seek": 45628, "start": 459.79999999999995, "end": 463.23999999999995, "text": " And we're going to label this circle ML for machine learning.", "tokens": [50540, 400, 321, 434, 516, 281, 7645, 341, 6329, 21601, 337, 3479, 2539, 13, 50712], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 152, "seek": 45628, "start": 463.59999999999997, "end": 466.35999999999996, "text": " Now notice I put this inside of the artificial intelligence circle.", "tokens": [50730, 823, 3449, 286, 829, 341, 1854, 295, 264, 11677, 7599, 6329, 13, 50868], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 153, "seek": 45628, "start": 466.47999999999996, "end": 470.44, "text": " This is because machine learning is a part of artificial intelligence.", "tokens": [50874, 639, 307, 570, 3479, 2539, 307, 257, 644, 295, 11677, 7599, 13, 51072], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 154, "seek": 45628, "start": 471.0, "end": 472.84, "text": " Now, what is machine learning?", "tokens": [51100, 823, 11, 437, 307, 3479, 2539, 30, 51192], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 155, "seek": 45628, "start": 473.32, "end": 477.88, "text": " Well, what we talked about previously was kind of the idea that AI used to just", "tokens": [51216, 1042, 11, 437, 321, 2825, 466, 8046, 390, 733, 295, 264, 1558, 300, 7318, 1143, 281, 445, 51444], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 156, "seek": 45628, "start": 477.88, "end": 480.03999999999996, "text": " be a predefined set of rules, right?", "tokens": [51444, 312, 257, 659, 37716, 992, 295, 4474, 11, 558, 30, 51552], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 157, "seek": 45628, "start": 480.55999999999995, "end": 484.79999999999995, "text": " Where what would happen is we would feed some data, we would go through the rules", "tokens": [51578, 2305, 437, 576, 1051, 307, 321, 576, 3154, 512, 1412, 11, 321, 576, 352, 807, 264, 4474, 51790], "temperature": 0.0, "avg_logprob": -0.14703466872538418, "compression_ratio": 1.8125, "no_speech_prob": 0.0012841987190768123}, {"id": 158, "seek": 48480, "start": 484.8, "end": 487.0, "text": " by and then analyze the data with the rules.", "tokens": [50364, 538, 293, 550, 12477, 264, 1412, 365, 264, 4474, 13, 50474], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 159, "seek": 48480, "start": 487.0, "end": 489.76, "text": " And then we'd spit out some output, which would be, you know, what we're going to do.", "tokens": [50474, 400, 550, 321, 1116, 22127, 484, 512, 5598, 11, 597, 576, 312, 11, 291, 458, 11, 437, 321, 434, 516, 281, 360, 13, 50612], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 160, "seek": 48480, "start": 490.08, "end": 494.40000000000003, "text": " So in the classic example of chess, say we're in check, well, we pass that board", "tokens": [50628, 407, 294, 264, 7230, 1365, 295, 24122, 11, 584, 321, 434, 294, 1520, 11, 731, 11, 321, 1320, 300, 3150, 50844], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 161, "seek": 48480, "start": 494.40000000000003, "end": 498.08000000000004, "text": " information to the computer, it looks at its sets of rules, it determines we're in", "tokens": [50844, 1589, 281, 264, 3820, 11, 309, 1542, 412, 1080, 6352, 295, 4474, 11, 309, 24799, 321, 434, 294, 51028], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 162, "seek": 48480, "start": 498.08000000000004, "end": 499.88, "text": " check, and then it moves us somewhere else.", "tokens": [51028, 1520, 11, 293, 550, 309, 6067, 505, 4079, 1646, 13, 51118], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 163, "seek": 48480, "start": 500.28000000000003, "end": 502.68, "text": " Now, what is machine learning in contrast to that?", "tokens": [51138, 823, 11, 437, 307, 3479, 2539, 294, 8712, 281, 300, 30, 51258], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 164, "seek": 48480, "start": 503.0, "end": 506.92, "text": " Well, machine learning is kind of the first field that's actually figuring out", "tokens": [51274, 1042, 11, 3479, 2539, 307, 733, 295, 264, 700, 2519, 300, 311, 767, 15213, 484, 51470], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 165, "seek": 48480, "start": 506.92, "end": 508.08000000000004, "text": " the rules for us.", "tokens": [51470, 264, 4474, 337, 505, 13, 51528], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 166, "seek": 48480, "start": 508.40000000000003, "end": 512.28, "text": " So rather than us hard coding the rules into the computer, what machine learning", "tokens": [51544, 407, 2831, 813, 505, 1152, 17720, 264, 4474, 666, 264, 3820, 11, 437, 3479, 2539, 51738], "temperature": 0.0, "avg_logprob": -0.13580060167377497, "compression_ratio": 1.8774834437086092, "no_speech_prob": 0.001244743587449193}, {"id": 167, "seek": 51228, "start": 512.3199999999999, "end": 516.92, "text": " attempts to do is take the data and take what the output should be and figure", "tokens": [50366, 15257, 281, 360, 307, 747, 264, 1412, 293, 747, 437, 264, 5598, 820, 312, 293, 2573, 50596], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 168, "seek": 51228, "start": 516.92, "end": 518.04, "text": " out the rules for us.", "tokens": [50596, 484, 264, 4474, 337, 505, 13, 50652], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 169, "seek": 51228, "start": 518.24, "end": 521.52, "text": " So you'll often hear that, you know, machine learning requires a lot of data", "tokens": [50662, 407, 291, 603, 2049, 1568, 300, 11, 291, 458, 11, 3479, 2539, 7029, 257, 688, 295, 1412, 50826], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 170, "seek": 51228, "start": 521.52, "end": 526.52, "text": " and you need ton of examples and, you know, input data to really train a good", "tokens": [50826, 293, 291, 643, 2952, 295, 5110, 293, 11, 291, 458, 11, 4846, 1412, 281, 534, 3847, 257, 665, 51076], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 171, "seek": 51228, "start": 526.52, "end": 529.88, "text": " model. Well, the reason for that is because the way that machine learning", "tokens": [51076, 2316, 13, 1042, 11, 264, 1778, 337, 300, 307, 570, 264, 636, 300, 3479, 2539, 51244], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 172, "seek": 51228, "start": 529.88, "end": 532.1999999999999, "text": " works is it generates the rules for us.", "tokens": [51244, 1985, 307, 309, 23815, 264, 4474, 337, 505, 13, 51360], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 173, "seek": 51228, "start": 532.4399999999999, "end": 535.88, "text": " We give it some input data, we give it what the output data should be.", "tokens": [51372, 492, 976, 309, 512, 4846, 1412, 11, 321, 976, 309, 437, 264, 5598, 1412, 820, 312, 13, 51544], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 174, "seek": 51228, "start": 536.12, "end": 540.36, "text": " And then it looks at that information and figures out what rules can we generate", "tokens": [51556, 400, 550, 309, 1542, 412, 300, 1589, 293, 9624, 484, 437, 4474, 393, 321, 8460, 51768], "temperature": 0.0, "avg_logprob": -0.09511189060356781, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.017982929944992065}, {"id": 175, "seek": 54036, "start": 540.5600000000001, "end": 544.76, "text": " so that when we look at new data, we can have the best possible output for that.", "tokens": [50374, 370, 300, 562, 321, 574, 412, 777, 1412, 11, 321, 393, 362, 264, 1151, 1944, 5598, 337, 300, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 176, "seek": 54036, "start": 544.96, "end": 548.44, "text": " Now, that's also why a lot of the times machine learning models do not have", "tokens": [50594, 823, 11, 300, 311, 611, 983, 257, 688, 295, 264, 1413, 3479, 2539, 5245, 360, 406, 362, 50768], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 177, "seek": 54036, "start": 548.44, "end": 552.52, "text": " a hundred percent accuracy, which means that they may not necessarily get the", "tokens": [50768, 257, 3262, 3043, 14170, 11, 597, 1355, 300, 436, 815, 406, 4725, 483, 264, 50972], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 178, "seek": 54036, "start": 552.52, "end": 554.4, "text": " correct answer every single time.", "tokens": [50972, 3006, 1867, 633, 2167, 565, 13, 51066], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 179, "seek": 54036, "start": 554.6800000000001, "end": 558.24, "text": " And our goal when we create machine learning models is to raise our accuracy", "tokens": [51080, 400, 527, 3387, 562, 321, 1884, 3479, 2539, 5245, 307, 281, 5300, 527, 14170, 51258], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 180, "seek": 54036, "start": 558.24, "end": 562.32, "text": " as high as possible, which means it's going to make the fewest mistakes possible.", "tokens": [51258, 382, 1090, 382, 1944, 11, 597, 1355, 309, 311, 516, 281, 652, 264, 1326, 377, 8038, 1944, 13, 51462], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 181, "seek": 54036, "start": 562.36, "end": 565.36, "text": " Because just like a human, you know, our machine learning models, which are", "tokens": [51464, 1436, 445, 411, 257, 1952, 11, 291, 458, 11, 527, 3479, 2539, 5245, 11, 597, 366, 51614], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 182, "seek": 54036, "start": 565.36, "end": 568.8000000000001, "text": " trying to simulate, you know, human behavior can make mistakes.", "tokens": [51614, 1382, 281, 27817, 11, 291, 458, 11, 1952, 5223, 393, 652, 8038, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10693557880542896, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.013634881936013699}, {"id": 183, "seek": 56880, "start": 568.92, "end": 571.92, "text": " But to summarize that, essentially, machine learning, the difference", "tokens": [50370, 583, 281, 20858, 300, 11, 4476, 11, 3479, 2539, 11, 264, 2649, 50520], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 184, "seek": 56880, "start": 571.92, "end": 576.5999999999999, "text": " between that and kind of, you know, algorithms and basic artificial intelligence", "tokens": [50520, 1296, 300, 293, 733, 295, 11, 291, 458, 11, 14642, 293, 3875, 11677, 7599, 50754], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 185, "seek": 56880, "start": 576.88, "end": 581.0, "text": " is the fact that rather get that rather than us, the programmer giving it the", "tokens": [50768, 307, 264, 1186, 300, 2831, 483, 300, 2831, 813, 505, 11, 264, 32116, 2902, 309, 264, 50974], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 186, "seek": 56880, "start": 581.0, "end": 583.7199999999999, "text": " rules, it figures out the rules for us.", "tokens": [50974, 4474, 11, 309, 9624, 484, 264, 4474, 337, 505, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 187, "seek": 56880, "start": 583.88, "end": 587.52, "text": " And we might not necessarily know explicitly what those rules are when we", "tokens": [51118, 400, 321, 1062, 406, 4725, 458, 20803, 437, 729, 4474, 366, 562, 321, 51300], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 188, "seek": 56880, "start": 587.52, "end": 589.8399999999999, "text": " look at machine learning and create machine learning models.", "tokens": [51300, 574, 412, 3479, 2539, 293, 1884, 3479, 2539, 5245, 13, 51416], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 189, "seek": 56880, "start": 590.12, "end": 593.64, "text": " But we know that we're giving some input data, we're giving the expected", "tokens": [51430, 583, 321, 458, 300, 321, 434, 2902, 512, 4846, 1412, 11, 321, 434, 2902, 264, 5176, 51606], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 190, "seek": 56880, "start": 593.64, "end": 597.64, "text": " output data, and then it looks at all of that information, does some algorithms,", "tokens": [51606, 5598, 1412, 11, 293, 550, 309, 1542, 412, 439, 295, 300, 1589, 11, 775, 512, 14642, 11, 51806], "temperature": 0.0, "avg_logprob": -0.12030727144271608, "compression_ratio": 1.872053872053872, "no_speech_prob": 0.0023229450453072786}, {"id": 191, "seek": 59764, "start": 597.68, "end": 602.04, "text": " which we'll talk about later on that, and figures out the rules for us so that", "tokens": [50366, 597, 321, 603, 751, 466, 1780, 322, 300, 11, 293, 9624, 484, 264, 4474, 337, 505, 370, 300, 50584], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 192, "seek": 59764, "start": 602.04, "end": 605.52, "text": " later when we give it some input data, and we don't know the output data, it", "tokens": [50584, 1780, 562, 321, 976, 309, 512, 4846, 1412, 11, 293, 321, 500, 380, 458, 264, 5598, 1412, 11, 309, 50758], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 193, "seek": 59764, "start": 605.52, "end": 608.64, "text": " can use those rules that it's figured out from our examples and all that", "tokens": [50758, 393, 764, 729, 4474, 300, 309, 311, 8932, 484, 490, 527, 5110, 293, 439, 300, 50914], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 194, "seek": 59764, "start": 608.64, "end": 611.28, "text": " training data that we gave it to generate some output.", "tokens": [50914, 3097, 1412, 300, 321, 2729, 309, 281, 8460, 512, 5598, 13, 51046], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 195, "seek": 59764, "start": 612.0, "end": 613.56, "text": " Okay, so that's machine learning.", "tokens": [51082, 1033, 11, 370, 300, 311, 3479, 2539, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 196, "seek": 59764, "start": 613.96, "end": 615.8, "text": " Now we've covered AI and machine learning.", "tokens": [51180, 823, 321, 600, 5343, 7318, 293, 3479, 2539, 13, 51272], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 197, "seek": 59764, "start": 616.04, "end": 618.96, "text": " And now it's time to cover neural networks or deep learning.", "tokens": [51284, 400, 586, 309, 311, 565, 281, 2060, 18161, 9590, 420, 2452, 2539, 13, 51430], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 198, "seek": 59764, "start": 619.48, "end": 622.84, "text": " Now this circle gets to go right inside of the machine learning right here.", "tokens": [51456, 823, 341, 6329, 2170, 281, 352, 558, 1854, 295, 264, 3479, 2539, 558, 510, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 199, "seek": 59764, "start": 623.08, "end": 626.4, "text": " I'm just going to label this one NN, which stands for neural networks.", "tokens": [51636, 286, 478, 445, 516, 281, 7645, 341, 472, 426, 45, 11, 597, 7382, 337, 18161, 9590, 13, 51802], "temperature": 0.0, "avg_logprob": -0.10218303285796067, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0005883881822228432}, {"id": 200, "seek": 62640, "start": 626.84, "end": 628.92, "text": " Now neural networks get a big hype.", "tokens": [50386, 823, 18161, 9590, 483, 257, 955, 24144, 13, 50490], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 201, "seek": 62640, "start": 628.9599999999999, "end": 631.64, "text": " They're usually what the first, you know, when you get into machine learning,", "tokens": [50492, 814, 434, 2673, 437, 264, 700, 11, 291, 458, 11, 562, 291, 483, 666, 3479, 2539, 11, 50626], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 202, "seek": 62640, "start": 631.64, "end": 634.48, "text": " you want to learn neural networks, you're kind of like neural networks are", "tokens": [50626, 291, 528, 281, 1466, 18161, 9590, 11, 291, 434, 733, 295, 411, 18161, 9590, 366, 50768], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 203, "seek": 62640, "start": 634.48, "end": 635.92, "text": " cool, they're capable of a lot.", "tokens": [50768, 1627, 11, 436, 434, 8189, 295, 257, 688, 13, 50840], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 204, "seek": 62640, "start": 636.52, "end": 638.16, "text": " But let's discuss what these really are.", "tokens": [50870, 583, 718, 311, 2248, 437, 613, 534, 366, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 205, "seek": 62640, "start": 638.4, "end": 642.4, "text": " So the easiest way to define a neural network is it is a form of machine", "tokens": [50964, 407, 264, 12889, 636, 281, 6964, 257, 18161, 3209, 307, 309, 307, 257, 1254, 295, 3479, 51164], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 206, "seek": 62640, "start": 642.4, "end": 645.92, "text": " learning that uses a layered representation of data.", "tokens": [51164, 2539, 300, 4960, 257, 34666, 10290, 295, 1412, 13, 51340], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 207, "seek": 62640, "start": 646.24, "end": 648.9599999999999, "text": " Now we're not going to really understand this completely right now.", "tokens": [51356, 823, 321, 434, 406, 516, 281, 534, 1223, 341, 2584, 558, 586, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 208, "seek": 62640, "start": 648.9599999999999, "end": 652.52, "text": " But as we get further in that should start to make more sense as a definition.", "tokens": [51492, 583, 382, 321, 483, 3052, 294, 300, 820, 722, 281, 652, 544, 2020, 382, 257, 7123, 13, 51670], "temperature": 0.0, "avg_logprob": -0.12021674966453609, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0012842357391491532}, {"id": 209, "seek": 65252, "start": 653.0, "end": 656.6, "text": " But what I need to kind of illustrate to you is that in the previous example,", "tokens": [50388, 583, 437, 286, 643, 281, 733, 295, 23221, 281, 291, 307, 300, 294, 264, 3894, 1365, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 210, "seek": 65252, "start": 656.6, "end": 659.24, "text": " where we just talked about machine learning, essentially what we had is we", "tokens": [50568, 689, 321, 445, 2825, 466, 3479, 2539, 11, 4476, 437, 321, 632, 307, 321, 50700], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 211, "seek": 65252, "start": 659.24, "end": 661.64, "text": " had some input bubbles, which I'm going to define as these.", "tokens": [50700, 632, 512, 4846, 16295, 11, 597, 286, 478, 516, 281, 6964, 382, 613, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 212, "seek": 65252, "start": 661.96, "end": 664.64, "text": " We had some set of rules that is going to be in between here.", "tokens": [50836, 492, 632, 512, 992, 295, 4474, 300, 307, 516, 281, 312, 294, 1296, 510, 13, 50970], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 213, "seek": 65252, "start": 664.68, "end": 665.84, "text": " And then we had some output.", "tokens": [50972, 400, 550, 321, 632, 512, 5598, 13, 51030], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 214, "seek": 65252, "start": 666.16, "end": 669.28, "text": " And what would happen is we feed this input to this set of rules.", "tokens": [51046, 400, 437, 576, 1051, 307, 321, 3154, 341, 4846, 281, 341, 992, 295, 4474, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 215, "seek": 65252, "start": 670.0799999999999, "end": 671.4, "text": " Something happens in here.", "tokens": [51242, 6595, 2314, 294, 510, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 216, "seek": 65252, "start": 671.56, "end": 672.72, "text": " And then we get some output.", "tokens": [51316, 400, 550, 321, 483, 512, 5598, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 217, "seek": 65252, "start": 673.04, "end": 675.3199999999999, "text": " And then that is what, you know, our program does.", "tokens": [51390, 400, 550, 300, 307, 437, 11, 291, 458, 11, 527, 1461, 775, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 218, "seek": 65252, "start": 675.3199999999999, "end": 676.68, "text": " So that's what we get from the model.", "tokens": [51504, 407, 300, 311, 437, 321, 483, 490, 264, 2316, 13, 51572], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 219, "seek": 65252, "start": 676.72, "end": 678.6, "text": " We pretty much just have two layers.", "tokens": [51574, 492, 1238, 709, 445, 362, 732, 7914, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 220, "seek": 65252, "start": 678.64, "end": 681.56, "text": " We have kind of the input layer, the output layer.", "tokens": [51670, 492, 362, 733, 295, 264, 4846, 4583, 11, 264, 5598, 4583, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1298521193989947, "compression_ratio": 1.9419354838709677, "no_speech_prob": 0.02930680848658085}, {"id": 221, "seek": 68156, "start": 681.76, "end": 684.7199999999999, "text": " And the rules are kind of just what connects those two layers together.", "tokens": [50374, 400, 264, 4474, 366, 733, 295, 445, 437, 16967, 729, 732, 7914, 1214, 13, 50522], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 222, "seek": 68156, "start": 685.28, "end": 690.5999999999999, "text": " Now in neural networks and what we call deep learning, we have more than two layers.", "tokens": [50550, 823, 294, 18161, 9590, 293, 437, 321, 818, 2452, 2539, 11, 321, 362, 544, 813, 732, 7914, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 223, "seek": 68156, "start": 690.5999999999999, "end": 693.56, "text": " Now I'm just trying to erase all this quickly so I can show you that.", "tokens": [50816, 823, 286, 478, 445, 1382, 281, 23525, 439, 341, 2661, 370, 286, 393, 855, 291, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 224, "seek": 68156, "start": 693.88, "end": 696.8399999999999, "text": " So let's say, and I'll draw this one another color, because why not?", "tokens": [50980, 407, 718, 311, 584, 11, 293, 286, 603, 2642, 341, 472, 1071, 2017, 11, 570, 983, 406, 30, 51128], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 225, "seek": 68156, "start": 696.88, "end": 701.0, "text": " If we're talking about neural networks, what we might have, and this will vary.", "tokens": [51130, 759, 321, 434, 1417, 466, 18161, 9590, 11, 437, 321, 1062, 362, 11, 293, 341, 486, 10559, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 226, "seek": 68156, "start": 701.0, "end": 704.52, "text": " And I'll talk about this in a second, is the fact that we have an input layer,", "tokens": [51336, 400, 286, 603, 751, 466, 341, 294, 257, 1150, 11, 307, 264, 1186, 300, 321, 362, 364, 4846, 4583, 11, 51512], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 227, "seek": 68156, "start": 704.52, "end": 706.04, "text": " which will be our first layer of data.", "tokens": [51512, 597, 486, 312, 527, 700, 4583, 295, 1412, 13, 51588], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 228, "seek": 68156, "start": 706.1999999999999, "end": 710.5999999999999, "text": " We could have some layers in between this layer that are all connected together.", "tokens": [51596, 492, 727, 362, 512, 7914, 294, 1296, 341, 4583, 300, 366, 439, 4582, 1214, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11084033680610916, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.002472385298460722}, {"id": 229, "seek": 71060, "start": 711.16, "end": 713.24, "text": " And then we could have some output layer.", "tokens": [50392, 400, 550, 321, 727, 362, 512, 5598, 4583, 13, 50496], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 230, "seek": 71060, "start": 713.52, "end": 718.36, "text": " So essentially, what happens is our data is going to be transformed", "tokens": [50510, 407, 4476, 11, 437, 2314, 307, 527, 1412, 307, 516, 281, 312, 16894, 50752], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 231, "seek": 71060, "start": 718.36, "end": 721.48, "text": " through different layers, and different things are going to happen.", "tokens": [50752, 807, 819, 7914, 11, 293, 819, 721, 366, 516, 281, 1051, 13, 50908], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 232, "seek": 71060, "start": 721.48, "end": 724.6800000000001, "text": " There's going to be different connections between these layers.", "tokens": [50908, 821, 311, 516, 281, 312, 819, 9271, 1296, 613, 7914, 13, 51068], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 233, "seek": 71060, "start": 725.0, "end": 727.08, "text": " And then eventually we'll reach an output.", "tokens": [51084, 400, 550, 4728, 321, 603, 2524, 364, 5598, 13, 51188], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 234, "seek": 71060, "start": 727.28, "end": 731.12, "text": " Now it's very difficult to explain neural networks without going completely in depth.", "tokens": [51198, 823, 309, 311, 588, 2252, 281, 2903, 18161, 9590, 1553, 516, 2584, 294, 7161, 13, 51390], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 235, "seek": 71060, "start": 731.12, "end": 733.16, "text": " So I'll cover a few more notes that I have here.", "tokens": [51390, 407, 286, 603, 2060, 257, 1326, 544, 5570, 300, 286, 362, 510, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 236, "seek": 71060, "start": 733.64, "end": 736.84, "text": " Essentially, in neural networks, we just have multiple layers.", "tokens": [51516, 23596, 11, 294, 18161, 9590, 11, 321, 445, 362, 3866, 7914, 13, 51676], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 237, "seek": 71060, "start": 736.84, "end": 738.32, "text": " That's kind of the way to think of them.", "tokens": [51676, 663, 311, 733, 295, 264, 636, 281, 519, 295, 552, 13, 51750], "temperature": 0.0, "avg_logprob": -0.12401081848144531, "compression_ratio": 1.7789115646258504, "no_speech_prob": 0.0015486297197639942}, {"id": 238, "seek": 73832, "start": 738.6, "end": 742.1600000000001, "text": " And as we see machine learning, you guys should start to understand this more.", "tokens": [50378, 400, 382, 321, 536, 3479, 2539, 11, 291, 1074, 820, 722, 281, 1223, 341, 544, 13, 50556], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 239, "seek": 73832, "start": 742.6400000000001, "end": 745.48, "text": " But just understand that we're dealing with multiple layers.", "tokens": [50580, 583, 445, 1223, 300, 321, 434, 6260, 365, 3866, 7914, 13, 50722], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 240, "seek": 73832, "start": 745.48, "end": 750.44, "text": " And a lot of people actually call this a multi stage information extraction process.", "tokens": [50722, 400, 257, 688, 295, 561, 767, 818, 341, 257, 4825, 3233, 1589, 30197, 1399, 13, 50970], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 241, "seek": 73832, "start": 750.6800000000001, "end": 752.0400000000001, "text": " Now, I did not come up with that term.", "tokens": [50982, 823, 11, 286, 630, 406, 808, 493, 365, 300, 1433, 13, 51050], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 242, "seek": 73832, "start": 752.0400000000001, "end": 753.72, "text": " I think that's from a book or something.", "tokens": [51050, 286, 519, 300, 311, 490, 257, 1446, 420, 746, 13, 51134], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 243, "seek": 73832, "start": 753.72, "end": 757.6, "text": " But essentially what ends up happening is we have our data at this first layer,", "tokens": [51134, 583, 4476, 437, 5314, 493, 2737, 307, 321, 362, 527, 1412, 412, 341, 700, 4583, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 244, "seek": 73832, "start": 757.6, "end": 760.9200000000001, "text": " which is that input information, which we're going to be passing to the model", "tokens": [51328, 597, 307, 300, 4846, 1589, 11, 597, 321, 434, 516, 281, 312, 8437, 281, 264, 2316, 51494], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 245, "seek": 73832, "start": 760.9200000000001, "end": 762.44, "text": " that we're going to do something with.", "tokens": [51494, 300, 321, 434, 516, 281, 360, 746, 365, 13, 51570], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 246, "seek": 73832, "start": 762.44, "end": 765.72, "text": " It then goes to another layer where it will be transformed.", "tokens": [51570, 467, 550, 1709, 281, 1071, 4583, 689, 309, 486, 312, 16894, 13, 51734], "temperature": 0.0, "avg_logprob": -0.10835757676292868, "compression_ratio": 1.8096774193548386, "no_speech_prob": 0.008314204402267933}, {"id": 247, "seek": 76572, "start": 765.72, "end": 769.88, "text": " It will change into something else using a predefined kind of set of", "tokens": [50364, 467, 486, 1319, 666, 746, 1646, 1228, 257, 659, 37716, 733, 295, 992, 295, 50572], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 248, "seek": 76572, "start": 770.52, "end": 772.6800000000001, "text": " rules and weights that we'll talk about later.", "tokens": [50604, 4474, 293, 17443, 300, 321, 603, 751, 466, 1780, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 249, "seek": 76572, "start": 773.08, "end": 776.12, "text": " Then it will pass through all of these different layers where different", "tokens": [50732, 1396, 309, 486, 1320, 807, 439, 295, 613, 819, 7914, 689, 819, 50884], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 250, "seek": 76572, "start": 776.12, "end": 779.28, "text": " kind of features of the data, which again, we'll discuss in a second,", "tokens": [50884, 733, 295, 4122, 295, 264, 1412, 11, 597, 797, 11, 321, 603, 2248, 294, 257, 1150, 11, 51042], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 251, "seek": 76572, "start": 779.6, "end": 783.5600000000001, "text": " will be extracted, will be figured out, will be found until eventually", "tokens": [51058, 486, 312, 34086, 11, 486, 312, 8932, 484, 11, 486, 312, 1352, 1826, 4728, 51256], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 252, "seek": 76572, "start": 783.5600000000001, "end": 786.5600000000001, "text": " we reach an output layer where we can kind of combine everything", "tokens": [51256, 321, 2524, 364, 5598, 4583, 689, 321, 393, 733, 295, 10432, 1203, 51406], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 253, "seek": 76572, "start": 786.5600000000001, "end": 791.0400000000001, "text": " we've discovered about the data into some kind of output that's meaningful to our program.", "tokens": [51406, 321, 600, 6941, 466, 264, 1412, 666, 512, 733, 295, 5598, 300, 311, 10995, 281, 527, 1461, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 254, "seek": 76572, "start": 791.48, "end": 794.44, "text": " So that's kind of the best that I can do to explain neural networks", "tokens": [51652, 407, 300, 311, 733, 295, 264, 1151, 300, 286, 393, 360, 281, 2903, 18161, 9590, 51800], "temperature": 0.0, "avg_logprob": -0.1026546258192796, "compression_ratio": 1.84, "no_speech_prob": 0.0060970596969127655}, {"id": 255, "seek": 79444, "start": 794.44, "end": 796.1600000000001, "text": " without going on to a deeper level.", "tokens": [50364, 1553, 516, 322, 281, 257, 7731, 1496, 13, 50450], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 256, "seek": 79444, "start": 796.1600000000001, "end": 799.2, "text": " I understand that a lot of you probably don't understand what they are right now.", "tokens": [50450, 286, 1223, 300, 257, 688, 295, 291, 1391, 500, 380, 1223, 437, 436, 366, 558, 586, 13, 50602], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 257, "seek": 79444, "start": 799.2, "end": 800.48, "text": " And that's totally fine.", "tokens": [50602, 400, 300, 311, 3879, 2489, 13, 50666], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 258, "seek": 79444, "start": 800.48, "end": 803.5200000000001, "text": " But just know that there are layered representation of data.", "tokens": [50666, 583, 445, 458, 300, 456, 366, 34666, 10290, 295, 1412, 13, 50818], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 259, "seek": 79444, "start": 803.5600000000001, "end": 807.84, "text": " We have multiple layers of information, whereas in standard machine learning,", "tokens": [50820, 492, 362, 3866, 7914, 295, 1589, 11, 9735, 294, 3832, 3479, 2539, 11, 51034], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 260, "seek": 79444, "start": 807.84, "end": 812.0, "text": " we only have, you know, one or two layers and an artificial intelligence.", "tokens": [51034, 321, 787, 362, 11, 291, 458, 11, 472, 420, 732, 7914, 293, 364, 11677, 7599, 13, 51242], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 261, "seek": 79444, "start": 812.0, "end": 816.4000000000001, "text": " In general, we don't necessarily have to have like a predefined set of layers.", "tokens": [51242, 682, 2674, 11, 321, 500, 380, 4725, 362, 281, 362, 411, 257, 659, 37716, 992, 295, 7914, 13, 51462], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 262, "seek": 79444, "start": 817.08, "end": 820.32, "text": " OK, so that is pretty much it for neural networks.", "tokens": [51496, 2264, 11, 370, 300, 307, 1238, 709, 309, 337, 18161, 9590, 13, 51658], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 263, "seek": 79444, "start": 820.32, "end": 823.48, "text": " There's one last thing I will say about them is that they're actually not", "tokens": [51658, 821, 311, 472, 1036, 551, 286, 486, 584, 466, 552, 307, 300, 436, 434, 767, 406, 51816], "temperature": 0.0, "avg_logprob": -0.13881143166200957, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.004609235096722841}, {"id": 264, "seek": 82348, "start": 823.48, "end": 824.9200000000001, "text": " modeled after the brain.", "tokens": [50364, 37140, 934, 264, 3567, 13, 50436], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 265, "seek": 82348, "start": 824.9200000000001, "end": 828.4, "text": " So a lot of people seem to think that neural networks are modeled after the brain", "tokens": [50436, 407, 257, 688, 295, 561, 1643, 281, 519, 300, 18161, 9590, 366, 37140, 934, 264, 3567, 50610], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 266, "seek": 82348, "start": 828.4, "end": 832.88, "text": " and the fact that you have neurons firing in your brain, and that can relate to neural networks.", "tokens": [50610, 293, 264, 1186, 300, 291, 362, 22027, 16045, 294, 428, 3567, 11, 293, 300, 393, 10961, 281, 18161, 9590, 13, 50834], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 267, "seek": 82348, "start": 832.96, "end": 837.0, "text": " Now, there is a biological inspiration for the name neural networks", "tokens": [50838, 823, 11, 456, 307, 257, 13910, 10249, 337, 264, 1315, 18161, 9590, 51040], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 268, "seek": 82348, "start": 837.0, "end": 840.96, "text": " in the way that they work from, you know, human biology, but it is not", "tokens": [51040, 294, 264, 636, 300, 436, 589, 490, 11, 291, 458, 11, 1952, 14956, 11, 457, 309, 307, 406, 51238], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 269, "seek": 82348, "start": 840.96, "end": 843.32, "text": " necessarily modeled about the way that our brain works.", "tokens": [51238, 4725, 37140, 466, 264, 636, 300, 527, 3567, 1985, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 270, "seek": 82348, "start": 843.32, "end": 847.16, "text": " And in fact, we actually don't really know how a lot of the things in our brain", "tokens": [51356, 400, 294, 1186, 11, 321, 767, 500, 380, 534, 458, 577, 257, 688, 295, 264, 721, 294, 527, 3567, 51548], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 271, "seek": 82348, "start": 847.16, "end": 848.12, "text": " operate and work.", "tokens": [51548, 9651, 293, 589, 13, 51596], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 272, "seek": 82348, "start": 848.12, "end": 851.04, "text": " So it would be impossible for us to say that neural networks are modeled", "tokens": [51596, 407, 309, 576, 312, 6243, 337, 505, 281, 584, 300, 18161, 9590, 366, 37140, 51742], "temperature": 0.0, "avg_logprob": -0.10530747538027556, "compression_ratio": 2.0035211267605635, "no_speech_prob": 0.021606845781207085}, {"id": 273, "seek": 85104, "start": 851.04, "end": 855.16, "text": " after the brain, because we actually don't know how information is kind of", "tokens": [50364, 934, 264, 3567, 11, 570, 321, 767, 500, 380, 458, 577, 1589, 307, 733, 295, 50570], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 274, "seek": 85104, "start": 855.16, "end": 858.88, "text": " happens and occurs and transfers through our brain, or at least we don't know", "tokens": [50570, 2314, 293, 11843, 293, 29137, 807, 527, 3567, 11, 420, 412, 1935, 321, 500, 380, 458, 50756], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 275, "seek": 85104, "start": 858.88, "end": 862.28, "text": " enough to be able to say this is exactly what it is a neural network.", "tokens": [50756, 1547, 281, 312, 1075, 281, 584, 341, 307, 2293, 437, 309, 307, 257, 18161, 3209, 13, 50926], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 276, "seek": 85104, "start": 862.28, "end": 864.52, "text": " So anyways, that was kind of the last point there.", "tokens": [50926, 407, 13448, 11, 300, 390, 733, 295, 264, 1036, 935, 456, 13, 51038], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 277, "seek": 85104, "start": 864.52, "end": 866.92, "text": " OK, so now we need to talk about data.", "tokens": [51038, 2264, 11, 370, 586, 321, 643, 281, 751, 466, 1412, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 278, "seek": 85104, "start": 866.92, "end": 870.52, "text": " Now, data is the most important part of machine learning and artificial", "tokens": [51158, 823, 11, 1412, 307, 264, 881, 1021, 644, 295, 3479, 2539, 293, 11677, 51338], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 279, "seek": 85104, "start": 870.52, "end": 872.4, "text": " intelligence neural networks as well.", "tokens": [51338, 7599, 18161, 9590, 382, 731, 13, 51432], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 280, "seek": 85104, "start": 872.4, "end": 876.9599999999999, "text": " And it's very important that we understand how important data is and what", "tokens": [51432, 400, 309, 311, 588, 1021, 300, 321, 1223, 577, 1021, 1412, 307, 293, 437, 51660], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 281, "seek": 85104, "start": 876.9599999999999, "end": 879.8, "text": " the different kind of parts of it are, because they're going to be referenced", "tokens": [51660, 264, 819, 733, 295, 3166, 295, 309, 366, 11, 570, 436, 434, 516, 281, 312, 32734, 51802], "temperature": 0.0, "avg_logprob": -0.10312423431616036, "compression_ratio": 1.79375, "no_speech_prob": 0.003945052623748779}, {"id": 282, "seek": 87980, "start": 879.8, "end": 882.4799999999999, "text": " a lot in any of the resources that we're using.", "tokens": [50364, 257, 688, 294, 604, 295, 264, 3593, 300, 321, 434, 1228, 13, 50498], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 283, "seek": 87980, "start": 882.4799999999999, "end": 885.3199999999999, "text": " Now, what I want to do is just create an example here where I'm going to make a", "tokens": [50498, 823, 11, 437, 286, 528, 281, 360, 307, 445, 1884, 364, 1365, 510, 689, 286, 478, 516, 281, 652, 257, 50640], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 284, "seek": 87980, "start": 885.3199999999999, "end": 890.5999999999999, "text": " data set that is about students final grades in like a school system.", "tokens": [50640, 1412, 992, 300, 307, 466, 1731, 2572, 18041, 294, 411, 257, 1395, 1185, 13, 50904], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 285, "seek": 87980, "start": 890.5999999999999, "end": 893.4799999999999, "text": " So essentially, we're going to make this a very easy example where all we're", "tokens": [50904, 407, 4476, 11, 321, 434, 516, 281, 652, 341, 257, 588, 1858, 1365, 689, 439, 321, 434, 51048], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 286, "seek": 87980, "start": 893.4799999999999, "end": 896.9599999999999, "text": " going to have for this data set is we're going to have information about students.", "tokens": [51048, 516, 281, 362, 337, 341, 1412, 992, 307, 321, 434, 516, 281, 362, 1589, 466, 1731, 13, 51222], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 287, "seek": 87980, "start": 896.9599999999999, "end": 901.3599999999999, "text": " So we're going to have their midterm one grade, their midterm two grade, and then", "tokens": [51222, 407, 321, 434, 516, 281, 362, 641, 2062, 7039, 472, 7204, 11, 641, 2062, 7039, 732, 7204, 11, 293, 550, 51442], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 288, "seek": 87980, "start": 901.3599999999999, "end": 902.76, "text": " we're going to have their final grade.", "tokens": [51442, 321, 434, 516, 281, 362, 641, 2572, 7204, 13, 51512], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 289, "seek": 87980, "start": 902.76, "end": 906.92, "text": " So I'm just going to say midterm one.", "tokens": [51512, 407, 286, 478, 445, 516, 281, 584, 2062, 7039, 472, 13, 51720], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 290, "seek": 87980, "start": 906.92, "end": 908.7199999999999, "text": " And again, excuse my handwriting here.", "tokens": [51720, 400, 797, 11, 8960, 452, 39179, 510, 13, 51810], "temperature": 0.0, "avg_logprob": -0.12153229842314849, "compression_ratio": 2.0555555555555554, "no_speech_prob": 0.019120942801237106}, {"id": 291, "seek": 90872, "start": 908.76, "end": 911.6800000000001, "text": " It's not the easiest thing to write with this drawing tablet.", "tokens": [50366, 467, 311, 406, 264, 12889, 551, 281, 2464, 365, 341, 6316, 14136, 13, 50512], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 292, "seek": 90872, "start": 911.6800000000001, "end": 913.9200000000001, "text": " And then I'll just do final.", "tokens": [50512, 400, 550, 286, 603, 445, 360, 2572, 13, 50624], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 293, "seek": 90872, "start": 913.9200000000001, "end": 915.5600000000001, "text": " So this is going to be our data set.", "tokens": [50624, 407, 341, 307, 516, 281, 312, 527, 1412, 992, 13, 50706], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 294, "seek": 90872, "start": 915.5600000000001, "end": 919.12, "text": " And we'll actually see some similar data sets to this as we go through and do", "tokens": [50706, 400, 321, 603, 767, 536, 512, 2531, 1412, 6352, 281, 341, 382, 321, 352, 807, 293, 360, 50884], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 295, "seek": 90872, "start": 919.12, "end": 920.76, "text": " some examples later on.", "tokens": [50884, 512, 5110, 1780, 322, 13, 50966], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 296, "seek": 90872, "start": 920.76, "end": 925.08, "text": " So for student one, which we'll just put some students here, we're going to have", "tokens": [50966, 407, 337, 3107, 472, 11, 597, 321, 603, 445, 829, 512, 1731, 510, 11, 321, 434, 516, 281, 362, 51182], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 297, "seek": 90872, "start": 925.08, "end": 928.8000000000001, "text": " their midterm one grade, maybe that's a 70, their midterm two grade, maybe that", "tokens": [51182, 641, 2062, 7039, 472, 7204, 11, 1310, 300, 311, 257, 5285, 11, 641, 2062, 7039, 732, 7204, 11, 1310, 300, 51368], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 298, "seek": 90872, "start": 928.8000000000001, "end": 929.44, "text": " was an 80.", "tokens": [51368, 390, 364, 4688, 13, 51400], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 299, "seek": 90872, "start": 929.44, "end": 933.76, "text": " And then let's say their final was like their final term grade, not just the", "tokens": [51400, 400, 550, 718, 311, 584, 641, 2572, 390, 411, 641, 2572, 1433, 7204, 11, 406, 445, 264, 51616], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 300, "seek": 90872, "start": 933.76, "end": 935.32, "text": " mark on the final exam.", "tokens": [51616, 1491, 322, 264, 2572, 1139, 13, 51694], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 301, "seek": 90872, "start": 935.32, "end": 937.48, "text": " Let's give them a 77.", "tokens": [51694, 961, 311, 976, 552, 257, 25546, 13, 51802], "temperature": 0.0, "avg_logprob": -0.13906932830810548, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.0009109904058277607}, {"id": 302, "seek": 93748, "start": 937.52, "end": 939.72, "text": " Now, for midterm one, we can give someone a 60.", "tokens": [50366, 823, 11, 337, 2062, 7039, 472, 11, 321, 393, 976, 1580, 257, 4060, 13, 50476], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 303, "seek": 93748, "start": 939.72, "end": 941.12, "text": " Maybe we give them a 90.", "tokens": [50476, 2704, 321, 976, 552, 257, 4289, 13, 50546], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 304, "seek": 93748, "start": 941.12, "end": 945.84, "text": " And then we determined that the final grade on their exam was let's say an 84.", "tokens": [50546, 400, 550, 321, 9540, 300, 264, 2572, 7204, 322, 641, 1139, 390, 718, 311, 584, 364, 29018, 13, 50782], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 305, "seek": 93748, "start": 946.2, "end": 948.44, "text": " And then we could do something with maybe a lower grade here.", "tokens": [50800, 400, 550, 321, 727, 360, 746, 365, 1310, 257, 3126, 7204, 510, 13, 50912], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 306, "seek": 93748, "start": 948.44, "end": 954.5600000000001, "text": " So 40, 50, and then maybe they got a 38 or something in the final grade.", "tokens": [50912, 407, 3356, 11, 2625, 11, 293, 550, 1310, 436, 658, 257, 12843, 420, 746, 294, 264, 2572, 7204, 13, 51218], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 307, "seek": 93748, "start": 954.88, "end": 958.88, "text": " Now, obviously, we could have some other information here that we're omitting.", "tokens": [51234, 823, 11, 2745, 11, 321, 727, 362, 512, 661, 1589, 510, 300, 321, 434, 3406, 2414, 13, 51434], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 308, "seek": 93748, "start": 958.88, "end": 961.88, "text": " Like maybe there was some exam, some assignments, whatever, some other things", "tokens": [51434, 1743, 1310, 456, 390, 512, 1139, 11, 512, 22546, 11, 2035, 11, 512, 661, 721, 51584], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 309, "seek": 93748, "start": 961.88, "end": 963.5600000000001, "text": " they did that contributed to their grade.", "tokens": [51584, 436, 630, 300, 18434, 281, 641, 7204, 13, 51668], "temperature": 0.0, "avg_logprob": -0.17658381499061288, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0008295389707200229}, {"id": 310, "seek": 96356, "start": 963.8, "end": 967.8399999999999, "text": " But the problem that I want to consider here is the fact that given our midterm", "tokens": [50376, 583, 264, 1154, 300, 286, 528, 281, 1949, 510, 307, 264, 1186, 300, 2212, 527, 2062, 7039, 50578], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 311, "seek": 96356, "start": 967.8399999999999, "end": 971.4, "text": " one grade and our midterm two grade and our final grade, how can I use this", "tokens": [50578, 472, 7204, 293, 527, 2062, 7039, 732, 7204, 293, 527, 2572, 7204, 11, 577, 393, 286, 764, 341, 50756], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 312, "seek": 96356, "start": 971.4, "end": 974.5999999999999, "text": " information to predict any one of these three columns?", "tokens": [50756, 1589, 281, 6069, 604, 472, 295, 613, 1045, 13766, 30, 50916], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 313, "seek": 96356, "start": 974.8399999999999, "end": 979.0799999999999, "text": " So if I were given a student's midterm one grade, and I were given a student's", "tokens": [50928, 407, 498, 286, 645, 2212, 257, 3107, 311, 2062, 7039, 472, 7204, 11, 293, 286, 645, 2212, 257, 3107, 311, 51140], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 314, "seek": 96356, "start": 979.16, "end": 982.16, "text": " final grade, how could I predict their midterm two grade?", "tokens": [51144, 2572, 7204, 11, 577, 727, 286, 6069, 641, 2062, 7039, 732, 7204, 30, 51294], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 315, "seek": 96356, "start": 983.1199999999999, "end": 986.56, "text": " So this is where we're going to talk about features and labels.", "tokens": [51342, 407, 341, 307, 689, 321, 434, 516, 281, 751, 466, 4122, 293, 16949, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 316, "seek": 96356, "start": 986.76, "end": 990.68, "text": " Now, whatever information we have, that is the input information, which is the", "tokens": [51524, 823, 11, 2035, 1589, 321, 362, 11, 300, 307, 264, 4846, 1589, 11, 597, 307, 264, 51720], "temperature": 0.0, "avg_logprob": -0.10687028977178759, "compression_ratio": 1.9838056680161944, "no_speech_prob": 0.05663785710930824}, {"id": 317, "seek": 99068, "start": 990.68, "end": 994.04, "text": " information we will always have that we need to give to the model to get", "tokens": [50364, 1589, 321, 486, 1009, 362, 300, 321, 643, 281, 976, 281, 264, 2316, 281, 483, 50532], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 318, "seek": 99068, "start": 994.04, "end": 996.3199999999999, "text": " some output is what we call our features.", "tokens": [50532, 512, 5598, 307, 437, 321, 818, 527, 4122, 13, 50646], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 319, "seek": 99068, "start": 996.4799999999999, "end": 999.9599999999999, "text": " So in the example where we're trying to predict midterm two, and let's just do", "tokens": [50654, 407, 294, 264, 1365, 689, 321, 434, 1382, 281, 6069, 2062, 7039, 732, 11, 293, 718, 311, 445, 360, 50828], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 320, "seek": 99068, "start": 999.9599999999999, "end": 1001.8, "text": " this and highlight this in red.", "tokens": [50828, 341, 293, 5078, 341, 294, 2182, 13, 50920], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 321, "seek": 99068, "start": 1001.8, "end": 1006.5999999999999, "text": " So we understand what we would have as our features, our input information", "tokens": [50920, 407, 321, 1223, 437, 321, 576, 362, 382, 527, 4122, 11, 527, 4846, 1589, 51160], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 322, "seek": 99068, "start": 1006.5999999999999, "end": 1010.4399999999999, "text": " are going to be midterm one and final, because this is the information", "tokens": [51160, 366, 516, 281, 312, 2062, 7039, 472, 293, 2572, 11, 570, 341, 307, 264, 1589, 51352], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 323, "seek": 99068, "start": 1010.4399999999999, "end": 1012.92, "text": " we are going to use to predict something.", "tokens": [51352, 321, 366, 516, 281, 764, 281, 6069, 746, 13, 51476], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 324, "seek": 99068, "start": 1012.9599999999999, "end": 1015.8, "text": " It is the input, it is what we need to give the model.", "tokens": [51478, 467, 307, 264, 4846, 11, 309, 307, 437, 321, 643, 281, 976, 264, 2316, 13, 51620], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 325, "seek": 99068, "start": 1015.9599999999999, "end": 1019.4399999999999, "text": " And if we're training a model to look at midterm one and final grade, whenever", "tokens": [51628, 400, 498, 321, 434, 3097, 257, 2316, 281, 574, 412, 2062, 7039, 472, 293, 2572, 7204, 11, 5699, 51802], "temperature": 0.0, "avg_logprob": -0.11260215948659477, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.006289233453571796}, {"id": 326, "seek": 101944, "start": 1019.44, "end": 1022.8000000000001, "text": " we want to make a new prediction, we need to have that information to do so.", "tokens": [50364, 321, 528, 281, 652, 257, 777, 17630, 11, 321, 643, 281, 362, 300, 1589, 281, 360, 370, 13, 50532], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 327, "seek": 101944, "start": 1023.1600000000001, "end": 1024.72, "text": " Now, what's highlighted in red.", "tokens": [50550, 823, 11, 437, 311, 17173, 294, 2182, 13, 50628], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 328, "seek": 101944, "start": 1024.72, "end": 1029.3200000000002, "text": " So this midterm two here is what we would call the label or the output.", "tokens": [50628, 407, 341, 2062, 7039, 732, 510, 307, 437, 321, 576, 818, 264, 7645, 420, 264, 5598, 13, 50858], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 329, "seek": 101944, "start": 1029.6000000000001, "end": 1033.8, "text": " Now, the label is simply what we are trying to look for or predict.", "tokens": [50872, 823, 11, 264, 7645, 307, 2935, 437, 321, 366, 1382, 281, 574, 337, 420, 6069, 13, 51082], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 330, "seek": 101944, "start": 1033.96, "end": 1038.2, "text": " So when we talk about features versus labels, features is our input information,", "tokens": [51090, 407, 562, 321, 751, 466, 4122, 5717, 16949, 11, 4122, 307, 527, 4846, 1589, 11, 51302], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 331, "seek": 101944, "start": 1038.24, "end": 1041.48, "text": " the information that we have that we need to use to make a prediction.", "tokens": [51304, 264, 1589, 300, 321, 362, 300, 321, 643, 281, 764, 281, 652, 257, 17630, 13, 51466], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 332, "seek": 101944, "start": 1041.76, "end": 1045.24, "text": " And our label is that output information that is just representing, you know,", "tokens": [51480, 400, 527, 7645, 307, 300, 5598, 1589, 300, 307, 445, 13460, 11, 291, 458, 11, 51654], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 333, "seek": 101944, "start": 1045.24, "end": 1046.3600000000001, "text": " what we're looking for.", "tokens": [51654, 437, 321, 434, 1237, 337, 13, 51710], "temperature": 0.0, "avg_logprob": -0.11947335302829742, "compression_ratio": 1.9307692307692308, "no_speech_prob": 0.0013249363983049989}, {"id": 334, "seek": 104636, "start": 1046.6, "end": 1051.1599999999999, "text": " So when we feed our features to a model, it will give to us a label.", "tokens": [50376, 407, 562, 321, 3154, 527, 4122, 281, 257, 2316, 11, 309, 486, 976, 281, 505, 257, 7645, 13, 50604], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 335, "seek": 104636, "start": 1051.28, "end": 1053.24, "text": " And that is kind of the point that we need to understand.", "tokens": [50610, 400, 300, 307, 733, 295, 264, 935, 300, 321, 643, 281, 1223, 13, 50708], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 336, "seek": 104636, "start": 1053.52, "end": 1054.76, "text": " So that was the basic here.", "tokens": [50722, 407, 300, 390, 264, 3875, 510, 13, 50784], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 337, "seek": 104636, "start": 1055.3999999999999, "end": 1058.24, "text": " And now I'm just going to talk a little bit more about data, because we will", "tokens": [50816, 400, 586, 286, 478, 445, 516, 281, 751, 257, 707, 857, 544, 466, 1412, 11, 570, 321, 486, 50958], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 338, "seek": 104636, "start": 1058.24, "end": 1061.7199999999998, "text": " get into this more as we continue going and about the importance of it.", "tokens": [50958, 483, 666, 341, 544, 382, 321, 2354, 516, 293, 466, 264, 7379, 295, 309, 13, 51132], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 339, "seek": 104636, "start": 1062.0, "end": 1066.12, "text": " So the reason why data is so important is this is kind of the key thing", "tokens": [51146, 407, 264, 1778, 983, 1412, 307, 370, 1021, 307, 341, 307, 733, 295, 264, 2141, 551, 51352], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 340, "seek": 104636, "start": 1066.12, "end": 1068.12, "text": " that we use to create models.", "tokens": [51352, 300, 321, 764, 281, 1884, 5245, 13, 51452], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 341, "seek": 104636, "start": 1068.32, "end": 1072.32, "text": " So whenever we're doing AI and machine learning, we need data pretty much.", "tokens": [51462, 407, 5699, 321, 434, 884, 7318, 293, 3479, 2539, 11, 321, 643, 1412, 1238, 709, 13, 51662], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 342, "seek": 104636, "start": 1072.32, "end": 1075.6, "text": " Unless you're doing a very specific type of machine learning and artificial", "tokens": [51662, 16581, 291, 434, 884, 257, 588, 2685, 2010, 295, 3479, 2539, 293, 11677, 51826], "temperature": 0.0, "avg_logprob": -0.13233155470628005, "compression_ratio": 1.8410596026490067, "no_speech_prob": 0.0021825097501277924}, {"id": 343, "seek": 107560, "start": 1075.6399999999999, "end": 1077.24, "text": " intelligence, which we'll talk about later.", "tokens": [50366, 7599, 11, 597, 321, 603, 751, 466, 1780, 13, 50446], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 344, "seek": 107560, "start": 1077.76, "end": 1080.24, "text": " Now, for most of these models, we need tons of different data.", "tokens": [50472, 823, 11, 337, 881, 295, 613, 5245, 11, 321, 643, 9131, 295, 819, 1412, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 345, "seek": 107560, "start": 1080.28, "end": 1081.9599999999998, "text": " We need tons of different examples.", "tokens": [50598, 492, 643, 9131, 295, 819, 5110, 13, 50682], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 346, "seek": 107560, "start": 1082.1599999999999, "end": 1085.56, "text": " And that's because we know how machine learning works now, which is essentially", "tokens": [50692, 400, 300, 311, 570, 321, 458, 577, 3479, 2539, 1985, 586, 11, 597, 307, 4476, 50862], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 347, "seek": 107560, "start": 1085.84, "end": 1088.9599999999998, "text": " we're trying to come up with rules for a data set.", "tokens": [50876, 321, 434, 1382, 281, 808, 493, 365, 4474, 337, 257, 1412, 992, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 348, "seek": 107560, "start": 1089.1599999999999, "end": 1090.36, "text": " We have some input information.", "tokens": [51042, 492, 362, 512, 4846, 1589, 13, 51102], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 349, "seek": 107560, "start": 1090.36, "end": 1093.3999999999999, "text": " We have some output information or some features and some labels.", "tokens": [51102, 492, 362, 512, 5598, 1589, 420, 512, 4122, 293, 512, 16949, 13, 51254], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 350, "seek": 107560, "start": 1093.6399999999999, "end": 1096.56, "text": " We can give that to a model and tell it to start training.", "tokens": [51266, 492, 393, 976, 300, 281, 257, 2316, 293, 980, 309, 281, 722, 3097, 13, 51412], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 351, "seek": 107560, "start": 1096.6799999999998, "end": 1100.32, "text": " And what it will do is come up with rules such that we can just give some", "tokens": [51418, 400, 437, 309, 486, 360, 307, 808, 493, 365, 4474, 1270, 300, 321, 393, 445, 976, 512, 51600], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 352, "seek": 107560, "start": 1100.32, "end": 1102.0, "text": " features to the model in the future.", "tokens": [51600, 4122, 281, 264, 2316, 294, 264, 2027, 13, 51684], "temperature": 0.0, "avg_logprob": -0.12245587335116621, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.0033759695943444967}, {"id": 353, "seek": 110200, "start": 1102.16, "end": 1105.52, "text": " And then it should be able to give us a pretty good estimate of what the output", "tokens": [50372, 400, 550, 309, 820, 312, 1075, 281, 976, 505, 257, 1238, 665, 12539, 295, 437, 264, 5598, 50540], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 354, "seek": 110200, "start": 1105.52, "end": 1110.24, "text": " should be. So when we're training, we have a set of training data.", "tokens": [50540, 820, 312, 13, 407, 562, 321, 434, 3097, 11, 321, 362, 257, 992, 295, 3097, 1412, 13, 50776], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 355, "seek": 110200, "start": 1110.44, "end": 1114.56, "text": " And that is data where we have all of the features and all of the labels.", "tokens": [50786, 400, 300, 307, 1412, 689, 321, 362, 439, 295, 264, 4122, 293, 439, 295, 264, 16949, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 356, "seek": 110200, "start": 1114.56, "end": 1116.32, "text": " So we have all of this information.", "tokens": [50992, 407, 321, 362, 439, 295, 341, 1589, 13, 51080], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 357, "seek": 110200, "start": 1116.8, "end": 1120.84, "text": " Then when we're going to test the model or use the model later on, we would not", "tokens": [51104, 1396, 562, 321, 434, 516, 281, 1500, 264, 2316, 420, 764, 264, 2316, 1780, 322, 11, 321, 576, 406, 51306], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 358, "seek": 110200, "start": 1120.84, "end": 1122.44, "text": " have this midterm to information.", "tokens": [51306, 362, 341, 2062, 7039, 281, 1589, 13, 51386], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 359, "seek": 110200, "start": 1122.44, "end": 1123.96, "text": " We wouldn't pass this to the model.", "tokens": [51386, 492, 2759, 380, 1320, 341, 281, 264, 2316, 13, 51462], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 360, "seek": 110200, "start": 1124.16, "end": 1127.44, "text": " We would just pass our features, which is midterm one and final.", "tokens": [51472, 492, 576, 445, 1320, 527, 4122, 11, 597, 307, 2062, 7039, 472, 293, 2572, 13, 51636], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 361, "seek": 110200, "start": 1127.44, "end": 1129.4, "text": " And then we would get the output of midterm two.", "tokens": [51636, 400, 550, 321, 576, 483, 264, 5598, 295, 2062, 7039, 732, 13, 51734], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 362, "seek": 110200, "start": 1129.8, "end": 1130.96, "text": " So I hope that makes sense.", "tokens": [51754, 407, 286, 1454, 300, 1669, 2020, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11452445983886719, "compression_ratio": 2.0296296296296297, "no_speech_prob": 0.006289098411798477}, {"id": 363, "seek": 113096, "start": 1131.2, "end": 1133.4, "text": " That just means data is extremely important.", "tokens": [50376, 663, 445, 1355, 1412, 307, 4664, 1021, 13, 50486], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 364, "seek": 113096, "start": 1133.4, "end": 1137.3600000000001, "text": " If we're feeding incorrect data or data that we shouldn't be using to the model,", "tokens": [50486, 759, 321, 434, 12919, 18424, 1412, 420, 1412, 300, 321, 4659, 380, 312, 1228, 281, 264, 2316, 11, 50684], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 365, "seek": 113096, "start": 1137.52, "end": 1139.8400000000001, "text": " that could definitely result in a lot of mistakes.", "tokens": [50692, 300, 727, 2138, 1874, 294, 257, 688, 295, 8038, 13, 50808], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 366, "seek": 113096, "start": 1140.04, "end": 1143.16, "text": " And if we have incorrect output information or incorrect input", "tokens": [50818, 400, 498, 321, 362, 18424, 5598, 1589, 420, 18424, 4846, 50974], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 367, "seek": 113096, "start": 1143.16, "end": 1146.08, "text": " information, that is going to cause a lot of mistakes as well, because that is", "tokens": [50974, 1589, 11, 300, 307, 516, 281, 3082, 257, 688, 295, 8038, 382, 731, 11, 570, 300, 307, 51120], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 368, "seek": 113096, "start": 1146.08, "end": 1150.0, "text": " essentially what the model is using to learn and to kind of develop and figure", "tokens": [51120, 4476, 437, 264, 2316, 307, 1228, 281, 1466, 293, 281, 733, 295, 1499, 293, 2573, 51316], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 369, "seek": 113096, "start": 1150.0, "end": 1152.16, "text": " out what it's going to do with new input information.", "tokens": [51316, 484, 437, 309, 311, 516, 281, 360, 365, 777, 4846, 1589, 13, 51424], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 370, "seek": 113096, "start": 1152.52, "end": 1154.2, "text": " So that means that is enough of data.", "tokens": [51442, 407, 300, 1355, 300, 307, 1547, 295, 1412, 13, 51526], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 371, "seek": 113096, "start": 1154.24, "end": 1156.88, "text": " Now let's talk about the different types of machine learning.", "tokens": [51528, 823, 718, 311, 751, 466, 264, 819, 3467, 295, 3479, 2539, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 372, "seek": 113096, "start": 1157.3600000000001, "end": 1159.76, "text": " OK, so now that we've discussed the difference between artificial", "tokens": [51684, 2264, 11, 370, 586, 300, 321, 600, 7152, 264, 2649, 1296, 11677, 51804], "temperature": 0.0, "avg_logprob": -0.13164876740554282, "compression_ratio": 1.8984615384615384, "no_speech_prob": 0.00327260117046535}, {"id": 373, "seek": 115976, "start": 1159.76, "end": 1163.84, "text": " intelligence, machine learning and neural networks, we have a kind of decent idea", "tokens": [50364, 7599, 11, 3479, 2539, 293, 18161, 9590, 11, 321, 362, 257, 733, 295, 8681, 1558, 50568], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 374, "seek": 115976, "start": 1163.84, "end": 1167.0, "text": " about what data is in the difference between features and labels.", "tokens": [50568, 466, 437, 1412, 307, 294, 264, 2649, 1296, 4122, 293, 16949, 13, 50726], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 375, "seek": 115976, "start": 1167.32, "end": 1171.4, "text": " It's time to talk about the different types of machine learning specifically,", "tokens": [50742, 467, 311, 565, 281, 751, 466, 264, 819, 3467, 295, 3479, 2539, 4682, 11, 50946], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 376, "seek": 115976, "start": 1171.76, "end": 1176.32, "text": " which are unsupervised learning, supervised learning and reinforcement learning.", "tokens": [50964, 597, 366, 2693, 12879, 24420, 2539, 11, 46533, 2539, 293, 29280, 2539, 13, 51192], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 377, "seek": 115976, "start": 1176.6, "end": 1179.56, "text": " Now, these are just the different types of learning, the different types", "tokens": [51206, 823, 11, 613, 366, 445, 264, 819, 3467, 295, 2539, 11, 264, 819, 3467, 51354], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 378, "seek": 115976, "start": 1179.56, "end": 1181.08, "text": " of figuring things out.", "tokens": [51354, 295, 15213, 721, 484, 13, 51430], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 379, "seek": 115976, "start": 1181.08, "end": 1184.52, "text": " Now, different kind of algorithms fit into these different categories", "tokens": [51430, 823, 11, 819, 733, 295, 14642, 3318, 666, 613, 819, 10479, 51602], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 380, "seek": 115976, "start": 1184.52, "end": 1188.44, "text": " from within artificial intelligence, within machine learning and within neural networks.", "tokens": [51602, 490, 1951, 11677, 7599, 11, 1951, 3479, 2539, 293, 1951, 18161, 9590, 13, 51798], "temperature": 0.0, "avg_logprob": -0.11511306762695313, "compression_ratio": 2.120754716981132, "no_speech_prob": 0.002800552872940898}, {"id": 381, "seek": 118844, "start": 1188.88, "end": 1191.76, "text": " So the first one we're going to talk about is supervised learning,", "tokens": [50386, 407, 264, 700, 472, 321, 434, 516, 281, 751, 466, 307, 46533, 2539, 11, 50530], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 382, "seek": 118844, "start": 1191.76, "end": 1193.68, "text": " which is kind of what we've already discussed.", "tokens": [50530, 597, 307, 733, 295, 437, 321, 600, 1217, 7152, 13, 50626], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 383, "seek": 118844, "start": 1193.92, "end": 1198.52, "text": " So I'll just write supervised up here again, excuse the handwriting.", "tokens": [50638, 407, 286, 603, 445, 2464, 46533, 493, 510, 797, 11, 8960, 264, 39179, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 384, "seek": 118844, "start": 1199.2, "end": 1201.0, "text": " So supervised learning.", "tokens": [50902, 407, 46533, 2539, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 385, "seek": 118844, "start": 1201.0, "end": 1202.3600000000001, "text": " Now, what is this?", "tokens": [50992, 823, 11, 437, 307, 341, 30, 51060], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 386, "seek": 118844, "start": 1202.3600000000001, "end": 1204.64, "text": " Well, supervised learning is kind of everything we've already learned,", "tokens": [51060, 1042, 11, 46533, 2539, 307, 733, 295, 1203, 321, 600, 1217, 3264, 11, 51174], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 387, "seek": 118844, "start": 1204.64, "end": 1207.24, "text": " which is we have some features.", "tokens": [51174, 597, 307, 321, 362, 512, 4122, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 388, "seek": 118844, "start": 1207.28, "end": 1209.76, "text": " So we'll write our features like this, right?", "tokens": [51306, 407, 321, 603, 2464, 527, 4122, 411, 341, 11, 558, 30, 51430], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 389, "seek": 118844, "start": 1210.0, "end": 1214.2, "text": " We have some features and those features correspond to some label", "tokens": [51442, 492, 362, 512, 4122, 293, 729, 4122, 6805, 281, 512, 7645, 51652], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 390, "seek": 118844, "start": 1214.2, "end": 1215.64, "text": " or potentially labels.", "tokens": [51652, 420, 7263, 16949, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 391, "seek": 118844, "start": 1215.64, "end": 1217.76, "text": " Sometimes we might predict more than one information.", "tokens": [51724, 4803, 321, 1062, 6069, 544, 813, 472, 1589, 13, 51830], "temperature": 0.0, "avg_logprob": -0.1343488017405112, "compression_ratio": 1.9077490774907748, "no_speech_prob": 0.0012841408606618643}, {"id": 392, "seek": 121776, "start": 1218.16, "end": 1221.84, "text": " So when we have this information, we have the features and we have the labels.", "tokens": [50384, 407, 562, 321, 362, 341, 1589, 11, 321, 362, 264, 4122, 293, 321, 362, 264, 16949, 13, 50568], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 393, "seek": 121776, "start": 1222.04, "end": 1225.36, "text": " What we do is we pass this information to some machine learning model.", "tokens": [50578, 708, 321, 360, 307, 321, 1320, 341, 1589, 281, 512, 3479, 2539, 2316, 13, 50744], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 394, "seek": 121776, "start": 1225.6, "end": 1227.12, "text": " It figures out the rules for us.", "tokens": [50756, 467, 9624, 484, 264, 4474, 337, 505, 13, 50832], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 395, "seek": 121776, "start": 1227.12, "end": 1231.04, "text": " And then later on, all we need is the features and it will give us some labels", "tokens": [50832, 400, 550, 1780, 322, 11, 439, 321, 643, 307, 264, 4122, 293, 309, 486, 976, 505, 512, 16949, 51028], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 396, "seek": 121776, "start": 1231.04, "end": 1232.12, "text": " using those rules.", "tokens": [51028, 1228, 729, 4474, 13, 51082], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 397, "seek": 121776, "start": 1232.12, "end": 1236.28, "text": " But essentially, what supervised learning is, is when we have both of this information.", "tokens": [51082, 583, 4476, 11, 437, 46533, 2539, 307, 11, 307, 562, 321, 362, 1293, 295, 341, 1589, 13, 51290], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 398, "seek": 121776, "start": 1236.68, "end": 1240.04, "text": " The reason it's called supervised is because what ends up happening", "tokens": [51310, 440, 1778, 309, 311, 1219, 46533, 307, 570, 437, 5314, 493, 2737, 51478], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 399, "seek": 121776, "start": 1240.04, "end": 1244.2, "text": " when we train our machine learning model is we pass the input information.", "tokens": [51478, 562, 321, 3847, 527, 3479, 2539, 2316, 307, 321, 1320, 264, 4846, 1589, 13, 51686], "temperature": 0.0, "avg_logprob": -0.10935762280323466, "compression_ratio": 2.0118110236220472, "no_speech_prob": 0.0006461399025283754}, {"id": 400, "seek": 124420, "start": 1244.52, "end": 1248.28, "text": " It makes some arbitrary prediction using the rules it already knows.", "tokens": [50380, 467, 1669, 512, 23211, 17630, 1228, 264, 4474, 309, 1217, 3255, 13, 50568], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 401, "seek": 124420, "start": 1248.44, "end": 1251.64, "text": " And then it compares that prediction that it made to what the actual", "tokens": [50576, 400, 550, 309, 38334, 300, 17630, 300, 309, 1027, 281, 437, 264, 3539, 50736], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 402, "seek": 124420, "start": 1251.64, "end": 1253.88, "text": " prediction is, which is this label.", "tokens": [50736, 17630, 307, 11, 597, 307, 341, 7645, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 403, "seek": 124420, "start": 1254.16, "end": 1258.72, "text": " So we supervise the model and we say, OK, so you predicted that the color was red,", "tokens": [50862, 407, 321, 37971, 908, 264, 2316, 293, 321, 584, 11, 2264, 11, 370, 291, 19147, 300, 264, 2017, 390, 2182, 11, 51090], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 404, "seek": 124420, "start": 1258.92, "end": 1261.72, "text": " but really the color of whatever we passed in should have been blue.", "tokens": [51100, 457, 534, 264, 2017, 295, 2035, 321, 4678, 294, 820, 362, 668, 3344, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 405, "seek": 124420, "start": 1261.88, "end": 1265.24, "text": " So we need to tweak you just a little bit so that you get a little bit better", "tokens": [51248, 407, 321, 643, 281, 29879, 291, 445, 257, 707, 857, 370, 300, 291, 483, 257, 707, 857, 1101, 51416], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 406, "seek": 124420, "start": 1265.24, "end": 1266.96, "text": " and you move in the correct direction.", "tokens": [51416, 293, 291, 1286, 294, 264, 3006, 3513, 13, 51502], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 407, "seek": 124420, "start": 1266.96, "end": 1268.8400000000001, "text": " And that's kind of the way that this works.", "tokens": [51502, 400, 300, 311, 733, 295, 264, 636, 300, 341, 1985, 13, 51596], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 408, "seek": 124420, "start": 1268.8400000000001, "end": 1271.68, "text": " For example, say we're predicting, you know, students final grade.", "tokens": [51596, 1171, 1365, 11, 584, 321, 434, 32884, 11, 291, 458, 11, 1731, 2572, 7204, 13, 51738], "temperature": 0.0, "avg_logprob": -0.13879165649414063, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.023683125153183937}, {"id": 409, "seek": 127168, "start": 1271.96, "end": 1276.8400000000001, "text": " Well, if we predict that the final grade is 76, but the actual grade is 77,", "tokens": [50378, 1042, 11, 498, 321, 6069, 300, 264, 2572, 7204, 307, 24733, 11, 457, 264, 3539, 7204, 307, 25546, 11, 50622], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 410, "seek": 127168, "start": 1277.04, "end": 1279.16, "text": " we were pretty close, but we're not quite there.", "tokens": [50632, 321, 645, 1238, 1998, 11, 457, 321, 434, 406, 1596, 456, 13, 50738], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 411, "seek": 127168, "start": 1279.3200000000002, "end": 1282.3200000000002, "text": " So we supervise the model and we say, hey, we're going to tweak you", "tokens": [50746, 407, 321, 37971, 908, 264, 2316, 293, 321, 584, 11, 4177, 11, 321, 434, 516, 281, 29879, 291, 50896], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 412, "seek": 127168, "start": 1282.3200000000002, "end": 1284.52, "text": " just a little bit, move you in the correct direction.", "tokens": [50896, 445, 257, 707, 857, 11, 1286, 291, 294, 264, 3006, 3513, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 413, "seek": 127168, "start": 1284.72, "end": 1286.3600000000001, "text": " And hopefully we get you to 77.", "tokens": [51016, 400, 4696, 321, 483, 291, 281, 25546, 13, 51098], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 414, "seek": 127168, "start": 1286.92, "end": 1289.52, "text": " And that is kind of the way to explain this, right?", "tokens": [51126, 400, 300, 307, 733, 295, 264, 636, 281, 2903, 341, 11, 558, 30, 51256], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 415, "seek": 127168, "start": 1289.52, "end": 1291.3600000000001, "text": " You have the features, you have the labels.", "tokens": [51256, 509, 362, 264, 4122, 11, 291, 362, 264, 16949, 13, 51348], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 416, "seek": 127168, "start": 1291.52, "end": 1295.0800000000002, "text": " When you pass the features, the model has some rules that it's already built.", "tokens": [51356, 1133, 291, 1320, 264, 4122, 11, 264, 2316, 575, 512, 4474, 300, 309, 311, 1217, 3094, 13, 51534], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 417, "seek": 127168, "start": 1295.1200000000001, "end": 1296.3200000000002, "text": " It makes a prediction.", "tokens": [51536, 467, 1669, 257, 17630, 13, 51596], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 418, "seek": 127168, "start": 1296.48, "end": 1300.1200000000001, "text": " And then it compares that prediction to the label and then re tweaks the model", "tokens": [51604, 400, 550, 309, 38334, 300, 17630, 281, 264, 7645, 293, 550, 319, 46664, 264, 2316, 51786], "temperature": 0.0, "avg_logprob": -0.14801819908697875, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.003172221127897501}, {"id": 419, "seek": 130012, "start": 1300.1999999999998, "end": 1304.9199999999998, "text": " and continues doing this with thousands upon thousands upon thousands of pieces", "tokens": [50368, 293, 6515, 884, 341, 365, 5383, 3564, 5383, 3564, 5383, 295, 3755, 50604], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 420, "seek": 130012, "start": 1304.9199999999998, "end": 1308.52, "text": " of data, until eventually it gets so good that we can stop training it.", "tokens": [50604, 295, 1412, 11, 1826, 4728, 309, 2170, 370, 665, 300, 321, 393, 1590, 3097, 309, 13, 50784], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 421, "seek": 130012, "start": 1308.6799999999998, "end": 1310.6, "text": " And that is what supervised learning is.", "tokens": [50792, 400, 300, 307, 437, 46533, 2539, 307, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 422, "seek": 130012, "start": 1310.8, "end": 1312.52, "text": " It's the most common type of learning.", "tokens": [50898, 467, 311, 264, 881, 2689, 2010, 295, 2539, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 423, "seek": 130012, "start": 1312.52, "end": 1315.2399999999998, "text": " It's definitely the most applicable in a lot of instances.", "tokens": [50984, 467, 311, 2138, 264, 881, 21142, 294, 257, 688, 295, 14519, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 424, "seek": 130012, "start": 1315.4399999999998, "end": 1318.6, "text": " And most machine learning algorithms that are actually used", "tokens": [51130, 400, 881, 3479, 2539, 14642, 300, 366, 767, 1143, 51288], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 425, "seek": 130012, "start": 1318.6, "end": 1320.56, "text": " use a form of supervised machine learning.", "tokens": [51288, 764, 257, 1254, 295, 46533, 3479, 2539, 13, 51386], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 426, "seek": 130012, "start": 1320.8799999999999, "end": 1324.2399999999998, "text": " A lot of people seem to think that this is, you know, a less complicated,", "tokens": [51402, 316, 688, 295, 561, 1643, 281, 519, 300, 341, 307, 11, 291, 458, 11, 257, 1570, 6179, 11, 51570], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 427, "seek": 130012, "start": 1324.2399999999998, "end": 1327.8, "text": " less advanced way of doing things that is definitely not true.", "tokens": [51570, 1570, 7339, 636, 295, 884, 721, 300, 307, 2138, 406, 2074, 13, 51748], "temperature": 0.0, "avg_logprob": -0.1368983022628292, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.020957034081220627}, {"id": 428, "seek": 132780, "start": 1327.84, "end": 1329.9199999999998, "text": " All of the different methods I'm going to tell you have different", "tokens": [50366, 1057, 295, 264, 819, 7150, 286, 478, 516, 281, 980, 291, 362, 819, 50470], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 429, "seek": 132780, "start": 1329.9199999999998, "end": 1331.52, "text": " advantages and disadvantages.", "tokens": [50470, 14906, 293, 37431, 13, 50550], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 430, "seek": 132780, "start": 1331.72, "end": 1335.6, "text": " And this has a massive advantage when you have a ton of information", "tokens": [50560, 400, 341, 575, 257, 5994, 5002, 562, 291, 362, 257, 2952, 295, 1589, 50754], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 431, "seek": 132780, "start": 1335.6, "end": 1338.36, "text": " and you have the output of that information as well.", "tokens": [50754, 293, 291, 362, 264, 5598, 295, 300, 1589, 382, 731, 13, 50892], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 432, "seek": 132780, "start": 1338.56, "end": 1340.6, "text": " But sometimes we don't have the luxury of doing that.", "tokens": [50902, 583, 2171, 321, 500, 380, 362, 264, 15558, 295, 884, 300, 13, 51004], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 433, "seek": 132780, "start": 1340.6, "end": 1342.76, "text": " And that's where we talk about unsupervised learning.", "tokens": [51004, 400, 300, 311, 689, 321, 751, 466, 2693, 12879, 24420, 2539, 13, 51112], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 434, "seek": 132780, "start": 1343.12, "end": 1345.6, "text": " So hopefully that made sense for supervised learning.", "tokens": [51130, 407, 4696, 300, 1027, 2020, 337, 46533, 2539, 13, 51254], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 435, "seek": 132780, "start": 1346.08, "end": 1347.44, "text": " Tried my best to explain that.", "tokens": [51278, 314, 2428, 452, 1151, 281, 2903, 300, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 436, "seek": 132780, "start": 1347.6399999999999, "end": 1350.2, "text": " And now let's go into or sorry for supervised learning.", "tokens": [51356, 400, 586, 718, 311, 352, 666, 420, 2597, 337, 46533, 2539, 13, 51484], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 437, "seek": 132780, "start": 1350.24, "end": 1352.52, "text": " Now let's go into unsupervised learning.", "tokens": [51486, 823, 718, 311, 352, 666, 2693, 12879, 24420, 2539, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 438, "seek": 132780, "start": 1353.2, "end": 1356.04, "text": " So if we know the definition of supervised learning,", "tokens": [51634, 407, 498, 321, 458, 264, 7123, 295, 46533, 2539, 11, 51776], "temperature": 0.0, "avg_logprob": -0.13670416845791583, "compression_ratio": 2.0780669144981414, "no_speech_prob": 0.015422491356730461}, {"id": 439, "seek": 135604, "start": 1356.28, "end": 1359.3999999999999, "text": " we should hopefully be able to come up with a definition of unsupervised", "tokens": [50376, 321, 820, 4696, 312, 1075, 281, 808, 493, 365, 257, 7123, 295, 2693, 12879, 24420, 50532], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 440, "seek": 135604, "start": 1359.3999999999999, "end": 1362.3999999999999, "text": " learning, which is when we only have features.", "tokens": [50532, 2539, 11, 597, 307, 562, 321, 787, 362, 4122, 13, 50682], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 441, "seek": 135604, "start": 1362.76, "end": 1367.76, "text": " So given a bunch of features like this and absolutely no labels,", "tokens": [50700, 407, 2212, 257, 3840, 295, 4122, 411, 341, 293, 3122, 572, 16949, 11, 50950], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 442, "seek": 135604, "start": 1367.76, "end": 1372.3999999999999, "text": " no output for these features, what we want to do is have the model", "tokens": [50950, 572, 5598, 337, 613, 4122, 11, 437, 321, 528, 281, 360, 307, 362, 264, 2316, 51182], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 443, "seek": 135604, "start": 1372.3999999999999, "end": 1374.32, "text": " come up with those labels for us.", "tokens": [51182, 808, 493, 365, 729, 16949, 337, 505, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 444, "seek": 135604, "start": 1374.52, "end": 1375.48, "text": " Now, this is kind of weird.", "tokens": [51288, 823, 11, 341, 307, 733, 295, 3657, 13, 51336], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 445, "seek": 135604, "start": 1375.48, "end": 1377.44, "text": " You're kind of like, wait, how does that work?", "tokens": [51336, 509, 434, 733, 295, 411, 11, 1699, 11, 577, 775, 300, 589, 30, 51434], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 446, "seek": 135604, "start": 1377.44, "end": 1379.0, "text": " Why would we even want to do that?", "tokens": [51434, 1545, 576, 321, 754, 528, 281, 360, 300, 30, 51512], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 447, "seek": 135604, "start": 1379.24, "end": 1380.84, "text": " Well, let's take this for an example.", "tokens": [51524, 1042, 11, 718, 311, 747, 341, 337, 364, 1365, 13, 51604], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 448, "seek": 135604, "start": 1381.32, "end": 1384.36, "text": " We have some access, some axes of data.", "tokens": [51628, 492, 362, 512, 2105, 11, 512, 35387, 295, 1412, 13, 51780], "temperature": 0.0, "avg_logprob": -0.13309759842722038, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0043307882733643055}, {"id": 449, "seek": 138436, "start": 1384.36, "end": 1387.04, "text": " Okay, and we have like a two dimensional data point.", "tokens": [50364, 1033, 11, 293, 321, 362, 411, 257, 732, 18795, 1412, 935, 13, 50498], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 450, "seek": 138436, "start": 1387.04, "end": 1390.6, "text": " So I'm just going to call this, let's say X and let's say Y.", "tokens": [50498, 407, 286, 478, 445, 516, 281, 818, 341, 11, 718, 311, 584, 1783, 293, 718, 311, 584, 398, 13, 50676], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 451, "seek": 138436, "start": 1390.8, "end": 1391.1999999999998, "text": " Okay.", "tokens": [50686, 1033, 13, 50706], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 452, "seek": 138436, "start": 1391.6, "end": 1394.3999999999999, "text": " And I'm going to just put a bunch of dots on the screen that kind of", "tokens": [50726, 400, 286, 478, 516, 281, 445, 829, 257, 3840, 295, 15026, 322, 264, 2568, 300, 733, 295, 50866], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 453, "seek": 138436, "start": 1394.3999999999999, "end": 1397.36, "text": " represents like maybe a scatter plot of some of our different data.", "tokens": [50866, 8855, 411, 1310, 257, 34951, 7542, 295, 512, 295, 527, 819, 1412, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 454, "seek": 138436, "start": 1398.36, "end": 1401.84, "text": " And I'm just going to put some dots specifically closer to other ones.", "tokens": [51064, 400, 286, 478, 445, 516, 281, 829, 512, 15026, 4682, 4966, 281, 661, 2306, 13, 51238], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 455, "seek": 138436, "start": 1401.84, "end": 1404.6399999999999, "text": " Just so you guys kind of get the point of what we're trying to do here.", "tokens": [51238, 1449, 370, 291, 1074, 733, 295, 483, 264, 935, 295, 437, 321, 434, 1382, 281, 360, 510, 13, 51378], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 456, "seek": 138436, "start": 1404.9199999999998, "end": 1406.36, "text": " So let's do that.", "tokens": [51392, 407, 718, 311, 360, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 457, "seek": 138436, "start": 1406.8, "end": 1407.0, "text": " Okay.", "tokens": [51486, 1033, 13, 51496], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 458, "seek": 138436, "start": 1407.0, "end": 1410.52, "text": " So let's say I have this data set, this here is what we're working with.", "tokens": [51496, 407, 718, 311, 584, 286, 362, 341, 1412, 992, 11, 341, 510, 307, 437, 321, 434, 1364, 365, 13, 51672], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 459, "seek": 138436, "start": 1410.56, "end": 1412.24, "text": " And we have these features.", "tokens": [51674, 400, 321, 362, 613, 4122, 13, 51758], "temperature": 0.0, "avg_logprob": -0.14386608398038578, "compression_ratio": 1.8194444444444444, "no_speech_prob": 0.0018099774606525898}, {"id": 460, "seek": 141224, "start": 1412.6, "end": 1415.88, "text": " The features in this instance are going to be X and Y, right?", "tokens": [50382, 440, 4122, 294, 341, 5197, 366, 516, 281, 312, 1783, 293, 398, 11, 558, 30, 50546], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 461, "seek": 141224, "start": 1415.88, "end": 1418.32, "text": " So X and Y are my features.", "tokens": [50546, 407, 1783, 293, 398, 366, 452, 4122, 13, 50668], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 462, "seek": 141224, "start": 1418.52, "end": 1421.88, "text": " Now, we don't have any output specifically for these data points.", "tokens": [50678, 823, 11, 321, 500, 380, 362, 604, 5598, 4682, 337, 613, 1412, 2793, 13, 50846], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 463, "seek": 141224, "start": 1422.16, "end": 1425.52, "text": " What we actually want to do is we want to create some kind of model", "tokens": [50860, 708, 321, 767, 528, 281, 360, 307, 321, 528, 281, 1884, 512, 733, 295, 2316, 51028], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 464, "seek": 141224, "start": 1426.16, "end": 1430.4, "text": " that can cluster these data points, which means figure out kind of, you know,", "tokens": [51060, 300, 393, 13630, 613, 1412, 2793, 11, 597, 1355, 2573, 484, 733, 295, 11, 291, 458, 11, 51272], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 465, "seek": 141224, "start": 1430.44, "end": 1434.6, "text": " unique groups of data and say, okay, so you're in group one, you're in group two,", "tokens": [51274, 3845, 3935, 295, 1412, 293, 584, 11, 1392, 11, 370, 291, 434, 294, 1594, 472, 11, 291, 434, 294, 1594, 732, 11, 51482], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 466, "seek": 141224, "start": 1434.8, "end": 1436.48, "text": " you're in group three, and you're in group four.", "tokens": [51492, 291, 434, 294, 1594, 1045, 11, 293, 291, 434, 294, 1594, 1451, 13, 51576], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 467, "seek": 141224, "start": 1437.1200000000001, "end": 1441.36, "text": " We may not necessarily know how many groups we have, although sometimes we do.", "tokens": [51608, 492, 815, 406, 4725, 458, 577, 867, 3935, 321, 362, 11, 4878, 2171, 321, 360, 13, 51820], "temperature": 0.0, "avg_logprob": -0.11296450573465099, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0010986359557136893}, {"id": 468, "seek": 144136, "start": 1441.9199999999998, "end": 1445.36, "text": " But what we want to do is just group them and kind of say, okay, we want to", "tokens": [50392, 583, 437, 321, 528, 281, 360, 307, 445, 1594, 552, 293, 733, 295, 584, 11, 1392, 11, 321, 528, 281, 50564], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 469, "seek": 144136, "start": 1445.36, "end": 1448.6799999999998, "text": " figure out which ones are similar and we want to combine those together.", "tokens": [50564, 2573, 484, 597, 2306, 366, 2531, 293, 321, 528, 281, 10432, 729, 1214, 13, 50730], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 470, "seek": 144136, "start": 1449.0, "end": 1452.1599999999999, "text": " So hopefully what we would do with an unsupervised machine learning model", "tokens": [50746, 407, 4696, 437, 321, 576, 360, 365, 364, 2693, 12879, 24420, 3479, 2539, 2316, 50904], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 471, "seek": 144136, "start": 1452.1999999999998, "end": 1456.0, "text": " is pass all of these features and then have the model create kind of these", "tokens": [50906, 307, 1320, 439, 295, 613, 4122, 293, 550, 362, 264, 2316, 1884, 733, 295, 613, 51096], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 472, "seek": 144136, "start": 1456.0, "end": 1461.3999999999999, "text": " groupings. So like maybe this is a group, maybe this is a group, maybe this is a", "tokens": [51096, 1594, 1109, 13, 407, 411, 1310, 341, 307, 257, 1594, 11, 1310, 341, 307, 257, 1594, 11, 1310, 341, 307, 257, 51366], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 473, "seek": 144136, "start": 1461.3999999999999, "end": 1464.6, "text": " group, if we were having four groupings, and maybe if we had two groupings,", "tokens": [51366, 1594, 11, 498, 321, 645, 1419, 1451, 1594, 1109, 11, 293, 1310, 498, 321, 632, 732, 1594, 1109, 11, 51526], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 474, "seek": 144136, "start": 1464.8, "end": 1467.36, "text": " we might get groupings that look something like this, right?", "tokens": [51536, 321, 1062, 483, 1594, 1109, 300, 574, 746, 411, 341, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1342929204305013, "compression_ratio": 1.965648854961832, "no_speech_prob": 0.0006666459375992417}, {"id": 475, "seek": 146736, "start": 1467.76, "end": 1472.76, "text": " And then when we pass a new data point in, that could, we could figure out what", "tokens": [50384, 400, 550, 562, 321, 1320, 257, 777, 1412, 935, 294, 11, 300, 727, 11, 321, 727, 2573, 484, 437, 50634], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 476, "seek": 146736, "start": 1472.76, "end": 1476.08, "text": " group that was a part of by determining, you know, which one it is closer to.", "tokens": [50634, 1594, 300, 390, 257, 644, 295, 538, 23751, 11, 291, 458, 11, 597, 472, 309, 307, 4966, 281, 13, 50800], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 477, "seek": 146736, "start": 1476.7199999999998, "end": 1478.56, "text": " Now, this is kind of a rough example.", "tokens": [50832, 823, 11, 341, 307, 733, 295, 257, 5903, 1365, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 478, "seek": 146736, "start": 1478.56, "end": 1482.1599999999999, "text": " It's hard to again, explain all of these without going very in depth into", "tokens": [50924, 467, 311, 1152, 281, 797, 11, 2903, 439, 295, 613, 1553, 516, 588, 294, 7161, 666, 51104], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 479, "seek": 146736, "start": 1482.1599999999999, "end": 1486.0, "text": " the specific algorithms, but unsupervised machine learning or just learning", "tokens": [51104, 264, 2685, 14642, 11, 457, 2693, 12879, 24420, 3479, 2539, 420, 445, 2539, 51296], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 480, "seek": 146736, "start": 1486.0, "end": 1489.8, "text": " in general is when you don't have some output information, you actually want", "tokens": [51296, 294, 2674, 307, 562, 291, 500, 380, 362, 512, 5598, 1589, 11, 291, 767, 528, 51486], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 481, "seek": 146736, "start": 1489.84, "end": 1492.08, "text": " the model to figure out the output for you.", "tokens": [51488, 264, 2316, 281, 2573, 484, 264, 5598, 337, 291, 13, 51600], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 482, "seek": 146736, "start": 1492.32, "end": 1495.6399999999999, "text": " You don't really care how it gets there, you just want it to get there.", "tokens": [51612, 509, 500, 380, 534, 1127, 577, 309, 2170, 456, 11, 291, 445, 528, 309, 281, 483, 456, 13, 51778], "temperature": 0.0, "avg_logprob": -0.11950534322987431, "compression_ratio": 1.7079365079365079, "no_speech_prob": 0.01032711286097765}, {"id": 483, "seek": 149564, "start": 1495.8000000000002, "end": 1499.5600000000002, "text": " And again, a good example is clustering data points, and we'll talk about some", "tokens": [50372, 400, 797, 11, 257, 665, 1365, 307, 596, 48673, 1412, 2793, 11, 293, 321, 603, 751, 466, 512, 50560], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 484, "seek": 149564, "start": 1499.5600000000002, "end": 1503.4, "text": " specific applications of when we might even want to use that later on, just", "tokens": [50560, 2685, 5821, 295, 562, 321, 1062, 754, 528, 281, 764, 300, 1780, 322, 11, 445, 50752], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 485, "seek": 149564, "start": 1503.4, "end": 1506.88, "text": " understand you have the features, you don't have the labels, and you get the", "tokens": [50752, 1223, 291, 362, 264, 4122, 11, 291, 500, 380, 362, 264, 16949, 11, 293, 291, 483, 264, 50926], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 486, "seek": 149564, "start": 1506.92, "end": 1509.72, "text": " unsupervised model to kind of figure it out for you.", "tokens": [50928, 2693, 12879, 24420, 2316, 281, 733, 295, 2573, 309, 484, 337, 291, 13, 51068], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 487, "seek": 149564, "start": 1510.1200000000001, "end": 1513.64, "text": " Okay, so now our last type, which is very different than the two types I just", "tokens": [51088, 1033, 11, 370, 586, 527, 1036, 2010, 11, 597, 307, 588, 819, 813, 264, 732, 3467, 286, 445, 51264], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 488, "seek": 149564, "start": 1513.64, "end": 1515.76, "text": " explained is called reinforcement learning.", "tokens": [51264, 8825, 307, 1219, 29280, 2539, 13, 51370], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 489, "seek": 149564, "start": 1516.1200000000001, "end": 1519.0800000000002, "text": " Now personally, reinforcement learning, and I don't even know if I want to", "tokens": [51388, 823, 5665, 11, 29280, 2539, 11, 293, 286, 500, 380, 754, 458, 498, 286, 528, 281, 51536], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 490, "seek": 149564, "start": 1519.0800000000002, "end": 1522.5200000000002, "text": " spell this because I feel like I'm going to mess it up.", "tokens": [51536, 9827, 341, 570, 286, 841, 411, 286, 478, 516, 281, 2082, 309, 493, 13, 51708], "temperature": 0.0, "avg_logprob": -0.12547675238715278, "compression_ratio": 1.6940063091482649, "no_speech_prob": 0.009124177508056164}, {"id": 491, "seek": 152252, "start": 1523.24, "end": 1527.44, "text": " Reinforcement learning is the coolest type of machine learning, in my opinion.", "tokens": [50400, 42116, 9382, 2539, 307, 264, 22013, 2010, 295, 3479, 2539, 11, 294, 452, 4800, 13, 50610], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 492, "seek": 152252, "start": 1527.48, "end": 1531.8, "text": " And this is when you actually don't have any data, you have what you call an", "tokens": [50612, 400, 341, 307, 562, 291, 767, 500, 380, 362, 604, 1412, 11, 291, 362, 437, 291, 818, 364, 50828], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 493, "seek": 152252, "start": 1531.84, "end": 1534.6399999999999, "text": " agent, an environment and a reward.", "tokens": [50830, 9461, 11, 364, 2823, 293, 257, 7782, 13, 50970], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 494, "seek": 152252, "start": 1534.8799999999999, "end": 1539.44, "text": " I'm going to explain this very briefly with a very, very, very simple example", "tokens": [50982, 286, 478, 516, 281, 2903, 341, 588, 10515, 365, 257, 588, 11, 588, 11, 588, 2199, 1365, 51210], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 495, "seek": 152252, "start": 1539.44, "end": 1540.84, "text": " because it's hard to get too far.", "tokens": [51210, 570, 309, 311, 1152, 281, 483, 886, 1400, 13, 51280], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 496, "seek": 152252, "start": 1541.16, "end": 1544.32, "text": " So let's say we have a very basic game, you know, maybe we made this game", "tokens": [51296, 407, 718, 311, 584, 321, 362, 257, 588, 3875, 1216, 11, 291, 458, 11, 1310, 321, 1027, 341, 1216, 51454], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 497, "seek": 152252, "start": 1544.32, "end": 1548.08, "text": " ourselves, and essentially, the objective of the game is to get to the flag.", "tokens": [51454, 4175, 11, 293, 4476, 11, 264, 10024, 295, 264, 1216, 307, 281, 483, 281, 264, 7166, 13, 51642], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 498, "seek": 152252, "start": 1548.32, "end": 1549.6, "text": " Okay, that's all it is.", "tokens": [51654, 1033, 11, 300, 311, 439, 309, 307, 13, 51718], "temperature": 0.0, "avg_logprob": -0.13421005426451219, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.014501121826469898}, {"id": 499, "seek": 154960, "start": 1549.6, "end": 1553.36, "text": " We have some ground, you can move left or right, and we want to get to this", "tokens": [50364, 492, 362, 512, 2727, 11, 291, 393, 1286, 1411, 420, 558, 11, 293, 321, 528, 281, 483, 281, 341, 50552], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 500, "seek": 154960, "start": 1553.36, "end": 1558.0, "text": " flag. Well, we want to train some artificial intelligence, some machine", "tokens": [50552, 7166, 13, 1042, 11, 321, 528, 281, 3847, 512, 11677, 7599, 11, 512, 3479, 50784], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 501, "seek": 154960, "start": 1558.0, "end": 1560.3999999999999, "text": " learning model that can figure out how to do this.", "tokens": [50784, 2539, 2316, 300, 393, 2573, 484, 577, 281, 360, 341, 13, 50904], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 502, "seek": 154960, "start": 1560.8, "end": 1563.6399999999999, "text": " So what we do is we call this our agent.", "tokens": [50924, 407, 437, 321, 360, 307, 321, 818, 341, 527, 9461, 13, 51066], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 503, "seek": 154960, "start": 1564.84, "end": 1566.6399999999999, "text": " We call this entire thing.", "tokens": [51126, 492, 818, 341, 2302, 551, 13, 51216], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 504, "seek": 154960, "start": 1566.6399999999999, "end": 1569.1599999999999, "text": " So this whole thing here, the environment.", "tokens": [51216, 407, 341, 1379, 551, 510, 11, 264, 2823, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 505, "seek": 154960, "start": 1569.4399999999998, "end": 1571.1999999999998, "text": " So I guess I could write that here.", "tokens": [51356, 407, 286, 2041, 286, 727, 2464, 300, 510, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 506, "seek": 154960, "start": 1571.1999999999998, "end": 1575.6, "text": " So n by our meant think I spelled that correctly.", "tokens": [51444, 407, 297, 538, 527, 4140, 519, 286, 34388, 300, 8944, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 507, "seek": 154960, "start": 1576.04, "end": 1577.8, "text": " And then we have something called a reward.", "tokens": [51686, 400, 550, 321, 362, 746, 1219, 257, 7782, 13, 51774], "temperature": 0.0, "avg_logprob": -0.15797163057727975, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.013220292516052723}, {"id": 508, "seek": 157780, "start": 1578.2, "end": 1581.6399999999999, "text": " And a reward is essentially what the agent gets when it does something", "tokens": [50384, 400, 257, 7782, 307, 4476, 437, 264, 9461, 2170, 562, 309, 775, 746, 50556], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 509, "seek": 157780, "start": 1581.6399999999999, "end": 1585.52, "text": " correctly. So let's say the agent takes one step over this way.", "tokens": [50556, 8944, 13, 407, 718, 311, 584, 264, 9461, 2516, 472, 1823, 670, 341, 636, 13, 50750], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 510, "seek": 157780, "start": 1585.52, "end": 1586.9199999999998, "text": " So let's say he's a new position is here.", "tokens": [50750, 407, 718, 311, 584, 415, 311, 257, 777, 2535, 307, 510, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 511, "seek": 157780, "start": 1586.9199999999998, "end": 1587.8799999999999, "text": " I just don't want to keep drawing him.", "tokens": [50820, 286, 445, 500, 380, 528, 281, 1066, 6316, 796, 13, 50868], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 512, "seek": 157780, "start": 1587.8799999999999, "end": 1588.76, "text": " So I'm just going to use a dot.", "tokens": [50868, 407, 286, 478, 445, 516, 281, 764, 257, 5893, 13, 50912], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 513, "seek": 157780, "start": 1589.3999999999999, "end": 1591.6399999999999, "text": " Well, he got closer to the flag.", "tokens": [50944, 1042, 11, 415, 658, 4966, 281, 264, 7166, 13, 51056], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 514, "seek": 157780, "start": 1592.0, "end": 1595.3999999999999, "text": " So what I'm actually going to do is give him a plus two reward.", "tokens": [51074, 407, 437, 286, 478, 767, 516, 281, 360, 307, 976, 796, 257, 1804, 732, 7782, 13, 51244], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 515, "seek": 157780, "start": 1596.24, "end": 1598.24, "text": " So let's say he moves again closer to the flag.", "tokens": [51286, 407, 718, 311, 584, 415, 6067, 797, 4966, 281, 264, 7166, 13, 51386], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 516, "seek": 157780, "start": 1598.8799999999999, "end": 1601.96, "text": " Maybe I give him now plus one this time he got even closer.", "tokens": [51418, 2704, 286, 976, 796, 586, 1804, 472, 341, 565, 415, 658, 754, 4966, 13, 51572], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 517, "seek": 157780, "start": 1602.76, "end": 1605.8, "text": " And as he gets closer, I give him more and more reward.", "tokens": [51612, 400, 382, 415, 2170, 4966, 11, 286, 976, 796, 544, 293, 544, 7782, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09354259854271299, "compression_ratio": 1.8814814814814815, "no_speech_prob": 0.005729378201067448}, {"id": 518, "seek": 160580, "start": 1606.6399999999999, "end": 1608.72, "text": " Now what happens if he moves backwards?", "tokens": [50406, 823, 437, 2314, 498, 415, 6067, 12204, 30, 50510], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 519, "seek": 160580, "start": 1609.2, "end": 1611.8, "text": " So let's erase this and let's say that at some point in time,", "tokens": [50534, 407, 718, 311, 23525, 341, 293, 718, 311, 584, 300, 412, 512, 935, 294, 565, 11, 50664], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 520, "seek": 160580, "start": 1612.0, "end": 1615.68, "text": " rather than moving closer to the threat, the flag, he moves backwards.", "tokens": [50674, 2831, 813, 2684, 4966, 281, 264, 4734, 11, 264, 7166, 11, 415, 6067, 12204, 13, 50858], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 521, "seek": 160580, "start": 1616.32, "end": 1618.28, "text": " Well, he might get a negative reward.", "tokens": [50890, 1042, 11, 415, 1062, 483, 257, 3671, 7782, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 522, "seek": 160580, "start": 1619.0, "end": 1622.52, "text": " Now, essentially, what the objective of this agent is to do", "tokens": [51024, 823, 11, 4476, 11, 437, 264, 10024, 295, 341, 9461, 307, 281, 360, 51200], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 523, "seek": 160580, "start": 1623.08, "end": 1625.08, "text": " is to maximize its reward.", "tokens": [51228, 307, 281, 19874, 1080, 7782, 13, 51328], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 524, "seek": 160580, "start": 1625.08, "end": 1627.8799999999999, "text": " So if you give it a negative reward for moving backwards,", "tokens": [51328, 407, 498, 291, 976, 309, 257, 3671, 7782, 337, 2684, 12204, 11, 51468], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 525, "seek": 160580, "start": 1628.1599999999999, "end": 1629.56, "text": " it's going to remember that.", "tokens": [51482, 309, 311, 516, 281, 1604, 300, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 526, "seek": 160580, "start": 1629.56, "end": 1633.08, "text": " And it's going to say, OK, at this position here, where I was standing,", "tokens": [51552, 400, 309, 311, 516, 281, 584, 11, 2264, 11, 412, 341, 2535, 510, 11, 689, 286, 390, 4877, 11, 51728], "temperature": 0.0, "avg_logprob": -0.14272357177734374, "compression_ratio": 1.7471264367816093, "no_speech_prob": 0.0005357544287107885}, {"id": 527, "seek": 163308, "start": 1633.08, "end": 1635.8799999999999, "text": " when I moved backwards, I got a negative reward.", "tokens": [50364, 562, 286, 4259, 12204, 11, 286, 658, 257, 3671, 7782, 13, 50504], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 528, "seek": 163308, "start": 1636.28, "end": 1640.08, "text": " So if I get to this position again, I don't want to go backwards anymore.", "tokens": [50524, 407, 498, 286, 483, 281, 341, 2535, 797, 11, 286, 500, 380, 528, 281, 352, 12204, 3602, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 529, "seek": 163308, "start": 1640.3999999999999, "end": 1644.6399999999999, "text": " I want to go forwards because that should give me a positive reward.", "tokens": [50730, 286, 528, 281, 352, 30126, 570, 300, 820, 976, 385, 257, 3353, 7782, 13, 50942], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 530, "seek": 163308, "start": 1645.24, "end": 1647.9199999999998, "text": " And the whole point of this is we have this agent", "tokens": [50972, 400, 264, 1379, 935, 295, 341, 307, 321, 362, 341, 9461, 51106], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 531, "seek": 163308, "start": 1648.1599999999999, "end": 1651.84, "text": " that starts off with absolutely no idea, no kind of, you know,", "tokens": [51118, 300, 3719, 766, 365, 3122, 572, 1558, 11, 572, 733, 295, 11, 291, 458, 11, 51302], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 532, "seek": 163308, "start": 1651.8799999999999, "end": 1653.8, "text": " knowledge of the environment.", "tokens": [51304, 3601, 295, 264, 2823, 13, 51400], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 533, "seek": 163308, "start": 1653.8, "end": 1656.08, "text": " And what it does is it starts exploring.", "tokens": [51400, 400, 437, 309, 775, 307, 309, 3719, 12736, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 534, "seek": 163308, "start": 1656.08, "end": 1659.1599999999999, "text": " And it's a mixture of randomly exploring and exploring", "tokens": [51514, 400, 309, 311, 257, 9925, 295, 16979, 12736, 293, 12736, 51668], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 535, "seek": 163308, "start": 1659.1599999999999, "end": 1661.72, "text": " using kind of some of the things that's figured out so far", "tokens": [51668, 1228, 733, 295, 512, 295, 264, 721, 300, 311, 8932, 484, 370, 1400, 51796], "temperature": 0.0, "avg_logprob": -0.09417785160125249, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.005219260696321726}, {"id": 536, "seek": 166172, "start": 1661.88, "end": 1664.0, "text": " to try to maximize its reward.", "tokens": [50372, 281, 853, 281, 19874, 1080, 7782, 13, 50478], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 537, "seek": 166172, "start": 1664.24, "end": 1667.04, "text": " So eventually, when the agent gets to the flag,", "tokens": [50490, 407, 4728, 11, 562, 264, 9461, 2170, 281, 264, 7166, 11, 50630], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 538, "seek": 166172, "start": 1667.1200000000001, "end": 1670.52, "text": " it will have the most the highest possible reward that it can have.", "tokens": [50634, 309, 486, 362, 264, 881, 264, 6343, 1944, 7782, 300, 309, 393, 362, 13, 50804], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 539, "seek": 166172, "start": 1670.84, "end": 1674.48, "text": " And then next time that we plug this agent into the environment,", "tokens": [50820, 400, 550, 958, 565, 300, 321, 5452, 341, 9461, 666, 264, 2823, 11, 51002], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 540, "seek": 166172, "start": 1674.64, "end": 1676.92, "text": " it will know how to get to the flag immediately", "tokens": [51010, 309, 486, 458, 577, 281, 483, 281, 264, 7166, 4258, 51124], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 541, "seek": 166172, "start": 1676.92, "end": 1678.6000000000001, "text": " because it's kind of figured that out.", "tokens": [51124, 570, 309, 311, 733, 295, 8932, 300, 484, 13, 51208], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 542, "seek": 166172, "start": 1678.6000000000001, "end": 1680.84, "text": " It's determined that in all of these different positions,", "tokens": [51208, 467, 311, 9540, 300, 294, 439, 295, 613, 819, 8432, 11, 51320], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 543, "seek": 166172, "start": 1680.84, "end": 1683.48, "text": " if I move here, this is the best place to move.", "tokens": [51320, 498, 286, 1286, 510, 11, 341, 307, 264, 1151, 1081, 281, 1286, 13, 51452], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 544, "seek": 166172, "start": 1683.48, "end": 1685.68, "text": " So if I get in this position, move there.", "tokens": [51452, 407, 498, 286, 483, 294, 341, 2535, 11, 1286, 456, 13, 51562], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 545, "seek": 166172, "start": 1686.08, "end": 1689.2, "text": " Now, this is, again, hard to explain without more detailed examples", "tokens": [51582, 823, 11, 341, 307, 11, 797, 11, 1152, 281, 2903, 1553, 544, 9942, 5110, 51738], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 546, "seek": 166172, "start": 1689.2, "end": 1691.2, "text": " and going more mathematically and all of that.", "tokens": [51738, 293, 516, 544, 44003, 293, 439, 295, 300, 13, 51838], "temperature": 0.0, "avg_logprob": -0.11753885713342117, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.01168474555015564}, {"id": 547, "seek": 169120, "start": 1691.2, "end": 1693.1200000000001, "text": " But essentially, just understand we have the agent,", "tokens": [50364, 583, 4476, 11, 445, 1223, 321, 362, 264, 9461, 11, 50460], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 548, "seek": 169120, "start": 1693.1200000000001, "end": 1697.32, "text": " which is kind of what the thing is that's moving around in our environment.", "tokens": [50460, 597, 307, 733, 295, 437, 264, 551, 307, 300, 311, 2684, 926, 294, 527, 2823, 13, 50670], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 549, "seek": 169120, "start": 1697.6000000000001, "end": 1701.88, "text": " We have this environment, which is just what the agent can move around in.", "tokens": [50684, 492, 362, 341, 2823, 11, 597, 307, 445, 437, 264, 9461, 393, 1286, 926, 294, 13, 50898], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 550, "seek": 169120, "start": 1702.0800000000002, "end": 1703.4, "text": " And then we have a reward.", "tokens": [50908, 400, 550, 321, 362, 257, 7782, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 551, "seek": 169120, "start": 1703.4, "end": 1706.16, "text": " And the reward is what we need to figure out as the programmer,", "tokens": [50974, 400, 264, 7782, 307, 437, 321, 643, 281, 2573, 484, 382, 264, 32116, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 552, "seek": 169120, "start": 1706.16, "end": 1709.92, "text": " a way to reward the agent correctly so that it gets to the objective", "tokens": [51112, 257, 636, 281, 7782, 264, 9461, 8944, 370, 300, 309, 2170, 281, 264, 10024, 51300], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 553, "seek": 169120, "start": 1710.24, "end": 1712.48, "text": " in the best possible way.", "tokens": [51316, 294, 264, 1151, 1944, 636, 13, 51428], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 554, "seek": 169120, "start": 1712.48, "end": 1715.04, "text": " But the agent simply maximizes that reward.", "tokens": [51428, 583, 264, 9461, 2935, 5138, 5660, 300, 7782, 13, 51556], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 555, "seek": 169120, "start": 1715.1200000000001, "end": 1717.76, "text": " So it just figures out where I need to go to maximize that reward.", "tokens": [51560, 407, 309, 445, 9624, 484, 689, 286, 643, 281, 352, 281, 19874, 300, 7782, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 556, "seek": 169120, "start": 1717.88, "end": 1721.0800000000002, "text": " It starts at the beginning, kind of randomly exploring the environment", "tokens": [51698, 467, 3719, 412, 264, 2863, 11, 733, 295, 16979, 12736, 264, 2823, 51858], "temperature": 0.0, "avg_logprob": -0.11470998939892925, "compression_ratio": 1.952054794520548, "no_speech_prob": 0.0004044442030135542}, {"id": 557, "seek": 172108, "start": 1721.08, "end": 1724.12, "text": " because it doesn't know any of the rewards it gets at any of the positions.", "tokens": [50364, 570, 309, 1177, 380, 458, 604, 295, 264, 17203, 309, 2170, 412, 604, 295, 264, 8432, 13, 50516], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 558, "seek": 172108, "start": 1724.36, "end": 1726.48, "text": " And then as it explores some more different areas,", "tokens": [50528, 400, 550, 382, 309, 45473, 512, 544, 819, 3179, 11, 50634], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 559, "seek": 172108, "start": 1726.48, "end": 1729.6799999999998, "text": " it kind of figures out the rules and the way that the environment works", "tokens": [50634, 309, 733, 295, 9624, 484, 264, 4474, 293, 264, 636, 300, 264, 2823, 1985, 50794], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 560, "seek": 172108, "start": 1729.8799999999999, "end": 1732.84, "text": " and then will determine how to reach the objective,", "tokens": [50804, 293, 550, 486, 6997, 577, 281, 2524, 264, 10024, 11, 50952], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 561, "seek": 172108, "start": 1732.84, "end": 1734.56, "text": " which is whatever it is that it is.", "tokens": [50952, 597, 307, 2035, 309, 307, 300, 309, 307, 13, 51038], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 562, "seek": 172108, "start": 1734.56, "end": 1735.72, "text": " This is a very simple example.", "tokens": [51038, 639, 307, 257, 588, 2199, 1365, 13, 51096], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 563, "seek": 172108, "start": 1735.72, "end": 1739.12, "text": " You could train a reinforcement model to do this and, you know, like half a second, right?", "tokens": [51096, 509, 727, 3847, 257, 29280, 2316, 281, 360, 341, 293, 11, 291, 458, 11, 411, 1922, 257, 1150, 11, 558, 30, 51266], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 564, "seek": 172108, "start": 1739.36, "end": 1741.3999999999999, "text": " But there is way more advanced examples", "tokens": [51278, 583, 456, 307, 636, 544, 7339, 5110, 51380], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 565, "seek": 172108, "start": 1741.3999999999999, "end": 1743.8799999999999, "text": " and there's been examples of reinforcement learning,", "tokens": [51380, 293, 456, 311, 668, 5110, 295, 29280, 2539, 11, 51504], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 566, "seek": 172108, "start": 1744.08, "end": 1747.76, "text": " like of AI is pretty much figuring out how to play games together.", "tokens": [51514, 411, 295, 7318, 307, 1238, 709, 15213, 484, 577, 281, 862, 2813, 1214, 13, 51698], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 567, "seek": 172108, "start": 1747.76, "end": 1749.1999999999998, "text": " How to it's it's actually pretty cool.", "tokens": [51698, 1012, 281, 309, 311, 309, 311, 767, 1238, 1627, 13, 51770], "temperature": 0.0, "avg_logprob": -0.12359978023328279, "compression_ratio": 1.7594202898550724, "no_speech_prob": 0.0005357457557693124}, {"id": 568, "seek": 174920, "start": 1749.2, "end": 1751.28, "text": " Some of the stuff that reinforcement learning is doing.", "tokens": [50364, 2188, 295, 264, 1507, 300, 29280, 2539, 307, 884, 13, 50468], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 569, "seek": 174920, "start": 1751.48, "end": 1754.24, "text": " And it's a really awesome kind of advancement in the field,", "tokens": [50478, 400, 309, 311, 257, 534, 3476, 733, 295, 35764, 294, 264, 2519, 11, 50616], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 570, "seek": 174920, "start": 1754.24, "end": 1756.4, "text": " because it means we don't need all this data anymore.", "tokens": [50616, 570, 309, 1355, 321, 500, 380, 643, 439, 341, 1412, 3602, 13, 50724], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 571, "seek": 174920, "start": 1756.6000000000001, "end": 1759.8, "text": " We can just get this to kind of figure out how to do things for us", "tokens": [50734, 492, 393, 445, 483, 341, 281, 733, 295, 2573, 484, 577, 281, 360, 721, 337, 505, 50894], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 572, "seek": 174920, "start": 1759.8, "end": 1761.88, "text": " and explore the environment and learn on its own.", "tokens": [50894, 293, 6839, 264, 2823, 293, 1466, 322, 1080, 1065, 13, 50998], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 573, "seek": 174920, "start": 1762.16, "end": 1763.76, "text": " Now, this can take a really long time.", "tokens": [51012, 823, 11, 341, 393, 747, 257, 534, 938, 565, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 574, "seek": 174920, "start": 1763.76, "end": 1766.8, "text": " This can take a very short amount of time, really depends on the environment.", "tokens": [51092, 639, 393, 747, 257, 588, 2099, 2372, 295, 565, 11, 534, 5946, 322, 264, 2823, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 575, "seek": 174920, "start": 1767.0, "end": 1770.28, "text": " But a real application of this is training AIs to play games,", "tokens": [51254, 583, 257, 957, 3861, 295, 341, 307, 3097, 316, 6802, 281, 862, 2813, 11, 51418], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 576, "seek": 174920, "start": 1770.28, "end": 1772.8400000000001, "text": " as you might be able to tell by kind of what I was explaining here.", "tokens": [51418, 382, 291, 1062, 312, 1075, 281, 980, 538, 733, 295, 437, 286, 390, 13468, 510, 13, 51546], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 577, "seek": 174920, "start": 1773.1200000000001, "end": 1775.8400000000001, "text": " And yeah, so that is kind of the fundamental differences", "tokens": [51560, 400, 1338, 11, 370, 300, 307, 733, 295, 264, 8088, 7300, 51696], "temperature": 0.0, "avg_logprob": -0.12055437723795573, "compression_ratio": 1.7717717717717718, "no_speech_prob": 0.004754723981022835}, {"id": 578, "seek": 177584, "start": 1775.84, "end": 1779.1599999999999, "text": " between supervised, unsupervised and reinforcement learning.", "tokens": [50364, 1296, 46533, 11, 2693, 12879, 24420, 293, 29280, 2539, 13, 50530], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 579, "seek": 177584, "start": 1779.1599999999999, "end": 1781.9599999999998, "text": " We're going to cover all three of these topics throughout this course.", "tokens": [50530, 492, 434, 516, 281, 2060, 439, 1045, 295, 613, 8378, 3710, 341, 1164, 13, 50670], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 580, "seek": 177584, "start": 1782.24, "end": 1785.56, "text": " And it's really interesting to see some of the applications we can actually do with this.", "tokens": [50684, 400, 309, 311, 534, 1880, 281, 536, 512, 295, 264, 5821, 321, 393, 767, 360, 365, 341, 13, 50850], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 581, "seek": 177584, "start": 1785.8, "end": 1789.24, "text": " So with that being said, I'm going to kind of end what I'm going to call module one,", "tokens": [50862, 407, 365, 300, 885, 848, 11, 286, 478, 516, 281, 733, 295, 917, 437, 286, 478, 516, 281, 818, 10088, 472, 11, 51034], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 582, "seek": 177584, "start": 1789.24, "end": 1792.1599999999999, "text": " which is just a general overview of the different topics,", "tokens": [51034, 597, 307, 445, 257, 2674, 12492, 295, 264, 819, 8378, 11, 51180], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 583, "seek": 177584, "start": 1792.1599999999999, "end": 1794.6399999999999, "text": " some definitions and getting a fundamental knowledge.", "tokens": [51180, 512, 21988, 293, 1242, 257, 8088, 3601, 13, 51304], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 584, "seek": 177584, "start": 1794.9199999999998, "end": 1798.8799999999999, "text": " And in the next one, what we're going to be talking about is what TensorFlow is.", "tokens": [51318, 400, 294, 264, 958, 472, 11, 437, 321, 434, 516, 281, 312, 1417, 466, 307, 437, 37624, 307, 13, 51516], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 585, "seek": 177584, "start": 1798.8799999999999, "end": 1801.8, "text": " We're going to get into coding a little bit and we're going to discuss", "tokens": [51516, 492, 434, 516, 281, 483, 666, 17720, 257, 707, 857, 293, 321, 434, 516, 281, 2248, 51662], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 586, "seek": 177584, "start": 1802.1599999999999, "end": 1804.6399999999999, "text": " some different aspects of TensorFlow and things we need to know", "tokens": [51680, 512, 819, 7270, 295, 37624, 293, 721, 321, 643, 281, 458, 51804], "temperature": 0.0, "avg_logprob": -0.09697654263285183, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.03408968821167946}, {"id": 587, "seek": 180464, "start": 1804.64, "end": 1806.68, "text": " to be able to move forward and do some more advanced things.", "tokens": [50364, 281, 312, 1075, 281, 1286, 2128, 293, 360, 512, 544, 7339, 721, 13, 50466], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 588, "seek": 180464, "start": 1809.8400000000001, "end": 1812.5600000000002, "text": " So now in module two of this course, what we're going to be doing", "tokens": [50624, 407, 586, 294, 10088, 732, 295, 341, 1164, 11, 437, 321, 434, 516, 281, 312, 884, 50760], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 589, "seek": 180464, "start": 1812.5600000000002, "end": 1816.76, "text": " is getting a general introduction to TensorFlow, understanding what a tensor is,", "tokens": [50760, 307, 1242, 257, 2674, 9339, 281, 37624, 11, 3701, 437, 257, 40863, 307, 11, 50970], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 590, "seek": 180464, "start": 1816.96, "end": 1819.2800000000002, "text": " understanding shapes and data representation,", "tokens": [50980, 3701, 10854, 293, 1412, 10290, 11, 51096], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 591, "seek": 180464, "start": 1819.2800000000002, "end": 1823.0, "text": " and then how TensorFlow actually works on a bit of a lower level.", "tokens": [51096, 293, 550, 577, 37624, 767, 1985, 322, 257, 857, 295, 257, 3126, 1496, 13, 51282], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 592, "seek": 180464, "start": 1823.24, "end": 1826.2800000000002, "text": " This is very important because you can definitely go through and learn", "tokens": [51294, 639, 307, 588, 1021, 570, 291, 393, 2138, 352, 807, 293, 1466, 51446], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 593, "seek": 180464, "start": 1826.2800000000002, "end": 1829.72, "text": " how to do machine learning without kind of gaining this information and knowledge.", "tokens": [51446, 577, 281, 360, 3479, 2539, 1553, 733, 295, 19752, 341, 1589, 293, 3601, 13, 51618], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 594, "seek": 180464, "start": 1829.96, "end": 1832.3200000000002, "text": " But it makes it a lot more difficult to tweak your models", "tokens": [51630, 583, 309, 1669, 309, 257, 688, 544, 2252, 281, 29879, 428, 5245, 51748], "temperature": 0.0, "avg_logprob": -0.08450003231272978, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.0009696450433693826}, {"id": 595, "seek": 183232, "start": 1832.32, "end": 1834.9199999999998, "text": " and really understand what's going on if you don't, you know,", "tokens": [50364, 293, 534, 1223, 437, 311, 516, 322, 498, 291, 500, 380, 11, 291, 458, 11, 50494], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 596, "seek": 183232, "start": 1834.9199999999998, "end": 1839.08, "text": " have that fundamental lower level knowledge of how TensorFlow actually works", "tokens": [50494, 362, 300, 8088, 3126, 1496, 3601, 295, 577, 37624, 767, 1985, 50702], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 597, "seek": 183232, "start": 1839.08, "end": 1841.76, "text": " and operates. So that's exactly what we're going to cover here.", "tokens": [50702, 293, 22577, 13, 407, 300, 311, 2293, 437, 321, 434, 516, 281, 2060, 510, 13, 50836], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 598, "seek": 183232, "start": 1842.08, "end": 1844.32, "text": " Now, for those of you that don't know what TensorFlow is,", "tokens": [50852, 823, 11, 337, 729, 295, 291, 300, 500, 380, 458, 437, 37624, 307, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 599, "seek": 183232, "start": 1844.32, "end": 1847.3999999999999, "text": " essentially, this is an open source machine learning library.", "tokens": [50964, 4476, 11, 341, 307, 364, 1269, 4009, 3479, 2539, 6405, 13, 51118], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 600, "seek": 183232, "start": 1847.56, "end": 1849.24, "text": " It's one of the largest ones in the world.", "tokens": [51126, 467, 311, 472, 295, 264, 6443, 2306, 294, 264, 1002, 13, 51210], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 601, "seek": 183232, "start": 1849.24, "end": 1853.36, "text": " It's one of the most well known and it's maintained and supported by Google.", "tokens": [51210, 467, 311, 472, 295, 264, 881, 731, 2570, 293, 309, 311, 17578, 293, 8104, 538, 3329, 13, 51416], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 602, "seek": 183232, "start": 1853.72, "end": 1858.4399999999998, "text": " Now, TensorFlow essentially allows us to do and create machine learning models", "tokens": [51434, 823, 11, 37624, 4476, 4045, 505, 281, 360, 293, 1884, 3479, 2539, 5245, 51670], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 603, "seek": 183232, "start": 1858.4399999999998, "end": 1861.6, "text": " and neural networks and all of that without having to have a very complex", "tokens": [51670, 293, 18161, 9590, 293, 439, 295, 300, 1553, 1419, 281, 362, 257, 588, 3997, 51828], "temperature": 0.0, "avg_logprob": -0.10584238742260223, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0028890862595289946}, {"id": 604, "seek": 186160, "start": 1861.6, "end": 1865.8, "text": " math background. Now, as we get further in and we start discussing more in detail,", "tokens": [50364, 5221, 3678, 13, 823, 11, 382, 321, 483, 3052, 294, 293, 321, 722, 10850, 544, 294, 2607, 11, 50574], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 605, "seek": 186160, "start": 1865.8, "end": 1868.84, "text": " how neural networks work in machine learning algorithms actually function,", "tokens": [50574, 577, 18161, 9590, 589, 294, 3479, 2539, 14642, 767, 2445, 11, 50726], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 606, "seek": 186160, "start": 1869.08, "end": 1871.56, "text": " you'll realize there's a lot of math that goes into this.", "tokens": [50738, 291, 603, 4325, 456, 311, 257, 688, 295, 5221, 300, 1709, 666, 341, 13, 50862], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 607, "seek": 186160, "start": 1871.9199999999998, "end": 1875.6399999999999, "text": " Now, it starts off being very kind of fundamental, like basic calculus", "tokens": [50880, 823, 11, 309, 3719, 766, 885, 588, 733, 295, 8088, 11, 411, 3875, 33400, 51066], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 608, "seek": 186160, "start": 1875.6399999999999, "end": 1879.36, "text": " and basic linear algebra, and then it gets much more advanced into things", "tokens": [51066, 293, 3875, 8213, 21989, 11, 293, 550, 309, 2170, 709, 544, 7339, 666, 721, 51252], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 609, "seek": 186160, "start": 1879.36, "end": 1883.28, "text": " like gradient descent and some more regression techniques and classification.", "tokens": [51252, 411, 16235, 23475, 293, 512, 544, 24590, 7512, 293, 21538, 13, 51448], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 610, "seek": 186160, "start": 1883.6799999999998, "end": 1888.08, "text": " And essentially, you know, a lot of us don't know that and we don't really need to know that.", "tokens": [51468, 400, 4476, 11, 291, 458, 11, 257, 688, 295, 505, 500, 380, 458, 300, 293, 321, 500, 380, 534, 643, 281, 458, 300, 13, 51688], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 611, "seek": 186160, "start": 1888.28, "end": 1890.56, "text": " So long as we have a basic understanding of it,", "tokens": [51698, 407, 938, 382, 321, 362, 257, 3875, 3701, 295, 309, 11, 51812], "temperature": 0.0, "avg_logprob": -0.13281613102665654, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0010986720444634557}, {"id": 612, "seek": 189056, "start": 1890.84, "end": 1894.56, "text": " then we can use the tools that TensorFlow provides for us to create models.", "tokens": [50378, 550, 321, 393, 764, 264, 3873, 300, 37624, 6417, 337, 505, 281, 1884, 5245, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 613, "seek": 189056, "start": 1894.56, "end": 1896.28, "text": " And that's exactly what TensorFlow does.", "tokens": [50564, 400, 300, 311, 2293, 437, 37624, 775, 13, 50650], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 614, "seek": 189056, "start": 1896.6399999999999, "end": 1900.08, "text": " Now, what I'm in right now is what I call Google Collaboratory.", "tokens": [50668, 823, 11, 437, 286, 478, 294, 558, 586, 307, 437, 286, 818, 3329, 44483, 4745, 13, 50840], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 615, "seek": 189056, "start": 1900.1599999999999, "end": 1902.08, "text": " I'm going to talk about this more in depth in a second.", "tokens": [50844, 286, 478, 516, 281, 751, 466, 341, 544, 294, 7161, 294, 257, 1150, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 616, "seek": 189056, "start": 1902.08, "end": 1906.6799999999998, "text": " But what I've done for this whole course is I've transcribed very detailed", "tokens": [50940, 583, 437, 286, 600, 1096, 337, 341, 1379, 1164, 307, 286, 600, 1145, 18732, 588, 9942, 51170], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 617, "seek": 189056, "start": 1907.0, "end": 1909.44, "text": " everything that I'm going to be covering through each module.", "tokens": [51186, 1203, 300, 286, 478, 516, 281, 312, 10322, 807, 1184, 10088, 13, 51308], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 618, "seek": 189056, "start": 1909.6, "end": 1914.6, "text": " So this is kind of the transcription of module one, which is the introduction to TensorFlow.", "tokens": [51316, 407, 341, 307, 733, 295, 264, 35288, 295, 10088, 472, 11, 597, 307, 264, 9339, 281, 37624, 13, 51566], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 619, "seek": 189056, "start": 1914.84, "end": 1918.2, "text": " You can see it's not crazy long, but I wanted to do this so that any of you", "tokens": [51578, 509, 393, 536, 309, 311, 406, 3219, 938, 11, 457, 286, 1415, 281, 360, 341, 370, 300, 604, 295, 291, 51746], "temperature": 0.0, "avg_logprob": -0.12231626301786325, "compression_ratio": 1.7261146496815287, "no_speech_prob": 0.0043309456668794155}, {"id": 620, "seek": 191820, "start": 1918.24, "end": 1922.64, "text": " can follow along with kind of the text base and kind of my lecture notes.", "tokens": [50366, 393, 1524, 2051, 365, 733, 295, 264, 2487, 3096, 293, 733, 295, 452, 7991, 5570, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 621, "seek": 191820, "start": 1922.64, "end": 1925.04, "text": " I almost want to call them as I go through the different content.", "tokens": [50586, 286, 1920, 528, 281, 818, 552, 382, 286, 352, 807, 264, 819, 2701, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 622, "seek": 191820, "start": 1925.32, "end": 1928.72, "text": " So in the description, there will be links to all of these different notebooks.", "tokens": [50720, 407, 294, 264, 3855, 11, 456, 486, 312, 6123, 281, 439, 295, 613, 819, 43782, 13, 50890], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 623, "seek": 191820, "start": 1928.76, "end": 1932.52, "text": " This is in something called Google Collaboratory, which again, we're going to discuss in a second.", "tokens": [50892, 639, 307, 294, 746, 1219, 3329, 44483, 4745, 11, 597, 797, 11, 321, 434, 516, 281, 2248, 294, 257, 1150, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 624, "seek": 191820, "start": 1932.8, "end": 1937.24, "text": " But you can see here that I have a bunch of text and it gets down to some different coding aspects.", "tokens": [51094, 583, 291, 393, 536, 510, 300, 286, 362, 257, 3840, 295, 2487, 293, 309, 2170, 760, 281, 512, 819, 17720, 7270, 13, 51316], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 625, "seek": 191820, "start": 1937.48, "end": 1941.16, "text": " And what I'm going to be doing to make sure that I stay on track is simply following along", "tokens": [51328, 400, 437, 286, 478, 516, 281, 312, 884, 281, 652, 988, 300, 286, 1754, 322, 2837, 307, 2935, 3480, 2051, 51512], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 626, "seek": 191820, "start": 1941.16, "end": 1944.48, "text": " through this, I might deviate slightly, I might go into some other examples.", "tokens": [51512, 807, 341, 11, 286, 1062, 1905, 13024, 4748, 11, 286, 1062, 352, 666, 512, 661, 5110, 13, 51678], "temperature": 0.0, "avg_logprob": -0.1199597372135646, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.013220271095633507}, {"id": 627, "seek": 194448, "start": 1944.72, "end": 1948.24, "text": " This will be kind of everything that I'm going to be covering through each module.", "tokens": [50376, 639, 486, 312, 733, 295, 1203, 300, 286, 478, 516, 281, 312, 10322, 807, 1184, 10088, 13, 50552], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 628, "seek": 194448, "start": 1948.64, "end": 1951.72, "text": " So again, to follow along, click the link in the description.", "tokens": [50572, 407, 797, 11, 281, 1524, 2051, 11, 2052, 264, 2113, 294, 264, 3855, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 629, "seek": 194448, "start": 1952.28, "end": 1955.16, "text": " All right. So what can we do with TensorFlow?", "tokens": [50754, 1057, 558, 13, 407, 437, 393, 321, 360, 365, 37624, 30, 50898], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 630, "seek": 194448, "start": 1955.44, "end": 1957.84, "text": " Well, these are some of the different things I've listed them here.", "tokens": [50912, 1042, 11, 613, 366, 512, 295, 264, 819, 721, 286, 600, 10052, 552, 510, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 631, "seek": 194448, "start": 1957.84, "end": 1962.32, "text": " So I don't forget we can do image classification, data clustering, regression,", "tokens": [51032, 407, 286, 500, 380, 2870, 321, 393, 360, 3256, 21538, 11, 1412, 596, 48673, 11, 24590, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 632, "seek": 194448, "start": 1962.64, "end": 1966.56, "text": " reinforcement learning, natural language processing, and pretty much anything", "tokens": [51272, 29280, 2539, 11, 3303, 2856, 9007, 11, 293, 1238, 709, 1340, 51468], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 633, "seek": 194448, "start": 1966.56, "end": 1968.64, "text": " that you can imagine with machine learning.", "tokens": [51468, 300, 291, 393, 3811, 365, 3479, 2539, 13, 51572], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 634, "seek": 194448, "start": 1969.28, "end": 1972.76, "text": " Essentially, what TensorFlow does is gives us a library of tools", "tokens": [51604, 23596, 11, 437, 37624, 775, 307, 2709, 505, 257, 6405, 295, 3873, 51778], "temperature": 0.0, "avg_logprob": -0.1211480629153368, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.05498671159148216}, {"id": 635, "seek": 197276, "start": 1972.8799999999999, "end": 1976.8, "text": " that allow us to omit having to do these very complicated math operations.", "tokens": [50370, 300, 2089, 505, 281, 3406, 270, 1419, 281, 360, 613, 588, 6179, 5221, 7705, 13, 50566], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 636, "seek": 197276, "start": 1977.28, "end": 1978.72, "text": " It just does them for us.", "tokens": [50590, 467, 445, 775, 552, 337, 505, 13, 50662], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 637, "seek": 197276, "start": 1978.72, "end": 1981.68, "text": " Now, there is a bit that we need to know about them, but nothing too complex.", "tokens": [50662, 823, 11, 456, 307, 257, 857, 300, 321, 643, 281, 458, 466, 552, 11, 457, 1825, 886, 3997, 13, 50810], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 638, "seek": 197276, "start": 1982.2, "end": 1985.16, "text": " Now, let's talk about how TensorFlow actually works.", "tokens": [50836, 823, 11, 718, 311, 751, 466, 577, 37624, 767, 1985, 13, 50984], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 639, "seek": 197276, "start": 1985.64, "end": 1989.4, "text": " So TensorFlow has two main components that we need to understand", "tokens": [51008, 407, 37624, 575, 732, 2135, 6677, 300, 321, 643, 281, 1223, 51196], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 640, "seek": 197276, "start": 1989.76, "end": 1992.84, "text": " to figure out how operations and math are actually performed.", "tokens": [51214, 281, 2573, 484, 577, 7705, 293, 5221, 366, 767, 10332, 13, 51368], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 641, "seek": 197276, "start": 1993.08, "end": 1995.72, "text": " Now, we have something called graphs and sessions.", "tokens": [51380, 823, 11, 321, 362, 746, 1219, 24877, 293, 11081, 13, 51512], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 642, "seek": 197276, "start": 1996.2, "end": 2001.8799999999999, "text": " Now, the way that TensorFlow works is it creates a graph of partial computations.", "tokens": [51536, 823, 11, 264, 636, 300, 37624, 1985, 307, 309, 7829, 257, 4295, 295, 14641, 2807, 763, 13, 51820], "temperature": 0.0, "avg_logprob": -0.09535875240293872, "compression_ratio": 1.825278810408922, "no_speech_prob": 0.0028891577385365963}, {"id": 643, "seek": 200188, "start": 2002.2, "end": 2004.2, "text": " Now, I know this is going to sound a little bit complicated.", "tokens": [50380, 823, 11, 286, 458, 341, 307, 516, 281, 1626, 257, 707, 857, 6179, 13, 50480], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 644, "seek": 200188, "start": 2004.2, "end": 2007.72, "text": " Some of you guys just try to kind of forget about the complex vocabulary", "tokens": [50480, 2188, 295, 291, 1074, 445, 853, 281, 733, 295, 2870, 466, 264, 3997, 19864, 50656], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 645, "seek": 200188, "start": 2007.72, "end": 2012.2, "text": " and follow along. But essentially, what we do when we write code in TensorFlow", "tokens": [50656, 293, 1524, 2051, 13, 583, 4476, 11, 437, 321, 360, 562, 321, 2464, 3089, 294, 37624, 50880], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 646, "seek": 200188, "start": 2012.2, "end": 2014.0400000000002, "text": " is we create a graph.", "tokens": [50880, 307, 321, 1884, 257, 4295, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 647, "seek": 200188, "start": 2014.0400000000002, "end": 2018.1200000000001, "text": " So if I were to create some variable, that variable gets added to the graph.", "tokens": [50972, 407, 498, 286, 645, 281, 1884, 512, 7006, 11, 300, 7006, 2170, 3869, 281, 264, 4295, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 648, "seek": 200188, "start": 2018.4, "end": 2022.68, "text": " And maybe that variable is the sum or the summation of two other variables.", "tokens": [51190, 400, 1310, 300, 7006, 307, 264, 2408, 420, 264, 28811, 295, 732, 661, 9102, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 649, "seek": 200188, "start": 2023.0, "end": 2026.4, "text": " What the graph will define now is say, you know, we have variable one,", "tokens": [51420, 708, 264, 4295, 486, 6964, 586, 307, 584, 11, 291, 458, 11, 321, 362, 7006, 472, 11, 51590], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 650, "seek": 200188, "start": 2026.7600000000002, "end": 2031.0, "text": " which is equal to the sum of variable two and variable three.", "tokens": [51608, 597, 307, 2681, 281, 264, 2408, 295, 7006, 732, 293, 7006, 1045, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1042421634380634, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.0013668359024450183}, {"id": 651, "seek": 203100, "start": 2031.56, "end": 2035.56, "text": " But what we need to understand is that it doesn't actually evaluate that.", "tokens": [50392, 583, 437, 321, 643, 281, 1223, 307, 300, 309, 1177, 380, 767, 13059, 300, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 652, "seek": 203100, "start": 2035.56, "end": 2039.36, "text": " It simply states that that is the computation that we've defined.", "tokens": [50592, 467, 2935, 4368, 300, 300, 307, 264, 24903, 300, 321, 600, 7642, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 653, "seek": 203100, "start": 2039.76, "end": 2044.32, "text": " So it's almost like writing down an equation without actually performing any math.", "tokens": [50802, 407, 309, 311, 1920, 411, 3579, 760, 364, 5367, 1553, 767, 10205, 604, 5221, 13, 51030], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 654, "seek": 203100, "start": 2044.56, "end": 2047.24, "text": " We kind of just, you know, have that equation there.", "tokens": [51042, 492, 733, 295, 445, 11, 291, 458, 11, 362, 300, 5367, 456, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 655, "seek": 203100, "start": 2047.44, "end": 2050.84, "text": " We know that this is the value, but we haven't evaluated it.", "tokens": [51186, 492, 458, 300, 341, 307, 264, 2158, 11, 457, 321, 2378, 380, 25509, 309, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 656, "seek": 203100, "start": 2050.84, "end": 2053.28, "text": " So we don't know that the value is like seven per se.", "tokens": [51356, 407, 321, 500, 380, 458, 300, 264, 2158, 307, 411, 3407, 680, 369, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 657, "seek": 203100, "start": 2053.44, "end": 2057.2, "text": " We just know that it's the sum of, you know, vector one and vector two.", "tokens": [51486, 492, 445, 458, 300, 309, 311, 264, 2408, 295, 11, 291, 458, 11, 8062, 472, 293, 8062, 732, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 658, "seek": 203100, "start": 2057.24, "end": 2060.84, "text": " Or it's the sum of this or it's the cross product or the dot product.", "tokens": [51676, 1610, 309, 311, 264, 2408, 295, 341, 420, 309, 311, 264, 3278, 1674, 420, 264, 5893, 1674, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1103771618434361, "compression_ratio": 1.9275362318840579, "no_speech_prob": 0.0010648764437064528}, {"id": 659, "seek": 206084, "start": 2060.84, "end": 2064.08, "text": " We just define all of the different partial computations", "tokens": [50364, 492, 445, 6964, 439, 295, 264, 819, 14641, 2807, 763, 50526], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 660, "seek": 206084, "start": 2064.32, "end": 2066.6800000000003, "text": " because we haven't evaluated those computation yet.", "tokens": [50538, 570, 321, 2378, 380, 25509, 729, 24903, 1939, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 661, "seek": 206084, "start": 2066.88, "end": 2068.84, "text": " And that is what is stored in the graph.", "tokens": [50666, 400, 300, 307, 437, 307, 12187, 294, 264, 4295, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 662, "seek": 206084, "start": 2069.6800000000003, "end": 2072.4, "text": " Now, the reason it's called a graph is because different", "tokens": [50806, 823, 11, 264, 1778, 309, 311, 1219, 257, 4295, 307, 570, 819, 50942], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 663, "seek": 206084, "start": 2072.4, "end": 2074.6000000000004, "text": " computations can be related to each other.", "tokens": [50942, 2807, 763, 393, 312, 4077, 281, 1184, 661, 13, 51052], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 664, "seek": 206084, "start": 2074.92, "end": 2078.7200000000003, "text": " For example, if I want to figure out the value of vector one,", "tokens": [51068, 1171, 1365, 11, 498, 286, 528, 281, 2573, 484, 264, 2158, 295, 8062, 472, 11, 51258], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 665, "seek": 206084, "start": 2078.92, "end": 2082.8, "text": " but vector one is equal to the value of vector three plus vector four,", "tokens": [51268, 457, 8062, 472, 307, 2681, 281, 264, 2158, 295, 8062, 1045, 1804, 8062, 1451, 11, 51462], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 666, "seek": 206084, "start": 2083.0, "end": 2085.84, "text": " I need to determine the value of vector three and vector four", "tokens": [51472, 286, 643, 281, 6997, 264, 2158, 295, 8062, 1045, 293, 8062, 1451, 51614], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 667, "seek": 206084, "start": 2086.32, "end": 2088.2400000000002, "text": " before I can do that computation.", "tokens": [51638, 949, 286, 393, 360, 300, 24903, 13, 51734], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 668, "seek": 206084, "start": 2088.2400000000002, "end": 2089.8, "text": " So they're kind of linked together.", "tokens": [51734, 407, 436, 434, 733, 295, 9408, 1214, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11009985162306202, "compression_ratio": 1.8759124087591241, "no_speech_prob": 0.0006461674929596484}, {"id": 669, "seek": 208980, "start": 2089.8, "end": 2091.84, "text": " And I hope that makes a little bit of sense.", "tokens": [50364, 400, 286, 1454, 300, 1669, 257, 707, 857, 295, 2020, 13, 50466], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 670, "seek": 208980, "start": 2092.44, "end": 2094.2000000000003, "text": " Now, what is a session?", "tokens": [50496, 823, 11, 437, 307, 257, 5481, 30, 50584], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 671, "seek": 208980, "start": 2094.2000000000003, "end": 2098.92, "text": " Well, session is essentially a way to execute part or the entire graph.", "tokens": [50584, 1042, 11, 5481, 307, 4476, 257, 636, 281, 14483, 644, 420, 264, 2302, 4295, 13, 50820], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 672, "seek": 208980, "start": 2099.28, "end": 2103.84, "text": " So when we start a session, what we do is we start executing different aspects", "tokens": [50838, 407, 562, 321, 722, 257, 5481, 11, 437, 321, 360, 307, 321, 722, 32368, 819, 7270, 51066], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 673, "seek": 208980, "start": 2103.84, "end": 2106.96, "text": " of the graph. So we start at the lowest level of the graph", "tokens": [51066, 295, 264, 4295, 13, 407, 321, 722, 412, 264, 12437, 1496, 295, 264, 4295, 51222], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 674, "seek": 208980, "start": 2106.96, "end": 2108.88, "text": " where nothing is dependent on anything else.", "tokens": [51222, 689, 1825, 307, 12334, 322, 1340, 1646, 13, 51318], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 675, "seek": 208980, "start": 2108.88, "end": 2112.04, "text": " We have maybe constant values or something like that.", "tokens": [51318, 492, 362, 1310, 5754, 4190, 420, 746, 411, 300, 13, 51476], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 676, "seek": 208980, "start": 2112.28, "end": 2114.36, "text": " And then we move our way through the graph", "tokens": [51488, 400, 550, 321, 1286, 527, 636, 807, 264, 4295, 51592], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 677, "seek": 208980, "start": 2114.36, "end": 2117.96, "text": " and start doing all of the different partial computations that we've defined.", "tokens": [51592, 293, 722, 884, 439, 295, 264, 819, 14641, 2807, 763, 300, 321, 600, 7642, 13, 51772], "temperature": 0.0, "avg_logprob": -0.122748779296875, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0013248729519546032}, {"id": 678, "seek": 211796, "start": 2118.52, "end": 2120.2, "text": " Now, I hope that this isn't too confusing.", "tokens": [50392, 823, 11, 286, 1454, 300, 341, 1943, 380, 886, 13181, 13, 50476], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 679, "seek": 211796, "start": 2120.2, "end": 2121.64, "text": " I know this is kind of a lot of lingo.", "tokens": [50476, 286, 458, 341, 307, 733, 295, 257, 688, 295, 287, 18459, 13, 50548], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 680, "seek": 211796, "start": 2121.64, "end": 2123.64, "text": " You guys will understand this as we go through.", "tokens": [50548, 509, 1074, 486, 1223, 341, 382, 321, 352, 807, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 681, "seek": 211796, "start": 2123.84, "end": 2126.28, "text": " And again, you can read through some of these components here", "tokens": [50658, 400, 797, 11, 291, 393, 1401, 807, 512, 295, 613, 6677, 510, 50780], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 682, "seek": 211796, "start": 2126.28, "end": 2129.0, "text": " that I have in collaboratory, if I'm kind of skipping through anything,", "tokens": [50780, 300, 286, 362, 294, 5091, 4745, 11, 498, 286, 478, 733, 295, 31533, 807, 1340, 11, 50916], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 683, "seek": 211796, "start": 2129.0, "end": 2131.08, "text": " or you don't truly understand.", "tokens": [50916, 420, 291, 500, 380, 4908, 1223, 13, 51020], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 684, "seek": 211796, "start": 2131.32, "end": 2133.52, "text": " But that is the way that graphs and sessions work.", "tokens": [51032, 583, 300, 307, 264, 636, 300, 24877, 293, 11081, 589, 13, 51142], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 685, "seek": 211796, "start": 2133.88, "end": 2135.48, "text": " We won't go too in depth with them.", "tokens": [51160, 492, 1582, 380, 352, 886, 294, 7161, 365, 552, 13, 51240], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 686, "seek": 211796, "start": 2135.48, "end": 2138.16, "text": " We do need to understand that that is the way TensorFlow works.", "tokens": [51240, 492, 360, 643, 281, 1223, 300, 300, 307, 264, 636, 37624, 1985, 13, 51374], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 687, "seek": 211796, "start": 2138.16, "end": 2142.48, "text": " And there's some times where we can't use a specific value in our code yet", "tokens": [51374, 400, 456, 311, 512, 1413, 689, 321, 393, 380, 764, 257, 2685, 2158, 294, 527, 3089, 1939, 51590], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 688, "seek": 211796, "start": 2142.68, "end": 2145.0, "text": " because we haven't evaluated the graph.", "tokens": [51600, 570, 321, 2378, 380, 25509, 264, 4295, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 689, "seek": 211796, "start": 2145.0, "end": 2147.64, "text": " We haven't created a session and gotten the values yet.", "tokens": [51716, 492, 2378, 380, 2942, 257, 5481, 293, 5768, 264, 4190, 1939, 13, 51848], "temperature": 0.0, "avg_logprob": -0.12409641381466027, "compression_ratio": 1.8011695906432748, "no_speech_prob": 0.001987556926906109}, {"id": 690, "seek": 214764, "start": 2147.8399999999997, "end": 2150.2, "text": " Which we might need to do before we can actually, you know,", "tokens": [50374, 3013, 321, 1062, 643, 281, 360, 949, 321, 393, 767, 11, 291, 458, 11, 50492], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 691, "seek": 214764, "start": 2150.2, "end": 2151.7999999999997, "text": " use some specific value.", "tokens": [50492, 764, 512, 2685, 2158, 13, 50572], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 692, "seek": 214764, "start": 2151.7999999999997, "end": 2153.7999999999997, "text": " So that's just something to consider.", "tokens": [50572, 407, 300, 311, 445, 746, 281, 1949, 13, 50672], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 693, "seek": 214764, "start": 2153.7999999999997, "end": 2156.16, "text": " All right, so now we're actually going to get into coding,", "tokens": [50672, 1057, 558, 11, 370, 586, 321, 434, 767, 516, 281, 483, 666, 17720, 11, 50790], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 694, "seek": 214764, "start": 2156.16, "end": 2158.56, "text": " importing and installing TensorFlow.", "tokens": [50790, 43866, 293, 20762, 37624, 13, 50910], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 695, "seek": 214764, "start": 2158.8799999999997, "end": 2161.72, "text": " Now, this is where I'm going to introduce you to Google Collaboratory", "tokens": [50926, 823, 11, 341, 307, 689, 286, 478, 516, 281, 5366, 291, 281, 3329, 44483, 4745, 51068], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 696, "seek": 214764, "start": 2161.72, "end": 2163.52, "text": " and explain how you guys can follow along", "tokens": [51068, 293, 2903, 577, 291, 1074, 393, 1524, 2051, 51158], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 697, "seek": 214764, "start": 2163.52, "end": 2166.3599999999997, "text": " without having to install anything on your computer.", "tokens": [51158, 1553, 1419, 281, 3625, 1340, 322, 428, 3820, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 698, "seek": 214764, "start": 2166.52, "end": 2169.16, "text": " And it doesn't matter if you have like a really crappy computer", "tokens": [51308, 400, 309, 1177, 380, 1871, 498, 291, 362, 411, 257, 534, 36531, 3820, 51440], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 699, "seek": 214764, "start": 2169.16, "end": 2172.56, "text": " or even if you're on like an iPhone per se, you can actually do this,", "tokens": [51440, 420, 754, 498, 291, 434, 322, 411, 364, 7252, 680, 369, 11, 291, 393, 767, 360, 341, 11, 51610], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 700, "seek": 214764, "start": 2172.56, "end": 2173.7599999999998, "text": " which is amazing.", "tokens": [51610, 597, 307, 2243, 13, 51670], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 701, "seek": 214764, "start": 2173.7599999999998, "end": 2177.12, "text": " So all you need to do is Google, Google Collaboratory", "tokens": [51670, 407, 439, 291, 643, 281, 360, 307, 3329, 11, 3329, 44483, 4745, 51838], "temperature": 0.0, "avg_logprob": -0.11757077602361211, "compression_ratio": 1.7582089552238807, "no_speech_prob": 0.0010648665484040976}, {"id": 702, "seek": 217712, "start": 2177.3599999999997, "end": 2179.6, "text": " and create a new notebook.", "tokens": [50376, 293, 1884, 257, 777, 21060, 13, 50488], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 703, "seek": 217712, "start": 2179.6, "end": 2182.72, "text": " Now, what Google Collaboratory is, is essentially a free", "tokens": [50488, 823, 11, 437, 3329, 44483, 4745, 307, 11, 307, 4476, 257, 1737, 50644], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 704, "seek": 217712, "start": 2182.72, "end": 2185.04, "text": " Jupyter notebook in the cloud for you.", "tokens": [50644, 22125, 88, 391, 21060, 294, 264, 4588, 337, 291, 13, 50760], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 705, "seek": 217712, "start": 2185.3199999999997, "end": 2187.92, "text": " The way this works is you can open up this notebook.", "tokens": [50774, 440, 636, 341, 1985, 307, 291, 393, 1269, 493, 341, 21060, 13, 50904], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 706, "seek": 217712, "start": 2187.92, "end": 2190.48, "text": " You can see this is called I pi NB.", "tokens": [50904, 509, 393, 536, 341, 307, 1219, 286, 3895, 426, 33, 13, 51032], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 707, "seek": 217712, "start": 2191.2, "end": 2192.12, "text": " I yeah, what is that?", "tokens": [51068, 286, 1338, 11, 437, 307, 300, 30, 51114], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 708, "seek": 217712, "start": 2192.12, "end": 2195.4, "text": " I pi NB, which I think just stands for I Python notebook.", "tokens": [51114, 286, 3895, 426, 33, 11, 597, 286, 519, 445, 7382, 337, 286, 15329, 21060, 13, 51278], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 709, "seek": 217712, "start": 2195.68, "end": 2199.7599999999998, "text": " And what you can do in here is actually write code and write text as well.", "tokens": [51292, 400, 437, 291, 393, 360, 294, 510, 307, 767, 2464, 3089, 293, 2464, 2487, 382, 731, 13, 51496], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 710, "seek": 217712, "start": 2200.04, "end": 2203.7599999999998, "text": " So this in here is what it's called, you know, Google Collaboratory Notebook.", "tokens": [51510, 407, 341, 294, 510, 307, 437, 309, 311, 1219, 11, 291, 458, 11, 3329, 44483, 4745, 11633, 2939, 13, 51696], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 711, "seek": 217712, "start": 2203.96, "end": 2205.92, "text": " And essentially why it's called a notebook", "tokens": [51706, 400, 4476, 983, 309, 311, 1219, 257, 21060, 51804], "temperature": 0.0, "avg_logprob": -0.20797423493090295, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004068619571626186}, {"id": 712, "seek": 220592, "start": 2205.92, "end": 2209.32, "text": " is because not only can you put code, but you can also put notes,", "tokens": [50364, 307, 570, 406, 787, 393, 291, 829, 3089, 11, 457, 291, 393, 611, 829, 5570, 11, 50534], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 713, "seek": 220592, "start": 2209.32, "end": 2212.4, "text": " which is what I've done here with these specific titles.", "tokens": [50534, 597, 307, 437, 286, 600, 1096, 510, 365, 613, 2685, 12992, 13, 50688], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 714, "seek": 220592, "start": 2212.6800000000003, "end": 2215.28, "text": " So you can actually use Markdown inside of this.", "tokens": [50702, 407, 291, 393, 767, 764, 3934, 5093, 1854, 295, 341, 13, 50832], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 715, "seek": 220592, "start": 2215.28, "end": 2218.7200000000003, "text": " So if I open up one of these, you can see that I've used Markdown text", "tokens": [50832, 407, 498, 286, 1269, 493, 472, 295, 613, 11, 291, 393, 536, 300, 286, 600, 1143, 3934, 5093, 2487, 51004], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 716, "seek": 220592, "start": 2219.28, "end": 2221.28, "text": " to actually kind of create these sections.", "tokens": [51032, 281, 767, 733, 295, 1884, 613, 10863, 13, 51132], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 717, "seek": 220592, "start": 2221.64, "end": 2224.76, "text": " And yeah, that is kind of how Collaboratory works.", "tokens": [51150, 400, 1338, 11, 300, 307, 733, 295, 577, 44483, 4745, 1985, 13, 51306], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 718, "seek": 220592, "start": 2225.04, "end": 2228.6800000000003, "text": " But what you can do in Collaboratory is forget about having to install", "tokens": [51320, 583, 437, 291, 393, 360, 294, 44483, 4745, 307, 2870, 466, 1419, 281, 3625, 51502], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 719, "seek": 220592, "start": 2228.6800000000003, "end": 2231.84, "text": " all of these modules, they're already installed for you.", "tokens": [51502, 439, 295, 613, 16679, 11, 436, 434, 1217, 8899, 337, 291, 13, 51660], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 720, "seek": 220592, "start": 2232.08, "end": 2235.4, "text": " So what you're actually going to do when you open a Collaboratory window", "tokens": [51672, 407, 437, 291, 434, 767, 516, 281, 360, 562, 291, 1269, 257, 44483, 4745, 4910, 51838], "temperature": 0.0, "avg_logprob": -0.10005432238681711, "compression_ratio": 1.8842105263157896, "no_speech_prob": 0.002115479903295636}, {"id": 721, "seek": 223540, "start": 2235.4, "end": 2238.6800000000003, "text": " is Google is going to automatically connect you to one of their servers", "tokens": [50364, 307, 3329, 307, 516, 281, 6772, 1745, 291, 281, 472, 295, 641, 15909, 50528], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 722, "seek": 223540, "start": 2238.6800000000003, "end": 2242.6800000000003, "text": " or one of their machines that has all of this stuff done and set up for you.", "tokens": [50528, 420, 472, 295, 641, 8379, 300, 575, 439, 295, 341, 1507, 1096, 293, 992, 493, 337, 291, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 723, "seek": 223540, "start": 2242.88, "end": 2246.2000000000003, "text": " And you can start writing code and executing it off their machine", "tokens": [50738, 400, 291, 393, 722, 3579, 3089, 293, 32368, 309, 766, 641, 3479, 50904], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 724, "seek": 223540, "start": 2246.2000000000003, "end": 2247.8, "text": " and seeing the result.", "tokens": [50904, 293, 2577, 264, 1874, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 725, "seek": 223540, "start": 2247.8, "end": 2251.76, "text": " So for example, if I want to print hello like this,", "tokens": [50984, 407, 337, 1365, 11, 498, 286, 528, 281, 4482, 7751, 411, 341, 11, 51182], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 726, "seek": 223540, "start": 2251.76, "end": 2253.96, "text": " and I'll zoom in a little bit so you guys can read this.", "tokens": [51182, 293, 286, 603, 8863, 294, 257, 707, 857, 370, 291, 1074, 393, 1401, 341, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 727, "seek": 223540, "start": 2253.96, "end": 2257.88, "text": " All I do is I create a new code block, which I can do by clicking code.", "tokens": [51292, 1057, 286, 360, 307, 286, 1884, 257, 777, 3089, 3461, 11, 597, 286, 393, 360, 538, 9697, 3089, 13, 51488], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 728, "seek": 223540, "start": 2258.48, "end": 2260.8, "text": " Like that, I can delete one like that as well.", "tokens": [51518, 1743, 300, 11, 286, 393, 12097, 472, 411, 300, 382, 731, 13, 51634], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 729, "seek": 223540, "start": 2260.8, "end": 2262.92, "text": " And I hit run.", "tokens": [51634, 400, 286, 2045, 1190, 13, 51740], "temperature": 0.0, "avg_logprob": -0.1269459868922378, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0019876090809702873}, {"id": 730, "seek": 226292, "start": 2262.92, "end": 2266.04, "text": " Now notice, give it a second, it does take longer than typically on your own", "tokens": [50364, 823, 3449, 11, 976, 309, 257, 1150, 11, 309, 775, 747, 2854, 813, 5850, 322, 428, 1065, 50520], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 731, "seek": 226292, "start": 2266.04, "end": 2268.7200000000003, "text": " machine, and we get hello popping up here.", "tokens": [50520, 3479, 11, 293, 321, 483, 7751, 18374, 493, 510, 13, 50654], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 732, "seek": 226292, "start": 2269.0, "end": 2272.36, "text": " So the great thing about Collaboratory is the fact that we can have multiple", "tokens": [50668, 407, 264, 869, 551, 466, 44483, 4745, 307, 264, 1186, 300, 321, 393, 362, 3866, 50836], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 733, "seek": 226292, "start": 2272.36, "end": 2275.76, "text": " code blocks and we can run them in whatever sequence we want.", "tokens": [50836, 3089, 8474, 293, 321, 393, 1190, 552, 294, 2035, 8310, 321, 528, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 734, "seek": 226292, "start": 2276.04, "end": 2278.6800000000003, "text": " So to create another code block, you can just, you know, do another", "tokens": [51020, 407, 281, 1884, 1071, 3089, 3461, 11, 291, 393, 445, 11, 291, 458, 11, 360, 1071, 51152], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 735, "seek": 226292, "start": 2278.6800000000003, "end": 2281.84, "text": " code block from up here or by just by looking down here, you get code", "tokens": [51152, 3089, 3461, 490, 493, 510, 420, 538, 445, 538, 1237, 760, 510, 11, 291, 483, 3089, 51310], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 736, "seek": 226292, "start": 2281.84, "end": 2284.8, "text": " and you get text and I can run this in whatever order I want.", "tokens": [51310, 293, 291, 483, 2487, 293, 286, 393, 1190, 341, 294, 2035, 1668, 286, 528, 13, 51458], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 737, "seek": 226292, "start": 2284.8, "end": 2286.84, "text": " So I could do like print.", "tokens": [51458, 407, 286, 727, 360, 411, 4482, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 738, "seek": 226292, "start": 2286.84, "end": 2290.6800000000003, "text": " Yes, for example, I could run yes, and we'll see the output of yes.", "tokens": [51560, 1079, 11, 337, 1365, 11, 286, 727, 1190, 2086, 11, 293, 321, 603, 536, 264, 5598, 295, 2086, 13, 51752], "temperature": 0.0, "avg_logprob": -0.14204841046719938, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.0021155388094484806}, {"id": 739, "seek": 229068, "start": 2290.68, "end": 2294.04, "text": " And then I could print hello one more time and notice that it's showing me", "tokens": [50364, 400, 550, 286, 727, 4482, 7751, 472, 544, 565, 293, 3449, 300, 309, 311, 4099, 385, 50532], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 740, "seek": 229068, "start": 2294.04, "end": 2298.04, "text": " the number on this left hand side here on which these kind of code blocks were", "tokens": [50532, 264, 1230, 322, 341, 1411, 1011, 1252, 510, 322, 597, 613, 733, 295, 3089, 8474, 645, 50732], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 741, "seek": 229068, "start": 2298.04, "end": 2302.04, "text": " run. Now, all of these code blocks can kind of access each other.", "tokens": [50732, 1190, 13, 823, 11, 439, 295, 613, 3089, 8474, 393, 733, 295, 2105, 1184, 661, 13, 50932], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 742, "seek": 229068, "start": 2302.04, "end": 2306.3199999999997, "text": " So for example, I do define funk and we'll just take some parameter H.", "tokens": [50932, 407, 337, 1365, 11, 286, 360, 6964, 26476, 293, 321, 603, 445, 747, 512, 13075, 389, 13, 51146], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 743, "seek": 229068, "start": 2306.3599999999997, "end": 2310.8799999999997, "text": " And all we'll do is just print H. Well, if I create another code block down here,", "tokens": [51148, 400, 439, 321, 603, 360, 307, 445, 4482, 389, 13, 1042, 11, 498, 286, 1884, 1071, 3089, 3461, 760, 510, 11, 51374], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 744, "seek": 229068, "start": 2310.8799999999997, "end": 2317.08, "text": " so let's go code, I can call funk with say, hello, make sure I run this block", "tokens": [51374, 370, 718, 311, 352, 3089, 11, 286, 393, 818, 26476, 365, 584, 11, 7751, 11, 652, 988, 286, 1190, 341, 3461, 51684], "temperature": 0.0, "avg_logprob": -0.13702662129047488, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.007345146033912897}, {"id": 745, "seek": 231708, "start": 2317.08, "end": 2321.24, "text": " first, so we define the function. Now I'll run funk and notice we get the output", "tokens": [50364, 700, 11, 370, 321, 6964, 264, 2445, 13, 823, 286, 603, 1190, 26476, 293, 3449, 321, 483, 264, 5598, 50572], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 746, "seek": 231708, "start": 2321.24, "end": 2324.84, "text": " hello, so we can access all of the variables, all the functions, anything", "tokens": [50572, 7751, 11, 370, 321, 393, 2105, 439, 295, 264, 9102, 11, 439, 264, 6828, 11, 1340, 50752], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 747, "seek": 231708, "start": 2324.84, "end": 2328.24, "text": " we've defined in other code blocks from code blocks that are below it or code", "tokens": [50752, 321, 600, 7642, 294, 661, 3089, 8474, 490, 3089, 8474, 300, 366, 2507, 309, 420, 3089, 50922], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 748, "seek": 231708, "start": 2328.24, "end": 2331.6, "text": " blocks that have executed after it. Now, another thing that's great about", "tokens": [50922, 8474, 300, 362, 17577, 934, 309, 13, 823, 11, 1071, 551, 300, 311, 869, 466, 51090], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 749, "seek": 231708, "start": 2331.6, "end": 2334.88, "text": " collaboratory is the fact that we can import pretty much any module we can", "tokens": [51090, 5091, 4745, 307, 264, 1186, 300, 321, 393, 974, 1238, 709, 604, 10088, 321, 393, 51254], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 750, "seek": 231708, "start": 2334.88, "end": 2338.36, "text": " imagine, and we don't need to install it. So I'm not actually going to be going", "tokens": [51254, 3811, 11, 293, 321, 500, 380, 643, 281, 3625, 309, 13, 407, 286, 478, 406, 767, 516, 281, 312, 516, 51428], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 751, "seek": 231708, "start": 2338.36, "end": 2342.6, "text": " through how to install TensorFlow completely. There is a little bit on how", "tokens": [51428, 807, 577, 281, 3625, 37624, 2584, 13, 821, 307, 257, 707, 857, 322, 577, 51640], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 752, "seek": 231708, "start": 2342.6, "end": 2346.3199999999997, "text": " to install TensorFlow on your local machine inside of this notebook, which", "tokens": [51640, 281, 3625, 37624, 322, 428, 2654, 3479, 1854, 295, 341, 21060, 11, 597, 51826], "temperature": 0.0, "avg_logprob": -0.11428601774450851, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.06277254968881607}, {"id": 753, "seek": 234632, "start": 2346.36, "end": 2349.28, "text": " I'll refer you to. But essentially, if you know how to use pip, it's pretty", "tokens": [50366, 286, 603, 2864, 291, 281, 13, 583, 4476, 11, 498, 291, 458, 577, 281, 764, 8489, 11, 309, 311, 1238, 50512], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 754, "seek": 234632, "start": 2349.28, "end": 2353.0, "text": " straightforward, you can pip install TensorFlow, or pip install TensorFlow", "tokens": [50512, 15325, 11, 291, 393, 8489, 3625, 37624, 11, 420, 8489, 3625, 37624, 50698], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 755, "seek": 234632, "start": 2353.0, "end": 2356.7200000000003, "text": " GPU, if you have a compatible GPU, which you can check from the link that's in", "tokens": [50698, 18407, 11, 498, 291, 362, 257, 18218, 18407, 11, 597, 291, 393, 1520, 490, 264, 2113, 300, 311, 294, 50884], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 756, "seek": 234632, "start": 2356.7200000000003, "end": 2361.1200000000003, "text": " this notebook. Now, if I want to import something, what I can do is literally", "tokens": [50884, 341, 21060, 13, 823, 11, 498, 286, 528, 281, 974, 746, 11, 437, 286, 393, 360, 307, 3736, 51104], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 757, "seek": 234632, "start": 2361.1200000000003, "end": 2364.8, "text": " just write the import. So I can say import numpy like this. And usually numpy", "tokens": [51104, 445, 2464, 264, 974, 13, 407, 286, 393, 584, 974, 1031, 8200, 411, 341, 13, 400, 2673, 1031, 8200, 51288], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 758, "seek": 234632, "start": 2364.8, "end": 2368.32, "text": " is a module that you need to install. But we don't need to do that here. It's", "tokens": [51288, 307, 257, 10088, 300, 291, 643, 281, 3625, 13, 583, 321, 500, 380, 643, 281, 360, 300, 510, 13, 467, 311, 51464], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 759, "seek": 234632, "start": 2368.36, "end": 2371.2400000000002, "text": " already installed on the machine. So again, we hook up to those Google", "tokens": [51466, 1217, 8899, 322, 264, 3479, 13, 407, 797, 11, 321, 6328, 493, 281, 729, 3329, 51610], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 760, "seek": 234632, "start": 2371.2400000000002, "end": 2375.2000000000003, "text": " servers, we can use their hardware to perform machine learning. And this is", "tokens": [51610, 15909, 11, 321, 393, 764, 641, 8837, 281, 2042, 3479, 2539, 13, 400, 341, 307, 51808], "temperature": 0.0, "avg_logprob": -0.10978313616126965, "compression_ratio": 1.7784256559766765, "no_speech_prob": 0.03208974376320839}, {"id": 761, "seek": 237520, "start": 2375.2, "end": 2378.68, "text": " awesome. This is amazing. And it gives you performance benefits when you're", "tokens": [50364, 3476, 13, 639, 307, 2243, 13, 400, 309, 2709, 291, 3389, 5311, 562, 291, 434, 50538], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 762, "seek": 237520, "start": 2378.68, "end": 2382.24, "text": " running on like a lower kind of crappier machine, right? So we can have a look", "tokens": [50538, 2614, 322, 411, 257, 3126, 733, 295, 2094, 427, 811, 3479, 11, 558, 30, 407, 321, 393, 362, 257, 574, 50716], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 763, "seek": 237520, "start": 2382.24, "end": 2385.68, "text": " at the RAM in the disk space of our computer, we can see we have 12 gigs of", "tokens": [50716, 412, 264, 14561, 294, 264, 12355, 1901, 295, 527, 3820, 11, 321, 393, 536, 321, 362, 2272, 34586, 295, 50888], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 764, "seek": 237520, "start": 2385.68, "end": 2390.08, "text": " RAM, we're dealing with 107 gigabytes of data on our disk space. And we can", "tokens": [50888, 14561, 11, 321, 434, 6260, 365, 1266, 22, 42741, 295, 1412, 322, 527, 12355, 1901, 13, 400, 321, 393, 51108], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 765, "seek": 237520, "start": 2390.08, "end": 2394.08, "text": " obviously, you know, look at that if we want, we can connect to our local", "tokens": [51108, 2745, 11, 291, 458, 11, 574, 412, 300, 498, 321, 528, 11, 321, 393, 1745, 281, 527, 2654, 51308], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 766, "seek": 237520, "start": 2394.08, "end": 2396.96, "text": " runtime, which I believe connects to your local machine. But I'm not going to go", "tokens": [51308, 34474, 11, 597, 286, 1697, 16967, 281, 428, 2654, 3479, 13, 583, 286, 478, 406, 516, 281, 352, 51452], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 767, "seek": 237520, "start": 2396.96, "end": 2400.08, "text": " through all of that. I just want to show you guys some basic components of", "tokens": [51452, 807, 439, 295, 300, 13, 286, 445, 528, 281, 855, 291, 1074, 512, 3875, 6677, 295, 51608], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 768, "seek": 237520, "start": 2400.08, "end": 2403.68, "text": " collaboratory. Now, some other things that are important to understand is this", "tokens": [51608, 5091, 4745, 13, 823, 11, 512, 661, 721, 300, 366, 1021, 281, 1223, 307, 341, 51788], "temperature": 0.0, "avg_logprob": -0.0870947656752188, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.019714027643203735}, {"id": 769, "seek": 240368, "start": 2403.68, "end": 2408.8399999999997, "text": " runtime tab, which you might see me use. So restart runtime essentially clears", "tokens": [50364, 34474, 4421, 11, 597, 291, 1062, 536, 385, 764, 13, 407, 21022, 34474, 4476, 47033, 50622], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 770, "seek": 240368, "start": 2408.8399999999997, "end": 2412.7999999999997, "text": " all of your output, and just restarts whatever's happened. Because the great", "tokens": [50622, 439, 295, 428, 5598, 11, 293, 445, 1472, 11814, 2035, 311, 2011, 13, 1436, 264, 869, 50820], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 771, "seek": 240368, "start": 2412.7999999999997, "end": 2416.64, "text": " thing with collaboratory is since I can run specific code blocks, I don't need", "tokens": [50820, 551, 365, 5091, 4745, 307, 1670, 286, 393, 1190, 2685, 3089, 8474, 11, 286, 500, 380, 643, 51012], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 772, "seek": 240368, "start": 2416.64, "end": 2421.08, "text": " to execute the entire thing of code every time I want to run something. If I've", "tokens": [51012, 281, 14483, 264, 2302, 551, 295, 3089, 633, 565, 286, 528, 281, 1190, 746, 13, 759, 286, 600, 51234], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 773, "seek": 240368, "start": 2421.08, "end": 2425.52, "text": " just made a minor change in one code block, I can just run that code. Sorry, I", "tokens": [51234, 445, 1027, 257, 6696, 1319, 294, 472, 3089, 3461, 11, 286, 393, 445, 1190, 300, 3089, 13, 4919, 11, 286, 51456], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 774, "seek": 240368, "start": 2425.52, "end": 2429.16, "text": " can just run that code block. I don't need to run everything before it or even", "tokens": [51456, 393, 445, 1190, 300, 3089, 3461, 13, 286, 500, 380, 643, 281, 1190, 1203, 949, 309, 420, 754, 51638], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 775, "seek": 240368, "start": 2429.16, "end": 2433.2, "text": " everything after it, right? But sometimes you want to restart everything and just", "tokens": [51638, 1203, 934, 309, 11, 558, 30, 583, 2171, 291, 528, 281, 21022, 1203, 293, 445, 51840], "temperature": 0.0, "avg_logprob": -0.09120070139567057, "compression_ratio": 1.8843537414965987, "no_speech_prob": 0.022282911464571953}, {"id": 776, "seek": 243320, "start": 2433.24, "end": 2437.4399999999996, "text": " rerun everything. So to do that, you click restart runtime, that's just going to", "tokens": [50366, 43819, 409, 1203, 13, 407, 281, 360, 300, 11, 291, 2052, 21022, 34474, 11, 300, 311, 445, 516, 281, 50576], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 777, "seek": 243320, "start": 2437.4399999999996, "end": 2441.48, "text": " clear everything you have. And then restart and run all will restart the", "tokens": [50576, 1850, 1203, 291, 362, 13, 400, 550, 21022, 293, 1190, 439, 486, 21022, 264, 50778], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 778, "seek": 243320, "start": 2441.48, "end": 2446.48, "text": " runtime as well as run every single block of code you have in sequential order in", "tokens": [50778, 34474, 382, 731, 382, 1190, 633, 2167, 3461, 295, 3089, 291, 362, 294, 42881, 1668, 294, 51028], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 779, "seek": 243320, "start": 2446.48, "end": 2450.2, "text": " which it shows up in the thing. So I recommend you guys open up one of these", "tokens": [51028, 597, 309, 3110, 493, 294, 264, 551, 13, 407, 286, 2748, 291, 1074, 1269, 493, 472, 295, 613, 51214], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 780, "seek": 243320, "start": 2450.2, "end": 2453.48, "text": " windows. You can obviously follow along with this notebook if you want. But if", "tokens": [51214, 9309, 13, 509, 393, 2745, 1524, 2051, 365, 341, 21060, 498, 291, 528, 13, 583, 498, 51378], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 781, "seek": 243320, "start": 2453.48, "end": 2456.8799999999997, "text": " you want to type it out on your own and kind of mess with it, open up a notebook,", "tokens": [51378, 291, 528, 281, 2010, 309, 484, 322, 428, 1065, 293, 733, 295, 2082, 365, 309, 11, 1269, 493, 257, 21060, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 782, "seek": 243320, "start": 2457.12, "end": 2461.04, "text": " save it. It's very easy. And these are again, extremely similar to Jupiter", "tokens": [51560, 3155, 309, 13, 467, 311, 588, 1858, 13, 400, 613, 366, 797, 11, 4664, 2531, 281, 24567, 51756], "temperature": 0.0, "avg_logprob": -0.10708373132413321, "compression_ratio": 1.785016286644951, "no_speech_prob": 0.005554395727813244}, {"id": 783, "seek": 246104, "start": 2461.04, "end": 2466.16, "text": " notebooks, Jupiter notebooks, they're pretty much the same. Okay, so that is", "tokens": [50364, 43782, 11, 24567, 43782, 11, 436, 434, 1238, 709, 264, 912, 13, 1033, 11, 370, 300, 307, 50620], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 784, "seek": 246104, "start": 2466.16, "end": 2471.2, "text": " kind of the Google Collaboratory aspect how to use that. Let's get into importing", "tokens": [50620, 733, 295, 264, 3329, 44483, 4745, 4171, 577, 281, 764, 300, 13, 961, 311, 483, 666, 43866, 50872], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 785, "seek": 246104, "start": 2471.2, "end": 2475.16, "text": " TensorFlow. Now this is going to be kind of specific to Google Collaboratory. So", "tokens": [50872, 37624, 13, 823, 341, 307, 516, 281, 312, 733, 295, 2685, 281, 3329, 44483, 4745, 13, 407, 51070], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 786, "seek": 246104, "start": 2475.16, "end": 2478.32, "text": " you can see here, these are kind of the steps we need to follow to import", "tokens": [51070, 291, 393, 536, 510, 11, 613, 366, 733, 295, 264, 4439, 321, 643, 281, 1524, 281, 974, 51228], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 787, "seek": 246104, "start": 2478.32, "end": 2482.4, "text": " TensorFlow. So since we're working in Google Collaboratory, they have", "tokens": [51228, 37624, 13, 407, 1670, 321, 434, 1364, 294, 3329, 44483, 4745, 11, 436, 362, 51432], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 788, "seek": 246104, "start": 2482.4, "end": 2485.52, "text": " multiple versions of TensorFlow, they have the original version of TensorFlow,", "tokens": [51432, 3866, 9606, 295, 37624, 11, 436, 362, 264, 3380, 3037, 295, 37624, 11, 51588], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 789, "seek": 246104, "start": 2485.52, "end": 2490.36, "text": " which is 1.0, and the 2.0 version. Now to define the fact that we want to use", "tokens": [51588, 597, 307, 502, 13, 15, 11, 293, 264, 568, 13, 15, 3037, 13, 823, 281, 6964, 264, 1186, 300, 321, 528, 281, 764, 51830], "temperature": 0.0, "avg_logprob": -0.10096966330684833, "compression_ratio": 1.875, "no_speech_prob": 0.0021155322901904583}, {"id": 790, "seek": 249036, "start": 2490.36, "end": 2494.84, "text": " TensorFlow 2.0, just because we're in this notebook, we need to write this line", "tokens": [50364, 37624, 568, 13, 15, 11, 445, 570, 321, 434, 294, 341, 21060, 11, 321, 643, 281, 2464, 341, 1622, 50588], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 791, "seek": 249036, "start": 2494.84, "end": 2499.44, "text": " of code at the very beginning of all of our notebooks. So percent TensorFlow", "tokens": [50588, 295, 3089, 412, 264, 588, 2863, 295, 439, 295, 527, 43782, 13, 407, 3043, 37624, 50818], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 792, "seek": 249036, "start": 2499.44, "end": 2503.8, "text": " underscore version 2.x. Now this is simply just saying we need to use", "tokens": [50818, 37556, 3037, 568, 13, 87, 13, 823, 341, 307, 2935, 445, 1566, 321, 643, 281, 764, 51036], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 793, "seek": 249036, "start": 2503.8, "end": 2508.0, "text": " TensorFlow 2.x. So whatever version that is, and this is only required in a", "tokens": [51036, 37624, 568, 13, 87, 13, 407, 2035, 3037, 300, 307, 11, 293, 341, 307, 787, 4739, 294, 257, 51246], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 794, "seek": 249036, "start": 2508.0, "end": 2511.2400000000002, "text": " notebook, if you're doing this on your local machine in a text editor, you're", "tokens": [51246, 21060, 11, 498, 291, 434, 884, 341, 322, 428, 2654, 3479, 294, 257, 2487, 9839, 11, 291, 434, 51408], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 795, "seek": 249036, "start": 2511.2400000000002, "end": 2515.08, "text": " not going to need to write this. Now once we do that, we typically import", "tokens": [51408, 406, 516, 281, 643, 281, 2464, 341, 13, 823, 1564, 321, 360, 300, 11, 321, 5850, 974, 51600], "temperature": 0.0, "avg_logprob": -0.09259127755450387, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002631440758705139}, {"id": 796, "seek": 251508, "start": 2515.12, "end": 2520.0, "text": " TensorFlow as an alias name of TF. Now to do that, we simply import the", "tokens": [50366, 37624, 382, 364, 419, 4609, 1315, 295, 40964, 13, 823, 281, 360, 300, 11, 321, 2935, 974, 264, 50610], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 797, "seek": 251508, "start": 2520.0, "end": 2524.04, "text": " TensorFlow module, and then we write as TF. If you're on your local machine,", "tokens": [50610, 37624, 10088, 11, 293, 550, 321, 2464, 382, 40964, 13, 759, 291, 434, 322, 428, 2654, 3479, 11, 50812], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 798, "seek": 251508, "start": 2524.04, "end": 2527.2799999999997, "text": " again, you're going to need to install TensorFlow first to make sure that", "tokens": [50812, 797, 11, 291, 434, 516, 281, 643, 281, 3625, 37624, 700, 281, 652, 988, 300, 50974], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 799, "seek": 251508, "start": 2527.2799999999997, "end": 2530.04, "text": " you're able to do this. But since we're in Collaboratory, we don't need to do", "tokens": [50974, 291, 434, 1075, 281, 360, 341, 13, 583, 1670, 321, 434, 294, 44483, 4745, 11, 321, 500, 380, 643, 281, 360, 51112], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 800, "seek": 251508, "start": 2530.04, "end": 2534.6, "text": " that. Now, since we've defined the fact we're using version 2.x, when we", "tokens": [51112, 300, 13, 823, 11, 1670, 321, 600, 7642, 264, 1186, 321, 434, 1228, 3037, 568, 13, 87, 11, 562, 321, 51340], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 801, "seek": 251508, "start": 2534.6, "end": 2539.52, "text": " print the TensorFlow version, we can see here that it says version two, which is", "tokens": [51340, 4482, 264, 37624, 3037, 11, 321, 393, 536, 510, 300, 309, 1619, 3037, 732, 11, 597, 307, 51586], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 802, "seek": 251508, "start": 2539.52, "end": 2543.68, "text": " exactly what we're looking for. And then it says TensorFlow 2.1.0. So make", "tokens": [51586, 2293, 437, 321, 434, 1237, 337, 13, 400, 550, 309, 1619, 37624, 568, 13, 16, 13, 15, 13, 407, 652, 51794], "temperature": 0.0, "avg_logprob": -0.09704066967142039, "compression_ratio": 1.8304498269896194, "no_speech_prob": 0.030211137607693672}, {"id": 803, "seek": 254368, "start": 2543.68, "end": 2547.3199999999997, "text": " sure that you print your version, you're using version 2.0, because there is a", "tokens": [50364, 988, 300, 291, 4482, 428, 3037, 11, 291, 434, 1228, 3037, 568, 13, 15, 11, 570, 456, 307, 257, 50546], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 804, "seek": 254368, "start": 2547.3199999999997, "end": 2551.64, "text": " lot of what I'm using in this series that is kind of, if you're in TensorFlow", "tokens": [50546, 688, 295, 437, 286, 478, 1228, 294, 341, 2638, 300, 307, 733, 295, 11, 498, 291, 434, 294, 37624, 50762], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 805, "seek": 254368, "start": 2551.64, "end": 2555.44, "text": " 1.0, it's not going to work. So it's new in TensorFlow 2.0, or it's been", "tokens": [50762, 502, 13, 15, 11, 309, 311, 406, 516, 281, 589, 13, 407, 309, 311, 777, 294, 37624, 568, 13, 15, 11, 420, 309, 311, 668, 50952], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 806, "seek": 254368, "start": 2555.44, "end": 2559.16, "text": " refactored and the names have been changed. Okay, so now that we've done", "tokens": [50952, 1895, 578, 2769, 293, 264, 5288, 362, 668, 3105, 13, 1033, 11, 370, 586, 300, 321, 600, 1096, 51138], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 807, "seek": 254368, "start": 2559.16, "end": 2562.2799999999997, "text": " that, we've imported TensorFlow, we've got this here, and I'm actually going to", "tokens": [51138, 300, 11, 321, 600, 25524, 37624, 11, 321, 600, 658, 341, 510, 11, 293, 286, 478, 767, 516, 281, 51294], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 808, "seek": 254368, "start": 2562.2799999999997, "end": 2565.6, "text": " go to my fresh notebook and just do this. So we'll just copy these lines over", "tokens": [51294, 352, 281, 452, 4451, 21060, 293, 445, 360, 341, 13, 407, 321, 603, 445, 5055, 613, 3876, 670, 51460], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 809, "seek": 254368, "start": 2565.6, "end": 2569.0, "text": " just so we have some fresh code, and I don't have all this text that we have to", "tokens": [51460, 445, 370, 321, 362, 512, 4451, 3089, 11, 293, 286, 500, 380, 362, 439, 341, 2487, 300, 321, 362, 281, 51630], "temperature": 0.0, "avg_logprob": -0.08977827468475738, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.025175176560878754}, {"id": 810, "seek": 256900, "start": 2569.04, "end": 2575.2, "text": " deal with. So let's do this TensorFlow, let's import TensorFlow as TF, and then", "tokens": [50366, 2028, 365, 13, 407, 718, 311, 360, 341, 37624, 11, 718, 311, 974, 37624, 382, 40964, 11, 293, 550, 50674], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 811, "seek": 256900, "start": 2575.2, "end": 2581.84, "text": " we can print the TF dot version and have a look at that. So version. Okay, so", "tokens": [50674, 321, 393, 4482, 264, 40964, 5893, 3037, 293, 362, 257, 574, 412, 300, 13, 407, 3037, 13, 1033, 11, 370, 51006], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 812, "seek": 256900, "start": 2581.84, "end": 2585.16, "text": " let's run our code here, we can see TensorFlow is already loaded. Oh, it says", "tokens": [51006, 718, 311, 1190, 527, 3089, 510, 11, 321, 393, 536, 37624, 307, 1217, 13210, 13, 876, 11, 309, 1619, 51172], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 813, "seek": 256900, "start": 2585.16, "end": 2588.68, "text": " 1.0. So if you get this error, it's actually good, I ran into this where", "tokens": [51172, 502, 13, 15, 13, 407, 498, 291, 483, 341, 6713, 11, 309, 311, 767, 665, 11, 286, 5872, 666, 341, 689, 51348], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 814, "seek": 256900, "start": 2588.68, "end": 2591.92, "text": " TensorFlow has already been loaded. All you need to do is just restart your", "tokens": [51348, 37624, 575, 1217, 668, 13210, 13, 1057, 291, 643, 281, 360, 307, 445, 21022, 428, 51510], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 815, "seek": 256900, "start": 2591.92, "end": 2595.56, "text": " runtime. So I'm going to restart and run all just click Yes. And now we should", "tokens": [51510, 34474, 13, 407, 286, 478, 516, 281, 21022, 293, 1190, 439, 445, 2052, 1079, 13, 400, 586, 321, 820, 51692], "temperature": 0.0, "avg_logprob": -0.12404845830962413, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0600794181227684}, {"id": 816, "seek": 259556, "start": 2595.6, "end": 2599.88, "text": " see that we get that version 2.0. Once this starts running, give it a second", "tokens": [50366, 536, 300, 321, 483, 300, 3037, 568, 13, 15, 13, 3443, 341, 3719, 2614, 11, 976, 309, 257, 1150, 50580], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 817, "seek": 259556, "start": 2599.88, "end": 2605.2, "text": " TensorFlow 2.0 selected, we're going to import that module. And there we go, we", "tokens": [50580, 37624, 568, 13, 15, 8209, 11, 321, 434, 516, 281, 974, 300, 10088, 13, 400, 456, 321, 352, 11, 321, 50846], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 818, "seek": 259556, "start": 2605.2, "end": 2610.7999999999997, "text": " have version 2.0. Okay, so now it's time to talk about tensors. Now, what is a", "tokens": [50846, 362, 3037, 568, 13, 15, 13, 1033, 11, 370, 586, 309, 311, 565, 281, 751, 466, 10688, 830, 13, 823, 11, 437, 307, 257, 51126], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 819, "seek": 259556, "start": 2610.84, "end": 2614.4, "text": " tensor? Now, tensor just immediately seems kind of like a complicated name, you're", "tokens": [51128, 40863, 30, 823, 11, 40863, 445, 4258, 2544, 733, 295, 411, 257, 6179, 1315, 11, 291, 434, 51306], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 820, "seek": 259556, "start": 2614.4, "end": 2618.84, "text": " like, All right, tensor, like this is confusing. But what is it? Well, obviously", "tokens": [51306, 411, 11, 1057, 558, 11, 40863, 11, 411, 341, 307, 13181, 13, 583, 437, 307, 309, 30, 1042, 11, 2745, 51528], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 821, "seek": 259556, "start": 2618.84, "end": 2622.6, "text": " this is going to be a primary aspect of TensorFlow, considering the name", "tokens": [51528, 341, 307, 516, 281, 312, 257, 6194, 4171, 295, 37624, 11, 8079, 264, 1315, 51716], "temperature": 0.0, "avg_logprob": -0.12843288481235504, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.07366914302110672}, {"id": 822, "seek": 262260, "start": 2622.6, "end": 2627.72, "text": " similarities. And essentially, all it is is a vector generalized to higher", "tokens": [50364, 24197, 13, 400, 4476, 11, 439, 309, 307, 307, 257, 8062, 44498, 281, 2946, 50620], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 823, "seek": 262260, "start": 2627.72, "end": 2632.12, "text": " dimensions. Now, what is a vector? Well, if you've ever done any linear algebra", "tokens": [50620, 12819, 13, 823, 11, 437, 307, 257, 8062, 30, 1042, 11, 498, 291, 600, 1562, 1096, 604, 8213, 21989, 50840], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 824, "seek": 262260, "start": 2632.12, "end": 2635.48, "text": " or even some basic kind of vector calculus, you should hopefully know what", "tokens": [50840, 420, 754, 512, 3875, 733, 295, 8062, 33400, 11, 291, 820, 4696, 458, 437, 51008], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 825, "seek": 262260, "start": 2635.48, "end": 2640.0, "text": " that is. But essentially, it is kind of a data point is kind of the way that I", "tokens": [51008, 300, 307, 13, 583, 4476, 11, 309, 307, 733, 295, 257, 1412, 935, 307, 733, 295, 264, 636, 300, 286, 51234], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 826, "seek": 262260, "start": 2640.0, "end": 2643.92, "text": " like to describe it. And the reason we call it a vector is because it doesn't", "tokens": [51234, 411, 281, 6786, 309, 13, 400, 264, 1778, 321, 818, 309, 257, 8062, 307, 570, 309, 1177, 380, 51430], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 827, "seek": 262260, "start": 2643.92, "end": 2648.7599999999998, "text": " necessarily have a certain coordinate. So like if you're talking about a two", "tokens": [51430, 4725, 362, 257, 1629, 15670, 13, 407, 411, 498, 291, 434, 1417, 466, 257, 732, 51672], "temperature": 0.0, "avg_logprob": -0.0705664032383969, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.019122181460261345}, {"id": 828, "seek": 264876, "start": 2648.76, "end": 2652.84, "text": " dimensional data point, you have, you know, maybe an x and a y value, or like an", "tokens": [50364, 18795, 1412, 935, 11, 291, 362, 11, 291, 458, 11, 1310, 364, 2031, 293, 257, 288, 2158, 11, 420, 411, 364, 50568], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 829, "seek": 264876, "start": 2652.84, "end": 2658.0, "text": " x one value and an x two value. Now a vector can have any amount of dimensions", "tokens": [50568, 2031, 472, 2158, 293, 364, 2031, 732, 2158, 13, 823, 257, 8062, 393, 362, 604, 2372, 295, 12819, 50826], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 830, "seek": 264876, "start": 2658.0400000000004, "end": 2662.0, "text": " in it, it could have one dimension, which simply means it's just one number, could", "tokens": [50828, 294, 309, 11, 309, 727, 362, 472, 10139, 11, 597, 2935, 1355, 309, 311, 445, 472, 1230, 11, 727, 51026], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 831, "seek": 264876, "start": 2662.0, "end": 2665.48, "text": " have two dimensions, which means we're having two numbers. So like an x and a", "tokens": [51026, 362, 732, 12819, 11, 597, 1355, 321, 434, 1419, 732, 3547, 13, 407, 411, 364, 2031, 293, 257, 51200], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 832, "seek": 264876, "start": 2665.48, "end": 2669.6000000000004, "text": " y value, if we're thinking about a two dimensional graph, we'd have three", "tokens": [51200, 288, 2158, 11, 498, 321, 434, 1953, 466, 257, 732, 18795, 4295, 11, 321, 1116, 362, 1045, 51406], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 833, "seek": 264876, "start": 2669.6000000000004, "end": 2672.96, "text": " dimensions, if we're thinking about a three dimensional graph, so that would be", "tokens": [51406, 12819, 11, 498, 321, 434, 1953, 466, 257, 1045, 18795, 4295, 11, 370, 300, 576, 312, 51574], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 834, "seek": 264876, "start": 2672.96, "end": 2676.44, "text": " three data points, we could have four dimensions, if we're talking about", "tokens": [51574, 1045, 1412, 2793, 11, 321, 727, 362, 1451, 12819, 11, 498, 321, 434, 1417, 466, 51748], "temperature": 0.0, "avg_logprob": -0.11648253032139369, "compression_ratio": 2.2235772357723578, "no_speech_prob": 0.05499378964304924}, {"id": 835, "seek": 267644, "start": 2676.48, "end": 2680.36, "text": " sometimes some image data and some video data, five dimensions, and we can", "tokens": [50366, 2171, 512, 3256, 1412, 293, 512, 960, 1412, 11, 1732, 12819, 11, 293, 321, 393, 50560], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 836, "seek": 267644, "start": 2680.36, "end": 2685.12, "text": " keep going, going, going with vectors. So essentially, what a tensor is, and I'll", "tokens": [50560, 1066, 516, 11, 516, 11, 516, 365, 18875, 13, 407, 4476, 11, 437, 257, 40863, 307, 11, 293, 286, 603, 50798], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 837, "seek": 267644, "start": 2685.12, "end": 2688.16, "text": " just read this formal definition to make sure I haven't butchered anything", "tokens": [50798, 445, 1401, 341, 9860, 7123, 281, 652, 988, 286, 2378, 380, 457, 339, 4073, 1340, 50950], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 838, "seek": 267644, "start": 2688.16, "end": 2692.16, "text": " that's from the actual TensorFlow website. A tensor is a generalization of", "tokens": [50950, 300, 311, 490, 264, 3539, 37624, 3144, 13, 316, 40863, 307, 257, 2674, 2144, 295, 51150], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 839, "seek": 267644, "start": 2692.16, "end": 2696.48, "text": " vectors and matrices to potentially higher dimensions, internally TensorFlow", "tokens": [51150, 18875, 293, 32284, 281, 7263, 2946, 12819, 11, 19501, 37624, 51366], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 840, "seek": 267644, "start": 2696.48, "end": 2700.4, "text": " represents tensors as n dimensional arrays of base data types. Now we'll", "tokens": [51366, 8855, 10688, 830, 382, 297, 18795, 41011, 295, 3096, 1412, 3467, 13, 823, 321, 603, 51562], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 841, "seek": 267644, "start": 2700.4, "end": 2705.4, "text": " understand what that means in a second, but hopefully that makes sense. Now, since", "tokens": [51562, 1223, 437, 300, 1355, 294, 257, 1150, 11, 457, 4696, 300, 1669, 2020, 13, 823, 11, 1670, 51812], "temperature": 0.0, "avg_logprob": -0.09118598069601912, "compression_ratio": 1.75, "no_speech_prob": 0.010985842905938625}, {"id": 842, "seek": 270540, "start": 2705.4, "end": 2709.12, "text": " tensors are so important to TensorFlow, they're kind of the main object that", "tokens": [50364, 10688, 830, 366, 370, 1021, 281, 37624, 11, 436, 434, 733, 295, 264, 2135, 2657, 300, 50550], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 843, "seek": 270540, "start": 2709.12, "end": 2713.04, "text": " we're going to be working with, manipulating and viewing. And it's the main", "tokens": [50550, 321, 434, 516, 281, 312, 1364, 365, 11, 40805, 293, 17480, 13, 400, 309, 311, 264, 2135, 50746], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 844, "seek": 270540, "start": 2713.04, "end": 2717.52, "text": " object that's passed around through our program. Now, what we can see here is", "tokens": [50746, 2657, 300, 311, 4678, 926, 807, 527, 1461, 13, 823, 11, 437, 321, 393, 536, 510, 307, 50970], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 845, "seek": 270540, "start": 2717.52, "end": 2721.48, "text": " each tensor represents a partially defined computation that will eventually", "tokens": [50970, 1184, 40863, 8855, 257, 18886, 7642, 24903, 300, 486, 4728, 51168], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 846, "seek": 270540, "start": 2721.48, "end": 2725.88, "text": " produce a value. So just like we talked about in the graphs and sessions, what", "tokens": [51168, 5258, 257, 2158, 13, 407, 445, 411, 321, 2825, 466, 294, 264, 24877, 293, 11081, 11, 437, 51388], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 847, "seek": 270540, "start": 2725.88, "end": 2729.28, "text": " we're going to do is when we create our program, we're going to be creating a", "tokens": [51388, 321, 434, 516, 281, 360, 307, 562, 321, 1884, 527, 1461, 11, 321, 434, 516, 281, 312, 4084, 257, 51558], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 848, "seek": 270540, "start": 2729.28, "end": 2732.52, "text": " bunch of tensors and TensorFlow is going to be creating them as well. And those", "tokens": [51558, 3840, 295, 10688, 830, 293, 37624, 307, 516, 281, 312, 4084, 552, 382, 731, 13, 400, 729, 51720], "temperature": 0.0, "avg_logprob": -0.06849555821381798, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0013249274343252182}, {"id": 849, "seek": 273252, "start": 2732.52, "end": 2737.64, "text": " are going to store partially defined computations in the graph. Later, when we", "tokens": [50364, 366, 516, 281, 3531, 18886, 7642, 2807, 763, 294, 264, 4295, 13, 11965, 11, 562, 321, 50620], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 850, "seek": 273252, "start": 2737.64, "end": 2741.52, "text": " actually build the graph and have the session running, we will run different", "tokens": [50620, 767, 1322, 264, 4295, 293, 362, 264, 5481, 2614, 11, 321, 486, 1190, 819, 50814], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 851, "seek": 273252, "start": 2741.52, "end": 2744.8, "text": " parts of the graph, which means we'll execute different tensors, and be able", "tokens": [50814, 3166, 295, 264, 4295, 11, 597, 1355, 321, 603, 14483, 819, 10688, 830, 11, 293, 312, 1075, 50978], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 852, "seek": 273252, "start": 2744.8, "end": 2748.72, "text": " to get different results from our tensors. Now each tensor has what we call a", "tokens": [50978, 281, 483, 819, 3542, 490, 527, 10688, 830, 13, 823, 1184, 40863, 575, 437, 321, 818, 257, 51174], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 853, "seek": 273252, "start": 2748.72, "end": 2753.64, "text": " data type and a shape, and that's we're going to get into now. So a data type is", "tokens": [51174, 1412, 2010, 293, 257, 3909, 11, 293, 300, 311, 321, 434, 516, 281, 483, 666, 586, 13, 407, 257, 1412, 2010, 307, 51420], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 854, "seek": 273252, "start": 2753.64, "end": 2757.68, "text": " simply what kind of information is stored in the tensor. Now it's very rare that", "tokens": [51420, 2935, 437, 733, 295, 1589, 307, 12187, 294, 264, 40863, 13, 823, 309, 311, 588, 5892, 300, 51622], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 855, "seek": 273252, "start": 2757.68, "end": 2761.44, "text": " we see any data types different than numbers, although there is the data type", "tokens": [51622, 321, 536, 604, 1412, 3467, 819, 813, 3547, 11, 4878, 456, 307, 264, 1412, 2010, 51810], "temperature": 0.0, "avg_logprob": -0.09690834159281717, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.007815252989530563}, {"id": 856, "seek": 276144, "start": 2761.48, "end": 2764.56, "text": " of strings and a few others as well. But I haven't included all of them here", "tokens": [50366, 295, 13985, 293, 257, 1326, 2357, 382, 731, 13, 583, 286, 2378, 380, 5556, 439, 295, 552, 510, 50520], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 857, "seek": 276144, "start": 2764.56, "end": 2768.84, "text": " because they're not that important. But some examples we can see our float 32 in", "tokens": [50520, 570, 436, 434, 406, 300, 1021, 13, 583, 512, 5110, 321, 393, 536, 527, 15706, 8858, 294, 50734], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 858, "seek": 276144, "start": 2768.84, "end": 2774.7200000000003, "text": " 32 string and others. Now the shape is simply the representation of the", "tokens": [50734, 8858, 6798, 293, 2357, 13, 823, 264, 3909, 307, 2935, 264, 10290, 295, 264, 51028], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 859, "seek": 276144, "start": 2774.7200000000003, "end": 2778.6, "text": " tensor in terms of what dimension it is. And we'll get some examples because I", "tokens": [51028, 40863, 294, 2115, 295, 437, 10139, 309, 307, 13, 400, 321, 603, 483, 512, 5110, 570, 286, 51222], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 860, "seek": 276144, "start": 2778.6, "end": 2781.6, "text": " don't want to explain the shape until we can see some examples to really dial", "tokens": [51222, 500, 380, 528, 281, 2903, 264, 3909, 1826, 321, 393, 536, 512, 5110, 281, 534, 5502, 51372], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 861, "seek": 276144, "start": 2781.6, "end": 2786.36, "text": " in. But here are some examples of how we would create different tensors. So what", "tokens": [51372, 294, 13, 583, 510, 366, 512, 5110, 295, 577, 321, 576, 1884, 819, 10688, 830, 13, 407, 437, 51610], "temperature": 0.0, "avg_logprob": -0.08161886114823191, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.0059104315005242825}, {"id": 862, "seek": 278636, "start": 2786.36, "end": 2792.56, "text": " you can do is you can simply do TF dot variable. And then you can do the value", "tokens": [50364, 291, 393, 360, 307, 291, 393, 2935, 360, 40964, 5893, 7006, 13, 400, 550, 291, 393, 360, 264, 2158, 50674], "temperature": 0.0, "avg_logprob": -0.12954161213893517, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.01363559253513813}, {"id": 863, "seek": 278636, "start": 2792.56, "end": 2796.92, "text": " and the data type that your tensor is. So in this case, we've created a string", "tokens": [50674, 293, 264, 1412, 2010, 300, 428, 40863, 307, 13, 407, 294, 341, 1389, 11, 321, 600, 2942, 257, 6798, 50892], "temperature": 0.0, "avg_logprob": -0.12954161213893517, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.01363559253513813}, {"id": 864, "seek": 278636, "start": 2796.92, "end": 2801.96, "text": " tensor which stores one string. And it is TF dot strings, we define the data type", "tokens": [50892, 40863, 597, 9512, 472, 6798, 13, 400, 309, 307, 40964, 5893, 13985, 11, 321, 6964, 264, 1412, 2010, 51144], "temperature": 0.0, "avg_logprob": -0.12954161213893517, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.01363559253513813}, {"id": 865, "seek": 278636, "start": 2801.96, "end": 2807.36, "text": " second, we have a number tensor which stores some integer value. And then that", "tokens": [51144, 1150, 11, 321, 362, 257, 1230, 40863, 597, 9512, 512, 24922, 2158, 13, 400, 550, 300, 51414], "temperature": 0.0, "avg_logprob": -0.12954161213893517, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.01363559253513813}, {"id": 866, "seek": 278636, "start": 2807.36, "end": 2812.48, "text": " is up type TF int 16. And we have a floating point tensor, which stores a", "tokens": [51414, 307, 493, 2010, 40964, 560, 3165, 13, 400, 321, 362, 257, 12607, 935, 40863, 11, 597, 9512, 257, 51670], "temperature": 0.0, "avg_logprob": -0.12954161213893517, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.01363559253513813}, {"id": 867, "seek": 281248, "start": 2812.52, "end": 2818.2400000000002, "text": " simple floating point. Now these tensors have a shape of I believe it's going to", "tokens": [50366, 2199, 12607, 935, 13, 823, 613, 10688, 830, 362, 257, 3909, 295, 286, 1697, 309, 311, 516, 281, 50652], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 868, "seek": 281248, "start": 2818.2400000000002, "end": 2823.12, "text": " be one, which simply means they are a scalar. Now a scalar value and you might", "tokens": [50652, 312, 472, 11, 597, 2935, 1355, 436, 366, 257, 39684, 13, 823, 257, 39684, 2158, 293, 291, 1062, 50896], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 869, "seek": 281248, "start": 2823.12, "end": 2828.16, "text": " hear me say this a lot simply means just one value. That's all it means. When we", "tokens": [50896, 1568, 385, 584, 341, 257, 688, 2935, 1355, 445, 472, 2158, 13, 663, 311, 439, 309, 1355, 13, 1133, 321, 51148], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 870, "seek": 281248, "start": 2828.16, "end": 2832.72, "text": " talk about like vector values, that typically means more than one value. And", "tokens": [51148, 751, 466, 411, 8062, 4190, 11, 300, 5850, 1355, 544, 813, 472, 2158, 13, 400, 51376], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 871, "seek": 281248, "start": 2832.72, "end": 2836.84, "text": " we talk about matrices, we're having different it just it goes up but scalar", "tokens": [51376, 321, 751, 466, 32284, 11, 321, 434, 1419, 819, 309, 445, 309, 1709, 493, 457, 39684, 51582], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 872, "seek": 281248, "start": 2836.84, "end": 2842.2400000000002, "text": " simply means one number. So yeah, that is what we get for the different data", "tokens": [51582, 2935, 1355, 472, 1230, 13, 407, 1338, 11, 300, 307, 437, 321, 483, 337, 264, 819, 1412, 51852], "temperature": 0.0, "avg_logprob": -0.11937648967161017, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.017984123900532722}, {"id": 873, "seek": 284224, "start": 2842.2799999999997, "end": 2845.3199999999997, "text": " types and creating tensors, we're not really going to do this very much in our", "tokens": [50366, 3467, 293, 4084, 10688, 830, 11, 321, 434, 406, 534, 516, 281, 360, 341, 588, 709, 294, 527, 50518], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 874, "seek": 284224, "start": 2845.3199999999997, "end": 2849.4399999999996, "text": " program. But just for some examples here, that's how we do it. So we've imported", "tokens": [50518, 1461, 13, 583, 445, 337, 512, 5110, 510, 11, 300, 311, 577, 321, 360, 309, 13, 407, 321, 600, 25524, 50724], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 875, "seek": 284224, "start": 2849.4399999999996, "end": 2851.9599999999996, "text": " them. So I can actually run these. And I mean, we're not going to really get any", "tokens": [50724, 552, 13, 407, 286, 393, 767, 1190, 613, 13, 400, 286, 914, 11, 321, 434, 406, 516, 281, 534, 483, 604, 50850], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 876, "seek": 284224, "start": 2851.9599999999996, "end": 2856.2799999999997, "text": " output by running this code because well, there's nothing to see. But now we're", "tokens": [50850, 5598, 538, 2614, 341, 3089, 570, 731, 11, 456, 311, 1825, 281, 536, 13, 583, 586, 321, 434, 51066], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 877, "seek": 284224, "start": 2856.2799999999997, "end": 2861.04, "text": " going to talk about the rank slash degree of tensors. So another word for rank is", "tokens": [51066, 516, 281, 751, 466, 264, 6181, 17330, 4314, 295, 10688, 830, 13, 407, 1071, 1349, 337, 6181, 307, 51304], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 878, "seek": 284224, "start": 2861.04, "end": 2865.4399999999996, "text": " agree. So these are interchangeably. And again, this simply means the the number", "tokens": [51304, 3986, 13, 407, 613, 366, 30358, 1188, 13, 400, 797, 11, 341, 2935, 1355, 264, 264, 1230, 51524], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 879, "seek": 284224, "start": 2865.4399999999996, "end": 2870.8399999999997, "text": " of dimensions involved in the tensor. So when we create a tensor of rank zero,", "tokens": [51524, 295, 12819, 3288, 294, 264, 40863, 13, 407, 562, 321, 1884, 257, 40863, 295, 6181, 4018, 11, 51794], "temperature": 0.0, "avg_logprob": -0.10696283537766028, "compression_ratio": 1.772870662460568, "no_speech_prob": 0.0064869895577430725}, {"id": 880, "seek": 287084, "start": 2870.88, "end": 2874.8, "text": " which is what we've done up here, we call that a scalar. Now the reason this has", "tokens": [50366, 597, 307, 437, 321, 600, 1096, 493, 510, 11, 321, 818, 300, 257, 39684, 13, 823, 264, 1778, 341, 575, 50562], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 881, "seek": 287084, "start": 2874.8, "end": 2879.76, "text": " rank zero is because it's simply one thing, we don't have any dimensions to", "tokens": [50562, 6181, 4018, 307, 570, 309, 311, 2935, 472, 551, 11, 321, 500, 380, 362, 604, 12819, 281, 50810], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 882, "seek": 287084, "start": 2879.76, "end": 2884.32, "text": " this, there's like zero dimensionality of that. It was even a word, it's just one", "tokens": [50810, 341, 11, 456, 311, 411, 4018, 10139, 1860, 295, 300, 13, 467, 390, 754, 257, 1349, 11, 309, 311, 445, 472, 51038], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 883, "seek": 287084, "start": 2884.32, "end": 2890.4, "text": " value. Whereas here, we have an array. Now when we have an array or a list, we", "tokens": [51038, 2158, 13, 13813, 510, 11, 321, 362, 364, 10225, 13, 823, 562, 321, 362, 364, 10225, 420, 257, 1329, 11, 321, 51342], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 884, "seek": 287084, "start": 2890.4, "end": 2895.36, "text": " immediately have at least rank one. Now the reason for that is because this", "tokens": [51342, 4258, 362, 412, 1935, 6181, 472, 13, 823, 264, 1778, 337, 300, 307, 570, 341, 51590], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 885, "seek": 287084, "start": 2895.36, "end": 2899.1200000000003, "text": " array can store more than one value in one dimension, right? So I can do", "tokens": [51590, 10225, 393, 3531, 544, 813, 472, 2158, 294, 472, 10139, 11, 558, 30, 407, 286, 393, 360, 51778], "temperature": 0.0, "avg_logprob": -0.11709117889404297, "compression_ratio": 1.8203125, "no_speech_prob": 0.002889381255954504}, {"id": 886, "seek": 289912, "start": 2899.12, "end": 2904.7599999999998, "text": " something like test, I could do okay, I could do Tim, which is my name, and we", "tokens": [50364, 746, 411, 1500, 11, 286, 727, 360, 1392, 11, 286, 727, 360, 7172, 11, 597, 307, 452, 1315, 11, 293, 321, 50646], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 887, "seek": 289912, "start": 2904.7599999999998, "end": 2908.2799999999997, "text": " can run this and we're not going to get any output obviously here. But this is", "tokens": [50646, 393, 1190, 341, 293, 321, 434, 406, 516, 281, 483, 604, 5598, 2745, 510, 13, 583, 341, 307, 50822], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 888, "seek": 289912, "start": 2908.2799999999997, "end": 2914.08, "text": " what we would call a rank one tensor, because it is simply one list, one array,", "tokens": [50822, 437, 321, 576, 818, 257, 6181, 472, 40863, 11, 570, 309, 307, 2935, 472, 1329, 11, 472, 10225, 11, 51112], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 889, "seek": 289912, "start": 2914.2799999999997, "end": 2918.8399999999997, "text": " which means one dimension. And again, you know, that's also like a vector. Now", "tokens": [51122, 597, 1355, 472, 10139, 13, 400, 797, 11, 291, 458, 11, 300, 311, 611, 411, 257, 8062, 13, 823, 51350], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 890, "seek": 289912, "start": 2918.8399999999997, "end": 2923.2799999999997, "text": " this, what we're looking at here is a rank to tensor. The reason this is a rank", "tokens": [51350, 341, 11, 437, 321, 434, 1237, 412, 510, 307, 257, 6181, 281, 40863, 13, 440, 1778, 341, 307, 257, 6181, 51572], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 891, "seek": 289912, "start": 2923.3199999999997, "end": 2927.68, "text": " to tensor is because we have a list inside of a list, or in this case,", "tokens": [51574, 281, 40863, 307, 570, 321, 362, 257, 1329, 1854, 295, 257, 1329, 11, 420, 294, 341, 1389, 11, 51792], "temperature": 0.0, "avg_logprob": -0.1245284265325975, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.009707656688988209}, {"id": 892, "seek": 292768, "start": 2927.72, "end": 2932.0, "text": " multiple lists inside of a list. So the way that you can actually determine the", "tokens": [50366, 3866, 14511, 1854, 295, 257, 1329, 13, 407, 264, 636, 300, 291, 393, 767, 6997, 264, 50580], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 893, "seek": 292768, "start": 2932.0, "end": 2937.7599999999998, "text": " rank of a tensor is the deepest level of a nested list, at least in Python with", "tokens": [50580, 6181, 295, 257, 40863, 307, 264, 28288, 1496, 295, 257, 15646, 292, 1329, 11, 412, 1935, 294, 15329, 365, 50868], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 894, "seek": 292768, "start": 2937.7599999999998, "end": 2942.6, "text": " our representation, that's what that is. So here we can see we have a list inside", "tokens": [50868, 527, 10290, 11, 300, 311, 437, 300, 307, 13, 407, 510, 321, 393, 536, 321, 362, 257, 1329, 1854, 51110], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 895, "seek": 292768, "start": 2942.6, "end": 2946.3199999999997, "text": " of a list, and then another list inside of this upper list. So this would give us", "tokens": [51110, 295, 257, 1329, 11, 293, 550, 1071, 1329, 1854, 295, 341, 6597, 1329, 13, 407, 341, 576, 976, 505, 51296], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 896, "seek": 292768, "start": 2946.48, "end": 2951.24, "text": " rank two. And this is what we typically call a matrices. And this again, is going", "tokens": [51304, 6181, 732, 13, 400, 341, 307, 437, 321, 5850, 818, 257, 32284, 13, 400, 341, 797, 11, 307, 516, 51542], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 897, "seek": 292768, "start": 2951.24, "end": 2956.56, "text": " to be of TF dot strings. So that's the data type for this tensor variable. So all", "tokens": [51542, 281, 312, 295, 40964, 5893, 13985, 13, 407, 300, 311, 264, 1412, 2010, 337, 341, 40863, 7006, 13, 407, 439, 51808], "temperature": 0.0, "avg_logprob": -0.119066283816383, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.004754946101456881}, {"id": 898, "seek": 295656, "start": 2956.6, "end": 2960.12, "text": " of these that we've created are tensors, they have a data type, and they have some", "tokens": [50366, 295, 613, 300, 321, 600, 2942, 366, 10688, 830, 11, 436, 362, 257, 1412, 2010, 11, 293, 436, 362, 512, 50542], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 899, "seek": 295656, "start": 2960.12, "end": 2963.88, "text": " rank and some shape, and we're going to talk about the shape in a second. So to", "tokens": [50542, 6181, 293, 512, 3909, 11, 293, 321, 434, 516, 281, 751, 466, 264, 3909, 294, 257, 1150, 13, 407, 281, 50730], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 900, "seek": 295656, "start": 2963.88, "end": 2968.84, "text": " determine the rank of a tensor, we can simply use the method TF dot rank. So", "tokens": [50730, 6997, 264, 6181, 295, 257, 40863, 11, 321, 393, 2935, 764, 264, 3170, 40964, 5893, 6181, 13, 407, 50978], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 901, "seek": 295656, "start": 2968.84, "end": 2974.16, "text": " notice when I run this, we get the shape which is blank of rank to tensor. That's", "tokens": [50978, 3449, 562, 286, 1190, 341, 11, 321, 483, 264, 3909, 597, 307, 8247, 295, 6181, 281, 40863, 13, 663, 311, 51244], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 902, "seek": 295656, "start": 2974.16, "end": 2979.56, "text": " fine. And then we get num pi two, which simply means that this is of rank two. Now", "tokens": [51244, 2489, 13, 400, 550, 321, 483, 1031, 3895, 732, 11, 597, 2935, 1355, 300, 341, 307, 295, 6181, 732, 13, 823, 51514], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 903, "seek": 295656, "start": 2979.56, "end": 2984.16, "text": " if I go for that rank one tensor, and I print this out. So let's have a look at", "tokens": [51514, 498, 286, 352, 337, 300, 6181, 472, 40863, 11, 293, 286, 4482, 341, 484, 13, 407, 718, 311, 362, 257, 574, 412, 51744], "temperature": 0.0, "avg_logprob": -0.10154058668348524, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0025507889222353697}, {"id": 904, "seek": 298416, "start": 2984.16, "end": 2988.8399999999997, "text": " it, we get num pi one here, which is telling us that this is simply of rank", "tokens": [50364, 309, 11, 321, 483, 1031, 3895, 472, 510, 11, 597, 307, 3585, 505, 300, 341, 307, 2935, 295, 6181, 50598], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 905, "seek": 298416, "start": 2988.92, "end": 2992.3199999999997, "text": " one. Now if I want to use one of these ones up here and see what it is, so let's", "tokens": [50602, 472, 13, 823, 498, 286, 528, 281, 764, 472, 295, 613, 2306, 493, 510, 293, 536, 437, 309, 307, 11, 370, 718, 311, 50772], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 906, "seek": 298416, "start": 2992.3199999999997, "end": 2997.3199999999997, "text": " try it, we can do numbers. So TF dot ring numbers. So we'll print that here. And", "tokens": [50772, 853, 309, 11, 321, 393, 360, 3547, 13, 407, 40964, 5893, 4875, 3547, 13, 407, 321, 603, 4482, 300, 510, 13, 400, 51022], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 907, "seek": 298416, "start": 2997.3199999999997, "end": 3000.68, "text": " we get num pi zero, because that's rank zero, right? So we'll go back to what we", "tokens": [51022, 321, 483, 1031, 3895, 4018, 11, 570, 300, 311, 6181, 4018, 11, 558, 30, 407, 321, 603, 352, 646, 281, 437, 321, 51190], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 908, "seek": 298416, "start": 3000.68, "end": 3003.92, "text": " had, which was ranked to tensor. But again, those are kind of the examples we", "tokens": [51190, 632, 11, 597, 390, 20197, 281, 40863, 13, 583, 797, 11, 729, 366, 733, 295, 264, 5110, 321, 51352], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 909, "seek": 298416, "start": 3003.92, "end": 3008.08, "text": " want to look at. Okay, so shapes of a tensor. So this is a little bit different", "tokens": [51352, 528, 281, 574, 412, 13, 1033, 11, 370, 10854, 295, 257, 40863, 13, 407, 341, 307, 257, 707, 857, 819, 51560], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 910, "seek": 298416, "start": 3008.08, "end": 3013.52, "text": " now. What a shape simply tells us is how many items we have in each dimension. So", "tokens": [51560, 586, 13, 708, 257, 3909, 2935, 5112, 505, 307, 577, 867, 4754, 321, 362, 294, 1184, 10139, 13, 407, 51832], "temperature": 0.0, "avg_logprob": -0.11267092242930671, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.013635281473398209}, {"id": 911, "seek": 301352, "start": 3013.52, "end": 3018.72, "text": " in this case, when we're looking at rank two, tensor dot shape, so we have dot", "tokens": [50364, 294, 341, 1389, 11, 562, 321, 434, 1237, 412, 6181, 732, 11, 40863, 5893, 3909, 11, 370, 321, 362, 5893, 50624], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 912, "seek": 301352, "start": 3018.72, "end": 3022.96, "text": " shape here, that's an attribute of all of our tensors, we get two two. Now let's", "tokens": [50624, 3909, 510, 11, 300, 311, 364, 19667, 295, 439, 295, 527, 10688, 830, 11, 321, 483, 732, 732, 13, 823, 718, 311, 50836], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 913, "seek": 301352, "start": 3022.96, "end": 3028.4, "text": " look up here. What we have is Whoa, look at this two, and two. So we have two", "tokens": [50836, 574, 493, 510, 13, 708, 321, 362, 307, 7521, 11, 574, 412, 341, 732, 11, 293, 732, 13, 407, 321, 362, 732, 51108], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 914, "seek": 301352, "start": 3028.4, "end": 3031.12, "text": " elements in the first dimension, right, and then two elements in the second", "tokens": [51108, 4959, 294, 264, 700, 10139, 11, 558, 11, 293, 550, 732, 4959, 294, 264, 1150, 51244], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 915, "seek": 301352, "start": 3031.12, "end": 3035.2, "text": " dimension. That's pretty much what this is telling us. Now let's look at the rank", "tokens": [51244, 10139, 13, 663, 311, 1238, 709, 437, 341, 307, 3585, 505, 13, 823, 718, 311, 574, 412, 264, 6181, 51448], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 916, "seek": 301352, "start": 3035.48, "end": 3040.96, "text": " for the shape of rank one tensor, we get three. So because we only have a rank", "tokens": [51462, 337, 264, 3909, 295, 6181, 472, 40863, 11, 321, 483, 1045, 13, 407, 570, 321, 787, 362, 257, 6181, 51736], "temperature": 0.0, "avg_logprob": -0.13108480613650256, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0006263154791668057}, {"id": 917, "seek": 304096, "start": 3040.96, "end": 3046.56, "text": " one, notice we only get one number. Whereas when we had rank two, we got two", "tokens": [50364, 472, 11, 3449, 321, 787, 483, 472, 1230, 13, 13813, 562, 321, 632, 6181, 732, 11, 321, 658, 732, 50644], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 918, "seek": 304096, "start": 3046.56, "end": 3049.88, "text": " numbers, and it told us how many elements were in each of these lists, right? So if", "tokens": [50644, 3547, 11, 293, 309, 1907, 505, 577, 867, 4959, 645, 294, 1184, 295, 613, 14511, 11, 558, 30, 407, 498, 50810], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 919, "seek": 304096, "start": 3049.88, "end": 3054.48, "text": " I go and I add another one here, like that, and we have a look now at the shape.", "tokens": [50810, 286, 352, 293, 286, 909, 1071, 472, 510, 11, 411, 300, 11, 293, 321, 362, 257, 574, 586, 412, 264, 3909, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 920, "seek": 304096, "start": 3055.08, "end": 3060.4, "text": " Oops, I got to run this first. So that's something can convert non square to", "tokens": [51070, 21726, 11, 286, 658, 281, 1190, 341, 700, 13, 407, 300, 311, 746, 393, 7620, 2107, 3732, 281, 51336], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 921, "seek": 304096, "start": 3060.44, "end": 3064.32, "text": " tensor. Ah, sorry, so I need to have a uniform amount of elements in each one", "tokens": [51338, 40863, 13, 2438, 11, 2597, 11, 370, 286, 643, 281, 362, 257, 9452, 2372, 295, 4959, 294, 1184, 472, 51532], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 922, "seek": 304096, "start": 3064.32, "end": 3068.56, "text": " here, I can't just do what I did there. So add a third element here. Now what we", "tokens": [51532, 510, 11, 286, 393, 380, 445, 360, 437, 286, 630, 456, 13, 407, 909, 257, 2636, 4478, 510, 13, 823, 437, 321, 51744], "temperature": 0.0, "avg_logprob": -0.12957007849394386, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0047549894079566}, {"id": 923, "seek": 306856, "start": 3068.56, "end": 3074.12, "text": " can do is run this shouldn't get any issues. Let's have a look at the shape and", "tokens": [50364, 393, 360, 307, 1190, 341, 4659, 380, 483, 604, 2663, 13, 961, 311, 362, 257, 574, 412, 264, 3909, 293, 50642], "temperature": 0.0, "avg_logprob": -0.1362443056973544, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.01854465715587139}, {"id": 924, "seek": 306856, "start": 3074.12, "end": 3079.88, "text": " notice we get now two three. So we have two lists, and each of those lists have", "tokens": [50642, 3449, 321, 483, 586, 732, 1045, 13, 407, 321, 362, 732, 14511, 11, 293, 1184, 295, 729, 14511, 362, 50930], "temperature": 0.0, "avg_logprob": -0.1362443056973544, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.01854465715587139}, {"id": 925, "seek": 306856, "start": 3079.88, "end": 3084.0, "text": " three elements inside of them. So that's how the shape works. Now I could go ahead", "tokens": [50930, 1045, 4959, 1854, 295, 552, 13, 407, 300, 311, 577, 264, 3909, 1985, 13, 823, 286, 727, 352, 2286, 51136], "temperature": 0.0, "avg_logprob": -0.1362443056973544, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.01854465715587139}, {"id": 926, "seek": 306856, "start": 3084.04, "end": 3089.6, "text": " and add another list in here if I wanted to and I could say like, okay, okay,", "tokens": [51138, 293, 909, 1071, 1329, 294, 510, 498, 286, 1415, 281, 293, 286, 727, 584, 411, 11, 1392, 11, 1392, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1362443056973544, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.01854465715587139}, {"id": 927, "seek": 306856, "start": 3090.96, "end": 3095.4, "text": " okay, so let's run this hopefully no errors. Looks like we're good. Now let's", "tokens": [51484, 1392, 11, 370, 718, 311, 1190, 341, 4696, 572, 13603, 13, 10027, 411, 321, 434, 665, 13, 823, 718, 311, 51706], "temperature": 0.0, "avg_logprob": -0.1362443056973544, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.01854465715587139}, {"id": 928, "seek": 309540, "start": 3095.4, "end": 3098.4, "text": " look at the shape again. And now we get a shape of three, three, because we have", "tokens": [50364, 574, 412, 264, 3909, 797, 13, 400, 586, 321, 483, 257, 3909, 295, 1045, 11, 1045, 11, 570, 321, 362, 50514], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 929, "seek": 309540, "start": 3098.44, "end": 3102.64, "text": " three interior lists. And in each of those lists, we have three elements. And", "tokens": [50516, 1045, 10636, 14511, 13, 400, 294, 1184, 295, 729, 14511, 11, 321, 362, 1045, 4959, 13, 400, 50726], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 930, "seek": 309540, "start": 3102.64, "end": 3106.88, "text": " that is pretty much how that works. Now again, we could go even further here and", "tokens": [50726, 300, 307, 1238, 709, 577, 300, 1985, 13, 823, 797, 11, 321, 727, 352, 754, 3052, 510, 293, 50938], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 931, "seek": 309540, "start": 3106.88, "end": 3110.76, "text": " we could put another list inside of here that would give us a rank three tensor.", "tokens": [50938, 321, 727, 829, 1071, 1329, 1854, 295, 510, 300, 576, 976, 505, 257, 6181, 1045, 40863, 13, 51132], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 932, "seek": 309540, "start": 3110.76, "end": 3114.52, "text": " And we'd have to do that inside of all of these lists. And then what that would", "tokens": [51132, 400, 321, 1116, 362, 281, 360, 300, 1854, 295, 439, 295, 613, 14511, 13, 400, 550, 437, 300, 576, 51320], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 933, "seek": 309540, "start": 3114.52, "end": 3118.6800000000003, "text": " give us now would be three numbers representing how many elements we have in", "tokens": [51320, 976, 505, 586, 576, 312, 1045, 3547, 13460, 577, 867, 4959, 321, 362, 294, 51528], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 934, "seek": 309540, "start": 3118.7200000000003, "end": 3124.6, "text": " each of those different dimensions. Okay, so changing shape. Alright, so this is", "tokens": [51530, 1184, 295, 729, 819, 12819, 13, 1033, 11, 370, 4473, 3909, 13, 2798, 11, 370, 341, 307, 51824], "temperature": 0.0, "avg_logprob": -0.10359151121498882, "compression_ratio": 1.9857651245551602, "no_speech_prob": 0.01912228949368}, {"id": 935, "seek": 312460, "start": 3124.64, "end": 3128.0, "text": " what we need to do a lot of times when we're dealing with tensors and tensor", "tokens": [50366, 437, 321, 643, 281, 360, 257, 688, 295, 1413, 562, 321, 434, 6260, 365, 10688, 830, 293, 40863, 50534], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 936, "seek": 312460, "start": 3128.0, "end": 3132.36, "text": " flow. So essentially, there is many different shapes that can represent the", "tokens": [50534, 3095, 13, 407, 4476, 11, 456, 307, 867, 819, 10854, 300, 393, 2906, 264, 50752], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 937, "seek": 312460, "start": 3132.36, "end": 3138.16, "text": " same number of elements. So up here, we have three elements in a rank one", "tokens": [50752, 912, 1230, 295, 4959, 13, 407, 493, 510, 11, 321, 362, 1045, 4959, 294, 257, 6181, 472, 51042], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 938, "seek": 312460, "start": 3138.16, "end": 3143.4, "text": " tensor. And then here we have nine elements in a rank two tensor. Now there's", "tokens": [51042, 40863, 13, 400, 550, 510, 321, 362, 4949, 4959, 294, 257, 6181, 732, 40863, 13, 823, 456, 311, 51304], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 939, "seek": 312460, "start": 3143.4, "end": 3147.6, "text": " ways that we can reshape this data so that we have the same amount of", "tokens": [51304, 2098, 300, 321, 393, 725, 42406, 341, 1412, 370, 300, 321, 362, 264, 912, 2372, 295, 51514], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 940, "seek": 312460, "start": 3147.6, "end": 3151.8399999999997, "text": " elements, but in a different shape. For example, I could flatten this, right,", "tokens": [51514, 4959, 11, 457, 294, 257, 819, 3909, 13, 1171, 1365, 11, 286, 727, 24183, 341, 11, 558, 11, 51726], "temperature": 0.0, "avg_logprob": -0.09867895375127378, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.0015010540373623371}, {"id": 941, "seek": 315184, "start": 3151.84, "end": 3156.4, "text": " take all of these elements and throw them into a rank one tensor that simply is", "tokens": [50364, 747, 439, 295, 613, 4959, 293, 3507, 552, 666, 257, 6181, 472, 40863, 300, 2935, 307, 50592], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 942, "seek": 315184, "start": 3156.4, "end": 3160.84, "text": " a length of nine elements. So how do we do that? Well, let me just run this code", "tokens": [50592, 257, 4641, 295, 4949, 4959, 13, 407, 577, 360, 321, 360, 300, 30, 1042, 11, 718, 385, 445, 1190, 341, 3089, 50814], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 943, "seek": 315184, "start": 3160.84, "end": 3163.52, "text": " for us here and have a look at this. So what we've done is we've created tensor", "tokens": [50814, 337, 505, 510, 293, 362, 257, 574, 412, 341, 13, 407, 437, 321, 600, 1096, 307, 321, 600, 2942, 40863, 50948], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 944, "seek": 315184, "start": 3163.52, "end": 3168.0, "text": " one, that is TF dot ones, what this stands for is we're going to create a", "tokens": [50948, 472, 11, 300, 307, 40964, 5893, 2306, 11, 437, 341, 7382, 337, 307, 321, 434, 516, 281, 1884, 257, 51172], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 945, "seek": 315184, "start": 3168.0, "end": 3174.88, "text": " tensor that simply is populated completely with ones of this shape. So shape one,", "tokens": [51172, 40863, 300, 2935, 307, 32998, 2584, 365, 2306, 295, 341, 3909, 13, 407, 3909, 472, 11, 51516], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 946, "seek": 315184, "start": 3174.88, "end": 3178.56, "text": " two, three, which means, you know, that's the shape we're going to get. So let's", "tokens": [51516, 732, 11, 1045, 11, 597, 1355, 11, 291, 458, 11, 300, 311, 264, 3909, 321, 434, 516, 281, 483, 13, 407, 718, 311, 51700], "temperature": 0.0, "avg_logprob": -0.10734306010164955, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.017440402880311012}, {"id": 947, "seek": 317856, "start": 3178.56, "end": 3183.52, "text": " print this out and look at tensor one, just so I can better illustrate this. So", "tokens": [50364, 4482, 341, 484, 293, 574, 412, 40863, 472, 11, 445, 370, 286, 393, 1101, 23221, 341, 13, 407, 50612], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 948, "seek": 317856, "start": 3183.52, "end": 3189.64, "text": " tensor one, look at the shape that we have one, two, three, right? So we have one", "tokens": [50612, 40863, 472, 11, 574, 412, 264, 3909, 300, 321, 362, 472, 11, 732, 11, 1045, 11, 558, 30, 407, 321, 362, 472, 50918], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 949, "seek": 317856, "start": 3189.64, "end": 3193.44, "text": " interior list, which we're looking at here. And then we have two lists inside", "tokens": [50918, 10636, 1329, 11, 597, 321, 434, 1237, 412, 510, 13, 400, 550, 321, 362, 732, 14511, 1854, 51108], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 950, "seek": 317856, "start": 3193.44, "end": 3197.2, "text": " of that list. And then each of those lists, we have three elements. So that's", "tokens": [51108, 295, 300, 1329, 13, 400, 550, 1184, 295, 729, 14511, 11, 321, 362, 1045, 4959, 13, 407, 300, 311, 51296], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 951, "seek": 317856, "start": 3197.2, "end": 3201.7999999999997, "text": " the shape we just defined. Now we have six elements inside of here. So there", "tokens": [51296, 264, 3909, 321, 445, 7642, 13, 823, 321, 362, 2309, 4959, 1854, 295, 510, 13, 407, 456, 51526], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 952, "seek": 317856, "start": 3201.7999999999997, "end": 3205.68, "text": " must be a way that we can reshape this data to have six elements, but in a", "tokens": [51526, 1633, 312, 257, 636, 300, 321, 393, 725, 42406, 341, 1412, 281, 362, 2309, 4959, 11, 457, 294, 257, 51720], "temperature": 0.0, "avg_logprob": -0.0839046902126736, "compression_ratio": 1.93801652892562, "no_speech_prob": 0.00460939621552825}, {"id": 953, "seek": 320568, "start": 3205.72, "end": 3210.56, "text": " different shape. In fact, what we can do is reshape this into a two, three, one", "tokens": [50366, 819, 3909, 13, 682, 1186, 11, 437, 321, 393, 360, 307, 725, 42406, 341, 666, 257, 732, 11, 1045, 11, 472, 50608], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 954, "seek": 320568, "start": 3210.56, "end": 3214.2, "text": " shape, where we're going to have two lists, right? We're going to have three", "tokens": [50608, 3909, 11, 689, 321, 434, 516, 281, 362, 732, 14511, 11, 558, 30, 492, 434, 516, 281, 362, 1045, 50790], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 955, "seek": 320568, "start": 3214.2, "end": 3216.7999999999997, "text": " inside of those. And then inside of each of those, we're going to have one", "tokens": [50790, 1854, 295, 729, 13, 400, 550, 1854, 295, 1184, 295, 729, 11, 321, 434, 516, 281, 362, 472, 50920], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 956, "seek": 320568, "start": 3216.7999999999997, "end": 3220.6, "text": " element. So let's have a look at that one. So let's have a look at tensor two.", "tokens": [50920, 4478, 13, 407, 718, 311, 362, 257, 574, 412, 300, 472, 13, 407, 718, 311, 362, 257, 574, 412, 40863, 732, 13, 51110], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 957, "seek": 320568, "start": 3220.6, "end": 3223.3599999999997, "text": " Actually, what am I doing? We print all we can print all of them here. So let's", "tokens": [51110, 5135, 11, 437, 669, 286, 884, 30, 492, 4482, 439, 321, 393, 4482, 439, 295, 552, 510, 13, 407, 718, 311, 51248], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 958, "seek": 320568, "start": 3223.3599999999997, "end": 3226.68, "text": " just print them and have a look at them. So when we look at tensor one, we saw", "tokens": [51248, 445, 4482, 552, 293, 362, 257, 574, 412, 552, 13, 407, 562, 321, 574, 412, 40863, 472, 11, 321, 1866, 51414], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 959, "seek": 320568, "start": 3226.68, "end": 3230.8399999999997, "text": " this was a shape. And now we look at this tensor two. And we can see that we", "tokens": [51414, 341, 390, 257, 3909, 13, 400, 586, 321, 574, 412, 341, 40863, 732, 13, 400, 321, 393, 536, 300, 321, 51622], "temperature": 0.0, "avg_logprob": -0.08011837370076756, "compression_ratio": 2.116279069767442, "no_speech_prob": 0.011686260811984539}, {"id": 960, "seek": 323084, "start": 3230.84, "end": 3235.2400000000002, "text": " have two lists, right? Inside of each of those lists, we have three lists. And", "tokens": [50364, 362, 732, 14511, 11, 558, 30, 15123, 295, 1184, 295, 729, 14511, 11, 321, 362, 1045, 14511, 13, 400, 50584], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 961, "seek": 323084, "start": 3235.2400000000002, "end": 3239.7200000000003, "text": " inside of each of those lists, we have one element. Now, finally, our tensor", "tokens": [50584, 1854, 295, 1184, 295, 729, 14511, 11, 321, 362, 472, 4478, 13, 823, 11, 2721, 11, 527, 40863, 50808], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 962, "seek": 323084, "start": 3239.7200000000003, "end": 3245.1600000000003, "text": " three is a shape of three negative one. Well, what is negative one? When we put", "tokens": [50808, 1045, 307, 257, 3909, 295, 1045, 3671, 472, 13, 1042, 11, 437, 307, 3671, 472, 30, 1133, 321, 829, 51080], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 963, "seek": 323084, "start": 3245.1600000000003, "end": 3249.84, "text": " negative one here, what this does is infer what this number actually needs to", "tokens": [51080, 3671, 472, 510, 11, 437, 341, 775, 307, 13596, 437, 341, 1230, 767, 2203, 281, 51314], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 964, "seek": 323084, "start": 3249.84, "end": 3254.88, "text": " be. So if we define an initial shape of three, what this does is say, Okay, we're", "tokens": [51314, 312, 13, 407, 498, 321, 6964, 364, 5883, 3909, 295, 1045, 11, 437, 341, 775, 307, 584, 11, 1033, 11, 321, 434, 51566], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 965, "seek": 323084, "start": 3254.88, "end": 3259.6400000000003, "text": " going to have three lists. That's our first level. And then we need to figure", "tokens": [51566, 516, 281, 362, 1045, 14511, 13, 663, 311, 527, 700, 1496, 13, 400, 550, 321, 643, 281, 2573, 51804], "temperature": 0.0, "avg_logprob": -0.095126220703125, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.03845840319991112}, {"id": 966, "seek": 325964, "start": 3259.64, "end": 3263.24, "text": " out based on how many elements we have in this reshape, which is the method we're", "tokens": [50364, 484, 2361, 322, 577, 867, 4959, 321, 362, 294, 341, 725, 42406, 11, 597, 307, 264, 3170, 321, 434, 50544], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 967, "seek": 325964, "start": 3263.24, "end": 3266.44, "text": " using, which I didn't even talk about, which we'll go into a second, what this", "tokens": [50544, 1228, 11, 597, 286, 994, 380, 754, 751, 466, 11, 597, 321, 603, 352, 666, 257, 1150, 11, 437, 341, 50704], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 968, "seek": 325964, "start": 3266.44, "end": 3270.8399999999997, "text": " next dimension should be. Now, obviously, this is going to need to be three. So three", "tokens": [50704, 958, 10139, 820, 312, 13, 823, 11, 2745, 11, 341, 307, 516, 281, 643, 281, 312, 1045, 13, 407, 1045, 50924], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 969, "seek": 325964, "start": 3270.8399999999997, "end": 3274.24, "text": " three, right, because we're going to have three lists inside of each of those lists", "tokens": [50924, 1045, 11, 558, 11, 570, 321, 434, 516, 281, 362, 1045, 14511, 1854, 295, 1184, 295, 729, 14511, 51094], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 970, "seek": 325964, "start": 3274.24, "end": 3277.0, "text": " we need to have. Or actually, is that correct? Let's see if that's even the", "tokens": [51094, 321, 643, 281, 362, 13, 1610, 767, 11, 307, 300, 3006, 30, 961, 311, 536, 498, 300, 311, 754, 264, 51232], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 971, "seek": 325964, "start": 3277.0, "end": 3281.4, "text": " shape, three, two, my bad. So this actually needs to change to three, two, I", "tokens": [51232, 3909, 11, 1045, 11, 732, 11, 452, 1578, 13, 407, 341, 767, 2203, 281, 1319, 281, 1045, 11, 732, 11, 286, 51452], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 972, "seek": 325964, "start": 3281.4, "end": 3285.0, "text": " don't know why I wrote three, three there. But you get the point, right? So what", "tokens": [51452, 500, 380, 458, 983, 286, 4114, 1045, 11, 1045, 456, 13, 583, 291, 483, 264, 935, 11, 558, 30, 407, 437, 51632], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 973, "seek": 325964, "start": 3285.0, "end": 3288.16, "text": " this does is we have three lists, we have six elements, this number obviously needs", "tokens": [51632, 341, 775, 307, 321, 362, 1045, 14511, 11, 321, 362, 2309, 4959, 11, 341, 1230, 2745, 2203, 51790], "temperature": 0.0, "avg_logprob": -0.11732310810308347, "compression_ratio": 1.900293255131965, "no_speech_prob": 0.026753779500722885}, {"id": 974, "seek": 328816, "start": 3288.2, "end": 3291.72, "text": " to be two, because well, three times two is going to give us six. And that is", "tokens": [50366, 281, 312, 732, 11, 570, 731, 11, 1045, 1413, 732, 307, 516, 281, 976, 505, 2309, 13, 400, 300, 307, 50542], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 975, "seek": 328816, "start": 3291.72, "end": 3295.3599999999997, "text": " essentially how you can determine how many elements are actually in a tensor by", "tokens": [50542, 4476, 577, 291, 393, 6997, 577, 867, 4959, 366, 767, 294, 257, 40863, 538, 50724], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 976, "seek": 328816, "start": 3295.3599999999997, "end": 3299.6, "text": " just looking at its shape. Now, this is the reshape method, where all we need to", "tokens": [50724, 445, 1237, 412, 1080, 3909, 13, 823, 11, 341, 307, 264, 725, 42406, 3170, 11, 689, 439, 321, 643, 281, 50936], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 977, "seek": 328816, "start": 3299.6, "end": 3303.68, "text": " do is call tf dot reshape, give the tensor and give the shape we want to change", "tokens": [50936, 360, 307, 818, 256, 69, 5893, 725, 42406, 11, 976, 264, 40863, 293, 976, 264, 3909, 321, 528, 281, 1319, 51140], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 978, "seek": 328816, "start": 3303.68, "end": 3308.12, "text": " it to. So long as that's a valid shape. And when we multiply all of the numbers", "tokens": [51140, 309, 281, 13, 407, 938, 382, 300, 311, 257, 7363, 3909, 13, 400, 562, 321, 12972, 439, 295, 264, 3547, 51362], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 979, "seek": 328816, "start": 3308.12, "end": 3311.92, "text": " in here, it's equal to the number of elements in this tensor that will reshape", "tokens": [51362, 294, 510, 11, 309, 311, 2681, 281, 264, 1230, 295, 4959, 294, 341, 40863, 300, 486, 725, 42406, 51552], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 980, "seek": 328816, "start": 3311.92, "end": 3316.2799999999997, "text": " it for us and give us that new shaped data. This is very useful. We'll use this", "tokens": [51552, 309, 337, 505, 293, 976, 505, 300, 777, 13475, 1412, 13, 639, 307, 588, 4420, 13, 492, 603, 764, 341, 51770], "temperature": 0.0, "avg_logprob": -0.07513398737520785, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.04740874841809273}, {"id": 981, "seek": 331628, "start": 3316.32, "end": 3319.6800000000003, "text": " actually a lot as we go through TensorFlow. So make sure you're kind of", "tokens": [50366, 767, 257, 688, 382, 321, 352, 807, 37624, 13, 407, 652, 988, 291, 434, 733, 295, 50534], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 982, "seek": 331628, "start": 3319.6800000000003, "end": 3323.88, "text": " familiar with how that works. All right. So now we're moving on to types of", "tokens": [50534, 4963, 365, 577, 300, 1985, 13, 1057, 558, 13, 407, 586, 321, 434, 2684, 322, 281, 3467, 295, 50744], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 983, "seek": 331628, "start": 3323.92, "end": 3328.6800000000003, "text": " tensors. So there is a bunch of different types of tensors that we can use. So", "tokens": [50746, 10688, 830, 13, 407, 456, 307, 257, 3840, 295, 819, 3467, 295, 10688, 830, 300, 321, 393, 764, 13, 407, 50984], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 984, "seek": 331628, "start": 3328.6800000000003, "end": 3333.4, "text": " far, the only one we've looked at is variable. So we've created tf dot", "tokens": [50984, 1400, 11, 264, 787, 472, 321, 600, 2956, 412, 307, 7006, 13, 407, 321, 600, 2942, 256, 69, 5893, 51220], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 985, "seek": 331628, "start": 3333.4, "end": 3336.6800000000003, "text": " variables and kind of just hard coded our own tensors. We're not really going to", "tokens": [51220, 9102, 293, 733, 295, 445, 1152, 34874, 527, 1065, 10688, 830, 13, 492, 434, 406, 534, 516, 281, 51384], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 986, "seek": 331628, "start": 3336.6800000000003, "end": 3340.96, "text": " do that very much. But just for that example. So we have these different", "tokens": [51384, 360, 300, 588, 709, 13, 583, 445, 337, 300, 1365, 13, 407, 321, 362, 613, 819, 51598], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 987, "seek": 331628, "start": 3340.96, "end": 3345.6400000000003, "text": " types, we have constant placeholder sparse tensor variable. And there's actually", "tokens": [51598, 3467, 11, 321, 362, 5754, 1081, 20480, 637, 11668, 40863, 7006, 13, 400, 456, 311, 767, 51832], "temperature": 0.0, "avg_logprob": -0.11532297751886382, "compression_ratio": 1.7852348993288591, "no_speech_prob": 0.005910638719797134}, {"id": 988, "seek": 334564, "start": 3345.64, "end": 3349.96, "text": " a few other ones as well. Now, we're not going to really talk about these two", "tokens": [50364, 257, 1326, 661, 2306, 382, 731, 13, 823, 11, 321, 434, 406, 516, 281, 534, 751, 466, 613, 732, 50580], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 989, "seek": 334564, "start": 3350.0, "end": 3353.6, "text": " that much, although constant and variable are important to understand the", "tokens": [50582, 300, 709, 11, 4878, 5754, 293, 7006, 366, 1021, 281, 1223, 264, 50762], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 990, "seek": 334564, "start": 3353.6, "end": 3357.7599999999998, "text": " difference between. So we can read this says with the exception of variable, all", "tokens": [50762, 2649, 1296, 13, 407, 321, 393, 1401, 341, 1619, 365, 264, 11183, 295, 7006, 11, 439, 50970], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 991, "seek": 334564, "start": 3357.7599999999998, "end": 3361.08, "text": " of these tensors are immutable, meaning their value may not change during", "tokens": [50970, 295, 613, 10688, 830, 366, 3397, 32148, 11, 3620, 641, 2158, 815, 406, 1319, 1830, 51136], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 992, "seek": 334564, "start": 3361.12, "end": 3365.48, "text": " execution. So essentially, all of these when we create a tensor mean we have", "tokens": [51138, 15058, 13, 407, 4476, 11, 439, 295, 613, 562, 321, 1884, 257, 40863, 914, 321, 362, 51356], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 993, "seek": 334564, "start": 3365.48, "end": 3369.52, "text": " some constant value, which means that whatever we've defined here, it's not", "tokens": [51356, 512, 5754, 2158, 11, 597, 1355, 300, 2035, 321, 600, 7642, 510, 11, 309, 311, 406, 51558], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 994, "seek": 334564, "start": 3369.52, "end": 3374.16, "text": " going to change. Whereas the variable tensor could change. So that's just", "tokens": [51558, 516, 281, 1319, 13, 13813, 264, 7006, 40863, 727, 1319, 13, 407, 300, 311, 445, 51790], "temperature": 0.0, "avg_logprob": -0.08942886352539063, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.0017544623697176576}, {"id": 995, "seek": 337416, "start": 3374.16, "end": 3377.56, "text": " something to keep in mind when we use variable, that's because we think we", "tokens": [50364, 746, 281, 1066, 294, 1575, 562, 321, 764, 7006, 11, 300, 311, 570, 321, 519, 321, 50534], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 996, "seek": 337416, "start": 3377.56, "end": 3380.7599999999998, "text": " might need to change the value of that tensor later on. Whereas if we're using", "tokens": [50534, 1062, 643, 281, 1319, 264, 2158, 295, 300, 40863, 1780, 322, 13, 13813, 498, 321, 434, 1228, 50694], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 997, "seek": 337416, "start": 3380.7599999999998, "end": 3384.08, "text": " a constant value tensor, we cannot change it. So that's just something to keep", "tokens": [50694, 257, 5754, 2158, 40863, 11, 321, 2644, 1319, 309, 13, 407, 300, 311, 445, 746, 281, 1066, 50860], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 998, "seek": 337416, "start": 3384.08, "end": 3388.44, "text": " in mind, we can obviously copy it, but we can't change it. Okay, so evaluating", "tokens": [50860, 294, 1575, 11, 321, 393, 2745, 5055, 309, 11, 457, 321, 393, 380, 1319, 309, 13, 1033, 11, 370, 27479, 51078], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 999, "seek": 337416, "start": 3388.44, "end": 3391.08, "text": " tensors, we're almost at the end of the section, I know, and then we'll get into", "tokens": [51078, 10688, 830, 11, 321, 434, 1920, 412, 264, 917, 295, 264, 3541, 11, 286, 458, 11, 293, 550, 321, 603, 483, 666, 51210], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 1000, "seek": 337416, "start": 3391.08, "end": 3395.2799999999997, "text": " some more kind of deeper code. So there will be some times for this guide, we", "tokens": [51210, 512, 544, 733, 295, 7731, 3089, 13, 407, 456, 486, 312, 512, 1413, 337, 341, 5934, 11, 321, 51420], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 1001, "seek": 337416, "start": 3395.2799999999997, "end": 3398.96, "text": " need to evaluate a tense, of course, so what we need to do to evaluate a tensor", "tokens": [51420, 643, 281, 13059, 257, 18760, 11, 295, 1164, 11, 370, 437, 321, 643, 281, 360, 281, 13059, 257, 40863, 51604], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 1002, "seek": 337416, "start": 3398.96, "end": 3403.8799999999997, "text": " is create a session. Now, this isn't really like, we're not going to do this that", "tokens": [51604, 307, 1884, 257, 5481, 13, 823, 11, 341, 1943, 380, 534, 411, 11, 321, 434, 406, 516, 281, 360, 341, 300, 51850], "temperature": 0.0, "avg_logprob": -0.10285159519740514, "compression_ratio": 1.9386503067484662, "no_speech_prob": 0.01168608758598566}, {"id": 1003, "seek": 340388, "start": 3403.88, "end": 3407.0, "text": " much. But I just figured I'd mention it to make sure that you guys are aware of", "tokens": [50364, 709, 13, 583, 286, 445, 8932, 286, 1116, 2152, 309, 281, 652, 988, 300, 291, 1074, 366, 3650, 295, 50520], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1004, "seek": 340388, "start": 3407.0, "end": 3411.0, "text": " what I'm doing. If I start kind of typing this later on. Essentially, sometimes", "tokens": [50520, 437, 286, 478, 884, 13, 759, 286, 722, 733, 295, 18444, 341, 1780, 322, 13, 23596, 11, 2171, 50720], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1005, "seek": 340388, "start": 3411.0, "end": 3414.7200000000003, "text": " we have some tensor object. And throughout our code, we actually need to", "tokens": [50720, 321, 362, 512, 40863, 2657, 13, 400, 3710, 527, 3089, 11, 321, 767, 643, 281, 50906], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1006, "seek": 340388, "start": 3414.7200000000003, "end": 3419.52, "text": " evaluate it to be able to do something else. So to do that, all we need to do", "tokens": [50906, 13059, 309, 281, 312, 1075, 281, 360, 746, 1646, 13, 407, 281, 360, 300, 11, 439, 321, 643, 281, 360, 51146], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1007, "seek": 340388, "start": 3419.56, "end": 3424.2400000000002, "text": " is literally just use this kind of default template, a block of code. Well, we", "tokens": [51148, 307, 3736, 445, 764, 341, 733, 295, 7576, 12379, 11, 257, 3461, 295, 3089, 13, 1042, 11, 321, 51382], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1008, "seek": 340388, "start": 3424.4, "end": 3428.44, "text": " say with TF dot session, as some kind of session doesn't really matter what we", "tokens": [51390, 584, 365, 40964, 5893, 5481, 11, 382, 512, 733, 295, 5481, 1177, 380, 534, 1871, 437, 321, 51592], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1009, "seek": 340388, "start": 3428.44, "end": 3433.76, "text": " put here, then we can just do whatever the tensor name is dot eval. And calling", "tokens": [51592, 829, 510, 11, 550, 321, 393, 445, 360, 2035, 264, 40863, 1315, 307, 5893, 1073, 304, 13, 400, 5141, 51858], "temperature": 0.0, "avg_logprob": -0.12377986102037027, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.004069556016474962}, {"id": 1010, "seek": 343376, "start": 3433.76, "end": 3437.2400000000002, "text": " that will actually have TensorFlow just figure out what it needs to do to find", "tokens": [50364, 300, 486, 767, 362, 37624, 445, 2573, 484, 437, 309, 2203, 281, 360, 281, 915, 50538], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1011, "seek": 343376, "start": 3437.2400000000002, "end": 3440.6000000000004, "text": " the value of this tensor, it will evaluate it, and then it will allow us to", "tokens": [50538, 264, 2158, 295, 341, 40863, 11, 309, 486, 13059, 309, 11, 293, 550, 309, 486, 2089, 505, 281, 50706], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1012, "seek": 343376, "start": 3440.6000000000004, "end": 3443.6400000000003, "text": " actually use that value. So I put this in here, you guys can obviously read", "tokens": [50706, 767, 764, 300, 2158, 13, 407, 286, 829, 341, 294, 510, 11, 291, 1074, 393, 2745, 1401, 50858], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1013, "seek": 343376, "start": 3443.6400000000003, "end": 3446.96, "text": " through this if you want to understand some more in depth on how that works. And", "tokens": [50858, 807, 341, 498, 291, 528, 281, 1223, 512, 544, 294, 7161, 322, 577, 300, 1985, 13, 400, 51024], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1014, "seek": 343376, "start": 3446.96, "end": 3450.0800000000004, "text": " the source for this is straight from the TensorFlow website. A lot of this is", "tokens": [51024, 264, 4009, 337, 341, 307, 2997, 490, 264, 37624, 3144, 13, 316, 688, 295, 341, 307, 51180], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1015, "seek": 343376, "start": 3450.0800000000004, "end": 3454.2000000000003, "text": " straight up copied from there. And I've just kind of added my own spin to it and", "tokens": [51180, 2997, 493, 25365, 490, 456, 13, 400, 286, 600, 445, 733, 295, 3869, 452, 1065, 6060, 281, 309, 293, 51386], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1016, "seek": 343376, "start": 3454.2000000000003, "end": 3457.76, "text": " made it a little bit easier to understand. Okay, so we've done all that. So let's", "tokens": [51386, 1027, 309, 257, 707, 857, 3571, 281, 1223, 13, 1033, 11, 370, 321, 600, 1096, 439, 300, 13, 407, 718, 311, 51564], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1017, "seek": 343376, "start": 3457.76, "end": 3460.88, "text": " just go in here and do a few examples of reshaping just to make sure that", "tokens": [51564, 445, 352, 294, 510, 293, 360, 257, 1326, 5110, 295, 725, 71, 569, 278, 445, 281, 652, 988, 300, 51720], "temperature": 0.0, "avg_logprob": -0.07872674465179444, "compression_ratio": 1.819767441860465, "no_speech_prob": 0.002115469193086028}, {"id": 1018, "seek": 346088, "start": 3460.88, "end": 3463.8, "text": " everyone's kind of on the same page. And then we'll move on to actually talking", "tokens": [50364, 1518, 311, 733, 295, 322, 264, 912, 3028, 13, 400, 550, 321, 603, 1286, 322, 281, 767, 1417, 50510], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1019, "seek": 346088, "start": 3463.8, "end": 3467.84, "text": " about some simple learning algorithms. So I want to create a tensor that we can", "tokens": [50510, 466, 512, 2199, 2539, 14642, 13, 407, 286, 528, 281, 1884, 257, 40863, 300, 321, 393, 50712], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1020, "seek": 346088, "start": 3467.84, "end": 3471.96, "text": " kind of mess with in reshape. So what I'm going to do is just say t equals and", "tokens": [50712, 733, 295, 2082, 365, 294, 725, 42406, 13, 407, 437, 286, 478, 516, 281, 360, 307, 445, 584, 256, 6915, 293, 50918], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1021, "seek": 346088, "start": 3471.96, "end": 3477.56, "text": " we'll say TF dot ones. Now what TF dot ones does is just create again, all of", "tokens": [50918, 321, 603, 584, 40964, 5893, 2306, 13, 823, 437, 40964, 5893, 2306, 775, 307, 445, 1884, 797, 11, 439, 295, 51198], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1022, "seek": 346088, "start": 3477.56, "end": 3481.48, "text": " the values to be ones that we're going to have and whatever shape. Now we can", "tokens": [51198, 264, 4190, 281, 312, 2306, 300, 321, 434, 516, 281, 362, 293, 2035, 3909, 13, 823, 321, 393, 51394], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1023, "seek": 346088, "start": 3481.48, "end": 3485.1600000000003, "text": " also do zeros and zeros is just going to give us a bunch of zeros. And let's", "tokens": [51394, 611, 360, 35193, 293, 35193, 307, 445, 516, 281, 976, 505, 257, 3840, 295, 35193, 13, 400, 718, 311, 51578], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1024, "seek": 346088, "start": 3485.1600000000003, "end": 3488.12, "text": " create some like crazy shape and just visualize this. Let's see like a five", "tokens": [51578, 1884, 512, 411, 3219, 3909, 293, 445, 23273, 341, 13, 961, 311, 536, 411, 257, 1732, 51726], "temperature": 0.0, "avg_logprob": -0.08954124980502659, "compression_ratio": 1.8112582781456954, "no_speech_prob": 0.017439818009734154}, {"id": 1025, "seek": 348812, "start": 3488.12, "end": 3491.7599999999998, "text": " by five by five. So obviously, if we want to figure out how many elements are", "tokens": [50364, 538, 1732, 538, 1732, 13, 407, 2745, 11, 498, 321, 528, 281, 2573, 484, 577, 867, 4959, 366, 50546], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1026, "seek": 348812, "start": 3491.7599999999998, "end": 3494.72, "text": " going to be in here, we need to multiply this value. So I believe this is going to", "tokens": [50546, 516, 281, 312, 294, 510, 11, 321, 643, 281, 12972, 341, 2158, 13, 407, 286, 1697, 341, 307, 516, 281, 50694], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1027, "seek": 348812, "start": 3494.72, "end": 3498.6, "text": " be 625 because that should be five to the power of four. So five times five times", "tokens": [50694, 312, 1386, 6074, 570, 300, 820, 312, 1732, 281, 264, 1347, 295, 1451, 13, 407, 1732, 1413, 1732, 1413, 50888], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1028, "seek": 348812, "start": 3498.6, "end": 3503.16, "text": " five times five. And let's actually print T and have a look at that and see", "tokens": [50888, 1732, 1413, 1732, 13, 400, 718, 311, 767, 4482, 314, 293, 362, 257, 574, 412, 300, 293, 536, 51116], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1029, "seek": 348812, "start": 3503.16, "end": 3506.7599999999998, "text": " what this is. So we run this now. And you can see this is the output we're", "tokens": [51116, 437, 341, 307, 13, 407, 321, 1190, 341, 586, 13, 400, 291, 393, 536, 341, 307, 264, 5598, 321, 434, 51296], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1030, "seek": 348812, "start": 3506.7599999999998, "end": 3510.6, "text": " getting. So obviously, this is a pretty crazy looking tensor, but you get the", "tokens": [51296, 1242, 13, 407, 2745, 11, 341, 307, 257, 1238, 3219, 1237, 40863, 11, 457, 291, 483, 264, 51488], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1031, "seek": 348812, "start": 3510.6, "end": 3515.48, "text": " point, right? And it tells us the shape is 55555. Now watch what happens when I", "tokens": [51488, 935, 11, 558, 30, 400, 309, 5112, 505, 264, 3909, 307, 12330, 13622, 20, 13, 823, 1159, 437, 2314, 562, 286, 51732], "temperature": 0.0, "avg_logprob": -0.11093696491830302, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.019121095538139343}, {"id": 1032, "seek": 351548, "start": 3515.52, "end": 3520.16, "text": " reshape this tensor. So if I want to take all of these elements and flatten them", "tokens": [50366, 725, 42406, 341, 40863, 13, 407, 498, 286, 528, 281, 747, 439, 295, 613, 4959, 293, 24183, 552, 50598], "temperature": 0.0, "avg_logprob": -0.11112181000087572, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.024420730769634247}, {"id": 1033, "seek": 351548, "start": 3520.16, "end": 3528.16, "text": " out, what I could do is simply say, we'll say T equals TF dot reshape like", "tokens": [50598, 484, 11, 437, 286, 727, 360, 307, 2935, 584, 11, 321, 603, 584, 314, 6915, 40964, 5893, 725, 42406, 411, 50998], "temperature": 0.0, "avg_logprob": -0.11112181000087572, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.024420730769634247}, {"id": 1034, "seek": 351548, "start": 3528.16, "end": 3535.04, "text": " that. And we'll reshape the tensor T to just the shape 625. Now if we do this", "tokens": [50998, 300, 13, 400, 321, 603, 725, 42406, 264, 40863, 314, 281, 445, 264, 3909, 1386, 6074, 13, 823, 498, 321, 360, 341, 51342], "temperature": 0.0, "avg_logprob": -0.11112181000087572, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.024420730769634247}, {"id": 1035, "seek": 351548, "start": 3535.08, "end": 3541.2400000000002, "text": " and we run here, oops, I got a print T at the bottom after we've done that if I", "tokens": [51344, 293, 321, 1190, 510, 11, 34166, 11, 286, 658, 257, 4482, 314, 412, 264, 2767, 934, 321, 600, 1096, 300, 498, 286, 51652], "temperature": 0.0, "avg_logprob": -0.11112181000087572, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.024420730769634247}, {"id": 1036, "seek": 354124, "start": 3541.2799999999997, "end": 3546.24, "text": " could spell the print statement correctly, you can see that now we just get this", "tokens": [50366, 727, 9827, 264, 4482, 5629, 8944, 11, 291, 393, 536, 300, 586, 321, 445, 483, 341, 50614], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1037, "seek": 354124, "start": 3546.24, "end": 3552.08, "text": " massive list that just has 625 zeros. And again, if we wanted to reshape this to", "tokens": [50614, 5994, 1329, 300, 445, 575, 1386, 6074, 35193, 13, 400, 797, 11, 498, 321, 1415, 281, 725, 42406, 341, 281, 50906], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1038, "seek": 354124, "start": 3552.08, "end": 3555.3599999999997, "text": " something like 125, and maybe we weren't that good at math and couldn't figure out", "tokens": [50906, 746, 411, 25276, 11, 293, 1310, 321, 4999, 380, 300, 665, 412, 5221, 293, 2809, 380, 2573, 484, 51070], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1039, "seek": 354124, "start": 3555.3599999999997, "end": 3559.3199999999997, "text": " that this last value should be five, we could put a negative one, this would mean", "tokens": [51070, 300, 341, 1036, 2158, 820, 312, 1732, 11, 321, 727, 829, 257, 3671, 472, 11, 341, 576, 914, 51268], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1040, "seek": 354124, "start": 3559.3199999999997, "end": 3563.2, "text": " that TensorFlow would infer now what the shape needs to be. And now when we look", "tokens": [51268, 300, 37624, 576, 13596, 586, 437, 264, 3909, 2203, 281, 312, 13, 400, 586, 562, 321, 574, 51462], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1041, "seek": 354124, "start": 3563.2, "end": 3567.2, "text": " at it, we can see that we're what we're going to get is well, just simply five", "tokens": [51462, 412, 309, 11, 321, 393, 536, 300, 321, 434, 437, 321, 434, 516, 281, 483, 307, 731, 11, 445, 2935, 1732, 51662], "temperature": 0.0, "avg_logprob": -0.09869250174491637, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.016912758350372314}, {"id": 1042, "seek": 356720, "start": 3567.2, "end": 3571.6, "text": " kind of sets of these, I don't know, matrices, whatever you want to call them in", "tokens": [50364, 733, 295, 6352, 295, 613, 11, 286, 500, 380, 458, 11, 32284, 11, 2035, 291, 528, 281, 818, 552, 294, 50584], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1043, "seek": 356720, "start": 3571.6, "end": 3577.0, "text": " our shape is 125 five. So that is essentially how that works. So that's how", "tokens": [50584, 527, 3909, 307, 25276, 1732, 13, 407, 300, 307, 4476, 577, 300, 1985, 13, 407, 300, 311, 577, 50854], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1044, "seek": 356720, "start": 3577.0, "end": 3581.48, "text": " we reshape. That's how we kind of deal with tensors create variables, how that", "tokens": [50854, 321, 725, 42406, 13, 663, 311, 577, 321, 733, 295, 2028, 365, 10688, 830, 1884, 9102, 11, 577, 300, 51078], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1045, "seek": 356720, "start": 3581.48, "end": 3584.64, "text": " works in terms of sessions and graphs. And hopefully with that, that gives you", "tokens": [51078, 1985, 294, 2115, 295, 11081, 293, 24877, 13, 400, 4696, 365, 300, 11, 300, 2709, 291, 51236], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1046, "seek": 356720, "start": 3584.64, "end": 3590.6, "text": " enough of an understanding of tensors of shapes of ranks of value so that when we", "tokens": [51236, 1547, 295, 364, 3701, 295, 10688, 830, 295, 10854, 295, 21406, 295, 2158, 370, 300, 562, 321, 51534], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1047, "seek": 356720, "start": 3590.6, "end": 3593.4399999999996, "text": " move into the next part of the tutorial, where we're actually writing code, and I", "tokens": [51534, 1286, 666, 264, 958, 644, 295, 264, 7073, 11, 689, 321, 434, 767, 3579, 3089, 11, 293, 286, 51676], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1048, "seek": 356720, "start": 3593.4399999999996, "end": 3596.4399999999996, "text": " promise we're going to be writing some more advanced code, you'll understand how", "tokens": [51676, 6228, 321, 434, 516, 281, 312, 3579, 512, 544, 7339, 3089, 11, 291, 603, 1223, 577, 51826], "temperature": 0.0, "avg_logprob": -0.11853569575718471, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.04884467273950577}, {"id": 1049, "seek": 359644, "start": 3596.48, "end": 3603.48, "text": " that works. So with that being said, let's get into the next section. So welcome", "tokens": [50366, 300, 1985, 13, 407, 365, 300, 885, 848, 11, 718, 311, 483, 666, 264, 958, 3541, 13, 407, 2928, 50716], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1050, "seek": 359644, "start": 3603.48, "end": 3607.08, "text": " to module three of this course. Now what we're going to be doing in this module is", "tokens": [50716, 281, 10088, 1045, 295, 341, 1164, 13, 823, 437, 321, 434, 516, 281, 312, 884, 294, 341, 10088, 307, 50896], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1051, "seek": 359644, "start": 3607.08, "end": 3611.32, "text": " learning the core machine learning algorithms that come with TensorFlow. Now", "tokens": [50896, 2539, 264, 4965, 3479, 2539, 14642, 300, 808, 365, 37624, 13, 823, 51108], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1052, "seek": 359644, "start": 3611.32, "end": 3614.48, "text": " these algorithms are not specific to TensorFlow, but they are used within", "tokens": [51108, 613, 14642, 366, 406, 2685, 281, 37624, 11, 457, 436, 366, 1143, 1951, 51266], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1053, "seek": 359644, "start": 3614.48, "end": 3617.64, "text": " there and we'll use some tools from TensorFlow to kind of implement them. But", "tokens": [51266, 456, 293, 321, 603, 764, 512, 3873, 490, 37624, 281, 733, 295, 4445, 552, 13, 583, 51424], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1054, "seek": 359644, "start": 3617.64, "end": 3620.8, "text": " essentially, these are the building blocks before moving on to things like", "tokens": [51424, 4476, 11, 613, 366, 264, 2390, 8474, 949, 2684, 322, 281, 721, 411, 51582], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1055, "seek": 359644, "start": 3620.8, "end": 3624.28, "text": " neural networks and more advanced machine learning techniques. You really need to", "tokens": [51582, 18161, 9590, 293, 544, 7339, 3479, 2539, 7512, 13, 509, 534, 643, 281, 51756], "temperature": 0.0, "avg_logprob": -0.09534487448447992, "compression_ratio": 1.8, "no_speech_prob": 0.004198168404400349}, {"id": 1056, "seek": 362428, "start": 3624.32, "end": 3627.96, "text": " understand how these work because they're kind of used in a lot of different", "tokens": [50366, 1223, 577, 613, 589, 570, 436, 434, 733, 295, 1143, 294, 257, 688, 295, 819, 50548], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1057, "seek": 362428, "start": 3627.96, "end": 3630.96, "text": " techniques and combined together. And one of them but to show you is actually", "tokens": [50548, 7512, 293, 9354, 1214, 13, 400, 472, 295, 552, 457, 281, 855, 291, 307, 767, 50698], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1058, "seek": 362428, "start": 3630.96, "end": 3635.0800000000004, "text": " very powerful if you use it in the right way. A lot of what machine learning", "tokens": [50698, 588, 4005, 498, 291, 764, 309, 294, 264, 558, 636, 13, 316, 688, 295, 437, 3479, 2539, 50904], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1059, "seek": 362428, "start": 3635.0800000000004, "end": 3638.2000000000003, "text": " actually is in a lot of machine learning algorithms and implementations and", "tokens": [50904, 767, 307, 294, 257, 688, 295, 3479, 2539, 14642, 293, 4445, 763, 293, 51060], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1060, "seek": 362428, "start": 3638.2000000000003, "end": 3642.8, "text": " businesses and applications and stuff like that, actually just use pretty basic", "tokens": [51060, 6011, 293, 5821, 293, 1507, 411, 300, 11, 767, 445, 764, 1238, 3875, 51290], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1061, "seek": 362428, "start": 3643.1200000000003, "end": 3646.6800000000003, "text": " models, because these models are capable of actually doing, you know, very", "tokens": [51306, 5245, 11, 570, 613, 5245, 366, 8189, 295, 767, 884, 11, 291, 458, 11, 588, 51484], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1062, "seek": 362428, "start": 3646.6800000000003, "end": 3650.0800000000004, "text": " powerful things. When you're not dealing with anything that's crazy complicated,", "tokens": [51484, 4005, 721, 13, 1133, 291, 434, 406, 6260, 365, 1340, 300, 311, 3219, 6179, 11, 51654], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1063, "seek": 362428, "start": 3650.0800000000004, "end": 3653.6800000000003, "text": " you just need some basic machine learning, some basic classification, you can", "tokens": [51654, 291, 445, 643, 512, 3875, 3479, 2539, 11, 512, 3875, 21538, 11, 291, 393, 51834], "temperature": 0.0, "avg_logprob": -0.1259380340576172, "compression_ratio": 1.9226006191950464, "no_speech_prob": 0.030205003917217255}, {"id": 1064, "seek": 365368, "start": 3653.68, "end": 3657.68, "text": " use these kind of fundamental core learning algorithms. Now the first one", "tokens": [50364, 764, 613, 733, 295, 8088, 4965, 2539, 14642, 13, 823, 264, 700, 472, 50564], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1065, "seek": 365368, "start": 3657.68, "end": 3660.12, "text": " we're going to go through is a linear regression, but we will cover", "tokens": [50564, 321, 434, 516, 281, 352, 807, 307, 257, 8213, 24590, 11, 457, 321, 486, 2060, 50686], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1066, "seek": 365368, "start": 3660.12, "end": 3664.2799999999997, "text": " classification, clustering and hidden Markov models. And those are kind of", "tokens": [50686, 21538, 11, 596, 48673, 293, 7633, 3934, 5179, 5245, 13, 400, 729, 366, 733, 295, 50894], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1067, "seek": 365368, "start": 3664.2799999999997, "end": 3668.72, "text": " going to give us a good spread of the different core algorithms. Now there is", "tokens": [50894, 516, 281, 976, 505, 257, 665, 3974, 295, 264, 819, 4965, 14642, 13, 823, 456, 307, 51116], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1068, "seek": 365368, "start": 3668.7599999999998, "end": 3672.8399999999997, "text": " a ton, ton, like thousands of different machine learning algorithms. These are", "tokens": [51118, 257, 2952, 11, 2952, 11, 411, 5383, 295, 819, 3479, 2539, 14642, 13, 1981, 366, 51322], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1069, "seek": 365368, "start": 3672.8399999999997, "end": 3676.3199999999997, "text": " kind of the main categories that you'll cover. But within these categories,", "tokens": [51322, 733, 295, 264, 2135, 10479, 300, 291, 603, 2060, 13, 583, 1951, 613, 10479, 11, 51496], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1070, "seek": 365368, "start": 3676.3199999999997, "end": 3679.3999999999996, "text": " there is more specific algorithms that you can get into. I just feel like I", "tokens": [51496, 456, 307, 544, 2685, 14642, 300, 291, 393, 483, 666, 13, 286, 445, 841, 411, 286, 51650], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1071, "seek": 365368, "start": 3679.3999999999996, "end": 3682.68, "text": " need to mention that because I know a lot of you will have maybe seen some", "tokens": [51650, 643, 281, 2152, 300, 570, 286, 458, 257, 688, 295, 291, 486, 362, 1310, 1612, 512, 51814], "temperature": 0.0, "avg_logprob": -0.1025024661057287, "compression_ratio": 1.8927444794952681, "no_speech_prob": 0.010011186823248863}, {"id": 1072, "seek": 368268, "start": 3682.68, "end": 3685.3999999999996, "text": " different ways of doing things in this course might show you, you know, a", "tokens": [50364, 819, 2098, 295, 884, 721, 294, 341, 1164, 1062, 855, 291, 11, 291, 458, 11, 257, 50500], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1073, "seek": 368268, "start": 3685.3999999999996, "end": 3688.64, "text": " different perspective on that. So let me just quickly talk about how I'm going", "tokens": [50500, 819, 4585, 322, 300, 13, 407, 718, 385, 445, 2661, 751, 466, 577, 286, 478, 516, 50662], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1074, "seek": 368268, "start": 3688.64, "end": 3692.04, "text": " to go through this. It's very similar to before I have this notebook, as I've", "tokens": [50662, 281, 352, 807, 341, 13, 467, 311, 588, 2531, 281, 949, 286, 362, 341, 21060, 11, 382, 286, 600, 50832], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1075, "seek": 368268, "start": 3692.04, "end": 3695.04, "text": " kind of talked about, there is a link in the description, I would recommend that", "tokens": [50832, 733, 295, 2825, 466, 11, 456, 307, 257, 2113, 294, 264, 3855, 11, 286, 576, 2748, 300, 50982], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1076, "seek": 368268, "start": 3695.04, "end": 3698.44, "text": " you guys hit that and follow along with what I'm doing and read through the", "tokens": [50982, 291, 1074, 2045, 300, 293, 1524, 2051, 365, 437, 286, 478, 884, 293, 1401, 807, 264, 51152], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1077, "seek": 368268, "start": 3698.44, "end": 3701.44, "text": " notebook, but I will just be going through the notebook. And then occasionally", "tokens": [51152, 21060, 11, 457, 286, 486, 445, 312, 516, 807, 264, 21060, 13, 400, 550, 16895, 51302], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1078, "seek": 368268, "start": 3701.44, "end": 3705.64, "text": " what I will actually do, oops, I need to open this up here is go to this kind", "tokens": [51302, 437, 286, 486, 767, 360, 11, 34166, 11, 286, 643, 281, 1269, 341, 493, 510, 307, 352, 281, 341, 733, 51512], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1079, "seek": 368268, "start": 3705.64, "end": 3709.2, "text": " of untitled tab I have here and write some code in here. Because most of what", "tokens": [51512, 295, 1701, 270, 1493, 4421, 286, 362, 510, 293, 2464, 512, 3089, 294, 510, 13, 1436, 881, 295, 437, 51690], "temperature": 0.0, "avg_logprob": -0.07627327014238407, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.028426431119441986}, {"id": 1080, "seek": 370920, "start": 3709.24, "end": 3713.0, "text": " I'm going to do is just copy code over into here so we can see it all in kind", "tokens": [50366, 286, 478, 516, 281, 360, 307, 445, 5055, 3089, 670, 666, 510, 370, 321, 393, 536, 309, 439, 294, 733, 50554], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1081, "seek": 370920, "start": 3713.0, "end": 3717.2, "text": " of one block. And then we'll be good to go. And the last note before we really", "tokens": [50554, 295, 472, 3461, 13, 400, 550, 321, 603, 312, 665, 281, 352, 13, 400, 264, 1036, 3637, 949, 321, 534, 50764], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1082, "seek": 370920, "start": 3717.2, "end": 3720.0, "text": " get into it, and I'm sorry I'm talking a lot, but it is important to make you", "tokens": [50764, 483, 666, 309, 11, 293, 286, 478, 2597, 286, 478, 1417, 257, 688, 11, 457, 309, 307, 1021, 281, 652, 291, 50904], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1083, "seek": 370920, "start": 3720.0, "end": 3723.2, "text": " guys aware of this, you're going to see that we use a lot of complicated", "tokens": [50904, 1074, 3650, 295, 341, 11, 291, 434, 516, 281, 536, 300, 321, 764, 257, 688, 295, 6179, 51064], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1084, "seek": 370920, "start": 3723.2, "end": 3727.12, "text": " syntax throughout this kind of series and the rest of the course in general. I", "tokens": [51064, 28431, 3710, 341, 733, 295, 2638, 293, 264, 1472, 295, 264, 1164, 294, 2674, 13, 286, 51260], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1085, "seek": 370920, "start": 3727.12, "end": 3731.08, "text": " just want to make it extremely clear that you should not have to memorize or", "tokens": [51260, 445, 528, 281, 652, 309, 4664, 1850, 300, 291, 820, 406, 362, 281, 27478, 420, 51458], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1086, "seek": 370920, "start": 3731.08, "end": 3735.52, "text": " even feel obligated to memorize any of the syntax that you see, everything that", "tokens": [51458, 754, 841, 9270, 770, 281, 27478, 604, 295, 264, 28431, 300, 291, 536, 11, 1203, 300, 51680], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1087, "seek": 370920, "start": 3735.52, "end": 3739.16, "text": " you see here, I personally don't even have memorized is a lot of what's in here", "tokens": [51680, 291, 536, 510, 11, 286, 5665, 500, 380, 754, 362, 46677, 307, 257, 688, 295, 437, 311, 294, 510, 51862], "temperature": 0.0, "avg_logprob": -0.09049647531391662, "compression_ratio": 1.8597014925373134, "no_speech_prob": 0.026755452156066895}, {"id": 1088, "seek": 373916, "start": 3739.16, "end": 3742.64, "text": " that I can't just come up with on the top of my head. When we're dealing with", "tokens": [50364, 300, 286, 393, 380, 445, 808, 493, 365, 322, 264, 1192, 295, 452, 1378, 13, 1133, 321, 434, 6260, 365, 50538], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1089, "seek": 373916, "start": 3742.64, "end": 3746.96, "text": " kind of a library and modules so big that like TensorFlow, it's hard to", "tokens": [50538, 733, 295, 257, 6405, 293, 16679, 370, 955, 300, 411, 37624, 11, 309, 311, 1152, 281, 50754], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1090, "seek": 373916, "start": 3746.96, "end": 3750.2, "text": " memorize all those different components. So just make sure you understand what's", "tokens": [50754, 27478, 439, 729, 819, 6677, 13, 407, 445, 652, 988, 291, 1223, 437, 311, 50916], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1091, "seek": 373916, "start": 3750.2, "end": 3753.24, "text": " happening, but you don't need to memorize it. If you're ever going to need to use", "tokens": [50916, 2737, 11, 457, 291, 500, 380, 643, 281, 27478, 309, 13, 759, 291, 434, 1562, 516, 281, 643, 281, 764, 51068], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1092, "seek": 373916, "start": 3753.24, "end": 3755.8799999999997, "text": " any of these tools, you're going to look them up, you're going to see what it is", "tokens": [51068, 604, 295, 613, 3873, 11, 291, 434, 516, 281, 574, 552, 493, 11, 291, 434, 516, 281, 536, 437, 309, 307, 51200], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1093, "seek": 373916, "start": 3755.8799999999997, "end": 3758.2799999999997, "text": " you're going to be like, okay, I've used this before, you're going to understand", "tokens": [51200, 291, 434, 516, 281, 312, 411, 11, 1392, 11, 286, 600, 1143, 341, 949, 11, 291, 434, 516, 281, 1223, 51320], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1094, "seek": 373916, "start": 3758.2799999999997, "end": 3761.04, "text": " it, and then you can go ahead and you know, copy that code in and use it in", "tokens": [51320, 309, 11, 293, 550, 291, 393, 352, 2286, 293, 291, 458, 11, 5055, 300, 3089, 294, 293, 764, 309, 294, 51458], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1095, "seek": 373916, "start": 3761.04, "end": 3764.7599999999998, "text": " whatever way you need to, you don't need to memorize anything that we do. All", "tokens": [51458, 2035, 636, 291, 643, 281, 11, 291, 500, 380, 643, 281, 27478, 1340, 300, 321, 360, 13, 1057, 51644], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1096, "seek": 373916, "start": 3764.7599999999998, "end": 3768.72, "text": " right, so let's go ahead and get started with linear regression. So what is", "tokens": [51644, 558, 11, 370, 718, 311, 352, 2286, 293, 483, 1409, 365, 8213, 24590, 13, 407, 437, 307, 51842], "temperature": 0.0, "avg_logprob": -0.10249356300600114, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.00912384781986475}, {"id": 1097, "seek": 376872, "start": 3768.7599999999998, "end": 3772.52, "text": " linear regression? What's one of those basic forms of machine learning? And", "tokens": [50366, 8213, 24590, 30, 708, 311, 472, 295, 729, 3875, 6422, 295, 3479, 2539, 30, 400, 50554], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1098, "seek": 376872, "start": 3772.52, "end": 3776.64, "text": " essentially, what we try to do is have a linear correspondence between data", "tokens": [50554, 4476, 11, 437, 321, 853, 281, 360, 307, 362, 257, 8213, 38135, 1296, 1412, 50760], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1099, "seek": 376872, "start": 3776.64, "end": 3779.9599999999996, "text": " points. So I'm just going to scroll down here, do a good example. So what I've", "tokens": [50760, 2793, 13, 407, 286, 478, 445, 516, 281, 11369, 760, 510, 11, 360, 257, 665, 1365, 13, 407, 437, 286, 600, 50926], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1100, "seek": 376872, "start": 3779.9599999999996, "end": 3783.7599999999998, "text": " done is use map plot live just to plot a little graph here. So we can see this", "tokens": [50926, 1096, 307, 764, 4471, 7542, 1621, 445, 281, 7542, 257, 707, 4295, 510, 13, 407, 321, 393, 536, 341, 51116], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1101, "seek": 376872, "start": 3783.7599999999998, "end": 3787.3599999999997, "text": " one right here. And essentially, this is kind of our data set. This is what we'll", "tokens": [51116, 472, 558, 510, 13, 400, 4476, 11, 341, 307, 733, 295, 527, 1412, 992, 13, 639, 307, 437, 321, 603, 51296], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1102, "seek": 376872, "start": 3787.3599999999997, "end": 3791.7999999999997, "text": " call your data set. What we want to do is use linear regression to come up with", "tokens": [51296, 818, 428, 1412, 992, 13, 708, 321, 528, 281, 360, 307, 764, 8213, 24590, 281, 808, 493, 365, 51518], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1103, "seek": 376872, "start": 3791.8399999999997, "end": 3795.3999999999996, "text": " a model that can give us some good predictions for our data points. So in", "tokens": [51520, 257, 2316, 300, 393, 976, 505, 512, 665, 21264, 337, 527, 1412, 2793, 13, 407, 294, 51698], "temperature": 0.0, "avg_logprob": -0.09377266863267199, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.00857583899050951}, {"id": 1104, "seek": 379540, "start": 3795.4, "end": 3799.32, "text": " this instance, maybe what we want to do is given some x value for a data point,", "tokens": [50364, 341, 5197, 11, 1310, 437, 321, 528, 281, 360, 307, 2212, 512, 2031, 2158, 337, 257, 1412, 935, 11, 50560], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1105, "seek": 379540, "start": 3799.32, "end": 3803.7200000000003, "text": " we want to predict the y value. Now, in this case, we can see there is kind of", "tokens": [50560, 321, 528, 281, 6069, 264, 288, 2158, 13, 823, 11, 294, 341, 1389, 11, 321, 393, 536, 456, 307, 733, 295, 50780], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1106, "seek": 379540, "start": 3803.7200000000003, "end": 3808.36, "text": " some correspondence linearly for these data points. Now, what that means is we", "tokens": [50780, 512, 38135, 43586, 337, 613, 1412, 2793, 13, 823, 11, 437, 300, 1355, 307, 321, 51012], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1107, "seek": 379540, "start": 3808.36, "end": 3812.36, "text": " can draw something called a line of best fit through these data points that can", "tokens": [51012, 393, 2642, 746, 1219, 257, 1622, 295, 1151, 3318, 807, 613, 1412, 2793, 300, 393, 51212], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1108, "seek": 379540, "start": 3812.4, "end": 3816.32, "text": " kind of accurately classify them, if that makes any sense. So I'm going to", "tokens": [51214, 733, 295, 20095, 33872, 552, 11, 498, 300, 1669, 604, 2020, 13, 407, 286, 478, 516, 281, 51410], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1109, "seek": 379540, "start": 3816.32, "end": 3819.92, "text": " scroll down here and look at what our line of best fit for this data set", "tokens": [51410, 11369, 760, 510, 293, 574, 412, 437, 527, 1622, 295, 1151, 3318, 337, 341, 1412, 992, 51590], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1110, "seek": 379540, "start": 3819.92, "end": 3823.88, "text": " actually is, you can see this blue line, a pretty much, I mean, it is the", "tokens": [51590, 767, 307, 11, 291, 393, 536, 341, 3344, 1622, 11, 257, 1238, 709, 11, 286, 914, 11, 309, 307, 264, 51788], "temperature": 0.0, "avg_logprob": -0.10414720439224792, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0007321566808968782}, {"id": 1111, "seek": 382388, "start": 3823.88, "end": 3828.8, "text": " perfect line of best fit for this data set. And using this line, we can actually", "tokens": [50364, 2176, 1622, 295, 1151, 3318, 337, 341, 1412, 992, 13, 400, 1228, 341, 1622, 11, 321, 393, 767, 50610], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1112, "seek": 382388, "start": 3828.8, "end": 3833.52, "text": " predict future values in our data set. So essentially, linear regression is used", "tokens": [50610, 6069, 2027, 4190, 294, 527, 1412, 992, 13, 407, 4476, 11, 8213, 24590, 307, 1143, 50846], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1113, "seek": 382388, "start": 3833.52, "end": 3837.56, "text": " when you have data points that correlate in kind of a linear fashion. Now, this is", "tokens": [50846, 562, 291, 362, 1412, 2793, 300, 48742, 294, 733, 295, 257, 8213, 6700, 13, 823, 11, 341, 307, 51048], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1114, "seek": 382388, "start": 3837.56, "end": 3842.1600000000003, "text": " a very basic example, because we're doing this in two dimensions with x and y. But", "tokens": [51048, 257, 588, 3875, 1365, 11, 570, 321, 434, 884, 341, 294, 732, 12819, 365, 2031, 293, 288, 13, 583, 51278], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1115, "seek": 382388, "start": 3842.1600000000003, "end": 3845.7200000000003, "text": " oftentimes, what you'll have is you'll have data points that have, you know, eight", "tokens": [51278, 18349, 11, 437, 291, 603, 362, 307, 291, 603, 362, 1412, 2793, 300, 362, 11, 291, 458, 11, 3180, 51456], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1116, "seek": 382388, "start": 3845.7200000000003, "end": 3850.1600000000003, "text": " or nine kind of input values. So that gives us, you know, a nine dimensional kind", "tokens": [51456, 420, 4949, 733, 295, 4846, 4190, 13, 407, 300, 2709, 505, 11, 291, 458, 11, 257, 4949, 18795, 733, 51678], "temperature": 0.0, "avg_logprob": -0.07846521940387663, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0023965875152498484}, {"id": 1117, "seek": 385016, "start": 3850.2, "end": 3853.68, "text": " of data set. What we'll do is predict one of the different values. So in the", "tokens": [50366, 295, 1412, 992, 13, 708, 321, 603, 360, 307, 6069, 472, 295, 264, 819, 4190, 13, 407, 294, 264, 50540], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1118, "seek": 385016, "start": 3853.68, "end": 3856.2799999999997, "text": " instance where we were talking about students before, maybe we have a", "tokens": [50540, 5197, 689, 321, 645, 1417, 466, 1731, 949, 11, 1310, 321, 362, 257, 50670], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1119, "seek": 385016, "start": 3856.2799999999997, "end": 3860.0, "text": " student, what is it midterm grade, and their second midterm grade, and then we", "tokens": [50670, 3107, 11, 437, 307, 309, 2062, 7039, 7204, 11, 293, 641, 1150, 2062, 7039, 7204, 11, 293, 550, 321, 50856], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1120, "seek": 385016, "start": 3860.0, "end": 3863.92, "text": " want to predict their final grade, what we can do is use linear regression to do", "tokens": [50856, 528, 281, 6069, 641, 2572, 7204, 11, 437, 321, 393, 360, 307, 764, 8213, 24590, 281, 360, 51052], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1121, "seek": 385016, "start": 3863.92, "end": 3867.44, "text": " that, where our kind of input values are going to be the two midterm grades and", "tokens": [51052, 300, 11, 689, 527, 733, 295, 4846, 4190, 366, 516, 281, 312, 264, 732, 2062, 7039, 18041, 293, 51228], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1122, "seek": 385016, "start": 3867.44, "end": 3871.96, "text": " the output value is going to be that final grade that we're looking to predict. So", "tokens": [51228, 264, 5598, 2158, 307, 516, 281, 312, 300, 2572, 7204, 300, 321, 434, 1237, 281, 6069, 13, 407, 51454], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1123, "seek": 385016, "start": 3871.96, "end": 3875.7999999999997, "text": " if we were to plot that, we would plot that on a three dimensional graph, and we", "tokens": [51454, 498, 321, 645, 281, 7542, 300, 11, 321, 576, 7542, 300, 322, 257, 1045, 18795, 4295, 11, 293, 321, 51646], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1124, "seek": 385016, "start": 3875.7999999999997, "end": 3879.7999999999997, "text": " would draw a three dimensional line that would represent the line of best fit for", "tokens": [51646, 576, 2642, 257, 1045, 18795, 1622, 300, 576, 2906, 264, 1622, 295, 1151, 3318, 337, 51846], "temperature": 0.0, "avg_logprob": -0.08785110253554124, "compression_ratio": 2.065359477124183, "no_speech_prob": 0.025175882503390312}, {"id": 1125, "seek": 387980, "start": 3879.8, "end": 3883.2000000000003, "text": " that data set. Now, for any of you that don't know what line of best fit stands", "tokens": [50364, 300, 1412, 992, 13, 823, 11, 337, 604, 295, 291, 300, 500, 380, 458, 437, 1622, 295, 1151, 3318, 7382, 50534], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1126, "seek": 387980, "start": 3883.2000000000003, "end": 3886.92, "text": " for, it says line, or this is just the definition I got from this website here,", "tokens": [50534, 337, 11, 309, 1619, 1622, 11, 420, 341, 307, 445, 264, 7123, 286, 658, 490, 341, 3144, 510, 11, 50720], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1127, "seek": 387980, "start": 3887.1200000000003, "end": 3890.44, "text": " line of best fit refers to a line through a scatter plot of data points that", "tokens": [50730, 1622, 295, 1151, 3318, 14942, 281, 257, 1622, 807, 257, 34951, 7542, 295, 1412, 2793, 300, 50896], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1128, "seek": 387980, "start": 3890.44, "end": 3894.0800000000004, "text": " best expresses the relationship between those points. So exactly what I've kind", "tokens": [50896, 1151, 39204, 264, 2480, 1296, 729, 2793, 13, 407, 2293, 437, 286, 600, 733, 51078], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1129, "seek": 387980, "start": 3894.0800000000004, "end": 3898.32, "text": " of been trying to explain, when we have data that correlates linearly, and I", "tokens": [51078, 295, 668, 1382, 281, 2903, 11, 562, 321, 362, 1412, 300, 13983, 1024, 43586, 11, 293, 286, 51290], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1130, "seek": 387980, "start": 3898.36, "end": 3902.32, "text": " always butcher that word, what we can do is draw a line through it, and then we", "tokens": [51292, 1009, 41579, 300, 1349, 11, 437, 321, 393, 360, 307, 2642, 257, 1622, 807, 309, 11, 293, 550, 321, 51490], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1131, "seek": 387980, "start": 3902.32, "end": 3906.1600000000003, "text": " can use that line to predict new data points, because if that line is good,", "tokens": [51490, 393, 764, 300, 1622, 281, 6069, 777, 1412, 2793, 11, 570, 498, 300, 1622, 307, 665, 11, 51682], "temperature": 0.0, "avg_logprob": -0.1074759234552798, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0014102851273491979}, {"id": 1132, "seek": 390616, "start": 3906.2, "end": 3910.48, "text": " it's a good line of best fit for the data set, then hopefully we would assume", "tokens": [50366, 309, 311, 257, 665, 1622, 295, 1151, 3318, 337, 264, 1412, 992, 11, 550, 4696, 321, 576, 6552, 50580], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1133, "seek": 390616, "start": 3910.48, "end": 3914.3599999999997, "text": " that we can just, you know, pick some point, find where it would be on that", "tokens": [50580, 300, 321, 393, 445, 11, 291, 458, 11, 1888, 512, 935, 11, 915, 689, 309, 576, 312, 322, 300, 50774], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1134, "seek": 390616, "start": 3914.3599999999997, "end": 3918.16, "text": " line, and that'll be kind of our predicted value. So I'm going to go into an", "tokens": [50774, 1622, 11, 293, 300, 603, 312, 733, 295, 527, 19147, 2158, 13, 407, 286, 478, 516, 281, 352, 666, 364, 50964], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1135, "seek": 390616, "start": 3918.16, "end": 3920.7999999999997, "text": " example now where I start drawing and going into a little bit of math. So we", "tokens": [50964, 1365, 586, 689, 286, 722, 6316, 293, 516, 666, 257, 707, 857, 295, 5221, 13, 407, 321, 51096], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1136, "seek": 390616, "start": 3920.7999999999997, "end": 3923.96, "text": " understand how this works on a deeper level. But that should give you a", "tokens": [51096, 1223, 577, 341, 1985, 322, 257, 7731, 1496, 13, 583, 300, 820, 976, 291, 257, 51254], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1137, "seek": 390616, "start": 3923.96, "end": 3926.7999999999997, "text": " surface level understanding. So actually, I'll leave this up because I was", "tokens": [51254, 3753, 1496, 3701, 13, 407, 767, 11, 286, 603, 1856, 341, 493, 570, 286, 390, 51396], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1138, "seek": 390616, "start": 3926.96, "end": 3931.3999999999996, "text": " messing with this beforehand. This is kind of a data set that I've drawn on", "tokens": [51404, 23258, 365, 341, 22893, 13, 639, 307, 733, 295, 257, 1412, 992, 300, 286, 600, 10117, 322, 51626], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1139, "seek": 390616, "start": 3931.3999999999996, "end": 3936.0, "text": " here. So we have our x, and we have our y, and we have our line of best fit. Now,", "tokens": [51626, 510, 13, 407, 321, 362, 527, 2031, 11, 293, 321, 362, 527, 288, 11, 293, 321, 362, 527, 1622, 295, 1151, 3318, 13, 823, 11, 51856], "temperature": 0.0, "avg_logprob": -0.08281272213633467, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.014062239788472652}, {"id": 1140, "seek": 393600, "start": 3936.0, "end": 3939.64, "text": " what I want to do is I want to use this line of best fit to predict a new", "tokens": [50364, 437, 286, 528, 281, 360, 307, 286, 528, 281, 764, 341, 1622, 295, 1151, 3318, 281, 6069, 257, 777, 50546], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1141, "seek": 393600, "start": 3939.64, "end": 3942.92, "text": " data point. So all these red data points are ones that we've trained our model", "tokens": [50546, 1412, 935, 13, 407, 439, 613, 2182, 1412, 2793, 366, 2306, 300, 321, 600, 8895, 527, 2316, 50710], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1142, "seek": 393600, "start": 3942.92, "end": 3946.32, "text": " with their information that we gave to the model so that it could create this", "tokens": [50710, 365, 641, 1589, 300, 321, 2729, 281, 264, 2316, 370, 300, 309, 727, 1884, 341, 50880], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1143, "seek": 393600, "start": 3946.32, "end": 3950.6, "text": " line of best fit. Because essentially, all linear regression really does is look", "tokens": [50880, 1622, 295, 1151, 3318, 13, 1436, 4476, 11, 439, 8213, 24590, 534, 775, 307, 574, 51094], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1144, "seek": 393600, "start": 3950.6, "end": 3955.2, "text": " at all of these data points and create a line of best fit for them. That's all it", "tokens": [51094, 412, 439, 295, 613, 1412, 2793, 293, 1884, 257, 1622, 295, 1151, 3318, 337, 552, 13, 663, 311, 439, 309, 51324], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1145, "seek": 393600, "start": 3955.2, "end": 3959.72, "text": " does. It's pretty, I don't know the word for it. It's pretty easy to actually do", "tokens": [51324, 775, 13, 467, 311, 1238, 11, 286, 500, 380, 458, 264, 1349, 337, 309, 13, 467, 311, 1238, 1858, 281, 767, 360, 51550], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1146, "seek": 393600, "start": 3959.72, "end": 3962.6, "text": " this. This algorithm is not that complicated. It's not that advanced. And", "tokens": [51550, 341, 13, 639, 9284, 307, 406, 300, 6179, 13, 467, 311, 406, 300, 7339, 13, 400, 51694], "temperature": 0.0, "avg_logprob": -0.08815526962280273, "compression_ratio": 1.9501779359430604, "no_speech_prob": 0.0007096334011293948}, {"id": 1147, "seek": 396260, "start": 3962.64, "end": 3966.36, "text": " that's why we start with it here, because it just makes sense to explain. So I", "tokens": [50366, 300, 311, 983, 321, 722, 365, 309, 510, 11, 570, 309, 445, 1669, 2020, 281, 2903, 13, 407, 286, 50552], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1148, "seek": 396260, "start": 3966.36, "end": 3970.52, "text": " hope that a lot of you would know in two dimensions, a line can be defined as", "tokens": [50552, 1454, 300, 257, 688, 295, 291, 576, 458, 294, 732, 12819, 11, 257, 1622, 393, 312, 7642, 382, 50760], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1149, "seek": 396260, "start": 3970.52, "end": 3976.68, "text": " follows. So with the equation y equals mx plus b. Now b stands for the y", "tokens": [50760, 10002, 13, 407, 365, 264, 5367, 288, 6915, 275, 87, 1804, 272, 13, 823, 272, 7382, 337, 264, 288, 51068], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1150, "seek": 396260, "start": 3976.68, "end": 3980.2799999999997, "text": " intercept, which means somewhere on this line. So essentially, where the line", "tokens": [51068, 24700, 11, 597, 1355, 4079, 322, 341, 1622, 13, 407, 4476, 11, 689, 264, 1622, 51248], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1151, "seek": 396260, "start": 3980.2799999999997, "end": 3985.12, "text": " starts. So in this instance, our b value is going to be right here. So this is", "tokens": [51248, 3719, 13, 407, 294, 341, 5197, 11, 527, 272, 2158, 307, 516, 281, 312, 558, 510, 13, 407, 341, 307, 51490], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1152, "seek": 396260, "start": 3985.12, "end": 3989.12, "text": " going to be b, because that is the y intercept. So we could say that that's", "tokens": [51490, 516, 281, 312, 272, 11, 570, 300, 307, 264, 288, 24700, 13, 407, 321, 727, 584, 300, 300, 311, 51690], "temperature": 0.0, "avg_logprob": -0.09217136136947139, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.006903141271322966}, {"id": 1153, "seek": 398912, "start": 3989.16, "end": 3994.44, "text": " like maybe, you know, we go on, we'll do this, we'll say this is like 123, we", "tokens": [50366, 411, 1310, 11, 291, 458, 11, 321, 352, 322, 11, 321, 603, 360, 341, 11, 321, 603, 584, 341, 307, 411, 34466, 11, 321, 50630], "temperature": 0.0, "avg_logprob": -0.13930335998535157, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.015903402119874954}, {"id": 1154, "seek": 398912, "start": 3994.44, "end": 3999.2, "text": " might say b is something like 0.4, right? So I could just pencil that into 0.4.", "tokens": [50630, 1062, 584, 272, 307, 746, 411, 1958, 13, 19, 11, 558, 30, 407, 286, 727, 445, 10985, 300, 666, 1958, 13, 19, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13930335998535157, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.015903402119874954}, {"id": 1155, "seek": 398912, "start": 4000.12, "end": 4005.16, "text": " And then what is mx and y? Well, x and y stand for the coordinates of this", "tokens": [50914, 400, 550, 437, 307, 275, 87, 293, 288, 30, 1042, 11, 2031, 293, 288, 1463, 337, 264, 21056, 295, 341, 51166], "temperature": 0.0, "avg_logprob": -0.13930335998535157, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.015903402119874954}, {"id": 1156, "seek": 398912, "start": 4005.16, "end": 4010.3599999999997, "text": " data point. So this would have, you know, some x, y value. In this case, we might", "tokens": [51166, 1412, 935, 13, 407, 341, 576, 362, 11, 291, 458, 11, 512, 2031, 11, 288, 2158, 13, 682, 341, 1389, 11, 321, 1062, 51426], "temperature": 0.0, "avg_logprob": -0.13930335998535157, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.015903402119874954}, {"id": 1157, "seek": 398912, "start": 4010.3599999999997, "end": 4015.2, "text": " call it, you know, something like, what do you want to say to 2.7, that might be", "tokens": [51426, 818, 309, 11, 291, 458, 11, 746, 411, 11, 437, 360, 291, 528, 281, 584, 281, 568, 13, 22, 11, 300, 1062, 312, 51668], "temperature": 0.0, "avg_logprob": -0.13930335998535157, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.015903402119874954}, {"id": 1158, "seek": 401520, "start": 4015.2, "end": 4020.3999999999996, "text": " the value of this data point. So that's our x and y. And then our m stands for", "tokens": [50364, 264, 2158, 295, 341, 1412, 935, 13, 407, 300, 311, 527, 2031, 293, 288, 13, 400, 550, 527, 275, 7382, 337, 50624], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1159, "seek": 401520, "start": 4020.3999999999996, "end": 4025.08, "text": " the slope, which is probably the most important part. Now slope simply defines", "tokens": [50624, 264, 13525, 11, 597, 307, 1391, 264, 881, 1021, 644, 13, 823, 13525, 2935, 23122, 50858], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1160, "seek": 401520, "start": 4025.2799999999997, "end": 4029.2, "text": " the steepness of this line of best fit that we've done here. Now the way we", "tokens": [50868, 264, 16841, 1287, 295, 341, 1622, 295, 1151, 3318, 300, 321, 600, 1096, 510, 13, 823, 264, 636, 321, 51064], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1161, "seek": 401520, "start": 4029.2, "end": 4033.8399999999997, "text": " calculate slope is using rise over run. Now rise over run essentially just", "tokens": [51064, 8873, 13525, 307, 1228, 6272, 670, 1190, 13, 823, 6272, 670, 1190, 4476, 445, 51296], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1162, "seek": 401520, "start": 4033.8399999999997, "end": 4037.48, "text": " means how much we went up versus how much we went across. So if you want to", "tokens": [51296, 1355, 577, 709, 321, 1437, 493, 5717, 577, 709, 321, 1437, 2108, 13, 407, 498, 291, 528, 281, 51478], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1163, "seek": 401520, "start": 4037.48, "end": 4040.7999999999997, "text": " calculate the slope of a line, what you can actually do is just draw a triangle.", "tokens": [51478, 8873, 264, 13525, 295, 257, 1622, 11, 437, 291, 393, 767, 360, 307, 445, 2642, 257, 13369, 13, 51644], "temperature": 0.0, "avg_logprob": -0.07778081172654609, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03021157532930374}, {"id": 1164, "seek": 404080, "start": 4041.44, "end": 4045.84, "text": " So a right angle triangle anywhere on the line. So just pick two data points. And", "tokens": [50396, 407, 257, 558, 5802, 13369, 4992, 322, 264, 1622, 13, 407, 445, 1888, 732, 1412, 2793, 13, 400, 50616], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1165, "seek": 404080, "start": 4045.84, "end": 4049.7200000000003, "text": " what you can do is calculate this distance, and this distance. And then you", "tokens": [50616, 437, 291, 393, 360, 307, 8873, 341, 4560, 11, 293, 341, 4560, 13, 400, 550, 291, 50810], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1166, "seek": 404080, "start": 4049.7200000000003, "end": 4053.8, "text": " can simply divide the distance up by the distance across. And that gives you the", "tokens": [50810, 393, 2935, 9845, 264, 4560, 493, 538, 264, 4560, 2108, 13, 400, 300, 2709, 291, 264, 51014], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1167, "seek": 404080, "start": 4053.8, "end": 4056.6000000000004, "text": " slope. I'm not going to go too far into slope because I feel like you guys", "tokens": [51014, 13525, 13, 286, 478, 406, 516, 281, 352, 886, 1400, 666, 13525, 570, 286, 841, 411, 291, 1074, 51154], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1168, "seek": 404080, "start": 4056.6000000000004, "end": 4060.32, "text": " probably understand what that is. But let's just pick some values for this line.", "tokens": [51154, 1391, 1223, 437, 300, 307, 13, 583, 718, 311, 445, 1888, 512, 4190, 337, 341, 1622, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1169, "seek": 404080, "start": 4060.32, "end": 4063.2000000000003, "text": " And I want to actually show you some real examples of math and how we're going", "tokens": [51340, 400, 286, 528, 281, 767, 855, 291, 512, 957, 5110, 295, 5221, 293, 577, 321, 434, 516, 51484], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1170, "seek": 404080, "start": 4063.2000000000003, "end": 4066.8, "text": " to do this. So let's say that our linear regression algorithm, you know, comes up", "tokens": [51484, 281, 360, 341, 13, 407, 718, 311, 584, 300, 527, 8213, 24590, 9284, 11, 291, 458, 11, 1487, 493, 51664], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1171, "seek": 404080, "start": 4066.8, "end": 4069.84, "text": " with this line, I'm not going to discuss really how it does that, although it", "tokens": [51664, 365, 341, 1622, 11, 286, 478, 406, 516, 281, 2248, 534, 577, 309, 775, 300, 11, 4878, 309, 51816], "temperature": 0.0, "avg_logprob": -0.09282973465646148, "compression_ratio": 1.845481049562682, "no_speech_prob": 0.015422766096889973}, {"id": 1172, "seek": 406984, "start": 4069.84, "end": 4073.36, "text": " just pretty much looks at all these data points, and finds a line that you know,", "tokens": [50364, 445, 1238, 709, 1542, 412, 439, 613, 1412, 2793, 11, 293, 10704, 257, 1622, 300, 291, 458, 11, 50540], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1173, "seek": 406984, "start": 4073.4, "end": 4078.6000000000004, "text": " goes, it splits these data points evenly. So essentially, you want to be as close", "tokens": [50542, 1709, 11, 309, 37741, 613, 1412, 2793, 17658, 13, 407, 4476, 11, 291, 528, 281, 312, 382, 1998, 50802], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1174, "seek": 406984, "start": 4078.6000000000004, "end": 4082.6800000000003, "text": " to every data point as possible. And you want to have as many data points, you", "tokens": [50802, 281, 633, 1412, 935, 382, 1944, 13, 400, 291, 528, 281, 362, 382, 867, 1412, 2793, 11, 291, 51006], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1175, "seek": 406984, "start": 4082.6800000000003, "end": 4085.6000000000004, "text": " want to have like the same amount of data points on the left side and the right", "tokens": [51006, 528, 281, 362, 411, 264, 912, 2372, 295, 1412, 2793, 322, 264, 1411, 1252, 293, 264, 558, 51152], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1176, "seek": 406984, "start": 4085.6000000000004, "end": 4088.6000000000004, "text": " side of the line. So in this example, we have, you know, a data point on the", "tokens": [51152, 1252, 295, 264, 1622, 13, 407, 294, 341, 1365, 11, 321, 362, 11, 291, 458, 11, 257, 1412, 935, 322, 264, 51302], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1177, "seek": 406984, "start": 4088.6000000000004, "end": 4091.84, "text": " left, a data point on the left, we have what two that are pretty much on the", "tokens": [51302, 1411, 11, 257, 1412, 935, 322, 264, 1411, 11, 321, 362, 437, 732, 300, 366, 1238, 709, 322, 264, 51464], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1178, "seek": 406984, "start": 4091.84, "end": 4095.04, "text": " line. And then we have two that are on the right. So this is a pretty good line", "tokens": [51464, 1622, 13, 400, 550, 321, 362, 732, 300, 366, 322, 264, 558, 13, 407, 341, 307, 257, 1238, 665, 1622, 51624], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1179, "seek": 406984, "start": 4095.04, "end": 4099.400000000001, "text": " of best fit, because all of the points are very close to the line. And they", "tokens": [51624, 295, 1151, 3318, 11, 570, 439, 295, 264, 2793, 366, 588, 1998, 281, 264, 1622, 13, 400, 436, 51842], "temperature": 0.0, "avg_logprob": -0.08549904966068839, "compression_ratio": 2.2455516014234878, "no_speech_prob": 0.008314738981425762}, {"id": 1180, "seek": 409940, "start": 4099.44, "end": 4103.679999999999, "text": " split them evenly. So that's kind of how you come up with a line of best fit. So", "tokens": [50366, 7472, 552, 17658, 13, 407, 300, 311, 733, 295, 577, 291, 808, 493, 365, 257, 1622, 295, 1151, 3318, 13, 407, 50578], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1181, "seek": 409940, "start": 4103.679999999999, "end": 4107.639999999999, "text": " let's say that the equation for this line is something like y equals, let's just", "tokens": [50578, 718, 311, 584, 300, 264, 5367, 337, 341, 1622, 307, 746, 411, 288, 6915, 11, 718, 311, 445, 50776], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1182, "seek": 409940, "start": 4107.639999999999, "end": 4115.44, "text": " give it 1.5 and x plus and let's say that value is just 0.5 to make it easy. So", "tokens": [50776, 976, 309, 502, 13, 20, 293, 2031, 1804, 293, 718, 311, 584, 300, 2158, 307, 445, 1958, 13, 20, 281, 652, 309, 1858, 13, 407, 51166], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1183, "seek": 409940, "start": 4115.44, "end": 4119.2, "text": " this is going to be the equation of our line. Now notice that x and y don't have", "tokens": [51166, 341, 307, 516, 281, 312, 264, 5367, 295, 527, 1622, 13, 823, 3449, 300, 2031, 293, 288, 500, 380, 362, 51354], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1184, "seek": 409940, "start": 4119.2, "end": 4122.96, "text": " a value, that's because we need to give the value to come up with one of the", "tokens": [51354, 257, 2158, 11, 300, 311, 570, 321, 643, 281, 976, 264, 2158, 281, 808, 493, 365, 472, 295, 264, 51542], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1185, "seek": 409940, "start": 4122.96, "end": 4127.36, "text": " other ones. So what we can do is we can say if we have either the y value or we", "tokens": [51542, 661, 2306, 13, 407, 437, 321, 393, 360, 307, 321, 393, 584, 498, 321, 362, 2139, 264, 288, 2158, 420, 321, 51762], "temperature": 0.0, "avg_logprob": -0.07684634388356969, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0025507158134132624}, {"id": 1186, "seek": 412736, "start": 4127.36, "end": 4131.5599999999995, "text": " have the x value of some point, and we want to figure out, you know, where it", "tokens": [50364, 362, 264, 2031, 2158, 295, 512, 935, 11, 293, 321, 528, 281, 2573, 484, 11, 291, 458, 11, 689, 309, 50574], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1187, "seek": 412736, "start": 4131.5599999999995, "end": 4136.32, "text": " is on the line, what we can do is just feed one in, do a calculation, and that", "tokens": [50574, 307, 322, 264, 1622, 11, 437, 321, 393, 360, 307, 445, 3154, 472, 294, 11, 360, 257, 17108, 11, 293, 300, 50812], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1188, "seek": 412736, "start": 4136.32, "end": 4139.5199999999995, "text": " will actually give us the other value. So in this instance, let's say that you", "tokens": [50812, 486, 767, 976, 505, 264, 661, 2158, 13, 407, 294, 341, 5197, 11, 718, 311, 584, 300, 291, 50972], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1189, "seek": 412736, "start": 4139.5199999999995, "end": 4143.32, "text": " know, I'm trying to predict something and I'm given the that the fact that x", "tokens": [50972, 458, 11, 286, 478, 1382, 281, 6069, 746, 293, 286, 478, 2212, 264, 300, 264, 1186, 300, 2031, 51162], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1190, "seek": 412736, "start": 4143.36, "end": 4147.759999999999, "text": " equals two, I know that x equals two, and I want to figure out what y would be", "tokens": [51164, 6915, 732, 11, 286, 458, 300, 2031, 6915, 732, 11, 293, 286, 528, 281, 2573, 484, 437, 288, 576, 312, 51384], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1191, "seek": 412736, "start": 4147.799999999999, "end": 4152.28, "text": " if x equals two. Well, I can use this line to do so. So what I would do is I'm", "tokens": [51386, 498, 2031, 6915, 732, 13, 1042, 11, 286, 393, 764, 341, 1622, 281, 360, 370, 13, 407, 437, 286, 576, 360, 307, 286, 478, 51610], "temperature": 0.0, "avg_logprob": -0.11384648923520689, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.012819157913327217}, {"id": 1192, "seek": 415228, "start": 4152.28, "end": 4160.5599999999995, "text": " going to say y equals 1.5 times two plus 0.5. Now, all of you quick math majors", "tokens": [50364, 516, 281, 584, 288, 6915, 502, 13, 20, 1413, 732, 1804, 1958, 13, 20, 13, 823, 11, 439, 295, 291, 1702, 5221, 31770, 50778], "temperature": 0.0, "avg_logprob": -0.11588121071839944, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.04083763435482979}, {"id": 1193, "seek": 415228, "start": 4160.5599999999995, "end": 4165.48, "text": " out there give me the value of 3.5, which means that if x was at two, then I", "tokens": [50778, 484, 456, 976, 385, 264, 2158, 295, 805, 13, 20, 11, 597, 1355, 300, 498, 2031, 390, 412, 732, 11, 550, 286, 51024], "temperature": 0.0, "avg_logprob": -0.11588121071839944, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.04083763435482979}, {"id": 1194, "seek": 415228, "start": 4165.48, "end": 4169.92, "text": " would have my data point as a prediction here on this line. And I would say, okay,", "tokens": [51024, 576, 362, 452, 1412, 935, 382, 257, 17630, 510, 322, 341, 1622, 13, 400, 286, 576, 584, 11, 1392, 11, 51246], "temperature": 0.0, "avg_logprob": -0.11588121071839944, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.04083763435482979}, {"id": 1195, "seek": 415228, "start": 4169.92, "end": 4173.4, "text": " so if you're telling me x is two, my prediction is that y is going to be", "tokens": [51246, 370, 498, 291, 434, 3585, 385, 2031, 307, 732, 11, 452, 17630, 307, 300, 288, 307, 516, 281, 312, 51420], "temperature": 0.0, "avg_logprob": -0.11588121071839944, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.04083763435482979}, {"id": 1196, "seek": 415228, "start": 4173.4, "end": 4177.5599999999995, "text": " equal to 3.5, because given the line of best fit for this data set, that's where", "tokens": [51420, 2681, 281, 805, 13, 20, 11, 570, 2212, 264, 1622, 295, 1151, 3318, 337, 341, 1412, 992, 11, 300, 311, 689, 51628], "temperature": 0.0, "avg_logprob": -0.11588121071839944, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.04083763435482979}, {"id": 1197, "seek": 417756, "start": 4177.56, "end": 4182.4800000000005, "text": " this point will lie on that line. So I hope that makes sense. You can actually", "tokens": [50364, 341, 935, 486, 4544, 322, 300, 1622, 13, 407, 286, 1454, 300, 1669, 2020, 13, 509, 393, 767, 50610], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1198, "seek": 417756, "start": 4182.4800000000005, "end": 4186.68, "text": " do this the reverse way as well. So if I'm just given some y values to say, I", "tokens": [50610, 360, 341, 264, 9943, 636, 382, 731, 13, 407, 498, 286, 478, 445, 2212, 512, 288, 4190, 281, 584, 11, 286, 50820], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1199, "seek": 417756, "start": 4186.68, "end": 4191.84, "text": " know that, you know, my y value is at like 2.7 or something, I can plug that in,", "tokens": [50820, 458, 300, 11, 291, 458, 11, 452, 288, 2158, 307, 412, 411, 568, 13, 22, 420, 746, 11, 286, 393, 5452, 300, 294, 11, 51078], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1200, "seek": 417756, "start": 4191.84, "end": 4195.4400000000005, "text": " just rearrange the numbers in this equation and then solve for x. Now,", "tokens": [51078, 445, 39568, 264, 3547, 294, 341, 5367, 293, 550, 5039, 337, 2031, 13, 823, 11, 51258], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1201, "seek": 417756, "start": 4195.4400000000005, "end": 4198.72, "text": " obviously, this is a very basic example, because we're just doing all of this in", "tokens": [51258, 2745, 11, 341, 307, 257, 588, 3875, 1365, 11, 570, 321, 434, 445, 884, 439, 295, 341, 294, 51422], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1202, "seek": 417756, "start": 4198.72, "end": 4202.56, "text": " two dimensions. But you can do this in higher dimensions as well. So actually,", "tokens": [51422, 732, 12819, 13, 583, 291, 393, 360, 341, 294, 2946, 12819, 382, 731, 13, 407, 767, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1203, "seek": 417756, "start": 4202.56, "end": 4205.160000000001, "text": " most times, what's going to end up happening is you're going to have, you", "tokens": [51614, 881, 1413, 11, 437, 311, 516, 281, 917, 493, 2737, 307, 291, 434, 516, 281, 362, 11, 291, 51744], "temperature": 0.0, "avg_logprob": -0.10566887239209649, "compression_ratio": 1.7151898734177216, "no_speech_prob": 0.02032816968858242}, {"id": 1204, "seek": 420516, "start": 4205.2, "end": 4208.68, "text": " know, like eight or nine input variables. And then you're going to have one output", "tokens": [50366, 458, 11, 411, 3180, 420, 4949, 4846, 9102, 13, 400, 550, 291, 434, 516, 281, 362, 472, 5598, 50540], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1205, "seek": 420516, "start": 4208.68, "end": 4212.96, "text": " variable that you're predicting. Now, so long as our data points are correlated", "tokens": [50540, 7006, 300, 291, 434, 32884, 13, 823, 11, 370, 938, 382, 527, 1412, 2793, 366, 38574, 50754], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1206, "seek": 420516, "start": 4212.96, "end": 4216.599999999999, "text": " linearly in three dimensions, we can still do this. So I'm going to attempt to", "tokens": [50754, 43586, 294, 1045, 12819, 11, 321, 393, 920, 360, 341, 13, 407, 286, 478, 516, 281, 5217, 281, 50936], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1207, "seek": 420516, "start": 4216.599999999999, "end": 4219.88, "text": " show you this actually, in three dimensions, just to hopefully clear some", "tokens": [50936, 855, 291, 341, 767, 11, 294, 1045, 12819, 11, 445, 281, 4696, 1850, 512, 51100], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1208, "seek": 420516, "start": 4219.88, "end": 4224.28, "text": " things up, because it is important to kind of get a grasp and perspective of the", "tokens": [51100, 721, 493, 11, 570, 309, 307, 1021, 281, 733, 295, 483, 257, 21743, 293, 4585, 295, 264, 51320], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1209, "seek": 420516, "start": 4224.28, "end": 4228.72, "text": " different dimensions. So let's say we have a bunch of data points that are kind", "tokens": [51320, 819, 12819, 13, 407, 718, 311, 584, 321, 362, 257, 3840, 295, 1412, 2793, 300, 366, 733, 51542], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1210, "seek": 420516, "start": 4228.72, "end": 4234.04, "text": " of like this, now I'm trying my best to kind of draw them in some linear fashion", "tokens": [51542, 295, 411, 341, 11, 586, 286, 478, 1382, 452, 1151, 281, 733, 295, 2642, 552, 294, 512, 8213, 6700, 51808], "temperature": 0.0, "avg_logprob": -0.09692587675871672, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004754773806780577}, {"id": 1211, "seek": 423404, "start": 4234.12, "end": 4238.68, "text": " using like all the dimensions here. But it is hard because drawing in three", "tokens": [50368, 1228, 411, 439, 264, 12819, 510, 13, 583, 309, 307, 1152, 570, 6316, 294, 1045, 50596], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1212, "seek": 423404, "start": 4238.68, "end": 4242.28, "text": " dimensions on a two dimensional screen is not easy. Okay, so let's say this is", "tokens": [50596, 12819, 322, 257, 732, 18795, 2568, 307, 406, 1858, 13, 1033, 11, 370, 718, 311, 584, 341, 307, 50776], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1213, "seek": 423404, "start": 4242.28, "end": 4245.68, "text": " kind of like what our data points look like. Now, I would say that these", "tokens": [50776, 733, 295, 411, 437, 527, 1412, 2793, 574, 411, 13, 823, 11, 286, 576, 584, 300, 613, 50946], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1214, "seek": 423404, "start": 4245.68, "end": 4250.16, "text": " correlate linearly, like pretty, pretty well, they kind of go up in one fashion,", "tokens": [50946, 48742, 43586, 11, 411, 1238, 11, 1238, 731, 11, 436, 733, 295, 352, 493, 294, 472, 6700, 11, 51170], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1215, "seek": 423404, "start": 4250.16, "end": 4253.44, "text": " and we don't know the scale of this. So this is probably fun. So the line of", "tokens": [51170, 293, 321, 500, 380, 458, 264, 4373, 295, 341, 13, 407, 341, 307, 1391, 1019, 13, 407, 264, 1622, 295, 51334], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1216, "seek": 423404, "start": 4253.44, "end": 4257.72, "text": " best fit for this data set, and I'll just put my kind of thickness up might be", "tokens": [51334, 1151, 3318, 337, 341, 1412, 992, 11, 293, 286, 603, 445, 829, 452, 733, 295, 14855, 493, 1062, 312, 51548], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1217, "seek": 423404, "start": 4257.72, "end": 4263.08, "text": " something like this, right? Now notice that this line is in three dimensions,", "tokens": [51548, 746, 411, 341, 11, 558, 30, 823, 3449, 300, 341, 1622, 307, 294, 1045, 12819, 11, 51816], "temperature": 0.0, "avg_logprob": -0.09047363473357056, "compression_ratio": 1.7887788778877889, "no_speech_prob": 0.0024724602699279785}, {"id": 1218, "seek": 426308, "start": 4263.08, "end": 4268.5599999999995, "text": " right? This is going to cross our, I guess this is our x, y, and z axes. So we", "tokens": [50364, 558, 30, 639, 307, 516, 281, 3278, 527, 11, 286, 2041, 341, 307, 527, 2031, 11, 288, 11, 293, 710, 35387, 13, 407, 321, 50638], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1219, "seek": 426308, "start": 4268.5599999999995, "end": 4271.44, "text": " have a three dimensional line. Now the equation for this line is a little bit", "tokens": [50638, 362, 257, 1045, 18795, 1622, 13, 823, 264, 5367, 337, 341, 1622, 307, 257, 707, 857, 50782], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1220, "seek": 426308, "start": 4271.44, "end": 4274.92, "text": " more complicated. I'm not going to talk about exactly what it is. But essentially", "tokens": [50782, 544, 6179, 13, 286, 478, 406, 516, 281, 751, 466, 2293, 437, 309, 307, 13, 583, 4476, 50956], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1221, "seek": 426308, "start": 4274.92, "end": 4278.96, "text": " what we do is we make this line, and then we say, Okay, what value do I want to", "tokens": [50956, 437, 321, 360, 307, 321, 652, 341, 1622, 11, 293, 550, 321, 584, 11, 1033, 11, 437, 2158, 360, 286, 528, 281, 51158], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1222, "seek": 426308, "start": 4278.96, "end": 4285.76, "text": " predict? Do I want to predict y, x or z? Now, so long as I have two values, so two", "tokens": [51158, 6069, 30, 1144, 286, 528, 281, 6069, 288, 11, 2031, 420, 710, 30, 823, 11, 370, 938, 382, 286, 362, 732, 4190, 11, 370, 732, 51498], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1223, "seek": 426308, "start": 4285.8, "end": 4289.24, "text": " values, I can always predict the other one. So if I have, you know, the x, y of", "tokens": [51500, 4190, 11, 286, 393, 1009, 6069, 264, 661, 472, 13, 407, 498, 286, 362, 11, 291, 458, 11, 264, 2031, 11, 288, 295, 51672], "temperature": 0.0, "avg_logprob": -0.1275220018752078, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0009110228274948895}, {"id": 1224, "seek": 428924, "start": 4289.28, "end": 4295.12, "text": " the data point, that will give me the z. And if I have the z, y, that will give", "tokens": [50366, 264, 1412, 935, 11, 300, 486, 976, 385, 264, 710, 13, 400, 498, 286, 362, 264, 710, 11, 288, 11, 300, 486, 976, 50658], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1225, "seek": 428924, "start": 4295.12, "end": 4300.5199999999995, "text": " me the x. So so long as you have, you know, all of the data points, except one,", "tokens": [50658, 385, 264, 2031, 13, 407, 370, 938, 382, 291, 362, 11, 291, 458, 11, 439, 295, 264, 1412, 2793, 11, 3993, 472, 11, 50928], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1226, "seek": 428924, "start": 4300.679999999999, "end": 4304.32, "text": " you can always find what that point is, based on the fact that, you know, we have", "tokens": [50936, 291, 393, 1009, 915, 437, 300, 935, 307, 11, 2361, 322, 264, 1186, 300, 11, 291, 458, 11, 321, 362, 51118], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1227, "seek": 428924, "start": 4304.32, "end": 4307.679999999999, "text": " this line, and we're using that to predict. So I think I'm going to leave it at", "tokens": [51118, 341, 1622, 11, 293, 321, 434, 1228, 300, 281, 6069, 13, 407, 286, 519, 286, 478, 516, 281, 1856, 309, 412, 51286], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1228, "seek": 428924, "start": 4307.679999999999, "end": 4312.44, "text": " that for the explanation. I hope that makes sense. Again, just understand that we", "tokens": [51286, 300, 337, 264, 10835, 13, 286, 1454, 300, 1669, 2020, 13, 3764, 11, 445, 1223, 300, 321, 51524], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1229, "seek": 428924, "start": 4312.44, "end": 4316.2, "text": " use linear regression when our data points are correlated linearly. Now some", "tokens": [51524, 764, 8213, 24590, 562, 527, 1412, 2793, 366, 38574, 43586, 13, 823, 512, 51712], "temperature": 0.0, "avg_logprob": -0.09161236449962354, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.022973403334617615}, {"id": 1230, "seek": 431620, "start": 4316.2, "end": 4319.48, "text": " good examples of linear regression were, you know, that kind of student", "tokens": [50364, 665, 5110, 295, 8213, 24590, 645, 11, 291, 458, 11, 300, 733, 295, 3107, 50528], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1231, "seek": 431620, "start": 4319.48, "end": 4323.5199999999995, "text": " predicting the grade kind of thing, you would assume that if someone has, you", "tokens": [50528, 32884, 264, 7204, 733, 295, 551, 11, 291, 576, 6552, 300, 498, 1580, 575, 11, 291, 50730], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1232, "seek": 431620, "start": 4323.5199999999995, "end": 4327.08, "text": " know, a low grade, then they would finish with a lower grade, and you would", "tokens": [50730, 458, 11, 257, 2295, 7204, 11, 550, 436, 576, 2413, 365, 257, 3126, 7204, 11, 293, 291, 576, 50908], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1233, "seek": 431620, "start": 4327.08, "end": 4330.72, "text": " assume if they have a higher grade, they would finish with a higher grade. Now you", "tokens": [50908, 6552, 498, 436, 362, 257, 2946, 7204, 11, 436, 576, 2413, 365, 257, 2946, 7204, 13, 823, 291, 51090], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1234, "seek": 431620, "start": 4330.72, "end": 4334.679999999999, "text": " could also do something like predicting, you know, future life expectancy. Now this", "tokens": [51090, 727, 611, 360, 746, 411, 32884, 11, 291, 458, 11, 2027, 993, 42574, 13, 823, 341, 51288], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1235, "seek": 431620, "start": 4334.679999999999, "end": 4338.36, "text": " is kind of a darker example. But essentially, what you could think of here is", "tokens": [51288, 307, 733, 295, 257, 12741, 1365, 13, 583, 4476, 11, 437, 291, 727, 519, 295, 510, 307, 51472], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1236, "seek": 431620, "start": 4338.36, "end": 4343.08, "text": " if someone is older, they're expected to live, you know, like not as long. Or you", "tokens": [51472, 498, 1580, 307, 4906, 11, 436, 434, 5176, 281, 1621, 11, 291, 458, 11, 411, 406, 382, 938, 13, 1610, 291, 51708], "temperature": 0.0, "avg_logprob": -0.07829714522642248, "compression_ratio": 2.03690036900369, "no_speech_prob": 0.06007184460759163}, {"id": 1237, "seek": 434308, "start": 4343.08, "end": 4346.5199999999995, "text": " could look at health conditions, if someone is in critical illness condition,", "tokens": [50364, 727, 574, 412, 1585, 4487, 11, 498, 1580, 307, 294, 4924, 10152, 4188, 11, 50536], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1238, "seek": 434308, "start": 4346.5599999999995, "end": 4350.24, "text": " they have a critical illness, then chances are their life expectancy is lower. So", "tokens": [50538, 436, 362, 257, 4924, 10152, 11, 550, 10486, 366, 641, 993, 42574, 307, 3126, 13, 407, 50722], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1239, "seek": 434308, "start": 4350.24, "end": 4353.4, "text": " that's an example of something that is correlated linearly. Essentially,", "tokens": [50722, 300, 311, 364, 1365, 295, 746, 300, 307, 38574, 43586, 13, 23596, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1240, "seek": 434308, "start": 4353.4, "end": 4356.04, "text": " something goes up, and something goes down, or something goes up, the other", "tokens": [50880, 746, 1709, 493, 11, 293, 746, 1709, 760, 11, 420, 746, 1709, 493, 11, 264, 661, 51012], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1241, "seek": 434308, "start": 4356.04, "end": 4358.84, "text": " thing goes up. That's kind of what you need to think of when you think of a", "tokens": [51012, 551, 1709, 493, 13, 663, 311, 733, 295, 437, 291, 643, 281, 519, 295, 562, 291, 519, 295, 257, 51152], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1242, "seek": 434308, "start": 4358.84, "end": 4362.88, "text": " linear correlation. Now the magnitude of that correlation, so you know, how much", "tokens": [51152, 8213, 20009, 13, 823, 264, 15668, 295, 300, 20009, 11, 370, 291, 458, 11, 577, 709, 51354], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1243, "seek": 434308, "start": 4362.88, "end": 4366.32, "text": " does one go up versus how much one goes down is exactly what our algorithm", "tokens": [51354, 775, 472, 352, 493, 5717, 577, 709, 472, 1709, 760, 307, 2293, 437, 527, 9284, 51526], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1244, "seek": 434308, "start": 4366.32, "end": 4369.96, "text": " figures out for us, we just need to know to pick linear regression when we think", "tokens": [51526, 9624, 484, 337, 505, 11, 321, 445, 643, 281, 458, 281, 1888, 8213, 24590, 562, 321, 519, 51708], "temperature": 0.0, "avg_logprob": -0.10041900475819905, "compression_ratio": 1.9528301886792452, "no_speech_prob": 0.0066925459541380405}, {"id": 1245, "seek": 436996, "start": 4370.0, "end": 4373.72, "text": " things are going to be correlated in that sense. Okay, so that is enough of the", "tokens": [50366, 721, 366, 516, 281, 312, 38574, 294, 300, 2020, 13, 1033, 11, 370, 300, 307, 1547, 295, 264, 50552], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1246, "seek": 436996, "start": 4373.72, "end": 4376.76, "text": " explanation of linear regression. Now we're going to get into actually coding", "tokens": [50552, 10835, 295, 8213, 24590, 13, 823, 321, 434, 516, 281, 483, 666, 767, 17720, 50704], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1247, "seek": 436996, "start": 4376.76, "end": 4379.84, "text": " and creating a model. But we first need to talk about the data set that we're", "tokens": [50704, 293, 4084, 257, 2316, 13, 583, 321, 700, 643, 281, 751, 466, 264, 1412, 992, 300, 321, 434, 50858], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1248, "seek": 436996, "start": 4379.84, "end": 4382.76, "text": " going to use in the example we're going to kind of illustrate linear regression", "tokens": [50858, 516, 281, 764, 294, 264, 1365, 321, 434, 516, 281, 733, 295, 23221, 8213, 24590, 51004], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1249, "seek": 436996, "start": 4382.76, "end": 4386.68, "text": " with. Okay, so I'm here and I'm back in the notebook. Now these are the imports", "tokens": [51004, 365, 13, 1033, 11, 370, 286, 478, 510, 293, 286, 478, 646, 294, 264, 21060, 13, 823, 613, 366, 264, 41596, 51200], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1250, "seek": 436996, "start": 4386.68, "end": 4389.84, "text": " we need to start with to actually start programming and getting some stuff done.", "tokens": [51200, 321, 643, 281, 722, 365, 281, 767, 722, 9410, 293, 1242, 512, 1507, 1096, 13, 51358], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1251, "seek": 436996, "start": 4390.16, "end": 4394.16, "text": " Now the first thing we need to do is actually install SK learn. Now even if", "tokens": [51374, 823, 264, 700, 551, 321, 643, 281, 360, 307, 767, 3625, 21483, 1466, 13, 823, 754, 498, 51574], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1252, "seek": 436996, "start": 4394.16, "end": 4397.24, "text": " you're in a notebook, you actually need to do this because for some reason it", "tokens": [51574, 291, 434, 294, 257, 21060, 11, 291, 767, 643, 281, 360, 341, 570, 337, 512, 1778, 309, 51728], "temperature": 0.0, "avg_logprob": -0.08189016853282655, "compression_ratio": 1.974921630094044, "no_speech_prob": 0.01798374205827713}, {"id": 1253, "seek": 439724, "start": 4397.24, "end": 4400.0, "text": " doesn't come by default with the notebook. So to do this, we just did an", "tokens": [50364, 1177, 380, 808, 538, 7576, 365, 264, 21060, 13, 407, 281, 360, 341, 11, 321, 445, 630, 364, 50502], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1254, "seek": 439724, "start": 4400.0, "end": 4404.679999999999, "text": " exclamation point, pip install hyphen q SK learn. Now if you're going to be working", "tokens": [50502, 1624, 43233, 935, 11, 8489, 3625, 2477, 47059, 9505, 21483, 1466, 13, 823, 498, 291, 434, 516, 281, 312, 1364, 50736], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1255, "seek": 439724, "start": 4404.679999999999, "end": 4408.0, "text": " on your own machine, again, you can use pip to install this. And I'm assuming", "tokens": [50736, 322, 428, 1065, 3479, 11, 797, 11, 291, 393, 764, 8489, 281, 3625, 341, 13, 400, 286, 478, 11926, 50902], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1256, "seek": 439724, "start": 4408.0, "end": 4411.32, "text": " that you know to use pip if you're going to be going along in that direction. Now", "tokens": [50902, 300, 291, 458, 281, 764, 8489, 498, 291, 434, 516, 281, 312, 516, 2051, 294, 300, 3513, 13, 823, 51068], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1257, "seek": 439724, "start": 4411.32, "end": 4413.96, "text": " as before, since we're in the notebook, we need to define we're going to use", "tokens": [51068, 382, 949, 11, 1670, 321, 434, 294, 264, 21060, 11, 321, 643, 281, 6964, 321, 434, 516, 281, 764, 51200], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1258, "seek": 439724, "start": 4413.96, "end": 4418.679999999999, "text": " TensorFlow version to point x. So to do that, we're going to just, you know, do", "tokens": [51200, 37624, 3037, 281, 935, 2031, 13, 407, 281, 360, 300, 11, 321, 434, 516, 281, 445, 11, 291, 458, 11, 360, 51436], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1259, "seek": 439724, "start": 4418.679999999999, "end": 4421.92, "text": " that up here with the percent sign. And then we have all these imports, which", "tokens": [51436, 300, 493, 510, 365, 264, 3043, 1465, 13, 400, 550, 321, 362, 439, 613, 41596, 11, 597, 51598], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1260, "seek": 439724, "start": 4421.92, "end": 4425.4, "text": " we're going to be using throughout here. So from future import, absolutely import", "tokens": [51598, 321, 434, 516, 281, 312, 1228, 3710, 510, 13, 407, 490, 2027, 974, 11, 3122, 974, 51772], "temperature": 0.0, "avg_logprob": -0.10574150085449219, "compression_ratio": 1.8783382789317506, "no_speech_prob": 0.010012609884142876}, {"id": 1261, "seek": 442540, "start": 4425.44, "end": 4428.799999999999, "text": " division, print function, Unicode literals, and then obviously the big one. So", "tokens": [50366, 10044, 11, 4482, 2445, 11, 1156, 299, 1429, 2733, 1124, 11, 293, 550, 2745, 264, 955, 472, 13, 407, 50534], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1262, "seek": 442540, "start": 4428.799999999999, "end": 4434.2, "text": " NumPy, pandas, map plot lib, we're gonna be using I Python, we're gonna be using", "tokens": [50534, 22592, 47, 88, 11, 4565, 296, 11, 4471, 7542, 22854, 11, 321, 434, 799, 312, 1228, 286, 15329, 11, 321, 434, 799, 312, 1228, 50804], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1263, "seek": 442540, "start": 4434.2, "end": 4437.5599999999995, "text": " TensorFlow. And yeah, so I'm actually just gonna explain what some of these", "tokens": [50804, 37624, 13, 400, 1338, 11, 370, 286, 478, 767, 445, 799, 2903, 437, 512, 295, 613, 50972], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1264, "seek": 442540, "start": 4437.5599999999995, "end": 4441.5599999999995, "text": " modules are, because I feel like some of you may actually not know. NumPy is", "tokens": [50972, 16679, 366, 11, 570, 286, 841, 411, 512, 295, 291, 815, 767, 406, 458, 13, 22592, 47, 88, 307, 51172], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1265, "seek": 442540, "start": 4441.5599999999995, "end": 4447.36, "text": " essentially a very optimized version of arrays in Python. So what this allows", "tokens": [51172, 4476, 257, 588, 26941, 3037, 295, 41011, 294, 15329, 13, 407, 437, 341, 4045, 51462], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1266, "seek": 442540, "start": 4447.36, "end": 4451.679999999999, "text": " us to do is lots of kind of multi dimensional calculations. So essentially", "tokens": [51462, 505, 281, 360, 307, 3195, 295, 733, 295, 4825, 18795, 20448, 13, 407, 4476, 51678], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1267, "seek": 442540, "start": 4451.679999999999, "end": 4455.04, "text": " if you have a multi dimensional array, which we've talked about before, right", "tokens": [51678, 498, 291, 362, 257, 4825, 18795, 10225, 11, 597, 321, 600, 2825, 466, 949, 11, 558, 51846], "temperature": 0.0, "avg_logprob": -0.1304738556129345, "compression_ratio": 1.702194357366771, "no_speech_prob": 0.014501858502626419}, {"id": 1268, "seek": 445504, "start": 4455.04, "end": 4459.2, "text": " when we had, you know, those crazy shapes like 5555, NumPy allows us to", "tokens": [50364, 562, 321, 632, 11, 291, 458, 11, 729, 3219, 10854, 411, 12330, 13622, 11, 22592, 47, 88, 4045, 505, 281, 50572], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1269, "seek": 445504, "start": 4459.2, "end": 4462.88, "text": " represent data in that form, and then very quickly manipulate and perform", "tokens": [50572, 2906, 1412, 294, 300, 1254, 11, 293, 550, 588, 2661, 20459, 293, 2042, 50756], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1270, "seek": 445504, "start": 4462.88, "end": 4467.24, "text": " operations on it. So we can do things like cross product, dot product, matrix", "tokens": [50756, 7705, 322, 309, 13, 407, 321, 393, 360, 721, 411, 3278, 1674, 11, 5893, 1674, 11, 8141, 50974], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1271, "seek": 445504, "start": 4467.24, "end": 4471.6, "text": " addition, matrix subtraction, element wise addition, subtraction, you know,", "tokens": [50974, 4500, 11, 8141, 16390, 313, 11, 4478, 10829, 4500, 11, 16390, 313, 11, 291, 458, 11, 51192], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1272, "seek": 445504, "start": 4471.6, "end": 4475.32, "text": " vector operations, that's what this does for us. It's pretty complex, but we're", "tokens": [51192, 8062, 7705, 11, 300, 311, 437, 341, 775, 337, 505, 13, 467, 311, 1238, 3997, 11, 457, 321, 434, 51378], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1273, "seek": 445504, "start": 4475.32, "end": 4479.4, "text": " going to be using it a fair amount. Pandas. Now what pandas does is it's kind", "tokens": [51378, 516, 281, 312, 1228, 309, 257, 3143, 2372, 13, 16995, 296, 13, 823, 437, 4565, 296, 775, 307, 309, 311, 733, 51582], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1274, "seek": 445504, "start": 4479.4, "end": 4483.48, "text": " of a data analytics tool, I almost want to say, I don't know the formal", "tokens": [51582, 295, 257, 1412, 15370, 2290, 11, 286, 1920, 528, 281, 584, 11, 286, 500, 380, 458, 264, 9860, 51786], "temperature": 0.0, "avg_logprob": -0.10105395998273577, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.002322866814211011}, {"id": 1275, "seek": 448348, "start": 4483.48, "end": 4487.639999999999, "text": " definition of what pandas is. But it allows us to very easily manipulate", "tokens": [50364, 7123, 295, 437, 4565, 296, 307, 13, 583, 309, 4045, 505, 281, 588, 3612, 20459, 50572], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1276, "seek": 448348, "start": 4487.639999999999, "end": 4492.799999999999, "text": " data. So you know, load in data sets, view data sets, cut off specific columns", "tokens": [50572, 1412, 13, 407, 291, 458, 11, 3677, 294, 1412, 6352, 11, 1910, 1412, 6352, 11, 1723, 766, 2685, 13766, 50830], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1277, "seek": 448348, "start": 4492.799999999999, "end": 4496.44, "text": " or cut out rows from our data sets, visualize the data sets. That's what", "tokens": [50830, 420, 1723, 484, 13241, 490, 527, 1412, 6352, 11, 23273, 264, 1412, 6352, 13, 663, 311, 437, 51012], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1278, "seek": 448348, "start": 4496.44, "end": 4500.5199999999995, "text": " pandas does for us. Now map plot lib is actually a visualization of kind of", "tokens": [51012, 4565, 296, 775, 337, 505, 13, 823, 4471, 7542, 22854, 307, 767, 257, 25801, 295, 733, 295, 51216], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1279, "seek": 448348, "start": 4500.5199999999995, "end": 4504.08, "text": " graphs and charts. So we'll use that a little bit lower when I actually", "tokens": [51216, 24877, 293, 17767, 13, 407, 321, 603, 764, 300, 257, 707, 857, 3126, 562, 286, 767, 51394], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1280, "seek": 448348, "start": 4504.44, "end": 4509.12, "text": " graph some different aspects of our data set. The IPython display, this is", "tokens": [51412, 4295, 512, 819, 7270, 295, 527, 1412, 992, 13, 440, 8671, 88, 11943, 4674, 11, 341, 307, 51646], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1281, "seek": 448348, "start": 4509.12, "end": 4511.919999999999, "text": " just specific for this notebook, it's just to clear the output, there's", "tokens": [51646, 445, 2685, 337, 341, 21060, 11, 309, 311, 445, 281, 1850, 264, 5598, 11, 456, 311, 51786], "temperature": 0.0, "avg_logprob": -0.12290358005609728, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.002472465857863426}, {"id": 1282, "seek": 451192, "start": 4511.92, "end": 4515.4, "text": " nothing crazy with that. And then obviously, we know what TensorFlow is,", "tokens": [50364, 1825, 3219, 365, 300, 13, 400, 550, 2745, 11, 321, 458, 437, 37624, 307, 11, 50538], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1283, "seek": 451192, "start": 4515.6, "end": 4520.0, "text": " this crazy import for TensorFlow here. So compact v to feature column as FC,", "tokens": [50548, 341, 3219, 974, 337, 37624, 510, 13, 407, 14679, 371, 281, 4111, 7738, 382, 27168, 11, 50768], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1284, "seek": 451192, "start": 4520.0, "end": 4523.56, "text": " we'll talk about later, but we need something called a feature column when", "tokens": [50768, 321, 603, 751, 466, 1780, 11, 457, 321, 643, 746, 1219, 257, 4111, 7738, 562, 50946], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1285, "seek": 451192, "start": 4523.56, "end": 4527.6, "text": " we create a linear regression algorithm or model and TensorFlow. So we're going", "tokens": [50946, 321, 1884, 257, 8213, 24590, 9284, 420, 2316, 293, 37624, 13, 407, 321, 434, 516, 51148], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1286, "seek": 451192, "start": 4527.6, "end": 4531.64, "text": " to use that. Okay. So now that we've gone through all that, we need to start", "tokens": [51148, 281, 764, 300, 13, 1033, 13, 407, 586, 300, 321, 600, 2780, 807, 439, 300, 11, 321, 643, 281, 722, 51350], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1287, "seek": 451192, "start": 4531.64, "end": 4534.28, "text": " talking about the data set that we're going to use for linear regression. And", "tokens": [51350, 1417, 466, 264, 1412, 992, 300, 321, 434, 516, 281, 764, 337, 8213, 24590, 13, 400, 51482], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1288, "seek": 451192, "start": 4534.28, "end": 4537.2, "text": " for this example, because what we're going to do is, you know, actually create", "tokens": [51482, 337, 341, 1365, 11, 570, 437, 321, 434, 516, 281, 360, 307, 11, 291, 458, 11, 767, 1884, 51628], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1289, "seek": 451192, "start": 4537.2, "end": 4541.04, "text": " this model, and start using it to predict values. So the data set that we're", "tokens": [51628, 341, 2316, 11, 293, 722, 1228, 309, 281, 6069, 4190, 13, 407, 264, 1412, 992, 300, 321, 434, 51820], "temperature": 0.0, "avg_logprob": -0.12142608339423376, "compression_ratio": 1.9523809523809523, "no_speech_prob": 0.0031723384745419025}, {"id": 1290, "seek": 454104, "start": 4541.08, "end": 4544.08, "text": " going to use, actually, I need to read this, because I forget exactly what the", "tokens": [50366, 516, 281, 764, 11, 767, 11, 286, 643, 281, 1401, 341, 11, 570, 286, 2870, 2293, 437, 264, 50516], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1291, "seek": 454104, "start": 4544.08, "end": 4548.44, "text": " name of it is, is the Titanic data set, that's what it is. So essentially, what", "tokens": [50516, 1315, 295, 309, 307, 11, 307, 264, 42183, 1412, 992, 11, 300, 311, 437, 309, 307, 13, 407, 4476, 11, 437, 50734], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1292, "seek": 454104, "start": 4548.44, "end": 4552.32, "text": " this does is aim to predict who's going to survive or the likelihood that", "tokens": [50734, 341, 775, 307, 5939, 281, 6069, 567, 311, 516, 281, 7867, 420, 264, 22119, 300, 50928], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1293, "seek": 454104, "start": 4552.32, "end": 4557.2, "text": " someone will survive, being on the Titanic, given a bunch of information. So", "tokens": [50928, 1580, 486, 7867, 11, 885, 322, 264, 42183, 11, 2212, 257, 3840, 295, 1589, 13, 407, 51172], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1294, "seek": 454104, "start": 4557.2, "end": 4559.76, "text": " what we need to do is load in this data set. Now, I know this seems like a", "tokens": [51172, 437, 321, 643, 281, 360, 307, 3677, 294, 341, 1412, 992, 13, 823, 11, 286, 458, 341, 2544, 411, 257, 51300], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1295, "seek": 454104, "start": 4559.76, "end": 4562.88, "text": " bunch of gibberish, but this is how we need to load it. So we're going to use", "tokens": [51300, 3840, 295, 4553, 43189, 11, 457, 341, 307, 577, 321, 643, 281, 3677, 309, 13, 407, 321, 434, 516, 281, 764, 51456], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1296, "seek": 454104, "start": 4562.88, "end": 4569.48, "text": " pandas. So PD dot read CSV from this URL. So what this is going to do is take", "tokens": [51456, 4565, 296, 13, 407, 10464, 5893, 1401, 48814, 490, 341, 12905, 13, 407, 437, 341, 307, 516, 281, 360, 307, 747, 51786], "temperature": 0.0, "avg_logprob": -0.1059052493121173, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.004754701163619757}, {"id": 1297, "seek": 456948, "start": 4569.5199999999995, "end": 4573.48, "text": " this CSV file, which stands for comma, separated values. And we can actually", "tokens": [50366, 341, 48814, 3991, 11, 597, 7382, 337, 22117, 11, 12005, 4190, 13, 400, 321, 393, 767, 50564], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1298, "seek": 456948, "start": 4573.48, "end": 4577.4, "text": " look at this if we want, I think so I said, it said control click, let's see", "tokens": [50564, 574, 412, 341, 498, 321, 528, 11, 286, 519, 370, 286, 848, 11, 309, 848, 1969, 2052, 11, 718, 311, 536, 50760], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1299, "seek": 456948, "start": 4577.4, "end": 4580.919999999999, "text": " if this pops up. So let's actually download this. And let's open this up", "tokens": [50760, 498, 341, 16795, 493, 13, 407, 718, 311, 767, 5484, 341, 13, 400, 718, 311, 1269, 341, 493, 50936], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1300, "seek": 456948, "start": 4580.919999999999, "end": 4583.799999999999, "text": " ourselves and have a look at what it is in Excel. So I'm going to bring this", "tokens": [50936, 4175, 293, 362, 257, 574, 412, 437, 309, 307, 294, 19060, 13, 407, 286, 478, 516, 281, 1565, 341, 51080], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1301, "seek": 456948, "start": 4583.799999999999, "end": 4588.4, "text": " up here. You can see that link. And this is what our data set is. So we have", "tokens": [51080, 493, 510, 13, 509, 393, 536, 300, 2113, 13, 400, 341, 307, 437, 527, 1412, 992, 307, 13, 407, 321, 362, 51310], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1302, "seek": 456948, "start": 4588.4, "end": 4592.5199999999995, "text": " our columns, which just stand for, you know, what is it the different", "tokens": [51310, 527, 13766, 11, 597, 445, 1463, 337, 11, 291, 458, 11, 437, 307, 309, 264, 819, 51516], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1303, "seek": 456948, "start": 4592.5199999999995, "end": 4595.16, "text": " attributes in our data set of the different features and labels of our", "tokens": [51516, 17212, 294, 527, 1412, 992, 295, 264, 819, 4122, 293, 16949, 295, 527, 51648], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1304, "seek": 456948, "start": 4595.16, "end": 4598.759999999999, "text": " data set, we have survived. So this is what we're actually going to be aiming", "tokens": [51648, 1412, 992, 11, 321, 362, 14433, 13, 407, 341, 307, 437, 321, 434, 767, 516, 281, 312, 20253, 51828], "temperature": 0.0, "avg_logprob": -0.10898326635360718, "compression_ratio": 1.8777429467084639, "no_speech_prob": 0.038453757762908936}, {"id": 1305, "seek": 459876, "start": 4598.8, "end": 4602.04, "text": " to predict. So we're going to call this our label, right, or our output", "tokens": [50366, 281, 6069, 13, 407, 321, 434, 516, 281, 818, 341, 527, 7645, 11, 558, 11, 420, 527, 5598, 50528], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1306, "seek": 459876, "start": 4602.04, "end": 4607.56, "text": " information. So here, a zero stands for the fact that someone did not survive.", "tokens": [50528, 1589, 13, 407, 510, 11, 257, 4018, 7382, 337, 264, 1186, 300, 1580, 630, 406, 7867, 13, 50804], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1307, "seek": 459876, "start": 4607.6, "end": 4611.360000000001, "text": " And one stands for the fact that someone did survive. Now, just thinking about", "tokens": [50806, 400, 472, 7382, 337, 264, 1186, 300, 1580, 630, 7867, 13, 823, 11, 445, 1953, 466, 50994], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1308, "seek": 459876, "start": 4611.360000000001, "end": 4614.72, "text": " it on your own for a second, and looking at some of the categories we have up", "tokens": [50994, 309, 322, 428, 1065, 337, 257, 1150, 11, 293, 1237, 412, 512, 295, 264, 10479, 321, 362, 493, 51162], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1309, "seek": 459876, "start": 4614.72, "end": 4618.64, "text": " here, can you think about why linear regression would be a good algorithm for", "tokens": [51162, 510, 11, 393, 291, 519, 466, 983, 8213, 24590, 576, 312, 257, 665, 9284, 337, 51358], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1310, "seek": 459876, "start": 4618.64, "end": 4623.4800000000005, "text": " something like this? Well, for example, if someone is a female, we can kind of", "tokens": [51358, 746, 411, 341, 30, 1042, 11, 337, 1365, 11, 498, 1580, 307, 257, 6556, 11, 321, 393, 733, 295, 51600], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1311, "seek": 459876, "start": 4623.4800000000005, "end": 4625.96, "text": " assume that they're going to have a higher chance of surviving on the", "tokens": [51600, 6552, 300, 436, 434, 516, 281, 362, 257, 2946, 2931, 295, 24948, 322, 264, 51724], "temperature": 0.0, "avg_logprob": -0.08868911571072456, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005384004209190607}, {"id": 1312, "seek": 462596, "start": 4625.96, "end": 4629.68, "text": " Titanic, just because of, you know, the kind of the way that our culture works,", "tokens": [50364, 42183, 11, 445, 570, 295, 11, 291, 458, 11, 264, 733, 295, 264, 636, 300, 527, 3713, 1985, 11, 50550], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1313, "seek": 462596, "start": 4629.72, "end": 4632.28, "text": " you know, saving women and children first, right? And if we look through this", "tokens": [50552, 291, 458, 11, 6816, 2266, 293, 2227, 700, 11, 558, 30, 400, 498, 321, 574, 807, 341, 50680], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1314, "seek": 462596, "start": 4632.28, "end": 4636.52, "text": " data set, we'll see that when we see females, it's pretty rare that they", "tokens": [50680, 1412, 992, 11, 321, 603, 536, 300, 562, 321, 536, 21529, 11, 309, 311, 1238, 5892, 300, 436, 50892], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1315, "seek": 462596, "start": 4636.52, "end": 4639.76, "text": " don't survive. Although as I go through, there is quite a few that didn't", "tokens": [50892, 500, 380, 7867, 13, 5780, 382, 286, 352, 807, 11, 456, 307, 1596, 257, 1326, 300, 994, 380, 51054], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1316, "seek": 462596, "start": 4639.76, "end": 4642.44, "text": " survive. But if we look at it compared to males, you know, there's definitely", "tokens": [51054, 7867, 13, 583, 498, 321, 574, 412, 309, 5347, 281, 20776, 11, 291, 458, 11, 456, 311, 2138, 51188], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1317, "seek": 462596, "start": 4642.44, "end": 4646.96, "text": " strong correlation that being a female results in a stronger survival rate. Now,", "tokens": [51188, 2068, 20009, 300, 885, 257, 6556, 3542, 294, 257, 7249, 12559, 3314, 13, 823, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1318, "seek": 462596, "start": 4646.96, "end": 4650.08, "text": " if we look at age, right, can we think of how age might affect this? Well, I", "tokens": [51414, 498, 321, 574, 412, 3205, 11, 558, 11, 393, 321, 519, 295, 577, 3205, 1062, 3345, 341, 30, 1042, 11, 286, 51570], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1319, "seek": 462596, "start": 4650.08, "end": 4653.24, "text": " would assume if someone's way younger, they probably have a higher chance of", "tokens": [51570, 576, 6552, 498, 1580, 311, 636, 7037, 11, 436, 1391, 362, 257, 2946, 2931, 295, 51728], "temperature": 0.0, "avg_logprob": -0.07089261588810375, "compression_ratio": 1.762857142857143, "no_speech_prob": 0.00781533308327198}, {"id": 1320, "seek": 465324, "start": 4653.28, "end": 4658.32, "text": " surviving, because they would be, you know, prioritized in terms of lifeboats or", "tokens": [50366, 24948, 11, 570, 436, 576, 312, 11, 291, 458, 11, 14846, 1602, 294, 2115, 295, 993, 1763, 1720, 420, 50618], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1321, "seek": 465324, "start": 4658.32, "end": 4661.04, "text": " whatever it was. I don't know much about the Titanic. So I can't talk about that", "tokens": [50618, 2035, 309, 390, 13, 286, 500, 380, 458, 709, 466, 264, 42183, 13, 407, 286, 393, 380, 751, 466, 300, 50754], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1322, "seek": 465324, "start": 4661.04, "end": 4663.679999999999, "text": " specifically. But I'm just trying to go through the categories and explain to", "tokens": [50754, 4682, 13, 583, 286, 478, 445, 1382, 281, 352, 807, 264, 10479, 293, 2903, 281, 50886], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1323, "seek": 465324, "start": 4663.679999999999, "end": 4667.16, "text": " you why we picked this algorithm. Now, number of siblings, that one might not", "tokens": [50886, 291, 983, 321, 6183, 341, 9284, 13, 823, 11, 1230, 295, 20571, 11, 300, 472, 1062, 406, 51060], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1324, "seek": 465324, "start": 4667.16, "end": 4670.88, "text": " be as, you know, influential, in my opinion, parched. I don't actually", "tokens": [51060, 312, 382, 11, 291, 458, 11, 22215, 11, 294, 452, 4800, 11, 971, 19318, 13, 286, 500, 380, 767, 51246], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1325, "seek": 465324, "start": 4670.88, "end": 4676.76, "text": " remember what parched stands for. I think it is like what parched, I don't know", "tokens": [51246, 1604, 437, 971, 19318, 7382, 337, 13, 286, 519, 309, 307, 411, 437, 971, 19318, 11, 286, 500, 380, 458, 51540], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1326, "seek": 465324, "start": 4676.76, "end": 4679.599999999999, "text": " exactly what this column stands for. So unfortunately, I can't tell you guys", "tokens": [51540, 2293, 437, 341, 7738, 7382, 337, 13, 407, 7015, 11, 286, 393, 380, 980, 291, 1074, 51682], "temperature": 0.0, "avg_logprob": -0.12896403460435465, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.04741789773106575}, {"id": 1327, "seek": 467960, "start": 4679.64, "end": 4684.64, "text": " that one. But we'll talk about some more of the second fair. Again, not exactly", "tokens": [50366, 300, 472, 13, 583, 321, 603, 751, 466, 512, 544, 295, 264, 1150, 3143, 13, 3764, 11, 406, 2293, 50616], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1328, "seek": 467960, "start": 4684.64, "end": 4687.64, "text": " sure what fair stands for. I'm going to look on the TensorFlow website after", "tokens": [50616, 988, 437, 3143, 7382, 337, 13, 286, 478, 516, 281, 574, 322, 264, 37624, 3144, 934, 50766], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1329, "seek": 467960, "start": 4687.64, "end": 4691.56, "text": " this and get back to you guys. And we have a class. So class is what class they", "tokens": [50766, 341, 293, 483, 646, 281, 291, 1074, 13, 400, 321, 362, 257, 1508, 13, 407, 1508, 307, 437, 1508, 436, 50962], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1330, "seek": 467960, "start": 4691.56, "end": 4694.8, "text": " were on the boat, right? So first class, second class, third class. So you might", "tokens": [50962, 645, 322, 264, 6582, 11, 558, 30, 407, 700, 1508, 11, 1150, 1508, 11, 2636, 1508, 13, 407, 291, 1062, 51124], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1331, "seek": 467960, "start": 4694.8, "end": 4698.08, "text": " think someone that's in a higher class might have a higher chance of surviving.", "tokens": [51124, 519, 1580, 300, 311, 294, 257, 2946, 1508, 1062, 362, 257, 2946, 2931, 295, 24948, 13, 51288], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1332, "seek": 467960, "start": 4698.4800000000005, "end": 4702.400000000001, "text": " We have decks, this is what deck they were on when it crashed. So unknown is", "tokens": [51308, 492, 362, 32607, 11, 341, 307, 437, 9341, 436, 645, 322, 562, 309, 24190, 13, 407, 9841, 307, 51504], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1333, "seek": 467960, "start": 4702.400000000001, "end": 4706.160000000001, "text": " pretty common. And then we have all these other decks, you know, if someone got", "tokens": [51504, 1238, 2689, 13, 400, 550, 321, 362, 439, 613, 661, 32607, 11, 291, 458, 11, 498, 1580, 658, 51692], "temperature": 0.0, "avg_logprob": -0.10346744110534241, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.06752865016460419}, {"id": 1334, "seek": 470616, "start": 4706.2, "end": 4710.2, "text": " hit, if someone was standing on the deck that had the initial impact, we might", "tokens": [50366, 2045, 11, 498, 1580, 390, 4877, 322, 264, 9341, 300, 632, 264, 5883, 2712, 11, 321, 1062, 50566], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1335, "seek": 470616, "start": 4710.2, "end": 4714.0, "text": " assume that they would have a lower chance of survival. embark to is where", "tokens": [50566, 6552, 300, 436, 576, 362, 257, 3126, 2931, 295, 12559, 13, 29832, 281, 307, 689, 50756], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1336, "seek": 470616, "start": 4714.0, "end": 4717.32, "text": " they were going. And then are they alone? Yes or no. And this one, you know, this", "tokens": [50756, 436, 645, 516, 13, 400, 550, 366, 436, 3312, 30, 1079, 420, 572, 13, 400, 341, 472, 11, 291, 458, 11, 341, 50922], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1337, "seek": 470616, "start": 4717.32, "end": 4720.5199999999995, "text": " is interesting, we're going to see does this make an effect? If someone is alone,", "tokens": [50922, 307, 1880, 11, 321, 434, 516, 281, 536, 775, 341, 652, 364, 1802, 30, 759, 1580, 307, 3312, 11, 51082], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1338, "seek": 470616, "start": 4720.5199999999995, "end": 4723.92, "text": " is that a higher chance of survival? Is that a lower chance of survival? So this", "tokens": [51082, 307, 300, 257, 2946, 2931, 295, 12559, 30, 1119, 300, 257, 3126, 2931, 295, 12559, 30, 407, 341, 51252], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1339, "seek": 470616, "start": 4723.92, "end": 4726.2, "text": " is kind of interesting. And this is what I want you guys to think about is that", "tokens": [51252, 307, 733, 295, 1880, 13, 400, 341, 307, 437, 286, 528, 291, 1074, 281, 519, 466, 307, 300, 51366], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1340, "seek": 470616, "start": 4726.2, "end": 4730.5599999999995, "text": " when we have information and data like this, we don't necessarily know what", "tokens": [51366, 562, 321, 362, 1589, 293, 1412, 411, 341, 11, 321, 500, 380, 4725, 458, 437, 51584], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1341, "seek": 470616, "start": 4730.5599999999995, "end": 4734.4, "text": " correlations there might be. But we can kind of assume there's some linear", "tokens": [51584, 13983, 763, 456, 1062, 312, 13, 583, 321, 393, 733, 295, 6552, 456, 311, 512, 8213, 51776], "temperature": 0.0, "avg_logprob": -0.09131961532785923, "compression_ratio": 1.9294478527607362, "no_speech_prob": 0.004468096885830164}, {"id": 1342, "seek": 473440, "start": 4734.48, "end": 4738.0, "text": " thing that we're looking for some kind of pattern, right? Whereas if something is", "tokens": [50368, 551, 300, 321, 434, 1237, 337, 512, 733, 295, 5102, 11, 558, 30, 13813, 498, 746, 307, 50544], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1343, "seek": 473440, "start": 4738.0, "end": 4741.5199999999995, "text": " true, then you know, maybe it's more likely someone will survive. Whereas like,", "tokens": [50544, 2074, 11, 550, 291, 458, 11, 1310, 309, 311, 544, 3700, 1580, 486, 7867, 13, 13813, 411, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1344, "seek": 473440, "start": 4741.5199999999995, "end": 4745.08, "text": " if they're not alone, maybe it's less likely. And maybe there's no correlation", "tokens": [50720, 498, 436, 434, 406, 3312, 11, 1310, 309, 311, 1570, 3700, 13, 400, 1310, 456, 311, 572, 20009, 50898], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1345, "seek": 473440, "start": 4745.08, "end": 4748.839999999999, "text": " whatsoever. But that's where we're going to find out as we do this model. So let", "tokens": [50898, 17076, 13, 583, 300, 311, 689, 321, 434, 516, 281, 915, 484, 382, 321, 360, 341, 2316, 13, 407, 718, 51086], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1346, "seek": 473440, "start": 4748.839999999999, "end": 4752.28, "text": " me look actually, on the TensorFlow website and see if I can remember what", "tokens": [51086, 385, 574, 767, 11, 322, 264, 37624, 3144, 293, 536, 498, 286, 393, 1604, 437, 51258], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1347, "seek": 473440, "start": 4752.28, "end": 4756.599999999999, "text": " parched and I guess what fair was. So let's go up to the top here. Again, a lot", "tokens": [51258, 971, 19318, 293, 286, 2041, 437, 3143, 390, 13, 407, 718, 311, 352, 493, 281, 264, 1192, 510, 13, 3764, 11, 257, 688, 51474], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1348, "seek": 473440, "start": 4756.599999999999, "end": 4760.0, "text": " of this stuff is just straight up copied from the TensorFlow website. I've just", "tokens": [51474, 295, 341, 1507, 307, 445, 2997, 493, 25365, 490, 264, 37624, 3144, 13, 286, 600, 445, 51644], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1349, "seek": 473440, "start": 4760.0, "end": 4763.679999999999, "text": " added my own stuff to it. You can see like, I just copied all this, we're just", "tokens": [51644, 3869, 452, 1065, 1507, 281, 309, 13, 509, 393, 536, 411, 11, 286, 445, 25365, 439, 341, 11, 321, 434, 445, 51828], "temperature": 0.0, "avg_logprob": -0.11475510195077183, "compression_ratio": 1.809116809116809, "no_speech_prob": 0.050312355160713196}, {"id": 1350, "seek": 476368, "start": 4763.72, "end": 4767.88, "text": " bringing it in there. Let's see what it says about the different columns, if it", "tokens": [50366, 5062, 309, 294, 456, 13, 961, 311, 536, 437, 309, 1619, 466, 264, 819, 13766, 11, 498, 309, 50574], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1351, "seek": 476368, "start": 4767.88, "end": 4773.4400000000005, "text": " gives us any exact explanations. Okay, so I couldn't find what parts were fair", "tokens": [50574, 2709, 505, 604, 1900, 28708, 13, 1033, 11, 370, 286, 2809, 380, 915, 437, 3166, 645, 3143, 50852], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1352, "seek": 476368, "start": 4773.4400000000005, "end": 4776.320000000001, "text": " stands for. For some reason, it's not on the TensorFlow website, either. I", "tokens": [50852, 7382, 337, 13, 1171, 512, 1778, 11, 309, 311, 406, 322, 264, 37624, 3144, 11, 2139, 13, 286, 50996], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1353, "seek": 476368, "start": 4776.320000000001, "end": 4779.04, "text": " couldn't really find any information about it. If you guys know, you know,", "tokens": [50996, 2809, 380, 534, 915, 604, 1589, 466, 309, 13, 759, 291, 1074, 458, 11, 291, 458, 11, 51132], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1354, "seek": 476368, "start": 4779.04, "end": 4781.76, "text": " leave a comment down below, but it's not that important, we just want to use", "tokens": [51132, 1856, 257, 2871, 760, 2507, 11, 457, 309, 311, 406, 300, 1021, 11, 321, 445, 528, 281, 764, 51268], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1355, "seek": 476368, "start": 4781.76, "end": 4786.320000000001, "text": " this data to do a test. So what I've done here, if I've loaded in my data set,", "tokens": [51268, 341, 1412, 281, 360, 257, 1500, 13, 407, 437, 286, 600, 1096, 510, 11, 498, 286, 600, 13210, 294, 452, 1412, 992, 11, 51496], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1356, "seek": 476368, "start": 4786.320000000001, "end": 4790.56, "text": " and notice that I've loaded a training data set in a testing data set. Now we'll", "tokens": [51496, 293, 3449, 300, 286, 600, 13210, 257, 3097, 1412, 992, 294, 257, 4997, 1412, 992, 13, 823, 321, 603, 51708], "temperature": 0.0, "avg_logprob": -0.10561206896011144, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.001987665891647339}, {"id": 1357, "seek": 479056, "start": 4790.6, "end": 4794.160000000001, "text": " talk about this more later. This is important, I have two different data", "tokens": [50366, 751, 466, 341, 544, 1780, 13, 639, 307, 1021, 11, 286, 362, 732, 819, 1412, 50544], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1358, "seek": 479056, "start": 4794.160000000001, "end": 4798.64, "text": " sets, one to train the model with, and one to test the model with. Now kind of the", "tokens": [50544, 6352, 11, 472, 281, 3847, 264, 2316, 365, 11, 293, 472, 281, 1500, 264, 2316, 365, 13, 823, 733, 295, 264, 50768], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1359, "seek": 479056, "start": 4798.64, "end": 4802.080000000001, "text": " basic reason we would do this is because when we test our model for accuracy to", "tokens": [50768, 3875, 1778, 321, 576, 360, 341, 307, 570, 562, 321, 1500, 527, 2316, 337, 14170, 281, 50940], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1360, "seek": 479056, "start": 4802.080000000001, "end": 4805.8, "text": " see how well it's doing, it doesn't make sense to test it on data, it's already", "tokens": [50940, 536, 577, 731, 309, 311, 884, 11, 309, 1177, 380, 652, 2020, 281, 1500, 309, 322, 1412, 11, 309, 311, 1217, 51126], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1361, "seek": 479056, "start": 4805.8, "end": 4809.76, "text": " seen, it needs to see fresh data, so we can make sure there's no bias, and it", "tokens": [51126, 1612, 11, 309, 2203, 281, 536, 4451, 1412, 11, 370, 321, 393, 652, 988, 456, 311, 572, 12577, 11, 293, 309, 51324], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1362, "seek": 479056, "start": 4809.76, "end": 4814.84, "text": " hasn't simply just memorize the data, you know, that we had. Now what I'm doing", "tokens": [51324, 6132, 380, 2935, 445, 27478, 264, 1412, 11, 291, 458, 11, 300, 321, 632, 13, 823, 437, 286, 478, 884, 51578], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1363, "seek": 479056, "start": 4814.84, "end": 4819.8, "text": " here, at the bottom with this y train in this y eval, is I'm essentially popping", "tokens": [51578, 510, 11, 412, 264, 2767, 365, 341, 288, 3847, 294, 341, 288, 1073, 304, 11, 307, 286, 478, 4476, 18374, 51826], "temperature": 0.0, "avg_logprob": -0.12547863006591797, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0027147806249558926}, {"id": 1364, "seek": 481980, "start": 4819.88, "end": 4824.56, "text": " a column off of this data set. So if I print out the data set here, and I'm", "tokens": [50368, 257, 7738, 766, 295, 341, 1412, 992, 13, 407, 498, 286, 4482, 484, 264, 1412, 992, 510, 11, 293, 286, 478, 50602], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1365, "seek": 481980, "start": 4824.56, "end": 4827.12, "text": " actually I'm going to show you a cool trick with pandas that we can use to", "tokens": [50602, 767, 286, 478, 516, 281, 855, 291, 257, 1627, 4282, 365, 4565, 296, 300, 321, 393, 764, 281, 50730], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1366, "seek": 481980, "start": 4827.12, "end": 4833.52, "text": " look at this. So I can say D F train dot head. So if I look at this, by just", "tokens": [50730, 574, 412, 341, 13, 407, 286, 393, 584, 413, 479, 3847, 5893, 1378, 13, 407, 498, 286, 574, 412, 341, 11, 538, 445, 51050], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1367, "seek": 481980, "start": 4833.52, "end": 4836.8, "text": " looking at the head, and we'll print this out, oh, I might need to import some", "tokens": [51050, 1237, 412, 264, 1378, 11, 293, 321, 603, 4482, 341, 484, 11, 1954, 11, 286, 1062, 643, 281, 974, 512, 51214], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1368, "seek": 481980, "start": 4836.8, "end": 4840.88, "text": " stuff above. We'll see if this works or not. Yeah, so I need to just do these", "tokens": [51214, 1507, 3673, 13, 492, 603, 536, 498, 341, 1985, 420, 406, 13, 865, 11, 370, 286, 643, 281, 445, 360, 613, 51418], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1369, "seek": 481980, "start": 4840.88, "end": 4844.64, "text": " imports. So let's install. And let's do these imports. I'll wait for the", "tokens": [51418, 41596, 13, 407, 718, 311, 3625, 13, 400, 718, 311, 360, 613, 41596, 13, 286, 603, 1699, 337, 264, 51606], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1370, "seek": 481980, "start": 4844.64, "end": 4848.08, "text": " surrounding. Okay, so I've just selected TensorFlow 2.0. We're just importing", "tokens": [51606, 11498, 13, 1033, 11, 370, 286, 600, 445, 8209, 37624, 568, 13, 15, 13, 492, 434, 445, 43866, 51778], "temperature": 0.0, "avg_logprob": -0.12810891713851538, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.02368362620472908}, {"id": 1371, "seek": 484808, "start": 4848.08, "end": 4851.92, "text": " this now should be done in one second. And now what we'll do is we'll print", "tokens": [50364, 341, 586, 820, 312, 1096, 294, 472, 1150, 13, 400, 586, 437, 321, 603, 360, 307, 321, 603, 4482, 50556], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1372, "seek": 484808, "start": 4851.96, "end": 4856.2, "text": " out the the data frame here. So essentially what this does is load this", "tokens": [50558, 484, 264, 264, 1412, 3920, 510, 13, 407, 4476, 437, 341, 775, 307, 3677, 341, 50770], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1373, "seek": 484808, "start": 4856.2, "end": 4860.6, "text": " into a pandas data frame. This is a specific type of object. Now we're not", "tokens": [50770, 666, 257, 4565, 296, 1412, 3920, 13, 639, 307, 257, 2685, 2010, 295, 2657, 13, 823, 321, 434, 406, 50990], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1374, "seek": 484808, "start": 4860.6, "end": 4864.24, "text": " going to go into this specifically, but a data frame allows us to view a lot of", "tokens": [50990, 516, 281, 352, 666, 341, 4682, 11, 457, 257, 1412, 3920, 4045, 505, 281, 1910, 257, 688, 295, 51172], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1375, "seek": 484808, "start": 4864.24, "end": 4868.4, "text": " different aspects about the data and kind of store it in a nice form, as opposed", "tokens": [51172, 819, 7270, 466, 264, 1412, 293, 733, 295, 3531, 309, 294, 257, 1481, 1254, 11, 382, 8851, 51380], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1376, "seek": 484808, "start": 4868.4, "end": 4871.96, "text": " to just loading it in and storing it in like a list or a NumPy array, which we", "tokens": [51380, 281, 445, 15114, 309, 294, 293, 26085, 309, 294, 411, 257, 1329, 420, 257, 22592, 47, 88, 10225, 11, 597, 321, 51558], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1377, "seek": 484808, "start": 4871.96, "end": 4875.44, "text": " might do if we didn't know how to use pandas. This is a really nice way to do", "tokens": [51558, 1062, 360, 498, 321, 994, 380, 458, 577, 281, 764, 4565, 296, 13, 639, 307, 257, 534, 1481, 636, 281, 360, 51732], "temperature": 0.0, "avg_logprob": -0.07798816079962743, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.01691107638180256}, {"id": 1378, "seek": 487544, "start": 4875.5199999999995, "end": 4879.679999999999, "text": " it read CSV, load it into a data frame object, which actually means we can", "tokens": [50368, 309, 1401, 48814, 11, 3677, 309, 666, 257, 1412, 3920, 2657, 11, 597, 767, 1355, 321, 393, 50576], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1379, "seek": 487544, "start": 4879.679999999999, "end": 4883.799999999999, "text": " reference specific columns and specific rows in the data frame. So let's run this", "tokens": [50576, 6408, 2685, 13766, 293, 2685, 13241, 294, 264, 1412, 3920, 13, 407, 718, 311, 1190, 341, 50782], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1380, "seek": 487544, "start": 4883.799999999999, "end": 4890.759999999999, "text": " and just have a look at it. Yeah, I got need to print dftrain.head. So let's do", "tokens": [50782, 293, 445, 362, 257, 574, 412, 309, 13, 865, 11, 286, 658, 643, 281, 4482, 274, 844, 7146, 13, 1934, 13, 407, 718, 311, 360, 51130], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1381, "seek": 487544, "start": 4890.759999999999, "end": 4895.919999999999, "text": " that. And there we go. So this is what our data frame head looks like. Now head", "tokens": [51130, 300, 13, 400, 456, 321, 352, 13, 407, 341, 307, 437, 527, 1412, 3920, 1378, 1542, 411, 13, 823, 1378, 51388], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1382, "seek": 487544, "start": 4895.919999999999, "end": 4900.799999999999, "text": " what that does is show us the first five entries in our data set, as well as show", "tokens": [51388, 437, 300, 775, 307, 855, 505, 264, 700, 1732, 23041, 294, 527, 1412, 992, 11, 382, 731, 382, 855, 51632], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1383, "seek": 487544, "start": 4900.799999999999, "end": 4904.759999999999, "text": " us a lot of the different columns that are in it. Now since we have more than", "tokens": [51632, 505, 257, 688, 295, 264, 819, 13766, 300, 366, 294, 309, 13, 823, 1670, 321, 362, 544, 813, 51830], "temperature": 0.0, "avg_logprob": -0.13593472621237584, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.015904249623417854}, {"id": 1384, "seek": 490476, "start": 4904.76, "end": 4907.52, "text": " you know, we have a few different columns, it's not showing us all of them, it's", "tokens": [50364, 291, 458, 11, 321, 362, 257, 1326, 819, 13766, 11, 309, 311, 406, 4099, 505, 439, 295, 552, 11, 309, 311, 50502], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1385, "seek": 490476, "start": 4907.52, "end": 4910.92, "text": " just giving us the dot dot dot. But we can see this is what the data frame", "tokens": [50502, 445, 2902, 505, 264, 5893, 5893, 5893, 13, 583, 321, 393, 536, 341, 307, 437, 264, 1412, 3920, 50672], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1386, "seek": 490476, "start": 4910.92, "end": 4914.96, "text": " looks like. And this is kind of the representation internally. So we have", "tokens": [50672, 1542, 411, 13, 400, 341, 307, 733, 295, 264, 10290, 19501, 13, 407, 321, 362, 50874], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1387, "seek": 490476, "start": 4915.16, "end": 4920.04, "text": " entry zero, survived zero, survived one, we have male, female, all that. Now", "tokens": [50884, 8729, 4018, 11, 14433, 4018, 11, 14433, 472, 11, 321, 362, 7133, 11, 6556, 11, 439, 300, 13, 823, 51128], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1388, "seek": 490476, "start": 4920.04, "end": 4923.8, "text": " notice that this has the survived column. Okay. Because what I'm going to do is", "tokens": [51128, 3449, 300, 341, 575, 264, 14433, 7738, 13, 1033, 13, 1436, 437, 286, 478, 516, 281, 360, 307, 51316], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1389, "seek": 490476, "start": 4923.8, "end": 4930.4400000000005, "text": " I'm going to print the data frame head again. So dftrain.head after we run", "tokens": [51316, 286, 478, 516, 281, 4482, 264, 1412, 3920, 1378, 797, 13, 407, 274, 844, 7146, 13, 1934, 934, 321, 1190, 51648], "temperature": 0.0, "avg_logprob": -0.12222459411621094, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.005729437805712223}, {"id": 1390, "seek": 493044, "start": 4930.44, "end": 4935.839999999999, "text": " these two lines. Now what this line does is takes this entire survived column, so", "tokens": [50364, 613, 732, 3876, 13, 823, 437, 341, 1622, 775, 307, 2516, 341, 2302, 14433, 7738, 11, 370, 50634], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1391, "seek": 493044, "start": 4935.839999999999, "end": 4940.4, "text": " all these zeros and ones, and removes it from this data frame, so the head data", "tokens": [50634, 439, 613, 35193, 293, 2306, 11, 293, 30445, 309, 490, 341, 1412, 3920, 11, 370, 264, 1378, 1412, 50862], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1392, "seek": 493044, "start": 4940.4, "end": 4944.719999999999, "text": " frame, and stores it in the variable y train. The reason we need to do that is", "tokens": [50862, 3920, 11, 293, 9512, 309, 294, 264, 7006, 288, 3847, 13, 440, 1778, 321, 643, 281, 360, 300, 307, 51078], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1393, "seek": 493044, "start": 4944.719999999999, "end": 4947.48, "text": " because we need to separate the data, we're going to be classifying from the", "tokens": [51078, 570, 321, 643, 281, 4994, 264, 1412, 11, 321, 434, 516, 281, 312, 1508, 5489, 490, 264, 51216], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1394, "seek": 493044, "start": 4947.48, "end": 4951.48, "text": " data that is kind of our input information or our initial data set, right? So", "tokens": [51216, 1412, 300, 307, 733, 295, 527, 4846, 1589, 420, 527, 5883, 1412, 992, 11, 558, 30, 407, 51416], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1395, "seek": 493044, "start": 4951.48, "end": 4955.08, "text": " since we're looking for the survived information, we're going to put that in", "tokens": [51416, 1670, 321, 434, 1237, 337, 264, 14433, 1589, 11, 321, 434, 516, 281, 829, 300, 294, 51596], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1396, "seek": 493044, "start": 4955.08, "end": 4959.24, "text": " its own, you know, kind of variable store here. Now we'll do the same thing for", "tokens": [51596, 1080, 1065, 11, 291, 458, 11, 733, 295, 7006, 3531, 510, 13, 823, 321, 603, 360, 264, 912, 551, 337, 51804], "temperature": 0.0, "avg_logprob": -0.10187127930777413, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.06752751767635345}, {"id": 1397, "seek": 495924, "start": 4959.24, "end": 4964.24, "text": " the evaluation data set, which is DF evaluation or testing data. And notice", "tokens": [50364, 264, 13344, 1412, 992, 11, 597, 307, 48336, 13344, 420, 4997, 1412, 13, 400, 3449, 50614], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1398, "seek": 495924, "start": 4964.24, "end": 4968.84, "text": " that here this was trained on CSV, and this one was eval.csv. Now these have", "tokens": [50614, 300, 510, 341, 390, 8895, 322, 48814, 11, 293, 341, 472, 390, 1073, 304, 13, 14368, 85, 13, 823, 613, 362, 50844], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1399, "seek": 495924, "start": 4968.84, "end": 4973.32, "text": " the exact same form, they look the like completely identical, it's just that", "tokens": [50844, 264, 1900, 912, 1254, 11, 436, 574, 264, 411, 2584, 14800, 11, 309, 311, 445, 300, 51068], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1400, "seek": 495924, "start": 4973.32, "end": 4976.4, "text": " you know, some entries, we've just kind of arbitrarily split them. So we're going", "tokens": [51068, 291, 458, 11, 512, 23041, 11, 321, 600, 445, 733, 295, 19071, 3289, 7472, 552, 13, 407, 321, 434, 516, 51222], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1401, "seek": 495924, "start": 4976.4, "end": 4979.12, "text": " to have a lot of entries in this training set, and we'll have a few in the", "tokens": [51222, 281, 362, 257, 688, 295, 23041, 294, 341, 3097, 992, 11, 293, 321, 603, 362, 257, 1326, 294, 264, 51358], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1402, "seek": 495924, "start": 4979.12, "end": 4983.0, "text": " testing set that we'll just use to do an evaluation on the model later on. So we", "tokens": [51358, 4997, 992, 300, 321, 603, 445, 764, 281, 360, 364, 13344, 322, 264, 2316, 1780, 322, 13, 407, 321, 51552], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1403, "seek": 495924, "start": 4983.0, "end": 4987.8, "text": " pop them off by doing this pop removes and returns this column. So if I print", "tokens": [51552, 1665, 552, 766, 538, 884, 341, 1665, 30445, 293, 11247, 341, 7738, 13, 407, 498, 286, 4482, 51792], "temperature": 0.0, "avg_logprob": -0.1104618819443496, "compression_ratio": 1.7694805194805194, "no_speech_prob": 0.0019876589067280293}, {"id": 1404, "seek": 498780, "start": 4987.84, "end": 4991.16, "text": " out why train, which are actually let's look at this one first, just to show", "tokens": [50366, 484, 983, 3847, 11, 597, 366, 767, 718, 311, 574, 412, 341, 472, 700, 11, 445, 281, 855, 50532], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1405, "seek": 498780, "start": 4991.16, "end": 4995.2, "text": " you how it's been removed, we can see that we have the survived column here, we", "tokens": [50532, 291, 577, 309, 311, 668, 7261, 11, 321, 393, 536, 300, 321, 362, 264, 14433, 7738, 510, 11, 321, 50734], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1406, "seek": 498780, "start": 4995.2, "end": 4998.6, "text": " popped and now the survived column is removed from that data set. So that's", "tokens": [50734, 21545, 293, 586, 264, 14433, 7738, 307, 7261, 490, 300, 1412, 992, 13, 407, 300, 311, 50904], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1407, "seek": 498780, "start": 4998.6, "end": 5002.12, "text": " just important to understand. Now we can print out some other stuff too. So we can", "tokens": [50904, 445, 1021, 281, 1223, 13, 823, 321, 393, 4482, 484, 512, 661, 1507, 886, 13, 407, 321, 393, 51080], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1408, "seek": 498780, "start": 5002.12, "end": 5005.96, "text": " look at the why train and see what that is. Just to make sure we really", "tokens": [51080, 574, 412, 264, 983, 3847, 293, 536, 437, 300, 307, 13, 1449, 281, 652, 988, 321, 534, 51272], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1409, "seek": 498780, "start": 5005.96, "end": 5009.360000000001, "text": " understand this data. So let's look at why train. And you can see that we have", "tokens": [51272, 1223, 341, 1412, 13, 407, 718, 311, 574, 412, 983, 3847, 13, 400, 291, 393, 536, 300, 321, 362, 51442], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1410, "seek": 498780, "start": 5009.360000000001, "end": 5014.96, "text": " 626 or 627 entries and address, you know, zeros or ones representing whether", "tokens": [51442, 1386, 10880, 420, 1386, 10076, 23041, 293, 2985, 11, 291, 458, 11, 35193, 420, 2306, 13460, 1968, 51722], "temperature": 0.0, "avg_logprob": -0.14084531239100864, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.01912163756787777}, {"id": 1411, "seek": 501496, "start": 5014.96, "end": 5019.72, "text": " someone survived or whether they did not. Now the corresponding indexes in this", "tokens": [50364, 1580, 14433, 420, 1968, 436, 630, 406, 13, 823, 264, 11760, 8186, 279, 294, 341, 50602], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1412, "seek": 501496, "start": 5019.8, "end": 5024.16, "text": " kind of list or data frame correspond to the indexes in the testing and", "tokens": [50606, 733, 295, 1329, 420, 1412, 3920, 6805, 281, 264, 8186, 279, 294, 264, 4997, 293, 50824], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1413, "seek": 501496, "start": 5024.2, "end": 5028.2, "text": " training data frame. What I mean by that is, you know, entry zero in this", "tokens": [50826, 3097, 1412, 3920, 13, 708, 286, 914, 538, 300, 307, 11, 291, 458, 11, 8729, 4018, 294, 341, 51026], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1414, "seek": 501496, "start": 5028.24, "end": 5033.96, "text": " specific data frame corresponds to entry zero in our why train variable. So if", "tokens": [51028, 2685, 1412, 3920, 23249, 281, 8729, 4018, 294, 527, 983, 3847, 7006, 13, 407, 498, 51314], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1415, "seek": 501496, "start": 5033.96, "end": 5038.24, "text": " someone survived, you know, at entry zero, it would say one here, right? Or in", "tokens": [51314, 1580, 14433, 11, 291, 458, 11, 412, 8729, 4018, 11, 309, 576, 584, 472, 510, 11, 558, 30, 1610, 294, 51528], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1416, "seek": 501496, "start": 5038.24, "end": 5042.36, "text": " this case, entry zero did not survive. Now, I hope that's clear. I hope I'm not", "tokens": [51528, 341, 1389, 11, 8729, 4018, 630, 406, 7867, 13, 823, 11, 286, 1454, 300, 311, 1850, 13, 286, 1454, 286, 478, 406, 51734], "temperature": 0.0, "avg_logprob": -0.1115553158671916, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.007576576434075832}, {"id": 1417, "seek": 504236, "start": 5042.4, "end": 5046.32, "text": " confusing you with that. But I just want to show one more example to make sure. So", "tokens": [50366, 13181, 291, 365, 300, 13, 583, 286, 445, 528, 281, 855, 472, 544, 1365, 281, 652, 988, 13, 407, 50562], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1418, "seek": 504236, "start": 5046.32, "end": 5049.88, "text": " we'll say D F train zero, I'm going to print that and I'm going to print why", "tokens": [50562, 321, 603, 584, 413, 479, 3847, 4018, 11, 286, 478, 516, 281, 4482, 300, 293, 286, 478, 516, 281, 4482, 983, 50740], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1419, "seek": 504236, "start": 5049.88, "end": 5054.88, "text": " train at index zero. Oops, if I didn't mess up my brackets, and we'll have a", "tokens": [50740, 3847, 412, 8186, 4018, 13, 21726, 11, 498, 286, 994, 380, 2082, 493, 452, 26179, 11, 293, 321, 603, 362, 257, 50990], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1420, "seek": 504236, "start": 5054.88, "end": 5058.24, "text": " look at it. Okay, so I've just looked up the documentation because I totally", "tokens": [50990, 574, 412, 309, 13, 1033, 11, 370, 286, 600, 445, 2956, 493, 264, 14333, 570, 286, 3879, 51158], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1421, "seek": 504236, "start": 5058.24, "end": 5062.28, "text": " forgot that I couldn't do that. If I want to find one specific row in my data", "tokens": [51158, 5298, 300, 286, 2809, 380, 360, 300, 13, 759, 286, 528, 281, 915, 472, 2685, 5386, 294, 452, 1412, 51360], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1422, "seek": 504236, "start": 5062.28, "end": 5067.599999999999, "text": " frame, what I can do is print dot loc. So I do my data frame and then dot loc and", "tokens": [51360, 3920, 11, 437, 286, 393, 360, 307, 4482, 5893, 1628, 13, 407, 286, 360, 452, 1412, 3920, 293, 550, 5893, 1628, 293, 51626], "temperature": 0.0, "avg_logprob": -0.14624552619188352, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008061445318162441}, {"id": 1423, "seek": 506760, "start": 5067.6, "end": 5071.8, "text": " then whatever index I want. So in this case, I'm locating row zero, which is", "tokens": [50364, 550, 2035, 8186, 286, 528, 13, 407, 294, 341, 1389, 11, 286, 478, 1628, 990, 5386, 4018, 11, 597, 307, 50574], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1424, "seek": 506760, "start": 5071.8, "end": 5075.4800000000005, "text": " this. And then on the why train, I'm doing the same thing, I'm locating row", "tokens": [50574, 341, 13, 400, 550, 322, 264, 983, 3847, 11, 286, 478, 884, 264, 912, 551, 11, 286, 478, 1628, 990, 5386, 50758], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1425, "seek": 506760, "start": 5075.4800000000005, "end": 5080.280000000001, "text": " zero. Now what I had before, right, if I did D F train, and I put square brackets", "tokens": [50758, 4018, 13, 823, 437, 286, 632, 949, 11, 558, 11, 498, 286, 630, 413, 479, 3847, 11, 293, 286, 829, 3732, 26179, 50998], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1426, "seek": 506760, "start": 5080.52, "end": 5084.0, "text": " inside here, what I can actually do is reference a specific column. So if I", "tokens": [51010, 1854, 510, 11, 437, 286, 393, 767, 360, 307, 6408, 257, 2685, 7738, 13, 407, 498, 286, 51184], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1427, "seek": 506760, "start": 5084.0, "end": 5087.96, "text": " wanted to look at, you know, say the column for age, right, so we have a", "tokens": [51184, 1415, 281, 574, 412, 11, 291, 458, 11, 584, 264, 7738, 337, 3205, 11, 558, 11, 370, 321, 362, 257, 51382], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1428, "seek": 506760, "start": 5087.96, "end": 5092.84, "text": " column for age, what I can do is do D F train age. And then I can print this out", "tokens": [51382, 7738, 337, 3205, 11, 437, 286, 393, 360, 307, 360, 413, 479, 3847, 3205, 13, 400, 550, 286, 393, 4482, 341, 484, 51626], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1429, "seek": 506760, "start": 5092.84, "end": 5096.4400000000005, "text": " like this. And it gives me all of the different age values. So that's kind of", "tokens": [51626, 411, 341, 13, 400, 309, 2709, 385, 439, 295, 264, 819, 3205, 4190, 13, 407, 300, 311, 733, 295, 51806], "temperature": 0.0, "avg_logprob": -0.09559489815098465, "compression_ratio": 1.8006644518272426, "no_speech_prob": 0.010012974962592125}, {"id": 1430, "seek": 509644, "start": 5096.44, "end": 5099.799999999999, "text": " how we use a data frame, we'll see that as we go further on. Now let's go back to", "tokens": [50364, 577, 321, 764, 257, 1412, 3920, 11, 321, 603, 536, 300, 382, 321, 352, 3052, 322, 13, 823, 718, 311, 352, 646, 281, 50532], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1431, "seek": 509644, "start": 5099.799999999999, "end": 5103.5199999999995, "text": " the other example I had, because I just erased it, where I want to show you the", "tokens": [50532, 264, 661, 1365, 286, 632, 11, 570, 286, 445, 38359, 309, 11, 689, 286, 528, 281, 855, 291, 264, 50718], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1432, "seek": 509644, "start": 5103.919999999999, "end": 5108.96, "text": " row zero in the data frame that's training, and then in the why train, you", "tokens": [50738, 5386, 4018, 294, 264, 1412, 3920, 300, 311, 3097, 11, 293, 550, 294, 264, 983, 3847, 11, 291, 50990], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1433, "seek": 509644, "start": 5108.96, "end": 5113.08, "text": " know, output, whatever that is. So the survival. So you can see here that this", "tokens": [50990, 458, 11, 5598, 11, 2035, 300, 307, 13, 407, 264, 12559, 13, 407, 291, 393, 536, 510, 300, 341, 51196], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1434, "seek": 509644, "start": 5113.08, "end": 5117.5199999999995, "text": " is what we get from printing D F train loc zero. So row zero, this is all the", "tokens": [51196, 307, 437, 321, 483, 490, 14699, 413, 479, 3847, 1628, 4018, 13, 407, 5386, 4018, 11, 341, 307, 439, 264, 51418], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1435, "seek": 509644, "start": 5117.5199999999995, "end": 5121.2, "text": " information. And then here, this corresponds to the fact that they did not", "tokens": [51418, 1589, 13, 400, 550, 510, 11, 341, 23249, 281, 264, 1186, 300, 436, 630, 406, 51602], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1436, "seek": 509644, "start": 5121.2, "end": 5125.4, "text": " survive at row zero, because it's simply just the output is value zero. Now I", "tokens": [51602, 7867, 412, 5386, 4018, 11, 570, 309, 311, 2935, 445, 264, 5598, 307, 2158, 4018, 13, 823, 286, 51812], "temperature": 0.0, "avg_logprob": -0.11318137194659258, "compression_ratio": 1.832214765100671, "no_speech_prob": 0.00505980895832181}, {"id": 1437, "seek": 512540, "start": 5125.44, "end": 5129.32, "text": " know this is weird, it's saying like name, zero, d type, object, zero, don't", "tokens": [50366, 458, 341, 307, 3657, 11, 309, 311, 1566, 411, 1315, 11, 4018, 11, 274, 2010, 11, 2657, 11, 4018, 11, 500, 380, 50560], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1438, "seek": 512540, "start": 5129.32, "end": 5131.5599999999995, "text": " worry about that. It's just because it's trying to print it with some", "tokens": [50560, 3292, 466, 300, 13, 467, 311, 445, 570, 309, 311, 1382, 281, 4482, 309, 365, 512, 50672], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1439, "seek": 512540, "start": 5131.5599999999995, "end": 5135.879999999999, "text": " information. But essentially, this just means this person who was male 22 and had", "tokens": [50672, 1589, 13, 583, 4476, 11, 341, 445, 1355, 341, 954, 567, 390, 7133, 5853, 293, 632, 50888], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1440, "seek": 512540, "start": 5135.879999999999, "end": 5140.48, "text": " one sibling did not survive. Okay, so let's get out of this. Now we can close", "tokens": [50888, 472, 39409, 630, 406, 7867, 13, 1033, 11, 370, 718, 311, 483, 484, 295, 341, 13, 823, 321, 393, 1998, 51118], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1441, "seek": 512540, "start": 5140.48, "end": 5144.36, "text": " this and let's go to Oh, we've pretty much already done what I've just have", "tokens": [51118, 341, 293, 718, 311, 352, 281, 876, 11, 321, 600, 1238, 709, 1217, 1096, 437, 286, 600, 445, 362, 51312], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1442, "seek": 512540, "start": 5144.36, "end": 5147.799999999999, "text": " down here. But we can look at the data frame head. This is a little bit of a", "tokens": [51312, 760, 510, 13, 583, 321, 393, 574, 412, 264, 1412, 3920, 1378, 13, 639, 307, 257, 707, 857, 295, 257, 51484], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1443, "seek": 512540, "start": 5147.799999999999, "end": 5151.879999999999, "text": " nicer output when we just have D F train dot head, we can see that we get kind of", "tokens": [51484, 22842, 5598, 562, 321, 445, 362, 413, 479, 3847, 5893, 1378, 11, 321, 393, 536, 300, 321, 483, 733, 295, 51688], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1444, "seek": 512540, "start": 5151.879999999999, "end": 5155.28, "text": " a nice outputted little graph. We've already looked at this information. So we", "tokens": [51688, 257, 1481, 5598, 14727, 707, 4295, 13, 492, 600, 1217, 2956, 412, 341, 1589, 13, 407, 321, 51858], "temperature": 0.0, "avg_logprob": -0.1260006314232236, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0010004768846556544}, {"id": 1445, "seek": 515528, "start": 5155.32, "end": 5159.2, "text": " know kind of some of the attributes of the data set. Now we want to describe the", "tokens": [50366, 458, 733, 295, 512, 295, 264, 17212, 295, 264, 1412, 992, 13, 823, 321, 528, 281, 6786, 264, 50560], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1446, "seek": 515528, "start": 5159.2, "end": 5163.5199999999995, "text": " data set sometimes. What describe does is just give us some overall information. So", "tokens": [50560, 1412, 992, 2171, 13, 708, 6786, 775, 307, 445, 976, 505, 512, 4787, 1589, 13, 407, 50776], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1447, "seek": 515528, "start": 5163.5199999999995, "end": 5168.48, "text": " let's have a look at it here. We can see that we have 627 entries, the mean of", "tokens": [50776, 718, 311, 362, 257, 574, 412, 309, 510, 13, 492, 393, 536, 300, 321, 362, 1386, 10076, 23041, 11, 264, 914, 295, 51024], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1448, "seek": 515528, "start": 5168.5199999999995, "end": 5173.36, "text": " age is 29, the standard deviation is you know, 12 point whatever. And then we get", "tokens": [51026, 3205, 307, 9413, 11, 264, 3832, 25163, 307, 291, 458, 11, 2272, 935, 2035, 13, 400, 550, 321, 483, 51268], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1449, "seek": 515528, "start": 5173.36, "end": 5177.04, "text": " the same information about all of these other different attributes. So for example,", "tokens": [51268, 264, 912, 1589, 466, 439, 295, 613, 661, 819, 17212, 13, 407, 337, 1365, 11, 51452], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1450, "seek": 515528, "start": 5177.04, "end": 5180.32, "text": " it gives us you know, the mean fair, the minimum fair, and just some statistics,", "tokens": [51452, 309, 2709, 505, 291, 458, 11, 264, 914, 3143, 11, 264, 7285, 3143, 11, 293, 445, 512, 12523, 11, 51616], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1451, "seek": 515528, "start": 5180.36, "end": 5183.759999999999, "text": " because understand this great, if you don't doesn't really matter. The important", "tokens": [51618, 570, 1223, 341, 869, 11, 498, 291, 500, 380, 1177, 380, 534, 1871, 13, 440, 1021, 51788], "temperature": 0.0, "avg_logprob": -0.130776750280502, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.0022516006138175726}, {"id": 1452, "seek": 518376, "start": 5183.76, "end": 5186.56, "text": " thing to look at typically is just how many entries we have is sometimes we need", "tokens": [50364, 551, 281, 574, 412, 5850, 307, 445, 577, 867, 23041, 321, 362, 307, 2171, 321, 643, 50504], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1453, "seek": 518376, "start": 5186.56, "end": 5189.88, "text": " that information. And sometimes the mean can be helpful as well, because you can", "tokens": [50504, 300, 1589, 13, 400, 2171, 264, 914, 393, 312, 4961, 382, 731, 11, 570, 291, 393, 50670], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1454, "seek": 518376, "start": 5189.88, "end": 5194.08, "text": " kind of get an average of like what the average value is in the data set. So if", "tokens": [50670, 733, 295, 483, 364, 4274, 295, 411, 437, 264, 4274, 2158, 307, 294, 264, 1412, 992, 13, 407, 498, 50880], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1455, "seek": 518376, "start": 5194.08, "end": 5197.52, "text": " there's any bias later on, you can figure that out. But it's not crazy important.", "tokens": [50880, 456, 311, 604, 12577, 1780, 322, 11, 291, 393, 2573, 300, 484, 13, 583, 309, 311, 406, 3219, 1021, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1456, "seek": 518376, "start": 5197.92, "end": 5201.68, "text": " Okay, so let's have a look at the shape. So just like NumPy arrays and tensors", "tokens": [51072, 1033, 11, 370, 718, 311, 362, 257, 574, 412, 264, 3909, 13, 407, 445, 411, 22592, 47, 88, 41011, 293, 10688, 830, 51260], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1457, "seek": 518376, "start": 5201.68, "end": 5205.08, "text": " have a shape attribute, so do data frames. So we want to look at the shape, you", "tokens": [51260, 362, 257, 3909, 19667, 11, 370, 360, 1412, 12083, 13, 407, 321, 528, 281, 574, 412, 264, 3909, 11, 291, 51430], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1458, "seek": 518376, "start": 5205.08, "end": 5209.320000000001, "text": " know, we can just print out D F train dot shape, we get 627 by nine, which", "tokens": [51430, 458, 11, 321, 393, 445, 4482, 484, 413, 479, 3847, 5893, 3909, 11, 321, 483, 1386, 10076, 538, 4949, 11, 597, 51642], "temperature": 0.0, "avg_logprob": -0.10261337916056315, "compression_ratio": 1.7460815047021943, "no_speech_prob": 0.011685356497764587}, {"id": 1459, "seek": 520932, "start": 5209.32, "end": 5215.88, "text": " essentially means we have 627 rows, and nine columns or nine attributes. So yeah,", "tokens": [50364, 4476, 1355, 321, 362, 1386, 10076, 13241, 11, 293, 4949, 13766, 420, 4949, 17212, 13, 407, 1338, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1460, "seek": 520932, "start": 5215.88, "end": 5220.48, "text": " that's what it says here, you know, 627 entries, nine features, we can interchange", "tokens": [50692, 300, 311, 437, 309, 1619, 510, 11, 291, 458, 11, 1386, 10076, 23041, 11, 4949, 4122, 11, 321, 393, 30358, 50922], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1461, "seek": 520932, "start": 5220.48, "end": 5224.36, "text": " attributes and features. And we can look at the head information for why so we can", "tokens": [50922, 17212, 293, 4122, 13, 400, 321, 393, 574, 412, 264, 1378, 1589, 337, 983, 370, 321, 393, 51116], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1462, "seek": 520932, "start": 5224.36, "end": 5227.24, "text": " see that here, which we've already looked at before. And that gives us the name,", "tokens": [51116, 536, 300, 510, 11, 597, 321, 600, 1217, 2956, 412, 949, 13, 400, 300, 2709, 505, 264, 1315, 11, 51260], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1463, "seek": 520932, "start": 5227.28, "end": 5231.44, "text": " which was survived. Okay, so now what we can actually do is make some kind of", "tokens": [51262, 597, 390, 14433, 13, 1033, 11, 370, 586, 437, 321, 393, 767, 360, 307, 652, 512, 733, 295, 51470], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1464, "seek": 520932, "start": 5231.44, "end": 5234.5199999999995, "text": " graphs about this data. Now I've just stolen this code, you know, straight up", "tokens": [51470, 24877, 466, 341, 1412, 13, 823, 286, 600, 445, 15900, 341, 3089, 11, 291, 458, 11, 2997, 493, 51624], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1465, "seek": 520932, "start": 5234.5199999999995, "end": 5238.32, "text": " from the TensorFlow website, I wouldn't expect you guys to do any of this, you", "tokens": [51624, 490, 264, 37624, 3144, 11, 286, 2759, 380, 2066, 291, 1074, 281, 360, 604, 295, 341, 11, 291, 51814], "temperature": 0.0, "avg_logprob": -0.08882476886113484, "compression_ratio": 1.7164634146341464, "no_speech_prob": 0.00912505853921175}, {"id": 1466, "seek": 523832, "start": 5238.32, "end": 5241.759999999999, "text": " know, like output any of these values. What we're going to do is create a few", "tokens": [50364, 458, 11, 411, 5598, 604, 295, 613, 4190, 13, 708, 321, 434, 516, 281, 360, 307, 1884, 257, 1326, 50536], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1467, "seek": 523832, "start": 5241.759999999999, "end": 5245.36, "text": " histograms and some plots just to look at kind of some correlations in the data.", "tokens": [50536, 49816, 82, 293, 512, 28609, 445, 281, 574, 412, 733, 295, 512, 13983, 763, 294, 264, 1412, 13, 50716], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1468, "seek": 523832, "start": 5245.36, "end": 5248.679999999999, "text": " So that when we start creating this model, we have some intuition on what we", "tokens": [50716, 407, 300, 562, 321, 722, 4084, 341, 2316, 11, 321, 362, 512, 24002, 322, 437, 321, 50882], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1469, "seek": 523832, "start": 5248.679999999999, "end": 5253.599999999999, "text": " might expect. So let's look at age. So this gives us a histogram of the age. So", "tokens": [50882, 1062, 2066, 13, 407, 718, 311, 574, 412, 3205, 13, 407, 341, 2709, 505, 257, 49816, 295, 264, 3205, 13, 407, 51128], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1470, "seek": 523832, "start": 5253.599999999999, "end": 5258.5599999999995, "text": " we can see that there's about 25 people that are kind of between zero and five.", "tokens": [51128, 321, 393, 536, 300, 456, 311, 466, 3552, 561, 300, 366, 733, 295, 1296, 4018, 293, 1732, 13, 51376], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1471, "seek": 523832, "start": 5258.759999999999, "end": 5263.599999999999, "text": " There is you know, maybe like five people that are in between five and 10. And", "tokens": [51386, 821, 307, 291, 458, 11, 1310, 411, 1732, 561, 300, 366, 294, 1296, 1732, 293, 1266, 13, 400, 51628], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1472, "seek": 523832, "start": 5263.599999999999, "end": 5267.12, "text": " then the most amount of people are kind of in between their 20s and 30s. So in the", "tokens": [51628, 550, 264, 881, 2372, 295, 561, 366, 733, 295, 294, 1296, 641, 945, 82, 293, 2217, 82, 13, 407, 294, 264, 51804], "temperature": 0.0, "avg_logprob": -0.08559116052121532, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.0018673621816560626}, {"id": 1473, "seek": 526712, "start": 5267.12, "end": 5271.48, "text": " mid 20s, this is good information to know, because that's going to introduce a", "tokens": [50364, 2062, 945, 82, 11, 341, 307, 665, 1589, 281, 458, 11, 570, 300, 311, 516, 281, 5366, 257, 50582], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1474, "seek": 526712, "start": 5271.48, "end": 5275.92, "text": " little bit of bias into kind of our linear correlation graph, right? So just", "tokens": [50582, 707, 857, 295, 12577, 666, 733, 295, 527, 8213, 20009, 4295, 11, 558, 30, 407, 445, 50804], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1475, "seek": 526712, "start": 5276.16, "end": 5279.72, "text": " understanding, you know, that we have like a large subset, there's some outliers", "tokens": [50816, 3701, 11, 291, 458, 11, 300, 321, 362, 411, 257, 2416, 25993, 11, 456, 311, 512, 484, 23646, 50994], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1476, "seek": 526712, "start": 5279.72, "end": 5282.76, "text": " here, like there's one person that's 80, right over here, a few people that are", "tokens": [50994, 510, 11, 411, 456, 311, 472, 954, 300, 311, 4688, 11, 558, 670, 510, 11, 257, 1326, 561, 300, 366, 51146], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1477, "seek": 526712, "start": 5282.76, "end": 5286.08, "text": " 70, some important things to kind of understand before we move on to the", "tokens": [51146, 5285, 11, 512, 1021, 721, 281, 733, 295, 1223, 949, 321, 1286, 322, 281, 264, 51312], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1478, "seek": 526712, "start": 5286.08, "end": 5290.96, "text": " algorithm. So let's look at the sex values now. So this is how many female and", "tokens": [51312, 9284, 13, 407, 718, 311, 574, 412, 264, 3260, 4190, 586, 13, 407, 341, 307, 577, 867, 6556, 293, 51556], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1479, "seek": 526712, "start": 5290.96, "end": 5295.36, "text": " how many male, we can see that there's many more males than there is females. We", "tokens": [51556, 577, 867, 7133, 11, 321, 393, 536, 300, 456, 311, 867, 544, 20776, 813, 456, 307, 21529, 13, 492, 51776], "temperature": 0.0, "avg_logprob": -0.09747134540097933, "compression_ratio": 1.8, "no_speech_prob": 0.004331137519329786}, {"id": 1480, "seek": 529536, "start": 5295.4, "end": 5298.5599999999995, "text": " can have a look at the class. So we can see if they're in first, second or third", "tokens": [50366, 393, 362, 257, 574, 412, 264, 1508, 13, 407, 321, 393, 536, 498, 436, 434, 294, 700, 11, 1150, 420, 2636, 50524], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1481, "seek": 529536, "start": 5298.5599999999995, "end": 5303.799999999999, "text": " class, most people are in third, then followed by first and then second. And", "tokens": [50524, 1508, 11, 881, 561, 366, 294, 2636, 11, 550, 6263, 538, 700, 293, 550, 1150, 13, 400, 50786], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1482, "seek": 529536, "start": 5303.799999999999, "end": 5307.24, "text": " then lastly, we can look at what is this that we're doing? Oh, the percentage", "tokens": [50786, 550, 16386, 11, 321, 393, 574, 412, 437, 307, 341, 300, 321, 434, 884, 30, 876, 11, 264, 9668, 50958], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1483, "seek": 529536, "start": 5307.24, "end": 5312.32, "text": " survival by sex. So we can see how likely a specific person or a specific sex is", "tokens": [50958, 12559, 538, 3260, 13, 407, 321, 393, 536, 577, 3700, 257, 2685, 954, 420, 257, 2685, 3260, 307, 51212], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1484, "seek": 529536, "start": 5312.32, "end": 5316.24, "text": " to survive just by plotting this. So we can see that males have about a 20%", "tokens": [51212, 281, 7867, 445, 538, 41178, 341, 13, 407, 321, 393, 536, 300, 20776, 362, 466, 257, 945, 4, 51408], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1485, "seek": 529536, "start": 5316.24, "end": 5320.639999999999, "text": " survival rate, whereas females are all the way up to about 78%. So that's", "tokens": [51408, 12559, 3314, 11, 9735, 21529, 366, 439, 264, 636, 493, 281, 466, 26369, 6856, 407, 300, 311, 51628], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1486, "seek": 529536, "start": 5320.639999999999, "end": 5323.88, "text": " important to understand that kind of confirms that what we were looking at", "tokens": [51628, 1021, 281, 1223, 300, 733, 295, 39982, 300, 437, 321, 645, 1237, 412, 51790], "temperature": 0.0, "avg_logprob": -0.07475688989213902, "compression_ratio": 1.8338983050847457, "no_speech_prob": 0.003824333194643259}, {"id": 1487, "seek": 532388, "start": 5323.88, "end": 5326.92, "text": " before in the data set when we were exploring it. And you don't need to do", "tokens": [50364, 949, 294, 264, 1412, 992, 562, 321, 645, 12736, 309, 13, 400, 291, 500, 380, 643, 281, 360, 50516], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1488, "seek": 532388, "start": 5326.92, "end": 5330.16, "text": " this every time that you're looking at a data set, but it is good to kind of get", "tokens": [50516, 341, 633, 565, 300, 291, 434, 1237, 412, 257, 1412, 992, 11, 457, 309, 307, 665, 281, 733, 295, 483, 50678], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1489, "seek": 532388, "start": 5330.16, "end": 5333.0, "text": " some intuition about it. So this is what we've learned so far, majority", "tokens": [50678, 512, 24002, 466, 309, 13, 407, 341, 307, 437, 321, 600, 3264, 370, 1400, 11, 6286, 50820], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1490, "seek": 532388, "start": 5333.0, "end": 5336.72, "text": " passengers are in their 20s or 30s, then majority passengers are male, they're in", "tokens": [50820, 18436, 366, 294, 641, 945, 82, 420, 2217, 82, 11, 550, 6286, 18436, 366, 7133, 11, 436, 434, 294, 51006], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1491, "seek": 532388, "start": 5336.72, "end": 5339.96, "text": " third class, and females have a much higher chance of survival, kind of", "tokens": [51006, 2636, 1508, 11, 293, 21529, 362, 257, 709, 2946, 2931, 295, 12559, 11, 733, 295, 51168], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1492, "seek": 532388, "start": 5339.96, "end": 5343.04, "text": " already knew that. Alright, so training and testing data sets. Now we already", "tokens": [51168, 1217, 2586, 300, 13, 2798, 11, 370, 3097, 293, 4997, 1412, 6352, 13, 823, 321, 1217, 51322], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1493, "seek": 532388, "start": 5343.04, "end": 5346.04, "text": " kind of went through this all skim through it quickly. Essentially, what we", "tokens": [51322, 733, 295, 1437, 807, 341, 439, 1110, 332, 807, 309, 2661, 13, 23596, 11, 437, 321, 51472], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1494, "seek": 532388, "start": 5346.04, "end": 5350.6, "text": " did above is load in two different data sets. The first data set was that", "tokens": [51472, 630, 3673, 307, 3677, 294, 732, 819, 1412, 6352, 13, 440, 700, 1412, 992, 390, 300, 51700], "temperature": 0.0, "avg_logprob": -0.10291446958269392, "compression_ratio": 1.7859237536656891, "no_speech_prob": 0.004754878114908934}, {"id": 1495, "seek": 535060, "start": 5350.6, "end": 5354.92, "text": " training data set which had the shape of 627 by nine. What I'm actually going to", "tokens": [50364, 3097, 1412, 992, 597, 632, 264, 3909, 295, 1386, 10076, 538, 4949, 13, 708, 286, 478, 767, 516, 281, 50580], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1496, "seek": 535060, "start": 5354.92, "end": 5360.64, "text": " do is create a code block here, and just have a look at what was this Df eval dot", "tokens": [50580, 360, 307, 1884, 257, 3089, 3461, 510, 11, 293, 445, 362, 257, 574, 412, 437, 390, 341, 413, 69, 1073, 304, 5893, 50866], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1497, "seek": 535060, "start": 5360.64, "end": 5364.4400000000005, "text": " shape to show you how many entries we have in here. So here in our testing", "tokens": [50866, 3909, 281, 855, 291, 577, 867, 23041, 321, 362, 294, 510, 13, 407, 510, 294, 527, 4997, 51056], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1498, "seek": 535060, "start": 5364.4400000000005, "end": 5369.68, "text": " data set, you can see we have significantly less at 264 entries, or rows,", "tokens": [51056, 1412, 992, 11, 291, 393, 536, 321, 362, 10591, 1570, 412, 7551, 19, 23041, 11, 420, 13241, 11, 51318], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1499, "seek": 535060, "start": 5369.68, "end": 5373.400000000001, "text": " whatever you want to call them. So that's how many things we have to actually", "tokens": [51318, 2035, 291, 528, 281, 818, 552, 13, 407, 300, 311, 577, 867, 721, 321, 362, 281, 767, 51504], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1500, "seek": 535060, "start": 5373.400000000001, "end": 5377.08, "text": " test our model. So what we do is we use that training data to create the model,", "tokens": [51504, 1500, 527, 2316, 13, 407, 437, 321, 360, 307, 321, 764, 300, 3097, 1412, 281, 1884, 264, 2316, 11, 51688], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1501, "seek": 535060, "start": 5377.120000000001, "end": 5379.88, "text": " and then the testing data to evaluate it and make sure that it's working", "tokens": [51690, 293, 550, 264, 4997, 1412, 281, 13059, 309, 293, 652, 988, 300, 309, 311, 1364, 51828], "temperature": 0.0, "avg_logprob": -0.124077790266984, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.034094665199518204}, {"id": 1502, "seek": 537988, "start": 5379.88, "end": 5383.16, "text": " properly. So these things are important whenever we're doing machine learning", "tokens": [50364, 6108, 13, 407, 613, 721, 366, 1021, 5699, 321, 434, 884, 3479, 2539, 50528], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1503, "seek": 537988, "start": 5383.16, "end": 5388.12, "text": " models, we typically have testing and training data. And yeah, that is pretty", "tokens": [50528, 5245, 11, 321, 5850, 362, 4997, 293, 3097, 1412, 13, 400, 1338, 11, 300, 307, 1238, 50776], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1504, "seek": 537988, "start": 5388.12, "end": 5391.4400000000005, "text": " much it. Now I'm just going to take one second to copy over a lot of this code", "tokens": [50776, 709, 309, 13, 823, 286, 478, 445, 516, 281, 747, 472, 1150, 281, 5055, 670, 257, 688, 295, 341, 3089, 50942], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1505, "seek": 537988, "start": 5391.72, "end": 5395.4400000000005, "text": " into the kind of other notebook I have, just so we can see all of it at once. And", "tokens": [50956, 666, 264, 733, 295, 661, 21060, 286, 362, 11, 445, 370, 321, 393, 536, 439, 295, 309, 412, 1564, 13, 400, 51142], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1506, "seek": 537988, "start": 5395.4400000000005, "end": 5398.2, "text": " then I'll be back and we'll get into actually making the model. Okay, so I've", "tokens": [51142, 550, 286, 603, 312, 646, 293, 321, 603, 483, 666, 767, 1455, 264, 2316, 13, 1033, 11, 370, 286, 600, 51280], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1507, "seek": 537988, "start": 5398.2, "end": 5401.08, "text": " copied in some code here. I know this seems like a lot of kind of gibberish", "tokens": [51280, 25365, 294, 512, 3089, 510, 13, 286, 458, 341, 2544, 411, 257, 688, 295, 733, 295, 4553, 43189, 51424], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1508, "seek": 537988, "start": 5401.08, "end": 5404.08, "text": " right now, but I'm going to break down line by line what all this is doing and", "tokens": [51424, 558, 586, 11, 457, 286, 478, 516, 281, 1821, 760, 1622, 538, 1622, 437, 439, 341, 307, 884, 293, 51574], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1509, "seek": 537988, "start": 5404.08, "end": 5407.52, "text": " why we have this here. But we first need to discuss something called feature", "tokens": [51574, 983, 321, 362, 341, 510, 13, 583, 321, 700, 643, 281, 2248, 746, 1219, 4111, 51746], "temperature": 0.0, "avg_logprob": -0.09565929770469665, "compression_ratio": 1.738888888888889, "no_speech_prob": 0.0020506721921265125}, {"id": 1510, "seek": 540752, "start": 5407.52, "end": 5413.040000000001, "text": " columns and the difference between categorical and numeric data. So categorical", "tokens": [50364, 13766, 293, 264, 2649, 1296, 19250, 804, 293, 7866, 299, 1412, 13, 407, 19250, 804, 50640], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1511, "seek": 540752, "start": 5413.040000000001, "end": 5416.240000000001, "text": " data is actually fairly common. Now when we're looking at our data set, and", "tokens": [50640, 1412, 307, 767, 6457, 2689, 13, 823, 562, 321, 434, 1237, 412, 527, 1412, 992, 11, 293, 50800], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1512, "seek": 540752, "start": 5416.240000000001, "end": 5419.92, "text": " actually I can open I don't have it open in Excel anymore, but let's open this", "tokens": [50800, 767, 286, 393, 1269, 286, 500, 380, 362, 309, 1269, 294, 19060, 3602, 11, 457, 718, 311, 1269, 341, 50984], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1513, "seek": 540752, "start": 5419.92, "end": 5425.64, "text": " from my downloads. So let's go downloads. Where is this train? Okay, awesome. So we", "tokens": [50984, 490, 452, 36553, 13, 407, 718, 311, 352, 36553, 13, 2305, 307, 341, 3847, 30, 1033, 11, 3476, 13, 407, 321, 51270], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1514, "seek": 540752, "start": 5425.64, "end": 5430.4800000000005, "text": " have this Excel data sheet here. And we can see what a categorical data or what", "tokens": [51270, 362, 341, 19060, 1412, 8193, 510, 13, 400, 321, 393, 536, 437, 257, 19250, 804, 1412, 420, 437, 51512], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1515, "seek": 540752, "start": 5430.4800000000005, "end": 5435.200000000001, "text": " categorical data is is something that's not numeric. So for example, unknown, C", "tokens": [51512, 19250, 804, 1412, 307, 307, 746, 300, 311, 406, 7866, 299, 13, 407, 337, 1365, 11, 9841, 11, 383, 51748], "temperature": 0.0, "avg_logprob": -0.13736236386182832, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.02595341205596924}, {"id": 1516, "seek": 543520, "start": 5435.32, "end": 5441.04, "text": " first, third, city, and why right so anything that has different categories,", "tokens": [50370, 700, 11, 2636, 11, 2307, 11, 293, 983, 558, 370, 1340, 300, 575, 819, 10479, 11, 50656], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1517, "seek": 543520, "start": 5441.72, "end": 5444.8, "text": " there's going to be like a specific set of different categories there could be. So", "tokens": [50690, 456, 311, 516, 281, 312, 411, 257, 2685, 992, 295, 819, 10479, 456, 727, 312, 13, 407, 50844], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1518, "seek": 543520, "start": 5444.8, "end": 5448.48, "text": " for example, for age, kind of the set of values we get out for age, well, this is", "tokens": [50844, 337, 1365, 11, 337, 3205, 11, 733, 295, 264, 992, 295, 4190, 321, 483, 484, 337, 3205, 11, 731, 11, 341, 307, 51028], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1519, "seek": 543520, "start": 5448.48, "end": 5451.28, "text": " numeric. So that's different. But for categorical, we can have male or we", "tokens": [51028, 7866, 299, 13, 407, 300, 311, 819, 13, 583, 337, 19250, 804, 11, 321, 393, 362, 7133, 420, 321, 51168], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1520, "seek": 543520, "start": 5451.28, "end": 5454.639999999999, "text": " can have female. And I suppose we could have other but in this data set, we just", "tokens": [51168, 393, 362, 6556, 13, 400, 286, 7297, 321, 727, 362, 661, 457, 294, 341, 1412, 992, 11, 321, 445, 51336], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1521, "seek": 543520, "start": 5454.639999999999, "end": 5458.5599999999995, "text": " have male and we just have female. For class, we can have first, second, third.", "tokens": [51336, 362, 7133, 293, 321, 445, 362, 6556, 13, 1171, 1508, 11, 321, 393, 362, 700, 11, 1150, 11, 2636, 13, 51532], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1522, "seek": 543520, "start": 5458.8, "end": 5462.32, "text": " For deck, we can have unknown CA, I'm sure through all the letters of the", "tokens": [51544, 1171, 9341, 11, 321, 393, 362, 9841, 22852, 11, 286, 478, 988, 807, 439, 264, 7825, 295, 264, 51720], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1523, "seek": 543520, "start": 5462.32, "end": 5464.72, "text": " alphabet, but that is still considered categorical.", "tokens": [51720, 23339, 11, 457, 300, 307, 920, 4888, 19250, 804, 13, 51840], "temperature": 0.0, "avg_logprob": -0.16802986965903752, "compression_ratio": 1.9356913183279743, "no_speech_prob": 0.028429685160517693}, {"id": 1524, "seek": 546520, "start": 5465.32, "end": 5469.5199999999995, "text": " Now, what do we do with categorical data? Well, we always need to transform this", "tokens": [50370, 823, 11, 437, 360, 321, 360, 365, 19250, 804, 1412, 30, 1042, 11, 321, 1009, 643, 281, 4088, 341, 50580], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1525, "seek": 546520, "start": 5469.5199999999995, "end": 5474.16, "text": " data into numbers somehow. So what we actually end up doing is we encode", "tokens": [50580, 1412, 666, 3547, 6063, 13, 407, 437, 321, 767, 917, 493, 884, 307, 321, 2058, 1429, 50812], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1526, "seek": 546520, "start": 5474.16, "end": 5478.639999999999, "text": " this data using an integer value. So for the example of male and female, what", "tokens": [50812, 341, 1412, 1228, 364, 24922, 2158, 13, 407, 337, 264, 1365, 295, 7133, 293, 6556, 11, 437, 51036], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1527, "seek": 546520, "start": 5478.639999999999, "end": 5481.96, "text": " we might say, and this is what we're going to do in a second is that female is", "tokens": [51036, 321, 1062, 584, 11, 293, 341, 307, 437, 321, 434, 516, 281, 360, 294, 257, 1150, 307, 300, 6556, 307, 51202], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1528, "seek": 546520, "start": 5481.96, "end": 5486.12, "text": " represented by zero and male is represented by one. We do this because", "tokens": [51202, 10379, 538, 4018, 293, 7133, 307, 10379, 538, 472, 13, 492, 360, 341, 570, 51410], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1529, "seek": 546520, "start": 5486.12, "end": 5490.28, "text": " although it's interesting to know what the actual class is, the model doesn't", "tokens": [51410, 4878, 309, 311, 1880, 281, 458, 437, 264, 3539, 1508, 307, 11, 264, 2316, 1177, 380, 51618], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1530, "seek": 546520, "start": 5490.28, "end": 5492.96, "text": " care, right, female and male, it doesn't make a difference to it. It just needs", "tokens": [51618, 1127, 11, 558, 11, 6556, 293, 7133, 11, 309, 1177, 380, 652, 257, 2649, 281, 309, 13, 467, 445, 2203, 51752], "temperature": 0.0, "avg_logprob": -0.09289849322775136, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0007096176850609481}, {"id": 1531, "seek": 549296, "start": 5492.96, "end": 5496.08, "text": " to know that those values are the different are different or those values", "tokens": [50364, 281, 458, 300, 729, 4190, 366, 264, 819, 366, 819, 420, 729, 4190, 50520], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1532, "seek": 549296, "start": 5496.08, "end": 5499.64, "text": " are the same. So rather than using strings and trying to find some way to", "tokens": [50520, 366, 264, 912, 13, 407, 2831, 813, 1228, 13985, 293, 1382, 281, 915, 512, 636, 281, 50698], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1533, "seek": 549296, "start": 5499.64, "end": 5503.28, "text": " pass that in and do math with that, we need to turn those into integers, we", "tokens": [50698, 1320, 300, 294, 293, 360, 5221, 365, 300, 11, 321, 643, 281, 1261, 729, 666, 41674, 11, 321, 50880], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1534, "seek": 549296, "start": 5503.28, "end": 5507.8, "text": " turn those into zeros and ones, right? Now for class, right, so first, second,", "tokens": [50880, 1261, 729, 666, 35193, 293, 2306, 11, 558, 30, 823, 337, 1508, 11, 558, 11, 370, 700, 11, 1150, 11, 51106], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1535, "seek": 549296, "start": 5507.8, "end": 5510.32, "text": " third, you know, you guys can probably assume what we're going to encode this", "tokens": [51106, 2636, 11, 291, 458, 11, 291, 1074, 393, 1391, 6552, 437, 321, 434, 516, 281, 2058, 1429, 341, 51232], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1536, "seek": 549296, "start": 5510.32, "end": 5513.72, "text": " with, we're going to encode it with zero, one, two. Now, again, this doesn't", "tokens": [51232, 365, 11, 321, 434, 516, 281, 2058, 1429, 309, 365, 4018, 11, 472, 11, 732, 13, 823, 11, 797, 11, 341, 1177, 380, 51402], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1537, "seek": 549296, "start": 5513.72, "end": 5517.68, "text": " necessarily need to be in order. So third could be represented by one and", "tokens": [51402, 4725, 643, 281, 312, 294, 1668, 13, 407, 2636, 727, 312, 10379, 538, 472, 293, 51600], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1538, "seek": 549296, "start": 5517.68, "end": 5521.36, "text": " first could be represented by two, right? It doesn't need to be in order, it", "tokens": [51600, 700, 727, 312, 10379, 538, 732, 11, 558, 30, 467, 1177, 380, 643, 281, 312, 294, 1668, 11, 309, 51784], "temperature": 0.0, "avg_logprob": -0.12477608919143676, "compression_ratio": 2.0334448160535117, "no_speech_prob": 0.006487633101642132}, {"id": 1539, "seek": 552136, "start": 5521.36, "end": 5525.0, "text": " doesn't matter. So long as every third has the same number, every first has the", "tokens": [50364, 1177, 380, 1871, 13, 407, 938, 382, 633, 2636, 575, 264, 912, 1230, 11, 633, 700, 575, 264, 50546], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1540, "seek": 552136, "start": 5525.0, "end": 5527.799999999999, "text": " same number and every second has the same number. And then same thing with", "tokens": [50546, 912, 1230, 293, 633, 1150, 575, 264, 912, 1230, 13, 400, 550, 912, 551, 365, 50686], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1541, "seek": 552136, "start": 5527.799999999999, "end": 5531.5199999999995, "text": " that same thing with embark and same thing with alone. Now we could have an", "tokens": [50686, 300, 912, 551, 365, 29832, 293, 912, 551, 365, 3312, 13, 823, 321, 727, 362, 364, 50872], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1542, "seek": 552136, "start": 5531.5199999999995, "end": 5534.799999999999, "text": " instance where you know, we've encoded every single one of these values with a", "tokens": [50872, 5197, 689, 291, 458, 11, 321, 600, 2058, 12340, 633, 2167, 472, 295, 613, 4190, 365, 257, 51036], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1543, "seek": 552136, "start": 5534.799999999999, "end": 5539.0, "text": " different value. So in the, you know, rare occasion where there's one category", "tokens": [51036, 819, 2158, 13, 407, 294, 264, 11, 291, 458, 11, 5892, 9674, 689, 456, 311, 472, 7719, 51246], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1544, "seek": 552136, "start": 5539.0, "end": 5543.639999999999, "text": " that's categorical, and every single value in that category is different, then", "tokens": [51246, 300, 311, 19250, 804, 11, 293, 633, 2167, 2158, 294, 300, 7719, 307, 819, 11, 550, 51478], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1545, "seek": 552136, "start": 5543.639999999999, "end": 5548.599999999999, "text": " we will have, you know, 627 in this instance, different encoding labels that", "tokens": [51478, 321, 486, 362, 11, 291, 458, 11, 1386, 10076, 294, 341, 5197, 11, 819, 43430, 16949, 300, 51726], "temperature": 0.0, "avg_logprob": -0.09618108922784979, "compression_ratio": 2.0606060606060606, "no_speech_prob": 0.006289544049650431}, {"id": 1546, "seek": 554860, "start": 5548.64, "end": 5551.68, "text": " are going to be numbers, that's fine, we can do that. And actually, we don't", "tokens": [50366, 366, 516, 281, 312, 3547, 11, 300, 311, 2489, 11, 321, 393, 360, 300, 13, 400, 767, 11, 321, 500, 380, 50518], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1547, "seek": 554860, "start": 5551.68, "end": 5555.320000000001, "text": " really need to do that, because you're going to see how TensorFlow can handle", "tokens": [50518, 534, 643, 281, 360, 300, 11, 570, 291, 434, 516, 281, 536, 577, 37624, 393, 4813, 50700], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1548, "seek": 554860, "start": 5555.320000000001, "end": 5559.280000000001, "text": " that for us. So that is categorical data, numeric columns are pretty", "tokens": [50700, 300, 337, 505, 13, 407, 300, 307, 19250, 804, 1412, 11, 7866, 299, 13766, 366, 1238, 50898], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1549, "seek": 554860, "start": 5559.280000000001, "end": 5562.08, "text": " straightforward. They're anything that just have integer or float values", "tokens": [50898, 15325, 13, 814, 434, 1340, 300, 445, 362, 24922, 420, 15706, 4190, 51038], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1550, "seek": 554860, "start": 5562.08, "end": 5566.76, "text": " already. So in this case, age and fair. And yeah, so that's what we've done. We've", "tokens": [51038, 1217, 13, 407, 294, 341, 1389, 11, 3205, 293, 3143, 13, 400, 1338, 11, 370, 300, 311, 437, 321, 600, 1096, 13, 492, 600, 51272], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1551, "seek": 554860, "start": 5566.76, "end": 5570.68, "text": " just defined our categorical columns here, and our numeric columns here. This is", "tokens": [51272, 445, 7642, 527, 19250, 804, 13766, 510, 11, 293, 527, 7866, 299, 13766, 510, 13, 639, 307, 51468], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1552, "seek": 554860, "start": 5570.68, "end": 5573.360000000001, "text": " important because we're going to loop through them, which we're doing here to", "tokens": [51468, 1021, 570, 321, 434, 516, 281, 6367, 807, 552, 11, 597, 321, 434, 884, 510, 281, 51602], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1553, "seek": 554860, "start": 5573.360000000001, "end": 5576.64, "text": " create something called feature columns, feature columns are nothing special.", "tokens": [51602, 1884, 746, 1219, 4111, 13766, 11, 4111, 13766, 366, 1825, 2121, 13, 51766], "temperature": 0.0, "avg_logprob": -0.11412655679803145, "compression_ratio": 1.872340425531915, "no_speech_prob": 0.02930675819516182}, {"id": 1554, "seek": 557664, "start": 5576.68, "end": 5580.76, "text": " They're just what we need to feed to our linear estimator or linear model to", "tokens": [50366, 814, 434, 445, 437, 321, 643, 281, 3154, 281, 527, 8213, 8017, 1639, 420, 8213, 2316, 281, 50570], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1555, "seek": 557664, "start": 5580.76, "end": 5584.200000000001, "text": " actually make predictions. So kind of our steps here that we've gone through so", "tokens": [50570, 767, 652, 21264, 13, 407, 733, 295, 527, 4439, 510, 300, 321, 600, 2780, 807, 370, 50742], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1556, "seek": 557664, "start": 5584.200000000001, "end": 5589.200000000001, "text": " far is import, load the data set, explore the data set, make sure we", "tokens": [50742, 1400, 307, 974, 11, 3677, 264, 1412, 992, 11, 6839, 264, 1412, 992, 11, 652, 988, 321, 50992], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1557, "seek": 557664, "start": 5589.200000000001, "end": 5593.200000000001, "text": " understand it, create our categorical columns and our numeric columns. So I've", "tokens": [50992, 1223, 309, 11, 1884, 527, 19250, 804, 13766, 293, 527, 7866, 299, 13766, 13, 407, 286, 600, 51192], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1558, "seek": 557664, "start": 5593.200000000001, "end": 5597.76, "text": " just hard coded these in right like sex, parts, class, deck, alone, all these", "tokens": [51192, 445, 1152, 34874, 613, 294, 558, 411, 3260, 11, 3166, 11, 1508, 11, 9341, 11, 3312, 11, 439, 613, 51420], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1559, "seek": 557664, "start": 5597.76, "end": 5601.04, "text": " ones. And then same thing with the numeric columns. And then for a linear", "tokens": [51420, 2306, 13, 400, 550, 912, 551, 365, 264, 7866, 299, 13766, 13, 400, 550, 337, 257, 8213, 51584], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1560, "seek": 557664, "start": 5601.04, "end": 5604.4800000000005, "text": " estimator, we need to create these as feature columns using some kind of", "tokens": [51584, 8017, 1639, 11, 321, 643, 281, 1884, 613, 382, 4111, 13766, 1228, 512, 733, 295, 51756], "temperature": 0.0, "avg_logprob": -0.10941244239237771, "compression_ratio": 1.8692579505300353, "no_speech_prob": 0.0017006199341267347}, {"id": 1561, "seek": 560448, "start": 5604.48, "end": 5608.24, "text": " advanced syntax, which we're going to look at here. So we create a blank list,", "tokens": [50364, 7339, 28431, 11, 597, 321, 434, 516, 281, 574, 412, 510, 13, 407, 321, 1884, 257, 8247, 1329, 11, 50552], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1562, "seek": 560448, "start": 5608.24, "end": 5610.759999999999, "text": " which is our feature columns, which will just store our different feature", "tokens": [50552, 597, 307, 527, 4111, 13766, 11, 597, 486, 445, 3531, 527, 819, 4111, 50678], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1563, "seek": 560448, "start": 5610.759999999999, "end": 5616.12, "text": " columns. We loop through each feature name in the categorical columns. And what", "tokens": [50678, 13766, 13, 492, 6367, 807, 1184, 4111, 1315, 294, 264, 19250, 804, 13766, 13, 400, 437, 50946], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1564, "seek": 560448, "start": 5616.12, "end": 5621.12, "text": " we do is we define a vocabulary, which is equal to the data frame at that", "tokens": [50946, 321, 360, 307, 321, 6964, 257, 19864, 11, 597, 307, 2681, 281, 264, 1412, 3920, 412, 300, 51196], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1565, "seek": 560448, "start": 5621.12, "end": 5624.639999999999, "text": " feature name. So first, we would start with sex, then we go and siblings, then", "tokens": [51196, 4111, 1315, 13, 407, 700, 11, 321, 576, 722, 365, 3260, 11, 550, 321, 352, 293, 20571, 11, 550, 51372], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1566, "seek": 560448, "start": 5624.639999999999, "end": 5629.759999999999, "text": " we go parts, then we go class. And we get all of the different unique values. So", "tokens": [51372, 321, 352, 3166, 11, 550, 321, 352, 1508, 13, 400, 321, 483, 439, 295, 264, 819, 3845, 4190, 13, 407, 51628], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1567, "seek": 560448, "start": 5629.759999999999, "end": 5633.36, "text": " that's actually what this does dot unique, gets a list of all unique values", "tokens": [51628, 300, 311, 767, 437, 341, 775, 5893, 3845, 11, 2170, 257, 1329, 295, 439, 3845, 4190, 51808], "temperature": 0.0, "avg_logprob": -0.10540172633002787, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.002550787292420864}, {"id": 1568, "seek": 563336, "start": 5633.4, "end": 5637.839999999999, "text": " from the feature call. And I can print this out. She'll print this in a", "tokens": [50366, 490, 264, 4111, 818, 13, 400, 286, 393, 4482, 341, 484, 13, 1240, 603, 4482, 341, 294, 257, 50588], "temperature": 0.0, "avg_logprob": -0.12269495199392508, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.004069857764989138}, {"id": 1569, "seek": 563336, "start": 5637.839999999999, "end": 5640.32, "text": " different line, we'll just take this value and have a look at actually what", "tokens": [50588, 819, 1622, 11, 321, 603, 445, 747, 341, 2158, 293, 362, 257, 574, 412, 767, 437, 50712], "temperature": 0.0, "avg_logprob": -0.12269495199392508, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.004069857764989138}, {"id": 1570, "seek": 563336, "start": 5640.32, "end": 5646.5199999999995, "text": " this is, right? So if I run, I guess we'll have to run all these in order. And", "tokens": [50712, 341, 307, 11, 558, 30, 407, 498, 286, 1190, 11, 286, 2041, 321, 603, 362, 281, 1190, 439, 613, 294, 1668, 13, 400, 51022], "temperature": 0.0, "avg_logprob": -0.12269495199392508, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.004069857764989138}, {"id": 1571, "seek": 563336, "start": 5646.5199999999995, "end": 5650.719999999999, "text": " then we'll create a new code block while we wait for that to happen. Let's see if", "tokens": [51022, 550, 321, 603, 1884, 257, 777, 3089, 3461, 1339, 321, 1699, 337, 300, 281, 1051, 13, 961, 311, 536, 498, 51232], "temperature": 0.0, "avg_logprob": -0.12269495199392508, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.004069857764989138}, {"id": 1572, "seek": 563336, "start": 5650.719999999999, "end": 5662.36, "text": " we can get this installing fast enough. Run, run, run. Okay, now we go to df", "tokens": [51232, 321, 393, 483, 341, 20762, 2370, 1547, 13, 8950, 11, 1190, 11, 1190, 13, 1033, 11, 586, 321, 352, 281, 274, 69, 51814], "temperature": 0.0, "avg_logprob": -0.12269495199392508, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.004069857764989138}, {"id": 1573, "seek": 566236, "start": 5662.36, "end": 5665.4, "text": " train. And we can see this is what this looks like. So these are all the", "tokens": [50364, 3847, 13, 400, 321, 393, 536, 341, 307, 437, 341, 1542, 411, 13, 407, 613, 366, 439, 264, 50516], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1574, "seek": 566236, "start": 5665.4, "end": 5668.96, "text": " different unique values that we had in that specific feature name. Now that", "tokens": [50516, 819, 3845, 4190, 300, 321, 632, 294, 300, 2685, 4111, 1315, 13, 823, 300, 50694], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1575, "seek": 566236, "start": 5668.96, "end": 5673.88, "text": " feature name was what categorical columns? Oh, what I do feature name up,", "tokens": [50694, 4111, 1315, 390, 437, 19250, 804, 13766, 30, 876, 11, 437, 286, 360, 4111, 1315, 493, 11, 50940], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1576, "seek": 566236, "start": 5673.88, "end": 5676.92, "text": " sorry, that's going to be the unique one. Let's just put rather than feature", "tokens": [50940, 2597, 11, 300, 311, 516, 281, 312, 264, 3845, 472, 13, 961, 311, 445, 829, 2831, 813, 4111, 51092], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1577, "seek": 566236, "start": 5676.92, "end": 5681.12, "text": " name. Let's put sex, right? And let's have a look at what this is. So we can", "tokens": [51092, 1315, 13, 961, 311, 829, 3260, 11, 558, 30, 400, 718, 311, 362, 257, 574, 412, 437, 341, 307, 13, 407, 321, 393, 51302], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1578, "seek": 566236, "start": 5681.12, "end": 5684.92, "text": " see that the two unique values are male and female. Now I actually want to do", "tokens": [51302, 536, 300, 264, 732, 3845, 4190, 366, 7133, 293, 6556, 13, 823, 286, 767, 528, 281, 360, 51492], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1579, "seek": 566236, "start": 5685.48, "end": 5688.799999999999, "text": " what is it embark town? And I want to see what this one is. So how many", "tokens": [51520, 437, 307, 309, 29832, 3954, 30, 400, 286, 528, 281, 536, 437, 341, 472, 307, 13, 407, 577, 867, 51686], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1580, "seek": 566236, "start": 5688.799999999999, "end": 5691.92, "text": " different values we have. So we'll copy that in. And we can see we have", "tokens": [51686, 819, 4190, 321, 362, 13, 407, 321, 603, 5055, 300, 294, 13, 400, 321, 393, 536, 321, 362, 51842], "temperature": 0.0, "avg_logprob": -0.1231565946414147, "compression_ratio": 1.9478827361563518, "no_speech_prob": 0.12247920781373978}, {"id": 1581, "seek": 569192, "start": 5691.96, "end": 5696.52, "text": " Southampton cannot pronounce that and then the other cities and unknown. And", "tokens": [50366, 4242, 335, 21987, 2644, 19567, 300, 293, 550, 264, 661, 6486, 293, 9841, 13, 400, 50594], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1582, "seek": 569192, "start": 5696.52, "end": 5699.36, "text": " that is kind of how we get the unique value. So that's what that method is", "tokens": [50594, 300, 307, 733, 295, 577, 321, 483, 264, 3845, 2158, 13, 407, 300, 311, 437, 300, 3170, 307, 50736], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1583, "seek": 569192, "start": 5699.36, "end": 5702.16, "text": " doing there. Let's actually delete this code block because we don't need", "tokens": [50736, 884, 456, 13, 961, 311, 767, 12097, 341, 3089, 3461, 570, 321, 500, 380, 643, 50876], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1584, "seek": 569192, "start": 5702.16, "end": 5706.2, "text": " anymore. Alright, so that's what we do. And then what we do down here is we say", "tokens": [50876, 3602, 13, 2798, 11, 370, 300, 311, 437, 321, 360, 13, 400, 550, 437, 321, 360, 760, 510, 307, 321, 584, 51078], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1585, "seek": 569192, "start": 5706.2, "end": 5710.6, "text": " feature columns dot append. So just add to this list, the TensorFlow feature", "tokens": [51078, 4111, 13766, 5893, 34116, 13, 407, 445, 909, 281, 341, 1329, 11, 264, 37624, 4111, 51298], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1586, "seek": 569192, "start": 5710.6, "end": 5715.12, "text": " column dot categorical column with vocabulary list. Now I know this is a", "tokens": [51298, 7738, 5893, 19250, 804, 7738, 365, 19864, 1329, 13, 823, 286, 458, 341, 307, 257, 51524], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1587, "seek": 569192, "start": 5715.12, "end": 5717.76, "text": " mouthful, but this is kind of something again, you're just going to look up", "tokens": [51524, 4525, 906, 11, 457, 341, 307, 733, 295, 746, 797, 11, 291, 434, 445, 516, 281, 574, 493, 51656], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1588, "seek": 569192, "start": 5717.76, "end": 5720.4800000000005, "text": " when you need to use it, right? So understand you need to make feature", "tokens": [51656, 562, 291, 643, 281, 764, 309, 11, 558, 30, 407, 1223, 291, 643, 281, 652, 4111, 51792], "temperature": 0.0, "avg_logprob": -0.12256209266106814, "compression_ratio": 1.7886904761904763, "no_speech_prob": 0.0018100274028256536}, {"id": 1589, "seek": 572048, "start": 5720.48, "end": 5723.599999999999, "text": " columns for linear regression, you don't really need to completely understand", "tokens": [50364, 13766, 337, 8213, 24590, 11, 291, 500, 380, 534, 643, 281, 2584, 1223, 50520], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1590, "seek": 572048, "start": 5723.599999999999, "end": 5726.36, "text": " how, but you just need to know that that's something you need to do. And then", "tokens": [50520, 577, 11, 457, 291, 445, 643, 281, 458, 300, 300, 311, 746, 291, 643, 281, 360, 13, 400, 550, 50658], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1591, "seek": 572048, "start": 5726.36, "end": 5729.959999999999, "text": " you can look up the syntax and understand. So this is what this does. This is", "tokens": [50658, 291, 393, 574, 493, 264, 28431, 293, 1223, 13, 407, 341, 307, 437, 341, 775, 13, 639, 307, 50838], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1592, "seek": 572048, "start": 5729.959999999999, "end": 5734.2, "text": " actually going to create for us a column, it's going to be in the form of a", "tokens": [50838, 767, 516, 281, 1884, 337, 505, 257, 7738, 11, 309, 311, 516, 281, 312, 294, 264, 1254, 295, 257, 51050], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1593, "seek": 572048, "start": 5734.2, "end": 5738.4, "text": " like NumPy array kind of that has the feature name. So whatever one we've", "tokens": [51050, 411, 22592, 47, 88, 10225, 733, 295, 300, 575, 264, 4111, 1315, 13, 407, 2035, 472, 321, 600, 51260], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1594, "seek": 572048, "start": 5738.4, "end": 5742.759999999999, "text": " looped through, and then all the different vocabulary associated with it. Now", "tokens": [51260, 6367, 292, 807, 11, 293, 550, 439, 264, 819, 19864, 6615, 365, 309, 13, 823, 51478], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1595, "seek": 572048, "start": 5742.759999999999, "end": 5746.799999999999, "text": " we need this because we just need to create this column so that we can create", "tokens": [51478, 321, 643, 341, 570, 321, 445, 643, 281, 1884, 341, 7738, 370, 300, 321, 393, 1884, 51680], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1596, "seek": 572048, "start": 5746.799999999999, "end": 5750.28, "text": " our model using those different columns, if that makes any sense. So our linear", "tokens": [51680, 527, 2316, 1228, 729, 819, 13766, 11, 498, 300, 1669, 604, 2020, 13, 407, 527, 8213, 51854], "temperature": 0.0, "avg_logprob": -0.11617745851215563, "compression_ratio": 1.831360946745562, "no_speech_prob": 0.00884557981044054}, {"id": 1597, "seek": 575028, "start": 5750.28, "end": 5753.08, "text": " model needs to have, you know, all the different columns we're going to use, it", "tokens": [50364, 2316, 2203, 281, 362, 11, 291, 458, 11, 439, 264, 819, 13766, 321, 434, 516, 281, 764, 11, 309, 50504], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1598, "seek": 575028, "start": 5753.08, "end": 5756.4, "text": " needs to know all of the different entries that could be in that column, and", "tokens": [50504, 2203, 281, 458, 439, 295, 264, 819, 23041, 300, 727, 312, 294, 300, 7738, 11, 293, 50670], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1599, "seek": 575028, "start": 5756.4, "end": 5760.28, "text": " needs to know whether this is a categorical column or a numeric column. In", "tokens": [50670, 2203, 281, 458, 1968, 341, 307, 257, 19250, 804, 7738, 420, 257, 7866, 299, 7738, 13, 682, 50864], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1600, "seek": 575028, "start": 5760.28, "end": 5763.32, "text": " previous examples, what we might have done is actually change the data set", "tokens": [50864, 3894, 5110, 11, 437, 321, 1062, 362, 1096, 307, 767, 1319, 264, 1412, 992, 51016], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1601, "seek": 575028, "start": 5763.32, "end": 5767.32, "text": " manually, so encoded it manually. TensorFlow just can do this for us now", "tokens": [51016, 16945, 11, 370, 2058, 12340, 309, 16945, 13, 37624, 445, 393, 360, 341, 337, 505, 586, 51216], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1602, "seek": 575028, "start": 5767.32, "end": 5771.44, "text": " in TensorFlow 2.0. So we'll just use that too. Okay, so that's what we did with", "tokens": [51216, 294, 37624, 568, 13, 15, 13, 407, 321, 603, 445, 764, 300, 886, 13, 1033, 11, 370, 300, 311, 437, 321, 630, 365, 51422], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1603, "seek": 575028, "start": 5771.44, "end": 5774.96, "text": " these feature columns. Now for the numeric columns, a little bit different. It's", "tokens": [51422, 613, 4111, 13766, 13, 823, 337, 264, 7866, 299, 13766, 11, 257, 707, 857, 819, 13, 467, 311, 51598], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1604, "seek": 575028, "start": 5774.96, "end": 5777.8, "text": " actually easier. All we need to do is give the feature name and whatever the", "tokens": [51598, 767, 3571, 13, 1057, 321, 643, 281, 360, 307, 976, 264, 4111, 1315, 293, 2035, 264, 51740], "temperature": 0.0, "avg_logprob": -0.11655399298212331, "compression_ratio": 1.86404833836858, "no_speech_prob": 0.0006070490344427526}, {"id": 1605, "seek": 577780, "start": 5777.84, "end": 5781.52, "text": " data type is and create a column with that. So notice we don't, we can omit", "tokens": [50366, 1412, 2010, 307, 293, 1884, 257, 7738, 365, 300, 13, 407, 3449, 321, 500, 380, 11, 321, 393, 3406, 270, 50550], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1606, "seek": 577780, "start": 5781.52, "end": 5784.04, "text": " this unique value, because we know when it's numeric, that you know, there", "tokens": [50550, 341, 3845, 2158, 11, 570, 321, 458, 562, 309, 311, 7866, 299, 11, 300, 291, 458, 11, 456, 50676], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1607, "seek": 577780, "start": 5784.04, "end": 5786.84, "text": " could be an infinite amount of values. And then I've just printed out the", "tokens": [50676, 727, 312, 364, 13785, 2372, 295, 4190, 13, 400, 550, 286, 600, 445, 13567, 484, 264, 50816], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1608, "seek": 577780, "start": 5786.84, "end": 5789.64, "text": " feature columns, you can see what this looks like. So vocabulary list,", "tokens": [50816, 4111, 13766, 11, 291, 393, 536, 437, 341, 1542, 411, 13, 407, 19864, 1329, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1609, "seek": 577780, "start": 5789.64, "end": 5793.28, "text": " categorical column gives us the number of siblings. And then the vocabulary", "tokens": [50956, 19250, 804, 7738, 2709, 505, 264, 1230, 295, 20571, 13, 400, 550, 264, 19864, 51138], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1610, "seek": 577780, "start": 5793.28, "end": 5797.12, "text": " list is these are all the different encoding values that is created. And", "tokens": [51138, 1329, 307, 613, 366, 439, 264, 819, 43430, 4190, 300, 307, 2942, 13, 400, 51330], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1611, "seek": 577780, "start": 5797.12, "end": 5800.400000000001, "text": " then same thing, you know, we can go down here, parts, these are different", "tokens": [51330, 550, 912, 551, 11, 291, 458, 11, 321, 393, 352, 760, 510, 11, 3166, 11, 613, 366, 819, 51494], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1612, "seek": 577780, "start": 5800.400000000001, "end": 5803.72, "text": " encodings. So they're not necessarily in order is like what I was talking about", "tokens": [51494, 2058, 378, 1109, 13, 407, 436, 434, 406, 4725, 294, 1668, 307, 411, 437, 286, 390, 1417, 466, 51660], "temperature": 0.0, "avg_logprob": -0.11885453533652603, "compression_ratio": 1.826219512195122, "no_speech_prob": 0.006692410912364721}, {"id": 1613, "seek": 580372, "start": 5803.76, "end": 5810.56, "text": " before. Let's go do a numeric one. What do we have here? Yeah, so for a numeric", "tokens": [50366, 949, 13, 961, 311, 352, 360, 257, 7866, 299, 472, 13, 708, 360, 321, 362, 510, 30, 865, 11, 370, 337, 257, 7866, 299, 50706], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1614, "seek": 580372, "start": 5810.56, "end": 5814.400000000001, "text": " column, just as is the key, that's the shape we're expecting. And this is the", "tokens": [50706, 7738, 11, 445, 382, 307, 264, 2141, 11, 300, 311, 264, 3909, 321, 434, 9650, 13, 400, 341, 307, 264, 50898], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1615, "seek": 580372, "start": 5814.400000000001, "end": 5819.0, "text": " data type. So that is pretty much it. We're actually loading these in. So now", "tokens": [50898, 1412, 2010, 13, 407, 300, 307, 1238, 709, 309, 13, 492, 434, 767, 15114, 613, 294, 13, 407, 586, 51128], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1616, "seek": 580372, "start": 5819.0, "end": 5822.76, "text": " it's almost time to create the model. So what we're going to do to create the", "tokens": [51128, 309, 311, 1920, 565, 281, 1884, 264, 2316, 13, 407, 437, 321, 434, 516, 281, 360, 281, 1884, 264, 51316], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1617, "seek": 580372, "start": 5822.76, "end": 5827.12, "text": " model now is talk about first the training process and training some kind", "tokens": [51316, 2316, 586, 307, 751, 466, 700, 264, 3097, 1399, 293, 3097, 512, 733, 51534], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1618, "seek": 580372, "start": 5827.12, "end": 5831.64, "text": " of, you know, machine learning model. Okay, so the training process. Now the", "tokens": [51534, 295, 11, 291, 458, 11, 3479, 2539, 2316, 13, 1033, 11, 370, 264, 3097, 1399, 13, 823, 264, 51760], "temperature": 0.0, "avg_logprob": -0.08494531826710137, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.00433117151260376}, {"id": 1619, "seek": 583164, "start": 5831.68, "end": 5834.88, "text": " training process of our model is actually fairly simple, at least for a", "tokens": [50366, 3097, 1399, 295, 527, 2316, 307, 767, 6457, 2199, 11, 412, 1935, 337, 257, 50526], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1620, "seek": 583164, "start": 5834.88, "end": 5839.400000000001, "text": " linear model. Now, the way that we train the model is we feed it information,", "tokens": [50526, 8213, 2316, 13, 823, 11, 264, 636, 300, 321, 3847, 264, 2316, 307, 321, 3154, 309, 1589, 11, 50752], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1621, "seek": 583164, "start": 5839.400000000001, "end": 5844.04, "text": " right? So we feed it that those data points from our data set. But how do we", "tokens": [50752, 558, 30, 407, 321, 3154, 309, 300, 729, 1412, 2793, 490, 527, 1412, 992, 13, 583, 577, 360, 321, 50984], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1622, "seek": 583164, "start": 5844.04, "end": 5846.88, "text": " do that? Right? Like how do we feed that to the model? Do we just give it all at", "tokens": [50984, 360, 300, 30, 1779, 30, 1743, 577, 360, 321, 3154, 300, 281, 264, 2316, 30, 1144, 321, 445, 976, 309, 439, 412, 51126], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1623, "seek": 583164, "start": 5846.88, "end": 5851.200000000001, "text": " once? Well, in our case, we only have 627 rows, which isn't really that much", "tokens": [51126, 1564, 30, 1042, 11, 294, 527, 1389, 11, 321, 787, 362, 1386, 10076, 13241, 11, 597, 1943, 380, 534, 300, 709, 51342], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1624, "seek": 583164, "start": 5851.200000000001, "end": 5854.360000000001, "text": " data, like we can fit that in RAM in our computer, right? But what if we're", "tokens": [51342, 1412, 11, 411, 321, 393, 3318, 300, 294, 14561, 294, 527, 3820, 11, 558, 30, 583, 437, 498, 321, 434, 51500], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1625, "seek": 583164, "start": 5854.360000000001, "end": 5858.68, "text": " training a crazy machine learning model, and we have, you know, 25 terabytes of", "tokens": [51500, 3097, 257, 3219, 3479, 2539, 2316, 11, 293, 321, 362, 11, 291, 458, 11, 3552, 1796, 24538, 295, 51716], "temperature": 0.0, "avg_logprob": -0.09039687465977024, "compression_ratio": 1.7197452229299364, "no_speech_prob": 0.00047284303582273424}, {"id": 1626, "seek": 585868, "start": 5858.68, "end": 5862.240000000001, "text": " data that we need to pass it, we can't load that into RAM, at least I don't", "tokens": [50364, 1412, 300, 321, 643, 281, 1320, 309, 11, 321, 393, 380, 3677, 300, 666, 14561, 11, 412, 1935, 286, 500, 380, 50542], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1627, "seek": 585868, "start": 5862.240000000001, "end": 5865.360000000001, "text": " know any RAM that's that large. So we need to find a way that we can kind of", "tokens": [50542, 458, 604, 14561, 300, 311, 300, 2416, 13, 407, 321, 643, 281, 915, 257, 636, 300, 321, 393, 733, 295, 50698], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1628, "seek": 585868, "start": 5865.360000000001, "end": 5869.72, "text": " load it in what's called batches. So the way that we actually load this model is", "tokens": [50698, 3677, 309, 294, 437, 311, 1219, 15245, 279, 13, 407, 264, 636, 300, 321, 767, 3677, 341, 2316, 307, 50916], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1629, "seek": 585868, "start": 5869.72, "end": 5873.08, "text": " we load it in batches. Now we don't need to understand really kind of how this", "tokens": [50916, 321, 3677, 309, 294, 15245, 279, 13, 823, 321, 500, 380, 643, 281, 1223, 534, 733, 295, 577, 341, 51084], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1630, "seek": 585868, "start": 5873.08, "end": 5878.200000000001, "text": " process works and how batching kind of occurs. What we do is give 32 entries at", "tokens": [51084, 1399, 1985, 293, 577, 15245, 278, 733, 295, 11843, 13, 708, 321, 360, 307, 976, 8858, 23041, 412, 51340], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1631, "seek": 585868, "start": 5878.200000000001, "end": 5882.64, "text": " once to the model. Now the reason we don't just feed one at a time is because", "tokens": [51340, 1564, 281, 264, 2316, 13, 823, 264, 1778, 321, 500, 380, 445, 3154, 472, 412, 257, 565, 307, 570, 51562], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1632, "seek": 585868, "start": 5882.64, "end": 5887.4400000000005, "text": " that's a lot slower, we can load, you know, a small batch size of 32, that can", "tokens": [51562, 300, 311, 257, 688, 14009, 11, 321, 393, 3677, 11, 291, 458, 11, 257, 1359, 15245, 2744, 295, 8858, 11, 300, 393, 51802], "temperature": 0.0, "avg_logprob": -0.08544988136786918, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.008061286993324757}, {"id": 1633, "seek": 588744, "start": 5887.48, "end": 5890.5199999999995, "text": " increase our speed dramatically. And that's kind of a lower level", "tokens": [50366, 3488, 527, 3073, 17548, 13, 400, 300, 311, 733, 295, 257, 3126, 1496, 50518], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1634, "seek": 588744, "start": 5890.5199999999995, "end": 5894.08, "text": " understanding. So I'm not going to go too far into that. Now that we understand, we", "tokens": [50518, 3701, 13, 407, 286, 478, 406, 516, 281, 352, 886, 1400, 666, 300, 13, 823, 300, 321, 1223, 11, 321, 50696], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1635, "seek": 588744, "start": 5894.08, "end": 5897.28, "text": " kind of load it in batches, right? So we don't load it entirely all at once, we", "tokens": [50696, 733, 295, 3677, 309, 294, 15245, 279, 11, 558, 30, 407, 321, 500, 380, 3677, 309, 7696, 439, 412, 1564, 11, 321, 50856], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1636, "seek": 588744, "start": 5897.28, "end": 5902.719999999999, "text": " just load a specific set of kind of elements as we go. What we have is", "tokens": [50856, 445, 3677, 257, 2685, 992, 295, 733, 295, 4959, 382, 321, 352, 13, 708, 321, 362, 307, 51128], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1637, "seek": 588744, "start": 5902.719999999999, "end": 5908.08, "text": " called epochs. Now what are epochs? Well, epochs are essentially how many times the", "tokens": [51128, 1219, 30992, 28346, 13, 823, 437, 366, 30992, 28346, 30, 1042, 11, 30992, 28346, 366, 4476, 577, 867, 1413, 264, 51396], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1638, "seek": 588744, "start": 5908.08, "end": 5912.4, "text": " model is going to see the same data. So we might be the case, right? And when we", "tokens": [51396, 2316, 307, 516, 281, 536, 264, 912, 1412, 13, 407, 321, 1062, 312, 264, 1389, 11, 558, 30, 400, 562, 321, 51612], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1639, "seek": 588744, "start": 5912.4, "end": 5916.16, "text": " pass the data to our model, the first time, it's pretty bad, like it looks at the", "tokens": [51612, 1320, 264, 1412, 281, 527, 2316, 11, 264, 700, 565, 11, 309, 311, 1238, 1578, 11, 411, 309, 1542, 412, 264, 51800], "temperature": 0.0, "avg_logprob": -0.08571819941202799, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.007576279807835817}, {"id": 1640, "seek": 591616, "start": 5916.16, "end": 5919.32, "text": " model creates our line of best fit, but it's not great, it's not working", "tokens": [50364, 2316, 7829, 527, 1622, 295, 1151, 3318, 11, 457, 309, 311, 406, 869, 11, 309, 311, 406, 1364, 50522], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1641, "seek": 591616, "start": 5919.32, "end": 5922.599999999999, "text": " perfectly. So we need to use something called an epoch, which means we're just", "tokens": [50522, 6239, 13, 407, 321, 643, 281, 764, 746, 1219, 364, 30992, 339, 11, 597, 1355, 321, 434, 445, 50686], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1642, "seek": 591616, "start": 5922.599999999999, "end": 5926.5599999999995, "text": " going to feed the model, feed the data again, but in a different order. So we", "tokens": [50686, 516, 281, 3154, 264, 2316, 11, 3154, 264, 1412, 797, 11, 457, 294, 257, 819, 1668, 13, 407, 321, 50884], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1643, "seek": 591616, "start": 5926.5599999999995, "end": 5930.84, "text": " do this multiple times, so that the model will look at a data, look at the data in", "tokens": [50884, 360, 341, 3866, 1413, 11, 370, 300, 264, 2316, 486, 574, 412, 257, 1412, 11, 574, 412, 264, 1412, 294, 51098], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1644, "seek": 591616, "start": 5930.84, "end": 5934.72, "text": " a different way, then kind of a different form, and see the same data a few", "tokens": [51098, 257, 819, 636, 11, 550, 733, 295, 257, 819, 1254, 11, 293, 536, 264, 912, 1412, 257, 1326, 51292], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1645, "seek": 591616, "start": 5934.72, "end": 5938.08, "text": " different times and pick up on patterns. Because the first time it sees a new", "tokens": [51292, 819, 1413, 293, 1888, 493, 322, 8294, 13, 1436, 264, 700, 565, 309, 8194, 257, 777, 51460], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1646, "seek": 591616, "start": 5938.08, "end": 5941.24, "text": " data point, it's probably not going to have a good idea how to make a prediction", "tokens": [51460, 1412, 935, 11, 309, 311, 1391, 406, 516, 281, 362, 257, 665, 1558, 577, 281, 652, 257, 17630, 51618], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1647, "seek": 591616, "start": 5941.24, "end": 5945.599999999999, "text": " for that. So if we can feed it more and more and more, then you know, we can get", "tokens": [51618, 337, 300, 13, 407, 498, 321, 393, 3154, 309, 544, 293, 544, 293, 544, 11, 550, 291, 458, 11, 321, 393, 483, 51836], "temperature": 0.0, "avg_logprob": -0.10782897400998784, "compression_ratio": 1.9146341463414633, "no_speech_prob": 0.009124587289988995}, {"id": 1648, "seek": 594560, "start": 5945.64, "end": 5948.4400000000005, "text": " a better prediction. Now, this is where we talk about something called", "tokens": [50366, 257, 1101, 17630, 13, 823, 11, 341, 307, 689, 321, 751, 466, 746, 1219, 50506], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1649, "seek": 594560, "start": 5948.4800000000005, "end": 5953.72, "text": " overfitting, though, sometimes we can see the data too much, we can pass too much", "tokens": [50508, 670, 69, 2414, 11, 1673, 11, 2171, 321, 393, 536, 264, 1412, 886, 709, 11, 321, 393, 1320, 886, 709, 50770], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1650, "seek": 594560, "start": 5953.72, "end": 5957.64, "text": " data to our model, to the point where it just straight up memorizes those data", "tokens": [50770, 1412, 281, 527, 2316, 11, 281, 264, 935, 689, 309, 445, 2997, 493, 10560, 5660, 729, 1412, 50966], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1651, "seek": 594560, "start": 5957.64, "end": 5961.72, "text": " points. And it's, it's really good at classifying for those data points. But", "tokens": [50966, 2793, 13, 400, 309, 311, 11, 309, 311, 534, 665, 412, 1508, 5489, 337, 729, 1412, 2793, 13, 583, 51170], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1652, "seek": 594560, "start": 5961.72, "end": 5965.52, "text": " when we pass it some new data points, like our testing data, for example, it's", "tokens": [51170, 562, 321, 1320, 309, 512, 777, 1412, 2793, 11, 411, 527, 4997, 1412, 11, 337, 1365, 11, 309, 311, 51360], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1653, "seek": 594560, "start": 5965.52, "end": 5970.120000000001, "text": " horrible at kind of, you know, classifying those. So what we do to kind of", "tokens": [51360, 9263, 412, 733, 295, 11, 291, 458, 11, 1508, 5489, 729, 13, 407, 437, 321, 360, 281, 733, 295, 51590], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1654, "seek": 594560, "start": 5970.120000000001, "end": 5973.4800000000005, "text": " prevent this from happening, is we just make sure that we start with like a", "tokens": [51590, 4871, 341, 490, 2737, 11, 307, 321, 445, 652, 988, 300, 321, 722, 365, 411, 257, 51758], "temperature": 0.0, "avg_logprob": -0.09586035864693777, "compression_ratio": 1.8053691275167785, "no_speech_prob": 0.0021155811846256256}, {"id": 1655, "seek": 597348, "start": 5973.5199999999995, "end": 5976.36, "text": " lower amount of epochs, and then we can work our way up and kind of", "tokens": [50366, 3126, 2372, 295, 30992, 28346, 11, 293, 550, 321, 393, 589, 527, 636, 493, 293, 733, 295, 50508], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1656, "seek": 597348, "start": 5976.36, "end": 5980.44, "text": " incrementally change that if we need to, you know, go higher, right, we need more", "tokens": [50508, 26200, 379, 1319, 300, 498, 321, 643, 281, 11, 291, 458, 11, 352, 2946, 11, 558, 11, 321, 643, 544, 50712], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1657, "seek": 597348, "start": 5980.44, "end": 5984.959999999999, "text": " epochs. So yeah, so that's kind of it for epochs. Now, I will say that this", "tokens": [50712, 30992, 28346, 13, 407, 1338, 11, 370, 300, 311, 733, 295, 309, 337, 30992, 28346, 13, 823, 11, 286, 486, 584, 300, 341, 50938], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1658, "seek": 597348, "start": 5984.959999999999, "end": 5989.0, "text": " training process kind of applies to all the different, what is it, machine learning", "tokens": [50938, 3097, 1399, 733, 295, 13165, 281, 439, 264, 819, 11, 437, 307, 309, 11, 3479, 2539, 51140], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1659, "seek": 597348, "start": 5989.0, "end": 5991.959999999999, "text": " models that we're going to look at, we have epochs, we have batches, we have a", "tokens": [51140, 5245, 300, 321, 434, 516, 281, 574, 412, 11, 321, 362, 30992, 28346, 11, 321, 362, 15245, 279, 11, 321, 362, 257, 51288], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1660, "seek": 597348, "start": 5991.959999999999, "end": 5996.759999999999, "text": " batch size, and now we have something called an input function. Now, this is", "tokens": [51288, 15245, 2744, 11, 293, 586, 321, 362, 746, 1219, 364, 4846, 2445, 13, 823, 11, 341, 307, 51528], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1661, "seek": 597348, "start": 5997.08, "end": 6001.0, "text": " pretty complicated. This is the code for the input function. I don't like that we", "tokens": [51544, 1238, 6179, 13, 639, 307, 264, 3089, 337, 264, 4846, 2445, 13, 286, 500, 380, 411, 300, 321, 51740], "temperature": 0.0, "avg_logprob": -0.09150151758385985, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.04083871468901634}, {"id": 1662, "seek": 600100, "start": 6001.0, "end": 6005.88, "text": " need to do this. But it's necessary. So essentially, what an input function is", "tokens": [50364, 643, 281, 360, 341, 13, 583, 309, 311, 4818, 13, 407, 4476, 11, 437, 364, 4846, 2445, 307, 50608], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1663, "seek": 600100, "start": 6005.92, "end": 6010.32, "text": " is the way that we define how our data is going to be broke into epochs and into", "tokens": [50610, 307, 264, 636, 300, 321, 6964, 577, 527, 1412, 307, 516, 281, 312, 6902, 666, 30992, 28346, 293, 666, 50830], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1664, "seek": 600100, "start": 6010.32, "end": 6014.72, "text": " batches to feed to our model. Now, these, you probably aren't ever going to", "tokens": [50830, 15245, 279, 281, 3154, 281, 527, 2316, 13, 823, 11, 613, 11, 291, 1391, 3212, 380, 1562, 516, 281, 51050], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1665, "seek": 600100, "start": 6014.72, "end": 6018.72, "text": " really need to code like from scratch by yourself. But this is the one I've just", "tokens": [51050, 534, 643, 281, 3089, 411, 490, 8459, 538, 1803, 13, 583, 341, 307, 264, 472, 286, 600, 445, 51250], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1666, "seek": 600100, "start": 6018.72, "end": 6021.36, "text": " stolen from the TensorFlow website, pretty much like everything else that's", "tokens": [51250, 15900, 490, 264, 37624, 3144, 11, 1238, 709, 411, 1203, 1646, 300, 311, 51382], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1667, "seek": 600100, "start": 6021.36, "end": 6028.28, "text": " in the series. And what this does is it takes our data and encodes it in a tf", "tokens": [51382, 294, 264, 2638, 13, 400, 437, 341, 775, 307, 309, 2516, 527, 1412, 293, 2058, 4789, 309, 294, 257, 256, 69, 51728], "temperature": 0.0, "avg_logprob": -0.10854550658679399, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.008846520446240902}, {"id": 1668, "seek": 602828, "start": 6028.32, "end": 6034.08, "text": " dot data dot data set object. Now, this is because our model needs this specific", "tokens": [50366, 5893, 1412, 5893, 1412, 992, 2657, 13, 823, 11, 341, 307, 570, 527, 2316, 2203, 341, 2685, 50654], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1669, "seek": 602828, "start": 6034.12, "end": 6038.04, "text": " object to be able to work, it needs to see a data set object to be able to use", "tokens": [50656, 2657, 281, 312, 1075, 281, 589, 11, 309, 2203, 281, 536, 257, 1412, 992, 2657, 281, 312, 1075, 281, 764, 50852], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1670, "seek": 602828, "start": 6038.04, "end": 6042.32, "text": " that data to create the model. So what we need to do is take this pandas data", "tokens": [50852, 300, 1412, 281, 1884, 264, 2316, 13, 407, 437, 321, 643, 281, 360, 307, 747, 341, 4565, 296, 1412, 51066], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1671, "seek": 602828, "start": 6042.32, "end": 6046.2, "text": " frame, we need to turn it into that object. And the way we do that is with", "tokens": [51066, 3920, 11, 321, 643, 281, 1261, 309, 666, 300, 2657, 13, 400, 264, 636, 321, 360, 300, 307, 365, 51260], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1672, "seek": 602828, "start": 6046.2, "end": 6050.32, "text": " the input function. So we can see that what this is doing here. So this is make", "tokens": [51260, 264, 4846, 2445, 13, 407, 321, 393, 536, 300, 437, 341, 307, 884, 510, 13, 407, 341, 307, 652, 51466], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1673, "seek": 602828, "start": 6050.36, "end": 6054.28, "text": " input function, we actually have a function defined inside of another function. I", "tokens": [51468, 4846, 2445, 11, 321, 767, 362, 257, 2445, 7642, 1854, 295, 1071, 2445, 13, 286, 51664], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1674, "seek": 602828, "start": 6054.28, "end": 6057.639999999999, "text": " know this is kind of complicated for some of you guys. But and what I'm", "tokens": [51664, 458, 341, 307, 733, 295, 6179, 337, 512, 295, 291, 1074, 13, 583, 293, 437, 286, 478, 51832], "temperature": 0.0, "avg_logprob": -0.09244462134132922, "compression_ratio": 1.9361702127659575, "no_speech_prob": 0.043347328901290894}, {"id": 1675, "seek": 605764, "start": 6057.64, "end": 6060.160000000001, "text": " actually gonna do, sorry, I'm gonna just copy this into the other page, because I", "tokens": [50364, 767, 799, 360, 11, 2597, 11, 286, 478, 799, 445, 5055, 341, 666, 264, 661, 3028, 11, 570, 286, 50490], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1676, "seek": 605764, "start": 6060.160000000001, "end": 6063.280000000001, "text": " think it's easier to explain without all the text around. So let's create a new", "tokens": [50490, 519, 309, 311, 3571, 281, 2903, 1553, 439, 264, 2487, 926, 13, 407, 718, 311, 1884, 257, 777, 50646], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1677, "seek": 605764, "start": 6063.280000000001, "end": 6068.360000000001, "text": " code block. Let's paste this in. And let's have a look at what this does. So", "tokens": [50646, 3089, 3461, 13, 961, 311, 9163, 341, 294, 13, 400, 718, 311, 362, 257, 574, 412, 437, 341, 775, 13, 407, 50900], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1678, "seek": 605764, "start": 6068.360000000001, "end": 6073.240000000001, "text": " actually, let me just tab down. Okay, so make input function, we have our", "tokens": [50900, 767, 11, 718, 385, 445, 4421, 760, 13, 1033, 11, 370, 652, 4846, 2445, 11, 321, 362, 527, 51144], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1679, "seek": 605764, "start": 6073.240000000001, "end": 6077.08, "text": " parameters data data frame, which is our pandas data frame, our label data", "tokens": [51144, 9834, 1412, 1412, 3920, 11, 597, 307, 527, 4565, 296, 1412, 3920, 11, 527, 7645, 1412, 51336], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1680, "seek": 605764, "start": 6077.08, "end": 6082.64, "text": " frame, which stands for those labels. So that y train or that eval y eval, right,", "tokens": [51336, 3920, 11, 597, 7382, 337, 729, 16949, 13, 407, 300, 288, 3847, 420, 300, 1073, 304, 288, 1073, 304, 11, 558, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1681, "seek": 605764, "start": 6082.84, "end": 6085.68, "text": " we have a number of epochs, which is how many epochs we're going to do, we set", "tokens": [51624, 321, 362, 257, 1230, 295, 30992, 28346, 11, 597, 307, 577, 867, 30992, 28346, 321, 434, 516, 281, 360, 11, 321, 992, 51766], "temperature": 0.0, "avg_logprob": -0.12224009162501286, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0022516457829624414}, {"id": 1682, "seek": 608568, "start": 6085.72, "end": 6090.76, "text": " the default 10 shuffle, which means are we going to shuffle our data and mix it", "tokens": [50366, 264, 7576, 1266, 39426, 11, 597, 1355, 366, 321, 516, 281, 39426, 527, 1412, 293, 2890, 309, 50618], "temperature": 0.0, "avg_logprob": -0.15323899524046644, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.005554318893700838}, {"id": 1683, "seek": 608568, "start": 6090.76, "end": 6095.08, "text": " up before we pass it to the model, and batch size, which is how many elements", "tokens": [50618, 493, 949, 321, 1320, 309, 281, 264, 2316, 11, 293, 15245, 2744, 11, 597, 307, 577, 867, 4959, 50834], "temperature": 0.0, "avg_logprob": -0.15323899524046644, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.005554318893700838}, {"id": 1684, "seek": 608568, "start": 6095.12, "end": 6100.0, "text": " are we going to give to the data to the model? Well, it's training at once. Now,", "tokens": [50836, 366, 321, 516, 281, 976, 281, 264, 1412, 281, 264, 2316, 30, 1042, 11, 309, 311, 3097, 412, 1564, 13, 823, 11, 51080], "temperature": 0.0, "avg_logprob": -0.15323899524046644, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.005554318893700838}, {"id": 1685, "seek": 608568, "start": 6100.0, "end": 6104.72, "text": " what this does is we have an input function defined inside of this function. And", "tokens": [51080, 437, 341, 775, 307, 321, 362, 364, 4846, 2445, 7642, 1854, 295, 341, 2445, 13, 400, 51316], "temperature": 0.0, "avg_logprob": -0.15323899524046644, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.005554318893700838}, {"id": 1686, "seek": 608568, "start": 6104.72, "end": 6111.12, "text": " we say data set equals tensor frame dot data dot data set from tensor slices,", "tokens": [51316, 321, 584, 1412, 992, 6915, 40863, 3920, 5893, 1412, 5893, 1412, 992, 490, 40863, 19793, 11, 51636], "temperature": 0.0, "avg_logprob": -0.15323899524046644, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.005554318893700838}, {"id": 1687, "seek": 611112, "start": 6111.36, "end": 6115.32, "text": " dict data frame, a label data frame. Now, what this does, and we can read the", "tokens": [50376, 12569, 1412, 3920, 11, 257, 7645, 1412, 3920, 13, 823, 11, 437, 341, 775, 11, 293, 321, 393, 1401, 264, 50574], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1688, "seek": 611112, "start": 6115.32, "end": 6119.24, "text": " comment, I mean, create a tf dot data dot data set object with the data and", "tokens": [50574, 2871, 11, 286, 914, 11, 1884, 257, 256, 69, 5893, 1412, 5893, 1412, 992, 2657, 365, 264, 1412, 293, 50770], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1689, "seek": 611112, "start": 6119.24, "end": 6123.36, "text": " its label. Now, I can't explain to you like how this works on a lower level. But", "tokens": [50770, 1080, 7645, 13, 823, 11, 286, 393, 380, 2903, 281, 291, 411, 577, 341, 1985, 322, 257, 3126, 1496, 13, 583, 50976], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1690, "seek": 611112, "start": 6123.36, "end": 6127.84, "text": " essentially, we pass a dictionary representation of our data frame, which is", "tokens": [50976, 4476, 11, 321, 1320, 257, 25890, 10290, 295, 527, 1412, 3920, 11, 597, 307, 51200], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1691, "seek": 611112, "start": 6127.84, "end": 6131.84, "text": " whatever we passed in here. And then we pass the label data frame, which is", "tokens": [51200, 2035, 321, 4678, 294, 510, 13, 400, 550, 321, 1320, 264, 7645, 1412, 3920, 11, 597, 307, 51400], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1692, "seek": 611112, "start": 6131.84, "end": 6136.12, "text": " going to be, you know, all those y values. And we create this object. And", "tokens": [51400, 516, 281, 312, 11, 291, 458, 11, 439, 729, 288, 4190, 13, 400, 321, 1884, 341, 2657, 13, 400, 51614], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1693, "seek": 611112, "start": 6136.12, "end": 6140.5599999999995, "text": " that's what this line of code does. So tf data dot data set from tensor slices,", "tokens": [51614, 300, 311, 437, 341, 1622, 295, 3089, 775, 13, 407, 256, 69, 1412, 5893, 1412, 992, 490, 40863, 19793, 11, 51836], "temperature": 0.0, "avg_logprob": -0.13709295612491973, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.04207843542098999}, {"id": 1694, "seek": 614056, "start": 6140.68, "end": 6143.320000000001, "text": " which is just what you're going to use. I mean, we can read this documentation,", "tokens": [50370, 597, 307, 445, 437, 291, 434, 516, 281, 764, 13, 286, 914, 11, 321, 393, 1401, 341, 14333, 11, 50502], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1695, "seek": 614056, "start": 6143.320000000001, "end": 6147.320000000001, "text": " create a data set whose elements are slices of the given tensors, the given", "tokens": [50502, 1884, 257, 1412, 992, 6104, 4959, 366, 19793, 295, 264, 2212, 10688, 830, 11, 264, 2212, 50702], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1696, "seek": 614056, "start": 6147.320000000001, "end": 6151.0, "text": " tensors are sliced along their first dimension, this operation preserves the", "tokens": [50702, 10688, 830, 366, 27098, 2051, 641, 700, 10139, 11, 341, 6916, 1183, 9054, 264, 50886], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1697, "seek": 614056, "start": 6151.0, "end": 6154.6, "text": " structure of the input tensors, removing the first dimension of each tensor and", "tokens": [50886, 3877, 295, 264, 4846, 10688, 830, 11, 12720, 264, 700, 10139, 295, 1184, 40863, 293, 51066], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1698, "seek": 614056, "start": 6154.6, "end": 6157.76, "text": " using it as the data set dimension. So I mean, you guys can look at that, like", "tokens": [51066, 1228, 309, 382, 264, 1412, 992, 10139, 13, 407, 286, 914, 11, 291, 1074, 393, 574, 412, 300, 11, 411, 51224], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1699, "seek": 614056, "start": 6157.76, "end": 6160.96, "text": " read through the documentation, if you want. But essentially, what it does is", "tokens": [51224, 1401, 807, 264, 14333, 11, 498, 291, 528, 13, 583, 4476, 11, 437, 309, 775, 307, 51384], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1700, "seek": 614056, "start": 6160.96, "end": 6168.120000000001, "text": " create the data set object for us. Now, if shuffle DS equals DS dot shuffle 1000,", "tokens": [51384, 1884, 264, 1412, 992, 2657, 337, 505, 13, 823, 11, 498, 39426, 15816, 6915, 15816, 5893, 39426, 9714, 11, 51742], "temperature": 0.0, "avg_logprob": -0.11483312536168981, "compression_ratio": 1.8614864864864864, "no_speech_prob": 0.007119960617274046}, {"id": 1701, "seek": 616812, "start": 6168.28, "end": 6170.88, "text": " what this does is just shuffle the data set, you don't really need to", "tokens": [50372, 437, 341, 775, 307, 445, 39426, 264, 1412, 992, 11, 291, 500, 380, 534, 643, 281, 50502], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1702, "seek": 616812, "start": 6170.88, "end": 6175.04, "text": " understand more than that. And then what we do is we say data set equals data set", "tokens": [50502, 1223, 544, 813, 300, 13, 400, 550, 437, 321, 360, 307, 321, 584, 1412, 992, 6915, 1412, 992, 50710], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1703, "seek": 616812, "start": 6175.04, "end": 6179.24, "text": " dot batch, the batch size, which is going to be 32, and then repeat for the", "tokens": [50710, 5893, 15245, 11, 264, 15245, 2744, 11, 597, 307, 516, 281, 312, 8858, 11, 293, 550, 7149, 337, 264, 50920], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1704, "seek": 616812, "start": 6179.24, "end": 6182.76, "text": " number of epochs. So what this is going to do is essentially take our data set", "tokens": [50920, 1230, 295, 30992, 28346, 13, 407, 437, 341, 307, 516, 281, 360, 307, 4476, 747, 527, 1412, 992, 51096], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1705, "seek": 616812, "start": 6182.76, "end": 6188.28, "text": " and split it into the number of, I don't want to, what do I want to call it? Like", "tokens": [51096, 293, 7472, 309, 666, 264, 1230, 295, 11, 286, 500, 380, 528, 281, 11, 437, 360, 286, 528, 281, 818, 309, 30, 1743, 51372], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1706, "seek": 616812, "start": 6188.28, "end": 6192.48, "text": " blocks that are going to be passed to our model. So we can do this by knowing", "tokens": [51372, 8474, 300, 366, 516, 281, 312, 4678, 281, 527, 2316, 13, 407, 321, 393, 360, 341, 538, 5276, 51582], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1707, "seek": 616812, "start": 6192.48, "end": 6195.8, "text": " the batch size, it obviously knows how many elements because that's the data set", "tokens": [51582, 264, 15245, 2744, 11, 309, 2745, 3255, 577, 867, 4959, 570, 300, 311, 264, 1412, 992, 51748], "temperature": 0.0, "avg_logprob": -0.10287384192148845, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.08266576379537582}, {"id": 1708, "seek": 619580, "start": 6195.8, "end": 6200.04, "text": " object itself, and then repeat number of epochs. So this can figure out, you know,", "tokens": [50364, 2657, 2564, 11, 293, 550, 7149, 1230, 295, 30992, 28346, 13, 407, 341, 393, 2573, 484, 11, 291, 458, 11, 50576], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1709, "seek": 619580, "start": 6200.08, "end": 6206.4800000000005, "text": " how many one how many blocks do I need to split it into to feed it to my model. Now", "tokens": [50578, 577, 867, 472, 577, 867, 8474, 360, 286, 643, 281, 7472, 309, 666, 281, 3154, 309, 281, 452, 2316, 13, 823, 50898], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1710, "seek": 619580, "start": 6206.4800000000005, "end": 6210.56, "text": " return data set, simply from this function here, we'll return that data set", "tokens": [50898, 2736, 1412, 992, 11, 2935, 490, 341, 2445, 510, 11, 321, 603, 2736, 300, 1412, 992, 51102], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1711, "seek": 619580, "start": 6210.6, "end": 6214.88, "text": " object. And then on the outside return, we actually return this function. So", "tokens": [51104, 2657, 13, 400, 550, 322, 264, 2380, 2736, 11, 321, 767, 2736, 341, 2445, 13, 407, 51318], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1712, "seek": 619580, "start": 6214.88, "end": 6218.4400000000005, "text": " what this out exterior function does, and I'm really just trying to break this", "tokens": [51318, 437, 341, 484, 20677, 2445, 775, 11, 293, 286, 478, 534, 445, 1382, 281, 1821, 341, 51496], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1713, "seek": 619580, "start": 6218.4400000000005, "end": 6222.84, "text": " down. So you guys understand is make an input function, it literally makes a", "tokens": [51496, 760, 13, 407, 291, 1074, 1223, 307, 652, 364, 4846, 2445, 11, 309, 3736, 1669, 257, 51716], "temperature": 0.0, "avg_logprob": -0.11663416053066734, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.002800706308335066}, {"id": 1714, "seek": 622284, "start": 6222.84, "end": 6228.32, "text": " function and returns the function object to wherever we call it from. So that's", "tokens": [50364, 2445, 293, 11247, 264, 2445, 2657, 281, 8660, 321, 818, 309, 490, 13, 407, 300, 311, 50638], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1715, "seek": 622284, "start": 6228.32, "end": 6232.92, "text": " how that works. Now we have a train input function and an eval input function. And", "tokens": [50638, 577, 300, 1985, 13, 823, 321, 362, 257, 3847, 4846, 2445, 293, 364, 1073, 304, 4846, 2445, 13, 400, 50868], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1716, "seek": 622284, "start": 6232.92, "end": 6236.16, "text": " what we need to do to create these, it's just use this function that we've", "tokens": [50868, 437, 321, 643, 281, 360, 281, 1884, 613, 11, 309, 311, 445, 764, 341, 2445, 300, 321, 600, 51030], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1717, "seek": 622284, "start": 6236.16, "end": 6241.32, "text": " defined above. So we say make input function, df train, y train, so our data", "tokens": [51030, 7642, 3673, 13, 407, 321, 584, 652, 4846, 2445, 11, 274, 69, 3847, 11, 288, 3847, 11, 370, 527, 1412, 51288], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1718, "seek": 622284, "start": 6241.32, "end": 6244.96, "text": " frame for training and our data frame for the labels of that. So we can see the", "tokens": [51288, 3920, 337, 3097, 293, 527, 1412, 3920, 337, 264, 16949, 295, 300, 13, 407, 321, 393, 536, 264, 51470], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1719, "seek": 622284, "start": 6244.96, "end": 6248.32, "text": " comment, you know, here, we will call the input function, right? And then eval", "tokens": [51470, 2871, 11, 291, 458, 11, 510, 11, 321, 486, 818, 264, 4846, 2445, 11, 558, 30, 400, 550, 1073, 304, 51638], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1720, "seek": 622284, "start": 6248.32, "end": 6252.2, "text": " train, so it's going to be the same thing, except for the evaluation, we don't", "tokens": [51638, 3847, 11, 370, 309, 311, 516, 281, 312, 264, 912, 551, 11, 3993, 337, 264, 13344, 11, 321, 500, 380, 51832], "temperature": 0.0, "avg_logprob": -0.11026312380420919, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0078149838373065}, {"id": 1721, "seek": 625220, "start": 6252.24, "end": 6255.679999999999, "text": " need to shuffle the data because we're not training it, we only need one epoch.", "tokens": [50366, 643, 281, 39426, 264, 1412, 570, 321, 434, 406, 3097, 309, 11, 321, 787, 643, 472, 30992, 339, 13, 50538], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1722, "seek": 625220, "start": 6255.72, "end": 6259.72, "text": " Because again, we're just training it. And we'll pass the evaluation data set and", "tokens": [50540, 1436, 797, 11, 321, 434, 445, 3097, 309, 13, 400, 321, 603, 1320, 264, 13344, 1412, 992, 293, 50740], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1723, "seek": 625220, "start": 6259.72, "end": 6265.44, "text": " the evaluation value from Y. Okay, so that's it for making the input function. Now", "tokens": [50740, 264, 13344, 2158, 490, 398, 13, 1033, 11, 370, 300, 311, 309, 337, 1455, 264, 4846, 2445, 13, 823, 51026], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1724, "seek": 625220, "start": 6265.44, "end": 6269.599999999999, "text": " I know this is complicated, but that's the way we have to do it. And unfortunately,", "tokens": [51026, 286, 458, 341, 307, 6179, 11, 457, 300, 311, 264, 636, 321, 362, 281, 360, 309, 13, 400, 7015, 11, 51234], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1725, "seek": 625220, "start": 6269.599999999999, "end": 6272.5199999999995, "text": " if you don't understand after that, there's not much more I can do, you might", "tokens": [51234, 498, 291, 500, 380, 1223, 934, 300, 11, 456, 311, 406, 709, 544, 286, 393, 360, 11, 291, 1062, 51380], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1726, "seek": 625220, "start": 6272.5199999999995, "end": 6276.32, "text": " just have to read through some of the documentation. Alright, creating the", "tokens": [51380, 445, 362, 281, 1401, 807, 512, 295, 264, 14333, 13, 2798, 11, 4084, 264, 51570], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1727, "seek": 625220, "start": 6276.32, "end": 6279.24, "text": " model, we are finally here, I know this has been a while, but I need to get", "tokens": [51570, 2316, 11, 321, 366, 2721, 510, 11, 286, 458, 341, 575, 668, 257, 1339, 11, 457, 286, 643, 281, 483, 51716], "temperature": 0.0, "avg_logprob": -0.11253620016163793, "compression_ratio": 1.7626582278481013, "no_speech_prob": 0.0069023799151182175}, {"id": 1728, "seek": 627924, "start": 6279.28, "end": 6282.36, "text": " through everything. So linear estimate, so we're going to copy this, and I'm just", "tokens": [50366, 807, 1203, 13, 407, 8213, 12539, 11, 370, 321, 434, 516, 281, 5055, 341, 11, 293, 286, 478, 445, 50520], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1729, "seek": 627924, "start": 6282.36, "end": 6286.2, "text": " going to put it in here and we'll talk about what this does. So linear underscore", "tokens": [50520, 516, 281, 829, 309, 294, 510, 293, 321, 603, 751, 466, 437, 341, 775, 13, 407, 8213, 37556, 50712], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1730, "seek": 627924, "start": 6286.2, "end": 6292.5199999999995, "text": " EST equals tf dot estimator dot linear classifier, and we're giving it the feature", "tokens": [50712, 47140, 6915, 256, 69, 5893, 8017, 1639, 5893, 8213, 1508, 9902, 11, 293, 321, 434, 2902, 309, 264, 4111, 51028], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1731, "seek": 627924, "start": 6292.5199999999995, "end": 6295.639999999999, "text": " columns that we created up here. So this work was not for nothing, we have this", "tokens": [51028, 13766, 300, 321, 2942, 493, 510, 13, 407, 341, 589, 390, 406, 337, 1825, 11, 321, 362, 341, 51184], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1732, "seek": 627924, "start": 6295.639999999999, "end": 6300.36, "text": " feature column, which defines, you know, what is in every single, like what should", "tokens": [51184, 4111, 7738, 11, 597, 23122, 11, 291, 458, 11, 437, 307, 294, 633, 2167, 11, 411, 437, 820, 51420], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1733, "seek": 627924, "start": 6300.36, "end": 6305.4, "text": " we expect for our input data, we pass that to a linear classifier object from", "tokens": [51420, 321, 2066, 337, 527, 4846, 1412, 11, 321, 1320, 300, 281, 257, 8213, 1508, 9902, 2657, 490, 51672], "temperature": 0.0, "avg_logprob": -0.13916273039530933, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.024416685104370117}, {"id": 1734, "seek": 630540, "start": 6305.4, "end": 6310.32, "text": " the estimator module from TensorFlow. And then that creates the model for us. Now,", "tokens": [50364, 264, 8017, 1639, 10088, 490, 37624, 13, 400, 550, 300, 7829, 264, 2316, 337, 505, 13, 823, 11, 50610], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1735, "seek": 630540, "start": 6310.44, "end": 6313.16, "text": " this again is syntax that you don't need to memorize, you just need to understand", "tokens": [50616, 341, 797, 307, 28431, 300, 291, 500, 380, 643, 281, 27478, 11, 291, 445, 643, 281, 1223, 50752], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1736, "seek": 630540, "start": 6313.16, "end": 6316.799999999999, "text": " how it works. What we're doing is creating an estimator, all of these kind of core", "tokens": [50752, 577, 309, 1985, 13, 708, 321, 434, 884, 307, 4084, 364, 8017, 1639, 11, 439, 295, 613, 733, 295, 4965, 50934], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1737, "seek": 630540, "start": 6316.799999999999, "end": 6320.48, "text": " learning algorithms use what's called estimators, which are just basic", "tokens": [50934, 2539, 14642, 764, 437, 311, 1219, 8017, 3391, 11, 597, 366, 445, 3875, 51118], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1738, "seek": 630540, "start": 6320.48, "end": 6324.04, "text": " implementations of algorithms and TensorFlow. And again, pass the feature", "tokens": [51118, 4445, 763, 295, 14642, 293, 37624, 13, 400, 797, 11, 1320, 264, 4111, 51296], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1739, "seek": 630540, "start": 6324.04, "end": 6328.879999999999, "text": " columns. That's how that works. Alright, so now let's go to training the model. Okay,", "tokens": [51296, 13766, 13, 663, 311, 577, 300, 1985, 13, 2798, 11, 370, 586, 718, 311, 352, 281, 3097, 264, 2316, 13, 1033, 11, 51538], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1740, "seek": 630540, "start": 6328.879999999999, "end": 6331.36, "text": " so I'm just going to copy this again, I know you guys think I'm just copying the", "tokens": [51538, 370, 286, 478, 445, 516, 281, 5055, 341, 797, 11, 286, 458, 291, 1074, 519, 286, 478, 445, 27976, 264, 51662], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1741, "seek": 630540, "start": 6331.36, "end": 6334.36, "text": " code back and forth, but I'm not going to memorize the syntax, I just want to", "tokens": [51662, 3089, 646, 293, 5220, 11, 457, 286, 478, 406, 516, 281, 27478, 264, 28431, 11, 286, 445, 528, 281, 51812], "temperature": 0.0, "avg_logprob": -0.1090433120727539, "compression_ratio": 1.8410404624277457, "no_speech_prob": 0.05338403210043907}, {"id": 1742, "seek": 633436, "start": 6334.36, "end": 6337.48, "text": " explain to you how all this works. And again, you guys will have all this code,", "tokens": [50364, 2903, 281, 291, 577, 439, 341, 1985, 13, 400, 797, 11, 291, 1074, 486, 362, 439, 341, 3089, 11, 50520], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1743, "seek": 633436, "start": 6337.48, "end": 6341.839999999999, "text": " you can mess with it, play with it, and learn on your own that way. So to train", "tokens": [50520, 291, 393, 2082, 365, 309, 11, 862, 365, 309, 11, 293, 1466, 322, 428, 1065, 300, 636, 13, 407, 281, 3847, 50738], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1744, "seek": 633436, "start": 6341.88, "end": 6346.4, "text": " is really easy. All we need to do, I say linear EST dot train, and then just give", "tokens": [50740, 307, 534, 1858, 13, 1057, 321, 643, 281, 360, 11, 286, 584, 8213, 47140, 5893, 3847, 11, 293, 550, 445, 976, 50966], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1745, "seek": 633436, "start": 6346.4, "end": 6350.4, "text": " that input function. So that input function that we created up here, right,", "tokens": [50966, 300, 4846, 2445, 13, 407, 300, 4846, 2445, 300, 321, 2942, 493, 510, 11, 558, 11, 51166], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1746, "seek": 633436, "start": 6350.4, "end": 6354.92, "text": " which was returned from make input function, like this train input function", "tokens": [51166, 597, 390, 8752, 490, 652, 4846, 2445, 11, 411, 341, 3847, 4846, 2445, 51392], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1747, "seek": 633436, "start": 6354.92, "end": 6359.599999999999, "text": " here is actually equal to a function, it's equal to a function object itself. If I", "tokens": [51392, 510, 307, 767, 2681, 281, 257, 2445, 11, 309, 311, 2681, 281, 257, 2445, 2657, 2564, 13, 759, 286, 51626], "temperature": 0.0, "avg_logprob": -0.09923710086481358, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.05181949585676193}, {"id": 1748, "seek": 635960, "start": 6359.64, "end": 6365.160000000001, "text": " were to call train underscore input function like this, this would actually", "tokens": [50366, 645, 281, 818, 3847, 37556, 4846, 2445, 411, 341, 11, 341, 576, 767, 50642], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1749, "seek": 635960, "start": 6365.200000000001, "end": 6369.120000000001, "text": " call this function. That's how this works in Python. It's a little bit of a", "tokens": [50644, 818, 341, 2445, 13, 663, 311, 577, 341, 1985, 294, 15329, 13, 467, 311, 257, 707, 857, 295, 257, 50840], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1750, "seek": 635960, "start": 6369.120000000001, "end": 6373.92, "text": " complicated syntax, but that's how it works. We pass that function here. And then", "tokens": [50840, 6179, 28431, 11, 457, 300, 311, 577, 309, 1985, 13, 492, 1320, 300, 2445, 510, 13, 400, 550, 51080], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1751, "seek": 635960, "start": 6373.92, "end": 6377.68, "text": " this will use the function to grab all of the input that we need and train the", "tokens": [51080, 341, 486, 764, 264, 2445, 281, 4444, 439, 295, 264, 4846, 300, 321, 643, 293, 3847, 264, 51268], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1752, "seek": 635960, "start": 6377.68, "end": 6381.88, "text": " model. Now the result is going to be rather than trained, we're going to", "tokens": [51268, 2316, 13, 823, 264, 1874, 307, 516, 281, 312, 2831, 813, 8895, 11, 321, 434, 516, 281, 51478], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1753, "seek": 635960, "start": 6381.88, "end": 6385.240000000001, "text": " evaluate right and notice that we didn't store this one in a variable, but we're", "tokens": [51478, 13059, 558, 293, 3449, 300, 321, 994, 380, 3531, 341, 472, 294, 257, 7006, 11, 457, 321, 434, 51646], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1754, "seek": 635960, "start": 6385.240000000001, "end": 6389.360000000001, "text": " storing the result in a variable so that we can look at it. Now clear output is", "tokens": [51646, 26085, 264, 1874, 294, 257, 7006, 370, 300, 321, 393, 574, 412, 309, 13, 823, 1850, 5598, 307, 51852], "temperature": 0.0, "avg_logprob": -0.10632090708788704, "compression_ratio": 1.8508474576271186, "no_speech_prob": 0.03845948353409767}, {"id": 1755, "seek": 638936, "start": 6389.4, "end": 6392.5199999999995, "text": " just from what we import above just going to clear the console output, because", "tokens": [50366, 445, 490, 437, 321, 974, 3673, 445, 516, 281, 1850, 264, 11076, 5598, 11, 570, 50522], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1756, "seek": 638936, "start": 6392.5199999999995, "end": 6395.88, "text": " there will be some output while we're training. And then we can present print", "tokens": [50522, 456, 486, 312, 512, 5598, 1339, 321, 434, 3097, 13, 400, 550, 321, 393, 1974, 4482, 50690], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1757, "seek": 638936, "start": 6395.92, "end": 6400.44, "text": " the accuracy of this model. So let's actually run this and see how this", "tokens": [50692, 264, 14170, 295, 341, 2316, 13, 407, 718, 311, 767, 1190, 341, 293, 536, 577, 341, 50918], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1758, "seek": 638936, "start": 6400.44, "end": 6403.639999999999, "text": " works. This will take a second. So I'll be back once this is done. Okay, so we're", "tokens": [50918, 1985, 13, 639, 486, 747, 257, 1150, 13, 407, 286, 603, 312, 646, 1564, 341, 307, 1096, 13, 1033, 11, 370, 321, 434, 51078], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1759, "seek": 638936, "start": 6403.639999999999, "end": 6409.679999999999, "text": " back, and we've got a 73.8% accuracy. So essentially what we've done right is", "tokens": [51078, 646, 11, 293, 321, 600, 658, 257, 28387, 13, 23, 4, 14170, 13, 407, 4476, 437, 321, 600, 1096, 558, 307, 51380], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1760, "seek": 638936, "start": 6409.679999999999, "end": 6412.5599999999995, "text": " we've trained the model, you might have seen a bunch of output while you were", "tokens": [51380, 321, 600, 8895, 264, 2316, 11, 291, 1062, 362, 1612, 257, 3840, 295, 5598, 1339, 291, 645, 51524], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1761, "seek": 638936, "start": 6412.5599999999995, "end": 6416.639999999999, "text": " doing this on your screen. And then we printed out the accuracy after", "tokens": [51524, 884, 341, 322, 428, 2568, 13, 400, 550, 321, 13567, 484, 264, 14170, 934, 51728], "temperature": 0.0, "avg_logprob": -0.11149307610332101, "compression_ratio": 1.768976897689769, "no_speech_prob": 0.002115589566528797}, {"id": 1762, "seek": 641664, "start": 6416.68, "end": 6420.4800000000005, "text": " evaluating the model. This accuracy isn't very good. But for our first shot,", "tokens": [50366, 27479, 264, 2316, 13, 639, 14170, 1943, 380, 588, 665, 13, 583, 337, 527, 700, 3347, 11, 50556], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1763, "seek": 641664, "start": 6420.4800000000005, "end": 6424.68, "text": " this okay, we're going to talk about how to improve this in a second. Okay, so", "tokens": [50556, 341, 1392, 11, 321, 434, 516, 281, 751, 466, 577, 281, 3470, 341, 294, 257, 1150, 13, 1033, 11, 370, 50766], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1764, "seek": 641664, "start": 6424.68, "end": 6428.04, "text": " we've evaluated the data set, we stored that in result. I want to actually look", "tokens": [50766, 321, 600, 25509, 264, 1412, 992, 11, 321, 12187, 300, 294, 1874, 13, 286, 528, 281, 767, 574, 50934], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1765, "seek": 641664, "start": 6428.04, "end": 6432.12, "text": " at what result is, because obviously you can see we've referenced the accuracy", "tokens": [50934, 412, 437, 1874, 307, 11, 570, 2745, 291, 393, 536, 321, 600, 32734, 264, 14170, 51138], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1766, "seek": 641664, "start": 6432.12, "end": 6435.8, "text": " part, like you know, as if this was a Python dictionary. So let's run this one", "tokens": [51138, 644, 11, 411, 291, 458, 11, 382, 498, 341, 390, 257, 15329, 25890, 13, 407, 718, 311, 1190, 341, 472, 51322], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1767, "seek": 641664, "start": 6435.8, "end": 6439.4400000000005, "text": " more time. Oh, this is going to take a second again. So okay, so we printed out", "tokens": [51322, 544, 565, 13, 876, 11, 341, 307, 516, 281, 747, 257, 1150, 797, 13, 407, 1392, 11, 370, 321, 13567, 484, 51504], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1768, "seek": 641664, "start": 6439.4400000000005, "end": 6442.52, "text": " result here, and we can see that we have actually a bunch of different values. So", "tokens": [51504, 1874, 510, 11, 293, 321, 393, 536, 300, 321, 362, 767, 257, 3840, 295, 819, 4190, 13, 407, 51658], "temperature": 0.0, "avg_logprob": -0.1149298174627896, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.01590481773018837}, {"id": 1769, "seek": 644252, "start": 6442.52, "end": 6446.56, "text": " we have accuracy, accuracy baseline, AUC, and all these different kinds of", "tokens": [50364, 321, 362, 14170, 11, 14170, 20518, 11, 7171, 34, 11, 293, 439, 613, 819, 3685, 295, 50566], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1770, "seek": 644252, "start": 6446.56, "end": 6450.240000000001, "text": " statistical values. Now, these aren't really going to mean much to you guys, but", "tokens": [50566, 22820, 4190, 13, 823, 11, 613, 3212, 380, 534, 516, 281, 914, 709, 281, 291, 1074, 11, 457, 50750], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1771, "seek": 644252, "start": 6450.240000000001, "end": 6453.360000000001, "text": " I just want to show you that we do have those statistics and to access any", "tokens": [50750, 286, 445, 528, 281, 855, 291, 300, 321, 360, 362, 729, 12523, 293, 281, 2105, 604, 50906], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1772, "seek": 644252, "start": 6453.360000000001, "end": 6456.4800000000005, "text": " specific one, this is really just a dictionary object. So we can just", "tokens": [50906, 2685, 472, 11, 341, 307, 534, 445, 257, 25890, 2657, 13, 407, 321, 393, 445, 51062], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1773, "seek": 644252, "start": 6456.4800000000005, "end": 6460.52, "text": " reference the key that we want, which is what we did with accuracy. Now, notice", "tokens": [51062, 6408, 264, 2141, 300, 321, 528, 11, 597, 307, 437, 321, 630, 365, 14170, 13, 823, 11, 3449, 51264], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1774, "seek": 644252, "start": 6460.76, "end": 6465.52, "text": " our accuracy actually changed here. We went to 76. The reason for this is like", "tokens": [51276, 527, 14170, 767, 3105, 510, 13, 492, 1437, 281, 24733, 13, 440, 1778, 337, 341, 307, 411, 51514], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1775, "seek": 644252, "start": 6465.52, "end": 6468.68, "text": " I said, you know, our data is getting shuffled, it's getting put in a different", "tokens": [51514, 286, 848, 11, 291, 458, 11, 527, 1412, 307, 1242, 402, 33974, 11, 309, 311, 1242, 829, 294, 257, 819, 51672], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1776, "seek": 644252, "start": 6468.68, "end": 6472.400000000001, "text": " order. And based on the order in which we see data, our model will, you know,", "tokens": [51672, 1668, 13, 400, 2361, 322, 264, 1668, 294, 597, 321, 536, 1412, 11, 527, 2316, 486, 11, 291, 458, 11, 51858], "temperature": 0.0, "avg_logprob": -0.11744982079614567, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004468011669814587}, {"id": 1777, "seek": 647240, "start": 6472.44, "end": 6476.36, "text": " make different predictions and be trained differently. So if we had, you know,", "tokens": [50366, 652, 819, 21264, 293, 312, 8895, 7614, 13, 407, 498, 321, 632, 11, 291, 458, 11, 50562], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1778, "seek": 647240, "start": 6476.719999999999, "end": 6481.36, "text": " another epoch, right, if I change epochs to say 11 or 15, our accuracy will", "tokens": [50580, 1071, 30992, 339, 11, 558, 11, 498, 286, 1319, 30992, 28346, 281, 584, 2975, 420, 2119, 11, 527, 14170, 486, 50812], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1779, "seek": 647240, "start": 6481.36, "end": 6484.16, "text": " change. Now it might go up, it might go down. That's something we have to play", "tokens": [50812, 1319, 13, 823, 309, 1062, 352, 493, 11, 309, 1062, 352, 760, 13, 663, 311, 746, 321, 362, 281, 862, 50952], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1780, "seek": 647240, "start": 6484.16, "end": 6487.48, "text": " with as you know, our machine, a machine learning developer, right? That's what", "tokens": [50952, 365, 382, 291, 458, 11, 527, 3479, 11, 257, 3479, 2539, 10754, 11, 558, 30, 663, 311, 437, 51118], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1781, "seek": 647240, "start": 6487.48, "end": 6491.12, "text": " your goal is, is to get the most accurate model. Okay, so now it's time to", "tokens": [51118, 428, 3387, 307, 11, 307, 281, 483, 264, 881, 8559, 2316, 13, 1033, 11, 370, 586, 309, 311, 565, 281, 51300], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1782, "seek": 647240, "start": 6491.12, "end": 6494.24, "text": " actually use the model to make predictions. So up until this point, we've", "tokens": [51300, 767, 764, 264, 2316, 281, 652, 21264, 13, 407, 493, 1826, 341, 935, 11, 321, 600, 51456], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1783, "seek": 647240, "start": 6494.24, "end": 6497.599999999999, "text": " just been doing a lot of work to understand how to create the model, you", "tokens": [51456, 445, 668, 884, 257, 688, 295, 589, 281, 1223, 577, 281, 1884, 264, 2316, 11, 291, 51624], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1784, "seek": 647240, "start": 6497.599999999999, "end": 6501.5599999999995, "text": " know, what the model is, how we make an input function, training, testing data, I", "tokens": [51624, 458, 11, 437, 264, 2316, 307, 11, 577, 321, 652, 364, 4846, 2445, 11, 3097, 11, 4997, 1412, 11, 286, 51822], "temperature": 0.0, "avg_logprob": -0.09399700746303652, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.004904449451714754}, {"id": 1785, "seek": 650156, "start": 6501.56, "end": 6506.160000000001, "text": " know a lot, a lot, a lot of stuff. Now to actually use this model and like make", "tokens": [50364, 458, 257, 688, 11, 257, 688, 11, 257, 688, 295, 1507, 13, 823, 281, 767, 764, 341, 2316, 293, 411, 652, 50594], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1786, "seek": 650156, "start": 6506.240000000001, "end": 6510.0, "text": " accurate predictions with it is somewhat difficult, but I'm going to show you", "tokens": [50598, 8559, 21264, 365, 309, 307, 8344, 2252, 11, 457, 286, 478, 516, 281, 855, 291, 50786], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1787, "seek": 650156, "start": 6510.0, "end": 6514.84, "text": " how. So essentially, TensorFlow models are built to make predictions on a lot", "tokens": [50786, 577, 13, 407, 4476, 11, 37624, 5245, 366, 3094, 281, 652, 21264, 322, 257, 688, 51028], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1788, "seek": 650156, "start": 6514.84, "end": 6519.320000000001, "text": " of things at once. They're not great at making predictions on like one piece of", "tokens": [51028, 295, 721, 412, 1564, 13, 814, 434, 406, 869, 412, 1455, 21264, 322, 411, 472, 2522, 295, 51252], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1789, "seek": 650156, "start": 6519.320000000001, "end": 6523.04, "text": " data, you just want like one passenger to make a prediction for, they're much", "tokens": [51252, 1412, 11, 291, 445, 528, 411, 472, 18707, 281, 652, 257, 17630, 337, 11, 436, 434, 709, 51438], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1790, "seek": 650156, "start": 6523.04, "end": 6526.6, "text": " better at working in like large batches of data. Now you can definitely do it", "tokens": [51438, 1101, 412, 1364, 294, 411, 2416, 15245, 279, 295, 1412, 13, 823, 291, 393, 2138, 360, 309, 51616], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1791, "seek": 650156, "start": 6526.6, "end": 6530.160000000001, "text": " with one, but I'm just going to show you how we can make a prediction for every", "tokens": [51616, 365, 472, 11, 457, 286, 478, 445, 516, 281, 855, 291, 577, 321, 393, 652, 257, 17630, 337, 633, 51794], "temperature": 0.0, "avg_logprob": -0.09996448294089659, "compression_ratio": 1.9065743944636677, "no_speech_prob": 0.003593121189624071}, {"id": 1792, "seek": 653016, "start": 6530.2, "end": 6534.599999999999, "text": " single point that's in that evaluation data set. So right now we looked at the", "tokens": [50366, 2167, 935, 300, 311, 294, 300, 13344, 1412, 992, 13, 407, 558, 586, 321, 2956, 412, 264, 50586], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1793, "seek": 653016, "start": 6534.599999999999, "end": 6538.44, "text": " accuracy, and the way we determine the accuracy was by essentially comparing", "tokens": [50586, 14170, 11, 293, 264, 636, 321, 6997, 264, 14170, 390, 538, 4476, 15763, 50778], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1794, "seek": 653016, "start": 6538.599999999999, "end": 6542.84, "text": " the results that the predictions gave from our model versus what the actual", "tokens": [50786, 264, 3542, 300, 264, 21264, 2729, 490, 527, 2316, 5717, 437, 264, 3539, 50998], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1795, "seek": 653016, "start": 6542.84, "end": 6546.4, "text": " results were for every single one of those passengers. And that's how we came", "tokens": [50998, 3542, 645, 337, 633, 2167, 472, 295, 729, 18436, 13, 400, 300, 311, 577, 321, 1361, 51176], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1796, "seek": 653016, "start": 6546.4, "end": 6551.8, "text": " up with an accuracy of 76%. Now if we want to actually check and get predictions", "tokens": [51176, 493, 365, 364, 14170, 295, 24733, 6856, 823, 498, 321, 528, 281, 767, 1520, 293, 483, 21264, 51446], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1797, "seek": 653016, "start": 6551.8, "end": 6555.4, "text": " from the model and see what those actual predictions are, what we can do is use", "tokens": [51446, 490, 264, 2316, 293, 536, 437, 729, 3539, 21264, 366, 11, 437, 321, 393, 360, 307, 764, 51626], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1798, "seek": 653016, "start": 6555.44, "end": 6559.76, "text": " a method called dot predict. So what I'm going to do is I'm going to say, I", "tokens": [51628, 257, 3170, 1219, 5893, 6069, 13, 407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 584, 11, 286, 51844], "temperature": 0.0, "avg_logprob": -0.10068579453688402, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.004467911086976528}, {"id": 1799, "seek": 655976, "start": 6559.76, "end": 6565.24, "text": " guess, results like this equals, and in this case, we're going to do the model", "tokens": [50364, 2041, 11, 3542, 411, 341, 6915, 11, 293, 294, 341, 1389, 11, 321, 434, 516, 281, 360, 264, 2316, 50638], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1800, "seek": 655976, "start": 6565.24, "end": 6571.52, "text": " name, which is linear EST dot predict. And then inside here, what we're going to", "tokens": [50638, 1315, 11, 597, 307, 8213, 47140, 5893, 6069, 13, 400, 550, 1854, 510, 11, 437, 321, 434, 516, 281, 50952], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1801, "seek": 655976, "start": 6571.52, "end": 6576.52, "text": " pass is that input function we use for the evaluation. So just like, you know, we", "tokens": [50952, 1320, 307, 300, 4846, 2445, 321, 764, 337, 264, 13344, 13, 407, 445, 411, 11, 291, 458, 11, 321, 51202], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1802, "seek": 655976, "start": 6576.52, "end": 6580.16, "text": " need to pass an input function to actually train the model, we also need to", "tokens": [51202, 643, 281, 1320, 364, 4846, 2445, 281, 767, 3847, 264, 2316, 11, 321, 611, 643, 281, 51384], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1803, "seek": 655976, "start": 6580.16, "end": 6583.68, "text": " pass an input function to make a prediction. Now this input function could", "tokens": [51384, 1320, 364, 4846, 2445, 281, 652, 257, 17630, 13, 823, 341, 4846, 2445, 727, 51560], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1804, "seek": 655976, "start": 6583.68, "end": 6586.8, "text": " be a little bit different, we can modify this a bit if we wanted to, but to keep", "tokens": [51560, 312, 257, 707, 857, 819, 11, 321, 393, 16927, 341, 257, 857, 498, 321, 1415, 281, 11, 457, 281, 1066, 51716], "temperature": 0.0, "avg_logprob": -0.09824752019456595, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.001410210388712585}, {"id": 1805, "seek": 658680, "start": 6586.8, "end": 6590.28, "text": " things simple, we use the same one for now. So what I'm going to do is just use", "tokens": [50364, 721, 2199, 11, 321, 764, 264, 912, 472, 337, 586, 13, 407, 437, 286, 478, 516, 281, 360, 307, 445, 764, 50538], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1806, "seek": 658680, "start": 6590.28, "end": 6593.360000000001, "text": " this eval input function. So the one we've already created where we did, you", "tokens": [50538, 341, 1073, 304, 4846, 2445, 13, 407, 264, 472, 321, 600, 1217, 2942, 689, 321, 630, 11, 291, 50692], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1807, "seek": 658680, "start": 6593.360000000001, "end": 6597.4800000000005, "text": " know, one epoch, we don't need to shuffle because it's just the evaluation set. So", "tokens": [50692, 458, 11, 472, 30992, 339, 11, 321, 500, 380, 643, 281, 39426, 570, 309, 311, 445, 264, 13344, 992, 13, 407, 50898], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1808, "seek": 658680, "start": 6597.4800000000005, "end": 6601.64, "text": " inside here, we're going to eval input function. Now what we need to do though is", "tokens": [50898, 1854, 510, 11, 321, 434, 516, 281, 1073, 304, 4846, 2445, 13, 823, 437, 321, 643, 281, 360, 1673, 307, 51106], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1809, "seek": 658680, "start": 6601.64, "end": 6605.400000000001, "text": " convert this to a list, just because we're going to loop through it. And I'm", "tokens": [51106, 7620, 341, 281, 257, 1329, 11, 445, 570, 321, 434, 516, 281, 6367, 807, 309, 13, 400, 286, 478, 51294], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1810, "seek": 658680, "start": 6605.400000000001, "end": 6608.4800000000005, "text": " actually going to print out this value so we can see what it is before we go to", "tokens": [51294, 767, 516, 281, 4482, 484, 341, 2158, 370, 321, 393, 536, 437, 309, 307, 949, 321, 352, 281, 51448], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1811, "seek": 658680, "start": 6608.4800000000005, "end": 6613.4400000000005, "text": " the next step. So let's run this and have a look at what we get. Okay, so we get", "tokens": [51448, 264, 958, 1823, 13, 407, 718, 311, 1190, 341, 293, 362, 257, 574, 412, 437, 321, 483, 13, 1033, 11, 370, 321, 483, 51696], "temperature": 0.0, "avg_logprob": -0.08953774892366849, "compression_ratio": 1.8509933774834437, "no_speech_prob": 0.015904158353805542}, {"id": 1812, "seek": 661344, "start": 6613.48, "end": 6618.32, "text": " logistics array, we can see all these different values. So we have, you know,", "tokens": [50366, 27420, 10225, 11, 321, 393, 536, 439, 613, 819, 4190, 13, 407, 321, 362, 11, 291, 458, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1813, "seek": 661344, "start": 6618.32, "end": 6623.839999999999, "text": " this array with this value, we have probabilities, this value. And this is", "tokens": [50608, 341, 10225, 365, 341, 2158, 11, 321, 362, 33783, 11, 341, 2158, 13, 400, 341, 307, 50884], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1814, "seek": 661344, "start": 6623.839999999999, "end": 6626.44, "text": " kind of what we're getting. So we're getting logistic, all classes, like", "tokens": [50884, 733, 295, 437, 321, 434, 1242, 13, 407, 321, 434, 1242, 3565, 3142, 11, 439, 5359, 11, 411, 51014], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1815, "seek": 661344, "start": 6626.44, "end": 6629.719999999999, "text": " there's all this random stuff. What you hopefully should notice, and I know I'm", "tokens": [51014, 456, 311, 439, 341, 4974, 1507, 13, 708, 291, 4696, 820, 3449, 11, 293, 286, 458, 286, 478, 51178], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1816, "seek": 661344, "start": 6629.719999999999, "end": 6633.44, "text": " just like whizzing through is that we have a dictionary that represents the", "tokens": [51178, 445, 411, 315, 8072, 278, 807, 307, 300, 321, 362, 257, 25890, 300, 8855, 264, 51364], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1817, "seek": 661344, "start": 6633.44, "end": 6637.24, "text": " predictions. And I'll see if I can find the end of the dictionary here for every", "tokens": [51364, 21264, 13, 400, 286, 603, 536, 498, 286, 393, 915, 264, 917, 295, 264, 25890, 510, 337, 633, 51554], "temperature": 0.0, "avg_logprob": -0.12052081181452824, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.07583591341972351}, {"id": 1818, "seek": 663724, "start": 6637.24, "end": 6643.8, "text": " single, what is it prediction? So since we've passed, you know, 267 input data", "tokens": [50364, 2167, 11, 437, 307, 309, 17630, 30, 407, 1670, 321, 600, 4678, 11, 291, 458, 11, 7551, 22, 4846, 1412, 50692], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1819, "seek": 663724, "start": 6643.8, "end": 6648.36, "text": " from this, you know, eval input function, what was returned to us is a list of all", "tokens": [50692, 490, 341, 11, 291, 458, 11, 1073, 304, 4846, 2445, 11, 437, 390, 8752, 281, 505, 307, 257, 1329, 295, 439, 50920], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1820, "seek": 663724, "start": 6648.36, "end": 6651.88, "text": " of these different dictionaries that represent each prediction. So what we", "tokens": [50920, 295, 613, 819, 22352, 4889, 300, 2906, 1184, 17630, 13, 407, 437, 321, 51096], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1821, "seek": 663724, "start": 6651.88, "end": 6656.679999999999, "text": " need to do is look at each dictionary so that we can determine what the actual", "tokens": [51096, 643, 281, 360, 307, 574, 412, 1184, 25890, 370, 300, 321, 393, 6997, 437, 264, 3539, 51336], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1822, "seek": 663724, "start": 6656.679999999999, "end": 6662.44, "text": " prediction was. So what I'm going to do is actually just present to result. I'm", "tokens": [51336, 17630, 390, 13, 407, 437, 286, 478, 516, 281, 360, 307, 767, 445, 1974, 281, 1874, 13, 286, 478, 51624], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1823, "seek": 663724, "start": 6662.44, "end": 6665.12, "text": " wondering to result zero, because this is a list. So that should mean we can", "tokens": [51624, 6359, 281, 1874, 4018, 11, 570, 341, 307, 257, 1329, 13, 407, 300, 820, 914, 321, 393, 51758], "temperature": 0.0, "avg_logprob": -0.13807183901468914, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.03963127359747887}, {"id": 1824, "seek": 666512, "start": 6665.12, "end": 6669.2, "text": " index it. So we can actually look at one prediction. Okay, so this is the", "tokens": [50364, 8186, 309, 13, 407, 321, 393, 767, 574, 412, 472, 17630, 13, 1033, 11, 370, 341, 307, 264, 50568], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1825, "seek": 666512, "start": 6669.28, "end": 6673.44, "text": " dictionary of one prediction. So I know this seems like a lot. But this is what", "tokens": [50572, 25890, 295, 472, 17630, 13, 407, 286, 458, 341, 2544, 411, 257, 688, 13, 583, 341, 307, 437, 50780], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1826, "seek": 666512, "start": 6673.44, "end": 6676.84, "text": " we have. This is our prediction. So logistics, we get some array, we have", "tokens": [50780, 321, 362, 13, 639, 307, 527, 17630, 13, 407, 27420, 11, 321, 483, 512, 10225, 11, 321, 362, 50950], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1827, "seek": 666512, "start": 6676.84, "end": 6681.44, "text": " logistic in here in this dictionary, and then we have probabilities. So what I", "tokens": [50950, 3565, 3142, 294, 510, 294, 341, 25890, 11, 293, 550, 321, 362, 33783, 13, 407, 437, 286, 51180], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1828, "seek": 666512, "start": 6681.44, "end": 6686.24, "text": " actually want is probability. Now, since what we ended up having was a", "tokens": [51180, 767, 528, 307, 8482, 13, 823, 11, 1670, 437, 321, 4590, 493, 1419, 390, 257, 51420], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1829, "seek": 666512, "start": 6686.24, "end": 6690.12, "text": " prediction of two classes, right, either zero or one, we're predicting either", "tokens": [51420, 17630, 295, 732, 5359, 11, 558, 11, 2139, 4018, 420, 472, 11, 321, 434, 32884, 2139, 51614], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1830, "seek": 666512, "start": 6690.12, "end": 6693.8, "text": " someone survived, or they didn't survive, or what their percentage should be. We", "tokens": [51614, 1580, 14433, 11, 420, 436, 994, 380, 7867, 11, 420, 437, 641, 9668, 820, 312, 13, 492, 51798], "temperature": 0.0, "avg_logprob": -0.09240591261121962, "compression_ratio": 1.8546712802768166, "no_speech_prob": 0.009707538411021233}, {"id": 1831, "seek": 669380, "start": 6693.8, "end": 6699.04, "text": " can see that the percentage of survival here is actually 96%. And the percentage", "tokens": [50364, 393, 536, 300, 264, 9668, 295, 12559, 510, 307, 767, 24124, 6856, 400, 264, 9668, 50626], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1832, "seek": 669380, "start": 6699.08, "end": 6703.52, "text": " that it thinks that it won't survive is, you know, 3.3%. So if we want to", "tokens": [50628, 300, 309, 7309, 300, 309, 1582, 380, 7867, 307, 11, 291, 458, 11, 805, 13, 18, 6856, 407, 498, 321, 528, 281, 50850], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1833, "seek": 669380, "start": 6703.56, "end": 6709.68, "text": " access this, what we need to do is click do result at some index. So whatever,", "tokens": [50852, 2105, 341, 11, 437, 321, 643, 281, 360, 307, 2052, 360, 1874, 412, 512, 8186, 13, 407, 2035, 11, 51158], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1834, "seek": 669380, "start": 6709.68, "end": 6713.08, "text": " you know, one we want. So we're gonna say result. And then here, we're going to", "tokens": [51158, 291, 458, 11, 472, 321, 528, 13, 407, 321, 434, 799, 584, 1874, 13, 400, 550, 510, 11, 321, 434, 516, 281, 51328], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1835, "seek": 669380, "start": 6713.08, "end": 6716.68, "text": " put probabilities. So I'm just going to print that like that. And then we can", "tokens": [51328, 829, 33783, 13, 407, 286, 478, 445, 516, 281, 4482, 300, 411, 300, 13, 400, 550, 321, 393, 51508], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1836, "seek": 669380, "start": 6716.68, "end": 6720.68, "text": " see the probabilities. So let's run this. And now we see our probabilities are", "tokens": [51508, 536, 264, 33783, 13, 407, 718, 311, 1190, 341, 13, 400, 586, 321, 536, 527, 33783, 366, 51708], "temperature": 0.0, "avg_logprob": -0.10550201596237543, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.0013669657055288553}, {"id": 1837, "seek": 672068, "start": 6720.72, "end": 6726.88, "text": " 96 and 33. Now, if we want the probability of survival, so I think I", "tokens": [50366, 24124, 293, 11816, 13, 823, 11, 498, 321, 528, 264, 8482, 295, 12559, 11, 370, 286, 519, 286, 50674], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1838, "seek": 672068, "start": 6726.88, "end": 6730.64, "text": " actually might have messed this up, I'm pretty sure the survival probability is", "tokens": [50674, 767, 1062, 362, 16507, 341, 493, 11, 286, 478, 1238, 988, 264, 12559, 8482, 307, 50862], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1839, "seek": 672068, "start": 6730.64, "end": 6734.400000000001, "text": " actually the last one, whereas like the non survival is the first one, because", "tokens": [50862, 767, 264, 1036, 472, 11, 9735, 411, 264, 2107, 12559, 307, 264, 700, 472, 11, 570, 51050], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1840, "seek": 672068, "start": 6734.400000000001, "end": 6737.400000000001, "text": " zero means you didn't survive and one means you did survive. So that's my", "tokens": [51050, 4018, 1355, 291, 994, 380, 7867, 293, 472, 1355, 291, 630, 7867, 13, 407, 300, 311, 452, 51200], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1841, "seek": 672068, "start": 6737.400000000001, "end": 6740.96, "text": " bad, I messed that up. So I actually want their chance of survival, I'll index", "tokens": [51200, 1578, 11, 286, 16507, 300, 493, 13, 407, 286, 767, 528, 641, 2931, 295, 12559, 11, 286, 603, 8186, 51378], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1842, "seek": 672068, "start": 6740.96, "end": 6746.72, "text": " one. So if I index one, you see we get 3.3%. But if I wanted their chance of", "tokens": [51378, 472, 13, 407, 498, 286, 8186, 472, 11, 291, 536, 321, 483, 805, 13, 18, 6856, 583, 498, 286, 1415, 641, 2931, 295, 51666], "temperature": 0.0, "avg_logprob": -0.10553003530033299, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.02228306233882904}, {"id": 1843, "seek": 674672, "start": 6746.72, "end": 6750.8, "text": " survival, not surviving, I would index zero. And that makes sense because zero", "tokens": [50364, 12559, 11, 406, 24948, 11, 286, 576, 8186, 4018, 13, 400, 300, 1669, 2020, 570, 4018, 50568], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1844, "seek": 674672, "start": 6750.8, "end": 6754.400000000001, "text": " is you know, what we're looking like zero represents they didn't survive, whereas", "tokens": [50568, 307, 291, 458, 11, 437, 321, 434, 1237, 411, 4018, 8855, 436, 994, 380, 7867, 11, 9735, 50748], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1845, "seek": 674672, "start": 6754.400000000001, "end": 6758.4400000000005, "text": " one represents they did survive. So that's kind of how we do that. So that's", "tokens": [50748, 472, 8855, 436, 630, 7867, 13, 407, 300, 311, 733, 295, 577, 321, 360, 300, 13, 407, 300, 311, 50950], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1846, "seek": 674672, "start": 6758.4400000000005, "end": 6761.96, "text": " how we get them. Now, if we wanted to loop through all of these, we could we", "tokens": [50950, 577, 321, 483, 552, 13, 823, 11, 498, 321, 1415, 281, 6367, 807, 439, 295, 613, 11, 321, 727, 321, 51126], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1847, "seek": 674672, "start": 6761.96, "end": 6765.0, "text": " could loop through every dictionary, we could print every single probability of", "tokens": [51126, 727, 6367, 807, 633, 25890, 11, 321, 727, 4482, 633, 2167, 8482, 295, 51278], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1848, "seek": 674672, "start": 6765.0, "end": 6768.56, "text": " each person. We can also look at that person's stats and then look at their", "tokens": [51278, 1184, 954, 13, 492, 393, 611, 574, 412, 300, 954, 311, 18152, 293, 550, 574, 412, 641, 51456], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1849, "seek": 674672, "start": 6768.56, "end": 6773.0, "text": " probability. So let's see the probability of surviving is in this case, you", "tokens": [51456, 8482, 13, 407, 718, 311, 536, 264, 8482, 295, 24948, 307, 294, 341, 1389, 11, 291, 51678], "temperature": 0.0, "avg_logprob": -0.11798754734779472, "compression_ratio": 1.9225352112676057, "no_speech_prob": 0.031138604506850243}, {"id": 1850, "seek": 677300, "start": 6773.0, "end": 6777.32, "text": " know, 3%, or whatever it was 3.3%. But let's look at the person that we were", "tokens": [50364, 458, 11, 805, 8923, 420, 2035, 309, 390, 805, 13, 18, 6856, 583, 718, 311, 574, 412, 264, 954, 300, 321, 645, 50580], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1851, "seek": 677300, "start": 6777.32, "end": 6781.88, "text": " actually predicting them and see if that makes sense. So if I go eval, or what", "tokens": [50580, 767, 32884, 552, 293, 536, 498, 300, 1669, 2020, 13, 407, 498, 286, 352, 1073, 304, 11, 420, 437, 50808], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1852, "seek": 677300, "start": 6781.88, "end": 6789.24, "text": " was it df eval dot loc, zero, we print that and then we print the result. What", "tokens": [50808, 390, 309, 274, 69, 1073, 304, 5893, 1628, 11, 4018, 11, 321, 4482, 300, 293, 550, 321, 4482, 264, 1874, 13, 708, 51176], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1853, "seek": 677300, "start": 6789.24, "end": 6793.84, "text": " we can see is that for the person who was male and 35 that had no siblings, their", "tokens": [51176, 321, 393, 536, 307, 300, 337, 264, 954, 567, 390, 7133, 293, 6976, 300, 632, 572, 20571, 11, 641, 51406], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1854, "seek": 677300, "start": 6793.84, "end": 6796.68, "text": " fair was this, they're in third class, we don't know what deck they were on and", "tokens": [51406, 3143, 390, 341, 11, 436, 434, 294, 2636, 1508, 11, 321, 500, 380, 458, 437, 9341, 436, 645, 322, 293, 51548], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1855, "seek": 677300, "start": 6796.68, "end": 6801.56, "text": " they were alone. They have a 3.3% chance of survive. Now, if we change this, we", "tokens": [51548, 436, 645, 3312, 13, 814, 362, 257, 805, 13, 18, 4, 2931, 295, 7867, 13, 823, 11, 498, 321, 1319, 341, 11, 321, 51792], "temperature": 0.0, "avg_logprob": -0.13664712151177497, "compression_ratio": 1.7, "no_speech_prob": 0.022975225001573563}, {"id": 1856, "seek": 680156, "start": 6801.6, "end": 6806.0, "text": " could go like to to let's have a look at this second person and see what their", "tokens": [50366, 727, 352, 411, 281, 281, 718, 311, 362, 257, 574, 412, 341, 1150, 954, 293, 536, 437, 641, 50586], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1857, "seek": 680156, "start": 6806.0, "end": 6809.8, "text": " chance of survival is. Okay, so they have a higher percent chance of 38% chance", "tokens": [50586, 2931, 295, 12559, 307, 13, 1033, 11, 370, 436, 362, 257, 2946, 3043, 2931, 295, 12843, 4, 2931, 50776], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1858, "seek": 680156, "start": 6809.84, "end": 6812.84, "text": " they're female, they're a little bit older. So that might be a reason why their", "tokens": [50778, 436, 434, 6556, 11, 436, 434, 257, 707, 857, 4906, 13, 407, 300, 1062, 312, 257, 1778, 983, 641, 50928], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1859, "seek": 680156, "start": 6812.84, "end": 6816.84, "text": " survival rates a bit lower. I mean, we can keep doing this and look through and", "tokens": [50928, 12559, 6846, 257, 857, 3126, 13, 286, 914, 11, 321, 393, 1066, 884, 341, 293, 574, 807, 293, 51128], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1860, "seek": 680156, "start": 6816.84, "end": 6821.88, "text": " see what it is, right? If we want to get the actual value, like if this person", "tokens": [51128, 536, 437, 309, 307, 11, 558, 30, 759, 321, 528, 281, 483, 264, 3539, 2158, 11, 411, 498, 341, 954, 51380], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1861, "seek": 680156, "start": 6821.88, "end": 6828.04, "text": " survived, or if they didn't survive, and what I can do is I can print df eval.", "tokens": [51380, 14433, 11, 420, 498, 436, 994, 380, 7867, 11, 293, 437, 286, 393, 360, 307, 286, 393, 4482, 274, 69, 1073, 304, 13, 51688], "temperature": 0.0, "avg_logprob": -0.12924864842341496, "compression_ratio": 1.75, "no_speech_prob": 0.016401322558522224}, {"id": 1862, "seek": 682804, "start": 6828.72, "end": 6833.16, "text": " Actually, it's not going to be eval, it's going to be y underscore eval. Yeah. And", "tokens": [50398, 5135, 11, 309, 311, 406, 516, 281, 312, 1073, 304, 11, 309, 311, 516, 281, 312, 288, 37556, 1073, 304, 13, 865, 13, 400, 50620], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1863, "seek": 682804, "start": 6833.16, "end": 6838.24, "text": " that's going to be loc three. Now this will give us if they survived or not. So", "tokens": [50620, 300, 311, 516, 281, 312, 1628, 1045, 13, 823, 341, 486, 976, 505, 498, 436, 14433, 420, 406, 13, 407, 50874], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1864, "seek": 682804, "start": 6838.24, "end": 6842.04, "text": " actually, in this case, that person did survive, but we're only predicting a 32%.", "tokens": [50874, 767, 11, 294, 341, 1389, 11, 300, 954, 630, 7867, 11, 457, 321, 434, 787, 32884, 257, 8858, 6856, 51064], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1865, "seek": 682804, "start": 6842.32, "end": 6845.2, "text": " So you can see that that's, you know, represented in the fact that we only have", "tokens": [51078, 407, 291, 393, 536, 300, 300, 311, 11, 291, 458, 11, 10379, 294, 264, 1186, 300, 321, 787, 362, 51222], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1866, "seek": 682804, "start": 6845.2, "end": 6849.72, "text": " about a 76% accuracy, because this model is not perfect. And in this instance, it", "tokens": [51222, 466, 257, 24733, 4, 14170, 11, 570, 341, 2316, 307, 406, 2176, 13, 400, 294, 341, 5197, 11, 309, 51448], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1867, "seek": 682804, "start": 6849.72, "end": 6853.56, "text": " was pretty bad. It's saying they have a 32% chance of surviving, but they actually", "tokens": [51448, 390, 1238, 1578, 13, 467, 311, 1566, 436, 362, 257, 8858, 4, 2931, 295, 24948, 11, 457, 436, 767, 51640], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1868, "seek": 682804, "start": 6853.56, "end": 6856.4, "text": " did survive. So maybe that should be higher, right? So we could change this", "tokens": [51640, 630, 7867, 13, 407, 1310, 300, 820, 312, 2946, 11, 558, 30, 407, 321, 727, 1319, 341, 51782], "temperature": 0.0, "avg_logprob": -0.11282201530107486, "compression_ratio": 1.810897435897436, "no_speech_prob": 0.007344686891883612}, {"id": 1869, "seek": 685640, "start": 6856.4, "end": 6861.5599999999995, "text": " number, go for four, I'm just messing around and showing you guys, you know, how", "tokens": [50364, 1230, 11, 352, 337, 1451, 11, 286, 478, 445, 23258, 926, 293, 4099, 291, 1074, 11, 291, 458, 11, 577, 50622], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1870, "seek": 685640, "start": 6861.5599999999995, "end": 6865.92, "text": " we use this. So in this one, you know, same thing, this person survived, although", "tokens": [50622, 321, 764, 341, 13, 407, 294, 341, 472, 11, 291, 458, 11, 912, 551, 11, 341, 954, 14433, 11, 4878, 50840], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1871, "seek": 685640, "start": 6866.799999999999, "end": 6870.639999999999, "text": " what is it, they only were given a 14% chance of survival. So anyways, that is", "tokens": [50884, 437, 307, 309, 11, 436, 787, 645, 2212, 257, 3499, 4, 2931, 295, 12559, 13, 407, 13448, 11, 300, 307, 51076], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1872, "seek": 685640, "start": 6870.639999999999, "end": 6874.0, "text": " how that works. This is how we actually make predictions and look at the", "tokens": [51076, 577, 300, 1985, 13, 639, 307, 577, 321, 767, 652, 21264, 293, 574, 412, 264, 51244], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1873, "seek": 685640, "start": 6874.0, "end": 6877.44, "text": " predictions, you understand that now what's happening is I've converted this to", "tokens": [51244, 21264, 11, 291, 1223, 300, 586, 437, 311, 2737, 307, 286, 600, 16424, 341, 281, 51416], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1874, "seek": 685640, "start": 6877.44, "end": 6880.639999999999, "text": " a list just because this is actually a generator object, which means it's meant", "tokens": [51416, 257, 1329, 445, 570, 341, 307, 767, 257, 19265, 2657, 11, 597, 1355, 309, 311, 4140, 51576], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1875, "seek": 685640, "start": 6880.639999999999, "end": 6883.5599999999995, "text": " to just be looped through rather than just look at it with a list, but that's", "tokens": [51576, 281, 445, 312, 6367, 292, 807, 2831, 813, 445, 574, 412, 309, 365, 257, 1329, 11, 457, 300, 311, 51722], "temperature": 0.0, "avg_logprob": -0.09698197520371024, "compression_ratio": 1.77491961414791, "no_speech_prob": 0.0012842296855524182}, {"id": 1876, "seek": 688356, "start": 6883.56, "end": 6887.68, "text": " fine, we'll use a list. And then we can just print out, you know, result at", "tokens": [50364, 2489, 11, 321, 603, 764, 257, 1329, 13, 400, 550, 321, 393, 445, 4482, 484, 11, 291, 458, 11, 1874, 412, 50570], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1877, "seek": 688356, "start": 6887.68, "end": 6891.080000000001, "text": " whatever index probabilities, and then one to represent their chance of", "tokens": [50570, 2035, 8186, 33783, 11, 293, 550, 472, 281, 2906, 641, 2931, 295, 50740], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1878, "seek": 688356, "start": 6891.080000000001, "end": 6895.0, "text": " survival. Okay, so that has been it for linear regression. Now let's get into", "tokens": [50740, 12559, 13, 1033, 11, 370, 300, 575, 668, 309, 337, 8213, 24590, 13, 823, 718, 311, 483, 666, 50936], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1879, "seek": 688356, "start": 6895.0, "end": 6899.200000000001, "text": " classification. And now we are on to classification. So essentially,", "tokens": [50936, 21538, 13, 400, 586, 321, 366, 322, 281, 21538, 13, 407, 4476, 11, 51146], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1880, "seek": 688356, "start": 6899.200000000001, "end": 6904.68, "text": " classification is differentiating between, you know, data points and separating", "tokens": [51146, 21538, 307, 27372, 990, 1296, 11, 291, 458, 11, 1412, 2793, 293, 29279, 51420], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1881, "seek": 688356, "start": 6904.68, "end": 6908.400000000001, "text": " them into classes. So rather than predicting a numeric value, which we did", "tokens": [51420, 552, 666, 5359, 13, 407, 2831, 813, 32884, 257, 7866, 299, 2158, 11, 597, 321, 630, 51606], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1882, "seek": 688356, "start": 6908.400000000001, "end": 6911.84, "text": " with regression earlier, so linear regression, and you know, the percentage", "tokens": [51606, 365, 24590, 3071, 11, 370, 8213, 24590, 11, 293, 291, 458, 11, 264, 9668, 51778], "temperature": 0.0, "avg_logprob": -0.10607651191029122, "compression_ratio": 1.8041237113402062, "no_speech_prob": 0.02675546519458294}, {"id": 1883, "seek": 691184, "start": 6911.88, "end": 6916.52, "text": " survival chance, which is a numeric value, we actually want to predict classes. So", "tokens": [50366, 12559, 2931, 11, 597, 307, 257, 7866, 299, 2158, 11, 321, 767, 528, 281, 6069, 5359, 13, 407, 50598], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1884, "seek": 691184, "start": 6916.52, "end": 6921.16, "text": " what we're going to end up doing is predicting the probability that a specific", "tokens": [50598, 437, 321, 434, 516, 281, 917, 493, 884, 307, 32884, 264, 8482, 300, 257, 2685, 50830], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1885, "seek": 691184, "start": 6921.16, "end": 6924.88, "text": " data point or a specific entry or whatever we're going to call it is within", "tokens": [50830, 1412, 935, 420, 257, 2685, 8729, 420, 2035, 321, 434, 516, 281, 818, 309, 307, 1951, 51016], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1886, "seek": 691184, "start": 6924.88, "end": 6927.96, "text": " all of the different classes it could be. So for the example here, we're going to", "tokens": [51016, 439, 295, 264, 819, 5359, 309, 727, 312, 13, 407, 337, 264, 1365, 510, 11, 321, 434, 516, 281, 51170], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1887, "seek": 691184, "start": 6927.96, "end": 6932.56, "text": " use flowers. So it's called the iris. I think it's the iris flower data set or", "tokens": [51170, 764, 8085, 13, 407, 309, 311, 1219, 264, 3418, 271, 13, 286, 519, 309, 311, 264, 3418, 271, 8617, 1412, 992, 420, 51400], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1888, "seek": 691184, "start": 6932.56, "end": 6935.360000000001, "text": " something like that. And we're going to use some different properties of flowers", "tokens": [51400, 746, 411, 300, 13, 400, 321, 434, 516, 281, 764, 512, 819, 7221, 295, 8085, 51540], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1889, "seek": 691184, "start": 6935.360000000001, "end": 6938.4800000000005, "text": " to predict what species of flower it is. So that's the difference between", "tokens": [51540, 281, 6069, 437, 6172, 295, 8617, 309, 307, 13, 407, 300, 311, 264, 2649, 1296, 51696], "temperature": 0.0, "avg_logprob": -0.09252743861254524, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.00154867023229599}, {"id": 1890, "seek": 693848, "start": 6938.48, "end": 6942.0, "text": " classification and regression. Now I'm not going to talk about the specific", "tokens": [50364, 21538, 293, 24590, 13, 823, 286, 478, 406, 516, 281, 751, 466, 264, 2685, 50540], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1891, "seek": 693848, "start": 6942.0, "end": 6945.2, "text": " algorithm we're going to use here for classification, because there's just so", "tokens": [50540, 9284, 321, 434, 516, 281, 764, 510, 337, 21538, 11, 570, 456, 311, 445, 370, 50700], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1892, "seek": 693848, "start": 6945.24, "end": 6950.639999999999, "text": " many different ones you can use. But yeah, I mean, if you really care about how", "tokens": [50702, 867, 819, 2306, 291, 393, 764, 13, 583, 1338, 11, 286, 914, 11, 498, 291, 534, 1127, 466, 577, 50972], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1893, "seek": 693848, "start": 6950.639999999999, "end": 6954.0, "text": " they work on a lower mathematical level, I'm not going to be explaining that", "tokens": [50972, 436, 589, 322, 257, 3126, 18894, 1496, 11, 286, 478, 406, 516, 281, 312, 13468, 300, 51140], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1894, "seek": 693848, "start": 6954.0, "end": 6957.12, "text": " because it doesn't make sense to explain it for one algorithm when there's like", "tokens": [51140, 570, 309, 1177, 380, 652, 2020, 281, 2903, 309, 337, 472, 9284, 562, 456, 311, 411, 51296], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1895, "seek": 693848, "start": 6957.16, "end": 6960.839999999999, "text": " hundreds and they all work a little bit differently. So you guys can kind of look", "tokens": [51298, 6779, 293, 436, 439, 589, 257, 707, 857, 7614, 13, 407, 291, 1074, 393, 733, 295, 574, 51482], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1896, "seek": 693848, "start": 6960.839999999999, "end": 6964.879999999999, "text": " that up. And I'll tell you some resources and where you can find that. I'm also", "tokens": [51482, 300, 493, 13, 400, 286, 603, 980, 291, 512, 3593, 293, 689, 291, 393, 915, 300, 13, 286, 478, 611, 51684], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1897, "seek": 693848, "start": 6964.879999999999, "end": 6967.879999999999, "text": " going to go faster through this example, just because I've already covered kind", "tokens": [51684, 516, 281, 352, 4663, 807, 341, 1365, 11, 445, 570, 286, 600, 1217, 5343, 733, 51834], "temperature": 0.0, "avg_logprob": -0.07412164688110351, "compression_ratio": 1.8213256484149856, "no_speech_prob": 0.05339210852980614}, {"id": 1898, "seek": 696788, "start": 6967.88, "end": 6970.96, "text": " of a lot of the fundamental stuff in linear regression. So hopefully we should", "tokens": [50364, 295, 257, 688, 295, 264, 8088, 1507, 294, 8213, 24590, 13, 407, 4696, 321, 820, 50518], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1899, "seek": 696788, "start": 6970.96, "end": 6974.36, "text": " get this one done a little bit quicker and move on to the next kind of aspects", "tokens": [50518, 483, 341, 472, 1096, 257, 707, 857, 16255, 293, 1286, 322, 281, 264, 958, 733, 295, 7270, 50688], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1900, "seek": 696788, "start": 6974.36, "end": 6978.72, "text": " in this series. Alright, so first steps, load tensor flow, import tensor flow, we've", "tokens": [50688, 294, 341, 2638, 13, 2798, 11, 370, 700, 4439, 11, 3677, 40863, 3095, 11, 974, 40863, 3095, 11, 321, 600, 50906], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1901, "seek": 696788, "start": 6978.72, "end": 6982.4400000000005, "text": " done that already, data set, we need to talk about this. So the data that we're", "tokens": [50906, 1096, 300, 1217, 11, 1412, 992, 11, 321, 643, 281, 751, 466, 341, 13, 407, 264, 1412, 300, 321, 434, 51092], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1902, "seek": 696788, "start": 6982.4400000000005, "end": 6986.56, "text": " using is that iris flowers data set like I talked about. And this specific data set", "tokens": [51092, 1228, 307, 300, 3418, 271, 8085, 1412, 992, 411, 286, 2825, 466, 13, 400, 341, 2685, 1412, 992, 51298], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1903, "seek": 696788, "start": 6986.56, "end": 6989.68, "text": " separates flowers into three different species. So we have these different", "tokens": [51298, 34149, 8085, 666, 1045, 819, 6172, 13, 407, 321, 362, 613, 819, 51454], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1904, "seek": 696788, "start": 6989.68, "end": 6993.56, "text": " species. This is the information we have. So septal length width, petal length,", "tokens": [51454, 6172, 13, 639, 307, 264, 1589, 321, 362, 13, 407, 23891, 304, 4641, 11402, 11, 3817, 304, 4641, 11, 51648], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1905, "seek": 696788, "start": 6993.56, "end": 6996.4800000000005, "text": " petal width, we're going to use that information obviously to make the", "tokens": [51648, 3817, 304, 11402, 11, 321, 434, 516, 281, 764, 300, 1589, 2745, 281, 652, 264, 51794], "temperature": 0.0, "avg_logprob": -0.13075030933726917, "compression_ratio": 1.864306784660767, "no_speech_prob": 0.004754628520458937}, {"id": 1906, "seek": 699648, "start": 6996.48, "end": 7000.839999999999, "text": " predictions. So given this information, you know, in our final model, can it tell", "tokens": [50364, 21264, 13, 407, 2212, 341, 1589, 11, 291, 458, 11, 294, 527, 2572, 2316, 11, 393, 309, 980, 50582], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1907, "seek": 699648, "start": 7000.839999999999, "end": 7005.04, "text": " us which one of these flowers it's most likely to be? Okay, so what we're going to", "tokens": [50582, 505, 597, 472, 295, 613, 8085, 309, 311, 881, 3700, 281, 312, 30, 1033, 11, 370, 437, 321, 434, 516, 281, 50792], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1908, "seek": 699648, "start": 7005.04, "end": 7009.32, "text": " do now is define the CSV column names and species. So the column names are just", "tokens": [50792, 360, 586, 307, 6964, 264, 48814, 7738, 5288, 293, 6172, 13, 407, 264, 7738, 5288, 366, 445, 51006], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1909, "seek": 699648, "start": 7009.32, "end": 7012.639999999999, "text": " going to define what we're going to have in our data set is like the headers for", "tokens": [51006, 516, 281, 6964, 437, 321, 434, 516, 281, 362, 294, 527, 1412, 992, 307, 411, 264, 45101, 337, 51172], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1910, "seek": 699648, "start": 7012.639999999999, "end": 7016.24, "text": " the columns, species obviously is just the species and we'll throw them there.", "tokens": [51172, 264, 13766, 11, 6172, 2745, 307, 445, 264, 6172, 293, 321, 603, 3507, 552, 456, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1911, "seek": 699648, "start": 7017.24, "end": 7020.24, "text": " Alright, so now we're going to load in our data sets. So this is going to be", "tokens": [51402, 2798, 11, 370, 586, 321, 434, 516, 281, 3677, 294, 527, 1412, 6352, 13, 407, 341, 307, 516, 281, 312, 51552], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1912, "seek": 699648, "start": 7020.24, "end": 7023.04, "text": " different every time you're kind of working with models, depending on where", "tokens": [51552, 819, 633, 565, 291, 434, 733, 295, 1364, 365, 5245, 11, 5413, 322, 689, 51692], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1913, "seek": 699648, "start": 7023.04, "end": 7025.44, "text": " you're getting your data from. In our example, we're going to get it from", "tokens": [51692, 291, 434, 1242, 428, 1412, 490, 13, 682, 527, 1365, 11, 321, 434, 516, 281, 483, 309, 490, 51812], "temperature": 0.0, "avg_logprob": -0.1135055661201477, "compression_ratio": 1.912121212121212, "no_speech_prob": 0.0019266129238530993}, {"id": 1914, "seek": 702544, "start": 7025.48, "end": 7030.28, "text": " Keras, which is kind of in sub module of TensorFlow. It has a lot of useful", "tokens": [50366, 591, 6985, 11, 597, 307, 733, 295, 294, 1422, 10088, 295, 37624, 13, 467, 575, 257, 688, 295, 4420, 50606], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1915, "seek": 702544, "start": 7030.28, "end": 7035.04, "text": " data sets and tools that we'll be using throughout the series. But keras.utils.get", "tokens": [50606, 1412, 6352, 293, 3873, 300, 321, 603, 312, 1228, 3710, 264, 2638, 13, 583, 350, 6985, 13, 325, 4174, 13, 847, 50844], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1916, "seek": 702544, "start": 7035.04, "end": 7038.4, "text": " file, again, don't really focus on this, just understand what this is going to", "tokens": [50844, 3991, 11, 797, 11, 500, 380, 534, 1879, 322, 341, 11, 445, 1223, 437, 341, 307, 516, 281, 51012], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1917, "seek": 702544, "start": 7038.4, "end": 7043.36, "text": " do is save this file onto our computer as iris training dot CSV, grab it from", "tokens": [51012, 360, 307, 3155, 341, 3991, 3911, 527, 3820, 382, 3418, 271, 3097, 5893, 48814, 11, 4444, 309, 490, 51260], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1918, "seek": 702544, "start": 7043.36, "end": 7047.12, "text": " this link. And then what we're going to do down here is load the train and test", "tokens": [51260, 341, 2113, 13, 400, 550, 437, 321, 434, 516, 281, 360, 760, 510, 307, 3677, 264, 3847, 293, 1500, 51448], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1919, "seek": 702544, "start": 7047.12, "end": 7051.28, "text": " and again, notice this training and this is testing into two separate data", "tokens": [51448, 293, 797, 11, 3449, 341, 3097, 293, 341, 307, 4997, 666, 732, 4994, 1412, 51656], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1920, "seek": 702544, "start": 7051.28, "end": 7055.04, "text": " frames. So here we're going to use the names of the columns as the CSV column", "tokens": [51656, 12083, 13, 407, 510, 321, 434, 516, 281, 764, 264, 5288, 295, 264, 13766, 382, 264, 48814, 7738, 51844], "temperature": 0.0, "avg_logprob": -0.1572417279223462, "compression_ratio": 1.773462783171521, "no_speech_prob": 0.010326792486011982}, {"id": 1921, "seek": 705504, "start": 7055.04, "end": 7058.24, "text": " names, we're going to use the path as whatever we loaded here, header equals", "tokens": [50364, 5288, 11, 321, 434, 516, 281, 764, 264, 3100, 382, 2035, 321, 13210, 510, 11, 23117, 6915, 50524], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1922, "seek": 705504, "start": 7058.24, "end": 7063.32, "text": " zero, which just means row zero is the header. Alright, so now we will move down", "tokens": [50524, 4018, 11, 597, 445, 1355, 5386, 4018, 307, 264, 23117, 13, 2798, 11, 370, 586, 321, 486, 1286, 760, 50778], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1923, "seek": 705504, "start": 7063.32, "end": 7066.56, "text": " and we'll have a look at our data set. So like we've done before, oh, I've got", "tokens": [50778, 293, 321, 603, 362, 257, 574, 412, 527, 1412, 992, 13, 407, 411, 321, 600, 1096, 949, 11, 1954, 11, 286, 600, 658, 50940], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1924, "seek": 705504, "start": 7066.56, "end": 7071.2, "text": " to run this code first. CSV column names. Okay, so we've just we're just running", "tokens": [50940, 281, 1190, 341, 3089, 700, 13, 48814, 7738, 5288, 13, 1033, 11, 370, 321, 600, 445, 321, 434, 445, 2614, 51172], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1925, "seek": 705504, "start": 7071.2, "end": 7075.2, "text": " things in the wrong order here, apparently. Okay, so let's look at the head. So", "tokens": [51172, 721, 294, 264, 2085, 1668, 510, 11, 7970, 13, 1033, 11, 370, 718, 311, 574, 412, 264, 1378, 13, 407, 51372], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1926, "seek": 705504, "start": 7075.2, "end": 7078.36, "text": " we can see this is kind of what our data frame looks like. And notice that our", "tokens": [51372, 321, 393, 536, 341, 307, 733, 295, 437, 527, 1412, 3920, 1542, 411, 13, 400, 3449, 300, 527, 51530], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1927, "seek": 705504, "start": 7078.36, "end": 7083.08, "text": " species here are actually defined numerically. So rather than before, when", "tokens": [51530, 6172, 510, 366, 767, 7642, 7866, 984, 13, 407, 2831, 813, 949, 11, 562, 51766], "temperature": 0.0, "avg_logprob": -0.08341538176244619, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0012065116316080093}, {"id": 1928, "seek": 708308, "start": 7083.08, "end": 7086.08, "text": " we had to do that thing where you know, we made those feature columns, and we", "tokens": [50364, 321, 632, 281, 360, 300, 551, 689, 291, 458, 11, 321, 1027, 729, 4111, 13766, 11, 293, 321, 50514], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1929, "seek": 708308, "start": 7086.08, "end": 7089.8, "text": " converted the categorical data into numeric data with those kind of weird", "tokens": [50514, 16424, 264, 19250, 804, 1412, 666, 7866, 299, 1412, 365, 729, 733, 295, 3657, 50700], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1930, "seek": 708308, "start": 7089.8, "end": 7094.36, "text": " TensorFlow tools. This is actually already encoded for us. Now zero stands", "tokens": [50700, 37624, 3873, 13, 639, 307, 767, 1217, 2058, 12340, 337, 505, 13, 823, 4018, 7382, 50928], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1931, "seek": 708308, "start": 7094.36, "end": 7099.2, "text": " for SITOSA. And then one and two obviously stand for these ones respectively. And", "tokens": [50928, 337, 318, 3927, 4367, 32, 13, 400, 550, 472, 293, 732, 2745, 1463, 337, 613, 2306, 25009, 13, 400, 51170], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1932, "seek": 708308, "start": 7099.2, "end": 7104.08, "text": " that's how that works. Now these I believe are in centimeters, the septal length,", "tokens": [51170, 300, 311, 577, 300, 1985, 13, 823, 613, 286, 1697, 366, 294, 23300, 11, 264, 23891, 304, 4641, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1933, "seek": 708308, "start": 7104.32, "end": 7108.24, "text": " petal length, petal width, that's not super important. But sometimes you do", "tokens": [51426, 3817, 304, 4641, 11, 3817, 304, 11402, 11, 300, 311, 406, 1687, 1021, 13, 583, 2171, 291, 360, 51622], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1934, "seek": 708308, "start": 7108.24, "end": 7112.4, "text": " want to know that information. Okay, so now we're going to pop up those columns", "tokens": [51622, 528, 281, 458, 300, 1589, 13, 1033, 11, 370, 586, 321, 434, 516, 281, 1665, 493, 729, 13766, 51830], "temperature": 0.0, "avg_logprob": -0.15101694538645502, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.005729860160499811}, {"id": 1935, "seek": 711240, "start": 7112.4, "end": 7116.48, "text": " for the species like we did before and separate that into train white test, why", "tokens": [50364, 337, 264, 6172, 411, 321, 630, 949, 293, 4994, 300, 666, 3847, 2418, 1500, 11, 983, 50568], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1936, "seek": 711240, "start": 7116.48, "end": 7121.04, "text": " and then have a look at the head again. So let's do that and run this notice that", "tokens": [50568, 293, 550, 362, 257, 574, 412, 264, 1378, 797, 13, 407, 718, 311, 360, 300, 293, 1190, 341, 3449, 300, 50796], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1937, "seek": 711240, "start": 7121.04, "end": 7125.2, "text": " is gone. Again, we've talked about how that works. And then these, if we want to", "tokens": [50796, 307, 2780, 13, 3764, 11, 321, 600, 2825, 466, 577, 300, 1985, 13, 400, 550, 613, 11, 498, 321, 528, 281, 51004], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1938, "seek": 711240, "start": 7125.2, "end": 7129.2, "text": " have a look at them, and actually, let's do this, by just having a new block,", "tokens": [51004, 362, 257, 574, 412, 552, 11, 293, 767, 11, 718, 311, 360, 341, 11, 538, 445, 1419, 257, 777, 3461, 11, 51204], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1939, "seek": 711240, "start": 7129.2, "end": 7137.879999999999, "text": " let's say train underscore y dot, what is it dot head? If I could spell head", "tokens": [51204, 718, 311, 584, 3847, 37556, 288, 5893, 11, 437, 307, 309, 5893, 1378, 30, 759, 286, 727, 9827, 1378, 51638], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1940, "seek": 711240, "start": 7137.879999999999, "end": 7141.719999999999, "text": " correctly. Okay, so we run head, and we can see this is what it looks like", "tokens": [51638, 8944, 13, 1033, 11, 370, 321, 1190, 1378, 11, 293, 321, 393, 536, 341, 307, 437, 309, 1542, 411, 51830], "temperature": 0.0, "avg_logprob": -0.14518655630258415, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.005729949101805687}, {"id": 1941, "seek": 714172, "start": 7141.72, "end": 7145.400000000001, "text": " nothing special. That's what we're getting. Alright, so let's delete that. Let's", "tokens": [50364, 1825, 2121, 13, 663, 311, 437, 321, 434, 1242, 13, 2798, 11, 370, 718, 311, 12097, 300, 13, 961, 311, 50548], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1942, "seek": 714172, "start": 7145.400000000001, "end": 7148.4400000000005, "text": " look at the shape of our training data. I mean, we can probably guess what it is", "tokens": [50548, 574, 412, 264, 3909, 295, 527, 3097, 1412, 13, 286, 914, 11, 321, 393, 1391, 2041, 437, 309, 307, 50700], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1943, "seek": 714172, "start": 7148.4400000000005, "end": 7152.0, "text": " already, right? We're going to have shape four, because we have four features. And", "tokens": [50700, 1217, 11, 558, 30, 492, 434, 516, 281, 362, 3909, 1451, 11, 570, 321, 362, 1451, 4122, 13, 400, 50878], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1944, "seek": 714172, "start": 7152.0, "end": 7156.04, "text": " then how many entries do we have? Well, I'm sure this will tell us so 120 entries", "tokens": [50878, 550, 577, 867, 23041, 360, 321, 362, 30, 1042, 11, 286, 478, 988, 341, 486, 980, 505, 370, 10411, 23041, 51080], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1945, "seek": 714172, "start": 7156.2, "end": 7160.52, "text": " in shape four. Awesome. That's our shape. Okay, input function. So we're moving", "tokens": [51088, 294, 3909, 1451, 13, 10391, 13, 663, 311, 527, 3909, 13, 1033, 11, 4846, 2445, 13, 407, 321, 434, 2684, 51304], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1946, "seek": 714172, "start": 7160.52, "end": 7162.92, "text": " fast here already, we're getting into a lot of the coding. So what I'm actually", "tokens": [51304, 2370, 510, 1217, 11, 321, 434, 1242, 666, 257, 688, 295, 264, 17720, 13, 407, 437, 286, 478, 767, 51424], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1947, "seek": 714172, "start": 7162.92, "end": 7166.360000000001, "text": " going to do is again, copy this over into a separate document, and I'll be back in", "tokens": [51424, 516, 281, 360, 307, 797, 11, 5055, 341, 670, 666, 257, 4994, 4166, 11, 293, 286, 603, 312, 646, 294, 51596], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1948, "seek": 714172, "start": 7166.360000000001, "end": 7169.84, "text": " a second with all that. Okay, so input function time, we already know what the", "tokens": [51596, 257, 1150, 365, 439, 300, 13, 1033, 11, 370, 4846, 2445, 565, 11, 321, 1217, 458, 437, 264, 51770], "temperature": 0.0, "avg_logprob": -0.08873620336455416, "compression_ratio": 1.7753424657534247, "no_speech_prob": 0.006289087235927582}, {"id": 1949, "seek": 716984, "start": 7169.88, "end": 7174.04, "text": " input function does because we used it previously. Now this input function is a", "tokens": [50366, 4846, 2445, 775, 570, 321, 1143, 309, 8046, 13, 823, 341, 4846, 2445, 307, 257, 50574], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1950, "seek": 716984, "start": 7174.04, "end": 7178.360000000001, "text": " little bit different than before, just because we're kind of changing things", "tokens": [50574, 707, 857, 819, 813, 949, 11, 445, 570, 321, 434, 733, 295, 4473, 721, 50790], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1951, "seek": 716984, "start": 7178.360000000001, "end": 7183.64, "text": " slightly. So here, we don't actually have any, what do you call it, we don't have", "tokens": [50790, 4748, 13, 407, 510, 11, 321, 500, 380, 767, 362, 604, 11, 437, 360, 291, 818, 309, 11, 321, 500, 380, 362, 51054], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1952, "seek": 716984, "start": 7183.64, "end": 7187.24, "text": " any epochs, and our batch size is different. So what we've done here is", "tokens": [51054, 604, 30992, 28346, 11, 293, 527, 15245, 2744, 307, 819, 13, 407, 437, 321, 600, 1096, 510, 307, 51234], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1953, "seek": 716984, "start": 7187.24, "end": 7192.28, "text": " rather than actually, you know, defining like make input function, we just have", "tokens": [51234, 2831, 813, 767, 11, 291, 458, 11, 17827, 411, 652, 4846, 2445, 11, 321, 445, 362, 51486], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1954, "seek": 716984, "start": 7192.400000000001, "end": 7196.88, "text": " input function like this. And what we're going to do is a little bit different", "tokens": [51492, 4846, 2445, 411, 341, 13, 400, 437, 321, 434, 516, 281, 360, 307, 257, 707, 857, 819, 51716], "temperature": 0.0, "avg_logprob": -0.10801235560713143, "compression_ratio": 1.876, "no_speech_prob": 0.01406112965196371}, {"id": 1955, "seek": 719688, "start": 7196.88, "end": 7199.56, "text": " when we pass this input function, I'll kind of show you it's a little bit more", "tokens": [50364, 562, 321, 1320, 341, 4846, 2445, 11, 286, 603, 733, 295, 855, 291, 309, 311, 257, 707, 857, 544, 50498], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1956, "seek": 719688, "start": 7199.56, "end": 7202.68, "text": " complicated. But you can see that we've cleaned this up a little bit. So exactly", "tokens": [50498, 6179, 13, 583, 291, 393, 536, 300, 321, 600, 16146, 341, 493, 257, 707, 857, 13, 407, 2293, 50654], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1957, "seek": 719688, "start": 7202.68, "end": 7205.64, "text": " we're doing what we did do before, we're converting this data, which is our", "tokens": [50654, 321, 434, 884, 437, 321, 630, 360, 949, 11, 321, 434, 29942, 341, 1412, 11, 597, 307, 527, 50802], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1958, "seek": 719688, "start": 7205.64, "end": 7209.36, "text": " features, which we're passing in here into a data set. And then we're passing", "tokens": [50802, 4122, 11, 597, 321, 434, 8437, 294, 510, 666, 257, 1412, 992, 13, 400, 550, 321, 434, 8437, 50988], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1959, "seek": 719688, "start": 7209.36, "end": 7213.56, "text": " those labels as well. And then if we're training, so if training is true, what", "tokens": [50988, 729, 16949, 382, 731, 13, 400, 550, 498, 321, 434, 3097, 11, 370, 498, 3097, 307, 2074, 11, 437, 51198], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1960, "seek": 719688, "start": 7213.56, "end": 7217.4400000000005, "text": " we're going to do is say data set is equal to the data set dot shuffle. So we're", "tokens": [51198, 321, 434, 516, 281, 360, 307, 584, 1412, 992, 307, 2681, 281, 264, 1412, 992, 5893, 39426, 13, 407, 321, 434, 51392], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1961, "seek": 719688, "start": 7217.4400000000005, "end": 7221.28, "text": " going to shuffle that information, and then repeat that. And that is all we", "tokens": [51392, 516, 281, 39426, 300, 1589, 11, 293, 550, 7149, 300, 13, 400, 300, 307, 439, 321, 51584], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1962, "seek": 719688, "start": 7221.28, "end": 7225.92, "text": " really need to do. We can do data set dot batch at the batch size 256 return that.", "tokens": [51584, 534, 643, 281, 360, 13, 492, 393, 360, 1412, 992, 5893, 15245, 412, 264, 15245, 2744, 38882, 2736, 300, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11447587841285203, "compression_ratio": 1.9093655589123868, "no_speech_prob": 0.013220454566180706}, {"id": 1963, "seek": 722592, "start": 7226.24, "end": 7229.52, "text": " And we're good to go. So this is our input function. Again, these are kind of", "tokens": [50380, 400, 321, 434, 665, 281, 352, 13, 407, 341, 307, 527, 4846, 2445, 13, 3764, 11, 613, 366, 733, 295, 50544], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1964, "seek": 722592, "start": 7229.52, "end": 7233.04, "text": " complicated. You kind of have to just get experience seeing a bunch of", "tokens": [50544, 6179, 13, 509, 733, 295, 362, 281, 445, 483, 1752, 2577, 257, 3840, 295, 50720], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1965, "seek": 722592, "start": 7233.04, "end": 7236.32, "text": " different ones to understand how to actually make one on your own. For now", "tokens": [50720, 819, 2306, 281, 1223, 577, 281, 767, 652, 472, 322, 428, 1065, 13, 1171, 586, 50884], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1966, "seek": 722592, "start": 7236.32, "end": 7239.36, "text": " on, don't worry about it too much. You can pretty much just copy the input", "tokens": [50884, 322, 11, 500, 380, 3292, 466, 309, 886, 709, 13, 509, 393, 1238, 709, 445, 5055, 264, 4846, 51036], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1967, "seek": 722592, "start": 7239.36, "end": 7242.88, "text": " functions you've created before, and modify them very slightly if you're", "tokens": [51036, 6828, 291, 600, 2942, 949, 11, 293, 16927, 552, 588, 4748, 498, 291, 434, 51212], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1968, "seek": 722592, "start": 7242.88, "end": 7245.4800000000005, "text": " going to be doing your own models. But by the end of this, you should have a", "tokens": [51212, 516, 281, 312, 884, 428, 1065, 5245, 13, 583, 538, 264, 917, 295, 341, 11, 291, 820, 362, 257, 51342], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1969, "seek": 722592, "start": 7245.4800000000005, "end": 7248.52, "text": " good idea of how these input functions work. We will have seen like four or", "tokens": [51342, 665, 1558, 295, 577, 613, 4846, 6828, 589, 13, 492, 486, 362, 1612, 411, 1451, 420, 51494], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1970, "seek": 722592, "start": 7248.52, "end": 7251.76, "text": " five different ones. And then, you know, we can kind of mess with them and tweak", "tokens": [51494, 1732, 819, 2306, 13, 400, 550, 11, 291, 458, 11, 321, 393, 733, 295, 2082, 365, 552, 293, 29879, 51656], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1971, "seek": 722592, "start": 7251.76, "end": 7255.8, "text": " them as we go on, but don't focus on it too much. Okay, so input function, this", "tokens": [51656, 552, 382, 321, 352, 322, 11, 457, 500, 380, 1879, 322, 309, 886, 709, 13, 1033, 11, 370, 4846, 2445, 11, 341, 51858], "temperature": 0.0, "avg_logprob": -0.07976892977784582, "compression_ratio": 1.8563685636856369, "no_speech_prob": 0.015421275049448013}, {"id": 1972, "seek": 725580, "start": 7255.84, "end": 7259.0, "text": " is our input function, I'm not really going to go in too much more detail with", "tokens": [50366, 307, 527, 4846, 2445, 11, 286, 478, 406, 534, 516, 281, 352, 294, 886, 709, 544, 2607, 365, 50524], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1973, "seek": 725580, "start": 7259.0, "end": 7263.2, "text": " that. And now our feature columns. So this is again, pretty straightforward for", "tokens": [50524, 300, 13, 400, 586, 527, 4111, 13766, 13, 407, 341, 307, 797, 11, 1238, 15325, 337, 50734], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1974, "seek": 725580, "start": 7263.2, "end": 7266.08, "text": " the feature columns. All we need to do for this is since our all numeric feature", "tokens": [50734, 264, 4111, 13766, 13, 1057, 321, 643, 281, 360, 337, 341, 307, 1670, 527, 439, 7866, 299, 4111, 50878], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1975, "seek": 725580, "start": 7266.08, "end": 7269.12, "text": " columns is rather than having two for loops where we were separating the", "tokens": [50878, 13766, 307, 2831, 813, 1419, 732, 337, 16121, 689, 321, 645, 29279, 264, 51030], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1976, "seek": 725580, "start": 7269.12, "end": 7273.96, "text": " numeric and categorical feature columns before, we can just loop through all of", "tokens": [51030, 7866, 299, 293, 19250, 804, 4111, 13766, 949, 11, 321, 393, 445, 6367, 807, 439, 295, 51272], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1977, "seek": 725580, "start": 7273.96, "end": 7278.24, "text": " the keys in our training data set. And then we can append to my feature", "tokens": [51272, 264, 9317, 294, 527, 3097, 1412, 992, 13, 400, 550, 321, 393, 34116, 281, 452, 4111, 51486], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1978, "seek": 725580, "start": 7278.24, "end": 7283.24, "text": " columns blank list, the feature column, the numeric column, and the key is", "tokens": [51486, 13766, 8247, 1329, 11, 264, 4111, 7738, 11, 264, 7866, 299, 7738, 11, 293, 264, 2141, 307, 51736], "temperature": 0.0, "avg_logprob": -0.12175056017362154, "compression_ratio": 1.9528985507246377, "no_speech_prob": 0.0012064952170476317}, {"id": 1979, "seek": 728324, "start": 7283.24, "end": 7286.2, "text": " equal to whatever key we've looped through here. Now I'm going to show you", "tokens": [50364, 2681, 281, 2035, 2141, 321, 600, 6367, 292, 807, 510, 13, 823, 286, 478, 516, 281, 855, 291, 50512], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1980, "seek": 728324, "start": 7286.2, "end": 7289.5599999999995, "text": " what this means in case anyone's confused. Again, you can see when I print my", "tokens": [50512, 437, 341, 1355, 294, 1389, 2878, 311, 9019, 13, 3764, 11, 291, 393, 536, 562, 286, 4482, 452, 50680], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1981, "seek": 728324, "start": 7289.5599999999995, "end": 7294.12, "text": " feature columns, we get key equals septal length, we get our shape, and we get", "tokens": [50680, 4111, 13766, 11, 321, 483, 2141, 6915, 23891, 304, 4641, 11, 321, 483, 527, 3909, 11, 293, 321, 483, 50908], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1982, "seek": 728324, "start": 7294.16, "end": 7297.639999999999, "text": " all of that other nice information. So let's copy this into the other one and", "tokens": [50910, 439, 295, 300, 661, 1481, 1589, 13, 407, 718, 311, 5055, 341, 666, 264, 661, 472, 293, 51084], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1983, "seek": 728324, "start": 7297.639999999999, "end": 7302.84, "text": " have a look at our output after this. Okay, so my feature columns for key and", "tokens": [51084, 362, 257, 574, 412, 527, 5598, 934, 341, 13, 1033, 11, 370, 452, 4111, 13766, 337, 2141, 293, 51344], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1984, "seek": 728324, "start": 7302.84, "end": 7308.36, "text": " train dot keys. So notice train is here, train dot keys. What that does is", "tokens": [51344, 3847, 5893, 9317, 13, 407, 3449, 3847, 307, 510, 11, 3847, 5893, 9317, 13, 708, 300, 775, 307, 51620], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1985, "seek": 728324, "start": 7308.36, "end": 7312.36, "text": " actually give us all the columns. So this was a really quick and easy way to", "tokens": [51620, 767, 976, 505, 439, 264, 13766, 13, 407, 341, 390, 257, 534, 1702, 293, 1858, 636, 281, 51820], "temperature": 0.0, "avg_logprob": -0.09840443793763505, "compression_ratio": 1.7672131147540984, "no_speech_prob": 0.01640074886381626}, {"id": 1986, "seek": 731236, "start": 7312.36, "end": 7315.28, "text": " kind of loop through all the different columns. Although I could have looped", "tokens": [50364, 733, 295, 6367, 807, 439, 264, 819, 13766, 13, 5780, 286, 727, 362, 6367, 292, 50510], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1987, "seek": 731236, "start": 7315.28, "end": 7319.28, "text": " through CSV column names and just remove the species column to do that. But", "tokens": [50510, 807, 48814, 7738, 5288, 293, 445, 4159, 264, 6172, 7738, 281, 360, 300, 13, 583, 50710], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1988, "seek": 731236, "start": 7319.799999999999, "end": 7323.16, "text": " again, we don't really need to. So for key and train dot keys, my feature", "tokens": [50736, 797, 11, 321, 500, 380, 534, 643, 281, 13, 407, 337, 2141, 293, 3847, 5893, 9317, 11, 452, 4111, 50904], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1989, "seek": 731236, "start": 7323.16, "end": 7326.719999999999, "text": " columns dot append tf feature column, numeric column, key equals key, this", "tokens": [50904, 13766, 5893, 34116, 256, 69, 4111, 7738, 11, 7866, 299, 7738, 11, 2141, 6915, 2141, 11, 341, 51082], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1990, "seek": 731236, "start": 7326.719999999999, "end": 7329.08, "text": " was just going to create those feature columns, we don't need to do that", "tokens": [51082, 390, 445, 516, 281, 1884, 729, 4111, 13766, 11, 321, 500, 380, 643, 281, 360, 300, 51200], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1991, "seek": 731236, "start": 7329.08, "end": 7332.32, "text": " vocabulary thing and that dot unique because again, these are all already", "tokens": [51200, 19864, 551, 293, 300, 5893, 3845, 570, 797, 11, 613, 366, 439, 1217, 51362], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1992, "seek": 731236, "start": 7332.32, "end": 7336.28, "text": " encoded for us. Okay, awesome. So that was the next step. So let's go back", "tokens": [51362, 2058, 12340, 337, 505, 13, 1033, 11, 3476, 13, 407, 300, 390, 264, 958, 1823, 13, 407, 718, 311, 352, 646, 51560], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1993, "seek": 731236, "start": 7336.28, "end": 7339.48, "text": " here, building the model. Okay, so this is where we need to talk a bit more", "tokens": [51560, 510, 11, 2390, 264, 2316, 13, 1033, 11, 370, 341, 307, 689, 321, 643, 281, 751, 257, 857, 544, 51720], "temperature": 0.0, "avg_logprob": -0.12631421307333154, "compression_ratio": 1.8318042813455657, "no_speech_prob": 0.006902755703777075}, {"id": 1994, "seek": 733948, "start": 7339.48, "end": 7342.919999999999, "text": " in depth of what we're actually going to build. So the model for this is a", "tokens": [50364, 294, 7161, 295, 437, 321, 434, 767, 516, 281, 1322, 13, 407, 264, 2316, 337, 341, 307, 257, 50536], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 1995, "seek": 733948, "start": 7342.919999999999, "end": 7346.919999999999, "text": " classification model. Now there is like hundreds of different classification", "tokens": [50536, 21538, 2316, 13, 823, 456, 307, 411, 6779, 295, 819, 21538, 50736], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 1996, "seek": 733948, "start": 7346.919999999999, "end": 7350.799999999999, "text": " models we can use that are pre made in TensorFlow. And so far, what we've done", "tokens": [50736, 5245, 321, 393, 764, 300, 366, 659, 1027, 294, 37624, 13, 400, 370, 1400, 11, 437, 321, 600, 1096, 50930], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 1997, "seek": 733948, "start": 7350.799999999999, "end": 7355.0, "text": " with that linear classifier is that's a pre made model that we kind of just", "tokens": [50930, 365, 300, 8213, 1508, 9902, 307, 300, 311, 257, 659, 1027, 2316, 300, 321, 733, 295, 445, 51140], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 1998, "seek": 733948, "start": 7355.0, "end": 7358.919999999999, "text": " feed a little bit of information to and it just works for us. Now here we have", "tokens": [51140, 3154, 257, 707, 857, 295, 1589, 281, 293, 309, 445, 1985, 337, 505, 13, 823, 510, 321, 362, 51336], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 1999, "seek": 733948, "start": 7358.919999999999, "end": 7363.12, "text": " two kind of main choices that we can use for this kind of classification task", "tokens": [51336, 732, 733, 295, 2135, 7994, 300, 321, 393, 764, 337, 341, 733, 295, 21538, 5633, 51546], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 2000, "seek": 733948, "start": 7363.12, "end": 7367.12, "text": " that are pre built in TensorFlow, we have a DNN classifier, which stands for a", "tokens": [51546, 300, 366, 659, 3094, 294, 37624, 11, 321, 362, 257, 21500, 45, 1508, 9902, 11, 597, 7382, 337, 257, 51746], "temperature": 0.0, "avg_logprob": -0.10341900273373253, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.016400642693042755}, {"id": 2001, "seek": 736712, "start": 7367.16, "end": 7371.2, "text": " deep neural network, which we've talked about very vaguely, very briefly. And we", "tokens": [50366, 2452, 18161, 3209, 11, 597, 321, 600, 2825, 466, 588, 13501, 48863, 11, 588, 10515, 13, 400, 321, 50568], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2002, "seek": 736712, "start": 7371.2, "end": 7374.92, "text": " have a linear classifier. Now a linear classifier works very similarly to", "tokens": [50568, 362, 257, 8213, 1508, 9902, 13, 823, 257, 8213, 1508, 9902, 1985, 588, 14138, 281, 50754], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2003, "seek": 736712, "start": 7374.92, "end": 7378.76, "text": " linear regression, except it does classification, rather than regression. So", "tokens": [50754, 8213, 24590, 11, 3993, 309, 775, 21538, 11, 2831, 813, 24590, 13, 407, 50946], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2004, "seek": 736712, "start": 7378.76, "end": 7382.48, "text": " we get actually numeric value, or we get, sorry, you know, the labels like", "tokens": [50946, 321, 483, 767, 7866, 299, 2158, 11, 420, 321, 483, 11, 2597, 11, 291, 458, 11, 264, 16949, 411, 51132], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2005, "seek": 736712, "start": 7382.48, "end": 7387.5199999999995, "text": " probability of being a specific label, rather than a numeric value. But in this", "tokens": [51132, 8482, 295, 885, 257, 2685, 7645, 11, 2831, 813, 257, 7866, 299, 2158, 13, 583, 294, 341, 51384], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2006, "seek": 736712, "start": 7387.5199999999995, "end": 7390.68, "text": " instance, we're actually going to go with deep neural network. Now that's simply", "tokens": [51384, 5197, 11, 321, 434, 767, 516, 281, 352, 365, 2452, 18161, 3209, 13, 823, 300, 311, 2935, 51542], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2007, "seek": 736712, "start": 7390.68, "end": 7394.76, "text": " because TensorFlow on their website, like this is all of this is kind of", "tokens": [51542, 570, 37624, 322, 641, 3144, 11, 411, 341, 307, 439, 295, 341, 307, 733, 295, 51746], "temperature": 0.0, "avg_logprob": -0.11183617665217473, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.01590406522154808}, {"id": 2008, "seek": 739476, "start": 7394.8, "end": 7398.2, "text": " building off of TensorFlow website, just all the code is very similar. And I've", "tokens": [50366, 2390, 766, 295, 37624, 3144, 11, 445, 439, 264, 3089, 307, 588, 2531, 13, 400, 286, 600, 50536], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2009, "seek": 739476, "start": 7398.2, "end": 7402.280000000001, "text": " just added my own spin and explain things very in depth. They've recommended", "tokens": [50536, 445, 3869, 452, 1065, 6060, 293, 2903, 721, 588, 294, 7161, 13, 814, 600, 9628, 50740], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2010, "seek": 739476, "start": 7402.280000000001, "end": 7406.24, "text": " using that deep neural network for this is a better kind of choice. But typically", "tokens": [50740, 1228, 300, 2452, 18161, 3209, 337, 341, 307, 257, 1101, 733, 295, 3922, 13, 583, 5850, 50938], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2011, "seek": 739476, "start": 7406.24, "end": 7409.04, "text": " when you're creating machine learning apps, you'll mess around with different", "tokens": [50938, 562, 291, 434, 4084, 3479, 2539, 7733, 11, 291, 603, 2082, 926, 365, 819, 51078], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2012, "seek": 739476, "start": 7409.04, "end": 7412.4800000000005, "text": " models and kind of tweak them. And you'll notice that it's not that difficult to", "tokens": [51078, 5245, 293, 733, 295, 29879, 552, 13, 400, 291, 603, 3449, 300, 309, 311, 406, 300, 2252, 281, 51250], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2013, "seek": 739476, "start": 7412.4800000000005, "end": 7415.400000000001, "text": " change models, because most of the work comes from loading and kind of", "tokens": [51250, 1319, 5245, 11, 570, 881, 295, 264, 589, 1487, 490, 15114, 293, 733, 295, 51396], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2014, "seek": 739476, "start": 7415.400000000001, "end": 7420.400000000001, "text": " pre processing our data. Okay, so what we need to do is build a deep neural", "tokens": [51396, 659, 9007, 527, 1412, 13, 1033, 11, 370, 437, 321, 643, 281, 360, 307, 1322, 257, 2452, 18161, 51646], "temperature": 0.0, "avg_logprob": -0.08776938915252686, "compression_ratio": 1.6636085626911314, "no_speech_prob": 0.012818802148103714}, {"id": 2015, "seek": 742040, "start": 7420.4, "end": 7424.599999999999, "text": " network with two hidden later, two hidden layers with 30 nodes and 10 hidden", "tokens": [50364, 3209, 365, 732, 7633, 1780, 11, 732, 7633, 7914, 365, 2217, 13891, 293, 1266, 7633, 50574], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2016, "seek": 742040, "start": 7424.599999999999, "end": 7428.32, "text": " nodes each. Now I'm going to draw out the architecture of this neural network in", "tokens": [50574, 13891, 1184, 13, 823, 286, 478, 516, 281, 2642, 484, 264, 9482, 295, 341, 18161, 3209, 294, 50760], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2017, "seek": 742040, "start": 7428.32, "end": 7431.08, "text": " just one second. But I want to show you what we've done here. So we said", "tokens": [50760, 445, 472, 1150, 13, 583, 286, 528, 281, 855, 291, 437, 321, 600, 1096, 510, 13, 407, 321, 848, 50898], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2018, "seek": 742040, "start": 7431.08, "end": 7436.16, "text": " classifier equals tf dot estimator. So this estimator module just stores a", "tokens": [50898, 1508, 9902, 6915, 256, 69, 5893, 8017, 1639, 13, 407, 341, 8017, 1639, 10088, 445, 9512, 257, 51152], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2019, "seek": 742040, "start": 7436.16, "end": 7440.2, "text": " bunch of pre made models from TensorFlow. So in this case, DNN classifier is", "tokens": [51152, 3840, 295, 659, 1027, 5245, 490, 37624, 13, 407, 294, 341, 1389, 11, 21500, 45, 1508, 9902, 307, 51354], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2020, "seek": 742040, "start": 7440.2, "end": 7443.679999999999, "text": " one of those. What we need to do is pass our feature columns just like we did to", "tokens": [51354, 472, 295, 729, 13, 708, 321, 643, 281, 360, 307, 1320, 527, 4111, 13766, 445, 411, 321, 630, 281, 51528], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2021, "seek": 742040, "start": 7443.679999999999, "end": 7448.16, "text": " our linear classifier. And now we need to define the hidden units. Now hidden", "tokens": [51528, 527, 8213, 1508, 9902, 13, 400, 586, 321, 643, 281, 6964, 264, 7633, 6815, 13, 823, 7633, 51752], "temperature": 0.0, "avg_logprob": -0.10161010078761888, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.05031304433941841}, {"id": 2022, "seek": 744816, "start": 7448.2, "end": 7452.36, "text": " units is essentially us a building the architecture of the neural network. So", "tokens": [50366, 6815, 307, 4476, 505, 257, 2390, 264, 9482, 295, 264, 18161, 3209, 13, 407, 50574], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2023, "seek": 744816, "start": 7452.36, "end": 7455.84, "text": " like you saw before, we had an input layer, we had some like middle layers", "tokens": [50574, 411, 291, 1866, 949, 11, 321, 632, 364, 4846, 4583, 11, 321, 632, 512, 411, 2808, 7914, 50748], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2024, "seek": 744816, "start": 7455.84, "end": 7458.48, "text": " called our hidden layers in a neural network. And then we had our output", "tokens": [50748, 1219, 527, 7633, 7914, 294, 257, 18161, 3209, 13, 400, 550, 321, 632, 527, 5598, 50880], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2025, "seek": 744816, "start": 7458.48, "end": 7462.08, "text": " layer. I'm going to explain neural networks in the next module. So this will", "tokens": [50880, 4583, 13, 286, 478, 516, 281, 2903, 18161, 9590, 294, 264, 958, 10088, 13, 407, 341, 486, 51060], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2026, "seek": 744816, "start": 7462.08, "end": 7466.12, "text": " all kind of click and make sense. For now, we've arbitrarily decided 30 nodes in", "tokens": [51060, 439, 733, 295, 2052, 293, 652, 2020, 13, 1171, 586, 11, 321, 600, 19071, 3289, 3047, 2217, 13891, 294, 51262], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2027, "seek": 744816, "start": 7466.12, "end": 7470.5199999999995, "text": " the first hidden layer, 10 in the second, and the number of classes is going to", "tokens": [51262, 264, 700, 7633, 4583, 11, 1266, 294, 264, 1150, 11, 293, 264, 1230, 295, 5359, 307, 516, 281, 51482], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2028, "seek": 744816, "start": 7470.5199999999995, "end": 7473.32, "text": " be three. Now that's something that we need to decide. We know there's three", "tokens": [51482, 312, 1045, 13, 823, 300, 311, 746, 300, 321, 643, 281, 4536, 13, 492, 458, 456, 311, 1045, 51622], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2029, "seek": 744816, "start": 7473.32, "end": 7477.2, "text": " classes for the flowers. So that's what we've defined. Okay, so let's copy this", "tokens": [51622, 5359, 337, 264, 8085, 13, 407, 300, 311, 437, 321, 600, 7642, 13, 1033, 11, 370, 718, 311, 5055, 341, 51816], "temperature": 0.0, "avg_logprob": -0.09483548311086801, "compression_ratio": 1.8397626112759644, "no_speech_prob": 0.00327284331433475}, {"id": 2030, "seek": 747720, "start": 7477.2, "end": 7482.44, "text": " in. Go back to the other page here. And that is now our model. And now it is", "tokens": [50364, 294, 13, 1037, 646, 281, 264, 661, 3028, 510, 13, 400, 300, 307, 586, 527, 2316, 13, 400, 586, 309, 307, 50626], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2031, "seek": 747720, "start": 7482.44, "end": 7485.92, "text": " time to talk about how we can actually train the model, which is coming down", "tokens": [50626, 565, 281, 751, 466, 577, 321, 393, 767, 3847, 264, 2316, 11, 597, 307, 1348, 760, 50800], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2032, "seek": 747720, "start": 7485.92, "end": 7489.28, "text": " here. Okay, so I'm going to copy this, I'm going to paste it over here and", "tokens": [50800, 510, 13, 1033, 11, 370, 286, 478, 516, 281, 5055, 341, 11, 286, 478, 516, 281, 9163, 309, 670, 510, 293, 50968], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2033, "seek": 747720, "start": 7489.28, "end": 7492.48, "text": " let's just dig through this because this is a bit more of a complicated piece of", "tokens": [50968, 718, 311, 445, 2528, 807, 341, 570, 341, 307, 257, 857, 544, 295, 257, 6179, 2522, 295, 51128], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2034, "seek": 747720, "start": 7492.48, "end": 7496.04, "text": " code than we usually used to work with. I'm also going to remove these comments", "tokens": [51128, 3089, 813, 321, 2673, 1143, 281, 589, 365, 13, 286, 478, 611, 516, 281, 4159, 613, 3053, 51306], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2035, "seek": 747720, "start": 7496.04, "end": 7499.88, "text": " just to clean things up in here. So we've defined the classifier, which is a", "tokens": [51306, 445, 281, 2541, 721, 493, 294, 510, 13, 407, 321, 600, 7642, 264, 1508, 9902, 11, 597, 307, 257, 51498], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2036, "seek": 747720, "start": 7499.88, "end": 7503.04, "text": " deep neural network classifier, we have our feature columns hidden units", "tokens": [51498, 2452, 18161, 3209, 1508, 9902, 11, 321, 362, 527, 4111, 13766, 7633, 6815, 51656], "temperature": 0.0, "avg_logprob": -0.09372525215148926, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.004754609894007444}, {"id": 2037, "seek": 750304, "start": 7503.04, "end": 7508.44, "text": " classes. Now to train the classifier. So we have this input function here. This", "tokens": [50364, 5359, 13, 823, 281, 3847, 264, 1508, 9902, 13, 407, 321, 362, 341, 4846, 2445, 510, 13, 639, 50634], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2038, "seek": 750304, "start": 7508.44, "end": 7511.6, "text": " input function is different than the one we created previously. Remember when we", "tokens": [50634, 4846, 2445, 307, 819, 813, 264, 472, 321, 2942, 8046, 13, 5459, 562, 321, 50792], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2039, "seek": 750304, "start": 7511.6, "end": 7515.88, "text": " had previously was like make input, whatever function, I won't continue", "tokens": [50792, 632, 8046, 390, 411, 652, 4846, 11, 2035, 2445, 11, 286, 1582, 380, 2354, 51006], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2040, "seek": 750304, "start": 7515.88, "end": 7519.72, "text": " typing in the inside it to find another function. And it actually returned that", "tokens": [51006, 18444, 294, 264, 1854, 309, 281, 915, 1071, 2445, 13, 400, 309, 767, 8752, 300, 51198], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2041, "seek": 750304, "start": 7519.72, "end": 7523.84, "text": " function from this function. I know, complicated. If you're not a Python kind", "tokens": [51198, 2445, 490, 341, 2445, 13, 286, 458, 11, 6179, 13, 759, 291, 434, 406, 257, 15329, 733, 51404], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2042, "seek": 750304, "start": 7523.84, "end": 7527.72, "text": " of pro, I don't expect that to make perfect sense. But here, we just have a", "tokens": [51404, 295, 447, 11, 286, 500, 380, 2066, 300, 281, 652, 2176, 2020, 13, 583, 510, 11, 321, 445, 362, 257, 51598], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2043, "seek": 750304, "start": 7527.72, "end": 7530.96, "text": " function, right? We do not returning a function from another function, it's just", "tokens": [51598, 2445, 11, 558, 30, 492, 360, 406, 12678, 257, 2445, 490, 1071, 2445, 11, 309, 311, 445, 51760], "temperature": 0.0, "avg_logprob": -0.14690057011961027, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.02441856451332569}, {"id": 2044, "seek": 753096, "start": 7531.0, "end": 7537.52, "text": " one function. So when we want to use this to train our model, what we do is", "tokens": [50366, 472, 2445, 13, 407, 562, 321, 528, 281, 764, 341, 281, 3847, 527, 2316, 11, 437, 321, 360, 307, 50692], "temperature": 0.0, "avg_logprob": -0.07840485572814941, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.014500790275633335}, {"id": 2045, "seek": 753096, "start": 7537.52, "end": 7541.64, "text": " create something called a lambda. Now a lambda is an anonymous function that can", "tokens": [50692, 1884, 746, 1219, 257, 13607, 13, 823, 257, 13607, 307, 364, 24932, 2445, 300, 393, 50898], "temperature": 0.0, "avg_logprob": -0.07840485572814941, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.014500790275633335}, {"id": 2046, "seek": 753096, "start": 7541.64, "end": 7545.96, "text": " be defined in one line. When you write lambda, what that means is essentially", "tokens": [50898, 312, 7642, 294, 472, 1622, 13, 1133, 291, 2464, 13607, 11, 437, 300, 1355, 307, 4476, 51114], "temperature": 0.0, "avg_logprob": -0.07840485572814941, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.014500790275633335}, {"id": 2047, "seek": 753096, "start": 7545.96, "end": 7550.52, "text": " this is a function. So this is a function. And whatever's after the colon is what", "tokens": [51114, 341, 307, 257, 2445, 13, 407, 341, 307, 257, 2445, 13, 400, 2035, 311, 934, 264, 8255, 307, 437, 51342], "temperature": 0.0, "avg_logprob": -0.07840485572814941, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.014500790275633335}, {"id": 2048, "seek": 753096, "start": 7550.52, "end": 7554.96, "text": " this function does. Now this is a one line function. So like, if I create a", "tokens": [51342, 341, 2445, 775, 13, 823, 341, 307, 257, 472, 1622, 2445, 13, 407, 411, 11, 498, 286, 1884, 257, 51564], "temperature": 0.0, "avg_logprob": -0.07840485572814941, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.014500790275633335}, {"id": 2049, "seek": 755496, "start": 7554.96, "end": 7562.84, "text": " lambda here, right, and I say lambda, print, hi, and I said, x equals lambda,", "tokens": [50364, 13607, 510, 11, 558, 11, 293, 286, 584, 13607, 11, 4482, 11, 4879, 11, 293, 286, 848, 11, 2031, 6915, 13607, 11, 50758], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2050, "seek": 755496, "start": 7562.88, "end": 7568.04, "text": " and I called x like that, this works, this is a valid line of syntax. Actually,", "tokens": [50760, 293, 286, 1219, 2031, 411, 300, 11, 341, 1985, 11, 341, 307, 257, 7363, 1622, 295, 28431, 13, 5135, 11, 51018], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2051, "seek": 755496, "start": 7568.04, "end": 7570.92, "text": " I want to make sure that I'm not just like messing with you. And I say that", "tokens": [51018, 286, 528, 281, 652, 988, 300, 286, 478, 406, 445, 411, 23258, 365, 291, 13, 400, 286, 584, 300, 51162], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2052, "seek": 755496, "start": 7570.92, "end": 7575.04, "text": " and then this is actually correct. Okay, so sorry, I just accidentally trained", "tokens": [51162, 293, 550, 341, 307, 767, 3006, 13, 1033, 11, 370, 2597, 11, 286, 445, 15715, 8895, 51368], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2053, "seek": 755496, "start": 7575.04, "end": 7577.96, "text": " the model. So I just commented that out. You can see we're printing high, right?", "tokens": [51368, 264, 2316, 13, 407, 286, 445, 26940, 300, 484, 13, 509, 393, 536, 321, 434, 14699, 1090, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2054, "seek": 755496, "start": 7577.96, "end": 7581.2, "text": " At the bottom of the screen, I know it's kind of small, but does say hi. That's", "tokens": [51514, 1711, 264, 2767, 295, 264, 2568, 11, 286, 458, 309, 311, 733, 295, 1359, 11, 457, 775, 584, 4879, 13, 663, 311, 51676], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2055, "seek": 755496, "start": 7581.2, "end": 7583.84, "text": " how this works. Okay, so this is a cool thing. If you haven't seen this in", "tokens": [51676, 577, 341, 1985, 13, 1033, 11, 370, 341, 307, 257, 1627, 551, 13, 759, 291, 2378, 380, 1612, 341, 294, 51808], "temperature": 0.0, "avg_logprob": -0.13466681203534525, "compression_ratio": 1.7908496732026145, "no_speech_prob": 0.23926807940006256}, {"id": 2056, "seek": 758384, "start": 7583.88, "end": 7587.84, "text": " Python before, that's what a lambda does allows you to define a function in one", "tokens": [50366, 15329, 949, 11, 300, 311, 437, 257, 13607, 775, 4045, 291, 281, 6964, 257, 2445, 294, 472, 50564], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2057, "seek": 758384, "start": 7587.84, "end": 7591.76, "text": " line. Now the thing that's great about this is that we can say, like, you know,", "tokens": [50564, 1622, 13, 823, 264, 551, 300, 311, 869, 466, 341, 307, 300, 321, 393, 584, 11, 411, 11, 291, 458, 11, 50760], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2058, "seek": 758384, "start": 7591.8, "end": 7595.0, "text": " x equals lambda, and here put another function, which is exactly what will be", "tokens": [50762, 2031, 6915, 13607, 11, 293, 510, 829, 1071, 2445, 11, 597, 307, 2293, 437, 486, 312, 50922], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2059, "seek": 758384, "start": 7595.0, "end": 7598.96, "text": " done with this print function. And that means when we call x, it will, you know,", "tokens": [50922, 1096, 365, 341, 4482, 2445, 13, 400, 300, 1355, 562, 321, 818, 2031, 11, 309, 486, 11, 291, 458, 11, 51120], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2060, "seek": 758384, "start": 7599.0, "end": 7602.32, "text": " execute this function, which will just execute the other function. So it's kind", "tokens": [51122, 14483, 341, 2445, 11, 597, 486, 445, 14483, 264, 661, 2445, 13, 407, 309, 311, 733, 51288], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2061, "seek": 758384, "start": 7602.32, "end": 7606.2, "text": " of like a chain where you call x, x is a function. And inside that function, it", "tokens": [51288, 295, 411, 257, 5021, 689, 291, 818, 2031, 11, 2031, 307, 257, 2445, 13, 400, 1854, 300, 2445, 11, 309, 51482], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2062, "seek": 758384, "start": 7606.2, "end": 7608.92, "text": " does another function, right? It just like calling a function from inside a", "tokens": [51482, 775, 1071, 2445, 11, 558, 30, 467, 445, 411, 5141, 257, 2445, 490, 1854, 257, 51618], "temperature": 0.0, "avg_logprob": -0.09842625070125499, "compression_ratio": 1.9303135888501741, "no_speech_prob": 0.017982764169573784}, {"id": 2063, "seek": 760892, "start": 7608.92, "end": 7615.08, "text": " function. So what is lambda doing here? Well, since we need the actual function", "tokens": [50364, 2445, 13, 407, 437, 307, 13607, 884, 510, 30, 1042, 11, 1670, 321, 643, 264, 3539, 2445, 50672], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2064, "seek": 760892, "start": 7615.12, "end": 7621.2, "text": " object, what we do is we define a function that returns to us a function. So this", "tokens": [50674, 2657, 11, 437, 321, 360, 307, 321, 6964, 257, 2445, 300, 11247, 281, 505, 257, 2445, 13, 407, 341, 50978], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2065, "seek": 760892, "start": 7621.2, "end": 7626.12, "text": " actually just like it calls this function, when you put this here. Now there's no", "tokens": [50978, 767, 445, 411, 309, 5498, 341, 2445, 11, 562, 291, 829, 341, 510, 13, 823, 456, 311, 572, 51224], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2066, "seek": 760892, "start": 7626.16, "end": 7629.4800000000005, "text": " I can't it's it's very difficult to explain this if you don't really", "tokens": [51226, 286, 393, 380, 309, 311, 309, 311, 588, 2252, 281, 2903, 341, 498, 291, 500, 380, 534, 51392], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2067, "seek": 760892, "start": 7629.4800000000005, "end": 7633.6, "text": " understand the concept of lambdas, and you don't understand the input functions. But", "tokens": [51392, 1223, 264, 3410, 295, 10097, 27476, 11, 293, 291, 500, 380, 1223, 264, 4846, 6828, 13, 583, 51598], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2068, "seek": 760892, "start": 7633.6, "end": 7637.56, "text": " just know we're doing this because of the fact that we didn't embed another", "tokens": [51598, 445, 458, 321, 434, 884, 341, 570, 295, 264, 1186, 300, 321, 994, 380, 12240, 1071, 51796], "temperature": 0.0, "avg_logprob": -0.10389145350052138, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.050318941473960876}, {"id": 2069, "seek": 763756, "start": 7637.6, "end": 7642.160000000001, "text": " function and return the function object. If we had done that, if we had done that,", "tokens": [50366, 2445, 293, 2736, 264, 2445, 2657, 13, 759, 321, 632, 1096, 300, 11, 498, 321, 632, 1096, 300, 11, 50594], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2070, "seek": 763756, "start": 7642.200000000001, "end": 7646.080000000001, "text": " you know, input function that we had created before where we had the interior", "tokens": [50596, 291, 458, 11, 4846, 2445, 300, 321, 632, 2942, 949, 689, 321, 632, 264, 10636, 50790], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2071, "seek": 763756, "start": 7646.080000000001, "end": 7649.72, "text": " function, then we wouldn't need to do this because what would happen is we would", "tokens": [50790, 2445, 11, 550, 321, 2759, 380, 643, 281, 360, 341, 570, 437, 576, 1051, 307, 321, 576, 50972], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2072, "seek": 763756, "start": 7649.72, "end": 7656.0, "text": " return the input function, right, like that, which means when we passed it into", "tokens": [50972, 2736, 264, 4846, 2445, 11, 558, 11, 411, 300, 11, 597, 1355, 562, 321, 4678, 309, 666, 51286], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2073, "seek": 763756, "start": 7656.0, "end": 7660.92, "text": " here, it could just call that directly. It didn't need to have a lambda. Whereas", "tokens": [51286, 510, 11, 309, 727, 445, 818, 300, 3838, 13, 467, 994, 380, 643, 281, 362, 257, 13607, 13, 13813, 51532], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2074, "seek": 763756, "start": 7660.92, "end": 7666.120000000001, "text": " here, though, since we need to just put a lambda, we need to define what this is", "tokens": [51532, 510, 11, 1673, 11, 1670, 321, 643, 281, 445, 829, 257, 13607, 11, 321, 643, 281, 6964, 437, 341, 307, 51792], "temperature": 0.0, "avg_logprob": -0.07921542017913062, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.014954598620533943}, {"id": 2075, "seek": 766612, "start": 7666.12, "end": 7669.28, "text": " and then and then this works. That's just there's no other way to really explain", "tokens": [50364, 293, 550, 293, 550, 341, 1985, 13, 663, 311, 445, 456, 311, 572, 661, 636, 281, 534, 2903, 50522], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2076, "seek": 766612, "start": 7669.28, "end": 7673.64, "text": " this. So yeah, what we do is we create this input function. So we pass we have", "tokens": [50522, 341, 13, 407, 1338, 11, 437, 321, 360, 307, 321, 1884, 341, 4846, 2445, 13, 407, 321, 1320, 321, 362, 50740], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2077, "seek": 766612, "start": 7673.64, "end": 7678.16, "text": " train, we have train y and we have training equals true. And then we do steps", "tokens": [50740, 3847, 11, 321, 362, 3847, 288, 293, 321, 362, 3097, 6915, 2074, 13, 400, 550, 321, 360, 4439, 50966], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2078, "seek": 766612, "start": 7678.16, "end": 7682.88, "text": " equals 5000. So this is similar to an epoch, except this is just defining a set", "tokens": [50966, 6915, 23777, 13, 407, 341, 307, 2531, 281, 364, 30992, 339, 11, 3993, 341, 307, 445, 17827, 257, 992, 51202], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2079, "seek": 766612, "start": 7682.88, "end": 7685.76, "text": " amount of steps we're going to go through. So rather than saying like we'll go", "tokens": [51202, 2372, 295, 4439, 321, 434, 516, 281, 352, 807, 13, 407, 2831, 813, 1566, 411, 321, 603, 352, 51346], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2080, "seek": 766612, "start": 7685.76, "end": 7688.84, "text": " through the data set 10 times, we're just going to say we'll go through the data", "tokens": [51346, 807, 264, 1412, 992, 1266, 1413, 11, 321, 434, 445, 516, 281, 584, 321, 603, 352, 807, 264, 1412, 51500], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2081, "seek": 766612, "start": 7688.84, "end": 7693.72, "text": " set until we fit 5000 numbers, like 5000 things that have been looked at. So", "tokens": [51500, 992, 1826, 321, 3318, 23777, 3547, 11, 411, 23777, 721, 300, 362, 668, 2956, 412, 13, 407, 51744], "temperature": 0.0, "avg_logprob": -0.10344532933728448, "compression_ratio": 1.9236111111111112, "no_speech_prob": 0.004609106108546257}, {"id": 2082, "seek": 769372, "start": 7693.76, "end": 7696.96, "text": " that's what this does with that train. Let's run this and just look at the", "tokens": [50366, 300, 311, 437, 341, 775, 365, 300, 3847, 13, 961, 311, 1190, 341, 293, 445, 574, 412, 264, 50526], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2083, "seek": 769372, "start": 7696.96, "end": 7701.240000000001, "text": " training output from our model, it gives us some like, things here, we can kind of", "tokens": [50526, 3097, 5598, 490, 527, 2316, 11, 309, 2709, 505, 512, 411, 11, 721, 510, 11, 321, 393, 733, 295, 50740], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2084, "seek": 769372, "start": 7701.240000000001, "end": 7705.16, "text": " see how this is working. Notice that if I can stop here for a second, it tells us", "tokens": [50740, 536, 577, 341, 307, 1364, 13, 13428, 300, 498, 286, 393, 1590, 510, 337, 257, 1150, 11, 309, 5112, 505, 50936], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2085, "seek": 769372, "start": 7705.16, "end": 7710.08, "text": " the current step, it tells us the loss, the lowest, the lower this number, the", "tokens": [50936, 264, 2190, 1823, 11, 309, 5112, 505, 264, 4470, 11, 264, 12437, 11, 264, 3126, 341, 1230, 11, 264, 51182], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2086, "seek": 769372, "start": 7710.08, "end": 7714.2, "text": " better. And then it tells us global steps per second. So how many steps we're", "tokens": [51182, 1101, 13, 400, 550, 309, 5112, 505, 4338, 4439, 680, 1150, 13, 407, 577, 867, 4439, 321, 434, 51388], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2087, "seek": 769372, "start": 7714.2, "end": 7720.12, "text": " completing per second. Now at the end here, we get final step loss of 39, which", "tokens": [51388, 19472, 680, 1150, 13, 823, 412, 264, 917, 510, 11, 321, 483, 2572, 1823, 4470, 295, 15238, 11, 597, 51684], "temperature": 0.0, "avg_logprob": -0.10626520322063776, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0026315830182284117}, {"id": 2088, "seek": 772012, "start": 7720.12, "end": 7725.24, "text": " is pretty high, which means this is pretty bad. But that's fine. This is kind", "tokens": [50364, 307, 1238, 1090, 11, 597, 1355, 341, 307, 1238, 1578, 13, 583, 300, 311, 2489, 13, 639, 307, 733, 50620], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2089, "seek": 772012, "start": 7725.24, "end": 7728.5599999999995, "text": " of just our first test at training in neural network. So this is just giving", "tokens": [50620, 295, 445, 527, 700, 1500, 412, 3097, 294, 18161, 3209, 13, 407, 341, 307, 445, 2902, 50786], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2090, "seek": 772012, "start": 7728.5599999999995, "end": 7731.8, "text": " us output while it's training to kind of save what's happening. Now, in our case,", "tokens": [50786, 505, 5598, 1339, 309, 311, 3097, 281, 733, 295, 3155, 437, 311, 2737, 13, 823, 11, 294, 527, 1389, 11, 50948], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2091, "seek": 772012, "start": 7731.8, "end": 7735.16, "text": " we don't really care because this is a very small model. When you're training", "tokens": [50948, 321, 500, 380, 534, 1127, 570, 341, 307, 257, 588, 1359, 2316, 13, 1133, 291, 434, 3097, 51116], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2092, "seek": 772012, "start": 7735.16, "end": 7738.72, "text": " models that are massive and take terabytes of data, you kind of care about the", "tokens": [51116, 5245, 300, 366, 5994, 293, 747, 1796, 24538, 295, 1412, 11, 291, 733, 295, 1127, 466, 264, 51294], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2093, "seek": 772012, "start": 7738.72, "end": 7743.08, "text": " progress of them. So that's when you would use kind of that output, right? And", "tokens": [51294, 4205, 295, 552, 13, 407, 300, 311, 562, 291, 576, 764, 733, 295, 300, 5598, 11, 558, 30, 400, 51512], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2094, "seek": 772012, "start": 7743.08, "end": 7745.76, "text": " you would actually look at that. Okay, so now that we've trained the model, let's", "tokens": [51512, 291, 576, 767, 574, 412, 300, 13, 1033, 11, 370, 586, 300, 321, 600, 8895, 264, 2316, 11, 718, 311, 51646], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2095, "seek": 772012, "start": 7745.76, "end": 7749.5199999999995, "text": " actually do an evaluation on the model. So we're just going to say classifier dot", "tokens": [51646, 767, 360, 364, 13344, 322, 264, 2316, 13, 407, 321, 434, 445, 516, 281, 584, 1508, 9902, 5893, 51834], "temperature": 0.0, "avg_logprob": -0.12940970862784038, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.16880445182323456}, {"id": 2096, "seek": 774952, "start": 7749.56, "end": 7753.240000000001, "text": " evaluate. And what we're going to do is a very similar thing to what we've done", "tokens": [50366, 13059, 13, 400, 437, 321, 434, 516, 281, 360, 307, 257, 588, 2531, 551, 281, 437, 321, 600, 1096, 50550], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2097, "seek": 774952, "start": 7753.240000000001, "end": 7758.080000000001, "text": " here is just pass this input function, right, like here with a lambda once", "tokens": [50550, 510, 307, 445, 1320, 341, 4846, 2445, 11, 558, 11, 411, 510, 365, 257, 13607, 1564, 50792], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2098, "seek": 774952, "start": 7758.080000000001, "end": 7762.84, "text": " again, and reason we add the lambda when we don't have this like, double", "tokens": [50792, 797, 11, 293, 1778, 321, 909, 264, 13607, 562, 321, 500, 380, 362, 341, 411, 11, 3834, 51030], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2099, "seek": 774952, "start": 7762.84, "end": 7766.200000000001, "text": " function going on, like a nested function, we need the lambda. And then in", "tokens": [51030, 2445, 516, 322, 11, 411, 257, 15646, 292, 2445, 11, 321, 643, 264, 13607, 13, 400, 550, 294, 51198], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2100, "seek": 774952, "start": 7766.200000000001, "end": 7770.200000000001, "text": " here, what we do is rather than passing train and train y, we're going to pass", "tokens": [51198, 510, 11, 437, 321, 360, 307, 2831, 813, 8437, 3847, 293, 3847, 288, 11, 321, 434, 516, 281, 1320, 51398], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2101, "seek": 774952, "start": 7770.240000000001, "end": 7775.72, "text": " test, I believe. And I think it's I just called it test y. Okay, and then for", "tokens": [51400, 1500, 11, 286, 1697, 13, 400, 286, 519, 309, 311, 286, 445, 1219, 309, 1500, 288, 13, 1033, 11, 293, 550, 337, 51674], "temperature": 0.0, "avg_logprob": -0.1309423370361328, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.014500835910439491}, {"id": 2102, "seek": 777572, "start": 7775.72, "end": 7780.360000000001, "text": " training, obviously, this is false. So we can just set that false like that. I'm", "tokens": [50364, 3097, 11, 2745, 11, 341, 307, 7908, 13, 407, 321, 393, 445, 992, 300, 7908, 411, 300, 13, 286, 478, 50596], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2103, "seek": 777572, "start": 7780.360000000001, "end": 7782.76, "text": " just going to look at the other screen and make sure I didn't mess this up. Because", "tokens": [50596, 445, 516, 281, 574, 412, 264, 661, 2568, 293, 652, 988, 286, 994, 380, 2082, 341, 493, 13, 1436, 50716], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2104, "seek": 777572, "start": 7782.76, "end": 7786.72, "text": " again, I don't remember the syntax. Yes, a cluster classifier dot evaluate test", "tokens": [50716, 797, 11, 286, 500, 380, 1604, 264, 28431, 13, 1079, 11, 257, 13630, 1508, 9902, 5893, 13059, 1500, 50914], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2105, "seek": 777572, "start": 7786.72, "end": 7790.52, "text": " test y looks good to me. We'll take this print statement just so we get a nice", "tokens": [50914, 1500, 288, 1542, 665, 281, 385, 13, 492, 603, 747, 341, 4482, 5629, 445, 370, 321, 483, 257, 1481, 51104], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2106, "seek": 777572, "start": 7790.56, "end": 7795.280000000001, "text": " output for our accuracy. Okay, so let's look at this. Again, we're going to have", "tokens": [51106, 5598, 337, 527, 14170, 13, 1033, 11, 370, 718, 311, 574, 412, 341, 13, 3764, 11, 321, 434, 516, 281, 362, 51342], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2107, "seek": 777572, "start": 7795.280000000001, "end": 7799.04, "text": " to wait for this to train. But I will show you a way that we don't need to wait", "tokens": [51342, 281, 1699, 337, 341, 281, 3847, 13, 583, 286, 486, 855, 291, 257, 636, 300, 321, 500, 380, 643, 281, 1699, 51530], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2108, "seek": 777572, "start": 7799.04, "end": 7803.84, "text": " for this to train every time in one second. And I'll be right back. Okay, so", "tokens": [51530, 337, 341, 281, 3847, 633, 565, 294, 472, 1150, 13, 400, 286, 603, 312, 558, 646, 13, 1033, 11, 370, 51770], "temperature": 0.0, "avg_logprob": -0.10133491862903941, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.024420596659183502}, {"id": 2109, "seek": 780384, "start": 7803.88, "end": 7806.8, "text": " what I'm actually going to do, and I've just kind of pause like the execution of", "tokens": [50366, 437, 286, 478, 767, 516, 281, 360, 11, 293, 286, 600, 445, 733, 295, 10465, 411, 264, 15058, 295, 50512], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2110, "seek": 780384, "start": 7806.8, "end": 7811.72, "text": " this code is throw this in the next block under, because the nice thing about", "tokens": [50512, 341, 3089, 307, 3507, 341, 294, 264, 958, 3461, 833, 11, 570, 264, 1481, 551, 466, 50758], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2111, "seek": 780384, "start": 7811.72, "end": 7815.8, "text": " Google Collaboratory is that I can run this block of code, right, I can train", "tokens": [50758, 3329, 44483, 4745, 307, 300, 286, 393, 1190, 341, 3461, 295, 3089, 11, 558, 11, 286, 393, 3847, 50962], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2112, "seek": 780384, "start": 7815.8, "end": 7819.16, "text": " all this stuff, which is what I'll run now while we're talking just so it happens.", "tokens": [50962, 439, 341, 1507, 11, 597, 307, 437, 286, 603, 1190, 586, 1339, 321, 434, 1417, 445, 370, 309, 2314, 13, 51130], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2113, "seek": 780384, "start": 7819.4400000000005, "end": 7823.56, "text": " And then I can have another code block kind of below it, which I have here. And", "tokens": [51144, 400, 550, 286, 393, 362, 1071, 3089, 3461, 733, 295, 2507, 309, 11, 597, 286, 362, 510, 13, 400, 51350], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2114, "seek": 780384, "start": 7823.6, "end": 7826.8, "text": " it doesn't matter. I don't need to rerun that block every time I change", "tokens": [51352, 309, 1177, 380, 1871, 13, 286, 500, 380, 643, 281, 43819, 409, 300, 3461, 633, 565, 286, 1319, 51512], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2115, "seek": 780384, "start": 7826.8, "end": 7829.72, "text": " something here. So if I change something in any lower blocks, I don't need to", "tokens": [51512, 746, 510, 13, 407, 498, 286, 1319, 746, 294, 604, 3126, 8474, 11, 286, 500, 380, 643, 281, 51658], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2116, "seek": 780384, "start": 7829.72, "end": 7832.72, "text": " change the upper block, which means I don't need to wait for this to train every", "tokens": [51658, 1319, 264, 6597, 3461, 11, 597, 1355, 286, 500, 380, 643, 281, 1699, 337, 341, 281, 3847, 633, 51808], "temperature": 0.0, "avg_logprob": -0.10661137976297518, "compression_ratio": 1.920731707317073, "no_speech_prob": 0.007120856083929539}, {"id": 2117, "seek": 783272, "start": 7832.72, "end": 7836.6, "text": " time I want to do an evaluation on it. Anyways, so we've done this, we got test,", "tokens": [50364, 565, 286, 528, 281, 360, 364, 13344, 322, 309, 13, 15585, 11, 370, 321, 600, 1096, 341, 11, 321, 658, 1500, 11, 50558], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2118, "seek": 783272, "start": 7836.6, "end": 7840.88, "text": " we got test why I just need to change this instead of eval result. Actually, I", "tokens": [50558, 321, 658, 1500, 983, 286, 445, 643, 281, 1319, 341, 2602, 295, 1073, 304, 1874, 13, 5135, 11, 286, 50772], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2119, "seek": 783272, "start": 7840.88, "end": 7846.6, "text": " need to say eval underscore result equals classifier dot evaluate so that we can", "tokens": [50772, 643, 281, 584, 1073, 304, 37556, 1874, 6915, 1508, 9902, 5893, 13059, 370, 300, 321, 393, 51058], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2120, "seek": 783272, "start": 7846.6, "end": 7849.96, "text": " actually store this somewhere and get the answer. And now we'll print this and", "tokens": [51058, 767, 3531, 341, 4079, 293, 483, 264, 1867, 13, 400, 586, 321, 603, 4482, 341, 293, 51226], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2121, "seek": 783272, "start": 7849.96, "end": 7854.52, "text": " notice this happens much, much faster. We get a test accuracy of 80%. So if I were", "tokens": [51226, 3449, 341, 2314, 709, 11, 709, 4663, 13, 492, 483, 257, 1500, 14170, 295, 4688, 6856, 407, 498, 286, 645, 51454], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2122, "seek": 783272, "start": 7854.52, "end": 7858.4400000000005, "text": " to retrain the model, chances are this accuracy would change again, because of", "tokens": [51454, 281, 1533, 7146, 264, 2316, 11, 10486, 366, 341, 14170, 576, 1319, 797, 11, 570, 295, 51650], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2123, "seek": 783272, "start": 7858.4400000000005, "end": 7861.64, "text": " the order in which we're seeing different flowers. But this is pretty decent", "tokens": [51650, 264, 1668, 294, 597, 321, 434, 2577, 819, 8085, 13, 583, 341, 307, 1238, 8681, 51810], "temperature": 0.0, "avg_logprob": -0.11917475291660853, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.010011712089180946}, {"id": 2124, "seek": 786164, "start": 7861.68, "end": 7866.0, "text": " considering we don't have that much test data. And we don't really know what", "tokens": [50366, 8079, 321, 500, 380, 362, 300, 709, 1500, 1412, 13, 400, 321, 500, 380, 534, 458, 437, 50582], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2125, "seek": 786164, "start": 7866.0, "end": 7868.4400000000005, "text": " we're doing, right? We're kind of just messing around and experimenting for", "tokens": [50582, 321, 434, 884, 11, 558, 30, 492, 434, 733, 295, 445, 23258, 926, 293, 29070, 337, 50704], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2126, "seek": 786164, "start": 7868.4400000000005, "end": 7872.12, "text": " right now. So to get 80% is pretty good. Okay, so actually, what am I doing? We", "tokens": [50704, 558, 586, 13, 407, 281, 483, 4688, 4, 307, 1238, 665, 13, 1033, 11, 370, 767, 11, 437, 669, 286, 884, 30, 492, 50888], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2127, "seek": 786164, "start": 7872.12, "end": 7875.200000000001, "text": " need to go back now and do predictions. So how am I going to predict this for", "tokens": [50888, 643, 281, 352, 646, 586, 293, 360, 21264, 13, 407, 577, 669, 286, 516, 281, 6069, 341, 337, 51042], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2128, "seek": 786164, "start": 7875.200000000001, "end": 7879.4400000000005, "text": " specific flowers? So let's go back to our core learning algorithms. And let's go", "tokens": [51042, 2685, 8085, 30, 407, 718, 311, 352, 646, 281, 527, 4965, 2539, 14642, 13, 400, 718, 311, 352, 51254], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2129, "seek": 786164, "start": 7879.4400000000005, "end": 7884.12, "text": " to predictions. Now, I've written a script already, just to save a bit of time", "tokens": [51254, 281, 21264, 13, 823, 11, 286, 600, 3720, 257, 5755, 1217, 11, 445, 281, 3155, 257, 857, 295, 565, 51488], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2130, "seek": 786164, "start": 7884.12, "end": 7889.4800000000005, "text": " that allows us to do a prediction on any given flower. So what I'm going to do is", "tokens": [51488, 300, 4045, 505, 281, 360, 257, 17630, 322, 604, 2212, 8617, 13, 407, 437, 286, 478, 516, 281, 360, 307, 51756], "temperature": 0.0, "avg_logprob": -0.08072722688013194, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.00857621617615223}, {"id": 2131, "seek": 788948, "start": 7889.5199999999995, "end": 7893.32, "text": " create a new block down here, code block and copy this function in. Now we're", "tokens": [50366, 1884, 257, 777, 3461, 760, 510, 11, 3089, 3461, 293, 5055, 341, 2445, 294, 13, 823, 321, 434, 50556], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2132, "seek": 788948, "start": 7893.32, "end": 7896.4, "text": " going to digest this and kind of go through this on our own to make sure", "tokens": [50556, 516, 281, 13884, 341, 293, 733, 295, 352, 807, 341, 322, 527, 1065, 281, 652, 988, 50710], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2133, "seek": 788948, "start": 7896.4, "end": 7900.719999999999, "text": " this makes sense. But what this little script does is allow the user to type in", "tokens": [50710, 341, 1669, 2020, 13, 583, 437, 341, 707, 5755, 775, 307, 2089, 264, 4195, 281, 2010, 294, 50926], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2134, "seek": 788948, "start": 7900.719999999999, "end": 7905.2, "text": " some numbers. So the septal length width, and I guess petal length and width, and", "tokens": [50926, 512, 3547, 13, 407, 264, 23891, 304, 4641, 11402, 11, 293, 286, 2041, 3817, 304, 4641, 293, 11402, 11, 293, 51150], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2135, "seek": 788948, "start": 7905.2, "end": 7909.839999999999, "text": " then it will spit out to you what the predicted class of that flower is. So we", "tokens": [51150, 550, 309, 486, 22127, 484, 281, 291, 437, 264, 19147, 1508, 295, 300, 8617, 307, 13, 407, 321, 51382], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2136, "seek": 788948, "start": 7909.839999999999, "end": 7914.2, "text": " could do a prediction on every single one of our data points like we did", "tokens": [51382, 727, 360, 257, 17630, 322, 633, 2167, 472, 295, 527, 1412, 2793, 411, 321, 630, 51600], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2137, "seek": 788948, "start": 7914.2, "end": 7916.799999999999, "text": " previously. And we already know how to do that. I showed you that with linear", "tokens": [51600, 8046, 13, 400, 321, 1217, 458, 577, 281, 360, 300, 13, 286, 4712, 291, 300, 365, 8213, 51730], "temperature": 0.0, "avg_logprob": -0.11430913688492601, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.028429972007870674}, {"id": 2138, "seek": 791680, "start": 7916.84, "end": 7921.76, "text": " regression. But here I just wanted to do it on one entry. So what do we do? So I", "tokens": [50366, 24590, 13, 583, 510, 286, 445, 1415, 281, 360, 309, 322, 472, 8729, 13, 407, 437, 360, 321, 360, 30, 407, 286, 50612], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2139, "seek": 791680, "start": 7921.76, "end": 7926.360000000001, "text": " start by creating a input function, it's very basic, we have batch size 256. All", "tokens": [50612, 722, 538, 4084, 257, 4846, 2445, 11, 309, 311, 588, 3875, 11, 321, 362, 15245, 2744, 38882, 13, 1057, 50842], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2140, "seek": 791680, "start": 7926.360000000001, "end": 7930.28, "text": " we do is we give some features, and we created data set from those features", "tokens": [50842, 321, 360, 307, 321, 976, 512, 4122, 11, 293, 321, 2942, 1412, 992, 490, 729, 4122, 51038], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2141, "seek": 791680, "start": 7930.320000000001, "end": 7935.12, "text": " that's a dict and then dot batch and the batch size. So what this is doing is", "tokens": [51040, 300, 311, 257, 12569, 293, 550, 5893, 15245, 293, 264, 15245, 2744, 13, 407, 437, 341, 307, 884, 307, 51280], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2142, "seek": 791680, "start": 7935.12, "end": 7939.08, "text": " notice we don't give any y value, right? We don't give any labels. The reason we", "tokens": [51280, 3449, 321, 500, 380, 976, 604, 288, 2158, 11, 558, 30, 492, 500, 380, 976, 604, 16949, 13, 440, 1778, 321, 51478], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2143, "seek": 791680, "start": 7939.08, "end": 7942.88, "text": " do we don't do that is because when we're making a prediction, we don't know", "tokens": [51478, 360, 321, 500, 380, 360, 300, 307, 570, 562, 321, 434, 1455, 257, 17630, 11, 321, 500, 380, 458, 51668], "temperature": 0.0, "avg_logprob": -0.12338371276855468, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0113298911601305}, {"id": 2144, "seek": 794288, "start": 7942.88, "end": 7947.08, "text": " the label, right? Like we actually want that the model to give us the answer. So", "tokens": [50364, 264, 7645, 11, 558, 30, 1743, 321, 767, 528, 300, 264, 2316, 281, 976, 505, 264, 1867, 13, 407, 50574], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2145, "seek": 794288, "start": 7947.08, "end": 7950.4800000000005, "text": " here I wrote down the features, I created a predict dictionary, just because I'm", "tokens": [50574, 510, 286, 4114, 760, 264, 4122, 11, 286, 2942, 257, 6069, 25890, 11, 445, 570, 286, 478, 50744], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2146, "seek": 794288, "start": 7950.4800000000005, "end": 7953.88, "text": " going to add things to it. And then I just prompted here with a print statement,", "tokens": [50744, 516, 281, 909, 721, 281, 309, 13, 400, 550, 286, 445, 31042, 510, 365, 257, 4482, 5629, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2147, "seek": 794288, "start": 7953.88, "end": 7959.36, "text": " please type numeric values as prompted. So for feature and feature, valid equals", "tokens": [50914, 1767, 2010, 7866, 299, 4190, 382, 31042, 13, 407, 337, 4111, 293, 4111, 11, 7363, 6915, 51188], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2148, "seek": 794288, "start": 7959.36, "end": 7963.28, "text": " true, well valid, valid equals input feature colon. So this just means what", "tokens": [51188, 2074, 11, 731, 7363, 11, 7363, 6915, 4846, 4111, 8255, 13, 407, 341, 445, 1355, 437, 51384], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2149, "seek": 794288, "start": 7963.28, "end": 7966.12, "text": " we're going to do is for each feature, we're going to wait to get some valid", "tokens": [51384, 321, 434, 516, 281, 360, 307, 337, 1184, 4111, 11, 321, 434, 516, 281, 1699, 281, 483, 512, 7363, 51526], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2150, "seek": 794288, "start": 7966.12, "end": 7970.96, "text": " response. Once we get some valid response, what we're going to do is add that to", "tokens": [51526, 4134, 13, 3443, 321, 483, 512, 7363, 4134, 11, 437, 321, 434, 516, 281, 360, 307, 909, 300, 281, 51768], "temperature": 0.0, "avg_logprob": -0.1334409032549177, "compression_ratio": 1.901023890784983, "no_speech_prob": 0.046022284775972366}, {"id": 2151, "seek": 797096, "start": 7971.0, "end": 7975.0, "text": " our dictionary. So we're going to say predict feature. So whatever that feature", "tokens": [50366, 527, 25890, 13, 407, 321, 434, 516, 281, 584, 6069, 4111, 13, 407, 2035, 300, 4111, 50566], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2152, "seek": 797096, "start": 7975.0, "end": 7980.28, "text": " was, so septal length, septal width, petal length or pep, petal width is equal", "tokens": [50566, 390, 11, 370, 23891, 304, 4641, 11, 23891, 304, 11402, 11, 3817, 304, 4641, 420, 520, 79, 11, 3817, 304, 11402, 307, 2681, 50830], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2153, "seek": 797096, "start": 7980.28, "end": 7985.56, "text": " to a list that has in this instance, whatever that value was. Now the reason", "tokens": [50830, 281, 257, 1329, 300, 575, 294, 341, 5197, 11, 2035, 300, 2158, 390, 13, 823, 264, 1778, 51094], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2154, "seek": 797096, "start": 7985.56, "end": 7989.36, "text": " we need to do this is because again, the predict method from TensorFlow works", "tokens": [51094, 321, 643, 281, 360, 341, 307, 570, 797, 11, 264, 6069, 3170, 490, 37624, 1985, 51284], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2155, "seek": 797096, "start": 7989.36, "end": 7993.28, "text": " on predicting for multiple things, not just one value. So even if we only have", "tokens": [51284, 322, 32884, 337, 3866, 721, 11, 406, 445, 472, 2158, 13, 407, 754, 498, 321, 787, 362, 51480], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2156, "seek": 797096, "start": 7993.28, "end": 7996.8, "text": " one value we want to predict for it, we need to put it inside of a list because", "tokens": [51480, 472, 2158, 321, 528, 281, 6069, 337, 309, 11, 321, 643, 281, 829, 309, 1854, 295, 257, 1329, 570, 51656], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2157, "seek": 797096, "start": 7996.8, "end": 8000.32, "text": " it's expecting the fact that we will probably have more than one value in", "tokens": [51656, 309, 311, 9650, 264, 1186, 300, 321, 486, 1391, 362, 544, 813, 472, 2158, 294, 51832], "temperature": 0.0, "avg_logprob": -0.11711437114770862, "compression_ratio": 1.8892733564013842, "no_speech_prob": 0.0038242535665631294}, {"id": 2158, "seek": 800032, "start": 8000.36, "end": 8003.92, "text": " which we would have multiple values in the list, right, each representing a", "tokens": [50366, 597, 321, 576, 362, 3866, 4190, 294, 264, 1329, 11, 558, 11, 1184, 13460, 257, 50544], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2159, "seek": 800032, "start": 8003.92, "end": 8008.32, "text": " different row or a new flower to make a prediction for. Okay, now we say", "tokens": [50544, 819, 5386, 420, 257, 777, 8617, 281, 652, 257, 17630, 337, 13, 1033, 11, 586, 321, 584, 50764], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2160, "seek": 800032, "start": 8008.32, "end": 8011.719999999999, "text": " predictions equals classifier dot predict. And then in this case, we have input", "tokens": [50764, 21264, 6915, 1508, 9902, 5893, 6069, 13, 400, 550, 294, 341, 1389, 11, 321, 362, 4846, 50934], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2161, "seek": 800032, "start": 8011.719999999999, "end": 8016.24, "text": " function lambda input function predict, which is this input function up here. And", "tokens": [50934, 2445, 13607, 4846, 2445, 6069, 11, 597, 307, 341, 4846, 2445, 493, 510, 13, 400, 51160], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2162, "seek": 800032, "start": 8016.24, "end": 8020.28, "text": " then we say for prediction dictionaries, because remember, every prediction comes", "tokens": [51160, 550, 321, 584, 337, 17630, 22352, 4889, 11, 570, 1604, 11, 633, 17630, 1487, 51362], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2163, "seek": 800032, "start": 8020.28, "end": 8024.2, "text": " back as a dictionary in predictions, we'll say the class ID is equal to", "tokens": [51362, 646, 382, 257, 25890, 294, 21264, 11, 321, 603, 584, 264, 1508, 7348, 307, 2681, 281, 51558], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2164, "seek": 800032, "start": 8024.24, "end": 8029.16, "text": " whatever the class IDs of the prediction dictionary at zero. And these are", "tokens": [51560, 2035, 264, 1508, 48212, 295, 264, 17630, 25890, 412, 4018, 13, 400, 613, 366, 51806], "temperature": 0.0, "avg_logprob": -0.11185328941035076, "compression_ratio": 1.9388489208633093, "no_speech_prob": 0.0028006741777062416}, {"id": 2165, "seek": 802916, "start": 8029.16, "end": 8034.84, "text": " simply what I don't know exactly how to explain this. We'll look at in a second,", "tokens": [50364, 2935, 437, 286, 500, 380, 458, 2293, 577, 281, 2903, 341, 13, 492, 603, 574, 412, 294, 257, 1150, 11, 50648], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2166, "seek": 802916, "start": 8034.84, "end": 8038.08, "text": " and I'll go through that. And then we have the probability is equal to the", "tokens": [50648, 293, 286, 603, 352, 807, 300, 13, 400, 550, 321, 362, 264, 8482, 307, 2681, 281, 264, 50810], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2167, "seek": 802916, "start": 8038.08, "end": 8043.639999999999, "text": " prediction dictionary probabilities of class ID. Okay, then we're going to say", "tokens": [50810, 17630, 25890, 33783, 295, 1508, 7348, 13, 1033, 11, 550, 321, 434, 516, 281, 584, 51088], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2168, "seek": 802916, "start": 8043.639999999999, "end": 8047.96, "text": " print prediction is we're going to do this weird format thing, I just stole", "tokens": [51088, 4482, 17630, 307, 321, 434, 516, 281, 360, 341, 3657, 7877, 551, 11, 286, 445, 16326, 51304], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2169, "seek": 802916, "start": 8047.96, "end": 8052.36, "text": " this from TensorFlow. And it's going to be the species at the class ID, and then", "tokens": [51304, 341, 490, 37624, 13, 400, 309, 311, 516, 281, 312, 264, 6172, 412, 264, 1508, 7348, 11, 293, 550, 51524], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2170, "seek": 802916, "start": 8052.36, "end": 8055.639999999999, "text": " 100 times probability, which will give us actual integer value, we're going to", "tokens": [51524, 2319, 1413, 8482, 11, 597, 486, 976, 505, 3539, 24922, 2158, 11, 321, 434, 516, 281, 51688], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2171, "seek": 802916, "start": 8055.639999999999, "end": 8058.599999999999, "text": " digest this, but let's run this right now and have a look. So please type", "tokens": [51688, 13884, 341, 11, 457, 718, 311, 1190, 341, 558, 586, 293, 362, 257, 574, 13, 407, 1767, 2010, 51836], "temperature": 0.0, "avg_logprob": -0.128807931050767, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.001700613647699356}, {"id": 2172, "seek": 805860, "start": 8058.64, "end": 8064.160000000001, "text": " numeric values as prompted septal length, let's type like 2.4, septal width 2.6,", "tokens": [50366, 7866, 299, 4190, 382, 31042, 23891, 304, 4641, 11, 718, 311, 2010, 411, 568, 13, 19, 11, 23891, 304, 11402, 568, 13, 21, 11, 50642], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2173, "seek": 805860, "start": 8064.6, "end": 8070.320000000001, "text": " petal width, let's just say that's like 6.5. And yeah, petal width like 6.3. Okay,", "tokens": [50664, 3817, 304, 11402, 11, 718, 311, 445, 584, 300, 311, 411, 1386, 13, 20, 13, 400, 1338, 11, 3817, 304, 11402, 411, 1386, 13, 18, 13, 1033, 11, 50950], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2174, "seek": 805860, "start": 8070.360000000001, "end": 8074.52, "text": " so then it calls this and it says prediction is virginica, I guess that's", "tokens": [50952, 370, 550, 309, 5498, 341, 293, 309, 1619, 17630, 307, 26404, 2262, 11, 286, 2041, 300, 311, 51160], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2175, "seek": 805860, "start": 8074.56, "end": 8079.84, "text": " the the class we're going with. And it says that's an 83 or 86.3% chance that", "tokens": [51162, 264, 264, 1508, 321, 434, 516, 365, 13, 400, 309, 1619, 300, 311, 364, 30997, 420, 26687, 13, 18, 4, 2931, 300, 51426], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2176, "seek": 805860, "start": 8079.84, "end": 8083.92, "text": " that is the prediction. So yeah, that is how that works. So that's what this", "tokens": [51426, 300, 307, 264, 17630, 13, 407, 1338, 11, 300, 307, 577, 300, 1985, 13, 407, 300, 311, 437, 341, 51630], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2177, "seek": 805860, "start": 8083.92, "end": 8087.0, "text": " does. I wanted to give a little script, I wrote most of this, I mean, I stole", "tokens": [51630, 775, 13, 286, 1415, 281, 976, 257, 707, 5755, 11, 286, 4114, 881, 295, 341, 11, 286, 914, 11, 286, 16326, 51784], "temperature": 0.0, "avg_logprob": -0.15231177231361126, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0038240053690969944}, {"id": 2178, "seek": 808700, "start": 8087.04, "end": 8090.76, "text": " some of this from TensorFlow. But just to show you how we actually predict on", "tokens": [50366, 512, 295, 341, 490, 37624, 13, 583, 445, 281, 855, 291, 577, 321, 767, 6069, 322, 50552], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2179, "seek": 808700, "start": 8090.76, "end": 8095.36, "text": " one value. So let's look at these prediction dictionary, because I just", "tokens": [50552, 472, 2158, 13, 407, 718, 311, 574, 412, 613, 17630, 25890, 11, 570, 286, 445, 50782], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2180, "seek": 808700, "start": 8095.36, "end": 8099.08, "text": " want to show you what one of them actually is. So I'm going to say print,", "tokens": [50782, 528, 281, 855, 291, 437, 472, 295, 552, 767, 307, 13, 407, 286, 478, 516, 281, 584, 4482, 11, 50968], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2181, "seek": 808700, "start": 8100.24, "end": 8103.76, "text": " pred underscore dict. And then this will allow me to actually walk through what", "tokens": [51026, 3852, 37556, 12569, 13, 400, 550, 341, 486, 2089, 385, 281, 767, 1792, 807, 437, 51202], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2182, "seek": 808700, "start": 8103.76, "end": 8108.4, "text": " class IDs are probabilities are and how I've kind of done this. So let's run", "tokens": [51202, 1508, 48212, 366, 33783, 366, 293, 577, 286, 600, 733, 295, 1096, 341, 13, 407, 718, 311, 1190, 51434], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2183, "seek": 808700, "start": 8108.4, "end": 8113.64, "text": " this up to length. Okay, let's just go like 1.4, 2.3. I don't know what these", "tokens": [51434, 341, 493, 281, 4641, 13, 1033, 11, 718, 311, 445, 352, 411, 502, 13, 19, 11, 568, 13, 18, 13, 286, 500, 380, 458, 437, 613, 51696], "temperature": 0.0, "avg_logprob": -0.12724755449992856, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.004331142175942659}, {"id": 2184, "seek": 811364, "start": 8113.64, "end": 8119.84, "text": " values are going to end up being. And we get prediction is same one with 77.1%,", "tokens": [50364, 4190, 366, 516, 281, 917, 493, 885, 13, 400, 321, 483, 17630, 307, 912, 472, 365, 25546, 13, 16, 8923, 50674], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2185, "seek": 811364, "start": 8119.84, "end": 8123.68, "text": " which makes sense, because these values are similar kind of in difference to what", "tokens": [50674, 597, 1669, 2020, 11, 570, 613, 4190, 366, 2531, 733, 295, 294, 2649, 281, 437, 50866], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2186, "seek": 811364, "start": 8123.68, "end": 8127.360000000001, "text": " I did before. Okay, so this is the dictionary. So let's look for what we", "tokens": [50866, 286, 630, 949, 13, 1033, 11, 370, 341, 307, 264, 25890, 13, 407, 718, 311, 574, 337, 437, 321, 51050], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2187, "seek": 811364, "start": 8127.360000000001, "end": 8131.4800000000005, "text": " were looking for. So probabilities notice we get three probabilities, one for", "tokens": [51050, 645, 1237, 337, 13, 407, 33783, 3449, 321, 483, 1045, 33783, 11, 472, 337, 51256], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2188, "seek": 811364, "start": 8131.52, "end": 8135.320000000001, "text": " each of the different classes. So we can actually say what, you know, the", "tokens": [51258, 1184, 295, 264, 819, 5359, 13, 407, 321, 393, 767, 584, 437, 11, 291, 458, 11, 264, 51448], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2189, "seek": 811364, "start": 8135.88, "end": 8140.76, "text": " percentages for every single one of the predictions. Then what we have is class", "tokens": [51476, 42270, 337, 633, 2167, 472, 295, 264, 21264, 13, 1396, 437, 321, 362, 307, 1508, 51720], "temperature": 0.0, "avg_logprob": -0.1380374975371779, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0037070317193865776}, {"id": 2190, "seek": 814076, "start": 8140.8, "end": 8146.92, "text": " IDs. Now class IDs, what this does is tell us what class ID, it predicts is", "tokens": [50366, 48212, 13, 823, 1508, 48212, 11, 437, 341, 775, 307, 980, 505, 437, 1508, 7348, 11, 309, 6069, 82, 307, 50672], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2191, "seek": 814076, "start": 8146.96, "end": 8150.6, "text": " actually the flower, right? So here it says two, which means that this", "tokens": [50674, 767, 264, 8617, 11, 558, 30, 407, 510, 309, 1619, 732, 11, 597, 1355, 300, 341, 50856], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2192, "seek": 814076, "start": 8150.6, "end": 8155.0, "text": " probability at 77%. That's that index two in this array, right? So that's why", "tokens": [50856, 8482, 412, 25546, 6856, 663, 311, 300, 8186, 732, 294, 341, 10225, 11, 558, 30, 407, 300, 311, 983, 51076], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2193, "seek": 814076, "start": 8155.0, "end": 8159.68, "text": " this value is two. So it's saying that that class is to it thinks it's class", "tokens": [51076, 341, 2158, 307, 732, 13, 407, 309, 311, 1566, 300, 300, 1508, 307, 281, 309, 7309, 309, 311, 1508, 51310], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2194, "seek": 814076, "start": 8159.68, "end": 8165.280000000001, "text": " two, like that's whatever was encoded in our system is two. And that's how that", "tokens": [51310, 732, 11, 411, 300, 311, 2035, 390, 2058, 12340, 294, 527, 1185, 307, 732, 13, 400, 300, 311, 577, 300, 51590], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2195, "seek": 814076, "start": 8165.280000000001, "end": 8170.400000000001, "text": " works. So that's how I know, which one to print out is because this tells me", "tokens": [51590, 1985, 13, 407, 300, 311, 577, 286, 458, 11, 597, 472, 281, 4482, 484, 307, 570, 341, 5112, 385, 51846], "temperature": 0.0, "avg_logprob": -0.12174901436633012, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.007815008983016014}, {"id": 2196, "seek": 817040, "start": 8170.4, "end": 8175.16, "text": " it's class two. And I know for making this list all the way back up here, if I", "tokens": [50364, 309, 311, 1508, 732, 13, 400, 286, 458, 337, 1455, 341, 1329, 439, 264, 636, 646, 493, 510, 11, 498, 286, 50602], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2197, "seek": 817040, "start": 8175.16, "end": 8180.92, "text": " can get rid of this output. Where is it? When I say species, that number two is", "tokens": [50602, 393, 483, 3973, 295, 341, 5598, 13, 2305, 307, 309, 30, 1133, 286, 584, 6172, 11, 300, 1230, 732, 307, 50890], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2198, "seek": 817040, "start": 8180.92, "end": 8185.2, "text": " virginica, or I guess that's how you say it. So that is what the classification", "tokens": [50890, 26404, 2262, 11, 420, 286, 2041, 300, 311, 577, 291, 584, 309, 13, 407, 300, 307, 437, 264, 21538, 51104], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2199, "seek": 817040, "start": 8185.2, "end": 8187.839999999999, "text": " is. So that's what the prediction is. So that's how I do that. And that's how", "tokens": [51104, 307, 13, 407, 300, 311, 437, 264, 17630, 307, 13, 407, 300, 311, 577, 286, 360, 300, 13, 400, 300, 311, 577, 51236], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2200, "seek": 817040, "start": 8187.839999999999, "end": 8193.84, "text": " that works. Okay, so I think that is pretty much it for actually classification. So", "tokens": [51236, 300, 1985, 13, 1033, 11, 370, 286, 519, 300, 307, 1238, 709, 309, 337, 767, 21538, 13, 407, 51536], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2201, "seek": 817040, "start": 8193.84, "end": 8196.8, "text": " it was pretty basic. I'm going to go and see if there's anything else that I did", "tokens": [51536, 309, 390, 1238, 3875, 13, 286, 478, 516, 281, 352, 293, 536, 498, 456, 311, 1340, 1646, 300, 286, 630, 51684], "temperature": 0.0, "avg_logprob": -0.12000663244902197, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.0019265952287241817}, {"id": 2202, "seek": 819680, "start": 8196.84, "end": 8201.16, "text": " for classification in here. Okay, so here, I just put some examples. So here's", "tokens": [50366, 337, 21538, 294, 510, 13, 1033, 11, 370, 510, 11, 286, 445, 829, 512, 5110, 13, 407, 510, 311, 50582], "temperature": 0.0, "avg_logprob": -0.13785170041597805, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.1602138876914978}, {"id": 2203, "seek": 819680, "start": 8201.16, "end": 8204.279999999999, "text": " some example input and expected classes. So you guys could try to do these if you", "tokens": [50582, 512, 1365, 4846, 293, 5176, 5359, 13, 407, 291, 1074, 727, 853, 281, 360, 613, 498, 291, 50738], "temperature": 0.0, "avg_logprob": -0.13785170041597805, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.1602138876914978}, {"id": 2204, "seek": 819680, "start": 8204.279999999999, "end": 8211.92, "text": " want. So for example, this one, septal length, septal width. So for 5.1, 3.3, 1.7", "tokens": [50738, 528, 13, 407, 337, 1365, 11, 341, 472, 11, 23891, 304, 4641, 11, 23891, 304, 11402, 13, 407, 337, 1025, 13, 16, 11, 805, 13, 18, 11, 502, 13, 22, 51120], "temperature": 0.0, "avg_logprob": -0.13785170041597805, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.1602138876914978}, {"id": 2205, "seek": 819680, "start": 8211.92, "end": 8218.279999999999, "text": " and 0.5, the output should be Satosa. For 5.9, 3.0, 4.2, 1.5, it should be this", "tokens": [51120, 293, 1958, 13, 20, 11, 264, 5598, 820, 312, 5344, 6447, 13, 1171, 1025, 13, 24, 11, 805, 13, 15, 11, 1017, 13, 17, 11, 502, 13, 20, 11, 309, 820, 312, 341, 51438], "temperature": 0.0, "avg_logprob": -0.13785170041597805, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.1602138876914978}, {"id": 2206, "seek": 819680, "start": 8218.279999999999, "end": 8222.72, "text": " one. And then obviously this for this, just so you guys can mess with them if you", "tokens": [51438, 472, 13, 400, 550, 2745, 341, 337, 341, 11, 445, 370, 291, 1074, 393, 2082, 365, 552, 498, 291, 51660], "temperature": 0.0, "avg_logprob": -0.13785170041597805, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.1602138876914978}, {"id": 2207, "seek": 822272, "start": 8222.72, "end": 8227.08, "text": " want. But that's pretty much it for classification. And now on to clustering.", "tokens": [50364, 528, 13, 583, 300, 311, 1238, 709, 309, 337, 21538, 13, 400, 586, 322, 281, 596, 48673, 13, 50582], "temperature": 0.0, "avg_logprob": -0.10683770860944475, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.03409428149461746}, {"id": 2208, "seek": 822272, "start": 8228.679999999998, "end": 8233.16, "text": " Okay, so now we're moving on to clustering. Now clustering is the first unsupervised", "tokens": [50662, 1033, 11, 370, 586, 321, 434, 2684, 322, 281, 596, 48673, 13, 823, 596, 48673, 307, 264, 700, 2693, 12879, 24420, 50886], "temperature": 0.0, "avg_logprob": -0.10683770860944475, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.03409428149461746}, {"id": 2209, "seek": 822272, "start": 8233.519999999999, "end": 8237.599999999999, "text": " learning algorithm that we're going to see in this series. And it's very powerful. Now", "tokens": [50904, 2539, 9284, 300, 321, 434, 516, 281, 536, 294, 341, 2638, 13, 400, 309, 311, 588, 4005, 13, 823, 51108], "temperature": 0.0, "avg_logprob": -0.10683770860944475, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.03409428149461746}, {"id": 2210, "seek": 822272, "start": 8237.599999999999, "end": 8242.72, "text": " clustering only works for a very specific set of problems. And you use clustering", "tokens": [51108, 596, 48673, 787, 1985, 337, 257, 588, 2685, 992, 295, 2740, 13, 400, 291, 764, 596, 48673, 51364], "temperature": 0.0, "avg_logprob": -0.10683770860944475, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.03409428149461746}, {"id": 2211, "seek": 822272, "start": 8242.72, "end": 8246.76, "text": " when you have a bunch of input information or features, you don't have any labels or", "tokens": [51364, 562, 291, 362, 257, 3840, 295, 4846, 1589, 420, 4122, 11, 291, 500, 380, 362, 604, 16949, 420, 51566], "temperature": 0.0, "avg_logprob": -0.10683770860944475, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.03409428149461746}, {"id": 2212, "seek": 824676, "start": 8246.76, "end": 8259.44, "text": " open information. Essentially, what clustering does is finds clusters of like data points", "tokens": [50364, 1269, 1589, 13, 23596, 11, 437, 596, 48673, 775, 307, 10704, 23313, 295, 411, 1412, 2793, 50998], "temperature": 0.0, "avg_logprob": -0.1243161925350327, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.004609295167028904}, {"id": 2213, "seek": 824676, "start": 8259.64, "end": 8263.800000000001, "text": " and tells you the location of those clusters. So you give a bunch of training data, you", "tokens": [51008, 293, 5112, 291, 264, 4914, 295, 729, 23313, 13, 407, 291, 976, 257, 3840, 295, 3097, 1412, 11, 291, 51216], "temperature": 0.0, "avg_logprob": -0.1243161925350327, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.004609295167028904}, {"id": 2214, "seek": 824676, "start": 8263.800000000001, "end": 8268.2, "text": " can pick how many clusters you want to find. So maybe we're going to be classifying digits", "tokens": [51216, 393, 1888, 577, 867, 23313, 291, 528, 281, 915, 13, 407, 1310, 321, 434, 516, 281, 312, 1508, 5489, 27011, 51436], "temperature": 0.0, "avg_logprob": -0.1243161925350327, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.004609295167028904}, {"id": 2215, "seek": 824676, "start": 8268.2, "end": 8272.08, "text": " right handwritten digits using k means clustering. In that instance, we would have 10", "tokens": [51436, 558, 1011, 26859, 27011, 1228, 350, 1355, 596, 48673, 13, 682, 300, 5197, 11, 321, 576, 362, 1266, 51630], "temperature": 0.0, "avg_logprob": -0.1243161925350327, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.004609295167028904}, {"id": 2216, "seek": 827208, "start": 8272.08, "end": 8276.76, "text": " different clusters for the digits zero through nine. And you pass all this information and", "tokens": [50364, 819, 23313, 337, 264, 27011, 4018, 807, 4949, 13, 400, 291, 1320, 439, 341, 1589, 293, 50598], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2217, "seek": 827208, "start": 8276.76, "end": 8281.16, "text": " the algorithm actually finds those clusters in the data set for you. We're going to walk", "tokens": [50598, 264, 9284, 767, 10704, 729, 23313, 294, 264, 1412, 992, 337, 291, 13, 492, 434, 516, 281, 1792, 50818], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2218, "seek": 827208, "start": 8281.16, "end": 8285.52, "text": " through an example that'll make sense. But I just want to quickly explain the basic", "tokens": [50818, 807, 364, 1365, 300, 603, 652, 2020, 13, 583, 286, 445, 528, 281, 2661, 2903, 264, 3875, 51036], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2219, "seek": 827208, "start": 8285.56, "end": 8290.6, "text": " algorithm behind k means essentially the set of steps. These are going to walk you through", "tokens": [51038, 9284, 2261, 350, 1355, 4476, 264, 992, 295, 4439, 13, 1981, 366, 516, 281, 1792, 291, 807, 51290], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2220, "seek": 827208, "start": 8290.6, "end": 8296.039999999999, "text": " them and with a visual example. So we're going to start by randomly picking k points to", "tokens": [51290, 552, 293, 365, 257, 5056, 1365, 13, 407, 321, 434, 516, 281, 722, 538, 16979, 8867, 350, 2793, 281, 51562], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2221, "seek": 827208, "start": 8296.039999999999, "end": 8301.72, "text": " place k centroids. Now a centroid stands for where our current cluster is kind of", "tokens": [51562, 1081, 350, 24607, 3742, 13, 823, 257, 1489, 6490, 7382, 337, 689, 527, 2190, 13630, 307, 733, 295, 51846], "temperature": 0.0, "avg_logprob": -0.1371249992306493, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.05339575931429863}, {"id": 2222, "seek": 830172, "start": 8301.76, "end": 8305.88, "text": " defined. And we'll see it in a second. The next step is we're going to assign all of the", "tokens": [50366, 7642, 13, 400, 321, 603, 536, 309, 294, 257, 1150, 13, 440, 958, 1823, 307, 321, 434, 516, 281, 6269, 439, 295, 264, 50572], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2223, "seek": 830172, "start": 8305.88, "end": 8310.64, "text": " data points to the centroids by distance. So actually, now that I'm talking about this,", "tokens": [50572, 1412, 2793, 281, 264, 24607, 3742, 538, 4560, 13, 407, 767, 11, 586, 300, 286, 478, 1417, 466, 341, 11, 50810], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2224, "seek": 830172, "start": 8310.64, "end": 8313.92, "text": " I think it just makes more sense to get right into the example, because if I keep talking", "tokens": [50810, 286, 519, 309, 445, 1669, 544, 2020, 281, 483, 558, 666, 264, 1365, 11, 570, 498, 286, 1066, 1417, 50974], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2225, "seek": 830172, "start": 8313.92, "end": 8316.48, "text": " about this, you guys are probably just going to be confused, although I might come back", "tokens": [50974, 466, 341, 11, 291, 1074, 366, 1391, 445, 516, 281, 312, 9019, 11, 4878, 286, 1062, 808, 646, 51102], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2226, "seek": 830172, "start": 8316.48, "end": 8321.76, "text": " to this just to reference those points. Okay, so let's create a little graph like this in", "tokens": [51102, 281, 341, 445, 281, 6408, 729, 2793, 13, 1033, 11, 370, 718, 311, 1884, 257, 707, 4295, 411, 341, 294, 51366], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2227, "seek": 830172, "start": 8321.76, "end": 8326.24, "text": " two dimensions for our basic example. And let's make some data points here. So I'm just", "tokens": [51366, 732, 12819, 337, 527, 3875, 1365, 13, 400, 718, 311, 652, 512, 1412, 2793, 510, 13, 407, 286, 478, 445, 51590], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2228, "seek": 830172, "start": 8326.24, "end": 8329.519999999999, "text": " going to make them all read. And you're going to notice that I'm going to make this kind", "tokens": [51590, 516, 281, 652, 552, 439, 1401, 13, 400, 291, 434, 516, 281, 3449, 300, 286, 478, 516, 281, 652, 341, 733, 51754], "temperature": 0.0, "avg_logprob": -0.08121194657246777, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0060968659818172455}, {"id": 2229, "seek": 832952, "start": 8329.52, "end": 8333.960000000001, "text": " of easier for ourselves by putting them in like their own unique little groups, right?", "tokens": [50364, 295, 3571, 337, 4175, 538, 3372, 552, 294, 411, 641, 1065, 3845, 707, 3935, 11, 558, 30, 50586], "temperature": 0.0, "avg_logprob": -0.15507667178199405, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.012819832190871239}, {"id": 2230, "seek": 832952, "start": 8333.960000000001, "end": 8340.12, "text": " So actually, we'll add one up here. Then we can add some down here and down here. Now", "tokens": [50586, 407, 767, 11, 321, 603, 909, 472, 493, 510, 13, 1396, 321, 393, 909, 512, 760, 510, 293, 760, 510, 13, 823, 50894], "temperature": 0.0, "avg_logprob": -0.15507667178199405, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.012819832190871239}, {"id": 2231, "seek": 832952, "start": 8340.12, "end": 8344.6, "text": " the algorithm starts for k means clustering. And you guys understand how this works as", "tokens": [50894, 264, 9284, 3719, 337, 350, 1355, 596, 48673, 13, 400, 291, 1074, 1223, 577, 341, 1985, 382, 51118], "temperature": 0.0, "avg_logprob": -0.15507667178199405, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.012819832190871239}, {"id": 2232, "seek": 832952, "start": 8344.6, "end": 8351.04, "text": " we continue by randomly picking k centroids. Now I'm going to denote a centroid by a little", "tokens": [51118, 321, 2354, 538, 16979, 8867, 350, 24607, 3742, 13, 823, 286, 478, 516, 281, 45708, 257, 1489, 6490, 538, 257, 707, 51440], "temperature": 0.0, "avg_logprob": -0.15507667178199405, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.012819832190871239}, {"id": 2233, "seek": 832952, "start": 8351.04, "end": 8357.32, "text": " filled in triangle like this. And essentially what these are is where these different clusters", "tokens": [51440, 6412, 294, 13369, 411, 341, 13, 400, 4476, 437, 613, 366, 307, 689, 613, 819, 23313, 51754], "temperature": 0.0, "avg_logprob": -0.15507667178199405, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.012819832190871239}, {"id": 2234, "seek": 835732, "start": 8357.36, "end": 8362.279999999999, "text": " currently exist. So we start by randomly picking k, which is what we've defined. So like me", "tokens": [50366, 4362, 2514, 13, 407, 321, 722, 538, 16979, 8867, 350, 11, 597, 307, 437, 321, 600, 7642, 13, 407, 411, 385, 50612], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2235, "seek": 835732, "start": 8362.279999999999, "end": 8367.36, "text": " in this instance, that we're going to say k equals three, k centroid, wherever. So maybe", "tokens": [50612, 294, 341, 5197, 11, 300, 321, 434, 516, 281, 584, 350, 6915, 1045, 11, 350, 1489, 6490, 11, 8660, 13, 407, 1310, 50866], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2236, "seek": 835732, "start": 8367.36, "end": 8371.68, "text": " we put one, you know, somewhere like here, you know, I might not bother filling these", "tokens": [50866, 321, 829, 472, 11, 291, 458, 11, 4079, 411, 510, 11, 291, 458, 11, 286, 1062, 406, 8677, 10623, 613, 51082], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2237, "seek": 835732, "start": 8371.68, "end": 8375.48, "text": " in because they're going to take a while. Maybe we pull in here, maybe we end up putting", "tokens": [51082, 294, 570, 436, 434, 516, 281, 747, 257, 1339, 13, 2704, 321, 2235, 294, 510, 11, 1310, 321, 917, 493, 3372, 51272], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2238, "seek": 835732, "start": 8375.48, "end": 8380.92, "text": " one over here. Now, I've kind of put them close to where clusters are, but these are", "tokens": [51272, 472, 670, 510, 13, 823, 11, 286, 600, 733, 295, 829, 552, 1998, 281, 689, 23313, 366, 11, 457, 613, 366, 51544], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2239, "seek": 835732, "start": 8380.92, "end": 8387.08, "text": " going to be completely random. Now what happens next is each group, or each data point is", "tokens": [51544, 516, 281, 312, 2584, 4974, 13, 823, 437, 2314, 958, 307, 1184, 1594, 11, 420, 1184, 1412, 935, 307, 51852], "temperature": 0.0, "avg_logprob": -0.16294631405153137, "compression_ratio": 1.7966101694915255, "no_speech_prob": 0.26886141300201416}, {"id": 2240, "seek": 838708, "start": 8387.08, "end": 8392.52, "text": " assigned to a cluster by distance. So essentially, what we do is for every single data point", "tokens": [50364, 13279, 281, 257, 13630, 538, 4560, 13, 407, 4476, 11, 437, 321, 360, 307, 337, 633, 2167, 1412, 935, 50636], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2241, "seek": 838708, "start": 8392.52, "end": 8396.64, "text": " that we have, we find what's known as the Euclidean distance, or it actually could be", "tokens": [50636, 300, 321, 362, 11, 321, 915, 437, 311, 2570, 382, 264, 462, 1311, 31264, 282, 4560, 11, 420, 309, 767, 727, 312, 50842], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2242, "seek": 838708, "start": 8396.64, "end": 8400.76, "text": " a different distance, you'd use like Manhattan distance, if you guys know what that is, to", "tokens": [50842, 257, 819, 4560, 11, 291, 1116, 764, 411, 23633, 4560, 11, 498, 291, 1074, 458, 437, 300, 307, 11, 281, 51048], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2243, "seek": 838708, "start": 8400.76, "end": 8404.72, "text": " all of the centroids. So let's say we're looking at this data point here, what we do is find", "tokens": [51048, 439, 295, 264, 24607, 3742, 13, 407, 718, 311, 584, 321, 434, 1237, 412, 341, 1412, 935, 510, 11, 437, 321, 360, 307, 915, 51246], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2244, "seek": 838708, "start": 8404.72, "end": 8410.8, "text": " the distance to all of these different centroids. And we assign this data point to the closest", "tokens": [51246, 264, 4560, 281, 439, 295, 613, 819, 24607, 3742, 13, 400, 321, 6269, 341, 1412, 935, 281, 264, 13699, 51550], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2245, "seek": 838708, "start": 8410.8, "end": 8414.68, "text": " centroid. So the closest one by distance. Now in this instance is looking like it's", "tokens": [51550, 1489, 6490, 13, 407, 264, 13699, 472, 538, 4560, 13, 823, 294, 341, 5197, 307, 1237, 411, 309, 311, 51744], "temperature": 0.0, "avg_logprob": -0.10729808528927991, "compression_ratio": 2.0338345864661656, "no_speech_prob": 0.001324938260950148}, {"id": 2246, "seek": 841468, "start": 8414.68, "end": 8418.0, "text": " going to be a bit of a tie between this centroid and this centroid. But I'm going to give it", "tokens": [50364, 516, 281, 312, 257, 857, 295, 257, 7582, 1296, 341, 1489, 6490, 293, 341, 1489, 6490, 13, 583, 286, 478, 516, 281, 976, 309, 50530], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2247, "seek": 841468, "start": 8418.0, "end": 8423.720000000001, "text": " to the one on the left. So what we do is we're going to say this is now a part of this centroid.", "tokens": [50530, 281, 264, 472, 322, 264, 1411, 13, 407, 437, 321, 360, 307, 321, 434, 516, 281, 584, 341, 307, 586, 257, 644, 295, 341, 1489, 6490, 13, 50816], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2248, "seek": 841468, "start": 8423.720000000001, "end": 8428.0, "text": " So if I'm calling this like, let's just say this is centroid one, this is centroid two,", "tokens": [50816, 407, 498, 286, 478, 5141, 341, 411, 11, 718, 311, 445, 584, 341, 307, 1489, 6490, 472, 11, 341, 307, 1489, 6490, 732, 11, 51030], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2249, "seek": 841468, "start": 8428.0, "end": 8431.800000000001, "text": " and this is centroid three, then this now is going to be a part of centroid one, because", "tokens": [51030, 293, 341, 307, 1489, 6490, 1045, 11, 550, 341, 586, 307, 516, 281, 312, 257, 644, 295, 1489, 6490, 472, 11, 570, 51220], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2250, "seek": 841468, "start": 8431.800000000001, "end": 8435.64, "text": " it's closest to centroid one. And we can go through and we do this for every single data", "tokens": [51220, 309, 311, 13699, 281, 1489, 6490, 472, 13, 400, 321, 393, 352, 807, 293, 321, 360, 341, 337, 633, 2167, 1412, 51412], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2251, "seek": 841468, "start": 8435.64, "end": 8440.12, "text": " point. So obviously, we know all of these are going to be our ones, right? And we know", "tokens": [51412, 935, 13, 407, 2745, 11, 321, 458, 439, 295, 613, 366, 516, 281, 312, 527, 2306, 11, 558, 30, 400, 321, 458, 51636], "temperature": 0.0, "avg_logprob": -0.10765836145970728, "compression_ratio": 2.053030303030303, "no_speech_prob": 0.06753544509410858}, {"id": 2252, "seek": 844012, "start": 8440.160000000002, "end": 8444.36, "text": " these are going to be our two. So two, two, two. And then these are obviously going to", "tokens": [50366, 613, 366, 516, 281, 312, 527, 732, 13, 407, 732, 11, 732, 11, 732, 13, 400, 550, 613, 366, 2745, 516, 281, 50576], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2253, "seek": 844012, "start": 8444.36, "end": 8448.84, "text": " be our three. Now I'm actually just going to add a few other data points, because I want", "tokens": [50576, 312, 527, 1045, 13, 823, 286, 478, 767, 445, 516, 281, 909, 257, 1326, 661, 1412, 2793, 11, 570, 286, 528, 50800], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2254, "seek": 844012, "start": 8448.84, "end": 8455.0, "text": " to make this a little bit more sophisticated, almost, if that makes any sense. So add those", "tokens": [50800, 281, 652, 341, 257, 707, 857, 544, 16950, 11, 1920, 11, 498, 300, 1669, 604, 2020, 13, 407, 909, 729, 51108], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2255, "seek": 844012, "start": 8455.0, "end": 8458.92, "text": " data points here, we've been add one here, and that will give these labels. So these", "tokens": [51108, 1412, 2793, 510, 11, 321, 600, 668, 909, 472, 510, 11, 293, 300, 486, 976, 613, 16949, 13, 407, 613, 51304], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2256, "seek": 844012, "start": 8458.92, "end": 8462.08, "text": " ones are close. So I'm going to say this one's one, I'm going to say this one's two, I know", "tokens": [51304, 2306, 366, 1998, 13, 407, 286, 478, 516, 281, 584, 341, 472, 311, 472, 11, 286, 478, 516, 281, 584, 341, 472, 311, 732, 11, 286, 458, 51462], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2257, "seek": 844012, "start": 8462.08, "end": 8466.2, "text": " it's not closest to it. But just because I want to do that for now, we'll say two for", "tokens": [51462, 309, 311, 406, 13699, 281, 309, 13, 583, 445, 570, 286, 528, 281, 360, 300, 337, 586, 11, 321, 603, 584, 732, 337, 51668], "temperature": 0.0, "avg_logprob": -0.14785704320790816, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.19676652550697327}, {"id": 2258, "seek": 846620, "start": 8466.2, "end": 8470.12, "text": " that. And we'll say three here. Okay, so now that we've done that, we've labeled all these", "tokens": [50364, 300, 13, 400, 321, 603, 584, 1045, 510, 13, 1033, 11, 370, 586, 300, 321, 600, 1096, 300, 11, 321, 600, 21335, 439, 613, 50560], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2259, "seek": 846620, "start": 8470.12, "end": 8475.76, "text": " points, what we do is we now move these centroids that we've defined into the middle of all", "tokens": [50560, 2793, 11, 437, 321, 360, 307, 321, 586, 1286, 613, 1489, 340, 3742, 300, 321, 600, 7642, 666, 264, 2808, 295, 439, 50842], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2260, "seek": 846620, "start": 8475.76, "end": 8481.800000000001, "text": " of their data points. So what I do is I essentially find it's called center of mass, the center", "tokens": [50842, 295, 641, 1412, 2793, 13, 407, 437, 286, 360, 307, 286, 4476, 915, 309, 311, 1219, 3056, 295, 2758, 11, 264, 3056, 51144], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2261, "seek": 846620, "start": 8481.800000000001, "end": 8486.12, "text": " of mass between all of the data points that are labeled the same. So in this case, these", "tokens": [51144, 295, 2758, 1296, 439, 295, 264, 1412, 2793, 300, 366, 21335, 264, 912, 13, 407, 294, 341, 1389, 11, 613, 51360], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2262, "seek": 846620, "start": 8486.12, "end": 8489.36, "text": " will be all the ones that are labeled the same. And I take this centroid, which I'm", "tokens": [51360, 486, 312, 439, 264, 2306, 300, 366, 21335, 264, 912, 13, 400, 286, 747, 341, 1489, 6490, 11, 597, 286, 478, 51522], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2263, "seek": 846620, "start": 8489.36, "end": 8493.640000000001, "text": " going to have to erase, get rid of it here. And I put it right in the middle. So let's", "tokens": [51522, 516, 281, 362, 281, 23525, 11, 483, 3973, 295, 309, 510, 13, 400, 286, 829, 309, 558, 294, 264, 2808, 13, 407, 718, 311, 51736], "temperature": 0.0, "avg_logprob": -0.08569394325723453, "compression_ratio": 1.9925925925925927, "no_speech_prob": 0.08034275472164154}, {"id": 2264, "seek": 849364, "start": 8493.68, "end": 8498.08, "text": " go back to blue. And let's say the middle of these data points ends up being somewhere", "tokens": [50366, 352, 646, 281, 3344, 13, 400, 718, 311, 584, 264, 2808, 295, 613, 1412, 2793, 5314, 493, 885, 4079, 50586], "temperature": 0.0, "avg_logprob": -0.12635479434843985, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.01406268309801817}, {"id": 2265, "seek": 849364, "start": 8498.08, "end": 8502.92, "text": " around here. So we put it in here. And this is what we call center of mass. And this again,", "tokens": [50586, 926, 510, 13, 407, 321, 829, 309, 294, 510, 13, 400, 341, 307, 437, 321, 818, 3056, 295, 2758, 13, 400, 341, 797, 11, 50828], "temperature": 0.0, "avg_logprob": -0.12635479434843985, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.01406268309801817}, {"id": 2266, "seek": 849364, "start": 8502.92, "end": 8507.84, "text": " it'd be centroid two. So let's just erase this. And there we go. Now we do the same", "tokens": [50828, 309, 1116, 312, 1489, 6490, 732, 13, 407, 718, 311, 445, 23525, 341, 13, 400, 456, 321, 352, 13, 823, 321, 360, 264, 912, 51074], "temperature": 0.0, "avg_logprob": -0.12635479434843985, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.01406268309801817}, {"id": 2267, "seek": 849364, "start": 8507.84, "end": 8513.119999999999, "text": " thing with the other centroid. So let's remove these ones, to remove these ones. So for three,", "tokens": [51074, 551, 365, 264, 661, 1489, 6490, 13, 407, 718, 311, 4159, 613, 2306, 11, 281, 4159, 613, 2306, 13, 407, 337, 1045, 11, 51338], "temperature": 0.0, "avg_logprob": -0.12635479434843985, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.01406268309801817}, {"id": 2268, "seek": 849364, "start": 8513.119999999999, "end": 8518.68, "text": " I'm saying it's probably going to be somewhere in here. And then for one, our center of mass", "tokens": [51338, 286, 478, 1566, 309, 311, 1391, 516, 281, 312, 4079, 294, 510, 13, 400, 550, 337, 472, 11, 527, 3056, 295, 2758, 51616], "temperature": 0.0, "avg_logprob": -0.12635479434843985, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.01406268309801817}, {"id": 2269, "seek": 851868, "start": 8518.76, "end": 8525.76, "text": " is probably going to be located somewhere about here. Now what I do is I repeat the process", "tokens": [50368, 307, 1391, 516, 281, 312, 6870, 4079, 466, 510, 13, 823, 437, 286, 360, 307, 286, 7149, 264, 1399, 50718], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2270, "seek": 851868, "start": 8525.76, "end": 8530.76, "text": " that I just did. And I reassign all the points now to the closest centroid. So all these", "tokens": [50718, 300, 286, 445, 630, 13, 400, 286, 19486, 788, 439, 264, 2793, 586, 281, 264, 13699, 1489, 6490, 13, 407, 439, 613, 50968], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2271, "seek": 851868, "start": 8530.76, "end": 8534.960000000001, "text": " points are labeled one to all that, you know, we can kind of remove their labels. And this", "tokens": [50968, 2793, 366, 21335, 472, 281, 439, 300, 11, 291, 458, 11, 321, 393, 733, 295, 4159, 641, 16949, 13, 400, 341, 51178], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2272, "seek": 851868, "start": 8534.960000000001, "end": 8539.44, "text": " is just going to be great. Me trying to erase the labels, I shouldn't have wrote them on", "tokens": [51178, 307, 445, 516, 281, 312, 869, 13, 1923, 1382, 281, 23525, 264, 16949, 11, 286, 4659, 380, 362, 4114, 552, 322, 51402], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2273, "seek": 851868, "start": 8539.44, "end": 8543.04, "text": " top. But essentially, what we do is we're just going to be like reassigning them. So", "tokens": [51402, 1192, 13, 583, 4476, 11, 437, 321, 360, 307, 321, 434, 445, 516, 281, 312, 411, 19486, 9676, 552, 13, 407, 51582], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2274, "seek": 851868, "start": 8543.04, "end": 8546.44, "text": " I'm going to say, okay, so this is two, and we just do the same thing as before, find", "tokens": [51582, 286, 478, 516, 281, 584, 11, 1392, 11, 370, 341, 307, 732, 11, 293, 321, 445, 360, 264, 912, 551, 382, 949, 11, 915, 51752], "temperature": 0.0, "avg_logprob": -0.10374818049686056, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.009707672521471977}, {"id": 2275, "seek": 854644, "start": 8546.44, "end": 8550.6, "text": " the closest distance. So we'll say, you know, these can stay in the same cluster, maybe", "tokens": [50364, 264, 13699, 4560, 13, 407, 321, 603, 584, 11, 291, 458, 11, 613, 393, 1754, 294, 264, 912, 13630, 11, 1310, 50572], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2276, "seek": 854644, "start": 8550.6, "end": 8556.28, "text": " this one actually here gets changed to one now, because it's closest to centroid one.", "tokens": [50572, 341, 472, 767, 510, 2170, 3105, 281, 472, 586, 11, 570, 309, 311, 13699, 281, 1489, 6490, 472, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2277, "seek": 854644, "start": 8556.28, "end": 8559.44, "text": " And we just reassigned all these points. And maybe, you know, this one now, if it was two", "tokens": [50856, 400, 321, 445, 19486, 16690, 439, 613, 2793, 13, 400, 1310, 11, 291, 458, 11, 341, 472, 586, 11, 498, 309, 390, 732, 51014], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2278, "seek": 854644, "start": 8559.44, "end": 8564.2, "text": " before, let's say like this one's one, and we just reassigned them. Now we repeat this", "tokens": [51014, 949, 11, 718, 311, 584, 411, 341, 472, 311, 472, 11, 293, 321, 445, 19486, 16690, 552, 13, 823, 321, 7149, 341, 51252], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2279, "seek": 854644, "start": 8564.2, "end": 8570.92, "text": " process of finding the closest or assigning all the points that are closest centroid,", "tokens": [51252, 1399, 295, 5006, 264, 13699, 420, 49602, 439, 264, 2793, 300, 366, 13699, 1489, 6490, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2280, "seek": 854644, "start": 8570.92, "end": 8575.52, "text": " moving the centroid into the center of mass, and we keep doing this until eventually we", "tokens": [51588, 2684, 264, 1489, 6490, 666, 264, 3056, 295, 2758, 11, 293, 321, 1066, 884, 341, 1826, 4728, 321, 51818], "temperature": 0.0, "avg_logprob": -0.1082133386368142, "compression_ratio": 1.9054545454545455, "no_speech_prob": 0.003945126663893461}, {"id": 2281, "seek": 857552, "start": 8575.6, "end": 8580.32, "text": " reach a point where none of these points are changing which centroid they're a part of.", "tokens": [50368, 2524, 257, 935, 689, 6022, 295, 613, 2793, 366, 4473, 597, 1489, 6490, 436, 434, 257, 644, 295, 13, 50604], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2282, "seek": 857552, "start": 8580.32, "end": 8584.0, "text": " So eventually we reach a point where I'm just going to erase this and draw like a new graph", "tokens": [50604, 407, 4728, 321, 2524, 257, 935, 689, 286, 478, 445, 516, 281, 23525, 341, 293, 2642, 411, 257, 777, 4295, 50788], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2283, "seek": 857552, "start": 8584.0, "end": 8587.32, "text": " because it'll be a little bit cleaner. But what we have is, you know, like a bunch of", "tokens": [50788, 570, 309, 603, 312, 257, 707, 857, 16532, 13, 583, 437, 321, 362, 307, 11, 291, 458, 11, 411, 257, 3840, 295, 50954], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2284, "seek": 857552, "start": 8587.32, "end": 8593.08, "text": " data points. So we have some over here, some over here, maybe we'll just put some here,", "tokens": [50954, 1412, 2793, 13, 407, 321, 362, 512, 670, 510, 11, 512, 670, 510, 11, 1310, 321, 603, 445, 829, 512, 510, 11, 51242], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2285, "seek": 857552, "start": 8593.08, "end": 8597.44, "text": " and maybe we'll do like a K equals four example for this one. And we have all these centroids", "tokens": [51242, 293, 1310, 321, 603, 360, 411, 257, 591, 6915, 1451, 1365, 337, 341, 472, 13, 400, 321, 362, 439, 613, 1489, 340, 3742, 51460], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2286, "seek": 857552, "start": 8597.44, "end": 8601.76, "text": " and I'll just draw these centroids with blue again, that are directly in the middle of", "tokens": [51460, 293, 286, 603, 445, 2642, 613, 1489, 340, 3742, 365, 3344, 797, 11, 300, 366, 3838, 294, 264, 2808, 295, 51676], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2287, "seek": 857552, "start": 8601.76, "end": 8605.24, "text": " all of their data points, they're like as in the middle as they can get, none of our", "tokens": [51676, 439, 295, 641, 1412, 2793, 11, 436, 434, 411, 382, 294, 264, 2808, 382, 436, 393, 483, 11, 6022, 295, 527, 51850], "temperature": 0.0, "avg_logprob": -0.12083674939863522, "compression_ratio": 1.9465408805031446, "no_speech_prob": 0.04335969313979149}, {"id": 2288, "seek": 860524, "start": 8605.28, "end": 8610.92, "text": " data points have moved. And we call this now our cluster. So now we have these clusters,", "tokens": [50366, 1412, 2793, 362, 4259, 13, 400, 321, 818, 341, 586, 527, 13630, 13, 407, 586, 321, 362, 613, 23313, 11, 50648], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2289, "seek": 860524, "start": 8610.92, "end": 8614.28, "text": " we have these centroids, right, we know where they are. And what we do is when we have a", "tokens": [50648, 321, 362, 613, 1489, 340, 3742, 11, 558, 11, 321, 458, 689, 436, 366, 13, 400, 437, 321, 360, 307, 562, 321, 362, 257, 50816], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2290, "seek": 860524, "start": 8614.28, "end": 8618.72, "text": " new data point that we want to make a prediction for or figure out what cluster it's a part", "tokens": [50816, 777, 1412, 935, 300, 321, 528, 281, 652, 257, 17630, 337, 420, 2573, 484, 437, 13630, 309, 311, 257, 644, 51038], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2291, "seek": 860524, "start": 8618.72, "end": 8623.36, "text": " of, what we do is we will plot that data point. So let's say it's this new data point here,", "tokens": [51038, 295, 11, 437, 321, 360, 307, 321, 486, 7542, 300, 1412, 935, 13, 407, 718, 311, 584, 309, 311, 341, 777, 1412, 935, 510, 11, 51270], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2292, "seek": 860524, "start": 8623.36, "end": 8628.199999999999, "text": " we find the distance to all of the clusters that exist. And then we assign it to the closest", "tokens": [51270, 321, 915, 264, 4560, 281, 439, 295, 264, 23313, 300, 2514, 13, 400, 550, 321, 6269, 309, 281, 264, 13699, 51512], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2293, "seek": 860524, "start": 8628.199999999999, "end": 8632.36, "text": " one. So obviously it would be assigned to that one. And we can do this for any data point,", "tokens": [51512, 472, 13, 407, 2745, 309, 576, 312, 13279, 281, 300, 472, 13, 400, 321, 393, 360, 341, 337, 604, 1412, 935, 11, 51720], "temperature": 0.0, "avg_logprob": -0.12880804127660292, "compression_ratio": 2.033582089552239, "no_speech_prob": 0.001987649593502283}, {"id": 2294, "seek": 863236, "start": 8632.36, "end": 8636.560000000001, "text": " right? So even if I put a data point all the way over here, well, it's closest cluster", "tokens": [50364, 558, 30, 407, 754, 498, 286, 829, 257, 1412, 935, 439, 264, 636, 670, 510, 11, 731, 11, 309, 311, 13699, 13630, 50574], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2295, "seek": 863236, "start": 8636.560000000001, "end": 8642.16, "text": " is this, so it gets assigned to this cluster. And my output will be whatever this label", "tokens": [50574, 307, 341, 11, 370, 309, 2170, 13279, 281, 341, 13630, 13, 400, 452, 5598, 486, 312, 2035, 341, 7645, 50854], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2296, "seek": 863236, "start": 8642.16, "end": 8646.2, "text": " of this cluster is. And that's essentially how this works, you're just clustering data", "tokens": [50854, 295, 341, 13630, 307, 13, 400, 300, 311, 4476, 577, 341, 1985, 11, 291, 434, 445, 596, 48673, 1412, 51056], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2297, "seek": 863236, "start": 8646.2, "end": 8649.68, "text": " points, figuring out which ones are similar. And this is a pretty basic algorithm, I mean,", "tokens": [51056, 2793, 11, 15213, 484, 597, 2306, 366, 2531, 13, 400, 341, 307, 257, 1238, 3875, 9284, 11, 286, 914, 11, 51230], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2298, "seek": 863236, "start": 8649.68, "end": 8653.04, "text": " you draw your little triangle, you find the distance from every point of the triangle,", "tokens": [51230, 291, 2642, 428, 707, 13369, 11, 291, 915, 264, 4560, 490, 633, 935, 295, 264, 13369, 11, 51398], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2299, "seek": 863236, "start": 8653.04, "end": 8657.480000000001, "text": " or to all of the triangles, actually. And then what you do is just simply assign those", "tokens": [51398, 420, 281, 439, 295, 264, 29896, 11, 767, 13, 400, 550, 437, 291, 360, 307, 445, 2935, 6269, 729, 51620], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2300, "seek": 863236, "start": 8657.480000000001, "end": 8661.84, "text": " values to that centroid, you move that centroid to the center of mass, and you repeat this", "tokens": [51620, 4190, 281, 300, 1489, 6490, 11, 291, 1286, 300, 1489, 6490, 281, 264, 3056, 295, 2758, 11, 293, 291, 7149, 341, 51838], "temperature": 0.0, "avg_logprob": -0.13030284058813954, "compression_ratio": 1.8696969696969696, "no_speech_prob": 0.013635276816785336}, {"id": 2301, "seek": 866184, "start": 8661.86, "end": 8665.76, "text": " process constantly, until eventually you get to a point where none of your data points", "tokens": [50365, 1399, 6460, 11, 1826, 4728, 291, 483, 281, 257, 935, 689, 6022, 295, 428, 1412, 2793, 50560], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2302, "seek": 866184, "start": 8665.76, "end": 8670.24, "text": " are moving. That means you found the best clusters that you can, essentially. Now, the", "tokens": [50560, 366, 2684, 13, 663, 1355, 291, 1352, 264, 1151, 23313, 300, 291, 393, 11, 4476, 13, 823, 11, 264, 50784], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2303, "seek": 866184, "start": 8670.24, "end": 8674.44, "text": " only thing with this is you do need to know how many clusters you want for k means clustering,", "tokens": [50784, 787, 551, 365, 341, 307, 291, 360, 643, 281, 458, 577, 867, 23313, 291, 528, 337, 350, 1355, 596, 48673, 11, 50994], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2304, "seek": 866184, "start": 8674.44, "end": 8678.28, "text": " because k is a variable that you need to define. Although there is some algorithms that can", "tokens": [50994, 570, 350, 307, 257, 7006, 300, 291, 643, 281, 6964, 13, 5780, 456, 307, 512, 14642, 300, 393, 51186], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2305, "seek": 866184, "start": 8678.28, "end": 8682.6, "text": " actually determine the best amount of clusters for a specific data set. But that's a little", "tokens": [51186, 767, 6997, 264, 1151, 2372, 295, 23313, 337, 257, 2685, 1412, 992, 13, 583, 300, 311, 257, 707, 51402], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2306, "seek": 866184, "start": 8682.6, "end": 8686.12, "text": " bit beyond what we're going to be focused on focusing on right now. So that is pretty", "tokens": [51402, 857, 4399, 437, 321, 434, 516, 281, 312, 5178, 322, 8416, 322, 558, 586, 13, 407, 300, 307, 1238, 51578], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2307, "seek": 866184, "start": 8686.12, "end": 8690.28, "text": " much clustering. There's not really much more to talk about it, especially because we can't", "tokens": [51578, 709, 596, 48673, 13, 821, 311, 406, 534, 709, 544, 281, 751, 466, 309, 11, 2318, 570, 321, 393, 380, 51786], "temperature": 0.0, "avg_logprob": -0.12775921983783747, "compression_ratio": 1.7897727272727273, "no_speech_prob": 0.022973796352744102}, {"id": 2308, "seek": 869028, "start": 8690.28, "end": 8695.720000000001, "text": " really code anything for it now. So we're going to move on to hidden Markov models. Now", "tokens": [50364, 534, 3089, 1340, 337, 309, 586, 13, 407, 321, 434, 516, 281, 1286, 322, 281, 7633, 3934, 5179, 5245, 13, 823, 50636], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2309, "seek": 869028, "start": 8695.720000000001, "end": 8699.92, "text": " hidden Markov models are way different than what we've seen so far, we've been using kind", "tokens": [50636, 7633, 3934, 5179, 5245, 366, 636, 819, 813, 437, 321, 600, 1612, 370, 1400, 11, 321, 600, 668, 1228, 733, 50846], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2310, "seek": 869028, "start": 8699.92, "end": 8704.560000000001, "text": " of algorithms that rely on data. So like k means clustering, we gave a lot of data,", "tokens": [50846, 295, 14642, 300, 10687, 322, 1412, 13, 407, 411, 350, 1355, 596, 48673, 11, 321, 2729, 257, 688, 295, 1412, 11, 51078], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2311, "seek": 869028, "start": 8704.560000000001, "end": 8708.800000000001, "text": " and we know clustered all those data points found those centroids, use those centroids", "tokens": [51078, 293, 321, 458, 596, 38624, 439, 729, 1412, 2793, 1352, 729, 24607, 3742, 11, 764, 729, 24607, 3742, 51290], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2312, "seek": 869028, "start": 8708.800000000001, "end": 8714.0, "text": " to find where new data points should be. Same thing with linear regression and classification.", "tokens": [51290, 281, 915, 689, 777, 1412, 2793, 820, 312, 13, 10635, 551, 365, 8213, 24590, 293, 21538, 13, 51550], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2313, "seek": 869028, "start": 8714.0, "end": 8718.68, "text": " Whereas hidden Markov models, we actually deal with probability distributions. Now,", "tokens": [51550, 13813, 7633, 3934, 5179, 5245, 11, 321, 767, 2028, 365, 8482, 37870, 13, 823, 11, 51784], "temperature": 0.0, "avg_logprob": -0.14011529892209976, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.00941194873303175}, {"id": 2314, "seek": 871868, "start": 8718.68, "end": 8721.800000000001, "text": " the example we're going to go into here, and it's kind of I have to do a lot of examples", "tokens": [50364, 264, 1365, 321, 434, 516, 281, 352, 666, 510, 11, 293, 309, 311, 733, 295, 286, 362, 281, 360, 257, 688, 295, 5110, 50520], "temperature": 0.0, "avg_logprob": -0.12756167231379328, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.08267302066087723}, {"id": 2315, "seek": 871868, "start": 8721.800000000001, "end": 8727.08, "text": " for this because it's a very abstract concept is a basic weather model. So what we actually", "tokens": [50520, 337, 341, 570, 309, 311, 257, 588, 12649, 3410, 307, 257, 3875, 5503, 2316, 13, 407, 437, 321, 767, 50784], "temperature": 0.0, "avg_logprob": -0.12756167231379328, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.08267302066087723}, {"id": 2316, "seek": 871868, "start": 8727.08, "end": 8734.32, "text": " want to do is predict the weather on any given day, given the probability of different events", "tokens": [50784, 528, 281, 360, 307, 6069, 264, 5503, 322, 604, 2212, 786, 11, 2212, 264, 8482, 295, 819, 3931, 51146], "temperature": 0.0, "avg_logprob": -0.12756167231379328, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.08267302066087723}, {"id": 2317, "seek": 871868, "start": 8734.32, "end": 8738.64, "text": " occurring. So let's say we know, you know, maybe in like a simulated environment or something", "tokens": [51146, 18386, 13, 407, 718, 311, 584, 321, 458, 11, 291, 458, 11, 1310, 294, 411, 257, 41713, 2823, 420, 746, 51362], "temperature": 0.0, "avg_logprob": -0.12756167231379328, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.08267302066087723}, {"id": 2318, "seek": 871868, "start": 8738.64, "end": 8744.56, "text": " like that, this might be an application, that we have some specific things about our environment,", "tokens": [51362, 411, 300, 11, 341, 1062, 312, 364, 3861, 11, 300, 321, 362, 512, 2685, 721, 466, 527, 2823, 11, 51658], "temperature": 0.0, "avg_logprob": -0.12756167231379328, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.08267302066087723}, {"id": 2319, "seek": 874456, "start": 8744.64, "end": 8748.92, "text": " we know if it's sunny, there's an 80% chance that the next day, it's going to be sunny", "tokens": [50368, 321, 458, 498, 309, 311, 20412, 11, 456, 311, 364, 4688, 4, 2931, 300, 264, 958, 786, 11, 309, 311, 516, 281, 312, 20412, 50582], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2320, "seek": 874456, "start": 8748.92, "end": 8753.32, "text": " again, and a 20% chance that it's going to rain. Maybe we know some information about", "tokens": [50582, 797, 11, 293, 257, 945, 4, 2931, 300, 309, 311, 516, 281, 4830, 13, 2704, 321, 458, 512, 1589, 466, 50802], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2321, "seek": 874456, "start": 8753.32, "end": 8757.779999999999, "text": " sunny days and about cold days. And we also know some information about the average temperature", "tokens": [50802, 20412, 1708, 293, 466, 3554, 1708, 13, 400, 321, 611, 458, 512, 1589, 466, 264, 4274, 4292, 51025], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2322, "seek": 874456, "start": 8757.779999999999, "end": 8763.16, "text": " on those days. Using this information, we can create a hidden Markov model that will", "tokens": [51025, 322, 729, 1708, 13, 11142, 341, 1589, 11, 321, 393, 1884, 257, 7633, 3934, 5179, 2316, 300, 486, 51294], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2323, "seek": 874456, "start": 8763.16, "end": 8768.6, "text": " allow us to make a prediction for the weather in future days, given kind of that probability", "tokens": [51294, 2089, 505, 281, 652, 257, 17630, 337, 264, 5503, 294, 2027, 1708, 11, 2212, 733, 295, 300, 8482, 51566], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2324, "seek": 874456, "start": 8768.6, "end": 8772.359999999999, "text": " that we've discovered. Now you might be like, Well, how do we know this? Like how do I know", "tokens": [51566, 300, 321, 600, 6941, 13, 823, 291, 1062, 312, 411, 11, 1042, 11, 577, 360, 321, 458, 341, 30, 1743, 577, 360, 286, 458, 51754], "temperature": 0.0, "avg_logprob": -0.11627972920735677, "compression_ratio": 1.7933333333333332, "no_speech_prob": 0.4685479402542114}, {"id": 2325, "seek": 877236, "start": 8772.400000000001, "end": 8776.84, "text": " this probability? A lot of the times you actually do know the probability of certain events", "tokens": [50366, 341, 8482, 30, 316, 688, 295, 264, 1413, 291, 767, 360, 458, 264, 8482, 295, 1629, 3931, 50588], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2326, "seek": 877236, "start": 8776.84, "end": 8780.480000000001, "text": " occurring or certain things happening, which makes these models really good. But there's", "tokens": [50588, 18386, 420, 1629, 721, 2737, 11, 597, 1669, 613, 5245, 534, 665, 13, 583, 456, 311, 50770], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2327, "seek": 877236, "start": 8780.480000000001, "end": 8784.6, "text": " some times where what you actually do is you have a huge data set, and you calculate the", "tokens": [50770, 512, 1413, 689, 437, 291, 767, 360, 307, 291, 362, 257, 2603, 1412, 992, 11, 293, 291, 8873, 264, 50976], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2328, "seek": 877236, "start": 8784.6, "end": 8789.24, "text": " probability of things occurring based on that data set. So we're not going to do that part", "tokens": [50976, 8482, 295, 721, 18386, 2361, 322, 300, 1412, 992, 13, 407, 321, 434, 406, 516, 281, 360, 300, 644, 51208], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2329, "seek": 877236, "start": 8789.24, "end": 8791.76, "text": " because that's just kind of going a little bit too far. And the whole point of this is", "tokens": [51208, 570, 300, 311, 445, 733, 295, 516, 257, 707, 857, 886, 1400, 13, 400, 264, 1379, 935, 295, 341, 307, 51334], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2330, "seek": 877236, "start": 8791.76, "end": 8796.2, "text": " just to introduce you to some different models. But in this example, what we will do is use", "tokens": [51334, 445, 281, 5366, 291, 281, 512, 819, 5245, 13, 583, 294, 341, 1365, 11, 437, 321, 486, 360, 307, 764, 51556], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2331, "seek": 877236, "start": 8796.2, "end": 8800.720000000001, "text": " some predefined probability distributions. So let me just read out the exact definition", "tokens": [51556, 512, 659, 37716, 8482, 37870, 13, 407, 718, 385, 445, 1401, 484, 264, 1900, 7123, 51782], "temperature": 0.0, "avg_logprob": -0.09146190697038678, "compression_ratio": 1.877245508982036, "no_speech_prob": 0.08507633954286575}, {"id": 2332, "seek": 880072, "start": 8800.72, "end": 8804.199999999999, "text": " of a hidden Markov model will start going more in depth. So the hidden Markov model", "tokens": [50364, 295, 257, 7633, 3934, 5179, 2316, 486, 722, 516, 544, 294, 7161, 13, 407, 264, 7633, 3934, 5179, 2316, 50538], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2333, "seek": 880072, "start": 8804.199999999999, "end": 8808.64, "text": " is a finite set of states, each of which is associated with a generally multi dimensional", "tokens": [50538, 307, 257, 19362, 992, 295, 4368, 11, 1184, 295, 597, 307, 6615, 365, 257, 5101, 4825, 18795, 50760], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2334, "seek": 880072, "start": 8808.64, "end": 8813.16, "text": " probability distribution, transitions among the states are governed by a set of probabilities", "tokens": [50760, 8482, 7316, 11, 23767, 3654, 264, 4368, 366, 35529, 538, 257, 992, 295, 33783, 50986], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2335, "seek": 880072, "start": 8813.16, "end": 8819.039999999999, "text": " called transition probabilities. So in a hidden Markov model, we have a bunch of states. Now", "tokens": [50986, 1219, 6034, 33783, 13, 407, 294, 257, 7633, 3934, 5179, 2316, 11, 321, 362, 257, 3840, 295, 4368, 13, 823, 51280], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2336, "seek": 880072, "start": 8819.039999999999, "end": 8822.199999999999, "text": " in the example that I was just talking about with this weather model, the states we would", "tokens": [51280, 294, 264, 1365, 300, 286, 390, 445, 1417, 466, 365, 341, 5503, 2316, 11, 264, 4368, 321, 576, 51438], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2337, "seek": 880072, "start": 8822.199999999999, "end": 8829.519999999999, "text": " have is hot day and cold day. Now, these are what we call hidden, because never do we actually", "tokens": [51438, 362, 307, 2368, 786, 293, 3554, 786, 13, 823, 11, 613, 366, 437, 321, 818, 7633, 11, 570, 1128, 360, 321, 767, 51804], "temperature": 0.0, "avg_logprob": -0.10909258165667134, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.014955406077206135}, {"id": 2338, "seek": 882952, "start": 8829.560000000001, "end": 8834.880000000001, "text": " access or look at these states, while we interact with the model. In fact, what we look at is", "tokens": [50366, 2105, 420, 574, 412, 613, 4368, 11, 1339, 321, 4648, 365, 264, 2316, 13, 682, 1186, 11, 437, 321, 574, 412, 307, 50632], "temperature": 0.0, "avg_logprob": -0.10574305909020561, "compression_ratio": 1.9063829787234043, "no_speech_prob": 0.21724912524223328}, {"id": 2339, "seek": 882952, "start": 8834.880000000001, "end": 8839.32, "text": " something called observations. Now at each state, we have an observation, I'll give you", "tokens": [50632, 746, 1219, 18163, 13, 823, 412, 1184, 1785, 11, 321, 362, 364, 14816, 11, 286, 603, 976, 291, 50854], "temperature": 0.0, "avg_logprob": -0.10574305909020561, "compression_ratio": 1.9063829787234043, "no_speech_prob": 0.21724912524223328}, {"id": 2340, "seek": 882952, "start": 8839.32, "end": 8845.2, "text": " an example of an observation. If it is hot outside, Tim has an 80% chance of being happy.", "tokens": [50854, 364, 1365, 295, 364, 14816, 13, 759, 309, 307, 2368, 2380, 11, 7172, 575, 364, 4688, 4, 2931, 295, 885, 2055, 13, 51148], "temperature": 0.0, "avg_logprob": -0.10574305909020561, "compression_ratio": 1.9063829787234043, "no_speech_prob": 0.21724912524223328}, {"id": 2341, "seek": 882952, "start": 8845.2, "end": 8850.6, "text": " If it is cold outside, Tim has a 20% chance of being happy. That is an observation. So", "tokens": [51148, 759, 309, 307, 3554, 2380, 11, 7172, 575, 257, 945, 4, 2931, 295, 885, 2055, 13, 663, 307, 364, 14816, 13, 407, 51418], "temperature": 0.0, "avg_logprob": -0.10574305909020561, "compression_ratio": 1.9063829787234043, "no_speech_prob": 0.21724912524223328}, {"id": 2342, "seek": 882952, "start": 8850.6, "end": 8856.720000000001, "text": " at that state, we can observe the probability of something happening during that state is", "tokens": [51418, 412, 300, 1785, 11, 321, 393, 11441, 264, 8482, 295, 746, 2737, 1830, 300, 1785, 307, 51724], "temperature": 0.0, "avg_logprob": -0.10574305909020561, "compression_ratio": 1.9063829787234043, "no_speech_prob": 0.21724912524223328}, {"id": 2343, "seek": 885672, "start": 8856.76, "end": 8862.8, "text": " x, right, or is y or whatever it is. So we don't actually care about the states in particular,", "tokens": [50366, 2031, 11, 558, 11, 420, 307, 288, 420, 2035, 309, 307, 13, 407, 321, 500, 380, 767, 1127, 466, 264, 4368, 294, 1729, 11, 50668], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2344, "seek": 885672, "start": 8862.8, "end": 8866.439999999999, "text": " we care about the observations we get from that state. Now in our example, what we're", "tokens": [50668, 321, 1127, 466, 264, 18163, 321, 483, 490, 300, 1785, 13, 823, 294, 527, 1365, 11, 437, 321, 434, 50850], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2345, "seek": 885672, "start": 8866.439999999999, "end": 8870.88, "text": " actually going to do is we're going to look at the weather as an observation for the state. So", "tokens": [50850, 767, 516, 281, 360, 307, 321, 434, 516, 281, 574, 412, 264, 5503, 382, 364, 14816, 337, 264, 1785, 13, 407, 51072], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2346, "seek": 885672, "start": 8870.88, "end": 8875.76, "text": " for example, on a sunny day, the weather has, you know, the probability of being between", "tokens": [51072, 337, 1365, 11, 322, 257, 20412, 786, 11, 264, 5503, 575, 11, 291, 458, 11, 264, 8482, 295, 885, 1296, 51316], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2347, "seek": 885672, "start": 8875.76, "end": 8880.84, "text": " five and 15 degrees Celsius, with an average temperature of 11 degrees. That's like, that's", "tokens": [51316, 1732, 293, 2119, 5310, 22658, 11, 365, 364, 4274, 4292, 295, 2975, 5310, 13, 663, 311, 411, 11, 300, 311, 51570], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2348, "seek": 885672, "start": 8880.84, "end": 8885.72, "text": " a probability we can use. Now I know this is slightly abstract, but I just want to talk", "tokens": [51570, 257, 8482, 321, 393, 764, 13, 823, 286, 458, 341, 307, 4748, 12649, 11, 457, 286, 445, 528, 281, 751, 51814], "temperature": 0.0, "avg_logprob": -0.09457966210185617, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.025953365489840508}, {"id": 2349, "seek": 888572, "start": 8885.72, "end": 8889.439999999999, "text": " about the data we're going to work with here. I'm going to draw out a little example, go", "tokens": [50364, 466, 264, 1412, 321, 434, 516, 281, 589, 365, 510, 13, 286, 478, 516, 281, 2642, 484, 257, 707, 1365, 11, 352, 50550], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2350, "seek": 888572, "start": 8889.439999999999, "end": 8892.84, "text": " through it and then we'll actually get into the code. So let's start by discussing the", "tokens": [50550, 807, 309, 293, 550, 321, 603, 767, 483, 666, 264, 3089, 13, 407, 718, 311, 722, 538, 10850, 264, 50720], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2351, "seek": 888572, "start": 8892.84, "end": 8897.119999999999, "text": " type of data we're going to use. So typically in previous ones, right, we use like hundreds,", "tokens": [50720, 2010, 295, 1412, 321, 434, 516, 281, 764, 13, 407, 5850, 294, 3894, 2306, 11, 558, 11, 321, 764, 411, 6779, 11, 50934], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2352, "seek": 888572, "start": 8897.119999999999, "end": 8902.24, "text": " if not like thousands of entries or rows or data points for our models to train. For this,", "tokens": [50934, 498, 406, 411, 5383, 295, 23041, 420, 13241, 420, 1412, 2793, 337, 527, 5245, 281, 3847, 13, 1171, 341, 11, 51190], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2353, "seek": 888572, "start": 8902.24, "end": 8906.96, "text": " we don't need any of that. In fact, all we need is just constant values for probability", "tokens": [51190, 321, 500, 380, 643, 604, 295, 300, 13, 682, 1186, 11, 439, 321, 643, 307, 445, 5754, 4190, 337, 8482, 51426], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2354, "seek": 888572, "start": 8906.96, "end": 8912.279999999999, "text": " and or what is it transition distributions and observation distributions. Now what I'm", "tokens": [51426, 293, 420, 437, 307, 309, 6034, 37870, 293, 14816, 37870, 13, 823, 437, 286, 478, 51692], "temperature": 0.0, "avg_logprob": -0.12494431119976622, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.01205186266452074}, {"id": 2355, "seek": 891228, "start": 8912.320000000002, "end": 8916.720000000001, "text": " going to do is go in here and talk about states observations and transitions. So we have a", "tokens": [50366, 516, 281, 360, 307, 352, 294, 510, 293, 751, 466, 4368, 18163, 293, 23767, 13, 407, 321, 362, 257, 50586], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2356, "seek": 891228, "start": 8916.720000000001, "end": 8922.0, "text": " certain amount of states. Now we will define how many states we have, we don't really care", "tokens": [50586, 1629, 2372, 295, 4368, 13, 823, 321, 486, 6964, 577, 867, 4368, 321, 362, 11, 321, 500, 380, 534, 1127, 50850], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2357, "seek": 891228, "start": 8922.0, "end": 8927.28, "text": " what that state is. So we could have states, for example, like warm, cold, high, low, red,", "tokens": [50850, 437, 300, 1785, 307, 13, 407, 321, 727, 362, 4368, 11, 337, 1365, 11, 411, 4561, 11, 3554, 11, 1090, 11, 2295, 11, 2182, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2358, "seek": 891228, "start": 8927.28, "end": 8931.480000000001, "text": " green, blue, you have as many states as we want, we could have one state to be honest,", "tokens": [51114, 3092, 11, 3344, 11, 291, 362, 382, 867, 4368, 382, 321, 528, 11, 321, 727, 362, 472, 1785, 281, 312, 3245, 11, 51324], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2359, "seek": 891228, "start": 8931.480000000001, "end": 8934.76, "text": " although that would be kind of strange to have that. And these are called hidden because", "tokens": [51324, 4878, 300, 576, 312, 733, 295, 5861, 281, 362, 300, 13, 400, 613, 366, 1219, 7633, 570, 51488], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2360, "seek": 891228, "start": 8934.76, "end": 8939.640000000001, "text": " we don't directly observe. Now observations. So each state has a particular outcome or", "tokens": [51488, 321, 500, 380, 3838, 11441, 13, 823, 18163, 13, 407, 1184, 1785, 575, 257, 1729, 9700, 420, 51732], "temperature": 0.0, "avg_logprob": -0.1217485257049105, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0446753092110157}, {"id": 2361, "seek": 893964, "start": 8939.72, "end": 8944.199999999999, "text": " observation associated with it based on a probability distribution. So it could be the", "tokens": [50368, 14816, 6615, 365, 309, 2361, 322, 257, 8482, 7316, 13, 407, 309, 727, 312, 264, 50592], "temperature": 0.0, "avg_logprob": -0.12154942425814542, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.007120750844478607}, {"id": 2362, "seek": 893964, "start": 8944.199999999999, "end": 8950.439999999999, "text": " fact that during a hot day, it is 100% true that Tim is happy. Although, in a hot day, we", "tokens": [50592, 1186, 300, 1830, 257, 2368, 786, 11, 309, 307, 2319, 4, 2074, 300, 7172, 307, 2055, 13, 5780, 11, 294, 257, 2368, 786, 11, 321, 50904], "temperature": 0.0, "avg_logprob": -0.12154942425814542, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.007120750844478607}, {"id": 2363, "seek": 893964, "start": 8950.439999999999, "end": 8956.08, "text": " could observe that 80% of the time Tim is happy. And 20% of the time, he is sad, right? Those", "tokens": [50904, 727, 11441, 300, 4688, 4, 295, 264, 565, 7172, 307, 2055, 13, 400, 945, 4, 295, 264, 565, 11, 415, 307, 4227, 11, 558, 30, 3950, 51186], "temperature": 0.0, "avg_logprob": -0.12154942425814542, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.007120750844478607}, {"id": 2364, "seek": 893964, "start": 8956.08, "end": 8960.64, "text": " are observations we make about each state. And each state will have their different observations", "tokens": [51186, 366, 18163, 321, 652, 466, 1184, 1785, 13, 400, 1184, 1785, 486, 362, 641, 819, 18163, 51414], "temperature": 0.0, "avg_logprob": -0.12154942425814542, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.007120750844478607}, {"id": 2365, "seek": 893964, "start": 8960.64, "end": 8966.359999999999, "text": " and different probabilities of those observations occurring. So if we were just going to have", "tokens": [51414, 293, 819, 33783, 295, 729, 18163, 18386, 13, 407, 498, 321, 645, 445, 516, 281, 362, 51700], "temperature": 0.0, "avg_logprob": -0.12154942425814542, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.007120750844478607}, {"id": 2366, "seek": 896636, "start": 8966.44, "end": 8970.400000000001, "text": " like an outcome for the state, that means it's always the same, there's no probability that", "tokens": [50368, 411, 364, 9700, 337, 264, 1785, 11, 300, 1355, 309, 311, 1009, 264, 912, 11, 456, 311, 572, 8482, 300, 50566], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2367, "seek": 896636, "start": 8970.400000000001, "end": 8974.800000000001, "text": " something happens. And in that case, that's just called an outcome, because the probability", "tokens": [50566, 746, 2314, 13, 400, 294, 300, 1389, 11, 300, 311, 445, 1219, 364, 9700, 11, 570, 264, 8482, 50786], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2368, "seek": 896636, "start": 8974.800000000001, "end": 8980.6, "text": " of the event occurring will be 100%. Okay, then we have transitions. So each state will", "tokens": [50786, 295, 264, 2280, 18386, 486, 312, 2319, 6856, 1033, 11, 550, 321, 362, 23767, 13, 407, 1184, 1785, 486, 51076], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2369, "seek": 896636, "start": 8980.6, "end": 8984.880000000001, "text": " have a probability defining the likelihood of transitioning to a different state. So", "tokens": [51076, 362, 257, 8482, 17827, 264, 22119, 295, 33777, 281, 257, 819, 1785, 13, 407, 51290], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2370, "seek": 896636, "start": 8984.880000000001, "end": 8989.84, "text": " for example, if we have a hot day, there will be a percentage chance the next day will be", "tokens": [51290, 337, 1365, 11, 498, 321, 362, 257, 2368, 786, 11, 456, 486, 312, 257, 9668, 2931, 264, 958, 786, 486, 312, 51538], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2371, "seek": 896636, "start": 8989.84, "end": 8993.16, "text": " a cold day. And if we have a cold day, there will be a percentage chance that the next", "tokens": [51538, 257, 3554, 786, 13, 400, 498, 321, 362, 257, 3554, 786, 11, 456, 486, 312, 257, 9668, 2931, 300, 264, 958, 51704], "temperature": 0.0, "avg_logprob": -0.09442270083690253, "compression_ratio": 2.0037593984962405, "no_speech_prob": 0.0695156455039978}, {"id": 2372, "seek": 899316, "start": 8993.16, "end": 8997.68, "text": " day is either a hot day or a cold day. So we're going to go through like the exact what", "tokens": [50364, 786, 307, 2139, 257, 2368, 786, 420, 257, 3554, 786, 13, 407, 321, 434, 516, 281, 352, 807, 411, 264, 1900, 437, 50590], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2373, "seek": 899316, "start": 8997.68, "end": 9002.16, "text": " we have for our specific model below. But just understand there's a probability that", "tokens": [50590, 321, 362, 337, 527, 2685, 2316, 2507, 13, 583, 445, 1223, 456, 311, 257, 8482, 300, 50814], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2374, "seek": 899316, "start": 9002.16, "end": 9006.4, "text": " we could transition into a different state. And from each state, we can transition into", "tokens": [50814, 321, 727, 6034, 666, 257, 819, 1785, 13, 400, 490, 1184, 1785, 11, 321, 393, 6034, 666, 51026], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2375, "seek": 899316, "start": 9006.4, "end": 9012.28, "text": " every other state or a defined set of states given a certain probability. So I know it's", "tokens": [51026, 633, 661, 1785, 420, 257, 7642, 992, 295, 4368, 2212, 257, 1629, 8482, 13, 407, 286, 458, 309, 311, 51320], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2376, "seek": 899316, "start": 9012.28, "end": 9017.36, "text": " a mouthful, I know it's a lot. But let's go into a basic drawing example, because I just", "tokens": [51320, 257, 4525, 906, 11, 286, 458, 309, 311, 257, 688, 13, 583, 718, 311, 352, 666, 257, 3875, 6316, 1365, 11, 570, 286, 445, 51574], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2377, "seek": 899316, "start": 9017.36, "end": 9021.32, "text": " want to illustrate like graphically a little bit kind of how this works. In case these", "tokens": [51574, 528, 281, 23221, 411, 4295, 984, 257, 707, 857, 733, 295, 577, 341, 1985, 13, 682, 1389, 613, 51772], "temperature": 0.0, "avg_logprob": -0.10894594192504883, "compression_ratio": 1.710097719869707, "no_speech_prob": 0.07805601507425308}, {"id": 2378, "seek": 902132, "start": 9021.36, "end": 9026.119999999999, "text": " are ideas are a little bit too abstract for any of you. Okay, I'm just pulling out the", "tokens": [50366, 366, 3487, 366, 257, 707, 857, 886, 12649, 337, 604, 295, 291, 13, 1033, 11, 286, 478, 445, 8407, 484, 264, 50604], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2379, "seek": 902132, "start": 9026.119999999999, "end": 9032.039999999999, "text": " drawing tablet, just one second here, and let's do this basic weather model. So what", "tokens": [50604, 6316, 14136, 11, 445, 472, 1150, 510, 11, 293, 718, 311, 360, 341, 3875, 5503, 2316, 13, 407, 437, 50900], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2380, "seek": 902132, "start": 9032.039999999999, "end": 9036.84, "text": " I'm going to do is just simply draw two states. Actually, let's do it with some colors because", "tokens": [50900, 286, 478, 516, 281, 360, 307, 445, 2935, 2642, 732, 4368, 13, 5135, 11, 718, 311, 360, 309, 365, 512, 4577, 570, 51140], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2381, "seek": 902132, "start": 9036.84, "end": 9040.96, "text": " why not? So we're going to use yellow. And this is going to be our hot day. Okay, this", "tokens": [51140, 983, 406, 30, 407, 321, 434, 516, 281, 764, 5566, 13, 400, 341, 307, 516, 281, 312, 527, 2368, 786, 13, 1033, 11, 341, 51346], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2382, "seek": 902132, "start": 9040.96, "end": 9045.64, "text": " is going to be our sun. And then I'm just going to make a cloud. We'll just do like", "tokens": [51346, 307, 516, 281, 312, 527, 3295, 13, 400, 550, 286, 478, 445, 516, 281, 652, 257, 4588, 13, 492, 603, 445, 360, 411, 51580], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2383, "seek": 902132, "start": 9045.64, "end": 9050.52, "text": " a gray cloud. This will be my cloud. And we'll just say it's going to be raining over here.", "tokens": [51580, 257, 10855, 4588, 13, 639, 486, 312, 452, 4588, 13, 400, 321, 603, 445, 584, 309, 311, 516, 281, 312, 18441, 670, 510, 13, 51824], "temperature": 0.0, "avg_logprob": -0.13273204102808114, "compression_ratio": 1.8368055555555556, "no_speech_prob": 0.1329120695590973}, {"id": 2384, "seek": 905052, "start": 9050.52, "end": 9055.36, "text": " Okay, so these are my two states. Now, in each state, there's a probability of transitioning", "tokens": [50364, 1033, 11, 370, 613, 366, 452, 732, 4368, 13, 823, 11, 294, 1184, 1785, 11, 456, 311, 257, 8482, 295, 33777, 50606], "temperature": 0.0, "avg_logprob": -0.09213955879211426, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.003172278171405196}, {"id": 2385, "seek": 905052, "start": 9055.36, "end": 9062.44, "text": " to the other state. So for example, in a hot day, we have a let's say 20% chance of transitioning", "tokens": [50606, 281, 264, 661, 1785, 13, 407, 337, 1365, 11, 294, 257, 2368, 786, 11, 321, 362, 257, 718, 311, 584, 945, 4, 2931, 295, 33777, 50960], "temperature": 0.0, "avg_logprob": -0.09213955879211426, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.003172278171405196}, {"id": 2386, "seek": 905052, "start": 9062.44, "end": 9068.720000000001, "text": " to a cold day. And we have a 80% chance of transitioning to another hot day, like the", "tokens": [50960, 281, 257, 3554, 786, 13, 400, 321, 362, 257, 4688, 4, 2931, 295, 33777, 281, 1071, 2368, 786, 11, 411, 264, 51274], "temperature": 0.0, "avg_logprob": -0.09213955879211426, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.003172278171405196}, {"id": 2387, "seek": 905052, "start": 9068.720000000001, "end": 9074.48, "text": " next day, right? Now in a cold day, we have let's say a 30% chance of transitioning to", "tokens": [51274, 958, 786, 11, 558, 30, 823, 294, 257, 3554, 786, 11, 321, 362, 718, 311, 584, 257, 2217, 4, 2931, 295, 33777, 281, 51562], "temperature": 0.0, "avg_logprob": -0.09213955879211426, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.003172278171405196}, {"id": 2388, "seek": 907448, "start": 9074.52, "end": 9080.76, "text": " a hot day. And we have in this case, what is that going to be a 70% chance of transitioning", "tokens": [50366, 257, 2368, 786, 13, 400, 321, 362, 294, 341, 1389, 11, 437, 307, 300, 516, 281, 312, 257, 5285, 4, 2931, 295, 33777, 50678], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2389, "seek": 907448, "start": 9080.76, "end": 9085.359999999999, "text": " to another cold day. Now, on each of these days, we have a list of observations. So these", "tokens": [50678, 281, 1071, 3554, 786, 13, 823, 11, 322, 1184, 295, 613, 1708, 11, 321, 362, 257, 1329, 295, 18163, 13, 407, 613, 50908], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2390, "seek": 907448, "start": 9085.359999999999, "end": 9089.279999999999, "text": " are what we call states, right? So this could be s one, and this could be s two, it doesn't", "tokens": [50908, 366, 437, 321, 818, 4368, 11, 558, 30, 407, 341, 727, 312, 262, 472, 11, 293, 341, 727, 312, 262, 732, 11, 309, 1177, 380, 51104], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2391, "seek": 907448, "start": 9089.279999999999, "end": 9092.96, "text": " really matter, like if we name them or anything, we just we have two states, that's what we", "tokens": [51104, 534, 1871, 11, 411, 498, 321, 1315, 552, 420, 1340, 11, 321, 445, 321, 362, 732, 4368, 11, 300, 311, 437, 321, 51288], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2392, "seek": 907448, "start": 9092.96, "end": 9097.6, "text": " know. We know the transition probability, that's what we've just defined. Now we want", "tokens": [51288, 458, 13, 492, 458, 264, 6034, 8482, 11, 300, 311, 437, 321, 600, 445, 7642, 13, 823, 321, 528, 51520], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2393, "seek": 907448, "start": 9097.6, "end": 9102.88, "text": " the observation probability or distribution for that. So essentially, on a hot day, our", "tokens": [51520, 264, 14816, 8482, 420, 7316, 337, 300, 13, 407, 4476, 11, 322, 257, 2368, 786, 11, 527, 51784], "temperature": 0.0, "avg_logprob": -0.1263980462517537, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.28132766485214233}, {"id": 2394, "seek": 910288, "start": 9102.92, "end": 9110.0, "text": " observation is going to be that the temperature could be between 15 and 25 degrees Celsius", "tokens": [50366, 14816, 307, 516, 281, 312, 300, 264, 4292, 727, 312, 1296, 2119, 293, 3552, 5310, 22658, 50720], "temperature": 0.0, "avg_logprob": -0.15488838065754285, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01406171452254057}, {"id": 2395, "seek": 910288, "start": 9110.0, "end": 9117.599999999999, "text": " with an average temperature of let's say 20. So we can say observation, right? So say observation,", "tokens": [50720, 365, 364, 4274, 4292, 295, 718, 311, 584, 945, 13, 407, 321, 393, 584, 14816, 11, 558, 30, 407, 584, 14816, 11, 51100], "temperature": 0.0, "avg_logprob": -0.15488838065754285, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01406171452254057}, {"id": 2396, "seek": 910288, "start": 9117.599999999999, "end": 9123.279999999999, "text": " and we'll say that the mean so the average temperature is going to be 20. And then the", "tokens": [51100, 293, 321, 603, 584, 300, 264, 914, 370, 264, 4274, 4292, 307, 516, 281, 312, 945, 13, 400, 550, 264, 51384], "temperature": 0.0, "avg_logprob": -0.15488838065754285, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01406171452254057}, {"id": 2397, "seek": 910288, "start": 9123.279999999999, "end": 9129.199999999999, "text": " distribution for that will be like the minimum value is going to be 15. And the max is going", "tokens": [51384, 7316, 337, 300, 486, 312, 411, 264, 7285, 2158, 307, 516, 281, 312, 2119, 13, 400, 264, 11469, 307, 516, 51680], "temperature": 0.0, "avg_logprob": -0.15488838065754285, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01406171452254057}, {"id": 2398, "seek": 912920, "start": 9129.2, "end": 9134.12, "text": " to be 25. So this is what we call actually like a standard deviation. I'm not really", "tokens": [50364, 281, 312, 3552, 13, 407, 341, 307, 437, 321, 818, 767, 411, 257, 3832, 25163, 13, 286, 478, 406, 534, 50610], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2399, "seek": 912920, "start": 9134.12, "end": 9137.2, "text": " going to explain exactly what standard deviation is, although you can kind of think of it as", "tokens": [50610, 516, 281, 2903, 2293, 437, 3832, 25163, 307, 11, 4878, 291, 393, 733, 295, 519, 295, 309, 382, 50764], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2400, "seek": 912920, "start": 9137.2, "end": 9142.560000000001, "text": " something like this. So essentially, there's a mean, which is the middle point, the most", "tokens": [50764, 746, 411, 341, 13, 407, 4476, 11, 456, 311, 257, 914, 11, 597, 307, 264, 2808, 935, 11, 264, 881, 51032], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2401, "seek": 912920, "start": 9142.560000000001, "end": 9147.480000000001, "text": " common event that could occur. And at different levels of standard deviation, which is going", "tokens": [51032, 2689, 2280, 300, 727, 5160, 13, 400, 412, 819, 4358, 295, 3832, 25163, 11, 597, 307, 516, 51278], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2402, "seek": 912920, "start": 9147.480000000001, "end": 9150.640000000001, "text": " into statistics, which I don't really want to mention that much, because I'm definitely", "tokens": [51278, 666, 12523, 11, 597, 286, 500, 380, 534, 528, 281, 2152, 300, 709, 11, 570, 286, 478, 2138, 51436], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2403, "seek": 912920, "start": 9150.640000000001, "end": 9155.76, "text": " non expert. We have a probability of hitting different temperatures as we move to the left", "tokens": [51436, 2107, 5844, 13, 492, 362, 257, 8482, 295, 8850, 819, 12633, 382, 321, 1286, 281, 264, 1411, 51692], "temperature": 0.0, "avg_logprob": -0.07215971331442556, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.02930719591677189}, {"id": 2404, "seek": 915576, "start": 9155.800000000001, "end": 9161.16, "text": " and right of this value. So on this curve somewhere, we have 15. And on this curve to the", "tokens": [50366, 293, 558, 295, 341, 2158, 13, 407, 322, 341, 7605, 4079, 11, 321, 362, 2119, 13, 400, 322, 341, 7605, 281, 264, 50634], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2405, "seek": 915576, "start": 9161.16, "end": 9164.880000000001, "text": " right somewhere, we have 25. Now, we're just defining the fact that this is where we're", "tokens": [50634, 558, 4079, 11, 321, 362, 3552, 13, 823, 11, 321, 434, 445, 17827, 264, 1186, 300, 341, 307, 689, 321, 434, 50820], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2406, "seek": 915576, "start": 9164.880000000001, "end": 9169.960000000001, "text": " going to kind of end our curve. So we're going to say that like the probability is in between", "tokens": [50820, 516, 281, 733, 295, 917, 527, 7605, 13, 407, 321, 434, 516, 281, 584, 300, 411, 264, 8482, 307, 294, 1296, 51074], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2407, "seek": 915576, "start": 9169.960000000001, "end": 9174.28, "text": " these numbers, it's going to be in between 15 and 25 with an average of 20. And then our", "tokens": [51074, 613, 3547, 11, 309, 311, 516, 281, 312, 294, 1296, 2119, 293, 3552, 365, 364, 4274, 295, 945, 13, 400, 550, 527, 51290], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2408, "seek": 915576, "start": 9174.28, "end": 9178.76, "text": " model will kind of figure out some things to do with that. That's as far as I really", "tokens": [51290, 2316, 486, 733, 295, 2573, 484, 512, 721, 281, 360, 365, 300, 13, 663, 311, 382, 1400, 382, 286, 534, 51514], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2409, "seek": 915576, "start": 9178.76, "end": 9183.32, "text": " want to go in standard deviation. And I'm sure that's like a really horrible explanation.", "tokens": [51514, 528, 281, 352, 294, 3832, 25163, 13, 400, 286, 478, 988, 300, 311, 411, 257, 534, 9263, 10835, 13, 51742], "temperature": 0.0, "avg_logprob": -0.09002858443225888, "compression_ratio": 1.832191780821918, "no_speech_prob": 0.029309609904885292}, {"id": 2410, "seek": 918332, "start": 9183.36, "end": 9186.039999999999, "text": " That's kind of the best I'm going to give you guys for right now. Okay, so that's our", "tokens": [50366, 663, 311, 733, 295, 264, 1151, 286, 478, 516, 281, 976, 291, 1074, 337, 558, 586, 13, 1033, 11, 370, 300, 311, 527, 50500], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2411, "seek": 918332, "start": 9186.039999999999, "end": 9189.36, "text": " observation here. Now our observation over here is going to be similar. So we're going", "tokens": [50500, 14816, 510, 13, 823, 527, 14816, 670, 510, 307, 516, 281, 312, 2531, 13, 407, 321, 434, 516, 50666], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2412, "seek": 918332, "start": 9189.36, "end": 9194.16, "text": " to say mean on a cold day, temperature is going to be five degrees. We'll say the minimum", "tokens": [50666, 281, 584, 914, 322, 257, 3554, 786, 11, 4292, 307, 516, 281, 312, 1732, 5310, 13, 492, 603, 584, 264, 7285, 50906], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2413, "seek": 918332, "start": 9194.16, "end": 9198.48, "text": " temperature maybe is going to be something like negative five and the max could be something", "tokens": [50906, 4292, 1310, 307, 516, 281, 312, 746, 411, 3671, 1732, 293, 264, 11469, 727, 312, 746, 51122], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2414, "seek": 918332, "start": 9198.48, "end": 9204.76, "text": " like 15 or like, yeah, we can say 15. So we'll have some distribution, not just what we want", "tokens": [51122, 411, 2119, 420, 411, 11, 1338, 11, 321, 393, 584, 2119, 13, 407, 321, 603, 362, 512, 7316, 11, 406, 445, 437, 321, 528, 51436], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2415, "seek": 918332, "start": 9204.76, "end": 9208.72, "text": " to understand, right? And this is kind of a strange distribution because we're dealing", "tokens": [51436, 281, 1223, 11, 558, 30, 400, 341, 307, 733, 295, 257, 5861, 7316, 570, 321, 434, 6260, 51634], "temperature": 0.0, "avg_logprob": -0.13341705781176574, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.09007161855697632}, {"id": 2416, "seek": 920872, "start": 9208.76, "end": 9214.08, "text": " with what is it standard deviation, although we can just deal with like straight percentage", "tokens": [50366, 365, 437, 307, 309, 3832, 25163, 11, 4878, 321, 393, 445, 2028, 365, 411, 2997, 9668, 50632], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2417, "seek": 920872, "start": 9214.08, "end": 9218.48, "text": " observations. So for example, you know, there's a 20% chance that Tim is happy, or there's an", "tokens": [50632, 18163, 13, 407, 337, 1365, 11, 291, 458, 11, 456, 311, 257, 945, 4, 2931, 300, 7172, 307, 2055, 11, 420, 456, 311, 364, 50852], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2418, "seek": 920872, "start": 9218.48, "end": 9224.8, "text": " 80% chance that he is sad, like those are probabilities that we can have as our observation", "tokens": [50852, 4688, 4, 2931, 300, 415, 307, 4227, 11, 411, 729, 366, 33783, 300, 321, 393, 362, 382, 527, 14816, 51168], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2419, "seek": 920872, "start": 9224.8, "end": 9229.84, "text": " probabilities in the model. Okay, so there's a lot of lingo. There's a lot going on, we're", "tokens": [51168, 33783, 294, 264, 2316, 13, 1033, 11, 370, 456, 311, 257, 688, 295, 287, 18459, 13, 821, 311, 257, 688, 516, 322, 11, 321, 434, 51420], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2420, "seek": 920872, "start": 9229.84, "end": 9233.56, "text": " going to get into like a concrete example now. So hopefully this should make more sense. But", "tokens": [51420, 516, 281, 483, 666, 411, 257, 9859, 1365, 586, 13, 407, 4696, 341, 820, 652, 544, 2020, 13, 583, 51606], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2421, "seek": 920872, "start": 9233.56, "end": 9237.679999999998, "text": " again, just understand states, transitions, observations, we don't actually ever look at", "tokens": [51606, 797, 11, 445, 1223, 4368, 11, 23767, 11, 18163, 11, 321, 500, 380, 767, 1562, 574, 412, 51812], "temperature": 0.0, "avg_logprob": -0.12616659989997522, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.06952711939811707}, {"id": 2422, "seek": 923768, "start": 9237.68, "end": 9241.800000000001, "text": " the states, we just have to know how many we have, and the transition probability and", "tokens": [50364, 264, 4368, 11, 321, 445, 362, 281, 458, 577, 867, 321, 362, 11, 293, 264, 6034, 8482, 293, 50570], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2423, "seek": 923768, "start": 9241.800000000001, "end": 9247.880000000001, "text": " observation probability in each of them. Okay, so what I want to say now, though, is what", "tokens": [50570, 14816, 8482, 294, 1184, 295, 552, 13, 1033, 11, 370, 437, 286, 528, 281, 584, 586, 11, 1673, 11, 307, 437, 50874], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2424, "seek": 923768, "start": 9247.880000000001, "end": 9251.960000000001, "text": " do we even do with this model? So once I make this right, once I make this hidden markup", "tokens": [50874, 360, 321, 754, 360, 365, 341, 2316, 30, 407, 1564, 286, 652, 341, 558, 11, 1564, 286, 652, 341, 7633, 1491, 1010, 51078], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2425, "seek": 923768, "start": 9251.960000000001, "end": 9255.56, "text": " model, what's the point of it? Well, the point of it is to predict future events based on", "tokens": [51078, 2316, 11, 437, 311, 264, 935, 295, 309, 30, 1042, 11, 264, 935, 295, 309, 307, 281, 6069, 2027, 3931, 2361, 322, 51258], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2426, "seek": 923768, "start": 9255.56, "end": 9260.32, "text": " past events. So we know that probability distribution. And I want to predict the weather for the", "tokens": [51258, 1791, 3931, 13, 407, 321, 458, 300, 8482, 7316, 13, 400, 286, 528, 281, 6069, 264, 5503, 337, 264, 51496], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2427, "seek": 923768, "start": 9260.32, "end": 9264.44, "text": " next week, I can use that model to do that, because I can say, well, if the current day", "tokens": [51496, 958, 1243, 11, 286, 393, 764, 300, 2316, 281, 360, 300, 11, 570, 286, 393, 584, 11, 731, 11, 498, 264, 2190, 786, 51702], "temperature": 0.0, "avg_logprob": -0.1299896514673027, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.010985695756971836}, {"id": 2428, "seek": 926444, "start": 9264.52, "end": 9269.0, "text": " today is warm, then what is the likelihood that the next day tomorrow is going to be", "tokens": [50368, 965, 307, 4561, 11, 550, 437, 307, 264, 22119, 300, 264, 958, 786, 4153, 307, 516, 281, 312, 50592], "temperature": 0.0, "avg_logprob": -0.16367719723628119, "compression_ratio": 1.6541353383458646, "no_speech_prob": 0.2941210865974426}, {"id": 2429, "seek": 926444, "start": 9269.0, "end": 9273.480000000001, "text": " cold, right? And that's what we're kind of doing with this model, we're making predictions", "tokens": [50592, 3554, 11, 558, 30, 400, 300, 311, 437, 321, 434, 733, 295, 884, 365, 341, 2316, 11, 321, 434, 1455, 21264, 50816], "temperature": 0.0, "avg_logprob": -0.16367719723628119, "compression_ratio": 1.6541353383458646, "no_speech_prob": 0.2941210865974426}, {"id": 2430, "seek": 926444, "start": 9273.480000000001, "end": 9280.16, "text": " for the future based on probability of past events occur. Okay, so imports and so let's", "tokens": [50816, 337, 264, 2027, 2361, 322, 8482, 295, 1791, 3931, 5160, 13, 1033, 11, 370, 41596, 293, 370, 718, 311, 51150], "temperature": 0.0, "avg_logprob": -0.16367719723628119, "compression_ratio": 1.6541353383458646, "no_speech_prob": 0.2941210865974426}, {"id": 2431, "seek": 926444, "start": 9280.16, "end": 9286.0, "text": " just run this already loaded import tensorflow. And notice that here I've imported tensorflow", "tokens": [51150, 445, 1190, 341, 1217, 13210, 974, 40863, 10565, 13, 400, 3449, 300, 510, 286, 600, 25524, 40863, 10565, 51442], "temperature": 0.0, "avg_logprob": -0.16367719723628119, "compression_ratio": 1.6541353383458646, "no_speech_prob": 0.2941210865974426}, {"id": 2432, "seek": 926444, "start": 9286.0, "end": 9292.68, "text": " probability is TFP. This is because this is a separate module from TensorFlow that", "tokens": [51442, 8482, 307, 40964, 47, 13, 639, 307, 570, 341, 307, 257, 4994, 10088, 490, 37624, 300, 51776], "temperature": 0.0, "avg_logprob": -0.16367719723628119, "compression_ratio": 1.6541353383458646, "no_speech_prob": 0.2941210865974426}, {"id": 2433, "seek": 929268, "start": 9292.720000000001, "end": 9298.2, "text": " deals with probability. Now, we also need tensorflow two. But for this hidden markup", "tokens": [50366, 11215, 365, 8482, 13, 823, 11, 321, 611, 643, 40863, 10565, 732, 13, 583, 337, 341, 7633, 1491, 1010, 50640], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2434, "seek": 929268, "start": 9298.2, "end": 9302.56, "text": " model, we're going to use the tensorflow probability module, not a huge deal. Okay, so", "tokens": [50640, 2316, 11, 321, 434, 516, 281, 764, 264, 40863, 10565, 8482, 10088, 11, 406, 257, 2603, 2028, 13, 1033, 11, 370, 50858], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2435, "seek": 929268, "start": 9302.56, "end": 9306.960000000001, "text": " weather model. So this is just going to define what our model actually is so the different", "tokens": [50858, 5503, 2316, 13, 407, 341, 307, 445, 516, 281, 6964, 437, 527, 2316, 767, 307, 370, 264, 819, 51078], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2436, "seek": 929268, "start": 9306.960000000001, "end": 9311.44, "text": " parts of it. So this is taken directly from the documentation of tensorflow. You guys can", "tokens": [51078, 3166, 295, 309, 13, 407, 341, 307, 2726, 3838, 490, 264, 14333, 295, 40863, 10565, 13, 509, 1074, 393, 51302], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2437, "seek": 929268, "start": 9311.44, "end": 9315.28, "text": " see you know, where I have all this information from like I've sourced all of it. But essentially", "tokens": [51302, 536, 291, 458, 11, 689, 286, 362, 439, 341, 1589, 490, 411, 286, 600, 11006, 1232, 439, 295, 309, 13, 583, 4476, 51494], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2438, "seek": 929268, "start": 9315.28, "end": 9318.48, "text": " what the model we're going to try to create is that cold days are encoded by zero and", "tokens": [51494, 437, 264, 2316, 321, 434, 516, 281, 853, 281, 1884, 307, 300, 3554, 1708, 366, 2058, 12340, 538, 4018, 293, 51654], "temperature": 0.0, "avg_logprob": -0.13113791960522644, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.03621181100606918}, {"id": 2439, "seek": 931848, "start": 9318.52, "end": 9323.199999999999, "text": " hot days are encoded by one. The first day in our sequence has an 80% chance of being", "tokens": [50366, 2368, 1708, 366, 2058, 12340, 538, 472, 13, 440, 700, 786, 294, 527, 8310, 575, 364, 4688, 4, 2931, 295, 885, 50600], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2440, "seek": 931848, "start": 9323.199999999999, "end": 9326.68, "text": " cold. So whatever day we're starting out at has an 80% chance of being cold, which would", "tokens": [50600, 3554, 13, 407, 2035, 786, 321, 434, 2891, 484, 412, 575, 364, 4688, 4, 2931, 295, 885, 3554, 11, 597, 576, 50774], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2441, "seek": 931848, "start": 9326.68, "end": 9332.0, "text": " mean 20% chance of being warm. A cold day has a 30% chance of being followed by hot day.", "tokens": [50774, 914, 945, 4, 2931, 295, 885, 4561, 13, 316, 3554, 786, 575, 257, 2217, 4, 2931, 295, 885, 6263, 538, 2368, 786, 13, 51040], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2442, "seek": 931848, "start": 9332.0, "end": 9335.359999999999, "text": " And a hot day has a 20% chance of being followed by a cold day, which would mean you know,", "tokens": [51040, 400, 257, 2368, 786, 575, 257, 945, 4, 2931, 295, 885, 6263, 538, 257, 3554, 786, 11, 597, 576, 914, 291, 458, 11, 51208], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2443, "seek": 931848, "start": 9335.359999999999, "end": 9340.88, "text": " 70% cold to cold and 80% hot to hot. On each day, the temperature is normally distributed", "tokens": [51208, 5285, 4, 3554, 281, 3554, 293, 4688, 4, 2368, 281, 2368, 13, 1282, 1184, 786, 11, 264, 4292, 307, 5646, 12631, 51484], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2444, "seek": 931848, "start": 9340.88, "end": 9345.119999999999, "text": " with mean and standard deviation zero and five on a cold day and mean and standard deviation", "tokens": [51484, 365, 914, 293, 3832, 25163, 4018, 293, 1732, 322, 257, 3554, 786, 293, 914, 293, 3832, 25163, 51696], "temperature": 0.0, "avg_logprob": -0.14208951677594867, "compression_ratio": 2.0894941634241246, "no_speech_prob": 0.08034463971853256}, {"id": 2445, "seek": 934512, "start": 9345.160000000002, "end": 9349.76, "text": " 15 and 10 on a hot day. Now what that means standard deviation is essentially I mean, we", "tokens": [50366, 2119, 293, 1266, 322, 257, 2368, 786, 13, 823, 437, 300, 1355, 3832, 25163, 307, 4476, 286, 914, 11, 321, 50596], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2446, "seek": 934512, "start": 9349.76, "end": 9354.52, "text": " can read this thing here is that on a hot day, the average temperature is 15 that's mean", "tokens": [50596, 393, 1401, 341, 551, 510, 307, 300, 322, 257, 2368, 786, 11, 264, 4274, 4292, 307, 2119, 300, 311, 914, 50834], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2447, "seek": 934512, "start": 9354.52, "end": 9359.84, "text": " and ranges from five to 25 because the standard deviation is 10 of that, which just means 10", "tokens": [50834, 293, 22526, 490, 1732, 281, 3552, 570, 264, 3832, 25163, 307, 1266, 295, 300, 11, 597, 445, 1355, 1266, 51100], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2448, "seek": 934512, "start": 9359.84, "end": 9364.68, "text": " on each side kind of the min max value. Again, I'm not in statistics. So please don't quote", "tokens": [51100, 322, 1184, 1252, 733, 295, 264, 923, 11469, 2158, 13, 3764, 11, 286, 478, 406, 294, 12523, 13, 407, 1767, 500, 380, 6513, 51342], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2449, "seek": 934512, "start": 9364.68, "end": 9368.480000000001, "text": " me on any definitions of standard deviation. I just trying to explain it enough so that", "tokens": [51342, 385, 322, 604, 21988, 295, 3832, 25163, 13, 286, 445, 1382, 281, 2903, 309, 1547, 370, 300, 51532], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2450, "seek": 934512, "start": 9368.480000000001, "end": 9373.2, "text": " you guys can understand what we're doing. Okay, so what we're going to do to model this", "tokens": [51532, 291, 1074, 393, 1223, 437, 321, 434, 884, 13, 1033, 11, 370, 437, 321, 434, 516, 281, 360, 281, 2316, 341, 51768], "temperature": 0.0, "avg_logprob": -0.11695401348284822, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.2507958710193634}, {"id": 2451, "seek": 937320, "start": 9373.2, "end": 9377.16, "text": " and I'm just kind of going through this fairly quickly because it's pretty easy to really do", "tokens": [50364, 293, 286, 478, 445, 733, 295, 516, 807, 341, 6457, 2661, 570, 309, 311, 1238, 1858, 281, 534, 360, 50562], "temperature": 0.0, "avg_logprob": -0.12074005498295337, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.08754651248455048}, {"id": 2452, "seek": 937320, "start": 9377.16, "end": 9383.92, "text": " this is I'm going to load the TensorFlow probability distributions kind of module and just save", "tokens": [50562, 341, 307, 286, 478, 516, 281, 3677, 264, 37624, 8482, 37870, 733, 295, 10088, 293, 445, 3155, 50900], "temperature": 0.0, "avg_logprob": -0.12074005498295337, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.08754651248455048}, {"id": 2453, "seek": 937320, "start": 9383.92, "end": 9389.400000000001, "text": " that as TFD. And I'm just going to do that so I don't need to write TFP dot distributions dot", "tokens": [50900, 300, 382, 40964, 35, 13, 400, 286, 478, 445, 516, 281, 360, 300, 370, 286, 500, 380, 643, 281, 2464, 40964, 47, 5893, 37870, 5893, 51174], "temperature": 0.0, "avg_logprob": -0.12074005498295337, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.08754651248455048}, {"id": 2454, "seek": 937320, "start": 9389.400000000001, "end": 9393.400000000001, "text": " all of this, I can just kind of shortcut it. So you'll notice I'm referencing TFD here,", "tokens": [51174, 439, 295, 341, 11, 286, 393, 445, 733, 295, 24822, 309, 13, 407, 291, 603, 3449, 286, 478, 40582, 40964, 35, 510, 11, 51374], "temperature": 0.0, "avg_logprob": -0.12074005498295337, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.08754651248455048}, {"id": 2455, "seek": 937320, "start": 9393.400000000001, "end": 9398.560000000001, "text": " which just stands for TFP dot distributions and TFP is TensorFlow probability. Okay,", "tokens": [51374, 597, 445, 7382, 337, 40964, 47, 5893, 37870, 293, 40964, 47, 307, 37624, 8482, 13, 1033, 11, 51632], "temperature": 0.0, "avg_logprob": -0.12074005498295337, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.08754651248455048}, {"id": 2456, "seek": 939856, "start": 9398.72, "end": 9405.199999999999, "text": " so my initial distribution is TensorFlow probability distributions dot categorical. This", "tokens": [50372, 370, 452, 5883, 7316, 307, 37624, 8482, 37870, 5893, 19250, 804, 13, 639, 50696], "temperature": 0.0, "avg_logprob": -0.13189678892083123, "compression_ratio": 1.80078125, "no_speech_prob": 0.006692206021398306}, {"id": 2457, "seek": 939856, "start": 9405.199999999999, "end": 9410.56, "text": " is probability of 80% and 20%. Now this refers to point two. So let's look at point two. The", "tokens": [50696, 307, 8482, 295, 4688, 4, 293, 945, 6856, 823, 341, 14942, 281, 935, 732, 13, 407, 718, 311, 574, 412, 935, 732, 13, 440, 50964], "temperature": 0.0, "avg_logprob": -0.13189678892083123, "compression_ratio": 1.80078125, "no_speech_prob": 0.006692206021398306}, {"id": 2458, "seek": 939856, "start": 9410.56, "end": 9415.039999999999, "text": " first day in our sequence has an 80% chance of being cold. So we're saying that that's", "tokens": [50964, 700, 786, 294, 527, 8310, 575, 364, 4688, 4, 2931, 295, 885, 3554, 13, 407, 321, 434, 1566, 300, 300, 311, 51188], "temperature": 0.0, "avg_logprob": -0.13189678892083123, "compression_ratio": 1.80078125, "no_speech_prob": 0.006692206021398306}, {"id": 2459, "seek": 939856, "start": 9415.039999999999, "end": 9420.64, "text": " essentially what this is the initial distribution of being cold is 80%. And then 20% after categorical", "tokens": [51188, 4476, 437, 341, 307, 264, 5883, 7316, 295, 885, 3554, 307, 4688, 6856, 400, 550, 945, 4, 934, 19250, 804, 51468], "temperature": 0.0, "avg_logprob": -0.13189678892083123, "compression_ratio": 1.80078125, "no_speech_prob": 0.006692206021398306}, {"id": 2460, "seek": 939856, "start": 9420.64, "end": 9426.439999999999, "text": " is just a way that we can do this distribution. Okay, so transition distribution. What is", "tokens": [51468, 307, 445, 257, 636, 300, 321, 393, 360, 341, 7316, 13, 1033, 11, 370, 6034, 7316, 13, 708, 307, 51758], "temperature": 0.0, "avg_logprob": -0.13189678892083123, "compression_ratio": 1.80078125, "no_speech_prob": 0.006692206021398306}, {"id": 2461, "seek": 942644, "start": 9426.44, "end": 9434.52, "text": " it TensorFlow probability categorical, the probability is 70% and 30% and 20% 80%. Now", "tokens": [50364, 309, 37624, 8482, 19250, 804, 11, 264, 8482, 307, 5285, 4, 293, 2217, 4, 293, 945, 4, 4688, 6856, 823, 50768], "temperature": 0.0, "avg_logprob": -0.11965198176247734, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.008576410822570324}, {"id": 2462, "seek": 942644, "start": 9434.52, "end": 9439.880000000001, "text": " notice that since we have two states, we've defined two probabilities. Notice since we have two", "tokens": [50768, 3449, 300, 1670, 321, 362, 732, 4368, 11, 321, 600, 7642, 732, 33783, 13, 13428, 1670, 321, 362, 732, 51036], "temperature": 0.0, "avg_logprob": -0.11965198176247734, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.008576410822570324}, {"id": 2463, "seek": 942644, "start": 9439.880000000001, "end": 9444.6, "text": " states, we have defined two probabilities, the probability of landing on each of these states", "tokens": [51036, 4368, 11, 321, 362, 7642, 732, 33783, 11, 264, 8482, 295, 11202, 322, 1184, 295, 613, 4368, 51272], "temperature": 0.0, "avg_logprob": -0.11965198176247734, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.008576410822570324}, {"id": 2464, "seek": 942644, "start": 9444.6, "end": 9448.92, "text": " at the very beginning of our sequence. This is the transition probability referred to points three", "tokens": [51272, 412, 264, 588, 2863, 295, 527, 8310, 13, 639, 307, 264, 6034, 8482, 10839, 281, 2793, 1045, 51488], "temperature": 0.0, "avg_logprob": -0.11965198176247734, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.008576410822570324}, {"id": 2465, "seek": 942644, "start": 9448.92, "end": 9456.12, "text": " and four above. So this is what we have here. So cold is 30% chance 20% chance for a hot day. And", "tokens": [51488, 293, 1451, 3673, 13, 407, 341, 307, 437, 321, 362, 510, 13, 407, 3554, 307, 2217, 4, 2931, 945, 4, 2931, 337, 257, 2368, 786, 13, 400, 51848], "temperature": 0.0, "avg_logprob": -0.11965198176247734, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.008576410822570324}, {"id": 2466, "seek": 945612, "start": 9456.12, "end": 9460.68, "text": " that's what we've defined. So we say this is going to be cold day state one, we have 70% chance", "tokens": [50364, 300, 311, 437, 321, 600, 7642, 13, 407, 321, 584, 341, 307, 516, 281, 312, 3554, 786, 1785, 472, 11, 321, 362, 5285, 4, 2931, 50592], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2467, "seek": 945612, "start": 9460.68, "end": 9464.84, "text": " of being cold day again, we have 30% chance of going hot day and then you know, reverse here.", "tokens": [50592, 295, 885, 3554, 786, 797, 11, 321, 362, 2217, 4, 2931, 295, 516, 2368, 786, 293, 550, 291, 458, 11, 9943, 510, 13, 50800], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2468, "seek": 945612, "start": 9465.720000000001, "end": 9471.400000000001, "text": " Okay, so observation distribution. Now this one is a little bit different, but essentially we do", "tokens": [50844, 1033, 11, 370, 14816, 7316, 13, 823, 341, 472, 307, 257, 707, 857, 819, 11, 457, 4476, 321, 360, 51128], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2469, "seek": 945612, "start": 9471.400000000001, "end": 9476.68, "text": " tfd dot normal. Now I don't know, I'm not going to explain exactly what all this is, but when you're", "tokens": [51128, 256, 69, 67, 5893, 2710, 13, 823, 286, 500, 380, 458, 11, 286, 478, 406, 516, 281, 2903, 2293, 437, 439, 341, 307, 11, 457, 562, 291, 434, 51392], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2470, "seek": 945612, "start": 9476.68, "end": 9480.52, "text": " doing standard deviation, you're going to do it like this, where you're going to say, look, which", "tokens": [51392, 884, 3832, 25163, 11, 291, 434, 516, 281, 360, 309, 411, 341, 11, 689, 291, 434, 516, 281, 584, 11, 574, 11, 597, 51584], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2471, "seek": 945612, "start": 9480.52, "end": 9485.0, "text": " stands for your average or your mean, right? So that was our average temperature is going to be", "tokens": [51584, 7382, 337, 428, 4274, 420, 428, 914, 11, 558, 30, 407, 300, 390, 527, 4274, 4292, 307, 516, 281, 312, 51808], "temperature": 0.0, "avg_logprob": -0.11038348549290707, "compression_ratio": 1.8213166144200628, "no_speech_prob": 0.0014102617278695107}, {"id": 2472, "seek": 948500, "start": 9485.0, "end": 9490.6, "text": " zero on a hot day, 15 on a cold day. The standard deviation on the cold days five, which means we", "tokens": [50364, 4018, 322, 257, 2368, 786, 11, 2119, 322, 257, 3554, 786, 13, 440, 3832, 25163, 322, 264, 3554, 1708, 1732, 11, 597, 1355, 321, 50644], "temperature": 0.0, "avg_logprob": -0.09979047931608606, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0028007961809635162}, {"id": 2473, "seek": 948500, "start": 9490.6, "end": 9497.4, "text": " range from five, or negative five to five degrees. And on a hot day, it's 10. So that is going to be", "tokens": [50644, 3613, 490, 1732, 11, 420, 3671, 1732, 281, 1732, 5310, 13, 400, 322, 257, 2368, 786, 11, 309, 311, 1266, 13, 407, 300, 307, 516, 281, 312, 50984], "temperature": 0.0, "avg_logprob": -0.09979047931608606, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0028007961809635162}, {"id": 2474, "seek": 948500, "start": 9497.4, "end": 9502.36, "text": " we go range from five to 25 degrees, and our average temperature is 15. Now the reason we've", "tokens": [50984, 321, 352, 3613, 490, 1732, 281, 3552, 5310, 11, 293, 527, 4274, 4292, 307, 2119, 13, 823, 264, 1778, 321, 600, 51232], "temperature": 0.0, "avg_logprob": -0.09979047931608606, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0028007961809635162}, {"id": 2475, "seek": 948500, "start": 9502.36, "end": 9507.24, "text": " added dot here is because these just need to be float values. So rather than inserting integers", "tokens": [51232, 3869, 5893, 510, 307, 570, 613, 445, 643, 281, 312, 15706, 4190, 13, 407, 2831, 813, 46567, 41674, 51476], "temperature": 0.0, "avg_logprob": -0.09979047931608606, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0028007961809635162}, {"id": 2476, "seek": 948500, "start": 9507.24, "end": 9512.68, "text": " here and having potentially type errors later on, we just have floats. Okay, so the low argument", "tokens": [51476, 510, 293, 1419, 7263, 2010, 13603, 1780, 322, 11, 321, 445, 362, 37878, 13, 1033, 11, 370, 264, 2295, 6770, 51748], "temperature": 0.0, "avg_logprob": -0.09979047931608606, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0028007961809635162}, {"id": 2477, "seek": 951268, "start": 9512.68, "end": 9515.800000000001, "text": " represents the mean and the scales of standard deviation. Yeah, exactly what we just defined", "tokens": [50364, 8855, 264, 914, 293, 264, 17408, 295, 3832, 25163, 13, 865, 11, 2293, 437, 321, 445, 7642, 50520], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2478, "seek": 951268, "start": 9515.800000000001, "end": 9520.52, "text": " there. Alright, so let's run this, I think we actually already did. And now we can create our", "tokens": [50520, 456, 13, 2798, 11, 370, 718, 311, 1190, 341, 11, 286, 519, 321, 767, 1217, 630, 13, 400, 586, 321, 393, 1884, 527, 50756], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2479, "seek": 951268, "start": 9520.52, "end": 9524.76, "text": " model. So to create the models pretty easy. I mean, all we do is say model equals TensorFlow", "tokens": [50756, 2316, 13, 407, 281, 1884, 264, 5245, 1238, 1858, 13, 286, 914, 11, 439, 321, 360, 307, 584, 2316, 6915, 37624, 50968], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2480, "seek": 951268, "start": 9524.76, "end": 9528.92, "text": " distribution dot hidden Markov model, give it the initial distribution, which is equal to", "tokens": [50968, 7316, 5893, 7633, 3934, 5179, 2316, 11, 976, 309, 264, 5883, 7316, 11, 597, 307, 2681, 281, 51176], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2481, "seek": 951268, "start": 9528.92, "end": 9533.960000000001, "text": " initial distribution, transition distribution, observation, distribution and steps. Now what", "tokens": [51176, 5883, 7316, 11, 6034, 7316, 11, 14816, 11, 7316, 293, 4439, 13, 823, 437, 51428], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2482, "seek": 951268, "start": 9533.960000000001, "end": 9539.720000000001, "text": " is steps? Well, steps is how many days we want to predict for. So the number of steps is how many", "tokens": [51428, 307, 4439, 30, 1042, 11, 4439, 307, 577, 867, 1708, 321, 528, 281, 6069, 337, 13, 407, 264, 1230, 295, 4439, 307, 577, 867, 51716], "temperature": 0.0, "avg_logprob": -0.09595171855046199, "compression_ratio": 1.8006430868167203, "no_speech_prob": 0.005059781018644571}, {"id": 2483, "seek": 953972, "start": 9539.72, "end": 9545.24, "text": " times we're going to step through this probability cycle, and run the model essentially. Now remember,", "tokens": [50364, 1413, 321, 434, 516, 281, 1823, 807, 341, 8482, 6586, 11, 293, 1190, 264, 2316, 4476, 13, 823, 1604, 11, 50640], "temperature": 0.0, "avg_logprob": -0.10759726515761367, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.005910541396588087}, {"id": 2484, "seek": 953972, "start": 9545.24, "end": 9549.24, "text": " what we want to do is we want to predict the average temperature on each day, right? Like", "tokens": [50640, 437, 321, 528, 281, 360, 307, 321, 528, 281, 6069, 264, 4274, 4292, 322, 1184, 786, 11, 558, 30, 1743, 50840], "temperature": 0.0, "avg_logprob": -0.10759726515761367, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.005910541396588087}, {"id": 2485, "seek": 953972, "start": 9549.24, "end": 9553.64, "text": " that's what the goal of our example is is to predict the average temperature. So given this", "tokens": [50840, 300, 311, 437, 264, 3387, 295, 527, 1365, 307, 307, 281, 6069, 264, 4274, 4292, 13, 407, 2212, 341, 51060], "temperature": 0.0, "avg_logprob": -0.10759726515761367, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.005910541396588087}, {"id": 2486, "seek": 953972, "start": 9553.64, "end": 9559.24, "text": " information, using these observations and using these transitions, what we'll do is predict that.", "tokens": [51060, 1589, 11, 1228, 613, 18163, 293, 1228, 613, 23767, 11, 437, 321, 603, 360, 307, 6069, 300, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10759726515761367, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.005910541396588087}, {"id": 2487, "seek": 953972, "start": 9559.24, "end": 9565.24, "text": " So I'm going to run this model. What is the issue here? tensor is on hash of tensor is", "tokens": [51340, 407, 286, 478, 516, 281, 1190, 341, 2316, 13, 708, 307, 264, 2734, 510, 30, 40863, 307, 322, 22019, 295, 40863, 307, 51640], "temperature": 0.0, "avg_logprob": -0.10759726515761367, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.005910541396588087}, {"id": 2488, "seek": 956524, "start": 9565.4, "end": 9569.48, "text": " okay, give me one sec, I'll have a look here, though I haven't had this issue before. Okay,", "tokens": [50372, 1392, 11, 976, 385, 472, 907, 11, 286, 603, 362, 257, 574, 510, 11, 1673, 286, 2378, 380, 632, 341, 2734, 949, 13, 1033, 11, 50576], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2489, "seek": 956524, "start": 9569.48, "end": 9574.52, "text": " so after a painful amount of searching on stack overflow and Google and actually just reading", "tokens": [50576, 370, 934, 257, 11697, 2372, 295, 10808, 322, 8630, 37772, 293, 3329, 293, 767, 445, 3760, 50828], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2490, "seek": 956524, "start": 9574.52, "end": 9579.32, "text": " through more documentation on TensorFlow, I have determined the issue. So remember the error was", "tokens": [50828, 807, 544, 14333, 322, 37624, 11, 286, 362, 9540, 264, 2734, 13, 407, 1604, 264, 6713, 390, 51068], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2491, "seek": 956524, "start": 9579.32, "end": 9583.64, "text": " we were getting on actually this line here, I think I can see what the output is. Oh, this", "tokens": [51068, 321, 645, 1242, 322, 767, 341, 1622, 510, 11, 286, 519, 286, 393, 536, 437, 264, 5598, 307, 13, 876, 11, 341, 51284], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2492, "seek": 956524, "start": 9583.64, "end": 9587.0, "text": " okay, well, this is a different error. But anyways, there was an error at this line. Essentially,", "tokens": [51284, 1392, 11, 731, 11, 341, 307, 257, 819, 6713, 13, 583, 13448, 11, 456, 390, 364, 6713, 412, 341, 1622, 13, 23596, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2493, "seek": 956524, "start": 9587.0, "end": 9591.8, "text": " what was happening is we have a mismatch between the two versions here. So the most recent version", "tokens": [51452, 437, 390, 2737, 307, 321, 362, 257, 23220, 852, 1296, 264, 732, 9606, 510, 13, 407, 264, 881, 5162, 3037, 51692], "temperature": 0.0, "avg_logprob": -0.1683318145083685, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.015422157943248749}, {"id": 2494, "seek": 959180, "start": 9592.439999999999, "end": 9598.119999999999, "text": " of TensorFlow is not compatible with the older version of TensorFlow probability, at least in", "tokens": [50396, 295, 37624, 307, 406, 18218, 365, 264, 4906, 3037, 295, 37624, 8482, 11, 412, 1935, 294, 50680], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2495, "seek": 959180, "start": 9598.119999999999, "end": 9601.72, "text": " the sense that the things that we're trying to do with it. So I just need to make sure that I", "tokens": [50680, 264, 2020, 300, 264, 721, 300, 321, 434, 1382, 281, 360, 365, 309, 13, 407, 286, 445, 643, 281, 652, 988, 300, 286, 50860], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2496, "seek": 959180, "start": 9601.72, "end": 9607.16, "text": " installed the most recent version of TensorFlow probability. So what you need to do if this", "tokens": [50860, 8899, 264, 881, 5162, 3037, 295, 37624, 8482, 13, 407, 437, 291, 643, 281, 360, 498, 341, 51132], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2497, "seek": 959180, "start": 9607.16, "end": 9611.96, "text": " is in your notebook, and this should actually work fine for you guys, because this will be updated", "tokens": [51132, 307, 294, 428, 21060, 11, 293, 341, 820, 767, 589, 2489, 337, 291, 1074, 11, 570, 341, 486, 312, 10588, 51372], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2498, "seek": 959180, "start": 9611.96, "end": 9615.96, "text": " by the time you get there. But in case you run into the issue, I'll, you know, deal with it.", "tokens": [51372, 538, 264, 565, 291, 483, 456, 13, 583, 294, 1389, 291, 1190, 666, 264, 2734, 11, 286, 603, 11, 291, 458, 11, 2028, 365, 309, 13, 51572], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2499, "seek": 959180, "start": 9615.96, "end": 9620.359999999999, "text": " But essentially, we're going to select version 2.x of TensorFlow, you're going to run this", "tokens": [51572, 583, 4476, 11, 321, 434, 516, 281, 3048, 3037, 568, 13, 87, 295, 37624, 11, 291, 434, 516, 281, 1190, 341, 51792], "temperature": 0.0, "avg_logprob": -0.07262451979365663, "compression_ratio": 1.8986486486486487, "no_speech_prob": 0.029304547235369682}, {"id": 2500, "seek": 962036, "start": 9620.44, "end": 9625.16, "text": " install commands, you're going to install TensorFlow probability, just run this command.", "tokens": [50368, 3625, 16901, 11, 291, 434, 516, 281, 3625, 37624, 8482, 11, 445, 1190, 341, 5622, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2501, "seek": 962036, "start": 9625.16, "end": 9630.12, "text": " Then after you run this command, you're going to need to restart your run times, go to run time,", "tokens": [50604, 1396, 934, 291, 1190, 341, 5622, 11, 291, 434, 516, 281, 643, 281, 21022, 428, 1190, 1413, 11, 352, 281, 1190, 565, 11, 50852], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2502, "seek": 962036, "start": 9630.12, "end": 9634.28, "text": " and then restart run time. And then you can just continue on with the script, select TensorFlow", "tokens": [50852, 293, 550, 21022, 1190, 565, 13, 400, 550, 291, 393, 445, 2354, 322, 365, 264, 5755, 11, 3048, 37624, 51060], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2503, "seek": 962036, "start": 9634.28, "end": 9639.0, "text": " 2.x again, do your imports, and then you know, we'll test if this is actually going to work for", "tokens": [51060, 568, 13, 87, 797, 11, 360, 428, 41596, 11, 293, 550, 291, 458, 11, 321, 603, 1500, 498, 341, 307, 767, 516, 281, 589, 337, 51296], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2504, "seek": 962036, "start": 9639.0, "end": 9644.84, "text": " us here, run our distributions, create the model without any issues this time, notice no red text,", "tokens": [51296, 505, 510, 11, 1190, 527, 37870, 11, 1884, 264, 2316, 1553, 604, 2663, 341, 565, 11, 3449, 572, 2182, 2487, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2505, "seek": 962036, "start": 9644.84, "end": 9649.640000000001, "text": " and then run this final line, which will give you the output. Now, this is what I wanted to", "tokens": [51588, 293, 550, 1190, 341, 2572, 1622, 11, 597, 486, 976, 291, 264, 5598, 13, 823, 11, 341, 307, 437, 286, 1415, 281, 51828], "temperature": 0.0, "avg_logprob": -0.09165340151105608, "compression_ratio": 1.862295081967213, "no_speech_prob": 0.00806090235710144}, {"id": 2506, "seek": 964964, "start": 9649.64, "end": 9653.88, "text": " talk about here that we didn't quite get to because we were having some bugs. But this is how we", "tokens": [50364, 751, 466, 510, 300, 321, 994, 380, 1596, 483, 281, 570, 321, 645, 1419, 512, 15120, 13, 583, 341, 307, 577, 321, 50576], "temperature": 0.0, "avg_logprob": -0.0712035363461791, "compression_ratio": 1.8264150943396227, "no_speech_prob": 0.007345210295170546}, {"id": 2507, "seek": 964964, "start": 9653.88, "end": 9659.48, "text": " can actually kind of run our model and see the output. So what you can do is do model dot mean,", "tokens": [50576, 393, 767, 733, 295, 1190, 527, 2316, 293, 536, 264, 5598, 13, 407, 437, 291, 393, 360, 307, 360, 2316, 5893, 914, 11, 50856], "temperature": 0.0, "avg_logprob": -0.0712035363461791, "compression_ratio": 1.8264150943396227, "no_speech_prob": 0.007345210295170546}, {"id": 2508, "seek": 964964, "start": 9659.48, "end": 9664.68, "text": " so you say mean equals model dot mean. And what this is going to do is essentially just calculate", "tokens": [50856, 370, 291, 584, 914, 6915, 2316, 5893, 914, 13, 400, 437, 341, 307, 516, 281, 360, 307, 4476, 445, 8873, 51116], "temperature": 0.0, "avg_logprob": -0.0712035363461791, "compression_ratio": 1.8264150943396227, "no_speech_prob": 0.007345210295170546}, {"id": 2509, "seek": 964964, "start": 9664.68, "end": 9670.68, "text": " the probability is going to essentially take that from the model. Now, when we have model dot mean,", "tokens": [51116, 264, 8482, 307, 516, 281, 4476, 747, 300, 490, 264, 2316, 13, 823, 11, 562, 321, 362, 2316, 5893, 914, 11, 51416], "temperature": 0.0, "avg_logprob": -0.0712035363461791, "compression_ratio": 1.8264150943396227, "no_speech_prob": 0.007345210295170546}, {"id": 2510, "seek": 964964, "start": 9670.68, "end": 9675.16, "text": " this is what we call, you know, a partially defined tensor. So remember our tensors were like", "tokens": [51416, 341, 307, 437, 321, 818, 11, 291, 458, 11, 257, 18886, 7642, 40863, 13, 407, 1604, 527, 10688, 830, 645, 411, 51640], "temperature": 0.0, "avg_logprob": -0.0712035363461791, "compression_ratio": 1.8264150943396227, "no_speech_prob": 0.007345210295170546}, {"id": 2511, "seek": 967516, "start": 9675.24, "end": 9679.72, "text": " partially defined computations. Well, that's what model dot mean actually is. That's what", "tokens": [50368, 18886, 7642, 2807, 763, 13, 1042, 11, 300, 311, 437, 2316, 5893, 914, 767, 307, 13, 663, 311, 437, 50592], "temperature": 0.0, "avg_logprob": -0.07441963412897373, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.04602763429284096}, {"id": 2512, "seek": 967516, "start": 9679.72, "end": 9685.24, "text": " this method is. So if we want to get the value of that, what we actually need to do is create a", "tokens": [50592, 341, 3170, 307, 13, 407, 498, 321, 528, 281, 483, 264, 2158, 295, 300, 11, 437, 321, 767, 643, 281, 360, 307, 1884, 257, 50868], "temperature": 0.0, "avg_logprob": -0.07441963412897373, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.04602763429284096}, {"id": 2513, "seek": 967516, "start": 9685.24, "end": 9690.44, "text": " new session in TensorFlow, run this part of the graph, which we're going to get by doing mean", "tokens": [50868, 777, 5481, 294, 37624, 11, 1190, 341, 644, 295, 264, 4295, 11, 597, 321, 434, 516, 281, 483, 538, 884, 914, 51128], "temperature": 0.0, "avg_logprob": -0.07441963412897373, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.04602763429284096}, {"id": 2514, "seek": 967516, "start": 9690.44, "end": 9694.52, "text": " dot numpy, and then we can print that out. So I know this might seem a little bit confusing,", "tokens": [51128, 5893, 1031, 8200, 11, 293, 550, 321, 393, 4482, 300, 484, 13, 407, 286, 458, 341, 1062, 1643, 257, 707, 857, 13181, 11, 51332], "temperature": 0.0, "avg_logprob": -0.07441963412897373, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.04602763429284096}, {"id": 2515, "seek": 967516, "start": 9694.52, "end": 9700.039999999999, "text": " but essentially to run a session in the new version of TensorFlow, so 2.x, or 2.1 or whatever", "tokens": [51332, 457, 4476, 281, 1190, 257, 5481, 294, 264, 777, 3037, 295, 37624, 11, 370, 568, 13, 87, 11, 420, 568, 13, 16, 420, 2035, 51608], "temperature": 0.0, "avg_logprob": -0.07441963412897373, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.04602763429284096}, {"id": 2516, "seek": 970004, "start": 9700.04, "end": 9707.400000000001, "text": " it is, you're going to type with TF dot compact dot v one dot session as sesh. And then I mean,", "tokens": [50364, 309, 307, 11, 291, 434, 516, 281, 2010, 365, 40964, 5893, 14679, 5893, 371, 472, 5893, 5481, 382, 5385, 71, 13, 400, 550, 286, 914, 11, 50732], "temperature": 0.0, "avg_logprob": -0.09657053984412851, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.16881531476974487}, {"id": 2517, "seek": 970004, "start": 9707.400000000001, "end": 9710.76, "text": " this doesn't really matter what you have here, but whatever you want. And then what I'm doing is", "tokens": [50732, 341, 1177, 380, 534, 1871, 437, 291, 362, 510, 11, 457, 2035, 291, 528, 13, 400, 550, 437, 286, 478, 884, 307, 50900], "temperature": 0.0, "avg_logprob": -0.09657053984412851, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.16881531476974487}, {"id": 2518, "seek": 970004, "start": 9710.76, "end": 9715.480000000001, "text": " just printing mean dot numpy. So to actually get the value from this here, this variable,", "tokens": [50900, 445, 14699, 914, 5893, 1031, 8200, 13, 407, 281, 767, 483, 264, 2158, 490, 341, 510, 11, 341, 7006, 11, 51136], "temperature": 0.0, "avg_logprob": -0.09657053984412851, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.16881531476974487}, {"id": 2519, "seek": 970004, "start": 9715.480000000001, "end": 9720.28, "text": " I call dot numpy. And then what it does is print out this array that gives me the expected", "tokens": [51136, 286, 818, 5893, 1031, 8200, 13, 400, 550, 437, 309, 775, 307, 4482, 484, 341, 10225, 300, 2709, 385, 264, 5176, 51376], "temperature": 0.0, "avg_logprob": -0.09657053984412851, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.16881531476974487}, {"id": 2520, "seek": 970004, "start": 9720.28, "end": 9727.0, "text": " temperatures on each day. So we have, you know, three, six, essentially 7.5, 8.25. And you can", "tokens": [51376, 12633, 322, 1184, 786, 13, 407, 321, 362, 11, 291, 458, 11, 1045, 11, 2309, 11, 4476, 1614, 13, 20, 11, 1649, 13, 6074, 13, 400, 291, 393, 51712], "temperature": 0.0, "avg_logprob": -0.09657053984412851, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.16881531476974487}, {"id": 2521, "seek": 972700, "start": 9727.08, "end": 9732.12, "text": " see these are the temperatures, based on the fact that we start with an initial probability of", "tokens": [50368, 536, 613, 366, 264, 12633, 11, 2361, 322, 264, 1186, 300, 321, 722, 365, 364, 5883, 8482, 295, 50620], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2522, "seek": 972700, "start": 9732.12, "end": 9736.36, "text": " starting on a cold day. So we kind of get that here, right, we're starting at three degrees.", "tokens": [50620, 2891, 322, 257, 3554, 786, 13, 407, 321, 733, 295, 483, 300, 510, 11, 558, 11, 321, 434, 2891, 412, 1045, 5310, 13, 50832], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2523, "seek": 972700, "start": 9736.36, "end": 9740.68, "text": " That's what it's determined, we're going to start at. And then we have all of these other", "tokens": [50832, 663, 311, 437, 309, 311, 9540, 11, 321, 434, 516, 281, 722, 412, 13, 400, 550, 321, 362, 439, 295, 613, 661, 51048], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2524, "seek": 972700, "start": 9740.68, "end": 9745.96, "text": " temperatures is predicting for the next days. Now notice if we recreate this model, so just", "tokens": [51048, 12633, 307, 32884, 337, 264, 958, 1708, 13, 823, 3449, 498, 321, 25833, 341, 2316, 11, 370, 445, 51312], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2525, "seek": 972700, "start": 9745.96, "end": 9750.76, "text": " rerun the distributions, rerun them and go model dot mean again, this stays the same, right? Well,", "tokens": [51312, 43819, 409, 264, 37870, 11, 43819, 409, 552, 293, 352, 2316, 5893, 914, 797, 11, 341, 10834, 264, 912, 11, 558, 30, 1042, 11, 51552], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2526, "seek": 972700, "start": 9750.76, "end": 9754.36, "text": " because our probabilities are the same, this model is going to do the calculation the exact", "tokens": [51552, 570, 527, 33783, 366, 264, 912, 11, 341, 2316, 307, 516, 281, 360, 264, 17108, 264, 1900, 51732], "temperature": 0.0, "avg_logprob": -0.08482800511752858, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.007576822768896818}, {"id": 2527, "seek": 975436, "start": 9754.36, "end": 9758.76, "text": " same, there's not really any training that goes into this. So we get, you know, very similar,", "tokens": [50364, 912, 11, 456, 311, 406, 534, 604, 3097, 300, 1709, 666, 341, 13, 407, 321, 483, 11, 291, 458, 11, 588, 2531, 11, 50584], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2528, "seek": 975436, "start": 9758.76, "end": 9762.68, "text": " if not the exact same values, I can't remember if these are identical, but that's what it looks", "tokens": [50584, 498, 406, 264, 1900, 912, 4190, 11, 286, 393, 380, 1604, 498, 613, 366, 14800, 11, 457, 300, 311, 437, 309, 1542, 50780], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2529, "seek": 975436, "start": 9762.68, "end": 9766.6, "text": " like to me. I mean, we can run this again, see, we get the same one, and we'll create the model", "tokens": [50780, 411, 281, 385, 13, 286, 914, 11, 321, 393, 1190, 341, 797, 11, 536, 11, 321, 483, 264, 912, 472, 11, 293, 321, 603, 1884, 264, 2316, 50976], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2530, "seek": 975436, "start": 9766.6, "end": 9770.12, "text": " one more time. And let me just check these values here to make sure I'm not lying to you as yes,", "tokens": [50976, 472, 544, 565, 13, 400, 718, 385, 445, 1520, 613, 4190, 510, 281, 652, 988, 286, 478, 406, 8493, 281, 291, 382, 2086, 11, 51152], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2531, "seek": 975436, "start": 9770.12, "end": 9773.960000000001, "text": " they are the exact same. Okay, so let's start messing with a few probabilities and see what we", "tokens": [51152, 436, 366, 264, 1900, 912, 13, 1033, 11, 370, 718, 311, 722, 23258, 365, 257, 1326, 33783, 293, 536, 437, 321, 51344], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2532, "seek": 975436, "start": 9773.960000000001, "end": 9780.52, "text": " can do to this temperature and see what changes we can cause. So if I do 0.5 here, and I do 0.5", "tokens": [51344, 393, 360, 281, 341, 4292, 293, 536, 437, 2962, 321, 393, 3082, 13, 407, 498, 286, 360, 1958, 13, 20, 510, 11, 293, 286, 360, 1958, 13, 20, 51672], "temperature": 0.0, "avg_logprob": -0.061197634762937915, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221326284110546}, {"id": 2533, "seek": 978052, "start": 9780.6, "end": 9785.560000000001, "text": " for the categorical probability, remember this refers to points three and four above. So it's", "tokens": [50368, 337, 264, 19250, 804, 8482, 11, 1604, 341, 14942, 281, 2793, 1045, 293, 1451, 3673, 13, 407, 309, 311, 50616], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2534, "seek": 978052, "start": 9785.560000000001, "end": 9789.880000000001, "text": " a cold day has a 30% chance of being followed by hot day and then a hot day has a 20% chance of", "tokens": [50616, 257, 3554, 786, 575, 257, 2217, 4, 2931, 295, 885, 6263, 538, 2368, 786, 293, 550, 257, 2368, 786, 575, 257, 945, 4, 2931, 295, 50832], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2535, "seek": 978052, "start": 9789.880000000001, "end": 9793.960000000001, "text": " being followed by cold day. So what I've just done now is change the probability to be 50%", "tokens": [50832, 885, 6263, 538, 3554, 786, 13, 407, 437, 286, 600, 445, 1096, 586, 307, 1319, 264, 8482, 281, 312, 2625, 4, 51036], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2536, "seek": 978052, "start": 9794.76, "end": 9799.24, "text": " so that a cold day now has a 50% chance of being followed by hot day and a 50% chance of", "tokens": [51076, 370, 300, 257, 3554, 786, 586, 575, 257, 2625, 4, 2931, 295, 885, 6263, 538, 2368, 786, 293, 257, 2625, 4, 2931, 295, 51300], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2537, "seek": 978052, "start": 9799.24, "end": 9804.36, "text": " being followed by cold day. And let's recreate this model. Let's rerun this and let's see if", "tokens": [51300, 885, 6263, 538, 3554, 786, 13, 400, 718, 311, 25833, 341, 2316, 13, 961, 311, 43819, 409, 341, 293, 718, 311, 536, 498, 51556], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2538, "seek": 978052, "start": 9804.36, "end": 9809.720000000001, "text": " we get a difference. But we do notice this, the temperature now has been a is going a little bit", "tokens": [51556, 321, 483, 257, 2649, 13, 583, 321, 360, 3449, 341, 11, 264, 4292, 586, 575, 668, 257, 307, 516, 257, 707, 857, 51824], "temperature": 0.0, "avg_logprob": -0.08812194616616177, "compression_ratio": 2.04014598540146, "no_speech_prob": 0.19675062596797943}, {"id": 2539, "seek": 980972, "start": 9809.72, "end": 9814.439999999999, "text": " higher. Now notice that we get the same starting temperature because that's just the average", "tokens": [50364, 2946, 13, 823, 3449, 300, 321, 483, 264, 912, 2891, 4292, 570, 300, 311, 445, 264, 4274, 50600], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2540, "seek": 980972, "start": 9814.439999999999, "end": 9818.76, "text": " based on this probability that we have here. But if we wanted to potentially start, you know,", "tokens": [50600, 2361, 322, 341, 8482, 300, 321, 362, 510, 13, 583, 498, 321, 1415, 281, 7263, 722, 11, 291, 458, 11, 50816], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2541, "seek": 980972, "start": 9818.76, "end": 9824.84, "text": " hotter, we could reverse these numbers, we go 0.2 0.8. Let's rerun all of this. And now look at", "tokens": [50816, 32149, 11, 321, 727, 9943, 613, 3547, 11, 321, 352, 1958, 13, 17, 1958, 13, 23, 13, 961, 311, 43819, 409, 439, 295, 341, 13, 400, 586, 574, 412, 51120], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2542, "seek": 980972, "start": 9824.84, "end": 9830.039999999999, "text": " this what our temperatures are, we start at 12. And then we actually drop our temperature down to 10.", "tokens": [51120, 341, 437, 527, 12633, 366, 11, 321, 722, 412, 2272, 13, 400, 550, 321, 767, 3270, 527, 4292, 760, 281, 1266, 13, 51380], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2543, "seek": 980972, "start": 9830.039999999999, "end": 9834.119999999999, "text": " So that's how this hidden Markov model works. Now this is nice, because you can just tweak the", "tokens": [51380, 407, 300, 311, 577, 341, 7633, 3934, 5179, 2316, 1985, 13, 823, 341, 307, 1481, 11, 570, 291, 393, 445, 29879, 264, 51584], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2544, "seek": 980972, "start": 9834.119999999999, "end": 9838.519999999999, "text": " probabilities. This happens pretty well instantly. And we can have a look at our output very nicely.", "tokens": [51584, 33783, 13, 639, 2314, 1238, 731, 13518, 13, 400, 321, 393, 362, 257, 574, 412, 527, 5598, 588, 9594, 13, 51804], "temperature": 0.0, "avg_logprob": -0.08271027935875787, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.00884634256362915}, {"id": 2545, "seek": 983852, "start": 9838.52, "end": 9843.32, "text": " So obviously, this is representing the temperature on our like the first day, this would be the", "tokens": [50364, 407, 2745, 11, 341, 307, 13460, 264, 4292, 322, 527, 411, 264, 700, 786, 11, 341, 576, 312, 264, 50604], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2546, "seek": 983852, "start": 9843.32, "end": 9848.2, "text": " second day, third day, fourth day, fifth, sixth, seventh, and obviously, like the more days you", "tokens": [50604, 1150, 786, 11, 2636, 786, 11, 6409, 786, 11, 9266, 11, 15102, 11, 17875, 11, 293, 2745, 11, 411, 264, 544, 1708, 291, 50848], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2547, "seek": 983852, "start": 9848.2, "end": 9853.0, "text": " go on, the least accurate, this is probably going to be because it just runs off probability. And", "tokens": [50848, 352, 322, 11, 264, 1935, 8559, 11, 341, 307, 1391, 516, 281, 312, 570, 309, 445, 6676, 766, 8482, 13, 400, 51088], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2548, "seek": 983852, "start": 9853.0, "end": 9856.52, "text": " if you're going to try to predict, you know, a year in advance, and you're using the weather that you", "tokens": [51088, 498, 291, 434, 516, 281, 853, 281, 6069, 11, 291, 458, 11, 257, 1064, 294, 7295, 11, 293, 291, 434, 1228, 264, 5503, 300, 291, 51264], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2549, "seek": 983852, "start": 9856.52, "end": 9860.6, "text": " have from I guess the previous year, you're probably not going to get a very accurate prediction.", "tokens": [51264, 362, 490, 286, 2041, 264, 3894, 1064, 11, 291, 434, 1391, 406, 516, 281, 483, 257, 588, 8559, 17630, 13, 51468], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2550, "seek": 983852, "start": 9860.6, "end": 9864.44, "text": " But anyways, these are hidden Markov models. They're not like extremely useful. There's", "tokens": [51468, 583, 13448, 11, 613, 366, 7633, 3934, 5179, 5245, 13, 814, 434, 406, 411, 4664, 4420, 13, 821, 311, 51660], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2551, "seek": 983852, "start": 9864.44, "end": 9867.800000000001, "text": " some situations where you might want to use something like this. So that's why we're", "tokens": [51660, 512, 6851, 689, 291, 1062, 528, 281, 764, 746, 411, 341, 13, 407, 300, 311, 983, 321, 434, 51828], "temperature": 0.0, "avg_logprob": -0.07685539292993013, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.013635355047881603}, {"id": 2552, "seek": 986780, "start": 9867.8, "end": 9871.48, "text": " implementing them in kind of in this course and showing you how they work. It's also another", "tokens": [50364, 18114, 552, 294, 733, 295, 294, 341, 1164, 293, 4099, 291, 577, 436, 589, 13, 467, 311, 611, 1071, 50548], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2553, "seek": 986780, "start": 9871.48, "end": 9875.56, "text": " feature of TensorFlow that a lot of people don't talk about or see. And you know, personally,", "tokens": [50548, 4111, 295, 37624, 300, 257, 688, 295, 561, 500, 380, 751, 466, 420, 536, 13, 400, 291, 458, 11, 5665, 11, 50752], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2554, "seek": 986780, "start": 9875.56, "end": 9879.72, "text": " I hadn't really heard of hidden Markov models until I started developing this course. So anyways,", "tokens": [50752, 286, 8782, 380, 534, 2198, 295, 7633, 3934, 5179, 5245, 1826, 286, 1409, 6416, 341, 1164, 13, 407, 13448, 11, 50960], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2555, "seek": 986780, "start": 9879.72, "end": 9885.24, "text": " that has been it for this module. Now I hope that this kind of gave you guys a little bit of an idea", "tokens": [50960, 300, 575, 668, 309, 337, 341, 10088, 13, 823, 286, 1454, 300, 341, 733, 295, 2729, 291, 1074, 257, 707, 857, 295, 364, 1558, 51236], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2556, "seek": 986780, "start": 9885.24, "end": 9888.84, "text": " of how we can actually implement some of these machine learning algorithms, a little bit of", "tokens": [51236, 295, 577, 321, 393, 767, 4445, 512, 295, 613, 3479, 2539, 14642, 11, 257, 707, 857, 295, 51416], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2557, "seek": 986780, "start": 9888.84, "end": 9893.64, "text": " idea of how to work with data, how we can feed that to a model, the importance between testing", "tokens": [51416, 1558, 295, 577, 281, 589, 365, 1412, 11, 577, 321, 393, 3154, 300, 281, 257, 2316, 11, 264, 7379, 1296, 4997, 51656], "temperature": 0.0, "avg_logprob": -0.05544590950012207, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.005384587682783604}, {"id": 2558, "seek": 989364, "start": 9893.64, "end": 9897.8, "text": " and training data. And then obviously, linear regression is one we focused a lot on. So I", "tokens": [50364, 293, 3097, 1412, 13, 400, 550, 2745, 11, 8213, 24590, 307, 472, 321, 5178, 257, 688, 322, 13, 407, 286, 50572], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2559, "seek": 989364, "start": 9897.8, "end": 9901.8, "text": " hope you guys are very comfortable with that algorithm. And then what was the last the second", "tokens": [50572, 1454, 291, 1074, 366, 588, 4619, 365, 300, 9284, 13, 400, 550, 437, 390, 264, 1036, 264, 1150, 50772], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2560, "seek": 989364, "start": 9901.8, "end": 9905.64, "text": " one we did, I got to go up to remember exactly the sequence we had here. So classification,", "tokens": [50772, 472, 321, 630, 11, 286, 658, 281, 352, 493, 281, 1604, 2293, 264, 8310, 321, 632, 510, 13, 407, 21538, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2561, "seek": 989364, "start": 9905.64, "end": 9910.76, "text": " that one was important as well. So I hope you guys really understood that clustering, we didn't go", "tokens": [50964, 300, 472, 390, 1021, 382, 731, 13, 407, 286, 1454, 291, 1074, 534, 7320, 300, 596, 48673, 11, 321, 994, 380, 352, 51220], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2562, "seek": 989364, "start": 9910.76, "end": 9914.92, "text": " too far into that. But again, this is an interesting algorithm. And if you need to do some kind of", "tokens": [51220, 886, 1400, 666, 300, 13, 583, 797, 11, 341, 307, 364, 1880, 9284, 13, 400, 498, 291, 643, 281, 360, 512, 733, 295, 51428], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2563, "seek": 989364, "start": 9914.92, "end": 9919.32, "text": " clustering, you now know of one algorithm to do that called K means clustering, and you understand", "tokens": [51428, 596, 48673, 11, 291, 586, 458, 295, 472, 9284, 281, 360, 300, 1219, 591, 1355, 596, 48673, 11, 293, 291, 1223, 51648], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2564, "seek": 989364, "start": 9919.32, "end": 9923.08, "text": " how that works. And now you know, hidden Markov models. So in the next module, we're going to", "tokens": [51648, 577, 300, 1985, 13, 400, 586, 291, 458, 11, 7633, 3934, 5179, 5245, 13, 407, 294, 264, 958, 10088, 11, 321, 434, 516, 281, 51836], "temperature": 0.0, "avg_logprob": -0.07261216134736032, "compression_ratio": 1.8448753462603877, "no_speech_prob": 0.09533222019672394}, {"id": 2565, "seek": 992308, "start": 9923.08, "end": 9926.92, "text": " start covering neural networks, we now have the knowledge we need to really dive in there and", "tokens": [50364, 722, 10322, 18161, 9590, 11, 321, 586, 362, 264, 3601, 321, 643, 281, 534, 9192, 294, 456, 293, 50556], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2566, "seek": 992308, "start": 9926.92, "end": 9930.6, "text": " start doing some cool stuff. And then in the future modules, we're going to do deep computer", "tokens": [50556, 722, 884, 512, 1627, 1507, 13, 400, 550, 294, 264, 2027, 16679, 11, 321, 434, 516, 281, 360, 2452, 3820, 50740], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2567, "seek": 992308, "start": 9930.6, "end": 9934.76, "text": " vision, I believe we're going to do chatbots with recurrent neural networks, and then some form", "tokens": [50740, 5201, 11, 286, 1697, 321, 434, 516, 281, 360, 5081, 65, 1971, 365, 18680, 1753, 18161, 9590, 11, 293, 550, 512, 1254, 50948], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2568, "seek": 992308, "start": 9934.76, "end": 9939.32, "text": " of reinforcement learning at the end. So with that being said, let's go to the next module.", "tokens": [50948, 295, 29280, 2539, 412, 264, 917, 13, 407, 365, 300, 885, 848, 11, 718, 311, 352, 281, 264, 958, 10088, 13, 51176], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2569, "seek": 992308, "start": 9942.68, "end": 9947.24, "text": " Hello, everybody, and welcome to module four. Now in this module of this course, we're going to be", "tokens": [51344, 2425, 11, 2201, 11, 293, 2928, 281, 10088, 1451, 13, 823, 294, 341, 10088, 295, 341, 1164, 11, 321, 434, 516, 281, 312, 51572], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2570, "seek": 992308, "start": 9947.24, "end": 9951.96, "text": " talking about neural networks, discussing how neural networks work, a little bit of the math", "tokens": [51572, 1417, 466, 18161, 9590, 11, 10850, 577, 18161, 9590, 589, 11, 257, 707, 857, 295, 264, 5221, 51808], "temperature": 0.0, "avg_logprob": -0.07352833924470124, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.002182557713240385}, {"id": 2571, "seek": 995196, "start": 9951.96, "end": 9957.4, "text": " behind them, talking about gradient descent and back propagation, and how information actually", "tokens": [50364, 2261, 552, 11, 1417, 466, 16235, 23475, 293, 646, 38377, 11, 293, 577, 1589, 767, 50636], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2572, "seek": 995196, "start": 9957.4, "end": 9960.919999999998, "text": " flows through the neural network, and then getting into an example where we use a neural", "tokens": [50636, 12867, 807, 264, 18161, 3209, 11, 293, 550, 1242, 666, 364, 1365, 689, 321, 764, 257, 18161, 50812], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2573, "seek": 995196, "start": 9960.919999999998, "end": 9965.4, "text": " network to classify articles of clothing. So I know that was a lot, but that's what we're", "tokens": [50812, 3209, 281, 33872, 11290, 295, 11502, 13, 407, 286, 458, 300, 390, 257, 688, 11, 457, 300, 311, 437, 321, 434, 51036], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2574, "seek": 995196, "start": 9965.4, "end": 9970.039999999999, "text": " going to be covering here. Now neural networks are complex. There's kind of a lot of components", "tokens": [51036, 516, 281, 312, 10322, 510, 13, 823, 18161, 9590, 366, 3997, 13, 821, 311, 733, 295, 257, 688, 295, 6677, 51268], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2575, "seek": 995196, "start": 9970.039999999999, "end": 9973.8, "text": " that go into them. And I'm going to apologize right now, because it's very difficult to explain", "tokens": [51268, 300, 352, 666, 552, 13, 400, 286, 478, 516, 281, 12328, 558, 586, 11, 570, 309, 311, 588, 2252, 281, 2903, 51456], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2576, "seek": 995196, "start": 9973.8, "end": 9978.199999999999, "text": " it all at once. What I'm going to be trying to do is kind of piece things together and explain", "tokens": [51456, 309, 439, 412, 1564, 13, 708, 286, 478, 516, 281, 312, 1382, 281, 360, 307, 733, 295, 2522, 721, 1214, 293, 2903, 51676], "temperature": 0.0, "avg_logprob": -0.06661077646108773, "compression_ratio": 1.75, "no_speech_prob": 0.005219570826739073}, {"id": 2577, "seek": 997820, "start": 9978.28, "end": 9983.320000000002, "text": " them in blocks. And then at the end, you know, kind of combine everything together. Now I will", "tokens": [50368, 552, 294, 8474, 13, 400, 550, 412, 264, 917, 11, 291, 458, 11, 733, 295, 10432, 1203, 1214, 13, 823, 286, 486, 50620], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2578, "seek": 997820, "start": 9983.320000000002, "end": 9987.480000000001, "text": " say in case any of you didn't watch the beginning of this course, I do have very horrible handwriting,", "tokens": [50620, 584, 294, 1389, 604, 295, 291, 994, 380, 1159, 264, 2863, 295, 341, 1164, 11, 286, 360, 362, 588, 9263, 39179, 11, 50828], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2579, "seek": 997820, "start": 9987.480000000001, "end": 9991.880000000001, "text": " but this is the easiest way to explain things to you guys. So bear with me, you know, I'm sure", "tokens": [50828, 457, 341, 307, 264, 12889, 636, 281, 2903, 721, 281, 291, 1074, 13, 407, 6155, 365, 385, 11, 291, 458, 11, 286, 478, 988, 51048], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2580, "seek": 997820, "start": 9991.880000000001, "end": 9995.880000000001, "text": " you'll be able to understand what I'm saying, but it might just be painful to read some of it.", "tokens": [51048, 291, 603, 312, 1075, 281, 1223, 437, 286, 478, 1566, 11, 457, 309, 1062, 445, 312, 11697, 281, 1401, 512, 295, 309, 13, 51248], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2581, "seek": 997820, "start": 9995.880000000001, "end": 9999.640000000001, "text": " All right, so let's get into it right away and start discussing what neural networks are and", "tokens": [51248, 1057, 558, 11, 370, 718, 311, 483, 666, 309, 558, 1314, 293, 722, 10850, 437, 18161, 9590, 366, 293, 51436], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2582, "seek": 997820, "start": 9999.640000000001, "end": 10004.68, "text": " how they work. Well, the whole point of a neural network is to provide, you know, classification", "tokens": [51436, 577, 436, 589, 13, 1042, 11, 264, 1379, 935, 295, 257, 18161, 3209, 307, 281, 2893, 11, 291, 458, 11, 21538, 51688], "temperature": 0.0, "avg_logprob": -0.047403907775878905, "compression_ratio": 1.7121661721068249, "no_speech_prob": 0.03963000699877739}, {"id": 2583, "seek": 1000468, "start": 10004.68, "end": 10009.800000000001, "text": " or predictions for us. So we have some input information, we feed it to the neural network,", "tokens": [50364, 420, 21264, 337, 505, 13, 407, 321, 362, 512, 4846, 1589, 11, 321, 3154, 309, 281, 264, 18161, 3209, 11, 50620], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2584, "seek": 1000468, "start": 10009.800000000001, "end": 10013.960000000001, "text": " and then we want it to give us some output. So if we think of the neural network as this black box,", "tokens": [50620, 293, 550, 321, 528, 309, 281, 976, 505, 512, 5598, 13, 407, 498, 321, 519, 295, 264, 18161, 3209, 382, 341, 2211, 2424, 11, 50828], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2585, "seek": 1000468, "start": 10013.960000000001, "end": 10017.64, "text": " we have all this input, right, we give all this data to the neural network, maybe we're talking", "tokens": [50828, 321, 362, 439, 341, 4846, 11, 558, 11, 321, 976, 439, 341, 1412, 281, 264, 18161, 3209, 11, 1310, 321, 434, 1417, 51012], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2586, "seek": 1000468, "start": 10017.64, "end": 10022.44, "text": " about an image, maybe we're talking about just some random data points, maybe we're talking about a", "tokens": [51012, 466, 364, 3256, 11, 1310, 321, 434, 1417, 466, 445, 512, 4974, 1412, 2793, 11, 1310, 321, 434, 1417, 466, 257, 51252], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2587, "seek": 1000468, "start": 10022.44, "end": 10027.880000000001, "text": " data set, and then we get some meaningful output. This is what we're looking at. So if we're just", "tokens": [51252, 1412, 992, 11, 293, 550, 321, 483, 512, 10995, 5598, 13, 639, 307, 437, 321, 434, 1237, 412, 13, 407, 498, 321, 434, 445, 51524], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2588, "seek": 1000468, "start": 10027.880000000001, "end": 10031.56, "text": " looking at a neural network from kind of the outside, we think of it as this magical black", "tokens": [51524, 1237, 412, 257, 18161, 3209, 490, 733, 295, 264, 2380, 11, 321, 519, 295, 309, 382, 341, 12066, 2211, 51708], "temperature": 0.0, "avg_logprob": -0.04225393467479282, "compression_ratio": 2.2325581395348837, "no_speech_prob": 0.08754430711269379}, {"id": 2589, "seek": 1003156, "start": 10031.56, "end": 10036.199999999999, "text": " box, we give some input, it gives us some output. And I mean, we could call this black box just some", "tokens": [50364, 2424, 11, 321, 976, 512, 4846, 11, 309, 2709, 505, 512, 5598, 13, 400, 286, 914, 11, 321, 727, 818, 341, 2211, 2424, 445, 512, 50596], "temperature": 0.0, "avg_logprob": -0.07335291211567228, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.08033575117588043}, {"id": 2590, "seek": 1003156, "start": 10036.199999999999, "end": 10040.519999999999, "text": " function, right, where it's a function of the input maps it to some output. And that's exactly", "tokens": [50596, 2445, 11, 558, 11, 689, 309, 311, 257, 2445, 295, 264, 4846, 11317, 309, 281, 512, 5598, 13, 400, 300, 311, 2293, 50812], "temperature": 0.0, "avg_logprob": -0.07335291211567228, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.08033575117588043}, {"id": 2591, "seek": 1003156, "start": 10040.519999999999, "end": 10045.8, "text": " what a neural network does. It takes input and maps that input to some output, just like any", "tokens": [50812, 437, 257, 18161, 3209, 775, 13, 467, 2516, 4846, 293, 11317, 300, 4846, 281, 512, 5598, 11, 445, 411, 604, 51076], "temperature": 0.0, "avg_logprob": -0.07335291211567228, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.08033575117588043}, {"id": 2592, "seek": 1003156, "start": 10045.8, "end": 10051.64, "text": " other function, right, just like if you had a straight line like this, this is a function,", "tokens": [51076, 661, 2445, 11, 558, 11, 445, 411, 498, 291, 632, 257, 2997, 1622, 411, 341, 11, 341, 307, 257, 2445, 11, 51368], "temperature": 0.0, "avg_logprob": -0.07335291211567228, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.08033575117588043}, {"id": 2593, "seek": 1003156, "start": 10051.64, "end": 10056.519999999999, "text": " you know, this is your line, you know, whatever it is, you're going to say y equals like four x,", "tokens": [51368, 291, 458, 11, 341, 307, 428, 1622, 11, 291, 458, 11, 2035, 309, 307, 11, 291, 434, 516, 281, 584, 288, 6915, 411, 1451, 2031, 11, 51612], "temperature": 0.0, "avg_logprob": -0.07335291211567228, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.08033575117588043}, {"id": 2594, "seek": 1005652, "start": 10056.6, "end": 10061.720000000001, "text": " maybe that's your line, you give some input x, and it gives you some value y, this is a mapping", "tokens": [50368, 1310, 300, 311, 428, 1622, 11, 291, 976, 512, 4846, 2031, 11, 293, 309, 2709, 291, 512, 2158, 288, 11, 341, 307, 257, 18350, 50624], "temperature": 0.0, "avg_logprob": -0.07484191258748372, "compression_ratio": 1.86328125, "no_speech_prob": 0.0064877839758992195}, {"id": 2595, "seek": 1005652, "start": 10061.720000000001, "end": 10068.04, "text": " of your input to your output. Alright, so now that we have that down, what is a neural network", "tokens": [50624, 295, 428, 4846, 281, 428, 5598, 13, 2798, 11, 370, 586, 300, 321, 362, 300, 760, 11, 437, 307, 257, 18161, 3209, 50940], "temperature": 0.0, "avg_logprob": -0.07484191258748372, "compression_ratio": 1.86328125, "no_speech_prob": 0.0064877839758992195}, {"id": 2596, "seek": 1005652, "start": 10068.04, "end": 10072.92, "text": " made up of? Well, a neural network is made up of layers. And remember, we talked about the layered", "tokens": [50940, 1027, 493, 295, 30, 1042, 11, 257, 18161, 3209, 307, 1027, 493, 295, 7914, 13, 400, 1604, 11, 321, 2825, 466, 264, 34666, 51184], "temperature": 0.0, "avg_logprob": -0.07484191258748372, "compression_ratio": 1.86328125, "no_speech_prob": 0.0064877839758992195}, {"id": 2597, "seek": 1005652, "start": 10072.92, "end": 10077.880000000001, "text": " representation of data when we talked about neural networks. So I'm going to draw a very basic", "tokens": [51184, 10290, 295, 1412, 562, 321, 2825, 466, 18161, 9590, 13, 407, 286, 478, 516, 281, 2642, 257, 588, 3875, 51432], "temperature": 0.0, "avg_logprob": -0.07484191258748372, "compression_ratio": 1.86328125, "no_speech_prob": 0.0064877839758992195}, {"id": 2598, "seek": 1005652, "start": 10077.880000000001, "end": 10084.36, "text": " neural network, we're going to start with the input layer. Now the input layer is always the", "tokens": [51432, 18161, 3209, 11, 321, 434, 516, 281, 722, 365, 264, 4846, 4583, 13, 823, 264, 4846, 4583, 307, 1009, 264, 51756], "temperature": 0.0, "avg_logprob": -0.07484191258748372, "compression_ratio": 1.86328125, "no_speech_prob": 0.0064877839758992195}, {"id": 2599, "seek": 1008436, "start": 10084.36, "end": 10089.720000000001, "text": " first layer in our neural network. And it is what is going to accept our raw data. Now what I mean", "tokens": [50364, 700, 4583, 294, 527, 18161, 3209, 13, 400, 309, 307, 437, 307, 516, 281, 3241, 527, 8936, 1412, 13, 823, 437, 286, 914, 50632], "temperature": 0.0, "avg_logprob": -0.04542034467061361, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.050314176827669144}, {"id": 2600, "seek": 1008436, "start": 10089.720000000001, "end": 10095.400000000001, "text": " by raw data is whatever data we like want to give to the network, whatever we want to classify", "tokens": [50632, 538, 8936, 1412, 307, 2035, 1412, 321, 411, 528, 281, 976, 281, 264, 3209, 11, 2035, 321, 528, 281, 33872, 50916], "temperature": 0.0, "avg_logprob": -0.04542034467061361, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.050314176827669144}, {"id": 2601, "seek": 1008436, "start": 10095.400000000001, "end": 10100.52, "text": " whatever our input information is, that's what this layer is going to receive in the neural", "tokens": [50916, 2035, 527, 4846, 1589, 307, 11, 300, 311, 437, 341, 4583, 307, 516, 281, 4774, 294, 264, 18161, 51172], "temperature": 0.0, "avg_logprob": -0.04542034467061361, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.050314176827669144}, {"id": 2602, "seek": 1008436, "start": 10100.52, "end": 10105.480000000001, "text": " network. So we can say, you know, these arrows represent our input, and they come to our first", "tokens": [51172, 3209, 13, 407, 321, 393, 584, 11, 291, 458, 11, 613, 19669, 2906, 527, 4846, 11, 293, 436, 808, 281, 527, 700, 51420], "temperature": 0.0, "avg_logprob": -0.04542034467061361, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.050314176827669144}, {"id": 2603, "seek": 1008436, "start": 10105.480000000001, "end": 10110.84, "text": " input layer. So this means, for example, if you had an image, and this image, and I'll just draw", "tokens": [51420, 4846, 4583, 13, 407, 341, 1355, 11, 337, 1365, 11, 498, 291, 632, 364, 3256, 11, 293, 341, 3256, 11, 293, 286, 603, 445, 2642, 51688], "temperature": 0.0, "avg_logprob": -0.04542034467061361, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.050314176827669144}, {"id": 2604, "seek": 1011084, "start": 10110.92, "end": 10114.92, "text": " like one like this, let's say this our image, and it has all these different pixels, right,", "tokens": [50368, 411, 472, 411, 341, 11, 718, 311, 584, 341, 527, 3256, 11, 293, 309, 575, 439, 613, 819, 18668, 11, 558, 11, 50568], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2605, "seek": 1011084, "start": 10114.92, "end": 10118.28, "text": " all these different pixels in the image, and you want to make a classification on this image.", "tokens": [50568, 439, 613, 819, 18668, 294, 264, 3256, 11, 293, 291, 528, 281, 652, 257, 21538, 322, 341, 3256, 13, 50736], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2606, "seek": 1011084, "start": 10119.0, "end": 10124.36, "text": " Well, maybe it has a width and a height and a classic width and height example is 28 by 28.", "tokens": [50772, 1042, 11, 1310, 309, 575, 257, 11402, 293, 257, 6681, 293, 257, 7230, 11402, 293, 6681, 1365, 307, 7562, 538, 7562, 13, 51040], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2607, "seek": 1011084, "start": 10124.36, "end": 10128.52, "text": " If you had 28 by 28 pixels, and you want to make a classification on this image,", "tokens": [51040, 759, 291, 632, 7562, 538, 7562, 18668, 11, 293, 291, 528, 281, 652, 257, 21538, 322, 341, 3256, 11, 51248], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2608, "seek": 1011084, "start": 10129.08, "end": 10133.16, "text": " how many input neurons you think you would need in your neural network to do this?", "tokens": [51276, 577, 867, 4846, 22027, 291, 519, 291, 576, 643, 294, 428, 18161, 3209, 281, 360, 341, 30, 51480], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2609, "seek": 1011084, "start": 10134.12, "end": 10137.56, "text": " Well, this is kind of, you know, a tough question if you don't know a lot about neural networks.", "tokens": [51528, 1042, 11, 341, 307, 733, 295, 11, 291, 458, 11, 257, 4930, 1168, 498, 291, 500, 380, 458, 257, 688, 466, 18161, 9590, 13, 51700], "temperature": 0.0, "avg_logprob": -0.08098966361832445, "compression_ratio": 2.030188679245283, "no_speech_prob": 0.07366339862346649}, {"id": 2610, "seek": 1013756, "start": 10138.519999999999, "end": 10142.359999999999, "text": " If you're predicting for the image, if you're going to be looking at the entire image to", "tokens": [50412, 759, 291, 434, 32884, 337, 264, 3256, 11, 498, 291, 434, 516, 281, 312, 1237, 412, 264, 2302, 3256, 281, 50604], "temperature": 0.0, "avg_logprob": -0.06892841403223887, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.004904955625534058}, {"id": 2611, "seek": 1013756, "start": 10142.359999999999, "end": 10148.199999999999, "text": " make a prediction, you're going to need every single one of those pixels, which is 28 times 28", "tokens": [50604, 652, 257, 17630, 11, 291, 434, 516, 281, 643, 633, 2167, 472, 295, 729, 18668, 11, 597, 307, 7562, 1413, 7562, 50896], "temperature": 0.0, "avg_logprob": -0.06892841403223887, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.004904955625534058}, {"id": 2612, "seek": 1013756, "start": 10148.199999999999, "end": 10154.119999999999, "text": " pixels, which I believe is something like 784. I could be wrong on that number, but I believe", "tokens": [50896, 18668, 11, 597, 286, 1697, 307, 746, 411, 1614, 25494, 13, 286, 727, 312, 2085, 322, 300, 1230, 11, 457, 286, 1697, 51192], "temperature": 0.0, "avg_logprob": -0.06892841403223887, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.004904955625534058}, {"id": 2613, "seek": 1013756, "start": 10154.119999999999, "end": 10160.439999999999, "text": " that's what it is. So you would need 784 input input neurons. Now, that's totally fine. That", "tokens": [51192, 300, 311, 437, 309, 307, 13, 407, 291, 576, 643, 1614, 25494, 4846, 4846, 22027, 13, 823, 11, 300, 311, 3879, 2489, 13, 663, 51508], "temperature": 0.0, "avg_logprob": -0.06892841403223887, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.004904955625534058}, {"id": 2614, "seek": 1013756, "start": 10160.439999999999, "end": 10164.119999999999, "text": " might seem like a big number, but we deal with massive numbers when it comes to computers. So", "tokens": [51508, 1062, 1643, 411, 257, 955, 1230, 11, 457, 321, 2028, 365, 5994, 3547, 562, 309, 1487, 281, 10807, 13, 407, 51692], "temperature": 0.0, "avg_logprob": -0.06892841403223887, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.004904955625534058}, {"id": 2615, "seek": 1016412, "start": 10164.2, "end": 10168.68, "text": " this really isn't that many. But that's an example of, you know, how you would use a neural network", "tokens": [50368, 341, 534, 1943, 380, 300, 867, 13, 583, 300, 311, 364, 1365, 295, 11, 291, 458, 11, 577, 291, 576, 764, 257, 18161, 3209, 50592], "temperature": 0.0, "avg_logprob": -0.08043732077388441, "compression_ratio": 1.8, "no_speech_prob": 0.004905126057565212}, {"id": 2616, "seek": 1016412, "start": 10168.68, "end": 10175.08, "text": " input layer to represent an image, you would have 784 input neurons, and you would pass", "tokens": [50592, 4846, 4583, 281, 2906, 364, 3256, 11, 291, 576, 362, 1614, 25494, 4846, 22027, 11, 293, 291, 576, 1320, 50912], "temperature": 0.0, "avg_logprob": -0.08043732077388441, "compression_ratio": 1.8, "no_speech_prob": 0.004905126057565212}, {"id": 2617, "seek": 1016412, "start": 10175.08, "end": 10179.240000000002, "text": " one pixel to every single one of those neurons. Now, if we're doing an example where maybe we", "tokens": [50912, 472, 19261, 281, 633, 2167, 472, 295, 729, 22027, 13, 823, 11, 498, 321, 434, 884, 364, 1365, 689, 1310, 321, 51120], "temperature": 0.0, "avg_logprob": -0.08043732077388441, "compression_ratio": 1.8, "no_speech_prob": 0.004905126057565212}, {"id": 2618, "seek": 1016412, "start": 10179.240000000002, "end": 10184.6, "text": " just have one piece of input information, maybe it's literally just one number. Well, then all", "tokens": [51120, 445, 362, 472, 2522, 295, 4846, 1589, 11, 1310, 309, 311, 3736, 445, 472, 1230, 13, 1042, 11, 550, 439, 51388], "temperature": 0.0, "avg_logprob": -0.08043732077388441, "compression_ratio": 1.8, "no_speech_prob": 0.004905126057565212}, {"id": 2619, "seek": 1016412, "start": 10184.6, "end": 10190.92, "text": " we need is one input nerve. If we have an example where we have four pieces of information, we would", "tokens": [51388, 321, 643, 307, 472, 4846, 16355, 13, 759, 321, 362, 364, 1365, 689, 321, 362, 1451, 3755, 295, 1589, 11, 321, 576, 51704], "temperature": 0.0, "avg_logprob": -0.08043732077388441, "compression_ratio": 1.8, "no_speech_prob": 0.004905126057565212}, {"id": 2620, "seek": 1019092, "start": 10190.92, "end": 10196.04, "text": " need four input neurons, right? Now, this can get a little bit more complicated. But that's", "tokens": [50364, 643, 1451, 4846, 22027, 11, 558, 30, 823, 11, 341, 393, 483, 257, 707, 857, 544, 6179, 13, 583, 300, 311, 50620], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2621, "seek": 1019092, "start": 10196.04, "end": 10199.72, "text": " the basis that I want you to understand is, you know, the pieces of input you're going to have", "tokens": [50620, 264, 5143, 300, 286, 528, 291, 281, 1223, 307, 11, 291, 458, 11, 264, 3755, 295, 4846, 291, 434, 516, 281, 362, 50804], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2622, "seek": 1019092, "start": 10199.72, "end": 10204.2, "text": " regardless of what they are, you need one input neuron for each piece of that information, unless", "tokens": [50804, 10060, 295, 437, 436, 366, 11, 291, 643, 472, 4846, 34090, 337, 1184, 2522, 295, 300, 1589, 11, 5969, 51028], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2623, "seek": 1019092, "start": 10204.2, "end": 10208.04, "text": " you're going to be reshaping or putting that information in different form. Okay, so let's", "tokens": [51028, 291, 434, 516, 281, 312, 725, 71, 569, 278, 420, 3372, 300, 1589, 294, 819, 1254, 13, 1033, 11, 370, 718, 311, 51220], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2624, "seek": 1019092, "start": 10208.04, "end": 10213.64, "text": " just actually skip ahead and go to now our output layer. So this is going to be our output. Now,", "tokens": [51220, 445, 767, 10023, 2286, 293, 352, 281, 586, 527, 5598, 4583, 13, 407, 341, 307, 516, 281, 312, 527, 5598, 13, 823, 11, 51500], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2625, "seek": 1019092, "start": 10213.64, "end": 10218.44, "text": " what is our output layer? Well, our output layer is going to have as many neurons. And again,", "tokens": [51500, 437, 307, 527, 5598, 4583, 30, 1042, 11, 527, 5598, 4583, 307, 516, 281, 362, 382, 867, 22027, 13, 400, 797, 11, 51740], "temperature": 0.0, "avg_logprob": -0.057804254385141224, "compression_ratio": 1.867986798679868, "no_speech_prob": 0.02033047005534172}, {"id": 2626, "seek": 1021844, "start": 10218.44, "end": 10224.68, "text": " the neurons are just representing like a node in the layer as output pieces that we want. Now,", "tokens": [50364, 264, 22027, 366, 445, 13460, 411, 257, 9984, 294, 264, 4583, 382, 5598, 3755, 300, 321, 528, 13, 823, 11, 50676], "temperature": 0.0, "avg_logprob": -0.067284383693663, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.006097066681832075}, {"id": 2627, "seek": 1021844, "start": 10224.68, "end": 10229.800000000001, "text": " let's say we're doing a classification for images, right? And maybe there's two classes", "tokens": [50676, 718, 311, 584, 321, 434, 884, 257, 21538, 337, 5267, 11, 558, 30, 400, 1310, 456, 311, 732, 5359, 50932], "temperature": 0.0, "avg_logprob": -0.067284383693663, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.006097066681832075}, {"id": 2628, "seek": 1021844, "start": 10229.800000000001, "end": 10234.84, "text": " that we could represent. Well, there's a few different ways we could design our output layer.", "tokens": [50932, 300, 321, 727, 2906, 13, 1042, 11, 456, 311, 257, 1326, 819, 2098, 321, 727, 1715, 527, 5598, 4583, 13, 51184], "temperature": 0.0, "avg_logprob": -0.067284383693663, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.006097066681832075}, {"id": 2629, "seek": 1021844, "start": 10234.84, "end": 10239.720000000001, "text": " What we could do is say, okay, we're going to use one output neuron. This output neuron is going to", "tokens": [51184, 708, 321, 727, 360, 307, 584, 11, 1392, 11, 321, 434, 516, 281, 764, 472, 5598, 34090, 13, 639, 5598, 34090, 307, 516, 281, 51428], "temperature": 0.0, "avg_logprob": -0.067284383693663, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.006097066681832075}, {"id": 2630, "seek": 1021844, "start": 10239.720000000001, "end": 10246.92, "text": " give us some value. We want this value to be between zero and one. And we'll say that's inclusive.", "tokens": [51428, 976, 505, 512, 2158, 13, 492, 528, 341, 2158, 281, 312, 1296, 4018, 293, 472, 13, 400, 321, 603, 584, 300, 311, 13429, 13, 51788], "temperature": 0.0, "avg_logprob": -0.067284383693663, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.006097066681832075}, {"id": 2631, "seek": 1024692, "start": 10247.56, "end": 10253.0, "text": " Now, what we can do now if we're predicting two classes say, Okay, so if my output neuron is", "tokens": [50396, 823, 11, 437, 321, 393, 360, 586, 498, 321, 434, 32884, 732, 5359, 584, 11, 1033, 11, 370, 498, 452, 5598, 34090, 307, 50668], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2632, "seek": 1024692, "start": 10253.0, "end": 10258.12, "text": " going to give me some value, if that value is closer to zero, then that's going to be class zero.", "tokens": [50668, 516, 281, 976, 385, 512, 2158, 11, 498, 300, 2158, 307, 4966, 281, 4018, 11, 550, 300, 311, 516, 281, 312, 1508, 4018, 13, 50924], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2633, "seek": 1024692, "start": 10258.12, "end": 10262.92, "text": " If this value is closer to one, it's going to be class one, right? And that would mean", "tokens": [50924, 759, 341, 2158, 307, 4966, 281, 472, 11, 309, 311, 516, 281, 312, 1508, 472, 11, 558, 30, 400, 300, 576, 914, 51164], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2634, "seek": 1024692, "start": 10263.64, "end": 10267.4, "text": " when we have our training data, right, and we talked about training and testing data,", "tokens": [51200, 562, 321, 362, 527, 3097, 1412, 11, 558, 11, 293, 321, 2825, 466, 3097, 293, 4997, 1412, 11, 51388], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2635, "seek": 1024692, "start": 10267.4, "end": 10273.16, "text": " we'd give our input and our output would need to be the value zero or one, because it's either", "tokens": [51388, 321, 1116, 976, 527, 4846, 293, 527, 5598, 576, 643, 281, 312, 264, 2158, 4018, 420, 472, 11, 570, 309, 311, 2139, 51676], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2636, "seek": 1024692, "start": 10273.16, "end": 10276.52, "text": " the correct class, which is zero, right, or the correct class, which is one. So like our", "tokens": [51676, 264, 3006, 1508, 11, 597, 307, 4018, 11, 558, 11, 420, 264, 3006, 1508, 11, 597, 307, 472, 13, 407, 411, 527, 51844], "temperature": 0.0, "avg_logprob": -0.10012043846978082, "compression_ratio": 1.9818840579710144, "no_speech_prob": 0.0004878434119746089}, {"id": 2637, "seek": 1027692, "start": 10276.92, "end": 10281.8, "text": " what am I saying, our labels for our training data set would be zero and one. And then this value", "tokens": [50364, 437, 669, 286, 1566, 11, 527, 16949, 337, 527, 3097, 1412, 992, 576, 312, 4018, 293, 472, 13, 400, 550, 341, 2158, 50608], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2638, "seek": 1027692, "start": 10281.8, "end": 10286.04, "text": " on our output neuron will be guaranteed to be between zero and one, based on something that", "tokens": [50608, 322, 527, 5598, 34090, 486, 312, 18031, 281, 312, 1296, 4018, 293, 472, 11, 2361, 322, 746, 300, 50820], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2639, "seek": 1027692, "start": 10286.04, "end": 10289.64, "text": " I'm going to talk about a little bit later. That's one way to approach it, right? We have a single", "tokens": [50820, 286, 478, 516, 281, 751, 466, 257, 707, 857, 1780, 13, 663, 311, 472, 636, 281, 3109, 309, 11, 558, 30, 492, 362, 257, 2167, 51000], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2640, "seek": 1027692, "start": 10289.64, "end": 10295.16, "text": " value, we look at that value. And based on what that value is, we can determine, you know, what", "tokens": [51000, 2158, 11, 321, 574, 412, 300, 2158, 13, 400, 2361, 322, 437, 300, 2158, 307, 11, 321, 393, 6997, 11, 291, 458, 11, 437, 51276], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2641, "seek": 1027692, "start": 10295.16, "end": 10300.04, "text": " class we predicted, not work sometimes. But in other instances, when we're doing classification,", "tokens": [51276, 1508, 321, 19147, 11, 406, 589, 2171, 13, 583, 294, 661, 14519, 11, 562, 321, 434, 884, 21538, 11, 51520], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2642, "seek": 1027692, "start": 10300.04, "end": 10305.72, "text": " what makes more sense is to have as many output neurons as classes you're looking to predict for.", "tokens": [51520, 437, 1669, 544, 2020, 307, 281, 362, 382, 867, 5598, 22027, 382, 5359, 291, 434, 1237, 281, 6069, 337, 13, 51804], "temperature": 0.0, "avg_logprob": -0.0821874779714665, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0006878284038975835}, {"id": 2643, "seek": 1030572, "start": 10305.72, "end": 10309.32, "text": " So let's say we're going to have, you know, like five classes that we're predicting for maybe", "tokens": [50364, 407, 718, 311, 584, 321, 434, 516, 281, 362, 11, 291, 458, 11, 411, 1732, 5359, 300, 321, 434, 32884, 337, 1310, 50544], "temperature": 0.0, "avg_logprob": -0.07461483725186052, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.0007793385884724557}, {"id": 2644, "seek": 1030572, "start": 10309.32, "end": 10314.679999999998, "text": " these three pieces of input information are enough to make that prediction. Well, we'd actually have", "tokens": [50544, 613, 1045, 3755, 295, 4846, 1589, 366, 1547, 281, 652, 300, 17630, 13, 1042, 11, 321, 1116, 767, 362, 50812], "temperature": 0.0, "avg_logprob": -0.07461483725186052, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.0007793385884724557}, {"id": 2645, "seek": 1030572, "start": 10314.679999999998, "end": 10321.0, "text": " five output neurons. And each of these neurons would have a value between zero and one. And the", "tokens": [50812, 1732, 5598, 22027, 13, 400, 1184, 295, 613, 22027, 576, 362, 257, 2158, 1296, 4018, 293, 472, 13, 400, 264, 51128], "temperature": 0.0, "avg_logprob": -0.07461483725186052, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.0007793385884724557}, {"id": 2646, "seek": 1030572, "start": 10321.0, "end": 10327.16, "text": " combination, so the sum of every single one of these values would be equal to one. Now, can you", "tokens": [51128, 6562, 11, 370, 264, 2408, 295, 633, 2167, 472, 295, 613, 4190, 576, 312, 2681, 281, 472, 13, 823, 11, 393, 291, 51436], "temperature": 0.0, "avg_logprob": -0.07461483725186052, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.0007793385884724557}, {"id": 2647, "seek": 1030572, "start": 10327.16, "end": 10332.439999999999, "text": " think of what this means? If every single one of these neurons is a value between zero and one,", "tokens": [51436, 519, 295, 437, 341, 1355, 30, 759, 633, 2167, 472, 295, 613, 22027, 307, 257, 2158, 1296, 4018, 293, 472, 11, 51700], "temperature": 0.0, "avg_logprob": -0.07461483725186052, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.0007793385884724557}, {"id": 2648, "seek": 1033244, "start": 10332.44, "end": 10337.0, "text": " and their sum is one, what does this look like to you? Well, to me, this looks like a probability", "tokens": [50364, 293, 641, 2408, 307, 472, 11, 437, 775, 341, 574, 411, 281, 291, 30, 1042, 11, 281, 385, 11, 341, 1542, 411, 257, 8482, 50592], "temperature": 0.0, "avg_logprob": -0.06962383036710779, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008061310276389122}, {"id": 2649, "seek": 1033244, "start": 10337.0, "end": 10341.24, "text": " distribution. And essentially, what's going to happen is we're going to make predictions for how", "tokens": [50592, 7316, 13, 400, 4476, 11, 437, 311, 516, 281, 1051, 307, 321, 434, 516, 281, 652, 21264, 337, 577, 50804], "temperature": 0.0, "avg_logprob": -0.06962383036710779, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008061310276389122}, {"id": 2650, "seek": 1033244, "start": 10341.24, "end": 10347.480000000001, "text": " strongly we think each input information is each class. So if we think that it's like class one,", "tokens": [50804, 10613, 321, 519, 1184, 4846, 1589, 307, 1184, 1508, 13, 407, 498, 321, 519, 300, 309, 311, 411, 1508, 472, 11, 51116], "temperature": 0.0, "avg_logprob": -0.06962383036710779, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008061310276389122}, {"id": 2651, "seek": 1033244, "start": 10347.480000000001, "end": 10352.92, "text": " maybe we'll just label these like this, then what we would do is say, okay, this is going to be", "tokens": [51116, 1310, 321, 603, 445, 7645, 613, 411, 341, 11, 550, 437, 321, 576, 360, 307, 584, 11, 1392, 11, 341, 307, 516, 281, 312, 51388], "temperature": 0.0, "avg_logprob": -0.06962383036710779, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008061310276389122}, {"id": 2652, "seek": 1035292, "start": 10352.92, "end": 10362.92, "text": " 0.9 representing 90%. Maybe this is like 0.001, maybe this is 0.05, 0.003, you get the point,", "tokens": [50364, 1958, 13, 24, 13460, 4289, 6856, 2704, 341, 307, 411, 1958, 13, 628, 16, 11, 1310, 341, 307, 1958, 13, 13328, 11, 1958, 13, 628, 18, 11, 291, 483, 264, 935, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09499852922227647, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.09533920884132385}, {"id": 2653, "seek": 1035292, "start": 10362.92, "end": 10366.6, "text": " it's going to add up to one, and this is a probability distribution for our output layer.", "tokens": [50864, 309, 311, 516, 281, 909, 493, 281, 472, 11, 293, 341, 307, 257, 8482, 7316, 337, 527, 5598, 4583, 13, 51048], "temperature": 0.0, "avg_logprob": -0.09499852922227647, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.09533920884132385}, {"id": 2654, "seek": 1035292, "start": 10367.16, "end": 10371.48, "text": " So that's a way to do it as well. And then obviously, if we're doing some kind of regression task,", "tokens": [51076, 407, 300, 311, 257, 636, 281, 360, 309, 382, 731, 13, 400, 550, 2745, 11, 498, 321, 434, 884, 512, 733, 295, 24590, 5633, 11, 51292], "temperature": 0.0, "avg_logprob": -0.09499852922227647, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.09533920884132385}, {"id": 2655, "seek": 1035292, "start": 10371.48, "end": 10375.72, "text": " we can just have one neuron and that will just predict some value. And we'll define, you know,", "tokens": [51292, 321, 393, 445, 362, 472, 34090, 293, 300, 486, 445, 6069, 512, 2158, 13, 400, 321, 603, 6964, 11, 291, 458, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09499852922227647, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.09533920884132385}, {"id": 2656, "seek": 1035292, "start": 10375.72, "end": 10381.16, "text": " what we want that value to be. Okay, so that's my example for my output. Now let's erase this", "tokens": [51504, 437, 321, 528, 300, 2158, 281, 312, 13, 1033, 11, 370, 300, 311, 452, 1365, 337, 452, 5598, 13, 823, 718, 311, 23525, 341, 51776], "temperature": 0.0, "avg_logprob": -0.09499852922227647, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.09533920884132385}, {"id": 2657, "seek": 1038116, "start": 10381.16, "end": 10384.2, "text": " and let's actually just go back to one output neuron, because that's what I want to use for", "tokens": [50364, 293, 718, 311, 767, 445, 352, 646, 281, 472, 5598, 34090, 11, 570, 300, 311, 437, 286, 528, 281, 764, 337, 50516], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2658, "seek": 1038116, "start": 10384.2, "end": 10390.2, "text": " this example. Now, we have something in between these layers, because obviously, you know, we", "tokens": [50516, 341, 1365, 13, 823, 11, 321, 362, 746, 294, 1296, 613, 7914, 11, 570, 2745, 11, 291, 458, 11, 321, 50816], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2659, "seek": 1038116, "start": 10390.2, "end": 10394.68, "text": " can't just go from input to output with nothing else. What we have here is called a hidden layer.", "tokens": [50816, 393, 380, 445, 352, 490, 4846, 281, 5598, 365, 1825, 1646, 13, 708, 321, 362, 510, 307, 1219, 257, 7633, 4583, 13, 51040], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2660, "seek": 1038116, "start": 10395.4, "end": 10398.52, "text": " Now, in neural networks, we can have many different hidden layers, we can have, you know,", "tokens": [51076, 823, 11, 294, 18161, 9590, 11, 321, 393, 362, 867, 819, 7633, 7914, 11, 321, 393, 362, 11, 291, 458, 11, 51232], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2661, "seek": 1038116, "start": 10398.52, "end": 10402.92, "text": " hidden layers that are connecting to other hidden layers, and like we could have hundreds,", "tokens": [51232, 7633, 7914, 300, 366, 11015, 281, 661, 7633, 7914, 11, 293, 411, 321, 727, 362, 6779, 11, 51452], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2662, "seek": 1038116, "start": 10402.92, "end": 10408.36, "text": " thousands, if we wanted to, for this basic example, we'll use one. And I'll write this as hidden.", "tokens": [51452, 5383, 11, 498, 321, 1415, 281, 11, 337, 341, 3875, 1365, 11, 321, 603, 764, 472, 13, 400, 286, 603, 2464, 341, 382, 7633, 13, 51724], "temperature": 0.0, "avg_logprob": -0.06962704658508301, "compression_ratio": 1.8922558922558923, "no_speech_prob": 0.005910766310989857}, {"id": 2663, "seek": 1040836, "start": 10409.32, "end": 10413.32, "text": " So now we have our three layers. Now, why is this called hidden? The reason this is called", "tokens": [50412, 407, 586, 321, 362, 527, 1045, 7914, 13, 823, 11, 983, 307, 341, 1219, 7633, 30, 440, 1778, 341, 307, 1219, 50612], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2664, "seek": 1040836, "start": 10413.32, "end": 10417.720000000001, "text": " hidden is because we don't observe it when we're using the neural network, we pass information", "tokens": [50612, 7633, 307, 570, 321, 500, 380, 11441, 309, 562, 321, 434, 1228, 264, 18161, 3209, 11, 321, 1320, 1589, 50832], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2665, "seek": 1040836, "start": 10417.720000000001, "end": 10421.800000000001, "text": " to the input layer, we get information from the output layer, we don't know what happens", "tokens": [50832, 281, 264, 4846, 4583, 11, 321, 483, 1589, 490, 264, 5598, 4583, 11, 321, 500, 380, 458, 437, 2314, 51036], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2666, "seek": 1040836, "start": 10421.800000000001, "end": 10426.6, "text": " in this hidden layer or in these hidden layers. Now, how are these layers connected to each other?", "tokens": [51036, 294, 341, 7633, 4583, 420, 294, 613, 7633, 7914, 13, 823, 11, 577, 366, 613, 7914, 4582, 281, 1184, 661, 30, 51276], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2667, "seek": 1040836, "start": 10426.6, "end": 10430.2, "text": " How do we get from this input layer to the hidden layer to the output layer and get some", "tokens": [51276, 1012, 360, 321, 483, 490, 341, 4846, 4583, 281, 264, 7633, 4583, 281, 264, 5598, 4583, 293, 483, 512, 51456], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2668, "seek": 1040836, "start": 10430.2, "end": 10436.2, "text": " meaningful output? Well, every single layer is connected to another layer with something called", "tokens": [51456, 10995, 5598, 30, 1042, 11, 633, 2167, 4583, 307, 4582, 281, 1071, 4583, 365, 746, 1219, 51756], "temperature": 0.0, "avg_logprob": -0.0579706679942996, "compression_ratio": 2.0071942446043165, "no_speech_prob": 0.002050660317763686}, {"id": 2669, "seek": 1043620, "start": 10436.28, "end": 10440.12, "text": " weights. Now, we can have different kind of architectures of connections, which means I", "tokens": [50368, 17443, 13, 823, 11, 321, 393, 362, 819, 733, 295, 6331, 1303, 295, 9271, 11, 597, 1355, 286, 50560], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2670, "seek": 1043620, "start": 10440.12, "end": 10445.08, "text": " could have something like this one connects to this, this connects to this, this connects to this,", "tokens": [50560, 727, 362, 746, 411, 341, 472, 16967, 281, 341, 11, 341, 16967, 281, 341, 11, 341, 16967, 281, 341, 11, 50808], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2671, "seek": 1043620, "start": 10445.08, "end": 10449.08, "text": " and that could be like my connection kind of architecture, right? We could have another one", "tokens": [50808, 293, 300, 727, 312, 411, 452, 4984, 733, 295, 9482, 11, 558, 30, 492, 727, 362, 1071, 472, 51008], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2672, "seek": 1043620, "start": 10449.08, "end": 10454.92, "text": " where this one goes here. And you know, maybe this one goes here. And actually, after I've drawn", "tokens": [51008, 689, 341, 472, 1709, 510, 13, 400, 291, 458, 11, 1310, 341, 472, 1709, 510, 13, 400, 767, 11, 934, 286, 600, 10117, 51300], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2673, "seek": 1043620, "start": 10454.92, "end": 10459.800000000001, "text": " this line, now we get what we're going to be talking about a lot, which is called a densely", "tokens": [51300, 341, 1622, 11, 586, 321, 483, 437, 321, 434, 516, 281, 312, 1417, 466, 257, 688, 11, 597, 307, 1219, 257, 24505, 736, 51544], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2674, "seek": 1043620, "start": 10459.800000000001, "end": 10465.16, "text": " connected neural network. Now a densely connected neural network or a densely connected layer,", "tokens": [51544, 4582, 18161, 3209, 13, 823, 257, 24505, 736, 4582, 18161, 3209, 420, 257, 24505, 736, 4582, 4583, 11, 51812], "temperature": 0.0, "avg_logprob": -0.07561982567630597, "compression_ratio": 2.16988416988417, "no_speech_prob": 0.03409404680132866}, {"id": 2675, "seek": 1046516, "start": 10465.24, "end": 10470.119999999999, "text": " essentially means that is connected to every node from the previous layer. So in this case,", "tokens": [50368, 4476, 1355, 300, 307, 4582, 281, 633, 9984, 490, 264, 3894, 4583, 13, 407, 294, 341, 1389, 11, 50612], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2676, "seek": 1046516, "start": 10470.119999999999, "end": 10475.72, "text": " you can see every single node in the input layer is connected to every single node in the output", "tokens": [50612, 291, 393, 536, 633, 2167, 9984, 294, 264, 4846, 4583, 307, 4582, 281, 633, 2167, 9984, 294, 264, 5598, 50892], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2677, "seek": 1046516, "start": 10475.72, "end": 10480.76, "text": " layer or in the hidden layer, my bad. And these connections are what we call weights. Now these", "tokens": [50892, 4583, 420, 294, 264, 7633, 4583, 11, 452, 1578, 13, 400, 613, 9271, 366, 437, 321, 818, 17443, 13, 823, 613, 51144], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2678, "seek": 1046516, "start": 10480.76, "end": 10486.36, "text": " weights are actually what the neural network is going to change and optimize to determine the", "tokens": [51144, 17443, 366, 767, 437, 264, 18161, 3209, 307, 516, 281, 1319, 293, 19719, 281, 6997, 264, 51424], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2679, "seek": 1046516, "start": 10486.36, "end": 10490.36, "text": " mapping from our input to our output. Because again, remember, that's what we're trying to do.", "tokens": [51424, 18350, 490, 527, 4846, 281, 527, 5598, 13, 1436, 797, 11, 1604, 11, 300, 311, 437, 321, 434, 1382, 281, 360, 13, 51624], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2680, "seek": 1046516, "start": 10490.36, "end": 10494.52, "text": " We have some kind of function, we give some input, it gives us some output. How do we get that input", "tokens": [51624, 492, 362, 512, 733, 295, 2445, 11, 321, 976, 512, 4846, 11, 309, 2709, 505, 512, 5598, 13, 1012, 360, 321, 483, 300, 4846, 51832], "temperature": 0.0, "avg_logprob": -0.06343145512822848, "compression_ratio": 1.900662251655629, "no_speech_prob": 0.0010004168143495917}, {"id": 2681, "seek": 1049452, "start": 10494.52, "end": 10498.68, "text": " and output? Well, by modifying these weights, it's a little bit more complex, but this is the", "tokens": [50364, 293, 5598, 30, 1042, 11, 538, 42626, 613, 17443, 11, 309, 311, 257, 707, 857, 544, 3997, 11, 457, 341, 307, 264, 50572], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2682, "seek": 1049452, "start": 10498.68, "end": 10502.92, "text": " starting. So these lines that I've drawn are really just numbers. And every single one of these", "tokens": [50572, 2891, 13, 407, 613, 3876, 300, 286, 600, 10117, 366, 534, 445, 3547, 13, 400, 633, 2167, 472, 295, 613, 50784], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2683, "seek": 1049452, "start": 10502.92, "end": 10507.560000000001, "text": " lines is some numeric value. Typically, these numeric values are between zero and one, but", "tokens": [50784, 3876, 307, 512, 7866, 299, 2158, 13, 23129, 11, 613, 7866, 299, 4190, 366, 1296, 4018, 293, 472, 11, 457, 51016], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2684, "seek": 1049452, "start": 10507.560000000001, "end": 10512.28, "text": " they can be large, they can be negative. It really depends on what kind of network you're doing and", "tokens": [51016, 436, 393, 312, 2416, 11, 436, 393, 312, 3671, 13, 467, 534, 5946, 322, 437, 733, 295, 3209, 291, 434, 884, 293, 51252], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2685, "seek": 1049452, "start": 10512.28, "end": 10517.24, "text": " how you've designed it. Now, let's just write some random numbers, we have like 0.1, this could", "tokens": [51252, 577, 291, 600, 4761, 309, 13, 823, 11, 718, 311, 445, 2464, 512, 4974, 3547, 11, 321, 362, 411, 1958, 13, 16, 11, 341, 727, 51500], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2686, "seek": 1049452, "start": 10517.24, "end": 10521.800000000001, "text": " be like 0.7, you get the point, right? We just have numbers for every single one of these lines.", "tokens": [51500, 312, 411, 1958, 13, 22, 11, 291, 483, 264, 935, 11, 558, 30, 492, 445, 362, 3547, 337, 633, 2167, 472, 295, 613, 3876, 13, 51728], "temperature": 0.0, "avg_logprob": -0.097684789348293, "compression_ratio": 1.790625, "no_speech_prob": 0.004905021749436855}, {"id": 2687, "seek": 1052180, "start": 10522.599999999999, "end": 10526.439999999999, "text": " And these are what we call the trainable parameters that our neural network will", "tokens": [50404, 400, 613, 366, 437, 321, 818, 264, 3847, 712, 9834, 300, 527, 18161, 3209, 486, 50596], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2688, "seek": 1052180, "start": 10526.439999999999, "end": 10532.279999999999, "text": " actually tweak and change as we train to get the best possible result. So we have these", "tokens": [50596, 767, 29879, 293, 1319, 382, 321, 3847, 281, 483, 264, 1151, 1944, 1874, 13, 407, 321, 362, 613, 50888], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2689, "seek": 1052180, "start": 10532.279999999999, "end": 10535.8, "text": " connections. Now our hidden layer is connected to our output layer as well. This is again,", "tokens": [50888, 9271, 13, 823, 527, 7633, 4583, 307, 4582, 281, 527, 5598, 4583, 382, 731, 13, 639, 307, 797, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2690, "seek": 1052180, "start": 10535.8, "end": 10541.8, "text": " another densely connected layer, because every layer or every nor neuron from the previous layer", "tokens": [51064, 1071, 24505, 736, 4582, 4583, 11, 570, 633, 4583, 420, 633, 6051, 34090, 490, 264, 3894, 4583, 51364], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2691, "seek": 1052180, "start": 10541.8, "end": 10545.96, "text": " is connected to every neuron from the next layer, you would like to determine how many", "tokens": [51364, 307, 4582, 281, 633, 34090, 490, 264, 958, 4583, 11, 291, 576, 411, 281, 6997, 577, 867, 51572], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2692, "seek": 1052180, "start": 10545.96, "end": 10549.88, "text": " connections you have, what you can do is say there's three neurons here, there's two neurons", "tokens": [51572, 9271, 291, 362, 11, 437, 291, 393, 360, 307, 584, 456, 311, 1045, 22027, 510, 11, 456, 311, 732, 22027, 51768], "temperature": 0.0, "avg_logprob": -0.09161147276560465, "compression_ratio": 1.8873239436619718, "no_speech_prob": 0.0024725431576371193}, {"id": 2693, "seek": 1054988, "start": 10549.88, "end": 10555.24, "text": " here, three times two equals six connections. That's how that works from layers. And then", "tokens": [50364, 510, 11, 1045, 1413, 732, 6915, 2309, 9271, 13, 663, 311, 577, 300, 1985, 490, 7914, 13, 400, 550, 50632], "temperature": 0.0, "avg_logprob": -0.07535080909729004, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0020506836008280516}, {"id": 2694, "seek": 1054988, "start": 10555.24, "end": 10559.88, "text": " obviously, you can just multiply all of the neurons together as you go through and determine", "tokens": [50632, 2745, 11, 291, 393, 445, 12972, 439, 295, 264, 22027, 1214, 382, 291, 352, 807, 293, 6997, 50864], "temperature": 0.0, "avg_logprob": -0.07535080909729004, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0020506836008280516}, {"id": 2695, "seek": 1054988, "start": 10560.679999999998, "end": 10565.96, "text": " what that's going to be. Okay, so that is how we connect these layers, we have these weights. So", "tokens": [50904, 437, 300, 311, 516, 281, 312, 13, 1033, 11, 370, 300, 307, 577, 321, 1745, 613, 7914, 11, 321, 362, 613, 17443, 13, 407, 51168], "temperature": 0.0, "avg_logprob": -0.07535080909729004, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0020506836008280516}, {"id": 2696, "seek": 1054988, "start": 10565.96, "end": 10570.119999999999, "text": " let's just write a w on here. So we remember that those are weights. Now, we also have something", "tokens": [51168, 718, 311, 445, 2464, 257, 261, 322, 510, 13, 407, 321, 1604, 300, 729, 366, 17443, 13, 823, 11, 321, 611, 362, 746, 51376], "temperature": 0.0, "avg_logprob": -0.07535080909729004, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0020506836008280516}, {"id": 2697, "seek": 1054988, "start": 10570.119999999999, "end": 10576.519999999999, "text": " called biases. So let's add a bias here, I'm going to label this B. Now biases are a little bit", "tokens": [51376, 1219, 32152, 13, 407, 718, 311, 909, 257, 12577, 510, 11, 286, 478, 516, 281, 7645, 341, 363, 13, 823, 32152, 366, 257, 707, 857, 51696], "temperature": 0.0, "avg_logprob": -0.07535080909729004, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0020506836008280516}, {"id": 2698, "seek": 1057652, "start": 10576.6, "end": 10582.04, "text": " different than these nodes we have regularly. There's only one bias, and a bias exists in", "tokens": [50368, 819, 813, 613, 13891, 321, 362, 11672, 13, 821, 311, 787, 472, 12577, 11, 293, 257, 12577, 8198, 294, 50640], "temperature": 0.0, "avg_logprob": -0.04908650381523266, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.013221824541687965}, {"id": 2699, "seek": 1057652, "start": 10582.04, "end": 10587.32, "text": " the previous layer to the layer that it affects. So in this case, what we actually have is a bias", "tokens": [50640, 264, 3894, 4583, 281, 264, 4583, 300, 309, 11807, 13, 407, 294, 341, 1389, 11, 437, 321, 767, 362, 307, 257, 12577, 50904], "temperature": 0.0, "avg_logprob": -0.04908650381523266, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.013221824541687965}, {"id": 2700, "seek": 1057652, "start": 10587.32, "end": 10593.16, "text": " that connects to each neuron in the next layer from this layer, right? So it's still densely connected.", "tokens": [50904, 300, 16967, 281, 1184, 34090, 294, 264, 958, 4583, 490, 341, 4583, 11, 558, 30, 407, 309, 311, 920, 24505, 736, 4582, 13, 51196], "temperature": 0.0, "avg_logprob": -0.04908650381523266, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.013221824541687965}, {"id": 2701, "seek": 1057652, "start": 10594.2, "end": 10598.6, "text": " But it's just a little bit different. Now notice that this bias doesn't have an arrow beside it", "tokens": [51248, 583, 309, 311, 445, 257, 707, 857, 819, 13, 823, 3449, 300, 341, 12577, 1177, 380, 362, 364, 11610, 15726, 309, 51468], "temperature": 0.0, "avg_logprob": -0.04908650381523266, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.013221824541687965}, {"id": 2702, "seek": 1057652, "start": 10598.6, "end": 10604.04, "text": " because this doesn't take any input information. This is another trainable parameter for the", "tokens": [51468, 570, 341, 1177, 380, 747, 604, 4846, 1589, 13, 639, 307, 1071, 3847, 712, 13075, 337, 264, 51740], "temperature": 0.0, "avg_logprob": -0.04908650381523266, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.013221824541687965}, {"id": 2703, "seek": 1060404, "start": 10604.04, "end": 10610.52, "text": " network. And this bias is just some constant numeric value that we're going to connect to the", "tokens": [50364, 3209, 13, 400, 341, 12577, 307, 445, 512, 5754, 7866, 299, 2158, 300, 321, 434, 516, 281, 1745, 281, 264, 50688], "temperature": 0.0, "avg_logprob": -0.08330457786033893, "compression_ratio": 1.8046875, "no_speech_prob": 0.003593460889533162}, {"id": 2704, "seek": 1060404, "start": 10610.52, "end": 10615.640000000001, "text": " hidden layer. So we can do a few things with it. Now these weights always have a value of one.", "tokens": [50688, 7633, 4583, 13, 407, 321, 393, 360, 257, 1326, 721, 365, 309, 13, 823, 613, 17443, 1009, 362, 257, 2158, 295, 472, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08330457786033893, "compression_ratio": 1.8046875, "no_speech_prob": 0.003593460889533162}, {"id": 2705, "seek": 1060404, "start": 10616.28, "end": 10620.04, "text": " We're going to talk about why they have a value of one in a second. But just know that whenever", "tokens": [50976, 492, 434, 516, 281, 751, 466, 983, 436, 362, 257, 2158, 295, 472, 294, 257, 1150, 13, 583, 445, 458, 300, 5699, 51164], "temperature": 0.0, "avg_logprob": -0.08330457786033893, "compression_ratio": 1.8046875, "no_speech_prob": 0.003593460889533162}, {"id": 2706, "seek": 1060404, "start": 10620.04, "end": 10625.720000000001, "text": " a bias is connected to another layer or to another neuron, its weight is typically one.", "tokens": [51164, 257, 12577, 307, 4582, 281, 1071, 4583, 420, 281, 1071, 34090, 11, 1080, 3364, 307, 5850, 472, 13, 51448], "temperature": 0.0, "avg_logprob": -0.08330457786033893, "compression_ratio": 1.8046875, "no_speech_prob": 0.003593460889533162}, {"id": 2707, "seek": 1060404, "start": 10626.68, "end": 10631.080000000002, "text": " Okay, so we have that connected, we have our bias, and that actually means we have a bias", "tokens": [51496, 1033, 11, 370, 321, 362, 300, 4582, 11, 321, 362, 527, 12577, 11, 293, 300, 767, 1355, 321, 362, 257, 12577, 51716], "temperature": 0.0, "avg_logprob": -0.08330457786033893, "compression_ratio": 1.8046875, "no_speech_prob": 0.003593460889533162}, {"id": 2708, "seek": 1063108, "start": 10631.08, "end": 10636.52, "text": " here as well. And this bias connects to this. Notice that our biases do not connect with each", "tokens": [50364, 510, 382, 731, 13, 400, 341, 12577, 16967, 281, 341, 13, 13428, 300, 527, 32152, 360, 406, 1745, 365, 1184, 50636], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2709, "seek": 1063108, "start": 10636.52, "end": 10639.96, "text": " other. The reason for this, again, is they're just some constant value, and they're just something", "tokens": [50636, 661, 13, 440, 1778, 337, 341, 11, 797, 11, 307, 436, 434, 445, 512, 5754, 2158, 11, 293, 436, 434, 445, 746, 50808], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2710, "seek": 1063108, "start": 10639.96, "end": 10645.16, "text": " we're kind of adding into the network is another trainable parameter that we can use. Now let's", "tokens": [50808, 321, 434, 733, 295, 5127, 666, 264, 3209, 307, 1071, 3847, 712, 13075, 300, 321, 393, 764, 13, 823, 718, 311, 51068], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2711, "seek": 1063108, "start": 10645.16, "end": 10649.48, "text": " talk about how we actually pass information through the network and why we even use these", "tokens": [51068, 751, 466, 577, 321, 767, 1320, 1589, 807, 264, 3209, 293, 983, 321, 754, 764, 613, 51284], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2712, "seek": 1063108, "start": 10649.48, "end": 10654.92, "text": " weights and biases of what they do. So let's say we have, I can't really think of a good example,", "tokens": [51284, 17443, 293, 32152, 295, 437, 436, 360, 13, 407, 718, 311, 584, 321, 362, 11, 286, 393, 380, 534, 519, 295, 257, 665, 1365, 11, 51556], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2713, "seek": 1063108, "start": 10654.92, "end": 10660.36, "text": " so we're just going to do some arbitrary stuff. Let's say we have like data points, right? X, Y, Z,", "tokens": [51556, 370, 321, 434, 445, 516, 281, 360, 512, 23211, 1507, 13, 961, 311, 584, 321, 362, 411, 1412, 2793, 11, 558, 30, 1783, 11, 398, 11, 1176, 11, 51828], "temperature": 0.0, "avg_logprob": -0.10086559427195582, "compression_ratio": 1.750759878419453, "no_speech_prob": 0.0015011271461844444}, {"id": 2714, "seek": 1066108, "start": 10661.48, "end": 10665.64, "text": " and all of these data points have some mapped value, right? There's some value that we're", "tokens": [50384, 293, 439, 295, 613, 1412, 2793, 362, 512, 33318, 2158, 11, 558, 30, 821, 311, 512, 2158, 300, 321, 434, 50592], "temperature": 0.0, "avg_logprob": -0.07795049927451393, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0007321520242840052}, {"id": 2715, "seek": 1066108, "start": 10665.64, "end": 10668.92, "text": " looking for for them, or there's some class we're trying to put them in, maybe we're clustering", "tokens": [50592, 1237, 337, 337, 552, 11, 420, 456, 311, 512, 1508, 321, 434, 1382, 281, 829, 552, 294, 11, 1310, 321, 434, 596, 48673, 50756], "temperature": 0.0, "avg_logprob": -0.07795049927451393, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0007321520242840052}, {"id": 2716, "seek": 1066108, "start": 10668.92, "end": 10676.28, "text": " them between like, red dots and blue dots. So let's do that. Let's say an XYZ is either a part of", "tokens": [50756, 552, 1296, 411, 11, 2182, 15026, 293, 3344, 15026, 13, 407, 718, 311, 360, 300, 13, 961, 311, 584, 364, 48826, 57, 307, 2139, 257, 644, 295, 51124], "temperature": 0.0, "avg_logprob": -0.07795049927451393, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0007321520242840052}, {"id": 2717, "seek": 1066108, "start": 10676.28, "end": 10682.36, "text": " the red class, or the blue class, let's just do that. So what we want this output neuron to give", "tokens": [51124, 264, 2182, 1508, 11, 420, 264, 3344, 1508, 11, 718, 311, 445, 360, 300, 13, 407, 437, 321, 528, 341, 5598, 34090, 281, 976, 51428], "temperature": 0.0, "avg_logprob": -0.07795049927451393, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0007321520242840052}, {"id": 2718, "seek": 1066108, "start": 10682.36, "end": 10687.08, "text": " us is red or blue. So what I'm going to do is say since it's just one class, we'll get this", "tokens": [51428, 505, 307, 2182, 420, 3344, 13, 407, 437, 286, 478, 516, 281, 360, 307, 584, 1670, 309, 311, 445, 472, 1508, 11, 321, 603, 483, 341, 51664], "temperature": 0.0, "avg_logprob": -0.07795049927451393, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0007321520242840052}, {"id": 2719, "seek": 1068708, "start": 10687.16, "end": 10692.52, "text": " output neuron in between the range is your own one, we'll say, okay, if it's closer to zero,", "tokens": [50368, 5598, 34090, 294, 1296, 264, 3613, 307, 428, 1065, 472, 11, 321, 603, 584, 11, 1392, 11, 498, 309, 311, 4966, 281, 4018, 11, 50636], "temperature": 0.0, "avg_logprob": -0.08885269300312015, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.011686631478369236}, {"id": 2720, "seek": 1068708, "start": 10692.52, "end": 10697.16, "text": " that's red, if it's closer to one, that's blue. And that's what we'll do for this network. And for", "tokens": [50636, 300, 311, 2182, 11, 498, 309, 311, 4966, 281, 472, 11, 300, 311, 3344, 13, 400, 300, 311, 437, 321, 603, 360, 337, 341, 3209, 13, 400, 337, 50868], "temperature": 0.0, "avg_logprob": -0.08885269300312015, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.011686631478369236}, {"id": 2721, "seek": 1068708, "start": 10697.16, "end": 10704.2, "text": " this example, now our input neurons are going to obviously be X, Y, and Z. So let's pick some", "tokens": [50868, 341, 1365, 11, 586, 527, 4846, 22027, 366, 516, 281, 2745, 312, 1783, 11, 398, 11, 293, 1176, 13, 407, 718, 311, 1888, 512, 51220], "temperature": 0.0, "avg_logprob": -0.08885269300312015, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.011686631478369236}, {"id": 2722, "seek": 1068708, "start": 10704.2, "end": 10708.6, "text": " data point. And let's say we have, you know, the value two, two, two, that's our data point. And", "tokens": [51220, 1412, 935, 13, 400, 718, 311, 584, 321, 362, 11, 291, 458, 11, 264, 2158, 732, 11, 732, 11, 732, 11, 300, 311, 527, 1412, 935, 13, 400, 51440], "temperature": 0.0, "avg_logprob": -0.08885269300312015, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.011686631478369236}, {"id": 2723, "seek": 1068708, "start": 10708.6, "end": 10714.12, "text": " we want to predict whether it's red or blue. How do we pass it through? Well, what we need to do", "tokens": [51440, 321, 528, 281, 6069, 1968, 309, 311, 2182, 420, 3344, 13, 1012, 360, 321, 1320, 309, 807, 30, 1042, 11, 437, 321, 643, 281, 360, 51716], "temperature": 0.0, "avg_logprob": -0.08885269300312015, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.011686631478369236}, {"id": 2724, "seek": 1071412, "start": 10714.12, "end": 10720.2, "text": " is determine how we can, you know, find the value of this hidden layer node, we already know the", "tokens": [50364, 307, 6997, 577, 321, 393, 11, 291, 458, 11, 915, 264, 2158, 295, 341, 7633, 4583, 9984, 11, 321, 1217, 458, 264, 50668], "temperature": 0.0, "avg_logprob": -0.07259668731689453, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.03732139989733696}, {"id": 2725, "seek": 1071412, "start": 10720.2, "end": 10724.84, "text": " value of these input node, but now we need to go to the next layer using these connections and find", "tokens": [50668, 2158, 295, 613, 4846, 9984, 11, 457, 586, 321, 643, 281, 352, 281, 264, 958, 4583, 1228, 613, 9271, 293, 915, 50900], "temperature": 0.0, "avg_logprob": -0.07259668731689453, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.03732139989733696}, {"id": 2726, "seek": 1071412, "start": 10724.84, "end": 10729.320000000002, "text": " what the value of these nodes are. Well, the way we determine these values is I'm going to say,", "tokens": [50900, 437, 264, 2158, 295, 613, 13891, 366, 13, 1042, 11, 264, 636, 321, 6997, 613, 4190, 307, 286, 478, 516, 281, 584, 11, 51124], "temperature": 0.0, "avg_logprob": -0.07259668731689453, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.03732139989733696}, {"id": 2727, "seek": 1071412, "start": 10729.320000000002, "end": 10734.92, "text": " and I've just said n one, just to represent like this is a node, like this is node one, maybe this", "tokens": [51124, 293, 286, 600, 445, 848, 297, 472, 11, 445, 281, 2906, 411, 341, 307, 257, 9984, 11, 411, 341, 307, 9984, 472, 11, 1310, 341, 51404], "temperature": 0.0, "avg_logprob": -0.07259668731689453, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.03732139989733696}, {"id": 2728, "seek": 1071412, "start": 10734.92, "end": 10741.880000000001, "text": " one should be node two, is equal to what we call the weighted sum of all of the previous nodes that", "tokens": [51404, 472, 820, 312, 9984, 732, 11, 307, 2681, 281, 437, 321, 818, 264, 32807, 2408, 295, 439, 295, 264, 3894, 13891, 300, 51752], "temperature": 0.0, "avg_logprob": -0.07259668731689453, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.03732139989733696}, {"id": 2729, "seek": 1074188, "start": 10741.88, "end": 10747.48, "text": " are connected to it. If that makes any sense to you guys. So a weighted sum is something like this.", "tokens": [50364, 366, 4582, 281, 309, 13, 759, 300, 1669, 604, 2020, 281, 291, 1074, 13, 407, 257, 32807, 2408, 307, 746, 411, 341, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1468900388425535, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.009411784820258617}, {"id": 2730, "seek": 1074188, "start": 10747.48, "end": 10751.0, "text": " So I'm just going to write the equation, I'll explain it, I'm going to say n one is equal to", "tokens": [50644, 407, 286, 478, 445, 516, 281, 2464, 264, 5367, 11, 286, 603, 2903, 309, 11, 286, 478, 516, 281, 584, 297, 472, 307, 2681, 281, 50820], "temperature": 0.0, "avg_logprob": -0.1468900388425535, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.009411784820258617}, {"id": 2731, "seek": 1074188, "start": 10751.0, "end": 10759.32, "text": " the sum of, let's not say n equals zero, let's say, I equals zero to n of, in this case, we're", "tokens": [50820, 264, 2408, 295, 11, 718, 311, 406, 584, 297, 6915, 4018, 11, 718, 311, 584, 11, 286, 6915, 4018, 281, 297, 295, 11, 294, 341, 1389, 11, 321, 434, 51236], "temperature": 0.0, "avg_logprob": -0.1468900388425535, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.009411784820258617}, {"id": 2732, "seek": 1074188, "start": 10759.32, "end": 10766.92, "text": " going to say w i times x i plus b. Now, I know this equation looks really mathy and complicated,", "tokens": [51236, 516, 281, 584, 261, 741, 1413, 2031, 741, 1804, 272, 13, 823, 11, 286, 458, 341, 5367, 1542, 534, 5221, 88, 293, 6179, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1468900388425535, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.009411784820258617}, {"id": 2733, "seek": 1076692, "start": 10766.92, "end": 10772.44, "text": " it's really not what this symbol and this equation here means is take the weighted sum", "tokens": [50364, 309, 311, 534, 406, 437, 341, 5986, 293, 341, 5367, 510, 1355, 307, 747, 264, 32807, 2408, 50640], "temperature": 0.0, "avg_logprob": -0.09563091595967611, "compression_ratio": 2.062222222222222, "no_speech_prob": 0.13293452560901642}, {"id": 2734, "seek": 1076692, "start": 10772.44, "end": 10777.48, "text": " of all the neurons that are connected to this neuron. So in this case, we have a neuron x neuron", "tokens": [50640, 295, 439, 264, 22027, 300, 366, 4582, 281, 341, 34090, 13, 407, 294, 341, 1389, 11, 321, 362, 257, 34090, 2031, 34090, 50892], "temperature": 0.0, "avg_logprob": -0.09563091595967611, "compression_ratio": 2.062222222222222, "no_speech_prob": 0.13293452560901642}, {"id": 2735, "seek": 1076692, "start": 10777.48, "end": 10783.48, "text": " y and neuron z connected to n one. So when we take the weighted sum, or we calculate this,", "tokens": [50892, 288, 293, 34090, 710, 4582, 281, 297, 472, 13, 407, 562, 321, 747, 264, 32807, 2408, 11, 420, 321, 8873, 341, 11, 51192], "temperature": 0.0, "avg_logprob": -0.09563091595967611, "compression_ratio": 2.062222222222222, "no_speech_prob": 0.13293452560901642}, {"id": 2736, "seek": 1076692, "start": 10783.48, "end": 10789.960000000001, "text": " what this is really equal to is the weight at neuron x, we can say w x times the value at", "tokens": [51192, 437, 341, 307, 534, 2681, 281, 307, 264, 3364, 412, 34090, 2031, 11, 321, 393, 584, 261, 2031, 1413, 264, 2158, 412, 51516], "temperature": 0.0, "avg_logprob": -0.09563091595967611, "compression_ratio": 2.062222222222222, "no_speech_prob": 0.13293452560901642}, {"id": 2737, "seek": 1076692, "start": 10789.960000000001, "end": 10796.6, "text": " neuron x, which in this case, is just equal to two, right, plus whatever the weight is at neuron y.", "tokens": [51516, 34090, 2031, 11, 597, 294, 341, 1389, 11, 307, 445, 2681, 281, 732, 11, 558, 11, 1804, 2035, 264, 3364, 307, 412, 34090, 288, 13, 51848], "temperature": 0.0, "avg_logprob": -0.09563091595967611, "compression_ratio": 2.062222222222222, "no_speech_prob": 0.13293452560901642}, {"id": 2738, "seek": 1079660, "start": 10796.6, "end": 10803.08, "text": " So in this case, this is w y. And then times two, and then you get the point where we have", "tokens": [50364, 407, 294, 341, 1389, 11, 341, 307, 261, 288, 13, 400, 550, 1413, 732, 11, 293, 550, 291, 483, 264, 935, 689, 321, 362, 50688], "temperature": 0.0, "avg_logprob": -0.07331979880898686, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0017006610287353396}, {"id": 2739, "seek": 1079660, "start": 10803.08, "end": 10808.28, "text": " w z and I'm trying on the edge of my drawing tablet to write this times two. Now, obviously,", "tokens": [50688, 261, 710, 293, 286, 478, 1382, 322, 264, 4691, 295, 452, 6316, 14136, 281, 2464, 341, 1413, 732, 13, 823, 11, 2745, 11, 50948], "temperature": 0.0, "avg_logprob": -0.07331979880898686, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0017006610287353396}, {"id": 2740, "seek": 1079660, "start": 10808.28, "end": 10812.28, "text": " these weights have some numeric value. Now, when we start our neural network, these weights are", "tokens": [50948, 613, 17443, 362, 512, 7866, 299, 2158, 13, 823, 11, 562, 321, 722, 527, 18161, 3209, 11, 613, 17443, 366, 51148], "temperature": 0.0, "avg_logprob": -0.07331979880898686, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0017006610287353396}, {"id": 2741, "seek": 1079660, "start": 10812.28, "end": 10817.16, "text": " just completely random. They don't make any sense or just some random values that we can use. As the", "tokens": [51148, 445, 2584, 4974, 13, 814, 500, 380, 652, 604, 2020, 420, 445, 512, 4974, 4190, 300, 321, 393, 764, 13, 1018, 264, 51392], "temperature": 0.0, "avg_logprob": -0.07331979880898686, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0017006610287353396}, {"id": 2742, "seek": 1079660, "start": 10817.16, "end": 10821.640000000001, "text": " neural network gets better, these weights are updated and changed to make more sense in our", "tokens": [51392, 18161, 3209, 2170, 1101, 11, 613, 17443, 366, 10588, 293, 3105, 281, 652, 544, 2020, 294, 527, 51616], "temperature": 0.0, "avg_logprob": -0.07331979880898686, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0017006610287353396}, {"id": 2743, "seek": 1082164, "start": 10821.64, "end": 10826.439999999999, "text": " network. So right now, we'll just leave them as w x, w y, w z. But no, these are some numeric", "tokens": [50364, 3209, 13, 407, 558, 586, 11, 321, 603, 445, 1856, 552, 382, 261, 2031, 11, 261, 288, 11, 261, 710, 13, 583, 572, 11, 613, 366, 512, 7866, 299, 50604], "temperature": 0.0, "avg_logprob": -0.08057202790912829, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.023686526343226433}, {"id": 2744, "seek": 1082164, "start": 10826.439999999999, "end": 10832.279999999999, "text": " values. So this returns to a sum value, right, some value, let's just call this value v. And", "tokens": [50604, 4190, 13, 407, 341, 11247, 281, 257, 2408, 2158, 11, 558, 11, 512, 2158, 11, 718, 311, 445, 818, 341, 2158, 371, 13, 400, 50896], "temperature": 0.0, "avg_logprob": -0.08057202790912829, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.023686526343226433}, {"id": 2745, "seek": 1082164, "start": 10832.279999999999, "end": 10838.039999999999, "text": " that's what this is equal to. So v. Then what we do is we add the bias. Now remember, the bias", "tokens": [50896, 300, 311, 437, 341, 307, 2681, 281, 13, 407, 371, 13, 1396, 437, 321, 360, 307, 321, 909, 264, 12577, 13, 823, 1604, 11, 264, 12577, 51184], "temperature": 0.0, "avg_logprob": -0.08057202790912829, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.023686526343226433}, {"id": 2746, "seek": 1082164, "start": 10838.039999999999, "end": 10844.76, "text": " was connected with a weight of one, which means if we take the weighted sum of the bias, right,", "tokens": [51184, 390, 4582, 365, 257, 3364, 295, 472, 11, 597, 1355, 498, 321, 747, 264, 32807, 2408, 295, 264, 12577, 11, 558, 11, 51520], "temperature": 0.0, "avg_logprob": -0.08057202790912829, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.023686526343226433}, {"id": 2747, "seek": 1082164, "start": 10844.76, "end": 10849.64, "text": " all we're doing is adding whatever that biases value was. So if this bias value was 100,", "tokens": [51520, 439, 321, 434, 884, 307, 5127, 2035, 300, 32152, 2158, 390, 13, 407, 498, 341, 12577, 2158, 390, 2319, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08057202790912829, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.023686526343226433}, {"id": 2748, "seek": 1084964, "start": 10849.64, "end": 10854.84, "text": " then what we do is we add 100. Now, I've just written the plus B to explicitly state the fact", "tokens": [50364, 550, 437, 321, 360, 307, 321, 909, 2319, 13, 823, 11, 286, 600, 445, 3720, 264, 1804, 363, 281, 20803, 1785, 264, 1186, 50624], "temperature": 0.0, "avg_logprob": -0.0895960623757881, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.008846541866660118}, {"id": 2749, "seek": 1084964, "start": 10854.84, "end": 10859.16, "text": " that we're adding the bias, although it could really be considered as a part of the summation", "tokens": [50624, 300, 321, 434, 5127, 264, 12577, 11, 4878, 309, 727, 534, 312, 4888, 382, 257, 644, 295, 264, 28811, 50840], "temperature": 0.0, "avg_logprob": -0.0895960623757881, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.008846541866660118}, {"id": 2750, "seek": 1084964, "start": 10859.16, "end": 10864.84, "text": " equation, because it's another connection to the neural. Now, let's just talk about what", "tokens": [50840, 5367, 11, 570, 309, 311, 1071, 4984, 281, 264, 18161, 13, 823, 11, 718, 311, 445, 751, 466, 437, 51124], "temperature": 0.0, "avg_logprob": -0.0895960623757881, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.008846541866660118}, {"id": 2751, "seek": 1084964, "start": 10864.84, "end": 10869.72, "text": " this symbol means for anyone that's confused about that. Essentially, this stands for sum,", "tokens": [51124, 341, 5986, 1355, 337, 2878, 300, 311, 9019, 466, 300, 13, 23596, 11, 341, 7382, 337, 2408, 11, 51368], "temperature": 0.0, "avg_logprob": -0.0895960623757881, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.008846541866660118}, {"id": 2752, "seek": 1084964, "start": 10869.72, "end": 10876.119999999999, "text": " I stands for an index, and n stands for what index will go up to now n means how many neurons we", "tokens": [51368, 286, 7382, 337, 364, 8186, 11, 293, 297, 7382, 337, 437, 8186, 486, 352, 493, 281, 586, 297, 1355, 577, 867, 22027, 321, 51688], "temperature": 0.0, "avg_logprob": -0.0895960623757881, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.008846541866660118}, {"id": 2753, "seek": 1087612, "start": 10876.12, "end": 10881.320000000002, "text": " had in the previous layer. And then what we're doing here is saying wi xi. So we're going to say", "tokens": [50364, 632, 294, 264, 3894, 4583, 13, 400, 550, 437, 321, 434, 884, 510, 307, 1566, 26393, 36800, 13, 407, 321, 434, 516, 281, 584, 50624], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2754, "seek": 1087612, "start": 10881.320000000002, "end": 10887.400000000001, "text": " weight zero x zero plus weight one x one plus weight two x two, it's almost like a for loop where", "tokens": [50624, 3364, 4018, 2031, 4018, 1804, 3364, 472, 2031, 472, 1804, 3364, 732, 2031, 732, 11, 309, 311, 1920, 411, 257, 337, 6367, 689, 50928], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2755, "seek": 1087612, "start": 10887.400000000001, "end": 10891.880000000001, "text": " we're just adding them all together. And then we add the B. And I hope that makes enough sense", "tokens": [50928, 321, 434, 445, 5127, 552, 439, 1214, 13, 400, 550, 321, 909, 264, 363, 13, 400, 286, 1454, 300, 1669, 1547, 2020, 51152], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2756, "seek": 1087612, "start": 10891.880000000001, "end": 10896.52, "text": " so that we understand that. So that is our weighted sum and our bias. So essentially, what we do is", "tokens": [51152, 370, 300, 321, 1223, 300, 13, 407, 300, 307, 527, 32807, 2408, 293, 527, 12577, 13, 407, 4476, 11, 437, 321, 360, 307, 51384], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2757, "seek": 1087612, "start": 10896.52, "end": 10900.12, "text": " we go through and we calculate these values. So this gets some value, maybe this value is like", "tokens": [51384, 321, 352, 807, 293, 321, 8873, 613, 4190, 13, 407, 341, 2170, 512, 2158, 11, 1310, 341, 2158, 307, 411, 51564], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2758, "seek": 1087612, "start": 10900.12, "end": 10905.720000000001, "text": " 0.3, maybe this value seven, whatever it is, and we do the same thing now at our output neuron.", "tokens": [51564, 1958, 13, 18, 11, 1310, 341, 2158, 3407, 11, 2035, 309, 307, 11, 293, 321, 360, 264, 912, 551, 586, 412, 527, 5598, 34090, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10936889901066459, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.038461435586214066}, {"id": 2759, "seek": 1090572, "start": 10905.72, "end": 10911.08, "text": " So we take the weighted sum of this value times its weight. And then we take the weighted sum,", "tokens": [50364, 407, 321, 747, 264, 32807, 2408, 295, 341, 2158, 1413, 1080, 3364, 13, 400, 550, 321, 747, 264, 32807, 2408, 11, 50632], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2760, "seek": 1090572, "start": 10911.08, "end": 10917.08, "text": " so this value times its weight, plus the bias, this is given some value here. And then we can", "tokens": [50632, 370, 341, 2158, 1413, 1080, 3364, 11, 1804, 264, 12577, 11, 341, 307, 2212, 512, 2158, 510, 13, 400, 550, 321, 393, 50932], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2761, "seek": 1090572, "start": 10917.08, "end": 10922.279999999999, "text": " look at that value and determine what the output of our neural network is. So that is pretty much", "tokens": [50932, 574, 412, 300, 2158, 293, 6997, 437, 264, 5598, 295, 527, 18161, 3209, 307, 13, 407, 300, 307, 1238, 709, 51192], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2762, "seek": 1090572, "start": 10922.279999999999, "end": 10926.599999999999, "text": " how that works in terms of the weighted sums, the weights and the biases. Now, let's talk about the", "tokens": [51192, 577, 300, 1985, 294, 2115, 295, 264, 32807, 34499, 11, 264, 17443, 293, 264, 32152, 13, 823, 11, 718, 311, 751, 466, 264, 51408], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2763, "seek": 1090572, "start": 10926.599999999999, "end": 10930.92, "text": " kind of the training process and another thing called an activation function. So I've lied to", "tokens": [51408, 733, 295, 264, 3097, 1399, 293, 1071, 551, 1219, 364, 24433, 2445, 13, 407, 286, 600, 20101, 281, 51624], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2764, "seek": 1090572, "start": 10930.92, "end": 10934.199999999999, "text": " a little bit because I've said I'm just going to start erasing some stuff. So we have a little bit", "tokens": [51624, 257, 707, 857, 570, 286, 600, 848, 286, 478, 445, 516, 281, 722, 1189, 3349, 512, 1507, 13, 407, 321, 362, 257, 707, 857, 51788], "temperature": 0.0, "avg_logprob": -0.06332524393645811, "compression_ratio": 1.9560810810810811, "no_speech_prob": 0.0005883929552510381}, {"id": 2765, "seek": 1093420, "start": 10934.2, "end": 10939.0, "text": " more room on here. So I've lied to you and I've said that this is completely how this works.", "tokens": [50364, 544, 1808, 322, 510, 13, 407, 286, 600, 20101, 281, 291, 293, 286, 600, 848, 300, 341, 307, 2584, 577, 341, 1985, 13, 50604], "temperature": 0.0, "avg_logprob": -0.060176841143904064, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0075767324306070805}, {"id": 2766, "seek": 1093420, "start": 10939.0, "end": 10943.08, "text": " Well, we're missing one key feature that I want to talk about, which is called an activation", "tokens": [50604, 1042, 11, 321, 434, 5361, 472, 2141, 4111, 300, 286, 528, 281, 751, 466, 11, 597, 307, 1219, 364, 24433, 50808], "temperature": 0.0, "avg_logprob": -0.060176841143904064, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0075767324306070805}, {"id": 2767, "seek": 1093420, "start": 10943.08, "end": 10949.320000000002, "text": " function. Now remember how we want this value to be in between zero and one right at our output layer.", "tokens": [50808, 2445, 13, 823, 1604, 577, 321, 528, 341, 2158, 281, 312, 294, 1296, 4018, 293, 472, 558, 412, 527, 5598, 4583, 13, 51120], "temperature": 0.0, "avg_logprob": -0.060176841143904064, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0075767324306070805}, {"id": 2768, "seek": 1093420, "start": 10949.320000000002, "end": 10953.400000000001, "text": " Well, right now, we can't really guarantee that that's going to happen. I mean, especially for", "tokens": [51120, 1042, 11, 558, 586, 11, 321, 393, 380, 534, 10815, 300, 300, 311, 516, 281, 1051, 13, 286, 914, 11, 2318, 337, 51324], "temperature": 0.0, "avg_logprob": -0.060176841143904064, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0075767324306070805}, {"id": 2769, "seek": 1093420, "start": 10953.400000000001, "end": 10958.44, "text": " starting with random weights and random biases in our neural network, we're passing this information", "tokens": [51324, 2891, 365, 4974, 17443, 293, 4974, 32152, 294, 527, 18161, 3209, 11, 321, 434, 8437, 341, 1589, 51576], "temperature": 0.0, "avg_logprob": -0.060176841143904064, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0075767324306070805}, {"id": 2770, "seek": 1095844, "start": 10958.44, "end": 10964.12, "text": " through, we could get to this, you know, point here, we could have like 700 as our value.", "tokens": [50364, 807, 11, 321, 727, 483, 281, 341, 11, 291, 458, 11, 935, 510, 11, 321, 727, 362, 411, 15204, 382, 527, 2158, 13, 50648], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2771, "seek": 1095844, "start": 10964.84, "end": 10968.6, "text": " That's kind of crazy to me, right? We have this huge value, how do we look at 700 and", "tokens": [50684, 663, 311, 733, 295, 3219, 281, 385, 11, 558, 30, 492, 362, 341, 2603, 2158, 11, 577, 360, 321, 574, 412, 15204, 293, 50872], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2772, "seek": 1095844, "start": 10968.6, "end": 10972.12, "text": " determine whether this is red or whether this is blue? Well, we can use something called an", "tokens": [50872, 6997, 1968, 341, 307, 2182, 420, 1968, 341, 307, 3344, 30, 1042, 11, 321, 393, 764, 746, 1219, 364, 51048], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2773, "seek": 1095844, "start": 10972.12, "end": 10976.28, "text": " activation function. Now, I'm going to go back to my slides here, whatever you want to call this", "tokens": [51048, 24433, 2445, 13, 823, 11, 286, 478, 516, 281, 352, 646, 281, 452, 9788, 510, 11, 2035, 291, 528, 281, 818, 341, 51256], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2774, "seek": 1095844, "start": 10976.28, "end": 10980.2, "text": " this notebook, just to talk about what an activation function is. And you guys can see here, you can", "tokens": [51256, 341, 21060, 11, 445, 281, 751, 466, 437, 364, 24433, 2445, 307, 13, 400, 291, 1074, 393, 536, 510, 11, 291, 393, 51452], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2775, "seek": 1095844, "start": 10980.2, "end": 10985.800000000001, "text": " follow along, I have all the equations kind of written out here as well. So let's go to activation", "tokens": [51452, 1524, 2051, 11, 286, 362, 439, 264, 11787, 733, 295, 3720, 484, 510, 382, 731, 13, 407, 718, 311, 352, 281, 24433, 51732], "temperature": 0.0, "avg_logprob": -0.07681196804704338, "compression_ratio": 1.8019169329073483, "no_speech_prob": 0.1259021759033203}, {"id": 2776, "seek": 1098580, "start": 10985.88, "end": 10990.359999999999, "text": " function, which is right here. Okay. So these are some examples of an activation function. And I", "tokens": [50368, 2445, 11, 597, 307, 558, 510, 13, 1033, 13, 407, 613, 366, 512, 5110, 295, 364, 24433, 2445, 13, 400, 286, 50592], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2777, "seek": 1098580, "start": 10990.359999999999, "end": 10994.679999999998, "text": " just want you to look at what they do. So this first one is called rectified linear unit. Now", "tokens": [50592, 445, 528, 291, 281, 574, 412, 437, 436, 360, 13, 407, 341, 700, 472, 307, 1219, 11048, 2587, 8213, 4985, 13, 823, 50808], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2778, "seek": 1098580, "start": 10994.679999999998, "end": 11000.199999999999, "text": " notice that essentially what this activation function does is take any values that are less", "tokens": [50808, 3449, 300, 4476, 437, 341, 24433, 2445, 775, 307, 747, 604, 4190, 300, 366, 1570, 51084], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2779, "seek": 1098580, "start": 11000.199999999999, "end": 11005.08, "text": " than zero and just make them zero. So any x values that are, you know, in the negative, it just makes", "tokens": [51084, 813, 4018, 293, 445, 652, 552, 4018, 13, 407, 604, 2031, 4190, 300, 366, 11, 291, 458, 11, 294, 264, 3671, 11, 309, 445, 1669, 51328], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2780, "seek": 1098580, "start": 11005.08, "end": 11010.439999999999, "text": " their y zero. And then any values that are positive, it's just equal to whatever their positive value", "tokens": [51328, 641, 288, 4018, 13, 400, 550, 604, 4190, 300, 366, 3353, 11, 309, 311, 445, 2681, 281, 2035, 641, 3353, 2158, 51596], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2781, "seek": 1098580, "start": 11010.439999999999, "end": 11015.16, "text": " is. So if it's 10, it's 10. This allows us to just pretty much eliminate any negative numbers,", "tokens": [51596, 307, 13, 407, 498, 309, 311, 1266, 11, 309, 311, 1266, 13, 639, 4045, 505, 281, 445, 1238, 709, 13819, 604, 3671, 3547, 11, 51832], "temperature": 0.0, "avg_logprob": -0.05689083018773038, "compression_ratio": 1.8741935483870968, "no_speech_prob": 0.0044681113213300705}, {"id": 2782, "seek": 1101516, "start": 11015.16, "end": 11020.76, "text": " right? That's kind of what rectified linear unit does. Now 10 h or hyperbolic tangent.", "tokens": [50364, 558, 30, 663, 311, 733, 295, 437, 11048, 2587, 8213, 4985, 775, 13, 823, 1266, 276, 420, 9848, 65, 7940, 27747, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10537687670282957, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0005192963872104883}, {"id": 2783, "seek": 1101516, "start": 11021.4, "end": 11026.52, "text": " What does this do? This actually squishes our values between negative one and one. So it takes", "tokens": [50676, 708, 775, 341, 360, 30, 639, 767, 2339, 16423, 527, 4190, 1296, 3671, 472, 293, 472, 13, 407, 309, 2516, 50932], "temperature": 0.0, "avg_logprob": -0.10537687670282957, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0005192963872104883}, {"id": 2784, "seek": 1101516, "start": 11026.52, "end": 11031.72, "text": " whatever values we have, and the more positive they are, the closer to one they are, the more", "tokens": [50932, 2035, 4190, 321, 362, 11, 293, 264, 544, 3353, 436, 366, 11, 264, 4966, 281, 472, 436, 366, 11, 264, 544, 51192], "temperature": 0.0, "avg_logprob": -0.10537687670282957, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0005192963872104883}, {"id": 2785, "seek": 1101516, "start": 11031.72, "end": 11036.36, "text": " negative they are, the closer to negative one they are. So when we see why this might be useful,", "tokens": [51192, 3671, 436, 366, 11, 264, 4966, 281, 3671, 472, 436, 366, 13, 407, 562, 321, 536, 983, 341, 1062, 312, 4420, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10537687670282957, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0005192963872104883}, {"id": 2786, "seek": 1101516, "start": 11036.36, "end": 11040.2, "text": " right for a neural network, and then last one is sigmoid, what this does is squish our values", "tokens": [51424, 558, 337, 257, 18161, 3209, 11, 293, 550, 1036, 472, 307, 4556, 3280, 327, 11, 437, 341, 775, 307, 31379, 527, 4190, 51616], "temperature": 0.0, "avg_logprob": -0.10537687670282957, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0005192963872104883}, {"id": 2787, "seek": 1104020, "start": 11040.2, "end": 11045.400000000001, "text": " between zero and one, a lot of people call it like the squishifier function. Because all it does", "tokens": [50364, 1296, 4018, 293, 472, 11, 257, 688, 295, 561, 818, 309, 411, 264, 31379, 9902, 2445, 13, 1436, 439, 309, 775, 50624], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2788, "seek": 1104020, "start": 11045.400000000001, "end": 11049.960000000001, "text": " is take any extremely negative numbers and put them closer to zero and any extremely positive", "tokens": [50624, 307, 747, 604, 4664, 3671, 3547, 293, 829, 552, 4966, 281, 4018, 293, 604, 4664, 3353, 50852], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2789, "seek": 1104020, "start": 11049.960000000001, "end": 11053.560000000001, "text": " numbers and put them close to one, any values in between, you're going to get some number that's", "tokens": [50852, 3547, 293, 829, 552, 1998, 281, 472, 11, 604, 4190, 294, 1296, 11, 291, 434, 516, 281, 483, 512, 1230, 300, 311, 51032], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2790, "seek": 1104020, "start": 11053.560000000001, "end": 11058.76, "text": " kind of in between that, based on the equation one over one plus e to the negative z. And this is", "tokens": [51032, 733, 295, 294, 1296, 300, 11, 2361, 322, 264, 5367, 472, 670, 472, 1804, 308, 281, 264, 3671, 710, 13, 400, 341, 307, 51292], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2791, "seek": 1104020, "start": 11058.76, "end": 11063.640000000001, "text": " theta z, I guess, is equal to that. Okay, so that's how that works. Those are some activation", "tokens": [51292, 9725, 710, 11, 286, 2041, 11, 307, 2681, 281, 300, 13, 1033, 11, 370, 300, 311, 577, 300, 1985, 13, 3950, 366, 512, 24433, 51536], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2792, "seek": 1104020, "start": 11063.640000000001, "end": 11067.240000000002, "text": " functions. Now I hope that's not too much math for you. But let's talk about how we use them,", "tokens": [51536, 6828, 13, 823, 286, 1454, 300, 311, 406, 886, 709, 5221, 337, 291, 13, 583, 718, 311, 751, 466, 577, 321, 764, 552, 11, 51716], "temperature": 0.0, "avg_logprob": -0.09116869502597386, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.015905560925602913}, {"id": 2793, "seek": 1106724, "start": 11067.24, "end": 11072.119999999999, "text": " right? So essentially, what we do is at each of our neurons, we're going to have an activation", "tokens": [50364, 558, 30, 407, 4476, 11, 437, 321, 360, 307, 412, 1184, 295, 527, 22027, 11, 321, 434, 516, 281, 362, 364, 24433, 50608], "temperature": 0.0, "avg_logprob": -0.052013464648314196, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.012430119328200817}, {"id": 2794, "seek": 1106724, "start": 11072.119999999999, "end": 11077.56, "text": " function that is applied to the output of that neuron. So we take this this weighted sum plus", "tokens": [50608, 2445, 300, 307, 6456, 281, 264, 5598, 295, 300, 34090, 13, 407, 321, 747, 341, 341, 32807, 2408, 1804, 50880], "temperature": 0.0, "avg_logprob": -0.052013464648314196, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.012430119328200817}, {"id": 2795, "seek": 1106724, "start": 11077.56, "end": 11083.72, "text": " the bias, and then we apply an activation function to it before we send that value to the next neuron.", "tokens": [50880, 264, 12577, 11, 293, 550, 321, 3079, 364, 24433, 2445, 281, 309, 949, 321, 2845, 300, 2158, 281, 264, 958, 34090, 13, 51188], "temperature": 0.0, "avg_logprob": -0.052013464648314196, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.012430119328200817}, {"id": 2796, "seek": 1106724, "start": 11083.72, "end": 11090.68, "text": " So in this case, n one isn't actually just equal to this, what n one is equal to is n one is equal", "tokens": [51188, 407, 294, 341, 1389, 11, 297, 472, 1943, 380, 767, 445, 2681, 281, 341, 11, 437, 297, 472, 307, 2681, 281, 307, 297, 472, 307, 2681, 51536], "temperature": 0.0, "avg_logprob": -0.052013464648314196, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.012430119328200817}, {"id": 2797, "seek": 1109068, "start": 11090.68, "end": 11096.76, "text": " to f, which stands for activation function of this equation, right? So we say I equals zero,", "tokens": [50364, 281, 283, 11, 597, 7382, 337, 24433, 2445, 295, 341, 5367, 11, 558, 30, 407, 321, 584, 286, 6915, 4018, 11, 50668], "temperature": 0.0, "avg_logprob": -0.14909669186206573, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.024421608075499535}, {"id": 2798, "seek": 1109068, "start": 11098.04, "end": 11106.52, "text": " w i x i plus B. And that's what n one's value is equal to when it comes to this output neuron.", "tokens": [50732, 261, 741, 2031, 741, 1804, 363, 13, 400, 300, 311, 437, 297, 472, 311, 2158, 307, 2681, 281, 562, 309, 1487, 281, 341, 5598, 34090, 13, 51156], "temperature": 0.0, "avg_logprob": -0.14909669186206573, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.024421608075499535}, {"id": 2799, "seek": 1109068, "start": 11107.16, "end": 11111.4, "text": " So each of these have an activation function on them. And two has the same activation function", "tokens": [51188, 407, 1184, 295, 613, 362, 364, 24433, 2445, 322, 552, 13, 400, 732, 575, 264, 912, 24433, 2445, 51400], "temperature": 0.0, "avg_logprob": -0.14909669186206573, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.024421608075499535}, {"id": 2800, "seek": 1109068, "start": 11111.4, "end": 11116.28, "text": " as n one. And we can define what activation function we want to apply at each neuron.", "tokens": [51400, 382, 297, 472, 13, 400, 321, 393, 6964, 437, 24433, 2445, 321, 528, 281, 3079, 412, 1184, 34090, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14909669186206573, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.024421608075499535}, {"id": 2801, "seek": 1111628, "start": 11116.92, "end": 11120.84, "text": " Now at our output neuron, the activation function is very important, because we need to determine", "tokens": [50396, 823, 412, 527, 5598, 34090, 11, 264, 24433, 2445, 307, 588, 1021, 11, 570, 321, 643, 281, 6997, 50592], "temperature": 0.0, "avg_logprob": -0.0820061900034672, "compression_ratio": 2.0041322314049586, "no_speech_prob": 0.008577050641179085}, {"id": 2802, "seek": 1111628, "start": 11120.84, "end": 11124.04, "text": " what we want our value to look like. Do we want it between negative one on one? Do we want it", "tokens": [50592, 437, 321, 528, 527, 2158, 281, 574, 411, 13, 1144, 321, 528, 309, 1296, 3671, 472, 322, 472, 30, 1144, 321, 528, 309, 50752], "temperature": 0.0, "avg_logprob": -0.0820061900034672, "compression_ratio": 2.0041322314049586, "no_speech_prob": 0.008577050641179085}, {"id": 2803, "seek": 1111628, "start": 11124.04, "end": 11129.960000000001, "text": " between zero and one? Or do we want it to be some massively large number? Do we want it between zero", "tokens": [50752, 1296, 4018, 293, 472, 30, 1610, 360, 321, 528, 309, 281, 312, 512, 29379, 2416, 1230, 30, 1144, 321, 528, 309, 1296, 4018, 51048], "temperature": 0.0, "avg_logprob": -0.0820061900034672, "compression_ratio": 2.0041322314049586, "no_speech_prob": 0.008577050641179085}, {"id": 2804, "seek": 1111628, "start": 11129.960000000001, "end": 11135.0, "text": " and positive infinity? What do we want? Right? So what we do is we pick some activation function", "tokens": [51048, 293, 3353, 13202, 30, 708, 360, 321, 528, 30, 1779, 30, 407, 437, 321, 360, 307, 321, 1888, 512, 24433, 2445, 51300], "temperature": 0.0, "avg_logprob": -0.0820061900034672, "compression_ratio": 2.0041322314049586, "no_speech_prob": 0.008577050641179085}, {"id": 2805, "seek": 1111628, "start": 11135.0, "end": 11141.0, "text": " for our output neuron. And based on what I said, where we want our values between zero and one,", "tokens": [51300, 337, 527, 5598, 34090, 13, 400, 2361, 322, 437, 286, 848, 11, 689, 321, 528, 527, 4190, 1296, 4018, 293, 472, 11, 51600], "temperature": 0.0, "avg_logprob": -0.0820061900034672, "compression_ratio": 2.0041322314049586, "no_speech_prob": 0.008577050641179085}, {"id": 2806, "seek": 1114100, "start": 11141.08, "end": 11148.04, "text": " I'm going to be picking the sigmoid function. So sigmoid, recall, squishes our values between", "tokens": [50368, 286, 478, 516, 281, 312, 8867, 264, 4556, 3280, 327, 2445, 13, 407, 4556, 3280, 327, 11, 9901, 11, 2339, 16423, 527, 4190, 1296, 50716], "temperature": 0.0, "avg_logprob": -0.0824338175215811, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.023686591535806656}, {"id": 2807, "seek": 1114100, "start": 11148.04, "end": 11155.16, "text": " zero and one. So what we'll do here is we'll take n one, right? So n one times whatever the weight", "tokens": [50716, 4018, 293, 472, 13, 407, 437, 321, 603, 360, 510, 307, 321, 603, 747, 297, 472, 11, 558, 30, 407, 297, 472, 1413, 2035, 264, 3364, 51072], "temperature": 0.0, "avg_logprob": -0.0824338175215811, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.023686591535806656}, {"id": 2808, "seek": 1114100, "start": 11155.16, "end": 11163.24, "text": " is there. So weight zero, plus n two times weight one, plus a bias, and apply sigmoid.", "tokens": [51072, 307, 456, 13, 407, 3364, 4018, 11, 1804, 297, 732, 1413, 3364, 472, 11, 1804, 257, 12577, 11, 293, 3079, 4556, 3280, 327, 13, 51476], "temperature": 0.0, "avg_logprob": -0.0824338175215811, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.023686591535806656}, {"id": 2809, "seek": 1114100, "start": 11164.28, "end": 11170.12, "text": " And then this will give us some value between zero and one, then we can look at that value and we", "tokens": [51528, 400, 550, 341, 486, 976, 505, 512, 2158, 1296, 4018, 293, 472, 11, 550, 321, 393, 574, 412, 300, 2158, 293, 321, 51820], "temperature": 0.0, "avg_logprob": -0.0824338175215811, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.023686591535806656}, {"id": 2810, "seek": 1117012, "start": 11170.12, "end": 11174.52, "text": " can determine what the output of this network is. So that's great. And that makes sense. Why", "tokens": [50364, 393, 6997, 437, 264, 5598, 295, 341, 3209, 307, 13, 407, 300, 311, 869, 13, 400, 300, 1669, 2020, 13, 1545, 50584], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2811, "seek": 1117012, "start": 11174.52, "end": 11179.400000000001, "text": " we would use that on the output neuron, right? So we can squish our value in between some kind", "tokens": [50584, 321, 576, 764, 300, 322, 264, 5598, 34090, 11, 558, 30, 407, 321, 393, 31379, 527, 2158, 294, 1296, 512, 733, 50828], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2812, "seek": 1117012, "start": 11179.400000000001, "end": 11182.44, "text": " of value. So we can actually look at it and determine, you know, what to do with it, rather", "tokens": [50828, 295, 2158, 13, 407, 321, 393, 767, 574, 412, 309, 293, 6997, 11, 291, 458, 11, 437, 281, 360, 365, 309, 11, 2831, 50980], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2813, "seek": 1117012, "start": 11182.44, "end": 11187.08, "text": " than just having these crazy, and I want to see if I can make this eraser any bigger. Ah, that's", "tokens": [50980, 813, 445, 1419, 613, 3219, 11, 293, 286, 528, 281, 536, 498, 286, 393, 652, 341, 46018, 604, 3801, 13, 2438, 11, 300, 311, 51212], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2814, "seek": 1117012, "start": 11187.08, "end": 11192.68, "text": " much better. Okay. So there we go. Let's just erase some of this. And now let's talk about why we", "tokens": [51212, 709, 1101, 13, 1033, 13, 407, 456, 321, 352, 13, 961, 311, 445, 23525, 512, 295, 341, 13, 400, 586, 718, 311, 751, 466, 983, 321, 51492], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2815, "seek": 1117012, "start": 11192.68, "end": 11197.480000000001, "text": " would use the activation function on like an intermediate layer like this. Well, the whole", "tokens": [51492, 576, 764, 264, 24433, 2445, 322, 411, 364, 19376, 4583, 411, 341, 13, 1042, 11, 264, 1379, 51732], "temperature": 0.0, "avg_logprob": -0.07001015911363576, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.0021155800204724073}, {"id": 2816, "seek": 1119748, "start": 11197.48, "end": 11203.48, "text": " point of an activation function is to introduce complexity into our neural network. So essentially,", "tokens": [50364, 935, 295, 364, 24433, 2445, 307, 281, 5366, 14024, 666, 527, 18161, 3209, 13, 407, 4476, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2817, "seek": 1119748, "start": 11203.48, "end": 11207.72, "text": " you know, we just have these basic weights and these biases. And this is kind of just,", "tokens": [50664, 291, 458, 11, 321, 445, 362, 613, 3875, 17443, 293, 613, 32152, 13, 400, 341, 307, 733, 295, 445, 11, 50876], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2818, "seek": 1119748, "start": 11207.72, "end": 11211.0, "text": " you know, like a complex function at this point, we have a bunch of weights, we have a bunch of", "tokens": [50876, 291, 458, 11, 411, 257, 3997, 2445, 412, 341, 935, 11, 321, 362, 257, 3840, 295, 17443, 11, 321, 362, 257, 3840, 295, 51040], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2819, "seek": 1119748, "start": 11211.0, "end": 11214.6, "text": " biases. And those are the only things that we're training. And the only things that we're changing", "tokens": [51040, 32152, 13, 400, 729, 366, 264, 787, 721, 300, 321, 434, 3097, 13, 400, 264, 787, 721, 300, 321, 434, 4473, 51220], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2820, "seek": 1119748, "start": 11214.6, "end": 11219.88, "text": " to make our network better. Now, what an activation function can do is, for example,", "tokens": [51220, 281, 652, 527, 3209, 1101, 13, 823, 11, 437, 364, 24433, 2445, 393, 360, 307, 11, 337, 1365, 11, 51484], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2821, "seek": 1119748, "start": 11219.88, "end": 11223.0, "text": " take a bunch of points that are on the same like plane, right? So let's just say,", "tokens": [51484, 747, 257, 3840, 295, 2793, 300, 366, 322, 264, 912, 411, 5720, 11, 558, 30, 407, 718, 311, 445, 584, 11, 51640], "temperature": 0.0, "avg_logprob": -0.07527314645272715, "compression_ratio": 1.96415770609319, "no_speech_prob": 0.0041985297575592995}, {"id": 2822, "seek": 1122300, "start": 11223.88, "end": 11229.56, "text": " these are in some plane. If we can apply an activation function of these, where we", "tokens": [50408, 613, 366, 294, 512, 5720, 13, 759, 321, 393, 3079, 364, 24433, 2445, 295, 613, 11, 689, 321, 50692], "temperature": 0.0, "avg_logprob": -0.11915470304943267, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.009707936085760593}, {"id": 2823, "seek": 1122300, "start": 11230.52, "end": 11234.92, "text": " introduce a higher dimensionality, so an activation function like sigmoid that is like a", "tokens": [50740, 5366, 257, 2946, 10139, 1860, 11, 370, 364, 24433, 2445, 411, 4556, 3280, 327, 300, 307, 411, 257, 50960], "temperature": 0.0, "avg_logprob": -0.11915470304943267, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.009707936085760593}, {"id": 2824, "seek": 1122300, "start": 11234.92, "end": 11241.8, "text": " higher dimension function, we can hopefully spread these points out and move them up or down off the", "tokens": [50960, 2946, 10139, 2445, 11, 321, 393, 4696, 3974, 613, 2793, 484, 293, 1286, 552, 493, 420, 760, 766, 264, 51304], "temperature": 0.0, "avg_logprob": -0.11915470304943267, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.009707936085760593}, {"id": 2825, "seek": 1122300, "start": 11241.8, "end": 11248.76, "text": " plane in a hopes of extracting kind of some different features. Now, it's hard to explain", "tokens": [51304, 5720, 294, 257, 13681, 295, 49844, 733, 295, 512, 819, 4122, 13, 823, 11, 309, 311, 1152, 281, 2903, 51652], "temperature": 0.0, "avg_logprob": -0.11915470304943267, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.009707936085760593}, {"id": 2826, "seek": 1124876, "start": 11248.76, "end": 11253.800000000001, "text": " this until we get into the training process of the neural network. But I'm hoping this is maybe", "tokens": [50364, 341, 1826, 321, 483, 666, 264, 3097, 1399, 295, 264, 18161, 3209, 13, 583, 286, 478, 7159, 341, 307, 1310, 50616], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2827, "seek": 1124876, "start": 11253.800000000001, "end": 11259.0, "text": " giving you a little bit of idea, if we can introduce a complex activation function into this kind of", "tokens": [50616, 2902, 291, 257, 707, 857, 295, 1558, 11, 498, 321, 393, 5366, 257, 3997, 24433, 2445, 666, 341, 733, 295, 50876], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2828, "seek": 1124876, "start": 11259.0, "end": 11263.56, "text": " process, then it allows us to make some more complex predictions, we can pick up on some", "tokens": [50876, 1399, 11, 550, 309, 4045, 505, 281, 652, 512, 544, 3997, 21264, 11, 321, 393, 1888, 493, 322, 512, 51104], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2829, "seek": 1124876, "start": 11263.56, "end": 11268.36, "text": " different patterns. If I can see that, you know, when sigmoid or rectify linear unit is applied", "tokens": [51104, 819, 8294, 13, 759, 286, 393, 536, 300, 11, 291, 458, 11, 562, 4556, 3280, 327, 420, 11048, 2505, 8213, 4985, 307, 6456, 51344], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2830, "seek": 1124876, "start": 11268.36, "end": 11273.0, "text": " to this output, it moves my point up or it moves it down or moves it in like whatever direction", "tokens": [51344, 281, 341, 5598, 11, 309, 6067, 452, 935, 493, 420, 309, 6067, 309, 760, 420, 6067, 309, 294, 411, 2035, 3513, 51576], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2831, "seek": 1124876, "start": 11273.0, "end": 11277.56, "text": " and n dimensional space, then I can determine specific patterns I couldn't determine in the", "tokens": [51576, 293, 297, 18795, 1901, 11, 550, 286, 393, 6997, 2685, 8294, 286, 2809, 380, 6997, 294, 264, 51804], "temperature": 0.0, "avg_logprob": -0.0642928080772286, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.10085760802030563}, {"id": 2832, "seek": 1127756, "start": 11277.56, "end": 11281.96, "text": " previous dimension. That's just like if we're looking at something in two dimensions, if I can", "tokens": [50364, 3894, 10139, 13, 663, 311, 445, 411, 498, 321, 434, 1237, 412, 746, 294, 732, 12819, 11, 498, 286, 393, 50584], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2833, "seek": 1127756, "start": 11281.96, "end": 11285.88, "text": " move that into three dimensions, I immediately see more detail, there's more things that I can", "tokens": [50584, 1286, 300, 666, 1045, 12819, 11, 286, 4258, 536, 544, 2607, 11, 456, 311, 544, 721, 300, 286, 393, 50780], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2834, "seek": 1127756, "start": 11285.88, "end": 11290.359999999999, "text": " look at, right? And I'll try to do a good example of why we might use it like this. So let's say", "tokens": [50780, 574, 412, 11, 558, 30, 400, 286, 603, 853, 281, 360, 257, 665, 1365, 295, 983, 321, 1062, 764, 309, 411, 341, 13, 407, 718, 311, 584, 51004], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2835, "seek": 1127756, "start": 11290.359999999999, "end": 11294.279999999999, "text": " we have a square, right, like this, right? And I ask you, I'm like, tell me some information", "tokens": [51004, 321, 362, 257, 3732, 11, 558, 11, 411, 341, 11, 558, 30, 400, 286, 1029, 291, 11, 286, 478, 411, 11, 980, 385, 512, 1589, 51200], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2836, "seek": 1127756, "start": 11294.279999999999, "end": 11297.08, "text": " about the square. Well, what you can tell me immediately is you can tell me the width, you", "tokens": [51200, 466, 264, 3732, 13, 1042, 11, 437, 291, 393, 980, 385, 4258, 307, 291, 393, 980, 385, 264, 11402, 11, 291, 51340], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2837, "seek": 1127756, "start": 11297.08, "end": 11301.16, "text": " can tell me the height, and I guess you could tell me the color, right? You can tell me it has", "tokens": [51340, 393, 980, 385, 264, 6681, 11, 293, 286, 2041, 291, 727, 980, 385, 264, 2017, 11, 558, 30, 509, 393, 980, 385, 309, 575, 51544], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2838, "seek": 1127756, "start": 11301.16, "end": 11304.92, "text": " one face, you can tell me it has four vertexes, you can tell me a fair amount about the square,", "tokens": [51544, 472, 1851, 11, 291, 393, 980, 385, 309, 575, 1451, 28162, 279, 11, 291, 393, 980, 385, 257, 3143, 2372, 466, 264, 3732, 11, 51732], "temperature": 0.0, "avg_logprob": -0.07830968228253452, "compression_ratio": 2.0786163522012577, "no_speech_prob": 0.034093692898750305}, {"id": 2839, "seek": 1130492, "start": 11304.92, "end": 11309.4, "text": " you can tell me its area. Now what happens as soon as I extend the square and I make it into a cube?", "tokens": [50364, 291, 393, 980, 385, 1080, 1859, 13, 823, 437, 2314, 382, 2321, 382, 286, 10101, 264, 3732, 293, 286, 652, 309, 666, 257, 13728, 30, 50588], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2840, "seek": 1130492, "start": 11310.28, "end": 11314.76, "text": " Well, now you can immediately tell me a lot more information, you can tell me, you know, the height,", "tokens": [50632, 1042, 11, 586, 291, 393, 4258, 980, 385, 257, 688, 544, 1589, 11, 291, 393, 980, 385, 11, 291, 458, 11, 264, 6681, 11, 50856], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2841, "seek": 1130492, "start": 11315.64, "end": 11320.6, "text": " or I guess the depth with height depth, yeah, whatever you want to call it there. You can tell", "tokens": [50900, 420, 286, 2041, 264, 7161, 365, 6681, 7161, 11, 1338, 11, 2035, 291, 528, 281, 818, 309, 456, 13, 509, 393, 980, 51148], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2842, "seek": 1130492, "start": 11320.6, "end": 11324.6, "text": " me how many faces it has, you can tell me what color each of the faces are, you can tell me how", "tokens": [51148, 385, 577, 867, 8475, 309, 575, 11, 291, 393, 980, 385, 437, 2017, 1184, 295, 264, 8475, 366, 11, 291, 393, 980, 385, 577, 51348], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2843, "seek": 1130492, "start": 11324.6, "end": 11329.88, "text": " many vertexes you can tell me if this cube or the square, this rectangle is uniform or not,", "tokens": [51348, 867, 28162, 279, 291, 393, 980, 385, 498, 341, 13728, 420, 264, 3732, 11, 341, 21930, 307, 9452, 420, 406, 11, 51612], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2844, "seek": 1130492, "start": 11329.88, "end": 11333.88, "text": " and you can pick up on a lot more information. So that's kind of I mean, this is a very over", "tokens": [51612, 293, 291, 393, 1888, 493, 322, 257, 688, 544, 1589, 13, 407, 300, 311, 733, 295, 286, 914, 11, 341, 307, 257, 588, 670, 51812], "temperature": 0.0, "avg_logprob": -0.0947880930714793, "compression_ratio": 1.9559322033898305, "no_speech_prob": 0.05183766782283783}, {"id": 2845, "seek": 1133388, "start": 11333.88, "end": 11339.16, "text": " simplification of what this actually does. But this is kind of the concept, right, is that if we", "tokens": [50364, 6883, 3774, 295, 437, 341, 767, 775, 13, 583, 341, 307, 733, 295, 264, 3410, 11, 558, 11, 307, 300, 498, 321, 50628], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2846, "seek": 1133388, "start": 11339.16, "end": 11343.72, "text": " are in two dimensions, if we can somehow move our data points into a higher dimension by applying", "tokens": [50628, 366, 294, 732, 12819, 11, 498, 321, 393, 6063, 1286, 527, 1412, 2793, 666, 257, 2946, 10139, 538, 9275, 50856], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2847, "seek": 1133388, "start": 11343.72, "end": 11348.359999999999, "text": " some function to them, then what we can do is get more information and extract more information", "tokens": [50856, 512, 2445, 281, 552, 11, 550, 437, 321, 393, 360, 307, 483, 544, 1589, 293, 8947, 544, 1589, 51088], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2848, "seek": 1133388, "start": 11348.359999999999, "end": 11353.32, "text": " about the data points, which will lead to better predictions. Okay, so now that we've talked about", "tokens": [51088, 466, 264, 1412, 2793, 11, 597, 486, 1477, 281, 1101, 21264, 13, 1033, 11, 370, 586, 300, 321, 600, 2825, 466, 51336], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2849, "seek": 1133388, "start": 11353.32, "end": 11356.359999999999, "text": " all this, it's time to talk about how neural networks train. And I think you guys are ready", "tokens": [51336, 439, 341, 11, 309, 311, 565, 281, 751, 466, 577, 18161, 9590, 3847, 13, 400, 286, 519, 291, 1074, 366, 1919, 51488], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2850, "seek": 1133388, "start": 11356.359999999999, "end": 11361.4, "text": " for this. This is a little bit more complicated. But again, it's not that crazy. Alright, so we", "tokens": [51488, 337, 341, 13, 639, 307, 257, 707, 857, 544, 6179, 13, 583, 797, 11, 309, 311, 406, 300, 3219, 13, 2798, 11, 370, 321, 51740], "temperature": 0.0, "avg_logprob": -0.04842703290980497, "compression_ratio": 1.7863777089783281, "no_speech_prob": 0.017982639372348785}, {"id": 2851, "seek": 1136140, "start": 11361.48, "end": 11365.4, "text": " talked about these weights and biases. And these weights and biases are what our network will", "tokens": [50368, 2825, 466, 613, 17443, 293, 32152, 13, 400, 613, 17443, 293, 32152, 366, 437, 527, 3209, 486, 50564], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2852, "seek": 1136140, "start": 11365.4, "end": 11371.0, "text": " come up with and determine to, you know, like make the network better. So essentially, what we're", "tokens": [50564, 808, 493, 365, 293, 6997, 281, 11, 291, 458, 11, 411, 652, 264, 3209, 1101, 13, 407, 4476, 11, 437, 321, 434, 50844], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2853, "seek": 1136140, "start": 11371.0, "end": 11376.119999999999, "text": " going to do now is talk about something called a loss function. So as our network starts, right,", "tokens": [50844, 516, 281, 360, 586, 307, 751, 466, 746, 1219, 257, 4470, 2445, 13, 407, 382, 527, 3209, 3719, 11, 558, 11, 51100], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2854, "seek": 1136140, "start": 11376.119999999999, "end": 11381.0, "text": " the way that we train it, just like we've trained other networks, or other machine learning models", "tokens": [51100, 264, 636, 300, 321, 3847, 309, 11, 445, 411, 321, 600, 8895, 661, 9590, 11, 420, 661, 3479, 2539, 5245, 51344], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2855, "seek": 1136140, "start": 11381.0, "end": 11385.96, "text": " is we give it some information, we give it what the expected output is. And then we just see what", "tokens": [51344, 307, 321, 976, 309, 512, 1589, 11, 321, 976, 309, 437, 264, 5176, 5598, 307, 13, 400, 550, 321, 445, 536, 437, 51592], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2856, "seek": 1136140, "start": 11385.96, "end": 11390.44, "text": " the expected output or what the output was from the network, compare it to the expected output", "tokens": [51592, 264, 5176, 5598, 420, 437, 264, 5598, 390, 490, 264, 3209, 11, 6794, 309, 281, 264, 5176, 5598, 51816], "temperature": 0.0, "avg_logprob": -0.051221505919499186, "compression_ratio": 2.0, "no_speech_prob": 0.0028007954824715853}, {"id": 2857, "seek": 1139044, "start": 11390.44, "end": 11395.24, "text": " and modify it like that. So essentially, what we start with is we say, okay, 222, we say this", "tokens": [50364, 293, 16927, 309, 411, 300, 13, 407, 4476, 11, 437, 321, 722, 365, 307, 321, 584, 11, 1392, 11, 5853, 17, 11, 321, 584, 341, 50604], "temperature": 0.0, "avg_logprob": -0.09800875656248077, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0034831471275538206}, {"id": 2858, "seek": 1139044, "start": 11395.24, "end": 11400.84, "text": " class is red, which I forget what I labeled that was as but let's just say, like that was a zero,", "tokens": [50604, 1508, 307, 2182, 11, 597, 286, 2870, 437, 286, 21335, 300, 390, 382, 457, 718, 311, 445, 584, 11, 411, 300, 390, 257, 4018, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09800875656248077, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0034831471275538206}, {"id": 2859, "seek": 1139044, "start": 11400.84, "end": 11407.0, "text": " okay. So this class is zero. So I want this network to give me a zero for the point 222. Now,", "tokens": [50884, 1392, 13, 407, 341, 1508, 307, 4018, 13, 407, 286, 528, 341, 3209, 281, 976, 385, 257, 4018, 337, 264, 935, 5853, 17, 13, 823, 11, 51192], "temperature": 0.0, "avg_logprob": -0.09800875656248077, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0034831471275538206}, {"id": 2860, "seek": 1139044, "start": 11407.0, "end": 11412.36, "text": " this network starts with completely random weights and completely random biases. So chances are,", "tokens": [51192, 341, 3209, 3719, 365, 2584, 4974, 17443, 293, 2584, 4974, 32152, 13, 407, 10486, 366, 11, 51460], "temperature": 0.0, "avg_logprob": -0.09800875656248077, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0034831471275538206}, {"id": 2861, "seek": 1139044, "start": 11412.36, "end": 11416.84, "text": " when we get to this output here, we're not going to get zero, maybe we get some value after applying", "tokens": [51460, 562, 321, 483, 281, 341, 5598, 510, 11, 321, 434, 406, 516, 281, 483, 4018, 11, 1310, 321, 483, 512, 2158, 934, 9275, 51684], "temperature": 0.0, "avg_logprob": -0.09800875656248077, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0034831471275538206}, {"id": 2862, "seek": 1141684, "start": 11416.84, "end": 11424.04, "text": " the sigmoid function, that's like 0.7. Well, this is pretty far away from red. But how far away is", "tokens": [50364, 264, 4556, 3280, 327, 2445, 11, 300, 311, 411, 1958, 13, 22, 13, 1042, 11, 341, 307, 1238, 1400, 1314, 490, 2182, 13, 583, 577, 1400, 1314, 307, 50724], "temperature": 0.0, "avg_logprob": -0.058594533712557045, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.025954851880669594}, {"id": 2863, "seek": 1141684, "start": 11424.04, "end": 11429.4, "text": " it? Well, this is where we use something called a loss function. Now, what a loss function does is", "tokens": [50724, 309, 30, 1042, 11, 341, 307, 689, 321, 764, 746, 1219, 257, 4470, 2445, 13, 823, 11, 437, 257, 4470, 2445, 775, 307, 50992], "temperature": 0.0, "avg_logprob": -0.058594533712557045, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.025954851880669594}, {"id": 2864, "seek": 1141684, "start": 11429.4, "end": 11436.44, "text": " calculate how far away our output was from our expected output. So if our expected output is", "tokens": [50992, 8873, 577, 1400, 1314, 527, 5598, 390, 490, 527, 5176, 5598, 13, 407, 498, 527, 5176, 5598, 307, 51344], "temperature": 0.0, "avg_logprob": -0.058594533712557045, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.025954851880669594}, {"id": 2865, "seek": 1141684, "start": 11436.44, "end": 11442.2, "text": " zero, and our output was 0.7, the loss function is going to give us some value that represents", "tokens": [51344, 4018, 11, 293, 527, 5598, 390, 1958, 13, 22, 11, 264, 4470, 2445, 307, 516, 281, 976, 505, 512, 2158, 300, 8855, 51632], "temperature": 0.0, "avg_logprob": -0.058594533712557045, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.025954851880669594}, {"id": 2866, "seek": 1144220, "start": 11442.2, "end": 11447.800000000001, "text": " like how bad or how good this network was. Now, if it tells us this network was really bad,", "tokens": [50364, 411, 577, 1578, 420, 577, 665, 341, 3209, 390, 13, 823, 11, 498, 309, 5112, 505, 341, 3209, 390, 534, 1578, 11, 50644], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2867, "seek": 1144220, "start": 11447.800000000001, "end": 11452.52, "text": " it gives us like a really high loss, then that tells us that we need to tweak the weights and", "tokens": [50644, 309, 2709, 505, 411, 257, 534, 1090, 4470, 11, 550, 300, 5112, 505, 300, 321, 643, 281, 29879, 264, 17443, 293, 50880], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2868, "seek": 1144220, "start": 11452.52, "end": 11458.04, "text": " biases more and move the network in a different direction. We're starting to get into gradient", "tokens": [50880, 32152, 544, 293, 1286, 264, 3209, 294, 257, 819, 3513, 13, 492, 434, 2891, 281, 483, 666, 16235, 51156], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2869, "seek": 1144220, "start": 11458.04, "end": 11462.12, "text": " descent. But let's understand the loss function first. So it's going to say, if it was really", "tokens": [51156, 23475, 13, 583, 718, 311, 1223, 264, 4470, 2445, 700, 13, 407, 309, 311, 516, 281, 584, 11, 498, 309, 390, 534, 51360], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2870, "seek": 1144220, "start": 11462.12, "end": 11465.800000000001, "text": " bad, let's move it more, let's change the weights more drastically, let's change the biases more", "tokens": [51360, 1578, 11, 718, 311, 1286, 309, 544, 11, 718, 311, 1319, 264, 17443, 544, 29673, 11, 718, 311, 1319, 264, 32152, 544, 51544], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2871, "seek": 1144220, "start": 11465.800000000001, "end": 11470.84, "text": " drastically. Whereas, if it was really good, it'll be like, Okay, so that one was actually decent,", "tokens": [51544, 29673, 13, 13813, 11, 498, 309, 390, 534, 665, 11, 309, 603, 312, 411, 11, 1033, 11, 370, 300, 472, 390, 767, 8681, 11, 51796], "temperature": 0.0, "avg_logprob": -0.06199120474862052, "compression_ratio": 2.0, "no_speech_prob": 0.008577109314501286}, {"id": 2872, "seek": 1147084, "start": 11470.84, "end": 11474.6, "text": " you know, you only need to tweak a little bit, and you only need to move this, this and this.", "tokens": [50364, 291, 458, 11, 291, 787, 643, 281, 29879, 257, 707, 857, 11, 293, 291, 787, 643, 281, 1286, 341, 11, 341, 293, 341, 13, 50552], "temperature": 0.0, "avg_logprob": -0.12722471664691792, "compression_ratio": 1.78, "no_speech_prob": 0.005059929098933935}, {"id": 2873, "seek": 1147084, "start": 11474.6, "end": 11478.68, "text": " So that's good. And that's the point of this loss function, it just calculates some value,", "tokens": [50552, 407, 300, 311, 665, 13, 400, 300, 311, 264, 935, 295, 341, 4470, 2445, 11, 309, 445, 4322, 1024, 512, 2158, 11, 50756], "temperature": 0.0, "avg_logprob": -0.12722471664691792, "compression_ratio": 1.78, "no_speech_prob": 0.005059929098933935}, {"id": 2874, "seek": 1147084, "start": 11478.68, "end": 11482.6, "text": " the higher the value, the worse our network was a few examples of loss function.", "tokens": [50756, 264, 2946, 264, 2158, 11, 264, 5324, 527, 3209, 390, 257, 1326, 5110, 295, 4470, 2445, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12722471664691792, "compression_ratio": 1.78, "no_speech_prob": 0.005059929098933935}, {"id": 2875, "seek": 1147084, "start": 11483.32, "end": 11490.52, "text": " Let's go down here, because I think I had a few optimizer loss here, mean squared error,", "tokens": [50988, 961, 311, 352, 760, 510, 11, 570, 286, 519, 286, 632, 257, 1326, 5028, 6545, 4470, 510, 11, 914, 8889, 6713, 11, 51348], "temperature": 0.0, "avg_logprob": -0.12722471664691792, "compression_ratio": 1.78, "no_speech_prob": 0.005059929098933935}, {"id": 2876, "seek": 1147084, "start": 11490.52, "end": 11496.2, "text": " mean absolute error and hinge loss. Now mean absolute error, you know, let's actually just", "tokens": [51348, 914, 8236, 6713, 293, 28822, 4470, 13, 823, 914, 8236, 6713, 11, 291, 458, 11, 718, 311, 767, 445, 51632], "temperature": 0.0, "avg_logprob": -0.12722471664691792, "compression_ratio": 1.78, "no_speech_prob": 0.005059929098933935}, {"id": 2877, "seek": 1149620, "start": 11496.28, "end": 11504.12, "text": " look one up here. So mean, absolute error, and have a look at what this is. So images,", "tokens": [50368, 574, 472, 493, 510, 13, 407, 914, 11, 8236, 6713, 11, 293, 362, 257, 574, 412, 437, 341, 307, 13, 407, 5267, 11, 50760], "temperature": 0.0, "avg_logprob": -0.09872113022149778, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.025176165625452995}, {"id": 2878, "seek": 1149620, "start": 11504.12, "end": 11511.560000000001, "text": " let's pick something. This is mean absolute error. This is the equation for mean absolute error.", "tokens": [50760, 718, 311, 1888, 746, 13, 639, 307, 914, 8236, 6713, 13, 639, 307, 264, 5367, 337, 914, 8236, 6713, 13, 51132], "temperature": 0.0, "avg_logprob": -0.09872113022149778, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.025176165625452995}, {"id": 2879, "seek": 1149620, "start": 11511.560000000001, "end": 11519.720000000001, "text": " Okay, so the summation of the absolute value of yi minus lambda of xi over n. Now, this is kind of", "tokens": [51132, 1033, 11, 370, 264, 28811, 295, 264, 8236, 2158, 295, 288, 72, 3175, 13607, 295, 36800, 670, 297, 13, 823, 11, 341, 307, 733, 295, 51540], "temperature": 0.0, "avg_logprob": -0.09872113022149778, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.025176165625452995}, {"id": 2880, "seek": 1149620, "start": 11519.720000000001, "end": 11523.400000000001, "text": " complicated. I'm not going to go into it too much. I was expecting I was hoping I was going to get", "tokens": [51540, 6179, 13, 286, 478, 406, 516, 281, 352, 666, 309, 886, 709, 13, 286, 390, 9650, 286, 390, 7159, 286, 390, 516, 281, 483, 51724], "temperature": 0.0, "avg_logprob": -0.09872113022149778, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.025176165625452995}, {"id": 2881, "seek": 1152340, "start": 11523.4, "end": 11533.96, "text": " like a better example for mean squared error. Okay, so these are the three loss functions here.", "tokens": [50364, 411, 257, 1101, 1365, 337, 914, 8889, 6713, 13, 1033, 11, 370, 613, 366, 264, 1045, 4470, 6828, 510, 13, 50892], "temperature": 0.0, "avg_logprob": -0.09817533977961136, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.008315023966133595}, {"id": 2882, "seek": 1152340, "start": 11533.96, "end": 11537.96, "text": " So mean squared error, mean absolute error, hinge loss, obviously, there's a ton more that we could", "tokens": [50892, 407, 914, 8889, 6713, 11, 914, 8236, 6713, 11, 28822, 4470, 11, 2745, 11, 456, 311, 257, 2952, 544, 300, 321, 727, 51092], "temperature": 0.0, "avg_logprob": -0.09817533977961136, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.008315023966133595}, {"id": 2883, "seek": 1152340, "start": 11537.96, "end": 11542.279999999999, "text": " use. I'm not going to talk about which how each of these work specifically, I mean, you can look", "tokens": [51092, 764, 13, 286, 478, 406, 516, 281, 751, 466, 597, 577, 1184, 295, 613, 589, 4682, 11, 286, 914, 11, 291, 393, 574, 51308], "temperature": 0.0, "avg_logprob": -0.09817533977961136, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.008315023966133595}, {"id": 2884, "seek": 1152340, "start": 11542.279999999999, "end": 11546.92, "text": " them up pretty easily. And also, so you know, these are also referenced as cost functions,", "tokens": [51308, 552, 493, 1238, 3612, 13, 400, 611, 11, 370, 291, 458, 11, 613, 366, 611, 32734, 382, 2063, 6828, 11, 51540], "temperature": 0.0, "avg_logprob": -0.09817533977961136, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.008315023966133595}, {"id": 2885, "seek": 1152340, "start": 11546.92, "end": 11552.68, "text": " so cost or loss, you might just hear these, these terms kind of interchanged cost and loss", "tokens": [51540, 370, 2063, 420, 4470, 11, 291, 1062, 445, 1568, 613, 11, 613, 2115, 733, 295, 728, 339, 10296, 2063, 293, 4470, 51828], "temperature": 0.0, "avg_logprob": -0.09817533977961136, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.008315023966133595}, {"id": 2886, "seek": 1155268, "start": 11552.68, "end": 11557.0, "text": " essentially mean the same thing, you want your network to cost the least, you want your network", "tokens": [50364, 4476, 914, 264, 912, 551, 11, 291, 528, 428, 3209, 281, 2063, 264, 1935, 11, 291, 528, 428, 3209, 50580], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2887, "seek": 1155268, "start": 11557.0, "end": 11561.720000000001, "text": " to have the least amount of loss. Okay, so now that we have talked about the loss function,", "tokens": [50580, 281, 362, 264, 1935, 2372, 295, 4470, 13, 1033, 11, 370, 586, 300, 321, 362, 2825, 466, 264, 4470, 2445, 11, 50816], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2888, "seek": 1155268, "start": 11562.36, "end": 11567.800000000001, "text": " we need to talk about how we actually update these weights and biases. Now, actually, let's", "tokens": [50848, 321, 643, 281, 751, 466, 577, 321, 767, 5623, 613, 17443, 293, 32152, 13, 823, 11, 767, 11, 718, 311, 51120], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2889, "seek": 1155268, "start": 11567.800000000001, "end": 11571.800000000001, "text": " go back to here, because I think I had some notes on it. This is what we call gradient descent.", "tokens": [51120, 352, 646, 281, 510, 11, 570, 286, 519, 286, 632, 512, 5570, 322, 309, 13, 639, 307, 437, 321, 818, 16235, 23475, 13, 51320], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2890, "seek": 1155268, "start": 11572.52, "end": 11576.84, "text": " So essentially, the parameters for our network are weights and biases. And by changing these", "tokens": [51356, 407, 4476, 11, 264, 9834, 337, 527, 3209, 366, 17443, 293, 32152, 13, 400, 538, 4473, 613, 51572], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2891, "seek": 1155268, "start": 11576.84, "end": 11581.64, "text": " weights and biases, we will, you know, either make the network better or make the network worse,", "tokens": [51572, 17443, 293, 32152, 11, 321, 486, 11, 291, 458, 11, 2139, 652, 264, 3209, 1101, 420, 652, 264, 3209, 5324, 11, 51812], "temperature": 0.0, "avg_logprob": -0.060957548353407116, "compression_ratio": 1.9618055555555556, "no_speech_prob": 0.0032728747464716434}, {"id": 2892, "seek": 1158164, "start": 11581.72, "end": 11585.88, "text": " the loss function will determine if the network is getting better, if it's getting worse, and then", "tokens": [50368, 264, 4470, 2445, 486, 6997, 498, 264, 3209, 307, 1242, 1101, 11, 498, 309, 311, 1242, 5324, 11, 293, 550, 50576], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2893, "seek": 1158164, "start": 11585.88, "end": 11591.24, "text": " we can determine how we're going to move the network to change that. So this is now gradient", "tokens": [50576, 321, 393, 6997, 577, 321, 434, 516, 281, 1286, 264, 3209, 281, 1319, 300, 13, 407, 341, 307, 586, 16235, 50844], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2894, "seek": 1158164, "start": 11591.24, "end": 11595.88, "text": " descent, where the math gets a little bit more complicated. So this is an example of what your", "tokens": [50844, 23475, 11, 689, 264, 5221, 2170, 257, 707, 857, 544, 6179, 13, 407, 341, 307, 364, 1365, 295, 437, 428, 51076], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2895, "seek": 1158164, "start": 11595.88, "end": 11602.359999999999, "text": " neural network function might look like. Now, as you have higher dimensional math, you have,", "tokens": [51076, 18161, 3209, 2445, 1062, 574, 411, 13, 823, 11, 382, 291, 362, 2946, 18795, 5221, 11, 291, 362, 11, 51400], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2896, "seek": 1158164, "start": 11602.359999999999, "end": 11606.599999999999, "text": " you know, a lot more dimensions, a lot more space to explore when it comes to creating", "tokens": [51400, 291, 458, 11, 257, 688, 544, 12819, 11, 257, 688, 544, 1901, 281, 6839, 562, 309, 1487, 281, 4084, 51612], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2897, "seek": 1158164, "start": 11606.599999999999, "end": 11611.16, "text": " different parameters and creating different biases and activation functions and all of that.", "tokens": [51612, 819, 9834, 293, 4084, 819, 32152, 293, 24433, 6828, 293, 439, 295, 300, 13, 51840], "temperature": 0.0, "avg_logprob": -0.054077946950518894, "compression_ratio": 1.901360544217687, "no_speech_prob": 0.0015486564952880144}, {"id": 2898, "seek": 1161116, "start": 11611.24, "end": 11615.24, "text": " So as we apply our activation functions, we're kind of spreading our network into higher", "tokens": [50368, 407, 382, 321, 3079, 527, 24433, 6828, 11, 321, 434, 733, 295, 15232, 527, 3209, 666, 2946, 50568], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2899, "seek": 1161116, "start": 11615.24, "end": 11619.24, "text": " dimensions, which just makes things much more complicated. Now, essentially, what we're trying", "tokens": [50568, 12819, 11, 597, 445, 1669, 721, 709, 544, 6179, 13, 823, 11, 4476, 11, 437, 321, 434, 1382, 50768], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2900, "seek": 1161116, "start": 11619.24, "end": 11624.28, "text": " to do with the neural network is optimize this loss function. This loss function is telling us", "tokens": [50768, 281, 360, 365, 264, 18161, 3209, 307, 19719, 341, 4470, 2445, 13, 639, 4470, 2445, 307, 3585, 505, 51020], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2901, "seek": 1161116, "start": 11624.28, "end": 11628.52, "text": " how good it is or how bad it is. So if we can get this loss function as low as possible,", "tokens": [51020, 577, 665, 309, 307, 420, 577, 1578, 309, 307, 13, 407, 498, 321, 393, 483, 341, 4470, 2445, 382, 2295, 382, 1944, 11, 51232], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2902, "seek": 1161116, "start": 11628.52, "end": 11632.92, "text": " then that means we should technically have the best neural network. So this is our kind of", "tokens": [51232, 550, 300, 1355, 321, 820, 12120, 362, 264, 1151, 18161, 3209, 13, 407, 341, 307, 527, 733, 295, 51452], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2903, "seek": 1161116, "start": 11632.92, "end": 11637.8, "text": " loss functions, like mapping or whatever, what we're looking for is something called a global", "tokens": [51452, 4470, 6828, 11, 411, 18350, 420, 2035, 11, 437, 321, 434, 1237, 337, 307, 746, 1219, 257, 4338, 51696], "temperature": 0.0, "avg_logprob": -0.05293568353804331, "compression_ratio": 1.8523489932885906, "no_speech_prob": 0.000755380664486438}, {"id": 2904, "seek": 1163780, "start": 11637.8, "end": 11643.56, "text": " minimum, we're looking for the minimum point where we get the least possible loss from our", "tokens": [50364, 7285, 11, 321, 434, 1237, 337, 264, 7285, 935, 689, 321, 483, 264, 1935, 1944, 4470, 490, 527, 50652], "temperature": 0.0, "avg_logprob": -0.06543851798435427, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.01854470930993557}, {"id": 2905, "seek": 1163780, "start": 11643.56, "end": 11647.88, "text": " neural network. So if we start where these red circles are, right, and I've just stole this", "tokens": [50652, 18161, 3209, 13, 407, 498, 321, 722, 689, 613, 2182, 13040, 366, 11, 558, 11, 293, 286, 600, 445, 16326, 341, 50868], "temperature": 0.0, "avg_logprob": -0.06543851798435427, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.01854470930993557}, {"id": 2906, "seek": 1163780, "start": 11647.88, "end": 11654.119999999999, "text": " image off Google images, what we're trying to do is move downwards into this global global", "tokens": [50868, 3256, 766, 3329, 5267, 11, 437, 321, 434, 1382, 281, 360, 307, 1286, 39880, 666, 341, 4338, 4338, 51180], "temperature": 0.0, "avg_logprob": -0.06543851798435427, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.01854470930993557}, {"id": 2907, "seek": 1163780, "start": 11654.119999999999, "end": 11658.92, "text": " minimum. And this is with a process of called gradient descent. So we calculate this loss,", "tokens": [51180, 7285, 13, 400, 341, 307, 365, 257, 1399, 295, 1219, 16235, 23475, 13, 407, 321, 8873, 341, 4470, 11, 51420], "temperature": 0.0, "avg_logprob": -0.06543851798435427, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.01854470930993557}, {"id": 2908, "seek": 1163780, "start": 11658.92, "end": 11664.439999999999, "text": " and we use an algorithm called gradient descent, which tells us what direction we need to move", "tokens": [51420, 293, 321, 764, 364, 9284, 1219, 16235, 23475, 11, 597, 5112, 505, 437, 3513, 321, 643, 281, 1286, 51696], "temperature": 0.0, "avg_logprob": -0.06543851798435427, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.01854470930993557}, {"id": 2909, "seek": 1166444, "start": 11664.52, "end": 11669.4, "text": " our function to determine or to get to this global minimum. So it essentially looks where", "tokens": [50368, 527, 2445, 281, 6997, 420, 281, 483, 281, 341, 4338, 7285, 13, 407, 309, 4476, 1542, 689, 50612], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2910, "seek": 1166444, "start": 11669.4, "end": 11673.4, "text": " we are. It says this was the loss. And it says, okay, I'm going to calculate what's called a", "tokens": [50612, 321, 366, 13, 467, 1619, 341, 390, 264, 4470, 13, 400, 309, 1619, 11, 1392, 11, 286, 478, 516, 281, 8873, 437, 311, 1219, 257, 50812], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2911, "seek": 1166444, "start": 11673.4, "end": 11678.04, "text": " gradient, which is literally just a steepness or a direction. And we're going to move in that", "tokens": [50812, 16235, 11, 597, 307, 3736, 445, 257, 16841, 1287, 420, 257, 3513, 13, 400, 321, 434, 516, 281, 1286, 294, 300, 51044], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2912, "seek": 1166444, "start": 11678.04, "end": 11683.0, "text": " direction. And then the algorithm called brought back propagation, we'll go backwards through", "tokens": [51044, 3513, 13, 400, 550, 264, 9284, 1219, 3038, 646, 38377, 11, 321, 603, 352, 12204, 807, 51292], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2913, "seek": 1166444, "start": 11683.0, "end": 11688.04, "text": " the network and update the weights and biases so that we move in that direction. Now, I think this", "tokens": [51292, 264, 3209, 293, 5623, 264, 17443, 293, 32152, 370, 300, 321, 1286, 294, 300, 3513, 13, 823, 11, 286, 519, 341, 51544], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2914, "seek": 1166444, "start": 11688.04, "end": 11692.04, "text": " is as far as I really want to go, because I know this is getting more complicated already,", "tokens": [51544, 307, 382, 1400, 382, 286, 534, 528, 281, 352, 11, 570, 286, 458, 341, 307, 1242, 544, 6179, 1217, 11, 51744], "temperature": 0.0, "avg_logprob": -0.08330323091193811, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.005910517647862434}, {"id": 2915, "seek": 1169204, "start": 11692.04, "end": 11696.76, "text": " then some of you guys probably can handle and that I can probably explain. But that's kind of", "tokens": [50364, 550, 512, 295, 291, 1074, 1391, 393, 4813, 293, 300, 286, 393, 1391, 2903, 13, 583, 300, 311, 733, 295, 50600], "temperature": 0.0, "avg_logprob": -0.09187173421404003, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.012430237606167793}, {"id": 2916, "seek": 1169204, "start": 11696.76, "end": 11700.76, "text": " the basic principle. We'll go back to the drawing board and we'll do a very quick recap before we", "tokens": [50600, 264, 3875, 8665, 13, 492, 603, 352, 646, 281, 264, 6316, 3150, 293, 321, 603, 360, 257, 588, 1702, 20928, 949, 321, 50800], "temperature": 0.0, "avg_logprob": -0.09187173421404003, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.012430237606167793}, {"id": 2917, "seek": 1169204, "start": 11700.76, "end": 11706.76, "text": " get into some of the other stuff, neural networks, input, output hidden layers connected with weights,", "tokens": [50800, 483, 666, 512, 295, 264, 661, 1507, 11, 18161, 9590, 11, 4846, 11, 5598, 7633, 7914, 4582, 365, 17443, 11, 51100], "temperature": 0.0, "avg_logprob": -0.09187173421404003, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.012430237606167793}, {"id": 2918, "seek": 1169204, "start": 11706.76, "end": 11711.720000000001, "text": " there's biases that connect to each layer. These biases can be thought of as y intercepts, they'll", "tokens": [51100, 456, 311, 32152, 300, 1745, 281, 1184, 4583, 13, 1981, 32152, 393, 312, 1194, 295, 382, 288, 24700, 82, 11, 436, 603, 51348], "temperature": 0.0, "avg_logprob": -0.09187173421404003, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.012430237606167793}, {"id": 2919, "seek": 1169204, "start": 11711.720000000001, "end": 11717.480000000001, "text": " simply move completely up or move completely down that entire, you know, activation function,", "tokens": [51348, 2935, 1286, 2584, 493, 420, 1286, 2584, 760, 300, 2302, 11, 291, 458, 11, 24433, 2445, 11, 51636], "temperature": 0.0, "avg_logprob": -0.09187173421404003, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.012430237606167793}, {"id": 2920, "seek": 1171748, "start": 11717.48, "end": 11721.96, "text": " right, we're shifting things left or right, because this will allow us to get a better", "tokens": [50364, 558, 11, 321, 434, 17573, 721, 1411, 420, 558, 11, 570, 341, 486, 2089, 505, 281, 483, 257, 1101, 50588], "temperature": 0.0, "avg_logprob": -0.07820164154623156, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.014062648639082909}, {"id": 2921, "seek": 1171748, "start": 11721.96, "end": 11726.6, "text": " prediction and have another parameter that we can train and add a little bit of complexity", "tokens": [50588, 17630, 293, 362, 1071, 13075, 300, 321, 393, 3847, 293, 909, 257, 707, 857, 295, 14024, 50820], "temperature": 0.0, "avg_logprob": -0.07820164154623156, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.014062648639082909}, {"id": 2922, "seek": 1171748, "start": 11726.6, "end": 11732.119999999999, "text": " to our neural network model. Now, the way that information is passed through these layers is", "tokens": [50820, 281, 527, 18161, 3209, 2316, 13, 823, 11, 264, 636, 300, 1589, 307, 4678, 807, 613, 7914, 307, 51096], "temperature": 0.0, "avg_logprob": -0.07820164154623156, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.014062648639082909}, {"id": 2923, "seek": 1171748, "start": 11732.119999999999, "end": 11737.8, "text": " we take the weighted sum at a neuron of all of the connected neurons to it, we then add this", "tokens": [51096, 321, 747, 264, 32807, 2408, 412, 257, 34090, 295, 439, 295, 264, 4582, 22027, 281, 309, 11, 321, 550, 909, 341, 51380], "temperature": 0.0, "avg_logprob": -0.07820164154623156, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.014062648639082909}, {"id": 2924, "seek": 1171748, "start": 11737.8, "end": 11743.72, "text": " bias neuron, and we apply some activation function that's going to put this, you know, these values", "tokens": [51380, 12577, 34090, 11, 293, 321, 3079, 512, 24433, 2445, 300, 311, 516, 281, 829, 341, 11, 291, 458, 11, 613, 4190, 51676], "temperature": 0.0, "avg_logprob": -0.07820164154623156, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.014062648639082909}, {"id": 2925, "seek": 1174372, "start": 11743.72, "end": 11748.599999999999, "text": " in between two set values. So for example, when we talk about sigmoid, that's going to squish our", "tokens": [50364, 294, 1296, 732, 992, 4190, 13, 407, 337, 1365, 11, 562, 321, 751, 466, 4556, 3280, 327, 11, 300, 311, 516, 281, 31379, 527, 50608], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2926, "seek": 1174372, "start": 11748.599999999999, "end": 11752.599999999999, "text": " values between zero and one, when we talk about hyperbolic tangent, that's going to squish our", "tokens": [50608, 4190, 1296, 4018, 293, 472, 11, 562, 321, 751, 466, 9848, 65, 7940, 27747, 11, 300, 311, 516, 281, 31379, 527, 50808], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2927, "seek": 1174372, "start": 11752.599999999999, "end": 11756.76, "text": " values between negative one and one. And when we talk about rectifier linear unit, that's going to", "tokens": [50808, 4190, 1296, 3671, 472, 293, 472, 13, 400, 562, 321, 751, 466, 11048, 9902, 8213, 4985, 11, 300, 311, 516, 281, 51016], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2928, "seek": 1174372, "start": 11756.76, "end": 11761.32, "text": " squish our values between zero and positive infinity. So we apply those activation functions,", "tokens": [51016, 31379, 527, 4190, 1296, 4018, 293, 3353, 13202, 13, 407, 321, 3079, 729, 24433, 6828, 11, 51244], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2929, "seek": 1174372, "start": 11761.32, "end": 11766.039999999999, "text": " and then we continue the process. So n one gets its value and two gets its value. And then finally,", "tokens": [51244, 293, 550, 321, 2354, 264, 1399, 13, 407, 297, 472, 2170, 1080, 2158, 293, 732, 2170, 1080, 2158, 13, 400, 550, 2721, 11, 51480], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2930, "seek": 1174372, "start": 11766.039999999999, "end": 11769.48, "text": " we make our way to our output layer, we might have passed through some other hidden layers", "tokens": [51480, 321, 652, 527, 636, 281, 527, 5598, 4583, 11, 321, 1062, 362, 4678, 807, 512, 661, 7633, 7914, 51652], "temperature": 0.0, "avg_logprob": -0.056167254482742644, "compression_ratio": 2.0945454545454547, "no_speech_prob": 0.010651899501681328}, {"id": 2931, "seek": 1176948, "start": 11769.48, "end": 11774.199999999999, "text": " before that. And then we do the same thing, we take the weighted sum, we add the bias,", "tokens": [50364, 949, 300, 13, 400, 550, 321, 360, 264, 912, 551, 11, 321, 747, 264, 32807, 2408, 11, 321, 909, 264, 12577, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08998597052789503, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.010986325331032276}, {"id": 2932, "seek": 1176948, "start": 11774.199999999999, "end": 11779.88, "text": " we apply an activation function, we look at the output, and we determine whether we know we are", "tokens": [50600, 321, 3079, 364, 24433, 2445, 11, 321, 574, 412, 264, 5598, 11, 293, 321, 6997, 1968, 321, 458, 321, 366, 50884], "temperature": 0.0, "avg_logprob": -0.08998597052789503, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.010986325331032276}, {"id": 2933, "seek": 1176948, "start": 11779.88, "end": 11784.439999999999, "text": " a class y or we are class z or whether this is the value we're looking for. And and that's how", "tokens": [50884, 257, 1508, 288, 420, 321, 366, 1508, 710, 420, 1968, 341, 307, 264, 2158, 321, 434, 1237, 337, 13, 400, 293, 300, 311, 577, 51112], "temperature": 0.0, "avg_logprob": -0.08998597052789503, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.010986325331032276}, {"id": 2934, "seek": 1176948, "start": 11784.439999999999, "end": 11789.88, "text": " it works. Now we're at the training process, right? So we're doing this now, that's kind of how this", "tokens": [51112, 309, 1985, 13, 823, 321, 434, 412, 264, 3097, 1399, 11, 558, 30, 407, 321, 434, 884, 341, 586, 11, 300, 311, 733, 295, 577, 341, 51384], "temperature": 0.0, "avg_logprob": -0.08998597052789503, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.010986325331032276}, {"id": 2935, "seek": 1176948, "start": 11789.88, "end": 11794.279999999999, "text": " worked when we were making a prediction. So when we're training, essentially, what happens is we", "tokens": [51384, 2732, 562, 321, 645, 1455, 257, 17630, 13, 407, 562, 321, 434, 3097, 11, 4476, 11, 437, 2314, 307, 321, 51604], "temperature": 0.0, "avg_logprob": -0.08998597052789503, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.010986325331032276}, {"id": 2936, "seek": 1179428, "start": 11794.28, "end": 11801.0, "text": " just make predictions, we compare those predictions to whatever these expected value should be using", "tokens": [50364, 445, 652, 21264, 11, 321, 6794, 729, 21264, 281, 2035, 613, 5176, 2158, 820, 312, 1228, 50700], "temperature": 0.0, "avg_logprob": -0.06317992437453497, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.035142168402671814}, {"id": 2937, "seek": 1179428, "start": 11801.0, "end": 11806.84, "text": " this loss function. Then we calculate what's called a gradient, a gradient is the direction we need", "tokens": [50700, 341, 4470, 2445, 13, 1396, 321, 8873, 437, 311, 1219, 257, 16235, 11, 257, 16235, 307, 264, 3513, 321, 643, 50992], "temperature": 0.0, "avg_logprob": -0.06317992437453497, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.035142168402671814}, {"id": 2938, "seek": 1179428, "start": 11806.84, "end": 11811.560000000001, "text": " to move to minimize this loss function. And this is where the advanced math happens and why I'm", "tokens": [50992, 281, 1286, 281, 17522, 341, 4470, 2445, 13, 400, 341, 307, 689, 264, 7339, 5221, 2314, 293, 983, 286, 478, 51228], "temperature": 0.0, "avg_logprob": -0.06317992437453497, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.035142168402671814}, {"id": 2939, "seek": 1179428, "start": 11811.560000000001, "end": 11816.76, "text": " kind of skimming over this aspect. And then we use an algorithm called back propagation, where we", "tokens": [51228, 733, 295, 1110, 40471, 670, 341, 4171, 13, 400, 550, 321, 764, 364, 9284, 1219, 646, 38377, 11, 689, 321, 51488], "temperature": 0.0, "avg_logprob": -0.06317992437453497, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.035142168402671814}, {"id": 2940, "seek": 1179428, "start": 11816.76, "end": 11821.720000000001, "text": " step backwards through the network, and update the weights and biases, according to the gradient", "tokens": [51488, 1823, 12204, 807, 264, 3209, 11, 293, 5623, 264, 17443, 293, 32152, 11, 4650, 281, 264, 16235, 51736], "temperature": 0.0, "avg_logprob": -0.06317992437453497, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.035142168402671814}, {"id": 2941, "seek": 1182172, "start": 11821.8, "end": 11827.64, "text": " that we calculated. Now that is pretty much how this works. So you know, the more info we have,", "tokens": [50368, 300, 321, 15598, 13, 823, 300, 307, 1238, 709, 577, 341, 1985, 13, 407, 291, 458, 11, 264, 544, 13614, 321, 362, 11, 50660], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2942, "seek": 1182172, "start": 11828.439999999999, "end": 11832.599999999999, "text": " likely unless we're overfitting, but you know, if we have a lot of data, if we can keep feeding", "tokens": [50700, 3700, 5969, 321, 434, 670, 69, 2414, 11, 457, 291, 458, 11, 498, 321, 362, 257, 688, 295, 1412, 11, 498, 321, 393, 1066, 12919, 50908], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2943, "seek": 1182172, "start": 11832.599999999999, "end": 11837.24, "text": " the network, it starts off being really horrible, having no idea what's going on. And then as more", "tokens": [50908, 264, 3209, 11, 309, 3719, 766, 885, 534, 9263, 11, 1419, 572, 1558, 437, 311, 516, 322, 13, 400, 550, 382, 544, 51140], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2944, "seek": 1182172, "start": 11837.24, "end": 11841.96, "text": " and more information comes in, it updates these weights and biases gets better and better sees", "tokens": [51140, 293, 544, 1589, 1487, 294, 11, 309, 9205, 613, 17443, 293, 32152, 2170, 1101, 293, 1101, 8194, 51376], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2945, "seek": 1182172, "start": 11841.96, "end": 11846.439999999999, "text": " more examples. And after you know, a certain amount of epochs or certain amount of pieces of", "tokens": [51376, 544, 5110, 13, 400, 934, 291, 458, 11, 257, 1629, 2372, 295, 30992, 28346, 420, 1629, 2372, 295, 3755, 295, 51600], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2946, "seek": 1182172, "start": 11846.439999999999, "end": 11850.76, "text": " information, our network is making better and better predictions and having a lower and lower", "tokens": [51600, 1589, 11, 527, 3209, 307, 1455, 1101, 293, 1101, 21264, 293, 1419, 257, 3126, 293, 3126, 51816], "temperature": 0.0, "avg_logprob": -0.06313827264047887, "compression_ratio": 1.8754098360655738, "no_speech_prob": 0.008315136656165123}, {"id": 2947, "seek": 1185076, "start": 11850.76, "end": 11855.32, "text": " loss. And the way we will calculate how well our network is doing is by passing it, you know,", "tokens": [50364, 4470, 13, 400, 264, 636, 321, 486, 8873, 577, 731, 527, 3209, 307, 884, 307, 538, 8437, 309, 11, 291, 458, 11, 50592], "temperature": 0.0, "avg_logprob": -0.06938849150679494, "compression_ratio": 1.783882783882784, "no_speech_prob": 0.005554556380957365}, {"id": 2948, "seek": 1185076, "start": 11855.32, "end": 11862.68, "text": " our validation data set, where it can say, okay, so we got an 85% accuracy on this data set, we're", "tokens": [50592, 527, 24071, 1412, 992, 11, 689, 309, 393, 584, 11, 1392, 11, 370, 321, 658, 364, 14695, 4, 14170, 322, 341, 1412, 992, 11, 321, 434, 50960], "temperature": 0.0, "avg_logprob": -0.06938849150679494, "compression_ratio": 1.783882783882784, "no_speech_prob": 0.005554556380957365}, {"id": 2949, "seek": 1185076, "start": 11862.68, "end": 11867.24, "text": " doing okay, you know, let's tweak this, let's tweak that, let's do this. So the loss function,", "tokens": [50960, 884, 1392, 11, 291, 458, 11, 718, 311, 29879, 341, 11, 718, 311, 29879, 300, 11, 718, 311, 360, 341, 13, 407, 264, 4470, 2445, 11, 51188], "temperature": 0.0, "avg_logprob": -0.06938849150679494, "compression_ratio": 1.783882783882784, "no_speech_prob": 0.005554556380957365}, {"id": 2950, "seek": 1185076, "start": 11867.24, "end": 11872.68, "text": " the lower this is the better, also known as the cost function. And that is kind of neural networks", "tokens": [51188, 264, 3126, 341, 307, 264, 1101, 11, 611, 2570, 382, 264, 2063, 2445, 13, 400, 300, 307, 733, 295, 18161, 9590, 51460], "temperature": 0.0, "avg_logprob": -0.06938849150679494, "compression_ratio": 1.783882783882784, "no_speech_prob": 0.005554556380957365}, {"id": 2951, "seek": 1185076, "start": 11872.68, "end": 11877.48, "text": " in a nutshell. Now I know this wasn't really in a nutshell, because it was 30 minutes long. But that", "tokens": [51460, 294, 257, 37711, 13, 823, 286, 458, 341, 2067, 380, 534, 294, 257, 37711, 11, 570, 309, 390, 2217, 2077, 938, 13, 583, 300, 51700], "temperature": 0.0, "avg_logprob": -0.06938849150679494, "compression_ratio": 1.783882783882784, "no_speech_prob": 0.005554556380957365}, {"id": 2952, "seek": 1187748, "start": 11877.48, "end": 11881.72, "text": " is, you know, as much of an explanation as I can really give you without going too far into the", "tokens": [50364, 307, 11, 291, 458, 11, 382, 709, 295, 364, 10835, 382, 286, 393, 534, 976, 291, 1553, 516, 886, 1400, 666, 264, 50576], "temperature": 0.0, "avg_logprob": -0.06104128761629088, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.01798507198691368}, {"id": 2953, "seek": 1187748, "start": 11881.72, "end": 11886.039999999999, "text": " mathematics behind everything. And again, remember, the activation function is to move us up in", "tokens": [50576, 18666, 2261, 1203, 13, 400, 797, 11, 1604, 11, 264, 24433, 2445, 307, 281, 1286, 505, 493, 294, 50792], "temperature": 0.0, "avg_logprob": -0.06104128761629088, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.01798507198691368}, {"id": 2954, "seek": 1187748, "start": 11886.039999999999, "end": 11890.76, "text": " dimensionality. The bias is another layer of complexity and a trainable parameter for our", "tokens": [50792, 10139, 1860, 13, 440, 12577, 307, 1071, 4583, 295, 14024, 293, 257, 3847, 712, 13075, 337, 527, 51028], "temperature": 0.0, "avg_logprob": -0.06104128761629088, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.01798507198691368}, {"id": 2955, "seek": 1187748, "start": 11890.76, "end": 11897.0, "text": " network allows us to shift this kind of activation function left, right up, down. And yeah, that", "tokens": [51028, 3209, 4045, 505, 281, 5513, 341, 733, 295, 24433, 2445, 1411, 11, 558, 493, 11, 760, 13, 400, 1338, 11, 300, 51340], "temperature": 0.0, "avg_logprob": -0.06104128761629088, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.01798507198691368}, {"id": 2956, "seek": 1187748, "start": 11897.0, "end": 11903.4, "text": " is how that works. Okay, so now we have an optimizer. This is kind of the last thing on", "tokens": [51340, 307, 577, 300, 1985, 13, 1033, 11, 370, 586, 321, 362, 364, 5028, 6545, 13, 639, 307, 733, 295, 264, 1036, 551, 322, 51660], "temperature": 0.0, "avg_logprob": -0.06104128761629088, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.01798507198691368}, {"id": 2957, "seek": 1190340, "start": 11903.48, "end": 11907.88, "text": " how neural networks work. optimizer is literally just the algorithm that does the gradient descent", "tokens": [50368, 577, 18161, 9590, 589, 13, 5028, 6545, 307, 3736, 445, 264, 9284, 300, 775, 264, 16235, 23475, 50588], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2958, "seek": 1190340, "start": 11907.88, "end": 11912.52, "text": " and back propagation for us. So I mean, you guys can read through some of them here, we'll be using", "tokens": [50588, 293, 646, 38377, 337, 505, 13, 407, 286, 914, 11, 291, 1074, 393, 1401, 807, 512, 295, 552, 510, 11, 321, 603, 312, 1228, 50820], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2959, "seek": 1190340, "start": 11913.32, "end": 11917.56, "text": " probably the atom optimizer for most of our examples, although there's, you know, lots of", "tokens": [50860, 1391, 264, 12018, 5028, 6545, 337, 881, 295, 527, 5110, 11, 4878, 456, 311, 11, 291, 458, 11, 3195, 295, 51072], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2960, "seek": 1190340, "start": 11917.56, "end": 11922.199999999999, "text": " different ones that we can pick from. Now this optimization technique, again, is just a different", "tokens": [51072, 819, 2306, 300, 321, 393, 1888, 490, 13, 823, 341, 19618, 6532, 11, 797, 11, 307, 445, 257, 819, 51304], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2961, "seek": 1190340, "start": 11922.199999999999, "end": 11925.56, "text": " algorithm. There's some of them are faster, some of them are slower, some of them work a little bit", "tokens": [51304, 9284, 13, 821, 311, 512, 295, 552, 366, 4663, 11, 512, 295, 552, 366, 14009, 11, 512, 295, 552, 589, 257, 707, 857, 51472], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2962, "seek": 1190340, "start": 11925.56, "end": 11930.119999999999, "text": " differently. And we're not really going to get into picking optimizers in this course, because", "tokens": [51472, 7614, 13, 400, 321, 434, 406, 534, 516, 281, 483, 666, 8867, 5028, 22525, 294, 341, 1164, 11, 570, 51700], "temperature": 0.0, "avg_logprob": -0.0752191753948436, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.06753495335578918}, {"id": 2963, "seek": 1193012, "start": 11930.2, "end": 11934.6, "text": " that's more of an advanced machine learning technique. All right, so enough explaining,", "tokens": [50368, 300, 311, 544, 295, 364, 7339, 3479, 2539, 6532, 13, 1057, 558, 11, 370, 1547, 13468, 11, 50588], "temperature": 0.0, "avg_logprob": -0.14183539264606979, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.01363550778478384}, {"id": 2964, "seek": 1193012, "start": 11934.6, "end": 11941.480000000001, "text": " enough math, enough drawings, enough talking. Now it is time to create our first official", "tokens": [50588, 1547, 5221, 11, 1547, 18618, 11, 1547, 1417, 13, 823, 309, 307, 565, 281, 1884, 527, 700, 4783, 50932], "temperature": 0.0, "avg_logprob": -0.14183539264606979, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.01363550778478384}, {"id": 2965, "seek": 1193012, "start": 11941.480000000001, "end": 11946.2, "text": " neural network. Now these are the imports we're going to need. So import TensorFlow is TF from", "tokens": [50932, 18161, 3209, 13, 823, 613, 366, 264, 41596, 321, 434, 516, 281, 643, 13, 407, 974, 37624, 307, 40964, 490, 51168], "temperature": 0.0, "avg_logprob": -0.14183539264606979, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.01363550778478384}, {"id": 2966, "seek": 1193012, "start": 11946.2, "end": 11950.2, "text": " TensorFlow import Keras again, so this does actually come with TensorFlow. I forget if I", "tokens": [51168, 37624, 974, 591, 6985, 797, 11, 370, 341, 775, 767, 808, 365, 37624, 13, 286, 2870, 498, 286, 51368], "temperature": 0.0, "avg_logprob": -0.14183539264606979, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.01363550778478384}, {"id": 2967, "seek": 1193012, "start": 11950.2, "end": 11955.800000000001, "text": " said you need to install that before. My apologies and then import numpy as NP, import map plot", "tokens": [51368, 848, 291, 643, 281, 3625, 300, 949, 13, 1222, 34929, 293, 550, 974, 1031, 8200, 382, 38611, 11, 974, 4471, 7542, 51648], "temperature": 0.0, "avg_logprob": -0.14183539264606979, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.01363550778478384}, {"id": 2968, "seek": 1195580, "start": 11955.88, "end": 11961.0, "text": " live dot pi plot as PLT. Alright, so I'm going to do actually similar thing to what I did before", "tokens": [50368, 1621, 5893, 3895, 7542, 382, 6999, 51, 13, 2798, 11, 370, 286, 478, 516, 281, 360, 767, 2531, 551, 281, 437, 286, 630, 949, 50624], "temperature": 0.0, "avg_logprob": -0.11359910170237224, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.024418499320745468}, {"id": 2969, "seek": 1195580, "start": 11961.0, "end": 11965.16, "text": " where I'm kind of just going to copy some of this code into another notebook, just to make sure", "tokens": [50624, 689, 286, 478, 733, 295, 445, 516, 281, 5055, 512, 295, 341, 3089, 666, 1071, 21060, 11, 445, 281, 652, 988, 50832], "temperature": 0.0, "avg_logprob": -0.11359910170237224, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.024418499320745468}, {"id": 2970, "seek": 1195580, "start": 11965.16, "end": 11969.88, "text": " that we can look at everything at the end, and then kind of step through the code step by step", "tokens": [50832, 300, 321, 393, 574, 412, 1203, 412, 264, 917, 11, 293, 550, 733, 295, 1823, 807, 264, 3089, 1823, 538, 1823, 51068], "temperature": 0.0, "avg_logprob": -0.11359910170237224, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.024418499320745468}, {"id": 2971, "seek": 1195580, "start": 11969.88, "end": 11975.96, "text": " rather than all of the text kind of happening here. Alright, so the data set, and the problem", "tokens": [51068, 2831, 813, 439, 295, 264, 2487, 733, 295, 2737, 510, 13, 2798, 11, 370, 264, 1412, 992, 11, 293, 264, 1154, 51372], "temperature": 0.0, "avg_logprob": -0.11359910170237224, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.024418499320745468}, {"id": 2972, "seek": 1195580, "start": 11975.96, "end": 11980.92, "text": " we are going to consider for our first neural network is the fashion MNIST data set. Now the", "tokens": [51372, 321, 366, 516, 281, 1949, 337, 527, 700, 18161, 3209, 307, 264, 6700, 376, 45, 19756, 1412, 992, 13, 823, 264, 51620], "temperature": 0.0, "avg_logprob": -0.11359910170237224, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.024418499320745468}, {"id": 2973, "seek": 1198092, "start": 11981.0, "end": 11986.68, "text": " fashion MNIST data set contains 60,000 images for training and 10,000 images for validating and", "tokens": [50368, 6700, 376, 45, 19756, 1412, 992, 8306, 4060, 11, 1360, 5267, 337, 3097, 293, 1266, 11, 1360, 5267, 337, 7363, 990, 293, 50652], "temperature": 0.0, "avg_logprob": -0.08356399337450664, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.023688239976763725}, {"id": 2974, "seek": 1198092, "start": 11986.68, "end": 11994.6, "text": " testing 70,000 images. And it is essentially pixel data of clothing articles. So what we're", "tokens": [50652, 4997, 5285, 11, 1360, 5267, 13, 400, 309, 307, 4476, 19261, 1412, 295, 11502, 11290, 13, 407, 437, 321, 434, 51048], "temperature": 0.0, "avg_logprob": -0.08356399337450664, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.023688239976763725}, {"id": 2975, "seek": 1198092, "start": 11994.6, "end": 11999.88, "text": " going to do to load in this data set from Keras, this actually built into Keras, it's meant as", "tokens": [51048, 516, 281, 360, 281, 3677, 294, 341, 1412, 992, 490, 591, 6985, 11, 341, 767, 3094, 666, 591, 6985, 11, 309, 311, 4140, 382, 51312], "temperature": 0.0, "avg_logprob": -0.08356399337450664, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.023688239976763725}, {"id": 2976, "seek": 1198092, "start": 11999.88, "end": 12006.28, "text": " like a beginner, like testing training data set, we're going to say fashion underscore MNIST", "tokens": [51312, 411, 257, 22080, 11, 411, 4997, 3097, 1412, 992, 11, 321, 434, 516, 281, 584, 6700, 37556, 376, 45, 19756, 51632], "temperature": 0.0, "avg_logprob": -0.08356399337450664, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.023688239976763725}, {"id": 2977, "seek": 1200628, "start": 12006.28, "end": 12012.92, "text": " equals Keras dot data sets dot fashion MNIST. Now this will get the data set object, and then we", "tokens": [50364, 6915, 591, 6985, 5893, 1412, 6352, 5893, 6700, 376, 45, 19756, 13, 823, 341, 486, 483, 264, 1412, 992, 2657, 11, 293, 550, 321, 50696], "temperature": 0.0, "avg_logprob": -0.08411769104003906, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.013635337352752686}, {"id": 2978, "seek": 1200628, "start": 12012.92, "end": 12018.92, "text": " can load that object by doing fashion MNIST dot load data. Now by doing this by having the tuples", "tokens": [50696, 393, 3677, 300, 2657, 538, 884, 6700, 376, 45, 19756, 5893, 3677, 1412, 13, 823, 538, 884, 341, 538, 1419, 264, 2604, 2622, 50996], "temperature": 0.0, "avg_logprob": -0.08411769104003906, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.013635337352752686}, {"id": 2979, "seek": 1200628, "start": 12018.92, "end": 12025.480000000001, "text": " train images train labels, test images test labels equals this, this will automatically split our", "tokens": [50996, 3847, 5267, 3847, 16949, 11, 1500, 5267, 1500, 16949, 6915, 341, 11, 341, 486, 6772, 7472, 527, 51324], "temperature": 0.0, "avg_logprob": -0.08411769104003906, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.013635337352752686}, {"id": 2980, "seek": 1200628, "start": 12025.480000000001, "end": 12030.76, "text": " data into the sets that we need. So we need the training, and we need the testing. And again,", "tokens": [51324, 1412, 666, 264, 6352, 300, 321, 643, 13, 407, 321, 643, 264, 3097, 11, 293, 321, 643, 264, 4997, 13, 400, 797, 11, 51588], "temperature": 0.0, "avg_logprob": -0.08411769104003906, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.013635337352752686}, {"id": 2981, "seek": 1200628, "start": 12030.76, "end": 12035.16, "text": " we've talked about all of that. So I'm going to kind of skim through that. And now we have it in", "tokens": [51588, 321, 600, 2825, 466, 439, 295, 300, 13, 407, 286, 478, 516, 281, 733, 295, 1110, 332, 807, 300, 13, 400, 586, 321, 362, 309, 294, 51808], "temperature": 0.0, "avg_logprob": -0.08411769104003906, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.013635337352752686}, {"id": 2982, "seek": 1203516, "start": 12035.16, "end": 12039.8, "text": " all of these kind of tuples here. Alright, so let's have a look at this data set to see what", "tokens": [50364, 439, 295, 613, 733, 295, 2604, 2622, 510, 13, 2798, 11, 370, 718, 311, 362, 257, 574, 412, 341, 1412, 992, 281, 536, 437, 50596], "temperature": 0.0, "avg_logprob": -0.07761760090672692, "compression_ratio": 1.796875, "no_speech_prob": 0.008846024051308632}, {"id": 2983, "seek": 1203516, "start": 12039.8, "end": 12042.92, "text": " we're working with. Okay, so let's run some of this code, let's get this import going,", "tokens": [50596, 321, 434, 1364, 365, 13, 1033, 11, 370, 718, 311, 1190, 512, 295, 341, 3089, 11, 718, 311, 483, 341, 974, 516, 11, 50752], "temperature": 0.0, "avg_logprob": -0.07761760090672692, "compression_ratio": 1.796875, "no_speech_prob": 0.008846024051308632}, {"id": 2984, "seek": 1203516, "start": 12043.64, "end": 12049.88, "text": " if it doesn't take forever. Okay, let's get the data sets. Yeah, this will take a second to", "tokens": [50788, 498, 309, 1177, 380, 747, 5680, 13, 1033, 11, 718, 311, 483, 264, 1412, 6352, 13, 865, 11, 341, 486, 747, 257, 1150, 281, 51100], "temperature": 0.0, "avg_logprob": -0.07761760090672692, "compression_ratio": 1.796875, "no_speech_prob": 0.008846024051308632}, {"id": 2985, "seek": 1203516, "start": 12049.88, "end": 12054.36, "text": " download for you guys, if you don't already have it cached. And then we'll go train images dot", "tokens": [51100, 5484, 337, 291, 1074, 11, 498, 291, 500, 380, 1217, 362, 309, 269, 15095, 13, 400, 550, 321, 603, 352, 3847, 5267, 5893, 51324], "temperature": 0.0, "avg_logprob": -0.07761760090672692, "compression_ratio": 1.796875, "no_speech_prob": 0.008846024051308632}, {"id": 2986, "seek": 1203516, "start": 12054.36, "end": 12059.88, "text": " shape. And let's look at what one of the images looks like. Or sorry, what our data set looks", "tokens": [51324, 3909, 13, 400, 718, 311, 574, 412, 437, 472, 295, 264, 5267, 1542, 411, 13, 1610, 2597, 11, 437, 527, 1412, 992, 1542, 51600], "temperature": 0.0, "avg_logprob": -0.07761760090672692, "compression_ratio": 1.796875, "no_speech_prob": 0.008846024051308632}, {"id": 2987, "seek": 1205988, "start": 12059.96, "end": 12066.199999999999, "text": " like. So we have 60,000 images that are 28 by 28. Now what that means is we have 28 pixels or 28", "tokens": [50368, 411, 13, 407, 321, 362, 4060, 11, 1360, 5267, 300, 366, 7562, 538, 7562, 13, 823, 437, 300, 1355, 307, 321, 362, 7562, 18668, 420, 7562, 50680], "temperature": 0.0, "avg_logprob": -0.07418114702466508, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.03209485858678818}, {"id": 2988, "seek": 1205988, "start": 12066.199999999999, "end": 12071.08, "text": " rows of 28 pixels, right? So that's kind of what our, you know, information is. So we're going to", "tokens": [50680, 13241, 295, 7562, 18668, 11, 558, 30, 407, 300, 311, 733, 295, 437, 527, 11, 291, 458, 11, 1589, 307, 13, 407, 321, 434, 516, 281, 50924], "temperature": 0.0, "avg_logprob": -0.07418114702466508, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.03209485858678818}, {"id": 2989, "seek": 1205988, "start": 12071.08, "end": 12079.24, "text": " have in total 784 pixels, which I've denoted here. So let's have a look at one pixel. So to reference", "tokens": [50924, 362, 294, 3217, 1614, 25494, 18668, 11, 597, 286, 600, 1441, 23325, 510, 13, 407, 718, 311, 362, 257, 574, 412, 472, 19261, 13, 407, 281, 6408, 51332], "temperature": 0.0, "avg_logprob": -0.07418114702466508, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.03209485858678818}, {"id": 2990, "seek": 1205988, "start": 12079.24, "end": 12084.679999999998, "text": " one pixel, this is what I what I'm doing, this comes in as a, actually, I'm not sure what type of", "tokens": [51332, 472, 19261, 11, 341, 307, 437, 286, 437, 286, 478, 884, 11, 341, 1487, 294, 382, 257, 11, 767, 11, 286, 478, 406, 988, 437, 2010, 295, 51604], "temperature": 0.0, "avg_logprob": -0.07418114702466508, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.03209485858678818}, {"id": 2991, "seek": 1205988, "start": 12084.679999999998, "end": 12089.4, "text": " data frame this is. But let's have a look at it. So let's say type of train underscore images,", "tokens": [51604, 1412, 3920, 341, 307, 13, 583, 718, 311, 362, 257, 574, 412, 309, 13, 407, 718, 311, 584, 2010, 295, 3847, 37556, 5267, 11, 51840], "temperature": 0.0, "avg_logprob": -0.07418114702466508, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.03209485858678818}, {"id": 2992, "seek": 1208940, "start": 12089.4, "end": 12095.72, "text": " because I want to see that. So that's an numpy array. So to reference the different indexes in", "tokens": [50364, 570, 286, 528, 281, 536, 300, 13, 407, 300, 311, 364, 1031, 8200, 10225, 13, 407, 281, 6408, 264, 819, 8186, 279, 294, 50680], "temperature": 0.0, "avg_logprob": -0.10327233494939031, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.007120180409401655}, {"id": 2993, "seek": 1208940, "start": 12095.72, "end": 12100.76, "text": " this is similar to pandas, we're just going to do zero comma 23 comma 23, which stands for, you", "tokens": [50680, 341, 307, 2531, 281, 4565, 296, 11, 321, 434, 445, 516, 281, 360, 4018, 22117, 6673, 22117, 6673, 11, 597, 7382, 337, 11, 291, 50932], "temperature": 0.0, "avg_logprob": -0.10327233494939031, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.007120180409401655}, {"id": 2994, "seek": 1208940, "start": 12100.76, "end": 12108.359999999999, "text": " know, image zero 23, and then 23. And this gives us one pixel. So row 23 column 23, which will be", "tokens": [50932, 458, 11, 3256, 4018, 6673, 11, 293, 550, 6673, 13, 400, 341, 2709, 505, 472, 19261, 13, 407, 5386, 6673, 7738, 6673, 11, 597, 486, 312, 51312], "temperature": 0.0, "avg_logprob": -0.10327233494939031, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.007120180409401655}, {"id": 2995, "seek": 1208940, "start": 12108.359999999999, "end": 12115.48, "text": " that. Okay, so let's run this. And let's see this value is 194. Okay, so that's kind of interesting.", "tokens": [51312, 300, 13, 1033, 11, 370, 718, 311, 1190, 341, 13, 400, 718, 311, 536, 341, 2158, 307, 9754, 13, 1033, 11, 370, 300, 311, 733, 295, 1880, 13, 51668], "temperature": 0.0, "avg_logprob": -0.10327233494939031, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.007120180409401655}, {"id": 2996, "seek": 1211548, "start": 12115.48, "end": 12119.8, "text": " That's what one pixel looks like. So let's look at what multiple pixels look like. So we'll print", "tokens": [50364, 663, 311, 437, 472, 19261, 1542, 411, 13, 407, 718, 311, 574, 412, 437, 3866, 18668, 574, 411, 13, 407, 321, 603, 4482, 50580], "temperature": 0.0, "avg_logprob": -0.09323421988900252, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.05183565244078636}, {"id": 2997, "seek": 1211548, "start": 12120.439999999999, "end": 12128.039999999999, "text": " train underscore images. And okay, so we get all these zeros, let's print train images, zero,", "tokens": [50612, 3847, 37556, 5267, 13, 400, 1392, 11, 370, 321, 483, 439, 613, 35193, 11, 718, 311, 4482, 3847, 5267, 11, 4018, 11, 50992], "temperature": 0.0, "avg_logprob": -0.09323421988900252, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.05183565244078636}, {"id": 2998, "seek": 1211548, "start": 12128.039999999999, "end": 12133.88, "text": " colon, that should work for us. And we're getting all these zeros. Okay, so that's the border of", "tokens": [50992, 8255, 11, 300, 820, 589, 337, 505, 13, 400, 321, 434, 1242, 439, 613, 35193, 13, 1033, 11, 370, 300, 311, 264, 7838, 295, 51284], "temperature": 0.0, "avg_logprob": -0.09323421988900252, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.05183565244078636}, {"id": 2999, "seek": 1211548, "start": 12133.88, "end": 12139.16, "text": " the picture. That's okay, I can't show you what I wanted to show you. Anyways, one pixel. And I", "tokens": [51284, 264, 3036, 13, 663, 311, 1392, 11, 286, 393, 380, 855, 291, 437, 286, 1415, 281, 855, 291, 13, 15585, 11, 472, 19261, 13, 400, 286, 51548], "temperature": 0.0, "avg_logprob": -0.09323421988900252, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.05183565244078636}, {"id": 3000, "seek": 1211548, "start": 12139.16, "end": 12144.68, "text": " wanted to have you guys guess it is simply just represented by a number between zero and 255.", "tokens": [51548, 1415, 281, 362, 291, 1074, 2041, 309, 307, 2935, 445, 10379, 538, 257, 1230, 1296, 4018, 293, 3552, 20, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09323421988900252, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.05183565244078636}, {"id": 3001, "seek": 1214468, "start": 12145.24, "end": 12149.56, "text": " Now what this stands for is the grayscale value of this pixel. So we're dealing with grayscale", "tokens": [50392, 823, 437, 341, 7382, 337, 307, 264, 677, 3772, 37088, 2158, 295, 341, 19261, 13, 407, 321, 434, 6260, 365, 677, 3772, 37088, 50608], "temperature": 0.0, "avg_logprob": -0.10593701153993607, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0004044681554660201}, {"id": 3002, "seek": 1214468, "start": 12149.56, "end": 12155.720000000001, "text": " images, although we can deal with, you know, 3d, 4d, 5d images as well, or not 5d images,", "tokens": [50608, 5267, 11, 4878, 321, 393, 2028, 365, 11, 291, 458, 11, 805, 67, 11, 1017, 67, 11, 1025, 67, 5267, 382, 731, 11, 420, 406, 1025, 67, 5267, 11, 50916], "temperature": 0.0, "avg_logprob": -0.10593701153993607, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0004044681554660201}, {"id": 3003, "seek": 1214468, "start": 12155.720000000001, "end": 12160.44, "text": " but we can deal with images that have like RGB values. First, so for example, we could have", "tokens": [50916, 457, 321, 393, 2028, 365, 5267, 300, 362, 411, 31231, 4190, 13, 2386, 11, 370, 337, 1365, 11, 321, 727, 362, 51152], "temperature": 0.0, "avg_logprob": -0.10593701153993607, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0004044681554660201}, {"id": 3004, "seek": 1214468, "start": 12160.44, "end": 12165.720000000001, "text": " a number between zero 255, another number between zero and 255 and another number between zero and", "tokens": [51152, 257, 1230, 1296, 4018, 3552, 20, 11, 1071, 1230, 1296, 4018, 293, 3552, 20, 293, 1071, 1230, 1296, 4018, 293, 51416], "temperature": 0.0, "avg_logprob": -0.10593701153993607, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0004044681554660201}, {"id": 3005, "seek": 1214468, "start": 12165.720000000001, "end": 12171.960000000001, "text": " 255 for every single pixel, right? Whereas this one is just one simple static value. Okay, so it", "tokens": [51416, 3552, 20, 337, 633, 2167, 19261, 11, 558, 30, 13813, 341, 472, 307, 445, 472, 2199, 13437, 2158, 13, 1033, 11, 370, 309, 51728], "temperature": 0.0, "avg_logprob": -0.10593701153993607, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0004044681554660201}, {"id": 3006, "seek": 1217196, "start": 12171.96, "end": 12177.64, "text": " says that here, a pixel values between zero and 255, zero being black and 255 being white. So", "tokens": [50364, 1619, 300, 510, 11, 257, 19261, 4190, 1296, 4018, 293, 3552, 20, 11, 4018, 885, 2211, 293, 3552, 20, 885, 2418, 13, 407, 50648], "temperature": 0.0, "avg_logprob": -0.08449076849316793, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0002868516312446445}, {"id": 3007, "seek": 1217196, "start": 12177.64, "end": 12182.359999999999, "text": " essentially, you know, if it's 255, that means that this is white, if it's zero, that means that", "tokens": [50648, 4476, 11, 291, 458, 11, 498, 309, 311, 3552, 20, 11, 300, 1355, 300, 341, 307, 2418, 11, 498, 309, 311, 4018, 11, 300, 1355, 300, 50884], "temperature": 0.0, "avg_logprob": -0.08449076849316793, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0002868516312446445}, {"id": 3008, "seek": 1217196, "start": 12182.359999999999, "end": 12187.32, "text": " it is black. Alright, so let's have a look at the first 10 training labels. So that was our", "tokens": [50884, 309, 307, 2211, 13, 2798, 11, 370, 718, 311, 362, 257, 574, 412, 264, 700, 1266, 3097, 16949, 13, 407, 300, 390, 527, 51132], "temperature": 0.0, "avg_logprob": -0.08449076849316793, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0002868516312446445}, {"id": 3009, "seek": 1217196, "start": 12187.32, "end": 12192.199999999999, "text": " training images. Now what are the training labels? Okay, so we have an array, and we get values from", "tokens": [51132, 3097, 5267, 13, 823, 437, 366, 264, 3097, 16949, 30, 1033, 11, 370, 321, 362, 364, 10225, 11, 293, 321, 483, 4190, 490, 51376], "temperature": 0.0, "avg_logprob": -0.08449076849316793, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0002868516312446445}, {"id": 3010, "seek": 1217196, "start": 12192.199999999999, "end": 12199.16, "text": " zero to nine. Now this is because we have 10 different classes that we could have for our", "tokens": [51376, 4018, 281, 4949, 13, 823, 341, 307, 570, 321, 362, 1266, 819, 5359, 300, 321, 727, 362, 337, 527, 51724], "temperature": 0.0, "avg_logprob": -0.08449076849316793, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0002868516312446445}, {"id": 3011, "seek": 1219916, "start": 12199.24, "end": 12204.039999999999, "text": " dataset. So there's 10 different articles of clothing that are represented. I don't know what", "tokens": [50368, 28872, 13, 407, 456, 311, 1266, 819, 11290, 295, 11502, 300, 366, 10379, 13, 286, 500, 380, 458, 437, 50608], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3012, "seek": 1219916, "start": 12204.039999999999, "end": 12209.32, "text": " all of them are, although they are right here. So t shirt, trouser, pullover, dress, coat, sandals,", "tokens": [50608, 439, 295, 552, 366, 11, 4878, 436, 366, 558, 510, 13, 407, 256, 8336, 11, 34156, 260, 11, 2235, 3570, 11, 5231, 11, 10690, 11, 4932, 1124, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3013, "seek": 1219916, "start": 12209.32, "end": 12215.4, "text": " shirt, sneaker, bag, ankle boot. Okay, so let's run this class names, just so that we have that", "tokens": [50872, 8336, 11, 9244, 4003, 11, 3411, 11, 21999, 11450, 13, 1033, 11, 370, 718, 311, 1190, 341, 1508, 5288, 11, 445, 370, 300, 321, 362, 300, 51176], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3014, "seek": 1219916, "start": 12215.4, "end": 12220.2, "text": " saved. And now what I'm going to do is use matplotlib to show you what one of the images looks like.", "tokens": [51176, 6624, 13, 400, 586, 437, 286, 478, 516, 281, 360, 307, 764, 3803, 564, 310, 38270, 281, 855, 291, 437, 472, 295, 264, 5267, 1542, 411, 13, 51416], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3015, "seek": 1219916, "start": 12220.2, "end": 12224.6, "text": " So in this case, this is a shirt. I know this is printing out kind of weird, but I'm just showing", "tokens": [51416, 407, 294, 341, 1389, 11, 341, 307, 257, 8336, 13, 286, 458, 341, 307, 14699, 484, 733, 295, 3657, 11, 457, 286, 478, 445, 4099, 51636], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3016, "seek": 1219916, "start": 12224.6, "end": 12228.28, "text": " the image. I know it's like different colors, but that's because if we don't define that we're", "tokens": [51636, 264, 3256, 13, 286, 458, 309, 311, 411, 819, 4577, 11, 457, 300, 311, 570, 498, 321, 500, 380, 6964, 300, 321, 434, 51820], "temperature": 0.0, "avg_logprob": -0.09119527722582405, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.03513937070965767}, {"id": 3017, "seek": 1222828, "start": 12228.28, "end": 12232.52, "text": " drawing a grayscale, it's going to do this. But anyways, that is what we get for the shirt. So", "tokens": [50364, 6316, 257, 677, 3772, 37088, 11, 309, 311, 516, 281, 360, 341, 13, 583, 13448, 11, 300, 307, 437, 321, 483, 337, 264, 8336, 13, 407, 50576], "temperature": 0.0, "avg_logprob": -0.08540487990659826, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.020330287516117096}, {"id": 3018, "seek": 1222828, "start": 12232.52, "end": 12237.16, "text": " let's go to another image and let's have a look at what this one is. I actually don't know what", "tokens": [50576, 718, 311, 352, 281, 1071, 3256, 293, 718, 311, 362, 257, 574, 412, 437, 341, 472, 307, 13, 286, 767, 500, 380, 458, 437, 50808], "temperature": 0.0, "avg_logprob": -0.08540487990659826, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.020330287516117096}, {"id": 3019, "seek": 1222828, "start": 12237.16, "end": 12244.92, "text": " that is. So we'll skip that maybe that's a what is it t shirt or top. This I guess is going to be", "tokens": [50808, 300, 307, 13, 407, 321, 603, 10023, 300, 1310, 300, 311, 257, 437, 307, 309, 256, 8336, 420, 1192, 13, 639, 286, 2041, 307, 516, 281, 312, 51196], "temperature": 0.0, "avg_logprob": -0.08540487990659826, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.020330287516117096}, {"id": 3020, "seek": 1222828, "start": 12244.92, "end": 12250.84, "text": " like a dress. Yeah, so we do have dressed there. Let's go for have a look at this. And some of", "tokens": [51196, 411, 257, 5231, 13, 865, 11, 370, 321, 360, 362, 12386, 456, 13, 961, 311, 352, 337, 362, 257, 574, 412, 341, 13, 400, 512, 295, 51492], "temperature": 0.0, "avg_logprob": -0.08540487990659826, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.020330287516117096}, {"id": 3021, "seek": 1222828, "start": 12250.84, "end": 12256.04, "text": " these are like hard to even make out when I'm looking at them myself. And then I guess this", "tokens": [51492, 613, 366, 411, 1152, 281, 754, 652, 484, 562, 286, 478, 1237, 412, 552, 2059, 13, 400, 550, 286, 2041, 341, 51752], "temperature": 0.0, "avg_logprob": -0.08540487990659826, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.020330287516117096}, {"id": 3022, "seek": 1225604, "start": 12256.12, "end": 12259.320000000002, "text": " will be like a hoodie or something. I'm trying to get one of the sandals to show you guys a", "tokens": [50368, 486, 312, 411, 257, 41191, 420, 746, 13, 286, 478, 1382, 281, 483, 472, 295, 264, 4932, 1124, 281, 855, 291, 1074, 257, 50528], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3023, "seek": 1225604, "start": 12259.320000000002, "end": 12263.720000000001, "text": " few different ones. There we go. So that is a sandal or a sneaker. Okay, so that is kind of", "tokens": [50528, 1326, 819, 2306, 13, 821, 321, 352, 13, 407, 300, 307, 257, 4932, 304, 420, 257, 9244, 4003, 13, 1033, 11, 370, 300, 307, 733, 295, 50748], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3024, "seek": 1225604, "start": 12263.720000000001, "end": 12267.880000000001, "text": " how we do that and how we look at the different images. So if you wanted to draw it out, all you", "tokens": [50748, 577, 321, 360, 300, 293, 577, 321, 574, 412, 264, 819, 5267, 13, 407, 498, 291, 1415, 281, 2642, 309, 484, 11, 439, 291, 50956], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3025, "seek": 1225604, "start": 12267.880000000001, "end": 12272.6, "text": " do is just make a figure, you just show the image, do the color bar, which is just giving you this,", "tokens": [50956, 360, 307, 445, 652, 257, 2573, 11, 291, 445, 855, 264, 3256, 11, 360, 264, 2017, 2159, 11, 597, 307, 445, 2902, 291, 341, 11, 51192], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3026, "seek": 1225604, "start": 12272.6, "end": 12275.560000000001, "text": " then you're going to say, I don't want to grid and then you can just show the image, right? Because", "tokens": [51192, 550, 291, 434, 516, 281, 584, 11, 286, 500, 380, 528, 281, 10748, 293, 550, 291, 393, 445, 855, 264, 3256, 11, 558, 30, 1436, 51340], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3027, "seek": 1225604, "start": 12275.560000000001, "end": 12281.0, "text": " if you don't have this line here, and you show with the grid, oh, it's actually not showing the grid", "tokens": [51340, 498, 291, 500, 380, 362, 341, 1622, 510, 11, 293, 291, 855, 365, 264, 10748, 11, 1954, 11, 309, 311, 767, 406, 4099, 264, 10748, 51612], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3028, "seek": 1225604, "start": 12281.0, "end": 12285.0, "text": " that's interesting. Although I thought it was going to show me those pixelated grid. So I guess", "tokens": [51612, 300, 311, 1880, 13, 5780, 286, 1194, 309, 390, 516, 281, 855, 385, 729, 19261, 770, 10748, 13, 407, 286, 2041, 51812], "temperature": 0.0, "avg_logprob": -0.09550840016957876, "compression_ratio": 1.8701657458563536, "no_speech_prob": 0.010327075608074665}, {"id": 3029, "seek": 1228500, "start": 12285.08, "end": 12290.04, "text": " you don't need that line. Alright, so data pre processing. Alright, so this is an important", "tokens": [50368, 291, 500, 380, 643, 300, 1622, 13, 2798, 11, 370, 1412, 659, 9007, 13, 2798, 11, 370, 341, 307, 364, 1021, 50616], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3030, "seek": 1228500, "start": 12290.04, "end": 12294.92, "text": " step in neural networks. And a lot of times when we have our data, we have it in these like random", "tokens": [50616, 1823, 294, 18161, 9590, 13, 400, 257, 688, 295, 1413, 562, 321, 362, 527, 1412, 11, 321, 362, 309, 294, 613, 411, 4974, 50860], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3031, "seek": 1228500, "start": 12294.92, "end": 12299.24, "text": " forms, or we're missing data, or there's information we don't know, or that we haven't seen. And", "tokens": [50860, 6422, 11, 420, 321, 434, 5361, 1412, 11, 420, 456, 311, 1589, 321, 500, 380, 458, 11, 420, 300, 321, 2378, 380, 1612, 13, 400, 51076], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3032, "seek": 1228500, "start": 12299.24, "end": 12303.8, "text": " typically what we need to do is pre process it. Now, what I'm going to do here is squish all my", "tokens": [51076, 5850, 437, 321, 643, 281, 360, 307, 659, 1399, 309, 13, 823, 11, 437, 286, 478, 516, 281, 360, 510, 307, 31379, 439, 452, 51304], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3033, "seek": 1228500, "start": 12303.8, "end": 12308.84, "text": " values between zero and one. Typically, it's a good idea to get all of your input values in a", "tokens": [51304, 4190, 1296, 4018, 293, 472, 13, 23129, 11, 309, 311, 257, 665, 1558, 281, 483, 439, 295, 428, 4846, 4190, 294, 257, 51556], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3034, "seek": 1228500, "start": 12308.84, "end": 12313.56, "text": " neural network in between, like that range in between, I would say negative one and one is what", "tokens": [51556, 18161, 3209, 294, 1296, 11, 411, 300, 3613, 294, 1296, 11, 286, 576, 584, 3671, 472, 293, 472, 307, 437, 51792], "temperature": 0.0, "avg_logprob": -0.07573578341695286, "compression_ratio": 1.8483870967741935, "no_speech_prob": 0.0017006555572152138}, {"id": 3035, "seek": 1231356, "start": 12313.64, "end": 12317.48, "text": " you're trying to do, you're trying to make your numbers as small as possible to feed to the neural", "tokens": [50368, 291, 434, 1382, 281, 360, 11, 291, 434, 1382, 281, 652, 428, 3547, 382, 1359, 382, 1944, 281, 3154, 281, 264, 18161, 50560], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3036, "seek": 1231356, "start": 12317.48, "end": 12321.88, "text": " network. The reason for this is your neural network starts out with random weights and biases", "tokens": [50560, 3209, 13, 440, 1778, 337, 341, 307, 428, 18161, 3209, 3719, 484, 365, 4974, 17443, 293, 32152, 50780], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3037, "seek": 1231356, "start": 12321.88, "end": 12327.24, "text": " that are in between the range zero and one, unless you change that value. So if you have massive", "tokens": [50780, 300, 366, 294, 1296, 264, 3613, 4018, 293, 472, 11, 5969, 291, 1319, 300, 2158, 13, 407, 498, 291, 362, 5994, 51048], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3038, "seek": 1231356, "start": 12327.24, "end": 12331.4, "text": " input information and tiny weights, then you're kind of having a bit of a mismatch, and you're", "tokens": [51048, 4846, 1589, 293, 5870, 17443, 11, 550, 291, 434, 733, 295, 1419, 257, 857, 295, 257, 23220, 852, 11, 293, 291, 434, 51256], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3039, "seek": 1231356, "start": 12331.4, "end": 12336.039999999999, "text": " going to make it much more difficult for your network to actually classify your information,", "tokens": [51256, 516, 281, 652, 309, 709, 544, 2252, 337, 428, 3209, 281, 767, 33872, 428, 1589, 11, 51488], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3040, "seek": 1231356, "start": 12336.039999999999, "end": 12340.039999999999, "text": " because it's going to have to work harder to update those weights and biases to reduce how", "tokens": [51488, 570, 309, 311, 516, 281, 362, 281, 589, 6081, 281, 5623, 729, 17443, 293, 32152, 281, 5407, 577, 51688], "temperature": 0.0, "avg_logprob": -0.05130472916823167, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.0017006497364491224}, {"id": 3041, "seek": 1234004, "start": 12340.04, "end": 12345.800000000001, "text": " large those values are going to be, if that makes any sense. So it usually is a good idea to", "tokens": [50364, 2416, 729, 4190, 366, 516, 281, 312, 11, 498, 300, 1669, 604, 2020, 13, 407, 309, 2673, 307, 257, 665, 1558, 281, 50652], "temperature": 0.0, "avg_logprob": -0.06356769048867106, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007345333695411682}, {"id": 3042, "seek": 1234004, "start": 12345.800000000001, "end": 12351.160000000002, "text": " pre process these and make them in between the value of zero and one. Now, since we know that", "tokens": [50652, 659, 1399, 613, 293, 652, 552, 294, 1296, 264, 2158, 295, 4018, 293, 472, 13, 823, 11, 1670, 321, 458, 300, 50920], "temperature": 0.0, "avg_logprob": -0.06356769048867106, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007345333695411682}, {"id": 3043, "seek": 1234004, "start": 12351.160000000002, "end": 12355.960000000001, "text": " we're just going to have pixel values that are in the range of 255, we can just divide by 255,", "tokens": [50920, 321, 434, 445, 516, 281, 362, 19261, 4190, 300, 366, 294, 264, 3613, 295, 3552, 20, 11, 321, 393, 445, 9845, 538, 3552, 20, 11, 51160], "temperature": 0.0, "avg_logprob": -0.06356769048867106, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007345333695411682}, {"id": 3044, "seek": 1234004, "start": 12355.960000000001, "end": 12361.640000000001, "text": " and that will automatically scale it down for us. Although it is extremely important that we do this", "tokens": [51160, 293, 300, 486, 6772, 4373, 309, 760, 337, 505, 13, 5780, 309, 307, 4664, 1021, 300, 321, 360, 341, 51444], "temperature": 0.0, "avg_logprob": -0.06356769048867106, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007345333695411682}, {"id": 3045, "seek": 1234004, "start": 12361.640000000001, "end": 12367.640000000001, "text": " to not only the training images, but the testing images as well. If you just pre process your", "tokens": [51444, 281, 406, 787, 264, 3097, 5267, 11, 457, 264, 4997, 5267, 382, 731, 13, 759, 291, 445, 659, 1399, 428, 51744], "temperature": 0.0, "avg_logprob": -0.06356769048867106, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007345333695411682}, {"id": 3046, "seek": 1236764, "start": 12367.64, "end": 12372.76, "text": " training images, and then you pass in, you know, new data that's not pre processed, that's going to", "tokens": [50364, 3097, 5267, 11, 293, 550, 291, 1320, 294, 11, 291, 458, 11, 777, 1412, 300, 311, 406, 659, 18846, 11, 300, 311, 516, 281, 50620], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3047, "seek": 1236764, "start": 12372.76, "end": 12377.0, "text": " be a huge issue. You need to make sure that your data comes in the same form. And that means when", "tokens": [50620, 312, 257, 2603, 2734, 13, 509, 643, 281, 652, 988, 300, 428, 1412, 1487, 294, 264, 912, 1254, 13, 400, 300, 1355, 562, 50832], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3048, "seek": 1236764, "start": 12377.0, "end": 12382.359999999999, "text": " we're using the model to to make predictions, whatever, you know, I guess it pixel data we", "tokens": [50832, 321, 434, 1228, 264, 2316, 281, 281, 652, 21264, 11, 2035, 11, 291, 458, 11, 286, 2041, 309, 19261, 1412, 321, 51100], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3049, "seek": 1236764, "start": 12382.359999999999, "end": 12386.92, "text": " have, we need to pre process in the same way that we pre processed our other data. Okay,", "tokens": [51100, 362, 11, 321, 643, 281, 659, 1399, 294, 264, 912, 636, 300, 321, 659, 18846, 527, 661, 1412, 13, 1033, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3050, "seek": 1236764, "start": 12386.92, "end": 12391.8, "text": " so let's pre process that so train images and test images. And I'm just going to actually steal", "tokens": [51328, 370, 718, 311, 659, 1399, 300, 370, 3847, 5267, 293, 1500, 5267, 13, 400, 286, 478, 445, 516, 281, 767, 11009, 51572], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3051, "seek": 1236764, "start": 12392.76, "end": 12396.68, "text": " some of this stuff here, and throw it in my other one before we get too far. So let's get this", "tokens": [51620, 512, 295, 341, 1507, 510, 11, 293, 3507, 309, 294, 452, 661, 472, 949, 321, 483, 886, 1400, 13, 407, 718, 311, 483, 341, 51816], "temperature": 0.0, "avg_logprob": -0.08469892836905815, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0052198064513504505}, {"id": 3052, "seek": 1239668, "start": 12396.76, "end": 12402.52, "text": " data sets. And let's throw it in here, just so we can come back and reference all this together.", "tokens": [50368, 1412, 6352, 13, 400, 718, 311, 3507, 309, 294, 510, 11, 445, 370, 321, 393, 808, 646, 293, 6408, 439, 341, 1214, 13, 50656], "temperature": 0.0, "avg_logprob": -0.09850898155799279, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.008846737444400787}, {"id": 3053, "seek": 1239668, "start": 12402.52, "end": 12409.08, "text": " Let's go class names. We don't actually need the figures, a few things I can skip, we do need", "tokens": [50656, 961, 311, 352, 1508, 5288, 13, 492, 500, 380, 767, 643, 264, 9624, 11, 257, 1326, 721, 286, 393, 10023, 11, 321, 360, 643, 50984], "temperature": 0.0, "avg_logprob": -0.09850898155799279, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.008846737444400787}, {"id": 3054, "seek": 1239668, "start": 12409.08, "end": 12416.04, "text": " this pre processing step. Like that, if I could go over here. And then what else do we need,", "tokens": [50984, 341, 659, 9007, 1823, 13, 1743, 300, 11, 498, 286, 727, 352, 670, 510, 13, 400, 550, 437, 1646, 360, 321, 643, 11, 51332], "temperature": 0.0, "avg_logprob": -0.09850898155799279, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.008846737444400787}, {"id": 3055, "seek": 1239668, "start": 12416.04, "end": 12419.48, "text": " we're going to need this model. Okay, so let's actually just copy the model into this and just", "tokens": [51332, 321, 434, 516, 281, 643, 341, 2316, 13, 1033, 11, 370, 718, 311, 767, 445, 5055, 264, 2316, 666, 341, 293, 445, 51504], "temperature": 0.0, "avg_logprob": -0.09850898155799279, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.008846737444400787}, {"id": 3056, "seek": 1239668, "start": 12419.48, "end": 12425.16, "text": " make it a little bit cleaner. We can have a look at it. So new code block model. Okay, so model,", "tokens": [51504, 652, 309, 257, 707, 857, 16532, 13, 492, 393, 362, 257, 574, 412, 309, 13, 407, 777, 3089, 3461, 2316, 13, 1033, 11, 370, 2316, 11, 51788], "temperature": 0.0, "avg_logprob": -0.09850898155799279, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.008846737444400787}, {"id": 3057, "seek": 1242516, "start": 12425.88, "end": 12429.72, "text": " creating our model. Now creating our models actually really easy. I'm hoping what you guys", "tokens": [50400, 4084, 527, 2316, 13, 823, 4084, 527, 5245, 767, 534, 1858, 13, 286, 478, 7159, 437, 291, 1074, 50592], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3058, "seek": 1242516, "start": 12429.72, "end": 12433.96, "text": " have realized so far is that data is usually the hardest part of machine learning and neural", "tokens": [50592, 362, 5334, 370, 1400, 307, 300, 1412, 307, 2673, 264, 13158, 644, 295, 3479, 2539, 293, 18161, 50804], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3059, "seek": 1242516, "start": 12433.96, "end": 12438.039999999999, "text": " networks, getting your data in the right form, the right shape and you know, pre processed", "tokens": [50804, 9590, 11, 1242, 428, 1412, 294, 264, 558, 1254, 11, 264, 558, 3909, 293, 291, 458, 11, 659, 18846, 51008], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3060, "seek": 1242516, "start": 12438.039999999999, "end": 12442.119999999999, "text": " correctly, building the models usually pretty easy because we have tools like TensorFlow and Keras", "tokens": [51008, 8944, 11, 2390, 264, 5245, 2673, 1238, 1858, 570, 321, 362, 3873, 411, 37624, 293, 591, 6985, 51212], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3061, "seek": 1242516, "start": 12442.119999999999, "end": 12447.32, "text": " that can do it for us. So we're going to say model equals Keras dot sequential. Now sequential", "tokens": [51212, 300, 393, 360, 309, 337, 505, 13, 407, 321, 434, 516, 281, 584, 2316, 6915, 591, 6985, 5893, 42881, 13, 823, 42881, 51472], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3062, "seek": 1242516, "start": 12447.32, "end": 12451.16, "text": " simply stands for the most basic form of neural network, which we've talked about so far, which", "tokens": [51472, 2935, 7382, 337, 264, 881, 3875, 1254, 295, 18161, 3209, 11, 597, 321, 600, 2825, 466, 370, 1400, 11, 597, 51664], "temperature": 0.0, "avg_logprob": -0.0925763498141071, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0033762105740606785}, {"id": 3063, "seek": 1245116, "start": 12451.16, "end": 12455.88, "text": " is just information going from the left side to the right side, passing through the layers,", "tokens": [50364, 307, 445, 1589, 516, 490, 264, 1411, 1252, 281, 264, 558, 1252, 11, 8437, 807, 264, 7914, 11, 50600], "temperature": 0.0, "avg_logprob": -0.0657566153484842, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.008061185479164124}, {"id": 3064, "seek": 1245116, "start": 12455.88, "end": 12460.68, "text": " sequentially, right, called sequential, we have not talked about recurrent or convolutional", "tokens": [50600, 5123, 3137, 11, 558, 11, 1219, 42881, 11, 321, 362, 406, 2825, 466, 18680, 1753, 420, 45216, 304, 50840], "temperature": 0.0, "avg_logprob": -0.0657566153484842, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.008061185479164124}, {"id": 3065, "seek": 1245116, "start": 12460.68, "end": 12466.92, "text": " neural networks yet. Now what we're going to do here is go Keras dot layers dot flat. So sorry,", "tokens": [50840, 18161, 9590, 1939, 13, 823, 437, 321, 434, 516, 281, 360, 510, 307, 352, 591, 6985, 5893, 7914, 5893, 4962, 13, 407, 2597, 11, 51152], "temperature": 0.0, "avg_logprob": -0.0657566153484842, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.008061185479164124}, {"id": 3066, "seek": 1245116, "start": 12466.92, "end": 12471.24, "text": " inside here, we're going to define the layers that we want in our neural network. This first", "tokens": [51152, 1854, 510, 11, 321, 434, 516, 281, 6964, 264, 7914, 300, 321, 528, 294, 527, 18161, 3209, 13, 639, 700, 51368], "temperature": 0.0, "avg_logprob": -0.0657566153484842, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.008061185479164124}, {"id": 3067, "seek": 1245116, "start": 12471.24, "end": 12477.72, "text": " layer is our input layer. And what flatten does is allows us to take in a shape of 28 by 28,", "tokens": [51368, 4583, 307, 527, 4846, 4583, 13, 400, 437, 24183, 775, 307, 4045, 505, 281, 747, 294, 257, 3909, 295, 7562, 538, 7562, 11, 51692], "temperature": 0.0, "avg_logprob": -0.0657566153484842, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.008061185479164124}, {"id": 3068, "seek": 1247772, "start": 12478.359999999999, "end": 12485.64, "text": " which we've defined here, and flatten all of the pixels into 784 pixels. So we take this 28 by 28", "tokens": [50396, 597, 321, 600, 7642, 510, 11, 293, 24183, 439, 295, 264, 18668, 666, 1614, 25494, 18668, 13, 407, 321, 747, 341, 7562, 538, 7562, 50760], "temperature": 0.0, "avg_logprob": -0.08977820656516335, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.00460931658744812}, {"id": 3069, "seek": 1247772, "start": 12485.64, "end": 12491.48, "text": " kind of matrix like structure, and just flatten it out. And Keras will do that for us, we don't", "tokens": [50760, 733, 295, 8141, 411, 3877, 11, 293, 445, 24183, 309, 484, 13, 400, 591, 6985, 486, 360, 300, 337, 505, 11, 321, 500, 380, 51052], "temperature": 0.0, "avg_logprob": -0.08977820656516335, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.00460931658744812}, {"id": 3070, "seek": 1247772, "start": 12491.48, "end": 12497.08, "text": " actually need to take our you know, matrix data and transform before passing. So we've done that.", "tokens": [51052, 767, 643, 281, 747, 527, 291, 458, 11, 8141, 1412, 293, 4088, 949, 8437, 13, 407, 321, 600, 1096, 300, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08977820656516335, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.00460931658744812}, {"id": 3071, "seek": 1247772, "start": 12497.8, "end": 12506.279999999999, "text": " Next, we have Keras dot layers dot dense 128 activation equals rectify linear unit. So this", "tokens": [51368, 3087, 11, 321, 362, 591, 6985, 5893, 7914, 5893, 18011, 29810, 24433, 6915, 11048, 2505, 8213, 4985, 13, 407, 341, 51792], "temperature": 0.0, "avg_logprob": -0.08977820656516335, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.00460931658744812}, {"id": 3072, "seek": 1250628, "start": 12506.36, "end": 12512.76, "text": " is our first hidden layer, layer two, right, that's what I've denoted here. And this is a dense layer.", "tokens": [50368, 307, 527, 700, 7633, 4583, 11, 4583, 732, 11, 558, 11, 300, 311, 437, 286, 600, 1441, 23325, 510, 13, 400, 341, 307, 257, 18011, 4583, 13, 50688], "temperature": 0.0, "avg_logprob": -0.08947943185121958, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0010649313917383552}, {"id": 3073, "seek": 1250628, "start": 12512.76, "end": 12518.68, "text": " Now dense, again, means that all of the, what is it, the neurons in the previous layer are connected", "tokens": [50688, 823, 18011, 11, 797, 11, 1355, 300, 439, 295, 264, 11, 437, 307, 309, 11, 264, 22027, 294, 264, 3894, 4583, 366, 4582, 50984], "temperature": 0.0, "avg_logprob": -0.08947943185121958, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0010649313917383552}, {"id": 3074, "seek": 1250628, "start": 12518.68, "end": 12524.68, "text": " to every neuron in this layer. So we have 828 neurons here. How do we pick that number? We", "tokens": [50984, 281, 633, 34090, 294, 341, 4583, 13, 407, 321, 362, 1649, 11205, 22027, 510, 13, 1012, 360, 321, 1888, 300, 1230, 30, 492, 51284], "temperature": 0.0, "avg_logprob": -0.08947943185121958, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0010649313917383552}, {"id": 3075, "seek": 1250628, "start": 12524.68, "end": 12528.44, "text": " don't know, we kind of just came up with it. Usually, it's a good idea that you're going to do", "tokens": [51284, 500, 380, 458, 11, 321, 733, 295, 445, 1361, 493, 365, 309, 13, 11419, 11, 309, 311, 257, 665, 1558, 300, 291, 434, 516, 281, 360, 51472], "temperature": 0.0, "avg_logprob": -0.08947943185121958, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0010649313917383552}, {"id": 3076, "seek": 1250628, "start": 12528.44, "end": 12533.16, "text": " this as like a little bit smaller than what your input layer is, although sometimes it's going to", "tokens": [51472, 341, 382, 411, 257, 707, 857, 4356, 813, 437, 428, 4846, 4583, 307, 11, 4878, 2171, 309, 311, 516, 281, 51708], "temperature": 0.0, "avg_logprob": -0.08947943185121958, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0010649313917383552}, {"id": 3077, "seek": 1253316, "start": 12533.16, "end": 12536.76, "text": " be bigger, you know, sometimes it's going to be half the size, it really depends on the problem,", "tokens": [50364, 312, 3801, 11, 291, 458, 11, 2171, 309, 311, 516, 281, 312, 1922, 264, 2744, 11, 309, 534, 5946, 322, 264, 1154, 11, 50544], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3078, "seek": 1253316, "start": 12536.76, "end": 12540.84, "text": " I can't really give you a straight answer for that. And then our activation function will", "tokens": [50544, 286, 393, 380, 534, 976, 291, 257, 2997, 1867, 337, 300, 13, 400, 550, 527, 24433, 2445, 486, 50748], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3079, "seek": 1253316, "start": 12540.84, "end": 12545.88, "text": " define as rectify linear unit. Now we could pick a bunch of different activation functions, there's", "tokens": [50748, 6964, 382, 11048, 2505, 8213, 4985, 13, 823, 321, 727, 1888, 257, 3840, 295, 819, 24433, 6828, 11, 456, 311, 51000], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3080, "seek": 1253316, "start": 12545.88, "end": 12551.4, "text": " time we can pick sigmoid, we could pick 10 h, which is hyperbolic tangent, doesn't really matter.", "tokens": [51000, 565, 321, 393, 1888, 4556, 3280, 327, 11, 321, 727, 1888, 1266, 276, 11, 597, 307, 9848, 65, 7940, 27747, 11, 1177, 380, 534, 1871, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3081, "seek": 1253316, "start": 12551.96, "end": 12556.2, "text": " And then we're going to define our last layer, which is our output layer, which is a dense layer", "tokens": [51304, 400, 550, 321, 434, 516, 281, 6964, 527, 1036, 4583, 11, 597, 307, 527, 5598, 4583, 11, 597, 307, 257, 18011, 4583, 51516], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3082, "seek": 1253316, "start": 12556.2, "end": 12562.2, "text": " of 10 output neurons with the activation of softmax. Okay, so can we think of why we would have picked", "tokens": [51516, 295, 1266, 5598, 22027, 365, 264, 24433, 295, 2787, 41167, 13, 1033, 11, 370, 393, 321, 519, 295, 983, 321, 576, 362, 6183, 51816], "temperature": 0.0, "avg_logprob": -0.08447218594485767, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.016401510685682297}, {"id": 3083, "seek": 1256220, "start": 12562.28, "end": 12566.44, "text": " 10 here, right? I'll give you guys a second to think about it, based on the fact that our output", "tokens": [50368, 1266, 510, 11, 558, 30, 286, 603, 976, 291, 1074, 257, 1150, 281, 519, 466, 309, 11, 2361, 322, 264, 1186, 300, 527, 5598, 50576], "temperature": 0.0, "avg_logprob": -0.04624769726737601, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.008846140466630459}, {"id": 3084, "seek": 1256220, "start": 12566.44, "end": 12571.480000000001, "text": " layer, you know, is supposed to have as many neurons as classes we're going to predict for.", "tokens": [50576, 4583, 11, 291, 458, 11, 307, 3442, 281, 362, 382, 867, 22027, 382, 5359, 321, 434, 516, 281, 6069, 337, 13, 50828], "temperature": 0.0, "avg_logprob": -0.04624769726737601, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.008846140466630459}, {"id": 3085, "seek": 1256220, "start": 12571.480000000001, "end": 12577.560000000001, "text": " So that is exactly what we have 10. If we look, we have 10 classes here. So we're going to have 10", "tokens": [50828, 407, 300, 307, 2293, 437, 321, 362, 1266, 13, 759, 321, 574, 11, 321, 362, 1266, 5359, 510, 13, 407, 321, 434, 516, 281, 362, 1266, 51132], "temperature": 0.0, "avg_logprob": -0.04624769726737601, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.008846140466630459}, {"id": 3086, "seek": 1256220, "start": 12577.560000000001, "end": 12583.480000000001, "text": " output neurons in our output layer. And again, we're going to have this probability distribution.", "tokens": [51132, 5598, 22027, 294, 527, 5598, 4583, 13, 400, 797, 11, 321, 434, 516, 281, 362, 341, 8482, 7316, 13, 51428], "temperature": 0.0, "avg_logprob": -0.04624769726737601, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.008846140466630459}, {"id": 3087, "seek": 1256220, "start": 12583.480000000001, "end": 12589.16, "text": " And the way we do that is using the activation function softmax. So softmax will make sure", "tokens": [51428, 400, 264, 636, 321, 360, 300, 307, 1228, 264, 24433, 2445, 2787, 41167, 13, 407, 2787, 41167, 486, 652, 988, 51712], "temperature": 0.0, "avg_logprob": -0.04624769726737601, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.008846140466630459}, {"id": 3088, "seek": 1258916, "start": 12589.24, "end": 12594.039999999999, "text": " that all of the values of our neurons add up to one, and that they're between zero and one.", "tokens": [50368, 300, 439, 295, 264, 4190, 295, 527, 22027, 909, 493, 281, 472, 11, 293, 300, 436, 434, 1296, 4018, 293, 472, 13, 50608], "temperature": 0.0, "avg_logprob": -0.0933503732085228, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.053395289927721024}, {"id": 3089, "seek": 1258916, "start": 12594.76, "end": 12599.0, "text": " So that is our, our model, we've created the model now. So let's actually run this.", "tokens": [50644, 407, 300, 307, 527, 11, 527, 2316, 11, 321, 600, 2942, 264, 2316, 586, 13, 407, 718, 311, 767, 1190, 341, 13, 50856], "temperature": 0.0, "avg_logprob": -0.0933503732085228, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.053395289927721024}, {"id": 3090, "seek": 1258916, "start": 12600.84, "end": 12605.56, "text": " See, are we going to get any errors here? Is this going to run? And then we'll run the model,", "tokens": [50948, 3008, 11, 366, 321, 516, 281, 483, 604, 13603, 510, 30, 1119, 341, 516, 281, 1190, 30, 400, 550, 321, 603, 1190, 264, 2316, 11, 51184], "temperature": 0.0, "avg_logprob": -0.0933503732085228, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.053395289927721024}, {"id": 3091, "seek": 1258916, "start": 12605.56, "end": 12609.32, "text": " and then we'll go on to the next step, which actually going to be training and testing the", "tokens": [51184, 293, 550, 321, 603, 352, 322, 281, 264, 958, 1823, 11, 597, 767, 516, 281, 312, 3097, 293, 4997, 264, 51372], "temperature": 0.0, "avg_logprob": -0.0933503732085228, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.053395289927721024}, {"id": 3092, "seek": 1258916, "start": 12609.32, "end": 12614.039999999999, "text": " model. Okay, so let's create the model now, shouldn't get any issues, and we're good. And now let's", "tokens": [51372, 2316, 13, 1033, 11, 370, 718, 311, 1884, 264, 2316, 586, 11, 4659, 380, 483, 604, 2663, 11, 293, 321, 434, 665, 13, 400, 586, 718, 311, 51608], "temperature": 0.0, "avg_logprob": -0.0933503732085228, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.053395289927721024}, {"id": 3093, "seek": 1261404, "start": 12614.04, "end": 12619.080000000002, "text": " move on to the next step. I'm forgetting what it is, though, which is training the model. Oh,", "tokens": [50364, 1286, 322, 281, 264, 958, 1823, 13, 286, 478, 25428, 437, 309, 307, 11, 1673, 11, 597, 307, 3097, 264, 2316, 13, 876, 11, 50616], "temperature": 0.0, "avg_logprob": -0.08622150733822682, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.05499983951449394}, {"id": 3094, "seek": 1261404, "start": 12619.080000000002, "end": 12624.44, "text": " sorry, compiling the model. Okay, so compiling the model. So we've built now what we call the", "tokens": [50616, 2597, 11, 715, 4883, 264, 2316, 13, 1033, 11, 370, 715, 4883, 264, 2316, 13, 407, 321, 600, 3094, 586, 437, 321, 818, 264, 50884], "temperature": 0.0, "avg_logprob": -0.08622150733822682, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.05499983951449394}, {"id": 3095, "seek": 1261404, "start": 12624.44, "end": 12628.92, "text": " architecture of our neural network, right, we've defined the amount of neurons in each layer,", "tokens": [50884, 9482, 295, 527, 18161, 3209, 11, 558, 11, 321, 600, 7642, 264, 2372, 295, 22027, 294, 1184, 4583, 11, 51108], "temperature": 0.0, "avg_logprob": -0.08622150733822682, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.05499983951449394}, {"id": 3096, "seek": 1261404, "start": 12628.92, "end": 12633.880000000001, "text": " we've defined the activation function, and we define the type of layer and the type of connections.", "tokens": [51108, 321, 600, 7642, 264, 24433, 2445, 11, 293, 321, 6964, 264, 2010, 295, 4583, 293, 264, 2010, 295, 9271, 13, 51356], "temperature": 0.0, "avg_logprob": -0.08622150733822682, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.05499983951449394}, {"id": 3097, "seek": 1261404, "start": 12633.880000000001, "end": 12638.84, "text": " The next thing we need to pick is the optimizer, the loss and the metrics we're going to be looking", "tokens": [51356, 440, 958, 551, 321, 643, 281, 1888, 307, 264, 5028, 6545, 11, 264, 4470, 293, 264, 16367, 321, 434, 516, 281, 312, 1237, 51604], "temperature": 0.0, "avg_logprob": -0.08622150733822682, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.05499983951449394}, {"id": 3098, "seek": 1263884, "start": 12638.84, "end": 12643.880000000001, "text": " at. So the optimizer we're going to use is Adam. This is again, just the algorithm that performs", "tokens": [50364, 412, 13, 407, 264, 5028, 6545, 321, 434, 516, 281, 764, 307, 7938, 13, 639, 307, 797, 11, 445, 264, 9284, 300, 26213, 50616], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3099, "seek": 1263884, "start": 12643.880000000001, "end": 12648.52, "text": " the gradient descent, you don't really need to look at these too much, you can read up on some", "tokens": [50616, 264, 16235, 23475, 11, 291, 500, 380, 534, 643, 281, 574, 412, 613, 886, 709, 11, 291, 393, 1401, 493, 322, 512, 50848], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3100, "seek": 1263884, "start": 12648.52, "end": 12653.64, "text": " different activation functions or sorry, optimizers, if you want to kind of see the difference between", "tokens": [50848, 819, 24433, 6828, 420, 2597, 11, 5028, 22525, 11, 498, 291, 528, 281, 733, 295, 536, 264, 2649, 1296, 51104], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3101, "seek": 1263884, "start": 12653.64, "end": 12659.56, "text": " them, but that's not crazy, we're going to pick a loss. So in this case, sparse categorical cross", "tokens": [51104, 552, 11, 457, 300, 311, 406, 3219, 11, 321, 434, 516, 281, 1888, 257, 4470, 13, 407, 294, 341, 1389, 11, 637, 11668, 19250, 804, 3278, 51400], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3102, "seek": 1263884, "start": 12659.56, "end": 12663.24, "text": " entropy, again, not going to go into depth about that, you guys can look that up if you want to", "tokens": [51400, 30867, 11, 797, 11, 406, 516, 281, 352, 666, 7161, 466, 300, 11, 291, 1074, 393, 574, 300, 493, 498, 291, 528, 281, 51584], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3103, "seek": 1263884, "start": 12663.24, "end": 12667.8, "text": " see how it works. And then metrics. So what we're looking for the output that we want to see from", "tokens": [51584, 536, 577, 309, 1985, 13, 400, 550, 16367, 13, 407, 437, 321, 434, 1237, 337, 264, 5598, 300, 321, 528, 281, 536, 490, 51812], "temperature": 0.0, "avg_logprob": -0.07003661473592122, "compression_ratio": 1.803076923076923, "no_speech_prob": 0.007120555266737938}, {"id": 3104, "seek": 1266780, "start": 12667.8, "end": 12673.8, "text": " the network, which is accuracy. Now from, you know, kind of right now with our current knowledge,", "tokens": [50364, 264, 3209, 11, 597, 307, 14170, 13, 823, 490, 11, 291, 458, 11, 733, 295, 558, 586, 365, 527, 2190, 3601, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0713753820467396, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.0016483620274811983}, {"id": 3105, "seek": 1266780, "start": 12673.8, "end": 12678.359999999999, "text": " we're just going to stick with this as what we're going to compile our neural networks with,", "tokens": [50664, 321, 434, 445, 516, 281, 2897, 365, 341, 382, 437, 321, 434, 516, 281, 31413, 527, 18161, 9590, 365, 11, 50892], "temperature": 0.0, "avg_logprob": -0.0713753820467396, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.0016483620274811983}, {"id": 3106, "seek": 1266780, "start": 12678.359999999999, "end": 12683.72, "text": " we can pick different values if we want. And these are what we call, what is it hyper parameter", "tokens": [50892, 321, 393, 1888, 819, 4190, 498, 321, 528, 13, 400, 613, 366, 437, 321, 818, 11, 437, 307, 309, 9848, 13075, 51160], "temperature": 0.0, "avg_logprob": -0.0713753820467396, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.0016483620274811983}, {"id": 3107, "seek": 1266780, "start": 12683.72, "end": 12688.679999999998, "text": " tuning. So the parameters that are inside here, so like the weights and the biases are things that", "tokens": [51160, 15164, 13, 407, 264, 9834, 300, 366, 1854, 510, 11, 370, 411, 264, 17443, 293, 264, 32152, 366, 721, 300, 51408], "temperature": 0.0, "avg_logprob": -0.0713753820467396, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.0016483620274811983}, {"id": 3108, "seek": 1266780, "start": 12688.679999999998, "end": 12694.119999999999, "text": " we can't manually change. But these are things that we can change, right, the optimizer, the loss,", "tokens": [51408, 321, 393, 380, 16945, 1319, 13, 583, 613, 366, 721, 300, 321, 393, 1319, 11, 558, 11, 264, 5028, 6545, 11, 264, 4470, 11, 51680], "temperature": 0.0, "avg_logprob": -0.0713753820467396, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.0016483620274811983}, {"id": 3109, "seek": 1269412, "start": 12694.12, "end": 12698.84, "text": " the metrics, the activation function, we can change that. So these are called hyper parameters,", "tokens": [50364, 264, 16367, 11, 264, 24433, 2445, 11, 321, 393, 1319, 300, 13, 407, 613, 366, 1219, 9848, 9834, 11, 50600], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3110, "seek": 1269412, "start": 12698.84, "end": 12704.52, "text": " same thing with the number of neurons in each layer. So hyper parameter tuning is a process of", "tokens": [50600, 912, 551, 365, 264, 1230, 295, 22027, 294, 1184, 4583, 13, 407, 9848, 13075, 15164, 307, 257, 1399, 295, 50884], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3111, "seek": 1269412, "start": 12704.52, "end": 12709.960000000001, "text": " changing all of these values and looking at how models perform with different hyper parameters", "tokens": [50884, 4473, 439, 295, 613, 4190, 293, 1237, 412, 577, 5245, 2042, 365, 819, 9848, 9834, 51156], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3112, "seek": 1269412, "start": 12709.960000000001, "end": 12713.720000000001, "text": " change. So I'm not really going to talk about that too much. But that is something to note,", "tokens": [51156, 1319, 13, 407, 286, 478, 406, 534, 516, 281, 751, 466, 300, 886, 709, 13, 583, 300, 307, 746, 281, 3637, 11, 51344], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3113, "seek": 1269412, "start": 12713.720000000001, "end": 12718.52, "text": " because you'll probably hear that, you know, this hyper parameter kind of idea. Okay, so we've", "tokens": [51344, 570, 291, 603, 1391, 1568, 300, 11, 291, 458, 11, 341, 9848, 13075, 733, 295, 1558, 13, 1033, 11, 370, 321, 600, 51584], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3114, "seek": 1269412, "start": 12718.52, "end": 12722.28, "text": " compiled the model now using this, which just means we've picked all the different things that", "tokens": [51584, 36548, 264, 2316, 586, 1228, 341, 11, 597, 445, 1355, 321, 600, 6183, 439, 264, 819, 721, 300, 51772], "temperature": 0.0, "avg_logprob": -0.04880474149718765, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.00757673429325223}, {"id": 3115, "seek": 1272228, "start": 12722.28, "end": 12727.08, "text": " we need to use for it. And now on to training the model. So I'm just going to copy this in.", "tokens": [50364, 321, 643, 281, 764, 337, 309, 13, 400, 586, 322, 281, 3097, 264, 2316, 13, 407, 286, 478, 445, 516, 281, 5055, 341, 294, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3116, "seek": 1272228, "start": 12728.04, "end": 12733.640000000001, "text": " Again, remember this, these parts are pretty syntaxually heavy, but fairly easy to actually do.", "tokens": [50652, 3764, 11, 1604, 341, 11, 613, 3166, 366, 1238, 28431, 671, 4676, 11, 457, 6457, 1858, 281, 767, 360, 13, 50932], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3117, "seek": 1272228, "start": 12733.640000000001, "end": 12738.12, "text": " So we're going to fit the model. So fit just means we're fitting it to the training data. It's", "tokens": [50932, 407, 321, 434, 516, 281, 3318, 264, 2316, 13, 407, 3318, 445, 1355, 321, 434, 15669, 309, 281, 264, 3097, 1412, 13, 467, 311, 51156], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3118, "seek": 1272228, "start": 12738.12, "end": 12741.880000000001, "text": " another word for training, essentially. So we're going to pass it the training images,", "tokens": [51156, 1071, 1349, 337, 3097, 11, 4476, 13, 407, 321, 434, 516, 281, 1320, 309, 264, 3097, 5267, 11, 51344], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3119, "seek": 1272228, "start": 12741.880000000001, "end": 12746.6, "text": " the training labels, and notice how much easier it is to pass this. Now, we don't need to do this", "tokens": [51344, 264, 3097, 16949, 11, 293, 3449, 577, 709, 3571, 309, 307, 281, 1320, 341, 13, 823, 11, 321, 500, 380, 643, 281, 360, 341, 51580], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3120, "seek": 1272228, "start": 12746.6, "end": 12750.28, "text": " input function, we don't need to do all of that, because Keras can handle it for us. And we define", "tokens": [51580, 4846, 2445, 11, 321, 500, 380, 643, 281, 360, 439, 295, 300, 11, 570, 591, 6985, 393, 4813, 309, 337, 505, 13, 400, 321, 6964, 51764], "temperature": 0.0, "avg_logprob": -0.09055590947469075, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.0018675047904253006}, {"id": 3121, "seek": 1275028, "start": 12750.28, "end": 12756.12, "text": " our epochs as 10 epochs is another hyper parameter that you could tune and change if you wanted to.", "tokens": [50364, 527, 30992, 28346, 382, 1266, 30992, 28346, 307, 1071, 9848, 13075, 300, 291, 727, 10864, 293, 1319, 498, 291, 1415, 281, 13, 50656], "temperature": 0.0, "avg_logprob": -0.06586377804095929, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.010327249765396118}, {"id": 3122, "seek": 1275028, "start": 12756.68, "end": 12761.24, "text": " All right, so that will actually fit our model. So what I'm going to do is put this in another", "tokens": [50684, 1057, 558, 11, 370, 300, 486, 767, 3318, 527, 2316, 13, 407, 437, 286, 478, 516, 281, 360, 307, 829, 341, 294, 1071, 50912], "temperature": 0.0, "avg_logprob": -0.06586377804095929, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.010327249765396118}, {"id": 3123, "seek": 1275028, "start": 12761.24, "end": 12766.36, "text": " code block. So I don't need to keep retraining this. So we'll go like that. And let's actually", "tokens": [50912, 3089, 3461, 13, 407, 286, 500, 380, 643, 281, 1066, 49356, 1760, 341, 13, 407, 321, 603, 352, 411, 300, 13, 400, 718, 311, 767, 51168], "temperature": 0.0, "avg_logprob": -0.06586377804095929, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.010327249765396118}, {"id": 3124, "seek": 1275028, "start": 12766.36, "end": 12770.84, "text": " look at this training process. So we've run the model, this should compile. And now let's fit", "tokens": [51168, 574, 412, 341, 3097, 1399, 13, 407, 321, 600, 1190, 264, 2316, 11, 341, 820, 31413, 13, 400, 586, 718, 311, 3318, 51392], "temperature": 0.0, "avg_logprob": -0.06586377804095929, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.010327249765396118}, {"id": 3125, "seek": 1275028, "start": 12770.84, "end": 12776.12, "text": " it and let's see what we actually end up getting. Alright, so epoch one, and we can see that we're", "tokens": [51392, 309, 293, 718, 311, 536, 437, 321, 767, 917, 493, 1242, 13, 2798, 11, 370, 30992, 339, 472, 11, 293, 321, 393, 536, 300, 321, 434, 51656], "temperature": 0.0, "avg_logprob": -0.06586377804095929, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.010327249765396118}, {"id": 3126, "seek": 1277612, "start": 12776.12, "end": 12780.6, "text": " getting a loss and we're getting accuracy printing out on the side here. Now, this is going to take", "tokens": [50364, 1242, 257, 4470, 293, 321, 434, 1242, 14170, 14699, 484, 322, 264, 1252, 510, 13, 823, 11, 341, 307, 516, 281, 747, 50588], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3127, "seek": 1277612, "start": 12780.6, "end": 12785.400000000001, "text": " a second, like this is going to take a few minutes, as opposed to our other models that we made are", "tokens": [50588, 257, 1150, 11, 411, 341, 307, 516, 281, 747, 257, 1326, 2077, 11, 382, 8851, 281, 527, 661, 5245, 300, 321, 1027, 366, 50828], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3128, "seek": 1277612, "start": 12785.400000000001, "end": 12790.44, "text": " not a few minutes, but you know, a few seconds, when you have 60,000 images, and you have a network", "tokens": [50828, 406, 257, 1326, 2077, 11, 457, 291, 458, 11, 257, 1326, 3949, 11, 562, 291, 362, 4060, 11, 1360, 5267, 11, 293, 291, 362, 257, 3209, 51080], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3129, "seek": 1277612, "start": 12790.44, "end": 12796.68, "text": " that's comprised of 784 neurons, 128 neurons, and then 10 neurons, you have a lot of weights and", "tokens": [51080, 300, 311, 38062, 295, 1614, 25494, 22027, 11, 29810, 22027, 11, 293, 550, 1266, 22027, 11, 291, 362, 257, 688, 295, 17443, 293, 51392], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3130, "seek": 1277612, "start": 12796.68, "end": 12801.160000000002, "text": " biases and a lot of math that needs to go on. So this will take a few seconds to run. Now,", "tokens": [51392, 32152, 293, 257, 688, 295, 5221, 300, 2203, 281, 352, 322, 13, 407, 341, 486, 747, 257, 1326, 3949, 281, 1190, 13, 823, 11, 51616], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3131, "seek": 1277612, "start": 12801.160000000002, "end": 12804.76, "text": " if you're on a much faster computer, you'll probably be faster than this. But this is why I", "tokens": [51616, 498, 291, 434, 322, 257, 709, 4663, 3820, 11, 291, 603, 1391, 312, 4663, 813, 341, 13, 583, 341, 307, 983, 286, 51796], "temperature": 0.0, "avg_logprob": -0.057955816194608614, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.037320833653211594}, {"id": 3132, "seek": 1280476, "start": 12804.76, "end": 12808.6, "text": " like Google Collaboratory, because you know, this isn't using any of my computer's resources", "tokens": [50364, 411, 3329, 44483, 4745, 11, 570, 291, 458, 11, 341, 1943, 380, 1228, 604, 295, 452, 3820, 311, 3593, 50556], "temperature": 0.0, "avg_logprob": -0.085628738976959, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.015904713422060013}, {"id": 3133, "seek": 1280476, "start": 12808.6, "end": 12815.16, "text": " to train. It's using this. And we can see, like the RAM and the disk. How do I look at this?", "tokens": [50556, 281, 3847, 13, 467, 311, 1228, 341, 13, 400, 321, 393, 536, 11, 411, 264, 14561, 293, 264, 12355, 13, 1012, 360, 286, 574, 412, 341, 30, 50884], "temperature": 0.0, "avg_logprob": -0.085628738976959, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.015904713422060013}, {"id": 3134, "seek": 1280476, "start": 12816.12, "end": 12820.68, "text": " In this network? Oh, is it going to let me look at this now? Okay, I don't know why it's not letting", "tokens": [50932, 682, 341, 3209, 30, 876, 11, 307, 309, 516, 281, 718, 385, 574, 412, 341, 586, 30, 1033, 11, 286, 500, 380, 458, 983, 309, 311, 406, 8295, 51160], "temperature": 0.0, "avg_logprob": -0.085628738976959, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.015904713422060013}, {"id": 3135, "seek": 1280476, "start": 12820.68, "end": 12825.4, "text": " me click this, but usually you can have a look at it. And now we've trained and we've fit the", "tokens": [51160, 385, 2052, 341, 11, 457, 2673, 291, 393, 362, 257, 574, 412, 309, 13, 400, 586, 321, 600, 8895, 293, 321, 600, 3318, 264, 51396], "temperature": 0.0, "avg_logprob": -0.085628738976959, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.015904713422060013}, {"id": 3136, "seek": 1280476, "start": 12825.4, "end": 12831.24, "text": " model. So we can see that we have an accuracy of 91%. But the thing is, this is the accuracy", "tokens": [51396, 2316, 13, 407, 321, 393, 536, 300, 321, 362, 364, 14170, 295, 31064, 6856, 583, 264, 551, 307, 11, 341, 307, 264, 14170, 51688], "temperature": 0.0, "avg_logprob": -0.085628738976959, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.015904713422060013}, {"id": 3137, "seek": 1283124, "start": 12831.96, "end": 12837.48, "text": " on or testing or our training data. So now if we want to find what the true accuracy is,", "tokens": [50400, 322, 420, 4997, 420, 527, 3097, 1412, 13, 407, 586, 498, 321, 528, 281, 915, 437, 264, 2074, 14170, 307, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09359065346095873, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.02595251053571701}, {"id": 3138, "seek": 1283124, "start": 12838.039999999999, "end": 12842.039999999999, "text": " what we need to do is actually test it on our testing data. So I'm going to steal", "tokens": [50704, 437, 321, 643, 281, 360, 307, 767, 1500, 309, 322, 527, 4997, 1412, 13, 407, 286, 478, 516, 281, 11009, 50904], "temperature": 0.0, "avg_logprob": -0.09359065346095873, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.02595251053571701}, {"id": 3139, "seek": 1283124, "start": 12842.039999999999, "end": 12846.52, "text": " this line of code here. This is how we test our model. Pretty straightforward. I'll just", "tokens": [50904, 341, 1622, 295, 3089, 510, 13, 639, 307, 577, 321, 1500, 527, 2316, 13, 10693, 15325, 13, 286, 603, 445, 51128], "temperature": 0.0, "avg_logprob": -0.09359065346095873, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.02595251053571701}, {"id": 3140, "seek": 1283124, "start": 12846.52, "end": 12852.119999999999, "text": " close this. So let's go into code block. So we have test loss test accuracy is model dot", "tokens": [51128, 1998, 341, 13, 407, 718, 311, 352, 666, 3089, 3461, 13, 407, 321, 362, 1500, 4470, 1500, 14170, 307, 2316, 5893, 51408], "temperature": 0.0, "avg_logprob": -0.09359065346095873, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.02595251053571701}, {"id": 3141, "seek": 1283124, "start": 12852.119999999999, "end": 12858.92, "text": " evaluate test images test labels verbose equals one. Now what is verbose? I was hoping it was", "tokens": [51408, 13059, 1500, 5267, 1500, 16949, 9595, 541, 6915, 472, 13, 823, 437, 307, 9595, 541, 30, 286, 390, 7159, 309, 390, 51748], "temperature": 0.0, "avg_logprob": -0.09359065346095873, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.02595251053571701}, {"id": 3142, "seek": 1285892, "start": 12858.92, "end": 12863.16, "text": " going to give me the thing so I could just read it to you guys. But verbose essentially is just", "tokens": [50364, 516, 281, 976, 385, 264, 551, 370, 286, 727, 445, 1401, 309, 281, 291, 1074, 13, 583, 9595, 541, 4476, 307, 445, 50576], "temperature": 0.0, "avg_logprob": -0.09077800909678141, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.009707414545118809}, {"id": 3143, "seek": 1285892, "start": 12863.16, "end": 12868.12, "text": " are we looking at output or not? So like how much information are we seeing as this model evaluates?", "tokens": [50576, 366, 321, 1237, 412, 5598, 420, 406, 30, 407, 411, 577, 709, 1589, 366, 321, 2577, 382, 341, 2316, 6133, 1024, 30, 50824], "temperature": 0.0, "avg_logprob": -0.09077800909678141, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.009707414545118809}, {"id": 3144, "seek": 1285892, "start": 12868.76, "end": 12873.08, "text": " It's like how much is printing out to the console? That's what that means. And yes,", "tokens": [50856, 467, 311, 411, 577, 709, 307, 14699, 484, 281, 264, 11076, 30, 663, 311, 437, 300, 1355, 13, 400, 2086, 11, 51072], "temperature": 0.0, "avg_logprob": -0.09077800909678141, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.009707414545118809}, {"id": 3145, "seek": 1285892, "start": 12873.08, "end": 12877.56, "text": " this will just split up kind of the metrics that are returned to this into test loss and test accuracy", "tokens": [51072, 341, 486, 445, 7472, 493, 733, 295, 264, 16367, 300, 366, 8752, 281, 341, 666, 1500, 4470, 293, 1500, 14170, 51296], "temperature": 0.0, "avg_logprob": -0.09077800909678141, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.009707414545118809}, {"id": 3146, "seek": 1285892, "start": 12877.56, "end": 12883.64, "text": " so we can have a look at it. Now you will notice when I run this, that the accuracy will likely", "tokens": [51296, 370, 321, 393, 362, 257, 574, 412, 309, 13, 823, 291, 486, 3449, 562, 286, 1190, 341, 11, 300, 264, 14170, 486, 3700, 51600], "temperature": 0.0, "avg_logprob": -0.09077800909678141, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.009707414545118809}, {"id": 3147, "seek": 1288364, "start": 12883.64, "end": 12888.84, "text": " be lower on this than it was on our model. So actually, the accuracy we had from this", "tokens": [50364, 312, 3126, 322, 341, 813, 309, 390, 322, 527, 2316, 13, 407, 767, 11, 264, 14170, 321, 632, 490, 341, 50624], "temperature": 0.0, "avg_logprob": -0.04353990637022873, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.1441187560558319}, {"id": 3148, "seek": 1288364, "start": 12888.84, "end": 12894.68, "text": " was about 91. And now we're only getting 88.5. So this is an example of something we call", "tokens": [50624, 390, 466, 31064, 13, 400, 586, 321, 434, 787, 1242, 24587, 13, 20, 13, 407, 341, 307, 364, 1365, 295, 746, 321, 818, 50916], "temperature": 0.0, "avg_logprob": -0.04353990637022873, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.1441187560558319}, {"id": 3149, "seek": 1288364, "start": 12894.68, "end": 12900.76, "text": " overfitting. Our model seemed like it was doing really well on the testing data or sorry,", "tokens": [50916, 670, 69, 2414, 13, 2621, 2316, 6576, 411, 309, 390, 884, 534, 731, 322, 264, 4997, 1412, 420, 2597, 11, 51220], "temperature": 0.0, "avg_logprob": -0.04353990637022873, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.1441187560558319}, {"id": 3150, "seek": 1288364, "start": 12900.76, "end": 12906.439999999999, "text": " the training data. But that's because it was seeing that data so often, right with 10 epochs,", "tokens": [51220, 264, 3097, 1412, 13, 583, 300, 311, 570, 309, 390, 2577, 300, 1412, 370, 2049, 11, 558, 365, 1266, 30992, 28346, 11, 51504], "temperature": 0.0, "avg_logprob": -0.04353990637022873, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.1441187560558319}, {"id": 3151, "seek": 1288364, "start": 12906.439999999999, "end": 12912.279999999999, "text": " it started to just kind of memorize that data and get good at seeing that data. Whereas now", "tokens": [51504, 309, 1409, 281, 445, 733, 295, 27478, 300, 1412, 293, 483, 665, 412, 2577, 300, 1412, 13, 13813, 586, 51796], "temperature": 0.0, "avg_logprob": -0.04353990637022873, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.1441187560558319}, {"id": 3152, "seek": 1291228, "start": 12912.28, "end": 12918.04, "text": " when we pass it new data that it's never seen before, it's only 88.5% accurate, which means", "tokens": [50364, 562, 321, 1320, 309, 777, 1412, 300, 309, 311, 1128, 1612, 949, 11, 309, 311, 787, 24587, 13, 20, 4, 8559, 11, 597, 1355, 50652], "temperature": 0.0, "avg_logprob": -0.0681445083618164, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0008830276783555746}, {"id": 3153, "seek": 1291228, "start": 12918.04, "end": 12923.16, "text": " we overfit our model. And it's not as good at generalizing for other data sets, which is usually", "tokens": [50652, 321, 670, 6845, 527, 2316, 13, 400, 309, 311, 406, 382, 665, 412, 2674, 3319, 337, 661, 1412, 6352, 11, 597, 307, 2673, 50908], "temperature": 0.0, "avg_logprob": -0.0681445083618164, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0008830276783555746}, {"id": 3154, "seek": 1291228, "start": 12923.16, "end": 12927.560000000001, "text": " the goal, right? When we create a model, we want the highest accuracy possible, but we want the", "tokens": [50908, 264, 3387, 11, 558, 30, 1133, 321, 1884, 257, 2316, 11, 321, 528, 264, 6343, 14170, 1944, 11, 457, 321, 528, 264, 51128], "temperature": 0.0, "avg_logprob": -0.0681445083618164, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0008830276783555746}, {"id": 3155, "seek": 1291228, "start": 12927.560000000001, "end": 12934.44, "text": " highest accuracy possible on new data. So we need to make sure our model generalizes properly.", "tokens": [51128, 6343, 14170, 1944, 322, 777, 1412, 13, 407, 321, 643, 281, 652, 988, 527, 2316, 2674, 5660, 6108, 13, 51472], "temperature": 0.0, "avg_logprob": -0.0681445083618164, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0008830276783555746}, {"id": 3156, "seek": 1291228, "start": 12934.44, "end": 12938.76, "text": " Now in this instance, you know, like, it's, it's hard to figure out how do we do that because", "tokens": [51472, 823, 294, 341, 5197, 11, 291, 458, 11, 411, 11, 309, 311, 11, 309, 311, 1152, 281, 2573, 484, 577, 360, 321, 360, 300, 570, 51688], "temperature": 0.0, "avg_logprob": -0.0681445083618164, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.0008830276783555746}, {"id": 3157, "seek": 1293876, "start": 12938.76, "end": 12943.08, "text": " we don't know that much about neural networks. But this is the idea of overfitting and of", "tokens": [50364, 321, 500, 380, 458, 300, 709, 466, 18161, 9590, 13, 583, 341, 307, 264, 1558, 295, 670, 69, 2414, 293, 295, 50580], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3158, "seek": 1293876, "start": 12943.08, "end": 12947.64, "text": " hyper parameter tuning, right? So if we can start changing some of this architecture, and we can", "tokens": [50580, 9848, 13075, 15164, 11, 558, 30, 407, 498, 321, 393, 722, 4473, 512, 295, 341, 9482, 11, 293, 321, 393, 50808], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3159, "seek": 1293876, "start": 12947.64, "end": 12952.44, "text": " change maybe the optimizer, the loss function, maybe we go epochs eight, let's see if this", "tokens": [50808, 1319, 1310, 264, 5028, 6545, 11, 264, 4470, 2445, 11, 1310, 321, 352, 30992, 28346, 3180, 11, 718, 311, 536, 498, 341, 51048], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3160, "seek": 1293876, "start": 12952.44, "end": 12957.800000000001, "text": " does any better, right? So let's now fit the model with eight epochs, we'll have a look at what this", "tokens": [51048, 775, 604, 1101, 11, 558, 30, 407, 718, 311, 586, 3318, 264, 2316, 365, 3180, 30992, 28346, 11, 321, 603, 362, 257, 574, 412, 437, 341, 51316], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3161, "seek": 1293876, "start": 12957.800000000001, "end": 12962.84, "text": " accuracy is. And then we'll test it and see if we get a higher accuracy on the testing data set.", "tokens": [51316, 14170, 307, 13, 400, 550, 321, 603, 1500, 309, 293, 536, 498, 321, 483, 257, 2946, 14170, 322, 264, 4997, 1412, 992, 13, 51568], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3162, "seek": 1293876, "start": 12962.84, "end": 12967.24, "text": " And this is kind of the idea of that hyper parameter tuning, right? Well, we just look at", "tokens": [51568, 400, 341, 307, 733, 295, 264, 1558, 295, 300, 9848, 13075, 15164, 11, 558, 30, 1042, 11, 321, 445, 574, 412, 51788], "temperature": 0.0, "avg_logprob": -0.07424546585602014, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.008846770040690899}, {"id": 3163, "seek": 1296724, "start": 12967.24, "end": 12972.68, "text": " each epoch, or not each epoch, we look at each parameter, we tweak them a little bit. And usually", "tokens": [50364, 1184, 30992, 339, 11, 420, 406, 1184, 30992, 339, 11, 321, 574, 412, 1184, 13075, 11, 321, 29879, 552, 257, 707, 857, 13, 400, 2673, 50636], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3164, "seek": 1296724, "start": 12972.68, "end": 12977.96, "text": " we'll like write some code that automates this for us. But that's the idea is we want to get the most", "tokens": [50636, 321, 603, 411, 2464, 512, 3089, 300, 3553, 1024, 341, 337, 505, 13, 583, 300, 311, 264, 1558, 307, 321, 528, 281, 483, 264, 881, 50900], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3165, "seek": 1296724, "start": 12977.96, "end": 12983.24, "text": " generalized accuracy that we can. So I'll wait for this to train. We're actually almost done. So I", "tokens": [50900, 44498, 14170, 300, 321, 393, 13, 407, 286, 603, 1699, 337, 341, 281, 3847, 13, 492, 434, 767, 1920, 1096, 13, 407, 286, 51164], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3166, "seek": 1296724, "start": 12983.24, "end": 12988.039999999999, "text": " won't even bother cutting the video. And then we'll run at this evaluation. And we'll see now if we", "tokens": [51164, 1582, 380, 754, 8677, 6492, 264, 960, 13, 400, 550, 321, 603, 1190, 412, 341, 13344, 13, 400, 321, 603, 536, 586, 498, 321, 51404], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3167, "seek": 1296724, "start": 12988.039999999999, "end": 12991.64, "text": " got a better accuracy. Now I'm getting a little bit scared because the accuracy is getting very", "tokens": [51404, 658, 257, 1101, 14170, 13, 823, 286, 478, 1242, 257, 707, 857, 5338, 570, 264, 14170, 307, 1242, 588, 51584], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3168, "seek": 1296724, "start": 12991.64, "end": 12996.039999999999, "text": " high here. And sometimes, you know, like you want the accuracy to be high on your training data.", "tokens": [51584, 1090, 510, 13, 400, 2171, 11, 291, 458, 11, 411, 291, 528, 264, 14170, 281, 312, 1090, 322, 428, 3097, 1412, 13, 51804], "temperature": 0.0, "avg_logprob": -0.07005204652485095, "compression_ratio": 1.8297213622291022, "no_speech_prob": 0.006903303321450949}, {"id": 3169, "seek": 1299604, "start": 12996.12, "end": 12999.720000000001, "text": " But when it gets to a point where it's very high, you're in a situation where it's likely", "tokens": [50368, 583, 562, 309, 2170, 281, 257, 935, 689, 309, 311, 588, 1090, 11, 291, 434, 294, 257, 2590, 689, 309, 311, 3700, 50548], "temperature": 0.0, "avg_logprob": -0.06975583062655684, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.001926644123159349}, {"id": 3170, "seek": 1299604, "start": 12999.720000000001, "end": 13005.240000000002, "text": " that you've overfit. So let's look at this now. And let's see what we get. So 88.4. So we actually", "tokens": [50548, 300, 291, 600, 670, 6845, 13, 407, 718, 311, 574, 412, 341, 586, 13, 400, 718, 311, 536, 437, 321, 483, 13, 407, 24587, 13, 19, 13, 407, 321, 767, 50824], "temperature": 0.0, "avg_logprob": -0.06975583062655684, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.001926644123159349}, {"id": 3171, "seek": 1299604, "start": 13005.240000000002, "end": 13009.160000000002, "text": " dropped down a little bit. And it seemed like those epochs didn't make a big difference. So maybe", "tokens": [50824, 8119, 760, 257, 707, 857, 13, 400, 309, 6576, 411, 729, 30992, 28346, 994, 380, 652, 257, 955, 2649, 13, 407, 1310, 51020], "temperature": 0.0, "avg_logprob": -0.06975583062655684, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.001926644123159349}, {"id": 3172, "seek": 1299604, "start": 13009.160000000002, "end": 13014.92, "text": " if I train it on one epoch, let's have an idea and see what this does. You know, make your prediction,", "tokens": [51020, 498, 286, 3847, 309, 322, 472, 30992, 339, 11, 718, 311, 362, 364, 1558, 293, 536, 437, 341, 775, 13, 509, 458, 11, 652, 428, 17630, 11, 51308], "temperature": 0.0, "avg_logprob": -0.06975583062655684, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.001926644123159349}, {"id": 3173, "seek": 1299604, "start": 13014.92, "end": 13018.44, "text": " you think we're going to be better, do you think we're going to be worse? It's only seen the training", "tokens": [51308, 291, 519, 321, 434, 516, 281, 312, 1101, 11, 360, 291, 519, 321, 434, 516, 281, 312, 5324, 30, 467, 311, 787, 1612, 264, 3097, 51484], "temperature": 0.0, "avg_logprob": -0.06975583062655684, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.001926644123159349}, {"id": 3174, "seek": 1301844, "start": 13018.44, "end": 13027.08, "text": " data one time. Let's run this. And let's see 89.34. So in this situation, less epochs was actually", "tokens": [50364, 1412, 472, 565, 13, 961, 311, 1190, 341, 13, 400, 718, 311, 536, 31877, 13, 12249, 13, 407, 294, 341, 2590, 11, 1570, 30992, 28346, 390, 767, 50796], "temperature": 0.0, "avg_logprob": -0.0673872781178308, "compression_ratio": 1.7392857142857143, "no_speech_prob": 0.24502475559711456}, {"id": 3175, "seek": 1301844, "start": 13027.08, "end": 13032.12, "text": " better. So that's something to consider. You know, a lot of people I see just go like 100 epochs and", "tokens": [50796, 1101, 13, 407, 300, 311, 746, 281, 1949, 13, 509, 458, 11, 257, 688, 295, 561, 286, 536, 445, 352, 411, 2319, 30992, 28346, 293, 51048], "temperature": 0.0, "avg_logprob": -0.0673872781178308, "compression_ratio": 1.7392857142857143, "no_speech_prob": 0.24502475559711456}, {"id": 3176, "seek": 1301844, "start": 13032.12, "end": 13036.76, "text": " just think their model is going to be great. That's actually not good to do. A lot of the", "tokens": [51048, 445, 519, 641, 2316, 307, 516, 281, 312, 869, 13, 663, 311, 767, 406, 665, 281, 360, 13, 316, 688, 295, 264, 51280], "temperature": 0.0, "avg_logprob": -0.0673872781178308, "compression_ratio": 1.7392857142857143, "no_speech_prob": 0.24502475559711456}, {"id": 3177, "seek": 1301844, "start": 13036.76, "end": 13040.04, "text": " times you're going to have a worse model because what's going to end up happening is it's going to", "tokens": [51280, 1413, 291, 434, 516, 281, 362, 257, 5324, 2316, 570, 437, 311, 516, 281, 917, 493, 2737, 307, 309, 311, 516, 281, 51444], "temperature": 0.0, "avg_logprob": -0.0673872781178308, "compression_ratio": 1.7392857142857143, "no_speech_prob": 0.24502475559711456}, {"id": 3178, "seek": 1301844, "start": 13040.04, "end": 13046.52, "text": " be seeing the same information so much tweaking so specifically to that information that it's seen", "tokens": [51444, 312, 2577, 264, 912, 1589, 370, 709, 6986, 2456, 370, 4682, 281, 300, 1589, 300, 309, 311, 1612, 51768], "temperature": 0.0, "avg_logprob": -0.0673872781178308, "compression_ratio": 1.7392857142857143, "no_speech_prob": 0.24502475559711456}, {"id": 3179, "seek": 1304652, "start": 13046.6, "end": 13050.92, "text": " that when you show it new data, it can't actually, you know, classify and generalize on that.", "tokens": [50368, 300, 562, 291, 855, 309, 777, 1412, 11, 309, 393, 380, 767, 11, 291, 458, 11, 33872, 293, 2674, 1125, 322, 300, 13, 50584], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3180, "seek": 1304652, "start": 13051.640000000001, "end": 13055.560000000001, "text": " All right. So let's go back and let's see what else we're doing now with this. Okay,", "tokens": [50620, 1057, 558, 13, 407, 718, 311, 352, 646, 293, 718, 311, 536, 437, 1646, 321, 434, 884, 586, 365, 341, 13, 1033, 11, 50816], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3181, "seek": 1304652, "start": 13055.560000000001, "end": 13059.32, "text": " so now that we've done that, we need to make predictions. So to make predictions is actually", "tokens": [50816, 370, 586, 300, 321, 600, 1096, 300, 11, 321, 643, 281, 652, 21264, 13, 407, 281, 652, 21264, 307, 767, 51004], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3182, "seek": 1304652, "start": 13059.32, "end": 13064.52, "text": " pretty easy. So I'm actually just going to copy this line in, we'll go into a new code block down", "tokens": [51004, 1238, 1858, 13, 407, 286, 478, 767, 445, 516, 281, 5055, 341, 1622, 294, 11, 321, 603, 352, 666, 257, 777, 3089, 3461, 760, 51264], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3183, "seek": 1304652, "start": 13064.52, "end": 13069.880000000001, "text": " here. So all you have to do is say model that predict, and then you're going to give it an array", "tokens": [51264, 510, 13, 407, 439, 291, 362, 281, 360, 307, 584, 2316, 300, 6069, 11, 293, 550, 291, 434, 516, 281, 976, 309, 364, 10225, 51532], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3184, "seek": 1304652, "start": 13069.880000000001, "end": 13074.52, "text": " of images that you want to predict on. So in this case, if we look at test images shape, so actually", "tokens": [51532, 295, 5267, 300, 291, 528, 281, 6069, 322, 13, 407, 294, 341, 1389, 11, 498, 321, 574, 412, 1500, 5267, 3909, 11, 370, 767, 51764], "temperature": 0.0, "avg_logprob": -0.0791176808507819, "compression_ratio": 1.8, "no_speech_prob": 0.005059926770627499}, {"id": 3185, "seek": 1307452, "start": 13074.52, "end": 13081.48, "text": " let's make a new code block and let's go here. So let's say test underscore images dot shape.", "tokens": [50364, 718, 311, 652, 257, 777, 3089, 3461, 293, 718, 311, 352, 510, 13, 407, 718, 311, 584, 1500, 37556, 5267, 5893, 3909, 13, 50712], "temperature": 0.0, "avg_logprob": -0.07045753871169046, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0021156184375286102}, {"id": 3186, "seek": 1307452, "start": 13083.640000000001, "end": 13090.04, "text": " All right, give me a second. So we have 10,000 by 28 by 28. So this is an array of 10,000 entries", "tokens": [50820, 1057, 558, 11, 976, 385, 257, 1150, 13, 407, 321, 362, 1266, 11, 1360, 538, 7562, 538, 7562, 13, 407, 341, 307, 364, 10225, 295, 1266, 11, 1360, 23041, 51140], "temperature": 0.0, "avg_logprob": -0.07045753871169046, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0021156184375286102}, {"id": 3187, "seek": 1307452, "start": 13090.04, "end": 13095.720000000001, "text": " of images. Now, if I just wanted to predict on one image, what I could do is say test images,", "tokens": [51140, 295, 5267, 13, 823, 11, 498, 286, 445, 1415, 281, 6069, 322, 472, 3256, 11, 437, 286, 727, 360, 307, 584, 1500, 5267, 11, 51424], "temperature": 0.0, "avg_logprob": -0.07045753871169046, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0021156184375286102}, {"id": 3188, "seek": 1307452, "start": 13095.720000000001, "end": 13101.48, "text": " zero and then put that inside of an array. The reason I need to do that is because the data", "tokens": [51424, 4018, 293, 550, 829, 300, 1854, 295, 364, 10225, 13, 440, 1778, 286, 643, 281, 360, 300, 307, 570, 264, 1412, 51712], "temperature": 0.0, "avg_logprob": -0.07045753871169046, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0021156184375286102}, {"id": 3189, "seek": 1310148, "start": 13101.48, "end": 13106.84, "text": " that this model is used to see in is an array of images to make a prediction on, that's what this", "tokens": [50364, 300, 341, 2316, 307, 1143, 281, 536, 294, 307, 364, 10225, 295, 5267, 281, 652, 257, 17630, 322, 11, 300, 311, 437, 341, 50632], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3190, "seek": 1310148, "start": 13106.84, "end": 13111.72, "text": " predict method needs. And it's much better at making predictions on many things at once than", "tokens": [50632, 6069, 3170, 2203, 13, 400, 309, 311, 709, 1101, 412, 1455, 21264, 322, 867, 721, 412, 1564, 813, 50876], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3191, "seek": 1310148, "start": 13111.72, "end": 13117.56, "text": " just one specific item. So if you are predicting one item only, you do need to put it in an array,", "tokens": [50876, 445, 472, 2685, 3174, 13, 407, 498, 291, 366, 32884, 472, 3174, 787, 11, 291, 360, 643, 281, 829, 309, 294, 364, 10225, 11, 51168], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3192, "seek": 1310148, "start": 13117.56, "end": 13121.32, "text": " because it's used to seeing that form. So we could do this. I'm, I mean, I'm just going to leave it.", "tokens": [51168, 570, 309, 311, 1143, 281, 2577, 300, 1254, 13, 407, 321, 727, 360, 341, 13, 286, 478, 11, 286, 914, 11, 286, 478, 445, 516, 281, 1856, 309, 13, 51356], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3193, "seek": 1310148, "start": 13121.32, "end": 13124.92, "text": " So we're just going to predict on every single one of the test images, because then we can have", "tokens": [51356, 407, 321, 434, 445, 516, 281, 6069, 322, 633, 2167, 472, 295, 264, 1500, 5267, 11, 570, 550, 321, 393, 362, 51536], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3194, "seek": 1310148, "start": 13124.92, "end": 13129.16, "text": " a look at a cool function I've kind of made. So let's actually do this predictions equals model", "tokens": [51536, 257, 574, 412, 257, 1627, 2445, 286, 600, 733, 295, 1027, 13, 407, 718, 311, 767, 360, 341, 21264, 6915, 2316, 51748], "temperature": 0.0, "avg_logprob": -0.07159470240275065, "compression_ratio": 1.8957654723127035, "no_speech_prob": 0.01590500958263874}, {"id": 3195, "seek": 1312916, "start": 13129.24, "end": 13135.64, "text": " dot predict test images. I mean, let's print predictions. And look at actually what it is.", "tokens": [50368, 5893, 6069, 1500, 5267, 13, 286, 914, 11, 718, 311, 4482, 21264, 13, 400, 574, 412, 767, 437, 309, 307, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11660978794097901, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0064876447431743145}, {"id": 3196, "seek": 1312916, "start": 13136.28, "end": 13141.32, "text": " Where is my autocomplete? There it is. Okay. So let's have a look. Is this some object? Whoa,", "tokens": [50720, 2305, 307, 452, 45833, 298, 17220, 30, 821, 309, 307, 13, 1033, 13, 407, 718, 311, 362, 257, 574, 13, 1119, 341, 512, 2657, 30, 7521, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11660978794097901, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0064876447431743145}, {"id": 3197, "seek": 1312916, "start": 13141.32, "end": 13147.8, "text": " okay. So this is a raise of arrays that looks like we have some like really tiny numbers in them.", "tokens": [50972, 1392, 13, 407, 341, 307, 257, 5300, 295, 41011, 300, 1542, 411, 321, 362, 512, 411, 534, 5870, 3547, 294, 552, 13, 51296], "temperature": 0.0, "avg_logprob": -0.11660978794097901, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0064876447431743145}, {"id": 3198, "seek": 1312916, "start": 13147.8, "end": 13153.8, "text": " So what this is, is essentially every single, you know, prediction or every single image has", "tokens": [51296, 407, 437, 341, 307, 11, 307, 4476, 633, 2167, 11, 291, 458, 11, 17630, 420, 633, 2167, 3256, 575, 51596], "temperature": 0.0, "avg_logprob": -0.11660978794097901, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0064876447431743145}, {"id": 3199, "seek": 1312916, "start": 13153.8, "end": 13157.4, "text": " a list that represents the prediction for it, just like we've done with kind of the linear", "tokens": [51596, 257, 1329, 300, 8855, 264, 17630, 337, 309, 11, 445, 411, 321, 600, 1096, 365, 733, 295, 264, 8213, 51776], "temperature": 0.0, "avg_logprob": -0.11660978794097901, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0064876447431743145}, {"id": 3200, "seek": 1315740, "start": 13157.4, "end": 13162.359999999999, "text": " models and stuff like that. So if I want to see the prediction for test image zero, I would say", "tokens": [50364, 5245, 293, 1507, 411, 300, 13, 407, 498, 286, 528, 281, 536, 264, 17630, 337, 1500, 3256, 4018, 11, 286, 576, 584, 50612], "temperature": 0.0, "avg_logprob": -0.07291249332264957, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001926642027683556}, {"id": 3201, "seek": 1315740, "start": 13162.359999999999, "end": 13167.72, "text": " prediction zero, right? Let's print this out. And this is the array that we're getting. These", "tokens": [50612, 17630, 4018, 11, 558, 30, 961, 311, 4482, 341, 484, 13, 400, 341, 307, 264, 10225, 300, 321, 434, 1242, 13, 1981, 50880], "temperature": 0.0, "avg_logprob": -0.07291249332264957, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001926642027683556}, {"id": 3202, "seek": 1315740, "start": 13167.72, "end": 13173.96, "text": " this is the probability distribution that was calculated on our output layer for, you know,", "tokens": [50880, 341, 307, 264, 8482, 7316, 300, 390, 15598, 322, 527, 5598, 4583, 337, 11, 291, 458, 11, 51192], "temperature": 0.0, "avg_logprob": -0.07291249332264957, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001926642027683556}, {"id": 3203, "seek": 1315740, "start": 13173.96, "end": 13179.08, "text": " these, what is it for that image? So if we want to figure out what class we actually think that", "tokens": [51192, 613, 11, 437, 307, 309, 337, 300, 3256, 30, 407, 498, 321, 528, 281, 2573, 484, 437, 1508, 321, 767, 519, 300, 51448], "temperature": 0.0, "avg_logprob": -0.07291249332264957, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001926642027683556}, {"id": 3204, "seek": 1315740, "start": 13179.08, "end": 13184.84, "text": " this is predicting for, we can use a cool function from NumPy called arg max, which essentially is", "tokens": [51448, 341, 307, 32884, 337, 11, 321, 393, 764, 257, 1627, 2445, 490, 22592, 47, 88, 1219, 3882, 11469, 11, 597, 4476, 307, 51736], "temperature": 0.0, "avg_logprob": -0.07291249332264957, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001926642027683556}, {"id": 3205, "seek": 1318484, "start": 13184.92, "end": 13190.2, "text": " just going to take the index, this is going to return to us the index of the maximum value in", "tokens": [50368, 445, 516, 281, 747, 264, 8186, 11, 341, 307, 516, 281, 2736, 281, 505, 264, 8186, 295, 264, 6674, 2158, 294, 50632], "temperature": 0.0, "avg_logprob": -0.0682789787413582, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.011686344631016254}, {"id": 3206, "seek": 1318484, "start": 13190.2, "end": 13195.0, "text": " this list. So let's say that it was I'm looking for the least negative, which I believe is this,", "tokens": [50632, 341, 1329, 13, 407, 718, 311, 584, 300, 309, 390, 286, 478, 1237, 337, 264, 1935, 3671, 11, 597, 286, 1697, 307, 341, 11, 50872], "temperature": 0.0, "avg_logprob": -0.0682789787413582, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.011686344631016254}, {"id": 3207, "seek": 1318484, "start": 13195.0, "end": 13200.76, "text": " so this should be nine, this should return to us nine, because this is the index of the highest", "tokens": [50872, 370, 341, 820, 312, 4949, 11, 341, 820, 2736, 281, 505, 4949, 11, 570, 341, 307, 264, 8186, 295, 264, 6343, 51160], "temperature": 0.0, "avg_logprob": -0.0682789787413582, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.011686344631016254}, {"id": 3208, "seek": 1318484, "start": 13200.76, "end": 13206.28, "text": " value in this list. Unless I'm just wrong when I'm looking at the negatives here. So nine, that's", "tokens": [51160, 2158, 294, 341, 1329, 13, 16581, 286, 478, 445, 2085, 562, 286, 478, 1237, 412, 264, 40019, 510, 13, 407, 4949, 11, 300, 311, 51436], "temperature": 0.0, "avg_logprob": -0.0682789787413582, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.011686344631016254}, {"id": 3209, "seek": 1318484, "start": 13206.28, "end": 13212.2, "text": " what we got. Okay, so now if we want to see what the actual classes, while we have our class names", "tokens": [51436, 437, 321, 658, 13, 1033, 11, 370, 586, 498, 321, 528, 281, 536, 437, 264, 3539, 5359, 11, 1339, 321, 362, 527, 1508, 5288, 51732], "temperature": 0.0, "avg_logprob": -0.0682789787413582, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.011686344631016254}, {"id": 3210, "seek": 1321220, "start": 13212.2, "end": 13216.84, "text": " up here, so we know class nine is actually ankle boot. So let's see if this is actually an ankle", "tokens": [50364, 493, 510, 11, 370, 321, 458, 1508, 4949, 307, 767, 21999, 11450, 13, 407, 718, 311, 536, 498, 341, 307, 767, 364, 21999, 50596], "temperature": 0.0, "avg_logprob": -0.07129686528986151, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.008315389975905418}, {"id": 3211, "seek": 1321220, "start": 13216.84, "end": 13223.320000000002, "text": " boot. So I'm just going to do class underscore names, I think that's what I called it, like this,", "tokens": [50596, 11450, 13, 407, 286, 478, 445, 516, 281, 360, 1508, 37556, 5288, 11, 286, 519, 300, 311, 437, 286, 1219, 309, 11, 411, 341, 11, 50920], "temperature": 0.0, "avg_logprob": -0.07129686528986151, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.008315389975905418}, {"id": 3212, "seek": 1321220, "start": 13223.320000000002, "end": 13228.52, "text": " so that should print out what it thinks it is. Yeah, class underscore names. But now let's actually", "tokens": [50920, 370, 300, 820, 4482, 484, 437, 309, 7309, 309, 307, 13, 865, 11, 1508, 37556, 5288, 13, 583, 586, 718, 311, 767, 51180], "temperature": 0.0, "avg_logprob": -0.07129686528986151, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.008315389975905418}, {"id": 3213, "seek": 1321220, "start": 13228.52, "end": 13233.560000000001, "text": " show the image of this prediction. So to do that, I'm just going to steal some code from here because", "tokens": [51180, 855, 264, 3256, 295, 341, 17630, 13, 407, 281, 360, 300, 11, 286, 478, 445, 516, 281, 11009, 512, 3089, 490, 510, 570, 51432], "temperature": 0.0, "avg_logprob": -0.07129686528986151, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.008315389975905418}, {"id": 3214, "seek": 1321220, "start": 13234.36, "end": 13237.960000000001, "text": " I don't remember all the syntax off the top of my head. So this", "tokens": [51472, 286, 500, 380, 1604, 439, 264, 28431, 766, 264, 1192, 295, 452, 1378, 13, 407, 341, 51652], "temperature": 0.0, "avg_logprob": -0.07129686528986151, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.008315389975905418}, {"id": 3215, "seek": 1323796, "start": 13238.759999999998, "end": 13243.64, "text": " is what it looks like. So let's steal this figure. Let's show this and let's see if it actually looks", "tokens": [50404, 307, 437, 309, 1542, 411, 13, 407, 718, 311, 11009, 341, 2573, 13, 961, 311, 855, 341, 293, 718, 311, 536, 498, 309, 767, 1542, 50648], "temperature": 0.0, "avg_logprob": -0.21897243529327157, "compression_ratio": 1.8661710037174721, "no_speech_prob": 0.006487826816737652}, {"id": 3216, "seek": 1323796, "start": 13243.64, "end": 13249.64, "text": " like an ankle boot. So to do that, we're going to say test underscore images zero, because obviously", "tokens": [50648, 411, 364, 21999, 11450, 13, 407, 281, 360, 300, 11, 321, 434, 516, 281, 584, 1500, 37556, 5267, 4018, 11, 570, 2745, 50948], "temperature": 0.0, "avg_logprob": -0.21897243529327157, "compression_ratio": 1.8661710037174721, "no_speech_prob": 0.006487826816737652}, {"id": 3217, "seek": 1323796, "start": 13249.64, "end": 13255.24, "text": " image zero corresponds to predict prediction zero. And that will show this and see what we get. Okay,", "tokens": [50948, 3256, 4018, 23249, 281, 6069, 17630, 4018, 13, 400, 300, 486, 855, 341, 293, 536, 437, 321, 483, 13, 1033, 11, 51228], "temperature": 0.0, "avg_logprob": -0.21897243529327157, "compression_ratio": 1.8661710037174721, "no_speech_prob": 0.006487826816737652}, {"id": 3218, "seek": 1323796, "start": 13255.24, "end": 13260.599999999999, "text": " so ankle boot, and we'll be looking at the image is actually an ankle boot. And we can do this for", "tokens": [51228, 370, 21999, 11450, 11, 293, 321, 603, 312, 1237, 412, 264, 3256, 307, 767, 364, 21999, 11450, 13, 400, 321, 393, 360, 341, 337, 51496], "temperature": 0.0, "avg_logprob": -0.21897243529327157, "compression_ratio": 1.8661710037174721, "no_speech_prob": 0.006487826816737652}, {"id": 3219, "seek": 1323796, "start": 13260.599999999999, "end": 13266.119999999999, "text": " any of the images that we want, right? So if I do prediction one, prediction one, now let's have a", "tokens": [51496, 604, 295, 264, 5267, 300, 321, 528, 11, 558, 30, 407, 498, 286, 360, 17630, 472, 11, 17630, 472, 11, 586, 718, 311, 362, 257, 51772], "temperature": 0.0, "avg_logprob": -0.21897243529327157, "compression_ratio": 1.8661710037174721, "no_speech_prob": 0.006487826816737652}, {"id": 3220, "seek": 1326612, "start": 13266.12, "end": 13270.76, "text": " look pull over kind of looks like a pull over to me. I mean, I don't know if it actually is,", "tokens": [50364, 574, 2235, 670, 733, 295, 1542, 411, 257, 2235, 670, 281, 385, 13, 286, 914, 11, 286, 500, 380, 458, 498, 309, 767, 307, 11, 50596], "temperature": 0.0, "avg_logprob": -0.1068971138874083, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.0066923946142196655}, {"id": 3221, "seek": 1326612, "start": 13270.76, "end": 13279.08, "text": " but that's what it looks like. You do to to have a look here. Okay, trouser. Yep, looks like trousers", "tokens": [50596, 457, 300, 311, 437, 309, 1542, 411, 13, 509, 360, 281, 281, 362, 257, 574, 510, 13, 1033, 11, 34156, 260, 13, 7010, 11, 1542, 411, 41463, 51012], "temperature": 0.0, "avg_logprob": -0.1068971138874083, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.0066923946142196655}, {"id": 3222, "seek": 1326612, "start": 13279.08, "end": 13283.160000000002, "text": " to me. And we can see that that is how we get the predictions from our model, we use model dot", "tokens": [51012, 281, 385, 13, 400, 321, 393, 536, 300, 300, 307, 577, 321, 483, 264, 21264, 490, 527, 2316, 11, 321, 764, 2316, 5893, 51216], "temperature": 0.0, "avg_logprob": -0.1068971138874083, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.0066923946142196655}, {"id": 3223, "seek": 1326612, "start": 13283.160000000002, "end": 13289.400000000001, "text": " predict. Alright, so let's move down here now to the next thing that we did. Alright, so we've", "tokens": [51216, 6069, 13, 2798, 11, 370, 718, 311, 1286, 760, 510, 586, 281, 264, 958, 551, 300, 321, 630, 13, 2798, 11, 370, 321, 600, 51528], "temperature": 0.0, "avg_logprob": -0.1068971138874083, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.0066923946142196655}, {"id": 3224, "seek": 1326612, "start": 13289.400000000001, "end": 13293.720000000001, "text": " already done that. So verifying predictions. Okay, so this is actually a cool kind of script", "tokens": [51528, 1217, 1096, 300, 13, 407, 1306, 5489, 21264, 13, 1033, 11, 370, 341, 307, 767, 257, 1627, 733, 295, 5755, 51744], "temperature": 0.0, "avg_logprob": -0.1068971138874083, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.0066923946142196655}, {"id": 3225, "seek": 1329372, "start": 13293.72, "end": 13299.16, "text": " that I wrote, I'll zoom out a little bit so we can read it. What this does is let us use our model", "tokens": [50364, 300, 286, 4114, 11, 286, 603, 8863, 484, 257, 707, 857, 370, 321, 393, 1401, 309, 13, 708, 341, 775, 307, 718, 505, 764, 527, 2316, 50636], "temperature": 0.0, "avg_logprob": -0.07730776977539063, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.05499555915594101}, {"id": 3226, "seek": 1329372, "start": 13299.16, "end": 13304.519999999999, "text": " to actually make. And I've stolen some of this from TensorFlow to make predictions on any", "tokens": [50636, 281, 767, 652, 13, 400, 286, 600, 15900, 512, 295, 341, 490, 37624, 281, 652, 21264, 322, 604, 50904], "temperature": 0.0, "avg_logprob": -0.07730776977539063, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.05499555915594101}, {"id": 3227, "seek": 1329372, "start": 13304.519999999999, "end": 13308.84, "text": " entry that we want. So what it's going to do is ask us to type in some number, we're going to type", "tokens": [50904, 8729, 300, 321, 528, 13, 407, 437, 309, 311, 516, 281, 360, 307, 1029, 505, 281, 2010, 294, 512, 1230, 11, 321, 434, 516, 281, 2010, 51120], "temperature": 0.0, "avg_logprob": -0.07730776977539063, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.05499555915594101}, {"id": 3228, "seek": 1329372, "start": 13308.84, "end": 13313.24, "text": " in that number, it's going to find that image in the test data set, it's going to make your", "tokens": [51120, 294, 300, 1230, 11, 309, 311, 516, 281, 915, 300, 3256, 294, 264, 1500, 1412, 992, 11, 309, 311, 516, 281, 652, 428, 51340], "temperature": 0.0, "avg_logprob": -0.07730776977539063, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.05499555915594101}, {"id": 3229, "seek": 1329372, "start": 13313.24, "end": 13318.599999999999, "text": " prediction on that from the model, and then show us what it actually is versus what it was predicted", "tokens": [51340, 17630, 322, 300, 490, 264, 2316, 11, 293, 550, 855, 505, 437, 309, 767, 307, 5717, 437, 309, 390, 19147, 51608], "temperature": 0.0, "avg_logprob": -0.07730776977539063, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.05499555915594101}, {"id": 3230, "seek": 1331860, "start": 13318.6, "end": 13323.56, "text": " being. Now, I just need to actually run. Actually, let's just steal this code and bring it in the", "tokens": [50364, 885, 13, 823, 11, 286, 445, 643, 281, 767, 1190, 13, 5135, 11, 718, 311, 445, 11009, 341, 3089, 293, 1565, 309, 294, 264, 50612], "temperature": 0.0, "avg_logprob": -0.09970273362829331, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.05833766236901283}, {"id": 3231, "seek": 1331860, "start": 13323.56, "end": 13326.84, "text": " other one, because I've already trained the model there. So we don't have to wait again. So let's", "tokens": [50612, 661, 472, 11, 570, 286, 600, 1217, 8895, 264, 2316, 456, 13, 407, 321, 500, 380, 362, 281, 1699, 797, 13, 407, 718, 311, 50776], "temperature": 0.0, "avg_logprob": -0.09970273362829331, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.05833766236901283}, {"id": 3232, "seek": 1331860, "start": 13326.84, "end": 13335.24, "text": " go f 11, f 11, let's go to a new code block, and run that. So let's run this script. Have a look", "tokens": [50776, 352, 283, 2975, 11, 283, 2975, 11, 718, 311, 352, 281, 257, 777, 3089, 3461, 11, 293, 1190, 300, 13, 407, 718, 311, 1190, 341, 5755, 13, 3560, 257, 574, 51196], "temperature": 0.0, "avg_logprob": -0.09970273362829331, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.05833766236901283}, {"id": 3233, "seek": 1331860, "start": 13335.24, "end": 13339.880000000001, "text": " down here. So pick a number, we'll pick some number, let's go 45. And then what it's going to do is", "tokens": [51196, 760, 510, 13, 407, 1888, 257, 1230, 11, 321, 603, 1888, 512, 1230, 11, 718, 311, 352, 6905, 13, 400, 550, 437, 309, 311, 516, 281, 360, 307, 51428], "temperature": 0.0, "avg_logprob": -0.09970273362829331, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.05833766236901283}, {"id": 3234, "seek": 1331860, "start": 13339.880000000001, "end": 13345.32, "text": " say expected sneaker, guess sneaker, and actually show us the image that's there. So we can see", "tokens": [51428, 584, 5176, 9244, 4003, 11, 2041, 9244, 4003, 11, 293, 767, 855, 505, 264, 3256, 300, 311, 456, 13, 407, 321, 393, 536, 51700], "temperature": 0.0, "avg_logprob": -0.09970273362829331, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.05833766236901283}, {"id": 3235, "seek": 1334532, "start": 13345.32, "end": 13350.36, "text": " this is what you know, our pixel kind of data looks like. And this is what the expected was,", "tokens": [50364, 341, 307, 437, 291, 458, 11, 527, 19261, 733, 295, 1412, 1542, 411, 13, 400, 341, 307, 437, 264, 5176, 390, 11, 50616], "temperature": 0.0, "avg_logprob": -0.06525284903390068, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02930780127644539}, {"id": 3236, "seek": 1334532, "start": 13350.36, "end": 13354.119999999999, "text": " and this is what the guess was from the neural network. Now we can do the same thing if we run", "tokens": [50616, 293, 341, 307, 437, 264, 2041, 390, 490, 264, 18161, 3209, 13, 823, 321, 393, 360, 264, 912, 551, 498, 321, 1190, 50804], "temperature": 0.0, "avg_logprob": -0.06525284903390068, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02930780127644539}, {"id": 3237, "seek": 1334532, "start": 13354.119999999999, "end": 13361.0, "text": " it again, pick a number 34. Let's see here, expected bag, guess bag. So that's kind of showing you", "tokens": [50804, 309, 797, 11, 1888, 257, 1230, 12790, 13, 961, 311, 536, 510, 11, 5176, 3411, 11, 2041, 3411, 13, 407, 300, 311, 733, 295, 4099, 291, 51148], "temperature": 0.0, "avg_logprob": -0.06525284903390068, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02930780127644539}, {"id": 3238, "seek": 1334532, "start": 13361.0, "end": 13367.48, "text": " how we can actually use this model. So anyways, that has been it for this kind of module on neural", "tokens": [51148, 577, 321, 393, 767, 764, 341, 2316, 13, 407, 13448, 11, 300, 575, 668, 309, 337, 341, 733, 295, 10088, 322, 18161, 51472], "temperature": 0.0, "avg_logprob": -0.06525284903390068, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02930780127644539}, {"id": 3239, "seek": 1334532, "start": 13367.48, "end": 13372.119999999999, "text": " networks. Now I did this in about an hour, I'm hoping I explained a good amount that you guys", "tokens": [51472, 9590, 13, 823, 286, 630, 341, 294, 466, 364, 1773, 11, 286, 478, 7159, 286, 8825, 257, 665, 2372, 300, 291, 1074, 51704], "temperature": 0.0, "avg_logprob": -0.06525284903390068, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02930780127644539}, {"id": 3240, "seek": 1337212, "start": 13372.12, "end": 13376.76, "text": " understand now how neural networks work. In the next module, we're going to move on to convolutional", "tokens": [50364, 1223, 586, 577, 18161, 9590, 589, 13, 682, 264, 958, 10088, 11, 321, 434, 516, 281, 1286, 322, 281, 45216, 304, 50596], "temperature": 0.0, "avg_logprob": -0.06443746001632125, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.008846520446240902}, {"id": 3241, "seek": 1337212, "start": 13376.76, "end": 13380.76, "text": " neural networks, which again should help, you know, kind of get your understanding of neural", "tokens": [50596, 18161, 9590, 11, 597, 797, 820, 854, 11, 291, 458, 11, 733, 295, 483, 428, 3701, 295, 18161, 50796], "temperature": 0.0, "avg_logprob": -0.06443746001632125, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.008846520446240902}, {"id": 3242, "seek": 1337212, "start": 13380.76, "end": 13385.480000000001, "text": " networks up as well as learn how we can do deep computer vision, object recognition and detection", "tokens": [50796, 9590, 493, 382, 731, 382, 1466, 577, 321, 393, 360, 2452, 3820, 5201, 11, 2657, 11150, 293, 17784, 51032], "temperature": 0.0, "avg_logprob": -0.06443746001632125, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.008846520446240902}, {"id": 3243, "seek": 1337212, "start": 13385.480000000001, "end": 13389.640000000001, "text": " using convolutional neural networks. So that being said, let's get into the next module.", "tokens": [51032, 1228, 45216, 304, 18161, 9590, 13, 407, 300, 885, 848, 11, 718, 311, 483, 666, 264, 958, 10088, 13, 51240], "temperature": 0.0, "avg_logprob": -0.06443746001632125, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.008846520446240902}, {"id": 3244, "seek": 1337212, "start": 13392.92, "end": 13397.720000000001, "text": " Hello, everyone, and welcome to the next module in this TensorFlow course. So what we're going", "tokens": [51404, 2425, 11, 1518, 11, 293, 2928, 281, 264, 958, 10088, 294, 341, 37624, 1164, 13, 407, 437, 321, 434, 516, 51644], "temperature": 0.0, "avg_logprob": -0.06443746001632125, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.008846520446240902}, {"id": 3245, "seek": 1339772, "start": 13397.72, "end": 13402.199999999999, "text": " to be doing here is talking about deep computer vision, which is very exciting, very cool. This", "tokens": [50364, 281, 312, 884, 510, 307, 1417, 466, 2452, 3820, 5201, 11, 597, 307, 588, 4670, 11, 588, 1627, 13, 639, 50588], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3246, "seek": 1339772, "start": 13402.199999999999, "end": 13406.519999999999, "text": " has been used for all kinds of things you ever seen the self driving cars, for example, Tesla,", "tokens": [50588, 575, 668, 1143, 337, 439, 3685, 295, 721, 291, 1562, 1612, 264, 2698, 4840, 5163, 11, 337, 1365, 11, 13666, 11, 50804], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3247, "seek": 1339772, "start": 13406.519999999999, "end": 13412.76, "text": " they actually use a TensorFlow deep learning model, obviously very complicated, more than I can", "tokens": [50804, 436, 767, 764, 257, 37624, 2452, 2539, 2316, 11, 2745, 588, 6179, 11, 544, 813, 286, 393, 51116], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3248, "seek": 1339772, "start": 13412.76, "end": 13417.24, "text": " really explain here to do a lot of their computer vision for self driving, we've used computer vision", "tokens": [51116, 534, 2903, 510, 281, 360, 257, 688, 295, 641, 3820, 5201, 337, 2698, 4840, 11, 321, 600, 1143, 3820, 5201, 51340], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3249, "seek": 1339772, "start": 13417.24, "end": 13421.8, "text": " in the medicine field, computer vision is actually used in sports a lot for things like goal line", "tokens": [51340, 294, 264, 7195, 2519, 11, 3820, 5201, 307, 767, 1143, 294, 6573, 257, 688, 337, 721, 411, 3387, 1622, 51568], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3250, "seek": 1339772, "start": 13421.8, "end": 13426.599999999999, "text": " technology and even detecting images and players on the field doing analysis, there's lots of cool", "tokens": [51568, 2899, 293, 754, 40237, 5267, 293, 4150, 322, 264, 2519, 884, 5215, 11, 456, 311, 3195, 295, 1627, 51808], "temperature": 0.0, "avg_logprob": -0.0962433704110079, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.06186380237340927}, {"id": 3251, "seek": 1342660, "start": 13426.68, "end": 13430.44, "text": " things are doing with it nowadays. And for our purposes, what we're going to be doing is using", "tokens": [50368, 721, 366, 884, 365, 309, 13434, 13, 400, 337, 527, 9932, 11, 437, 321, 434, 516, 281, 312, 884, 307, 1228, 50556], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3252, "seek": 1342660, "start": 13430.44, "end": 13435.960000000001, "text": " this for to perform classification, although it can be used for object detection and recognition,", "tokens": [50556, 341, 337, 281, 2042, 21538, 11, 4878, 309, 393, 312, 1143, 337, 2657, 17784, 293, 11150, 11, 50832], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3253, "seek": 1342660, "start": 13435.960000000001, "end": 13440.36, "text": " as well as facial detection and recognition as well. So all kinds of applications, in my opinion,", "tokens": [50832, 382, 731, 382, 15642, 17784, 293, 11150, 382, 731, 13, 407, 439, 3685, 295, 5821, 11, 294, 452, 4800, 11, 51052], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3254, "seek": 1342660, "start": 13440.36, "end": 13445.16, "text": " one of the cooler things in deep learning that we're doing right now. And let's go ahead and talk", "tokens": [51052, 472, 295, 264, 15566, 721, 294, 2452, 2539, 300, 321, 434, 884, 558, 586, 13, 400, 718, 311, 352, 2286, 293, 751, 51292], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3255, "seek": 1342660, "start": 13445.16, "end": 13448.68, "text": " about what we're actually going to be focusing on here. So we're going to start by discussing what", "tokens": [51292, 466, 437, 321, 434, 767, 516, 281, 312, 8416, 322, 510, 13, 407, 321, 434, 516, 281, 722, 538, 10850, 437, 51468], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3256, "seek": 1342660, "start": 13448.68, "end": 13453.56, "text": " a convolutional neural network is, which is essentially the way that we do deep learning,", "tokens": [51468, 257, 45216, 304, 18161, 3209, 307, 11, 597, 307, 4476, 264, 636, 300, 321, 360, 2452, 2539, 11, 51712], "temperature": 0.0, "avg_logprob": -0.046329028624341004, "compression_ratio": 1.9105960264900663, "no_speech_prob": 0.0015977623406797647}, {"id": 3257, "seek": 1345356, "start": 13453.64, "end": 13457.08, "text": " we're going to learn about image data. So what's the difference between image data and other", "tokens": [50368, 321, 434, 516, 281, 1466, 466, 3256, 1412, 13, 407, 437, 311, 264, 2649, 1296, 3256, 1412, 293, 661, 50540], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3258, "seek": 1345356, "start": 13457.08, "end": 13461.8, "text": " regular data? We're going to talk about convolutional layers and pooling layers and how stacks of those", "tokens": [50540, 3890, 1412, 30, 492, 434, 516, 281, 751, 466, 45216, 304, 7914, 293, 7005, 278, 7914, 293, 577, 30792, 295, 729, 50776], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3259, "seek": 1345356, "start": 13461.8, "end": 13466.76, "text": " work together as what we call a convolutional base for our convolutional neural network. We're", "tokens": [50776, 589, 1214, 382, 437, 321, 818, 257, 45216, 304, 3096, 337, 527, 45216, 304, 18161, 3209, 13, 492, 434, 51024], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3260, "seek": 1345356, "start": 13466.76, "end": 13471.8, "text": " going to talk about CNN architectures and get into actually using pre trained models that have been", "tokens": [51024, 516, 281, 751, 466, 24859, 6331, 1303, 293, 483, 666, 767, 1228, 659, 8895, 5245, 300, 362, 668, 51276], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3261, "seek": 1345356, "start": 13471.8, "end": 13476.039999999999, "text": " developed by companies such as Google and TensorFlow themselves to perform classification", "tokens": [51276, 4743, 538, 3431, 1270, 382, 3329, 293, 37624, 2969, 281, 2042, 21538, 51488], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3262, "seek": 1345356, "start": 13476.039999999999, "end": 13480.84, "text": " tasks for us. So that is pretty much the breakdown of what we're about to learn. There's quite a", "tokens": [51488, 9608, 337, 505, 13, 407, 300, 307, 1238, 709, 264, 18188, 295, 437, 321, 434, 466, 281, 1466, 13, 821, 311, 1596, 257, 51728], "temperature": 0.0, "avg_logprob": -0.06784725946093363, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.0035932492464780807}, {"id": 3263, "seek": 1348084, "start": 13480.84, "end": 13484.52, "text": " bit in this module, it's probably the more difficult one or the most difficult one we've", "tokens": [50364, 857, 294, 341, 10088, 11, 309, 311, 1391, 264, 544, 2252, 472, 420, 264, 881, 2252, 472, 321, 600, 50548], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3264, "seek": 1348084, "start": 13484.52, "end": 13488.2, "text": " been doing so far. So if you do get lost at any point, or you don't understand some of it,", "tokens": [50548, 668, 884, 370, 1400, 13, 407, 498, 291, 360, 483, 2731, 412, 604, 935, 11, 420, 291, 500, 380, 1223, 512, 295, 309, 11, 50732], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3265, "seek": 1348084, "start": 13488.2, "end": 13492.36, "text": " don't feel bad, this stuff is very difficult. And I would obviously recommend reading through", "tokens": [50732, 500, 380, 841, 1578, 11, 341, 1507, 307, 588, 2252, 13, 400, 286, 576, 2745, 2748, 3760, 807, 50940], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3266, "seek": 1348084, "start": 13492.36, "end": 13495.960000000001, "text": " some of the descriptions I have here in this notebook, which again, you can find from the", "tokens": [50940, 512, 295, 264, 24406, 286, 362, 510, 294, 341, 21060, 11, 597, 797, 11, 291, 393, 915, 490, 264, 51120], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3267, "seek": 1348084, "start": 13495.960000000001, "end": 13500.36, "text": " link in the description or looking up some things that maybe I don't go into enough enough depth", "tokens": [51120, 2113, 294, 264, 3855, 420, 1237, 493, 512, 721, 300, 1310, 286, 500, 380, 352, 666, 1547, 1547, 7161, 51340], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3268, "seek": 1348084, "start": 13500.36, "end": 13505.56, "text": " about in your own time, as I can't really spend, you know, 10, 11 hours explaining a convolutional", "tokens": [51340, 466, 294, 428, 1065, 565, 11, 382, 286, 393, 380, 534, 3496, 11, 291, 458, 11, 1266, 11, 2975, 2496, 13468, 257, 45216, 304, 51600], "temperature": 0.0, "avg_logprob": -0.0695294637749665, "compression_ratio": 1.72, "no_speech_prob": 0.08507998287677765}, {"id": 3269, "seek": 1350556, "start": 13505.64, "end": 13511.0, "text": " neural network. So let's now talk about image data, which is the first thing we need to understand.", "tokens": [50368, 18161, 3209, 13, 407, 718, 311, 586, 751, 466, 3256, 1412, 11, 597, 307, 264, 700, 551, 321, 643, 281, 1223, 13, 50636], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3270, "seek": 1350556, "start": 13511.0, "end": 13516.199999999999, "text": " So in our previous examples, what we did with when we had a neural network is we had two dimensional", "tokens": [50636, 407, 294, 527, 3894, 5110, 11, 437, 321, 630, 365, 562, 321, 632, 257, 18161, 3209, 307, 321, 632, 732, 18795, 50896], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3271, "seek": 1350556, "start": 13516.199999999999, "end": 13520.359999999999, "text": " data, right, we had a width and height when we were trying to classify some kind of images using", "tokens": [50896, 1412, 11, 558, 11, 321, 632, 257, 11402, 293, 6681, 562, 321, 645, 1382, 281, 33872, 512, 733, 295, 5267, 1228, 51104], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3272, "seek": 1350556, "start": 13520.359999999999, "end": 13524.92, "text": " a dense neural network. And well, that's what we use two dimensions. Well, with an image, we", "tokens": [51104, 257, 18011, 18161, 3209, 13, 400, 731, 11, 300, 311, 437, 321, 764, 732, 12819, 13, 1042, 11, 365, 364, 3256, 11, 321, 51332], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3273, "seek": 1350556, "start": 13524.92, "end": 13530.199999999999, "text": " actually have three dimensions. And what makes up those dimensions? Well, we have a height, and we", "tokens": [51332, 767, 362, 1045, 12819, 13, 400, 437, 1669, 493, 729, 12819, 30, 1042, 11, 321, 362, 257, 6681, 11, 293, 321, 51596], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3274, "seek": 1350556, "start": 13530.199999999999, "end": 13534.279999999999, "text": " have a width, and then we have something called a color channels. Now, it's very important to", "tokens": [51596, 362, 257, 11402, 11, 293, 550, 321, 362, 746, 1219, 257, 2017, 9235, 13, 823, 11, 309, 311, 588, 1021, 281, 51800], "temperature": 0.0, "avg_logprob": -0.0817317829265461, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.10084891319274902}, {"id": 3275, "seek": 1353428, "start": 13534.28, "end": 13538.44, "text": " understand this, because we're going to see this a lot as we get into convolutional networks, that", "tokens": [50364, 1223, 341, 11, 570, 321, 434, 516, 281, 536, 341, 257, 688, 382, 321, 483, 666, 45216, 304, 9590, 11, 300, 50572], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3276, "seek": 1353428, "start": 13538.44, "end": 13544.2, "text": " the same image is really represented by three specific layers, right? We have the first layer,", "tokens": [50572, 264, 912, 3256, 307, 534, 10379, 538, 1045, 2685, 7914, 11, 558, 30, 492, 362, 264, 700, 4583, 11, 50860], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3277, "seek": 1353428, "start": 13544.2, "end": 13549.08, "text": " which tells us all of the red values of the pixels, the second layer, which tells us all the green", "tokens": [50860, 597, 5112, 505, 439, 295, 264, 2182, 4190, 295, 264, 18668, 11, 264, 1150, 4583, 11, 597, 5112, 505, 439, 264, 3092, 51104], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3278, "seek": 1353428, "start": 13549.08, "end": 13553.480000000001, "text": " values, and the third layer, which tells us all the blue values. So in this case, those are the", "tokens": [51104, 4190, 11, 293, 264, 2636, 4583, 11, 597, 5112, 505, 439, 264, 3344, 4190, 13, 407, 294, 341, 1389, 11, 729, 366, 264, 51324], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3279, "seek": 1353428, "start": 13553.480000000001, "end": 13558.04, "text": " covered channels. And we're going to be talking about channels in depth quite a bit in this series.", "tokens": [51324, 5343, 9235, 13, 400, 321, 434, 516, 281, 312, 1417, 466, 9235, 294, 7161, 1596, 257, 857, 294, 341, 2638, 13, 51552], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3280, "seek": 1353428, "start": 13558.04, "end": 13563.08, "text": " So just understand that although you think of an image as a two dimensional kind of thing,", "tokens": [51552, 407, 445, 1223, 300, 4878, 291, 519, 295, 364, 3256, 382, 257, 732, 18795, 733, 295, 551, 11, 51804], "temperature": 0.0, "avg_logprob": -0.07429039305534915, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.010650952346622944}, {"id": 3281, "seek": 1356308, "start": 13563.16, "end": 13567.8, "text": " and our computer, it's really represented by three dimensions where these channels are telling us", "tokens": [50368, 293, 527, 3820, 11, 309, 311, 534, 10379, 538, 1045, 12819, 689, 613, 9235, 366, 3585, 505, 50600], "temperature": 0.0, "avg_logprob": -0.06442638665191398, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.0023229641374200583}, {"id": 3282, "seek": 1356308, "start": 13567.8, "end": 13573.24, "text": " the color of each pixel. Because remember, in red, green, blue, you have three values for each pixel,", "tokens": [50600, 264, 2017, 295, 1184, 19261, 13, 1436, 1604, 11, 294, 2182, 11, 3092, 11, 3344, 11, 291, 362, 1045, 4190, 337, 1184, 19261, 11, 50872], "temperature": 0.0, "avg_logprob": -0.06442638665191398, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.0023229641374200583}, {"id": 3283, "seek": 1356308, "start": 13573.24, "end": 13578.12, "text": " which means that you're going to need three layers to represent that pixel, right? So this is what", "tokens": [50872, 597, 1355, 300, 291, 434, 516, 281, 643, 1045, 7914, 281, 2906, 300, 19261, 11, 558, 30, 407, 341, 307, 437, 51116], "temperature": 0.0, "avg_logprob": -0.06442638665191398, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.0023229641374200583}, {"id": 3284, "seek": 1356308, "start": 13578.12, "end": 13583.16, "text": " we can kind of think of it as a stack of layers. And in this case, a stack of pixels, right, or", "tokens": [51116, 321, 393, 733, 295, 519, 295, 309, 382, 257, 8630, 295, 7914, 13, 400, 294, 341, 1389, 11, 257, 8630, 295, 18668, 11, 558, 11, 420, 51368], "temperature": 0.0, "avg_logprob": -0.06442638665191398, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.0023229641374200583}, {"id": 3285, "seek": 1356308, "start": 13583.16, "end": 13587.4, "text": " stack of colors really telling us the value for each pixel. So if we were to draw this to the", "tokens": [51368, 8630, 295, 4577, 534, 3585, 505, 264, 2158, 337, 1184, 19261, 13, 407, 498, 321, 645, 281, 2642, 341, 281, 264, 51580], "temperature": 0.0, "avg_logprob": -0.06442638665191398, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.0023229641374200583}, {"id": 3286, "seek": 1358740, "start": 13587.4, "end": 13593.88, "text": " screen, we would get the blue, green and red values of each pixel, determine the color of it,", "tokens": [50364, 2568, 11, 321, 576, 483, 264, 3344, 11, 3092, 293, 2182, 4190, 295, 1184, 19261, 11, 6997, 264, 2017, 295, 309, 11, 50688], "temperature": 0.0, "avg_logprob": -0.0846229876790728, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.01322203129529953}, {"id": 3287, "seek": 1358740, "start": 13593.88, "end": 13599.0, "text": " and then draw the two dimensional image right based on the width and the height. Okay, so now", "tokens": [50688, 293, 550, 2642, 264, 732, 18795, 3256, 558, 2361, 322, 264, 11402, 293, 264, 6681, 13, 1033, 11, 370, 586, 50944], "temperature": 0.0, "avg_logprob": -0.0846229876790728, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.01322203129529953}, {"id": 3288, "seek": 1358740, "start": 13599.0, "end": 13602.52, "text": " we're going to talk about a convolutional neural network and the difference between that in a dense", "tokens": [50944, 321, 434, 516, 281, 751, 466, 257, 45216, 304, 18161, 3209, 293, 264, 2649, 1296, 300, 294, 257, 18011, 51120], "temperature": 0.0, "avg_logprob": -0.0846229876790728, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.01322203129529953}, {"id": 3289, "seek": 1358740, "start": 13602.52, "end": 13606.68, "text": " neural network. So in our previous examples, when we use the dense neural network to do some kind", "tokens": [51120, 18161, 3209, 13, 407, 294, 527, 3894, 5110, 11, 562, 321, 764, 264, 18011, 18161, 3209, 281, 360, 512, 733, 51328], "temperature": 0.0, "avg_logprob": -0.0846229876790728, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.01322203129529953}, {"id": 3290, "seek": 1358740, "start": 13606.68, "end": 13612.279999999999, "text": " of image classification, like that fashion, and this data set, what it essentially did was look", "tokens": [51328, 295, 3256, 21538, 11, 411, 300, 6700, 11, 293, 341, 1412, 992, 11, 437, 309, 4476, 630, 390, 574, 51608], "temperature": 0.0, "avg_logprob": -0.0846229876790728, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.01322203129529953}, {"id": 3291, "seek": 1361228, "start": 13612.28, "end": 13618.52, "text": " at the entire image at once and determined based on finding features in specific areas of the image,", "tokens": [50364, 412, 264, 2302, 3256, 412, 1564, 293, 9540, 2361, 322, 5006, 4122, 294, 2685, 3179, 295, 264, 3256, 11, 50676], "temperature": 0.0, "avg_logprob": -0.05408040612144808, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.12248636782169342}, {"id": 3292, "seek": 1361228, "start": 13618.52, "end": 13623.24, "text": " what that image was, right? Maybe it found an edge here, a line here, maybe it found a shape,", "tokens": [50676, 437, 300, 3256, 390, 11, 558, 30, 2704, 309, 1352, 364, 4691, 510, 11, 257, 1622, 510, 11, 1310, 309, 1352, 257, 3909, 11, 50912], "temperature": 0.0, "avg_logprob": -0.05408040612144808, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.12248636782169342}, {"id": 3293, "seek": 1361228, "start": 13623.24, "end": 13627.560000000001, "text": " maybe it found a horizontal diagonal line. The important thing to understand, though, is that", "tokens": [50912, 1310, 309, 1352, 257, 12750, 21539, 1622, 13, 440, 1021, 551, 281, 1223, 11, 1673, 11, 307, 300, 51128], "temperature": 0.0, "avg_logprob": -0.05408040612144808, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.12248636782169342}, {"id": 3294, "seek": 1361228, "start": 13627.560000000001, "end": 13632.2, "text": " when it found these patterns and learned the patterns that made up specific shapes, it learned", "tokens": [51128, 562, 309, 1352, 613, 8294, 293, 3264, 264, 8294, 300, 1027, 493, 2685, 10854, 11, 309, 3264, 51360], "temperature": 0.0, "avg_logprob": -0.05408040612144808, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.12248636782169342}, {"id": 3295, "seek": 1361228, "start": 13632.2, "end": 13637.400000000001, "text": " them in specific areas. It knew that if we're in between, for example, looking at this cat image,", "tokens": [51360, 552, 294, 2685, 3179, 13, 467, 2586, 300, 498, 321, 434, 294, 1296, 11, 337, 1365, 11, 1237, 412, 341, 3857, 3256, 11, 51620], "temperature": 0.0, "avg_logprob": -0.05408040612144808, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.12248636782169342}, {"id": 3296, "seek": 1363740, "start": 13637.48, "end": 13642.119999999999, "text": " we're going to classify this as a cat, if an eye exists on, you know, the left side of the screen", "tokens": [50368, 321, 434, 516, 281, 33872, 341, 382, 257, 3857, 11, 498, 364, 3313, 8198, 322, 11, 291, 458, 11, 264, 1411, 1252, 295, 264, 2568, 50600], "temperature": 0.0, "avg_logprob": -0.06460659844534737, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.11592358350753784}, {"id": 3297, "seek": 1363740, "start": 13642.119999999999, "end": 13647.48, "text": " where the eyes are here, then that's a cat. It doesn't necessarily know that if we flipped this", "tokens": [50600, 689, 264, 2575, 366, 510, 11, 550, 300, 311, 257, 3857, 13, 467, 1177, 380, 4725, 458, 300, 498, 321, 26273, 341, 50868], "temperature": 0.0, "avg_logprob": -0.06460659844534737, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.11592358350753784}, {"id": 3298, "seek": 1363740, "start": 13647.48, "end": 13652.68, "text": " cat, we did a horizontal flip of this cat, and the eyes were over here, that that is a pattern that", "tokens": [50868, 3857, 11, 321, 630, 257, 12750, 7929, 295, 341, 3857, 11, 293, 264, 2575, 645, 670, 510, 11, 300, 300, 307, 257, 5102, 300, 51128], "temperature": 0.0, "avg_logprob": -0.06460659844534737, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.11592358350753784}, {"id": 3299, "seek": 1363740, "start": 13652.68, "end": 13657.72, "text": " makes up a cat. So the idea is that the dense network looks at things globally, it looks at the", "tokens": [51128, 1669, 493, 257, 3857, 13, 407, 264, 1558, 307, 300, 264, 18011, 3209, 1542, 412, 721, 18958, 11, 309, 1542, 412, 264, 51380], "temperature": 0.0, "avg_logprob": -0.06460659844534737, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.11592358350753784}, {"id": 3300, "seek": 1363740, "start": 13657.72, "end": 13663.64, "text": " entire image and learns patterns in specific areas. That's why we need things to be centered, we need", "tokens": [51380, 2302, 3256, 293, 27152, 8294, 294, 2685, 3179, 13, 663, 311, 983, 321, 643, 721, 281, 312, 18988, 11, 321, 643, 51676], "temperature": 0.0, "avg_logprob": -0.06460659844534737, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.11592358350753784}, {"id": 3301, "seek": 1366364, "start": 13663.64, "end": 13668.68, "text": " things to be very similar when we use a dense neural network to actually perform image classification,", "tokens": [50364, 721, 281, 312, 588, 2531, 562, 321, 764, 257, 18011, 18161, 3209, 281, 767, 2042, 3256, 21538, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07751289035962976, "compression_ratio": 1.7455197132616487, "no_speech_prob": 0.006289448589086533}, {"id": 3302, "seek": 1366364, "start": 13669.32, "end": 13674.279999999999, "text": " because it cannot learn local patterns, and apply those to different areas of the image.", "tokens": [50648, 570, 309, 2644, 1466, 2654, 8294, 11, 293, 3079, 729, 281, 819, 3179, 295, 264, 3256, 13, 50896], "temperature": 0.0, "avg_logprob": -0.07751289035962976, "compression_ratio": 1.7455197132616487, "no_speech_prob": 0.006289448589086533}, {"id": 3303, "seek": 1366364, "start": 13674.279999999999, "end": 13678.599999999999, "text": " So for example, some patterns we might look for, when we're looking at an image like a cat here", "tokens": [50896, 407, 337, 1365, 11, 512, 8294, 321, 1062, 574, 337, 11, 562, 321, 434, 1237, 412, 364, 3256, 411, 257, 3857, 510, 51112], "temperature": 0.0, "avg_logprob": -0.07751289035962976, "compression_ratio": 1.7455197132616487, "no_speech_prob": 0.006289448589086533}, {"id": 3304, "seek": 1366364, "start": 13678.599999999999, "end": 13683.0, "text": " would be something like this, right, we would hope that maybe we could find a few ears, we could find", "tokens": [51112, 576, 312, 746, 411, 341, 11, 558, 11, 321, 576, 1454, 300, 1310, 321, 727, 915, 257, 1326, 8798, 11, 321, 727, 915, 51332], "temperature": 0.0, "avg_logprob": -0.07751289035962976, "compression_ratio": 1.7455197132616487, "no_speech_prob": 0.006289448589086533}, {"id": 3305, "seek": 1366364, "start": 13683.0, "end": 13690.199999999999, "text": " the eyes, the nose, and you know, the paws here. And those features would tell us that this makes", "tokens": [51332, 264, 2575, 11, 264, 6690, 11, 293, 291, 458, 11, 264, 46768, 510, 13, 400, 729, 4122, 576, 980, 505, 300, 341, 1669, 51692], "temperature": 0.0, "avg_logprob": -0.07751289035962976, "compression_ratio": 1.7455197132616487, "no_speech_prob": 0.006289448589086533}, {"id": 3306, "seek": 1369020, "start": 13690.28, "end": 13695.400000000001, "text": " up a cat. Now with a dense neural network, it would find these features, it would learn them,", "tokens": [50368, 493, 257, 3857, 13, 823, 365, 257, 18011, 18161, 3209, 11, 309, 576, 915, 613, 4122, 11, 309, 576, 1466, 552, 11, 50624], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3307, "seek": 1369020, "start": 13695.400000000001, "end": 13700.12, "text": " learn these patterns, we only learn them in this specific area where they're boxed off,", "tokens": [50624, 1466, 613, 8294, 11, 321, 787, 1466, 552, 294, 341, 2685, 1859, 689, 436, 434, 2424, 292, 766, 11, 50860], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3308, "seek": 1369020, "start": 13700.12, "end": 13703.960000000001, "text": " which means if I horizontally flip this image, right, and I go like that,", "tokens": [50860, 597, 1355, 498, 286, 33796, 7929, 341, 3256, 11, 558, 11, 293, 286, 352, 411, 300, 11, 51052], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3309, "seek": 1369020, "start": 13703.960000000001, "end": 13707.960000000001, "text": " then it's not going to know that that's a cat, because it learned that pattern in a specific", "tokens": [51052, 550, 309, 311, 406, 516, 281, 458, 300, 300, 311, 257, 3857, 11, 570, 309, 3264, 300, 5102, 294, 257, 2685, 51252], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3310, "seek": 1369020, "start": 13707.960000000001, "end": 13713.08, "text": " area, it'll need to relearn that pattern in the other area. Now a convolutional neural network,", "tokens": [51252, 1859, 11, 309, 603, 643, 281, 2951, 1083, 300, 5102, 294, 264, 661, 1859, 13, 823, 257, 45216, 304, 18161, 3209, 11, 51508], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3311, "seek": 1369020, "start": 13713.08, "end": 13718.12, "text": " on the other hand, learns local patterns. So rather than learning that the ear exists in,", "tokens": [51508, 322, 264, 661, 1011, 11, 27152, 2654, 8294, 13, 407, 2831, 813, 2539, 300, 264, 1273, 8198, 294, 11, 51760], "temperature": 0.0, "avg_logprob": -0.06867292034092234, "compression_ratio": 1.8736842105263158, "no_speech_prob": 0.013635315001010895}, {"id": 3312, "seek": 1371812, "start": 13718.12, "end": 13722.68, "text": " you know, this specific location, it just learns that this is what an ear looks like,", "tokens": [50364, 291, 458, 11, 341, 2685, 4914, 11, 309, 445, 27152, 300, 341, 307, 437, 364, 1273, 1542, 411, 11, 50592], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3313, "seek": 1371812, "start": 13722.68, "end": 13727.480000000001, "text": " and it can find that anywhere in the image. And we'll talk about how we do that as we get to the", "tokens": [50592, 293, 309, 393, 915, 300, 4992, 294, 264, 3256, 13, 400, 321, 603, 751, 466, 577, 321, 360, 300, 382, 321, 483, 281, 264, 50832], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3314, "seek": 1371812, "start": 13727.480000000001, "end": 13731.880000000001, "text": " explanation. But the whole point is that our convolutional neural network will scan through", "tokens": [50832, 10835, 13, 583, 264, 1379, 935, 307, 300, 527, 45216, 304, 18161, 3209, 486, 11049, 807, 51052], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3315, "seek": 1371812, "start": 13731.880000000001, "end": 13737.720000000001, "text": " our entire image, it will pick up features and find features in the image. And then based on the", "tokens": [51052, 527, 2302, 3256, 11, 309, 486, 1888, 493, 4122, 293, 915, 4122, 294, 264, 3256, 13, 400, 550, 2361, 322, 264, 51344], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3316, "seek": 1371812, "start": 13737.720000000001, "end": 13742.92, "text": " features that exist in that image will pass that actually to a dense neural network or a dense", "tokens": [51344, 4122, 300, 2514, 294, 300, 3256, 486, 1320, 300, 767, 281, 257, 18011, 18161, 3209, 420, 257, 18011, 51604], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3317, "seek": 1371812, "start": 13742.92, "end": 13747.720000000001, "text": " classifier, it will look at the presence of these features and determine, you know, the combination", "tokens": [51604, 1508, 9902, 11, 309, 486, 574, 412, 264, 6814, 295, 613, 4122, 293, 6997, 11, 291, 458, 11, 264, 6562, 51844], "temperature": 0.0, "avg_logprob": -0.0615544319152832, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004609099589288235}, {"id": 3318, "seek": 1374772, "start": 13747.8, "end": 13752.279999999999, "text": " of these presences of features that make up specific classes or make up specific objects.", "tokens": [50368, 295, 613, 1183, 2667, 295, 4122, 300, 652, 493, 2685, 5359, 420, 652, 493, 2685, 6565, 13, 50592], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3319, "seek": 1374772, "start": 13752.279999999999, "end": 13756.519999999999, "text": " So that's kind of the point. I hope that makes sense. The main thing to remember is that dense", "tokens": [50592, 407, 300, 311, 733, 295, 264, 935, 13, 286, 1454, 300, 1669, 2020, 13, 440, 2135, 551, 281, 1604, 307, 300, 18011, 50804], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3320, "seek": 1374772, "start": 13756.519999999999, "end": 13761.4, "text": " neural networks work on a global scale, meaning they learn global patterns, which are specific and", "tokens": [50804, 18161, 9590, 589, 322, 257, 4338, 4373, 11, 3620, 436, 1466, 4338, 8294, 11, 597, 366, 2685, 293, 51048], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3321, "seek": 1374772, "start": 13761.4, "end": 13767.32, "text": " are found in specific areas. Whereas convolutional neural networks or convolutional layers will", "tokens": [51048, 366, 1352, 294, 2685, 3179, 13, 13813, 45216, 304, 18161, 9590, 420, 45216, 304, 7914, 486, 51344], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3322, "seek": 1374772, "start": 13767.32, "end": 13771.8, "text": " find patterns that exist anywhere in the image, because they know what the pattern looks like,", "tokens": [51344, 915, 8294, 300, 2514, 4992, 294, 264, 3256, 11, 570, 436, 458, 437, 264, 5102, 1542, 411, 11, 51568], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3323, "seek": 1374772, "start": 13771.8, "end": 13777.56, "text": " not that it just exists in a specific area. Alright, so how they work, right? So let's see", "tokens": [51568, 406, 300, 309, 445, 8198, 294, 257, 2685, 1859, 13, 2798, 11, 370, 577, 436, 589, 11, 558, 30, 407, 718, 311, 536, 51856], "temperature": 0.0, "avg_logprob": -0.08743264526128769, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.002396470634266734}, {"id": 3324, "seek": 1377756, "start": 13777.56, "end": 13781.72, "text": " when a neural network, regular neural network looks at this dog image, this is a good example,", "tokens": [50364, 562, 257, 18161, 3209, 11, 3890, 18161, 3209, 1542, 412, 341, 3000, 3256, 11, 341, 307, 257, 665, 1365, 11, 50572], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3325, "seek": 1377756, "start": 13781.72, "end": 13787.16, "text": " I should have been using this before, it will find that there's two eyes that exist here, right?", "tokens": [50572, 286, 820, 362, 668, 1228, 341, 949, 11, 309, 486, 915, 300, 456, 311, 732, 2575, 300, 2514, 510, 11, 558, 30, 50844], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3326, "seek": 1377756, "start": 13787.16, "end": 13791.8, "text": " And we'll say, okay, so I found that these eyes make up a dog. This is its training image, for", "tokens": [50844, 400, 321, 603, 584, 11, 1392, 11, 370, 286, 1352, 300, 613, 2575, 652, 493, 257, 3000, 13, 639, 307, 1080, 3097, 3256, 11, 337, 51076], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3327, "seek": 1377756, "start": 13791.8, "end": 13795.88, "text": " example, and it's like, okay, so this pattern makes up the dog, the IR is in this location.", "tokens": [51076, 1365, 11, 293, 309, 311, 411, 11, 1392, 11, 370, 341, 5102, 1669, 493, 264, 3000, 11, 264, 16486, 307, 294, 341, 4914, 13, 51280], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3328, "seek": 1377756, "start": 13796.68, "end": 13801.88, "text": " Now, what happens when we do this? And we flip the image to the other side. Well, our neural", "tokens": [51320, 823, 11, 437, 2314, 562, 321, 360, 341, 30, 400, 321, 7929, 264, 3256, 281, 264, 661, 1252, 13, 1042, 11, 527, 18161, 51580], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3329, "seek": 1377756, "start": 13801.88, "end": 13805.64, "text": " network starts looking for these eyes, right on the left side of the image where it found them", "tokens": [51580, 3209, 3719, 1237, 337, 613, 2575, 11, 558, 322, 264, 1411, 1252, 295, 264, 3256, 689, 309, 1352, 552, 51768], "temperature": 0.0, "avg_logprob": -0.09664090474446614, "compression_ratio": 1.8803986710963456, "no_speech_prob": 0.0014549564803019166}, {"id": 3330, "seek": 1380564, "start": 13805.64, "end": 13810.039999999999, "text": " previously and where it was trained on. It obviously doesn't find them there. And so it says", "tokens": [50364, 8046, 293, 689, 309, 390, 8895, 322, 13, 467, 2745, 1177, 380, 915, 552, 456, 13, 400, 370, 309, 1619, 50584], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3331, "seek": 1380564, "start": 13810.039999999999, "end": 13814.279999999999, "text": " that our image isn't a dog, although it clearly is a dog, it's just a dog that's orientated", "tokens": [50584, 300, 527, 3256, 1943, 380, 257, 3000, 11, 4878, 309, 4448, 307, 257, 3000, 11, 309, 311, 445, 257, 3000, 300, 311, 8579, 770, 50796], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3332, "seek": 1380564, "start": 13814.279999999999, "end": 13818.199999999999, "text": " differently. In fact, it's just flipped horizontally, right? Or actually, I guess I would say", "tokens": [50796, 7614, 13, 682, 1186, 11, 309, 311, 445, 26273, 33796, 11, 558, 30, 1610, 767, 11, 286, 2041, 286, 576, 584, 50992], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3333, "seek": 1380564, "start": 13818.199999999999, "end": 13823.64, "text": " vertically, flip vertically. So since it doesn't find the eyes in this location, and it can only", "tokens": [50992, 28450, 11, 7929, 28450, 13, 407, 1670, 309, 1177, 380, 915, 264, 2575, 294, 341, 4914, 11, 293, 309, 393, 787, 51264], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3334, "seek": 1380564, "start": 13823.64, "end": 13828.199999999999, "text": " look at patterns that it's learned in specific locations, it knows that this, or it's going", "tokens": [51264, 574, 412, 8294, 300, 309, 311, 3264, 294, 2685, 9253, 11, 309, 3255, 300, 341, 11, 420, 309, 311, 516, 51492], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3335, "seek": 1380564, "start": 13828.199999999999, "end": 13833.08, "text": " to say this isn't a dog, even though it is. Whereas our convolutional layer will find the eyes", "tokens": [51492, 281, 584, 341, 1943, 380, 257, 3000, 11, 754, 1673, 309, 307, 13, 13813, 527, 45216, 304, 4583, 486, 915, 264, 2575, 51736], "temperature": 0.0, "avg_logprob": -0.07900474440883583, "compression_ratio": 1.8129032258064517, "no_speech_prob": 0.0064875842072069645}, {"id": 3336, "seek": 1383308, "start": 13833.08, "end": 13837.72, "text": " regardless of where they are in the image, and still tell us that this is a dog, because even", "tokens": [50364, 10060, 295, 689, 436, 366, 294, 264, 3256, 11, 293, 920, 980, 505, 300, 341, 307, 257, 3000, 11, 570, 754, 50596], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3337, "seek": 1383308, "start": 13837.72, "end": 13841.48, "text": " though the dogs moved over, it knows what an eye looks like, so it can find the eye anywhere in", "tokens": [50596, 1673, 264, 7197, 4259, 670, 11, 309, 3255, 437, 364, 3313, 1542, 411, 11, 370, 309, 393, 915, 264, 3313, 4992, 294, 50784], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3338, "seek": 1383308, "start": 13841.48, "end": 13846.76, "text": " the image. So that's kind of the point of the convolutional neural network and the convolutional", "tokens": [50784, 264, 3256, 13, 407, 300, 311, 733, 295, 264, 935, 295, 264, 45216, 304, 18161, 3209, 293, 264, 45216, 304, 51048], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3339, "seek": 1383308, "start": 13846.76, "end": 13852.039999999999, "text": " layer. And what the convolutional layer does is look at our image and essentially feedback to us,", "tokens": [51048, 4583, 13, 400, 437, 264, 45216, 304, 4583, 775, 307, 574, 412, 527, 3256, 293, 4476, 5824, 281, 505, 11, 51312], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3340, "seek": 1383308, "start": 13852.039999999999, "end": 13857.08, "text": " what we call an output feature map that tells us about the presence of specific features,", "tokens": [51312, 437, 321, 818, 364, 5598, 4111, 4471, 300, 5112, 505, 466, 264, 6814, 295, 2685, 4122, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3341, "seek": 1383308, "start": 13857.08, "end": 13862.039999999999, "text": " or what we're going to call filters in our image. So that is kind of the way that works.", "tokens": [51564, 420, 437, 321, 434, 516, 281, 818, 15995, 294, 527, 3256, 13, 407, 300, 307, 733, 295, 264, 636, 300, 1985, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07043222820057589, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0062894015572965145}, {"id": 3342, "seek": 1386204, "start": 13862.76, "end": 13867.160000000002, "text": " Now, essentially, the thing we have to remember is that our dense neural networks output just a", "tokens": [50400, 823, 11, 4476, 11, 264, 551, 321, 362, 281, 1604, 307, 300, 527, 18011, 18161, 9590, 5598, 445, 257, 50620], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3343, "seek": 1386204, "start": 13867.160000000002, "end": 13871.640000000001, "text": " bunch of numeric values. Whereas what our convolutional layers are actually going to be", "tokens": [50620, 3840, 295, 7866, 299, 4190, 13, 13813, 437, 527, 45216, 304, 7914, 366, 767, 516, 281, 312, 50844], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3344, "seek": 1386204, "start": 13871.640000000001, "end": 13876.04, "text": " doing is outputting what we call a feature map. Now I'm going to scroll down here to show you", "tokens": [50844, 884, 307, 5598, 783, 437, 321, 818, 257, 4111, 4471, 13, 823, 286, 478, 516, 281, 11369, 760, 510, 281, 855, 291, 51064], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3345, "seek": 1386204, "start": 13876.04, "end": 13880.84, "text": " this example. What we're actually going to do is run what we call a filter over our image,", "tokens": [51064, 341, 1365, 13, 708, 321, 434, 767, 516, 281, 360, 307, 1190, 437, 321, 818, 257, 6608, 670, 527, 3256, 11, 51304], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3346, "seek": 1386204, "start": 13880.84, "end": 13884.28, "text": " we're going to sample the image at all these different areas. And then we're going to create", "tokens": [51304, 321, 434, 516, 281, 6889, 264, 3256, 412, 439, 613, 819, 3179, 13, 400, 550, 321, 434, 516, 281, 1884, 51476], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3347, "seek": 1386204, "start": 13884.28, "end": 13889.960000000001, "text": " what we call an output feature map that quantifies the presence of the filters pattern at different", "tokens": [51476, 437, 321, 818, 364, 5598, 4111, 4471, 300, 4426, 11221, 264, 6814, 295, 264, 15995, 5102, 412, 819, 51760], "temperature": 0.0, "avg_logprob": -0.06564644820817554, "compression_ratio": 1.8952702702702702, "no_speech_prob": 0.0028007219079881907}, {"id": 3348, "seek": 1388996, "start": 13889.96, "end": 13895.88, "text": " locations. And we'll run many, many, many different filters over our image at a time.", "tokens": [50364, 9253, 13, 400, 321, 603, 1190, 867, 11, 867, 11, 867, 819, 15995, 670, 527, 3256, 412, 257, 565, 13, 50660], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3349, "seek": 1388996, "start": 13895.88, "end": 13899.8, "text": " So we have all these different feature maps telling us about the presence of all these", "tokens": [50660, 407, 321, 362, 439, 613, 819, 4111, 11317, 3585, 505, 466, 264, 6814, 295, 439, 613, 50856], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3350, "seek": 1388996, "start": 13899.8, "end": 13904.919999999998, "text": " different features. So one convolutional layer, we'll start by doing that with very small,", "tokens": [50856, 819, 4122, 13, 407, 472, 45216, 304, 4583, 11, 321, 603, 722, 538, 884, 300, 365, 588, 1359, 11, 51112], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3351, "seek": 1388996, "start": 13904.919999999998, "end": 13910.199999999999, "text": " simple filters such as straight lines like this. And then other convolutional layers on top of", "tokens": [51112, 2199, 15995, 1270, 382, 2997, 3876, 411, 341, 13, 400, 550, 661, 45216, 304, 7914, 322, 1192, 295, 51376], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3352, "seek": 1388996, "start": 13910.199999999999, "end": 13914.839999999998, "text": " that, right, because it's going to return a map that looks something like this out of the layer,", "tokens": [51376, 300, 11, 558, 11, 570, 309, 311, 516, 281, 2736, 257, 4471, 300, 1542, 746, 411, 341, 484, 295, 264, 4583, 11, 51608], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3353, "seek": 1388996, "start": 13914.839999999998, "end": 13919.48, "text": " we'll take this map in now, the one that was created from the previous layer, and say, okay,", "tokens": [51608, 321, 603, 747, 341, 4471, 294, 586, 11, 264, 472, 300, 390, 2942, 490, 264, 3894, 4583, 11, 293, 584, 11, 1392, 11, 51840], "temperature": 0.0, "avg_logprob": -0.06278596502361876, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.003075097920373082}, {"id": 3354, "seek": 1391948, "start": 13919.48, "end": 13923.32, "text": " what this map is representing to me, for example, the presence of these diagonal lines,", "tokens": [50364, 437, 341, 4471, 307, 13460, 281, 385, 11, 337, 1365, 11, 264, 6814, 295, 613, 21539, 3876, 11, 50556], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3355, "seek": 1391948, "start": 13923.88, "end": 13928.279999999999, "text": " let me try to look for curves, right, or let me try to look for edges. So it will look at the", "tokens": [50584, 718, 385, 853, 281, 574, 337, 19490, 11, 558, 11, 420, 718, 385, 853, 281, 574, 337, 8819, 13, 407, 309, 486, 574, 412, 264, 50804], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3356, "seek": 1391948, "start": 13928.279999999999, "end": 13932.52, "text": " presence of the features from the previous convolutional layer, and then say, okay, well,", "tokens": [50804, 6814, 295, 264, 4122, 490, 264, 3894, 45216, 304, 4583, 11, 293, 550, 584, 11, 1392, 11, 731, 11, 51016], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3357, "seek": 1391948, "start": 13932.52, "end": 13937.08, "text": " if I have all these lines combined together, that makes up an edge, and it will look for that,", "tokens": [51016, 498, 286, 362, 439, 613, 3876, 9354, 1214, 11, 300, 1669, 493, 364, 4691, 11, 293, 309, 486, 574, 337, 300, 11, 51244], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3358, "seek": 1391948, "start": 13937.08, "end": 13941.32, "text": " right? And that's kind of the way that a convolutional neural network works and why we", "tokens": [51244, 558, 30, 400, 300, 311, 733, 295, 264, 636, 300, 257, 45216, 304, 18161, 3209, 1985, 293, 983, 321, 51456], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3359, "seek": 1391948, "start": 13941.32, "end": 13945.4, "text": " stack these different layers. Now, we also use something called pooling, and there's a few", "tokens": [51456, 8630, 613, 819, 7914, 13, 823, 11, 321, 611, 764, 746, 1219, 7005, 278, 11, 293, 456, 311, 257, 1326, 51660], "temperature": 0.0, "avg_logprob": -0.08594778854481495, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.0011334612499922514}, {"id": 3360, "seek": 1394540, "start": 13945.4, "end": 13949.56, "text": " other things that we're going to get into. But that is the basics, I'm going to go into a drawing", "tokens": [50364, 661, 721, 300, 321, 434, 516, 281, 483, 666, 13, 583, 300, 307, 264, 14688, 11, 286, 478, 516, 281, 352, 666, 257, 6316, 50572], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3361, "seek": 1394540, "start": 13949.56, "end": 13954.199999999999, "text": " example and show you exactly how that works. But hopefully this makes a little bit of sense", "tokens": [50572, 1365, 293, 855, 291, 2293, 577, 300, 1985, 13, 583, 4696, 341, 1669, 257, 707, 857, 295, 2020, 50804], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3362, "seek": 1394540, "start": 13954.199999999999, "end": 13959.24, "text": " that the convolutional layer returns a feature map that quantifies the presence of a filter", "tokens": [50804, 300, 264, 45216, 304, 4583, 11247, 257, 4111, 4471, 300, 4426, 11221, 264, 6814, 295, 257, 6608, 51056], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3363, "seek": 1394540, "start": 13959.24, "end": 13964.199999999999, "text": " at a specific location. And this filter, the advantage of it is that we slide it across", "tokens": [51056, 412, 257, 2685, 4914, 13, 400, 341, 6608, 11, 264, 5002, 295, 309, 307, 300, 321, 4137, 309, 2108, 51304], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3364, "seek": 1394540, "start": 13964.199999999999, "end": 13969.24, "text": " the entire image. So if this filter or this feature is presence anywhere in the image,", "tokens": [51304, 264, 2302, 3256, 13, 407, 498, 341, 6608, 420, 341, 4111, 307, 6814, 4992, 294, 264, 3256, 11, 51556], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3365, "seek": 1394540, "start": 13969.24, "end": 13972.76, "text": " we will know about it rather than in our dense network, where it had to learn that pattern", "tokens": [51556, 321, 486, 458, 466, 309, 2831, 813, 294, 527, 18011, 3209, 11, 689, 309, 632, 281, 1466, 300, 5102, 51732], "temperature": 0.0, "avg_logprob": -0.05546683673710786, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0875372663140297}, {"id": 3366, "seek": 1397276, "start": 13972.76, "end": 13978.12, "text": " in a specific global location. Okay, so let's get on the drawing tablet and do a few examples.", "tokens": [50364, 294, 257, 2685, 4338, 4914, 13, 1033, 11, 370, 718, 311, 483, 322, 264, 6316, 14136, 293, 360, 257, 1326, 5110, 13, 50632], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3367, "seek": 1397276, "start": 13978.12, "end": 13981.48, "text": " All right, so I'm here on my drawing tablet, and we're going to explain exactly how a", "tokens": [50632, 1057, 558, 11, 370, 286, 478, 510, 322, 452, 6316, 14136, 11, 293, 321, 434, 516, 281, 2903, 2293, 577, 257, 50800], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3368, "seek": 1397276, "start": 13981.48, "end": 13987.16, "text": " convolutional layer works, and how the network kind of works together. So this is an image I've", "tokens": [50800, 45216, 304, 4583, 1985, 11, 293, 577, 264, 3209, 733, 295, 1985, 1214, 13, 407, 341, 307, 364, 3256, 286, 600, 51084], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3369, "seek": 1397276, "start": 13987.16, "end": 13991.880000000001, "text": " drawn on the left side of our screen here. I know this is very basic, you know, this is just an X,", "tokens": [51084, 10117, 322, 264, 1411, 1252, 295, 527, 2568, 510, 13, 286, 458, 341, 307, 588, 3875, 11, 291, 458, 11, 341, 307, 445, 364, 1783, 11, 51320], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3370, "seek": 1397276, "start": 13991.880000000001, "end": 13995.08, "text": " right? This is what our images, we're just going to assume this is grayscale, we're going to avoid", "tokens": [51320, 558, 30, 639, 307, 437, 527, 5267, 11, 321, 434, 445, 516, 281, 6552, 341, 307, 677, 3772, 37088, 11, 321, 434, 516, 281, 5042, 51480], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3371, "seek": 1397276, "start": 13995.08, "end": 14000.12, "text": " doing anything with color channels the second just because they're not that important. But", "tokens": [51480, 884, 1340, 365, 2017, 9235, 264, 1150, 445, 570, 436, 434, 406, 300, 1021, 13, 583, 51732], "temperature": 0.0, "avg_logprob": -0.0746069283320986, "compression_ratio": 1.7601246105919004, "no_speech_prob": 0.00857667624950409}, {"id": 3372, "seek": 1400012, "start": 14000.12, "end": 14004.04, "text": " just understand that what I'm going to show you does apply to color channels as well and to", "tokens": [50364, 445, 1223, 300, 437, 286, 478, 516, 281, 855, 291, 775, 3079, 281, 2017, 9235, 382, 731, 293, 281, 50560], "temperature": 0.0, "avg_logprob": -0.08063475868918679, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0009398955153301358}, {"id": 3373, "seek": 1400012, "start": 14004.04, "end": 14009.08, "text": " multiple kind of layers and depth. And then if we can understand it on a simple level,", "tokens": [50560, 3866, 733, 295, 7914, 293, 7161, 13, 400, 550, 498, 321, 393, 1223, 309, 322, 257, 2199, 1496, 11, 50812], "temperature": 0.0, "avg_logprob": -0.08063475868918679, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0009398955153301358}, {"id": 3374, "seek": 1400012, "start": 14009.08, "end": 14014.84, "text": " we should be able to understand it more thoroughly. So what we want essentially is our convolutional", "tokens": [50812, 321, 820, 312, 1075, 281, 1223, 309, 544, 17987, 13, 407, 437, 321, 528, 4476, 307, 527, 45216, 304, 51100], "temperature": 0.0, "avg_logprob": -0.08063475868918679, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0009398955153301358}, {"id": 3375, "seek": 1400012, "start": 14014.84, "end": 14020.12, "text": " layer to give us some output. It's meaningful about this image. So we're going to assume this is", "tokens": [51100, 4583, 281, 976, 505, 512, 5598, 13, 467, 311, 10995, 466, 341, 3256, 13, 407, 321, 434, 516, 281, 6552, 341, 307, 51364], "temperature": 0.0, "avg_logprob": -0.08063475868918679, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0009398955153301358}, {"id": 3376, "seek": 1400012, "start": 14020.12, "end": 14025.400000000001, "text": " the first convolutional layer. And what it needs to do essentially is return to us some feature", "tokens": [51364, 264, 700, 45216, 304, 4583, 13, 400, 437, 309, 2203, 281, 360, 4476, 307, 2736, 281, 505, 512, 4111, 51628], "temperature": 0.0, "avg_logprob": -0.08063475868918679, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0009398955153301358}, {"id": 3377, "seek": 1402540, "start": 14025.4, "end": 14031.56, "text": " map that tells us about the presence of specific what we call filters in this image. So each", "tokens": [50364, 4471, 300, 5112, 505, 466, 264, 6814, 295, 2685, 437, 321, 818, 15995, 294, 341, 3256, 13, 407, 1184, 50672], "temperature": 0.0, "avg_logprob": -0.0712473366850166, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.02595365233719349}, {"id": 3378, "seek": 1402540, "start": 14031.56, "end": 14037.32, "text": " convolutional layer has a few properties to it. The first one is going to be the input size.", "tokens": [50672, 45216, 304, 4583, 575, 257, 1326, 7221, 281, 309, 13, 440, 700, 472, 307, 516, 281, 312, 264, 4846, 2744, 13, 50960], "temperature": 0.0, "avg_logprob": -0.0712473366850166, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.02595365233719349}, {"id": 3379, "seek": 1402540, "start": 14038.359999999999, "end": 14045.4, "text": " So what can we expect? Wow, what is that that was as the as the input size, how many filters", "tokens": [51012, 407, 437, 393, 321, 2066, 30, 3153, 11, 437, 307, 300, 300, 390, 382, 264, 382, 264, 4846, 2744, 11, 577, 867, 15995, 51364], "temperature": 0.0, "avg_logprob": -0.0712473366850166, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.02595365233719349}, {"id": 3380, "seek": 1402540, "start": 14045.4, "end": 14050.76, "text": " are we going to have so filters like this? And what's the sample size of our filters?", "tokens": [51364, 366, 321, 516, 281, 362, 370, 15995, 411, 341, 30, 400, 437, 311, 264, 6889, 2744, 295, 527, 15995, 30, 51632], "temperature": 0.0, "avg_logprob": -0.0712473366850166, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.02595365233719349}, {"id": 3381, "seek": 1405076, "start": 14051.72, "end": 14057.4, "text": " That's what we need to know for each of our convolutional neural networks. So essentially,", "tokens": [50412, 663, 311, 437, 321, 643, 281, 458, 337, 1184, 295, 527, 45216, 304, 18161, 9590, 13, 407, 4476, 11, 50696], "temperature": 0.0, "avg_logprob": -0.07101843357086182, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.004198568407446146}, {"id": 3382, "seek": 1405076, "start": 14057.4, "end": 14061.08, "text": " what is a filter? Well, a filter is just some pattern of pixels. And we saw them before,", "tokens": [50696, 437, 307, 257, 6608, 30, 1042, 11, 257, 6608, 307, 445, 512, 5102, 295, 18668, 13, 400, 321, 1866, 552, 949, 11, 50880], "temperature": 0.0, "avg_logprob": -0.07101843357086182, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.004198568407446146}, {"id": 3383, "seek": 1405076, "start": 14061.08, "end": 14066.44, "text": " we'll do a pretty basic one here, as the filter we're going to look for, which will look something", "tokens": [50880, 321, 603, 360, 257, 1238, 3875, 472, 510, 11, 382, 264, 6608, 321, 434, 516, 281, 574, 337, 11, 597, 486, 574, 746, 51148], "temperature": 0.0, "avg_logprob": -0.07101843357086182, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.004198568407446146}, {"id": 3384, "seek": 1405076, "start": 14066.44, "end": 14072.28, "text": " like this. This will be the first filter we're going to look for just to illustrate how this works.", "tokens": [51148, 411, 341, 13, 639, 486, 312, 264, 700, 6608, 321, 434, 516, 281, 574, 337, 445, 281, 23221, 577, 341, 1985, 13, 51440], "temperature": 0.0, "avg_logprob": -0.07101843357086182, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.004198568407446146}, {"id": 3385, "seek": 1405076, "start": 14072.28, "end": 14077.48, "text": " But the idea is that at each convolutional layer, we look for many different filters. And in fact,", "tokens": [51440, 583, 264, 1558, 307, 300, 412, 1184, 45216, 304, 4583, 11, 321, 574, 337, 867, 819, 15995, 13, 400, 294, 1186, 11, 51700], "temperature": 0.0, "avg_logprob": -0.07101843357086182, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.004198568407446146}, {"id": 3386, "seek": 1407748, "start": 14077.48, "end": 14084.359999999999, "text": " the number we're typically looking for is actually about times 32 filters. Sometimes we have 64", "tokens": [50364, 264, 1230, 321, 434, 5850, 1237, 337, 307, 767, 466, 1413, 8858, 15995, 13, 4803, 321, 362, 12145, 50708], "temperature": 0.0, "avg_logprob": -0.05044185528989698, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.07157188653945923}, {"id": 3387, "seek": 1407748, "start": 14084.359999999999, "end": 14090.119999999999, "text": " filters as well. And sometimes even 128. So we can do as many filters as we want, as few filters", "tokens": [50708, 15995, 382, 731, 13, 400, 2171, 754, 29810, 13, 407, 321, 393, 360, 382, 867, 15995, 382, 321, 528, 11, 382, 1326, 15995, 50996], "temperature": 0.0, "avg_logprob": -0.05044185528989698, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.07157188653945923}, {"id": 3388, "seek": 1407748, "start": 14090.119999999999, "end": 14095.8, "text": " as we want. But the filters are what is going to be trained. So this filter is actually what is", "tokens": [50996, 382, 321, 528, 13, 583, 264, 15995, 366, 437, 307, 516, 281, 312, 8895, 13, 407, 341, 6608, 307, 767, 437, 307, 51280], "temperature": 0.0, "avg_logprob": -0.05044185528989698, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.07157188653945923}, {"id": 3389, "seek": 1407748, "start": 14095.8, "end": 14101.32, "text": " going to be found by the neural network. It's what's going to change. It's, you know, this is", "tokens": [51280, 516, 281, 312, 1352, 538, 264, 18161, 3209, 13, 467, 311, 437, 311, 516, 281, 1319, 13, 467, 311, 11, 291, 458, 11, 341, 307, 51556], "temperature": 0.0, "avg_logprob": -0.05044185528989698, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.07157188653945923}, {"id": 3390, "seek": 1407748, "start": 14101.32, "end": 14105.0, "text": " essentially what we're looking for. This is what's created in the program. And that's kind of like", "tokens": [51556, 4476, 437, 321, 434, 1237, 337, 13, 639, 307, 437, 311, 2942, 294, 264, 1461, 13, 400, 300, 311, 733, 295, 411, 51740], "temperature": 0.0, "avg_logprob": -0.05044185528989698, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.07157188653945923}, {"id": 3391, "seek": 1410500, "start": 14105.0, "end": 14111.08, "text": " the trainable parameter of a convolutional neural network is the filter. So the amount of filters", "tokens": [50364, 264, 3847, 712, 13075, 295, 257, 45216, 304, 18161, 3209, 307, 264, 6608, 13, 407, 264, 2372, 295, 15995, 50668], "temperature": 0.0, "avg_logprob": -0.05845028305053711, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.004468101542443037}, {"id": 3392, "seek": 1410500, "start": 14111.08, "end": 14116.68, "text": " and what they are will change as the program goes on, as we're learning more and figuring out what", "tokens": [50668, 293, 437, 436, 366, 486, 1319, 382, 264, 1461, 1709, 322, 11, 382, 321, 434, 2539, 544, 293, 15213, 484, 437, 50948], "temperature": 0.0, "avg_logprob": -0.05845028305053711, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.004468101542443037}, {"id": 3393, "seek": 1410500, "start": 14116.68, "end": 14121.48, "text": " features that make up, you know, a specific image. So I'm going to get rid of this stuff right now,", "tokens": [50948, 4122, 300, 652, 493, 11, 291, 458, 11, 257, 2685, 3256, 13, 407, 286, 478, 516, 281, 483, 3973, 295, 341, 1507, 558, 586, 11, 51188], "temperature": 0.0, "avg_logprob": -0.05845028305053711, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.004468101542443037}, {"id": 3394, "seek": 1410500, "start": 14121.48, "end": 14126.52, "text": " just so we can draw and do a basic example. But I want to show you how we look for a filter in the", "tokens": [51188, 445, 370, 321, 393, 2642, 293, 360, 257, 3875, 1365, 13, 583, 286, 528, 281, 855, 291, 577, 321, 574, 337, 257, 6608, 294, 264, 51440], "temperature": 0.0, "avg_logprob": -0.05845028305053711, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.004468101542443037}, {"id": 3395, "seek": 1410500, "start": 14126.52, "end": 14131.48, "text": " image. So we have filters, right, they'll come up with them, they're gonna start completely random,", "tokens": [51440, 3256, 13, 407, 321, 362, 15995, 11, 558, 11, 436, 603, 808, 493, 365, 552, 11, 436, 434, 799, 722, 2584, 4974, 11, 51688], "temperature": 0.0, "avg_logprob": -0.05845028305053711, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.004468101542443037}, {"id": 3396, "seek": 1413148, "start": 14131.48, "end": 14135.24, "text": " but they'll change as we go on. So let's say the filter we're looking for is that one I drew", "tokens": [50364, 457, 436, 603, 1319, 382, 321, 352, 322, 13, 407, 718, 311, 584, 264, 6608, 321, 434, 1237, 337, 307, 300, 472, 286, 12804, 50552], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3397, "seek": 1413148, "start": 14135.24, "end": 14138.359999999999, "text": " before, I'm just going to redraw it at the top here a little bit smaller. And we'll just say", "tokens": [50552, 949, 11, 286, 478, 445, 516, 281, 2182, 5131, 309, 412, 264, 1192, 510, 257, 707, 857, 4356, 13, 400, 321, 603, 445, 584, 50708], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3398, "seek": 1413148, "start": 14138.359999999999, "end": 14143.8, "text": " it's a diagonal line, right? But another filter we could look for might be something like, you know,", "tokens": [50708, 309, 311, 257, 21539, 1622, 11, 558, 30, 583, 1071, 6608, 321, 727, 574, 337, 1062, 312, 746, 411, 11, 291, 458, 11, 50980], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3399, "seek": 1413148, "start": 14143.8, "end": 14148.84, "text": " a straight line, just like that all across, we could have a horizontal line. And in fact, we'll", "tokens": [50980, 257, 2997, 1622, 11, 445, 411, 300, 439, 2108, 11, 321, 727, 362, 257, 12750, 1622, 13, 400, 294, 1186, 11, 321, 603, 51232], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3400, "seek": 1413148, "start": 14148.84, "end": 14153.4, "text": " have 32 of them. And when we're doing just, you know, three by three grids of filters, well,", "tokens": [51232, 362, 8858, 295, 552, 13, 400, 562, 321, 434, 884, 445, 11, 291, 458, 11, 1045, 538, 1045, 677, 3742, 295, 15995, 11, 731, 11, 51460], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3401, "seek": 1413148, "start": 14154.039999999999, "end": 14157.64, "text": " there's not that many, you know, combinations, we're going to do at least grayscale wise.", "tokens": [51492, 456, 311, 406, 300, 867, 11, 291, 458, 11, 21267, 11, 321, 434, 516, 281, 360, 412, 1935, 677, 3772, 37088, 10829, 13, 51672], "temperature": 0.0, "avg_logprob": -0.05758585074009039, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.014956080354750156}, {"id": 3402, "seek": 1415764, "start": 14158.199999999999, "end": 14162.68, "text": " So what we'll do is we'll define the sample size, which is how big our filter is going to be three", "tokens": [50392, 407, 437, 321, 603, 360, 307, 321, 603, 6964, 264, 6889, 2744, 11, 597, 307, 577, 955, 527, 6608, 307, 516, 281, 312, 1045, 50616], "temperature": 0.0, "avg_logprob": -0.07168575256101546, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.00628933310508728}, {"id": 3403, "seek": 1415764, "start": 14162.68, "end": 14166.599999999999, "text": " by three, which we know right now, which means that what we're going to do is we're going to look", "tokens": [50616, 538, 1045, 11, 597, 321, 458, 558, 586, 11, 597, 1355, 300, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 574, 50812], "temperature": 0.0, "avg_logprob": -0.07168575256101546, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.00628933310508728}, {"id": 3404, "seek": 1415764, "start": 14166.599999999999, "end": 14174.039999999999, "text": " at three by three spots in our image, and look at the pixels, and try to find how closely these", "tokens": [50812, 412, 1045, 538, 1045, 10681, 294, 527, 3256, 11, 293, 574, 412, 264, 18668, 11, 293, 853, 281, 915, 577, 8185, 613, 51184], "temperature": 0.0, "avg_logprob": -0.07168575256101546, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.00628933310508728}, {"id": 3405, "seek": 1415764, "start": 14174.039999999999, "end": 14179.0, "text": " filters match with the pixels we're looking at on each sample. So what this is going to do,", "tokens": [51184, 15995, 2995, 365, 264, 18668, 321, 434, 1237, 412, 322, 1184, 6889, 13, 407, 437, 341, 307, 516, 281, 360, 11, 51432], "temperature": 0.0, "avg_logprob": -0.07168575256101546, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.00628933310508728}, {"id": 3406, "seek": 1415764, "start": 14179.0, "end": 14183.88, "text": " this convolutional layer is going to output us what we call a feature map, which can be a little", "tokens": [51432, 341, 45216, 304, 4583, 307, 516, 281, 5598, 505, 437, 321, 818, 257, 4111, 4471, 11, 597, 393, 312, 257, 707, 51676], "temperature": 0.0, "avg_logprob": -0.07168575256101546, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.00628933310508728}, {"id": 3407, "seek": 1418388, "start": 14183.88, "end": 14188.839999999998, "text": " bit smaller than the original image. And you'll see why in a second, but that tells us about the", "tokens": [50364, 857, 4356, 813, 264, 3380, 3256, 13, 400, 291, 603, 536, 983, 294, 257, 1150, 11, 457, 300, 5112, 505, 466, 264, 50612], "temperature": 0.0, "avg_logprob": -0.08395233647576694, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00884639099240303}, {"id": 3408, "seek": 1418388, "start": 14188.839999999998, "end": 14194.119999999999, "text": " presence of specific features in areas of the image. So since we're looking for two filters here,", "tokens": [50612, 6814, 295, 2685, 4122, 294, 3179, 295, 264, 3256, 13, 407, 1670, 321, 434, 1237, 337, 732, 15995, 510, 11, 50876], "temperature": 0.0, "avg_logprob": -0.08395233647576694, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00884639099240303}, {"id": 3409, "seek": 1418388, "start": 14194.119999999999, "end": 14197.8, "text": " actually, we'll do two filters, which means that we're actually going to have a depth to", "tokens": [50876, 767, 11, 321, 603, 360, 732, 15995, 11, 597, 1355, 300, 321, 434, 767, 516, 281, 362, 257, 7161, 281, 51060], "temperature": 0.0, "avg_logprob": -0.08395233647576694, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00884639099240303}, {"id": 3410, "seek": 1418388, "start": 14198.92, "end": 14202.92, "text": " feature map being returned to us, right? Because for two filters, that means we need two maps,", "tokens": [51116, 4111, 4471, 885, 8752, 281, 505, 11, 558, 30, 1436, 337, 732, 15995, 11, 300, 1355, 321, 643, 732, 11317, 11, 51316], "temperature": 0.0, "avg_logprob": -0.08395233647576694, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00884639099240303}, {"id": 3411, "seek": 1418388, "start": 14203.56, "end": 14208.759999999998, "text": " quantifying the presence of both of those filters. So for this green box that we're looking on at", "tokens": [51348, 4426, 5489, 264, 6814, 295, 1293, 295, 729, 15995, 13, 407, 337, 341, 3092, 2424, 300, 321, 434, 1237, 322, 412, 51608], "temperature": 0.0, "avg_logprob": -0.08395233647576694, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00884639099240303}, {"id": 3412, "seek": 1420876, "start": 14208.76, "end": 14213.880000000001, "text": " the left side here, we'll look for this first filter here. And what do we get? Well, the way we", "tokens": [50364, 264, 1411, 1252, 510, 11, 321, 603, 574, 337, 341, 700, 6608, 510, 13, 400, 437, 360, 321, 483, 30, 1042, 11, 264, 636, 321, 50620], "temperature": 0.0, "avg_logprob": -0.0701350467961009, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.028432700783014297}, {"id": 3413, "seek": 1420876, "start": 14213.880000000001, "end": 14218.68, "text": " actually do this, the way we look at this filter, is we take the cross product, or actually not the", "tokens": [50620, 767, 360, 341, 11, 264, 636, 321, 574, 412, 341, 6608, 11, 307, 321, 747, 264, 3278, 1674, 11, 420, 767, 406, 264, 50860], "temperature": 0.0, "avg_logprob": -0.0701350467961009, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.028432700783014297}, {"id": 3414, "seek": 1420876, "start": 14218.68, "end": 14223.48, "text": " cross product, the dot product, sorry, between this little green box and this filter, right,", "tokens": [50860, 3278, 1674, 11, 264, 5893, 1674, 11, 2597, 11, 1296, 341, 707, 3092, 2424, 293, 341, 6608, 11, 558, 11, 51100], "temperature": 0.0, "avg_logprob": -0.0701350467961009, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.028432700783014297}, {"id": 3415, "seek": 1420876, "start": 14223.48, "end": 14228.04, "text": " because they're both pixels, they're both actually numeric values down at the bottom. So what we do", "tokens": [51100, 570, 436, 434, 1293, 18668, 11, 436, 434, 1293, 767, 7866, 299, 4190, 760, 412, 264, 2767, 13, 407, 437, 321, 360, 51328], "temperature": 0.0, "avg_logprob": -0.0701350467961009, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.028432700783014297}, {"id": 3416, "seek": 1420876, "start": 14228.04, "end": 14233.880000000001, "text": " is we take that dot product, which essentially means we're element wise, adding, or what is it", "tokens": [51328, 307, 321, 747, 300, 5893, 1674, 11, 597, 4476, 1355, 321, 434, 4478, 10829, 11, 5127, 11, 420, 437, 307, 309, 51620], "temperature": 0.0, "avg_logprob": -0.0701350467961009, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.028432700783014297}, {"id": 3417, "seek": 1423388, "start": 14233.88, "end": 14238.92, "text": " element wise, multiplying all of these pixels by each other. So if this pixel values is zero,", "tokens": [50364, 4478, 10829, 11, 30955, 439, 295, 613, 18668, 538, 1184, 661, 13, 407, 498, 341, 19261, 4190, 307, 4018, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3418, "seek": 1423388, "start": 14238.92, "end": 14242.839999999998, "text": " right, because it's white, or it could be the other way around, we could say white is one,", "tokens": [50616, 558, 11, 570, 309, 311, 2418, 11, 420, 309, 727, 312, 264, 661, 636, 926, 11, 321, 727, 584, 2418, 307, 472, 11, 50812], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3419, "seek": 1423388, "start": 14242.839999999998, "end": 14247.72, "text": " black is zero, it doesn't really matter, right? If this is a zero, and this is a one, these are", "tokens": [50812, 2211, 307, 4018, 11, 309, 1177, 380, 534, 1871, 11, 558, 30, 759, 341, 307, 257, 4018, 11, 293, 341, 307, 257, 472, 11, 613, 366, 51056], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3420, "seek": 1423388, "start": 14247.72, "end": 14252.679999999998, "text": " obviously very different. And when we do the dot product of those two, so we multiply them together,", "tokens": [51056, 2745, 588, 819, 13, 400, 562, 321, 360, 264, 5893, 1674, 295, 729, 732, 11, 370, 321, 12972, 552, 1214, 11, 51304], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3421, "seek": 1423388, "start": 14252.679999999998, "end": 14256.92, "text": " then in our output feature, we would have a zero, right, that's kind of the way it works. So we do", "tokens": [51304, 550, 294, 527, 5598, 4111, 11, 321, 576, 362, 257, 4018, 11, 558, 11, 300, 311, 733, 295, 264, 636, 309, 1985, 13, 407, 321, 360, 51516], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3422, "seek": 1423388, "start": 14256.92, "end": 14262.039999999999, "text": " this dot product of this entire thing. If you don't know the dot product is I'm not really going to", "tokens": [51516, 341, 5893, 1674, 295, 341, 2302, 551, 13, 759, 291, 500, 380, 458, 264, 5893, 1674, 307, 286, 478, 406, 534, 516, 281, 51772], "temperature": 0.0, "avg_logprob": -0.07377829582862605, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.037323735654354095}, {"id": 3423, "seek": 1426204, "start": 14262.04, "end": 14265.560000000001, "text": " go into that. But we do the dot product, and that gives us some value essentially telling us how", "tokens": [50364, 352, 666, 300, 13, 583, 321, 360, 264, 5893, 1674, 11, 293, 300, 2709, 505, 512, 2158, 4476, 3585, 505, 577, 50540], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3424, "seek": 1426204, "start": 14265.560000000001, "end": 14270.68, "text": " similar these two blocks are. So how similar this sample is that we're taking of the image and the", "tokens": [50540, 2531, 613, 732, 8474, 366, 13, 407, 577, 2531, 341, 6889, 307, 300, 321, 434, 1940, 295, 264, 3256, 293, 264, 50796], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3425, "seek": 1426204, "start": 14270.68, "end": 14275.160000000002, "text": " filter that we're looking for. They're very similar, we're going to likely put a one or something telling", "tokens": [50796, 6608, 300, 321, 434, 1237, 337, 13, 814, 434, 588, 2531, 11, 321, 434, 516, 281, 3700, 829, 257, 472, 420, 746, 3585, 51020], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3426, "seek": 1426204, "start": 14275.160000000002, "end": 14279.240000000002, "text": " us, you know, they're very close together. They're not similar at all, we're going to put a zero. So", "tokens": [51020, 505, 11, 291, 458, 11, 436, 434, 588, 1998, 1214, 13, 814, 434, 406, 2531, 412, 439, 11, 321, 434, 516, 281, 829, 257, 4018, 13, 407, 51224], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3427, "seek": 1426204, "start": 14279.240000000002, "end": 14283.400000000001, "text": " in this case, for our first filter, we're probably going to have a value because this middle pixel", "tokens": [51224, 294, 341, 1389, 11, 337, 527, 700, 6608, 11, 321, 434, 1391, 516, 281, 362, 257, 2158, 570, 341, 2808, 19261, 51432], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3428, "seek": 1426204, "start": 14283.400000000001, "end": 14288.6, "text": " is the same as something like 0.12, right? But the all the other values are different. So it's", "tokens": [51432, 307, 264, 912, 382, 746, 411, 1958, 13, 4762, 11, 558, 30, 583, 264, 439, 264, 661, 4190, 366, 819, 13, 407, 309, 311, 51692], "temperature": 0.0, "avg_logprob": -0.07981427300055295, "compression_ratio": 1.8980891719745223, "no_speech_prob": 0.022282078862190247}, {"id": 3429, "seek": 1428860, "start": 14288.6, "end": 14294.44, "text": " not going to be very similar whatsoever. So then what we're going to do now is we'll look at the", "tokens": [50364, 406, 516, 281, 312, 588, 2531, 17076, 13, 407, 550, 437, 321, 434, 516, 281, 360, 586, 307, 321, 603, 574, 412, 264, 50656], "temperature": 0.0, "avg_logprob": -0.057909015595443605, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022517128381878138}, {"id": 3430, "seek": 1428860, "start": 14294.44, "end": 14299.4, "text": " actually second filter, which is this horizontal line. And in fact, we're going to get a very", "tokens": [50656, 767, 1150, 6608, 11, 597, 307, 341, 12750, 1622, 13, 400, 294, 1186, 11, 321, 434, 516, 281, 483, 257, 588, 50904], "temperature": 0.0, "avg_logprob": -0.057909015595443605, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022517128381878138}, {"id": 3431, "seek": 1428860, "start": 14299.4, "end": 14303.960000000001, "text": " similar output response here, probably something like, you know, 0.12, that's going to go in the", "tokens": [50904, 2531, 5598, 4134, 510, 11, 1391, 746, 411, 11, 291, 458, 11, 1958, 13, 4762, 11, 300, 311, 516, 281, 352, 294, 264, 51132], "temperature": 0.0, "avg_logprob": -0.057909015595443605, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022517128381878138}, {"id": 3432, "seek": 1428860, "start": 14303.960000000001, "end": 14309.720000000001, "text": " top left. And again, these are both maps representing each filter, right? So now we'll move our green", "tokens": [51132, 1192, 1411, 13, 400, 797, 11, 613, 366, 1293, 11317, 13460, 1184, 6608, 11, 558, 30, 407, 586, 321, 603, 1286, 527, 3092, 51420], "temperature": 0.0, "avg_logprob": -0.057909015595443605, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022517128381878138}, {"id": 3433, "seek": 1428860, "start": 14309.720000000001, "end": 14317.56, "text": " box over one, like this. So just shift that over one. And now we'll start looking at the next section.", "tokens": [51420, 2424, 670, 472, 11, 411, 341, 13, 407, 445, 5513, 300, 670, 472, 13, 400, 586, 321, 603, 722, 1237, 412, 264, 958, 3541, 13, 51812], "temperature": 0.0, "avg_logprob": -0.057909015595443605, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022517128381878138}, {"id": 3434, "seek": 1431756, "start": 14317.56, "end": 14321.72, "text": " And in fact, I'm going to see if I can erase this just to make it a little bit cleaner here.", "tokens": [50364, 400, 294, 1186, 11, 286, 478, 516, 281, 536, 498, 286, 393, 23525, 341, 445, 281, 652, 309, 257, 707, 857, 16532, 510, 13, 50572], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3435, "seek": 1431756, "start": 14324.119999999999, "end": 14328.359999999999, "text": " Get rid of the green, there we go. Okay, so we'll move this box over like this. And now we'll", "tokens": [50692, 3240, 3973, 295, 264, 3092, 11, 456, 321, 352, 13, 1033, 11, 370, 321, 603, 1286, 341, 2424, 670, 411, 341, 13, 400, 586, 321, 603, 50904], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3436, "seek": 1431756, "start": 14328.359999999999, "end": 14332.519999999999, "text": " start looking at this one, we'll do the exact same thing we did again before. So we're going to say,", "tokens": [50904, 722, 1237, 412, 341, 472, 11, 321, 603, 360, 264, 1900, 912, 551, 321, 630, 797, 949, 13, 407, 321, 434, 516, 281, 584, 11, 51112], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3437, "seek": 1431756, "start": 14332.519999999999, "end": 14336.519999999999, "text": " alright, how similar are these? Well, they're not similar at all. So we're going to get a zero for", "tokens": [51112, 5845, 11, 577, 2531, 366, 613, 30, 1042, 11, 436, 434, 406, 2531, 412, 439, 13, 407, 321, 434, 516, 281, 483, 257, 4018, 337, 51312], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3438, "seek": 1431756, "start": 14336.519999999999, "end": 14341.96, "text": " that first filter. How similar the other ones? Oh, actually, they're like a little bit similar.", "tokens": [51312, 300, 700, 6608, 13, 1012, 2531, 264, 661, 2306, 30, 876, 11, 767, 11, 436, 434, 411, 257, 707, 857, 2531, 13, 51584], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3439, "seek": 1431756, "start": 14341.96, "end": 14345.48, "text": " There's a lot of white that's kind of in the same space, like, you know, stuff like that. So we'll", "tokens": [51584, 821, 311, 257, 688, 295, 2418, 300, 311, 733, 295, 294, 264, 912, 1901, 11, 411, 11, 291, 458, 11, 1507, 411, 300, 13, 407, 321, 603, 51760], "temperature": 0.0, "avg_logprob": -0.07726148593645155, "compression_ratio": 1.7987616099071206, "no_speech_prob": 0.0016484379302710295}, {"id": 3440, "seek": 1434548, "start": 14345.48, "end": 14350.76, "text": " say maybe this is like 0.7, right? I'm just randomly picking these numbers. They are going to be much", "tokens": [50364, 584, 1310, 341, 307, 411, 1958, 13, 22, 11, 558, 30, 286, 478, 445, 16979, 8867, 613, 3547, 13, 814, 366, 516, 281, 312, 709, 50628], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3441, "seek": 1434548, "start": 14350.76, "end": 14355.0, "text": " different than what I'm putting in here. But I'm just trying to get you to understand what's kind", "tokens": [50628, 819, 813, 437, 286, 478, 3372, 294, 510, 13, 583, 286, 478, 445, 1382, 281, 483, 291, 281, 1223, 437, 311, 733, 50840], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3442, "seek": 1434548, "start": 14355.0, "end": 14358.52, "text": " of happening, right? And this is completely random, the way I'm making the numbers to just make sure", "tokens": [50840, 295, 2737, 11, 558, 30, 400, 341, 307, 2584, 4974, 11, 264, 636, 286, 478, 1455, 264, 3547, 281, 445, 652, 988, 51016], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3443, "seek": 1434548, "start": 14358.52, "end": 14361.88, "text": " you understand that because this is not exactly what it would look like. Okay, so then we're going", "tokens": [51016, 291, 1223, 300, 570, 341, 307, 406, 2293, 437, 309, 576, 574, 411, 13, 1033, 11, 370, 550, 321, 434, 516, 51184], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3444, "seek": 1434548, "start": 14361.88, "end": 14365.16, "text": " to move the box over one more time. Let's just erase this to keep this clean. This will be the", "tokens": [51184, 281, 1286, 264, 2424, 670, 472, 544, 565, 13, 961, 311, 445, 23525, 341, 281, 1066, 341, 2541, 13, 639, 486, 312, 264, 51348], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3445, "seek": 1434548, "start": 14365.16, "end": 14369.8, "text": " last time we do this for the purpose of this example. And now what we're going to have is wow,", "tokens": [51348, 1036, 565, 321, 360, 341, 337, 264, 4334, 295, 341, 1365, 13, 400, 586, 437, 321, 434, 516, 281, 362, 307, 6076, 11, 51580], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3446, "seek": 1434548, "start": 14369.8, "end": 14374.039999999999, "text": " we have a perfect match for the first filter. So we put one, the other ones like ads kind of", "tokens": [51580, 321, 362, 257, 2176, 2995, 337, 264, 700, 6608, 13, 407, 321, 829, 472, 11, 264, 661, 2306, 411, 10342, 733, 295, 51792], "temperature": 0.0, "avg_logprob": -0.07118524902168362, "compression_ratio": 1.8382749326145553, "no_speech_prob": 0.005729950498789549}, {"id": 3447, "seek": 1437404, "start": 14374.12, "end": 14378.04, "text": " similar as a few things that are different. So maybe this gets like a 0.4 or something, right?", "tokens": [50368, 2531, 382, 257, 1326, 721, 300, 366, 819, 13, 407, 1310, 341, 2170, 411, 257, 1958, 13, 19, 420, 746, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.13538214739631205, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.025173457339406013}, {"id": 3448, "seek": 1437404, "start": 14378.04, "end": 14381.160000000002, "text": " Whatever they are, we end up getting some value. So we'll fill in all these values, let's just", "tokens": [50564, 8541, 436, 366, 11, 321, 917, 493, 1242, 512, 2158, 13, 407, 321, 603, 2836, 294, 439, 613, 4190, 11, 718, 311, 445, 50720], "temperature": 0.0, "avg_logprob": -0.13538214739631205, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.025173457339406013}, {"id": 3449, "seek": 1437404, "start": 14381.160000000002, "end": 14386.28, "text": " put some arbitrary values here for now, just so we can do something with the example 0.7, 0,", "tokens": [50720, 829, 512, 23211, 4190, 510, 337, 586, 11, 445, 370, 321, 393, 360, 746, 365, 264, 1365, 1958, 13, 22, 11, 1958, 11, 50976], "temperature": 0.0, "avg_logprob": -0.13538214739631205, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.025173457339406013}, {"id": 3450, "seek": 1437404, "start": 14387.0, "end": 14400.04, "text": " 0.12, 0.42, 0.3, 0.9, 0.1, again, completely random, 0.4, 0.6. Alright, so this is now what", "tokens": [51012, 1958, 13, 4762, 11, 1958, 13, 15628, 11, 1958, 13, 18, 11, 1958, 13, 24, 11, 1958, 13, 16, 11, 797, 11, 2584, 4974, 11, 1958, 13, 19, 11, 1958, 13, 21, 13, 2798, 11, 370, 341, 307, 586, 437, 51664], "temperature": 0.0, "avg_logprob": -0.13538214739631205, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.025173457339406013}, {"id": 3451, "seek": 1440004, "start": 14400.04, "end": 14406.12, "text": " we've gotten our response map from looking at two filters on our original image of five by five.", "tokens": [50364, 321, 600, 5768, 527, 4134, 4471, 490, 1237, 412, 732, 15995, 322, 527, 3380, 3256, 295, 1732, 538, 1732, 13, 50668], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3452, "seek": 1440004, "start": 14406.12, "end": 14410.28, "text": " Now notice that the size of these is three by three. And obviously, the reason for that is", "tokens": [50668, 823, 3449, 300, 264, 2744, 295, 613, 307, 1045, 538, 1045, 13, 400, 2745, 11, 264, 1778, 337, 300, 307, 50876], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3453, "seek": 1440004, "start": 14410.28, "end": 14415.160000000002, "text": " because in a five by five image, when we're taking three by three samples, well, we can only take", "tokens": [50876, 570, 294, 257, 1732, 538, 1732, 3256, 11, 562, 321, 434, 1940, 1045, 538, 1045, 10938, 11, 731, 11, 321, 393, 787, 747, 51120], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3454, "seek": 1440004, "start": 14415.160000000002, "end": 14420.04, "text": " nine three by three samples, because when we go down a row, right, we're going to move down one,", "tokens": [51120, 4949, 1045, 538, 1045, 10938, 11, 570, 562, 321, 352, 760, 257, 5386, 11, 558, 11, 321, 434, 516, 281, 1286, 760, 472, 11, 51364], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3455, "seek": 1440004, "start": 14420.04, "end": 14423.240000000002, "text": " and we're going to do the same thing we did before of this, these three by three samples. And if we", "tokens": [51364, 293, 321, 434, 516, 281, 360, 264, 912, 551, 321, 630, 949, 295, 341, 11, 613, 1045, 538, 1045, 10938, 13, 400, 498, 321, 51524], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3456, "seek": 1440004, "start": 14423.240000000002, "end": 14428.28, "text": " add the amount of times we can do that, well, we just get three by three, which is not. So this", "tokens": [51524, 909, 264, 2372, 295, 1413, 321, 393, 360, 300, 11, 731, 11, 321, 445, 483, 1045, 538, 1045, 11, 597, 307, 406, 13, 407, 341, 51776], "temperature": 0.0, "avg_logprob": -0.056121501922607425, "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.09533550590276718}, {"id": 3457, "seek": 1442828, "start": 14428.28, "end": 14433.560000000001, "text": " now is kind of telling us of the presence of features in this original image map. Now the", "tokens": [50364, 586, 307, 733, 295, 3585, 505, 295, 264, 6814, 295, 4122, 294, 341, 3380, 3256, 4471, 13, 823, 264, 50628], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3458, "seek": 1442828, "start": 14433.560000000001, "end": 14439.400000000001, "text": " thing is, though, we're going to do this 64 times, right, for 64 filters or 32 filters or the amount", "tokens": [50628, 551, 307, 11, 1673, 11, 321, 434, 516, 281, 360, 341, 12145, 1413, 11, 558, 11, 337, 12145, 15995, 420, 8858, 15995, 420, 264, 2372, 50920], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3459, "seek": 1442828, "start": 14439.400000000001, "end": 14443.960000000001, "text": " of filters that we have. So we're going to have a lot of layers, like a ton of different layers,", "tokens": [50920, 295, 15995, 300, 321, 362, 13, 407, 321, 434, 516, 281, 362, 257, 688, 295, 7914, 11, 411, 257, 2952, 295, 819, 7914, 11, 51148], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3460, "seek": 1442828, "start": 14443.960000000001, "end": 14448.2, "text": " which means that we're going to be constantly expanding as we go through the convolutional", "tokens": [51148, 597, 1355, 300, 321, 434, 516, 281, 312, 6460, 14702, 382, 321, 352, 807, 264, 45216, 304, 51360], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3461, "seek": 1442828, "start": 14448.2, "end": 14454.36, "text": " layers, the depth of this, this kind of output feature map. And that means that there's a lot", "tokens": [51360, 7914, 11, 264, 7161, 295, 341, 11, 341, 733, 295, 5598, 4111, 4471, 13, 400, 300, 1355, 300, 456, 311, 257, 688, 51668], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3462, "seek": 1442828, "start": 14454.36, "end": 14457.560000000001, "text": " of computations that need to be done. And essentially, that means that this can be very", "tokens": [51668, 295, 2807, 763, 300, 643, 281, 312, 1096, 13, 400, 4476, 11, 300, 1355, 300, 341, 393, 312, 588, 51828], "temperature": 0.0, "avg_logprob": -0.05745462690080915, "compression_ratio": 1.9243986254295533, "no_speech_prob": 0.0021825903095304966}, {"id": 3463, "seek": 1445756, "start": 14457.56, "end": 14462.6, "text": " slow. So now we need to talk about an operation called pooling. So I'll backtrack a little bit,", "tokens": [50364, 2964, 13, 407, 586, 321, 643, 281, 751, 466, 364, 6916, 1219, 7005, 278, 13, 407, 286, 603, 646, 19466, 257, 707, 857, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07105299204337497, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0023229666985571384}, {"id": 3464, "seek": 1445756, "start": 14462.6, "end": 14466.6, "text": " but we will talk about pooling in a second. What's going to happen, right, is when we have all these", "tokens": [50616, 457, 321, 486, 751, 466, 7005, 278, 294, 257, 1150, 13, 708, 311, 516, 281, 1051, 11, 558, 11, 307, 562, 321, 362, 439, 613, 50816], "temperature": 0.0, "avg_logprob": -0.07105299204337497, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0023229666985571384}, {"id": 3465, "seek": 1445756, "start": 14466.6, "end": 14472.039999999999, "text": " layers that are generated, so this is called the output feature map, right, from this original", "tokens": [50816, 7914, 300, 366, 10833, 11, 370, 341, 307, 1219, 264, 5598, 4111, 4471, 11, 558, 11, 490, 341, 3380, 51088], "temperature": 0.0, "avg_logprob": -0.07105299204337497, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0023229666985571384}, {"id": 3466, "seek": 1445756, "start": 14472.039999999999, "end": 14476.76, "text": " image. What we're going to do is the next convolutional layer in the network is now going", "tokens": [51088, 3256, 13, 708, 321, 434, 516, 281, 360, 307, 264, 958, 45216, 304, 4583, 294, 264, 3209, 307, 586, 516, 51324], "temperature": 0.0, "avg_logprob": -0.07105299204337497, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0023229666985571384}, {"id": 3467, "seek": 1445756, "start": 14476.76, "end": 14482.039999999999, "text": " to do the process we just talked about, except on this output feature map, which means that", "tokens": [51324, 281, 360, 264, 1399, 321, 445, 2825, 466, 11, 3993, 322, 341, 5598, 4111, 4471, 11, 597, 1355, 300, 51588], "temperature": 0.0, "avg_logprob": -0.07105299204337497, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0023229666985571384}, {"id": 3468, "seek": 1448204, "start": 14482.12, "end": 14487.960000000001, "text": " since this one was picking up things like lines and edges, right, the next convolutional layer,", "tokens": [50368, 1670, 341, 472, 390, 8867, 493, 721, 411, 3876, 293, 8819, 11, 558, 11, 264, 958, 45216, 304, 4583, 11, 50660], "temperature": 0.0, "avg_logprob": -0.06985386282996794, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.007576375734061003}, {"id": 3469, "seek": 1448204, "start": 14487.960000000001, "end": 14494.12, "text": " we'll pick up combinations of lines and edges and maybe find what a curve is, right, we'll slowly", "tokens": [50660, 321, 603, 1888, 493, 21267, 295, 3876, 293, 8819, 293, 1310, 915, 437, 257, 7605, 307, 11, 558, 11, 321, 603, 5692, 50968], "temperature": 0.0, "avg_logprob": -0.06985386282996794, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.007576375734061003}, {"id": 3470, "seek": 1448204, "start": 14494.12, "end": 14499.640000000001, "text": " work our way up from very, very small amount of pixels, defining more and more, almost I want to", "tokens": [50968, 589, 527, 636, 493, 490, 588, 11, 588, 1359, 2372, 295, 18668, 11, 17827, 544, 293, 544, 11, 1920, 286, 528, 281, 51244], "temperature": 0.0, "avg_logprob": -0.06985386282996794, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.007576375734061003}, {"id": 3471, "seek": 1448204, "start": 14499.640000000001, "end": 14505.720000000001, "text": " say abstract, different features that exist in the image. And this is what really allows us to", "tokens": [51244, 584, 12649, 11, 819, 4122, 300, 2514, 294, 264, 3256, 13, 400, 341, 307, 437, 534, 4045, 505, 281, 51548], "temperature": 0.0, "avg_logprob": -0.06985386282996794, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.007576375734061003}, {"id": 3472, "seek": 1448204, "start": 14505.720000000001, "end": 14509.160000000002, "text": " do some amazing things with a convolutional neural network, when we have a ton of different", "tokens": [51548, 360, 512, 2243, 721, 365, 257, 45216, 304, 18161, 3209, 11, 562, 321, 362, 257, 2952, 295, 819, 51720], "temperature": 0.0, "avg_logprob": -0.06985386282996794, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.007576375734061003}, {"id": 3473, "seek": 1450916, "start": 14509.16, "end": 14513.48, "text": " layers stacking up on each other, we can pick out all the small little edges, which are pretty", "tokens": [50364, 7914, 41376, 493, 322, 1184, 661, 11, 321, 393, 1888, 484, 439, 264, 1359, 707, 8819, 11, 597, 366, 1238, 50580], "temperature": 0.0, "avg_logprob": -0.07932351945756792, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.1402987241744995}, {"id": 3474, "seek": 1450916, "start": 14513.48, "end": 14519.16, "text": " easy to find. And with all these combinations of layers working together, we can even find things", "tokens": [50580, 1858, 281, 915, 13, 400, 365, 439, 613, 21267, 295, 7914, 1364, 1214, 11, 321, 393, 754, 915, 721, 50864], "temperature": 0.0, "avg_logprob": -0.07932351945756792, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.1402987241744995}, {"id": 3475, "seek": 1450916, "start": 14519.16, "end": 14527.16, "text": " like, say, eyes, right, or feet, or heads or face, right, we can find very complicated structures,", "tokens": [50864, 411, 11, 584, 11, 2575, 11, 558, 11, 420, 3521, 11, 420, 8050, 420, 1851, 11, 558, 11, 321, 393, 915, 588, 6179, 9227, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07932351945756792, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.1402987241744995}, {"id": 3476, "seek": 1450916, "start": 14527.16, "end": 14531.88, "text": " because we slowly work our way up starting by solving very easy problem, which are like finding", "tokens": [51264, 570, 321, 5692, 589, 527, 636, 493, 2891, 538, 12606, 588, 1858, 1154, 11, 597, 366, 411, 5006, 51500], "temperature": 0.0, "avg_logprob": -0.07932351945756792, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.1402987241744995}, {"id": 3477, "seek": 1450916, "start": 14531.88, "end": 14536.84, "text": " lines, and then finding combinations of lines, combination of edges, shapes and very abstract", "tokens": [51500, 3876, 11, 293, 550, 5006, 21267, 295, 3876, 11, 6562, 295, 8819, 11, 10854, 293, 588, 12649, 51748], "temperature": 0.0, "avg_logprob": -0.07932351945756792, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.1402987241744995}, {"id": 3478, "seek": 1453684, "start": 14536.92, "end": 14542.04, "text": " things. That's how this convolutional network works. So we've done that now, it's now time to", "tokens": [50368, 721, 13, 663, 311, 577, 341, 45216, 304, 3209, 1985, 13, 407, 321, 600, 1096, 300, 586, 11, 309, 311, 586, 565, 281, 50624], "temperature": 0.0, "avg_logprob": -0.1055470069249471, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.02757939323782921}, {"id": 3479, "seek": 1453684, "start": 14542.04, "end": 14546.84, "text": " talk about pooling. And we'll also talk about pat actually, we'll go padding first before we go", "tokens": [50624, 751, 466, 7005, 278, 13, 400, 321, 603, 611, 751, 466, 1947, 767, 11, 321, 603, 352, 39562, 700, 949, 321, 352, 50864], "temperature": 0.0, "avg_logprob": -0.1055470069249471, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.02757939323782921}, {"id": 3480, "seek": 1453684, "start": 14546.84, "end": 14550.84, "text": " pooling, I just, it doesn't really matter what order we talk about this in. But I just think", "tokens": [50864, 7005, 278, 11, 286, 445, 11, 309, 1177, 380, 534, 1871, 437, 1668, 321, 751, 466, 341, 294, 13, 583, 286, 445, 519, 51064], "temperature": 0.0, "avg_logprob": -0.1055470069249471, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.02757939323782921}, {"id": 3481, "seek": 1453684, "start": 14550.84, "end": 14555.960000000001, "text": " padding makes sense based on the way we're going right now. So sometimes we want to make sure that", "tokens": [51064, 39562, 1669, 2020, 2361, 322, 264, 636, 321, 434, 516, 558, 586, 13, 407, 2171, 321, 528, 281, 652, 988, 300, 51320], "temperature": 0.0, "avg_logprob": -0.1055470069249471, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.02757939323782921}, {"id": 3482, "seek": 1453684, "start": 14555.960000000001, "end": 14563.72, "text": " the output feature map from our original image here is the same dimensions or same size as this,", "tokens": [51320, 264, 5598, 4111, 4471, 490, 527, 3380, 3256, 510, 307, 264, 912, 12819, 420, 912, 2744, 382, 341, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1055470069249471, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.02757939323782921}, {"id": 3483, "seek": 1456372, "start": 14563.72, "end": 14568.679999999998, "text": " right? So this was five by five, obviously. And this is three by three. So if we want this to be", "tokens": [50364, 558, 30, 407, 341, 390, 1732, 538, 1732, 11, 2745, 13, 400, 341, 307, 1045, 538, 1045, 13, 407, 498, 321, 528, 341, 281, 312, 50612], "temperature": 0.0, "avg_logprob": -0.08337431806858962, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.037317417562007904}, {"id": 3484, "seek": 1456372, "start": 14568.679999999998, "end": 14573.32, "text": " five by five, as an output, what we need to do is add something called padding to our original", "tokens": [50612, 1732, 538, 1732, 11, 382, 364, 5598, 11, 437, 321, 643, 281, 360, 307, 909, 746, 1219, 39562, 281, 527, 3380, 50844], "temperature": 0.0, "avg_logprob": -0.08337431806858962, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.037317417562007904}, {"id": 3485, "seek": 1456372, "start": 14573.32, "end": 14578.119999999999, "text": " image. So padding is essentially just adding an extra row and column on each side of our image", "tokens": [50844, 3256, 13, 407, 39562, 307, 4476, 445, 5127, 364, 2857, 5386, 293, 7738, 322, 1184, 1252, 295, 527, 3256, 51084], "temperature": 0.0, "avg_logprob": -0.08337431806858962, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.037317417562007904}, {"id": 3486, "seek": 1456372, "start": 14578.119999999999, "end": 14583.32, "text": " here. So that when we, and we just fill in all these pixels in like kind of the padded pixels", "tokens": [51084, 510, 13, 407, 300, 562, 321, 11, 293, 321, 445, 2836, 294, 439, 613, 18668, 294, 411, 733, 295, 264, 6887, 9207, 18668, 51344], "temperature": 0.0, "avg_logprob": -0.08337431806858962, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.037317417562007904}, {"id": 3487, "seek": 1456372, "start": 14583.32, "end": 14588.439999999999, "text": " here, I just blank random pixels, they don't mean anything. Essentially, why we do that is so that", "tokens": [51344, 510, 11, 286, 445, 8247, 4974, 18668, 11, 436, 500, 380, 914, 1340, 13, 23596, 11, 983, 321, 360, 300, 307, 370, 300, 51600], "temperature": 0.0, "avg_logprob": -0.08337431806858962, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.037317417562007904}, {"id": 3488, "seek": 1458844, "start": 14588.52, "end": 14595.08, "text": " when we do our three by three sample size here like this, we can take a three by three sample", "tokens": [50368, 562, 321, 360, 527, 1045, 538, 1045, 6889, 2744, 510, 411, 341, 11, 321, 393, 747, 257, 1045, 538, 1045, 6889, 50696], "temperature": 0.0, "avg_logprob": -0.06926749182529136, "compression_ratio": 2.03781512605042, "no_speech_prob": 0.02758154459297657}, {"id": 3489, "seek": 1458844, "start": 14595.08, "end": 14600.84, "text": " where every single pixel is in the center of that sample. Because right now, this pixel is not in", "tokens": [50696, 689, 633, 2167, 19261, 307, 294, 264, 3056, 295, 300, 6889, 13, 1436, 558, 586, 11, 341, 19261, 307, 406, 294, 50984], "temperature": 0.0, "avg_logprob": -0.06926749182529136, "compression_ratio": 2.03781512605042, "no_speech_prob": 0.02758154459297657}, {"id": 3490, "seek": 1458844, "start": 14600.84, "end": 14605.880000000001, "text": " the center. This pixel can never be in the center, this pixel can never be in the center, only,", "tokens": [50984, 264, 3056, 13, 639, 19261, 393, 1128, 312, 294, 264, 3056, 11, 341, 19261, 393, 1128, 312, 294, 264, 3056, 11, 787, 11, 51236], "temperature": 0.0, "avg_logprob": -0.06926749182529136, "compression_ratio": 2.03781512605042, "no_speech_prob": 0.02758154459297657}, {"id": 3491, "seek": 1458844, "start": 14605.880000000001, "end": 14610.84, "text": " you know, a few pixels get to be in the center. And what this allows us to do is generate an output", "tokens": [51236, 291, 458, 11, 257, 1326, 18668, 483, 281, 312, 294, 264, 3056, 13, 400, 437, 341, 4045, 505, 281, 360, 307, 8460, 364, 5598, 51484], "temperature": 0.0, "avg_logprob": -0.06926749182529136, "compression_ratio": 2.03781512605042, "no_speech_prob": 0.02758154459297657}, {"id": 3492, "seek": 1458844, "start": 14610.84, "end": 14615.880000000001, "text": " map that is the same size as our original input, and allows us to look at features that are maybe", "tokens": [51484, 4471, 300, 307, 264, 912, 2744, 382, 527, 3380, 4846, 11, 293, 4045, 505, 281, 574, 412, 4122, 300, 366, 1310, 51736], "temperature": 0.0, "avg_logprob": -0.06926749182529136, "compression_ratio": 2.03781512605042, "no_speech_prob": 0.02758154459297657}, {"id": 3493, "seek": 1461588, "start": 14615.88, "end": 14620.679999999998, "text": " right on the edges of images that we might not have been able to see before. Now, this isn't", "tokens": [50364, 558, 322, 264, 8819, 295, 5267, 300, 321, 1062, 406, 362, 668, 1075, 281, 536, 949, 13, 823, 11, 341, 1943, 380, 50604], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3494, "seek": 1461588, "start": 14620.679999999998, "end": 14624.279999999999, "text": " super important when you go to like very large images, but it just something to consider you", "tokens": [50604, 1687, 1021, 562, 291, 352, 281, 411, 588, 2416, 5267, 11, 457, 309, 445, 746, 281, 1949, 291, 50784], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3495, "seek": 1461588, "start": 14624.279999999999, "end": 14629.24, "text": " can add padding, we may do this as we get through our examples. And there's also something called", "tokens": [50784, 393, 909, 39562, 11, 321, 815, 360, 341, 382, 321, 483, 807, 527, 5110, 13, 400, 456, 311, 611, 746, 1219, 51032], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3496, "seek": 1461588, "start": 14629.24, "end": 14634.359999999999, "text": " stride, which I want to talk about as well. So what a stride is is essentially how much we move", "tokens": [51032, 1056, 482, 11, 597, 286, 528, 281, 751, 466, 382, 731, 13, 407, 437, 257, 1056, 482, 307, 307, 4476, 577, 709, 321, 1286, 51288], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3497, "seek": 1461588, "start": 14634.359999999999, "end": 14640.359999999999, "text": " this sample box, every time that we're about to move it, right? So before, like, so let's say", "tokens": [51288, 341, 6889, 2424, 11, 633, 565, 300, 321, 434, 466, 281, 1286, 309, 11, 558, 30, 407, 949, 11, 411, 11, 370, 718, 311, 584, 51588], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3498, "seek": 1461588, "start": 14640.359999999999, "end": 14644.439999999999, "text": " we're doing example with padding here, right, we are first sample, we would take here. And again,", "tokens": [51588, 321, 434, 884, 1365, 365, 39562, 510, 11, 558, 11, 321, 366, 700, 6889, 11, 321, 576, 747, 510, 13, 400, 797, 11, 51792], "temperature": 0.0, "avg_logprob": -0.0914245332990374, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.009411700069904327}, {"id": 3499, "seek": 1464444, "start": 14644.44, "end": 14649.720000000001, "text": " these pixels are just added, we added them in to make this work better for us. You would assume", "tokens": [50364, 613, 18668, 366, 445, 3869, 11, 321, 3869, 552, 294, 281, 652, 341, 589, 1101, 337, 505, 13, 509, 576, 6552, 50628], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3500, "seek": 1464444, "start": 14649.720000000001, "end": 14653.640000000001, "text": " that the next time we move the box, we're going to move it one pixel over, that's called a stride of", "tokens": [50628, 300, 264, 958, 565, 321, 1286, 264, 2424, 11, 321, 434, 516, 281, 1286, 309, 472, 19261, 670, 11, 300, 311, 1219, 257, 1056, 482, 295, 50824], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3501, "seek": 1464444, "start": 14653.640000000001, "end": 14658.76, "text": " one, we can do that. But we also can employ stride of two, which means we'll move over by two.", "tokens": [50824, 472, 11, 321, 393, 360, 300, 13, 583, 321, 611, 393, 3188, 1056, 482, 295, 732, 11, 597, 1355, 321, 603, 1286, 670, 538, 732, 13, 51080], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3502, "seek": 1464444, "start": 14659.4, "end": 14663.560000000001, "text": " Obviously, the larger your stride, the smaller your output feature map is going to be. So you", "tokens": [51112, 7580, 11, 264, 4833, 428, 1056, 482, 11, 264, 4356, 428, 5598, 4111, 4471, 307, 516, 281, 312, 13, 407, 291, 51320], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3503, "seek": 1464444, "start": 14663.560000000001, "end": 14667.0, "text": " might want to add more padding. Well, you don't want to add too much padding, but it's just", "tokens": [51320, 1062, 528, 281, 909, 544, 39562, 13, 1042, 11, 291, 500, 380, 528, 281, 909, 886, 709, 39562, 11, 457, 309, 311, 445, 51492], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3504, "seek": 1464444, "start": 14667.0, "end": 14672.36, "text": " something to consider. And we will use a stride in different instances. Okay, so that's great.", "tokens": [51492, 746, 281, 1949, 13, 400, 321, 486, 764, 257, 1056, 482, 294, 819, 14519, 13, 1033, 11, 370, 300, 311, 869, 13, 51760], "temperature": 0.0, "avg_logprob": -0.07203678679622076, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.002980900462716818}, {"id": 3505, "seek": 1467236, "start": 14672.36, "end": 14676.2, "text": " That hopefully makes sense. Let's erase this. Now we don't need this anymore. We talked about", "tokens": [50364, 663, 4696, 1669, 2020, 13, 961, 311, 23525, 341, 13, 823, 321, 500, 380, 643, 341, 3602, 13, 492, 2825, 466, 50556], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3506, "seek": 1467236, "start": 14676.2, "end": 14679.800000000001, "text": " padding, we talked about the stride. And now we're going to talk about a pooling operation,", "tokens": [50556, 39562, 11, 321, 2825, 466, 264, 1056, 482, 13, 400, 586, 321, 434, 516, 281, 751, 466, 257, 7005, 278, 6916, 11, 50736], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3507, "seek": 1467236, "start": 14679.800000000001, "end": 14685.560000000001, "text": " which is very important. So kind of the idea is that we're going to have a ton of layers, right,", "tokens": [50736, 597, 307, 588, 1021, 13, 407, 733, 295, 264, 1558, 307, 300, 321, 434, 516, 281, 362, 257, 2952, 295, 7914, 11, 558, 11, 51024], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3508, "seek": 1467236, "start": 14685.560000000001, "end": 14689.32, "text": " for all these filters. And we're just going to have a lot of numbers, a lot of computations. And", "tokens": [51024, 337, 439, 613, 15995, 13, 400, 321, 434, 445, 516, 281, 362, 257, 688, 295, 3547, 11, 257, 688, 295, 2807, 763, 13, 400, 51212], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3509, "seek": 1467236, "start": 14689.32, "end": 14692.68, "text": " there must be some way to make these a little bit simpler, a little bit easier to use. Well,", "tokens": [51212, 456, 1633, 312, 512, 636, 281, 652, 613, 257, 707, 857, 18587, 11, 257, 707, 857, 3571, 281, 764, 13, 1042, 11, 51380], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3510, "seek": 1467236, "start": 14692.68, "end": 14697.24, "text": " yes, that's true. And there is a way to do that. And that's called pooling. So there's three different", "tokens": [51380, 2086, 11, 300, 311, 2074, 13, 400, 456, 307, 257, 636, 281, 360, 300, 13, 400, 300, 311, 1219, 7005, 278, 13, 407, 456, 311, 1045, 819, 51608], "temperature": 0.0, "avg_logprob": -0.0716747370633212, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0031725189182907343}, {"id": 3511, "seek": 1469724, "start": 14697.24, "end": 14704.039999999999, "text": " types of pooling. Well, it is more but the basic ones are min, max, and average. And essentially,", "tokens": [50364, 3467, 295, 7005, 278, 13, 1042, 11, 309, 307, 544, 457, 264, 3875, 2306, 366, 923, 11, 11469, 11, 293, 4274, 13, 400, 4476, 11, 50704], "temperature": 0.0, "avg_logprob": -0.09708143802399331, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.031141158193349838}, {"id": 3512, "seek": 1469724, "start": 14704.039999999999, "end": 14711.16, "text": " a pooling operation is just taking specific values from a sample of the output feature map. So", "tokens": [50704, 257, 7005, 278, 6916, 307, 445, 1940, 2685, 4190, 490, 257, 6889, 295, 264, 5598, 4111, 4471, 13, 407, 51060], "temperature": 0.0, "avg_logprob": -0.09708143802399331, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.031141158193349838}, {"id": 3513, "seek": 1469724, "start": 14711.16, "end": 14716.28, "text": " once we generate this output feature map, what we do to reduce its dimensionality and just make it", "tokens": [51060, 1564, 321, 8460, 341, 5598, 4111, 4471, 11, 437, 321, 360, 281, 5407, 1080, 10139, 1860, 293, 445, 652, 309, 51316], "temperature": 0.0, "avg_logprob": -0.09708143802399331, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.031141158193349838}, {"id": 3514, "seek": 1469724, "start": 14716.28, "end": 14723.24, "text": " a little bit easier to work with is what we sample typically two by two areas of this output feature", "tokens": [51316, 257, 707, 857, 3571, 281, 589, 365, 307, 437, 321, 6889, 5850, 732, 538, 732, 3179, 295, 341, 5598, 4111, 51664], "temperature": 0.0, "avg_logprob": -0.09708143802399331, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.031141158193349838}, {"id": 3515, "seek": 1472324, "start": 14723.24, "end": 14729.64, "text": " map. And just take either the min, max or average value of all the values inside of here and map", "tokens": [50364, 4471, 13, 400, 445, 747, 2139, 264, 923, 11, 11469, 420, 4274, 2158, 295, 439, 264, 4190, 1854, 295, 510, 293, 4471, 50684], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3516, "seek": 1472324, "start": 14729.64, "end": 14735.48, "text": " these, we're going to go back this way to a new feature map that's twice, like one times the size", "tokens": [50684, 613, 11, 321, 434, 516, 281, 352, 646, 341, 636, 281, 257, 777, 4111, 4471, 300, 311, 6091, 11, 411, 472, 1413, 264, 2744, 50976], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3517, "seek": 1472324, "start": 14735.48, "end": 14739.24, "text": " essentially, or not, what am I saying, two times smaller than this original map. And it's kind of", "tokens": [50976, 4476, 11, 420, 406, 11, 437, 669, 286, 1566, 11, 732, 1413, 4356, 813, 341, 3380, 4471, 13, 400, 309, 311, 733, 295, 51164], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3518, "seek": 1472324, "start": 14739.24, "end": 14742.84, "text": " hard with three, like three by three, to really show you this. But essentially, what's going to", "tokens": [51164, 1152, 365, 1045, 11, 411, 1045, 538, 1045, 11, 281, 534, 855, 291, 341, 13, 583, 4476, 11, 437, 311, 516, 281, 51344], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3519, "seek": 1472324, "start": 14742.84, "end": 14747.64, "text": " end up happening is we're going to have something like this. So we're going to take the sample here,", "tokens": [51344, 917, 493, 2737, 307, 321, 434, 516, 281, 362, 746, 411, 341, 13, 407, 321, 434, 516, 281, 747, 264, 6889, 510, 11, 51584], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3520, "seek": 1472324, "start": 14747.64, "end": 14751.32, "text": " we're going to say, okay, what are we doing min, max or average pooling, if we're doing min pooling,", "tokens": [51584, 321, 434, 516, 281, 584, 11, 1392, 11, 437, 366, 321, 884, 923, 11, 11469, 420, 4274, 7005, 278, 11, 498, 321, 434, 884, 923, 7005, 278, 11, 51768], "temperature": 0.0, "avg_logprob": -0.10954158734052609, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.027582816779613495}, {"id": 3521, "seek": 1475132, "start": 14751.4, "end": 14754.92, "text": " we're going to take the smallest value, which means we'll take zero. If we're doing max pooling,", "tokens": [50368, 321, 434, 516, 281, 747, 264, 16998, 2158, 11, 597, 1355, 321, 603, 747, 4018, 13, 759, 321, 434, 884, 11469, 7005, 278, 11, 50544], "temperature": 0.0, "avg_logprob": -0.09054457301824866, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.007120825815945864}, {"id": 3522, "seek": 1475132, "start": 14754.92, "end": 14759.32, "text": " we'll take the maximum value, which means we'll take 0.3. If we're doing average, we're probably", "tokens": [50544, 321, 603, 747, 264, 6674, 2158, 11, 597, 1355, 321, 603, 747, 1958, 13, 18, 13, 759, 321, 434, 884, 4274, 11, 321, 434, 1391, 50764], "temperature": 0.0, "avg_logprob": -0.09054457301824866, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.007120825815945864}, {"id": 3523, "seek": 1475132, "start": 14759.32, "end": 14764.44, "text": " going to get an average value of close to what 0.2, maybe. So let's say 0.2, we'll go there.", "tokens": [50764, 516, 281, 483, 364, 4274, 2158, 295, 1998, 281, 437, 1958, 13, 17, 11, 1310, 13, 407, 718, 311, 584, 1958, 13, 17, 11, 321, 603, 352, 456, 13, 51020], "temperature": 0.0, "avg_logprob": -0.09054457301824866, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.007120825815945864}, {"id": 3524, "seek": 1475132, "start": 14765.0, "end": 14770.199999999999, "text": " That's how we do that with pooling. Again, just to make this feature map smaller. So we'll do that", "tokens": [51048, 663, 311, 577, 321, 360, 300, 365, 7005, 278, 13, 3764, 11, 445, 281, 652, 341, 4111, 4471, 4356, 13, 407, 321, 603, 360, 300, 51308], "temperature": 0.0, "avg_logprob": -0.09054457301824866, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.007120825815945864}, {"id": 3525, "seek": 1475132, "start": 14770.199999999999, "end": 14776.119999999999, "text": " for both of the filters. But let's just say this is 0.2. Let's say this here that I'm blocking off", "tokens": [51308, 337, 1293, 295, 264, 15995, 13, 583, 718, 311, 445, 584, 341, 307, 1958, 13, 17, 13, 961, 311, 584, 341, 510, 300, 286, 478, 17776, 766, 51604], "temperature": 0.0, "avg_logprob": -0.09054457301824866, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.007120825815945864}, {"id": 3526, "seek": 1477612, "start": 14776.12, "end": 14783.160000000002, "text": " is, I don't know, what is this going to be 0.6? It's hard to do this average with four numbers,", "tokens": [50364, 307, 11, 286, 500, 380, 458, 11, 437, 307, 341, 516, 281, 312, 1958, 13, 21, 30, 467, 311, 1152, 281, 360, 341, 4274, 365, 1451, 3547, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10293650419815727, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.012053130194544792}, {"id": 3527, "seek": 1477612, "start": 14783.160000000002, "end": 14791.240000000002, "text": " let's say this one down here is going to be 0. I don't know, let's just do two one or something.", "tokens": [50716, 718, 311, 584, 341, 472, 760, 510, 307, 516, 281, 312, 1958, 13, 286, 500, 380, 458, 11, 718, 311, 445, 360, 732, 472, 420, 746, 13, 51120], "temperature": 0.0, "avg_logprob": -0.10293650419815727, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.012053130194544792}, {"id": 3528, "seek": 1477612, "start": 14791.240000000002, "end": 14795.240000000002, "text": " And then this last one here, we okay, we got some bigger values, maybe this will be like 0.4.", "tokens": [51120, 400, 550, 341, 1036, 472, 510, 11, 321, 1392, 11, 321, 658, 512, 3801, 4190, 11, 1310, 341, 486, 312, 411, 1958, 13, 19, 13, 51320], "temperature": 0.0, "avg_logprob": -0.10293650419815727, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.012053130194544792}, {"id": 3529, "seek": 1477612, "start": 14796.36, "end": 14801.0, "text": " Okay, so that's one, the one down here will have some values of its own, we'll just do squiggles", "tokens": [51376, 1033, 11, 370, 300, 311, 472, 11, 264, 472, 760, 510, 486, 362, 512, 4190, 295, 1080, 1065, 11, 321, 603, 445, 360, 2339, 19469, 51608], "temperature": 0.0, "avg_logprob": -0.10293650419815727, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.012053130194544792}, {"id": 3530, "seek": 1480100, "start": 14801.0, "end": 14805.8, "text": " to represent that it has something. And we've effectively done a max pooling operation on", "tokens": [50364, 281, 2906, 300, 309, 575, 746, 13, 400, 321, 600, 8659, 1096, 257, 11469, 7005, 278, 6916, 322, 50604], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3531, "seek": 1480100, "start": 14805.8, "end": 14811.16, "text": " this, we've reduced the size of it by about half. And that is kind of how that works. Now,", "tokens": [50604, 341, 11, 321, 600, 9212, 264, 2744, 295, 309, 538, 466, 1922, 13, 400, 300, 307, 733, 295, 577, 300, 1985, 13, 823, 11, 50872], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3532, "seek": 1480100, "start": 14811.16, "end": 14817.16, "text": " typically what we do is we use a two by two pooling or like sample size like that, with a", "tokens": [50872, 5850, 437, 321, 360, 307, 321, 764, 257, 732, 538, 732, 7005, 278, 420, 411, 6889, 2744, 411, 300, 11, 365, 257, 51172], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3533, "seek": 1480100, "start": 14817.16, "end": 14822.2, "text": " stride of two, which actually means that we would straw it like this. But since we're not going to", "tokens": [51172, 1056, 482, 295, 732, 11, 597, 767, 1355, 300, 321, 576, 10099, 309, 411, 341, 13, 583, 1670, 321, 434, 406, 516, 281, 51424], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3534, "seek": 1480100, "start": 14822.2, "end": 14827.32, "text": " do padding on this layer right now, we'll just do a stride of one. And this is how we pull it.", "tokens": [51424, 360, 39562, 322, 341, 4583, 558, 586, 11, 321, 603, 445, 360, 257, 1056, 482, 295, 472, 13, 400, 341, 307, 577, 321, 2235, 309, 13, 51680], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3535, "seek": 1480100, "start": 14827.32, "end": 14830.6, "text": " Now the different kinds of pooling are used for different kind of things. The reason we would", "tokens": [51680, 823, 264, 819, 3685, 295, 7005, 278, 366, 1143, 337, 819, 733, 295, 721, 13, 440, 1778, 321, 576, 51844], "temperature": 0.0, "avg_logprob": -0.08143539951272206, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.14031632244586945}, {"id": 3536, "seek": 1483060, "start": 14830.6, "end": 14837.4, "text": " use a max pooling operation is to pretty much tell us about the maximum presence of a feature in that", "tokens": [50364, 764, 257, 11469, 7005, 278, 6916, 307, 281, 1238, 709, 980, 505, 466, 264, 6674, 6814, 295, 257, 4111, 294, 300, 50704], "temperature": 0.0, "avg_logprob": -0.07295829109523608, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.003593366127461195}, {"id": 3537, "seek": 1483060, "start": 14837.4, "end": 14843.24, "text": " kind of local area. We really only care if the feature exists, where if it doesn't exist, an", "tokens": [50704, 733, 295, 2654, 1859, 13, 492, 534, 787, 1127, 498, 264, 4111, 8198, 11, 689, 498, 309, 1177, 380, 2514, 11, 364, 50996], "temperature": 0.0, "avg_logprob": -0.07295829109523608, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.003593366127461195}, {"id": 3538, "seek": 1483060, "start": 14843.24, "end": 14847.640000000001, "text": " average pooling is not very often used, although in this case, we did use an average pooling.", "tokens": [50996, 4274, 7005, 278, 307, 406, 588, 2049, 1143, 11, 4878, 294, 341, 1389, 11, 321, 630, 764, 364, 4274, 7005, 278, 13, 51216], "temperature": 0.0, "avg_logprob": -0.07295829109523608, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.003593366127461195}, {"id": 3539, "seek": 1483060, "start": 14848.36, "end": 14851.800000000001, "text": " But you know, it's just different kinds of pooling and average tells you about the average", "tokens": [51252, 583, 291, 458, 11, 309, 311, 445, 819, 3685, 295, 7005, 278, 293, 4274, 5112, 291, 466, 264, 4274, 51424], "temperature": 0.0, "avg_logprob": -0.07295829109523608, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.003593366127461195}, {"id": 3540, "seek": 1483060, "start": 14851.800000000001, "end": 14856.92, "text": " presence of the feature in that area. Max tells you about is that feature present in that area", "tokens": [51424, 6814, 295, 264, 4111, 294, 300, 1859, 13, 7402, 5112, 291, 466, 307, 300, 4111, 1974, 294, 300, 1859, 51680], "temperature": 0.0, "avg_logprob": -0.07295829109523608, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.003593366127461195}, {"id": 3541, "seek": 1485692, "start": 14856.92, "end": 14860.92, "text": " at all and men tells you does it not exist. If it doesn't exist, right, we're just going to have a", "tokens": [50364, 412, 439, 293, 1706, 5112, 291, 775, 309, 406, 2514, 13, 759, 309, 1177, 380, 2514, 11, 558, 11, 321, 434, 445, 516, 281, 362, 257, 50564], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3542, "seek": 1485692, "start": 14860.92, "end": 14866.04, "text": " zero if there's even one zero in that area. So that's the point of pooling. That's the point of", "tokens": [50564, 4018, 498, 456, 311, 754, 472, 4018, 294, 300, 1859, 13, 407, 300, 311, 264, 935, 295, 7005, 278, 13, 663, 311, 264, 935, 295, 50820], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3543, "seek": 1485692, "start": 14866.04, "end": 14869.48, "text": " convolutional layers. I think I'm done with the white boarding for now, we're actually going to", "tokens": [50820, 45216, 304, 7914, 13, 286, 519, 286, 478, 1096, 365, 264, 2418, 30528, 337, 586, 11, 321, 434, 767, 516, 281, 50992], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3544, "seek": 1485692, "start": 14869.48, "end": 14873.960000000001, "text": " start getting into a little bit of code and talking about creating our own convolutional networks,", "tokens": [50992, 722, 1242, 666, 257, 707, 857, 295, 3089, 293, 1417, 466, 4084, 527, 1065, 45216, 304, 9590, 11, 51216], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3545, "seek": 1485692, "start": 14873.960000000001, "end": 14878.04, "text": " which hopefully will make this a lot more clear. So let's go ahead and get into that. All right,", "tokens": [51216, 597, 4696, 486, 652, 341, 257, 688, 544, 1850, 13, 407, 718, 311, 352, 2286, 293, 483, 666, 300, 13, 1057, 558, 11, 51420], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3546, "seek": 1485692, "start": 14878.04, "end": 14881.960000000001, "text": " so now it is time to create our first convolutional neural network. Now we're going to be using", "tokens": [51420, 370, 586, 309, 307, 565, 281, 1884, 527, 700, 45216, 304, 18161, 3209, 13, 823, 321, 434, 516, 281, 312, 1228, 51616], "temperature": 0.0, "avg_logprob": -0.0992838563145818, "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.019121749326586723}, {"id": 3547, "seek": 1488196, "start": 14881.96, "end": 14887.48, "text": " Keras to do this. And we're also going to be using the CI FAR image data set that contains 60,000", "tokens": [50364, 591, 6985, 281, 360, 341, 13, 400, 321, 434, 611, 516, 281, 312, 1228, 264, 37777, 479, 1899, 3256, 1412, 992, 300, 8306, 4060, 11, 1360, 50640], "temperature": 0.0, "avg_logprob": -0.11138712349584547, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.2172757387161255}, {"id": 3548, "seek": 1488196, "start": 14887.48, "end": 14892.759999999998, "text": " images of 10 different classes of everyday objects. Now these images are 32 by 32, which", "tokens": [50640, 5267, 295, 1266, 819, 5359, 295, 7429, 6565, 13, 823, 613, 5267, 366, 8858, 538, 8858, 11, 597, 50904], "temperature": 0.0, "avg_logprob": -0.11138712349584547, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.2172757387161255}, {"id": 3549, "seek": 1488196, "start": 14892.759999999998, "end": 14898.759999999998, "text": " essentially means they are blurs, and they are colorful. Now, I just want to emphasize as we", "tokens": [50904, 4476, 1355, 436, 366, 888, 2156, 11, 293, 436, 366, 18506, 13, 823, 11, 286, 445, 528, 281, 16078, 382, 321, 51204], "temperature": 0.0, "avg_logprob": -0.11138712349584547, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.2172757387161255}, {"id": 3550, "seek": 1488196, "start": 14898.759999999998, "end": 14903.08, "text": " get into this, that the reason I'm not typing all of these lines out and I just have them in here", "tokens": [51204, 483, 666, 341, 11, 300, 264, 1778, 286, 478, 406, 18444, 439, 295, 613, 3876, 484, 293, 286, 445, 362, 552, 294, 510, 51420], "temperature": 0.0, "avg_logprob": -0.11138712349584547, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.2172757387161255}, {"id": 3551, "seek": 1488196, "start": 14903.08, "end": 14907.4, "text": " already is because this is likely what you guys will be using or doing when you actually make", "tokens": [51420, 1217, 307, 570, 341, 307, 3700, 437, 291, 1074, 486, 312, 1228, 420, 884, 562, 291, 767, 652, 51636], "temperature": 0.0, "avg_logprob": -0.11138712349584547, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.2172757387161255}, {"id": 3552, "seek": 1490740, "start": 14907.4, "end": 14912.279999999999, "text": " your own models. Chances are that you are not going to sit unless you're a pro at TensorFlow,", "tokens": [50364, 428, 1065, 5245, 13, 761, 2676, 366, 300, 291, 366, 406, 516, 281, 1394, 5969, 291, 434, 257, 447, 412, 37624, 11, 50608], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3553, "seek": 1490740, "start": 14912.279999999999, "end": 14917.0, "text": " and I am not even there yet either with my knowledge of it, and have all of the lines memorized and", "tokens": [50608, 293, 286, 669, 406, 754, 456, 1939, 2139, 365, 452, 3601, 295, 309, 11, 293, 362, 439, 295, 264, 3876, 46677, 293, 50844], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3554, "seek": 1490740, "start": 14917.0, "end": 14921.72, "text": " not have to go reference the syntax. So the point is here, so long as you can understand why this", "tokens": [50844, 406, 362, 281, 352, 6408, 264, 28431, 13, 407, 264, 935, 307, 510, 11, 370, 938, 382, 291, 393, 1223, 983, 341, 51080], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3555, "seek": 1490740, "start": 14921.72, "end": 14926.199999999999, "text": " works and what these lines are doing, you're going to be fine, you don't need to memorize them and I", "tokens": [51080, 1985, 293, 437, 613, 3876, 366, 884, 11, 291, 434, 516, 281, 312, 2489, 11, 291, 500, 380, 643, 281, 27478, 552, 293, 286, 51304], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3556, "seek": 1490740, "start": 14926.199999999999, "end": 14930.359999999999, "text": " have not memorized them. And I don't I look up for the documentation, I copy and paste what I need,", "tokens": [51304, 362, 406, 46677, 552, 13, 400, 286, 500, 380, 286, 574, 493, 337, 264, 14333, 11, 286, 5055, 293, 9163, 437, 286, 643, 11, 51512], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3557, "seek": 1490740, "start": 14930.359999999999, "end": 14934.119999999999, "text": " I alter them, I write a little bit of my own code. But that's kind of what you're going to end up", "tokens": [51512, 286, 11337, 552, 11, 286, 2464, 257, 707, 857, 295, 452, 1065, 3089, 13, 583, 300, 311, 733, 295, 437, 291, 434, 516, 281, 917, 493, 51700], "temperature": 0.0, "avg_logprob": -0.07446316929606649, "compression_ratio": 1.7824773413897281, "no_speech_prob": 0.1519984006881714}, {"id": 3558, "seek": 1493412, "start": 14934.12, "end": 14939.400000000001, "text": " doing. So that's what I'm doing here. So this is the image data set. We have truck, horse, ship,", "tokens": [50364, 884, 13, 407, 300, 311, 437, 286, 478, 884, 510, 13, 407, 341, 307, 264, 3256, 1412, 992, 13, 492, 362, 5898, 11, 6832, 11, 5374, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3559, "seek": 1493412, "start": 14939.400000000001, "end": 14943.720000000001, "text": " airplane, you know, just some everyday, regular objects, there is 60,000 images, as we said,", "tokens": [50628, 17130, 11, 291, 458, 11, 445, 512, 7429, 11, 3890, 6565, 11, 456, 307, 4060, 11, 1360, 5267, 11, 382, 321, 848, 11, 50844], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3560, "seek": 1493412, "start": 14943.720000000001, "end": 14949.880000000001, "text": " and 6000 images of each class. So we don't have too many images of just one specific class. So", "tokens": [50844, 293, 41789, 5267, 295, 1184, 1508, 13, 407, 321, 500, 380, 362, 886, 867, 5267, 295, 445, 472, 2685, 1508, 13, 407, 51152], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3561, "seek": 1493412, "start": 14949.880000000001, "end": 14955.160000000002, "text": " we'll start by importing our modules. So TensorFlow, we're going to import TensorFlow dot Keras,", "tokens": [51152, 321, 603, 722, 538, 43866, 527, 16679, 13, 407, 37624, 11, 321, 434, 516, 281, 974, 37624, 5893, 591, 6985, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3562, "seek": 1493412, "start": 14955.160000000002, "end": 14960.04, "text": " we're going to use the data set built into Keras for this. So that's the CI FR image data set,", "tokens": [51416, 321, 434, 516, 281, 764, 264, 1412, 992, 3094, 666, 591, 6985, 337, 341, 13, 407, 300, 311, 264, 37777, 15288, 3256, 1412, 992, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3563, "seek": 1493412, "start": 14960.04, "end": 14963.720000000001, "text": " which you can actually look at just by clicking at this, it'll bring you and give the information", "tokens": [51660, 597, 291, 393, 767, 574, 412, 445, 538, 9697, 412, 341, 11, 309, 603, 1565, 291, 293, 976, 264, 1589, 51844], "temperature": 0.0, "avg_logprob": -0.1110716744473106, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.037322379648685455}, {"id": 3564, "seek": 1496372, "start": 14963.72, "end": 14968.199999999999, "text": " about the data set. Although we don't need that right now, because I already know the information", "tokens": [50364, 466, 264, 1412, 992, 13, 5780, 321, 500, 380, 643, 300, 558, 586, 11, 570, 286, 1217, 458, 264, 1589, 50588], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3565, "seek": 1496372, "start": 14968.199999999999, "end": 14972.92, "text": " about it. And now we're just going to load our images in. So again, this stuff, the way this", "tokens": [50588, 466, 309, 13, 400, 586, 321, 434, 445, 516, 281, 3677, 527, 5267, 294, 13, 407, 797, 11, 341, 1507, 11, 264, 636, 341, 50824], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3566, "seek": 1496372, "start": 14972.92, "end": 14979.08, "text": " works is you're gonna say data sets dot CI FR 10 dot load data. Now this loads it in as like a very", "tokens": [50824, 1985, 307, 291, 434, 799, 584, 1412, 6352, 5893, 37777, 15288, 1266, 5893, 3677, 1412, 13, 823, 341, 12668, 309, 294, 382, 411, 257, 588, 51132], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3567, "seek": 1496372, "start": 14979.08, "end": 14983.96, "text": " strange TensorFlow object, that's like a data set object. So this is different from what we've", "tokens": [51132, 5861, 37624, 2657, 11, 300, 311, 411, 257, 1412, 992, 2657, 13, 407, 341, 307, 819, 490, 437, 321, 600, 51376], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3568, "seek": 1496372, "start": 14983.96, "end": 14989.4, "text": " used before, where some of our objects have actually been like in NumPy arrays, where we can look at", "tokens": [51376, 1143, 949, 11, 689, 512, 295, 527, 6565, 362, 767, 668, 411, 294, 22592, 47, 88, 41011, 11, 689, 321, 393, 574, 412, 51648], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3569, "seek": 1496372, "start": 14989.4, "end": 14993.32, "text": " them better. This is not going to be in that. So just something to keep in mind here. And we're", "tokens": [51648, 552, 1101, 13, 639, 307, 406, 516, 281, 312, 294, 300, 13, 407, 445, 746, 281, 1066, 294, 1575, 510, 13, 400, 321, 434, 51844], "temperature": 0.0, "avg_logprob": -0.10925389607747396, "compression_ratio": 1.7270029673590503, "no_speech_prob": 0.004069718066602945}, {"id": 3570, "seek": 1499332, "start": 14993.32, "end": 14998.68, "text": " going to normalize this data into train images and test images, but just dividing both of them by", "tokens": [50364, 516, 281, 2710, 1125, 341, 1412, 666, 3847, 5267, 293, 1500, 5267, 11, 457, 445, 26764, 1293, 295, 552, 538, 50632], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3571, "seek": 1499332, "start": 14998.68, "end": 15003.24, "text": " 255. Now, again, we're doing that because we want to make sure that our values are between zero and", "tokens": [50632, 3552, 20, 13, 823, 11, 797, 11, 321, 434, 884, 300, 570, 321, 528, 281, 652, 988, 300, 527, 4190, 366, 1296, 4018, 293, 50860], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3572, "seek": 1499332, "start": 15003.24, "end": 15007.0, "text": " one, because that's just a lot better to work with in our neural networks, rather than large", "tokens": [50860, 472, 11, 570, 300, 311, 445, 257, 688, 1101, 281, 589, 365, 294, 527, 18161, 9590, 11, 2831, 813, 2416, 51048], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3573, "seek": 1499332, "start": 15007.0, "end": 15011.96, "text": " integer values, just causes, you know, some things to mess up sometimes. Now class names,", "tokens": [51048, 24922, 4190, 11, 445, 7700, 11, 291, 458, 11, 512, 721, 281, 2082, 493, 2171, 13, 823, 1508, 5288, 11, 51296], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3574, "seek": 1499332, "start": 15011.96, "end": 15015.4, "text": " we're just going to find a list here. So we have all the class names so that zero represents", "tokens": [51296, 321, 434, 445, 516, 281, 915, 257, 1329, 510, 13, 407, 321, 362, 439, 264, 1508, 5288, 370, 300, 4018, 8855, 51468], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3575, "seek": 1499332, "start": 15015.4, "end": 15021.88, "text": " airplane one auto avail so far in tilt truck, run that block of code here, we'll download this", "tokens": [51468, 17130, 472, 8399, 2327, 370, 1400, 294, 18446, 5898, 11, 1190, 300, 3461, 295, 3089, 510, 11, 321, 603, 5484, 341, 51792], "temperature": 0.0, "avg_logprob": -0.10530455453055246, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.000939897436182946}, {"id": 3576, "seek": 1502188, "start": 15021.88, "end": 15026.119999999999, "text": " data set, although I don't think it takes that long to do that. So okay, so wait, I guess,", "tokens": [50364, 1412, 992, 11, 4878, 286, 500, 380, 519, 309, 2516, 300, 938, 281, 360, 300, 13, 407, 1392, 11, 370, 1699, 11, 286, 2041, 11, 50576], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3577, "seek": 1502188, "start": 15026.119999999999, "end": 15030.519999999999, "text": " yeah, I guess that's good. I think we're okay there. And now let's just have a look at actually", "tokens": [50576, 1338, 11, 286, 2041, 300, 311, 665, 13, 286, 519, 321, 434, 1392, 456, 13, 400, 586, 718, 311, 445, 362, 257, 574, 412, 767, 50796], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3578, "seek": 1502188, "start": 15030.519999999999, "end": 15035.16, "text": " some of the images here by running this script. So we can see this is a truck can change the", "tokens": [50796, 512, 295, 264, 5267, 510, 538, 2614, 341, 5755, 13, 407, 321, 393, 536, 341, 307, 257, 5898, 393, 1319, 264, 51028], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3579, "seek": 1502188, "start": 15035.16, "end": 15041.56, "text": " image index to be two, we can see this is another truck. Let's go to say six, we get a bird. And", "tokens": [51028, 3256, 8186, 281, 312, 732, 11, 321, 393, 536, 341, 307, 1071, 5898, 13, 961, 311, 352, 281, 584, 2309, 11, 321, 483, 257, 5255, 13, 400, 51348], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3580, "seek": 1502188, "start": 15041.56, "end": 15044.92, "text": " you can see these are really blurry. But that's fine. For this example, we're just trying to get", "tokens": [51348, 291, 393, 536, 613, 366, 534, 37644, 13, 583, 300, 311, 2489, 13, 1171, 341, 1365, 11, 321, 434, 445, 1382, 281, 483, 51516], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3581, "seek": 1502188, "start": 15044.92, "end": 15049.4, "text": " something that works all right. Okay, so that's a horse, you know, you get the point. All right.", "tokens": [51516, 746, 300, 1985, 439, 558, 13, 1033, 11, 370, 300, 311, 257, 6832, 11, 291, 458, 11, 291, 483, 264, 935, 13, 1057, 558, 13, 51740], "temperature": 0.0, "avg_logprob": -0.07297737002372742, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.012819936498999596}, {"id": 3582, "seek": 1504940, "start": 15049.4, "end": 15054.44, "text": " So now CNN architecture. So essentially, we've already talked about how a convolutional neural", "tokens": [50364, 407, 586, 24859, 9482, 13, 407, 4476, 11, 321, 600, 1217, 2825, 466, 577, 257, 45216, 304, 18161, 50616], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3583, "seek": 1504940, "start": 15054.44, "end": 15058.52, "text": " network works, we haven't talked about the architecture and how we actually make one.", "tokens": [50616, 3209, 1985, 11, 321, 2378, 380, 2825, 466, 264, 9482, 293, 577, 321, 767, 652, 472, 13, 50820], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3584, "seek": 1504940, "start": 15058.52, "end": 15063.16, "text": " Essentially, what we do is we stack a bunch of convolutional layers and max pooling,", "tokens": [50820, 23596, 11, 437, 321, 360, 307, 321, 8630, 257, 3840, 295, 45216, 304, 7914, 293, 11469, 7005, 278, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3585, "seek": 1504940, "start": 15063.16, "end": 15068.039999999999, "text": " min pooling or average pooling layers together in something like this, right. So after each", "tokens": [51052, 923, 7005, 278, 420, 4274, 7005, 278, 7914, 1214, 294, 746, 411, 341, 11, 558, 13, 407, 934, 1184, 51296], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3586, "seek": 1504940, "start": 15068.039999999999, "end": 15072.68, "text": " convolutional layer, we have a max pooling layer, some kind of pooling layer typically to reduce", "tokens": [51296, 45216, 304, 4583, 11, 321, 362, 257, 11469, 7005, 278, 4583, 11, 512, 733, 295, 7005, 278, 4583, 5850, 281, 5407, 51528], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3587, "seek": 1504940, "start": 15072.68, "end": 15076.68, "text": " the dimensionality, although you don't need that, you could just go straight into three", "tokens": [51528, 264, 10139, 1860, 11, 4878, 291, 500, 380, 643, 300, 11, 291, 727, 445, 352, 2997, 666, 1045, 51728], "temperature": 0.0, "avg_logprob": -0.08246513396974593, "compression_ratio": 1.8885017421602788, "no_speech_prob": 0.005554468370974064}, {"id": 3588, "seek": 1507668, "start": 15076.76, "end": 15082.52, "text": " convolutional layers. And on our first layer, what we do is we define the amount of filters", "tokens": [50368, 45216, 304, 7914, 13, 400, 322, 527, 700, 4583, 11, 437, 321, 360, 307, 321, 6964, 264, 2372, 295, 15995, 50656], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3589, "seek": 1507668, "start": 15082.52, "end": 15088.12, "text": " just like here, we define the sample size. So how big are those filters and activation function,", "tokens": [50656, 445, 411, 510, 11, 321, 6964, 264, 6889, 2744, 13, 407, 577, 955, 366, 729, 15995, 293, 24433, 2445, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3590, "seek": 1507668, "start": 15088.12, "end": 15093.720000000001, "text": " which essentially means after we apply that, what is it that cross not cross product dot", "tokens": [50936, 597, 4476, 1355, 934, 321, 3079, 300, 11, 437, 307, 309, 300, 3278, 406, 3278, 1674, 5893, 51216], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3591, "seek": 1507668, "start": 15093.720000000001, "end": 15098.28, "text": " product operation that we talked about, we'll apply rectifier linear unit to that and then", "tokens": [51216, 1674, 6916, 300, 321, 2825, 466, 11, 321, 603, 3079, 11048, 9902, 8213, 4985, 281, 300, 293, 550, 51444], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3592, "seek": 1507668, "start": 15098.28, "end": 15102.36, "text": " put that in the output feature map. Again, we've talked about activations functions before. So I", "tokens": [51444, 829, 300, 294, 264, 5598, 4111, 4471, 13, 3764, 11, 321, 600, 2825, 466, 2430, 763, 6828, 949, 13, 407, 286, 51648], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3593, "seek": 1507668, "start": 15102.36, "end": 15106.04, "text": " won't go too far into depth with them. And then we define the input shape, which essentially", "tokens": [51648, 1582, 380, 352, 886, 1400, 666, 7161, 365, 552, 13, 400, 550, 321, 6964, 264, 4846, 3909, 11, 597, 4476, 51832], "temperature": 0.0, "avg_logprob": -0.08002460186298077, "compression_ratio": 1.8915254237288135, "no_speech_prob": 0.028432557359337807}, {"id": 3594, "seek": 1510604, "start": 15106.04, "end": 15110.52, "text": " means what can we expect in this first layer? Well, 32 by 32 by three, these ones, we don't", "tokens": [50364, 1355, 437, 393, 321, 2066, 294, 341, 700, 4583, 30, 1042, 11, 8858, 538, 8858, 538, 1045, 11, 613, 2306, 11, 321, 500, 380, 50588], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3595, "seek": 1510604, "start": 15110.52, "end": 15113.720000000001, "text": " need to do that, because they're going to figure out what that is based on the input from the", "tokens": [50588, 643, 281, 360, 300, 11, 570, 436, 434, 516, 281, 2573, 484, 437, 300, 307, 2361, 322, 264, 4846, 490, 264, 50748], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3596, "seek": 1510604, "start": 15113.720000000001, "end": 15118.52, "text": " previous layer. Alright, so these are just a breakdown of the layers. The convolution or the", "tokens": [50748, 3894, 4583, 13, 2798, 11, 370, 613, 366, 445, 257, 18188, 295, 264, 7914, 13, 440, 45216, 420, 264, 50988], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3597, "seek": 1510604, "start": 15118.52, "end": 15122.2, "text": " max pooling layers here, two by two, essentially means that what we're going to do is we're going", "tokens": [50988, 11469, 7005, 278, 7914, 510, 11, 732, 538, 732, 11, 4476, 1355, 300, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 51172], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3598, "seek": 1510604, "start": 15122.2, "end": 15126.76, "text": " to have a two by two sample size with actually a stride of two. Again, the whole point of this", "tokens": [51172, 281, 362, 257, 732, 538, 732, 6889, 2744, 365, 767, 257, 1056, 482, 295, 732, 13, 3764, 11, 264, 1379, 935, 295, 341, 51400], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3599, "seek": 1510604, "start": 15126.76, "end": 15132.76, "text": " is to actually divide or, you know, shrink it by a factor of two, how large each of these layers", "tokens": [51400, 307, 281, 767, 9845, 420, 11, 291, 458, 11, 23060, 309, 538, 257, 5952, 295, 732, 11, 577, 2416, 1184, 295, 613, 7914, 51700], "temperature": 0.0, "avg_logprob": -0.06843795906118795, "compression_ratio": 1.758513931888545, "no_speech_prob": 0.008846290409564972}, {"id": 3600, "seek": 1513276, "start": 15132.84, "end": 15137.72, "text": " are. Alright, so now let's have a summary. It's already printed out here. We can see that we", "tokens": [50368, 366, 13, 2798, 11, 370, 586, 718, 311, 362, 257, 12691, 13, 467, 311, 1217, 13567, 484, 510, 13, 492, 393, 536, 300, 321, 50612], "temperature": 0.0, "avg_logprob": -0.1432194709777832, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.028433067724108696}, {"id": 3601, "seek": 1513276, "start": 15137.72, "end": 15144.52, "text": " have, oh, wait, is this correct? mobile net v two, I don't think that's correct. That's because I", "tokens": [50612, 362, 11, 1954, 11, 1699, 11, 307, 341, 3006, 30, 6013, 2533, 371, 732, 11, 286, 500, 380, 519, 300, 311, 3006, 13, 663, 311, 570, 286, 50952], "temperature": 0.0, "avg_logprob": -0.1432194709777832, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.028433067724108696}, {"id": 3602, "seek": 1513276, "start": 15144.52, "end": 15149.4, "text": " haven't run this one. My apologies on that guys, this is from something later in the tutorial,", "tokens": [50952, 2378, 380, 1190, 341, 472, 13, 1222, 34929, 322, 300, 1074, 11, 341, 307, 490, 746, 1780, 294, 264, 7073, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1432194709777832, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.028433067724108696}, {"id": 3603, "seek": 1513276, "start": 15149.4, "end": 15153.64, "text": " we can see that we have calm 2d as our first layer. This is the output shape of that layer.", "tokens": [51196, 321, 393, 536, 300, 321, 362, 7151, 568, 67, 382, 527, 700, 4583, 13, 639, 307, 264, 5598, 3909, 295, 300, 4583, 13, 51408], "temperature": 0.0, "avg_logprob": -0.1432194709777832, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.028433067724108696}, {"id": 3604, "seek": 1513276, "start": 15153.64, "end": 15161.08, "text": " Notice that it is not 32 by 32 by 32. It is 30 by 30 by 32, because when we do that sampling", "tokens": [51408, 13428, 300, 309, 307, 406, 8858, 538, 8858, 538, 8858, 13, 467, 307, 2217, 538, 2217, 538, 8858, 11, 570, 562, 321, 360, 300, 21179, 51780], "temperature": 0.0, "avg_logprob": -0.1432194709777832, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.028433067724108696}, {"id": 3605, "seek": 1516108, "start": 15161.08, "end": 15165.16, "text": " without padding, right, that's what we're going to get. We're going to get two pixels less,", "tokens": [50364, 1553, 39562, 11, 558, 11, 300, 311, 437, 321, 434, 516, 281, 483, 13, 492, 434, 516, 281, 483, 732, 18668, 1570, 11, 50568], "temperature": 0.0, "avg_logprob": -0.09460972647630532, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.010985681787133217}, {"id": 3606, "seek": 1516108, "start": 15165.96, "end": 15171.88, "text": " because the amount of samples we can take. Alright, next, we have the max pooling 2d layer.", "tokens": [50608, 570, 264, 2372, 295, 10938, 321, 393, 747, 13, 2798, 11, 958, 11, 321, 362, 264, 11469, 7005, 278, 568, 67, 4583, 13, 50904], "temperature": 0.0, "avg_logprob": -0.09460972647630532, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.010985681787133217}, {"id": 3607, "seek": 1516108, "start": 15171.88, "end": 15177.64, "text": " So this now says the output shape is 15 by 15 by 32, which means we've shrunk this shape by a", "tokens": [50904, 407, 341, 586, 1619, 264, 5598, 3909, 307, 2119, 538, 2119, 538, 8858, 11, 597, 1355, 321, 600, 9884, 3197, 341, 3909, 538, 257, 51192], "temperature": 0.0, "avg_logprob": -0.09460972647630532, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.010985681787133217}, {"id": 3608, "seek": 1516108, "start": 15177.64, "end": 15183.88, "text": " factor of two, we do a convolution on this, which means that now we get 1313 and we're doing 64,", "tokens": [51192, 5952, 295, 732, 11, 321, 360, 257, 45216, 322, 341, 11, 597, 1355, 300, 586, 321, 483, 3705, 7668, 293, 321, 434, 884, 12145, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09460972647630532, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.010985681787133217}, {"id": 3609, "seek": 1516108, "start": 15183.88, "end": 15189.64, "text": " because we're going to take 64 filters this time. And then max pooling again, we go six by six by", "tokens": [51504, 570, 321, 434, 516, 281, 747, 12145, 15995, 341, 565, 13, 400, 550, 11469, 7005, 278, 797, 11, 321, 352, 2309, 538, 2309, 538, 51792], "temperature": 0.0, "avg_logprob": -0.09460972647630532, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.010985681787133217}, {"id": 3610, "seek": 1518964, "start": 15190.199999999999, "end": 15194.76, "text": " 64, because we're going to divide this again by factor of two. Notice that it just rounded,", "tokens": [50392, 12145, 11, 570, 321, 434, 516, 281, 9845, 341, 797, 538, 5952, 295, 732, 13, 13428, 300, 309, 445, 23382, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09480973052978516, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.002050649607554078}, {"id": 3611, "seek": 1518964, "start": 15194.76, "end": 15199.56, "text": " right? And then calm 2d. So another layer here, we get four by four by 64, again, because of the", "tokens": [50620, 558, 30, 400, 550, 7151, 568, 67, 13, 407, 1071, 4583, 510, 11, 321, 483, 1451, 538, 1451, 538, 12145, 11, 797, 11, 570, 295, 264, 50860], "temperature": 0.0, "avg_logprob": -0.09480973052978516, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.002050649607554078}, {"id": 3612, "seek": 1518964, "start": 15199.56, "end": 15205.96, "text": " way we take those values. So this is what we've defined so far. But this is not the end of our", "tokens": [50860, 636, 321, 747, 729, 4190, 13, 407, 341, 307, 437, 321, 600, 7642, 370, 1400, 13, 583, 341, 307, 406, 264, 917, 295, 527, 51180], "temperature": 0.0, "avg_logprob": -0.09480973052978516, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.002050649607554078}, {"id": 3613, "seek": 1518964, "start": 15205.96, "end": 15211.32, "text": " convolutional neural network. In fact, this doesn't really mean much to us, right? This just tells us", "tokens": [51180, 45216, 304, 18161, 3209, 13, 682, 1186, 11, 341, 1177, 380, 534, 914, 709, 281, 505, 11, 558, 30, 639, 445, 5112, 505, 51448], "temperature": 0.0, "avg_logprob": -0.09480973052978516, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.002050649607554078}, {"id": 3614, "seek": 1518964, "start": 15211.32, "end": 15215.96, "text": " about the presence of specific features, as we've gone through this convolution base, which is what", "tokens": [51448, 466, 264, 6814, 295, 2685, 4122, 11, 382, 321, 600, 2780, 807, 341, 45216, 3096, 11, 597, 307, 437, 51680], "temperature": 0.0, "avg_logprob": -0.09480973052978516, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.002050649607554078}, {"id": 3615, "seek": 1521596, "start": 15215.96, "end": 15221.72, "text": " this is called the stack of convolution and max pooling layers. So what we actually need to do", "tokens": [50364, 341, 307, 1219, 264, 8630, 295, 45216, 293, 11469, 7005, 278, 7914, 13, 407, 437, 321, 767, 643, 281, 360, 50652], "temperature": 0.0, "avg_logprob": -0.03766598267988725, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.01363502349704504}, {"id": 3616, "seek": 1521596, "start": 15221.72, "end": 15227.56, "text": " is now pass this information into some kind of dense layer classifier, which is actually going to", "tokens": [50652, 307, 586, 1320, 341, 1589, 666, 512, 733, 295, 18011, 4583, 1508, 9902, 11, 597, 307, 767, 516, 281, 50944], "temperature": 0.0, "avg_logprob": -0.03766598267988725, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.01363502349704504}, {"id": 3617, "seek": 1521596, "start": 15227.56, "end": 15233.16, "text": " take this pixel data that we've kind of calculated and found. So the almost extraction of features", "tokens": [50944, 747, 341, 19261, 1412, 300, 321, 600, 733, 295, 15598, 293, 1352, 13, 407, 264, 1920, 30197, 295, 4122, 51224], "temperature": 0.0, "avg_logprob": -0.03766598267988725, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.01363502349704504}, {"id": 3618, "seek": 1521596, "start": 15233.16, "end": 15238.359999999999, "text": " that exist in the image, and tell us which combination of these features map to either,", "tokens": [51224, 300, 2514, 294, 264, 3256, 11, 293, 980, 505, 597, 6562, 295, 613, 4122, 4471, 281, 2139, 11, 51484], "temperature": 0.0, "avg_logprob": -0.03766598267988725, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.01363502349704504}, {"id": 3619, "seek": 1521596, "start": 15238.359999999999, "end": 15243.32, "text": " you know, what one of these 10 classes are. So that's kind of the point you do this convolution", "tokens": [51484, 291, 458, 11, 437, 472, 295, 613, 1266, 5359, 366, 13, 407, 300, 311, 733, 295, 264, 935, 291, 360, 341, 45216, 51732], "temperature": 0.0, "avg_logprob": -0.03766598267988725, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.01363502349704504}, {"id": 3620, "seek": 1524332, "start": 15243.32, "end": 15248.92, "text": " base, which extracts all of the features out of your image. And then you use the dense network", "tokens": [50364, 3096, 11, 597, 8947, 82, 439, 295, 264, 4122, 484, 295, 428, 3256, 13, 400, 550, 291, 764, 264, 18011, 3209, 50644], "temperature": 0.0, "avg_logprob": -0.09354185864208191, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.017439473420381546}, {"id": 3621, "seek": 1524332, "start": 15248.92, "end": 15253.88, "text": " to say, Okay, well, if these combination of features exist, then that means this image is this,", "tokens": [50644, 281, 584, 11, 1033, 11, 731, 11, 498, 613, 6562, 295, 4122, 2514, 11, 550, 300, 1355, 341, 3256, 307, 341, 11, 50892], "temperature": 0.0, "avg_logprob": -0.09354185864208191, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.017439473420381546}, {"id": 3622, "seek": 1524332, "start": 15253.88, "end": 15258.92, "text": " otherwise, it's this and that and so on. So that's what we're doing here. Alright, so let's say adding", "tokens": [50892, 5911, 11, 309, 311, 341, 293, 300, 293, 370, 322, 13, 407, 300, 311, 437, 321, 434, 884, 510, 13, 2798, 11, 370, 718, 311, 584, 5127, 51144], "temperature": 0.0, "avg_logprob": -0.09354185864208191, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.017439473420381546}, {"id": 3623, "seek": 1524332, "start": 15258.92, "end": 15262.92, "text": " the dense layers. So to add the dense layers pretty easy model dot add is just how we add them,", "tokens": [51144, 264, 18011, 7914, 13, 407, 281, 909, 264, 18011, 7914, 1238, 1858, 2316, 5893, 909, 307, 445, 577, 321, 909, 552, 11, 51344], "temperature": 0.0, "avg_logprob": -0.09354185864208191, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.017439473420381546}, {"id": 3624, "seek": 1524332, "start": 15262.92, "end": 15268.68, "text": " right? So we're going to flatten all of those pixels was which essentially means take the four", "tokens": [51344, 558, 30, 407, 321, 434, 516, 281, 24183, 439, 295, 729, 18668, 390, 597, 4476, 1355, 747, 264, 1451, 51632], "temperature": 0.0, "avg_logprob": -0.09354185864208191, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.017439473420381546}, {"id": 3625, "seek": 1526868, "start": 15268.68, "end": 15273.08, "text": " by four by 64, and just put those all into a straight line, like we've done before. So just", "tokens": [50364, 538, 1451, 538, 12145, 11, 293, 445, 829, 729, 439, 666, 257, 2997, 1622, 11, 411, 321, 600, 1096, 949, 13, 407, 445, 50584], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3626, "seek": 1526868, "start": 15273.08, "end": 15279.0, "text": " one dimensional, then we're going to have a 64 neuron dense layer that connects all of those", "tokens": [50584, 472, 18795, 11, 550, 321, 434, 516, 281, 362, 257, 12145, 34090, 18011, 4583, 300, 16967, 439, 295, 729, 50880], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3627, "seek": 1526868, "start": 15279.0, "end": 15283.48, "text": " things to it with an activation function of rectifier linear unit, then our output layer of", "tokens": [50880, 721, 281, 309, 365, 364, 24433, 2445, 295, 11048, 9902, 8213, 4985, 11, 550, 527, 5598, 4583, 295, 51104], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3628, "seek": 1526868, "start": 15283.48, "end": 15288.36, "text": " a dense layer with 10 neurons, obviously 10, because that's the amount of classes we have for", "tokens": [51104, 257, 18011, 4583, 365, 1266, 22027, 11, 2745, 1266, 11, 570, 300, 311, 264, 2372, 295, 5359, 321, 362, 337, 51348], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3629, "seek": 1526868, "start": 15288.36, "end": 15292.04, "text": " this problem. So let's run this here, we'll add those layers, let's look at a summary and see", "tokens": [51348, 341, 1154, 13, 407, 718, 311, 1190, 341, 510, 11, 321, 603, 909, 729, 7914, 11, 718, 311, 574, 412, 257, 12691, 293, 536, 51532], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3630, "seek": 1526868, "start": 15292.04, "end": 15298.36, "text": " how things have changed now. So we go from four by four by 64 to 2024. Notice that that is precisely", "tokens": [51532, 577, 721, 362, 3105, 586, 13, 407, 321, 352, 490, 1451, 538, 1451, 538, 12145, 281, 945, 7911, 13, 13428, 300, 300, 307, 13402, 51848], "temperature": 0.0, "avg_logprob": -0.08079624847627022, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.04741828516125679}, {"id": 3631, "seek": 1529836, "start": 15298.36, "end": 15303.32, "text": " the calculation of four times four times 64. That's how we get that number here. Then we have a", "tokens": [50364, 264, 17108, 295, 1451, 1413, 1451, 1413, 12145, 13, 663, 311, 577, 321, 483, 300, 1230, 510, 13, 1396, 321, 362, 257, 50612], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3632, "seek": 1529836, "start": 15303.32, "end": 15307.0, "text": " dense layer and another dense layer. And this is our output layer. Finally, this is what we're", "tokens": [50612, 18011, 4583, 293, 1071, 18011, 4583, 13, 400, 341, 307, 527, 5598, 4583, 13, 6288, 11, 341, 307, 437, 321, 434, 50796], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3633, "seek": 1529836, "start": 15307.0, "end": 15311.16, "text": " getting is we're going to get 10 neurons out. So essentially, just a list of values. And that's", "tokens": [50796, 1242, 307, 321, 434, 516, 281, 483, 1266, 22027, 484, 13, 407, 4476, 11, 445, 257, 1329, 295, 4190, 13, 400, 300, 311, 51004], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3634, "seek": 1529836, "start": 15311.16, "end": 15317.480000000001, "text": " how we can determine which class is predicted. So this up to here is the convolutional base,", "tokens": [51004, 577, 321, 393, 6997, 597, 1508, 307, 19147, 13, 407, 341, 493, 281, 510, 307, 264, 45216, 304, 3096, 11, 51320], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3635, "seek": 1529836, "start": 15317.480000000001, "end": 15321.960000000001, "text": " this is what we call the classifier, and they work together to essentially extract the features,", "tokens": [51320, 341, 307, 437, 321, 818, 264, 1508, 9902, 11, 293, 436, 589, 1214, 281, 4476, 8947, 264, 4122, 11, 51544], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3636, "seek": 1529836, "start": 15321.960000000001, "end": 15327.0, "text": " and then look at the features and predict the actual object or whatever it is the class. Alright,", "tokens": [51544, 293, 550, 574, 412, 264, 4122, 293, 6069, 264, 3539, 2657, 420, 2035, 309, 307, 264, 1508, 13, 2798, 11, 51796], "temperature": 0.0, "avg_logprob": -0.06641789305981972, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004069857764989138}, {"id": 3637, "seek": 1532700, "start": 15327.64, "end": 15330.36, "text": " so that's how that works. Now it's time to train again, we'll go through this quickly.", "tokens": [50396, 370, 300, 311, 577, 300, 1985, 13, 823, 309, 311, 565, 281, 3847, 797, 11, 321, 603, 352, 807, 341, 2661, 13, 50532], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3638, "seek": 1532700, "start": 15331.0, "end": 15335.0, "text": " I believe I've already trained this this takes a long time to train. So I'm actually going to", "tokens": [50564, 286, 1697, 286, 600, 1217, 8895, 341, 341, 2516, 257, 938, 565, 281, 3847, 13, 407, 286, 478, 767, 516, 281, 50764], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3639, "seek": 1532700, "start": 15335.0, "end": 15341.8, "text": " reduce the epochs here to just be for I'd recommend you guys train this on higher. So like 10, if", "tokens": [50764, 5407, 264, 30992, 28346, 510, 281, 445, 312, 337, 286, 1116, 2748, 291, 1074, 3847, 341, 322, 2946, 13, 407, 411, 1266, 11, 498, 51104], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3640, "seek": 1532700, "start": 15341.8, "end": 15345.56, "text": " you're going to do it, it does take a while. So for our purposes, and for my time, we'll leave", "tokens": [51104, 291, 434, 516, 281, 360, 309, 11, 309, 775, 747, 257, 1339, 13, 407, 337, 527, 9932, 11, 293, 337, 452, 565, 11, 321, 603, 1856, 51292], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3641, "seek": 1532700, "start": 15345.56, "end": 15349.4, "text": " a little bit shorter right now, but you should be getting about 70% accuracy. And you can see I've", "tokens": [51292, 257, 707, 857, 11639, 558, 586, 11, 457, 291, 820, 312, 1242, 466, 5285, 4, 14170, 13, 400, 291, 393, 536, 286, 600, 51484], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3642, "seek": 1532700, "start": 15349.4, "end": 15353.72, "text": " trained this previously, if you train it on 10 epochs, but I'm just going to train up to four,", "tokens": [51484, 8895, 341, 8046, 11, 498, 291, 3847, 309, 322, 1266, 30992, 28346, 11, 457, 286, 478, 445, 516, 281, 3847, 493, 281, 1451, 11, 51700], "temperature": 0.0, "avg_logprob": -0.09351550758659065, "compression_ratio": 1.771875, "no_speech_prob": 0.0026315590366721153}, {"id": 3643, "seek": 1535372, "start": 15353.8, "end": 15358.039999999999, "text": " we get our 6768%. And that should be fine. So we'll be back once this is trained,", "tokens": [50368, 321, 483, 527, 23879, 27102, 6856, 400, 300, 820, 312, 2489, 13, 407, 321, 603, 312, 646, 1564, 341, 307, 8895, 11, 50580], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3644, "seek": 1535372, "start": 15358.039999999999, "end": 15361.88, "text": " then we'll talk about how some of this works. Okay, so the model is finally finished training,", "tokens": [50580, 550, 321, 603, 751, 466, 577, 512, 295, 341, 1985, 13, 1033, 11, 370, 264, 2316, 307, 2721, 4335, 3097, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3645, "seek": 1535372, "start": 15361.88, "end": 15367.8, "text": " we did about four epochs, you can see we got an accuracy about 67% on the evaluation data.", "tokens": [50772, 321, 630, 466, 1451, 30992, 28346, 11, 291, 393, 536, 321, 658, 364, 14170, 466, 23879, 4, 322, 264, 13344, 1412, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3646, "seek": 1535372, "start": 15367.8, "end": 15374.119999999999, "text": " To quickly go over this stuff. optimizers, Adam talked about that before. loss function is sparse", "tokens": [51068, 1407, 2661, 352, 670, 341, 1507, 13, 5028, 22525, 11, 7938, 2825, 466, 300, 949, 13, 4470, 2445, 307, 637, 11668, 51384], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3647, "seek": 1535372, "start": 15374.119999999999, "end": 15378.439999999999, "text": " categorical cross entropy. That one, I mean, you can read this if you want computes the cross", "tokens": [51384, 19250, 804, 3278, 30867, 13, 663, 472, 11, 286, 914, 11, 291, 393, 1401, 341, 498, 291, 528, 715, 1819, 264, 3278, 51600], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3648, "seek": 1535372, "start": 15378.439999999999, "end": 15382.92, "text": " entropy loss between the labels and predictions. And I'm not going to go into that. But these", "tokens": [51600, 30867, 4470, 1296, 264, 16949, 293, 21264, 13, 400, 286, 478, 406, 516, 281, 352, 666, 300, 13, 583, 613, 51824], "temperature": 0.0, "avg_logprob": -0.1419548383900817, "compression_ratio": 1.6706948640483383, "no_speech_prob": 0.010013142600655556}, {"id": 3649, "seek": 1538292, "start": 15382.92, "end": 15386.36, "text": " kind of things are things that you can look up if you really understand why they work.", "tokens": [50364, 733, 295, 721, 366, 721, 300, 291, 393, 574, 493, 498, 291, 534, 1223, 983, 436, 589, 13, 50536], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3650, "seek": 1538292, "start": 15386.92, "end": 15390.84, "text": " For most problems, you can just if you want to figure out what, you know, loss function", "tokens": [50564, 1171, 881, 2740, 11, 291, 393, 445, 498, 291, 528, 281, 2573, 484, 437, 11, 291, 458, 11, 4470, 2445, 50760], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3651, "seek": 1538292, "start": 15390.84, "end": 15396.04, "text": " or optimizer to use, just use the basics, like use Adam, use a categorical cross entropy,", "tokens": [50760, 420, 5028, 6545, 281, 764, 11, 445, 764, 264, 14688, 11, 411, 764, 7938, 11, 764, 257, 19250, 804, 3278, 30867, 11, 51020], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3652, "seek": 1538292, "start": 15396.04, "end": 15400.04, "text": " using a classification task, you want to do something like this, there's just you can go", "tokens": [51020, 1228, 257, 21538, 5633, 11, 291, 528, 281, 360, 746, 411, 341, 11, 456, 311, 445, 291, 393, 352, 51220], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3653, "seek": 1538292, "start": 15400.04, "end": 15404.28, "text": " up and look kind of all of the different loss functions, and it'll tell you when to use which", "tokens": [51220, 493, 293, 574, 733, 295, 439, 295, 264, 819, 4470, 6828, 11, 293, 309, 603, 980, 291, 562, 281, 764, 597, 51432], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3654, "seek": 1538292, "start": 15404.28, "end": 15409.48, "text": " one and you can kind of mess with them and tweak them if you want. Now history equals model dot", "tokens": [51432, 472, 293, 291, 393, 733, 295, 2082, 365, 552, 293, 29879, 552, 498, 291, 528, 13, 823, 2503, 6915, 2316, 5893, 51692], "temperature": 0.0, "avg_logprob": -0.10382955339219835, "compression_ratio": 1.8282828282828283, "no_speech_prob": 0.0018674785969778895}, {"id": 3655, "seek": 1540948, "start": 15409.48, "end": 15414.039999999999, "text": " fit. This is just so we can access some of the statistics from this model dot fit. Obviously,", "tokens": [50364, 3318, 13, 639, 307, 445, 370, 321, 393, 2105, 512, 295, 264, 12523, 490, 341, 2316, 5893, 3318, 13, 7580, 11, 50592], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3656, "seek": 1540948, "start": 15414.039999999999, "end": 15418.84, "text": " it's just training the data to this test images, test labels and train images and train labels", "tokens": [50592, 309, 311, 445, 3097, 264, 1412, 281, 341, 1500, 5267, 11, 1500, 16949, 293, 3847, 5267, 293, 3847, 16949, 50832], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3657, "seek": 1540948, "start": 15418.84, "end": 15423.56, "text": " where this is the validation data suite. So evaluating the model, we want to evaluate the", "tokens": [50832, 689, 341, 307, 264, 24071, 1412, 14205, 13, 407, 27479, 264, 2316, 11, 321, 528, 281, 13059, 264, 51068], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3658, "seek": 1540948, "start": 15423.56, "end": 15427.08, "text": " model, we can evaluate it now on the test images and test labels, we're obviously going to get", "tokens": [51068, 2316, 11, 321, 393, 13059, 309, 586, 322, 264, 1500, 5267, 293, 1500, 16949, 11, 321, 434, 2745, 516, 281, 483, 51244], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3659, "seek": 1540948, "start": 15427.08, "end": 15431.48, "text": " the same thing because the valuation is test images and test labels. So we should get the", "tokens": [51244, 264, 912, 551, 570, 264, 38546, 307, 1500, 5267, 293, 1500, 16949, 13, 407, 321, 820, 483, 264, 51464], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3660, "seek": 1540948, "start": 15431.48, "end": 15437.48, "text": " same accuracy as 6735, which we do right here. Alright, so there we go, we get about 70%. If", "tokens": [51464, 912, 14170, 382, 23879, 8794, 11, 597, 321, 360, 558, 510, 13, 2798, 11, 370, 456, 321, 352, 11, 321, 483, 466, 5285, 6856, 759, 51764], "temperature": 0.0, "avg_logprob": -0.10819808875813204, "compression_ratio": 1.9305555555555556, "no_speech_prob": 0.019122902303934097}, {"id": 3661, "seek": 1543748, "start": 15437.56, "end": 15441.08, "text": " you guys train this on 10 epochs, you should get close to 70. I'm a little bit lower just", "tokens": [50368, 291, 1074, 3847, 341, 322, 1266, 30992, 28346, 11, 291, 820, 483, 1998, 281, 5285, 13, 286, 478, 257, 707, 857, 3126, 445, 50544], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3662, "seek": 1543748, "start": 15441.08, "end": 15445.4, "text": " because I didn't want to go that high. And that is now the model. I mean, we could use this if", "tokens": [50544, 570, 286, 994, 380, 528, 281, 352, 300, 1090, 13, 400, 300, 307, 586, 264, 2316, 13, 286, 914, 11, 321, 727, 764, 341, 498, 50760], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3663, "seek": 1543748, "start": 15445.4, "end": 15449.4, "text": " we want, we could use predict, we could pass in some image, and we could see the prediction for", "tokens": [50760, 321, 528, 11, 321, 727, 764, 6069, 11, 321, 727, 1320, 294, 512, 3256, 11, 293, 321, 727, 536, 264, 17630, 337, 50960], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3664, "seek": 1543748, "start": 15449.4, "end": 15453.0, "text": " it. I'm not going to do that just because we've already talked about that enough. And I want to", "tokens": [50960, 309, 13, 286, 478, 406, 516, 281, 360, 300, 445, 570, 321, 600, 1217, 2825, 466, 300, 1547, 13, 400, 286, 528, 281, 51140], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3665, "seek": 1543748, "start": 15453.0, "end": 15458.119999999999, "text": " get into some of the cooler stuff when we're working with smaller data sets. So the basic idea", "tokens": [51140, 483, 666, 512, 295, 264, 15566, 1507, 562, 321, 434, 1364, 365, 4356, 1412, 6352, 13, 407, 264, 3875, 1558, 51396], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3666, "seek": 1543748, "start": 15458.119999999999, "end": 15462.039999999999, "text": " here is this is actually a pretty small data set, right? We use about 60,000 images. And if you", "tokens": [51396, 510, 307, 341, 307, 767, 257, 1238, 1359, 1412, 992, 11, 558, 30, 492, 764, 466, 4060, 11, 1360, 5267, 13, 400, 498, 291, 51592], "temperature": 0.0, "avg_logprob": -0.07585285515185224, "compression_ratio": 1.771875, "no_speech_prob": 0.008576981723308563}, {"id": 3667, "seek": 1546204, "start": 15462.04, "end": 15467.880000000001, "text": " think about the amount of different patterns we need to pick up to classify, you know, things like", "tokens": [50364, 519, 466, 264, 2372, 295, 819, 8294, 321, 643, 281, 1888, 493, 281, 33872, 11, 291, 458, 11, 721, 411, 50656], "temperature": 0.0, "avg_logprob": -0.07527248258513164, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.06007770821452141}, {"id": 3668, "seek": 1546204, "start": 15467.880000000001, "end": 15473.480000000001, "text": " horses versus trucks, that's a pretty difficult task to do, which means that we need a lot of data.", "tokens": [50656, 13112, 5717, 16156, 11, 300, 311, 257, 1238, 2252, 5633, 281, 360, 11, 597, 1355, 300, 321, 643, 257, 688, 295, 1412, 13, 50936], "temperature": 0.0, "avg_logprob": -0.07527248258513164, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.06007770821452141}, {"id": 3669, "seek": 1546204, "start": 15473.480000000001, "end": 15478.2, "text": " And in fact, some of the best convolutional networks that are out there are trained on millions", "tokens": [50936, 400, 294, 1186, 11, 512, 295, 264, 1151, 45216, 304, 9590, 300, 366, 484, 456, 366, 8895, 322, 6803, 51172], "temperature": 0.0, "avg_logprob": -0.07527248258513164, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.06007770821452141}, {"id": 3670, "seek": 1546204, "start": 15478.2, "end": 15482.68, "text": " of pieces of, you know, sample information or data. So obviously, we don't have that kind of data.", "tokens": [51172, 295, 3755, 295, 11, 291, 458, 11, 6889, 1589, 420, 1412, 13, 407, 2745, 11, 321, 500, 380, 362, 300, 733, 295, 1412, 13, 51396], "temperature": 0.0, "avg_logprob": -0.07527248258513164, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.06007770821452141}, {"id": 3671, "seek": 1546204, "start": 15482.68, "end": 15487.480000000001, "text": " So how can we work with, you know, a few images, maybe like a few 1000 images, and still get a", "tokens": [51396, 407, 577, 393, 321, 589, 365, 11, 291, 458, 11, 257, 1326, 5267, 11, 1310, 411, 257, 1326, 9714, 5267, 11, 293, 920, 483, 257, 51636], "temperature": 0.0, "avg_logprob": -0.07527248258513164, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.06007770821452141}, {"id": 3672, "seek": 1548748, "start": 15487.48, "end": 15492.52, "text": " decent model. Well, the thing is, you can't unless we use some of the techniques that I have to show", "tokens": [50364, 8681, 2316, 13, 1042, 11, 264, 551, 307, 11, 291, 393, 380, 5969, 321, 764, 512, 295, 264, 7512, 300, 286, 362, 281, 855, 50616], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3673, "seek": 1548748, "start": 15492.52, "end": 15498.119999999999, "text": " you. So working with small data sets. So just like I mentioned, it's difficult to create a very good", "tokens": [50616, 291, 13, 407, 1364, 365, 1359, 1412, 6352, 13, 407, 445, 411, 286, 2835, 11, 309, 311, 2252, 281, 1884, 257, 588, 665, 50896], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3674, "seek": 1548748, "start": 15498.119999999999, "end": 15503.32, "text": " convolutional neural network from scratch, if you're using a small amount of data, that is why we", "tokens": [50896, 45216, 304, 18161, 3209, 490, 8459, 11, 498, 291, 434, 1228, 257, 1359, 2372, 295, 1412, 11, 300, 307, 983, 321, 51156], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3675, "seek": 1548748, "start": 15503.32, "end": 15508.84, "text": " can actually employ these techniques, the first one data augmentation, but also using pre trained", "tokens": [51156, 393, 767, 3188, 613, 7512, 11, 264, 700, 472, 1412, 14501, 19631, 11, 457, 611, 1228, 659, 8895, 51432], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3676, "seek": 1548748, "start": 15508.84, "end": 15512.439999999999, "text": " models to kind of accomplish what we need to do. And that's what we're going to be talking about", "tokens": [51432, 5245, 281, 733, 295, 9021, 437, 321, 643, 281, 360, 13, 400, 300, 311, 437, 321, 434, 516, 281, 312, 1417, 466, 51612], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3677, "seek": 1548748, "start": 15512.439999999999, "end": 15515.48, "text": " now in the second part of the tutorial, we're going to create another convolutional neural", "tokens": [51612, 586, 294, 264, 1150, 644, 295, 264, 7073, 11, 321, 434, 516, 281, 1884, 1071, 45216, 304, 18161, 51764], "temperature": 0.0, "avg_logprob": -0.07699463708060128, "compression_ratio": 1.778115501519757, "no_speech_prob": 0.02033107355237007}, {"id": 3678, "seek": 1551548, "start": 15515.56, "end": 15519.32, "text": " network. So just to clarify, this is created, we've made the model up here already. This is", "tokens": [50368, 3209, 13, 407, 445, 281, 17594, 11, 341, 307, 2942, 11, 321, 600, 1027, 264, 2316, 493, 510, 1217, 13, 639, 307, 50556], "temperature": 0.0, "avg_logprob": -0.07919179825555711, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0029807279352098703}, {"id": 3679, "seek": 1551548, "start": 15519.32, "end": 15523.0, "text": " all we need to do to do it. This is the architecture. And this was just to get you familiar with the", "tokens": [50556, 439, 321, 643, 281, 360, 281, 360, 309, 13, 639, 307, 264, 9482, 13, 400, 341, 390, 445, 281, 483, 291, 4963, 365, 264, 50740], "temperature": 0.0, "avg_logprob": -0.07919179825555711, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0029807279352098703}, {"id": 3680, "seek": 1551548, "start": 15523.0, "end": 15530.52, "text": " idea. So data augmentation. So this is basically the idea, if you have one image, we can turn that", "tokens": [50740, 1558, 13, 407, 1412, 14501, 19631, 13, 407, 341, 307, 1936, 264, 1558, 11, 498, 291, 362, 472, 3256, 11, 321, 393, 1261, 300, 51116], "temperature": 0.0, "avg_logprob": -0.07919179825555711, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0029807279352098703}, {"id": 3681, "seek": 1551548, "start": 15530.52, "end": 15537.56, "text": " image into several different images, and train and pass all those images to our, our model. So", "tokens": [51116, 3256, 666, 2940, 819, 5267, 11, 293, 3847, 293, 1320, 439, 729, 5267, 281, 527, 11, 527, 2316, 13, 407, 51468], "temperature": 0.0, "avg_logprob": -0.07919179825555711, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0029807279352098703}, {"id": 3682, "seek": 1551548, "start": 15537.56, "end": 15542.52, "text": " essentially, if we can rotate the image, if we can flip it, if we can stretch it, compress it,", "tokens": [51468, 4476, 11, 498, 321, 393, 13121, 264, 3256, 11, 498, 321, 393, 7929, 309, 11, 498, 321, 393, 5985, 309, 11, 14778, 309, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07919179825555711, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0029807279352098703}, {"id": 3683, "seek": 1554252, "start": 15543.08, "end": 15547.560000000001, "text": " you know, shift it, zoom it, whatever it is, and pass that to our model, it should be better at", "tokens": [50392, 291, 458, 11, 5513, 309, 11, 8863, 309, 11, 2035, 309, 307, 11, 293, 1320, 300, 281, 527, 2316, 11, 309, 820, 312, 1101, 412, 50616], "temperature": 0.0, "avg_logprob": -0.06630126190185547, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.010011552833020687}, {"id": 3684, "seek": 1554252, "start": 15547.560000000001, "end": 15553.720000000001, "text": " generalizing, because we'll see the same image, but modified and augmented multiple times, which", "tokens": [50616, 2674, 3319, 11, 570, 321, 603, 536, 264, 912, 3256, 11, 457, 15873, 293, 36155, 3866, 1413, 11, 597, 50924], "temperature": 0.0, "avg_logprob": -0.06630126190185547, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.010011552833020687}, {"id": 3685, "seek": 1554252, "start": 15553.720000000001, "end": 15560.84, "text": " means that we can turn a data set say of 10,000 images into 40,000 images, by doing four augmentations", "tokens": [50924, 1355, 300, 321, 393, 1261, 257, 1412, 992, 584, 295, 1266, 11, 1360, 5267, 666, 3356, 11, 1360, 5267, 11, 538, 884, 1451, 29919, 763, 51280], "temperature": 0.0, "avg_logprob": -0.06630126190185547, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.010011552833020687}, {"id": 3686, "seek": 1554252, "start": 15560.84, "end": 15566.6, "text": " on every single image. Now, obviously, you still want a lot of unique images, but this technique", "tokens": [51280, 322, 633, 2167, 3256, 13, 823, 11, 2745, 11, 291, 920, 528, 257, 688, 295, 3845, 5267, 11, 457, 341, 6532, 51568], "temperature": 0.0, "avg_logprob": -0.06630126190185547, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.010011552833020687}, {"id": 3687, "seek": 1554252, "start": 15566.6, "end": 15571.560000000001, "text": " can help a lot and is used quite a bit, because that allows our kind of model to be able to pick", "tokens": [51568, 393, 854, 257, 688, 293, 307, 1143, 1596, 257, 857, 11, 570, 300, 4045, 527, 733, 295, 2316, 281, 312, 1075, 281, 1888, 51816], "temperature": 0.0, "avg_logprob": -0.06630126190185547, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.010011552833020687}, {"id": 3688, "seek": 1557156, "start": 15571.56, "end": 15575.4, "text": " up images that maybe are orientated differently or zoomed in a bit or stretch something different,", "tokens": [50364, 493, 5267, 300, 1310, 366, 8579, 770, 7614, 420, 8863, 292, 294, 257, 857, 420, 5985, 746, 819, 11, 50556], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3689, "seek": 1557156, "start": 15575.4, "end": 15579.8, "text": " right, just better at generalizing, which is the whole point. So I'm not going to go through this", "tokens": [50556, 558, 11, 445, 1101, 412, 2674, 3319, 11, 597, 307, 264, 1379, 935, 13, 407, 286, 478, 406, 516, 281, 352, 807, 341, 50776], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3690, "seek": 1557156, "start": 15579.8, "end": 15584.279999999999, "text": " in too depth, too much depth, but this is essentially a script that does data augmentation", "tokens": [50776, 294, 886, 7161, 11, 886, 709, 7161, 11, 457, 341, 307, 4476, 257, 5755, 300, 775, 1412, 14501, 19631, 51000], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3691, "seek": 1557156, "start": 15584.279999999999, "end": 15590.439999999999, "text": " for you. We're gonna use this image data generator from the Keras dot preprocessing dot image module,", "tokens": [51000, 337, 291, 13, 492, 434, 799, 764, 341, 3256, 1412, 19265, 490, 264, 591, 6985, 5893, 2666, 340, 780, 278, 5893, 3256, 10088, 11, 51308], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3692, "seek": 1557156, "start": 15591.08, "end": 15595.32, "text": " we're going to create an image data generator object. Now essentially, what this allows us to", "tokens": [51340, 321, 434, 516, 281, 1884, 364, 3256, 1412, 19265, 2657, 13, 823, 4476, 11, 437, 341, 4045, 505, 281, 51552], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3693, "seek": 1557156, "start": 15595.32, "end": 15599.72, "text": " do is specify some parameters on how we want to modify our image. In this case, we have the", "tokens": [51552, 360, 307, 16500, 512, 9834, 322, 577, 321, 528, 281, 16927, 527, 3256, 13, 682, 341, 1389, 11, 321, 362, 264, 51772], "temperature": 0.0, "avg_logprob": -0.1148052421405161, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.0023965586442500353}, {"id": 3694, "seek": 1559972, "start": 15599.72, "end": 15606.679999999998, "text": " rotation range, some shifts, shear, zoom horizontal flip and the mode. Now I'm not going to go into", "tokens": [50364, 12447, 3613, 11, 512, 19201, 11, 24082, 11, 8863, 12750, 7929, 293, 264, 4391, 13, 823, 286, 478, 406, 516, 281, 352, 666, 50712], "temperature": 0.0, "avg_logprob": -0.0852488762091014, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006903325207531452}, {"id": 3695, "seek": 1559972, "start": 15606.679999999998, "end": 15610.439999999999, "text": " how this works, you can look at the documentation if you'd like. But essentially, this will just", "tokens": [50712, 577, 341, 1985, 11, 291, 393, 574, 412, 264, 14333, 498, 291, 1116, 411, 13, 583, 4476, 11, 341, 486, 445, 50900], "temperature": 0.0, "avg_logprob": -0.0852488762091014, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006903325207531452}, {"id": 3696, "seek": 1559972, "start": 15610.439999999999, "end": 15616.279999999999, "text": " allow us to augment our images. Now what I'm going to do is pick one arbitrary image from the test", "tokens": [50900, 2089, 505, 281, 29919, 527, 5267, 13, 823, 437, 286, 478, 516, 281, 360, 307, 1888, 472, 23211, 3256, 490, 264, 1500, 51192], "temperature": 0.0, "avg_logprob": -0.0852488762091014, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006903325207531452}, {"id": 3697, "seek": 1559972, "start": 15616.279999999999, "end": 15621.16, "text": " image data set, just our test image, I guess, group of photos, whatever you want to call it.", "tokens": [51192, 3256, 1412, 992, 11, 445, 527, 1500, 3256, 11, 286, 2041, 11, 1594, 295, 5787, 11, 2035, 291, 528, 281, 818, 309, 13, 51436], "temperature": 0.0, "avg_logprob": -0.0852488762091014, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006903325207531452}, {"id": 3698, "seek": 1559972, "start": 15621.16, "end": 15625.32, "text": " I'm going to convert that to an image array, which essentially takes it from the weird data set", "tokens": [51436, 286, 478, 516, 281, 7620, 300, 281, 364, 3256, 10225, 11, 597, 4476, 2516, 309, 490, 264, 3657, 1412, 992, 51644], "temperature": 0.0, "avg_logprob": -0.0852488762091014, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006903325207531452}, {"id": 3699, "seek": 1562532, "start": 15625.32, "end": 15630.52, "text": " object that it kind of is and turns it into a NumPy array. Then we're going to reshape this.", "tokens": [50364, 2657, 300, 309, 733, 295, 307, 293, 4523, 309, 666, 257, 22592, 47, 88, 10225, 13, 1396, 321, 434, 516, 281, 725, 42406, 341, 13, 50624], "temperature": 0.0, "avg_logprob": -0.08773857263418344, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.01798439770936966}, {"id": 3700, "seek": 1562532, "start": 15630.52, "end": 15634.6, "text": " So that's in the form one comma, which essentially means one, and then this will figure out what", "tokens": [50624, 407, 300, 311, 294, 264, 1254, 472, 22117, 11, 597, 4476, 1355, 472, 11, 293, 550, 341, 486, 2573, 484, 437, 50828], "temperature": 0.0, "avg_logprob": -0.08773857263418344, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.01798439770936966}, {"id": 3701, "seek": 1562532, "start": 15634.6, "end": 15639.72, "text": " the rest of the shape should be. Oh, sorry, one and then plus the image shape, which is whatever", "tokens": [50828, 264, 1472, 295, 264, 3909, 820, 312, 13, 876, 11, 2597, 11, 472, 293, 550, 1804, 264, 3256, 3909, 11, 597, 307, 2035, 51084], "temperature": 0.0, "avg_logprob": -0.08773857263418344, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.01798439770936966}, {"id": 3702, "seek": 1562532, "start": 15639.72, "end": 15645.96, "text": " this shape is. So we'll reshape that. And then what we're going to do is we're going to say for batch", "tokens": [51084, 341, 3909, 307, 13, 407, 321, 603, 725, 42406, 300, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 337, 15245, 51396], "temperature": 0.0, "avg_logprob": -0.08773857263418344, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.01798439770936966}, {"id": 3703, "seek": 1562532, "start": 15645.96, "end": 15651.72, "text": " in data flow gen dot flow. Talk about how that works in a second. Essentially, this is just going", "tokens": [51396, 294, 1412, 3095, 1049, 5893, 3095, 13, 8780, 466, 577, 300, 1985, 294, 257, 1150, 13, 23596, 11, 341, 307, 445, 516, 51684], "temperature": 0.0, "avg_logprob": -0.08773857263418344, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.01798439770936966}, {"id": 3704, "seek": 1565172, "start": 15651.8, "end": 15656.119999999999, "text": " to augment the image for us and actually save it onto our drive. So in this instance, what's", "tokens": [50368, 281, 29919, 264, 3256, 337, 505, 293, 767, 3155, 309, 3911, 527, 3332, 13, 407, 294, 341, 5197, 11, 437, 311, 50584], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3705, "seek": 1565172, "start": 15656.119999999999, "end": 15660.84, "text": " going to happen is this data gen dot flow is going to take the image which we've created here, right?", "tokens": [50584, 516, 281, 1051, 307, 341, 1412, 1049, 5893, 3095, 307, 516, 281, 747, 264, 3256, 597, 321, 600, 2942, 510, 11, 558, 30, 50820], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3706, "seek": 1565172, "start": 15660.84, "end": 15665.08, "text": " And we formatted it correctly by doing these two steps, which you need to do beforehand,", "tokens": [50820, 400, 321, 1254, 32509, 309, 8944, 538, 884, 613, 732, 4439, 11, 597, 291, 643, 281, 360, 22893, 11, 51032], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3707, "seek": 1565172, "start": 15665.08, "end": 15669.4, "text": " it's going to save this image as test dot jpeg. And this will be the prefix, which means there'll", "tokens": [51032, 309, 311, 516, 281, 3155, 341, 3256, 382, 1500, 5893, 361, 494, 70, 13, 400, 341, 486, 312, 264, 46969, 11, 597, 1355, 456, 603, 51248], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3708, "seek": 1565172, "start": 15669.4, "end": 15674.84, "text": " be some information after. And it will do this as many times until we break. So essentially,", "tokens": [51248, 312, 512, 1589, 934, 13, 400, 309, 486, 360, 341, 382, 867, 1413, 1826, 321, 1821, 13, 407, 4476, 11, 51520], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3709, "seek": 1565172, "start": 15674.84, "end": 15679.64, "text": " given an image, it will do test one, test two, test three, test four, test five, with random", "tokens": [51520, 2212, 364, 3256, 11, 309, 486, 360, 1500, 472, 11, 1500, 732, 11, 1500, 1045, 11, 1500, 1451, 11, 1500, 1732, 11, 365, 4974, 51760], "temperature": 0.0, "avg_logprob": -0.10531970899399012, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.01590483821928501}, {"id": 3710, "seek": 1567964, "start": 15679.64, "end": 15685.16, "text": " augmentations using this, until eventually we decided to break out of this. Now what I'm doing", "tokens": [50364, 29919, 763, 1228, 341, 11, 1826, 4728, 321, 3047, 281, 1821, 484, 295, 341, 13, 823, 437, 286, 478, 884, 50640], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3711, "seek": 1567964, "start": 15685.16, "end": 15689.48, "text": " is just showing the image by doing this and batch zero is just showing us the you know,", "tokens": [50640, 307, 445, 4099, 264, 3256, 538, 884, 341, 293, 15245, 4018, 307, 445, 4099, 505, 264, 291, 458, 11, 50856], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3712, "seek": 1567964, "start": 15689.48, "end": 15695.08, "text": " that first image in there. And that's kind of how this works. So you can mess with the script", "tokens": [50856, 300, 700, 3256, 294, 456, 13, 400, 300, 311, 733, 295, 577, 341, 1985, 13, 407, 291, 393, 2082, 365, 264, 5755, 51136], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3713, "seek": 1567964, "start": 15695.08, "end": 15697.96, "text": " and figure out a way to use it. But I would recommend if you want to do data augmentation,", "tokens": [51136, 293, 2573, 484, 257, 636, 281, 764, 309, 13, 583, 286, 576, 2748, 498, 291, 528, 281, 360, 1412, 14501, 19631, 11, 51280], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3714, "seek": 1567964, "start": 15697.96, "end": 15702.359999999999, "text": " just look into image data generator. This is something that I just want to show you so", "tokens": [51280, 445, 574, 666, 3256, 1412, 19265, 13, 639, 307, 746, 300, 286, 445, 528, 281, 855, 291, 370, 51500], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3715, "seek": 1567964, "start": 15702.359999999999, "end": 15705.96, "text": " you're aware of and I'll just run it so you can see exactly how this works. So essentially,", "tokens": [51500, 291, 434, 3650, 295, 293, 286, 603, 445, 1190, 309, 370, 291, 393, 536, 2293, 577, 341, 1985, 13, 407, 4476, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07822467636888045, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.0022517733741551638}, {"id": 3716, "seek": 1570596, "start": 15705.96, "end": 15709.32, "text": " given an image of a truck, what it will do is augmented in these different ways.", "tokens": [50364, 2212, 364, 3256, 295, 257, 5898, 11, 437, 309, 486, 360, 307, 36155, 294, 613, 819, 2098, 13, 50532], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3717, "seek": 1570596, "start": 15709.88, "end": 15715.16, "text": " You can see kind of the shifts, the translations, the rotations, all of that. And we'll do", "tokens": [50560, 509, 393, 536, 733, 295, 264, 19201, 11, 264, 37578, 11, 264, 44796, 11, 439, 295, 300, 13, 400, 321, 603, 360, 50824], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3718, "seek": 1570596, "start": 15716.039999999999, "end": 15718.919999999998, "text": " actually a different image here to see what one looks like. Let's just do image say 20.", "tokens": [50868, 767, 257, 819, 3256, 510, 281, 536, 437, 472, 1542, 411, 13, 961, 311, 445, 360, 3256, 584, 945, 13, 51012], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3719, "seek": 1570596, "start": 15719.8, "end": 15724.039999999999, "text": " See if we get something different. So in this case, I believe this is maybe like a deer or", "tokens": [51056, 3008, 498, 321, 483, 746, 819, 13, 407, 294, 341, 1389, 11, 286, 1697, 341, 307, 1310, 411, 257, 17120, 420, 51268], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3720, "seek": 1570596, "start": 15724.039999999999, "end": 15728.119999999999, "text": " rabbit or a dog or something. I don't really know exactly what it is because it's so blurry.", "tokens": [51268, 19509, 420, 257, 3000, 420, 746, 13, 286, 500, 380, 534, 458, 2293, 437, 309, 307, 570, 309, 311, 370, 37644, 13, 51472], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3721, "seek": 1570596, "start": 15728.119999999999, "end": 15731.56, "text": " But you can see that's kind of the shifts we're getting. And it makes sense because you want", "tokens": [51472, 583, 291, 393, 536, 300, 311, 733, 295, 264, 19201, 321, 434, 1242, 13, 400, 309, 1669, 2020, 570, 291, 528, 51644], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3722, "seek": 1570596, "start": 15731.56, "end": 15735.56, "text": " to have images in different areas so that we have a better generalization. All right,", "tokens": [51644, 281, 362, 5267, 294, 819, 3179, 370, 300, 321, 362, 257, 1101, 2674, 2144, 13, 1057, 558, 11, 51844], "temperature": 0.0, "avg_logprob": -0.08729734903649439, "compression_ratio": 1.808139534883721, "no_speech_prob": 0.034095291048288345}, {"id": 3723, "seek": 1573556, "start": 15735.64, "end": 15740.76, "text": " let's close that. Okay, so now we're going to talk about using or sorry, what is it pre trained", "tokens": [50368, 718, 311, 1998, 300, 13, 1033, 11, 370, 586, 321, 434, 516, 281, 751, 466, 1228, 420, 2597, 11, 437, 307, 309, 659, 8895, 50624], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3724, "seek": 1573556, "start": 15740.76, "end": 15745.32, "text": " models? Okay, so we talked about data augmentation. That's a great technique if you want to increase", "tokens": [50624, 5245, 30, 1033, 11, 370, 321, 2825, 466, 1412, 14501, 19631, 13, 663, 311, 257, 869, 6532, 498, 291, 528, 281, 3488, 50852], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3725, "seek": 1573556, "start": 15745.32, "end": 15749.48, "text": " the size of your data set. But what if even after that, we still don't have enough images in our", "tokens": [50852, 264, 2744, 295, 428, 1412, 992, 13, 583, 437, 498, 754, 934, 300, 11, 321, 920, 500, 380, 362, 1547, 5267, 294, 527, 51060], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3726, "seek": 1573556, "start": 15749.48, "end": 15754.359999999999, "text": " data set? Well, what we can do is use something called a pre trained model. Now companies like", "tokens": [51060, 1412, 992, 30, 1042, 11, 437, 321, 393, 360, 307, 764, 746, 1219, 257, 659, 8895, 2316, 13, 823, 3431, 411, 51304], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3727, "seek": 1573556, "start": 15754.359999999999, "end": 15759.56, "text": " Google and you know, TensorFlow, which is owned by Google, make their own amazing convolutional", "tokens": [51304, 3329, 293, 291, 458, 11, 37624, 11, 597, 307, 11684, 538, 3329, 11, 652, 641, 1065, 2243, 45216, 304, 51564], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3728, "seek": 1573556, "start": 15759.56, "end": 15763.32, "text": " neural networks that are completely open source that we can use. So what we're going to do is", "tokens": [51564, 18161, 9590, 300, 366, 2584, 1269, 4009, 300, 321, 393, 764, 13, 407, 437, 321, 434, 516, 281, 360, 307, 51752], "temperature": 0.0, "avg_logprob": -0.09972438945636883, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.001133497222326696}, {"id": 3729, "seek": 1576332, "start": 15763.32, "end": 15767.96, "text": " actually use part of a convolutional neural network that they've trained already on, I believe", "tokens": [50364, 767, 764, 644, 295, 257, 45216, 304, 18161, 3209, 300, 436, 600, 8895, 1217, 322, 11, 286, 1697, 50596], "temperature": 0.0, "avg_logprob": -0.06701623291528526, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.013221281580626965}, {"id": 3730, "seek": 1576332, "start": 15767.96, "end": 15774.52, "text": " 1.4 million images. And we're just going to use part of that model as kind of the base of our", "tokens": [50596, 502, 13, 19, 2459, 5267, 13, 400, 321, 434, 445, 516, 281, 764, 644, 295, 300, 2316, 382, 733, 295, 264, 3096, 295, 527, 50924], "temperature": 0.0, "avg_logprob": -0.06701623291528526, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.013221281580626965}, {"id": 3731, "seek": 1576332, "start": 15774.52, "end": 15779.48, "text": " models that we have a really good starting point. And all we need to do is what's called fine tune,", "tokens": [50924, 5245, 300, 321, 362, 257, 534, 665, 2891, 935, 13, 400, 439, 321, 643, 281, 360, 307, 437, 311, 1219, 2489, 10864, 11, 51172], "temperature": 0.0, "avg_logprob": -0.06701623291528526, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.013221281580626965}, {"id": 3732, "seek": 1576332, "start": 15779.48, "end": 15784.52, "text": " the last few layers of that network, so that they work a little bit better for our purposes.", "tokens": [51172, 264, 1036, 1326, 7914, 295, 300, 3209, 11, 370, 300, 436, 589, 257, 707, 857, 1101, 337, 527, 9932, 13, 51424], "temperature": 0.0, "avg_logprob": -0.06701623291528526, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.013221281580626965}, {"id": 3733, "seek": 1576332, "start": 15785.24, "end": 15789.96, "text": " So what we're going to do essentially say, All right, we have this model that Google's trained,", "tokens": [51460, 407, 437, 321, 434, 516, 281, 360, 4476, 584, 11, 1057, 558, 11, 321, 362, 341, 2316, 300, 3329, 311, 8895, 11, 51696], "temperature": 0.0, "avg_logprob": -0.06701623291528526, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.013221281580626965}, {"id": 3734, "seek": 1578996, "start": 15789.96, "end": 15794.279999999999, "text": " they've trained it on 1.4 million images, it's capable of classifying, let's say 1000 different", "tokens": [50364, 436, 600, 8895, 309, 322, 502, 13, 19, 2459, 5267, 11, 309, 311, 8189, 295, 1508, 5489, 11, 718, 311, 584, 9714, 819, 50580], "temperature": 0.0, "avg_logprob": -0.06768679815875597, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.003824130166321993}, {"id": 3735, "seek": 1578996, "start": 15794.279999999999, "end": 15799.48, "text": " classes, which is actually the example we'll look at later. So obviously, the beginning of that model", "tokens": [50580, 5359, 11, 597, 307, 767, 264, 1365, 321, 603, 574, 412, 1780, 13, 407, 2745, 11, 264, 2863, 295, 300, 2316, 50840], "temperature": 0.0, "avg_logprob": -0.06768679815875597, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.003824130166321993}, {"id": 3736, "seek": 1578996, "start": 15800.119999999999, "end": 15805.4, "text": " is what's picking up on the smaller edges, and you know, kind of the very general things that", "tokens": [50872, 307, 437, 311, 8867, 493, 322, 264, 4356, 8819, 11, 293, 291, 458, 11, 733, 295, 264, 588, 2674, 721, 300, 51136], "temperature": 0.0, "avg_logprob": -0.06768679815875597, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.003824130166321993}, {"id": 3737, "seek": 1578996, "start": 15805.4, "end": 15810.599999999999, "text": " appear in all of our images. So if we can use the base of that model, so kind of the beginning of", "tokens": [51136, 4204, 294, 439, 295, 527, 5267, 13, 407, 498, 321, 393, 764, 264, 3096, 295, 300, 2316, 11, 370, 733, 295, 264, 2863, 295, 51396], "temperature": 0.0, "avg_logprob": -0.06768679815875597, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.003824130166321993}, {"id": 3738, "seek": 1578996, "start": 15810.599999999999, "end": 15815.16, "text": " it, that does a really good job picking up on edges and general things that will apply to any", "tokens": [51396, 309, 11, 300, 775, 257, 534, 665, 1691, 8867, 493, 322, 8819, 293, 2674, 721, 300, 486, 3079, 281, 604, 51624], "temperature": 0.0, "avg_logprob": -0.06768679815875597, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.003824130166321993}, {"id": 3739, "seek": 1581516, "start": 15815.16, "end": 15820.2, "text": " images. Then what we can do is just change the top layers of that model a tiny bit or add our own", "tokens": [50364, 5267, 13, 1396, 437, 321, 393, 360, 307, 445, 1319, 264, 1192, 7914, 295, 300, 2316, 257, 5870, 857, 420, 909, 527, 1065, 50616], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3740, "seek": 1581516, "start": 15820.2, "end": 15825.24, "text": " layers to it to classify for the problem that we want. And that should be a very effective way", "tokens": [50616, 7914, 281, 309, 281, 33872, 337, 264, 1154, 300, 321, 528, 13, 400, 300, 820, 312, 257, 588, 4942, 636, 50868], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3741, "seek": 1581516, "start": 15825.24, "end": 15829.96, "text": " to use this pre trained model. We're saying we're going to use the beginning part that's really", "tokens": [50868, 281, 764, 341, 659, 8895, 2316, 13, 492, 434, 1566, 321, 434, 516, 281, 764, 264, 2863, 644, 300, 311, 534, 51104], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3742, "seek": 1581516, "start": 15829.96, "end": 15834.84, "text": " good at kind of the generalization step, then we'll pass it into our own layers that we'll do", "tokens": [51104, 665, 412, 733, 295, 264, 2674, 2144, 1823, 11, 550, 321, 603, 1320, 309, 666, 527, 1065, 7914, 300, 321, 603, 360, 51348], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3743, "seek": 1581516, "start": 15834.84, "end": 15839.48, "text": " whatever we need to do specifically for our problem. That's what's like the fine tuning", "tokens": [51348, 2035, 321, 643, 281, 360, 4682, 337, 527, 1154, 13, 663, 311, 437, 311, 411, 264, 2489, 15164, 51580], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3744, "seek": 1581516, "start": 15839.48, "end": 15842.68, "text": " step. And then we should have a model that works pretty well. And in fact, that's what we're going", "tokens": [51580, 1823, 13, 400, 550, 321, 820, 362, 257, 2316, 300, 1985, 1238, 731, 13, 400, 294, 1186, 11, 300, 311, 437, 321, 434, 516, 51740], "temperature": 0.0, "avg_logprob": -0.0502456611310932, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.03621671348810196}, {"id": 3745, "seek": 1584268, "start": 15842.76, "end": 15846.92, "text": " to do in this example now. So that's kind of the point of what I'm talking about here is using", "tokens": [50368, 281, 360, 294, 341, 1365, 586, 13, 407, 300, 311, 733, 295, 264, 935, 295, 437, 286, 478, 1417, 466, 510, 307, 1228, 50576], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3746, "seek": 1584268, "start": 15846.92, "end": 15851.24, "text": " part of a model that already exists, that's very good at generalizing, and it's been trained on", "tokens": [50576, 644, 295, 257, 2316, 300, 1217, 8198, 11, 300, 311, 588, 665, 412, 2674, 3319, 11, 293, 309, 311, 668, 8895, 322, 50792], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3747, "seek": 1584268, "start": 15851.24, "end": 15856.76, "text": " so many different images. And then we'll pass our own training data in, we won't modify the", "tokens": [50792, 370, 867, 819, 5267, 13, 400, 550, 321, 603, 1320, 527, 1065, 3097, 1412, 294, 11, 321, 1582, 380, 16927, 264, 51068], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3748, "seek": 1584268, "start": 15856.76, "end": 15861.4, "text": " beginning aspect of our neural network, because it already works really well, we'll just modify the", "tokens": [51068, 2863, 4171, 295, 527, 18161, 3209, 11, 570, 309, 1217, 1985, 534, 731, 11, 321, 603, 445, 16927, 264, 51300], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3749, "seek": 1584268, "start": 15861.4, "end": 15866.76, "text": " last few layers that are really good at classifying, for example, just cats and dogs, which is exactly", "tokens": [51300, 1036, 1326, 7914, 300, 366, 534, 665, 412, 1508, 5489, 11, 337, 1365, 11, 445, 11111, 293, 7197, 11, 597, 307, 2293, 51568], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3750, "seek": 1584268, "start": 15866.76, "end": 15870.04, "text": " the example we're actually going to do here. So I hope that makes sense as we get through this", "tokens": [51568, 264, 1365, 321, 434, 767, 516, 281, 360, 510, 13, 407, 286, 1454, 300, 1669, 2020, 382, 321, 483, 807, 341, 51732], "temperature": 0.0, "avg_logprob": -0.06785511634719203, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.031139614060521126}, {"id": 3751, "seek": 1587004, "start": 15870.12, "end": 15874.2, "text": " should be cleared up a little bit. But using a pretrained model is now the section we're", "tokens": [50368, 820, 312, 19725, 493, 257, 707, 857, 13, 583, 1228, 257, 1162, 31774, 2316, 307, 586, 264, 3541, 321, 434, 50572], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3752, "seek": 1587004, "start": 15874.2, "end": 15877.960000000001, "text": " getting into. So this is based on this documentation, as always, I'm referencing", "tokens": [50572, 1242, 666, 13, 407, 341, 307, 2361, 322, 341, 14333, 11, 382, 1009, 11, 286, 478, 40582, 50760], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3753, "seek": 1587004, "start": 15877.960000000001, "end": 15881.640000000001, "text": " everything. So you guys can go see that if you'd like, and do our imports like this,", "tokens": [50760, 1203, 13, 407, 291, 1074, 393, 352, 536, 300, 498, 291, 1116, 411, 11, 293, 360, 527, 41596, 411, 341, 11, 50944], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3754, "seek": 1587004, "start": 15881.640000000001, "end": 15885.160000000002, "text": " we're going to load a data set that actually takes a second to load the data set, I believe,", "tokens": [50944, 321, 434, 516, 281, 3677, 257, 1412, 992, 300, 767, 2516, 257, 1150, 281, 3677, 264, 1412, 992, 11, 286, 1697, 11, 51120], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3755, "seek": 1587004, "start": 15885.160000000002, "end": 15890.6, "text": " oh, maybe not. And essentially, the problem we're doing is trying to classify dogs versus cats with", "tokens": [51120, 1954, 11, 1310, 406, 13, 400, 4476, 11, 264, 1154, 321, 434, 884, 307, 1382, 281, 33872, 7197, 5717, 11111, 365, 51392], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3756, "seek": 1587004, "start": 15890.6, "end": 15896.04, "text": " a fair degree of accuracy. In fact, we'd like to get above 90%. So this is the data set we're", "tokens": [51392, 257, 3143, 4314, 295, 14170, 13, 682, 1186, 11, 321, 1116, 411, 281, 483, 3673, 4289, 6856, 407, 341, 307, 264, 1412, 992, 321, 434, 51664], "temperature": 0.0, "avg_logprob": -0.08386985233851842, "compression_ratio": 1.6959247648902822, "no_speech_prob": 0.014955655671656132}, {"id": 3757, "seek": 1589604, "start": 15896.04, "end": 15901.800000000001, "text": " loading in from TensorFlow data sets as TFDS. This is kind of a weird way to load it in again,", "tokens": [50364, 15114, 294, 490, 37624, 1412, 6352, 382, 40964, 11844, 13, 639, 307, 733, 295, 257, 3657, 636, 281, 3677, 309, 294, 797, 11, 50652], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3758, "seek": 1589604, "start": 15901.800000000001, "end": 15905.880000000001, "text": " stuff like this, you just have to reference the documentation, I can explain it to you, but", "tokens": [50652, 1507, 411, 341, 11, 291, 445, 362, 281, 6408, 264, 14333, 11, 286, 393, 2903, 309, 281, 291, 11, 457, 50856], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3759, "seek": 1589604, "start": 15905.880000000001, "end": 15908.84, "text": " it's not really going to help when the next example is going to be a different way of", "tokens": [50856, 309, 311, 406, 534, 516, 281, 854, 562, 264, 958, 1365, 307, 516, 281, 312, 257, 819, 636, 295, 51004], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3760, "seek": 1589604, "start": 15908.84, "end": 15912.6, "text": " loading the data, right? So so long as you know how to get the data in the correct form,", "tokens": [51004, 15114, 264, 1412, 11, 558, 30, 407, 370, 938, 382, 291, 458, 577, 281, 483, 264, 1412, 294, 264, 3006, 1254, 11, 51192], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3761, "seek": 1589604, "start": 15912.6, "end": 15916.44, "text": " you can get it into some kind of NumPy array, you can split it into training, testing and", "tokens": [51192, 291, 393, 483, 309, 666, 512, 733, 295, 22592, 47, 88, 10225, 11, 291, 393, 7472, 309, 666, 3097, 11, 4997, 293, 51384], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3762, "seek": 1589604, "start": 15916.44, "end": 15920.6, "text": " validation data, you should be okay. And if you're using a TensorFlow data set, it should", "tokens": [51384, 24071, 1412, 11, 291, 820, 312, 1392, 13, 400, 498, 291, 434, 1228, 257, 37624, 1412, 992, 11, 309, 820, 51592], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3763, "seek": 1589604, "start": 15920.6, "end": 15925.160000000002, "text": " tell you in the documentation how to load it in properly. So we'll load it in here, we're training", "tokens": [51592, 980, 291, 294, 264, 14333, 577, 281, 3677, 309, 294, 6108, 13, 407, 321, 603, 3677, 309, 294, 510, 11, 321, 434, 3097, 51820], "temperature": 0.0, "avg_logprob": -0.06330739801580255, "compression_ratio": 1.833810888252149, "no_speech_prob": 0.034096430987119675}, {"id": 3764, "seek": 1592516, "start": 15925.16, "end": 15931.4, "text": " 80% train will go 10% for what is it raw validation and 10% for the testing data.", "tokens": [50364, 4688, 4, 3847, 486, 352, 1266, 4, 337, 437, 307, 309, 8936, 24071, 293, 1266, 4, 337, 264, 4997, 1412, 13, 50676], "temperature": 0.0, "avg_logprob": -0.09178522077657408, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.002631436102092266}, {"id": 3765, "seek": 1592516, "start": 15932.119999999999, "end": 15936.44, "text": " So we've loaded that. And now what we're doing here is just we're going to look at a few images.", "tokens": [50712, 407, 321, 600, 13210, 300, 13, 400, 586, 437, 321, 434, 884, 510, 307, 445, 321, 434, 516, 281, 574, 412, 257, 1326, 5267, 13, 50928], "temperature": 0.0, "avg_logprob": -0.09178522077657408, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.002631436102092266}, {"id": 3766, "seek": 1592516, "start": 15936.44, "end": 15940.68, "text": " So this actually creates a function, I know this is a weird thing, this is pretty unique to this", "tokens": [50928, 407, 341, 767, 7829, 257, 2445, 11, 286, 458, 341, 307, 257, 3657, 551, 11, 341, 307, 1238, 3845, 281, 341, 51140], "temperature": 0.0, "avg_logprob": -0.09178522077657408, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.002631436102092266}, {"id": 3767, "seek": 1592516, "start": 15940.68, "end": 15947.0, "text": " example, that allows us to call this function with some integer, essentially, and get what the", "tokens": [51140, 1365, 11, 300, 4045, 505, 281, 818, 341, 2445, 365, 512, 24922, 11, 4476, 11, 293, 483, 437, 264, 51456], "temperature": 0.0, "avg_logprob": -0.09178522077657408, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.002631436102092266}, {"id": 3768, "seek": 1592516, "start": 15947.0, "end": 15951.24, "text": " actual string representation of that is to the label for it. And what I'm doing here is just", "tokens": [51456, 3539, 6798, 10290, 295, 300, 307, 281, 264, 7645, 337, 309, 13, 400, 437, 286, 478, 884, 510, 307, 445, 51668], "temperature": 0.0, "avg_logprob": -0.09178522077657408, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.002631436102092266}, {"id": 3769, "seek": 1595124, "start": 15951.24, "end": 15955.72, "text": " taking two images from our raw training data set, and just displaying them. And you can see", "tokens": [50364, 1940, 732, 5267, 490, 527, 8936, 3097, 1412, 992, 11, 293, 445, 36834, 552, 13, 400, 291, 393, 536, 50588], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3770, "seek": 1595124, "start": 15955.72, "end": 15961.32, "text": " that's where we're getting here dog and dog. If I go ahead and take five, we'll see, these are", "tokens": [50588, 300, 311, 689, 321, 434, 1242, 510, 3000, 293, 3000, 13, 759, 286, 352, 2286, 293, 747, 1732, 11, 321, 603, 536, 11, 613, 366, 50868], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3771, "seek": 1595124, "start": 15961.32, "end": 15967.4, "text": " what our images look like. Right? So here's example of a dog, we have a cat, right? And so on so forth,", "tokens": [50868, 437, 527, 5267, 574, 411, 13, 1779, 30, 407, 510, 311, 1365, 295, 257, 3000, 11, 321, 362, 257, 3857, 11, 558, 30, 400, 370, 322, 370, 5220, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3772, "seek": 1595124, "start": 15967.4, "end": 15971.96, "text": " you kind of you get the you get the point there. Now, notice, though, that these images are", "tokens": [51172, 291, 733, 295, 291, 483, 264, 291, 483, 264, 935, 456, 13, 823, 11, 3449, 11, 1673, 11, 300, 613, 5267, 366, 51400], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3773, "seek": 1595124, "start": 15971.96, "end": 15975.32, "text": " different dimensions. In fact, none of these images other than these two actually are the same", "tokens": [51400, 819, 12819, 13, 682, 1186, 11, 6022, 295, 613, 5267, 661, 813, 613, 732, 767, 366, 264, 912, 51568], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3774, "seek": 1595124, "start": 15975.32, "end": 15979.64, "text": " dimension at all. Oh, actually, I don't think these ones are either. So obviously, there's a", "tokens": [51568, 10139, 412, 439, 13, 876, 11, 767, 11, 286, 500, 380, 519, 613, 2306, 366, 2139, 13, 407, 2745, 11, 456, 311, 257, 51784], "temperature": 0.0, "avg_logprob": -0.10661034901936849, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.009411906823515892}, {"id": 3775, "seek": 1597964, "start": 15979.64, "end": 15983.72, "text": " step that we need to do, which is we need to scale all these images to be the same size.", "tokens": [50364, 1823, 300, 321, 643, 281, 360, 11, 597, 307, 321, 643, 281, 4373, 439, 613, 5267, 281, 312, 264, 912, 2744, 13, 50568], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3776, "seek": 1597964, "start": 15984.439999999999, "end": 15988.84, "text": " So to do that, what we're going to do is write a little function like this, and essentially,", "tokens": [50604, 407, 281, 360, 300, 11, 437, 321, 434, 516, 281, 360, 307, 2464, 257, 707, 2445, 411, 341, 11, 293, 4476, 11, 50824], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3777, "seek": 1597964, "start": 15988.84, "end": 15994.279999999999, "text": " we'll return an image that is reshaped. So I guess that is reshaped to the image size, which I'm", "tokens": [50824, 321, 603, 2736, 364, 3256, 300, 307, 725, 71, 18653, 13, 407, 286, 2041, 300, 307, 725, 71, 18653, 281, 264, 3256, 2744, 11, 597, 286, 478, 51096], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3778, "seek": 1597964, "start": 15994.279999999999, "end": 15999.16, "text": " going to set at 160 by 160. Now, we can make this bigger if we want. But the problem sometimes is", "tokens": [51096, 516, 281, 992, 412, 21243, 538, 21243, 13, 823, 11, 321, 393, 652, 341, 3801, 498, 321, 528, 13, 583, 264, 1154, 2171, 307, 51340], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3779, "seek": 1597964, "start": 15999.16, "end": 16004.68, "text": " if you make an image that is bigger than like you want to make your image bigger than most of your", "tokens": [51340, 498, 291, 652, 364, 3256, 300, 307, 3801, 813, 411, 291, 528, 281, 652, 428, 3256, 3801, 813, 881, 295, 428, 51616], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3780, "seek": 1597964, "start": 16004.68, "end": 16008.519999999999, "text": " data set examples, and that means you're going to be really stretching a lot of the examples out", "tokens": [51616, 1412, 992, 5110, 11, 293, 300, 1355, 291, 434, 516, 281, 312, 534, 19632, 257, 688, 295, 264, 5110, 484, 51808], "temperature": 0.0, "avg_logprob": -0.08211116790771485, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.01798216812312603}, {"id": 3781, "seek": 1600852, "start": 16008.52, "end": 16012.76, "text": " and you're losing a lot of detail. So it's much better to make the image size smaller rather", "tokens": [50364, 293, 291, 434, 7027, 257, 688, 295, 2607, 13, 407, 309, 311, 709, 1101, 281, 652, 264, 3256, 2744, 4356, 2831, 50576], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3782, "seek": 1600852, "start": 16012.76, "end": 16016.92, "text": " than bigger. You might say, well, if you make it smaller, you're going to lose detail too. But", "tokens": [50576, 813, 3801, 13, 509, 1062, 584, 11, 731, 11, 498, 291, 652, 309, 4356, 11, 291, 434, 516, 281, 3624, 2607, 886, 13, 583, 50784], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3783, "seek": 1600852, "start": 16016.92, "end": 16021.800000000001, "text": " it's just it's better to compress it smaller than it is to go really big, even just when it comes", "tokens": [50784, 309, 311, 445, 309, 311, 1101, 281, 14778, 309, 4356, 813, 309, 307, 281, 352, 534, 955, 11, 754, 445, 562, 309, 1487, 51028], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3784, "seek": 1600852, "start": 16021.800000000001, "end": 16025.880000000001, "text": " to the amount of training time and how complex networks going to be. So that's something to", "tokens": [51028, 281, 264, 2372, 295, 3097, 565, 293, 577, 3997, 9590, 516, 281, 312, 13, 407, 300, 311, 746, 281, 51232], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3785, "seek": 1600852, "start": 16025.880000000001, "end": 16029.4, "text": " consider. You can mess around with those when you're making your own networks. But again,", "tokens": [51232, 1949, 13, 509, 393, 2082, 926, 365, 729, 562, 291, 434, 1455, 428, 1065, 9590, 13, 583, 797, 11, 51408], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3786, "seek": 1600852, "start": 16029.4, "end": 16033.48, "text": " smaller is typically better in my opinion, you don't want to go too small, but something that's", "tokens": [51408, 4356, 307, 5850, 1101, 294, 452, 4800, 11, 291, 500, 380, 528, 281, 352, 886, 1359, 11, 457, 746, 300, 311, 51612], "temperature": 0.0, "avg_logprob": -0.07620773585975593, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.011685668490827084}, {"id": 3787, "seek": 1603348, "start": 16033.48, "end": 16038.199999999999, "text": " like, you know, half the size of what an average image would be. Alright, so we're going to go", "tokens": [50364, 411, 11, 291, 458, 11, 1922, 264, 2744, 295, 437, 364, 4274, 3256, 576, 312, 13, 2798, 11, 370, 321, 434, 516, 281, 352, 50600], "temperature": 0.0, "avg_logprob": -0.07938478159350018, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.044675130397081375}, {"id": 3788, "seek": 1603348, "start": 16038.199999999999, "end": 16042.359999999999, "text": " format example. So we're going to just take an image and a label. And what this will do is return", "tokens": [50600, 7877, 1365, 13, 407, 321, 434, 516, 281, 445, 747, 364, 3256, 293, 257, 7645, 13, 400, 437, 341, 486, 360, 307, 2736, 50808], "temperature": 0.0, "avg_logprob": -0.07938478159350018, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.044675130397081375}, {"id": 3789, "seek": 1603348, "start": 16042.359999999999, "end": 16046.84, "text": " to us just the reshaped image and label. So in this case, we're going to cast, which means convert", "tokens": [50808, 281, 505, 445, 264, 725, 71, 18653, 3256, 293, 7645, 13, 407, 294, 341, 1389, 11, 321, 434, 516, 281, 4193, 11, 597, 1355, 7620, 51032], "temperature": 0.0, "avg_logprob": -0.07938478159350018, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.044675130397081375}, {"id": 3790, "seek": 1603348, "start": 16046.84, "end": 16051.88, "text": " every single pixel in our image to be a float 32 value, because it could be integers, we're then", "tokens": [51032, 633, 2167, 19261, 294, 527, 3256, 281, 312, 257, 15706, 8858, 2158, 11, 570, 309, 727, 312, 41674, 11, 321, 434, 550, 51284], "temperature": 0.0, "avg_logprob": -0.07938478159350018, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.044675130397081375}, {"id": 3791, "seek": 1603348, "start": 16051.88, "end": 16059.4, "text": " going to divide that by 127.5, which taken is exactly half of 255. And then subtract one,", "tokens": [51284, 516, 281, 9845, 300, 538, 47561, 13, 20, 11, 597, 2726, 307, 2293, 1922, 295, 3552, 20, 13, 400, 550, 16390, 472, 11, 51660], "temperature": 0.0, "avg_logprob": -0.07938478159350018, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.044675130397081375}, {"id": 3792, "seek": 1605940, "start": 16059.48, "end": 16064.359999999999, "text": " then we're going to resize this image to be the image size. So sorry, the image will be", "tokens": [50368, 550, 321, 434, 516, 281, 50069, 341, 3256, 281, 312, 264, 3256, 2744, 13, 407, 2597, 11, 264, 3256, 486, 312, 50612], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3793, "seek": 1605940, "start": 16064.359999999999, "end": 16069.48, "text": " resized to the image size of 160 by 160 and we'll return the new image and the label. So now we", "tokens": [50612, 725, 1602, 281, 264, 3256, 2744, 295, 21243, 538, 21243, 293, 321, 603, 2736, 264, 777, 3256, 293, 264, 7645, 13, 407, 586, 321, 50868], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3794, "seek": 1605940, "start": 16069.48, "end": 16073.32, "text": " can apply this function to all of our images using map, if you don't know what map is, essentially,", "tokens": [50868, 393, 3079, 341, 2445, 281, 439, 295, 527, 5267, 1228, 4471, 11, 498, 291, 500, 380, 458, 437, 4471, 307, 11, 4476, 11, 51060], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3795, "seek": 1605940, "start": 16073.32, "end": 16078.359999999999, "text": " it takes every single example in in this case, going to be raw train and applies the function", "tokens": [51060, 309, 2516, 633, 2167, 1365, 294, 294, 341, 1389, 11, 516, 281, 312, 8936, 3847, 293, 13165, 264, 2445, 51312], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3796, "seek": 1605940, "start": 16078.359999999999, "end": 16084.039999999999, "text": " to it, which will mean that it will convert raw train into images that are all resized to 160", "tokens": [51312, 281, 309, 11, 597, 486, 914, 300, 309, 486, 7620, 8936, 3847, 666, 5267, 300, 366, 439, 725, 1602, 281, 21243, 51596], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3797, "seek": 1605940, "start": 16084.039999999999, "end": 16089.32, "text": " by 160. And we'll do the same thing for validation and test. So run that no issue there. Now", "tokens": [51596, 538, 21243, 13, 400, 321, 603, 360, 264, 912, 551, 337, 24071, 293, 1500, 13, 407, 1190, 300, 572, 2734, 456, 13, 823, 51860], "temperature": 0.0, "avg_logprob": -0.1105218690017174, "compression_ratio": 1.8862876254180603, "no_speech_prob": 0.0039450544863939285}, {"id": 3798, "seek": 1608932, "start": 16089.32, "end": 16093.88, "text": " let's have a look at our images and see what we get. And there we are. Now I've just messed up", "tokens": [50364, 718, 311, 362, 257, 574, 412, 527, 5267, 293, 536, 437, 321, 483, 13, 400, 456, 321, 366, 13, 823, 286, 600, 445, 16507, 493, 50592], "temperature": 0.0, "avg_logprob": -0.08648453202358512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00159778434317559}, {"id": 3799, "seek": 1608932, "start": 16093.88, "end": 16100.52, "text": " the color because I didn't add a CMAP thing, which I think I needed. Where was the CMAP?", "tokens": [50592, 264, 2017, 570, 286, 994, 380, 909, 257, 20424, 4715, 551, 11, 597, 286, 519, 286, 2978, 13, 2305, 390, 264, 20424, 4715, 30, 50924], "temperature": 0.0, "avg_logprob": -0.08648453202358512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00159778434317559}, {"id": 3800, "seek": 1608932, "start": 16102.039999999999, "end": 16104.92, "text": " Anyways, you know what, that's fine for now. This is what our images look like. This is the", "tokens": [51000, 15585, 11, 291, 458, 437, 11, 300, 311, 2489, 337, 586, 13, 639, 307, 437, 527, 5267, 574, 411, 13, 639, 307, 264, 51144], "temperature": 0.0, "avg_logprob": -0.08648453202358512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00159778434317559}, {"id": 3801, "seek": 1608932, "start": 16104.92, "end": 16111.4, "text": " resize. Now we get all images 160 by 160. And we are good to go. Alright, so", "tokens": [51144, 50069, 13, 823, 321, 483, 439, 5267, 21243, 538, 21243, 13, 400, 321, 366, 665, 281, 352, 13, 2798, 11, 370, 51468], "temperature": 0.0, "avg_logprob": -0.08648453202358512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00159778434317559}, {"id": 3802, "seek": 1608932, "start": 16112.68, "end": 16116.36, "text": " now let's have a look at the shape of an original image versus our new image. So I mean, this was", "tokens": [51532, 586, 718, 311, 362, 257, 574, 412, 264, 3909, 295, 364, 3380, 3256, 5717, 527, 777, 3256, 13, 407, 286, 914, 11, 341, 390, 51716], "temperature": 0.0, "avg_logprob": -0.08648453202358512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00159778434317559}, {"id": 3803, "seek": 1611636, "start": 16116.44, "end": 16121.960000000001, "text": " just to prove that essentially our original shapes were like 262 409 by some random values,", "tokens": [50368, 445, 281, 7081, 300, 4476, 527, 3380, 10854, 645, 411, 7551, 17, 3356, 24, 538, 512, 4974, 4190, 11, 50644], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3804, "seek": 1611636, "start": 16121.960000000001, "end": 16126.84, "text": " and they're all reshaped now to 160 160 by three, three obviously is the color channel of the images.", "tokens": [50644, 293, 436, 434, 439, 725, 71, 18653, 586, 281, 21243, 21243, 538, 1045, 11, 1045, 2745, 307, 264, 2017, 2269, 295, 264, 5267, 13, 50888], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3805, "seek": 1611636, "start": 16127.480000000001, "end": 16131.400000000001, "text": " Alright, so picking a pre trained model. So this is the next step, this is probably one of the", "tokens": [50920, 2798, 11, 370, 8867, 257, 659, 8895, 2316, 13, 407, 341, 307, 264, 958, 1823, 11, 341, 307, 1391, 472, 295, 264, 51116], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3806, "seek": 1611636, "start": 16131.400000000001, "end": 16135.560000000001, "text": " harder steps is picking a model that you would actually like to use the base up. Now we're going", "tokens": [51116, 6081, 4439, 307, 8867, 257, 2316, 300, 291, 576, 767, 411, 281, 764, 264, 3096, 493, 13, 823, 321, 434, 516, 51324], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3807, "seek": 1611636, "start": 16135.560000000001, "end": 16139.720000000001, "text": " to use one called mobile net v two, which is actually from Google, it's built into TensorFlow", "tokens": [51324, 281, 764, 472, 1219, 6013, 2533, 371, 732, 11, 597, 307, 767, 490, 3329, 11, 309, 311, 3094, 666, 37624, 51532], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3808, "seek": 1611636, "start": 16139.720000000001, "end": 16143.640000000001, "text": " itself. That's why I've picked it. And all we're going to do is set this. So essentially, we're", "tokens": [51532, 2564, 13, 663, 311, 983, 286, 600, 6183, 309, 13, 400, 439, 321, 434, 516, 281, 360, 307, 992, 341, 13, 407, 4476, 11, 321, 434, 51728], "temperature": 0.0, "avg_logprob": -0.11920419457840593, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.006289374083280563}, {"id": 3809, "seek": 1614364, "start": 16143.64, "end": 16149.24, "text": " going to say the base model in our code is equal to TF dot keras dot applications dot mobile net", "tokens": [50364, 516, 281, 584, 264, 3096, 2316, 294, 527, 3089, 307, 2681, 281, 40964, 5893, 350, 6985, 5893, 5821, 5893, 6013, 2533, 50644], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3810, "seek": 1614364, "start": 16149.24, "end": 16153.96, "text": " v two, which is just telling us the architecture of the model that we want. And we'll have a look", "tokens": [50644, 371, 732, 11, 597, 307, 445, 3585, 505, 264, 9482, 295, 264, 2316, 300, 321, 528, 13, 400, 321, 603, 362, 257, 574, 50880], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3811, "seek": 1614364, "start": 16153.96, "end": 16158.359999999999, "text": " at it down below here. In just a second, we'll define the input shape, which is important,", "tokens": [50880, 412, 309, 760, 2507, 510, 13, 682, 445, 257, 1150, 11, 321, 603, 6964, 264, 4846, 3909, 11, 597, 307, 1021, 11, 51100], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3812, "seek": 1614364, "start": 16158.359999999999, "end": 16162.68, "text": " because this can take any input shape that we want. So we'll change it to 160 160 by three,", "tokens": [51100, 570, 341, 393, 747, 604, 4846, 3909, 300, 321, 528, 13, 407, 321, 603, 1319, 309, 281, 21243, 21243, 538, 1045, 11, 51316], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3813, "seek": 1614364, "start": 16162.68, "end": 16168.279999999999, "text": " which we've defined up here, include top, very important means do we include the", "tokens": [51316, 597, 321, 600, 7642, 493, 510, 11, 4090, 1192, 11, 588, 1021, 1355, 360, 321, 4090, 264, 51596], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3814, "seek": 1614364, "start": 16168.279999999999, "end": 16172.84, "text": " classifier that comes with this network already or not. Now in our case, we're going to be", "tokens": [51596, 1508, 9902, 300, 1487, 365, 341, 3209, 1217, 420, 406, 13, 823, 294, 527, 1389, 11, 321, 434, 516, 281, 312, 51824], "temperature": 0.0, "avg_logprob": -0.09729006631033761, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.010326938703656197}, {"id": 3815, "seek": 1617284, "start": 16172.84, "end": 16178.6, "text": " retraining parts of this network so that it works specifically for dogs and cats, and not for 1000", "tokens": [50364, 49356, 1760, 3166, 295, 341, 3209, 370, 300, 309, 1985, 4682, 337, 7197, 293, 11111, 11, 293, 406, 337, 9714, 50652], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3816, "seek": 1617284, "start": 16178.6, "end": 16183.48, "text": " different classes, which is what this model was actually aimed to do is train a 1.4 million images", "tokens": [50652, 819, 5359, 11, 597, 307, 437, 341, 2316, 390, 767, 20540, 281, 360, 307, 3847, 257, 502, 13, 19, 2459, 5267, 50896], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3817, "seek": 1617284, "start": 16183.48, "end": 16188.28, "text": " for 1000 different classes of everyday objects. So we're going to not include the top, which means", "tokens": [50896, 337, 9714, 819, 5359, 295, 7429, 6565, 13, 407, 321, 434, 516, 281, 406, 4090, 264, 1192, 11, 597, 1355, 51136], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3818, "seek": 1617284, "start": 16188.28, "end": 16193.4, "text": " I don't include the classifier for these 1000 classes. And we're going to load the weights", "tokens": [51136, 286, 500, 380, 4090, 264, 1508, 9902, 337, 613, 9714, 5359, 13, 400, 321, 434, 516, 281, 3677, 264, 17443, 51392], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3819, "seek": 1617284, "start": 16193.4, "end": 16198.2, "text": " from what's called image net, which is just a specific save of the weights. So this is the", "tokens": [51392, 490, 437, 311, 1219, 3256, 2533, 11, 597, 307, 445, 257, 2685, 3155, 295, 264, 17443, 13, 407, 341, 307, 264, 51632], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3820, "seek": 1617284, "start": 16198.2, "end": 16202.12, "text": " architecture. And this is kind of the data that we're filling in for that architecture. So the", "tokens": [51632, 9482, 13, 400, 341, 307, 733, 295, 264, 1412, 300, 321, 434, 10623, 294, 337, 300, 9482, 13, 407, 264, 51828], "temperature": 0.0, "avg_logprob": -0.07551111894495346, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0031723042484372854}, {"id": 3821, "seek": 1620212, "start": 16202.12, "end": 16206.92, "text": " weights, and we'll load that in which we have here. So base model, now let's look at it. So", "tokens": [50364, 17443, 11, 293, 321, 603, 3677, 300, 294, 597, 321, 362, 510, 13, 407, 3096, 2316, 11, 586, 718, 311, 574, 412, 309, 13, 407, 50604], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3822, "seek": 1620212, "start": 16206.92, "end": 16211.160000000002, "text": " let's have a summary. You can see this is a pretty crazy model. I mean, we would never be", "tokens": [50604, 718, 311, 362, 257, 12691, 13, 509, 393, 536, 341, 307, 257, 1238, 3219, 2316, 13, 286, 914, 11, 321, 576, 1128, 312, 50816], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3823, "seek": 1620212, "start": 16211.160000000002, "end": 16214.84, "text": " expected to create something like this by ourselves. This is, you know, teams of data", "tokens": [50816, 5176, 281, 1884, 746, 411, 341, 538, 4175, 13, 639, 307, 11, 291, 458, 11, 5491, 295, 1412, 51000], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3824, "seek": 1620212, "start": 16214.84, "end": 16219.08, "text": " scientists, PhD students, engineers, would I write the experts in the field that have created a", "tokens": [51000, 7708, 11, 14476, 1731, 11, 11955, 11, 576, 286, 2464, 264, 8572, 294, 264, 2519, 300, 362, 2942, 257, 51212], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3825, "seek": 1620212, "start": 16219.08, "end": 16223.160000000002, "text": " network like this. So that's why we're going to use it because it works so effectively for", "tokens": [51212, 3209, 411, 341, 13, 407, 300, 311, 983, 321, 434, 516, 281, 764, 309, 570, 309, 1985, 370, 8659, 337, 51416], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3826, "seek": 1620212, "start": 16223.160000000002, "end": 16227.800000000001, "text": " the generalization at the beginning, which is what we want. And then we can take those features", "tokens": [51416, 264, 2674, 2144, 412, 264, 2863, 11, 597, 307, 437, 321, 528, 13, 400, 550, 321, 393, 747, 729, 4122, 51648], "temperature": 0.0, "avg_logprob": -0.07784997816566083, "compression_ratio": 1.6717325227963526, "no_speech_prob": 0.0013249675976112485}, {"id": 3827, "seek": 1622780, "start": 16227.8, "end": 16232.679999999998, "text": " that this takes out. So in five by five by 1280, which is what I want us to focus on the output", "tokens": [50364, 300, 341, 2516, 484, 13, 407, 294, 1732, 538, 1732, 538, 2272, 4702, 11, 597, 307, 437, 286, 528, 505, 281, 1879, 322, 264, 5598, 50608], "temperature": 0.0, "avg_logprob": -0.08349737044303648, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.02096213400363922}, {"id": 3828, "seek": 1622780, "start": 16232.679999999998, "end": 16238.119999999999, "text": " of this actual network here. So really, you can see this last layer, we're going to take this and", "tokens": [50608, 295, 341, 3539, 3209, 510, 13, 407, 534, 11, 291, 393, 536, 341, 1036, 4583, 11, 321, 434, 516, 281, 747, 341, 293, 50880], "temperature": 0.0, "avg_logprob": -0.08349737044303648, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.02096213400363922}, {"id": 3829, "seek": 1622780, "start": 16238.119999999999, "end": 16244.199999999999, "text": " using this information, pass that to some more convolutional layers and actually our own classifier,", "tokens": [50880, 1228, 341, 1589, 11, 1320, 300, 281, 512, 544, 45216, 304, 7914, 293, 767, 527, 1065, 1508, 9902, 11, 51184], "temperature": 0.0, "avg_logprob": -0.08349737044303648, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.02096213400363922}, {"id": 3830, "seek": 1622780, "start": 16244.199999999999, "end": 16250.359999999999, "text": " I believe, and use that to predict versus dogs versus cats. So at this point, the base model", "tokens": [51184, 286, 1697, 11, 293, 764, 300, 281, 6069, 5717, 7197, 5717, 11111, 13, 407, 412, 341, 935, 11, 264, 3096, 2316, 51492], "temperature": 0.0, "avg_logprob": -0.08349737044303648, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.02096213400363922}, {"id": 3831, "seek": 1622780, "start": 16250.359999999999, "end": 16255.0, "text": " will simply output a shape 32 by five by five by 1280. That's the tensor that we're going to get", "tokens": [51492, 486, 2935, 5598, 257, 3909, 8858, 538, 1732, 538, 1732, 538, 2272, 4702, 13, 663, 311, 264, 40863, 300, 321, 434, 516, 281, 483, 51724], "temperature": 0.0, "avg_logprob": -0.08349737044303648, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.02096213400363922}, {"id": 3832, "seek": 1625500, "start": 16255.0, "end": 16259.08, "text": " out of this, that's the shape, you can watch how this kind of works as you go through it.", "tokens": [50364, 484, 295, 341, 11, 300, 311, 264, 3909, 11, 291, 393, 1159, 577, 341, 733, 295, 1985, 382, 291, 352, 807, 309, 13, 50568], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3833, "seek": 1625500, "start": 16260.2, "end": 16264.84, "text": " And yes, all right. So we can just have a look at this here. This what I wanted to do essentially", "tokens": [50624, 400, 2086, 11, 439, 558, 13, 407, 321, 393, 445, 362, 257, 574, 412, 341, 510, 13, 639, 437, 286, 1415, 281, 360, 4476, 50856], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3834, "seek": 1625500, "start": 16264.84, "end": 16269.72, "text": " was just look at what the actual shape was going to be. So 32 five by five by 1280, just because", "tokens": [50856, 390, 445, 574, 412, 437, 264, 3539, 3909, 390, 516, 281, 312, 13, 407, 8858, 1732, 538, 1732, 538, 2272, 4702, 11, 445, 570, 51100], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3835, "seek": 1625500, "start": 16269.72, "end": 16274.12, "text": " this gives us none until it knows what the input is. And now it's time to talk about freezing the", "tokens": [51100, 341, 2709, 505, 6022, 1826, 309, 3255, 437, 264, 4846, 307, 13, 400, 586, 309, 311, 565, 281, 751, 466, 20200, 264, 51320], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3836, "seek": 1625500, "start": 16274.12, "end": 16278.92, "text": " base. So essentially, the point is, we want to use this as the base of our network, which means", "tokens": [51320, 3096, 13, 407, 4476, 11, 264, 935, 307, 11, 321, 528, 281, 764, 341, 382, 264, 3096, 295, 527, 3209, 11, 597, 1355, 51560], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3837, "seek": 1625500, "start": 16278.92, "end": 16283.64, "text": " we don't want to change it. If we just put this network in right now is the base to our neural", "tokens": [51560, 321, 500, 380, 528, 281, 1319, 309, 13, 759, 321, 445, 829, 341, 3209, 294, 558, 586, 307, 264, 3096, 281, 527, 18161, 51796], "temperature": 0.0, "avg_logprob": -0.08030380298888762, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.016401585191488266}, {"id": 3838, "seek": 1628364, "start": 16283.64, "end": 16288.519999999999, "text": " network. Well, what's going to happen is, it's going to start retraining all these weights and", "tokens": [50364, 3209, 13, 1042, 11, 437, 311, 516, 281, 1051, 307, 11, 309, 311, 516, 281, 722, 49356, 1760, 439, 613, 17443, 293, 50608], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3839, "seek": 1628364, "start": 16288.519999999999, "end": 16294.599999999999, "text": " biases. And in fact, it's going to train 2.257 million more weights and biases, when in fact,", "tokens": [50608, 32152, 13, 400, 294, 1186, 11, 309, 311, 516, 281, 3847, 568, 13, 6074, 22, 2459, 544, 17443, 293, 32152, 11, 562, 294, 1186, 11, 50912], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3840, "seek": 1628364, "start": 16294.599999999999, "end": 16297.96, "text": " we don't want to change these because these have already been defined, they've been set. And we", "tokens": [50912, 321, 500, 380, 528, 281, 1319, 613, 570, 613, 362, 1217, 668, 7642, 11, 436, 600, 668, 992, 13, 400, 321, 51080], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3841, "seek": 1628364, "start": 16297.96, "end": 16301.96, "text": " know that they work well for the problem already, right, they worked well for classifying 1000", "tokens": [51080, 458, 300, 436, 589, 731, 337, 264, 1154, 1217, 11, 558, 11, 436, 2732, 731, 337, 1508, 5489, 9714, 51280], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3842, "seek": 1628364, "start": 16301.96, "end": 16305.48, "text": " classes. Why are we going to touch this now? And if we were going to touch this, what's the point", "tokens": [51280, 5359, 13, 1545, 366, 321, 516, 281, 2557, 341, 586, 30, 400, 498, 321, 645, 516, 281, 2557, 341, 11, 437, 311, 264, 935, 51456], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3843, "seek": 1628364, "start": 16305.48, "end": 16309.24, "text": " of even using this base, right, we don't want to train this, we want to leave it the same.", "tokens": [51456, 295, 754, 1228, 341, 3096, 11, 558, 11, 321, 500, 380, 528, 281, 3847, 341, 11, 321, 528, 281, 1856, 309, 264, 912, 13, 51644], "temperature": 0.0, "avg_logprob": -0.07225732995359689, "compression_ratio": 1.986013986013986, "no_speech_prob": 0.012052581645548344}, {"id": 3844, "seek": 1630924, "start": 16309.32, "end": 16313.32, "text": " So to do that, we're just going to freeze it. Now, freezing is a pretty, I mean, it just", "tokens": [50368, 407, 281, 360, 300, 11, 321, 434, 445, 516, 281, 15959, 309, 13, 823, 11, 20200, 307, 257, 1238, 11, 286, 914, 11, 309, 445, 50568], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3845, "seek": 1630924, "start": 16313.32, "end": 16318.76, "text": " essentially means turning the trainable attribute of a layer off or of the model off. So what we do", "tokens": [50568, 4476, 1355, 6246, 264, 3847, 712, 19667, 295, 257, 4583, 766, 420, 295, 264, 2316, 766, 13, 407, 437, 321, 360, 50840], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3846, "seek": 1630924, "start": 16318.76, "end": 16322.119999999999, "text": " is you just say base model dot trainable equals false, which essentially means that we are no", "tokens": [50840, 307, 291, 445, 584, 3096, 2316, 5893, 3847, 712, 6915, 7908, 11, 597, 4476, 1355, 300, 321, 366, 572, 51008], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3847, "seek": 1630924, "start": 16322.119999999999, "end": 16326.68, "text": " longer going to be training any aspect of that, I want to say model, although we'll just call it", "tokens": [51008, 2854, 516, 281, 312, 3097, 604, 4171, 295, 300, 11, 286, 528, 281, 584, 2316, 11, 4878, 321, 603, 445, 818, 309, 51236], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3848, "seek": 1630924, "start": 16326.68, "end": 16331.24, "text": " the base layer for now, or the base model. So now if we look at the summary, we can see when we", "tokens": [51236, 264, 3096, 4583, 337, 586, 11, 420, 264, 3096, 2316, 13, 407, 586, 498, 321, 574, 412, 264, 12691, 11, 321, 393, 536, 562, 321, 51464], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3849, "seek": 1630924, "start": 16331.24, "end": 16337.48, "text": " scroll down to the bottom, if we get there any day soon, that now the trainable parameters is", "tokens": [51464, 11369, 760, 281, 264, 2767, 11, 498, 321, 483, 456, 604, 786, 2321, 11, 300, 586, 264, 3847, 712, 9834, 307, 51776], "temperature": 0.0, "avg_logprob": -0.07231612432570685, "compression_ratio": 1.8655737704918032, "no_speech_prob": 0.005729880649596453}, {"id": 3850, "seek": 1633748, "start": 16337.48, "end": 16343.16, "text": " zero instead of 2.257 million, which it was before. And now it's time to add our own classifier on", "tokens": [50364, 4018, 2602, 295, 568, 13, 6074, 22, 2459, 11, 597, 309, 390, 949, 13, 400, 586, 309, 311, 565, 281, 909, 527, 1065, 1508, 9902, 322, 50648], "temperature": 0.0, "avg_logprob": -0.07239198303222656, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.03308059647679329}, {"id": 3851, "seek": 1633748, "start": 16343.16, "end": 16347.24, "text": " top of this. So essentially, we've got a pretty good network, right, five by five by 1280s,", "tokens": [50648, 1192, 295, 341, 13, 407, 4476, 11, 321, 600, 658, 257, 1238, 665, 3209, 11, 558, 11, 1732, 538, 1732, 538, 2272, 4702, 82, 11, 50852], "temperature": 0.0, "avg_logprob": -0.07239198303222656, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.03308059647679329}, {"id": 3852, "seek": 1633748, "start": 16347.24, "end": 16353.32, "text": " our last output. And what we want to do now is take that. And we want to use it to classify", "tokens": [50852, 527, 1036, 5598, 13, 400, 437, 321, 528, 281, 360, 586, 307, 747, 300, 13, 400, 321, 528, 281, 764, 309, 281, 33872, 51156], "temperature": 0.0, "avg_logprob": -0.07239198303222656, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.03308059647679329}, {"id": 3853, "seek": 1633748, "start": 16353.32, "end": 16358.52, "text": " either cat or either dog, right? So what we're going to do is add a global average layer,", "tokens": [51156, 2139, 3857, 420, 2139, 3000, 11, 558, 30, 407, 437, 321, 434, 516, 281, 360, 307, 909, 257, 4338, 4274, 4583, 11, 51416], "temperature": 0.0, "avg_logprob": -0.07239198303222656, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.03308059647679329}, {"id": 3854, "seek": 1633748, "start": 16358.52, "end": 16364.76, "text": " which essentially is going to take the entire average of every single so 1280 different layers", "tokens": [51416, 597, 4476, 307, 516, 281, 747, 264, 2302, 4274, 295, 633, 2167, 370, 2272, 4702, 819, 7914, 51728], "temperature": 0.0, "avg_logprob": -0.07239198303222656, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.03308059647679329}, {"id": 3855, "seek": 1636476, "start": 16364.84, "end": 16370.68, "text": " that are five by five, and put that into a 1d tensor, which is kind of flattening that for us.", "tokens": [50368, 300, 366, 1732, 538, 1732, 11, 293, 829, 300, 666, 257, 502, 67, 40863, 11, 597, 307, 733, 295, 24183, 278, 300, 337, 505, 13, 50660], "temperature": 0.0, "avg_logprob": -0.07553718936058783, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0028893589042127132}, {"id": 3856, "seek": 1636476, "start": 16370.68, "end": 16375.0, "text": " So we do that global average pooling. And then we're just going to add the prediction layer,", "tokens": [50660, 407, 321, 360, 300, 4338, 4274, 7005, 278, 13, 400, 550, 321, 434, 445, 516, 281, 909, 264, 17630, 4583, 11, 50876], "temperature": 0.0, "avg_logprob": -0.07553718936058783, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0028893589042127132}, {"id": 3857, "seek": 1636476, "start": 16375.0, "end": 16379.800000000001, "text": " which essentially is going to just be one dense node. And since we're only classifying two different", "tokens": [50876, 597, 4476, 307, 516, 281, 445, 312, 472, 18011, 9984, 13, 400, 1670, 321, 434, 787, 1508, 5489, 732, 819, 51116], "temperature": 0.0, "avg_logprob": -0.07553718936058783, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0028893589042127132}, {"id": 3858, "seek": 1636476, "start": 16379.800000000001, "end": 16384.68, "text": " classes, right, dogs and cats, we only need one, then we're going to add all these models together.", "tokens": [51116, 5359, 11, 558, 11, 7197, 293, 11111, 11, 321, 787, 643, 472, 11, 550, 321, 434, 516, 281, 909, 439, 613, 5245, 1214, 13, 51360], "temperature": 0.0, "avg_logprob": -0.07553718936058783, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0028893589042127132}, {"id": 3859, "seek": 1636476, "start": 16384.68, "end": 16389.08, "text": " So the base model, and I guess layers, the global average layer that we define there, and then the", "tokens": [51360, 407, 264, 3096, 2316, 11, 293, 286, 2041, 7914, 11, 264, 4338, 4274, 4583, 300, 321, 6964, 456, 11, 293, 550, 264, 51580], "temperature": 0.0, "avg_logprob": -0.07553718936058783, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0028893589042127132}, {"id": 3860, "seek": 1638908, "start": 16389.08, "end": 16394.920000000002, "text": " prediction layer to create our final model. So let's do this global average layer, prediction", "tokens": [50364, 17630, 4583, 281, 1884, 527, 2572, 2316, 13, 407, 718, 311, 360, 341, 4338, 4274, 4583, 11, 17630, 50656], "temperature": 0.0, "avg_logprob": -0.1135343631108602, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.06371074169874191}, {"id": 3861, "seek": 1638908, "start": 16394.920000000002, "end": 16400.760000000002, "text": " layer model, give that a second to kind of run there. Now when we look at the summary, we can see", "tokens": [50656, 4583, 2316, 11, 976, 300, 257, 1150, 281, 733, 295, 1190, 456, 13, 823, 562, 321, 574, 412, 264, 12691, 11, 321, 393, 536, 50948], "temperature": 0.0, "avg_logprob": -0.1135343631108602, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.06371074169874191}, {"id": 3862, "seek": 1638908, "start": 16400.760000000002, "end": 16405.72, "text": " we have mobile net v2, which is actually a model, but that is our base layer. And that's fine,", "tokens": [50948, 321, 362, 6013, 2533, 371, 17, 11, 597, 307, 767, 257, 2316, 11, 457, 300, 307, 527, 3096, 4583, 13, 400, 300, 311, 2489, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1135343631108602, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.06371074169874191}, {"id": 3863, "seek": 1638908, "start": 16405.72, "end": 16411.640000000003, "text": " because the output shape is that then global average pooling, which again, just takes this", "tokens": [51196, 570, 264, 5598, 3909, 307, 300, 550, 4338, 4274, 7005, 278, 11, 597, 797, 11, 445, 2516, 341, 51492], "temperature": 0.0, "avg_logprob": -0.1135343631108602, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.06371074169874191}, {"id": 3864, "seek": 1638908, "start": 16411.640000000003, "end": 16416.600000000002, "text": " flattens it out does the average for us. And then finally, our dense layer, which is going to", "tokens": [51492, 932, 1591, 694, 309, 484, 775, 264, 4274, 337, 505, 13, 400, 550, 2721, 11, 527, 18011, 4583, 11, 597, 307, 516, 281, 51740], "temperature": 0.0, "avg_logprob": -0.1135343631108602, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.06371074169874191}, {"id": 3865, "seek": 1641660, "start": 16416.6, "end": 16422.68, "text": " simply have one neuron, which is going to be our output. Now notice that we have 2.25 and nine", "tokens": [50364, 2935, 362, 472, 34090, 11, 597, 307, 516, 281, 312, 527, 5598, 13, 823, 3449, 300, 321, 362, 568, 13, 6074, 293, 4949, 50668], "temperature": 0.0, "avg_logprob": -0.06905882589278682, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.008061549626290798}, {"id": 3866, "seek": 1641660, "start": 16422.68, "end": 16428.92, "text": " million parameters in total, and only 1281 of them are trainable. That's because we have 1280", "tokens": [50668, 2459, 9834, 294, 3217, 11, 293, 787, 2272, 32875, 295, 552, 366, 3847, 712, 13, 663, 311, 570, 321, 362, 2272, 4702, 50980], "temperature": 0.0, "avg_logprob": -0.06905882589278682, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.008061549626290798}, {"id": 3867, "seek": 1641660, "start": 16428.92, "end": 16435.0, "text": " connections from this layer to this layer, which means 1280 weights and one bias. So that is what", "tokens": [50980, 9271, 490, 341, 4583, 281, 341, 4583, 11, 597, 1355, 2272, 4702, 17443, 293, 472, 12577, 13, 407, 300, 307, 437, 51284], "temperature": 0.0, "avg_logprob": -0.06905882589278682, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.008061549626290798}, {"id": 3868, "seek": 1641660, "start": 16435.0, "end": 16438.76, "text": " we're doing. This is what we have created now, this base, the majority of the network has been", "tokens": [51284, 321, 434, 884, 13, 639, 307, 437, 321, 362, 2942, 586, 11, 341, 3096, 11, 264, 6286, 295, 264, 3209, 575, 668, 51472], "temperature": 0.0, "avg_logprob": -0.06905882589278682, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.008061549626290798}, {"id": 3869, "seek": 1641660, "start": 16438.76, "end": 16443.559999999998, "text": " done for us. And we just add our own little classifier on top of this. And now we're going to", "tokens": [51472, 1096, 337, 505, 13, 400, 321, 445, 909, 527, 1065, 707, 1508, 9902, 322, 1192, 295, 341, 13, 400, 586, 321, 434, 516, 281, 51712], "temperature": 0.0, "avg_logprob": -0.06905882589278682, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.008061549626290798}, {"id": 3870, "seek": 1644356, "start": 16443.56, "end": 16448.600000000002, "text": " feed some training samples and data to this. Remember, we're not training this base layer", "tokens": [50364, 3154, 512, 3097, 10938, 293, 1412, 281, 341, 13, 5459, 11, 321, 434, 406, 3097, 341, 3096, 4583, 50616], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3871, "seek": 1644356, "start": 16448.600000000002, "end": 16452.84, "text": " whatsoever. So the only thing that needs to be learned is the weights and biases on these two", "tokens": [50616, 17076, 13, 407, 264, 787, 551, 300, 2203, 281, 312, 3264, 307, 264, 17443, 293, 32152, 322, 613, 732, 50828], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3872, "seek": 1644356, "start": 16452.84, "end": 16458.04, "text": " layers here. Once we have that, we should have a decent model ready to go. So let's actually train", "tokens": [50828, 7914, 510, 13, 3443, 321, 362, 300, 11, 321, 820, 362, 257, 8681, 2316, 1919, 281, 352, 13, 407, 718, 311, 767, 3847, 51088], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3873, "seek": 1644356, "start": 16458.04, "end": 16463.4, "text": " this now. I'm going to compile this here, I'm picking a learning rate that's very slow, what", "tokens": [51088, 341, 586, 13, 286, 478, 516, 281, 31413, 341, 510, 11, 286, 478, 8867, 257, 2539, 3314, 300, 311, 588, 2964, 11, 437, 51356], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3874, "seek": 1644356, "start": 16463.4, "end": 16467.800000000003, "text": " essentially what the learning rate means is how much am I allowed to modify the weights and biases", "tokens": [51356, 4476, 437, 264, 2539, 3314, 1355, 307, 577, 709, 669, 286, 4350, 281, 16927, 264, 17443, 293, 32152, 51576], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3875, "seek": 1644356, "start": 16467.800000000003, "end": 16472.04, "text": " of this network, which is what I've done, just made that very low, because we don't want to make", "tokens": [51576, 295, 341, 3209, 11, 597, 307, 437, 286, 600, 1096, 11, 445, 1027, 300, 588, 2295, 11, 570, 321, 500, 380, 528, 281, 652, 51788], "temperature": 0.0, "avg_logprob": -0.05873007740048196, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.007345373742282391}, {"id": 3876, "seek": 1647204, "start": 16472.04, "end": 16477.8, "text": " any major changes if we don't have to, because we're already using a base model that exists,", "tokens": [50364, 604, 2563, 2962, 498, 321, 500, 380, 362, 281, 11, 570, 321, 434, 1217, 1228, 257, 3096, 2316, 300, 8198, 11, 50652], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3877, "seek": 1647204, "start": 16477.8, "end": 16481.16, "text": " right? So we'll set the learning rate, I'm not going to talk about what this does specifically,", "tokens": [50652, 558, 30, 407, 321, 603, 992, 264, 2539, 3314, 11, 286, 478, 406, 516, 281, 751, 466, 437, 341, 775, 4682, 11, 50820], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3878, "seek": 1647204, "start": 16481.16, "end": 16485.4, "text": " you can look that up if you'd like to. And then the loss function will use binary cross entropy,", "tokens": [50820, 291, 393, 574, 300, 493, 498, 291, 1116, 411, 281, 13, 400, 550, 264, 4470, 2445, 486, 764, 17434, 3278, 30867, 11, 51032], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3879, "seek": 1647204, "start": 16485.4, "end": 16490.280000000002, "text": " just because we're using two classes, if you're using more than more than two classes, you would", "tokens": [51032, 445, 570, 321, 434, 1228, 732, 5359, 11, 498, 291, 434, 1228, 544, 813, 544, 813, 732, 5359, 11, 291, 576, 51276], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3880, "seek": 1647204, "start": 16490.280000000002, "end": 16495.56, "text": " just have cross entropy, or some other type of cross entropy. And then what we're going to do is", "tokens": [51276, 445, 362, 3278, 30867, 11, 420, 512, 661, 2010, 295, 3278, 30867, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 51540], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3881, "seek": 1647204, "start": 16495.56, "end": 16501.64, "text": " actually evaluate the model right now, before we even train it. So I've compiled it, I've just set", "tokens": [51540, 767, 13059, 264, 2316, 558, 586, 11, 949, 321, 754, 3847, 309, 13, 407, 286, 600, 36548, 309, 11, 286, 600, 445, 992, 51844], "temperature": 0.0, "avg_logprob": -0.06723682798188309, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.006289484910666943}, {"id": 3882, "seek": 1650164, "start": 16501.72, "end": 16507.559999999998, "text": " what we'll end up using. But I want to evaluate the model currently, without training it whatsoever,", "tokens": [50368, 437, 321, 603, 917, 493, 1228, 13, 583, 286, 528, 281, 13059, 264, 2316, 4362, 11, 1553, 3097, 309, 17076, 11, 50660], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3883, "seek": 1650164, "start": 16507.559999999998, "end": 16513.64, "text": " on our validation data or validation batches, and see what it actually looks like, what it", "tokens": [50660, 322, 527, 24071, 1412, 420, 24071, 15245, 279, 11, 293, 536, 437, 309, 767, 1542, 411, 11, 437, 309, 50964], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3884, "seek": 1650164, "start": 16513.64, "end": 16518.12, "text": " actually, you know, what we're getting right now, with the current base model being the way it is,", "tokens": [50964, 767, 11, 291, 458, 11, 437, 321, 434, 1242, 558, 586, 11, 365, 264, 2190, 3096, 2316, 885, 264, 636, 309, 307, 11, 51188], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3885, "seek": 1650164, "start": 16518.12, "end": 16522.04, "text": " and not having changed the weights and biases that completely random from the global average", "tokens": [51188, 293, 406, 1419, 3105, 264, 17443, 293, 32152, 300, 2584, 4974, 490, 264, 4338, 4274, 51384], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3886, "seek": 1650164, "start": 16522.04, "end": 16526.92, "text": " pooling in the dense layer. So let's evaluate. Let's see what we get as an accuracy. Okay,", "tokens": [51384, 7005, 278, 294, 264, 18011, 4583, 13, 407, 718, 311, 13059, 13, 961, 311, 536, 437, 321, 483, 382, 364, 14170, 13, 1033, 11, 51628], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3887, "seek": 1650164, "start": 16526.92, "end": 16531.32, "text": " so we can actually see that with the random weights and biases for those last layer that we added,", "tokens": [51628, 370, 321, 393, 767, 536, 300, 365, 264, 4974, 17443, 293, 32152, 337, 729, 1036, 4583, 300, 321, 3869, 11, 51848], "temperature": 0.0, "avg_logprob": -0.08408275886818214, "compression_ratio": 1.8603896103896105, "no_speech_prob": 0.007344944402575493}, {"id": 3888, "seek": 1653164, "start": 16531.72, "end": 16536.04, "text": " we're getting an accuracy of 56%, which pretty much means that it's guessing, right? It's,", "tokens": [50368, 321, 434, 1242, 364, 14170, 295, 19687, 8923, 597, 1238, 709, 1355, 300, 309, 311, 17939, 11, 558, 30, 467, 311, 11, 50584], "temperature": 0.0, "avg_logprob": -0.09660844069260817, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0012065359624102712}, {"id": 3889, "seek": 1653164, "start": 16536.04, "end": 16540.76, "text": " you know, 50% is only two classes. So if we got anything lower than 50, like 50 should have been", "tokens": [50584, 291, 458, 11, 2625, 4, 307, 787, 732, 5359, 13, 407, 498, 321, 658, 1340, 3126, 813, 2625, 11, 411, 2625, 820, 362, 668, 50820], "temperature": 0.0, "avg_logprob": -0.09660844069260817, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0012065359624102712}, {"id": 3890, "seek": 1653164, "start": 16540.76, "end": 16546.04, "text": " our guess, which is what we're getting. So now what we're going to do. And actually, I've trained", "tokens": [50820, 527, 2041, 11, 597, 307, 437, 321, 434, 1242, 13, 407, 586, 437, 321, 434, 516, 281, 360, 13, 400, 767, 11, 286, 600, 8895, 51084], "temperature": 0.0, "avg_logprob": -0.09660844069260817, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0012065359624102712}, {"id": 3891, "seek": 1653164, "start": 16546.04, "end": 16553.0, "text": " this already, I think so I might not have to do it again, is train this model on all of our images.", "tokens": [51084, 341, 1217, 11, 286, 519, 370, 286, 1062, 406, 362, 281, 360, 309, 797, 11, 307, 3847, 341, 2316, 322, 439, 295, 527, 5267, 13, 51432], "temperature": 0.0, "avg_logprob": -0.09660844069260817, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0012065359624102712}, {"id": 3892, "seek": 1653164, "start": 16553.0, "end": 16557.559999999998, "text": " So all of our images and cats and cats and dogs that we've loaded in before, which will allow us", "tokens": [51432, 407, 439, 295, 527, 5267, 293, 11111, 293, 11111, 293, 7197, 300, 321, 600, 13210, 294, 949, 11, 597, 486, 2089, 505, 51660], "temperature": 0.0, "avg_logprob": -0.09660844069260817, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0012065359624102712}, {"id": 3893, "seek": 1655756, "start": 16557.640000000003, "end": 16563.16, "text": " now to modify these weights and biases of this layer. So hopefully it can determine what features", "tokens": [50368, 586, 281, 16927, 613, 17443, 293, 32152, 295, 341, 4583, 13, 407, 4696, 309, 393, 6997, 437, 4122, 50644], "temperature": 0.0, "avg_logprob": -0.06315096479947449, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.022975163534283638}, {"id": 3894, "seek": 1655756, "start": 16563.16, "end": 16568.04, "text": " need to be present for a dog to be a dog and for a cat to be a cat, right? And then it can make a", "tokens": [50644, 643, 281, 312, 1974, 337, 257, 3000, 281, 312, 257, 3000, 293, 337, 257, 3857, 281, 312, 257, 3857, 11, 558, 30, 400, 550, 309, 393, 652, 257, 50888], "temperature": 0.0, "avg_logprob": -0.06315096479947449, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.022975163534283638}, {"id": 3895, "seek": 1655756, "start": 16568.04, "end": 16571.960000000003, "text": " pretty good prediction. In fact, I'm not going to train this in front of us right now, because", "tokens": [50888, 1238, 665, 17630, 13, 682, 1186, 11, 286, 478, 406, 516, 281, 3847, 341, 294, 1868, 295, 505, 558, 586, 11, 570, 51084], "temperature": 0.0, "avg_logprob": -0.06315096479947449, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.022975163534283638}, {"id": 3896, "seek": 1655756, "start": 16571.960000000003, "end": 16576.440000000002, "text": " it actually takes close to an hour to train just because there is a lot of images that it needs", "tokens": [51084, 309, 767, 2516, 1998, 281, 364, 1773, 281, 3847, 445, 570, 456, 307, 257, 688, 295, 5267, 300, 309, 2203, 51308], "temperature": 0.0, "avg_logprob": -0.06315096479947449, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.022975163534283638}, {"id": 3897, "seek": 1655756, "start": 16576.440000000002, "end": 16581.960000000003, "text": " to look at and a lot of calculations that need to happen. But when you do end up training this,", "tokens": [51308, 281, 574, 412, 293, 257, 688, 295, 20448, 300, 643, 281, 1051, 13, 583, 562, 291, 360, 917, 493, 3097, 341, 11, 51584], "temperature": 0.0, "avg_logprob": -0.06315096479947449, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.022975163534283638}, {"id": 3898, "seek": 1658196, "start": 16581.96, "end": 16587.559999999998, "text": " you end up getting an accuracy of a close to 92 or 93%, which is pretty good,", "tokens": [50364, 291, 917, 493, 1242, 364, 14170, 295, 257, 1998, 281, 28225, 420, 28876, 8923, 597, 307, 1238, 665, 11, 50644], "temperature": 0.0, "avg_logprob": -0.0725170544215611, "compression_ratio": 1.607773851590106, "no_speech_prob": 0.03409621864557266}, {"id": 3899, "seek": 1658196, "start": 16587.559999999998, "end": 16593.16, "text": " considering the fact that all we did was use an original layer, like base layer that classified", "tokens": [50644, 8079, 264, 1186, 300, 439, 321, 630, 390, 764, 364, 3380, 4583, 11, 411, 3096, 4583, 300, 20627, 50924], "temperature": 0.0, "avg_logprob": -0.0725170544215611, "compression_ratio": 1.607773851590106, "no_speech_prob": 0.03409621864557266}, {"id": 3900, "seek": 1658196, "start": 16593.16, "end": 16598.52, "text": " up to 1000 different images, so very general, and applied that just to cats and dogs by adding", "tokens": [50924, 493, 281, 9714, 819, 5267, 11, 370, 588, 2674, 11, 293, 6456, 300, 445, 281, 11111, 293, 7197, 538, 5127, 51192], "temperature": 0.0, "avg_logprob": -0.0725170544215611, "compression_ratio": 1.607773851590106, "no_speech_prob": 0.03409621864557266}, {"id": 3901, "seek": 1658196, "start": 16598.52, "end": 16603.239999999998, "text": " our dense layer classifier on top. So you can see this was kind of the accuracy I had from", "tokens": [51192, 527, 18011, 4583, 1508, 9902, 322, 1192, 13, 407, 291, 393, 536, 341, 390, 733, 295, 264, 14170, 286, 632, 490, 51428], "temperature": 0.0, "avg_logprob": -0.0725170544215611, "compression_ratio": 1.607773851590106, "no_speech_prob": 0.03409621864557266}, {"id": 3902, "seek": 1658196, "start": 16603.239999999998, "end": 16607.0, "text": " training this previously, I don't want to train again, because it takes so long. But I did want", "tokens": [51428, 3097, 341, 8046, 11, 286, 500, 380, 528, 281, 3847, 797, 11, 570, 309, 2516, 370, 938, 13, 583, 286, 630, 528, 51616], "temperature": 0.0, "avg_logprob": -0.0725170544215611, "compression_ratio": 1.607773851590106, "no_speech_prob": 0.03409621864557266}, {"id": 3903, "seek": 1660700, "start": 16607.08, "end": 16613.48, "text": " to show that you can save a model and load a model by doing this syntax. So essentially,", "tokens": [50368, 281, 855, 300, 291, 393, 3155, 257, 2316, 293, 3677, 257, 2316, 538, 884, 341, 28431, 13, 407, 4476, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09667504628499349, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.2627734839916229}, {"id": 3904, "seek": 1660700, "start": 16613.48, "end": 16618.36, "text": " on your model object, you can call model dot save, save it as whatever name you like dot h5,", "tokens": [50688, 322, 428, 2316, 2657, 11, 291, 393, 818, 2316, 5893, 3155, 11, 3155, 309, 382, 2035, 1315, 291, 411, 5893, 276, 20, 11, 50932], "temperature": 0.0, "avg_logprob": -0.09667504628499349, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.2627734839916229}, {"id": 3905, "seek": 1660700, "start": 16618.36, "end": 16623.96, "text": " which is just a format for saving models and Keras is specific to Keras, not TensorFlow.", "tokens": [50932, 597, 307, 445, 257, 7877, 337, 6816, 5245, 293, 591, 6985, 307, 2685, 281, 591, 6985, 11, 406, 37624, 13, 51212], "temperature": 0.0, "avg_logprob": -0.09667504628499349, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.2627734839916229}, {"id": 3906, "seek": 1660700, "start": 16623.96, "end": 16628.6, "text": " And then you can load the model by doing this. So this is useful because after you train this", "tokens": [51212, 400, 550, 291, 393, 3677, 264, 2316, 538, 884, 341, 13, 407, 341, 307, 4420, 570, 934, 291, 3847, 341, 51444], "temperature": 0.0, "avg_logprob": -0.09667504628499349, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.2627734839916229}, {"id": 3907, "seek": 1660700, "start": 16628.6, "end": 16632.6, "text": " for an hour, obviously, you don't want to retrain this if you don't have to to actually use it to", "tokens": [51444, 337, 364, 1773, 11, 2745, 11, 291, 500, 380, 528, 281, 1533, 7146, 341, 498, 291, 500, 380, 362, 281, 281, 767, 764, 309, 281, 51644], "temperature": 0.0, "avg_logprob": -0.09667504628499349, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.2627734839916229}, {"id": 3908, "seek": 1663260, "start": 16632.6, "end": 16638.28, "text": " make predictions. So you can just load the model. Now, I'm not going to go into using the model", "tokens": [50364, 652, 21264, 13, 407, 291, 393, 445, 3677, 264, 2316, 13, 823, 11, 286, 478, 406, 516, 281, 352, 666, 1228, 264, 2316, 50648], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3909, "seek": 1663260, "start": 16638.28, "end": 16641.879999999997, "text": " specifically, you guys can look up the documentation to do that. We're at the point now where I've", "tokens": [50648, 4682, 11, 291, 1074, 393, 574, 493, 264, 14333, 281, 360, 300, 13, 492, 434, 412, 264, 935, 586, 689, 286, 600, 50828], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3910, "seek": 1663260, "start": 16641.879999999997, "end": 16646.28, "text": " showed you so much syntax on predicting and how we actually use the models. But the basic idea", "tokens": [50828, 4712, 291, 370, 709, 28431, 322, 32884, 293, 577, 321, 767, 764, 264, 5245, 13, 583, 264, 3875, 1558, 51048], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3911, "seek": 1663260, "start": 16646.28, "end": 16650.12, "text": " would be to do model dot predict, right? And then you can see that it's even giving me the input", "tokens": [51048, 576, 312, 281, 360, 2316, 5893, 6069, 11, 558, 30, 400, 550, 291, 393, 536, 300, 309, 311, 754, 2902, 385, 264, 4846, 51240], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3912, "seek": 1663260, "start": 16650.12, "end": 16655.079999999998, "text": " here. So model dot predict, give it some x batch size or both, right, because it will predict on", "tokens": [51240, 510, 13, 407, 2316, 5893, 6069, 11, 976, 309, 512, 2031, 15245, 2744, 420, 1293, 11, 558, 11, 570, 309, 486, 6069, 322, 51488], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3913, "seek": 1663260, "start": 16655.079999999998, "end": 16658.92, "text": " multiple things. And that will spit back to you a class, which then you can figure out, okay,", "tokens": [51488, 3866, 721, 13, 400, 300, 486, 22127, 646, 281, 291, 257, 1508, 11, 597, 550, 291, 393, 2573, 484, 11, 1392, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07853445288253157, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.02675672434270382}, {"id": 3914, "seek": 1665892, "start": 16658.92, "end": 16662.92, "text": " this is a cat, or this is a dog, you're going to pass this obviously the same input information", "tokens": [50364, 341, 307, 257, 3857, 11, 420, 341, 307, 257, 3000, 11, 291, 434, 516, 281, 1320, 341, 2745, 264, 912, 4846, 1589, 50564], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3915, "seek": 1665892, "start": 16662.92, "end": 16667.32, "text": " we have before, which is 160 by 160 by three. And that will make the prediction for you.", "tokens": [50564, 321, 362, 949, 11, 597, 307, 21243, 538, 21243, 538, 1045, 13, 400, 300, 486, 652, 264, 17630, 337, 291, 13, 50784], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3916, "seek": 1665892, "start": 16667.96, "end": 16672.6, "text": " So that's kind of the thing there. I was getting an OS error just because I hadn't saved this", "tokens": [50816, 407, 300, 311, 733, 295, 264, 551, 456, 13, 286, 390, 1242, 364, 12731, 6713, 445, 570, 286, 8782, 380, 6624, 341, 51048], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3917, "seek": 1665892, "start": 16672.6, "end": 16676.039999999997, "text": " previously. But that's how you save and load models, which I think is important when you're", "tokens": [51048, 8046, 13, 583, 300, 311, 577, 291, 3155, 293, 3677, 5245, 11, 597, 286, 519, 307, 1021, 562, 291, 434, 51220], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3918, "seek": 1665892, "start": 16676.039999999997, "end": 16680.679999999997, "text": " doing very large models. So when you fit this, feel free to change the epochs to be something", "tokens": [51220, 884, 588, 2416, 5245, 13, 407, 562, 291, 3318, 341, 11, 841, 1737, 281, 1319, 264, 30992, 28346, 281, 312, 746, 51452], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3919, "seek": 1665892, "start": 16680.679999999997, "end": 16686.199999999997, "text": " slower if you'd like, again, right, this takes a long time to actually end up running. But you", "tokens": [51452, 14009, 498, 291, 1116, 411, 11, 797, 11, 558, 11, 341, 2516, 257, 938, 565, 281, 767, 917, 493, 2614, 13, 583, 291, 51728], "temperature": 0.0, "avg_logprob": -0.06893015241289473, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.011330747976899147}, {"id": 3920, "seek": 1668620, "start": 16686.2, "end": 16690.920000000002, "text": " can see that the accuracy increases pretty well, exponentially, exponentially from when we didn't", "tokens": [50364, 393, 536, 300, 264, 14170, 8637, 1238, 731, 11, 37330, 11, 37330, 490, 562, 321, 994, 380, 50600], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3921, "seek": 1668620, "start": 16690.920000000002, "end": 16695.56, "text": " even have that classifier on it. Now, the last thing that I want to talk about is object detection,", "tokens": [50600, 754, 362, 300, 1508, 9902, 322, 309, 13, 823, 11, 264, 1036, 551, 300, 286, 528, 281, 751, 466, 307, 2657, 17784, 11, 50832], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3922, "seek": 1668620, "start": 16695.56, "end": 16698.920000000002, "text": " I'm just going to load up a page, we're not going to do any examples, I'm just going to give you a", "tokens": [50832, 286, 478, 445, 516, 281, 3677, 493, 257, 3028, 11, 321, 434, 406, 516, 281, 360, 604, 5110, 11, 286, 478, 445, 516, 281, 976, 291, 257, 51000], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3923, "seek": 1668620, "start": 16698.920000000002, "end": 16703.0, "text": " brief introduction, because we're kind of running out of time for this module, because you can use", "tokens": [51000, 5353, 9339, 11, 570, 321, 434, 733, 295, 2614, 484, 295, 565, 337, 341, 10088, 11, 570, 291, 393, 764, 51204], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3924, "seek": 1668620, "start": 16703.0, "end": 16707.64, "text": " TensorFlow to do object detection and recognition, which is kind of cool. So let's get into that", "tokens": [51204, 37624, 281, 360, 2657, 17784, 293, 11150, 11, 597, 307, 733, 295, 1627, 13, 407, 718, 311, 483, 666, 300, 51436], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3925, "seek": 1668620, "start": 16707.64, "end": 16712.440000000002, "text": " now. Okay, so right now, I'm on a GitHub page that's built by TensorFlow here, I'm going to leave", "tokens": [51436, 586, 13, 1033, 11, 370, 558, 586, 11, 286, 478, 322, 257, 23331, 3028, 300, 311, 3094, 538, 37624, 510, 11, 286, 478, 516, 281, 1856, 51676], "temperature": 0.0, "avg_logprob": -0.07287303120100579, "compression_ratio": 1.8209876543209877, "no_speech_prob": 0.0024725410621613264}, {"id": 3926, "seek": 1671244, "start": 16712.44, "end": 16716.6, "text": " that link in the notebook where it said object detection, so you guys can look at that. But", "tokens": [50364, 300, 2113, 294, 264, 21060, 689, 309, 848, 2657, 17784, 11, 370, 291, 1074, 393, 574, 412, 300, 13, 583, 50572], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3927, "seek": 1671244, "start": 16716.6, "end": 16720.6, "text": " essentially, there is an API for TensorFlow that does object detection for you. And in fact,", "tokens": [50572, 4476, 11, 456, 307, 364, 9362, 337, 37624, 300, 775, 2657, 17784, 337, 291, 13, 400, 294, 1186, 11, 50772], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3928, "seek": 1671244, "start": 16720.6, "end": 16724.28, "text": " it works very well, and even gives you confidence scores. So you can see this is what you'll", "tokens": [50772, 309, 1985, 588, 731, 11, 293, 754, 2709, 291, 6687, 13444, 13, 407, 291, 393, 536, 341, 307, 437, 291, 603, 50956], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3929, "seek": 1671244, "start": 16724.28, "end": 16728.199999999997, "text": " actually end up getting if you end up using this API. Now, unfortunately, we don't have time to go", "tokens": [50956, 767, 917, 493, 1242, 498, 291, 917, 493, 1228, 341, 9362, 13, 823, 11, 7015, 11, 321, 500, 380, 362, 565, 281, 352, 51152], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3930, "seek": 1671244, "start": 16728.199999999997, "end": 16732.28, "text": " through this because this will take a good amount of time to talk about the setup and how to actually", "tokens": [51152, 807, 341, 570, 341, 486, 747, 257, 665, 2372, 295, 565, 281, 751, 466, 264, 8657, 293, 577, 281, 767, 51356], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3931, "seek": 1671244, "start": 16732.28, "end": 16736.52, "text": " use this project properly. But if you go through this documentation, you should be able to figure", "tokens": [51356, 764, 341, 1716, 6108, 13, 583, 498, 291, 352, 807, 341, 14333, 11, 291, 820, 312, 1075, 281, 2573, 51568], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3932, "seek": 1671244, "start": 16736.52, "end": 16739.96, "text": " it out. And now you guys are familiar with TensorFlow, and you understand some of the", "tokens": [51568, 309, 484, 13, 400, 586, 291, 1074, 366, 4963, 365, 37624, 11, 293, 291, 1223, 512, 295, 264, 51740], "temperature": 0.0, "avg_logprob": -0.04879086445539425, "compression_ratio": 1.7940379403794038, "no_speech_prob": 0.08267880976200104}, {"id": 3933, "seek": 1673996, "start": 16739.96, "end": 16744.92, "text": " concepts here. This runs a very different model than what we've discussed before. Unfortunately,", "tokens": [50364, 10392, 510, 13, 639, 6676, 257, 588, 819, 2316, 813, 437, 321, 600, 7152, 949, 13, 8590, 11, 50612], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3934, "seek": 1673996, "start": 16744.92, "end": 16747.96, "text": " again, we don't have time to get into it. But just something I wanted to make clear is that you", "tokens": [50612, 797, 11, 321, 500, 380, 362, 565, 281, 483, 666, 309, 13, 583, 445, 746, 286, 1415, 281, 652, 1850, 307, 300, 291, 50764], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3935, "seek": 1673996, "start": 16747.96, "end": 16752.36, "text": " can do something like this with TensorFlow. And I will leave that resource so that if you'd like", "tokens": [50764, 393, 360, 746, 411, 341, 365, 37624, 13, 400, 286, 486, 1856, 300, 7684, 370, 300, 498, 291, 1116, 411, 50984], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3936, "seek": 1673996, "start": 16752.36, "end": 16756.68, "text": " to check this out, you can use it. There's also a great module in Python called facial recognition,", "tokens": [50984, 281, 1520, 341, 484, 11, 291, 393, 764, 309, 13, 821, 311, 611, 257, 869, 10088, 294, 15329, 1219, 15642, 11150, 11, 51200], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3937, "seek": 1673996, "start": 16756.68, "end": 16761.0, "text": " it's not a part of TensorFlow. But it does use some kind of convolutional neural network to do", "tokens": [51200, 309, 311, 406, 257, 644, 295, 37624, 13, 583, 309, 775, 764, 512, 733, 295, 45216, 304, 18161, 3209, 281, 360, 51416], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3938, "seek": 1673996, "start": 16761.0, "end": 16765.8, "text": " facial detection and recognition, which is pretty cool as well. So I'll put that link in here.", "tokens": [51416, 15642, 17784, 293, 11150, 11, 597, 307, 1238, 1627, 382, 731, 13, 407, 286, 603, 829, 300, 2113, 294, 510, 13, 51656], "temperature": 0.0, "avg_logprob": -0.05329269299404227, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.024420930072665215}, {"id": 3939, "seek": 1676580, "start": 16765.88, "end": 16770.68, "text": " But for that, for now, that's going to be our, what is it convolutional neural network kind of", "tokens": [50368, 583, 337, 300, 11, 337, 586, 11, 300, 311, 516, 281, 312, 527, 11, 437, 307, 309, 45216, 304, 18161, 3209, 733, 295, 50608], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3940, "seek": 1676580, "start": 16770.68, "end": 16776.2, "text": " module. So I hope that says cleared some things up on how deep vision works and how convolutional", "tokens": [50608, 10088, 13, 407, 286, 1454, 300, 1619, 19725, 512, 721, 493, 322, 577, 2452, 5201, 1985, 293, 577, 45216, 304, 50884], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3941, "seek": 1676580, "start": 16776.2, "end": 16780.68, "text": " neural networks work. I know I haven't gone into crazy examples of what I've shown you some different", "tokens": [50884, 18161, 9590, 589, 13, 286, 458, 286, 2378, 380, 2780, 666, 3219, 5110, 295, 437, 286, 600, 4898, 291, 512, 819, 51108], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3942, "seek": 1676580, "start": 16780.68, "end": 16786.36, "text": " techniques that hopefully you'll go look up kind of on your own and really dive into. Because now", "tokens": [51108, 7512, 300, 4696, 291, 603, 352, 574, 493, 733, 295, 322, 428, 1065, 293, 534, 9192, 666, 13, 1436, 586, 51392], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3943, "seek": 1676580, "start": 16786.36, "end": 16790.36, "text": " you have that base kind of domain knowledge where you're going to be able to follow along with the", "tokens": [51392, 291, 362, 300, 3096, 733, 295, 9274, 3601, 689, 291, 434, 516, 281, 312, 1075, 281, 1524, 2051, 365, 264, 51592], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3944, "seek": 1676580, "start": 16790.36, "end": 16794.76, "text": " tutorial and understand exactly what to do. And if you want to create your own model, so long as", "tokens": [51592, 7073, 293, 1223, 2293, 437, 281, 360, 13, 400, 498, 291, 528, 281, 1884, 428, 1065, 2316, 11, 370, 938, 382, 51812], "temperature": 0.0, "avg_logprob": -0.07610426703802974, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006097082048654556}, {"id": 3945, "seek": 1679476, "start": 16794.76, "end": 16799.48, "text": " you can get enough sufficient training data, you can load that training data into your computer,", "tokens": [50364, 291, 393, 483, 1547, 11563, 3097, 1412, 11, 291, 393, 3677, 300, 3097, 1412, 666, 428, 3820, 11, 50600], "temperature": 0.0, "avg_logprob": -0.14527391618297947, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.013635634444653988}, {"id": 3946, "seek": 1679476, "start": 16799.48, "end": 16804.76, "text": " put that in a NumPy array. Then what you can do is create a model like we've just done using even", "tokens": [50600, 829, 300, 294, 257, 22592, 47, 88, 10225, 13, 1396, 437, 291, 393, 360, 307, 1884, 257, 2316, 411, 321, 600, 445, 1096, 1228, 754, 50864], "temperature": 0.0, "avg_logprob": -0.14527391618297947, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.013635634444653988}, {"id": 3947, "seek": 1679476, "start": 16805.64, "end": 16810.44, "text": " something like the mobile nets, what was it v two that we talked about previously, if I could even", "tokens": [50908, 746, 411, 264, 6013, 36170, 11, 437, 390, 309, 371, 732, 300, 321, 2825, 466, 8046, 11, 498, 286, 727, 754, 51148], "temperature": 0.0, "avg_logprob": -0.14527391618297947, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.013635634444653988}, {"id": 3948, "seek": 1679476, "start": 16810.44, "end": 16815.48, "text": " get up and need to close this output. Oh my gosh, this is this is massive output here. Where is", "tokens": [51148, 483, 493, 293, 643, 281, 1998, 341, 5598, 13, 876, 452, 6502, 11, 341, 307, 341, 307, 5994, 5598, 510, 13, 2305, 307, 51400], "temperature": 0.0, "avg_logprob": -0.14527391618297947, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.013635634444653988}, {"id": 3949, "seek": 1679476, "start": 16815.48, "end": 16820.28, "text": " this begin to pre train model? Yeah, mobile net v two, you can use the base of that, and then add", "tokens": [51400, 341, 1841, 281, 659, 3847, 2316, 30, 865, 11, 6013, 2533, 371, 732, 11, 291, 393, 764, 264, 3096, 295, 300, 11, 293, 550, 909, 51640], "temperature": 0.0, "avg_logprob": -0.14527391618297947, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.013635634444653988}, {"id": 3950, "seek": 1682028, "start": 16820.28, "end": 16824.92, "text": " your own classifier on do a similar thing to what I've done with that dense neuron and that global", "tokens": [50364, 428, 1065, 1508, 9902, 322, 360, 257, 2531, 551, 281, 437, 286, 600, 1096, 365, 300, 18011, 34090, 293, 300, 4338, 50596], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3951, "seek": 1682028, "start": 16824.92, "end": 16828.68, "text": " average layer. And hopefully you should get a decent result from that. So this is just showing", "tokens": [50596, 4274, 4583, 13, 400, 4696, 291, 820, 483, 257, 8681, 1874, 490, 300, 13, 407, 341, 307, 445, 4099, 50784], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3952, "seek": 1682028, "start": 16828.68, "end": 16833.719999999998, "text": " you what you can do. Obviously, you can pick a different base layer, depending on what kind of", "tokens": [50784, 291, 437, 291, 393, 360, 13, 7580, 11, 291, 393, 1888, 257, 819, 3096, 4583, 11, 5413, 322, 437, 733, 295, 51036], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3953, "seek": 1682028, "start": 16833.719999999998, "end": 16837.559999999998, "text": " problem you're trying to solve. So anyways, that has been convolutional neural networks. I hope", "tokens": [51036, 1154, 291, 434, 1382, 281, 5039, 13, 407, 13448, 11, 300, 575, 668, 45216, 304, 18161, 9590, 13, 286, 1454, 51228], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3954, "seek": 1682028, "start": 16837.559999999998, "end": 16841.48, "text": " you enjoyed that module. Now we're on to recurrent neural networks, which is actually going to be", "tokens": [51228, 291, 4626, 300, 10088, 13, 823, 321, 434, 322, 281, 18680, 1753, 18161, 9590, 11, 597, 307, 767, 516, 281, 312, 51424], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3955, "seek": 1682028, "start": 16841.48, "end": 16848.84, "text": " pretty interesting. So I'll see you in that module. Hello, everyone, and welcome to the next module", "tokens": [51424, 1238, 1880, 13, 407, 286, 603, 536, 291, 294, 300, 10088, 13, 2425, 11, 1518, 11, 293, 2928, 281, 264, 958, 10088, 51792], "temperature": 0.0, "avg_logprob": -0.061351257821787956, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.014501663856208324}, {"id": 3956, "seek": 1684884, "start": 16848.84, "end": 16852.920000000002, "text": " in this course, which is covering natural language processing with recurrent neural", "tokens": [50364, 294, 341, 1164, 11, 597, 307, 10322, 3303, 2856, 9007, 365, 18680, 1753, 18161, 50568], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3957, "seek": 1684884, "start": 16852.920000000002, "end": 16858.44, "text": " networks. Now what we're going to be doing in this module here is first of all, first off discussing", "tokens": [50568, 9590, 13, 823, 437, 321, 434, 516, 281, 312, 884, 294, 341, 10088, 510, 307, 700, 295, 439, 11, 700, 766, 10850, 50844], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3958, "seek": 1684884, "start": 16858.44, "end": 16862.68, "text": " what natural language processing is, which I guess I'll start with here. Essentially, for those of", "tokens": [50844, 437, 3303, 2856, 9007, 307, 11, 597, 286, 2041, 286, 603, 722, 365, 510, 13, 23596, 11, 337, 729, 295, 51056], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3959, "seek": 1684884, "start": 16862.68, "end": 16868.52, "text": " you that don't know, natural language processing or NLP for short, is the field or discipline in", "tokens": [51056, 291, 300, 500, 380, 458, 11, 3303, 2856, 9007, 420, 426, 45196, 337, 2099, 11, 307, 264, 2519, 420, 13635, 294, 51348], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3960, "seek": 1684884, "start": 16868.52, "end": 16874.2, "text": " computing or machine learning that deals with trying to understand natural or human languages.", "tokens": [51348, 15866, 420, 3479, 2539, 300, 11215, 365, 1382, 281, 1223, 3303, 420, 1952, 8650, 13, 51632], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3961, "seek": 1684884, "start": 16874.2, "end": 16878.28, "text": " Now, the reason we call them natural is because these are not computer languages or", "tokens": [51632, 823, 11, 264, 1778, 321, 818, 552, 3303, 307, 570, 613, 366, 406, 3820, 8650, 420, 51836], "temperature": 0.0, "avg_logprob": -0.08080251881333649, "compression_ratio": 1.9143835616438356, "no_speech_prob": 0.03020821511745453}, {"id": 3962, "seek": 1687828, "start": 16878.28, "end": 16884.28, "text": " programming languages, per se. And actually, computers are quite bad at understanding textual", "tokens": [50364, 9410, 8650, 11, 680, 369, 13, 400, 767, 11, 10807, 366, 1596, 1578, 412, 3701, 2487, 901, 50664], "temperature": 0.0, "avg_logprob": -0.08209166196313235, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.002182537456974387}, {"id": 3963, "seek": 1687828, "start": 16884.28, "end": 16889.399999999998, "text": " information and human languages. And that's why we've come up with this entire discipline focused", "tokens": [50664, 1589, 293, 1952, 8650, 13, 400, 300, 311, 983, 321, 600, 808, 493, 365, 341, 2302, 13635, 5178, 50920], "temperature": 0.0, "avg_logprob": -0.08209166196313235, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.002182537456974387}, {"id": 3964, "seek": 1687828, "start": 16889.399999999998, "end": 16893.879999999997, "text": " on how they can do that. So we're going to do that using something called recurrent neural", "tokens": [50920, 322, 577, 436, 393, 360, 300, 13, 407, 321, 434, 516, 281, 360, 300, 1228, 746, 1219, 18680, 1753, 18161, 51144], "temperature": 0.0, "avg_logprob": -0.08209166196313235, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.002182537456974387}, {"id": 3965, "seek": 1687828, "start": 16893.879999999997, "end": 16898.92, "text": " networks. But some examples of natural language processing would be something like spell check,", "tokens": [51144, 9590, 13, 583, 512, 5110, 295, 3303, 2856, 9007, 576, 312, 746, 411, 9827, 1520, 11, 51396], "temperature": 0.0, "avg_logprob": -0.08209166196313235, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.002182537456974387}, {"id": 3966, "seek": 1687828, "start": 16898.92, "end": 16905.399999999998, "text": " autocomplete voice assistance, translation between languages, there's all different kinds of things,", "tokens": [51396, 45833, 298, 17220, 3177, 9683, 11, 12853, 1296, 8650, 11, 456, 311, 439, 819, 3685, 295, 721, 11, 51720], "temperature": 0.0, "avg_logprob": -0.08209166196313235, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.002182537456974387}, {"id": 3967, "seek": 1690540, "start": 16905.480000000003, "end": 16910.920000000002, "text": " chatbots, but essentially anything that deals with textual data. So you like paragraphs,", "tokens": [50368, 5081, 65, 1971, 11, 457, 4476, 1340, 300, 11215, 365, 2487, 901, 1412, 13, 407, 291, 411, 48910, 11, 50640], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3968, "seek": 1690540, "start": 16910.920000000002, "end": 16916.36, "text": " sentences, even words, that is probably going to be classified under natural language processing", "tokens": [50640, 16579, 11, 754, 2283, 11, 300, 307, 1391, 516, 281, 312, 20627, 833, 3303, 2856, 9007, 50912], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3969, "seek": 1690540, "start": 16916.36, "end": 16921.0, "text": " in terms of doing some kind of machine learning stuff with it. Now, we are going to be talking", "tokens": [50912, 294, 2115, 295, 884, 512, 733, 295, 3479, 2539, 1507, 365, 309, 13, 823, 11, 321, 366, 516, 281, 312, 1417, 51144], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3970, "seek": 1690540, "start": 16921.0, "end": 16925.0, "text": " about a different kind of neural network in this series called recurrent neural networks.", "tokens": [51144, 466, 257, 819, 733, 295, 18161, 3209, 294, 341, 2638, 1219, 18680, 1753, 18161, 9590, 13, 51344], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3971, "seek": 1690540, "start": 16925.0, "end": 16929.72, "text": " Now, these are very good at classifying and understanding textual data. And that's why we'll", "tokens": [51344, 823, 11, 613, 366, 588, 665, 412, 1508, 5489, 293, 3701, 2487, 901, 1412, 13, 400, 300, 311, 983, 321, 603, 51580], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3972, "seek": 1690540, "start": 16929.72, "end": 16934.440000000002, "text": " be using them. But they are fairly complex. And there's a lot of stuff that goes into them.", "tokens": [51580, 312, 1228, 552, 13, 583, 436, 366, 6457, 3997, 13, 400, 456, 311, 257, 688, 295, 1507, 300, 1709, 666, 552, 13, 51816], "temperature": 0.0, "avg_logprob": -0.05296069891877877, "compression_ratio": 1.7731629392971247, "no_speech_prob": 0.0029809316620230675}, {"id": 3973, "seek": 1693444, "start": 16934.44, "end": 16938.84, "text": " Now, in the interest of time, and just not knowing a lot of your math background, I'm not", "tokens": [50364, 823, 11, 294, 264, 1179, 295, 565, 11, 293, 445, 406, 5276, 257, 688, 295, 428, 5221, 3678, 11, 286, 478, 406, 50584], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3974, "seek": 1693444, "start": 16938.84, "end": 16943.879999999997, "text": " going to be getting into the exact details of how this works on a lower level, like I did when I", "tokens": [50584, 516, 281, 312, 1242, 666, 264, 1900, 4365, 295, 577, 341, 1985, 322, 257, 3126, 1496, 11, 411, 286, 630, 562, 286, 50836], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3975, "seek": 1693444, "start": 16943.879999999997, "end": 16949.0, "text": " explained kind of our, I guess, fundamental learning algorithms, which are a bit easier to grasp,", "tokens": [50836, 8825, 733, 295, 527, 11, 286, 2041, 11, 8088, 2539, 14642, 11, 597, 366, 257, 857, 3571, 281, 21743, 11, 51092], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3976, "seek": 1693444, "start": 16949.0, "end": 16953.079999999998, "text": " and even just regular neural networks in general, we're going to be kind of skipping over that and", "tokens": [51092, 293, 754, 445, 3890, 18161, 9590, 294, 2674, 11, 321, 434, 516, 281, 312, 733, 295, 31533, 670, 300, 293, 51296], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3977, "seek": 1693444, "start": 16953.079999999998, "end": 16959.48, "text": " really focusing on why this works the way it does, rather than how and when you should use this.", "tokens": [51296, 534, 8416, 322, 983, 341, 1985, 264, 636, 309, 775, 11, 2831, 813, 577, 293, 562, 291, 820, 764, 341, 13, 51616], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3978, "seek": 1693444, "start": 16960.199999999997, "end": 16963.559999999998, "text": " And then maybe understanding a few of the different kinds of layers that have to do with", "tokens": [51652, 400, 550, 1310, 3701, 257, 1326, 295, 264, 819, 3685, 295, 7914, 300, 362, 281, 360, 365, 51820], "temperature": 0.0, "avg_logprob": -0.05294076134176815, "compression_ratio": 1.7190332326283988, "no_speech_prob": 0.001324988785199821}, {"id": 3979, "seek": 1696356, "start": 16963.640000000003, "end": 16967.48, "text": " recurrent neural networks. But again, we're not going to get into the math. If you'd like to", "tokens": [50368, 18680, 1753, 18161, 9590, 13, 583, 797, 11, 321, 434, 406, 516, 281, 483, 666, 264, 5221, 13, 759, 291, 1116, 411, 281, 50560], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3980, "seek": 1696356, "start": 16967.48, "end": 16971.16, "text": " learn about that, there will be some sources at the bottom of the guide. And you can also just", "tokens": [50560, 1466, 466, 300, 11, 456, 486, 312, 512, 7139, 412, 264, 2767, 295, 264, 5934, 13, 400, 291, 393, 611, 445, 50744], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3981, "seek": 1696356, "start": 16971.16, "end": 16975.32, "text": " look up recurrent neural networks, and you'll find lots of resources that explain all of the", "tokens": [50744, 574, 493, 18680, 1753, 18161, 9590, 11, 293, 291, 603, 915, 3195, 295, 3593, 300, 2903, 439, 295, 264, 50952], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3982, "seek": 1696356, "start": 16975.32, "end": 16980.84, "text": " fancy math that goes on behind them. Now, the exact applications and kind of things will be working", "tokens": [50952, 10247, 5221, 300, 1709, 322, 2261, 552, 13, 823, 11, 264, 1900, 5821, 293, 733, 295, 721, 486, 312, 1364, 51228], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3983, "seek": 1696356, "start": 16980.84, "end": 16986.120000000003, "text": " towards here is sentiment analysis. That's the first kind of task or thing we're going to do.", "tokens": [51228, 3030, 510, 307, 16149, 5215, 13, 663, 311, 264, 700, 733, 295, 5633, 420, 551, 321, 434, 516, 281, 360, 13, 51492], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3984, "seek": 1696356, "start": 16986.120000000003, "end": 16990.2, "text": " We're actually going to use movie reviews and try to determine whether these movie reviews are", "tokens": [51492, 492, 434, 767, 516, 281, 764, 3169, 10229, 293, 853, 281, 6997, 1968, 613, 3169, 10229, 366, 51696], "temperature": 0.0, "avg_logprob": -0.06128374735514323, "compression_ratio": 1.8237179487179487, "no_speech_prob": 0.002550789387896657}, {"id": 3985, "seek": 1699020, "start": 16990.280000000002, "end": 16995.72, "text": " positive or negative by performing sentiment analysis on them. Now, if you're unfamiliar", "tokens": [50368, 3353, 420, 3671, 538, 10205, 16149, 5215, 322, 552, 13, 823, 11, 498, 291, 434, 29415, 50640], "temperature": 0.0, "avg_logprob": -0.06061139813175908, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.020961744710803032}, {"id": 3986, "seek": 1699020, "start": 16995.72, "end": 16999.32, "text": " with sentiment analysis, we'll talk about it more later, but essentially means trying to", "tokens": [50640, 365, 16149, 5215, 11, 321, 603, 751, 466, 309, 544, 1780, 11, 457, 4476, 1355, 1382, 281, 50820], "temperature": 0.0, "avg_logprob": -0.06061139813175908, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.020961744710803032}, {"id": 3987, "seek": 1699020, "start": 16999.32, "end": 17004.12, "text": " determine how positive or negative a sentence or piece of text is, which you can see why that would", "tokens": [50820, 6997, 577, 3353, 420, 3671, 257, 8174, 420, 2522, 295, 2487, 307, 11, 597, 291, 393, 536, 983, 300, 576, 51060], "temperature": 0.0, "avg_logprob": -0.06061139813175908, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.020961744710803032}, {"id": 3988, "seek": 1699020, "start": 17004.12, "end": 17010.12, "text": " be useful for movie reviews. Next, we're going to do character slash text generation. So essentially,", "tokens": [51060, 312, 4420, 337, 3169, 10229, 13, 3087, 11, 321, 434, 516, 281, 360, 2517, 17330, 2487, 5125, 13, 407, 4476, 11, 51360], "temperature": 0.0, "avg_logprob": -0.06061139813175908, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.020961744710803032}, {"id": 3989, "seek": 1699020, "start": 17010.12, "end": 17014.920000000002, "text": " we're going to use a natural language processing model, I guess, if you want to call it that,", "tokens": [51360, 321, 434, 516, 281, 764, 257, 3303, 2856, 9007, 2316, 11, 286, 2041, 11, 498, 291, 528, 281, 818, 309, 300, 11, 51600], "temperature": 0.0, "avg_logprob": -0.06061139813175908, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.020961744710803032}, {"id": 3990, "seek": 1701492, "start": 17015.559999999998, "end": 17020.92, "text": " to generate the next character in a sequence of text for us. And we're going to use that model a", "tokens": [50396, 281, 8460, 264, 958, 2517, 294, 257, 8310, 295, 2487, 337, 505, 13, 400, 321, 434, 516, 281, 764, 300, 2316, 257, 50664], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3991, "seek": 1701492, "start": 17020.92, "end": 17026.12, "text": " bunch of times to actually generate an entire play. Now, I know this seems a little bit ridiculous", "tokens": [50664, 3840, 295, 1413, 281, 767, 8460, 364, 2302, 862, 13, 823, 11, 286, 458, 341, 2544, 257, 707, 857, 11083, 50924], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3992, "seek": 1701492, "start": 17026.12, "end": 17030.519999999997, "text": " compared to some of the trivial examples we've done before. This will be quite a bit more code", "tokens": [50924, 5347, 281, 512, 295, 264, 26703, 5110, 321, 600, 1096, 949, 13, 639, 486, 312, 1596, 257, 857, 544, 3089, 51144], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3993, "seek": 1701492, "start": 17030.519999999997, "end": 17033.8, "text": " than anything we've really looked at yet. But this is very cool, because we're going to actually", "tokens": [51144, 813, 1340, 321, 600, 534, 2956, 412, 1939, 13, 583, 341, 307, 588, 1627, 11, 570, 321, 434, 516, 281, 767, 51308], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3994, "seek": 1701492, "start": 17033.8, "end": 17038.679999999997, "text": " going to train a model to learn how to write a play. That's literally what it's going to do.", "tokens": [51308, 516, 281, 3847, 257, 2316, 281, 1466, 577, 281, 2464, 257, 862, 13, 663, 311, 3736, 437, 309, 311, 516, 281, 360, 13, 51552], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3995, "seek": 1701492, "start": 17038.679999999997, "end": 17042.76, "text": " It's going to read through a play, I believe it's Romeo and Juliet. And then we're going to give it", "tokens": [51552, 467, 311, 516, 281, 1401, 807, 257, 862, 11, 286, 1697, 309, 311, 33563, 293, 33532, 13, 400, 550, 321, 434, 516, 281, 976, 309, 51756], "temperature": 0.0, "avg_logprob": -0.057821740775272766, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.2068445235490799}, {"id": 3996, "seek": 1704276, "start": 17042.84, "end": 17047.48, "text": " a little prompt when we're actually using the model and say, okay, this is the first part of", "tokens": [50368, 257, 707, 12391, 562, 321, 434, 767, 1228, 264, 2316, 293, 584, 11, 1392, 11, 341, 307, 264, 700, 644, 295, 50600], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 3997, "seek": 1704276, "start": 17047.48, "end": 17051.64, "text": " the play, write the rest of it. And then it will actually go and write the rest of the characters", "tokens": [50600, 264, 862, 11, 2464, 264, 1472, 295, 309, 13, 400, 550, 309, 486, 767, 352, 293, 2464, 264, 1472, 295, 264, 4342, 50808], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 3998, "seek": 1704276, "start": 17051.64, "end": 17055.559999999998, "text": " in the play. And we'll see that we can get something that's pretty good using the techniques that", "tokens": [50808, 294, 264, 862, 13, 400, 321, 603, 536, 300, 321, 393, 483, 746, 300, 311, 1238, 665, 1228, 264, 7512, 300, 51004], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 3999, "seek": 1704276, "start": 17055.559999999998, "end": 17060.76, "text": " we'll talk about. So the first thing that I want to do is talk about data. So I'm going to hop onto", "tokens": [51004, 321, 603, 751, 466, 13, 407, 264, 700, 551, 300, 286, 528, 281, 360, 307, 751, 466, 1412, 13, 407, 286, 478, 516, 281, 3818, 3911, 51264], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 4000, "seek": 1704276, "start": 17060.76, "end": 17066.039999999997, "text": " my drawing tablet here. And we're going to compare the difference between textual data and numeric", "tokens": [51264, 452, 6316, 14136, 510, 13, 400, 321, 434, 516, 281, 6794, 264, 2649, 1296, 2487, 901, 1412, 293, 7866, 299, 51528], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 4001, "seek": 1704276, "start": 17066.039999999997, "end": 17070.359999999997, "text": " data, like we've seen before, and why we're going to have to employ some pretty complex and", "tokens": [51528, 1412, 11, 411, 321, 600, 1612, 949, 11, 293, 983, 321, 434, 516, 281, 362, 281, 3188, 512, 1238, 3997, 293, 51744], "temperature": 0.0, "avg_logprob": -0.06864477354904701, "compression_ratio": 1.9172185430463575, "no_speech_prob": 0.012820146046578884}, {"id": 4002, "seek": 1707036, "start": 17070.36, "end": 17074.760000000002, "text": " different steps to turn something like this, you know, a block of text into some meaningful", "tokens": [50364, 819, 4439, 281, 1261, 746, 411, 341, 11, 291, 458, 11, 257, 3461, 295, 2487, 666, 512, 10995, 50584], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4003, "seek": 1707036, "start": 17074.760000000002, "end": 17079.56, "text": " information that our neural networks actually going to be actually going to be able to understand", "tokens": [50584, 1589, 300, 527, 18161, 9590, 767, 516, 281, 312, 767, 516, 281, 312, 1075, 281, 1223, 50824], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4004, "seek": 1707036, "start": 17079.56, "end": 17083.72, "text": " and process. So let's go ahead and get over to that. Okay, so now we're going to get into the", "tokens": [50824, 293, 1399, 13, 407, 718, 311, 352, 2286, 293, 483, 670, 281, 300, 13, 1033, 11, 370, 586, 321, 434, 516, 281, 483, 666, 264, 51032], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4005, "seek": 1707036, "start": 17083.72, "end": 17089.96, "text": " problem of how we can turn some textual data into numeric data that we can feed to our neural", "tokens": [51032, 1154, 295, 577, 321, 393, 1261, 512, 2487, 901, 1412, 666, 7866, 299, 1412, 300, 321, 393, 3154, 281, 527, 18161, 51344], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4006, "seek": 1707036, "start": 17089.96, "end": 17094.68, "text": " network. Now, this is a pretty interesting problem. And we'll kind of go through as we start going", "tokens": [51344, 3209, 13, 823, 11, 341, 307, 257, 1238, 1880, 1154, 13, 400, 321, 603, 733, 295, 352, 807, 382, 321, 722, 516, 51580], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4007, "seek": 1707036, "start": 17094.68, "end": 17098.36, "text": " through it, you should see why this is interesting and why there's a lot of difficulties with the", "tokens": [51580, 807, 309, 11, 291, 820, 536, 983, 341, 307, 1880, 293, 983, 456, 311, 257, 688, 295, 14399, 365, 264, 51764], "temperature": 0.0, "avg_logprob": -0.055579953333910775, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.010327421128749847}, {"id": 4008, "seek": 1709836, "start": 17098.36, "end": 17102.2, "text": " different methods that we pick. But the first method that I want to talk about is something", "tokens": [50364, 819, 7150, 300, 321, 1888, 13, 583, 264, 700, 3170, 300, 286, 528, 281, 751, 466, 307, 746, 50556], "temperature": 0.0, "avg_logprob": -0.06719207341692089, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004609367344528437}, {"id": 4009, "seek": 1709836, "start": 17102.2, "end": 17107.8, "text": " called bag of words, in terms of how we can kind of encode and pre process text into integers.", "tokens": [50556, 1219, 3411, 295, 2283, 11, 294, 2115, 295, 577, 321, 393, 733, 295, 2058, 1429, 293, 659, 1399, 2487, 666, 41674, 13, 50836], "temperature": 0.0, "avg_logprob": -0.06719207341692089, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004609367344528437}, {"id": 4010, "seek": 1709836, "start": 17107.8, "end": 17112.920000000002, "text": " Now, obviously, I'm not the first person to come up with this bag of words is a very famous,", "tokens": [50836, 823, 11, 2745, 11, 286, 478, 406, 264, 700, 954, 281, 808, 493, 365, 341, 3411, 295, 2283, 307, 257, 588, 4618, 11, 51092], "temperature": 0.0, "avg_logprob": -0.06719207341692089, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004609367344528437}, {"id": 4011, "seek": 1709836, "start": 17112.920000000002, "end": 17118.2, "text": " almost I want to say algorithm or method of converting textual data to numeric data, although", "tokens": [51092, 1920, 286, 528, 281, 584, 9284, 420, 3170, 295, 29942, 2487, 901, 1412, 281, 7866, 299, 1412, 11, 4878, 51356], "temperature": 0.0, "avg_logprob": -0.06719207341692089, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004609367344528437}, {"id": 4012, "seek": 1709836, "start": 17118.2, "end": 17123.4, "text": " it is pretty flawed and only really works for simple tasks. And we're going to understand why", "tokens": [51356, 309, 307, 1238, 38823, 293, 787, 534, 1985, 337, 2199, 9608, 13, 400, 321, 434, 516, 281, 1223, 983, 51616], "temperature": 0.0, "avg_logprob": -0.06719207341692089, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004609367344528437}, {"id": 4013, "seek": 1712340, "start": 17123.4, "end": 17128.280000000002, "text": " in a second. So we're going to call this bag of words. Essentially, what bag of words says is", "tokens": [50364, 294, 257, 1150, 13, 407, 321, 434, 516, 281, 818, 341, 3411, 295, 2283, 13, 23596, 11, 437, 3411, 295, 2283, 1619, 307, 50608], "temperature": 0.0, "avg_logprob": -0.07210175584002239, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.10966940224170685}, {"id": 4014, "seek": 1712340, "start": 17128.280000000002, "end": 17131.88, "text": " what we're going to do is we're going to look at our entire training data set, right, because we're", "tokens": [50608, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 574, 412, 527, 2302, 3097, 1412, 992, 11, 558, 11, 570, 321, 434, 50788], "temperature": 0.0, "avg_logprob": -0.07210175584002239, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.10966940224170685}, {"id": 4015, "seek": 1712340, "start": 17131.88, "end": 17136.120000000003, "text": " going to be turning our training data set into a form the network can understand. And we're going", "tokens": [50788, 516, 281, 312, 6246, 527, 3097, 1412, 992, 666, 257, 1254, 264, 3209, 393, 1223, 13, 400, 321, 434, 516, 51000], "temperature": 0.0, "avg_logprob": -0.07210175584002239, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.10966940224170685}, {"id": 4016, "seek": 1712340, "start": 17136.120000000003, "end": 17142.280000000002, "text": " to create a dictionary lookup of the vocabulary. Now, what I mean by that is we're going to say", "tokens": [51000, 281, 1884, 257, 25890, 574, 1010, 295, 264, 19864, 13, 823, 11, 437, 286, 914, 538, 300, 307, 321, 434, 516, 281, 584, 51308], "temperature": 0.0, "avg_logprob": -0.07210175584002239, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.10966940224170685}, {"id": 4017, "seek": 1712340, "start": 17142.280000000002, "end": 17149.08, "text": " that every single unique word in our data set is the vocabulary, right? That's the amount of words", "tokens": [51308, 300, 633, 2167, 3845, 1349, 294, 527, 1412, 992, 307, 264, 19864, 11, 558, 30, 663, 311, 264, 2372, 295, 2283, 51648], "temperature": 0.0, "avg_logprob": -0.07210175584002239, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.10966940224170685}, {"id": 4018, "seek": 1714908, "start": 17149.08, "end": 17153.320000000003, "text": " that the model is expected to understand, because we're, you know, going to show all those words", "tokens": [50364, 300, 264, 2316, 307, 5176, 281, 1223, 11, 570, 321, 434, 11, 291, 458, 11, 516, 281, 855, 439, 729, 2283, 50576], "temperature": 0.0, "avg_logprob": -0.06949470520019531, "compression_ratio": 1.924901185770751, "no_speech_prob": 0.025174761191010475}, {"id": 4019, "seek": 1714908, "start": 17153.320000000003, "end": 17157.4, "text": " to the model. And we're going to say that every single one of these words. So every single one", "tokens": [50576, 281, 264, 2316, 13, 400, 321, 434, 516, 281, 584, 300, 633, 2167, 472, 295, 613, 2283, 13, 407, 633, 2167, 472, 50780], "temperature": 0.0, "avg_logprob": -0.06949470520019531, "compression_ratio": 1.924901185770751, "no_speech_prob": 0.025174761191010475}, {"id": 4020, "seek": 1714908, "start": 17157.4, "end": 17162.440000000002, "text": " of these words in the vocabulary is going to be placed in a dictionary. And beside that, we're", "tokens": [50780, 295, 613, 2283, 294, 264, 19864, 307, 516, 281, 312, 7074, 294, 257, 25890, 13, 400, 15726, 300, 11, 321, 434, 51032], "temperature": 0.0, "avg_logprob": -0.06949470520019531, "compression_ratio": 1.924901185770751, "no_speech_prob": 0.025174761191010475}, {"id": 4021, "seek": 1714908, "start": 17162.440000000002, "end": 17167.320000000003, "text": " going to have some integer that represents it. So for example, maybe the vocabulary of our data", "tokens": [51032, 516, 281, 362, 512, 24922, 300, 8855, 309, 13, 407, 337, 1365, 11, 1310, 264, 19864, 295, 527, 1412, 51276], "temperature": 0.0, "avg_logprob": -0.06949470520019531, "compression_ratio": 1.924901185770751, "no_speech_prob": 0.025174761191010475}, {"id": 4022, "seek": 1714908, "start": 17167.320000000003, "end": 17175.960000000003, "text": " set is the words, you know, I, a, maybe Tim, maybe day, me, right, we're gonna have a bunch of arbitrary", "tokens": [51276, 992, 307, 264, 2283, 11, 291, 458, 11, 286, 11, 257, 11, 1310, 7172, 11, 1310, 786, 11, 385, 11, 558, 11, 321, 434, 799, 362, 257, 3840, 295, 23211, 51708], "temperature": 0.0, "avg_logprob": -0.06949470520019531, "compression_ratio": 1.924901185770751, "no_speech_prob": 0.025174761191010475}, {"id": 4023, "seek": 1717596, "start": 17175.96, "end": 17180.52, "text": " words, I'll just put dot dot dot to show that this kind of goes to the length of the vocabulary.", "tokens": [50364, 2283, 11, 286, 603, 445, 829, 5893, 5893, 5893, 281, 855, 300, 341, 733, 295, 1709, 281, 264, 4641, 295, 264, 19864, 13, 50592], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4024, "seek": 1717596, "start": 17180.52, "end": 17183.8, "text": " And every single one of these words will be placed in a dictionary, which we're going to just", "tokens": [50592, 400, 633, 2167, 472, 295, 613, 2283, 486, 312, 7074, 294, 257, 25890, 11, 597, 321, 434, 516, 281, 445, 50756], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4025, "seek": 1717596, "start": 17183.8, "end": 17189.32, "text": " going to call kind of our lookup table or word index table. And we're going to have a number", "tokens": [50756, 516, 281, 818, 733, 295, 527, 574, 1010, 3199, 420, 1349, 8186, 3199, 13, 400, 321, 434, 516, 281, 362, 257, 1230, 51032], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4026, "seek": 1717596, "start": 17189.32, "end": 17195.16, "text": " that represents every single one of them. So you can imagine that in very large data sets, we're", "tokens": [51032, 300, 8855, 633, 2167, 472, 295, 552, 13, 407, 291, 393, 3811, 300, 294, 588, 2416, 1412, 6352, 11, 321, 434, 51324], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4027, "seek": 1717596, "start": 17195.16, "end": 17200.28, "text": " going to have, you know, tens of thousands of hundreds of thousands, sometimes even maybe millions", "tokens": [51324, 516, 281, 362, 11, 291, 458, 11, 10688, 295, 5383, 295, 6779, 295, 5383, 11, 2171, 754, 1310, 6803, 51580], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4028, "seek": 1717596, "start": 17200.28, "end": 17204.68, "text": " of different words, and they're all going to be encoded by different integers. Now, the reason", "tokens": [51580, 295, 819, 2283, 11, 293, 436, 434, 439, 516, 281, 312, 2058, 12340, 538, 819, 41674, 13, 823, 11, 264, 1778, 51800], "temperature": 0.0, "avg_logprob": -0.07470464025224959, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.006692369002848864}, {"id": 4029, "seek": 1720468, "start": 17204.760000000002, "end": 17209.96, "text": " we call this bag of words is because what we're actually going to do when we look at a sentence", "tokens": [50368, 321, 818, 341, 3411, 295, 2283, 307, 570, 437, 321, 434, 767, 516, 281, 360, 562, 321, 574, 412, 257, 8174, 50628], "temperature": 0.0, "avg_logprob": -0.06374824165117622, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.02228349633514881}, {"id": 4030, "seek": 1720468, "start": 17209.96, "end": 17215.48, "text": " is we're only going to keep track of the words that are present and the frequency of those words.", "tokens": [50628, 307, 321, 434, 787, 516, 281, 1066, 2837, 295, 264, 2283, 300, 366, 1974, 293, 264, 7893, 295, 729, 2283, 13, 50904], "temperature": 0.0, "avg_logprob": -0.06374824165117622, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.02228349633514881}, {"id": 4031, "seek": 1720468, "start": 17215.48, "end": 17221.72, "text": " And in fact, what we'll do well is we'll create what we call a bag. And whenever we see a word", "tokens": [50904, 400, 294, 1186, 11, 437, 321, 603, 360, 731, 307, 321, 603, 1884, 437, 321, 818, 257, 3411, 13, 400, 5699, 321, 536, 257, 1349, 51216], "temperature": 0.0, "avg_logprob": -0.06374824165117622, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.02228349633514881}, {"id": 4032, "seek": 1720468, "start": 17221.72, "end": 17228.04, "text": " appears, we'll simply add its number into the bag. So if I have a sentence like, you know, I", "tokens": [51216, 7038, 11, 321, 603, 2935, 909, 1080, 1230, 666, 264, 3411, 13, 407, 498, 286, 362, 257, 8174, 411, 11, 291, 458, 11, 286, 51532], "temperature": 0.0, "avg_logprob": -0.06374824165117622, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.02228349633514881}, {"id": 4033, "seek": 1722804, "start": 17229.0, "end": 17236.36, "text": " am Tim day, day, I'm just going to do like a random sentence like that. Then what we're going", "tokens": [50412, 669, 7172, 786, 11, 786, 11, 286, 478, 445, 516, 281, 360, 411, 257, 4974, 8174, 411, 300, 13, 1396, 437, 321, 434, 516, 50780], "temperature": 0.0, "avg_logprob": -0.10949128702146198, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.04467615857720375}, {"id": 4034, "seek": 1722804, "start": 17236.36, "end": 17241.0, "text": " to do is every time we see a word, we're going to take its number and throw it into the bag. So", "tokens": [50780, 281, 360, 307, 633, 565, 321, 536, 257, 1349, 11, 321, 434, 516, 281, 747, 1080, 1230, 293, 3507, 309, 666, 264, 3411, 13, 407, 51012], "temperature": 0.0, "avg_logprob": -0.10949128702146198, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.04467615857720375}, {"id": 4035, "seek": 1722804, "start": 17241.0, "end": 17248.36, "text": " we're going to say, all right, I, that's zero, and that's one, Tim, that's two, day, that's three,", "tokens": [51012, 321, 434, 516, 281, 584, 11, 439, 558, 11, 286, 11, 300, 311, 4018, 11, 293, 300, 311, 472, 11, 7172, 11, 300, 311, 732, 11, 786, 11, 300, 311, 1045, 11, 51380], "temperature": 0.0, "avg_logprob": -0.10949128702146198, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.04467615857720375}, {"id": 4036, "seek": 1722804, "start": 17249.16, "end": 17253.96, "text": " that's three again. And notice that what's happening here is we're losing the ordering", "tokens": [51420, 300, 311, 1045, 797, 13, 400, 3449, 300, 437, 311, 2737, 510, 307, 321, 434, 7027, 264, 21739, 51660], "temperature": 0.0, "avg_logprob": -0.10949128702146198, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.04467615857720375}, {"id": 4037, "seek": 1725396, "start": 17254.04, "end": 17258.92, "text": " of these words, but we're just keeping track of the frequency. Now, there's lots of different", "tokens": [50368, 295, 613, 2283, 11, 457, 321, 434, 445, 5145, 2837, 295, 264, 7893, 13, 823, 11, 456, 311, 3195, 295, 819, 50612], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4038, "seek": 1725396, "start": 17258.92, "end": 17264.2, "text": " ways to kind of format how we want to do bag of words. But this is the basic idea, I'm not going", "tokens": [50612, 2098, 281, 733, 295, 7877, 577, 321, 528, 281, 360, 3411, 295, 2283, 13, 583, 341, 307, 264, 3875, 1558, 11, 286, 478, 406, 516, 50876], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4039, "seek": 1725396, "start": 17264.2, "end": 17267.879999999997, "text": " to go too far in because we're not actually really going to use this technique. But essentially,", "tokens": [50876, 281, 352, 886, 1400, 294, 570, 321, 434, 406, 767, 534, 516, 281, 764, 341, 6532, 13, 583, 4476, 11, 51060], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4040, "seek": 1725396, "start": 17267.879999999997, "end": 17272.84, "text": " you lose the ordering in which words appear, but you just keep track of the frequency and what", "tokens": [51060, 291, 3624, 264, 21739, 294, 597, 2283, 4204, 11, 457, 291, 445, 1066, 2837, 295, 264, 7893, 293, 437, 51308], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4041, "seek": 1725396, "start": 17272.84, "end": 17277.64, "text": " words appear. So this could be very useful when you're looking, you know, you're doing very simple", "tokens": [51308, 2283, 4204, 13, 407, 341, 727, 312, 588, 4420, 562, 291, 434, 1237, 11, 291, 458, 11, 291, 434, 884, 588, 2199, 51548], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4042, "seek": 1725396, "start": 17277.64, "end": 17283.399999999998, "text": " tasks where the presence of a certain word will really influence the kind of type of sentence", "tokens": [51548, 9608, 689, 264, 6814, 295, 257, 1629, 1349, 486, 534, 6503, 264, 733, 295, 2010, 295, 8174, 51836], "temperature": 0.0, "avg_logprob": -0.057306583376898285, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.11917192488908768}, {"id": 4043, "seek": 1728340, "start": 17283.480000000003, "end": 17287.24, "text": " that it is, or the meaning that you're going to get from it. But when we're looking at more", "tokens": [50368, 300, 309, 307, 11, 420, 264, 3620, 300, 291, 434, 516, 281, 483, 490, 309, 13, 583, 562, 321, 434, 1237, 412, 544, 50556], "temperature": 0.0, "avg_logprob": -0.0658029075560531, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.00317252054810524}, {"id": 4044, "seek": 1728340, "start": 17287.24, "end": 17292.04, "text": " complex input, where, you know, different words have different meanings, depending on where they", "tokens": [50556, 3997, 4846, 11, 689, 11, 291, 458, 11, 819, 2283, 362, 819, 28138, 11, 5413, 322, 689, 436, 50796], "temperature": 0.0, "avg_logprob": -0.0658029075560531, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.00317252054810524}, {"id": 4045, "seek": 1728340, "start": 17292.04, "end": 17298.36, "text": " are in a sentence, this is a pretty flawed way to encode this data. Now, I won't go much further", "tokens": [50796, 366, 294, 257, 8174, 11, 341, 307, 257, 1238, 38823, 636, 281, 2058, 1429, 341, 1412, 13, 823, 11, 286, 1582, 380, 352, 709, 3052, 51112], "temperature": 0.0, "avg_logprob": -0.0658029075560531, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.00317252054810524}, {"id": 4046, "seek": 1728340, "start": 17298.36, "end": 17303.480000000003, "text": " into this. This is not the exact way the bag of words works. But I just wanted to show you", "tokens": [51112, 666, 341, 13, 639, 307, 406, 264, 1900, 636, 264, 3411, 295, 2283, 1985, 13, 583, 286, 445, 1415, 281, 855, 291, 51368], "temperature": 0.0, "avg_logprob": -0.0658029075560531, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.00317252054810524}, {"id": 4047, "seek": 1728340, "start": 17303.480000000003, "end": 17308.84, "text": " kind of an idea here, which is we just encode every single unique word by an integer. And then we", "tokens": [51368, 733, 295, 364, 1558, 510, 11, 597, 307, 321, 445, 2058, 1429, 633, 2167, 3845, 1349, 538, 364, 24922, 13, 400, 550, 321, 51636], "temperature": 0.0, "avg_logprob": -0.0658029075560531, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.00317252054810524}, {"id": 4048, "seek": 1730884, "start": 17308.84, "end": 17313.8, "text": " don't even really care about where these words are. We just throw them into a bag and we say,", "tokens": [50364, 500, 380, 754, 534, 1127, 466, 689, 613, 2283, 366, 13, 492, 445, 3507, 552, 666, 257, 3411, 293, 321, 584, 11, 50612], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4049, "seek": 1730884, "start": 17313.8, "end": 17318.04, "text": " All right, you know, this is our bag right here that I'm doing the arrow to, we'll just throw in", "tokens": [50612, 1057, 558, 11, 291, 458, 11, 341, 307, 527, 3411, 558, 510, 300, 286, 478, 884, 264, 11610, 281, 11, 321, 603, 445, 3507, 294, 50824], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4050, "seek": 1730884, "start": 17318.04, "end": 17322.68, "text": " three as many times as you know, the word day appears, we'll throw in one as many times as the", "tokens": [50824, 1045, 382, 867, 1413, 382, 291, 458, 11, 264, 1349, 786, 7038, 11, 321, 603, 3507, 294, 472, 382, 867, 1413, 382, 264, 51056], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4051, "seek": 1730884, "start": 17322.68, "end": 17329.64, "text": " word, I guess, am appears, and so on and so forth. And then what will happen is we'll feed this bag", "tokens": [51056, 1349, 11, 286, 2041, 11, 669, 7038, 11, 293, 370, 322, 293, 370, 5220, 13, 400, 550, 437, 486, 1051, 307, 321, 603, 3154, 341, 3411, 51404], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4052, "seek": 1730884, "start": 17329.64, "end": 17333.72, "text": " to our neural network in some form, depending on the network that we're using. And it will just", "tokens": [51404, 281, 527, 18161, 3209, 294, 512, 1254, 11, 5413, 322, 264, 3209, 300, 321, 434, 1228, 13, 400, 309, 486, 445, 51608], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4053, "seek": 1730884, "start": 17333.72, "end": 17337.56, "text": " look at and say, Okay, so I have all these different numbers, that means these words are present", "tokens": [51608, 574, 412, 293, 584, 11, 1033, 11, 370, 286, 362, 439, 613, 819, 3547, 11, 300, 1355, 613, 2283, 366, 1974, 51800], "temperature": 0.0, "avg_logprob": -0.07159448924817537, "compression_ratio": 1.8827361563517915, "no_speech_prob": 0.005730034783482552}, {"id": 4054, "seek": 1733756, "start": 17337.640000000003, "end": 17341.640000000003, "text": " and try to do something with it. Now, I'm going to show you a few examples of where this kind of", "tokens": [50368, 293, 853, 281, 360, 746, 365, 309, 13, 823, 11, 286, 478, 516, 281, 855, 291, 257, 1326, 5110, 295, 689, 341, 733, 295, 50568], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4055, "seek": 1733756, "start": 17341.640000000003, "end": 17345.32, "text": " breaks down, but just understand that this is how this works. This is the first technique called", "tokens": [50568, 9857, 760, 11, 457, 445, 1223, 300, 341, 307, 577, 341, 1985, 13, 639, 307, 264, 700, 6532, 1219, 50752], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4056, "seek": 1733756, "start": 17345.32, "end": 17350.36, "text": " bag of words, which again, we will not be using. So what happens when we have a sentence where", "tokens": [50752, 3411, 295, 2283, 11, 597, 797, 11, 321, 486, 406, 312, 1228, 13, 407, 437, 2314, 562, 321, 362, 257, 8174, 689, 51004], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4057, "seek": 1733756, "start": 17350.36, "end": 17356.2, "text": " the same word conveys a very different meaning, right? And I'm actually, I think I have an example", "tokens": [51004, 264, 912, 1349, 18053, 749, 257, 588, 819, 3620, 11, 558, 30, 400, 286, 478, 767, 11, 286, 519, 286, 362, 364, 1365, 51296], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4058, "seek": 1733756, "start": 17356.2, "end": 17362.600000000002, "text": " on the slides here that I'll go into. Yes, select this. Okay, so for our bag of words technique,", "tokens": [51296, 322, 264, 9788, 510, 300, 286, 603, 352, 666, 13, 1079, 11, 3048, 341, 13, 1033, 11, 370, 337, 527, 3411, 295, 2283, 6532, 11, 51616], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4059, "seek": 1733756, "start": 17362.600000000002, "end": 17367.16, "text": " which we can kind of see here, maybe we'll go through it. Let's consider the two sentences", "tokens": [51616, 597, 321, 393, 733, 295, 536, 510, 11, 1310, 321, 603, 352, 807, 309, 13, 961, 311, 1949, 264, 732, 16579, 51844], "temperature": 0.0, "avg_logprob": -0.061535063244047616, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.005554216913878918}, {"id": 4060, "seek": 1736756, "start": 17367.56, "end": 17371.800000000003, "text": " where are they here? I thought the movie was going to be bad, but it was actually amazing.", "tokens": [50364, 689, 366, 436, 510, 30, 286, 1194, 264, 3169, 390, 516, 281, 312, 1578, 11, 457, 309, 390, 767, 2243, 13, 50576], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4061, "seek": 1736756, "start": 17371.800000000003, "end": 17376.2, "text": " And I thought the movie was going to be amazing, but it was actually bad, right? So consider these", "tokens": [50576, 400, 286, 1194, 264, 3169, 390, 516, 281, 312, 2243, 11, 457, 309, 390, 767, 1578, 11, 558, 30, 407, 1949, 613, 50796], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4062, "seek": 1736756, "start": 17376.2, "end": 17379.56, "text": " two sentences. Now, I know you guys already know what I'm going to get at. But essentially, these", "tokens": [50796, 732, 16579, 13, 823, 11, 286, 458, 291, 1074, 1217, 458, 437, 286, 478, 516, 281, 483, 412, 13, 583, 4476, 11, 613, 50964], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4063, "seek": 1736756, "start": 17379.56, "end": 17385.32, "text": " sentences use the exact same words. In fact, they use the exact same number of words, the exact same", "tokens": [50964, 16579, 764, 264, 1900, 912, 2283, 13, 682, 1186, 11, 436, 764, 264, 1900, 912, 1230, 295, 2283, 11, 264, 1900, 912, 51252], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4064, "seek": 1736756, "start": 17385.32, "end": 17391.800000000003, "text": " words in total. And well, they have a very different meaning. With our bag of words technique,", "tokens": [51252, 2283, 294, 3217, 13, 400, 731, 11, 436, 362, 257, 588, 819, 3620, 13, 2022, 527, 3411, 295, 2283, 6532, 11, 51576], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4065, "seek": 1736756, "start": 17391.800000000003, "end": 17397.08, "text": " we're actually going to encode these two sentences using the exact same representation. Because", "tokens": [51576, 321, 434, 767, 516, 281, 2058, 1429, 613, 732, 16579, 1228, 264, 1900, 912, 10290, 13, 1436, 51840], "temperature": 0.0, "avg_logprob": -0.06658792495727539, "compression_ratio": 2.0174216027874565, "no_speech_prob": 0.0008829917060211301}, {"id": 4066, "seek": 1739708, "start": 17397.160000000003, "end": 17402.440000000002, "text": " remember, all we do is we care about the frequency and what words appear, but we don't care about", "tokens": [50368, 1604, 11, 439, 321, 360, 307, 321, 1127, 466, 264, 7893, 293, 437, 2283, 4204, 11, 457, 321, 500, 380, 1127, 466, 50632], "temperature": 0.0, "avg_logprob": -0.06919066373966942, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.0012842601863667369}, {"id": 4067, "seek": 1739708, "start": 17402.440000000002, "end": 17408.68, "text": " where they appear. So we end up losing that meaning from the sentence, because the sentence I thought", "tokens": [50632, 689, 436, 4204, 13, 407, 321, 917, 493, 7027, 300, 3620, 490, 264, 8174, 11, 570, 264, 8174, 286, 1194, 50944], "temperature": 0.0, "avg_logprob": -0.06919066373966942, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.0012842601863667369}, {"id": 4068, "seek": 1739708, "start": 17408.68, "end": 17412.920000000002, "text": " the movie was going to be bad, but it was actually amazing is encoded and represented by the same", "tokens": [50944, 264, 3169, 390, 516, 281, 312, 1578, 11, 457, 309, 390, 767, 2243, 307, 2058, 12340, 293, 10379, 538, 264, 912, 51156], "temperature": 0.0, "avg_logprob": -0.06919066373966942, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.0012842601863667369}, {"id": 4069, "seek": 1739708, "start": 17412.920000000002, "end": 17417.640000000003, "text": " thing as this sentence is. So that, you know, obviously is an issue that's a flaw. And that's", "tokens": [51156, 551, 382, 341, 8174, 307, 13, 407, 300, 11, 291, 458, 11, 2745, 307, 364, 2734, 300, 311, 257, 13717, 13, 400, 300, 311, 51392], "temperature": 0.0, "avg_logprob": -0.06919066373966942, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.0012842601863667369}, {"id": 4070, "seek": 1739708, "start": 17417.640000000003, "end": 17422.760000000002, "text": " one of the reasons why bag of words is not very good to use, because we lose the context of the", "tokens": [51392, 472, 295, 264, 4112, 983, 3411, 295, 2283, 307, 406, 588, 665, 281, 764, 11, 570, 321, 3624, 264, 4319, 295, 264, 51648], "temperature": 0.0, "avg_logprob": -0.06919066373966942, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.0012842601863667369}, {"id": 4071, "seek": 1742276, "start": 17422.76, "end": 17427.559999999998, "text": " words within the sentence, we just pick up the frequency and the fact that these words exist.", "tokens": [50364, 2283, 1951, 264, 8174, 11, 321, 445, 1888, 493, 264, 7893, 293, 264, 1186, 300, 613, 2283, 2514, 13, 50604], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4072, "seek": 1742276, "start": 17427.559999999998, "end": 17431.079999999998, "text": " So that's the first technique that's called bag of words. I've actually written a little", "tokens": [50604, 407, 300, 311, 264, 700, 6532, 300, 311, 1219, 3411, 295, 2283, 13, 286, 600, 767, 3720, 257, 707, 50780], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4073, "seek": 1742276, "start": 17431.079999999998, "end": 17435.559999999998, "text": " function here that does this for us. This is not really the exact way that we would write a bag", "tokens": [50780, 2445, 510, 300, 775, 341, 337, 505, 13, 639, 307, 406, 534, 264, 1900, 636, 300, 321, 576, 2464, 257, 3411, 51004], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4074, "seek": 1742276, "start": 17435.559999999998, "end": 17441.719999999998, "text": " of words function. But you kind of get the idea that when I have a text, this is a test to see", "tokens": [51004, 295, 2283, 2445, 13, 583, 291, 733, 295, 483, 264, 1558, 300, 562, 286, 362, 257, 2487, 11, 341, 307, 257, 1500, 281, 536, 51312], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4075, "seek": 1742276, "start": 17441.719999999998, "end": 17447.559999999998, "text": " if this test will work is test a I just did a bunch of random stuff. So we can see what I'm", "tokens": [51312, 498, 341, 1500, 486, 589, 307, 1500, 257, 286, 445, 630, 257, 3840, 295, 4974, 1507, 13, 407, 321, 393, 536, 437, 286, 478, 51604], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4076, "seek": 1742276, "start": 17447.559999999998, "end": 17451.559999999998, "text": " doing is printing out the bag, which I get from this function. And you guys can look at this if", "tokens": [51604, 884, 307, 14699, 484, 264, 3411, 11, 597, 286, 483, 490, 341, 2445, 13, 400, 291, 1074, 393, 574, 412, 341, 498, 51804], "temperature": 0.0, "avg_logprob": -0.04849209151901565, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07806777954101562}, {"id": 4077, "seek": 1745156, "start": 17451.56, "end": 17455.4, "text": " you kind of want to see how this works. And essentially, what it tells us is the word one", "tokens": [50364, 291, 733, 295, 528, 281, 536, 577, 341, 1985, 13, 400, 4476, 11, 437, 309, 5112, 505, 307, 264, 1349, 472, 50556], "temperature": 0.0, "avg_logprob": -0.12022212320122837, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.008061516098678112}, {"id": 4078, "seek": 1745156, "start": 17455.4, "end": 17462.2, "text": " appears two times. Yes, the word two appears three times the word three appears three times word", "tokens": [50556, 7038, 732, 1413, 13, 1079, 11, 264, 1349, 732, 7038, 1045, 1413, 264, 1349, 1045, 7038, 1045, 1413, 1349, 50896], "temperature": 0.0, "avg_logprob": -0.12022212320122837, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.008061516098678112}, {"id": 4079, "seek": 1745156, "start": 17462.2, "end": 17467.32, "text": " four appears three times five ones, six, one, seven ones. So on, that's the information we get", "tokens": [50896, 1451, 7038, 1045, 1413, 1732, 2306, 11, 2309, 11, 472, 11, 3407, 2306, 13, 407, 322, 11, 300, 311, 264, 1589, 321, 483, 51152], "temperature": 0.0, "avg_logprob": -0.12022212320122837, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.008061516098678112}, {"id": 4080, "seek": 1745156, "start": 17467.32, "end": 17472.04, "text": " from our bag, right from that encoding. And then if we look up here, this is our vocabulary. So", "tokens": [51152, 490, 527, 3411, 11, 558, 490, 300, 43430, 13, 400, 550, 498, 321, 574, 493, 510, 11, 341, 307, 527, 19864, 13, 407, 51388], "temperature": 0.0, "avg_logprob": -0.12022212320122837, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.008061516098678112}, {"id": 4081, "seek": 1745156, "start": 17472.04, "end": 17476.440000000002, "text": " this stands for one is is two is three so on. And you can kind of get the idea from that.", "tokens": [51388, 341, 7382, 337, 472, 307, 307, 732, 307, 1045, 370, 322, 13, 400, 291, 393, 733, 295, 483, 264, 1558, 490, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.12022212320122837, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.008061516098678112}, {"id": 4082, "seek": 1747644, "start": 17477.239999999998, "end": 17482.12, "text": " So that is how we would use bag of words, right? If we did an encoding kind of like this, that's", "tokens": [50404, 407, 300, 307, 577, 321, 576, 764, 3411, 295, 2283, 11, 558, 30, 759, 321, 630, 364, 43430, 733, 295, 411, 341, 11, 300, 311, 50648], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4083, "seek": 1747644, "start": 17482.12, "end": 17486.92, "text": " what that does. And that's one way of encoding it. Now, I'm going to go back and we'll talk about", "tokens": [50648, 437, 300, 775, 13, 400, 300, 311, 472, 636, 295, 43430, 309, 13, 823, 11, 286, 478, 516, 281, 352, 646, 293, 321, 603, 751, 466, 50888], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4084, "seek": 1747644, "start": 17486.92, "end": 17491.16, "text": " another method here as well, actually, a few more methods before we get into anything further.", "tokens": [50888, 1071, 3170, 510, 382, 731, 11, 767, 11, 257, 1326, 544, 7150, 949, 321, 483, 666, 1340, 3052, 13, 51100], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4085, "seek": 1747644, "start": 17491.16, "end": 17495.399999999998, "text": " All right, so I'm sure a lot of you were looking at the previous example I did. And you saw the", "tokens": [51100, 1057, 558, 11, 370, 286, 478, 988, 257, 688, 295, 291, 645, 1237, 412, 264, 3894, 1365, 286, 630, 13, 400, 291, 1866, 264, 51312], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4086, "seek": 1747644, "start": 17495.399999999998, "end": 17501.48, "text": " fact that what I did was completely remove the idea of kind of sequence or ordering of words,", "tokens": [51312, 1186, 300, 437, 286, 630, 390, 2584, 4159, 264, 1558, 295, 733, 295, 8310, 420, 21739, 295, 2283, 11, 51616], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4087, "seek": 1747644, "start": 17501.48, "end": 17504.76, "text": " right? And what I did was just throw everything in a bag. And I said, All right, we're just going", "tokens": [51616, 558, 30, 400, 437, 286, 630, 390, 445, 3507, 1203, 294, 257, 3411, 13, 400, 286, 848, 11, 1057, 558, 11, 321, 434, 445, 516, 51780], "temperature": 0.0, "avg_logprob": -0.06869573342172723, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.005060006398707628}, {"id": 4088, "seek": 1750476, "start": 17504.76, "end": 17511.16, "text": " to keep track of the fact that we have, you know, three A's, or we have four, those or seven, Tim's", "tokens": [50364, 281, 1066, 2837, 295, 264, 1186, 300, 321, 362, 11, 291, 458, 11, 1045, 316, 311, 11, 420, 321, 362, 1451, 11, 729, 420, 3407, 11, 7172, 311, 50684], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4089, "seek": 1750476, "start": 17511.16, "end": 17514.92, "text": " right. And we're going to just going to lose the fact that, you know, words come after one", "tokens": [50684, 558, 13, 400, 321, 434, 516, 281, 445, 516, 281, 3624, 264, 1186, 300, 11, 291, 458, 11, 2283, 808, 934, 472, 50872], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4090, "seek": 1750476, "start": 17514.92, "end": 17518.76, "text": " each other, we're going to lose their ordering in the sentence. And that's how we're going to", "tokens": [50872, 1184, 661, 11, 321, 434, 516, 281, 3624, 641, 21739, 294, 264, 8174, 13, 400, 300, 311, 577, 321, 434, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4091, "seek": 1750476, "start": 17518.76, "end": 17523.32, "text": " encode it. And I'm sure a lot of you were saying, Well, why don't we just not lose the ordering of", "tokens": [51064, 2058, 1429, 309, 13, 400, 286, 478, 988, 257, 688, 295, 291, 645, 1566, 11, 1042, 11, 983, 500, 380, 321, 445, 406, 3624, 264, 21739, 295, 51292], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4092, "seek": 1750476, "start": 17523.32, "end": 17527.8, "text": " those words? We'll just encode every single word with an integer and just leave it in its space", "tokens": [51292, 729, 2283, 30, 492, 603, 445, 2058, 1429, 633, 2167, 1349, 365, 364, 24922, 293, 445, 1856, 309, 294, 1080, 1901, 51516], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4093, "seek": 1750476, "start": 17527.8, "end": 17531.559999999998, "text": " where it would have been in the original string. Okay, good idea. So what you're telling me to", "tokens": [51516, 689, 309, 576, 362, 668, 294, 264, 3380, 6798, 13, 1033, 11, 665, 1558, 13, 407, 437, 291, 434, 3585, 385, 281, 51704], "temperature": 0.0, "avg_logprob": -0.10301954318315555, "compression_ratio": 1.8397435897435896, "no_speech_prob": 0.03113807551562786}, {"id": 4094, "seek": 1753156, "start": 17531.56, "end": 17537.56, "text": " do is something like this, you know, Tim is here will be our sentence. Let's say we encode the word", "tokens": [50364, 360, 307, 746, 411, 341, 11, 291, 458, 11, 7172, 307, 510, 486, 312, 527, 8174, 13, 961, 311, 584, 321, 2058, 1429, 264, 1349, 50664], "temperature": 0.0, "avg_logprob": -0.10539011032350602, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04208316653966904}, {"id": 4095, "seek": 1753156, "start": 17537.56, "end": 17542.52, "text": " Tim with zero is as one, here's two. And then that means our translation goes zero, one, two.", "tokens": [50664, 7172, 365, 4018, 307, 382, 472, 11, 510, 311, 732, 13, 400, 550, 300, 1355, 527, 12853, 1709, 4018, 11, 472, 11, 732, 13, 50912], "temperature": 0.0, "avg_logprob": -0.10539011032350602, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04208316653966904}, {"id": 4096, "seek": 1753156, "start": 17543.16, "end": 17548.36, "text": " And that means, right, if we have a translation, say, like two, one, zero, even though these use", "tokens": [50944, 400, 300, 1355, 11, 558, 11, 498, 321, 362, 257, 12853, 11, 584, 11, 411, 732, 11, 472, 11, 4018, 11, 754, 1673, 613, 764, 51204], "temperature": 0.0, "avg_logprob": -0.10539011032350602, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04208316653966904}, {"id": 4097, "seek": 1753156, "start": 17548.36, "end": 17554.52, "text": " the exact same number of words and exact same representation for all these words. Well, this", "tokens": [51204, 264, 1900, 912, 1230, 295, 2283, 293, 1900, 912, 10290, 337, 439, 613, 2283, 13, 1042, 11, 341, 51512], "temperature": 0.0, "avg_logprob": -0.10539011032350602, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04208316653966904}, {"id": 4098, "seek": 1753156, "start": 17554.52, "end": 17559.08, "text": " is a different sentence. And our model should be able to tell that because these words come in a", "tokens": [51512, 307, 257, 819, 8174, 13, 400, 527, 2316, 820, 312, 1075, 281, 980, 300, 570, 613, 2283, 808, 294, 257, 51740], "temperature": 0.0, "avg_logprob": -0.10539011032350602, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04208316653966904}, {"id": 4099, "seek": 1755908, "start": 17559.08, "end": 17563.08, "text": " different order. And to you, good point, if you made that point, but I'm going to discuss where", "tokens": [50364, 819, 1668, 13, 400, 281, 291, 11, 665, 935, 11, 498, 291, 1027, 300, 935, 11, 457, 286, 478, 516, 281, 2248, 689, 50564], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4100, "seek": 1755908, "start": 17563.08, "end": 17568.2, "text": " this falls apart as well. And why we're not going to use this method. So although this does solve", "tokens": [50564, 341, 8804, 4936, 382, 731, 13, 400, 983, 321, 434, 406, 516, 281, 764, 341, 3170, 13, 407, 4878, 341, 775, 5039, 50820], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4101, "seek": 1755908, "start": 17568.2, "end": 17573.08, "text": " the problem I talked about previously, where we're going to kind of lose out on the context of a word,", "tokens": [50820, 264, 1154, 286, 2825, 466, 8046, 11, 689, 321, 434, 516, 281, 733, 295, 3624, 484, 322, 264, 4319, 295, 257, 1349, 11, 51064], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4102, "seek": 1755908, "start": 17573.08, "end": 17576.920000000002, "text": " there's still a lot of issues with this and they come, especially when you're dealing with very", "tokens": [51064, 456, 311, 920, 257, 688, 295, 2663, 365, 341, 293, 436, 808, 11, 2318, 562, 291, 434, 6260, 365, 588, 51256], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4103, "seek": 1755908, "start": 17576.920000000002, "end": 17581.960000000003, "text": " large vocabularies. Now, let's take an example where we actually have a vocabulary of say 100,000", "tokens": [51256, 2416, 2329, 455, 1040, 530, 13, 823, 11, 718, 311, 747, 364, 1365, 689, 321, 767, 362, 257, 19864, 295, 584, 2319, 11, 1360, 51508], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4104, "seek": 1755908, "start": 17581.960000000003, "end": 17587.800000000003, "text": " words. And we know that that means we're going to have to have 100,000 unique mappings from words", "tokens": [51508, 2283, 13, 400, 321, 458, 300, 300, 1355, 321, 434, 516, 281, 362, 281, 362, 2319, 11, 1360, 3845, 463, 28968, 490, 2283, 51800], "temperature": 0.0, "avg_logprob": -0.05994285993128015, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.003824280807748437}, {"id": 4105, "seek": 1758780, "start": 17587.8, "end": 17594.44, "text": " to integers. So let's say our mappings are something like this, one maps to the string happy,", "tokens": [50364, 281, 41674, 13, 407, 718, 311, 584, 527, 463, 28968, 366, 746, 411, 341, 11, 472, 11317, 281, 264, 6798, 2055, 11, 50696], "temperature": 0.0, "avg_logprob": -0.10092270149374907, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00145498919300735}, {"id": 4106, "seek": 1758780, "start": 17595.16, "end": 17604.2, "text": " the word happy, right, to maps to sad. And let's say that the string 100,000 or the number 100,000", "tokens": [50732, 264, 1349, 2055, 11, 558, 11, 281, 11317, 281, 4227, 13, 400, 718, 311, 584, 300, 264, 6798, 2319, 11, 1360, 420, 264, 1230, 2319, 11, 1360, 51184], "temperature": 0.0, "avg_logprob": -0.10092270149374907, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00145498919300735}, {"id": 4107, "seek": 1758780, "start": 17604.2, "end": 17610.12, "text": " maps to the word, I don't know, let's say good. Now, we know as humans, by kind of just thinking", "tokens": [51184, 11317, 281, 264, 1349, 11, 286, 500, 380, 458, 11, 718, 311, 584, 665, 13, 823, 11, 321, 458, 382, 6255, 11, 538, 733, 295, 445, 1953, 51480], "temperature": 0.0, "avg_logprob": -0.10092270149374907, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00145498919300735}, {"id": 4108, "seek": 1758780, "start": 17610.12, "end": 17614.44, "text": " about, let's consider the fact that we're going to try to classify sentences as a positive or", "tokens": [51480, 466, 11, 718, 311, 1949, 264, 1186, 300, 321, 434, 516, 281, 853, 281, 33872, 16579, 382, 257, 3353, 420, 51696], "temperature": 0.0, "avg_logprob": -0.10092270149374907, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00145498919300735}, {"id": 4109, "seek": 1761444, "start": 17614.44, "end": 17619.0, "text": " negative. So sentiment analysis, that the words happy and good in that regard, you know, sentiment", "tokens": [50364, 3671, 13, 407, 16149, 5215, 11, 300, 264, 2283, 2055, 293, 665, 294, 300, 3843, 11, 291, 458, 11, 16149, 50592], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4110, "seek": 1761444, "start": 17619.0, "end": 17623.0, "text": " analysis are probably pretty similar words, right? And then if we were going to group these words,", "tokens": [50592, 5215, 366, 1391, 1238, 2531, 2283, 11, 558, 30, 400, 550, 498, 321, 645, 516, 281, 1594, 613, 2283, 11, 50792], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4111, "seek": 1761444, "start": 17623.0, "end": 17627.0, "text": " we'd probably put them in a similar group, we'd classify them as similar words, we get probably", "tokens": [50792, 321, 1116, 1391, 829, 552, 294, 257, 2531, 1594, 11, 321, 1116, 33872, 552, 382, 2531, 2283, 11, 321, 483, 1391, 50992], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4112, "seek": 1761444, "start": 17627.0, "end": 17631.559999999998, "text": " interchange them in a sentence, and it wouldn't change the meaning a whole ton. I mean, it might,", "tokens": [50992, 30358, 552, 294, 257, 8174, 11, 293, 309, 2759, 380, 1319, 264, 3620, 257, 1379, 2952, 13, 286, 914, 11, 309, 1062, 11, 51220], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4113, "seek": 1761444, "start": 17631.559999999998, "end": 17636.6, "text": " but it might not as well. And that we could say these are kind of similar. But our model or our", "tokens": [51220, 457, 309, 1062, 406, 382, 731, 13, 400, 300, 321, 727, 584, 613, 366, 733, 295, 2531, 13, 583, 527, 2316, 420, 527, 51472], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4114, "seek": 1761444, "start": 17636.6, "end": 17643.399999999998, "text": " encoder, right, whatever we're doing to translate our text into integers here, has decided that 100,000", "tokens": [51472, 2058, 19866, 11, 558, 11, 2035, 321, 434, 884, 281, 13799, 527, 2487, 666, 41674, 510, 11, 575, 3047, 300, 2319, 11, 1360, 51812], "temperature": 0.0, "avg_logprob": -0.07378337780634563, "compression_ratio": 1.8526645768025078, "no_speech_prob": 0.032096680253744125}, {"id": 4115, "seek": 1764340, "start": 17643.480000000003, "end": 17647.08, "text": " is going to represent good, and one is going to represent happy. And well, there's an issue with", "tokens": [50368, 307, 516, 281, 2906, 665, 11, 293, 472, 307, 516, 281, 2906, 2055, 13, 400, 731, 11, 456, 311, 364, 2734, 365, 50548], "temperature": 0.0, "avg_logprob": -0.09998160203297933, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.014954933896660805}, {"id": 4116, "seek": 1764340, "start": 17647.08, "end": 17652.36, "text": " that, because that means when we pass in something like one, or 100,000 to our model, it's going to", "tokens": [50548, 300, 11, 570, 300, 1355, 562, 321, 1320, 294, 746, 411, 472, 11, 420, 2319, 11, 1360, 281, 527, 2316, 11, 309, 311, 516, 281, 50812], "temperature": 0.0, "avg_logprob": -0.09998160203297933, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.014954933896660805}, {"id": 4117, "seek": 1764340, "start": 17652.36, "end": 17657.800000000003, "text": " have a very difficult time determining the fact that one and 100,000, although they're 99,999,", "tokens": [50812, 362, 257, 588, 2252, 565, 23751, 264, 1186, 300, 472, 293, 2319, 11, 1360, 11, 4878, 436, 434, 11803, 11, 49017, 11, 51084], "temperature": 0.0, "avg_logprob": -0.09998160203297933, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.014954933896660805}, {"id": 4118, "seek": 1764340, "start": 17659.640000000003, "end": 17664.2, "text": " kind of units apart, are actually very similar words. And that's the issue we get into when we", "tokens": [51176, 733, 295, 6815, 4936, 11, 366, 767, 588, 2531, 2283, 13, 400, 300, 311, 264, 2734, 321, 483, 666, 562, 321, 51404], "temperature": 0.0, "avg_logprob": -0.09998160203297933, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.014954933896660805}, {"id": 4119, "seek": 1764340, "start": 17664.2, "end": 17668.920000000002, "text": " do something like this is that the numbers we decide to pick to represent each word are very", "tokens": [51404, 360, 746, 411, 341, 307, 300, 264, 3547, 321, 4536, 281, 1888, 281, 2906, 1184, 1349, 366, 588, 51640], "temperature": 0.0, "avg_logprob": -0.09998160203297933, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.014954933896660805}, {"id": 4120, "seek": 1766892, "start": 17668.92, "end": 17674.6, "text": " important. And we don't really have a way of being able to look at words group them and saying,", "tokens": [50364, 1021, 13, 400, 321, 500, 380, 534, 362, 257, 636, 295, 885, 1075, 281, 574, 412, 2283, 1594, 552, 293, 1566, 11, 50648], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4121, "seek": 1766892, "start": 17674.6, "end": 17679.719999999998, "text": " okay, well, we need to put all of the happy words and the range like zero to 100, all of the like", "tokens": [50648, 1392, 11, 731, 11, 321, 643, 281, 829, 439, 295, 264, 2055, 2283, 293, 264, 3613, 411, 4018, 281, 2319, 11, 439, 295, 264, 411, 50904], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4122, "seek": 1766892, "start": 17679.719999999998, "end": 17684.44, "text": " adjectives in this range, we don't really have a way to do that. And this gets even harder for our", "tokens": [50904, 29378, 1539, 294, 341, 3613, 11, 321, 500, 380, 534, 362, 257, 636, 281, 360, 300, 13, 400, 341, 2170, 754, 6081, 337, 527, 51140], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4123, "seek": 1766892, "start": 17684.44, "end": 17688.679999999997, "text": " model when we have these arbitrary mappings, right? And then we have something like two in", "tokens": [51140, 2316, 562, 321, 362, 613, 23211, 463, 28968, 11, 558, 30, 400, 550, 321, 362, 746, 411, 732, 294, 51352], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4124, "seek": 1766892, "start": 17688.679999999997, "end": 17694.039999999997, "text": " between where two is very close to one, right? Yet these words are complete opposites. In fact,", "tokens": [51352, 1296, 689, 732, 307, 588, 1998, 281, 472, 11, 558, 30, 10890, 613, 2283, 366, 3566, 4665, 3324, 13, 682, 1186, 11, 51620], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4125, "seek": 1766892, "start": 17694.039999999997, "end": 17698.28, "text": " I'd say they're probably polar opposites, our model trying to learn that the difference between one", "tokens": [51620, 286, 1116, 584, 436, 434, 1391, 12367, 4665, 3324, 11, 527, 2316, 1382, 281, 1466, 300, 264, 2649, 1296, 472, 51832], "temperature": 0.0, "avg_logprob": -0.07925001562458195, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.05499447509646416}, {"id": 4126, "seek": 1769828, "start": 17698.36, "end": 17703.32, "text": " and two is actually way larger than the difference between one and 100,000 is going to be very", "tokens": [50368, 293, 732, 307, 767, 636, 4833, 813, 264, 2649, 1296, 472, 293, 2319, 11, 1360, 307, 516, 281, 312, 588, 50616], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4127, "seek": 1769828, "start": 17703.32, "end": 17708.84, "text": " difficult. And say it's even able to do that, as soon as we throw in the mapping 900, right,", "tokens": [50616, 2252, 13, 400, 584, 309, 311, 754, 1075, 281, 360, 300, 11, 382, 2321, 382, 321, 3507, 294, 264, 18350, 22016, 11, 558, 11, 50892], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4128, "seek": 1769828, "start": 17708.84, "end": 17715.0, "text": " the 99,900, we put that as bad. Well, now it gets even more difficult, because it's now like, okay,", "tokens": [50892, 264, 11803, 11, 23943, 11, 321, 829, 300, 382, 1578, 13, 1042, 11, 586, 309, 2170, 754, 544, 2252, 11, 570, 309, 311, 586, 411, 11, 1392, 11, 51200], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4129, "seek": 1769828, "start": 17715.0, "end": 17719.079999999998, "text": " what the range is this big, then that means these words are actually very similar. But then you", "tokens": [51200, 437, 264, 3613, 307, 341, 955, 11, 550, 300, 1355, 613, 2283, 366, 767, 588, 2531, 13, 583, 550, 291, 51404], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4130, "seek": 1769828, "start": 17719.079999999998, "end": 17723.32, "text": " throw another word in here like this, and it messes up the entire system. So that's kind of", "tokens": [51404, 3507, 1071, 1349, 294, 510, 411, 341, 11, 293, 309, 2082, 279, 493, 264, 2302, 1185, 13, 407, 300, 311, 733, 295, 51616], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4131, "seek": 1769828, "start": 17723.32, "end": 17727.0, "text": " what I want to show is that that's where this breaks apart on these large vocabularies. And", "tokens": [51616, 437, 286, 528, 281, 855, 307, 300, 300, 311, 689, 341, 9857, 4936, 322, 613, 2416, 2329, 455, 1040, 530, 13, 400, 51800], "temperature": 0.0, "avg_logprob": -0.07842121124267579, "compression_ratio": 1.7554179566563468, "no_speech_prob": 0.005384461954236031}, {"id": 4132, "seek": 1772700, "start": 17727.08, "end": 17731.8, "text": " that's why I'm going to introduce us now to another concept called word embeddings. Now,", "tokens": [50368, 300, 311, 983, 286, 478, 516, 281, 5366, 505, 586, 281, 1071, 3410, 1219, 1349, 12240, 29432, 13, 823, 11, 50604], "temperature": 0.0, "avg_logprob": -0.06565428184250653, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0017006590496748686}, {"id": 4133, "seek": 1772700, "start": 17731.8, "end": 17736.68, "text": " what word embeddings does is essentially try to find a way to represent words that are similar", "tokens": [50604, 437, 1349, 12240, 29432, 775, 307, 4476, 853, 281, 915, 257, 636, 281, 2906, 2283, 300, 366, 2531, 50848], "temperature": 0.0, "avg_logprob": -0.06565428184250653, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0017006590496748686}, {"id": 4134, "seek": 1772700, "start": 17736.68, "end": 17742.36, "text": " using very similar numbers. And in fact, what a word embedding is actually going to do, I'll talk", "tokens": [50848, 1228, 588, 2531, 3547, 13, 400, 294, 1186, 11, 437, 257, 1349, 12240, 3584, 307, 767, 516, 281, 360, 11, 286, 603, 751, 51132], "temperature": 0.0, "avg_logprob": -0.06565428184250653, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0017006590496748686}, {"id": 4135, "seek": 1772700, "start": 17742.36, "end": 17749.56, "text": " about this more in detail as we go on, is classify or translate every single one of our words into", "tokens": [51132, 466, 341, 544, 294, 2607, 382, 321, 352, 322, 11, 307, 33872, 420, 13799, 633, 2167, 472, 295, 527, 2283, 666, 51492], "temperature": 0.0, "avg_logprob": -0.06565428184250653, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0017006590496748686}, {"id": 4136, "seek": 1772700, "start": 17749.56, "end": 17755.32, "text": " a vector. And that vector is going to have some, you know, n amount of dimensions. Usually, we're", "tokens": [51492, 257, 8062, 13, 400, 300, 8062, 307, 516, 281, 362, 512, 11, 291, 458, 11, 297, 2372, 295, 12819, 13, 11419, 11, 321, 434, 51780], "temperature": 0.0, "avg_logprob": -0.06565428184250653, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0017006590496748686}, {"id": 4137, "seek": 1775532, "start": 17755.32, "end": 17761.32, "text": " going to use something like 64, maybe 128 dimensions for each vector. And every single component of", "tokens": [50364, 516, 281, 764, 746, 411, 12145, 11, 1310, 29810, 12819, 337, 1184, 8062, 13, 400, 633, 2167, 6542, 295, 50664], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4138, "seek": 1775532, "start": 17761.32, "end": 17767.16, "text": " that vector will kind of tell us what group it belongs to or how similar it is to other words.", "tokens": [50664, 300, 8062, 486, 733, 295, 980, 505, 437, 1594, 309, 12953, 281, 420, 577, 2531, 309, 307, 281, 661, 2283, 13, 50956], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4139, "seek": 1775532, "start": 17767.16, "end": 17770.68, "text": " So let me give you an idea of what I mean. So we're going to create something called the word", "tokens": [50956, 407, 718, 385, 976, 291, 364, 1558, 295, 437, 286, 914, 13, 407, 321, 434, 516, 281, 1884, 746, 1219, 264, 1349, 51132], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4140, "seek": 1775532, "start": 17770.68, "end": 17775.56, "text": " embeddings. Now, don't ask why it's called embeddings, I don't know the exact reason, but I", "tokens": [51132, 12240, 29432, 13, 823, 11, 500, 380, 1029, 983, 309, 311, 1219, 12240, 29432, 11, 286, 500, 380, 458, 264, 1900, 1778, 11, 457, 286, 51376], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4141, "seek": 1775532, "start": 17775.56, "end": 17779.96, "text": " believe it's to have has to do something with the fact that they're vectors. And let's just say we", "tokens": [51376, 1697, 309, 311, 281, 362, 575, 281, 360, 746, 365, 264, 1186, 300, 436, 434, 18875, 13, 400, 718, 311, 445, 584, 321, 51596], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4142, "seek": 1775532, "start": 17779.96, "end": 17783.8, "text": " have a 3d plane like this. And we've already kind of looked at what vectors are before. So I'll skip", "tokens": [51596, 362, 257, 805, 67, 5720, 411, 341, 13, 400, 321, 600, 1217, 733, 295, 2956, 412, 437, 18875, 366, 949, 13, 407, 286, 603, 10023, 51788], "temperature": 0.0, "avg_logprob": -0.06694060844062959, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.001648418721742928}, {"id": 4143, "seek": 1778380, "start": 17783.88, "end": 17789.8, "text": " over explaining them. And what we're going to do is take some word. So let's say we have the word", "tokens": [50368, 670, 13468, 552, 13, 400, 437, 321, 434, 516, 281, 360, 307, 747, 512, 1349, 13, 407, 718, 311, 584, 321, 362, 264, 1349, 50664], "temperature": 0.0, "avg_logprob": -0.05132826805114746, "compression_ratio": 1.8883495145631068, "no_speech_prob": 0.03846141695976257}, {"id": 4144, "seek": 1778380, "start": 17789.8, "end": 17794.84, "text": " good. And instead of picking some integer to represent it, we're going to pick some vector,", "tokens": [50664, 665, 13, 400, 2602, 295, 8867, 512, 24922, 281, 2906, 309, 11, 321, 434, 516, 281, 1888, 512, 8062, 11, 50916], "temperature": 0.0, "avg_logprob": -0.05132826805114746, "compression_ratio": 1.8883495145631068, "no_speech_prob": 0.03846141695976257}, {"id": 4145, "seek": 1778380, "start": 17794.84, "end": 17798.68, "text": " which means we're going to draw some vector in this 3d space. Actually, let's make this a different", "tokens": [50916, 597, 1355, 321, 434, 516, 281, 2642, 512, 8062, 294, 341, 805, 67, 1901, 13, 5135, 11, 718, 311, 652, 341, 257, 819, 51108], "temperature": 0.0, "avg_logprob": -0.05132826805114746, "compression_ratio": 1.8883495145631068, "no_speech_prob": 0.03846141695976257}, {"id": 4146, "seek": 1778380, "start": 17798.68, "end": 17806.36, "text": " color. Let's make this vector say red, like this. And this vector represents this word good. And in", "tokens": [51108, 2017, 13, 961, 311, 652, 341, 8062, 584, 2182, 11, 411, 341, 13, 400, 341, 8062, 8855, 341, 1349, 665, 13, 400, 294, 51492], "temperature": 0.0, "avg_logprob": -0.05132826805114746, "compression_ratio": 1.8883495145631068, "no_speech_prob": 0.03846141695976257}, {"id": 4147, "seek": 1780636, "start": 17806.440000000002, "end": 17813.24, "text": " this case, we'll say we have x one, x two, x three is our dimensions, which means that every single", "tokens": [50368, 341, 1389, 11, 321, 603, 584, 321, 362, 2031, 472, 11, 2031, 732, 11, 2031, 1045, 307, 527, 12819, 11, 597, 1355, 300, 633, 2167, 50708], "temperature": 0.0, "avg_logprob": -0.06733680593556371, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.01854553446173668}, {"id": 4148, "seek": 1780636, "start": 17813.24, "end": 17818.12, "text": " word in our data set will be represented by three coordinates. So one vector with three different", "tokens": [50708, 1349, 294, 527, 1412, 992, 486, 312, 10379, 538, 1045, 21056, 13, 407, 472, 8062, 365, 1045, 819, 50952], "temperature": 0.0, "avg_logprob": -0.06733680593556371, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.01854553446173668}, {"id": 4149, "seek": 1780636, "start": 17818.12, "end": 17823.96, "text": " dimensions, where we have x one x two and x three. And our hope is that by using this word", "tokens": [50952, 12819, 11, 689, 321, 362, 2031, 472, 2031, 732, 293, 2031, 1045, 13, 400, 527, 1454, 307, 300, 538, 1228, 341, 1349, 51244], "temperature": 0.0, "avg_logprob": -0.06733680593556371, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.01854553446173668}, {"id": 4150, "seek": 1780636, "start": 17823.96, "end": 17828.68, "text": " embeddings layer, and we'll talk about how it accomplishes this in a second, is that we can", "tokens": [51244, 12240, 29432, 4583, 11, 293, 321, 603, 751, 466, 577, 309, 9021, 279, 341, 294, 257, 1150, 11, 307, 300, 321, 393, 51480], "temperature": 0.0, "avg_logprob": -0.06733680593556371, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.01854553446173668}, {"id": 4151, "seek": 1780636, "start": 17828.68, "end": 17833.64, "text": " have vectors that represent very similar words being very similar, which means that you know,", "tokens": [51480, 362, 18875, 300, 2906, 588, 2531, 2283, 885, 588, 2531, 11, 597, 1355, 300, 291, 458, 11, 51728], "temperature": 0.0, "avg_logprob": -0.06733680593556371, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.01854553446173668}, {"id": 4152, "seek": 1783364, "start": 17833.64, "end": 17839.96, "text": " if we have the vector good here, we would hope the vector happy from our previous example, right,", "tokens": [50364, 498, 321, 362, 264, 8062, 665, 510, 11, 321, 576, 1454, 264, 8062, 2055, 490, 527, 3894, 1365, 11, 558, 11, 50680], "temperature": 0.0, "avg_logprob": -0.06177651381292263, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.005729808006435633}, {"id": 4153, "seek": 1783364, "start": 17839.96, "end": 17845.16, "text": " will be a vector that points in a similar direction to it. That is kind of a similar looking thing", "tokens": [50680, 486, 312, 257, 8062, 300, 2793, 294, 257, 2531, 3513, 281, 309, 13, 663, 307, 733, 295, 257, 2531, 1237, 551, 50940], "temperature": 0.0, "avg_logprob": -0.06177651381292263, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.005729808006435633}, {"id": 4154, "seek": 1783364, "start": 17845.16, "end": 17850.36, "text": " where the angle between these two vectors, right, and maybe I'll draw it here so we can see is small", "tokens": [50940, 689, 264, 5802, 1296, 613, 732, 18875, 11, 558, 11, 293, 1310, 286, 603, 2642, 309, 510, 370, 321, 393, 536, 307, 1359, 51200], "temperature": 0.0, "avg_logprob": -0.06177651381292263, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.005729808006435633}, {"id": 4155, "seek": 1783364, "start": 17850.36, "end": 17855.48, "text": " so that we know that these words are similar. And then we would hope that if we had a word that", "tokens": [51200, 370, 300, 321, 458, 300, 613, 2283, 366, 2531, 13, 400, 550, 321, 576, 1454, 300, 498, 321, 632, 257, 1349, 300, 51456], "temperature": 0.0, "avg_logprob": -0.06177651381292263, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.005729808006435633}, {"id": 4156, "seek": 1783364, "start": 17855.48, "end": 17859.8, "text": " was much different, maybe say like the word bad, that that would point in a different direction,", "tokens": [51456, 390, 709, 819, 11, 1310, 584, 411, 264, 1349, 1578, 11, 300, 300, 576, 935, 294, 257, 819, 3513, 11, 51672], "temperature": 0.0, "avg_logprob": -0.06177651381292263, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.005729808006435633}, {"id": 4157, "seek": 1785980, "start": 17859.8, "end": 17864.68, "text": " the vector that represents it. And that that would tell our model, because the angle between", "tokens": [50364, 264, 8062, 300, 8855, 309, 13, 400, 300, 300, 576, 980, 527, 2316, 11, 570, 264, 5802, 1296, 50608], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4158, "seek": 1785980, "start": 17864.68, "end": 17869.0, "text": " these two vectors is so big, that these are very different words, right? Now, in theory,", "tokens": [50608, 613, 732, 18875, 307, 370, 955, 11, 300, 613, 366, 588, 819, 2283, 11, 558, 30, 823, 11, 294, 5261, 11, 50824], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4159, "seek": 1785980, "start": 17869.0, "end": 17873.32, "text": " does the embedding work layer work like this, you know, not always, but this is what it's", "tokens": [50824, 775, 264, 12240, 3584, 589, 4583, 589, 411, 341, 11, 291, 458, 11, 406, 1009, 11, 457, 341, 307, 437, 309, 311, 51040], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4160, "seek": 1785980, "start": 17873.32, "end": 17879.96, "text": " trying to do is essentially pick some representation in a vector form for each word. And then these", "tokens": [51040, 1382, 281, 360, 307, 4476, 1888, 512, 10290, 294, 257, 8062, 1254, 337, 1184, 1349, 13, 400, 550, 613, 51372], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4161, "seek": 1785980, "start": 17879.96, "end": 17883.88, "text": " vectors, we hope if they're similar words, they're going to be pointing in a very similar", "tokens": [51372, 18875, 11, 321, 1454, 498, 436, 434, 2531, 2283, 11, 436, 434, 516, 281, 312, 12166, 294, 257, 588, 2531, 51568], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4162, "seek": 1785980, "start": 17883.88, "end": 17888.92, "text": " direction. And that's kind of the best explanation of a word embeddings layer I can give you.", "tokens": [51568, 3513, 13, 400, 300, 311, 733, 295, 264, 1151, 10835, 295, 257, 1349, 12240, 29432, 4583, 286, 393, 976, 291, 13, 51820], "temperature": 0.0, "avg_logprob": -0.08426522855405454, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.09533354640007019}, {"id": 4163, "seek": 1788892, "start": 17888.92, "end": 17893.399999999998, "text": " Now, how do we do this, though, how do we actually, you know, go from word to vector,", "tokens": [50364, 823, 11, 577, 360, 321, 360, 341, 11, 1673, 11, 577, 360, 321, 767, 11, 291, 458, 11, 352, 490, 1349, 281, 8062, 11, 50588], "temperature": 0.0, "avg_logprob": -0.07137350957901752, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.000779348483774811}, {"id": 4164, "seek": 1788892, "start": 17894.44, "end": 17899.16, "text": " and have that be meaningful? Well, this is actually what we call a layer. So word embeddings", "tokens": [50640, 293, 362, 300, 312, 10995, 30, 1042, 11, 341, 307, 767, 437, 321, 818, 257, 4583, 13, 407, 1349, 12240, 29432, 50876], "temperature": 0.0, "avg_logprob": -0.07137350957901752, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.000779348483774811}, {"id": 4165, "seek": 1788892, "start": 17899.16, "end": 17903.559999999998, "text": " is actually a layer, and it's something we're going to add to our model. And that means that", "tokens": [50876, 307, 767, 257, 4583, 11, 293, 309, 311, 746, 321, 434, 516, 281, 909, 281, 527, 2316, 13, 400, 300, 1355, 300, 51096], "temperature": 0.0, "avg_logprob": -0.07137350957901752, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.000779348483774811}, {"id": 4166, "seek": 1788892, "start": 17903.559999999998, "end": 17908.92, "text": " this actually learns the embeddings for our words. And the way it does that is by trying to kind of", "tokens": [51096, 341, 767, 27152, 264, 12240, 29432, 337, 527, 2283, 13, 400, 264, 636, 309, 775, 300, 307, 538, 1382, 281, 733, 295, 51364], "temperature": 0.0, "avg_logprob": -0.07137350957901752, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.000779348483774811}, {"id": 4167, "seek": 1788892, "start": 17908.92, "end": 17914.76, "text": " pick out context in the sentence and determine based on where a word is in a sentence, kind of what", "tokens": [51364, 1888, 484, 4319, 294, 264, 8174, 293, 6997, 2361, 322, 689, 257, 1349, 307, 294, 257, 8174, 11, 733, 295, 437, 51656], "temperature": 0.0, "avg_logprob": -0.07137350957901752, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.000779348483774811}, {"id": 4168, "seek": 1791476, "start": 17914.84, "end": 17920.679999999997, "text": " it means, and then encodes it doing that. Now, I know that's kind of a rough explanation to give", "tokens": [50368, 309, 1355, 11, 293, 550, 2058, 4789, 309, 884, 300, 13, 823, 11, 286, 458, 300, 311, 733, 295, 257, 5903, 10835, 281, 976, 50660], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4169, "seek": 1791476, "start": 17920.679999999997, "end": 17924.76, "text": " to you guys, I don't want to go too far into word embeddings in terms of the math, because I don't", "tokens": [50660, 281, 291, 1074, 11, 286, 500, 380, 528, 281, 352, 886, 1400, 666, 1349, 12240, 29432, 294, 2115, 295, 264, 5221, 11, 570, 286, 500, 380, 50864], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4170, "seek": 1791476, "start": 17924.76, "end": 17929.079999999998, "text": " want to get, you know, waste our time or get too complicated if we don't need to. But just understand", "tokens": [50864, 528, 281, 483, 11, 291, 458, 11, 5964, 527, 565, 420, 483, 886, 6179, 498, 321, 500, 380, 643, 281, 13, 583, 445, 1223, 51080], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4171, "seek": 1791476, "start": 17929.079999999998, "end": 17933.399999999998, "text": " that our word embeddings are actually trained, and that the model actually learns these word", "tokens": [51080, 300, 527, 1349, 12240, 29432, 366, 767, 8895, 11, 293, 300, 264, 2316, 767, 27152, 613, 1349, 51296], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4172, "seek": 1791476, "start": 17933.399999999998, "end": 17939.16, "text": " embeddings as it goes. And we hope that by the time it's looked at enough training data, it's", "tokens": [51296, 12240, 29432, 382, 309, 1709, 13, 400, 321, 1454, 300, 538, 264, 565, 309, 311, 2956, 412, 1547, 3097, 1412, 11, 309, 311, 51584], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4173, "seek": 1791476, "start": 17939.16, "end": 17944.6, "text": " determined really good ways to represent all of our different words, so that they make sense to", "tokens": [51584, 9540, 534, 665, 2098, 281, 2906, 439, 295, 527, 819, 2283, 11, 370, 300, 436, 652, 2020, 281, 51856], "temperature": 0.0, "avg_logprob": -0.060010733669751314, "compression_ratio": 1.8238993710691824, "no_speech_prob": 0.017441175878047943}, {"id": 4174, "seek": 1794460, "start": 17944.6, "end": 17949.16, "text": " our model in the further layers. And we can use pre trained word embedding layers, if we'd like,", "tokens": [50364, 527, 2316, 294, 264, 3052, 7914, 13, 400, 321, 393, 764, 659, 8895, 1349, 12240, 3584, 7914, 11, 498, 321, 1116, 411, 11, 50592], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4175, "seek": 1794460, "start": 17949.16, "end": 17954.359999999997, "text": " just like we use that pre trained convolutional base in the previous section. And we might actually", "tokens": [50592, 445, 411, 321, 764, 300, 659, 8895, 45216, 304, 3096, 294, 264, 3894, 3541, 13, 400, 321, 1062, 767, 50852], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4176, "seek": 1794460, "start": 17954.359999999997, "end": 17957.8, "text": " end up doing that. Actually, probably not in this tutorial, but it is something to consider that", "tokens": [50852, 917, 493, 884, 300, 13, 5135, 11, 1391, 406, 294, 341, 7073, 11, 457, 309, 307, 746, 281, 1949, 300, 51024], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4177, "seek": 1794460, "start": 17957.8, "end": 17962.28, "text": " you can do that. So that's how word embeddings work. This is how we encode textual data. And this", "tokens": [51024, 291, 393, 360, 300, 13, 407, 300, 311, 577, 1349, 12240, 29432, 589, 13, 639, 307, 577, 321, 2058, 1429, 2487, 901, 1412, 13, 400, 341, 51248], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4178, "seek": 1794460, "start": 17962.28, "end": 17966.92, "text": " is why it's so important that we kind of consider the way that we pass information to our neural", "tokens": [51248, 307, 983, 309, 311, 370, 1021, 300, 321, 733, 295, 1949, 264, 636, 300, 321, 1320, 1589, 281, 527, 18161, 51480], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4179, "seek": 1794460, "start": 17966.92, "end": 17971.96, "text": " network, because it makes a huge difference. Okay, so now that we've talked about kind of the form", "tokens": [51480, 3209, 11, 570, 309, 1669, 257, 2603, 2649, 13, 1033, 11, 370, 586, 300, 321, 600, 2825, 466, 733, 295, 264, 1254, 51732], "temperature": 0.0, "avg_logprob": -0.05370146036148071, "compression_ratio": 1.8006134969325154, "no_speech_prob": 0.0031725072767585516}, {"id": 4180, "seek": 1797196, "start": 17972.04, "end": 17975.96, "text": " that we need to get our data in before we can pass it further in the neural network, right,", "tokens": [50368, 300, 321, 643, 281, 483, 527, 1412, 294, 949, 321, 393, 1320, 309, 3052, 294, 264, 18161, 3209, 11, 558, 11, 50564], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4181, "seek": 1797196, "start": 17975.96, "end": 17980.6, "text": " before it can get past that embedding layer, before it can get put in, put into any dense", "tokens": [50564, 949, 309, 393, 483, 1791, 300, 12240, 3584, 4583, 11, 949, 309, 393, 483, 829, 294, 11, 829, 666, 604, 18011, 50796], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4182, "seek": 1797196, "start": 17980.6, "end": 17985.0, "text": " neurons, before we can even really do any math with it, we need to turn it into numbers, right,", "tokens": [50796, 22027, 11, 949, 321, 393, 754, 534, 360, 604, 5221, 365, 309, 11, 321, 643, 281, 1261, 309, 666, 3547, 11, 558, 11, 51016], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4183, "seek": 1797196, "start": 17985.0, "end": 17990.04, "text": " our textual data. So now that we know that it's time to talk about recurrent neural networks.", "tokens": [51016, 527, 2487, 901, 1412, 13, 407, 586, 300, 321, 458, 300, 309, 311, 565, 281, 751, 466, 18680, 1753, 18161, 9590, 13, 51268], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4184, "seek": 1797196, "start": 17990.04, "end": 17994.28, "text": " Now recurrent neural networks are the type of networks we use when we process textual data,", "tokens": [51268, 823, 18680, 1753, 18161, 9590, 366, 264, 2010, 295, 9590, 321, 764, 562, 321, 1399, 2487, 901, 1412, 11, 51480], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4185, "seek": 1797196, "start": 17994.28, "end": 17998.6, "text": " typically, you don't always have to use these, but they are just the best for natural language", "tokens": [51480, 5850, 11, 291, 500, 380, 1009, 362, 281, 764, 613, 11, 457, 436, 366, 445, 264, 1151, 337, 3303, 2856, 51696], "temperature": 0.0, "avg_logprob": -0.0626232351575579, "compression_ratio": 1.951048951048951, "no_speech_prob": 0.014955165795981884}, {"id": 4186, "seek": 1799860, "start": 17998.6, "end": 18003.079999999998, "text": " processing. And that's why they're kind of their own class, right? Now, the fundamental difference", "tokens": [50364, 9007, 13, 400, 300, 311, 983, 436, 434, 733, 295, 641, 1065, 1508, 11, 558, 30, 823, 11, 264, 8088, 2649, 50588], "temperature": 0.0, "avg_logprob": -0.0686168063770641, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.00757668586447835}, {"id": 4187, "seek": 1799860, "start": 18003.079999999998, "end": 18007.32, "text": " between a recurrence neural network and something like a dense neural network, or a convolutional", "tokens": [50588, 1296, 257, 18680, 10760, 18161, 3209, 293, 746, 411, 257, 18011, 18161, 3209, 11, 420, 257, 45216, 304, 50800], "temperature": 0.0, "avg_logprob": -0.0686168063770641, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.00757668586447835}, {"id": 4188, "seek": 1799860, "start": 18007.32, "end": 18013.399999999998, "text": " neural network, is the fact that it contains an internal loop. Now, what this really means is", "tokens": [50800, 18161, 3209, 11, 307, 264, 1186, 300, 309, 8306, 364, 6920, 6367, 13, 823, 11, 437, 341, 534, 1355, 307, 51104], "temperature": 0.0, "avg_logprob": -0.0686168063770641, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.00757668586447835}, {"id": 4189, "seek": 1799860, "start": 18013.399999999998, "end": 18018.68, "text": " that the recurrent neural network does not process our entire data at once. So it doesn't", "tokens": [51104, 300, 264, 18680, 1753, 18161, 3209, 775, 406, 1399, 527, 2302, 1412, 412, 1564, 13, 407, 309, 1177, 380, 51368], "temperature": 0.0, "avg_logprob": -0.0686168063770641, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.00757668586447835}, {"id": 4190, "seek": 1799860, "start": 18018.68, "end": 18024.359999999997, "text": " process the entire training example, or the entire input to the model at once, what it does is", "tokens": [51368, 1399, 264, 2302, 3097, 1365, 11, 420, 264, 2302, 4846, 281, 264, 2316, 412, 1564, 11, 437, 309, 775, 307, 51652], "temperature": 0.0, "avg_logprob": -0.0686168063770641, "compression_ratio": 1.8410852713178294, "no_speech_prob": 0.00757668586447835}, {"id": 4191, "seek": 1802436, "start": 18024.440000000002, "end": 18030.920000000002, "text": " processes it at different time steps, and maintains what we call an internal memory, and kind of an", "tokens": [50368, 7555, 309, 412, 819, 565, 4439, 11, 293, 33385, 437, 321, 818, 364, 6920, 4675, 11, 293, 733, 295, 364, 50692], "temperature": 0.0, "avg_logprob": -0.08123241771351207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.04885253682732582}, {"id": 4192, "seek": 1802436, "start": 18030.920000000002, "end": 18037.24, "text": " internal state, so that when it looks at a new input, it will remember what it's seen previously", "tokens": [50692, 6920, 1785, 11, 370, 300, 562, 309, 1542, 412, 257, 777, 4846, 11, 309, 486, 1604, 437, 309, 311, 1612, 8046, 51008], "temperature": 0.0, "avg_logprob": -0.08123241771351207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.04885253682732582}, {"id": 4193, "seek": 1802436, "start": 18037.24, "end": 18042.600000000002, "text": " and treat that input based on kind of the context or the understanding it's already developed.", "tokens": [51008, 293, 2387, 300, 4846, 2361, 322, 733, 295, 264, 4319, 420, 264, 3701, 309, 311, 1217, 4743, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08123241771351207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.04885253682732582}, {"id": 4194, "seek": 1802436, "start": 18042.600000000002, "end": 18047.24, "text": " Now, I understand that this doesn't make any sense right now. But with a dense neural network,", "tokens": [51276, 823, 11, 286, 1223, 300, 341, 1177, 380, 652, 604, 2020, 558, 586, 13, 583, 365, 257, 18011, 18161, 3209, 11, 51508], "temperature": 0.0, "avg_logprob": -0.08123241771351207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.04885253682732582}, {"id": 4195, "seek": 1802436, "start": 18047.24, "end": 18052.68, "text": " or the neural networks we looked at so far, we call those something called feed forward neural", "tokens": [51508, 420, 264, 18161, 9590, 321, 2956, 412, 370, 1400, 11, 321, 818, 729, 746, 1219, 3154, 2128, 18161, 51780], "temperature": 0.0, "avg_logprob": -0.08123241771351207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.04885253682732582}, {"id": 4196, "seek": 1805268, "start": 18052.68, "end": 18058.28, "text": " networks. What that means is we give all of our data to it at once, and we pass that data from", "tokens": [50364, 9590, 13, 708, 300, 1355, 307, 321, 976, 439, 295, 527, 1412, 281, 309, 412, 1564, 11, 293, 321, 1320, 300, 1412, 490, 50644], "temperature": 0.0, "avg_logprob": -0.06731439638538521, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.019120626151561737}, {"id": 4197, "seek": 1805268, "start": 18058.28, "end": 18063.88, "text": " left to right, or I guess for you guys from left to right. So we give all of the information, you", "tokens": [50644, 1411, 281, 558, 11, 420, 286, 2041, 337, 291, 1074, 490, 1411, 281, 558, 13, 407, 321, 976, 439, 295, 264, 1589, 11, 291, 50924], "temperature": 0.0, "avg_logprob": -0.06731439638538521, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.019120626151561737}, {"id": 4198, "seek": 1805268, "start": 18063.88, "end": 18067.96, "text": " know, we would pass those through the convolutional layer to start, maybe we have passed them through", "tokens": [50924, 458, 11, 321, 576, 1320, 729, 807, 264, 45216, 304, 4583, 281, 722, 11, 1310, 321, 362, 4678, 552, 807, 51128], "temperature": 0.0, "avg_logprob": -0.06731439638538521, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.019120626151561737}, {"id": 4199, "seek": 1805268, "start": 18067.96, "end": 18073.0, "text": " dense neurons, but they get given all of the info. And then that information gets translated", "tokens": [51128, 18011, 22027, 11, 457, 436, 483, 2212, 439, 295, 264, 13614, 13, 400, 550, 300, 1589, 2170, 16805, 51380], "temperature": 0.0, "avg_logprob": -0.06731439638538521, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.019120626151561737}, {"id": 4200, "seek": 1805268, "start": 18073.0, "end": 18078.2, "text": " through the network to the very end, again, from left to right. Whereas here, with recurrent neural", "tokens": [51380, 807, 264, 3209, 281, 264, 588, 917, 11, 797, 11, 490, 1411, 281, 558, 13, 13813, 510, 11, 365, 18680, 1753, 18161, 51640], "temperature": 0.0, "avg_logprob": -0.06731439638538521, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.019120626151561737}, {"id": 4201, "seek": 1807820, "start": 18078.2, "end": 18083.4, "text": " networks, we actually have a loop, which means that we don't feed the entire textual data at", "tokens": [50364, 9590, 11, 321, 767, 362, 257, 6367, 11, 597, 1355, 300, 321, 500, 380, 3154, 264, 2302, 2487, 901, 1412, 412, 50624], "temperature": 0.0, "avg_logprob": -0.06769136522637038, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.3275725841522217}, {"id": 4202, "seek": 1807820, "start": 18083.4, "end": 18089.96, "text": " once, we actually feed one word at a time, it processes that word, generate some output based", "tokens": [50624, 1564, 11, 321, 767, 3154, 472, 1349, 412, 257, 565, 11, 309, 7555, 300, 1349, 11, 8460, 512, 5598, 2361, 50952], "temperature": 0.0, "avg_logprob": -0.06769136522637038, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.3275725841522217}, {"id": 4203, "seek": 1807820, "start": 18089.96, "end": 18095.8, "text": " on that word, and uses the internal memory state that it's keeping track of to do that as part of", "tokens": [50952, 322, 300, 1349, 11, 293, 4960, 264, 6920, 4675, 1785, 300, 309, 311, 5145, 2837, 295, 281, 360, 300, 382, 644, 295, 51244], "temperature": 0.0, "avg_logprob": -0.06769136522637038, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.3275725841522217}, {"id": 4204, "seek": 1807820, "start": 18095.8, "end": 18100.920000000002, "text": " the calculation. So essentially, the reason we do this is because just like humans, when we, you", "tokens": [51244, 264, 17108, 13, 407, 4476, 11, 264, 1778, 321, 360, 341, 307, 570, 445, 411, 6255, 11, 562, 321, 11, 291, 51500], "temperature": 0.0, "avg_logprob": -0.06769136522637038, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.3275725841522217}, {"id": 4205, "seek": 1807820, "start": 18100.920000000002, "end": 18107.24, "text": " know, look at text, we don't just take a photo of this text and process it all at once, we read it", "tokens": [51500, 458, 11, 574, 412, 2487, 11, 321, 500, 380, 445, 747, 257, 5052, 295, 341, 2487, 293, 1399, 309, 439, 412, 1564, 11, 321, 1401, 309, 51816], "temperature": 0.0, "avg_logprob": -0.06769136522637038, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.3275725841522217}, {"id": 4206, "seek": 1810724, "start": 18107.320000000003, "end": 18112.920000000002, "text": " left to right, word to word. And based on the words that we've already read, we start to slowly", "tokens": [50368, 1411, 281, 558, 11, 1349, 281, 1349, 13, 400, 2361, 322, 264, 2283, 300, 321, 600, 1217, 1401, 11, 321, 722, 281, 5692, 50648], "temperature": 0.0, "avg_logprob": -0.0702721611146004, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.002714833477512002}, {"id": 4207, "seek": 1810724, "start": 18112.920000000002, "end": 18119.16, "text": " develop an understanding of what we're reading, right? If I just read the word now, that doesn't", "tokens": [50648, 1499, 364, 3701, 295, 437, 321, 434, 3760, 11, 558, 30, 759, 286, 445, 1401, 264, 1349, 586, 11, 300, 1177, 380, 50960], "temperature": 0.0, "avg_logprob": -0.0702721611146004, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.002714833477512002}, {"id": 4208, "seek": 1810724, "start": 18119.16, "end": 18124.52, "text": " mean much to me, if I just read the word in code, that doesn't mean much. Whereas if I read the entire", "tokens": [50960, 914, 709, 281, 385, 11, 498, 286, 445, 1401, 264, 1349, 294, 3089, 11, 300, 1177, 380, 914, 709, 13, 13813, 498, 286, 1401, 264, 2302, 51228], "temperature": 0.0, "avg_logprob": -0.0702721611146004, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.002714833477512002}, {"id": 4209, "seek": 1810724, "start": 18124.52, "end": 18129.480000000003, "text": " sentence, now that we've learned a little bit about how we can encode text, I start to develop", "tokens": [51228, 8174, 11, 586, 300, 321, 600, 3264, 257, 707, 857, 466, 577, 321, 393, 2058, 1429, 2487, 11, 286, 722, 281, 1499, 51476], "temperature": 0.0, "avg_logprob": -0.0702721611146004, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.002714833477512002}, {"id": 4210, "seek": 1810724, "start": 18129.480000000003, "end": 18134.68, "text": " an understanding about what this next word means, based on the previous words before it, right?", "tokens": [51476, 364, 3701, 466, 437, 341, 958, 1349, 1355, 11, 2361, 322, 264, 3894, 2283, 949, 309, 11, 558, 30, 51736], "temperature": 0.0, "avg_logprob": -0.0702721611146004, "compression_ratio": 1.9676113360323886, "no_speech_prob": 0.002714833477512002}, {"id": 4211, "seek": 1813468, "start": 18134.760000000002, "end": 18138.2, "text": " And that's kind of the point here is that this is what a recurrent neural network is going to do", "tokens": [50368, 400, 300, 311, 733, 295, 264, 935, 510, 307, 300, 341, 307, 437, 257, 18680, 1753, 18161, 3209, 307, 516, 281, 360, 50540], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4212, "seek": 1813468, "start": 18138.2, "end": 18144.44, "text": " for us. It's going to read one word at a time, and slowly start building up its understanding", "tokens": [50540, 337, 505, 13, 467, 311, 516, 281, 1401, 472, 1349, 412, 257, 565, 11, 293, 5692, 722, 2390, 493, 1080, 3701, 50852], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4213, "seek": 1813468, "start": 18144.44, "end": 18149.96, "text": " of what the entire textual data means. And this works in kind of a more complicated sense than", "tokens": [50852, 295, 437, 264, 2302, 2487, 901, 1412, 1355, 13, 400, 341, 1985, 294, 733, 295, 257, 544, 6179, 2020, 813, 51128], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4214, "seek": 1813468, "start": 18149.96, "end": 18154.84, "text": " that will draw it out a little bit. But this is kind of what would happen if we on, I guess,", "tokens": [51128, 300, 486, 2642, 309, 484, 257, 707, 857, 13, 583, 341, 307, 733, 295, 437, 576, 1051, 498, 321, 322, 11, 286, 2041, 11, 51372], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4215, "seek": 1813468, "start": 18154.84, "end": 18159.24, "text": " unraveled a recurrent layer, because recurrent neural network, yes, it has a loop in it. But", "tokens": [51372, 40507, 292, 257, 18680, 1753, 4583, 11, 570, 18680, 1753, 18161, 3209, 11, 2086, 11, 309, 575, 257, 6367, 294, 309, 13, 583, 51592], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4216, "seek": 1813468, "start": 18159.24, "end": 18163.48, "text": " really the recurrent aspect of a neural network is the layer that implements this", "tokens": [51592, 534, 264, 18680, 1753, 4171, 295, 257, 18161, 3209, 307, 264, 4583, 300, 704, 17988, 341, 51804], "temperature": 0.0, "avg_logprob": -0.07093418666294643, "compression_ratio": 1.861952861952862, "no_speech_prob": 0.021611986681818962}, {"id": 4217, "seek": 1816348, "start": 18164.04, "end": 18169.0, "text": " recurrent functionality with a loop. Essentially, what we can see here is that if we're saying x", "tokens": [50392, 18680, 1753, 14980, 365, 257, 6367, 13, 23596, 11, 437, 321, 393, 536, 510, 307, 300, 498, 321, 434, 1566, 2031, 50640], "temperature": 0.0, "avg_logprob": -0.08986150756362797, "compression_ratio": 1.8832684824902723, "no_speech_prob": 0.01098570879548788}, {"id": 4218, "seek": 1816348, "start": 18169.0, "end": 18175.96, "text": " is our input and h is our output, x t is going to be our input at time t, whereas h t is going to", "tokens": [50640, 307, 527, 4846, 293, 276, 307, 527, 5598, 11, 2031, 256, 307, 516, 281, 312, 527, 4846, 412, 565, 256, 11, 9735, 276, 256, 307, 516, 281, 50988], "temperature": 0.0, "avg_logprob": -0.08986150756362797, "compression_ratio": 1.8832684824902723, "no_speech_prob": 0.01098570879548788}, {"id": 4219, "seek": 1816348, "start": 18175.96, "end": 18181.88, "text": " be our output at time t. If we had a text of say length four, so four words, like we've encoded", "tokens": [50988, 312, 527, 5598, 412, 565, 256, 13, 759, 321, 632, 257, 2487, 295, 584, 4641, 1451, 11, 370, 1451, 2283, 11, 411, 321, 600, 2058, 12340, 51284], "temperature": 0.0, "avg_logprob": -0.08986150756362797, "compression_ratio": 1.8832684824902723, "no_speech_prob": 0.01098570879548788}, {"id": 4220, "seek": 1816348, "start": 18181.88, "end": 18187.32, "text": " them into integers now at this point, the first input at time zero will be the first word into", "tokens": [51284, 552, 666, 41674, 586, 412, 341, 935, 11, 264, 700, 4846, 412, 565, 4018, 486, 312, 264, 700, 1349, 666, 51556], "temperature": 0.0, "avg_logprob": -0.08986150756362797, "compression_ratio": 1.8832684824902723, "no_speech_prob": 0.01098570879548788}, {"id": 4221, "seek": 1816348, "start": 18187.32, "end": 18192.68, "text": " our network, right, or the first word that this layer is going to see. And the output at that time", "tokens": [51556, 527, 3209, 11, 558, 11, 420, 264, 700, 1349, 300, 341, 4583, 307, 516, 281, 536, 13, 400, 264, 5598, 412, 300, 565, 51824], "temperature": 0.0, "avg_logprob": -0.08986150756362797, "compression_ratio": 1.8832684824902723, "no_speech_prob": 0.01098570879548788}, {"id": 4222, "seek": 1819268, "start": 18192.68, "end": 18197.72, "text": " is going to be our current understanding of the entire text after looking at just that one word.", "tokens": [50364, 307, 516, 281, 312, 527, 2190, 3701, 295, 264, 2302, 2487, 934, 1237, 412, 445, 300, 472, 1349, 13, 50616], "temperature": 0.0, "avg_logprob": -0.03840230161493475, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.004467911086976528}, {"id": 4223, "seek": 1819268, "start": 18198.6, "end": 18205.08, "text": " Next, what we're going to do is process input one, which will be the next word in the sentence.", "tokens": [50660, 3087, 11, 437, 321, 434, 516, 281, 360, 307, 1399, 4846, 472, 11, 597, 486, 312, 264, 958, 1349, 294, 264, 8174, 13, 50984], "temperature": 0.0, "avg_logprob": -0.03840230161493475, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.004467911086976528}, {"id": 4224, "seek": 1819268, "start": 18205.08, "end": 18210.68, "text": " But we're going to use the output from the previous kind of computation or the previous iteration", "tokens": [50984, 583, 321, 434, 516, 281, 764, 264, 5598, 490, 264, 3894, 733, 295, 24903, 420, 264, 3894, 24784, 51264], "temperature": 0.0, "avg_logprob": -0.03840230161493475, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.004467911086976528}, {"id": 4225, "seek": 1819268, "start": 18211.4, "end": 18216.36, "text": " to do this. So we're going to process this word in combination with what we've already seen,", "tokens": [51300, 281, 360, 341, 13, 407, 321, 434, 516, 281, 1399, 341, 1349, 294, 6562, 365, 437, 321, 600, 1217, 1612, 11, 51548], "temperature": 0.0, "avg_logprob": -0.03840230161493475, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.004467911086976528}, {"id": 4226, "seek": 1819268, "start": 18216.36, "end": 18220.04, "text": " and then have a new output, which hopefully should now give us an understanding of what", "tokens": [51548, 293, 550, 362, 257, 777, 5598, 11, 597, 4696, 820, 586, 976, 505, 364, 3701, 295, 437, 51732], "temperature": 0.0, "avg_logprob": -0.03840230161493475, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.004467911086976528}, {"id": 4227, "seek": 1822004, "start": 18220.120000000003, "end": 18226.280000000002, "text": " those two words mean. Next, we'll go to the third word. And so forth, and slowly start building", "tokens": [50368, 729, 732, 2283, 914, 13, 3087, 11, 321, 603, 352, 281, 264, 2636, 1349, 13, 400, 370, 5220, 11, 293, 5692, 722, 2390, 50676], "temperature": 0.0, "avg_logprob": -0.06710389178732167, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0019876344595104456}, {"id": 4228, "seek": 1822004, "start": 18226.280000000002, "end": 18231.72, "text": " our understanding of what the entire textual data means by building it up one by one. The reason", "tokens": [50676, 527, 3701, 295, 437, 264, 2302, 2487, 901, 1412, 1355, 538, 2390, 309, 493, 472, 538, 472, 13, 440, 1778, 50948], "temperature": 0.0, "avg_logprob": -0.06710389178732167, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0019876344595104456}, {"id": 4229, "seek": 1822004, "start": 18231.72, "end": 18236.36, "text": " we don't pass the entire sequence at once is because it's very, very difficult to just kind", "tokens": [50948, 321, 500, 380, 1320, 264, 2302, 8310, 412, 1564, 307, 570, 309, 311, 588, 11, 588, 2252, 281, 445, 733, 51180], "temperature": 0.0, "avg_logprob": -0.06710389178732167, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0019876344595104456}, {"id": 4230, "seek": 1822004, "start": 18236.36, "end": 18241.88, "text": " of look at this huge blob of integers and figure out what the entire thing means. If we can do it", "tokens": [51180, 295, 574, 412, 341, 2603, 46115, 295, 41674, 293, 2573, 484, 437, 264, 2302, 551, 1355, 13, 759, 321, 393, 360, 309, 51456], "temperature": 0.0, "avg_logprob": -0.06710389178732167, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0019876344595104456}, {"id": 4231, "seek": 1822004, "start": 18241.88, "end": 18246.84, "text": " one by one and understand the meaning of specific words based on the words that have came before", "tokens": [51456, 472, 538, 472, 293, 1223, 264, 3620, 295, 2685, 2283, 2361, 322, 264, 2283, 300, 362, 1361, 949, 51704], "temperature": 0.0, "avg_logprob": -0.06710389178732167, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0019876344595104456}, {"id": 4232, "seek": 1824684, "start": 18246.920000000002, "end": 18251.32, "text": " it and start learning those patterns, that's going to be a lot easier for a neural network to deal", "tokens": [50368, 309, 293, 722, 2539, 729, 8294, 11, 300, 311, 516, 281, 312, 257, 688, 3571, 337, 257, 18161, 3209, 281, 2028, 50588], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4233, "seek": 1824684, "start": 18251.32, "end": 18256.12, "text": " with than just passing it all at once, looking at it and trying to get some output. And that's", "tokens": [50588, 365, 813, 445, 8437, 309, 439, 412, 1564, 11, 1237, 412, 309, 293, 1382, 281, 483, 512, 5598, 13, 400, 300, 311, 50828], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4234, "seek": 1824684, "start": 18256.12, "end": 18260.6, "text": " why we have these recurrent layers, there's a few different types of them. And I'm going to go", "tokens": [50828, 983, 321, 362, 613, 18680, 1753, 7914, 11, 456, 311, 257, 1326, 819, 3467, 295, 552, 13, 400, 286, 478, 516, 281, 352, 51052], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4235, "seek": 1824684, "start": 18260.6, "end": 18265.08, "text": " through them, and then we'll talk a little bit more in depth of how they work. So the first one", "tokens": [51052, 807, 552, 11, 293, 550, 321, 603, 751, 257, 707, 857, 544, 294, 7161, 295, 577, 436, 589, 13, 407, 264, 700, 472, 51276], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4236, "seek": 1824684, "start": 18265.08, "end": 18270.52, "text": " is called long short term memory. And actually, in fact, before we get into this, let's, let's", "tokens": [51276, 307, 1219, 938, 2099, 1433, 4675, 13, 400, 767, 11, 294, 1186, 11, 949, 321, 483, 666, 341, 11, 718, 311, 11, 718, 311, 51548], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4237, "seek": 1824684, "start": 18270.52, "end": 18275.0, "text": " talk about just a first like a simple layer so that we kind of have a reference point before", "tokens": [51548, 751, 466, 445, 257, 700, 411, 257, 2199, 4583, 370, 300, 321, 733, 295, 362, 257, 6408, 935, 949, 51772], "temperature": 0.0, "avg_logprob": -0.08173855690106954, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.035141296684741974}, {"id": 4238, "seek": 1827500, "start": 18275.0, "end": 18280.28, "text": " going here. Okay, so this is kind of the example I want to use here to illustrate however current", "tokens": [50364, 516, 510, 13, 1033, 11, 370, 341, 307, 733, 295, 264, 1365, 286, 528, 281, 764, 510, 281, 23221, 4461, 2190, 50628], "temperature": 0.0, "avg_logprob": -0.056567459270871916, "compression_ratio": 1.786231884057971, "no_speech_prob": 0.003707067808136344}, {"id": 4239, "seek": 1827500, "start": 18280.28, "end": 18285.96, "text": " neural network works and a more teaching style rather than what I was doing before. So essentially,", "tokens": [50628, 18161, 3209, 1985, 293, 257, 544, 4571, 3758, 2831, 813, 437, 286, 390, 884, 949, 13, 407, 4476, 11, 50912], "temperature": 0.0, "avg_logprob": -0.056567459270871916, "compression_ratio": 1.786231884057971, "no_speech_prob": 0.003707067808136344}, {"id": 4240, "seek": 1827500, "start": 18285.96, "end": 18291.08, "text": " the way that this works is that this whole thing that I'm drawing here, right, all of this circle", "tokens": [50912, 264, 636, 300, 341, 1985, 307, 300, 341, 1379, 551, 300, 286, 478, 6316, 510, 11, 558, 11, 439, 295, 341, 6329, 51168], "temperature": 0.0, "avg_logprob": -0.056567459270871916, "compression_ratio": 1.786231884057971, "no_speech_prob": 0.003707067808136344}, {"id": 4241, "seek": 1827500, "start": 18291.08, "end": 18296.76, "text": " stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing", "tokens": [51168, 1507, 307, 534, 472, 4583, 13, 400, 437, 286, 478, 884, 558, 586, 307, 7697, 341, 4583, 4936, 293, 4099, 51452], "temperature": 0.0, "avg_logprob": -0.056567459270871916, "compression_ratio": 1.786231884057971, "no_speech_prob": 0.003707067808136344}, {"id": 4242, "seek": 1827500, "start": 18296.76, "end": 18303.0, "text": " you kind of how this works in a series of steps. So rather than passing all the information at once,", "tokens": [51452, 291, 733, 295, 577, 341, 1985, 294, 257, 2638, 295, 4439, 13, 407, 2831, 813, 8437, 439, 264, 1589, 412, 1564, 11, 51764], "temperature": 0.0, "avg_logprob": -0.056567459270871916, "compression_ratio": 1.786231884057971, "no_speech_prob": 0.003707067808136344}, {"id": 4243, "seek": 1830300, "start": 18303.08, "end": 18307.64, "text": " we're going to pass it as a sequence, which means that we're going to have all these different words", "tokens": [50368, 321, 434, 516, 281, 1320, 309, 382, 257, 8310, 11, 597, 1355, 300, 321, 434, 516, 281, 362, 439, 613, 819, 2283, 50596], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4244, "seek": 1830300, "start": 18307.64, "end": 18312.52, "text": " and we're going to pass them one at a time to the kind of to the layer, right, to this recurrent", "tokens": [50596, 293, 321, 434, 516, 281, 1320, 552, 472, 412, 257, 565, 281, 264, 733, 295, 281, 264, 4583, 11, 558, 11, 281, 341, 18680, 1753, 50840], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4245, "seek": 1830300, "start": 18312.52, "end": 18317.4, "text": " layer. So we're going to start from this left side over here. So this right, you know, start over", "tokens": [50840, 4583, 13, 407, 321, 434, 516, 281, 722, 490, 341, 1411, 1252, 670, 510, 13, 407, 341, 558, 11, 291, 458, 11, 722, 670, 51084], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4246, "seek": 1830300, "start": 18317.4, "end": 18323.32, "text": " here at time step zero, that's what zero means. So time step is just, you know, the order. In this", "tokens": [51084, 510, 412, 565, 1823, 4018, 11, 300, 311, 437, 4018, 1355, 13, 407, 565, 1823, 307, 445, 11, 291, 458, 11, 264, 1668, 13, 682, 341, 51380], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4247, "seek": 1830300, "start": 18323.32, "end": 18328.44, "text": " case, this is the first word. So let's say we have the sentence Hi, I am Tim, right, we've broken", "tokens": [51380, 1389, 11, 341, 307, 264, 700, 1349, 13, 407, 718, 311, 584, 321, 362, 264, 8174, 2421, 11, 286, 669, 7172, 11, 558, 11, 321, 600, 5463, 51636], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4248, "seek": 1830300, "start": 18328.44, "end": 18331.96, "text": " these down into vectors, they've been turned into their numbers, I'm just writing them here so we", "tokens": [51636, 613, 760, 666, 18875, 11, 436, 600, 668, 3574, 666, 641, 3547, 11, 286, 478, 445, 3579, 552, 510, 370, 321, 51812], "temperature": 0.0, "avg_logprob": -0.08620836749766608, "compression_ratio": 1.9536423841059603, "no_speech_prob": 0.0035933295730501413}, {"id": 4249, "seek": 1833196, "start": 18331.96, "end": 18337.719999999998, "text": " can kind of see what I mean in like a natural language. And they are the input to this recurrent", "tokens": [50364, 393, 733, 295, 536, 437, 286, 914, 294, 411, 257, 3303, 2856, 13, 400, 436, 366, 264, 4846, 281, 341, 18680, 1753, 50652], "temperature": 0.0, "avg_logprob": -0.06133976815238831, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0007793416152708232}, {"id": 4250, "seek": 1833196, "start": 18337.719999999998, "end": 18343.32, "text": " layer. So all of our different words, right, that's how many kind of little cells we're going to draw", "tokens": [50652, 4583, 13, 407, 439, 295, 527, 819, 2283, 11, 558, 11, 300, 311, 577, 867, 733, 295, 707, 5438, 321, 434, 516, 281, 2642, 50932], "temperature": 0.0, "avg_logprob": -0.06133976815238831, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0007793416152708232}, {"id": 4251, "seek": 1833196, "start": 18343.32, "end": 18346.92, "text": " here is how many words we have in this sequence that we're talking about. So in this case, we have", "tokens": [50932, 510, 307, 577, 867, 2283, 321, 362, 294, 341, 8310, 300, 321, 434, 1417, 466, 13, 407, 294, 341, 1389, 11, 321, 362, 51112], "temperature": 0.0, "avg_logprob": -0.06133976815238831, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0007793416152708232}, {"id": 4252, "seek": 1833196, "start": 18346.92, "end": 18352.12, "text": " four, right, four words. So that's why I've drawn four cells to illustrate that. Now what we do is", "tokens": [51112, 1451, 11, 558, 11, 1451, 2283, 13, 407, 300, 311, 983, 286, 600, 10117, 1451, 5438, 281, 23221, 300, 13, 823, 437, 321, 360, 307, 51372], "temperature": 0.0, "avg_logprob": -0.06133976815238831, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0007793416152708232}, {"id": 4253, "seek": 1833196, "start": 18352.12, "end": 18358.76, "text": " that time step zero, the internal state of this layer is nothing, there's no previous output,", "tokens": [51372, 300, 565, 1823, 4018, 11, 264, 6920, 1785, 295, 341, 4583, 307, 1825, 11, 456, 311, 572, 3894, 5598, 11, 51704], "temperature": 0.0, "avg_logprob": -0.06133976815238831, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0007793416152708232}, {"id": 4254, "seek": 1835876, "start": 18358.76, "end": 18364.76, "text": " we haven't seen anything yet, which means that this first kind of cell, which is what I'm looking", "tokens": [50364, 321, 2378, 380, 1612, 1340, 1939, 11, 597, 1355, 300, 341, 700, 733, 295, 2815, 11, 597, 307, 437, 286, 478, 1237, 50664], "temperature": 0.0, "avg_logprob": -0.08179653347946528, "compression_ratio": 1.9182879377431907, "no_speech_prob": 0.016913194209337234}, {"id": 4255, "seek": 1835876, "start": 18364.76, "end": 18369.96, "text": " at right here, what I'm drawing in this first cell is only going to look and consider this first word", "tokens": [50664, 412, 558, 510, 11, 437, 286, 478, 6316, 294, 341, 700, 2815, 307, 787, 516, 281, 574, 293, 1949, 341, 700, 1349, 50924], "temperature": 0.0, "avg_logprob": -0.08179653347946528, "compression_ratio": 1.9182879377431907, "no_speech_prob": 0.016913194209337234}, {"id": 4256, "seek": 1835876, "start": 18369.96, "end": 18374.76, "text": " and kind of make some prediction about it and do something with it. We're going to pass high to", "tokens": [50924, 293, 733, 295, 652, 512, 17630, 466, 309, 293, 360, 746, 365, 309, 13, 492, 434, 516, 281, 1320, 1090, 281, 51164], "temperature": 0.0, "avg_logprob": -0.08179653347946528, "compression_ratio": 1.9182879377431907, "no_speech_prob": 0.016913194209337234}, {"id": 4257, "seek": 1835876, "start": 18374.76, "end": 18379.559999999998, "text": " this cell, some math's going to go on in here. And then what it's going to do is it's going to output", "tokens": [51164, 341, 2815, 11, 512, 5221, 311, 516, 281, 352, 322, 294, 510, 13, 400, 550, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 5598, 51404], "temperature": 0.0, "avg_logprob": -0.08179653347946528, "compression_ratio": 1.9182879377431907, "no_speech_prob": 0.016913194209337234}, {"id": 4258, "seek": 1835876, "start": 18379.559999999998, "end": 18385.399999999998, "text": " some value, which, you know, tells us something about the word high, right, some numeric value,", "tokens": [51404, 512, 2158, 11, 597, 11, 291, 458, 11, 5112, 505, 746, 466, 264, 1349, 1090, 11, 558, 11, 512, 7866, 299, 2158, 11, 51696], "temperature": 0.0, "avg_logprob": -0.08179653347946528, "compression_ratio": 1.9182879377431907, "no_speech_prob": 0.016913194209337234}, {"id": 4259, "seek": 1838540, "start": 18385.4, "end": 18388.280000000002, "text": " we're not going to talk about what that is, but it's going to do is going to be some output.", "tokens": [50364, 321, 434, 406, 516, 281, 751, 466, 437, 300, 307, 11, 457, 309, 311, 516, 281, 360, 307, 516, 281, 312, 512, 5598, 13, 50508], "temperature": 0.0, "avg_logprob": -0.11218030112130302, "compression_ratio": 1.8, "no_speech_prob": 0.01798490807414055}, {"id": 4260, "seek": 1838540, "start": 18389.08, "end": 18394.2, "text": " Now, what happens is after this cell has finished processing this, so right, so this one's done,", "tokens": [50548, 823, 11, 437, 2314, 307, 934, 341, 2815, 575, 4335, 9007, 341, 11, 370, 558, 11, 370, 341, 472, 311, 1096, 11, 50804], "temperature": 0.0, "avg_logprob": -0.11218030112130302, "compression_ratio": 1.8, "no_speech_prob": 0.01798490807414055}, {"id": 4261, "seek": 1838540, "start": 18394.2, "end": 18399.08, "text": " this has completed h zero, the outputs there, we'll do a check mark to say that that's done,", "tokens": [50804, 341, 575, 7365, 276, 4018, 11, 264, 23930, 456, 11, 321, 603, 360, 257, 1520, 1491, 281, 584, 300, 300, 311, 1096, 11, 51048], "temperature": 0.0, "avg_logprob": -0.11218030112130302, "compression_ratio": 1.8, "no_speech_prob": 0.01798490807414055}, {"id": 4262, "seek": 1838540, "start": 18399.08, "end": 18404.84, "text": " it's finished processing, this output gets fed into actually the same thing again, we're kind of", "tokens": [51048, 309, 311, 4335, 9007, 11, 341, 5598, 2170, 4636, 666, 767, 264, 912, 551, 797, 11, 321, 434, 733, 295, 51336], "temperature": 0.0, "avg_logprob": -0.11218030112130302, "compression_ratio": 1.8, "no_speech_prob": 0.01798490807414055}, {"id": 4263, "seek": 1838540, "start": 18404.84, "end": 18412.120000000003, "text": " just keeping track of it. And now what we do is we process the next input, which is I, and we use", "tokens": [51336, 445, 5145, 2837, 295, 309, 13, 400, 586, 437, 321, 360, 307, 321, 1399, 264, 958, 4846, 11, 597, 307, 286, 11, 293, 321, 764, 51700], "temperature": 0.0, "avg_logprob": -0.11218030112130302, "compression_ratio": 1.8, "no_speech_prob": 0.01798490807414055}, {"id": 4264, "seek": 1841212, "start": 18412.12, "end": 18417.96, "text": " the output from the previous cell to process this and understand what it means. So now, technically,", "tokens": [50364, 264, 5598, 490, 264, 3894, 2815, 281, 1399, 341, 293, 1223, 437, 309, 1355, 13, 407, 586, 11, 12120, 11, 50656], "temperature": 0.0, "avg_logprob": -0.05175987449852196, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.01640159823000431}, {"id": 4265, "seek": 1841212, "start": 18417.96, "end": 18423.239999999998, "text": " we should have some output from the previous cell. So from whatever high was, right, we do some", "tokens": [50656, 321, 820, 362, 512, 5598, 490, 264, 3894, 2815, 13, 407, 490, 2035, 1090, 390, 11, 558, 11, 321, 360, 512, 50920], "temperature": 0.0, "avg_logprob": -0.05175987449852196, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.01640159823000431}, {"id": 4266, "seek": 1841212, "start": 18423.239999999998, "end": 18429.559999999998, "text": " analysis on the word I, we kind of combine these things together. And that's the output of this", "tokens": [50920, 5215, 322, 264, 1349, 286, 11, 321, 733, 295, 10432, 613, 721, 1214, 13, 400, 300, 311, 264, 5598, 295, 341, 51236], "temperature": 0.0, "avg_logprob": -0.05175987449852196, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.01640159823000431}, {"id": 4267, "seek": 1841212, "start": 18429.559999999998, "end": 18436.039999999997, "text": " cell is our understanding of not only the current input, but the previous input with the current", "tokens": [51236, 2815, 307, 527, 3701, 295, 406, 787, 264, 2190, 4846, 11, 457, 264, 3894, 4846, 365, 264, 2190, 51560], "temperature": 0.0, "avg_logprob": -0.05175987449852196, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.01640159823000431}, {"id": 4268, "seek": 1841212, "start": 18436.039999999997, "end": 18441.239999999998, "text": " input. So we're slowly kind of building up our understanding of what this word I means,", "tokens": [51560, 4846, 13, 407, 321, 434, 5692, 733, 295, 2390, 493, 527, 3701, 295, 437, 341, 1349, 286, 1355, 11, 51820], "temperature": 0.0, "avg_logprob": -0.05175987449852196, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.01640159823000431}, {"id": 4269, "seek": 1844124, "start": 18441.24, "end": 18445.800000000003, "text": " based on the words we saw before. And that's the point I'm trying to get at is that", "tokens": [50364, 2361, 322, 264, 2283, 321, 1866, 949, 13, 400, 300, 311, 264, 935, 286, 478, 1382, 281, 483, 412, 307, 300, 50592], "temperature": 0.0, "avg_logprob": -0.05823274244341934, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.005059924442321062}, {"id": 4270, "seek": 1844124, "start": 18445.800000000003, "end": 18450.920000000002, "text": " this network uses what it's seen previously to understand the next thing that it sees,", "tokens": [50592, 341, 3209, 4960, 437, 309, 311, 1612, 8046, 281, 1223, 264, 958, 551, 300, 309, 8194, 11, 50848], "temperature": 0.0, "avg_logprob": -0.05823274244341934, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.005059924442321062}, {"id": 4271, "seek": 1844124, "start": 18450.920000000002, "end": 18456.440000000002, "text": " it's building a context is trying to understand not only the word, but what the word means,", "tokens": [50848, 309, 311, 2390, 257, 4319, 307, 1382, 281, 1223, 406, 787, 264, 1349, 11, 457, 437, 264, 1349, 1355, 11, 51124], "temperature": 0.0, "avg_logprob": -0.05823274244341934, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.005059924442321062}, {"id": 4272, "seek": 1844124, "start": 18456.440000000002, "end": 18462.120000000003, "text": " you know, in relation to what's come before it. So that's what's happening here. So then", "tokens": [51124, 291, 458, 11, 294, 9721, 281, 437, 311, 808, 949, 309, 13, 407, 300, 311, 437, 311, 2737, 510, 13, 407, 550, 51408], "temperature": 0.0, "avg_logprob": -0.05823274244341934, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.005059924442321062}, {"id": 4273, "seek": 1844124, "start": 18462.120000000003, "end": 18467.56, "text": " this output here, right, we get some output, we finish this, we get some output h one,", "tokens": [51408, 341, 5598, 510, 11, 558, 11, 321, 483, 512, 5598, 11, 321, 2413, 341, 11, 321, 483, 512, 5598, 276, 472, 11, 51680], "temperature": 0.0, "avg_logprob": -0.05823274244341934, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.005059924442321062}, {"id": 4274, "seek": 1846756, "start": 18467.56, "end": 18473.24, "text": " h one is passed into here. And now we have the understanding of what high and I means,", "tokens": [50364, 276, 472, 307, 4678, 666, 510, 13, 400, 586, 321, 362, 264, 3701, 295, 437, 1090, 293, 286, 1355, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09261886630438071, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.018545037135481834}, {"id": 4275, "seek": 1846756, "start": 18473.24, "end": 18479.24, "text": " and we add am like that, we do some kind of computations, we build an understanding of what", "tokens": [50648, 293, 321, 909, 669, 411, 300, 11, 321, 360, 512, 733, 295, 2807, 763, 11, 321, 1322, 364, 3701, 295, 437, 50948], "temperature": 0.0, "avg_logprob": -0.09261886630438071, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.018545037135481834}, {"id": 4276, "seek": 1846756, "start": 18479.24, "end": 18486.120000000003, "text": " this sentence is. And then we get the output h two, that passes to h three. And now finally,", "tokens": [50948, 341, 8174, 307, 13, 400, 550, 321, 483, 264, 5598, 276, 732, 11, 300, 11335, 281, 276, 1045, 13, 400, 586, 2721, 11, 51292], "temperature": 0.0, "avg_logprob": -0.09261886630438071, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.018545037135481834}, {"id": 4277, "seek": 1846756, "start": 18486.120000000003, "end": 18489.72, "text": " we have this final output h three, which is going to understand hopefully,", "tokens": [51292, 321, 362, 341, 2572, 5598, 276, 1045, 11, 597, 307, 516, 281, 1223, 4696, 11, 51472], "temperature": 0.0, "avg_logprob": -0.09261886630438071, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.018545037135481834}, {"id": 4278, "seek": 1846756, "start": 18490.52, "end": 18496.600000000002, "text": " what this entire thing means. Now, this is good, this works fairly well. And this is called a", "tokens": [51512, 437, 341, 2302, 551, 1355, 13, 823, 11, 341, 307, 665, 11, 341, 1985, 6457, 731, 13, 400, 341, 307, 1219, 257, 51816], "temperature": 0.0, "avg_logprob": -0.09261886630438071, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.018545037135481834}, {"id": 4279, "seek": 1849660, "start": 18496.6, "end": 18502.68, "text": " simple RNN layer, which means that all we do is we take the output from the previous cell or the", "tokens": [50364, 2199, 45702, 45, 4583, 11, 597, 1355, 300, 439, 321, 360, 307, 321, 747, 264, 5598, 490, 264, 3894, 2815, 420, 264, 50668], "temperature": 0.0, "avg_logprob": -0.07537315202795941, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.007345003541558981}, {"id": 4280, "seek": 1849660, "start": 18502.68, "end": 18507.879999999997, "text": " previous iteration, because really, all of these cells is just an iteration almost in a for loop,", "tokens": [50668, 3894, 24784, 11, 570, 534, 11, 439, 295, 613, 5438, 307, 445, 364, 24784, 1920, 294, 257, 337, 6367, 11, 50928], "temperature": 0.0, "avg_logprob": -0.07537315202795941, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.007345003541558981}, {"id": 4281, "seek": 1849660, "start": 18507.879999999997, "end": 18513.239999999998, "text": " right, based on all the different words in our sequence. And we slowly start building to that", "tokens": [50928, 558, 11, 2361, 322, 439, 264, 819, 2283, 294, 527, 8310, 13, 400, 321, 5692, 722, 2390, 281, 300, 51196], "temperature": 0.0, "avg_logprob": -0.07537315202795941, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.007345003541558981}, {"id": 4282, "seek": 1849660, "start": 18513.239999999998, "end": 18519.719999999998, "text": " understanding as we go through the entire sequence. Now, the only issue with this is that as we have", "tokens": [51196, 3701, 382, 321, 352, 807, 264, 2302, 8310, 13, 823, 11, 264, 787, 2734, 365, 341, 307, 300, 382, 321, 362, 51520], "temperature": 0.0, "avg_logprob": -0.07537315202795941, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.007345003541558981}, {"id": 4283, "seek": 1851972, "start": 18519.72, "end": 18526.52, "text": " a very long sequence, so sequences of length, say 100 or 150, the beginning of those sequences", "tokens": [50364, 257, 588, 938, 8310, 11, 370, 22978, 295, 4641, 11, 584, 2319, 420, 8451, 11, 264, 2863, 295, 729, 22978, 50704], "temperature": 0.0, "avg_logprob": -0.11061368195906929, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.25085288286209106}, {"id": 4284, "seek": 1851972, "start": 18526.52, "end": 18531.88, "text": " starts to kind of get lost. As we go through this, because remember, all we're doing, right,", "tokens": [50704, 3719, 281, 733, 295, 483, 2731, 13, 1018, 321, 352, 807, 341, 11, 570, 1604, 11, 439, 321, 434, 884, 11, 558, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11061368195906929, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.25085288286209106}, {"id": 4285, "seek": 1851972, "start": 18531.88, "end": 18537.32, "text": " is the output from h two is really a combination of the output from h zero and h one, and then", "tokens": [50972, 307, 264, 5598, 490, 276, 732, 307, 534, 257, 6562, 295, 264, 5598, 490, 276, 4018, 293, 276, 472, 11, 293, 550, 51244], "temperature": 0.0, "avg_logprob": -0.11061368195906929, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.25085288286209106}, {"id": 4286, "seek": 1851972, "start": 18537.32, "end": 18542.2, "text": " there's a new word that we've looked at. And h three is now a combination of everything before it,", "tokens": [51244, 456, 311, 257, 777, 1349, 300, 321, 600, 2956, 412, 13, 400, 276, 1045, 307, 586, 257, 6562, 295, 1203, 949, 309, 11, 51488], "temperature": 0.0, "avg_logprob": -0.11061368195906929, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.25085288286209106}, {"id": 4287, "seek": 1851972, "start": 18542.2, "end": 18546.440000000002, "text": " and this new word. So it becomes increasingly difficult for our model to actually", "tokens": [51488, 293, 341, 777, 1349, 13, 407, 309, 3643, 12980, 2252, 337, 527, 2316, 281, 767, 51700], "temperature": 0.0, "avg_logprob": -0.11061368195906929, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.25085288286209106}, {"id": 4288, "seek": 1854644, "start": 18547.0, "end": 18552.039999999997, "text": " build a really good understanding of the text in general, when the sequence gets long, because", "tokens": [50392, 1322, 257, 534, 665, 3701, 295, 264, 2487, 294, 2674, 11, 562, 264, 8310, 2170, 938, 11, 570, 50644], "temperature": 0.0, "avg_logprob": -0.07142095184326172, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.016401519998908043}, {"id": 4289, "seek": 1854644, "start": 18552.039999999997, "end": 18556.92, "text": " it's hard for it to remember what it's seen at the very beginning, because that is now so insignificant,", "tokens": [50644, 309, 311, 1152, 337, 309, 281, 1604, 437, 309, 311, 1612, 412, 264, 588, 2863, 11, 570, 300, 307, 586, 370, 43685, 11, 50888], "temperature": 0.0, "avg_logprob": -0.07142095184326172, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.016401519998908043}, {"id": 4290, "seek": 1854644, "start": 18556.92, "end": 18561.8, "text": " there's been so many outputs tacked on to that, that it's hard for it to go back and see that if", "tokens": [50888, 456, 311, 668, 370, 867, 23930, 9426, 292, 322, 281, 300, 11, 300, 309, 311, 1152, 337, 309, 281, 352, 646, 293, 536, 300, 498, 51132], "temperature": 0.0, "avg_logprob": -0.07142095184326172, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.016401519998908043}, {"id": 4291, "seek": 1854644, "start": 18561.8, "end": 18566.52, "text": " that makes any sense. Okay, so what I'm going to do now is try to explain the next layer we're", "tokens": [51132, 300, 1669, 604, 2020, 13, 1033, 11, 370, 437, 286, 478, 516, 281, 360, 586, 307, 853, 281, 2903, 264, 958, 4583, 321, 434, 51368], "temperature": 0.0, "avg_logprob": -0.07142095184326172, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.016401519998908043}, {"id": 4292, "seek": 1854644, "start": 18566.52, "end": 18572.12, "text": " going to look at, which is called LSTM. So the previous layer we just looked at the recurrent", "tokens": [51368, 516, 281, 574, 412, 11, 597, 307, 1219, 441, 6840, 44, 13, 407, 264, 3894, 4583, 321, 445, 2956, 412, 264, 18680, 1753, 51648], "temperature": 0.0, "avg_logprob": -0.07142095184326172, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.016401519998908043}, {"id": 4293, "seek": 1857212, "start": 18572.12, "end": 18576.68, "text": " layer was called a simple RNN layer. So simple recurrent neural network layer, whatever you", "tokens": [50364, 4583, 390, 1219, 257, 2199, 45702, 45, 4583, 13, 407, 2199, 18680, 1753, 18161, 3209, 4583, 11, 2035, 291, 50592], "temperature": 0.0, "avg_logprob": -0.08197351781333365, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.1066557914018631}, {"id": 4294, "seek": 1857212, "start": 18576.68, "end": 18581.239999999998, "text": " want to call it, right, simple recurrent layer. Now we're going to talk about the layer, which is", "tokens": [50592, 528, 281, 818, 309, 11, 558, 11, 2199, 18680, 1753, 4583, 13, 823, 321, 434, 516, 281, 751, 466, 264, 4583, 11, 597, 307, 50820], "temperature": 0.0, "avg_logprob": -0.08197351781333365, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.1066557914018631}, {"id": 4295, "seek": 1857212, "start": 18581.239999999998, "end": 18587.64, "text": " LSTM, which stands for long, short term memory. Now, long and short are hyphenated together.", "tokens": [50820, 441, 6840, 44, 11, 597, 7382, 337, 938, 11, 2099, 1433, 4675, 13, 823, 11, 938, 293, 2099, 366, 2477, 47059, 770, 1214, 13, 51140], "temperature": 0.0, "avg_logprob": -0.08197351781333365, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.1066557914018631}, {"id": 4296, "seek": 1857212, "start": 18587.64, "end": 18591.32, "text": " But essentially, what we're doing, and it just gets a little bit more complex, but I won't go", "tokens": [51140, 583, 4476, 11, 437, 321, 434, 884, 11, 293, 309, 445, 2170, 257, 707, 857, 544, 3997, 11, 457, 286, 1582, 380, 352, 51324], "temperature": 0.0, "avg_logprob": -0.08197351781333365, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.1066557914018631}, {"id": 4297, "seek": 1857212, "start": 18591.32, "end": 18598.12, "text": " into the math, is we add another component that keeps track of the internal state. So right now,", "tokens": [51324, 666, 264, 5221, 11, 307, 321, 909, 1071, 6542, 300, 5965, 2837, 295, 264, 6920, 1785, 13, 407, 558, 586, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08197351781333365, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.1066557914018631}, {"id": 4298, "seek": 1859812, "start": 18598.12, "end": 18603.32, "text": " the only thing that we were tracking as kind of our internal state as the memory for this model", "tokens": [50364, 264, 787, 551, 300, 321, 645, 11603, 382, 733, 295, 527, 6920, 1785, 382, 264, 4675, 337, 341, 2316, 50624], "temperature": 0.0, "avg_logprob": -0.07577005853044226, "compression_ratio": 1.9068627450980393, "no_speech_prob": 0.01590484380722046}, {"id": 4299, "seek": 1859812, "start": 18603.32, "end": 18609.64, "text": " was the previous output. So whatever the previous output was. So for example, at time zero here,", "tokens": [50624, 390, 264, 3894, 5598, 13, 407, 2035, 264, 3894, 5598, 390, 13, 407, 337, 1365, 11, 412, 565, 4018, 510, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07577005853044226, "compression_ratio": 1.9068627450980393, "no_speech_prob": 0.01590484380722046}, {"id": 4300, "seek": 1859812, "start": 18610.28, "end": 18616.12, "text": " there was no previous output. So there was nothing being kept in this model. But at time one, the", "tokens": [50972, 456, 390, 572, 3894, 5598, 13, 407, 456, 390, 1825, 885, 4305, 294, 341, 2316, 13, 583, 412, 565, 472, 11, 264, 51264], "temperature": 0.0, "avg_logprob": -0.07577005853044226, "compression_ratio": 1.9068627450980393, "no_speech_prob": 0.01590484380722046}, {"id": 4301, "seek": 1859812, "start": 18616.12, "end": 18623.32, "text": " output from this cell right here was what we were storing. And then at cell two, the only thing we", "tokens": [51264, 5598, 490, 341, 2815, 558, 510, 390, 437, 321, 645, 26085, 13, 400, 550, 412, 2815, 732, 11, 264, 787, 551, 321, 51624], "temperature": 0.0, "avg_logprob": -0.07577005853044226, "compression_ratio": 1.9068627450980393, "no_speech_prob": 0.01590484380722046}, {"id": 4302, "seek": 1862332, "start": 18623.4, "end": 18630.44, "text": " were storing was the output at time one, right? And we've lost now the output from time zero.", "tokens": [50368, 645, 26085, 390, 264, 5598, 412, 565, 472, 11, 558, 30, 400, 321, 600, 2731, 586, 264, 5598, 490, 565, 4018, 13, 50720], "temperature": 0.0, "avg_logprob": -0.05825584075030159, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.013222026638686657}, {"id": 4303, "seek": 1862332, "start": 18631.079999999998, "end": 18637.0, "text": " What we're adding in long, short term memory is an ability to access the output from any", "tokens": [50752, 708, 321, 434, 5127, 294, 938, 11, 2099, 1433, 4675, 307, 364, 3485, 281, 2105, 264, 5598, 490, 604, 51048], "temperature": 0.0, "avg_logprob": -0.05825584075030159, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.013222026638686657}, {"id": 4304, "seek": 1862332, "start": 18637.0, "end": 18642.04, "text": " previous state at any point in the future when we want it. Now, what this means is that rather", "tokens": [51048, 3894, 1785, 412, 604, 935, 294, 264, 2027, 562, 321, 528, 309, 13, 823, 11, 437, 341, 1355, 307, 300, 2831, 51300], "temperature": 0.0, "avg_logprob": -0.05825584075030159, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.013222026638686657}, {"id": 4305, "seek": 1862332, "start": 18642.04, "end": 18647.8, "text": " than just keeping track of the previous output, we'll add all of the outputs that we've seen so far", "tokens": [51300, 813, 445, 5145, 2837, 295, 264, 3894, 5598, 11, 321, 603, 909, 439, 295, 264, 23930, 300, 321, 600, 1612, 370, 1400, 51588], "temperature": 0.0, "avg_logprob": -0.05825584075030159, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.013222026638686657}, {"id": 4306, "seek": 1862332, "start": 18647.8, "end": 18651.48, "text": " into what I'm going to call my little kind of conveyor belt, it's going to run at the top", "tokens": [51588, 666, 437, 286, 478, 516, 281, 818, 452, 707, 733, 295, 18053, 2454, 10750, 11, 309, 311, 516, 281, 1190, 412, 264, 1192, 51772], "temperature": 0.0, "avg_logprob": -0.05825584075030159, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.013222026638686657}, {"id": 4307, "seek": 1865148, "start": 18651.48, "end": 18654.92, "text": " up here. I know it's kind of hard to see, but it's just what I'm highlighting. It's almost just", "tokens": [50364, 493, 510, 13, 286, 458, 309, 311, 733, 295, 1152, 281, 536, 11, 457, 309, 311, 445, 437, 286, 478, 26551, 13, 467, 311, 1920, 445, 50536], "temperature": 0.0, "avg_logprob": -0.05055490273695726, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.025954216718673706}, {"id": 4308, "seek": 1865148, "start": 18654.92, "end": 18661.079999999998, "text": " like a lookup table that can tell us the output at any previous cell that we want. So we can kind", "tokens": [50536, 411, 257, 574, 1010, 3199, 300, 393, 980, 505, 264, 5598, 412, 604, 3894, 2815, 300, 321, 528, 13, 407, 321, 393, 733, 50844], "temperature": 0.0, "avg_logprob": -0.05055490273695726, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.025954216718673706}, {"id": 4309, "seek": 1865148, "start": 18661.079999999998, "end": 18665.079999999998, "text": " of add things to this conveyor belt, we can pull things off, we can look at them. And this just", "tokens": [50844, 295, 909, 721, 281, 341, 18053, 2454, 10750, 11, 321, 393, 2235, 721, 766, 11, 321, 393, 574, 412, 552, 13, 400, 341, 445, 51044], "temperature": 0.0, "avg_logprob": -0.05055490273695726, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.025954216718673706}, {"id": 4310, "seek": 1865148, "start": 18665.079999999998, "end": 18670.28, "text": " adds a little bit of complexity to the model. It allows us to not just remember the last state,", "tokens": [51044, 10860, 257, 707, 857, 295, 14024, 281, 264, 2316, 13, 467, 4045, 505, 281, 406, 445, 1604, 264, 1036, 1785, 11, 51304], "temperature": 0.0, "avg_logprob": -0.05055490273695726, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.025954216718673706}, {"id": 4311, "seek": 1865148, "start": 18670.28, "end": 18676.76, "text": " but look anywhere at any point in time, which can be useful. Now, I don't want to go into much", "tokens": [51304, 457, 574, 4992, 412, 604, 935, 294, 565, 11, 597, 393, 312, 4420, 13, 823, 11, 286, 500, 380, 528, 281, 352, 666, 709, 51628], "temperature": 0.0, "avg_logprob": -0.05055490273695726, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.025954216718673706}, {"id": 4312, "seek": 1867676, "start": 18676.76, "end": 18681.239999999998, "text": " more depth about exactly how this works. But essentially, you know, just think about the", "tokens": [50364, 544, 7161, 466, 2293, 577, 341, 1985, 13, 583, 4476, 11, 291, 458, 11, 445, 519, 466, 264, 50588], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4313, "seek": 1867676, "start": 18681.239999999998, "end": 18686.679999999997, "text": " idea that as the sequence gets very long, it's pretty easy to forget the things we saw at the", "tokens": [50588, 1558, 300, 382, 264, 8310, 2170, 588, 938, 11, 309, 311, 1238, 1858, 281, 2870, 264, 721, 321, 1866, 412, 264, 50860], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4314, "seek": 1867676, "start": 18686.679999999997, "end": 18690.92, "text": " beginning. So if we can keep track of some of the things we've seen at the beginning, and some of", "tokens": [50860, 2863, 13, 407, 498, 321, 393, 1066, 2837, 295, 512, 295, 264, 721, 321, 600, 1612, 412, 264, 2863, 11, 293, 512, 295, 51072], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4315, "seek": 1867676, "start": 18690.92, "end": 18695.32, "text": " the things in between on this little conveyor belt, and we can access them whenever we want,", "tokens": [51072, 264, 721, 294, 1296, 322, 341, 707, 18053, 2454, 10750, 11, 293, 321, 393, 2105, 552, 5699, 321, 528, 11, 51292], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4316, "seek": 1867676, "start": 18695.32, "end": 18699.079999999998, "text": " then that's going to make this probably a much more useful layer, right? We could look at the", "tokens": [51292, 550, 300, 311, 516, 281, 652, 341, 1391, 257, 709, 544, 4420, 4583, 11, 558, 30, 492, 727, 574, 412, 264, 51480], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4317, "seek": 1867676, "start": 18699.079999999998, "end": 18705.239999999998, "text": " first sentence and the last sentence of a big piece of text at any point that we want and say,", "tokens": [51480, 700, 8174, 293, 264, 1036, 8174, 295, 257, 955, 2522, 295, 2487, 412, 604, 935, 300, 321, 528, 293, 584, 11, 51788], "temperature": 0.0, "avg_logprob": -0.05749073926953302, "compression_ratio": 1.8012820512820513, "no_speech_prob": 0.1519973874092102}, {"id": 4318, "seek": 1870524, "start": 18705.24, "end": 18710.52, "text": " okay, you know, this tells us X about the meaning of this text, right? So that's what this LSTM", "tokens": [50364, 1392, 11, 291, 458, 11, 341, 5112, 505, 1783, 466, 264, 3620, 295, 341, 2487, 11, 558, 30, 407, 300, 311, 437, 341, 441, 6840, 44, 50628], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4319, "seek": 1870524, "start": 18710.52, "end": 18715.0, "text": " does. Again, I don't want to go too far. We've already spent a lot of time kind of covering,", "tokens": [50628, 775, 13, 3764, 11, 286, 500, 380, 528, 281, 352, 886, 1400, 13, 492, 600, 1217, 4418, 257, 688, 295, 565, 733, 295, 10322, 11, 50852], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4320, "seek": 1870524, "start": 18715.0, "end": 18718.84, "text": " you know, recurrent layers and how all this works. Anyways, if you do want to look it up,", "tokens": [50852, 291, 458, 11, 18680, 1753, 7914, 293, 577, 439, 341, 1985, 13, 15585, 11, 498, 291, 360, 528, 281, 574, 309, 493, 11, 51044], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4321, "seek": 1870524, "start": 18718.84, "end": 18722.920000000002, "text": " some great mathematical definitions, again, I will source everything at the bottom of this", "tokens": [51044, 512, 869, 18894, 21988, 11, 797, 11, 286, 486, 4009, 1203, 412, 264, 2767, 295, 341, 51248], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4322, "seek": 1870524, "start": 18722.920000000002, "end": 18727.4, "text": " document so you can go there. But again, that's LSTM, long short term memory, that's what we're", "tokens": [51248, 4166, 370, 291, 393, 352, 456, 13, 583, 797, 11, 300, 311, 441, 6840, 44, 11, 938, 2099, 1433, 4675, 11, 300, 311, 437, 321, 434, 51472], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4323, "seek": 1870524, "start": 18727.4, "end": 18732.52, "text": " going to use for some of our examples, although simple RNN does work fairly well for shorter", "tokens": [51472, 516, 281, 764, 337, 512, 295, 527, 5110, 11, 4878, 2199, 45702, 45, 775, 589, 6457, 731, 337, 11639, 51728], "temperature": 0.0, "avg_logprob": -0.06835693640996945, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02297511324286461}, {"id": 4324, "seek": 1873252, "start": 18732.52, "end": 18737.08, "text": " length sequences. And again, remember, we're treating our text as a sequence now, where we're", "tokens": [50364, 4641, 22978, 13, 400, 797, 11, 1604, 11, 321, 434, 15083, 527, 2487, 382, 257, 8310, 586, 11, 689, 321, 434, 50592], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4325, "seek": 1873252, "start": 18737.08, "end": 18741.72, "text": " going to feed each word into the recurrent layer, and it's going to slowly start to develop an", "tokens": [50592, 516, 281, 3154, 1184, 1349, 666, 264, 18680, 1753, 4583, 11, 293, 309, 311, 516, 281, 5692, 722, 281, 1499, 364, 50824], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4326, "seek": 1873252, "start": 18741.72, "end": 18745.96, "text": " understanding as it reads through each word, right and processes that. Okay, so now we are", "tokens": [50824, 3701, 382, 309, 15700, 807, 1184, 1349, 11, 558, 293, 7555, 300, 13, 1033, 11, 370, 586, 321, 366, 51036], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4327, "seek": 1873252, "start": 18745.96, "end": 18751.0, "text": " on to our first example, where we're going to be performing sentiment analysis on movie reviews", "tokens": [51036, 322, 281, 527, 700, 1365, 11, 689, 321, 434, 516, 281, 312, 10205, 16149, 5215, 322, 3169, 10229, 51288], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4328, "seek": 1873252, "start": 18751.0, "end": 18755.56, "text": " to determine whether they are positive reviews or negative reviews. Now, we already know what", "tokens": [51288, 281, 6997, 1968, 436, 366, 3353, 10229, 420, 3671, 10229, 13, 823, 11, 321, 1217, 458, 437, 51516], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4329, "seek": 1873252, "start": 18755.56, "end": 18759.16, "text": " sentiment means. That's essentially what I just described. So picking up, you know, whether a", "tokens": [51516, 16149, 1355, 13, 663, 311, 4476, 437, 286, 445, 7619, 13, 407, 8867, 493, 11, 291, 458, 11, 1968, 257, 51696], "temperature": 0.0, "avg_logprob": -0.08016817386333759, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.014502475038170815}, {"id": 4330, "seek": 1875916, "start": 18759.8, "end": 18764.6, "text": " block of text is considered positive or negative. And for this example, we're going to be using the", "tokens": [50396, 3461, 295, 2487, 307, 4888, 3353, 420, 3671, 13, 400, 337, 341, 1365, 11, 321, 434, 516, 281, 312, 1228, 264, 50636], "temperature": 0.0, "avg_logprob": -0.08995430870393736, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.015905270352959633}, {"id": 4331, "seek": 1875916, "start": 18764.6, "end": 18769.96, "text": " movie review data sets. Now, as per usual, this is based off of this TensorFlow tutorial slash", "tokens": [50636, 3169, 3131, 1412, 6352, 13, 823, 11, 382, 680, 7713, 11, 341, 307, 2361, 766, 295, 341, 37624, 7073, 17330, 50904], "temperature": 0.0, "avg_logprob": -0.08995430870393736, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.015905270352959633}, {"id": 4332, "seek": 1875916, "start": 18769.96, "end": 18775.32, "text": " guide. I found this one kind of confusing to follow in the TensorFlow website, but obviously", "tokens": [50904, 5934, 13, 286, 1352, 341, 472, 733, 295, 13181, 281, 1524, 294, 264, 37624, 3144, 11, 457, 2745, 51172], "temperature": 0.0, "avg_logprob": -0.08995430870393736, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.015905270352959633}, {"id": 4333, "seek": 1875916, "start": 18775.32, "end": 18779.64, "text": " you can follow along with that if you don't prefer that version over mine. But anyways,", "tokens": [51172, 291, 393, 1524, 2051, 365, 300, 498, 291, 500, 380, 4382, 300, 3037, 670, 3892, 13, 583, 13448, 11, 51388], "temperature": 0.0, "avg_logprob": -0.08995430870393736, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.015905270352959633}, {"id": 4334, "seek": 1875916, "start": 18779.64, "end": 18783.8, "text": " we're going to be talking about the movie review data set. So this data set is straight from Keras,", "tokens": [51388, 321, 434, 516, 281, 312, 1417, 466, 264, 3169, 3131, 1412, 992, 13, 407, 341, 1412, 992, 307, 2997, 490, 591, 6985, 11, 51596], "temperature": 0.0, "avg_logprob": -0.08995430870393736, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.015905270352959633}, {"id": 4335, "seek": 1878380, "start": 18783.88, "end": 18789.96, "text": " and it contains 25,000 reviews, which are already pre processed and labeled. Now, what that means", "tokens": [50368, 293, 309, 8306, 3552, 11, 1360, 10229, 11, 597, 366, 1217, 659, 18846, 293, 21335, 13, 823, 11, 437, 300, 1355, 50672], "temperature": 0.0, "avg_logprob": -0.08265728869680632, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.012820499017834663}, {"id": 4336, "seek": 1878380, "start": 18789.96, "end": 18794.76, "text": " for us is that every single word is actually already encoded by an integer. And in fact,", "tokens": [50672, 337, 505, 307, 300, 633, 2167, 1349, 307, 767, 1217, 2058, 12340, 538, 364, 24922, 13, 400, 294, 1186, 11, 50912], "temperature": 0.0, "avg_logprob": -0.08265728869680632, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.012820499017834663}, {"id": 4337, "seek": 1878380, "start": 18794.76, "end": 18800.36, "text": " they've done kind of a clever encoding system where what they've done is said, if a character is", "tokens": [50912, 436, 600, 1096, 733, 295, 257, 13494, 43430, 1185, 689, 437, 436, 600, 1096, 307, 848, 11, 498, 257, 2517, 307, 51192], "temperature": 0.0, "avg_logprob": -0.08265728869680632, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.012820499017834663}, {"id": 4338, "seek": 1878380, "start": 18800.36, "end": 18807.32, "text": " encoded by say integer zero, that represents how common that word is in the entire data set. So", "tokens": [51192, 2058, 12340, 538, 584, 24922, 4018, 11, 300, 8855, 577, 2689, 300, 1349, 307, 294, 264, 2302, 1412, 992, 13, 407, 51540], "temperature": 0.0, "avg_logprob": -0.08265728869680632, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.012820499017834663}, {"id": 4339, "seek": 1878380, "start": 18807.32, "end": 18811.48, "text": " if an integer was encoded, but are not interested, a word was encoded by integer three, that would", "tokens": [51540, 498, 364, 24922, 390, 2058, 12340, 11, 457, 366, 406, 3102, 11, 257, 1349, 390, 2058, 12340, 538, 24922, 1045, 11, 300, 576, 51748], "temperature": 0.0, "avg_logprob": -0.08265728869680632, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.012820499017834663}, {"id": 4340, "seek": 1881148, "start": 18811.48, "end": 18815.72, "text": " mean that it is the third most common word in the data set. And in this specific data set,", "tokens": [50364, 914, 300, 309, 307, 264, 2636, 881, 2689, 1349, 294, 264, 1412, 992, 13, 400, 294, 341, 2685, 1412, 992, 11, 50576], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4341, "seek": 1881148, "start": 18815.72, "end": 18821.72, "text": " we have a vocabulary size of 88,584 unique words, which means that something that was", "tokens": [50576, 321, 362, 257, 19864, 2744, 295, 24587, 11, 20419, 19, 3845, 2283, 11, 597, 1355, 300, 746, 300, 390, 50876], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4342, "seek": 1881148, "start": 18821.72, "end": 18827.64, "text": " classified as this. So 88,584 would be the least common word in the data set. So something to", "tokens": [50876, 20627, 382, 341, 13, 407, 24587, 11, 20419, 19, 576, 312, 264, 1935, 2689, 1349, 294, 264, 1412, 992, 13, 407, 746, 281, 51172], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4343, "seek": 1881148, "start": 18827.64, "end": 18831.72, "text": " keep in mind, we're going to load in the data set and do our imports just by hitting run here.", "tokens": [51172, 1066, 294, 1575, 11, 321, 434, 516, 281, 3677, 294, 264, 1412, 992, 293, 360, 527, 41596, 445, 538, 8850, 1190, 510, 13, 51376], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4344, "seek": 1881148, "start": 18831.72, "end": 18836.04, "text": " And as I've mentioned previously, you know, I'm not going to be typing this stuff out. It's just", "tokens": [51376, 400, 382, 286, 600, 2835, 8046, 11, 291, 458, 11, 286, 478, 406, 516, 281, 312, 18444, 341, 1507, 484, 13, 467, 311, 445, 51592], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4345, "seek": 1881148, "start": 18836.04, "end": 18840.52, "text": " it's kind of a waste of time. I don't have all the syntax memorized. I would never expect you", "tokens": [51592, 309, 311, 733, 295, 257, 5964, 295, 565, 13, 286, 500, 380, 362, 439, 264, 28431, 46677, 13, 286, 576, 1128, 2066, 291, 51816], "temperature": 0.0, "avg_logprob": -0.06533249057069117, "compression_ratio": 1.7993527508090614, "no_speech_prob": 0.03621300309896469}, {"id": 4346, "seek": 1884052, "start": 18840.52, "end": 18845.56, "text": " guys to memorize this either. But what I will do is obviously walk through the code step by step,", "tokens": [50364, 1074, 281, 27478, 341, 2139, 13, 583, 437, 286, 486, 360, 307, 2745, 1792, 807, 264, 3089, 1823, 538, 1823, 11, 50616], "temperature": 0.0, "avg_logprob": -0.08131995748301021, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.001206548884510994}, {"id": 4347, "seek": 1884052, "start": 18845.56, "end": 18851.64, "text": " and make sure you understand why it is that we have what we have here. Okay, so what we've done", "tokens": [50616, 293, 652, 988, 291, 1223, 983, 309, 307, 300, 321, 362, 437, 321, 362, 510, 13, 1033, 11, 370, 437, 321, 600, 1096, 50920], "temperature": 0.0, "avg_logprob": -0.08131995748301021, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.001206548884510994}, {"id": 4348, "seek": 1884052, "start": 18851.64, "end": 18857.64, "text": " is to find the vocabulary size, the max length of a review, and the batch size. Now what we've", "tokens": [50920, 307, 281, 915, 264, 19864, 2744, 11, 264, 11469, 4641, 295, 257, 3131, 11, 293, 264, 15245, 2744, 13, 823, 437, 321, 600, 51220], "temperature": 0.0, "avg_logprob": -0.08131995748301021, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.001206548884510994}, {"id": 4349, "seek": 1884052, "start": 18857.64, "end": 18862.44, "text": " done is just loaded in our data set by defining the vocabulary size. So this is just the words", "tokens": [51220, 1096, 307, 445, 13210, 294, 527, 1412, 992, 538, 17827, 264, 19864, 2744, 13, 407, 341, 307, 445, 264, 2283, 51460], "temperature": 0.0, "avg_logprob": -0.08131995748301021, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.001206548884510994}, {"id": 4350, "seek": 1884052, "start": 18862.44, "end": 18867.08, "text": " it will include. So in this case, all of them, then we have trained data, trained labels, test", "tokens": [51460, 309, 486, 4090, 13, 407, 294, 341, 1389, 11, 439, 295, 552, 11, 550, 321, 362, 8895, 1412, 11, 8895, 16949, 11, 1500, 51692], "temperature": 0.0, "avg_logprob": -0.08131995748301021, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.001206548884510994}, {"id": 4351, "seek": 1886708, "start": 18867.160000000003, "end": 18871.640000000003, "text": " data, test labels. And we can look at a review and see what it looks like by doing something", "tokens": [50368, 1412, 11, 1500, 16949, 13, 400, 321, 393, 574, 412, 257, 3131, 293, 536, 437, 309, 1542, 411, 538, 884, 746, 50592], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4352, "seek": 1886708, "start": 18871.640000000003, "end": 18876.120000000003, "text": " like this. So this is an example of our first review, we can see kind of the different encodings", "tokens": [50592, 411, 341, 13, 407, 341, 307, 364, 1365, 295, 527, 700, 3131, 11, 321, 393, 536, 733, 295, 264, 819, 2058, 378, 1109, 50816], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4353, "seek": 1886708, "start": 18876.120000000003, "end": 18880.84, "text": " for all of these words. And this is what it looks like, they're already in integer form.", "tokens": [50816, 337, 439, 295, 613, 2283, 13, 400, 341, 307, 437, 309, 1542, 411, 11, 436, 434, 1217, 294, 24922, 1254, 13, 51052], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4354, "seek": 1886708, "start": 18880.84, "end": 18885.56, "text": " Now, just something to note here is that the length of our reviews are not unique. So if I do", "tokens": [51052, 823, 11, 445, 746, 281, 3637, 510, 307, 300, 264, 4641, 295, 527, 10229, 366, 406, 3845, 13, 407, 498, 286, 360, 51288], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4355, "seek": 1886708, "start": 18885.56, "end": 18890.84, "text": " the length of trained data, I guess I wouldn't say unique, but I mean, they're just all different.", "tokens": [51288, 264, 4641, 295, 8895, 1412, 11, 286, 2041, 286, 2759, 380, 584, 3845, 11, 457, 286, 914, 11, 436, 434, 445, 439, 819, 13, 51552], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4356, "seek": 1886708, "start": 18890.84, "end": 18894.280000000002, "text": " So the length of trained data is zero is different than the length of trained data one,", "tokens": [51552, 407, 264, 4641, 295, 8895, 1412, 307, 4018, 307, 819, 813, 264, 4641, 295, 8895, 1412, 472, 11, 51724], "temperature": 0.0, "avg_logprob": -0.07736394788835432, "compression_ratio": 1.98932384341637, "no_speech_prob": 0.006097281351685524}, {"id": 4357, "seek": 1889428, "start": 18894.28, "end": 18897.879999999997, "text": " right? So that's something to consider as we go through this and something we're actually going", "tokens": [50364, 558, 30, 407, 300, 311, 746, 281, 1949, 382, 321, 352, 807, 341, 293, 746, 321, 434, 767, 516, 50544], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4358, "seek": 1889428, "start": 18897.879999999997, "end": 18902.68, "text": " to have to handle. Okay, so more pre processing. So this is what I was talking about. If you have", "tokens": [50544, 281, 362, 281, 4813, 13, 1033, 11, 370, 544, 659, 9007, 13, 407, 341, 307, 437, 286, 390, 1417, 466, 13, 759, 291, 362, 50784], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4359, "seek": 1889428, "start": 18902.68, "end": 18906.28, "text": " a look at our loaded interviews, we'll notice there are different lengths, this is an issue,", "tokens": [50784, 257, 574, 412, 527, 13210, 12318, 11, 321, 603, 3449, 456, 366, 819, 26329, 11, 341, 307, 364, 2734, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4360, "seek": 1889428, "start": 18906.28, "end": 18910.52, "text": " we cannot pass different length data into our neural network, which is true. Therefore, we", "tokens": [50964, 321, 2644, 1320, 819, 4641, 1412, 666, 527, 18161, 3209, 11, 597, 307, 2074, 13, 7504, 11, 321, 51176], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4361, "seek": 1889428, "start": 18910.52, "end": 18914.76, "text": " must make each review the same length. Okay, so what we're going to do for now is we're actually", "tokens": [51176, 1633, 652, 1184, 3131, 264, 912, 4641, 13, 1033, 11, 370, 437, 321, 434, 516, 281, 360, 337, 586, 307, 321, 434, 767, 51388], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4362, "seek": 1889428, "start": 18914.76, "end": 18919.8, "text": " going to pad our sequences. Now what that means is we're going to follow this kind of step that", "tokens": [51388, 516, 281, 6887, 527, 22978, 13, 823, 437, 300, 1355, 307, 321, 434, 516, 281, 1524, 341, 733, 295, 1823, 300, 51640], "temperature": 0.0, "avg_logprob": -0.09713094011485147, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.014502464793622494}, {"id": 4363, "seek": 1891980, "start": 18919.88, "end": 18925.88, "text": " I've talked about here. So if the review is greater than 250 words, we will trim off extra words,", "tokens": [50368, 286, 600, 2825, 466, 510, 13, 407, 498, 264, 3131, 307, 5044, 813, 11650, 2283, 11, 321, 486, 10445, 766, 2857, 2283, 11, 50668], "temperature": 0.0, "avg_logprob": -0.05151865734317439, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.019717592746019363}, {"id": 4364, "seek": 1891980, "start": 18925.88, "end": 18931.64, "text": " if the review is less than 250 words, we'll add the necessary amount of this should actually be", "tokens": [50668, 498, 264, 3131, 307, 1570, 813, 11650, 2283, 11, 321, 603, 909, 264, 4818, 2372, 295, 341, 820, 767, 312, 50956], "temperature": 0.0, "avg_logprob": -0.05151865734317439, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.019717592746019363}, {"id": 4365, "seek": 1891980, "start": 18931.64, "end": 18938.36, "text": " zeros in here, let's fix this of zeros to make it equal to 250. So what that means is we're", "tokens": [50956, 35193, 294, 510, 11, 718, 311, 3191, 341, 295, 35193, 281, 652, 309, 2681, 281, 11650, 13, 407, 437, 300, 1355, 307, 321, 434, 51292], "temperature": 0.0, "avg_logprob": -0.05151865734317439, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.019717592746019363}, {"id": 4366, "seek": 1891980, "start": 18938.36, "end": 18942.04, "text": " essentially going to add some kind of padding to our review. So in this case, I believe we're", "tokens": [51292, 4476, 516, 281, 909, 512, 733, 295, 39562, 281, 527, 3131, 13, 407, 294, 341, 1389, 11, 286, 1697, 321, 434, 51476], "temperature": 0.0, "avg_logprob": -0.05151865734317439, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.019717592746019363}, {"id": 4367, "seek": 1891980, "start": 18942.04, "end": 18946.28, "text": " actually going to pad to the left side, which means that say we have a review of length, you know,", "tokens": [51476, 767, 516, 281, 6887, 281, 264, 1411, 1252, 11, 597, 1355, 300, 584, 321, 362, 257, 3131, 295, 4641, 11, 291, 458, 11, 51688], "temperature": 0.0, "avg_logprob": -0.05151865734317439, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.019717592746019363}, {"id": 4368, "seek": 1894628, "start": 18946.36, "end": 18951.64, "text": " 200, we're going to add 50, just kind of blank words, which will represent with the index zero", "tokens": [50368, 2331, 11, 321, 434, 516, 281, 909, 2625, 11, 445, 733, 295, 8247, 2283, 11, 597, 486, 2906, 365, 264, 8186, 4018, 50632], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4369, "seek": 1894628, "start": 18951.64, "end": 18957.64, "text": " to the left side of the review to make it the necessary length. So that's, that's good, we'll", "tokens": [50632, 281, 264, 1411, 1252, 295, 264, 3131, 281, 652, 309, 264, 4818, 4641, 13, 407, 300, 311, 11, 300, 311, 665, 11, 321, 603, 50932], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4370, "seek": 1894628, "start": 18957.64, "end": 18962.199999999997, "text": " do that. So if we look at train data and test data, what this does is we're just going to use", "tokens": [50932, 360, 300, 13, 407, 498, 321, 574, 412, 3847, 1412, 293, 1500, 1412, 11, 437, 341, 775, 307, 321, 434, 445, 516, 281, 764, 51160], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4371, "seek": 1894628, "start": 18962.199999999997, "end": 18966.68, "text": " something from Keras, which we've imported above. So we're saying from Keras dot pre processing", "tokens": [51160, 746, 490, 591, 6985, 11, 597, 321, 600, 25524, 3673, 13, 407, 321, 434, 1566, 490, 591, 6985, 5893, 659, 9007, 51384], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4372, "seek": 1894628, "start": 18966.68, "end": 18971.0, "text": " import sequence, again, we're treating our text data as a sequence, as we've talked about,", "tokens": [51384, 974, 8310, 11, 797, 11, 321, 434, 15083, 527, 2487, 1412, 382, 257, 8310, 11, 382, 321, 600, 2825, 466, 11, 51600], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4373, "seek": 1894628, "start": 18971.0, "end": 18975.96, "text": " we're going to say sequence dot pad sequences, train data, and then we define the length that", "tokens": [51600, 321, 434, 516, 281, 584, 8310, 5893, 6887, 22978, 11, 3847, 1412, 11, 293, 550, 321, 6964, 264, 4641, 300, 51848], "temperature": 0.0, "avg_logprob": -0.09056852941643702, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.012430197559297085}, {"id": 4374, "seek": 1897596, "start": 18975.96, "end": 18981.079999999998, "text": " we want to pad it to. So that's what this will do. It will perform these steps that we've already", "tokens": [50364, 321, 528, 281, 6887, 309, 281, 13, 407, 300, 311, 437, 341, 486, 360, 13, 467, 486, 2042, 613, 4439, 300, 321, 600, 1217, 50620], "temperature": 0.0, "avg_logprob": -0.07092625544621395, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.000626322056632489}, {"id": 4375, "seek": 1897596, "start": 18981.079999999998, "end": 18985.559999999998, "text": " talked about. And again, we're just going to assign test data and train data to, you know,", "tokens": [50620, 2825, 466, 13, 400, 797, 11, 321, 434, 445, 516, 281, 6269, 1500, 1412, 293, 3847, 1412, 281, 11, 291, 458, 11, 50844], "temperature": 0.0, "avg_logprob": -0.07092625544621395, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.000626322056632489}, {"id": 4376, "seek": 1897596, "start": 18985.559999999998, "end": 18989.879999999997, "text": " whatever this does for us, we can pass the entire thing, it'll pad all of them for us at once.", "tokens": [50844, 2035, 341, 775, 337, 505, 11, 321, 393, 1320, 264, 2302, 551, 11, 309, 603, 6887, 439, 295, 552, 337, 505, 412, 1564, 13, 51060], "temperature": 0.0, "avg_logprob": -0.07092625544621395, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.000626322056632489}, {"id": 4377, "seek": 1897596, "start": 18990.6, "end": 18996.36, "text": " Okay, so let's run that. And then let's just have a look at say train data one now, because", "tokens": [51096, 1033, 11, 370, 718, 311, 1190, 300, 13, 400, 550, 718, 311, 445, 362, 257, 574, 412, 584, 3847, 1412, 472, 586, 11, 570, 51384], "temperature": 0.0, "avg_logprob": -0.07092625544621395, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.000626322056632489}, {"id": 4378, "seek": 1897596, "start": 18996.36, "end": 19002.2, "text": " remember, this was like 189, right? So if we look at train data, so train underscore data one,", "tokens": [51384, 1604, 11, 341, 390, 411, 2443, 24, 11, 558, 30, 407, 498, 321, 574, 412, 3847, 1412, 11, 370, 3847, 37556, 1412, 472, 11, 51676], "temperature": 0.0, "avg_logprob": -0.07092625544621395, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.000626322056632489}, {"id": 4379, "seek": 1900220, "start": 19003.0, "end": 19009.0, "text": " like that, we can see that as an array with a bunch of zeros before, because that is the padding", "tokens": [50404, 411, 300, 11, 321, 393, 536, 300, 382, 364, 10225, 365, 257, 3840, 295, 35193, 949, 11, 570, 300, 307, 264, 39562, 50704], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4380, "seek": 1900220, "start": 19009.0, "end": 19013.96, "text": " that we've employed to make it the correct length. Okay, so that's padding, that's something that", "tokens": [50704, 300, 321, 600, 20115, 281, 652, 309, 264, 3006, 4641, 13, 1033, 11, 370, 300, 311, 39562, 11, 300, 311, 746, 300, 50952], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4381, "seek": 1900220, "start": 19013.96, "end": 19018.440000000002, "text": " we're probably going to have to do most of the time, when we feed something to our neural networks.", "tokens": [50952, 321, 434, 1391, 516, 281, 362, 281, 360, 881, 295, 264, 565, 11, 562, 321, 3154, 746, 281, 527, 18161, 9590, 13, 51176], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4382, "seek": 1900220, "start": 19018.440000000002, "end": 19021.64, "text": " All right, so the next step is actually to create the model. Now this model is pretty", "tokens": [51176, 1057, 558, 11, 370, 264, 958, 1823, 307, 767, 281, 1884, 264, 2316, 13, 823, 341, 2316, 307, 1238, 51336], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4383, "seek": 1900220, "start": 19021.64, "end": 19027.24, "text": " straightforward. We have an embedding layer and LSTM in a dense layer here. So the reason we've", "tokens": [51336, 15325, 13, 492, 362, 364, 12240, 3584, 4583, 293, 441, 6840, 44, 294, 257, 18011, 4583, 510, 13, 407, 264, 1778, 321, 600, 51616], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4384, "seek": 1900220, "start": 19027.24, "end": 19031.56, "text": " done dense with the activation function of sigmoid at the end is because we're trying to", "tokens": [51616, 1096, 18011, 365, 264, 24433, 2445, 295, 4556, 3280, 327, 412, 264, 917, 307, 570, 321, 434, 1382, 281, 51832], "temperature": 0.0, "avg_logprob": -0.0961626336929646, "compression_ratio": 1.765625, "no_speech_prob": 0.01363587286323309}, {"id": 4385, "seek": 1903156, "start": 19031.56, "end": 19036.760000000002, "text": " pretty much predict the sentiment of this, right? Which means that if we have the sentiment between", "tokens": [50364, 1238, 709, 6069, 264, 16149, 295, 341, 11, 558, 30, 3013, 1355, 300, 498, 321, 362, 264, 16149, 1296, 50624], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4386, "seek": 1903156, "start": 19036.760000000002, "end": 19042.84, "text": " zero and one, then if a number is greater than 0.5, we could classify that as a positive review.", "tokens": [50624, 4018, 293, 472, 11, 550, 498, 257, 1230, 307, 5044, 813, 1958, 13, 20, 11, 321, 727, 33872, 300, 382, 257, 3353, 3131, 13, 50928], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4387, "seek": 1903156, "start": 19042.84, "end": 19046.68, "text": " And if it's less than 0.5 or equal, you know, whatever you want to set the bounds at,", "tokens": [50928, 400, 498, 309, 311, 1570, 813, 1958, 13, 20, 420, 2681, 11, 291, 458, 11, 2035, 291, 528, 281, 992, 264, 29905, 412, 11, 51120], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4388, "seek": 1903156, "start": 19046.68, "end": 19051.24, "text": " then we could say that's a negative review. So sigmoid, as we probably might recall,", "tokens": [51120, 550, 321, 727, 584, 300, 311, 257, 3671, 3131, 13, 407, 4556, 3280, 327, 11, 382, 321, 1391, 1062, 9901, 11, 51348], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4389, "seek": 1903156, "start": 19051.24, "end": 19055.640000000003, "text": " squishes our values between zero and one. So whatever the value is at the end of the network", "tokens": [51348, 2339, 16423, 527, 4190, 1296, 4018, 293, 472, 13, 407, 2035, 264, 2158, 307, 412, 264, 917, 295, 264, 3209, 51568], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4390, "seek": 1903156, "start": 19055.640000000003, "end": 19059.960000000003, "text": " will be between zero and one, which means that, you know, we can make the accurate prediction.", "tokens": [51568, 486, 312, 1296, 4018, 293, 472, 11, 597, 1355, 300, 11, 291, 458, 11, 321, 393, 652, 264, 8559, 17630, 13, 51784], "temperature": 0.0, "avg_logprob": -0.05396426140845239, "compression_ratio": 1.8624161073825503, "no_speech_prob": 0.006903174798935652}, {"id": 4391, "seek": 1905996, "start": 19060.76, "end": 19065.16, "text": " Now here, the reason we have the embedding layer, like, well, we've already pre processed our review", "tokens": [50404, 823, 510, 11, 264, 1778, 321, 362, 264, 12240, 3584, 4583, 11, 411, 11, 731, 11, 321, 600, 1217, 659, 18846, 527, 3131, 50624], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4392, "seek": 1905996, "start": 19065.16, "end": 19069.559999999998, "text": " is even though we've pre processed this with these integers, and they are a bit more meaningful than", "tokens": [50624, 307, 754, 1673, 321, 600, 659, 18846, 341, 365, 613, 41674, 11, 293, 436, 366, 257, 857, 544, 10995, 813, 50844], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4393, "seek": 1905996, "start": 19069.559999999998, "end": 19074.04, "text": " just our random lookup table that we've talked about before, we still want to pass that to an", "tokens": [50844, 445, 527, 4974, 574, 1010, 3199, 300, 321, 600, 2825, 466, 949, 11, 321, 920, 528, 281, 1320, 300, 281, 364, 51068], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4394, "seek": 1905996, "start": 19074.04, "end": 19079.64, "text": " embedding layer, which is going to find a way more meaningful representation for those numbers", "tokens": [51068, 12240, 3584, 4583, 11, 597, 307, 516, 281, 915, 257, 636, 544, 10995, 10290, 337, 729, 3547, 51348], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4395, "seek": 1905996, "start": 19079.64, "end": 19083.719999999998, "text": " than just their integer values already. So it's going to create those vectors for us. And this", "tokens": [51348, 813, 445, 641, 24922, 4190, 1217, 13, 407, 309, 311, 516, 281, 1884, 729, 18875, 337, 505, 13, 400, 341, 51552], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4396, "seek": 1905996, "start": 19083.719999999998, "end": 19089.559999999998, "text": " 32 is denoting the fact that we're going to make the output of every single one of our embeddings", "tokens": [51552, 8858, 307, 1441, 17001, 264, 1186, 300, 321, 434, 516, 281, 652, 264, 5598, 295, 633, 2167, 472, 295, 527, 12240, 29432, 51844], "temperature": 0.0, "avg_logprob": -0.0584007110038813, "compression_ratio": 1.8507936507936509, "no_speech_prob": 0.0012065613409504294}, {"id": 4397, "seek": 1908956, "start": 19089.640000000003, "end": 19095.48, "text": " or vectors that are created 32 dimensions, which means that when we pass them to the LSTM layer,", "tokens": [50368, 420, 18875, 300, 366, 2942, 8858, 12819, 11, 597, 1355, 300, 562, 321, 1320, 552, 281, 264, 441, 6840, 44, 4583, 11, 50660], "temperature": 0.0, "avg_logprob": -0.08327737401743404, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.005219799000769854}, {"id": 4398, "seek": 1908956, "start": 19095.48, "end": 19100.2, "text": " we need to tell the LSTM layer, it's going to have 32 dimensions for every single word,", "tokens": [50660, 321, 643, 281, 980, 264, 441, 6840, 44, 4583, 11, 309, 311, 516, 281, 362, 8858, 12819, 337, 633, 2167, 1349, 11, 50896], "temperature": 0.0, "avg_logprob": -0.08327737401743404, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.005219799000769854}, {"id": 4399, "seek": 1908956, "start": 19100.2, "end": 19104.68, "text": " which is what we're doing. And this will implement that long short term memory process we talked", "tokens": [50896, 597, 307, 437, 321, 434, 884, 13, 400, 341, 486, 4445, 300, 938, 2099, 1433, 4675, 1399, 321, 2825, 51120], "temperature": 0.0, "avg_logprob": -0.08327737401743404, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.005219799000769854}, {"id": 4400, "seek": 1908956, "start": 19104.68, "end": 19112.280000000002, "text": " about before, and output the final output to TF dot cares dot layers dot dense, which will tell us,", "tokens": [51120, 466, 949, 11, 293, 5598, 264, 2572, 5598, 281, 40964, 5893, 12310, 5893, 7914, 5893, 18011, 11, 597, 486, 980, 505, 11, 51500], "temperature": 0.0, "avg_logprob": -0.08327737401743404, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.005219799000769854}, {"id": 4401, "seek": 1908956, "start": 19112.280000000002, "end": 19118.52, "text": " you know, that's what this is, right? It'll make the prediction. So that's what this model is.", "tokens": [51500, 291, 458, 11, 300, 311, 437, 341, 307, 11, 558, 30, 467, 603, 652, 264, 17630, 13, 407, 300, 311, 437, 341, 2316, 307, 13, 51812], "temperature": 0.0, "avg_logprob": -0.08327737401743404, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.005219799000769854}, {"id": 4402, "seek": 1911852, "start": 19119.08, "end": 19123.72, "text": " We can see, give us a second to run here, the model summary, which is already printed out,", "tokens": [50392, 492, 393, 536, 11, 976, 505, 257, 1150, 281, 1190, 510, 11, 264, 2316, 12691, 11, 597, 307, 1217, 13567, 484, 11, 50624], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4403, "seek": 1911852, "start": 19123.72, "end": 19127.56, "text": " we can look at the fact that the embedding layer actually has the most amount of parameters,", "tokens": [50624, 321, 393, 574, 412, 264, 1186, 300, 264, 12240, 3584, 4583, 767, 575, 264, 881, 2372, 295, 9834, 11, 50816], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4404, "seek": 1911852, "start": 19127.56, "end": 19131.0, "text": " because essentially, it's trying to figure out, you know, all these different numbers,", "tokens": [50816, 570, 4476, 11, 309, 311, 1382, 281, 2573, 484, 11, 291, 458, 11, 439, 613, 819, 3547, 11, 50988], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4405, "seek": 1911852, "start": 19131.0, "end": 19136.12, "text": " how can we convert that into a tensor of 32 dimensions, which is not that easy to do. And", "tokens": [50988, 577, 393, 321, 7620, 300, 666, 257, 40863, 295, 8858, 12819, 11, 597, 307, 406, 300, 1858, 281, 360, 13, 400, 51244], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4406, "seek": 1911852, "start": 19136.12, "end": 19140.2, "text": " this is going to be the major aspect that's being trained. And then we have our LSTM layer,", "tokens": [51244, 341, 307, 516, 281, 312, 264, 2563, 4171, 300, 311, 885, 8895, 13, 400, 550, 321, 362, 527, 441, 6840, 44, 4583, 11, 51448], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4407, "seek": 1911852, "start": 19140.2, "end": 19144.920000000002, "text": " we can see the parameters there. And our final dense layer, which is eight getting 33 parameters,", "tokens": [51448, 321, 393, 536, 264, 9834, 456, 13, 400, 527, 2572, 18011, 4583, 11, 597, 307, 3180, 1242, 11816, 9834, 11, 51684], "temperature": 0.0, "avg_logprob": -0.048472265257452525, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.006097319535911083}, {"id": 4408, "seek": 1914492, "start": 19144.92, "end": 19150.679999999997, "text": " that's because the output from every single one of these dimensions 32 plus a bias node, right,", "tokens": [50364, 300, 311, 570, 264, 5598, 490, 633, 2167, 472, 295, 613, 12819, 8858, 1804, 257, 12577, 9984, 11, 558, 11, 50652], "temperature": 0.0, "avg_logprob": -0.11630121072133383, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0032728996593505144}, {"id": 4409, "seek": 1914492, "start": 19150.679999999997, "end": 19154.44, "text": " that we need. So that's what we'll get there. You can see model dot summary.", "tokens": [50652, 300, 321, 643, 13, 407, 300, 311, 437, 321, 603, 483, 456, 13, 509, 393, 536, 2316, 5893, 12691, 13, 50840], "temperature": 0.0, "avg_logprob": -0.11630121072133383, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0032728996593505144}, {"id": 4410, "seek": 1914492, "start": 19155.559999999998, "end": 19160.039999999997, "text": " We get the sequential model. Okay, so training. Alright, so now it's time to compile and train", "tokens": [50896, 492, 483, 264, 42881, 2316, 13, 1033, 11, 370, 3097, 13, 2798, 11, 370, 586, 309, 311, 565, 281, 31413, 293, 3847, 51120], "temperature": 0.0, "avg_logprob": -0.11630121072133383, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0032728996593505144}, {"id": 4411, "seek": 1914492, "start": 19160.039999999997, "end": 19164.12, "text": " the model, you can see I've already trained mine. What I'm going to say here is if you want to speed", "tokens": [51120, 264, 2316, 11, 291, 393, 536, 286, 600, 1217, 8895, 3892, 13, 708, 286, 478, 516, 281, 584, 510, 307, 498, 291, 528, 281, 3073, 51324], "temperature": 0.0, "avg_logprob": -0.11630121072133383, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0032728996593505144}, {"id": 4412, "seek": 1914492, "start": 19164.12, "end": 19167.48, "text": " up your training, because this will actually take a second, and we'll talk about why we pick these", "tokens": [51324, 493, 428, 3097, 11, 570, 341, 486, 767, 747, 257, 1150, 11, 293, 321, 603, 751, 466, 983, 321, 1888, 613, 51492], "temperature": 0.0, "avg_logprob": -0.11630121072133383, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0032728996593505144}, {"id": 4413, "seek": 1916748, "start": 19167.48, "end": 19175.64, "text": " things in a minute is go to runtime, change runtime type, and add a hardware accelerator of GPU.", "tokens": [50364, 721, 294, 257, 3456, 307, 352, 281, 34474, 11, 1319, 34474, 2010, 11, 293, 909, 257, 8837, 39889, 295, 18407, 13, 50772], "temperature": 0.0, "avg_logprob": -0.08773176773734714, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.05500103160738945}, {"id": 4414, "seek": 1916748, "start": 19176.36, "end": 19180.28, "text": " What this will allow you to do is utilize a GPU while you're training, which should speed up your", "tokens": [50808, 708, 341, 486, 2089, 291, 281, 360, 307, 16117, 257, 18407, 1339, 291, 434, 3097, 11, 597, 820, 3073, 493, 428, 51004], "temperature": 0.0, "avg_logprob": -0.08773176773734714, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.05500103160738945}, {"id": 4415, "seek": 1916748, "start": 19180.28, "end": 19185.56, "text": " training by about 10 to 20 times. So I probably should have mentioned that beforehand. But you", "tokens": [51004, 3097, 538, 466, 1266, 281, 945, 1413, 13, 407, 286, 1391, 820, 362, 2835, 300, 22893, 13, 583, 291, 51268], "temperature": 0.0, "avg_logprob": -0.08773176773734714, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.05500103160738945}, {"id": 4416, "seek": 1916748, "start": 19185.56, "end": 19191.72, "text": " can do that. And please do for these examples. So model dot compile. Alright, so we're compiling", "tokens": [51268, 393, 360, 300, 13, 400, 1767, 360, 337, 613, 5110, 13, 407, 2316, 5893, 31413, 13, 2798, 11, 370, 321, 434, 715, 4883, 51576], "temperature": 0.0, "avg_logprob": -0.08773176773734714, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.05500103160738945}, {"id": 4417, "seek": 1916748, "start": 19191.72, "end": 19196.28, "text": " our model, we're picking the loss function as binary cross entropy. The reason we're picking", "tokens": [51576, 527, 2316, 11, 321, 434, 8867, 264, 4470, 2445, 382, 17434, 3278, 30867, 13, 440, 1778, 321, 434, 8867, 51804], "temperature": 0.0, "avg_logprob": -0.08773176773734714, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.05500103160738945}, {"id": 4418, "seek": 1919628, "start": 19196.28, "end": 19200.92, "text": " this is because this is going to essentially tell us how far away we are from the correct", "tokens": [50364, 341, 307, 570, 341, 307, 516, 281, 4476, 980, 505, 577, 1400, 1314, 321, 366, 490, 264, 3006, 50596], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4419, "seek": 1919628, "start": 19201.719999999998, "end": 19206.039999999997, "text": " probability, right, because we have two different things we could be predicting. So you know, either", "tokens": [50636, 8482, 11, 558, 11, 570, 321, 362, 732, 819, 721, 321, 727, 312, 32884, 13, 407, 291, 458, 11, 2139, 50852], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4420, "seek": 1919628, "start": 19206.039999999997, "end": 19212.199999999997, "text": " zero or one, so positive or negative. So this will give us a correct loss for that kind of", "tokens": [50852, 4018, 420, 472, 11, 370, 3353, 420, 3671, 13, 407, 341, 486, 976, 505, 257, 3006, 4470, 337, 300, 733, 295, 51160], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4421, "seek": 1919628, "start": 19212.199999999997, "end": 19216.36, "text": " problem that we've talked about before. The optimizer, we're going to use rms prop. Again,", "tokens": [51160, 1154, 300, 321, 600, 2825, 466, 949, 13, 440, 5028, 6545, 11, 321, 434, 516, 281, 764, 367, 2592, 2365, 13, 3764, 11, 51368], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4422, "seek": 1919628, "start": 19216.36, "end": 19219.48, "text": " I'm not going to discuss all the different optimizers, you can look them up if you care", "tokens": [51368, 286, 478, 406, 516, 281, 2248, 439, 264, 819, 5028, 22525, 11, 291, 393, 574, 552, 493, 498, 291, 1127, 51524], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4423, "seek": 1919628, "start": 19219.48, "end": 19224.28, "text": " that much about what they do. And we're going to use metrics as ACC. One thing I will say is", "tokens": [51524, 300, 709, 466, 437, 436, 360, 13, 400, 321, 434, 516, 281, 764, 16367, 382, 42251, 13, 1485, 551, 286, 486, 584, 307, 51764], "temperature": 0.0, "avg_logprob": -0.08294384085017142, "compression_ratio": 1.7444794952681388, "no_speech_prob": 0.010651499964296818}, {"id": 4424, "seek": 1922428, "start": 19224.28, "end": 19228.92, "text": " the optimizer is not crazy important. For this one, you could use Adam if you wanted to, and it", "tokens": [50364, 264, 5028, 6545, 307, 406, 3219, 1021, 13, 1171, 341, 472, 11, 291, 727, 764, 7938, 498, 291, 1415, 281, 11, 293, 309, 50596], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4425, "seek": 1922428, "start": 19228.92, "end": 19234.039999999997, "text": " would still work fine. My usual go to is just use the atom optimizer unless you think there's a better", "tokens": [50596, 576, 920, 589, 2489, 13, 1222, 7713, 352, 281, 307, 445, 764, 264, 12018, 5028, 6545, 5969, 291, 519, 456, 311, 257, 1101, 50852], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4426, "seek": 1922428, "start": 19234.039999999997, "end": 19238.52, "text": " one to use. But anyways, that's something to mention. Okay, so finally, we will fit the model,", "tokens": [50852, 472, 281, 764, 13, 583, 13448, 11, 300, 311, 746, 281, 2152, 13, 1033, 11, 370, 2721, 11, 321, 486, 3318, 264, 2316, 11, 51076], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4427, "seek": 1922428, "start": 19238.52, "end": 19242.44, "text": " we've looked at the syntax a lot before. So model that fit, we'll give the training data,", "tokens": [51076, 321, 600, 2956, 412, 264, 28431, 257, 688, 949, 13, 407, 2316, 300, 3318, 11, 321, 603, 976, 264, 3097, 1412, 11, 51272], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4428, "seek": 1922428, "start": 19242.44, "end": 19247.719999999998, "text": " the training labels, the epochs, and we'll do a validation split of 20%. So that's what 0.2 stands", "tokens": [51272, 264, 3097, 16949, 11, 264, 30992, 28346, 11, 293, 321, 603, 360, 257, 24071, 7472, 295, 945, 6856, 407, 300, 311, 437, 1958, 13, 17, 7382, 51536], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4429, "seek": 1922428, "start": 19247.719999999998, "end": 19253.16, "text": " for, which means that what we're going to be doing is using 20% of the training data to actually", "tokens": [51536, 337, 11, 597, 1355, 300, 437, 321, 434, 516, 281, 312, 884, 307, 1228, 945, 4, 295, 264, 3097, 1412, 281, 767, 51808], "temperature": 0.0, "avg_logprob": -0.07531771721777977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0064878277480602264}, {"id": 4430, "seek": 1925316, "start": 19253.16, "end": 19257.88, "text": " evaluate and validate the model as we go through. And we can see that after training, which I've", "tokens": [50364, 13059, 293, 29562, 264, 2316, 382, 321, 352, 807, 13, 400, 321, 393, 536, 300, 934, 3097, 11, 597, 286, 600, 50600], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4431, "seek": 1925316, "start": 19257.88, "end": 19261.88, "text": " already done, and you guys are welcome to obviously do on your own computer, we kind of stall at an", "tokens": [50600, 1217, 1096, 11, 293, 291, 1074, 366, 2928, 281, 2745, 360, 322, 428, 1065, 3820, 11, 321, 733, 295, 19633, 412, 364, 50800], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4432, "seek": 1925316, "start": 19261.88, "end": 19268.44, "text": " evaluation accuracy of about 88%. Whereas the model actually gets overfit to about 97 98%.", "tokens": [50800, 13344, 14170, 295, 466, 24587, 6856, 13813, 264, 2316, 767, 2170, 670, 6845, 281, 466, 23399, 20860, 6856, 51128], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4433, "seek": 1925316, "start": 19269.32, "end": 19273.4, "text": " So what this is telling us essentially is that we don't have enough training data, and that", "tokens": [51172, 407, 437, 341, 307, 3585, 505, 4476, 307, 300, 321, 500, 380, 362, 1547, 3097, 1412, 11, 293, 300, 51376], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4434, "seek": 1925316, "start": 19273.4, "end": 19278.12, "text": " after we've even done just one epoch, we're pretty much stuck on the same validation accuracy,", "tokens": [51376, 934, 321, 600, 754, 1096, 445, 472, 30992, 339, 11, 321, 434, 1238, 709, 5541, 322, 264, 912, 24071, 14170, 11, 51612], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4435, "seek": 1925316, "start": 19278.12, "end": 19281.48, "text": " and that there's something that needs to change in the model to make it better. But for now,", "tokens": [51612, 293, 300, 456, 311, 746, 300, 2203, 281, 1319, 294, 264, 2316, 281, 652, 309, 1101, 13, 583, 337, 586, 11, 51780], "temperature": 0.0, "avg_logprob": -0.08575823727776022, "compression_ratio": 1.728658536585366, "no_speech_prob": 0.004609300754964352}, {"id": 4436, "seek": 1928148, "start": 19281.48, "end": 19285.72, "text": " that's fine, we'll leave it the way that it is. Okay, so now we can look at the results. I've", "tokens": [50364, 300, 311, 2489, 11, 321, 603, 1856, 309, 264, 636, 300, 309, 307, 13, 1033, 11, 370, 586, 321, 393, 574, 412, 264, 3542, 13, 286, 600, 50576], "temperature": 0.0, "avg_logprob": -0.07008153609647096, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0049051097594201565}, {"id": 4437, "seek": 1928148, "start": 19285.72, "end": 19290.76, "text": " already did the results here, just to again, speed up some time, but we'll do the evaluation on our", "tokens": [50576, 1217, 630, 264, 3542, 510, 11, 445, 281, 797, 11, 3073, 493, 512, 565, 11, 457, 321, 603, 360, 264, 13344, 322, 527, 50828], "temperature": 0.0, "avg_logprob": -0.07008153609647096, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0049051097594201565}, {"id": 4438, "seek": 1928148, "start": 19290.76, "end": 19296.44, "text": " test data and test labels to get a more accurate kind of result here. And that tells us we have", "tokens": [50828, 1500, 1412, 293, 1500, 16949, 281, 483, 257, 544, 8559, 733, 295, 1874, 510, 13, 400, 300, 5112, 505, 321, 362, 51112], "temperature": 0.0, "avg_logprob": -0.07008153609647096, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0049051097594201565}, {"id": 4439, "seek": 1928148, "start": 19296.44, "end": 19302.6, "text": " an accuracy of about 85.5%, which you know, isn't great, but it's decent considering that we didn't", "tokens": [51112, 364, 14170, 295, 466, 14695, 13, 20, 8923, 597, 291, 458, 11, 1943, 380, 869, 11, 457, 309, 311, 8681, 8079, 300, 321, 994, 380, 51420], "temperature": 0.0, "avg_logprob": -0.07008153609647096, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0049051097594201565}, {"id": 4440, "seek": 1928148, "start": 19302.6, "end": 19307.239999999998, "text": " really write that much code to get to the point that we're at right now. Okay, so that's what", "tokens": [51420, 534, 2464, 300, 709, 3089, 281, 483, 281, 264, 935, 300, 321, 434, 412, 558, 586, 13, 1033, 11, 370, 300, 311, 437, 51652], "temperature": 0.0, "avg_logprob": -0.07008153609647096, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0049051097594201565}, {"id": 4441, "seek": 1930724, "start": 19307.24, "end": 19310.920000000002, "text": " we're getting. The model has been trained. Again, it's not too complicated. And now we're", "tokens": [50364, 321, 434, 1242, 13, 440, 2316, 575, 668, 8895, 13, 3764, 11, 309, 311, 406, 886, 6179, 13, 400, 586, 321, 434, 50548], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4442, "seek": 1930724, "start": 19310.920000000002, "end": 19316.04, "text": " on to making predictions. So the idea is that now we've trained our model, and we want to actually", "tokens": [50548, 322, 281, 1455, 21264, 13, 407, 264, 1558, 307, 300, 586, 321, 600, 8895, 527, 2316, 11, 293, 321, 528, 281, 767, 50804], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4443, "seek": 1930724, "start": 19316.04, "end": 19322.280000000002, "text": " use it to make a prediction on some kind of movie review. So since our data was pre processed, when", "tokens": [50804, 764, 309, 281, 652, 257, 17630, 322, 512, 733, 295, 3169, 3131, 13, 407, 1670, 527, 1412, 390, 659, 18846, 11, 562, 51116], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4444, "seek": 1930724, "start": 19322.280000000002, "end": 19327.16, "text": " we gave it to the model, that means we actually need to process anything we want to make a prediction", "tokens": [51116, 321, 2729, 309, 281, 264, 2316, 11, 300, 1355, 321, 767, 643, 281, 1399, 1340, 321, 528, 281, 652, 257, 17630, 51360], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4445, "seek": 1930724, "start": 19327.16, "end": 19331.72, "text": " on in the exact same way, we need to use the same lookup table, we need to encode it, you know,", "tokens": [51360, 322, 294, 264, 1900, 912, 636, 11, 321, 643, 281, 764, 264, 912, 574, 1010, 3199, 11, 321, 643, 281, 2058, 1429, 309, 11, 291, 458, 11, 51588], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4446, "seek": 1930724, "start": 19331.72, "end": 19336.280000000002, "text": " precisely the same. Otherwise, when we give it to the model, it's going to think that the words", "tokens": [51588, 13402, 264, 912, 13, 10328, 11, 562, 321, 976, 309, 281, 264, 2316, 11, 309, 311, 516, 281, 519, 300, 264, 2283, 51816], "temperature": 0.0, "avg_logprob": -0.06419201532999674, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.07806677371263504}, {"id": 4447, "seek": 1933628, "start": 19336.28, "end": 19340.76, "text": " are different, and it's not going to make an accurate prediction. So what I've done here is", "tokens": [50364, 366, 819, 11, 293, 309, 311, 406, 516, 281, 652, 364, 8559, 17630, 13, 407, 437, 286, 600, 1096, 510, 307, 50588], "temperature": 0.0, "avg_logprob": -0.12208959611795717, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.0028007642831653357}, {"id": 4448, "seek": 1933628, "start": 19340.76, "end": 19347.96, "text": " I've made a function that will encode any text into what do you call the proper pre processed", "tokens": [50588, 286, 600, 1027, 257, 2445, 300, 486, 2058, 1429, 604, 2487, 666, 437, 360, 291, 818, 264, 2296, 659, 18846, 50948], "temperature": 0.0, "avg_logprob": -0.12208959611795717, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.0028007642831653357}, {"id": 4449, "seek": 1933628, "start": 19347.96, "end": 19353.0, "text": " kind of integers, right, just like our training data was pre processed. That's what this function", "tokens": [50948, 733, 295, 41674, 11, 558, 11, 445, 411, 527, 3097, 1412, 390, 659, 18846, 13, 663, 311, 437, 341, 2445, 51200], "temperature": 0.0, "avg_logprob": -0.12208959611795717, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.0028007642831653357}, {"id": 4450, "seek": 1933628, "start": 19353.0, "end": 19357.0, "text": " is going to do for us is pre processed some line of text. So what I've done is actually", "tokens": [51200, 307, 516, 281, 360, 337, 505, 307, 659, 18846, 512, 1622, 295, 2487, 13, 407, 437, 286, 600, 1096, 307, 767, 51400], "temperature": 0.0, "avg_logprob": -0.12208959611795717, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.0028007642831653357}, {"id": 4451, "seek": 1933628, "start": 19357.0, "end": 19365.16, "text": " gotten the lookup table. So essentially, the mappings from IBM, I be IMDB, I could read that", "tokens": [51400, 5768, 264, 574, 1010, 3199, 13, 407, 4476, 11, 264, 463, 28968, 490, 23487, 11, 286, 312, 21463, 27735, 11, 286, 727, 1401, 300, 51808], "temperature": 0.0, "avg_logprob": -0.12208959611795717, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.0028007642831653357}, {"id": 4452, "seek": 1936516, "start": 19365.16, "end": 19369.8, "text": " properly. From that data set that we loaded earlier. So let me go see if I can find where I", "tokens": [50364, 6108, 13, 3358, 300, 1412, 992, 300, 321, 13210, 3071, 13, 407, 718, 385, 352, 536, 498, 286, 393, 915, 689, 286, 50596], "temperature": 0.0, "avg_logprob": -0.12019467535819717, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.022282162681221962}, {"id": 4453, "seek": 1936516, "start": 19369.8, "end": 19376.36, "text": " defined IMDB, you can see up here. So keras dot data sets import IMDB, just like we loaded it in,", "tokens": [50596, 7642, 21463, 27735, 11, 291, 393, 536, 493, 510, 13, 407, 350, 6985, 5893, 1412, 6352, 974, 21463, 27735, 11, 445, 411, 321, 13210, 309, 294, 11, 50924], "temperature": 0.0, "avg_logprob": -0.12019467535819717, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.022282162681221962}, {"id": 4454, "seek": 1936516, "start": 19376.36, "end": 19381.16, "text": " we can also actually get all of the word indexes or that map, we can actually print this out if", "tokens": [50924, 321, 393, 611, 767, 483, 439, 295, 264, 1349, 8186, 279, 420, 300, 4471, 11, 321, 393, 767, 4482, 341, 484, 498, 51164], "temperature": 0.0, "avg_logprob": -0.12019467535819717, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.022282162681221962}, {"id": 4455, "seek": 1936516, "start": 19381.16, "end": 19385.72, "text": " we want to look at what it is after. But anyways, we have that mapping, which means that all we need", "tokens": [51164, 321, 528, 281, 574, 412, 437, 309, 307, 934, 13, 583, 13448, 11, 321, 362, 300, 18350, 11, 597, 1355, 300, 439, 321, 643, 51392], "temperature": 0.0, "avg_logprob": -0.12019467535819717, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.022282162681221962}, {"id": 4456, "seek": 1936516, "start": 19385.72, "end": 19393.48, "text": " to do is keras dot preprocessing dot text dot text, two word sequence, what this means is give", "tokens": [51392, 281, 360, 307, 350, 6985, 5893, 2666, 340, 780, 278, 5893, 2487, 5893, 2487, 11, 732, 1349, 8310, 11, 437, 341, 1355, 307, 976, 51780], "temperature": 0.0, "avg_logprob": -0.12019467535819717, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.022282162681221962}, {"id": 4457, "seek": 1939348, "start": 19393.72, "end": 19399.0, "text": " in some text convert all of that text into what we call tokens, which are just the individual", "tokens": [50376, 294, 512, 2487, 7620, 439, 295, 300, 2487, 666, 437, 321, 818, 22667, 11, 597, 366, 445, 264, 2609, 50640], "temperature": 0.0, "avg_logprob": -0.09413246355558696, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01640111766755581}, {"id": 4458, "seek": 1939348, "start": 19399.0, "end": 19403.8, "text": " words themselves. And then what we're going to do is just use a kind of for loop inside of here", "tokens": [50640, 2283, 2969, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 445, 764, 257, 733, 295, 337, 6367, 1854, 295, 510, 50880], "temperature": 0.0, "avg_logprob": -0.09413246355558696, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01640111766755581}, {"id": 4459, "seek": 1939348, "start": 19403.8, "end": 19410.6, "text": " that says word index at word, if word in word index, L zero for word in tokens. Now what this", "tokens": [50880, 300, 1619, 1349, 8186, 412, 1349, 11, 498, 1349, 294, 1349, 8186, 11, 441, 4018, 337, 1349, 294, 22667, 13, 823, 437, 341, 51220], "temperature": 0.0, "avg_logprob": -0.09413246355558696, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01640111766755581}, {"id": 4460, "seek": 1939348, "start": 19410.6, "end": 19418.04, "text": " means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary", "tokens": [51220, 1355, 307, 4476, 498, 264, 1349, 300, 311, 294, 613, 22667, 586, 307, 294, 527, 18350, 13, 407, 294, 300, 19864, 51592], "temperature": 0.0, "avg_logprob": -0.09413246355558696, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01640111766755581}, {"id": 4461, "seek": 1941804, "start": 19418.04, "end": 19424.04, "text": " of 88,000 words, then what we'll do is replace its location in the list with that specific word,", "tokens": [50364, 295, 24587, 11, 1360, 2283, 11, 550, 437, 321, 603, 360, 307, 7406, 1080, 4914, 294, 264, 1329, 365, 300, 2685, 1349, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07318665688497979, "compression_ratio": 1.8392156862745097, "no_speech_prob": 0.21202175319194794}, {"id": 4462, "seek": 1941804, "start": 19424.760000000002, "end": 19429.56, "text": " or with that specific integer that represents it, otherwise we'll put zero just to stand for,", "tokens": [50700, 420, 365, 300, 2685, 24922, 300, 8855, 309, 11, 5911, 321, 603, 829, 4018, 445, 281, 1463, 337, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07318665688497979, "compression_ratio": 1.8392156862745097, "no_speech_prob": 0.21202175319194794}, {"id": 4463, "seek": 1941804, "start": 19429.56, "end": 19434.84, "text": " you know, we don't know what this character is. And then what we'll do is return sequence dot pad", "tokens": [50940, 291, 458, 11, 321, 500, 380, 458, 437, 341, 2517, 307, 13, 400, 550, 437, 321, 603, 360, 307, 2736, 8310, 5893, 6887, 51204], "temperature": 0.0, "avg_logprob": -0.07318665688497979, "compression_ratio": 1.8392156862745097, "no_speech_prob": 0.21202175319194794}, {"id": 4464, "seek": 1941804, "start": 19434.84, "end": 19441.16, "text": " sequences, and we'll pad this token sequence, and just return actually the first index here.", "tokens": [51204, 22978, 11, 293, 321, 603, 6887, 341, 14862, 8310, 11, 293, 445, 2736, 767, 264, 700, 8186, 510, 13, 51520], "temperature": 0.0, "avg_logprob": -0.07318665688497979, "compression_ratio": 1.8392156862745097, "no_speech_prob": 0.21202175319194794}, {"id": 4465, "seek": 1941804, "start": 19441.16, "end": 19446.2, "text": " The reason we're doing that is because this pad sequences works on a list of sequences,", "tokens": [51520, 440, 1778, 321, 434, 884, 300, 307, 570, 341, 6887, 22978, 1985, 322, 257, 1329, 295, 22978, 11, 51772], "temperature": 0.0, "avg_logprob": -0.07318665688497979, "compression_ratio": 1.8392156862745097, "no_speech_prob": 0.21202175319194794}, {"id": 4466, "seek": 1944620, "start": 19446.280000000002, "end": 19450.760000000002, "text": " so multiple sequences. So we need to put this inside a list, which means that this is going", "tokens": [50368, 370, 3866, 22978, 13, 407, 321, 643, 281, 829, 341, 1854, 257, 1329, 11, 597, 1355, 300, 341, 307, 516, 50592], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4467, "seek": 1944620, "start": 19450.760000000002, "end": 19455.48, "text": " to return to us a list of lists. So we just obviously want the first entry, because we only", "tokens": [50592, 281, 2736, 281, 505, 257, 1329, 295, 14511, 13, 407, 321, 445, 2745, 528, 264, 700, 8729, 11, 570, 321, 787, 50828], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4468, "seek": 1944620, "start": 19455.48, "end": 19460.12, "text": " want, you know, that one sequence that we padded. So that's how this works. Sorry, that's a bit of", "tokens": [50828, 528, 11, 291, 458, 11, 300, 472, 8310, 300, 321, 6887, 9207, 13, 407, 300, 311, 577, 341, 1985, 13, 4919, 11, 300, 311, 257, 857, 295, 51060], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4469, "seek": 1944620, "start": 19460.12, "end": 19463.48, "text": " a mouthful to explain, but you guys can run through and print the stuff out if you want to see how", "tokens": [51060, 257, 4525, 906, 281, 2903, 11, 457, 291, 1074, 393, 1190, 807, 293, 4482, 264, 1507, 484, 498, 291, 528, 281, 536, 577, 51228], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4470, "seek": 1944620, "start": 19463.48, "end": 19467.96, "text": " all of it works specifically. But yeah, so we can run this cell and have a look at what this", "tokens": [51228, 439, 295, 309, 1985, 4682, 13, 583, 1338, 11, 370, 321, 393, 1190, 341, 2815, 293, 362, 257, 574, 412, 437, 341, 51452], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4471, "seek": 1944620, "start": 19467.96, "end": 19472.600000000002, "text": " actually does for us on some sample text. So that maybe was just amazing. So amazing, we can see", "tokens": [51452, 767, 775, 337, 505, 322, 512, 6889, 2487, 13, 407, 300, 1310, 390, 445, 2243, 13, 407, 2243, 11, 321, 393, 536, 51684], "temperature": 0.0, "avg_logprob": -0.06599863423597092, "compression_ratio": 1.8242811501597445, "no_speech_prob": 0.010012696497142315}, {"id": 4472, "seek": 1947260, "start": 19472.6, "end": 19476.84, "text": " we get the output that we were kind of expecting. So integer encoded words down here, and then a", "tokens": [50364, 321, 483, 264, 5598, 300, 321, 645, 733, 295, 9650, 13, 407, 24922, 2058, 12340, 2283, 760, 510, 11, 293, 550, 257, 50576], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4473, "seek": 1947260, "start": 19476.84, "end": 19481.559999999998, "text": " bunch of zeros just for all the padding. Now, while we're at it, I decided why not we why don't we", "tokens": [50576, 3840, 295, 35193, 445, 337, 439, 264, 39562, 13, 823, 11, 1339, 321, 434, 412, 309, 11, 286, 3047, 983, 406, 321, 983, 500, 380, 321, 50812], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4474, "seek": 1947260, "start": 19481.559999999998, "end": 19486.359999999997, "text": " make a decode function so that if we have any movie review like this, that's in the integer", "tokens": [50812, 652, 257, 979, 1429, 2445, 370, 300, 498, 321, 362, 604, 3169, 3131, 411, 341, 11, 300, 311, 294, 264, 24922, 51052], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4475, "seek": 1947260, "start": 19486.359999999997, "end": 19490.84, "text": " form, we can decode that into the text value. So the way we're going to do that is start by", "tokens": [51052, 1254, 11, 321, 393, 979, 1429, 300, 666, 264, 2487, 2158, 13, 407, 264, 636, 321, 434, 516, 281, 360, 300, 307, 722, 538, 51276], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4476, "seek": 1947260, "start": 19490.84, "end": 19495.879999999997, "text": " reversing the word index that we just created. Now the reason for that is the word index we", "tokens": [51276, 14582, 278, 264, 1349, 8186, 300, 321, 445, 2942, 13, 823, 264, 1778, 337, 300, 307, 264, 1349, 8186, 321, 51528], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4477, "seek": 1947260, "start": 19495.879999999997, "end": 19501.719999999998, "text": " looked at, which is this right, goes from word to integer. But we actually now want to go from", "tokens": [51528, 2956, 412, 11, 597, 307, 341, 558, 11, 1709, 490, 1349, 281, 24922, 13, 583, 321, 767, 586, 528, 281, 352, 490, 51820], "temperature": 0.0, "avg_logprob": -0.0729527889482127, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.017440855503082275}, {"id": 4478, "seek": 1950172, "start": 19501.72, "end": 19506.52, "text": " integer to word so that we can actually translate a sentence, right? So what I've done is made this", "tokens": [50364, 24922, 281, 1349, 370, 300, 321, 393, 767, 13799, 257, 8174, 11, 558, 30, 407, 437, 286, 600, 1096, 307, 1027, 341, 50604], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4479, "seek": 1950172, "start": 19506.52, "end": 19511.88, "text": " decode integers function, we've set the padding key as zero, which means that if we see zero,", "tokens": [50604, 979, 1429, 41674, 2445, 11, 321, 600, 992, 264, 39562, 2141, 382, 4018, 11, 597, 1355, 300, 498, 321, 536, 4018, 11, 50872], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4480, "seek": 1950172, "start": 19511.88, "end": 19516.04, "text": " that's really just means you know, nothing's there. We're going to create a text string,", "tokens": [50872, 300, 311, 534, 445, 1355, 291, 458, 11, 1825, 311, 456, 13, 492, 434, 516, 281, 1884, 257, 2487, 6798, 11, 51080], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4481, "seek": 1950172, "start": 19516.04, "end": 19521.0, "text": " which we're going to add to. And I'm just gonna say for num in integers, integers is our input,", "tokens": [51080, 597, 321, 434, 516, 281, 909, 281, 13, 400, 286, 478, 445, 799, 584, 337, 1031, 294, 41674, 11, 41674, 307, 527, 4846, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4482, "seek": 1950172, "start": 19521.0, "end": 19525.24, "text": " which will be a list that looks something like this or an array, whatever you want to call it,", "tokens": [51328, 597, 486, 312, 257, 1329, 300, 1542, 746, 411, 341, 420, 364, 10225, 11, 2035, 291, 528, 281, 818, 309, 11, 51540], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4483, "seek": 1950172, "start": 19525.24, "end": 19529.48, "text": " we're gonna say if number does not equal pad. So essentially, if the number is not zero, right,", "tokens": [51540, 321, 434, 799, 584, 498, 1230, 775, 406, 2681, 6887, 13, 407, 4476, 11, 498, 264, 1230, 307, 406, 4018, 11, 558, 11, 51752], "temperature": 0.0, "avg_logprob": -0.08847945725836721, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.00263152364641428}, {"id": 4484, "seek": 1952948, "start": 19529.48, "end": 19534.84, "text": " it's not padding, then what we'll do is add the lookup of reverse word index num. So whatever", "tokens": [50364, 309, 311, 406, 39562, 11, 550, 437, 321, 603, 360, 307, 909, 264, 574, 1010, 295, 9943, 1349, 8186, 1031, 13, 407, 2035, 50632], "temperature": 0.0, "avg_logprob": -0.09057747604500534, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.009707708843052387}, {"id": 4485, "seek": 1952948, "start": 19534.84, "end": 19541.079999999998, "text": " that number is, into this new string plus a space, and then just return text colon negative one,", "tokens": [50632, 300, 1230, 307, 11, 666, 341, 777, 6798, 1804, 257, 1901, 11, 293, 550, 445, 2736, 2487, 8255, 3671, 472, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09057747604500534, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.009707708843052387}, {"id": 4486, "seek": 1952948, "start": 19541.079999999998, "end": 19545.0, "text": " which means return everything except the last space that we would have added. And then if I", "tokens": [50944, 597, 1355, 2736, 1203, 3993, 264, 1036, 1901, 300, 321, 576, 362, 3869, 13, 400, 550, 498, 286, 51140], "temperature": 0.0, "avg_logprob": -0.09057747604500534, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.009707708843052387}, {"id": 4487, "seek": 1952948, "start": 19545.0, "end": 19552.28, "text": " print the decode integers, we can see that this encoded thing that we have before, which looks", "tokens": [51140, 4482, 264, 979, 1429, 41674, 11, 321, 393, 536, 300, 341, 2058, 12340, 551, 300, 321, 362, 949, 11, 597, 1542, 51504], "temperature": 0.0, "avg_logprob": -0.09057747604500534, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.009707708843052387}, {"id": 4488, "seek": 1952948, "start": 19552.28, "end": 19557.88, "text": " like this gets encoded by the string that movie was just amazing. So maybe sorry, not encoded,", "tokens": [51504, 411, 341, 2170, 2058, 12340, 538, 264, 6798, 300, 3169, 390, 445, 2243, 13, 407, 1310, 2597, 11, 406, 2058, 12340, 11, 51784], "temperature": 0.0, "avg_logprob": -0.09057747604500534, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.009707708843052387}, {"id": 4489, "seek": 1955788, "start": 19557.88, "end": 19563.0, "text": " decoded, because this was the encoded form. So that's how that works. Okay, so now it's", "tokens": [50364, 979, 12340, 11, 570, 341, 390, 264, 2058, 12340, 1254, 13, 407, 300, 311, 577, 300, 1985, 13, 1033, 11, 370, 586, 309, 311, 50620], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4490, "seek": 1955788, "start": 19563.0, "end": 19567.4, "text": " time to actually make a prediction. So I've written a function here that will make a prediction on", "tokens": [50620, 565, 281, 767, 652, 257, 17630, 13, 407, 286, 600, 3720, 257, 2445, 510, 300, 486, 652, 257, 17630, 322, 50840], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4491, "seek": 1955788, "start": 19567.4, "end": 19572.2, "text": " some piece of text as the movie review for us. And I'll just walk us through quickly how this", "tokens": [50840, 512, 2522, 295, 2487, 382, 264, 3169, 3131, 337, 505, 13, 400, 286, 603, 445, 1792, 505, 807, 2661, 577, 341, 51080], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4492, "seek": 1955788, "start": 19572.2, "end": 19576.36, "text": " works. And then I'll show us the actual output from our model, you know, making predictions like", "tokens": [51080, 1985, 13, 400, 550, 286, 603, 855, 505, 264, 3539, 5598, 490, 527, 2316, 11, 291, 458, 11, 1455, 21264, 411, 51288], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4493, "seek": 1955788, "start": 19576.36, "end": 19581.64, "text": " this. So what we say is we'll take some parameter text, which will be our movie review. And we're", "tokens": [51288, 341, 13, 407, 437, 321, 584, 307, 321, 603, 747, 512, 13075, 2487, 11, 597, 486, 312, 527, 3169, 3131, 13, 400, 321, 434, 51552], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4494, "seek": 1955788, "start": 19581.64, "end": 19586.84, "text": " going to encode that text using the encode text function we've created above. So just this one", "tokens": [51552, 516, 281, 2058, 1429, 300, 2487, 1228, 264, 2058, 1429, 2487, 2445, 321, 600, 2942, 3673, 13, 407, 445, 341, 472, 51812], "temperature": 0.0, "avg_logprob": -0.057833875458816, "compression_ratio": 1.8506493506493507, "no_speech_prob": 0.004754946101456881}, {"id": 4495, "seek": 1958684, "start": 19586.84, "end": 19592.12, "text": " right here that essentially takes our sequence of, you know, words, we get the pre processing,", "tokens": [50364, 558, 510, 300, 4476, 2516, 527, 8310, 295, 11, 291, 458, 11, 2283, 11, 321, 483, 264, 659, 9007, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1096621419562668, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0128195034340024}, {"id": 4496, "seek": 1958684, "start": 19592.12, "end": 19596.36, "text": " so turn that into a sequence, remove all the spaces, whatnot, you know, get the words,", "tokens": [50628, 370, 1261, 300, 666, 257, 8310, 11, 4159, 439, 264, 7673, 11, 25882, 11, 291, 458, 11, 483, 264, 2283, 11, 50840], "temperature": 0.0, "avg_logprob": -0.1096621419562668, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0128195034340024}, {"id": 4497, "seek": 1958684, "start": 19596.36, "end": 19602.44, "text": " then we turn those into the integers, we have that we return that. So here we have our proper", "tokens": [50840, 550, 321, 1261, 729, 666, 264, 41674, 11, 321, 362, 300, 321, 2736, 300, 13, 407, 510, 321, 362, 527, 2296, 51144], "temperature": 0.0, "avg_logprob": -0.1096621419562668, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0128195034340024}, {"id": 4498, "seek": 1958684, "start": 19602.44, "end": 19609.96, "text": " pre processed text. Then what we do is we create a blank NumPy array that is just a bunch of zeros", "tokens": [51144, 659, 18846, 2487, 13, 1396, 437, 321, 360, 307, 321, 1884, 257, 8247, 22592, 47, 88, 10225, 300, 307, 445, 257, 3840, 295, 35193, 51520], "temperature": 0.0, "avg_logprob": -0.1096621419562668, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0128195034340024}, {"id": 4499, "seek": 1958684, "start": 19610.6, "end": 19615.32, "text": " that's in the form one to 50 or in that shape. Now the reason I'm putting in that in that shape", "tokens": [51552, 300, 311, 294, 264, 1254, 472, 281, 2625, 420, 294, 300, 3909, 13, 823, 264, 1778, 286, 478, 3372, 294, 300, 294, 300, 3909, 51788], "temperature": 0.0, "avg_logprob": -0.1096621419562668, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0128195034340024}, {"id": 4500, "seek": 1961532, "start": 19615.32, "end": 19622.44, "text": " is because the shape that our model expects is something 250, which means some number of entries,", "tokens": [50364, 307, 570, 264, 3909, 300, 527, 2316, 33280, 307, 746, 11650, 11, 597, 1355, 512, 1230, 295, 23041, 11, 50720], "temperature": 0.0, "avg_logprob": -0.10569030680554978, "compression_ratio": 1.712962962962963, "no_speech_prob": 0.002472522435709834}, {"id": 4501, "seek": 1961532, "start": 19622.44, "end": 19628.04, "text": " and then 250 integers representing each word, right? Because that's the length of movie review", "tokens": [50720, 293, 550, 11650, 41674, 13460, 1184, 1349, 11, 558, 30, 1436, 300, 311, 264, 4641, 295, 3169, 3131, 51000], "temperature": 0.0, "avg_logprob": -0.10569030680554978, "compression_ratio": 1.712962962962963, "no_speech_prob": 0.002472522435709834}, {"id": 4502, "seek": 1961532, "start": 19628.68, "end": 19632.76, "text": " is what we've told the model is the length 250. So that's the length of the review.", "tokens": [51032, 307, 437, 321, 600, 1907, 264, 2316, 307, 264, 4641, 11650, 13, 407, 300, 311, 264, 4641, 295, 264, 3131, 13, 51236], "temperature": 0.0, "avg_logprob": -0.10569030680554978, "compression_ratio": 1.712962962962963, "no_speech_prob": 0.002472522435709834}, {"id": 4503, "seek": 1961532, "start": 19633.4, "end": 19638.92, "text": " Then what we do is we put pred zero. So that's what's up here, equals the encoded text. So we", "tokens": [51268, 1396, 437, 321, 360, 307, 321, 829, 3852, 4018, 13, 407, 300, 311, 437, 311, 493, 510, 11, 6915, 264, 2058, 12340, 2487, 13, 407, 321, 51544], "temperature": 0.0, "avg_logprob": -0.10569030680554978, "compression_ratio": 1.712962962962963, "no_speech_prob": 0.002472522435709834}, {"id": 4504, "seek": 1963892, "start": 19638.92, "end": 19646.519999999997, "text": " just essentially insert our one entry into this, this array we've created. Then what we do is say", "tokens": [50364, 445, 4476, 8969, 527, 472, 8729, 666, 341, 11, 341, 10225, 321, 600, 2942, 13, 1396, 437, 321, 360, 307, 584, 50744], "temperature": 0.0, "avg_logprob": -0.07092266877492269, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.006692672614008188}, {"id": 4505, "seek": 1963892, "start": 19646.519999999997, "end": 19653.32, "text": " modeled up predict on that array, and just return and print the result zero. Now, that's pretty", "tokens": [50744, 37140, 493, 6069, 322, 300, 10225, 11, 293, 445, 2736, 293, 4482, 264, 1874, 4018, 13, 823, 11, 300, 311, 1238, 51084], "temperature": 0.0, "avg_logprob": -0.07092266877492269, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.006692672614008188}, {"id": 4506, "seek": 1963892, "start": 19653.32, "end": 19657.48, "text": " much all there is to it. I mean, that's how it works. The reason we're doing result zero is", "tokens": [51084, 709, 439, 456, 307, 281, 309, 13, 286, 914, 11, 300, 311, 577, 309, 1985, 13, 440, 1778, 321, 434, 884, 1874, 4018, 307, 51292], "temperature": 0.0, "avg_logprob": -0.07092266877492269, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.006692672614008188}, {"id": 4507, "seek": 1963892, "start": 19657.48, "end": 19663.079999999998, "text": " because again, model is optimized to predict on multiple things, which means like I would have", "tokens": [51292, 570, 797, 11, 2316, 307, 26941, 281, 6069, 322, 3866, 721, 11, 597, 1355, 411, 286, 576, 362, 51572], "temperature": 0.0, "avg_logprob": -0.07092266877492269, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.006692672614008188}, {"id": 4508, "seek": 1963892, "start": 19663.079999999998, "end": 19668.28, "text": " to do, you know, list of encoded text, which is kind of what I've done by just doing this", "tokens": [51572, 281, 360, 11, 291, 458, 11, 1329, 295, 2058, 12340, 2487, 11, 597, 307, 733, 295, 437, 286, 600, 1096, 538, 445, 884, 341, 51832], "temperature": 0.0, "avg_logprob": -0.07092266877492269, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.006692672614008188}, {"id": 4509, "seek": 1966828, "start": 19668.28, "end": 19673.719999999998, "text": " prediction lines here, which means it's going to return to me an array of arrays. So if I want", "tokens": [50364, 17630, 3876, 510, 11, 597, 1355, 309, 311, 516, 281, 2736, 281, 385, 364, 10225, 295, 41011, 13, 407, 498, 286, 528, 50636], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4510, "seek": 1966828, "start": 19673.719999999998, "end": 19678.12, "text": " the first prediction, I need to index zero, because that will give me the prediction for", "tokens": [50636, 264, 700, 17630, 11, 286, 643, 281, 8186, 4018, 11, 570, 300, 486, 976, 385, 264, 17630, 337, 50856], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4511, "seek": 1966828, "start": 19678.12, "end": 19683.239999999998, "text": " our first and only entry. Alright, so I hope that makes sense. Now we have a positive review I've", "tokens": [50856, 527, 700, 293, 787, 8729, 13, 2798, 11, 370, 286, 1454, 300, 1669, 2020, 13, 823, 321, 362, 257, 3353, 3131, 286, 600, 51112], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4512, "seek": 1966828, "start": 19683.239999999998, "end": 19687.079999999998, "text": " written and a negative review, and we're just going to compare the analysis on both of them.", "tokens": [51112, 3720, 293, 257, 3671, 3131, 11, 293, 321, 434, 445, 516, 281, 6794, 264, 5215, 322, 1293, 295, 552, 13, 51304], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4513, "seek": 1966828, "start": 19687.079999999998, "end": 19690.36, "text": " So that movie was so awesome. I really loved it and would watch it again, because it was", "tokens": [51304, 407, 300, 3169, 390, 370, 3476, 13, 286, 534, 4333, 309, 293, 576, 1159, 309, 797, 11, 570, 309, 390, 51468], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4514, "seek": 1966828, "start": 19690.36, "end": 19694.039999999997, "text": " amazingly great. And then that movie sucked, I hated it and wouldn't watch it again, was one", "tokens": [51468, 31762, 869, 13, 400, 550, 300, 3169, 26503, 11, 286, 17398, 309, 293, 2759, 380, 1159, 309, 797, 11, 390, 472, 51652], "temperature": 0.0, "avg_logprob": -0.07402661226797795, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.002550828503444791}, {"id": 4515, "seek": 1969404, "start": 19694.120000000003, "end": 19698.280000000002, "text": " of the worst things I've ever watched. So let's look at this now. And we can see the first one", "tokens": [50368, 295, 264, 5855, 721, 286, 600, 1562, 6337, 13, 407, 718, 311, 574, 412, 341, 586, 13, 400, 321, 393, 536, 264, 700, 472, 50576], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4516, "seek": 1969404, "start": 19698.280000000002, "end": 19704.440000000002, "text": " gets predicted at 72% positive, whereas the other one is 23% positive. So essentially what that", "tokens": [50576, 2170, 19147, 412, 18731, 4, 3353, 11, 9735, 264, 661, 472, 307, 6673, 4, 3353, 13, 407, 4476, 437, 300, 50884], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4517, "seek": 1969404, "start": 19704.440000000002, "end": 19708.280000000002, "text": " means is that, you know, if the lower the number, the more negative we're predicting it is, the", "tokens": [50884, 1355, 307, 300, 11, 291, 458, 11, 498, 264, 3126, 264, 1230, 11, 264, 544, 3671, 321, 434, 32884, 309, 307, 11, 264, 51076], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4518, "seek": 1969404, "start": 19708.280000000002, "end": 19712.600000000002, "text": " higher the number, the more positive we're predicting it is. If we wanted to not just print", "tokens": [51076, 2946, 264, 1230, 11, 264, 544, 3353, 321, 434, 32884, 309, 307, 13, 759, 321, 1415, 281, 406, 445, 4482, 51292], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4519, "seek": 1969404, "start": 19712.600000000002, "end": 19717.72, "text": " out this value, and instead what we wanted to do was print out, you know, positive or negative,", "tokens": [51292, 484, 341, 2158, 11, 293, 2602, 437, 321, 1415, 281, 360, 390, 4482, 484, 11, 291, 458, 11, 3353, 420, 3671, 11, 51548], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4520, "seek": 1969404, "start": 19717.72, "end": 19722.600000000002, "text": " we could just make a little if statement that says if this number is greater than 0.5, say positive,", "tokens": [51548, 321, 727, 445, 652, 257, 707, 498, 5629, 300, 1619, 498, 341, 1230, 307, 5044, 813, 1958, 13, 20, 11, 584, 3353, 11, 51792], "temperature": 0.0, "avg_logprob": -0.0789804850539116, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.031141985207796097}, {"id": 4521, "seek": 1972260, "start": 19722.6, "end": 19728.199999999997, "text": " otherwise say not say negative, right? And I just want to show you that changing these reviews", "tokens": [50364, 5911, 584, 406, 584, 3671, 11, 558, 30, 400, 286, 445, 528, 281, 855, 291, 300, 4473, 613, 10229, 50644], "temperature": 0.0, "avg_logprob": -0.07823066200528826, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.005219772923737764}, {"id": 4522, "seek": 1972260, "start": 19728.199999999997, "end": 19733.239999999998, "text": " ever so slightly actually makes a big difference. So if I remove the word awesome, so that movie", "tokens": [50644, 1562, 370, 4748, 767, 1669, 257, 955, 2649, 13, 407, 498, 286, 4159, 264, 1349, 3476, 11, 370, 300, 3169, 50896], "temperature": 0.0, "avg_logprob": -0.07823066200528826, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.005219772923737764}, {"id": 4523, "seek": 1972260, "start": 19733.239999999998, "end": 19738.68, "text": " was so and then I run this, you can see that Oh, wow, this actually increases and goes up to 84%.", "tokens": [50896, 390, 370, 293, 550, 286, 1190, 341, 11, 291, 393, 536, 300, 876, 11, 6076, 11, 341, 767, 8637, 293, 1709, 493, 281, 29018, 6856, 51168], "temperature": 0.0, "avg_logprob": -0.07823066200528826, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.005219772923737764}, {"id": 4524, "seek": 1972260, "start": 19739.64, "end": 19744.12, "text": " So the presence of certain words in certain locations actually makes a big difference. And", "tokens": [51216, 407, 264, 6814, 295, 1629, 2283, 294, 1629, 9253, 767, 1669, 257, 955, 2649, 13, 400, 51440], "temperature": 0.0, "avg_logprob": -0.07823066200528826, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.005219772923737764}, {"id": 4525, "seek": 1972260, "start": 19744.12, "end": 19749.719999999998, "text": " especially when we have a shorter length review, right, if we have a longer length review, it", "tokens": [51440, 2318, 562, 321, 362, 257, 11639, 4641, 3131, 11, 558, 11, 498, 321, 362, 257, 2854, 4641, 3131, 11, 309, 51720], "temperature": 0.0, "avg_logprob": -0.07823066200528826, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.005219772923737764}, {"id": 4526, "seek": 1974972, "start": 19749.800000000003, "end": 19753.72, "text": " won't make that big of a difference. But even the removal of a few words here. And let's see,", "tokens": [50368, 1582, 380, 652, 300, 955, 295, 257, 2649, 13, 583, 754, 264, 17933, 295, 257, 1326, 2283, 510, 13, 400, 718, 311, 536, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4527, "seek": 1974972, "start": 19753.72, "end": 19759.56, "text": " so the removing the word awesome changed it by almost like 10%. Right. Now if I move, so let's", "tokens": [50564, 370, 264, 12720, 264, 1349, 3476, 3105, 309, 538, 1920, 411, 1266, 6856, 1779, 13, 823, 498, 286, 1286, 11, 370, 718, 311, 50856], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4528, "seek": 1974972, "start": 19759.56, "end": 19763.64, "text": " see if that makes a bigger difference. It makes a very little difference because it's learned,", "tokens": [50856, 536, 498, 300, 1669, 257, 3801, 2649, 13, 467, 1669, 257, 588, 707, 2649, 570, 309, 311, 3264, 11, 51060], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4529, "seek": 1974972, "start": 19763.64, "end": 19768.120000000003, "text": " at least the model, right, that the word so doesn't really make a huge impact into", "tokens": [51060, 412, 1935, 264, 2316, 11, 558, 11, 300, 264, 1349, 370, 1177, 380, 534, 652, 257, 2603, 2712, 666, 51284], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4530, "seek": 1974972, "start": 19768.120000000003, "end": 19773.08, "text": " the type of review. Whereas if I remove the word I, let's see if that makes a big impact,", "tokens": [51284, 264, 2010, 295, 3131, 13, 13813, 498, 286, 4159, 264, 1349, 286, 11, 718, 311, 536, 498, 300, 1669, 257, 955, 2712, 11, 51532], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4531, "seek": 1974972, "start": 19773.08, "end": 19777.32, "text": " probably not right now, it goes back up to 84. So that's cool. And that's something to play with", "tokens": [51532, 1391, 406, 558, 586, 11, 309, 1709, 646, 493, 281, 29018, 13, 407, 300, 311, 1627, 13, 400, 300, 311, 746, 281, 862, 365, 51744], "temperature": 0.0, "avg_logprob": -0.09656776794015545, "compression_ratio": 1.8071895424836601, "no_speech_prob": 0.04741746932268143}, {"id": 4532, "seek": 1977732, "start": 19777.32, "end": 19781.96, "text": " is removing certain words and seeing how much impact those actually carry. And even if I just", "tokens": [50364, 307, 12720, 1629, 2283, 293, 2577, 577, 709, 2712, 729, 767, 3985, 13, 400, 754, 498, 286, 445, 50596], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4533, "seek": 1977732, "start": 19781.96, "end": 19785.8, "text": " add the word great, like would great to watch it again, just in the middle of the sentence,", "tokens": [50596, 909, 264, 1349, 869, 11, 411, 576, 869, 281, 1159, 309, 797, 11, 445, 294, 264, 2808, 295, 264, 8174, 11, 50788], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4534, "seek": 1977732, "start": 19785.8, "end": 19789.72, "text": " doesn't have to make any sense. Let's look at this here. Oh, boom, we increase like a little", "tokens": [50788, 1177, 380, 362, 281, 652, 604, 2020, 13, 961, 311, 574, 412, 341, 510, 13, 876, 11, 9351, 11, 321, 3488, 411, 257, 707, 50984], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4535, "seek": 1977732, "start": 19789.72, "end": 19794.52, "text": " bit, right. And let's say if I add this movie, you really suck. Let's see if that makes a difference.", "tokens": [50984, 857, 11, 558, 13, 400, 718, 311, 584, 498, 286, 909, 341, 3169, 11, 291, 534, 9967, 13, 961, 311, 536, 498, 300, 1669, 257, 2649, 13, 51224], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4536, "seek": 1977732, "start": 19795.32, "end": 19799.72, "text": " No, that just reduces it like a tiny bit. So something cool, something to play with.", "tokens": [51264, 883, 11, 300, 445, 18081, 309, 411, 257, 5870, 857, 13, 407, 746, 1627, 11, 746, 281, 862, 365, 13, 51484], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4537, "seek": 1977732, "start": 19799.72, "end": 19804.28, "text": " Anyways, now let's move on to the next example. So now we're on to our last and final example,", "tokens": [51484, 15585, 11, 586, 718, 311, 1286, 322, 281, 264, 958, 1365, 13, 407, 586, 321, 434, 322, 281, 527, 1036, 293, 2572, 1365, 11, 51712], "temperature": 0.0, "avg_logprob": -0.0645350829975025, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.015422683209180832}, {"id": 4538, "seek": 1980428, "start": 19804.36, "end": 19809.399999999998, "text": " which is going to be creating a recurrent neural network play generator. Now, this is going to", "tokens": [50368, 597, 307, 516, 281, 312, 4084, 257, 18680, 1753, 18161, 3209, 862, 19265, 13, 823, 11, 341, 307, 516, 281, 50620], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4539, "seek": 1980428, "start": 19809.399999999998, "end": 19813.32, "text": " be the first kind of neural network we've done, that's actually going to be creating something", "tokens": [50620, 312, 264, 700, 733, 295, 18161, 3209, 321, 600, 1096, 11, 300, 311, 767, 516, 281, 312, 4084, 746, 50816], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4540, "seek": 1980428, "start": 19813.32, "end": 19818.68, "text": " for us. But essentially, what we're going to do is make a model that's capable of predicting the", "tokens": [50816, 337, 505, 13, 583, 4476, 11, 437, 321, 434, 516, 281, 360, 307, 652, 257, 2316, 300, 311, 8189, 295, 32884, 264, 51084], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4541, "seek": 1980428, "start": 19818.68, "end": 19823.8, "text": " next character in a sequence. So we're going to give it some sequence as an input. And what it's", "tokens": [51084, 958, 2517, 294, 257, 8310, 13, 407, 321, 434, 516, 281, 976, 309, 512, 8310, 382, 364, 4846, 13, 400, 437, 309, 311, 51340], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4542, "seek": 1980428, "start": 19823.8, "end": 19828.199999999997, "text": " going to do is just simply predict the most likely next character. Now, there's quite a bit", "tokens": [51340, 516, 281, 360, 307, 445, 2935, 6069, 264, 881, 3700, 958, 2517, 13, 823, 11, 456, 311, 1596, 257, 857, 51560], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4543, "seek": 1980428, "start": 19828.199999999997, "end": 19831.879999999997, "text": " that's going to go into this. But the way we're going to use this to predict a play is we're", "tokens": [51560, 300, 311, 516, 281, 352, 666, 341, 13, 583, 264, 636, 321, 434, 516, 281, 764, 341, 281, 6069, 257, 862, 307, 321, 434, 51744], "temperature": 0.0, "avg_logprob": -0.054212469450185, "compression_ratio": 1.9929824561403509, "no_speech_prob": 0.07805690169334412}, {"id": 4544, "seek": 1983188, "start": 19831.88, "end": 19837.4, "text": " going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then", "tokens": [50364, 516, 281, 3847, 264, 2316, 322, 257, 3840, 295, 22978, 295, 2487, 490, 264, 862, 33563, 293, 33532, 13, 400, 550, 50640], "temperature": 0.0, "avg_logprob": -0.07002930019212805, "compression_ratio": 1.876, "no_speech_prob": 0.006903444416821003}, {"id": 4545, "seek": 1983188, "start": 19837.4, "end": 19841.8, "text": " we're going to have it. So that we'll ask the model will give it some starting prompt, some", "tokens": [50640, 321, 434, 516, 281, 362, 309, 13, 407, 300, 321, 603, 1029, 264, 2316, 486, 976, 309, 512, 2891, 12391, 11, 512, 50860], "temperature": 0.0, "avg_logprob": -0.07002930019212805, "compression_ratio": 1.876, "no_speech_prob": 0.006903444416821003}, {"id": 4546, "seek": 1983188, "start": 19841.8, "end": 19846.84, "text": " string to start with. And that'll be the first thing we pass to it, it will predict to us what", "tokens": [50860, 6798, 281, 722, 365, 13, 400, 300, 603, 312, 264, 700, 551, 321, 1320, 281, 309, 11, 309, 486, 6069, 281, 505, 437, 51112], "temperature": 0.0, "avg_logprob": -0.07002930019212805, "compression_ratio": 1.876, "no_speech_prob": 0.006903444416821003}, {"id": 4547, "seek": 1983188, "start": 19846.84, "end": 19852.120000000003, "text": " the most likely next character for that sequence is. And we'll take the output from the model", "tokens": [51112, 264, 881, 3700, 958, 2517, 337, 300, 8310, 307, 13, 400, 321, 603, 747, 264, 5598, 490, 264, 2316, 51376], "temperature": 0.0, "avg_logprob": -0.07002930019212805, "compression_ratio": 1.876, "no_speech_prob": 0.006903444416821003}, {"id": 4548, "seek": 1983188, "start": 19852.120000000003, "end": 19857.4, "text": " and feed it as the input again to the model and keep predicting sequence of characters. So", "tokens": [51376, 293, 3154, 309, 382, 264, 4846, 797, 281, 264, 2316, 293, 1066, 32884, 8310, 295, 4342, 13, 407, 51640], "temperature": 0.0, "avg_logprob": -0.07002930019212805, "compression_ratio": 1.876, "no_speech_prob": 0.006903444416821003}, {"id": 4549, "seek": 1985740, "start": 19857.4, "end": 19862.280000000002, "text": " keep predicting the next character from the previous output as many times as we want to", "tokens": [50364, 1066, 32884, 264, 958, 2517, 490, 264, 3894, 5598, 382, 867, 1413, 382, 321, 528, 281, 50608], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4550, "seek": 1985740, "start": 19862.280000000002, "end": 19866.760000000002, "text": " generate an entire play. So we're going to have this neural network that's capable of predicting", "tokens": [50608, 8460, 364, 2302, 862, 13, 407, 321, 434, 516, 281, 362, 341, 18161, 3209, 300, 311, 8189, 295, 32884, 50832], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4551, "seek": 1985740, "start": 19866.760000000002, "end": 19872.920000000002, "text": " one letter at a time actually end up generating an entire play for us by running it multiple", "tokens": [50832, 472, 5063, 412, 257, 565, 767, 917, 493, 17746, 364, 2302, 862, 337, 505, 538, 2614, 309, 3866, 51140], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4552, "seek": 1985740, "start": 19872.920000000002, "end": 19878.280000000002, "text": " times on the previous output from the last iteration. Now, that's kind of the problem.", "tokens": [51140, 1413, 322, 264, 3894, 5598, 490, 264, 1036, 24784, 13, 823, 11, 300, 311, 733, 295, 264, 1154, 13, 51408], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4553, "seek": 1985740, "start": 19878.280000000002, "end": 19881.640000000003, "text": " That's what we're trying to solve. So let's go ahead and get into it and talk about what's", "tokens": [51408, 663, 311, 437, 321, 434, 1382, 281, 5039, 13, 407, 718, 311, 352, 2286, 293, 483, 666, 309, 293, 751, 466, 437, 311, 51576], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4554, "seek": 1985740, "start": 19881.640000000003, "end": 19885.800000000003, "text": " involved in doing this. So the first thing we're going to do obviously is our imports. So from", "tokens": [51576, 3288, 294, 884, 341, 13, 407, 264, 700, 551, 321, 434, 516, 281, 360, 2745, 307, 527, 41596, 13, 407, 490, 51784], "temperature": 0.0, "avg_logprob": -0.04750442138084998, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.06370777636766434}, {"id": 4555, "seek": 1988580, "start": 19885.8, "end": 19893.079999999998, "text": " Keras dot preprocessing import sequence, import Keras, we need TensorFlow NumPy and OS. So we'll", "tokens": [50364, 591, 6985, 5893, 2666, 340, 780, 278, 974, 8310, 11, 974, 591, 6985, 11, 321, 643, 37624, 22592, 47, 88, 293, 12731, 13, 407, 321, 603, 50728], "temperature": 0.0, "avg_logprob": -0.11437627848456888, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.011686568148434162}, {"id": 4556, "seek": 1988580, "start": 19893.079999999998, "end": 19898.04, "text": " load that in. And now what we're going to do is download the file. So the data set for Romeo and", "tokens": [50728, 3677, 300, 294, 13, 400, 586, 437, 321, 434, 516, 281, 360, 307, 5484, 264, 3991, 13, 407, 264, 1412, 992, 337, 33563, 293, 50976], "temperature": 0.0, "avg_logprob": -0.11437627848456888, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.011686568148434162}, {"id": 4557, "seek": 1988580, "start": 19898.04, "end": 19903.719999999998, "text": " Juliet, which we can get by using this line here. So Keras has this utils thing, which will allow", "tokens": [50976, 33532, 11, 597, 321, 393, 483, 538, 1228, 341, 1622, 510, 13, 407, 591, 6985, 575, 341, 2839, 4174, 551, 11, 597, 486, 2089, 51260], "temperature": 0.0, "avg_logprob": -0.11437627848456888, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.011686568148434162}, {"id": 4558, "seek": 1988580, "start": 19903.719999999998, "end": 19908.92, "text": " us to get a file, save it as whatever we want. In this case, we're going to save it as Shakespeare", "tokens": [51260, 505, 281, 483, 257, 3991, 11, 3155, 309, 382, 2035, 321, 528, 13, 682, 341, 1389, 11, 321, 434, 516, 281, 3155, 309, 382, 22825, 51520], "temperature": 0.0, "avg_logprob": -0.11437627848456888, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.011686568148434162}, {"id": 4559, "seek": 1988580, "start": 19908.92, "end": 19913.96, "text": " dot txt. And we're going to get that from this link. Now, I believe this is just some like shared", "tokens": [51520, 5893, 256, 734, 13, 400, 321, 434, 516, 281, 483, 300, 490, 341, 2113, 13, 823, 11, 286, 1697, 341, 307, 445, 512, 411, 5507, 51772], "temperature": 0.0, "avg_logprob": -0.11437627848456888, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.011686568148434162}, {"id": 4560, "seek": 1991396, "start": 19913.96, "end": 19919.0, "text": " drive that we have access to from Keras. So we'll load that in here. And then this will", "tokens": [50364, 3332, 300, 321, 362, 2105, 281, 490, 591, 6985, 13, 407, 321, 603, 3677, 300, 294, 510, 13, 400, 550, 341, 486, 50616], "temperature": 0.0, "avg_logprob": -0.05566636069876249, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0032729320228099823}, {"id": 4561, "seek": 1991396, "start": 19919.0, "end": 19924.52, "text": " simply give us the path on this machine. Because remember, this is Google Collaboratory to this", "tokens": [50616, 2935, 976, 505, 264, 3100, 322, 341, 3479, 13, 1436, 1604, 11, 341, 307, 3329, 44483, 4745, 281, 341, 50892], "temperature": 0.0, "avg_logprob": -0.05566636069876249, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0032729320228099823}, {"id": 4562, "seek": 1991396, "start": 19924.52, "end": 19929.48, "text": " text file. Now, if you want, you can actually load in your own text data. So we don't necessarily", "tokens": [50892, 2487, 3991, 13, 823, 11, 498, 291, 528, 11, 291, 393, 767, 3677, 294, 428, 1065, 2487, 1412, 13, 407, 321, 500, 380, 4725, 51140], "temperature": 0.0, "avg_logprob": -0.05566636069876249, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0032729320228099823}, {"id": 4563, "seek": 1991396, "start": 19929.48, "end": 19933.719999999998, "text": " need to use the Shakespeare play, we could use anything we want. In fact, an example that I'll", "tokens": [51140, 643, 281, 764, 264, 22825, 862, 11, 321, 727, 764, 1340, 321, 528, 13, 682, 1186, 11, 364, 1365, 300, 286, 603, 51352], "temperature": 0.0, "avg_logprob": -0.05566636069876249, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0032729320228099823}, {"id": 4564, "seek": 1991396, "start": 19933.719999999998, "end": 19938.92, "text": " show later is using the B movie script. But the way you do that is run this block of code here.", "tokens": [51352, 855, 1780, 307, 1228, 264, 363, 3169, 5755, 13, 583, 264, 636, 291, 360, 300, 307, 1190, 341, 3461, 295, 3089, 510, 13, 51612], "temperature": 0.0, "avg_logprob": -0.05566636069876249, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0032729320228099823}, {"id": 4565, "seek": 1993892, "start": 19939.879999999997, "end": 19944.28, "text": " And you'll see that it pops up this thing for choose files, just choose a file from your", "tokens": [50412, 400, 291, 603, 536, 300, 309, 16795, 493, 341, 551, 337, 2826, 7098, 11, 445, 2826, 257, 3991, 490, 428, 50632], "temperature": 0.0, "avg_logprob": -0.07102309417724609, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02368837781250477}, {"id": 4566, "seek": 1993892, "start": 19945.8, "end": 19951.399999999998, "text": " local computer. And then what that will do is just save this on Google Collaboratory. And", "tokens": [50708, 2654, 3820, 13, 400, 550, 437, 300, 486, 360, 307, 445, 3155, 341, 322, 3329, 44483, 4745, 13, 400, 50988], "temperature": 0.0, "avg_logprob": -0.07102309417724609, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02368837781250477}, {"id": 4567, "seek": 1993892, "start": 19951.399999999998, "end": 19955.399999999998, "text": " then that will allow you to actually use that. So make sure that's a text file that you're loading", "tokens": [50988, 550, 300, 486, 2089, 291, 281, 767, 764, 300, 13, 407, 652, 988, 300, 311, 257, 2487, 3991, 300, 291, 434, 15114, 51188], "temperature": 0.0, "avg_logprob": -0.07102309417724609, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02368837781250477}, {"id": 4568, "seek": 1993892, "start": 19955.399999999998, "end": 19961.0, "text": " in there. But regardless, that should work. And then from there, you'll be good to go. So if you,", "tokens": [51188, 294, 456, 13, 583, 10060, 11, 300, 820, 589, 13, 400, 550, 490, 456, 11, 291, 603, 312, 665, 281, 352, 13, 407, 498, 291, 11, 51468], "temperature": 0.0, "avg_logprob": -0.07102309417724609, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02368837781250477}, {"id": 4569, "seek": 1993892, "start": 19961.0, "end": 19964.44, "text": " you know, you don't need to do that, you can just run this block of code here, if you want to load", "tokens": [51468, 291, 458, 11, 291, 500, 380, 643, 281, 360, 300, 11, 291, 393, 445, 1190, 341, 3461, 295, 3089, 510, 11, 498, 291, 528, 281, 3677, 51640], "temperature": 0.0, "avg_logprob": -0.07102309417724609, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.02368837781250477}, {"id": 4570, "seek": 1996444, "start": 19964.6, "end": 19969.96, "text": " in the Shakespeare txt, but otherwise, you can load in your own file. Now, after we do that,", "tokens": [50372, 294, 264, 22825, 256, 734, 11, 457, 5911, 11, 291, 393, 3677, 294, 428, 1065, 3991, 13, 823, 11, 934, 321, 360, 300, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1006438842186561, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.010327480733394623}, {"id": 4571, "seek": 1996444, "start": 19969.96, "end": 19974.12, "text": " what we want to do is actually open this file. So remember, that was just saving the path to it.", "tokens": [50640, 437, 321, 528, 281, 360, 307, 767, 1269, 341, 3991, 13, 407, 1604, 11, 300, 390, 445, 6816, 264, 3100, 281, 309, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1006438842186561, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.010327480733394623}, {"id": 4572, "seek": 1996444, "start": 19974.12, "end": 19979.239999999998, "text": " So we'll open that file in RB mode, which is read bytes mode, I believe. And then we're going to", "tokens": [50848, 407, 321, 603, 1269, 300, 3991, 294, 40302, 4391, 11, 597, 307, 1401, 36088, 4391, 11, 286, 1697, 13, 400, 550, 321, 434, 516, 281, 51104], "temperature": 0.0, "avg_logprob": -0.1006438842186561, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.010327480733394623}, {"id": 4573, "seek": 1996444, "start": 19979.239999999998, "end": 19984.199999999997, "text": " say dot read, so we're going to read that in as an entire string, we're going to decode that into", "tokens": [51104, 584, 5893, 1401, 11, 370, 321, 434, 516, 281, 1401, 300, 294, 382, 364, 2302, 6798, 11, 321, 434, 516, 281, 979, 1429, 300, 666, 51352], "temperature": 0.0, "avg_logprob": -0.1006438842186561, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.010327480733394623}, {"id": 4574, "seek": 1996444, "start": 19984.199999999997, "end": 19989.16, "text": " utf a format. And then we're just printing the length of the text or the amount of characters", "tokens": [51352, 2839, 69, 257, 7877, 13, 400, 550, 321, 434, 445, 14699, 264, 4641, 295, 264, 2487, 420, 264, 2372, 295, 4342, 51600], "temperature": 0.0, "avg_logprob": -0.1006438842186561, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.010327480733394623}, {"id": 4575, "seek": 1998916, "start": 19989.24, "end": 19994.2, "text": " in the text. So if we do that, we can see we have the length of the text is 1.1 million", "tokens": [50368, 294, 264, 2487, 13, 407, 498, 321, 360, 300, 11, 321, 393, 536, 321, 362, 264, 4641, 295, 264, 2487, 307, 502, 13, 16, 2459, 50616], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4576, "seek": 1998916, "start": 19994.2, "end": 19998.92, "text": " characters, approximately. And then we can have a look at the first 250 characters by doing this.", "tokens": [50616, 4342, 11, 10447, 13, 400, 550, 321, 393, 362, 257, 574, 412, 264, 700, 11650, 4342, 538, 884, 341, 13, 50852], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4577, "seek": 1998916, "start": 19999.72, "end": 20003.8, "text": " So we can see that this is kind of what the plate looks like, we have whoever's speaking,", "tokens": [50892, 407, 321, 393, 536, 300, 341, 307, 733, 295, 437, 264, 5924, 1542, 411, 11, 321, 362, 11387, 311, 4124, 11, 51096], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4578, "seek": 1998916, "start": 20003.8, "end": 20009.64, "text": " colon, then some line, whoever's speaking, colon, some line, and there's all these break lines.", "tokens": [51096, 8255, 11, 550, 512, 1622, 11, 11387, 311, 4124, 11, 8255, 11, 512, 1622, 11, 293, 456, 311, 439, 613, 1821, 3876, 13, 51388], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4579, "seek": 1998916, "start": 20009.64, "end": 20014.04, "text": " So backslash ends, which are telling us, you know, go to the next line, right? So it's going", "tokens": [51388, 407, 646, 10418, 1299, 5314, 11, 597, 366, 3585, 505, 11, 291, 458, 11, 352, 281, 264, 958, 1622, 11, 558, 30, 407, 309, 311, 516, 51608], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4580, "seek": 1998916, "start": 20014.04, "end": 20017.88, "text": " to be important because we're going to hope that our neural network will be able to predict", "tokens": [51608, 281, 312, 1021, 570, 321, 434, 516, 281, 1454, 300, 527, 18161, 3209, 486, 312, 1075, 281, 6069, 51800], "temperature": 0.0, "avg_logprob": -0.08680540201615314, "compression_ratio": 1.834983498349835, "no_speech_prob": 0.03621654957532883}, {"id": 4581, "seek": 2001788, "start": 20017.88, "end": 20023.24, "text": " things like break lines and spaces, and even this kind of format as we teach it more and get", "tokens": [50364, 721, 411, 1821, 3876, 293, 7673, 11, 293, 754, 341, 733, 295, 7877, 382, 321, 2924, 309, 544, 293, 483, 50632], "temperature": 0.0, "avg_logprob": -0.06594466744807728, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0021156033035367727}, {"id": 4582, "seek": 2001788, "start": 20023.24, "end": 20029.4, "text": " further in. But now it's time to talk about encoding. So obviously, all of this text is in", "tokens": [50632, 3052, 294, 13, 583, 586, 309, 311, 565, 281, 751, 466, 43430, 13, 407, 2745, 11, 439, 295, 341, 2487, 307, 294, 50940], "temperature": 0.0, "avg_logprob": -0.06594466744807728, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0021156033035367727}, {"id": 4583, "seek": 2001788, "start": 20029.4, "end": 20034.52, "text": " text form, it's not pre processed for us, which means we need to pre process it and encode it", "tokens": [50940, 2487, 1254, 11, 309, 311, 406, 659, 18846, 337, 505, 11, 597, 1355, 321, 643, 281, 659, 1399, 309, 293, 2058, 1429, 309, 51196], "temperature": 0.0, "avg_logprob": -0.06594466744807728, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0021156033035367727}, {"id": 4584, "seek": 2001788, "start": 20034.52, "end": 20038.920000000002, "text": " as integers before we can move forward. Now, fortunately, for us, this problem is actually", "tokens": [51196, 382, 41674, 949, 321, 393, 1286, 2128, 13, 823, 11, 25511, 11, 337, 505, 11, 341, 1154, 307, 767, 51416], "temperature": 0.0, "avg_logprob": -0.06594466744807728, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0021156033035367727}, {"id": 4585, "seek": 2001788, "start": 20038.920000000002, "end": 20043.48, "text": " a little bit easier than the problem we discussed earlier with encoding words, because what we're", "tokens": [51416, 257, 707, 857, 3571, 813, 264, 1154, 321, 7152, 3071, 365, 43430, 2283, 11, 570, 437, 321, 434, 51644], "temperature": 0.0, "avg_logprob": -0.06594466744807728, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0021156033035367727}, {"id": 4586, "seek": 2004348, "start": 20043.48, "end": 20048.44, "text": " going to do is simply encode each character in the text with an integer. Now, you can imagine", "tokens": [50364, 516, 281, 360, 307, 2935, 2058, 1429, 1184, 2517, 294, 264, 2487, 365, 364, 24922, 13, 823, 11, 291, 393, 3811, 50612], "temperature": 0.0, "avg_logprob": -0.0575925060826489, "compression_ratio": 1.8735632183908046, "no_speech_prob": 0.021612966433167458}, {"id": 4587, "seek": 2004348, "start": 20048.44, "end": 20053.72, "text": " why this makes this easier, because there really is a finite set of characters, whereas there's", "tokens": [50612, 983, 341, 1669, 341, 3571, 11, 570, 456, 534, 307, 257, 19362, 992, 295, 4342, 11, 9735, 456, 311, 50876], "temperature": 0.0, "avg_logprob": -0.0575925060826489, "compression_ratio": 1.8735632183908046, "no_speech_prob": 0.021612966433167458}, {"id": 4588, "seek": 2004348, "start": 20053.72, "end": 20059.079999999998, "text": " kind of indefinite or, you know, I guess, infinite amount of words that could be created. So we're", "tokens": [50876, 733, 295, 24162, 5194, 642, 420, 11, 291, 458, 11, 286, 2041, 11, 13785, 2372, 295, 2283, 300, 727, 312, 2942, 13, 407, 321, 434, 51144], "temperature": 0.0, "avg_logprob": -0.0575925060826489, "compression_ratio": 1.8735632183908046, "no_speech_prob": 0.021612966433167458}, {"id": 4589, "seek": 2004348, "start": 20059.079999999998, "end": 20064.36, "text": " not really going to run into the problem where, you know, two words are encoded with such different", "tokens": [51144, 406, 534, 516, 281, 1190, 666, 264, 1154, 689, 11, 291, 458, 11, 732, 2283, 366, 2058, 12340, 365, 1270, 819, 51408], "temperature": 0.0, "avg_logprob": -0.0575925060826489, "compression_ratio": 1.8735632183908046, "no_speech_prob": 0.021612966433167458}, {"id": 4590, "seek": 2004348, "start": 20064.36, "end": 20068.68, "text": " or two characters are encoded with such different integers, that it makes it difficult for the model", "tokens": [51408, 420, 732, 4342, 366, 2058, 12340, 365, 1270, 819, 41674, 11, 300, 309, 1669, 309, 2252, 337, 264, 2316, 51624], "temperature": 0.0, "avg_logprob": -0.0575925060826489, "compression_ratio": 1.8735632183908046, "no_speech_prob": 0.021612966433167458}, {"id": 4591, "seek": 2006868, "start": 20068.68, "end": 20074.12, "text": " to understand. Because I mean, and we can look at what the value of vocab is here, we're only", "tokens": [50364, 281, 1223, 13, 1436, 286, 914, 11, 293, 321, 393, 574, 412, 437, 264, 2158, 295, 2329, 455, 307, 510, 11, 321, 434, 787, 50636], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4592, "seek": 2006868, "start": 20074.12, "end": 20078.44, "text": " going to have so many characters in the text. And for characters, it just doesn't matter as much,", "tokens": [50636, 516, 281, 362, 370, 867, 4342, 294, 264, 2487, 13, 400, 337, 4342, 11, 309, 445, 1177, 380, 1871, 382, 709, 11, 50852], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4593, "seek": 2006868, "start": 20078.44, "end": 20083.56, "text": " because you know, an R isn't like super meaningful compared to an A. So we can kind of encode in a", "tokens": [50852, 570, 291, 458, 11, 364, 497, 1943, 380, 411, 1687, 10995, 5347, 281, 364, 316, 13, 407, 321, 393, 733, 295, 2058, 1429, 294, 257, 51108], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4594, "seek": 2006868, "start": 20083.56, "end": 20087.4, "text": " simple format, which is what we're going to do. So essentially, we need to figure out how many", "tokens": [51108, 2199, 7877, 11, 597, 307, 437, 321, 434, 516, 281, 360, 13, 407, 4476, 11, 321, 643, 281, 2573, 484, 577, 867, 51300], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4595, "seek": 2006868, "start": 20087.4, "end": 20092.68, "text": " unique characters are in our vocabulary. So to do that, we're going to say vocab equals sorted,", "tokens": [51300, 3845, 4342, 366, 294, 527, 19864, 13, 407, 281, 360, 300, 11, 321, 434, 516, 281, 584, 2329, 455, 6915, 25462, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4596, "seek": 2006868, "start": 20092.68, "end": 20097.8, "text": " set text, this will sort all of the unique characters in the text. And then what we're", "tokens": [51564, 992, 2487, 11, 341, 486, 1333, 439, 295, 264, 3845, 4342, 294, 264, 2487, 13, 400, 550, 437, 321, 434, 51820], "temperature": 0.0, "avg_logprob": -0.07475315324412096, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.022974686697125435}, {"id": 4597, "seek": 2009780, "start": 20097.8, "end": 20102.84, "text": " going to do is create a mapping from unique characters to indices indices. So essentially,", "tokens": [50364, 516, 281, 360, 307, 1884, 257, 18350, 490, 3845, 4342, 281, 43840, 43840, 13, 407, 4476, 11, 50616], "temperature": 0.0, "avg_logprob": -0.09895960208588997, "compression_ratio": 1.9504132231404958, "no_speech_prob": 0.0009110159589909017}, {"id": 4598, "seek": 2009780, "start": 20102.84, "end": 20108.36, "text": " we're going to say UI, for IU in a new, a numerator vocabulary, what this will do is give us", "tokens": [50616, 321, 434, 516, 281, 584, 15682, 11, 337, 44218, 294, 257, 777, 11, 257, 30380, 19864, 11, 437, 341, 486, 360, 307, 976, 505, 50892], "temperature": 0.0, "avg_logprob": -0.09895960208588997, "compression_ratio": 1.9504132231404958, "no_speech_prob": 0.0009110159589909017}, {"id": 4599, "seek": 2009780, "start": 20109.239999999998, "end": 20114.68, "text": " essentially zero, whatever the string is, one, whatever the string is, two, whatever the string", "tokens": [50936, 4476, 4018, 11, 2035, 264, 6798, 307, 11, 472, 11, 2035, 264, 6798, 307, 11, 732, 11, 2035, 264, 6798, 51208], "temperature": 0.0, "avg_logprob": -0.09895960208588997, "compression_ratio": 1.9504132231404958, "no_speech_prob": 0.0009110159589909017}, {"id": 4600, "seek": 2009780, "start": 20114.68, "end": 20119.48, "text": " is for every single letter or character in our vocabulary, which will allow us to create this", "tokens": [51208, 307, 337, 633, 2167, 5063, 420, 2517, 294, 527, 19864, 11, 597, 486, 2089, 505, 281, 1884, 341, 51448], "temperature": 0.0, "avg_logprob": -0.09895960208588997, "compression_ratio": 1.9504132231404958, "no_speech_prob": 0.0009110159589909017}, {"id": 4601, "seek": 2009780, "start": 20119.48, "end": 20126.68, "text": " mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array,", "tokens": [51448, 18350, 13, 400, 550, 437, 321, 603, 360, 307, 445, 1261, 341, 5883, 19864, 666, 257, 1329, 420, 666, 364, 10225, 11, 51808], "temperature": 0.0, "avg_logprob": -0.09895960208588997, "compression_ratio": 1.9504132231404958, "no_speech_prob": 0.0009110159589909017}, {"id": 4602, "seek": 2012668, "start": 20126.760000000002, "end": 20131.96, "text": " so that we can just use the index at which a letter appears as the reverse mapping. So going", "tokens": [50368, 370, 300, 321, 393, 445, 764, 264, 8186, 412, 597, 257, 5063, 7038, 382, 264, 9943, 18350, 13, 407, 516, 50628], "temperature": 0.0, "avg_logprob": -0.06811937764913094, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.0008295453153550625}, {"id": 4603, "seek": 2012668, "start": 20131.96, "end": 20136.36, "text": " from index to letter, rather than lettered index, which is what this one's doing here.", "tokens": [50628, 490, 8186, 281, 5063, 11, 2831, 813, 5063, 292, 8186, 11, 597, 307, 437, 341, 472, 311, 884, 510, 13, 50848], "temperature": 0.0, "avg_logprob": -0.06811937764913094, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.0008295453153550625}, {"id": 4604, "seek": 2012668, "start": 20137.0, "end": 20142.68, "text": " Next, I've just written a function that takes some text and converts that to an int or the int", "tokens": [50880, 3087, 11, 286, 600, 445, 3720, 257, 2445, 300, 2516, 512, 2487, 293, 38874, 300, 281, 364, 560, 420, 264, 560, 51164], "temperature": 0.0, "avg_logprob": -0.06811937764913094, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.0008295453153550625}, {"id": 4605, "seek": 2012668, "start": 20142.68, "end": 20147.4, "text": " representation for it, just to make a little bit easier for us as we get later on in the tutorial.", "tokens": [51164, 10290, 337, 309, 11, 445, 281, 652, 257, 707, 857, 3571, 337, 505, 382, 321, 483, 1780, 322, 294, 264, 7073, 13, 51400], "temperature": 0.0, "avg_logprob": -0.06811937764913094, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.0008295453153550625}, {"id": 4606, "seek": 2012668, "start": 20147.4, "end": 20152.36, "text": " So we're just going to say NP dot array of in this case, and we're just going to convert every single", "tokens": [51400, 407, 321, 434, 445, 516, 281, 584, 38611, 5893, 10225, 295, 294, 341, 1389, 11, 293, 321, 434, 445, 516, 281, 7620, 633, 2167, 51648], "temperature": 0.0, "avg_logprob": -0.06811937764913094, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.0008295453153550625}, {"id": 4607, "seek": 2015236, "start": 20152.440000000002, "end": 20158.600000000002, "text": " character in our text into its integer representation by just referencing that character and putting", "tokens": [50368, 2517, 294, 527, 2487, 666, 1080, 24922, 10290, 538, 445, 40582, 300, 2517, 293, 3372, 50676], "temperature": 0.0, "avg_logprob": -0.08002514588205438, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0012842845171689987}, {"id": 4608, "seek": 2015236, "start": 20158.600000000002, "end": 20163.08, "text": " that in a list here, and then obviously converting that to NumPy array. So then if we wanted to have", "tokens": [50676, 300, 294, 257, 1329, 510, 11, 293, 550, 2745, 29942, 300, 281, 22592, 47, 88, 10225, 13, 407, 550, 498, 321, 1415, 281, 362, 50900], "temperature": 0.0, "avg_logprob": -0.08002514588205438, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0012842845171689987}, {"id": 4609, "seek": 2015236, "start": 20163.08, "end": 20169.48, "text": " a look at how this works, we can say text as int equals text to int text. So remember text is", "tokens": [50900, 257, 574, 412, 577, 341, 1985, 11, 321, 393, 584, 2487, 382, 560, 6915, 2487, 281, 560, 2487, 13, 407, 1604, 2487, 307, 51220], "temperature": 0.0, "avg_logprob": -0.08002514588205438, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0012842845171689987}, {"id": 4610, "seek": 2015236, "start": 20169.48, "end": 20174.52, "text": " that entire loaded file that we had above here. So we're just going to convert that to its integer", "tokens": [51220, 300, 2302, 13210, 3991, 300, 321, 632, 3673, 510, 13, 407, 321, 434, 445, 516, 281, 7620, 300, 281, 1080, 24922, 51472], "temperature": 0.0, "avg_logprob": -0.08002514588205438, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0012842845171689987}, {"id": 4611, "seek": 2015236, "start": 20174.52, "end": 20180.2, "text": " representation entirely using this function. And now we can look at how this works down here. So", "tokens": [51472, 10290, 7696, 1228, 341, 2445, 13, 400, 586, 321, 393, 574, 412, 577, 341, 1985, 760, 510, 13, 407, 51756], "temperature": 0.0, "avg_logprob": -0.08002514588205438, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0012842845171689987}, {"id": 4612, "seek": 2018020, "start": 20180.2, "end": 20188.52, "text": " we can see that the text for citizen, which is the first 13 letters is encoded by 1847 5657 581.", "tokens": [50364, 321, 393, 536, 300, 264, 2487, 337, 13326, 11, 597, 307, 264, 700, 3705, 7825, 307, 2058, 12340, 538, 2443, 14060, 19687, 19004, 21786, 16, 13, 50780], "temperature": 0.0, "avg_logprob": -0.10748662645854647, "compression_ratio": 1.5953947368421053, "no_speech_prob": 0.0006666951812803745}, {"id": 4613, "seek": 2018020, "start": 20188.52, "end": 20192.600000000002, "text": " And obviously each character has its own encoding, and you can go through and kind of figure out what", "tokens": [50780, 400, 2745, 1184, 2517, 575, 1080, 1065, 43430, 11, 293, 291, 393, 352, 807, 293, 733, 295, 2573, 484, 437, 50984], "temperature": 0.0, "avg_logprob": -0.10748662645854647, "compression_ratio": 1.5953947368421053, "no_speech_prob": 0.0006666951812803745}, {"id": 4614, "seek": 2018020, "start": 20192.600000000002, "end": 20198.04, "text": " they are based on the ones that are repeated, right? So that is how that works. Now I figured", "tokens": [50984, 436, 366, 2361, 322, 264, 2306, 300, 366, 10477, 11, 558, 30, 407, 300, 307, 577, 300, 1985, 13, 823, 286, 8932, 51256], "temperature": 0.0, "avg_logprob": -0.10748662645854647, "compression_ratio": 1.5953947368421053, "no_speech_prob": 0.0006666951812803745}, {"id": 4615, "seek": 2018020, "start": 20198.04, "end": 20201.4, "text": " while we were at it, we might as well write a function that goes the other way. So into text.", "tokens": [51256, 1339, 321, 645, 412, 309, 11, 321, 1062, 382, 731, 2464, 257, 2445, 300, 1709, 264, 661, 636, 13, 407, 666, 2487, 13, 51424], "temperature": 0.0, "avg_logprob": -0.10748662645854647, "compression_ratio": 1.5953947368421053, "no_speech_prob": 0.0006666951812803745}, {"id": 4616, "seek": 2018020, "start": 20202.36, "end": 20207.48, "text": " Reason I'm trying to convert this to a NumPy array first is just because we're going to be passing", "tokens": [51472, 39693, 286, 478, 1382, 281, 7620, 341, 281, 257, 22592, 47, 88, 10225, 700, 307, 445, 570, 321, 434, 516, 281, 312, 8437, 51728], "temperature": 0.0, "avg_logprob": -0.10748662645854647, "compression_ratio": 1.5953947368421053, "no_speech_prob": 0.0006666951812803745}, {"id": 4617, "seek": 2020748, "start": 20207.48, "end": 20212.12, "text": " in different objects potentially in here. So if it's not already a NumPy array, it needs to be", "tokens": [50364, 294, 819, 6565, 7263, 294, 510, 13, 407, 498, 309, 311, 406, 1217, 257, 22592, 47, 88, 10225, 11, 309, 2203, 281, 312, 50596], "temperature": 0.0, "avg_logprob": -0.10322711342259457, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.013635765761137009}, {"id": 4618, "seek": 2020748, "start": 20212.12, "end": 20217.72, "text": " a NumPy array, which is kind of what this is doing. Otherwise, we're just going to pass on that we", "tokens": [50596, 257, 22592, 47, 88, 10225, 11, 597, 307, 733, 295, 437, 341, 307, 884, 13, 10328, 11, 321, 434, 445, 516, 281, 1320, 322, 300, 321, 50876], "temperature": 0.0, "avg_logprob": -0.10322711342259457, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.013635765761137009}, {"id": 4619, "seek": 2020748, "start": 20217.72, "end": 20222.84, "text": " don't need to convert it to NumPy array, if it already is one, we can just join all of the characters", "tokens": [50876, 500, 380, 643, 281, 7620, 309, 281, 22592, 47, 88, 10225, 11, 498, 309, 1217, 307, 472, 11, 321, 393, 445, 3917, 439, 295, 264, 4342, 51132], "temperature": 0.0, "avg_logprob": -0.10322711342259457, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.013635765761137009}, {"id": 4620, "seek": 2020748, "start": 20222.84, "end": 20229.0, "text": " from this list into here. So that's essentially what this is doing for us. It's just joining", "tokens": [51132, 490, 341, 1329, 666, 510, 13, 407, 300, 311, 4476, 437, 341, 307, 884, 337, 505, 13, 467, 311, 445, 5549, 51440], "temperature": 0.0, "avg_logprob": -0.10322711342259457, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.013635765761137009}, {"id": 4621, "seek": 2020748, "start": 20229.0, "end": 20235.8, "text": " into text. And then we can see if we go into text, text is in colon 13, that translates that back to", "tokens": [51440, 666, 2487, 13, 400, 550, 321, 393, 536, 498, 321, 352, 666, 2487, 11, 2487, 307, 294, 8255, 3705, 11, 300, 28468, 300, 646, 281, 51780], "temperature": 0.0, "avg_logprob": -0.10322711342259457, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.013635765761137009}, {"id": 4622, "seek": 2023580, "start": 20235.8, "end": 20239.64, "text": " us for citizen, I mean, you can look more into this function if you want, but it's not that", "tokens": [50364, 505, 337, 13326, 11, 286, 914, 11, 291, 393, 574, 544, 666, 341, 2445, 498, 291, 528, 11, 457, 309, 311, 406, 300, 50556], "temperature": 0.0, "avg_logprob": -0.05990603052336594, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.004198556300252676}, {"id": 4623, "seek": 2023580, "start": 20239.64, "end": 20245.0, "text": " complicated. Okay, so now that we have all this text encoded as integers, what we need to do is", "tokens": [50556, 6179, 13, 1033, 11, 370, 586, 300, 321, 362, 439, 341, 2487, 2058, 12340, 382, 41674, 11, 437, 321, 643, 281, 360, 307, 50824], "temperature": 0.0, "avg_logprob": -0.05990603052336594, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.004198556300252676}, {"id": 4624, "seek": 2023580, "start": 20245.0, "end": 20250.6, "text": " create some training examples. It's not really feasible to just pass the entire you know, 1.1", "tokens": [50824, 1884, 512, 3097, 5110, 13, 467, 311, 406, 534, 26648, 281, 445, 1320, 264, 2302, 291, 458, 11, 502, 13, 16, 51104], "temperature": 0.0, "avg_logprob": -0.05990603052336594, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.004198556300252676}, {"id": 4625, "seek": 2023580, "start": 20250.6, "end": 20255.64, "text": " million characters to our model at once for training, we need to split that up into something", "tokens": [51104, 2459, 4342, 281, 527, 2316, 412, 1564, 337, 3097, 11, 321, 643, 281, 7472, 300, 493, 666, 746, 51356], "temperature": 0.0, "avg_logprob": -0.05990603052336594, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.004198556300252676}, {"id": 4626, "seek": 2023580, "start": 20255.64, "end": 20260.36, "text": " that's meaningful. So what we're actually going to be doing is creating training examples where we", "tokens": [51356, 300, 311, 10995, 13, 407, 437, 321, 434, 767, 516, 281, 312, 884, 307, 4084, 3097, 5110, 689, 321, 51592], "temperature": 0.0, "avg_logprob": -0.05990603052336594, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.004198556300252676}, {"id": 4627, "seek": 2026036, "start": 20260.36, "end": 20267.64, "text": " have the first, where the training input, right, so the input value is going to be some sequence", "tokens": [50364, 362, 264, 700, 11, 689, 264, 3097, 4846, 11, 558, 11, 370, 264, 4846, 2158, 307, 516, 281, 312, 512, 8310, 50728], "temperature": 0.0, "avg_logprob": -0.11789741035269088, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.07584293186664581}, {"id": 4628, "seek": 2026036, "start": 20267.64, "end": 20272.52, "text": " of some length, we'll pick the sequence length, in this case, we're actually going to pick 100.", "tokens": [50728, 295, 512, 4641, 11, 321, 603, 1888, 264, 8310, 4641, 11, 294, 341, 1389, 11, 321, 434, 767, 516, 281, 1888, 2319, 13, 50972], "temperature": 0.0, "avg_logprob": -0.11789741035269088, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.07584293186664581}, {"id": 4629, "seek": 2026036, "start": 20272.52, "end": 20277.8, "text": " And then the output or the expected output, so I guess like the label for that training example", "tokens": [50972, 400, 550, 264, 5598, 420, 264, 5176, 5598, 11, 370, 286, 2041, 411, 264, 7645, 337, 300, 3097, 1365, 51236], "temperature": 0.0, "avg_logprob": -0.11789741035269088, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.07584293186664581}, {"id": 4630, "seek": 2026036, "start": 20277.8, "end": 20283.48, "text": " is going to be the exact same sequence shifted right by one character. So essentially, I put a", "tokens": [51236, 307, 516, 281, 312, 264, 1900, 912, 8310, 18892, 558, 538, 472, 2517, 13, 407, 4476, 11, 286, 829, 257, 51520], "temperature": 0.0, "avg_logprob": -0.11789741035269088, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.07584293186664581}, {"id": 4631, "seek": 2026036, "start": 20283.48, "end": 20288.600000000002, "text": " good example here, our input will be something like hell, right? Now our output will be E L L O.", "tokens": [51520, 665, 1365, 510, 11, 527, 4846, 486, 312, 746, 411, 4921, 11, 558, 30, 823, 527, 5598, 486, 312, 462, 441, 441, 422, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11789741035269088, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.07584293186664581}, {"id": 4632, "seek": 2028860, "start": 20288.6, "end": 20293.239999999998, "text": " So what it's going to do is predict this last character, essentially. And these are what our", "tokens": [50364, 407, 437, 309, 311, 516, 281, 360, 307, 6069, 341, 1036, 2517, 11, 4476, 13, 400, 613, 366, 437, 527, 50596], "temperature": 0.0, "avg_logprob": -0.07136783928706728, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0010649275500327349}, {"id": 4633, "seek": 2028860, "start": 20293.239999999998, "end": 20298.12, "text": " training examples are going to look like. So the entire beginning sequence, and then the output", "tokens": [50596, 3097, 5110, 366, 516, 281, 574, 411, 13, 407, 264, 2302, 2863, 8310, 11, 293, 550, 264, 5598, 50840], "temperature": 0.0, "avg_logprob": -0.07136783928706728, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0010649275500327349}, {"id": 4634, "seek": 2028860, "start": 20298.12, "end": 20303.64, "text": " sequence should be that beginning sequence minus the first letter, but tack on what the last letter", "tokens": [50840, 8310, 820, 312, 300, 2863, 8310, 3175, 264, 700, 5063, 11, 457, 9426, 322, 437, 264, 1036, 5063, 51116], "temperature": 0.0, "avg_logprob": -0.07136783928706728, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0010649275500327349}, {"id": 4635, "seek": 2028860, "start": 20303.64, "end": 20308.76, "text": " should be. So that this way, we can look at some input sequence and then predict that output sequence", "tokens": [51116, 820, 312, 13, 407, 300, 341, 636, 11, 321, 393, 574, 412, 512, 4846, 8310, 293, 550, 6069, 300, 5598, 8310, 51372], "temperature": 0.0, "avg_logprob": -0.07136783928706728, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0010649275500327349}, {"id": 4636, "seek": 2028860, "start": 20308.76, "end": 20314.12, "text": " that you know, plus a character, right? Okay, so that's how that works. So now we're going to do is", "tokens": [51372, 300, 291, 458, 11, 1804, 257, 2517, 11, 558, 30, 1033, 11, 370, 300, 311, 577, 300, 1985, 13, 407, 586, 321, 434, 516, 281, 360, 307, 51640], "temperature": 0.0, "avg_logprob": -0.07136783928706728, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0010649275500327349}, {"id": 4637, "seek": 2031412, "start": 20314.12, "end": 20318.84, "text": " define a sequence length of 100. We're going to say the amount of examples per epoch is going to be", "tokens": [50364, 6964, 257, 8310, 4641, 295, 2319, 13, 492, 434, 516, 281, 584, 264, 2372, 295, 5110, 680, 30992, 339, 307, 516, 281, 312, 50600], "temperature": 0.0, "avg_logprob": -0.0750089409077062, "compression_ratio": 2.0375, "no_speech_prob": 0.058340881019830704}, {"id": 4638, "seek": 2031412, "start": 20318.84, "end": 20323.8, "text": " the length of the text divided by the sequence length plus one. The reason we're doing this is", "tokens": [50600, 264, 4641, 295, 264, 2487, 6666, 538, 264, 8310, 4641, 1804, 472, 13, 440, 1778, 321, 434, 884, 341, 307, 50848], "temperature": 0.0, "avg_logprob": -0.0750089409077062, "compression_ratio": 2.0375, "no_speech_prob": 0.058340881019830704}, {"id": 4639, "seek": 2031412, "start": 20323.8, "end": 20329.48, "text": " because for every training example, we need to create a sequence input that's 100 characters long,", "tokens": [50848, 570, 337, 633, 3097, 1365, 11, 321, 643, 281, 1884, 257, 8310, 4846, 300, 311, 2319, 4342, 938, 11, 51132], "temperature": 0.0, "avg_logprob": -0.0750089409077062, "compression_ratio": 2.0375, "no_speech_prob": 0.058340881019830704}, {"id": 4640, "seek": 2031412, "start": 20329.48, "end": 20334.28, "text": " and we need to create a sequence output that's 100 characters long, which means that we need to have", "tokens": [51132, 293, 321, 643, 281, 1884, 257, 8310, 5598, 300, 311, 2319, 4342, 938, 11, 597, 1355, 300, 321, 643, 281, 362, 51372], "temperature": 0.0, "avg_logprob": -0.0750089409077062, "compression_ratio": 2.0375, "no_speech_prob": 0.058340881019830704}, {"id": 4641, "seek": 2031412, "start": 20334.28, "end": 20340.76, "text": " 101 characters that we use for every training example, right? Hopefully that would make sense.", "tokens": [51372, 21055, 4342, 300, 321, 764, 337, 633, 3097, 1365, 11, 558, 30, 10429, 300, 576, 652, 2020, 13, 51696], "temperature": 0.0, "avg_logprob": -0.0750089409077062, "compression_ratio": 2.0375, "no_speech_prob": 0.058340881019830704}, {"id": 4642, "seek": 2034076, "start": 20340.84, "end": 20348.44, "text": " So what this next line here is going to do is convert our entire string data set into characters.", "tokens": [50368, 407, 437, 341, 958, 1622, 510, 307, 516, 281, 360, 307, 7620, 527, 2302, 6798, 1412, 992, 666, 4342, 13, 50748], "temperature": 0.0, "avg_logprob": -0.10204389018397178, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004905154928565025}, {"id": 4643, "seek": 2034076, "start": 20348.44, "end": 20352.679999999997, "text": " And it's actually going to allow us to have a stream of characters, which means that it's", "tokens": [50748, 400, 309, 311, 767, 516, 281, 2089, 505, 281, 362, 257, 4309, 295, 4342, 11, 597, 1355, 300, 309, 311, 50960], "temperature": 0.0, "avg_logprob": -0.10204389018397178, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004905154928565025}, {"id": 4644, "seek": 2034076, "start": 20352.679999999997, "end": 20359.16, "text": " going to essentially contain, you know, 1.1 million characters inside of this TF dot data set", "tokens": [50960, 516, 281, 4476, 5304, 11, 291, 458, 11, 502, 13, 16, 2459, 4342, 1854, 295, 341, 40964, 5893, 1412, 992, 51284], "temperature": 0.0, "avg_logprob": -0.10204389018397178, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004905154928565025}, {"id": 4645, "seek": 2034076, "start": 20359.879999999997, "end": 20364.76, "text": " object from tensor slices. That's what that's doing. Next, so let's run this and make sure this", "tokens": [51320, 2657, 490, 40863, 19793, 13, 663, 311, 437, 300, 311, 884, 13, 3087, 11, 370, 718, 311, 1190, 341, 293, 652, 988, 341, 51564], "temperature": 0.0, "avg_logprob": -0.10204389018397178, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004905154928565025}, {"id": 4646, "seek": 2036476, "start": 20364.76, "end": 20370.92, "text": " works. All right, what we're going to do is say sequences is equal to char data set dot batch", "tokens": [50364, 1985, 13, 1057, 558, 11, 437, 321, 434, 516, 281, 360, 307, 584, 22978, 307, 2681, 281, 1290, 1412, 992, 5893, 15245, 50672], "temperature": 0.0, "avg_logprob": -0.07777949630237016, "compression_ratio": 1.7992565055762082, "no_speech_prob": 0.09806754440069199}, {"id": 4647, "seek": 2036476, "start": 20370.92, "end": 20376.679999999997, "text": " sequence length is the length of each batch. So in this case, 101 and then drop remainder means", "tokens": [50672, 8310, 4641, 307, 264, 4641, 295, 1184, 15245, 13, 407, 294, 341, 1389, 11, 21055, 293, 550, 3270, 29837, 1355, 50960], "temperature": 0.0, "avg_logprob": -0.07777949630237016, "compression_ratio": 1.7992565055762082, "no_speech_prob": 0.09806754440069199}, {"id": 4648, "seek": 2036476, "start": 20376.679999999997, "end": 20383.719999999998, "text": " let's say that we have, you know, 105 characters in our text, well, since we need sequences of", "tokens": [50960, 718, 311, 584, 300, 321, 362, 11, 291, 458, 11, 33705, 4342, 294, 527, 2487, 11, 731, 11, 1670, 321, 643, 22978, 295, 51312], "temperature": 0.0, "avg_logprob": -0.07777949630237016, "compression_ratio": 1.7992565055762082, "no_speech_prob": 0.09806754440069199}, {"id": 4649, "seek": 2036476, "start": 20383.719999999998, "end": 20389.239999999998, "text": " length 101, we'll just drop the last four characters of our text, because we can't even put those into", "tokens": [51312, 4641, 21055, 11, 321, 603, 445, 3270, 264, 1036, 1451, 4342, 295, 527, 2487, 11, 570, 321, 393, 380, 754, 829, 729, 666, 51588], "temperature": 0.0, "avg_logprob": -0.07777949630237016, "compression_ratio": 1.7992565055762082, "no_speech_prob": 0.09806754440069199}, {"id": 4650, "seek": 2036476, "start": 20389.239999999998, "end": 20394.039999999997, "text": " a batch. So that's what this is doing for us is going to take our entire character data set here", "tokens": [51588, 257, 15245, 13, 407, 300, 311, 437, 341, 307, 884, 337, 505, 307, 516, 281, 747, 527, 2302, 2517, 1412, 992, 510, 51828], "temperature": 0.0, "avg_logprob": -0.07777949630237016, "compression_ratio": 1.7992565055762082, "no_speech_prob": 0.09806754440069199}, {"id": 4651, "seek": 2039404, "start": 20394.120000000003, "end": 20399.16, "text": " that we've created and batch it into length of 101, and then just drop the remainder. So that's", "tokens": [50368, 300, 321, 600, 2942, 293, 15245, 309, 666, 4641, 295, 21055, 11, 293, 550, 445, 3270, 264, 29837, 13, 407, 300, 311, 50620], "temperature": 0.0, "avg_logprob": -0.08760389245074729, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0037070370744913816}, {"id": 4652, "seek": 2039404, "start": 20399.16, "end": 20406.04, "text": " what we're going to do here. So sequences does now split input target. What this is going to do", "tokens": [50620, 437, 321, 434, 516, 281, 360, 510, 13, 407, 22978, 775, 586, 7472, 4846, 3779, 13, 708, 341, 307, 516, 281, 360, 50964], "temperature": 0.0, "avg_logprob": -0.08760389245074729, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0037070370744913816}, {"id": 4653, "seek": 2039404, "start": 20406.04, "end": 20411.4, "text": " essentially is just create those training examples that we needed. So taking this, these sequences", "tokens": [50964, 4476, 307, 445, 1884, 729, 3097, 5110, 300, 321, 2978, 13, 407, 1940, 341, 11, 613, 22978, 51232], "temperature": 0.0, "avg_logprob": -0.08760389245074729, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0037070370744913816}, {"id": 4654, "seek": 2039404, "start": 20411.4, "end": 20416.920000000002, "text": " of 101 length and converting them into the input and target text, and I'll show you how they work", "tokens": [51232, 295, 21055, 4641, 293, 29942, 552, 666, 264, 4846, 293, 3779, 2487, 11, 293, 286, 603, 855, 291, 577, 436, 589, 51508], "temperature": 0.0, "avg_logprob": -0.08760389245074729, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0037070370744913816}, {"id": 4655, "seek": 2039404, "start": 20416.920000000002, "end": 20423.48, "text": " in a second, we can do this convert the sequences to that by just mapping them to this function.", "tokens": [51508, 294, 257, 1150, 11, 321, 393, 360, 341, 7620, 264, 22978, 281, 300, 538, 445, 18350, 552, 281, 341, 2445, 13, 51836], "temperature": 0.0, "avg_logprob": -0.08760389245074729, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0037070370744913816}, {"id": 4656, "seek": 2042348, "start": 20423.48, "end": 20428.28, "text": " So that's what this function does. So if we say sequences dot map, and we put this function here,", "tokens": [50364, 407, 300, 311, 437, 341, 2445, 775, 13, 407, 498, 321, 584, 22978, 5893, 4471, 11, 293, 321, 829, 341, 2445, 510, 11, 50604], "temperature": 0.0, "avg_logprob": -0.08646456909179688, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008040626998990774}, {"id": 4657, "seek": 2042348, "start": 20428.28, "end": 20433.64, "text": " that means every single sequence will have this operation applied to it. And that will be stored", "tokens": [50604, 300, 1355, 633, 2167, 8310, 486, 362, 341, 6916, 6456, 281, 309, 13, 400, 300, 486, 312, 12187, 50872], "temperature": 0.0, "avg_logprob": -0.08646456909179688, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008040626998990774}, {"id": 4658, "seek": 2042348, "start": 20433.64, "end": 20438.84, "text": " inside this data set object. Or I guess you'd say object, but we'll also just say that's it's", "tokens": [50872, 1854, 341, 1412, 992, 2657, 13, 1610, 286, 2041, 291, 1116, 584, 2657, 11, 457, 321, 603, 611, 445, 584, 300, 311, 309, 311, 51132], "temperature": 0.0, "avg_logprob": -0.08646456909179688, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008040626998990774}, {"id": 4659, "seek": 2042348, "start": 20438.84, "end": 20443.399999999998, "text": " going to be, you know, the variable, right? So if we want to look at an example of how this works,", "tokens": [51132, 516, 281, 312, 11, 291, 458, 11, 264, 7006, 11, 558, 30, 407, 498, 321, 528, 281, 574, 412, 364, 1365, 295, 577, 341, 1985, 11, 51360], "temperature": 0.0, "avg_logprob": -0.08646456909179688, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008040626998990774}, {"id": 4660, "seek": 2042348, "start": 20444.12, "end": 20448.84, "text": " we can kind of see. So it just says example, the input will be first citizen, before we proceed", "tokens": [51396, 321, 393, 733, 295, 536, 13, 407, 309, 445, 1619, 1365, 11, 264, 4846, 486, 312, 700, 13326, 11, 949, 321, 8991, 51632], "temperature": 0.0, "avg_logprob": -0.08646456909179688, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008040626998990774}, {"id": 4661, "seek": 2044884, "start": 20448.84, "end": 20454.04, "text": " any further here, me speak, all speak, speak, first citizen, you and the output notice the first", "tokens": [50364, 604, 3052, 510, 11, 385, 1710, 11, 439, 1710, 11, 1710, 11, 700, 13326, 11, 291, 293, 264, 5598, 3449, 264, 700, 50624], "temperature": 0.0, "avg_logprob": -0.15876238686697824, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.11276939511299133}, {"id": 4662, "seek": 2044884, "start": 20454.04, "end": 20460.12, "text": " character is gone, starts at I. And the last character is actually just a space here. Whereas", "tokens": [50624, 2517, 307, 2780, 11, 3719, 412, 286, 13, 400, 264, 1036, 2517, 307, 767, 445, 257, 1901, 510, 13, 13813, 50928], "temperature": 0.0, "avg_logprob": -0.15876238686697824, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.11276939511299133}, {"id": 4663, "seek": 2044884, "start": 20460.12, "end": 20463.96, "text": " here, it didn't have a space, or you can see there's no space. Here, there is a space. That's", "tokens": [50928, 510, 11, 309, 994, 380, 362, 257, 1901, 11, 420, 291, 393, 536, 456, 311, 572, 1901, 13, 1692, 11, 456, 307, 257, 1901, 13, 663, 311, 51120], "temperature": 0.0, "avg_logprob": -0.15876238686697824, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.11276939511299133}, {"id": 4664, "seek": 2044884, "start": 20463.96, "end": 20469.0, "text": " kind of what I'm trying to highlight for you. The next example, we get our all resolved rather", "tokens": [51120, 733, 295, 437, 286, 478, 1382, 281, 5078, 337, 291, 13, 440, 958, 1365, 11, 321, 483, 527, 439, 20772, 2831, 51372], "temperature": 0.0, "avg_logprob": -0.15876238686697824, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.11276939511299133}, {"id": 4665, "seek": 2044884, "start": 20469.0, "end": 20472.52, "text": " to die rather than famine, whatever it goes to here, right? And then you can see here, we omit", "tokens": [51372, 281, 978, 2831, 813, 42790, 11, 2035, 309, 1709, 281, 510, 11, 558, 30, 400, 550, 291, 393, 536, 510, 11, 321, 3406, 270, 51548], "temperature": 0.0, "avg_logprob": -0.15876238686697824, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.11276939511299133}, {"id": 4666, "seek": 2047252, "start": 20472.600000000002, "end": 20478.600000000002, "text": " that a and the next letter is actually a K, right? That's added in there. So that's how that", "tokens": [50368, 300, 257, 293, 264, 958, 5063, 307, 767, 257, 591, 11, 558, 30, 663, 311, 3869, 294, 456, 13, 407, 300, 311, 577, 300, 50668], "temperature": 0.0, "avg_logprob": -0.10042829436015308, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.20176482200622559}, {"id": 4667, "seek": 2047252, "start": 20478.600000000002, "end": 20483.8, "text": " works. Okay, so next, we need to make training batches. So we're going to say the batch size", "tokens": [50668, 1985, 13, 1033, 11, 370, 958, 11, 321, 643, 281, 652, 3097, 15245, 279, 13, 407, 321, 434, 516, 281, 584, 264, 15245, 2744, 50928], "temperature": 0.0, "avg_logprob": -0.10042829436015308, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.20176482200622559}, {"id": 4668, "seek": 2047252, "start": 20483.8, "end": 20489.48, "text": " equals 64. The vocabulary size is the length of the vocabulary, which if you remember all the way", "tokens": [50928, 6915, 12145, 13, 440, 19864, 2744, 307, 264, 4641, 295, 264, 19864, 11, 597, 498, 291, 1604, 439, 264, 636, 51212], "temperature": 0.0, "avg_logprob": -0.10042829436015308, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.20176482200622559}, {"id": 4669, "seek": 2047252, "start": 20489.48, "end": 20495.16, "text": " back up to the top of the code, was the set or the sorted set of the text, which essentially", "tokens": [51212, 646, 493, 281, 264, 1192, 295, 264, 3089, 11, 390, 264, 992, 420, 264, 25462, 992, 295, 264, 2487, 11, 597, 4476, 51496], "temperature": 0.0, "avg_logprob": -0.10042829436015308, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.20176482200622559}, {"id": 4670, "seek": 2047252, "start": 20495.16, "end": 20500.68, "text": " told us how many unique characters are in there. The embedding dimension is 256. The RNN units", "tokens": [51496, 1907, 505, 577, 867, 3845, 4342, 366, 294, 456, 13, 440, 12240, 3584, 10139, 307, 38882, 13, 440, 45702, 45, 6815, 51772], "temperature": 0.0, "avg_logprob": -0.10042829436015308, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.20176482200622559}, {"id": 4671, "seek": 2050068, "start": 20500.760000000002, "end": 20506.68, "text": " is 1024. And the buffered size is 10,000. What we're going to do now is create a data set that", "tokens": [50368, 307, 1266, 7911, 13, 400, 264, 9204, 4073, 2744, 307, 1266, 11, 1360, 13, 708, 321, 434, 516, 281, 360, 586, 307, 1884, 257, 1412, 992, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4672, "seek": 2050068, "start": 20506.68, "end": 20511.48, "text": " shuffled, we're going to switch around all these sequences, so they don't get shown in the proper", "tokens": [50664, 402, 33974, 11, 321, 434, 516, 281, 3679, 926, 439, 613, 22978, 11, 370, 436, 500, 380, 483, 4898, 294, 264, 2296, 50904], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4673, "seek": 2050068, "start": 20511.48, "end": 20516.44, "text": " order, which we actually don't want. And then we're going to batch them by the batch size. So", "tokens": [50904, 1668, 11, 597, 321, 767, 500, 380, 528, 13, 400, 550, 321, 434, 516, 281, 15245, 552, 538, 264, 15245, 2744, 13, 407, 51152], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4674, "seek": 2050068, "start": 20516.44, "end": 20520.52, "text": " if we haven't kind of gone over what batching and all this does before, I mean, you can read", "tokens": [51152, 498, 321, 2378, 380, 733, 295, 2780, 670, 437, 15245, 278, 293, 439, 341, 775, 949, 11, 286, 914, 11, 291, 393, 1401, 51356], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4675, "seek": 2050068, "start": 20520.52, "end": 20524.36, "text": " these comments, this is straight from the TensorFlow documentation, what we want to do is", "tokens": [51356, 613, 3053, 11, 341, 307, 2997, 490, 264, 37624, 14333, 11, 437, 321, 528, 281, 360, 307, 51548], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4676, "seek": 2050068, "start": 20524.36, "end": 20529.96, "text": " feed our model 64 batches of data at a time. So what we're going to do is shuffle all of the data,", "tokens": [51548, 3154, 527, 2316, 12145, 15245, 279, 295, 1412, 412, 257, 565, 13, 407, 437, 321, 434, 516, 281, 360, 307, 39426, 439, 295, 264, 1412, 11, 51828], "temperature": 0.0, "avg_logprob": -0.08238001873618678, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.012430540286004543}, {"id": 4677, "seek": 2052996, "start": 20530.52, "end": 20534.44, "text": " batch it into that size, and then again, drop the remainder, if there's not enough batches,", "tokens": [50392, 15245, 309, 666, 300, 2744, 11, 293, 550, 797, 11, 3270, 264, 29837, 11, 498, 456, 311, 406, 1547, 15245, 279, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09167109245111134, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0017006646376103163}, {"id": 4678, "seek": 2052996, "start": 20534.44, "end": 20538.92, "text": " which is what we'll do. We're going to define the embedding dimension, which is essentially", "tokens": [50588, 597, 307, 437, 321, 603, 360, 13, 492, 434, 516, 281, 6964, 264, 12240, 3584, 10139, 11, 597, 307, 4476, 50812], "temperature": 0.0, "avg_logprob": -0.09167109245111134, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0017006646376103163}, {"id": 4679, "seek": 2052996, "start": 20539.48, "end": 20544.52, "text": " how big we want every single vector to represent our words are in the embedding layer. And then", "tokens": [50840, 577, 955, 321, 528, 633, 2167, 8062, 281, 2906, 527, 2283, 366, 294, 264, 12240, 3584, 4583, 13, 400, 550, 51092], "temperature": 0.0, "avg_logprob": -0.09167109245111134, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0017006646376103163}, {"id": 4680, "seek": 2052996, "start": 20544.52, "end": 20549.96, "text": " the RNN units, I won't really discuss what that is right now. But that's essentially how many", "tokens": [51092, 264, 45702, 45, 6815, 11, 286, 1582, 380, 534, 2248, 437, 300, 307, 558, 586, 13, 583, 300, 311, 4476, 577, 867, 51364], "temperature": 0.0, "avg_logprob": -0.09167109245111134, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0017006646376103163}, {"id": 4681, "seek": 2052996, "start": 20551.48, "end": 20555.0, "text": " it's hard to really just, I'm just going to omit describing at for right now, because I don't want", "tokens": [51440, 309, 311, 1152, 281, 534, 445, 11, 286, 478, 445, 516, 281, 3406, 270, 16141, 412, 337, 558, 586, 11, 570, 286, 500, 380, 528, 51616], "temperature": 0.0, "avg_logprob": -0.09167109245111134, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0017006646376103163}, {"id": 4682, "seek": 2055500, "start": 20555.0, "end": 20561.32, "text": " to butcher an explanation. It's not that important. Anyways, okay, so now we're going to go down to", "tokens": [50364, 281, 41579, 364, 10835, 13, 467, 311, 406, 300, 1021, 13, 15585, 11, 1392, 11, 370, 586, 321, 434, 516, 281, 352, 760, 281, 50680], "temperature": 0.0, "avg_logprob": -0.07735899460217184, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.009707865305244923}, {"id": 4683, "seek": 2055500, "start": 20561.32, "end": 20565.56, "text": " building the model. So we've kind of set these parameters up here. Remember what those are,", "tokens": [50680, 2390, 264, 2316, 13, 407, 321, 600, 733, 295, 992, 613, 9834, 493, 510, 13, 5459, 437, 729, 366, 11, 50892], "temperature": 0.0, "avg_logprob": -0.07735899460217184, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.009707865305244923}, {"id": 4684, "seek": 2055500, "start": 20565.56, "end": 20569.64, "text": " we've batched and we've shuffled the data set. And again, that's how this works. You can print", "tokens": [50892, 321, 600, 15245, 292, 293, 321, 600, 402, 33974, 264, 1412, 992, 13, 400, 797, 11, 300, 311, 577, 341, 1985, 13, 509, 393, 4482, 51096], "temperature": 0.0, "avg_logprob": -0.07735899460217184, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.009707865305244923}, {"id": 4685, "seek": 2055500, "start": 20569.64, "end": 20574.68, "text": " it out if you want to see what a batch actually looks like. But essentially, it's just 64 entries", "tokens": [51096, 309, 484, 498, 291, 528, 281, 536, 437, 257, 15245, 767, 1542, 411, 13, 583, 4476, 11, 309, 311, 445, 12145, 23041, 51348], "temperature": 0.0, "avg_logprob": -0.07735899460217184, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.009707865305244923}, {"id": 4686, "seek": 2055500, "start": 20574.68, "end": 20579.56, "text": " of those sequences, right? So 64 different training examples is what a batch that is.", "tokens": [51348, 295, 729, 22978, 11, 558, 30, 407, 12145, 819, 3097, 5110, 307, 437, 257, 15245, 300, 307, 13, 51592], "temperature": 0.0, "avg_logprob": -0.07735899460217184, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.009707865305244923}, {"id": 4687, "seek": 2057956, "start": 20580.36, "end": 20585.88, "text": " All right. So now we go down here, we're going to say build model, we're actually making a function", "tokens": [50404, 1057, 558, 13, 407, 586, 321, 352, 760, 510, 11, 321, 434, 516, 281, 584, 1322, 2316, 11, 321, 434, 767, 1455, 257, 2445, 50680], "temperature": 0.0, "avg_logprob": -0.1011197805404663, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.011686529032886028}, {"id": 4688, "seek": 2057956, "start": 20585.88, "end": 20590.52, "text": " that's going to return to us a built model. The reason for this is because", "tokens": [50680, 300, 311, 516, 281, 2736, 281, 505, 257, 3094, 2316, 13, 440, 1778, 337, 341, 307, 570, 50912], "temperature": 0.0, "avg_logprob": -0.1011197805404663, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.011686529032886028}, {"id": 4689, "seek": 2057956, "start": 20591.48, "end": 20596.84, "text": " right now, we're going to pass the model batches of size 64 for training, right? But what we're", "tokens": [50960, 558, 586, 11, 321, 434, 516, 281, 1320, 264, 2316, 15245, 279, 295, 2744, 12145, 337, 3097, 11, 558, 30, 583, 437, 321, 434, 51228], "temperature": 0.0, "avg_logprob": -0.1011197805404663, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.011686529032886028}, {"id": 4690, "seek": 2057956, "start": 20596.84, "end": 20602.280000000002, "text": " going to do later is save this model. And then we're going to patch pass it batches of one pieces", "tokens": [51228, 516, 281, 360, 1780, 307, 3155, 341, 2316, 13, 400, 550, 321, 434, 516, 281, 9972, 1320, 309, 15245, 279, 295, 472, 3755, 51500], "temperature": 0.0, "avg_logprob": -0.1011197805404663, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.011686529032886028}, {"id": 4691, "seek": 2057956, "start": 20602.280000000002, "end": 20607.4, "text": " of, you know, training whatever data, so that it can actually make a prediction on just one", "tokens": [51500, 295, 11, 291, 458, 11, 3097, 2035, 1412, 11, 370, 300, 309, 393, 767, 652, 257, 17630, 322, 445, 472, 51756], "temperature": 0.0, "avg_logprob": -0.1011197805404663, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.011686529032886028}, {"id": 4692, "seek": 2060740, "start": 20608.120000000003, "end": 20612.440000000002, "text": " piece of data. Because for right now, what it's going to do is takes a batch size of 64, it's", "tokens": [50400, 2522, 295, 1412, 13, 1436, 337, 558, 586, 11, 437, 309, 311, 516, 281, 360, 307, 2516, 257, 15245, 2744, 295, 12145, 11, 309, 311, 50616], "temperature": 0.0, "avg_logprob": -0.06123553466796875, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.0034832991659641266}, {"id": 4693, "seek": 2060740, "start": 20612.440000000002, "end": 20617.800000000003, "text": " going to take 64 training examples, and return to us 64 outputs. That's what this model is going", "tokens": [50616, 516, 281, 747, 12145, 3097, 5110, 11, 293, 2736, 281, 505, 12145, 23930, 13, 663, 311, 437, 341, 2316, 307, 516, 50884], "temperature": 0.0, "avg_logprob": -0.06123553466796875, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.0034832991659641266}, {"id": 4694, "seek": 2060740, "start": 20617.800000000003, "end": 20623.72, "text": " to be built to do the way we build it now to start. But later on, we're going to rebuild the model", "tokens": [50884, 281, 312, 3094, 281, 360, 264, 636, 321, 1322, 309, 586, 281, 722, 13, 583, 1780, 322, 11, 321, 434, 516, 281, 16877, 264, 2316, 51180], "temperature": 0.0, "avg_logprob": -0.06123553466796875, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.0034832991659641266}, {"id": 4695, "seek": 2060740, "start": 20623.72, "end": 20628.120000000003, "text": " using the same parameters that we've saved and trained for the model, but change it to just be", "tokens": [51180, 1228, 264, 912, 9834, 300, 321, 600, 6624, 293, 8895, 337, 264, 2316, 11, 457, 1319, 309, 281, 445, 312, 51400], "temperature": 0.0, "avg_logprob": -0.06123553466796875, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.0034832991659641266}, {"id": 4696, "seek": 2060740, "start": 20628.120000000003, "end": 20633.640000000003, "text": " a batch size of one, so that that way we can get one prediction for one input sequence, right?", "tokens": [51400, 257, 15245, 2744, 295, 472, 11, 370, 300, 300, 636, 321, 393, 483, 472, 17630, 337, 472, 4846, 8310, 11, 558, 30, 51676], "temperature": 0.0, "avg_logprob": -0.06123553466796875, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.0034832991659641266}, {"id": 4697, "seek": 2063364, "start": 20634.28, "end": 20638.6, "text": " So that's why I'm creating this build model function. Now in here, it's going to have the", "tokens": [50396, 407, 300, 311, 983, 286, 478, 4084, 341, 1322, 2316, 2445, 13, 823, 294, 510, 11, 309, 311, 516, 281, 362, 264, 50612], "temperature": 0.0, "avg_logprob": -0.0885074079529313, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.001064928830601275}, {"id": 4698, "seek": 2063364, "start": 20638.6, "end": 20644.44, "text": " vocabulary sizes, first argument, the embedding dimension, which remember was 256 as a second", "tokens": [50612, 19864, 11602, 11, 700, 6770, 11, 264, 12240, 3584, 10139, 11, 597, 1604, 390, 38882, 382, 257, 1150, 50904], "temperature": 0.0, "avg_logprob": -0.0885074079529313, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.001064928830601275}, {"id": 4699, "seek": 2063364, "start": 20644.44, "end": 20649.64, "text": " argument, but also these are the parameters up here, right? And then we're going to find the batch", "tokens": [50904, 6770, 11, 457, 611, 613, 366, 264, 9834, 493, 510, 11, 558, 30, 400, 550, 321, 434, 516, 281, 915, 264, 15245, 51164], "temperature": 0.0, "avg_logprob": -0.0885074079529313, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.001064928830601275}, {"id": 4700, "seek": 2063364, "start": 20649.64, "end": 20655.88, "text": " size as you know, batch size, none, what this none means is we don't know how long the sequences", "tokens": [51164, 2744, 382, 291, 458, 11, 15245, 2744, 11, 6022, 11, 437, 341, 6022, 1355, 307, 321, 500, 380, 458, 577, 938, 264, 22978, 51476], "temperature": 0.0, "avg_logprob": -0.0885074079529313, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.001064928830601275}, {"id": 4701, "seek": 2063364, "start": 20655.88, "end": 20661.88, "text": " are going to be in each batch. All we know is that we're going to have 64 entries in each batch.", "tokens": [51476, 366, 516, 281, 312, 294, 1184, 15245, 13, 1057, 321, 458, 307, 300, 321, 434, 516, 281, 362, 12145, 23041, 294, 1184, 15245, 13, 51776], "temperature": 0.0, "avg_logprob": -0.0885074079529313, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.001064928830601275}, {"id": 4702, "seek": 2066188, "start": 20661.88, "end": 20667.4, "text": " And then of those 64 entries, so training examples, right, we don't know how long each one", "tokens": [50364, 400, 550, 295, 729, 12145, 23041, 11, 370, 3097, 5110, 11, 558, 11, 321, 500, 380, 458, 577, 938, 1184, 472, 50640], "temperature": 0.0, "avg_logprob": -0.06805431659405048, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0012448100605979562}, {"id": 4703, "seek": 2066188, "start": 20667.4, "end": 20671.32, "text": " will be. Although in our case, we're going to use ones that are length 100. But when we actually", "tokens": [50640, 486, 312, 13, 5780, 294, 527, 1389, 11, 321, 434, 516, 281, 764, 2306, 300, 366, 4641, 2319, 13, 583, 562, 321, 767, 50836], "temperature": 0.0, "avg_logprob": -0.06805431659405048, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0012448100605979562}, {"id": 4704, "seek": 2066188, "start": 20671.32, "end": 20675.72, "text": " use the model to make predictions, we don't know how long the sequence is going to be that we input", "tokens": [50836, 764, 264, 2316, 281, 652, 21264, 11, 321, 500, 380, 458, 577, 938, 264, 8310, 307, 516, 281, 312, 300, 321, 4846, 51056], "temperature": 0.0, "avg_logprob": -0.06805431659405048, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0012448100605979562}, {"id": 4705, "seek": 2066188, "start": 20675.72, "end": 20681.0, "text": " so we leave this none. Next, we'll make an LSTM layer, which is a long short term memory RNN", "tokens": [51056, 370, 321, 1856, 341, 6022, 13, 3087, 11, 321, 603, 652, 364, 441, 6840, 44, 4583, 11, 597, 307, 257, 938, 2099, 1433, 4675, 45702, 45, 51320], "temperature": 0.0, "avg_logprob": -0.06805431659405048, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0012448100605979562}, {"id": 4706, "seek": 2066188, "start": 20681.0, "end": 20685.8, "text": " units, which is 1024, which again, I don't really want to explain, but you can look up if you want", "tokens": [51320, 6815, 11, 597, 307, 1266, 7911, 11, 597, 797, 11, 286, 500, 380, 534, 528, 281, 2903, 11, 457, 291, 393, 574, 493, 498, 291, 528, 51560], "temperature": 0.0, "avg_logprob": -0.06805431659405048, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0012448100605979562}, {"id": 4707, "seek": 2068580, "start": 20685.88, "end": 20694.76, "text": " return sequences means return the intermediate stage at every step. The reason we're doing this", "tokens": [50368, 2736, 22978, 1355, 2736, 264, 19376, 3233, 412, 633, 1823, 13, 440, 1778, 321, 434, 884, 341, 50812], "temperature": 0.0, "avg_logprob": -0.06520693567064073, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.024418827146291733}, {"id": 4708, "seek": 2068580, "start": 20695.8, "end": 20701.559999999998, "text": " is because we want to look at what the model seeing at the intermediate steps and not just", "tokens": [50864, 307, 570, 321, 528, 281, 574, 412, 437, 264, 2316, 2577, 412, 264, 19376, 4439, 293, 406, 445, 51152], "temperature": 0.0, "avg_logprob": -0.06520693567064073, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.024418827146291733}, {"id": 4709, "seek": 2068580, "start": 20701.559999999998, "end": 20706.68, "text": " the final stage. So if you leave this as false, and you don't set this to true, what happens is", "tokens": [51152, 264, 2572, 3233, 13, 407, 498, 291, 1856, 341, 382, 7908, 11, 293, 291, 500, 380, 992, 341, 281, 2074, 11, 437, 2314, 307, 51408], "temperature": 0.0, "avg_logprob": -0.06520693567064073, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.024418827146291733}, {"id": 4710, "seek": 2068580, "start": 20706.68, "end": 20713.48, "text": " this LSTM just returns one output that tells us what the model kind of found at the very last", "tokens": [51408, 341, 441, 6840, 44, 445, 11247, 472, 5598, 300, 5112, 505, 437, 264, 2316, 733, 295, 1352, 412, 264, 588, 1036, 51748], "temperature": 0.0, "avg_logprob": -0.06520693567064073, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.024418827146291733}, {"id": 4711, "seek": 2071348, "start": 20713.48, "end": 20718.6, "text": " time step. But we actually want the output at every single time step for this specific model.", "tokens": [50364, 565, 1823, 13, 583, 321, 767, 528, 264, 5598, 412, 633, 2167, 565, 1823, 337, 341, 2685, 2316, 13, 50620], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4712, "seek": 2071348, "start": 20718.6, "end": 20723.88, "text": " And that's why we're setting this true, stateful, not going to talk about that one right now,", "tokens": [50620, 400, 300, 311, 983, 321, 434, 3287, 341, 2074, 11, 1785, 906, 11, 406, 516, 281, 751, 466, 300, 472, 558, 586, 11, 50884], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4713, "seek": 2071348, "start": 20723.88, "end": 20727.239999999998, "text": " that's something you can look up if you want. And then recurrent initializer is just what", "tokens": [50884, 300, 311, 746, 291, 393, 574, 493, 498, 291, 528, 13, 400, 550, 18680, 1753, 5883, 6545, 307, 445, 437, 51052], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4714, "seek": 2071348, "start": 20727.239999999998, "end": 20731.96, "text": " these values are going to start at in the LSTM. We're just picking this because this is what", "tokens": [51052, 613, 4190, 366, 516, 281, 722, 412, 294, 264, 441, 6840, 44, 13, 492, 434, 445, 8867, 341, 570, 341, 307, 437, 51288], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4715, "seek": 2071348, "start": 20731.96, "end": 20737.399999999998, "text": " TensorFlow has kind of said is a good default to pick. I won't go into more depth about that", "tokens": [51288, 37624, 575, 733, 295, 848, 307, 257, 665, 7576, 281, 1888, 13, 286, 1582, 380, 352, 666, 544, 7161, 466, 300, 51560], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4716, "seek": 2071348, "start": 20737.399999999998, "end": 20742.68, "text": " again, things that you can look up more if you want. Finally, we have a dense layer, which is", "tokens": [51560, 797, 11, 721, 300, 291, 393, 574, 493, 544, 498, 291, 528, 13, 6288, 11, 321, 362, 257, 18011, 4583, 11, 597, 307, 51824], "temperature": 0.0, "avg_logprob": -0.07190574054986658, "compression_ratio": 1.735202492211838, "no_speech_prob": 0.02595433034002781}, {"id": 4717, "seek": 2074268, "start": 20742.68, "end": 20749.0, "text": " going to contain the amount of vocabulary size nodes. The reason we're doing this is because we", "tokens": [50364, 516, 281, 5304, 264, 2372, 295, 19864, 2744, 13891, 13, 440, 1778, 321, 434, 884, 341, 307, 570, 321, 50680], "temperature": 0.0, "avg_logprob": -0.05825556649102105, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.009123899042606354}, {"id": 4718, "seek": 2074268, "start": 20749.0, "end": 20754.2, "text": " want the final layer to have the amount of nodes in it equal to the amount of characters in the", "tokens": [50680, 528, 264, 2572, 4583, 281, 362, 264, 2372, 295, 13891, 294, 309, 2681, 281, 264, 2372, 295, 4342, 294, 264, 50940], "temperature": 0.0, "avg_logprob": -0.05825556649102105, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.009123899042606354}, {"id": 4719, "seek": 2074268, "start": 20754.2, "end": 20759.8, "text": " vocabulary. This way, every single one of those nodes can represent a probability distribution", "tokens": [50940, 19864, 13, 639, 636, 11, 633, 2167, 472, 295, 729, 13891, 393, 2906, 257, 8482, 7316, 51220], "temperature": 0.0, "avg_logprob": -0.05825556649102105, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.009123899042606354}, {"id": 4720, "seek": 2074268, "start": 20759.8, "end": 20766.2, "text": " that that character comes next. So all of those nodes value some sum together should give us the", "tokens": [51220, 300, 300, 2517, 1487, 958, 13, 407, 439, 295, 729, 13891, 2158, 512, 2408, 1214, 820, 976, 505, 264, 51540], "temperature": 0.0, "avg_logprob": -0.05825556649102105, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.009123899042606354}, {"id": 4721, "seek": 2074268, "start": 20766.2, "end": 20771.96, "text": " value of one. And that's going to allow us to look at that last layer as a predictive layer,", "tokens": [51540, 2158, 295, 472, 13, 400, 300, 311, 516, 281, 2089, 505, 281, 574, 412, 300, 1036, 4583, 382, 257, 35521, 4583, 11, 51828], "temperature": 0.0, "avg_logprob": -0.05825556649102105, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.009123899042606354}, {"id": 4722, "seek": 2077196, "start": 20771.96, "end": 20776.12, "text": " where it's telling us the probability that these characters come next, and we've discussed how", "tokens": [50364, 689, 309, 311, 3585, 505, 264, 8482, 300, 613, 4342, 808, 958, 11, 293, 321, 600, 7152, 577, 50572], "temperature": 0.0, "avg_logprob": -0.06461229566800392, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0018101370660588145}, {"id": 4723, "seek": 2077196, "start": 20776.12, "end": 20782.36, "text": " that's worked previously with other neural networks. So let's run this now. Name embedding", "tokens": [50572, 300, 311, 2732, 8046, 365, 661, 18161, 9590, 13, 407, 718, 311, 1190, 341, 586, 13, 13866, 12240, 3584, 50884], "temperature": 0.0, "avg_logprob": -0.06461229566800392, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0018101370660588145}, {"id": 4724, "seek": 2077196, "start": 20782.36, "end": 20787.8, "text": " dim is not defined, which I mean, believes I have not ran this yet. So now we run that,", "tokens": [50884, 5013, 307, 406, 7642, 11, 597, 286, 914, 11, 12307, 286, 362, 406, 5872, 341, 1939, 13, 407, 586, 321, 1190, 300, 11, 51156], "temperature": 0.0, "avg_logprob": -0.06461229566800392, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0018101370660588145}, {"id": 4725, "seek": 2077196, "start": 20787.8, "end": 20792.12, "text": " and we should be good. So if we look at the model summary, we can see we have our initial", "tokens": [51156, 293, 321, 820, 312, 665, 13, 407, 498, 321, 574, 412, 264, 2316, 12691, 11, 321, 393, 536, 321, 362, 527, 5883, 51372], "temperature": 0.0, "avg_logprob": -0.06461229566800392, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0018101370660588145}, {"id": 4726, "seek": 2077196, "start": 20792.12, "end": 20797.399999999998, "text": " embedding layer, we have our LSTM, and then we have our dense layer at the end. Now notice", "tokens": [51372, 12240, 3584, 4583, 11, 321, 362, 527, 441, 6840, 44, 11, 293, 550, 321, 362, 527, 18011, 4583, 412, 264, 917, 13, 823, 3449, 51636], "temperature": 0.0, "avg_logprob": -0.06461229566800392, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0018101370660588145}, {"id": 4727, "seek": 2079740, "start": 20797.4, "end": 20803.16, "text": " 64 is the batch size, right? That's the initial shape. None is the length of the sequence,", "tokens": [50364, 12145, 307, 264, 15245, 2744, 11, 558, 30, 663, 311, 264, 5883, 3909, 13, 14492, 307, 264, 4641, 295, 264, 8310, 11, 50652], "temperature": 0.0, "avg_logprob": -0.08479467252405679, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.011686508543789387}, {"id": 4728, "seek": 2079740, "start": 20803.16, "end": 20809.800000000003, "text": " which we don't know. And then this is going to be just the output dimension or sorry, this is", "tokens": [50652, 597, 321, 500, 380, 458, 13, 400, 550, 341, 307, 516, 281, 312, 445, 264, 5598, 10139, 420, 2597, 11, 341, 307, 50984], "temperature": 0.0, "avg_logprob": -0.08479467252405679, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.011686508543789387}, {"id": 4729, "seek": 2079740, "start": 20810.440000000002, "end": 20816.120000000003, "text": " the amount of values in the vector, right? So we're going to start with 256. We'll just do", "tokens": [51016, 264, 2372, 295, 4190, 294, 264, 8062, 11, 558, 30, 407, 321, 434, 516, 281, 722, 365, 38882, 13, 492, 603, 445, 360, 51300], "temperature": 0.0, "avg_logprob": -0.08479467252405679, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.011686508543789387}, {"id": 4730, "seek": 2079740, "start": 20816.120000000003, "end": 20821.16, "text": " 1,024 units in the LSTM and then 65 stands for the amount of nodes, because that is the", "tokens": [51300, 502, 11, 15, 7911, 6815, 294, 264, 441, 6840, 44, 293, 550, 11624, 7382, 337, 264, 2372, 295, 13891, 11, 570, 300, 307, 264, 51552], "temperature": 0.0, "avg_logprob": -0.08479467252405679, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.011686508543789387}, {"id": 4731, "seek": 2079740, "start": 20821.16, "end": 20826.440000000002, "text": " length of the vocabulary. Alright, so combined, that's how many trainable parameters we get.", "tokens": [51552, 4641, 295, 264, 19864, 13, 2798, 11, 370, 9354, 11, 300, 311, 577, 867, 3847, 712, 9834, 321, 483, 13, 51816], "temperature": 0.0, "avg_logprob": -0.08479467252405679, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.011686508543789387}, {"id": 4732, "seek": 2082644, "start": 20827.079999999998, "end": 20831.399999999998, "text": " You can see each of them for each layer. And now it's time to move on to the next section.", "tokens": [50396, 509, 393, 536, 1184, 295, 552, 337, 1184, 4583, 13, 400, 586, 309, 311, 565, 281, 1286, 322, 281, 264, 958, 3541, 13, 50612], "temperature": 0.0, "avg_logprob": -0.05598219564138365, "compression_ratio": 1.6441281138790036, "no_speech_prob": 0.0033764115069061518}, {"id": 4733, "seek": 2082644, "start": 20831.399999999998, "end": 20835.32, "text": " Okay, so now we're moving on to the next step of the tutorial, which is creating a loss function", "tokens": [50612, 1033, 11, 370, 586, 321, 434, 2684, 322, 281, 264, 958, 1823, 295, 264, 7073, 11, 597, 307, 4084, 257, 4470, 2445, 50808], "temperature": 0.0, "avg_logprob": -0.05598219564138365, "compression_ratio": 1.6441281138790036, "no_speech_prob": 0.0033764115069061518}, {"id": 4734, "seek": 2082644, "start": 20835.32, "end": 20839.559999999998, "text": " to compile our model with. Now, I'll talk about why we need to do this in a second,", "tokens": [50808, 281, 31413, 527, 2316, 365, 13, 823, 11, 286, 603, 751, 466, 983, 321, 643, 281, 360, 341, 294, 257, 1150, 11, 51020], "temperature": 0.0, "avg_logprob": -0.05598219564138365, "compression_ratio": 1.6441281138790036, "no_speech_prob": 0.0033764115069061518}, {"id": 4735, "seek": 2082644, "start": 20839.559999999998, "end": 20845.559999999998, "text": " but I first want to explore the output shape of our model. So remember, the input to our model", "tokens": [51020, 457, 286, 700, 528, 281, 6839, 264, 5598, 3909, 295, 527, 2316, 13, 407, 1604, 11, 264, 4846, 281, 527, 2316, 51320], "temperature": 0.0, "avg_logprob": -0.05598219564138365, "compression_ratio": 1.6441281138790036, "no_speech_prob": 0.0033764115069061518}, {"id": 4736, "seek": 2082644, "start": 20845.559999999998, "end": 20851.48, "text": " is something that is of length 64, because we're going to have batches of 64 training examples,", "tokens": [51320, 307, 746, 300, 307, 295, 4641, 12145, 11, 570, 321, 434, 516, 281, 362, 15245, 279, 295, 12145, 3097, 5110, 11, 51616], "temperature": 0.0, "avg_logprob": -0.05598219564138365, "compression_ratio": 1.6441281138790036, "no_speech_prob": 0.0033764115069061518}, {"id": 4737, "seek": 2085148, "start": 20851.48, "end": 20857.239999999998, "text": " right? So every time we feed our model, we're going to give it 64 training examples. Now what", "tokens": [50364, 558, 30, 407, 633, 565, 321, 3154, 527, 2316, 11, 321, 434, 516, 281, 976, 309, 12145, 3097, 5110, 13, 823, 437, 50652], "temperature": 0.0, "avg_logprob": -0.08446421312249225, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.014956044964492321}, {"id": 4738, "seek": 2085148, "start": 20857.239999999998, "end": 20862.92, "text": " those training examples are, are sequences of length 100. That's what I want you to remember.", "tokens": [50652, 729, 3097, 5110, 366, 11, 366, 22978, 295, 4641, 2319, 13, 663, 311, 437, 286, 528, 291, 281, 1604, 13, 50936], "temperature": 0.0, "avg_logprob": -0.08446421312249225, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.014956044964492321}, {"id": 4739, "seek": 2085148, "start": 20862.92, "end": 20870.2, "text": " We're passing 64 entries that are all of length 100 into the model as its training data, right?", "tokens": [50936, 492, 434, 8437, 12145, 23041, 300, 366, 439, 295, 4641, 2319, 666, 264, 2316, 382, 1080, 3097, 1412, 11, 558, 30, 51300], "temperature": 0.0, "avg_logprob": -0.08446421312249225, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.014956044964492321}, {"id": 4740, "seek": 2085148, "start": 20870.84, "end": 20875.399999999998, "text": " But sometimes, and when we make predictions with the model later on, we'll be passing it", "tokens": [51332, 583, 2171, 11, 293, 562, 321, 652, 21264, 365, 264, 2316, 1780, 322, 11, 321, 603, 312, 8437, 309, 51560], "temperature": 0.0, "avg_logprob": -0.08446421312249225, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.014956044964492321}, {"id": 4741, "seek": 2087540, "start": 20875.4, "end": 20881.4, "text": " just one entry that is of some variable length, right? And that's why we've created", "tokens": [50364, 445, 472, 8729, 300, 307, 295, 512, 7006, 4641, 11, 558, 30, 400, 300, 311, 983, 321, 600, 2942, 50664], "temperature": 0.0, "avg_logprob": -0.07649025674593651, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01640220172703266}, {"id": 4742, "seek": 2087540, "start": 20882.120000000003, "end": 20887.4, "text": " this build model function, so that we can build this model using the parameters that we've saved", "tokens": [50700, 341, 1322, 2316, 2445, 11, 370, 300, 321, 393, 1322, 341, 2316, 1228, 264, 9834, 300, 321, 600, 6624, 50964], "temperature": 0.0, "avg_logprob": -0.07649025674593651, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01640220172703266}, {"id": 4743, "seek": 2087540, "start": 20887.4, "end": 20894.52, "text": " later on, once we train the model, and it can expect a different input shape, right? Because", "tokens": [50964, 1780, 322, 11, 1564, 321, 3847, 264, 2316, 11, 293, 309, 393, 2066, 257, 819, 4846, 3909, 11, 558, 30, 1436, 51320], "temperature": 0.0, "avg_logprob": -0.07649025674593651, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01640220172703266}, {"id": 4744, "seek": 2087540, "start": 20894.52, "end": 20897.56, "text": " when we're training it, it's going to be given a different shape, and we're actually testing with", "tokens": [51320, 562, 321, 434, 3097, 309, 11, 309, 311, 516, 281, 312, 2212, 257, 819, 3909, 11, 293, 321, 434, 767, 4997, 365, 51472], "temperature": 0.0, "avg_logprob": -0.07649025674593651, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01640220172703266}, {"id": 4745, "seek": 2087540, "start": 20897.56, "end": 20903.800000000003, "text": " it. Now what I want to do is explore the output of this model, though, at the current point in time.", "tokens": [51472, 309, 13, 823, 437, 286, 528, 281, 360, 307, 6839, 264, 5598, 295, 341, 2316, 11, 1673, 11, 412, 264, 2190, 935, 294, 565, 13, 51784], "temperature": 0.0, "avg_logprob": -0.07649025674593651, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01640220172703266}, {"id": 4746, "seek": 2090380, "start": 20903.8, "end": 20910.6, "text": " So we've created a model that accepts a batch of 64 training examples that are length 100. So", "tokens": [50364, 407, 321, 600, 2942, 257, 2316, 300, 33538, 257, 15245, 295, 12145, 3097, 5110, 300, 366, 4641, 2319, 13, 407, 50704], "temperature": 0.0, "avg_logprob": -0.08406949550547499, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.00036828903830610216}, {"id": 4747, "seek": 2090380, "start": 20910.6, "end": 20915.64, "text": " let's just look at what the output is from the final layer. Give this a second to run.", "tokens": [50704, 718, 311, 445, 574, 412, 437, 264, 5598, 307, 490, 264, 2572, 4583, 13, 5303, 341, 257, 1150, 281, 1190, 13, 50956], "temperature": 0.0, "avg_logprob": -0.08406949550547499, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.00036828903830610216}, {"id": 4748, "seek": 2090380, "start": 20916.44, "end": 20924.36, "text": " We get 64, 165. And that represents the batch size, the sequence length, and the vocabulary", "tokens": [50996, 492, 483, 12145, 11, 3165, 20, 13, 400, 300, 8855, 264, 15245, 2744, 11, 264, 8310, 4641, 11, 293, 264, 19864, 51392], "temperature": 0.0, "avg_logprob": -0.08406949550547499, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.00036828903830610216}, {"id": 4749, "seek": 2090380, "start": 20924.36, "end": 20929.32, "text": " size. Now the reason for this is we have to remember that when we create a dense layer as our last", "tokens": [51392, 2744, 13, 823, 264, 1778, 337, 341, 307, 321, 362, 281, 1604, 300, 562, 321, 1884, 257, 18011, 4583, 382, 527, 1036, 51640], "temperature": 0.0, "avg_logprob": -0.08406949550547499, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.00036828903830610216}, {"id": 4750, "seek": 2092932, "start": 20929.32, "end": 20936.6, "text": " layer that has 65 nodes, every prediction is going to contain 65 numbers. And that's going", "tokens": [50364, 4583, 300, 575, 11624, 13891, 11, 633, 17630, 307, 516, 281, 5304, 11624, 3547, 13, 400, 300, 311, 516, 50728], "temperature": 0.0, "avg_logprob": -0.06748018094471522, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.006692481692880392}, {"id": 4751, "seek": 2092932, "start": 20936.6, "end": 20942.44, "text": " to be the probability of every one of those characters occurring, right? That's what that", "tokens": [50728, 281, 312, 264, 8482, 295, 633, 472, 295, 729, 4342, 18386, 11, 558, 30, 663, 311, 437, 300, 51020], "temperature": 0.0, "avg_logprob": -0.06748018094471522, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.006692481692880392}, {"id": 4752, "seek": 2092932, "start": 20942.44, "end": 20947.48, "text": " does at the last one for us. So obviously, our last dimension is going to be 65 for the vocabulary", "tokens": [51020, 775, 412, 264, 1036, 472, 337, 505, 13, 407, 2745, 11, 527, 1036, 10139, 307, 516, 281, 312, 11624, 337, 264, 19864, 51272], "temperature": 0.0, "avg_logprob": -0.06748018094471522, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.006692481692880392}, {"id": 4753, "seek": 2092932, "start": 20947.48, "end": 20951.48, "text": " size. This is a sequence length, and that's a batch, I just want to make sure this is really", "tokens": [51272, 2744, 13, 639, 307, 257, 8310, 4641, 11, 293, 300, 311, 257, 15245, 11, 286, 445, 528, 281, 652, 988, 341, 307, 534, 51472], "temperature": 0.0, "avg_logprob": -0.06748018094471522, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.006692481692880392}, {"id": 4754, "seek": 2092932, "start": 20951.48, "end": 20954.84, "text": " clear before we keep going. Otherwise, it's going to get very confusing very quickly.", "tokens": [51472, 1850, 949, 321, 1066, 516, 13, 10328, 11, 309, 311, 516, 281, 483, 588, 13181, 588, 2661, 13, 51640], "temperature": 0.0, "avg_logprob": -0.06748018094471522, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.006692481692880392}, {"id": 4755, "seek": 2095484, "start": 20955.48, "end": 20961.16, "text": " So what I want to do now is actually look at the length of the example batch predictions,", "tokens": [50396, 407, 437, 286, 528, 281, 360, 586, 307, 767, 574, 412, 264, 4641, 295, 264, 1365, 15245, 21264, 11, 50680], "temperature": 0.0, "avg_logprob": -0.07226158282078735, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.010327141731977463}, {"id": 4756, "seek": 2095484, "start": 20961.16, "end": 20965.0, "text": " and just print them out and look at what they actually are. So example batch predictions", "tokens": [50680, 293, 445, 4482, 552, 484, 293, 574, 412, 437, 436, 767, 366, 13, 407, 1365, 15245, 21264, 50872], "temperature": 0.0, "avg_logprob": -0.07226158282078735, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.010327141731977463}, {"id": 4757, "seek": 2095484, "start": 20965.0, "end": 20971.56, "text": " is what happens when I use my model on some random input example, actually, well, the first one", "tokens": [50872, 307, 437, 2314, 562, 286, 764, 452, 2316, 322, 512, 4974, 4846, 1365, 11, 767, 11, 731, 11, 264, 700, 472, 51200], "temperature": 0.0, "avg_logprob": -0.07226158282078735, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.010327141731977463}, {"id": 4758, "seek": 2095484, "start": 20971.56, "end": 20976.84, "text": " from my data set with when it's not trained. So I can actually use my model before it's", "tokens": [51200, 490, 452, 1412, 992, 365, 562, 309, 311, 406, 8895, 13, 407, 286, 393, 767, 764, 452, 2316, 949, 309, 311, 51464], "temperature": 0.0, "avg_logprob": -0.07226158282078735, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.010327141731977463}, {"id": 4759, "seek": 2095484, "start": 20976.84, "end": 20983.0, "text": " trained with random weights and random biases and parameters, by simply using model, and then I can", "tokens": [51464, 8895, 365, 4974, 17443, 293, 4974, 32152, 293, 9834, 11, 538, 2935, 1228, 2316, 11, 293, 550, 286, 393, 51772], "temperature": 0.0, "avg_logprob": -0.07226158282078735, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.010327141731977463}, {"id": 4760, "seek": 2098300, "start": 20983.0, "end": 20987.48, "text": " put the little brackets like this and just pass in some example that I want to get a prediction", "tokens": [50364, 829, 264, 707, 26179, 411, 341, 293, 445, 1320, 294, 512, 1365, 300, 286, 528, 281, 483, 257, 17630, 50588], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4761, "seek": 2098300, "start": 20987.48, "end": 20991.48, "text": " for. So that's what I'm going to do, I'm going to give it the first batch, and it can even it shows", "tokens": [50588, 337, 13, 407, 300, 311, 437, 286, 478, 516, 281, 360, 11, 286, 478, 516, 281, 976, 309, 264, 700, 15245, 11, 293, 309, 393, 754, 309, 3110, 50788], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4762, "seek": 2098300, "start": 20991.48, "end": 20996.2, "text": " me the shape of this batch 64 100, I'm going to pass that to the model, and it's going to give us a", "tokens": [50788, 385, 264, 3909, 295, 341, 15245, 12145, 2319, 11, 286, 478, 516, 281, 1320, 300, 281, 264, 2316, 11, 293, 309, 311, 516, 281, 976, 505, 257, 51024], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4763, "seek": 2098300, "start": 20996.2, "end": 21001.96, "text": " prediction for that. And in fact, it's actually going to give us a prediction for every single", "tokens": [51024, 17630, 337, 300, 13, 400, 294, 1186, 11, 309, 311, 767, 516, 281, 976, 505, 257, 17630, 337, 633, 2167, 51312], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4764, "seek": 2098300, "start": 21001.96, "end": 21006.04, "text": " element in the batch, right, every single training example in the batch, it's going to give us a", "tokens": [51312, 4478, 294, 264, 15245, 11, 558, 11, 633, 2167, 3097, 1365, 294, 264, 15245, 11, 309, 311, 516, 281, 976, 505, 257, 51516], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4765, "seek": 2098300, "start": 21006.04, "end": 21011.64, "text": " prediction for. So let's look at what those predictions are. So this is what we get, we get", "tokens": [51516, 17630, 337, 13, 407, 718, 311, 574, 412, 437, 729, 21264, 366, 13, 407, 341, 307, 437, 321, 483, 11, 321, 483, 51796], "temperature": 0.0, "avg_logprob": -0.07339134341792057, "compression_ratio": 2.1444444444444444, "no_speech_prob": 0.007815394550561905}, {"id": 4766, "seek": 2101164, "start": 21011.72, "end": 21019.239999999998, "text": " a length 64 tensor, right? And then inside of here, we get a list inside of a list or an array", "tokens": [50368, 257, 4641, 12145, 40863, 11, 558, 30, 400, 550, 1854, 295, 510, 11, 321, 483, 257, 1329, 1854, 295, 257, 1329, 420, 364, 10225, 50744], "temperature": 0.0, "avg_logprob": -0.07529341484889511, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.01743844710290432}, {"id": 4767, "seek": 2101164, "start": 21019.239999999998, "end": 21024.84, "text": " inside of an array with all of these different predictions. So we'll stop there for this, like", "tokens": [50744, 1854, 295, 364, 10225, 365, 439, 295, 613, 819, 21264, 13, 407, 321, 603, 1590, 456, 337, 341, 11, 411, 51024], "temperature": 0.0, "avg_logprob": -0.07529341484889511, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.01743844710290432}, {"id": 4768, "seek": 2101164, "start": 21024.84, "end": 21029.0, "text": " explaining this aspect here. But you can see we're getting 64 different predictions because", "tokens": [51024, 13468, 341, 4171, 510, 13, 583, 291, 393, 536, 321, 434, 1242, 12145, 819, 21264, 570, 51232], "temperature": 0.0, "avg_logprob": -0.07529341484889511, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.01743844710290432}, {"id": 4769, "seek": 2101164, "start": 21029.0, "end": 21035.079999999998, "text": " there's 64 elements in the batch. Now, let's look at one prediction. So let's look at the very first", "tokens": [51232, 456, 311, 12145, 4959, 294, 264, 15245, 13, 823, 11, 718, 311, 574, 412, 472, 17630, 13, 407, 718, 311, 574, 412, 264, 588, 700, 51536], "temperature": 0.0, "avg_logprob": -0.07529341484889511, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.01743844710290432}, {"id": 4770, "seek": 2101164, "start": 21035.079999999998, "end": 21041.559999999998, "text": " prediction for say the first element in the batch, right? So let's do that here. And we see now that", "tokens": [51536, 17630, 337, 584, 264, 700, 4478, 294, 264, 15245, 11, 558, 30, 407, 718, 311, 360, 300, 510, 13, 400, 321, 536, 586, 300, 51860], "temperature": 0.0, "avg_logprob": -0.07529341484889511, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.01743844710290432}, {"id": 4771, "seek": 2104156, "start": 21041.56, "end": 21048.600000000002, "text": " we get a length 100 tensor. And that this is what it looks like, there's still another layer inside.", "tokens": [50364, 321, 483, 257, 4641, 2319, 40863, 13, 400, 300, 341, 307, 437, 309, 1542, 411, 11, 456, 311, 920, 1071, 4583, 1854, 13, 50716], "temperature": 0.0, "avg_logprob": -0.07893825979793773, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0005192870157770813}, {"id": 4772, "seek": 2104156, "start": 21048.600000000002, "end": 21053.24, "text": " And in fact, we can see that there's another nested layer here, right, another nested array", "tokens": [50716, 400, 294, 1186, 11, 321, 393, 536, 300, 456, 311, 1071, 15646, 292, 4583, 510, 11, 558, 11, 1071, 15646, 292, 10225, 50948], "temperature": 0.0, "avg_logprob": -0.07893825979793773, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0005192870157770813}, {"id": 4773, "seek": 2104156, "start": 21053.24, "end": 21060.36, "text": " inside of this array. So the reason for this is because at every single time step, which means", "tokens": [50948, 1854, 295, 341, 10225, 13, 407, 264, 1778, 337, 341, 307, 570, 412, 633, 2167, 565, 1823, 11, 597, 1355, 51304], "temperature": 0.0, "avg_logprob": -0.07893825979793773, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0005192870157770813}, {"id": 4774, "seek": 2104156, "start": 21060.36, "end": 21063.640000000003, "text": " the length of the sequence, right, because remember, our recurrent neural network is going to feed", "tokens": [51304, 264, 4641, 295, 264, 8310, 11, 558, 11, 570, 1604, 11, 527, 18680, 1753, 18161, 3209, 307, 516, 281, 3154, 51468], "temperature": 0.0, "avg_logprob": -0.07893825979793773, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0005192870157770813}, {"id": 4775, "seek": 2104156, "start": 21063.640000000003, "end": 21068.84, "text": " one at a time every word in the sequence. In this case, our sequences are like the 100 at every", "tokens": [51468, 472, 412, 257, 565, 633, 1349, 294, 264, 8310, 13, 682, 341, 1389, 11, 527, 22978, 366, 411, 264, 2319, 412, 633, 51728], "temperature": 0.0, "avg_logprob": -0.07893825979793773, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0005192870157770813}, {"id": 4776, "seek": 2106884, "start": 21068.84, "end": 21075.56, "text": " time step, we're actually saving that output as a, as a prediction, right? And we're passing", "tokens": [50364, 565, 1823, 11, 321, 434, 767, 6816, 300, 5598, 382, 257, 11, 382, 257, 17630, 11, 558, 30, 400, 321, 434, 8437, 50700], "temperature": 0.0, "avg_logprob": -0.054763412475585936, "compression_ratio": 1.8906882591093117, "no_speech_prob": 0.005729919299483299}, {"id": 4777, "seek": 2106884, "start": 21075.56, "end": 21081.32, "text": " that back. So we can see that for one batch, one training, sorry, not one batch, one training", "tokens": [50700, 300, 646, 13, 407, 321, 393, 536, 300, 337, 472, 15245, 11, 472, 3097, 11, 2597, 11, 406, 472, 15245, 11, 472, 3097, 50988], "temperature": 0.0, "avg_logprob": -0.054763412475585936, "compression_ratio": 1.8906882591093117, "no_speech_prob": 0.005729919299483299}, {"id": 4778, "seek": 2106884, "start": 21081.32, "end": 21086.12, "text": " example, we get 100 outputs. And these outputs are in some shape, we'll talk about what those are", "tokens": [50988, 1365, 11, 321, 483, 2319, 23930, 13, 400, 613, 23930, 366, 294, 512, 3909, 11, 321, 603, 751, 466, 437, 729, 366, 51228], "temperature": 0.0, "avg_logprob": -0.054763412475585936, "compression_ratio": 1.8906882591093117, "no_speech_prob": 0.005729919299483299}, {"id": 4779, "seek": 2106884, "start": 21086.12, "end": 21091.08, "text": " in a second. So that's something to remember that for every single training example, we get", "tokens": [51228, 294, 257, 1150, 13, 407, 300, 311, 746, 281, 1604, 300, 337, 633, 2167, 3097, 1365, 11, 321, 483, 51476], "temperature": 0.0, "avg_logprob": -0.054763412475585936, "compression_ratio": 1.8906882591093117, "no_speech_prob": 0.005729919299483299}, {"id": 4780, "seek": 2106884, "start": 21091.08, "end": 21095.72, "text": " whatever the length of that training example was outputs, because that's the way that this", "tokens": [51476, 2035, 264, 4641, 295, 300, 3097, 1365, 390, 23930, 11, 570, 300, 311, 264, 636, 300, 341, 51708], "temperature": 0.0, "avg_logprob": -0.054763412475585936, "compression_ratio": 1.8906882591093117, "no_speech_prob": 0.005729919299483299}, {"id": 4781, "seek": 2109572, "start": 21095.72, "end": 21101.56, "text": " model works. And then finally, we look at the prediction at just the very first time step. So", "tokens": [50364, 2316, 1985, 13, 400, 550, 2721, 11, 321, 574, 412, 264, 17630, 412, 445, 264, 588, 700, 565, 1823, 13, 407, 50656], "temperature": 0.0, "avg_logprob": -0.043380166730310164, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.006289564073085785}, {"id": 4782, "seek": 2109572, "start": 21101.56, "end": 21106.600000000002, "text": " this is 100 different time steps. So let's look at the first time step and see what that prediction", "tokens": [50656, 341, 307, 2319, 819, 565, 4439, 13, 407, 718, 311, 574, 412, 264, 700, 565, 1823, 293, 536, 437, 300, 17630, 50908], "temperature": 0.0, "avg_logprob": -0.043380166730310164, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.006289564073085785}, {"id": 4783, "seek": 2109572, "start": 21106.600000000002, "end": 21113.24, "text": " is. And we can see that now we get a tensor of length 65. And this is telling us the probability", "tokens": [50908, 307, 13, 400, 321, 393, 536, 300, 586, 321, 483, 257, 40863, 295, 4641, 11624, 13, 400, 341, 307, 3585, 505, 264, 8482, 51240], "temperature": 0.0, "avg_logprob": -0.043380166730310164, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.006289564073085785}, {"id": 4784, "seek": 2109572, "start": 21113.24, "end": 21119.08, "text": " of every single character occurring next at the first time step. So that's what I wanted to walk", "tokens": [51240, 295, 633, 2167, 2517, 18386, 958, 412, 264, 700, 565, 1823, 13, 407, 300, 311, 437, 286, 1415, 281, 1792, 51532], "temperature": 0.0, "avg_logprob": -0.043380166730310164, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.006289564073085785}, {"id": 4785, "seek": 2109572, "start": 21119.08, "end": 21123.960000000003, "text": " through is showing you what's actually outputted from the model, the current way that it works.", "tokens": [51532, 807, 307, 4099, 291, 437, 311, 767, 5598, 14727, 490, 264, 2316, 11, 264, 2190, 636, 300, 309, 1985, 13, 51776], "temperature": 0.0, "avg_logprob": -0.043380166730310164, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.006289564073085785}, {"id": 4786, "seek": 2112396, "start": 21124.04, "end": 21130.92, "text": " And that's why we need to actually make our own loss function to be able to determine how, you", "tokens": [50368, 400, 300, 311, 983, 321, 643, 281, 767, 652, 527, 1065, 4470, 2445, 281, 312, 1075, 281, 6997, 577, 11, 291, 50712], "temperature": 0.0, "avg_logprob": -0.07227269593659821, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0015977929579094052}, {"id": 4787, "seek": 2112396, "start": 21130.92, "end": 21135.64, "text": " know, good our models performing, when it outputs something ridiculous that looks like this, because", "tokens": [50712, 458, 11, 665, 527, 5245, 10205, 11, 562, 309, 23930, 746, 11083, 300, 1542, 411, 341, 11, 570, 50948], "temperature": 0.0, "avg_logprob": -0.07227269593659821, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0015977929579094052}, {"id": 4788, "seek": 2112396, "start": 21135.64, "end": 21140.76, "text": " there is no just built in loss function and tensor flow that can look at a three dimensional", "tokens": [50948, 456, 307, 572, 445, 3094, 294, 4470, 2445, 293, 40863, 3095, 300, 393, 574, 412, 257, 1045, 18795, 51204], "temperature": 0.0, "avg_logprob": -0.07227269593659821, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0015977929579094052}, {"id": 4789, "seek": 2112396, "start": 21140.76, "end": 21146.44, "text": " nested array of probabilities over, you know, the vocabulary size, and tell us how different the", "tokens": [51204, 15646, 292, 10225, 295, 33783, 670, 11, 291, 458, 11, 264, 19864, 2744, 11, 293, 980, 505, 577, 819, 264, 51488], "temperature": 0.0, "avg_logprob": -0.07227269593659821, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0015977929579094052}, {"id": 4790, "seek": 2112396, "start": 21146.44, "end": 21151.719999999998, "text": " two things are. So we need to make our own loss function. So if we want to determine the predicted", "tokens": [51488, 732, 721, 366, 13, 407, 321, 643, 281, 652, 527, 1065, 4470, 2445, 13, 407, 498, 321, 528, 281, 6997, 264, 19147, 51752], "temperature": 0.0, "avg_logprob": -0.07227269593659821, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0015977929579094052}, {"id": 4791, "seek": 2115172, "start": 21151.72, "end": 21159.56, "text": " character from this array, so we'll go there now. What we can do is get the categorical, what's", "tokens": [50364, 2517, 490, 341, 10225, 11, 370, 321, 603, 352, 456, 586, 13, 708, 321, 393, 360, 307, 483, 264, 19250, 804, 11, 437, 311, 50756], "temperature": 0.0, "avg_logprob": -0.08386003670572233, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.001244814950041473}, {"id": 4792, "seek": 2115172, "start": 21159.56, "end": 21165.800000000003, "text": " this called, we can sample the categorical distribution. And that will tell us the predicted", "tokens": [50756, 341, 1219, 11, 321, 393, 6889, 264, 19250, 804, 7316, 13, 400, 300, 486, 980, 505, 264, 19147, 51068], "temperature": 0.0, "avg_logprob": -0.08386003670572233, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.001244814950041473}, {"id": 4793, "seek": 2115172, "start": 21165.800000000003, "end": 21170.280000000002, "text": " character. So what I mean is, let's just look at this. And then we'll explain this. So since our", "tokens": [51068, 2517, 13, 407, 437, 286, 914, 307, 11, 718, 311, 445, 574, 412, 341, 13, 400, 550, 321, 603, 2903, 341, 13, 407, 1670, 527, 51292], "temperature": 0.0, "avg_logprob": -0.08386003670572233, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.001244814950041473}, {"id": 4794, "seek": 2115172, "start": 21170.280000000002, "end": 21174.52, "text": " model works on random weights and biases right now, we haven't trained yet. This is actually", "tokens": [51292, 2316, 1985, 322, 4974, 17443, 293, 32152, 558, 586, 11, 321, 2378, 380, 8895, 1939, 13, 639, 307, 767, 51504], "temperature": 0.0, "avg_logprob": -0.08386003670572233, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.001244814950041473}, {"id": 4795, "seek": 2115172, "start": 21174.52, "end": 21179.72, "text": " all of the predicted characters that it had. So at every time step, at the first time step,", "tokens": [51504, 439, 295, 264, 19147, 4342, 300, 309, 632, 13, 407, 412, 633, 565, 1823, 11, 412, 264, 700, 565, 1823, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08386003670572233, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.001244814950041473}, {"id": 4796, "seek": 2117972, "start": 21179.800000000003, "end": 21185.32, "text": " it predicted h, then it predicted hyphen, then h, then g, then you, and so on so forth, you get", "tokens": [50368, 309, 19147, 276, 11, 550, 309, 19147, 2477, 47059, 11, 550, 276, 11, 550, 290, 11, 550, 291, 11, 293, 370, 322, 370, 5220, 11, 291, 483, 50644], "temperature": 0.0, "avg_logprob": -0.1499059761271757, "compression_ratio": 2.0210084033613445, "no_speech_prob": 0.006902955938130617}, {"id": 4797, "seek": 2117972, "start": 21185.32, "end": 21192.760000000002, "text": " the point, right? So what we're doing to get this value is we're going to sample the prediction. So", "tokens": [50644, 264, 935, 11, 558, 30, 407, 437, 321, 434, 884, 281, 483, 341, 2158, 307, 321, 434, 516, 281, 6889, 264, 17630, 13, 407, 51016], "temperature": 0.0, "avg_logprob": -0.1499059761271757, "compression_ratio": 2.0210084033613445, "no_speech_prob": 0.006902955938130617}, {"id": 4798, "seek": 2117972, "start": 21192.760000000002, "end": 21197.960000000003, "text": " at this, this is just the first time step, actually, we're sampled the prediction. Actually, no,", "tokens": [51016, 412, 341, 11, 341, 307, 445, 264, 700, 565, 1823, 11, 767, 11, 321, 434, 3247, 15551, 264, 17630, 13, 5135, 11, 572, 11, 51276], "temperature": 0.0, "avg_logprob": -0.1499059761271757, "compression_ratio": 2.0210084033613445, "no_speech_prob": 0.006902955938130617}, {"id": 4799, "seek": 2117972, "start": 21197.960000000003, "end": 21202.68, "text": " sorry, we're sampling every time stamp, my bad there. We're going to say sampled indices equals", "tokens": [51276, 2597, 11, 321, 434, 21179, 633, 565, 9921, 11, 452, 1578, 456, 13, 492, 434, 516, 281, 584, 3247, 15551, 43840, 6915, 51512], "temperature": 0.0, "avg_logprob": -0.1499059761271757, "compression_ratio": 2.0210084033613445, "no_speech_prob": 0.006902955938130617}, {"id": 4800, "seek": 2117972, "start": 21202.68, "end": 21207.0, "text": " NP dot reshapes, we're just reshaping this just changing the shape of it. We're going to say", "tokens": [51512, 38611, 5893, 725, 71, 569, 279, 11, 321, 434, 445, 725, 71, 569, 278, 341, 445, 4473, 264, 3909, 295, 309, 13, 492, 434, 516, 281, 584, 51728], "temperature": 0.0, "avg_logprob": -0.1499059761271757, "compression_ratio": 2.0210084033613445, "no_speech_prob": 0.006902955938130617}, {"id": 4801, "seek": 2120700, "start": 21207.08, "end": 21214.2, "text": " predicted characters equals int to text sampled indices. So it's, I really, it's hard to explain", "tokens": [50368, 19147, 4342, 6915, 560, 281, 2487, 3247, 15551, 43840, 13, 407, 309, 311, 11, 286, 534, 11, 309, 311, 1152, 281, 2903, 50724], "temperature": 0.0, "avg_logprob": -0.08532088735829228, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.007345417980104685}, {"id": 4802, "seek": 2120700, "start": 21214.2, "end": 21218.28, "text": " all this if you guys don't have a statistics kind of background a little bit to talk about why we're", "tokens": [50724, 439, 341, 498, 291, 1074, 500, 380, 362, 257, 12523, 733, 295, 3678, 257, 707, 857, 281, 751, 466, 983, 321, 434, 50928], "temperature": 0.0, "avg_logprob": -0.08532088735829228, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.007345417980104685}, {"id": 4803, "seek": 2120700, "start": 21218.28, "end": 21224.12, "text": " sampling and not just taking the argument max value of like this array, because you would think", "tokens": [50928, 21179, 293, 406, 445, 1940, 264, 6770, 11469, 2158, 295, 411, 341, 10225, 11, 570, 291, 576, 519, 51220], "temperature": 0.0, "avg_logprob": -0.08532088735829228, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.007345417980104685}, {"id": 4804, "seek": 2120700, "start": 21224.12, "end": 21227.56, "text": " that what we'll do is just take the one that has the highest probability out of here, and that will", "tokens": [51220, 300, 437, 321, 603, 360, 307, 445, 747, 264, 472, 300, 575, 264, 6343, 8482, 484, 295, 510, 11, 293, 300, 486, 51392], "temperature": 0.0, "avg_logprob": -0.08532088735829228, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.007345417980104685}, {"id": 4805, "seek": 2120700, "start": 21227.56, "end": 21232.6, "text": " be the index of the next predicted character. There's some issues with doing that for the loss", "tokens": [51392, 312, 264, 8186, 295, 264, 958, 19147, 2517, 13, 821, 311, 512, 2663, 365, 884, 300, 337, 264, 4470, 51644], "temperature": 0.0, "avg_logprob": -0.08532088735829228, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.007345417980104685}, {"id": 4806, "seek": 2123260, "start": 21232.68, "end": 21237.96, "text": " function. Just because if we do that, then what that means is we're going to kind of get stuck in", "tokens": [50368, 2445, 13, 1449, 570, 498, 321, 360, 300, 11, 550, 437, 300, 1355, 307, 321, 434, 516, 281, 733, 295, 483, 5541, 294, 50632], "temperature": 0.0, "avg_logprob": -0.07553787918778153, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.09008239954710007}, {"id": 4807, "seek": 2123260, "start": 21239.0, "end": 21243.079999999998, "text": " an infinite loop almost where we just keep accepting the biggest character. So what we'll", "tokens": [50684, 364, 13785, 6367, 1920, 689, 321, 445, 1066, 17391, 264, 3880, 2517, 13, 407, 437, 321, 603, 50888], "temperature": 0.0, "avg_logprob": -0.07553787918778153, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.09008239954710007}, {"id": 4808, "seek": 2123260, "start": 21243.079999999998, "end": 21250.039999999997, "text": " do is pick a character based on this probability distribution, kind of, yeah, again, it's hard.", "tokens": [50888, 360, 307, 1888, 257, 2517, 2361, 322, 341, 8482, 7316, 11, 733, 295, 11, 1338, 11, 797, 11, 309, 311, 1152, 13, 51236], "temperature": 0.0, "avg_logprob": -0.07553787918778153, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.09008239954710007}, {"id": 4809, "seek": 2123260, "start": 21250.039999999997, "end": 21253.96, "text": " It's called sampling the distribution. You can look that up if you don't know what that means,", "tokens": [51236, 467, 311, 1219, 21179, 264, 7316, 13, 509, 393, 574, 300, 493, 498, 291, 500, 380, 458, 437, 300, 1355, 11, 51432], "temperature": 0.0, "avg_logprob": -0.07553787918778153, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.09008239954710007}, {"id": 4810, "seek": 2123260, "start": 21253.96, "end": 21258.68, "text": " but sampling is just like trying to pick a character based on a probability distribution.", "tokens": [51432, 457, 21179, 307, 445, 411, 1382, 281, 1888, 257, 2517, 2361, 322, 257, 8482, 7316, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07553787918778153, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.09008239954710007}, {"id": 4811, "seek": 2125868, "start": 21258.68, "end": 21263.08, "text": " It doesn't guarantee that the character with the highest probability is going to be picked. It", "tokens": [50364, 467, 1177, 380, 10815, 300, 264, 2517, 365, 264, 6343, 8482, 307, 516, 281, 312, 6183, 13, 467, 50584], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4812, "seek": 2125868, "start": 21263.08, "end": 21268.52, "text": " just uses those probabilities to pick it. I hope that makes sense. I know that was like a really", "tokens": [50584, 445, 4960, 729, 33783, 281, 1888, 309, 13, 286, 1454, 300, 1669, 2020, 13, 286, 458, 300, 390, 411, 257, 534, 50856], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4813, "seek": 2125868, "start": 21268.52, "end": 21273.88, "text": " rambly definition, but that's the best I can do. So here, we reshape the array and convert all the", "tokens": [50856, 367, 2173, 356, 7123, 11, 457, 300, 311, 264, 1151, 286, 393, 360, 13, 407, 510, 11, 321, 725, 42406, 264, 10225, 293, 7620, 439, 264, 51124], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4814, "seek": 2125868, "start": 21273.88, "end": 21277.88, "text": " integers to numbers to see the actual characters. So that's what these two lines are doing here.", "tokens": [51124, 41674, 281, 3547, 281, 536, 264, 3539, 4342, 13, 407, 300, 311, 437, 613, 732, 3876, 366, 884, 510, 13, 51324], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4815, "seek": 2125868, "start": 21277.88, "end": 21282.28, "text": " And then I'm just showing the predicted characters by showing you this. And you know,", "tokens": [51324, 400, 550, 286, 478, 445, 4099, 264, 19147, 4342, 538, 4099, 291, 341, 13, 400, 291, 458, 11, 51544], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4816, "seek": 2125868, "start": 21282.28, "end": 21287.32, "text": " the character here is what was predicted at time step zero to be the next character and so on.", "tokens": [51544, 264, 2517, 510, 307, 437, 390, 19147, 412, 565, 1823, 4018, 281, 312, 264, 958, 2517, 293, 370, 322, 13, 51796], "temperature": 0.0, "avg_logprob": -0.05869078114084954, "compression_ratio": 1.8562091503267975, "no_speech_prob": 0.009125091135501862}, {"id": 4817, "seek": 2128732, "start": 21288.12, "end": 21293.32, "text": " Okay, so now we can create a loss function that actually handles this for us. So this is the", "tokens": [50404, 1033, 11, 370, 586, 321, 393, 1884, 257, 4470, 2445, 300, 767, 18722, 341, 337, 505, 13, 407, 341, 307, 264, 50664], "temperature": 0.0, "avg_logprob": -0.08475656745847591, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012842677533626556}, {"id": 4818, "seek": 2128732, "start": 21293.32, "end": 21299.16, "text": " loss function that we have. Keras has like a built in one that we can utilize, which is what", "tokens": [50664, 4470, 2445, 300, 321, 362, 13, 591, 6985, 575, 411, 257, 3094, 294, 472, 300, 321, 393, 16117, 11, 597, 307, 437, 50956], "temperature": 0.0, "avg_logprob": -0.08475656745847591, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012842677533626556}, {"id": 4819, "seek": 2128732, "start": 21299.16, "end": 21304.04, "text": " we're doing. But what this is going to do is take all of the labels and all of the probability", "tokens": [50956, 321, 434, 884, 13, 583, 437, 341, 307, 516, 281, 360, 307, 747, 439, 295, 264, 16949, 293, 439, 295, 264, 8482, 51200], "temperature": 0.0, "avg_logprob": -0.08475656745847591, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012842677533626556}, {"id": 4820, "seek": 2128732, "start": 21304.04, "end": 21308.68, "text": " distributions, which is what this is, logits, I'm not going to talk about that really. And we'll", "tokens": [51200, 37870, 11, 597, 307, 437, 341, 307, 11, 3565, 1208, 11, 286, 478, 406, 516, 281, 751, 466, 300, 534, 13, 400, 321, 603, 51432], "temperature": 0.0, "avg_logprob": -0.08475656745847591, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012842677533626556}, {"id": 4821, "seek": 2128732, "start": 21308.68, "end": 21314.28, "text": " compute a loss on those. So how different or how similar those two things are. Remember the goal", "tokens": [51432, 14722, 257, 4470, 322, 729, 13, 407, 577, 819, 420, 577, 2531, 729, 732, 721, 366, 13, 5459, 264, 3387, 51712], "temperature": 0.0, "avg_logprob": -0.08475656745847591, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012842677533626556}, {"id": 4822, "seek": 2131428, "start": 21314.28, "end": 21318.92, "text": " of our algorithm in the neural network is to reduce the loss, right? Okay, so next,", "tokens": [50364, 295, 527, 9284, 294, 264, 18161, 3209, 307, 281, 5407, 264, 4470, 11, 558, 30, 1033, 11, 370, 958, 11, 50596], "temperature": 0.0, "avg_logprob": -0.06121245297518643, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.00669262045994401}, {"id": 4823, "seek": 2131428, "start": 21318.92, "end": 21322.039999999997, "text": " we're going to compile the model, which we'll do here. So we're going to compile the model with", "tokens": [50596, 321, 434, 516, 281, 31413, 264, 2316, 11, 597, 321, 603, 360, 510, 13, 407, 321, 434, 516, 281, 31413, 264, 2316, 365, 50752], "temperature": 0.0, "avg_logprob": -0.06121245297518643, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.00669262045994401}, {"id": 4824, "seek": 2131428, "start": 21322.039999999997, "end": 21328.039999999997, "text": " the atom optimizer and the loss function as loss, which we defined here. And now we're going to", "tokens": [50752, 264, 12018, 5028, 6545, 293, 264, 4470, 2445, 382, 4470, 11, 597, 321, 7642, 510, 13, 400, 586, 321, 434, 516, 281, 51052], "temperature": 0.0, "avg_logprob": -0.06121245297518643, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.00669262045994401}, {"id": 4825, "seek": 2131428, "start": 21328.039999999997, "end": 21331.32, "text": " set up some checkpoints. I'm not going to talk about how these work, you can kind of just read", "tokens": [51052, 992, 493, 512, 1520, 20552, 13, 286, 478, 406, 516, 281, 751, 466, 577, 613, 589, 11, 291, 393, 733, 295, 445, 1401, 51216], "temperature": 0.0, "avg_logprob": -0.06121245297518643, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.00669262045994401}, {"id": 4826, "seek": 2131428, "start": 21331.32, "end": 21338.28, "text": " through this if you want. And then we're going to train the model. Remember to start your GPU", "tokens": [51216, 807, 341, 498, 291, 528, 13, 400, 550, 321, 434, 516, 281, 3847, 264, 2316, 13, 5459, 281, 722, 428, 18407, 51564], "temperature": 0.0, "avg_logprob": -0.06121245297518643, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.00669262045994401}, {"id": 4827, "seek": 2133828, "start": 21338.28, "end": 21344.6, "text": " hardware accelerator under runtime, change runtime type GPU, because if you do not, then this is", "tokens": [50364, 8837, 39889, 833, 34474, 11, 1319, 34474, 2010, 18407, 11, 570, 498, 291, 360, 406, 11, 550, 341, 307, 50680], "temperature": 0.0, "avg_logprob": -0.08114124479747954, "compression_ratio": 1.6539792387543253, "no_speech_prob": 0.182382270693779}, {"id": 4828, "seek": 2133828, "start": 21344.6, "end": 21349.96, "text": " going to be very slow. But once you do that, you can train the model. I've already trained it. But", "tokens": [50680, 516, 281, 312, 588, 2964, 13, 583, 1564, 291, 360, 300, 11, 291, 393, 3847, 264, 2316, 13, 286, 600, 1217, 8895, 309, 13, 583, 50948], "temperature": 0.0, "avg_logprob": -0.08114124479747954, "compression_ratio": 1.6539792387543253, "no_speech_prob": 0.182382270693779}, {"id": 4829, "seek": 2133828, "start": 21349.96, "end": 21355.0, "text": " if we go through this training, we can see it's going to say train for 172 steps. It's going to", "tokens": [50948, 498, 321, 352, 807, 341, 3097, 11, 321, 393, 536, 309, 311, 516, 281, 584, 3847, 337, 3282, 17, 4439, 13, 467, 311, 516, 281, 51200], "temperature": 0.0, "avg_logprob": -0.08114124479747954, "compression_ratio": 1.6539792387543253, "no_speech_prob": 0.182382270693779}, {"id": 4830, "seek": 2133828, "start": 21355.0, "end": 21359.16, "text": " take about, you know, 30 seconds per epoch, probably maybe a little bit less than that. And", "tokens": [51200, 747, 466, 11, 291, 458, 11, 2217, 3949, 680, 30992, 339, 11, 1391, 1310, 257, 707, 857, 1570, 813, 300, 13, 400, 51408], "temperature": 0.0, "avg_logprob": -0.08114124479747954, "compression_ratio": 1.6539792387543253, "no_speech_prob": 0.182382270693779}, {"id": 4831, "seek": 2133828, "start": 21359.16, "end": 21363.719999999998, "text": " the more epochs you run this for, the better it will get. This is a different we're not likely", "tokens": [51408, 264, 544, 30992, 28346, 291, 1190, 341, 337, 11, 264, 1101, 309, 486, 483, 13, 639, 307, 257, 819, 321, 434, 406, 3700, 51636], "temperature": 0.0, "avg_logprob": -0.08114124479747954, "compression_ratio": 1.6539792387543253, "no_speech_prob": 0.182382270693779}, {"id": 4832, "seek": 2136372, "start": 21363.72, "end": 21369.56, "text": " going to overfit here. So we can run this for like, say 100 epochs, if we wanted to. For our case,", "tokens": [50364, 516, 281, 670, 6845, 510, 13, 407, 321, 393, 1190, 341, 337, 411, 11, 584, 2319, 30992, 28346, 11, 498, 321, 1415, 281, 13, 1171, 527, 1389, 11, 50656], "temperature": 0.0, "avg_logprob": -0.08209848757143375, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.14801961183547974}, {"id": 4833, "seek": 2136372, "start": 21369.56, "end": 21374.440000000002, "text": " let's actually start by just training this on, let's say two epochs, just to see how it does.", "tokens": [50656, 718, 311, 767, 722, 538, 445, 3097, 341, 322, 11, 718, 311, 584, 732, 30992, 28346, 11, 445, 281, 536, 577, 309, 775, 13, 50900], "temperature": 0.0, "avg_logprob": -0.08209848757143375, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.14801961183547974}, {"id": 4834, "seek": 2136372, "start": 21374.440000000002, "end": 21380.280000000002, "text": " And then we'll train it on like 1020, 4050 and compare the results. But you'll notice the more", "tokens": [50900, 400, 550, 321, 603, 3847, 309, 322, 411, 1266, 2009, 11, 3356, 2803, 293, 6794, 264, 3542, 13, 583, 291, 603, 3449, 264, 544, 51192], "temperature": 0.0, "avg_logprob": -0.08209848757143375, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.14801961183547974}, {"id": 4835, "seek": 2136372, "start": 21380.280000000002, "end": 21383.960000000003, "text": " epochs, the better it's going to get. But just like for our case, we'll start with two, and then", "tokens": [51192, 30992, 28346, 11, 264, 1101, 309, 311, 516, 281, 483, 13, 583, 445, 411, 337, 527, 1389, 11, 321, 603, 722, 365, 732, 11, 293, 550, 51376], "temperature": 0.0, "avg_logprob": -0.08209848757143375, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.14801961183547974}, {"id": 4836, "seek": 2136372, "start": 21383.960000000003, "end": 21389.4, "text": " we'll work our way up. So while that trains, we'll actually explain the next aspect of this", "tokens": [51376, 321, 603, 589, 527, 636, 493, 13, 407, 1339, 300, 16329, 11, 321, 603, 767, 2903, 264, 958, 4171, 295, 341, 51648], "temperature": 0.0, "avg_logprob": -0.08209848757143375, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.14801961183547974}, {"id": 4837, "seek": 2138940, "start": 21389.480000000003, "end": 21394.52, "text": " without running the code. So essentially, what we need to do, after we've trained the model,", "tokens": [50368, 1553, 2614, 264, 3089, 13, 407, 4476, 11, 437, 321, 643, 281, 360, 11, 934, 321, 600, 8895, 264, 2316, 11, 50620], "temperature": 0.0, "avg_logprob": -0.08213605171392772, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0600799098610878}, {"id": 4838, "seek": 2138940, "start": 21394.52, "end": 21401.640000000003, "text": " we've initially the weights and biases, if we need to rebuild it using a new batch size of one.", "tokens": [50620, 321, 600, 9105, 264, 17443, 293, 32152, 11, 498, 321, 643, 281, 16877, 309, 1228, 257, 777, 15245, 2744, 295, 472, 13, 50976], "temperature": 0.0, "avg_logprob": -0.08213605171392772, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0600799098610878}, {"id": 4839, "seek": 2138940, "start": 21401.640000000003, "end": 21407.800000000003, "text": " So remember, the initial batch size was 64, which means that we'd have to pass it 64 inputs or", "tokens": [50976, 407, 1604, 11, 264, 5883, 15245, 2744, 390, 12145, 11, 597, 1355, 300, 321, 1116, 362, 281, 1320, 309, 12145, 15743, 420, 51284], "temperature": 0.0, "avg_logprob": -0.08213605171392772, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0600799098610878}, {"id": 4840, "seek": 2138940, "start": 21407.800000000003, "end": 21412.760000000002, "text": " sequences for it to work properly. But now what I've done is I'm going to rebuild the model and", "tokens": [51284, 22978, 337, 309, 281, 589, 6108, 13, 583, 586, 437, 286, 600, 1096, 307, 286, 478, 516, 281, 16877, 264, 2316, 293, 51532], "temperature": 0.0, "avg_logprob": -0.08213605171392772, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0600799098610878}, {"id": 4841, "seek": 2138940, "start": 21412.760000000002, "end": 21417.56, "text": " change it to a batch size of one, so that we can just pass it some sequence of whatever length we", "tokens": [51532, 1319, 309, 281, 257, 15245, 2744, 295, 472, 11, 370, 300, 321, 393, 445, 1320, 309, 512, 8310, 295, 2035, 4641, 321, 51772], "temperature": 0.0, "avg_logprob": -0.08213605171392772, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0600799098610878}, {"id": 4842, "seek": 2141756, "start": 21417.56, "end": 21423.08, "text": " want. And it will work. So if we run this, we've rebuilt the model with batch size one, that's", "tokens": [50364, 528, 13, 400, 309, 486, 589, 13, 407, 498, 321, 1190, 341, 11, 321, 600, 38532, 264, 2316, 365, 15245, 2744, 472, 11, 300, 311, 50640], "temperature": 0.0, "avg_logprob": -0.09451901285271895, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.02368738129734993}, {"id": 4843, "seek": 2141756, "start": 21423.08, "end": 21428.52, "text": " the only thing we've changed. And now what I can do is load the weights by saying model dot load", "tokens": [50640, 264, 787, 551, 321, 600, 3105, 13, 400, 586, 437, 286, 393, 360, 307, 3677, 264, 17443, 538, 1566, 2316, 5893, 3677, 50912], "temperature": 0.0, "avg_logprob": -0.09451901285271895, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.02368738129734993}, {"id": 4844, "seek": 2141756, "start": 21428.52, "end": 21435.32, "text": " weights tf dot train dot latest checkpoint checkpoint directory, and then build the model", "tokens": [50912, 17443, 256, 69, 5893, 3847, 5893, 6792, 42269, 42269, 21120, 11, 293, 550, 1322, 264, 2316, 51252], "temperature": 0.0, "avg_logprob": -0.09451901285271895, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.02368738129734993}, {"id": 4845, "seek": 2141756, "start": 21436.68, "end": 21443.72, "text": " using the tensor shape one none. I know sounds strange. This is how we do this rebuild the model.", "tokens": [51320, 1228, 264, 40863, 3909, 472, 6022, 13, 286, 458, 3263, 5861, 13, 639, 307, 577, 321, 360, 341, 16877, 264, 2316, 13, 51672], "temperature": 0.0, "avg_logprob": -0.09451901285271895, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.02368738129734993}, {"id": 4846, "seek": 2144372, "start": 21443.72, "end": 21448.2, "text": " One none is just saying expect the input one and then none means we don't know what the next", "tokens": [50364, 1485, 6022, 307, 445, 1566, 2066, 264, 4846, 472, 293, 550, 6022, 1355, 321, 500, 380, 458, 437, 264, 958, 50588], "temperature": 0.0, "avg_logprob": -0.09801711127871558, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.004331320058554411}, {"id": 4847, "seek": 2144372, "start": 21448.2, "end": 21455.64, "text": " dimension length will be. But here, checkpoint directory is just we've defined where on our", "tokens": [50588, 10139, 4641, 486, 312, 13, 583, 510, 11, 42269, 21120, 307, 445, 321, 600, 7642, 689, 322, 527, 50960], "temperature": 0.0, "avg_logprob": -0.09801711127871558, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.004331320058554411}, {"id": 4848, "seek": 2144372, "start": 21455.64, "end": 21460.600000000002, "text": " computer, we're going to save these TensorFlow checkpoints. This is just saying this is the", "tokens": [50960, 3820, 11, 321, 434, 516, 281, 3155, 613, 37624, 1520, 20552, 13, 639, 307, 445, 1566, 341, 307, 264, 51208], "temperature": 0.0, "avg_logprob": -0.09801711127871558, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.004331320058554411}, {"id": 4849, "seek": 2144372, "start": 21461.4, "end": 21465.0, "text": " was it the prefix we're going to save the checkpoint with. So we're going to do the", "tokens": [51248, 390, 309, 264, 46969, 321, 434, 516, 281, 3155, 264, 42269, 365, 13, 407, 321, 434, 516, 281, 360, 264, 51428], "temperature": 0.0, "avg_logprob": -0.09801711127871558, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.004331320058554411}, {"id": 4850, "seek": 2144372, "start": 21465.0, "end": 21470.04, "text": " checkpoint directory. And then checkpoint epoch where epoch will stand for obviously,", "tokens": [51428, 42269, 21120, 13, 400, 550, 42269, 30992, 339, 689, 30992, 339, 486, 1463, 337, 2745, 11, 51680], "temperature": 0.0, "avg_logprob": -0.09801711127871558, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.004331320058554411}, {"id": 4851, "seek": 2147004, "start": 21470.68, "end": 21475.16, "text": " whatever epoch we're on. So we'll save checkpoint here, we'll save a checkpoint at epoch one,", "tokens": [50396, 2035, 30992, 339, 321, 434, 322, 13, 407, 321, 603, 3155, 42269, 510, 11, 321, 603, 3155, 257, 42269, 412, 30992, 339, 472, 11, 50620], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4852, "seek": 2147004, "start": 21475.16, "end": 21480.440000000002, "text": " a checkpoint at epoch two, to get the latest checkpoint, we do this. And then if we wanted", "tokens": [50620, 257, 42269, 412, 30992, 339, 732, 11, 281, 483, 264, 6792, 42269, 11, 321, 360, 341, 13, 400, 550, 498, 321, 1415, 50884], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4853, "seek": 2147004, "start": 21480.440000000002, "end": 21485.72, "text": " to load any intermediate checkpoint, say like checkpoint 10, which is what I've defined here,", "tokens": [50884, 281, 3677, 604, 19376, 42269, 11, 584, 411, 42269, 1266, 11, 597, 307, 437, 286, 600, 7642, 510, 11, 51148], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4854, "seek": 2147004, "start": 21485.72, "end": 21490.280000000002, "text": " we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading", "tokens": [51148, 321, 727, 764, 341, 3461, 295, 3089, 760, 510, 13, 400, 286, 600, 445, 1152, 86, 1824, 264, 42269, 300, 286, 478, 15114, 51376], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4855, "seek": 2147004, "start": 21490.280000000002, "end": 21494.68, "text": " by saying tf dot train dot load checkpoint, whereas this one just gets the most recent. So", "tokens": [51376, 538, 1566, 256, 69, 5893, 3847, 5893, 3677, 42269, 11, 9735, 341, 472, 445, 2170, 264, 881, 5162, 13, 407, 51596], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4856, "seek": 2147004, "start": 21494.68, "end": 21498.280000000002, "text": " we'll get the most recent, which should be checkpoint two for me. And then what we're going", "tokens": [51596, 321, 603, 483, 264, 881, 5162, 11, 597, 820, 312, 42269, 732, 337, 385, 13, 400, 550, 437, 321, 434, 516, 51776], "temperature": 0.0, "avg_logprob": -0.08271572139713314, "compression_ratio": 2.0588235294117645, "no_speech_prob": 0.01450239960104227}, {"id": 4857, "seek": 2149828, "start": 21498.28, "end": 21503.64, "text": " to do is generate the text. So this function, I'll dig into it in a second, but I just want", "tokens": [50364, 281, 360, 307, 8460, 264, 2487, 13, 407, 341, 2445, 11, 286, 603, 2528, 666, 309, 294, 257, 1150, 11, 457, 286, 445, 528, 50632], "temperature": 0.0, "avg_logprob": -0.08562964751940816, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.04083934426307678}, {"id": 4858, "seek": 2149828, "start": 21503.64, "end": 21507.16, "text": " to run and show you how this works, because I feel like we've done a lot of work for not", "tokens": [50632, 281, 1190, 293, 855, 291, 577, 341, 1985, 11, 570, 286, 841, 411, 321, 600, 1096, 257, 688, 295, 589, 337, 406, 50808], "temperature": 0.0, "avg_logprob": -0.08562964751940816, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.04083934426307678}, {"id": 4859, "seek": 2149828, "start": 21507.16, "end": 21511.879999999997, "text": " very many results right now. And I'm just going to type in the string Romeo, and just show you", "tokens": [50808, 588, 867, 3542, 558, 586, 13, 400, 286, 478, 445, 516, 281, 2010, 294, 264, 6798, 33563, 11, 293, 445, 855, 291, 51044], "temperature": 0.0, "avg_logprob": -0.08562964751940816, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.04083934426307678}, {"id": 4860, "seek": 2149828, "start": 21511.879999999997, "end": 21516.92, "text": " that when I do this, we give it a second. And it will actually generate an output sequence", "tokens": [51044, 300, 562, 286, 360, 341, 11, 321, 976, 309, 257, 1150, 13, 400, 309, 486, 767, 8460, 364, 5598, 8310, 51296], "temperature": 0.0, "avg_logprob": -0.08562964751940816, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.04083934426307678}, {"id": 4861, "seek": 2149828, "start": 21516.92, "end": 21523.239999999998, "text": " like this. So we have Romeo us give this is the beginning of our sequence that says lady", "tokens": [51296, 411, 341, 13, 407, 321, 362, 33563, 505, 976, 341, 307, 264, 2863, 295, 527, 8310, 300, 1619, 7262, 51612], "temperature": 0.0, "avg_logprob": -0.08562964751940816, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.04083934426307678}, {"id": 4862, "seek": 2152324, "start": 21523.24, "end": 21530.04, "text": " Capulet food, Marathon father, gnomes come to those shall, right? So it's like pseudo English,", "tokens": [50364, 8363, 425, 302, 1755, 11, 2039, 18660, 3086, 11, 290, 15819, 279, 808, 281, 729, 4393, 11, 558, 30, 407, 309, 311, 411, 35899, 3669, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12410356867031788, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.3275643289089203}, {"id": 4863, "seek": 2152324, "start": 21530.04, "end": 21534.36, "text": " most of it are like kind of proper words. But again, this is because we trained it on just", "tokens": [50704, 881, 295, 309, 366, 411, 733, 295, 2296, 2283, 13, 583, 797, 11, 341, 307, 570, 321, 8895, 309, 322, 445, 50920], "temperature": 0.0, "avg_logprob": -0.12410356867031788, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.3275643289089203}, {"id": 4864, "seek": 2152324, "start": 21534.36, "end": 21540.120000000003, "text": " two epochs. So I'll talk about how we build this in a second. But if you wanted a better output", "tokens": [50920, 732, 30992, 28346, 13, 407, 286, 603, 751, 466, 577, 321, 1322, 341, 294, 257, 1150, 13, 583, 498, 291, 1415, 257, 1101, 5598, 51208], "temperature": 0.0, "avg_logprob": -0.12410356867031788, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.3275643289089203}, {"id": 4865, "seek": 2152324, "start": 21540.120000000003, "end": 21544.84, "text": " for this part, then you would train this on more epochs. So now let's talk about how I actually", "tokens": [51208, 337, 341, 644, 11, 550, 291, 576, 3847, 341, 322, 544, 30992, 28346, 13, 407, 586, 718, 311, 751, 466, 577, 286, 767, 51444], "temperature": 0.0, "avg_logprob": -0.12410356867031788, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.3275643289089203}, {"id": 4866, "seek": 2152324, "start": 21544.84, "end": 21550.36, "text": " generated that output. So we rebuilt the model to accept a batch size of one, which means that I", "tokens": [51444, 10833, 300, 5598, 13, 407, 321, 38532, 264, 2316, 281, 3241, 257, 15245, 2744, 295, 472, 11, 597, 1355, 300, 286, 51720], "temperature": 0.0, "avg_logprob": -0.12410356867031788, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.3275643289089203}, {"id": 4867, "seek": 2155036, "start": 21550.36, "end": 21555.32, "text": " can pass it a sequence of any length. And in fact, what I start by doing is passing the", "tokens": [50364, 393, 1320, 309, 257, 8310, 295, 604, 4641, 13, 400, 294, 1186, 11, 437, 286, 722, 538, 884, 307, 8437, 264, 50612], "temperature": 0.0, "avg_logprob": -0.06557211959571169, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.005384697113186121}, {"id": 4868, "seek": 2155036, "start": 21555.32, "end": 21561.48, "text": " sequence that I've typed in here, which was Romeo, then what that does is we run this function", "tokens": [50612, 8310, 300, 286, 600, 33941, 294, 510, 11, 597, 390, 33563, 11, 550, 437, 300, 775, 307, 321, 1190, 341, 2445, 50920], "temperature": 0.0, "avg_logprob": -0.06557211959571169, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.005384697113186121}, {"id": 4869, "seek": 2155036, "start": 21561.48, "end": 21565.32, "text": " generate text, I just stole this from TensorFlow's website, like I've stolen almost all of this", "tokens": [50920, 8460, 2487, 11, 286, 445, 16326, 341, 490, 37624, 311, 3144, 11, 411, 286, 600, 15900, 1920, 439, 295, 341, 51112], "temperature": 0.0, "avg_logprob": -0.06557211959571169, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.005384697113186121}, {"id": 4870, "seek": 2155036, "start": 21565.32, "end": 21571.8, "text": " code. And then we say the number of characters to generate is 800, the input evaluation, which", "tokens": [51112, 3089, 13, 400, 550, 321, 584, 264, 1230, 295, 4342, 281, 8460, 307, 13083, 11, 264, 4846, 13344, 11, 597, 51436], "temperature": 0.0, "avg_logprob": -0.06557211959571169, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.005384697113186121}, {"id": 4871, "seek": 2155036, "start": 21571.8, "end": 21577.4, "text": " is now like we need to pre process this text again, so that this works properly, we could", "tokens": [51436, 307, 586, 411, 321, 643, 281, 659, 1399, 341, 2487, 797, 11, 370, 300, 341, 1985, 6108, 11, 321, 727, 51716], "temperature": 0.0, "avg_logprob": -0.06557211959571169, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.005384697113186121}, {"id": 4872, "seek": 2157740, "start": 21577.4, "end": 21582.04, "text": " use my little function, or we can just write this line of code here, which does with the function", "tokens": [50364, 764, 452, 707, 2445, 11, 420, 321, 393, 445, 2464, 341, 1622, 295, 3089, 510, 11, 597, 775, 365, 264, 2445, 50596], "temperature": 0.0, "avg_logprob": -0.11598373594738189, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.03621552139520645}, {"id": 4873, "seek": 2157740, "start": 21582.04, "end": 21587.88, "text": " that I wrote does for us. So char to IDX S for S and start string, start string is what we typed", "tokens": [50596, 300, 286, 4114, 775, 337, 505, 13, 407, 1290, 281, 7348, 55, 318, 337, 318, 293, 722, 6798, 11, 722, 6798, 307, 437, 321, 33941, 50888], "temperature": 0.0, "avg_logprob": -0.11598373594738189, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.03621552139520645}, {"id": 4874, "seek": 2157740, "start": 21587.88, "end": 21593.08, "text": " in in that case, Romeo, then what we're going to do is expand the dimensions. So essentially turn", "tokens": [50888, 294, 294, 300, 1389, 11, 33563, 11, 550, 437, 321, 434, 516, 281, 360, 307, 5268, 264, 12819, 13, 407, 4476, 1261, 51148], "temperature": 0.0, "avg_logprob": -0.11598373594738189, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.03621552139520645}, {"id": 4875, "seek": 2157740, "start": 21593.640000000003, "end": 21600.2, "text": " just a list like this that has all these numbers, nine, eight, seven into a double list like this,", "tokens": [51176, 445, 257, 1329, 411, 341, 300, 575, 439, 613, 3547, 11, 4949, 11, 3180, 11, 3407, 666, 257, 3834, 1329, 411, 341, 11, 51504], "temperature": 0.0, "avg_logprob": -0.11598373594738189, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.03621552139520645}, {"id": 4876, "seek": 2157740, "start": 21600.2, "end": 21604.52, "text": " or just a nested list, because that's what it's expecting as the input one batch, one entry.", "tokens": [51504, 420, 445, 257, 15646, 292, 1329, 11, 570, 300, 311, 437, 309, 311, 9650, 382, 264, 4846, 472, 15245, 11, 472, 8729, 13, 51720], "temperature": 0.0, "avg_logprob": -0.11598373594738189, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.03621552139520645}, {"id": 4877, "seek": 2160452, "start": 21605.16, "end": 21609.24, "text": " Then what we do is we're going to say the string that we want to store, because we want to print", "tokens": [50396, 1396, 437, 321, 360, 307, 321, 434, 516, 281, 584, 264, 6798, 300, 321, 528, 281, 3531, 11, 570, 321, 528, 281, 4482, 50600], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4878, "seek": 2160452, "start": 21609.24, "end": 21615.32, "text": " this out at the end, right, we'll put in this text generated list, temperature equals 1.0.", "tokens": [50600, 341, 484, 412, 264, 917, 11, 558, 11, 321, 603, 829, 294, 341, 2487, 10833, 1329, 11, 4292, 6915, 502, 13, 15, 13, 50904], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4879, "seek": 2160452, "start": 21615.32, "end": 21620.04, "text": " What this will allow us to do is if we change this value to be higher, I mean, you can read", "tokens": [50904, 708, 341, 486, 2089, 505, 281, 360, 307, 498, 321, 1319, 341, 2158, 281, 312, 2946, 11, 286, 914, 11, 291, 393, 1401, 51140], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4880, "seek": 2160452, "start": 21620.04, "end": 21624.12, "text": " the comment here, right, low temperature results in more predictable text, higher temperature results", "tokens": [51140, 264, 2871, 510, 11, 558, 11, 2295, 4292, 3542, 294, 544, 27737, 2487, 11, 2946, 4292, 3542, 51344], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4881, "seek": 2160452, "start": 21624.12, "end": 21627.72, "text": " in more surprising text. So this is just a parameter to mess with if you want, you don't", "tokens": [51344, 294, 544, 8830, 2487, 13, 407, 341, 307, 445, 257, 13075, 281, 2082, 365, 498, 291, 528, 11, 291, 500, 380, 51524], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4882, "seek": 2160452, "start": 21627.72, "end": 21632.760000000002, "text": " necessarily need it. And I would like, I've just left mine at one for now, we're going to start", "tokens": [51524, 4725, 643, 309, 13, 400, 286, 576, 411, 11, 286, 600, 445, 1411, 3892, 412, 472, 337, 586, 11, 321, 434, 516, 281, 722, 51776], "temperature": 0.0, "avg_logprob": -0.09769355510843211, "compression_ratio": 1.814102564102564, "no_speech_prob": 0.016401931643486023}, {"id": 4883, "seek": 2163276, "start": 21632.76, "end": 21637.8, "text": " by resetting the states of the model. This is because when we rebuild the model, it's going to", "tokens": [50364, 538, 14322, 783, 264, 4368, 295, 264, 2316, 13, 639, 307, 570, 562, 321, 16877, 264, 2316, 11, 309, 311, 516, 281, 50616], "temperature": 0.0, "avg_logprob": -0.0890286979028734, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.08268000930547714}, {"id": 4884, "seek": 2163276, "start": 21637.8, "end": 21643.399999999998, "text": " have stored the last state that it remembered when it was training. So we need to clear that", "tokens": [50616, 362, 12187, 264, 1036, 1785, 300, 309, 13745, 562, 309, 390, 3097, 13, 407, 321, 643, 281, 1850, 300, 50896], "temperature": 0.0, "avg_logprob": -0.0890286979028734, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.08268000930547714}, {"id": 4885, "seek": 2163276, "start": 21643.399999999998, "end": 21648.519999999997, "text": " before we pass new input text to it. And we say for I and range num generate, which means however", "tokens": [50896, 949, 321, 1320, 777, 4846, 2487, 281, 309, 13, 400, 321, 584, 337, 286, 293, 3613, 1031, 8460, 11, 597, 1355, 4461, 51152], "temperature": 0.0, "avg_logprob": -0.0890286979028734, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.08268000930547714}, {"id": 4886, "seek": 2163276, "start": 21648.519999999997, "end": 21653.079999999998, "text": " many characters we want to generate, which is 800 here, what we're going to do is say predictions", "tokens": [51152, 867, 4342, 321, 528, 281, 8460, 11, 597, 307, 13083, 510, 11, 437, 321, 434, 516, 281, 360, 307, 584, 21264, 51380], "temperature": 0.0, "avg_logprob": -0.0890286979028734, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.08268000930547714}, {"id": 4887, "seek": 2163276, "start": 21653.079999999998, "end": 21658.679999999997, "text": " equals model input a vowel, that's going to start as the start string that's encoded, right.", "tokens": [51380, 6915, 2316, 4846, 257, 29410, 11, 300, 311, 516, 281, 722, 382, 264, 722, 6798, 300, 311, 2058, 12340, 11, 558, 13, 51660], "temperature": 0.0, "avg_logprob": -0.0890286979028734, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.08268000930547714}, {"id": 4888, "seek": 2165868, "start": 21659.48, "end": 21664.44, "text": " And then what we're going to do is say predictions equals TF dot squeeze prediction zero. What this", "tokens": [50404, 400, 550, 437, 321, 434, 516, 281, 360, 307, 584, 21264, 6915, 40964, 5893, 13578, 17630, 4018, 13, 708, 341, 50652], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4889, "seek": 2165868, "start": 21664.44, "end": 21669.32, "text": " does is take our predictions, which is going to be in a nested list, and just removes that", "tokens": [50652, 775, 307, 747, 527, 21264, 11, 597, 307, 516, 281, 312, 294, 257, 15646, 292, 1329, 11, 293, 445, 30445, 300, 50896], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4890, "seek": 2165868, "start": 21669.32, "end": 21674.04, "text": " exterior dimension. So we just have the predictions that we want, we don't have that extra dimension", "tokens": [50896, 20677, 10139, 13, 407, 321, 445, 362, 264, 21264, 300, 321, 528, 11, 321, 500, 380, 362, 300, 2857, 10139, 51132], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4891, "seek": 2165868, "start": 21674.04, "end": 21678.04, "text": " that we need to index again. And then we're going to say using a categorical distribution to predict", "tokens": [51132, 300, 321, 643, 281, 8186, 797, 13, 400, 550, 321, 434, 516, 281, 584, 1228, 257, 19250, 804, 7316, 281, 6069, 51332], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4892, "seek": 2165868, "start": 21678.04, "end": 21682.44, "text": " the character returned by the model, that's what it writes here. We'll divide by the temperature,", "tokens": [51332, 264, 2517, 8752, 538, 264, 2316, 11, 300, 311, 437, 309, 13657, 510, 13, 492, 603, 9845, 538, 264, 4292, 11, 51552], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4893, "seek": 2165868, "start": 21682.44, "end": 21688.2, "text": " if it's one, that's not going to do anything. We'll say predicted ID equals we'll sample", "tokens": [51552, 498, 309, 311, 472, 11, 300, 311, 406, 516, 281, 360, 1340, 13, 492, 603, 584, 19147, 7348, 6915, 321, 603, 6889, 51840], "temperature": 0.0, "avg_logprob": -0.0974842181308664, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.022284820675849915}, {"id": 4894, "seek": 2168820, "start": 21688.2, "end": 21693.56, "text": " whatever the output was from the model, which is what this is doing. And then we're going to", "tokens": [50364, 2035, 264, 5598, 390, 490, 264, 2316, 11, 597, 307, 437, 341, 307, 884, 13, 400, 550, 321, 434, 516, 281, 50632], "temperature": 0.0, "avg_logprob": -0.0898600419362386, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.005729855038225651}, {"id": 4895, "seek": 2168820, "start": 21693.56, "end": 21700.04, "text": " take that output. So the predicted ID, and we are going to add that to the input evaluation.", "tokens": [50632, 747, 300, 5598, 13, 407, 264, 19147, 7348, 11, 293, 321, 366, 516, 281, 909, 300, 281, 264, 4846, 13344, 13, 50956], "temperature": 0.0, "avg_logprob": -0.0898600419362386, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.005729855038225651}, {"id": 4896, "seek": 2168820, "start": 21700.84, "end": 21705.08, "text": " And then what we're going to say is text generated dot append, and we're going to convert the text", "tokens": [50996, 400, 550, 437, 321, 434, 516, 281, 584, 307, 2487, 10833, 5893, 34116, 11, 293, 321, 434, 516, 281, 7620, 264, 2487, 51208], "temperature": 0.0, "avg_logprob": -0.0898600419362386, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.005729855038225651}, {"id": 4897, "seek": 2168820, "start": 21705.08, "end": 21712.36, "text": " that are integers now, back into a string, and return all of this. Now I know this seems like", "tokens": [51208, 300, 366, 41674, 586, 11, 646, 666, 257, 6798, 11, 293, 2736, 439, 295, 341, 13, 823, 286, 458, 341, 2544, 411, 51572], "temperature": 0.0, "avg_logprob": -0.0898600419362386, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.005729855038225651}, {"id": 4898, "seek": 2168820, "start": 21712.36, "end": 21717.16, "text": " a lot. Again, this is just given to us by TensorFlow to, you know, create this aspect,", "tokens": [51572, 257, 688, 13, 3764, 11, 341, 307, 445, 2212, 281, 505, 538, 37624, 281, 11, 291, 458, 11, 1884, 341, 4171, 11, 51812], "temperature": 0.0, "avg_logprob": -0.0898600419362386, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.005729855038225651}, {"id": 4899, "seek": 2171716, "start": 21717.16, "end": 21720.76, "text": " you can read through the comments yourself, if you want to understand it more. But I think that", "tokens": [50364, 291, 393, 1401, 807, 264, 3053, 1803, 11, 498, 291, 528, 281, 1223, 309, 544, 13, 583, 286, 519, 300, 50544], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4900, "seek": 2171716, "start": 21720.76, "end": 21725.64, "text": " was a decent explanation of what this is doing. So yeah, that is how we can generate, you know,", "tokens": [50544, 390, 257, 8681, 10835, 295, 437, 341, 307, 884, 13, 407, 1338, 11, 300, 307, 577, 321, 393, 8460, 11, 291, 458, 11, 50788], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4901, "seek": 2171716, "start": 21725.64, "end": 21731.96, "text": " sequences using recurrent neural network. Now what I'm going to do is go to my other window", "tokens": [50788, 22978, 1228, 18680, 1753, 18161, 3209, 13, 823, 437, 286, 478, 516, 281, 360, 307, 352, 281, 452, 661, 4910, 51104], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4902, "seek": 2171716, "start": 21731.96, "end": 21736.12, "text": " here where I've actually typed all of the code, just in full and do a quick summary of everything", "tokens": [51104, 510, 689, 286, 600, 767, 33941, 439, 295, 264, 3089, 11, 445, 294, 1577, 293, 360, 257, 1702, 12691, 295, 1203, 51312], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4903, "seek": 2171716, "start": 21736.12, "end": 21739.48, "text": " that we've done, just because there was a lot that went on. And then from there, I'm actually", "tokens": [51312, 300, 321, 600, 1096, 11, 445, 570, 456, 390, 257, 688, 300, 1437, 322, 13, 400, 550, 490, 456, 11, 286, 478, 767, 51480], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4904, "seek": 2171716, "start": 21739.48, "end": 21743.96, "text": " going to train this on a B movie script and show you kind of how that works in comparison to the", "tokens": [51480, 516, 281, 3847, 341, 322, 257, 363, 3169, 5755, 293, 855, 291, 733, 295, 577, 300, 1985, 294, 9660, 281, 264, 51704], "temperature": 0.0, "avg_logprob": -0.07245251158593406, "compression_ratio": 1.7074626865671643, "no_speech_prob": 0.030209239572286606}, {"id": 4905, "seek": 2174396, "start": 21743.96, "end": 21749.16, "text": " Romeo and Juliet. Okay, so what I'm in now is just the exact same notebook we have before,", "tokens": [50364, 33563, 293, 33532, 13, 1033, 11, 370, 437, 286, 478, 294, 586, 307, 445, 264, 1900, 912, 21060, 321, 362, 949, 11, 50624], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4906, "seek": 2174396, "start": 21749.16, "end": 21754.04, "text": " but I've just pretty much copied all the text in here. Or it's the exact same code we had before.", "tokens": [50624, 457, 286, 600, 445, 1238, 709, 25365, 439, 264, 2487, 294, 510, 13, 1610, 309, 311, 264, 1900, 912, 3089, 321, 632, 949, 13, 50868], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4907, "seek": 2174396, "start": 21754.04, "end": 21757.559999999998, "text": " So we just don't have all that other text in between. So I can kind of do a short summary", "tokens": [50868, 407, 321, 445, 500, 380, 362, 439, 300, 661, 2487, 294, 1296, 13, 407, 286, 393, 733, 295, 360, 257, 2099, 12691, 51044], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4908, "seek": 2174396, "start": 21757.559999999998, "end": 21762.92, "text": " of what we did, as well as show you how this worked when I trained it on the B movie script.", "tokens": [51044, 295, 437, 321, 630, 11, 382, 731, 382, 855, 291, 577, 341, 2732, 562, 286, 8895, 309, 322, 264, 363, 3169, 5755, 13, 51312], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4909, "seek": 2174396, "start": 21762.92, "end": 21766.52, "text": " So I did mention I was going to show you that I'm not lying, I will show you can see I've", "tokens": [51312, 407, 286, 630, 2152, 286, 390, 516, 281, 855, 291, 300, 286, 478, 406, 8493, 11, 286, 486, 855, 291, 393, 536, 286, 600, 51492], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4910, "seek": 2174396, "start": 21766.52, "end": 21772.28, "text": " got B movie dot txt loaded in here. And in fact, actually, I'm going to show you this script first", "tokens": [51492, 658, 363, 3169, 5893, 256, 734, 13210, 294, 510, 13, 400, 294, 1186, 11, 767, 11, 286, 478, 516, 281, 855, 291, 341, 5755, 700, 51780], "temperature": 0.0, "avg_logprob": -0.06229981509121982, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.01744101755321026}, {"id": 4911, "seek": 2177228, "start": 21772.28, "end": 21776.92, "text": " to show you what it looks like. So this is what the B movie script looks like. You can see it", "tokens": [50364, 281, 855, 291, 437, 309, 1542, 411, 13, 407, 341, 307, 437, 264, 363, 3169, 5755, 1542, 411, 13, 509, 393, 536, 309, 50596], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4912, "seek": 2177228, "start": 21776.92, "end": 21782.199999999997, "text": " just like a long, you know, script of text, I just downloaded this for free off the internet.", "tokens": [50596, 445, 411, 257, 938, 11, 291, 458, 11, 5755, 295, 2487, 11, 286, 445, 21748, 341, 337, 1737, 766, 264, 4705, 13, 50860], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4913, "seek": 2177228, "start": 21782.199999999997, "end": 21786.28, "text": " And it's actually not as long as the Romeo and Juliet play. So we're not going to get as good", "tokens": [50860, 400, 309, 311, 767, 406, 382, 938, 382, 264, 33563, 293, 33532, 862, 13, 407, 321, 434, 406, 516, 281, 483, 382, 665, 51064], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4914, "seek": 2177228, "start": 21786.28, "end": 21791.16, "text": " of results from our model. But it should hopefully be okay. So we just start and I'm just going to", "tokens": [51064, 295, 3542, 490, 527, 2316, 13, 583, 309, 820, 4696, 312, 1392, 13, 407, 321, 445, 722, 293, 286, 478, 445, 516, 281, 51308], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4915, "seek": 2177228, "start": 21791.16, "end": 21794.28, "text": " do a brief summary. And then I'll show you the results from the B movie script, just so that", "tokens": [51308, 360, 257, 5353, 12691, 13, 400, 550, 286, 603, 855, 291, 264, 3542, 490, 264, 363, 3169, 5755, 11, 445, 370, 300, 51464], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4916, "seek": 2177228, "start": 21794.28, "end": 21798.12, "text": " people that are confused, maybe have something that wraps it up here. We're doing our imports.", "tokens": [51464, 561, 300, 366, 9019, 11, 1310, 362, 746, 300, 25831, 309, 493, 510, 13, 492, 434, 884, 527, 41596, 13, 51656], "temperature": 0.0, "avg_logprob": -0.0604683104015532, "compression_ratio": 1.7861635220125787, "no_speech_prob": 0.011330523528158665}, {"id": 4917, "seek": 2179812, "start": 21798.12, "end": 21802.52, "text": " I don't think I need to explain that this part up here is just loading in your file. Again,", "tokens": [50364, 286, 500, 380, 519, 286, 643, 281, 2903, 300, 341, 644, 493, 510, 307, 445, 15114, 294, 428, 3991, 13, 3764, 11, 50584], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4918, "seek": 2179812, "start": 21802.52, "end": 21807.239999999998, "text": " I don't think I need to explain that. Then we're actually going to read the file. So open it from", "tokens": [50584, 286, 500, 380, 519, 286, 643, 281, 2903, 300, 13, 1396, 321, 434, 767, 516, 281, 1401, 264, 3991, 13, 407, 1269, 309, 490, 50820], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4919, "seek": 2179812, "start": 21807.239999999998, "end": 21814.68, "text": " our directory, decode it into utf eight, we're going to create a vocabulary and encode all of", "tokens": [50820, 527, 21120, 11, 979, 1429, 309, 666, 2839, 69, 3180, 11, 321, 434, 516, 281, 1884, 257, 19864, 293, 2058, 1429, 439, 295, 51192], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4920, "seek": 2179812, "start": 21814.68, "end": 21819.32, "text": " the text that's inside of this file. Then what we're going to do is turn all of that text up", "tokens": [51192, 264, 2487, 300, 311, 1854, 295, 341, 3991, 13, 1396, 437, 321, 434, 516, 281, 360, 307, 1261, 439, 295, 300, 2487, 493, 51424], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4921, "seek": 2179812, "start": 21819.32, "end": 21823.48, "text": " into you know, the encoded version, we're writing a function here that goes the other way around.", "tokens": [51424, 666, 291, 458, 11, 264, 2058, 12340, 3037, 11, 321, 434, 3579, 257, 2445, 510, 300, 1709, 264, 661, 636, 926, 13, 51632], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4922, "seek": 2179812, "start": 21823.48, "end": 21827.96, "text": " So from int to text, not from text to int, we're going to define the sequence length that we", "tokens": [51632, 407, 490, 560, 281, 2487, 11, 406, 490, 2487, 281, 560, 11, 321, 434, 516, 281, 6964, 264, 8310, 4641, 300, 321, 51856], "temperature": 0.0, "avg_logprob": -0.08374974568684895, "compression_ratio": 1.9155405405405406, "no_speech_prob": 0.2750767767429352}, {"id": 4923, "seek": 2182796, "start": 21827.96, "end": 21832.68, "text": " want to train with, which will be sequence length of 100. You can decrease this value if you want,", "tokens": [50364, 528, 281, 3847, 365, 11, 597, 486, 312, 8310, 4641, 295, 2319, 13, 509, 393, 11514, 341, 2158, 498, 291, 528, 11, 50600], "temperature": 0.0, "avg_logprob": -0.10799754749644887, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.0017545279115438461}, {"id": 4924, "seek": 2182796, "start": 21832.68, "end": 21837.239999999998, "text": " you go 50, go 20, it doesn't really matter. It's up to you. It just that's going to determine", "tokens": [50600, 291, 352, 2625, 11, 352, 945, 11, 309, 1177, 380, 534, 1871, 13, 467, 311, 493, 281, 291, 13, 467, 445, 300, 311, 516, 281, 6997, 50828], "temperature": 0.0, "avg_logprob": -0.10799754749644887, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.0017545279115438461}, {"id": 4925, "seek": 2182796, "start": 21837.239999999998, "end": 21841.399999999998, "text": " how many training examples you're going to have right is the sequence length. Next, what we're", "tokens": [50828, 577, 867, 3097, 5110, 291, 434, 516, 281, 362, 558, 307, 264, 8310, 4641, 13, 3087, 11, 437, 321, 434, 51036], "temperature": 0.0, "avg_logprob": -0.10799754749644887, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.0017545279115438461}, {"id": 4926, "seek": 2182796, "start": 21841.399999999998, "end": 21846.68, "text": " going to do is create a character data set from tensor slices from text as int. What this is going", "tokens": [51036, 516, 281, 360, 307, 1884, 257, 2517, 1412, 992, 490, 40863, 19793, 490, 2487, 382, 560, 13, 708, 341, 307, 516, 51300], "temperature": 0.0, "avg_logprob": -0.10799754749644887, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.0017545279115438461}, {"id": 4927, "seek": 2182796, "start": 21846.68, "end": 21853.16, "text": " to do is just convert our entire text that's now an integer array into a bunch of slices of", "tokens": [51300, 281, 360, 307, 445, 7620, 527, 2302, 2487, 300, 311, 586, 364, 24922, 10225, 666, 257, 3840, 295, 19793, 295, 51624], "temperature": 0.0, "avg_logprob": -0.10799754749644887, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.0017545279115438461}, {"id": 4928, "seek": 2185316, "start": 21853.16, "end": 21857.24, "text": " characters. And so that's what this is doing here. So or not slices, what am I saying,", "tokens": [50364, 4342, 13, 400, 370, 300, 311, 437, 341, 307, 884, 510, 13, 407, 420, 406, 19793, 11, 437, 669, 286, 1566, 11, 50568], "temperature": 0.0, "avg_logprob": -0.12385399321205595, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.03409791365265846}, {"id": 4929, "seek": 2185316, "start": 21857.96, "end": 21863.24, "text": " you're just going to convert, like, split that entire array into just characters, like that's", "tokens": [50604, 291, 434, 445, 516, 281, 7620, 11, 411, 11, 7472, 300, 2302, 10225, 666, 445, 4342, 11, 411, 300, 311, 50868], "temperature": 0.0, "avg_logprob": -0.12385399321205595, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.03409791365265846}, {"id": 4930, "seek": 2185316, "start": 21863.24, "end": 21867.48, "text": " pretty much what it's doing. And then what we're going to say sequences equals char data set dot", "tokens": [50868, 1238, 709, 437, 309, 311, 884, 13, 400, 550, 437, 321, 434, 516, 281, 584, 22978, 6915, 1290, 1412, 992, 5893, 51080], "temperature": 0.0, "avg_logprob": -0.12385399321205595, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.03409791365265846}, {"id": 4931, "seek": 2185316, "start": 21867.48, "end": 21872.2, "text": " batch, which now is going to take all those characters and batch them in lengths of 101.", "tokens": [51080, 15245, 11, 597, 586, 307, 516, 281, 747, 439, 729, 4342, 293, 15245, 552, 294, 26329, 295, 21055, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12385399321205595, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.03409791365265846}, {"id": 4932, "seek": 2185316, "start": 21872.2, "end": 21877.56, "text": " What we're going to do then is split all of that into the training examples. So like this, right,", "tokens": [51316, 708, 321, 434, 516, 281, 360, 550, 307, 7472, 439, 295, 300, 666, 264, 3097, 5110, 13, 407, 411, 341, 11, 558, 11, 51584], "temperature": 0.0, "avg_logprob": -0.12385399321205595, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.03409791365265846}, {"id": 4933, "seek": 2187756, "start": 21877.56, "end": 21884.120000000003, "text": " he ll and then yellow, we're going to map this function to sequences, which means we're going", "tokens": [50364, 415, 4849, 293, 550, 5566, 11, 321, 434, 516, 281, 4471, 341, 2445, 281, 22978, 11, 597, 1355, 321, 434, 516, 50692], "temperature": 0.0, "avg_logprob": -0.11572587699220892, "compression_ratio": 1.904382470119522, "no_speech_prob": 0.046026937663555145}, {"id": 4934, "seek": 2187756, "start": 21884.120000000003, "end": 21889.800000000003, "text": " to apply this to every single sequence and store that in data set. Then we're going to find the", "tokens": [50692, 281, 3079, 341, 281, 633, 2167, 8310, 293, 3531, 300, 294, 1412, 992, 13, 1396, 321, 434, 516, 281, 915, 264, 50976], "temperature": 0.0, "avg_logprob": -0.11572587699220892, "compression_ratio": 1.904382470119522, "no_speech_prob": 0.046026937663555145}, {"id": 4935, "seek": 2187756, "start": 21889.800000000003, "end": 21894.760000000002, "text": " parameters for our initial network. We're going to shuffle the data set and batch that into now", "tokens": [50976, 9834, 337, 527, 5883, 3209, 13, 492, 434, 516, 281, 39426, 264, 1412, 992, 293, 15245, 300, 666, 586, 51224], "temperature": 0.0, "avg_logprob": -0.11572587699220892, "compression_ratio": 1.904382470119522, "no_speech_prob": 0.046026937663555145}, {"id": 4936, "seek": 2187756, "start": 21894.760000000002, "end": 21899.960000000003, "text": " 64 training examples. Then we're going to make the function that builds the model, which I've", "tokens": [51224, 12145, 3097, 5110, 13, 1396, 321, 434, 516, 281, 652, 264, 2445, 300, 15182, 264, 2316, 11, 597, 286, 600, 51484], "temperature": 0.0, "avg_logprob": -0.11572587699220892, "compression_ratio": 1.904382470119522, "no_speech_prob": 0.046026937663555145}, {"id": 4937, "seek": 2187756, "start": 21899.960000000003, "end": 21905.56, "text": " already discussed, we're going to actually build the model starting with a batch size of 64. We're", "tokens": [51484, 1217, 7152, 11, 321, 434, 516, 281, 767, 1322, 264, 2316, 2891, 365, 257, 15245, 2744, 295, 12145, 13, 492, 434, 51764], "temperature": 0.0, "avg_logprob": -0.11572587699220892, "compression_ratio": 1.904382470119522, "no_speech_prob": 0.046026937663555145}, {"id": 4938, "seek": 2190556, "start": 21905.56, "end": 21912.280000000002, "text": " going to create our loss function, compile the model, set our checkpoints for saving, and then", "tokens": [50364, 516, 281, 1884, 527, 4470, 2445, 11, 31413, 264, 2316, 11, 992, 527, 1520, 20552, 337, 6816, 11, 293, 550, 50700], "temperature": 0.0, "avg_logprob": -0.08603243100441109, "compression_ratio": 1.8825910931174088, "no_speech_prob": 0.0011335302842780948}, {"id": 4939, "seek": 2190556, "start": 21912.280000000002, "end": 21918.120000000003, "text": " train the model and make sure that we say checkpoint callback as the checkpoint callback", "tokens": [50700, 3847, 264, 2316, 293, 652, 988, 300, 321, 584, 42269, 818, 3207, 382, 264, 42269, 818, 3207, 50992], "temperature": 0.0, "avg_logprob": -0.08603243100441109, "compression_ratio": 1.8825910931174088, "no_speech_prob": 0.0011335302842780948}, {"id": 4940, "seek": 2190556, "start": 21918.120000000003, "end": 21922.280000000002, "text": " for the model, which means it's going to save every epoch, the weights that the model had", "tokens": [50992, 337, 264, 2316, 11, 597, 1355, 309, 311, 516, 281, 3155, 633, 30992, 339, 11, 264, 17443, 300, 264, 2316, 632, 51200], "temperature": 0.0, "avg_logprob": -0.08603243100441109, "compression_ratio": 1.8825910931174088, "no_speech_prob": 0.0011335302842780948}, {"id": 4941, "seek": 2190556, "start": 21922.280000000002, "end": 21927.88, "text": " computed at that epoch. So after we do that, then our models train so we've trained the model,", "tokens": [51200, 40610, 412, 300, 30992, 339, 13, 407, 934, 321, 360, 300, 11, 550, 527, 5245, 3847, 370, 321, 600, 8895, 264, 2316, 11, 51480], "temperature": 0.0, "avg_logprob": -0.08603243100441109, "compression_ratio": 1.8825910931174088, "no_speech_prob": 0.0011335302842780948}, {"id": 4942, "seek": 2190556, "start": 21927.88, "end": 21932.36, "text": " you can see I train this on 50 epochs for the B movie script. And then what we're going to do is", "tokens": [51480, 291, 393, 536, 286, 3847, 341, 322, 2625, 30992, 28346, 337, 264, 363, 3169, 5755, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 51704], "temperature": 0.0, "avg_logprob": -0.08603243100441109, "compression_ratio": 1.8825910931174088, "no_speech_prob": 0.0011335302842780948}, {"id": 4943, "seek": 2193236, "start": 21932.36, "end": 21939.0, "text": " build the model now with a batch size of one. So we can pass one example to it and get a prediction,", "tokens": [50364, 1322, 264, 2316, 586, 365, 257, 15245, 2744, 295, 472, 13, 407, 321, 393, 1320, 472, 1365, 281, 309, 293, 483, 257, 17630, 11, 50696], "temperature": 0.0, "avg_logprob": -0.07133911036643661, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.012053035199642181}, {"id": 4944, "seek": 2193236, "start": 21939.0, "end": 21943.0, "text": " we're going to load the most recent weights into our model from the checkpoint directory", "tokens": [50696, 321, 434, 516, 281, 3677, 264, 881, 5162, 17443, 666, 527, 2316, 490, 264, 42269, 21120, 50896], "temperature": 0.0, "avg_logprob": -0.07133911036643661, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.012053035199642181}, {"id": 4945, "seek": 2193236, "start": 21943.0, "end": 21947.32, "text": " that we defined above. And then what we're going to do is build the model and tell it to", "tokens": [50896, 300, 321, 7642, 3673, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 1322, 264, 2316, 293, 980, 309, 281, 51112], "temperature": 0.0, "avg_logprob": -0.07133911036643661, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.012053035199642181}, {"id": 4946, "seek": 2193236, "start": 21947.32, "end": 21953.96, "text": " expect the shape one, none as its initial input. Now none just means we don't know what that value", "tokens": [51112, 2066, 264, 3909, 472, 11, 6022, 382, 1080, 5883, 4846, 13, 823, 6022, 445, 1355, 321, 500, 380, 458, 437, 300, 2158, 51444], "temperature": 0.0, "avg_logprob": -0.07133911036643661, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.012053035199642181}, {"id": 4947, "seek": 2193236, "start": 21953.96, "end": 21957.64, "text": " is going to be, but we know we're going to have one entry. Alright, so now we have this generate", "tokens": [51444, 307, 516, 281, 312, 11, 457, 321, 458, 321, 434, 516, 281, 362, 472, 8729, 13, 2798, 11, 370, 586, 321, 362, 341, 8460, 51628], "temperature": 0.0, "avg_logprob": -0.07133911036643661, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.012053035199642181}, {"id": 4948, "seek": 2195764, "start": 21957.72, "end": 21962.28, "text": " text method, or function here, which I've already kind of went through how that works. And then", "tokens": [50368, 2487, 3170, 11, 420, 2445, 510, 11, 597, 286, 600, 1217, 733, 295, 1437, 807, 577, 300, 1985, 13, 400, 550, 50596], "temperature": 0.0, "avg_logprob": -0.11388340450468518, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.09533961862325668}, {"id": 4949, "seek": 2195764, "start": 21962.92, "end": 21967.64, "text": " we can see, if I type in input string, so we type, you know, input string, let's say,", "tokens": [50628, 321, 393, 536, 11, 498, 286, 2010, 294, 4846, 6798, 11, 370, 321, 2010, 11, 291, 458, 11, 4846, 6798, 11, 718, 311, 584, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11388340450468518, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.09533961862325668}, {"id": 4950, "seek": 2195764, "start": 21968.68, "end": 21975.48, "text": " of hello, and hit enter, we'll watch and we can see that the B movie, you know, trained model", "tokens": [50916, 295, 7751, 11, 293, 2045, 3242, 11, 321, 603, 1159, 293, 321, 393, 536, 300, 264, 363, 3169, 11, 291, 458, 11, 8895, 2316, 51256], "temperature": 0.0, "avg_logprob": -0.11388340450468518, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.09533961862325668}, {"id": 4951, "seek": 2195764, "start": 21975.48, "end": 21980.6, "text": " comes up with its output here. Now, unfortunately, the B movie script does not work as well as Romeo", "tokens": [51256, 1487, 493, 365, 1080, 5598, 510, 13, 823, 11, 7015, 11, 264, 363, 3169, 5755, 775, 406, 589, 382, 731, 382, 33563, 51512], "temperature": 0.0, "avg_logprob": -0.11388340450468518, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.09533961862325668}, {"id": 4952, "seek": 2195764, "start": 21980.6, "end": 21985.8, "text": " and Juliet. That's just because Romeo and Juliet is a much longer piece of text. It's much better", "tokens": [51512, 293, 33532, 13, 663, 311, 445, 570, 33563, 293, 33532, 307, 257, 709, 2854, 2522, 295, 2487, 13, 467, 311, 709, 1101, 51772], "temperature": 0.0, "avg_logprob": -0.11388340450468518, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.09533961862325668}, {"id": 4953, "seek": 2198580, "start": 21986.44, "end": 21991.559999999998, "text": " it's format a lot nicer and a lot more predictable. But yeah, you kind of get the idea here and it's", "tokens": [50396, 309, 311, 7877, 257, 688, 22842, 293, 257, 688, 544, 27737, 13, 583, 1338, 11, 291, 733, 295, 483, 264, 1558, 510, 293, 309, 311, 50652], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4954, "seek": 2198580, "start": 21991.559999999998, "end": 21995.96, "text": " kind of cool to see how this performs on different data. So I would highly recommend that you guys", "tokens": [50652, 733, 295, 1627, 281, 536, 577, 341, 26213, 322, 819, 1412, 13, 407, 286, 576, 5405, 2748, 300, 291, 1074, 50872], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4955, "seek": 2198580, "start": 21995.96, "end": 22000.92, "text": " find some training data that you could give this other than just the Romeo and Juliet or maybe", "tokens": [50872, 915, 512, 3097, 1412, 300, 291, 727, 976, 341, 661, 813, 445, 264, 33563, 293, 33532, 420, 1310, 51120], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4956, "seek": 2198580, "start": 22000.92, "end": 22005.64, "text": " even try another play or something and see what you can get out of it. Also, quick side note,", "tokens": [51120, 754, 853, 1071, 862, 420, 746, 293, 536, 437, 291, 393, 483, 484, 295, 309, 13, 2743, 11, 1702, 1252, 3637, 11, 51356], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4957, "seek": 2198580, "start": 22005.64, "end": 22010.04, "text": " to make your model better, increase the amount of epochs here. Ideally, you want this loss to", "tokens": [51356, 281, 652, 428, 2316, 1101, 11, 3488, 264, 2372, 295, 30992, 28346, 510, 13, 40817, 11, 291, 528, 341, 4470, 281, 51576], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4958, "seek": 2198580, "start": 22010.04, "end": 22015.239999999998, "text": " be as low as possible, you can see mine was still actually moving down at epoch 50. You will", "tokens": [51576, 312, 382, 2295, 382, 1944, 11, 291, 393, 536, 3892, 390, 920, 767, 2684, 760, 412, 30992, 339, 2625, 13, 509, 486, 51836], "temperature": 0.0, "avg_logprob": -0.08492620226363061, "compression_ratio": 1.7113095238095237, "no_speech_prob": 0.014502284117043018}, {"id": 4959, "seek": 2201524, "start": 22015.24, "end": 22019.72, "text": " reach a point where the amount of epochs won't make a difference. Although, with models like this,", "tokens": [50364, 2524, 257, 935, 689, 264, 2372, 295, 30992, 28346, 1582, 380, 652, 257, 2649, 13, 5780, 11, 365, 5245, 411, 341, 11, 50588], "temperature": 0.0, "avg_logprob": -0.06602259380061452, "compression_ratio": 1.7, "no_speech_prob": 0.004905005916953087}, {"id": 4960, "seek": 2201524, "start": 22019.72, "end": 22024.52, "text": " the more epochs typically the better, because it's difficult for it to kind of overfit, because all", "tokens": [50588, 264, 544, 30992, 28346, 5850, 264, 1101, 11, 570, 309, 311, 2252, 337, 309, 281, 733, 295, 670, 6845, 11, 570, 439, 50828], "temperature": 0.0, "avg_logprob": -0.06602259380061452, "compression_ratio": 1.7, "no_speech_prob": 0.004905005916953087}, {"id": 4961, "seek": 2201524, "start": 22024.52, "end": 22030.68, "text": " you want it to do really is just kind of learn how the language works and then be able to replicate", "tokens": [50828, 291, 528, 309, 281, 360, 534, 307, 445, 733, 295, 1466, 577, 264, 2856, 1985, 293, 550, 312, 1075, 281, 25356, 51136], "temperature": 0.0, "avg_logprob": -0.06602259380061452, "compression_ratio": 1.7, "no_speech_prob": 0.004905005916953087}, {"id": 4962, "seek": 2201524, "start": 22030.68, "end": 22035.56, "text": " that to you almost, right? So that's kind of the idea here. And with that being said, I'm going to", "tokens": [51136, 300, 281, 291, 1920, 11, 558, 30, 407, 300, 311, 733, 295, 264, 1558, 510, 13, 400, 365, 300, 885, 848, 11, 286, 478, 516, 281, 51380], "temperature": 0.0, "avg_logprob": -0.06602259380061452, "compression_ratio": 1.7, "no_speech_prob": 0.004905005916953087}, {"id": 4963, "seek": 2201524, "start": 22035.56, "end": 22040.600000000002, "text": " say that this section is probably done. Now, I know this was a long, probably confusing section", "tokens": [51380, 584, 300, 341, 3541, 307, 1391, 1096, 13, 823, 11, 286, 458, 341, 390, 257, 938, 11, 1391, 13181, 3541, 51632], "temperature": 0.0, "avg_logprob": -0.06602259380061452, "compression_ratio": 1.7, "no_speech_prob": 0.004905005916953087}, {"id": 4964, "seek": 2204060, "start": 22040.6, "end": 22044.68, "text": " for a lot of you. But this is, you know, what happens when you start getting into some more", "tokens": [50364, 337, 257, 688, 295, 291, 13, 583, 341, 307, 11, 291, 458, 11, 437, 2314, 562, 291, 722, 1242, 666, 512, 544, 50568], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4965, "seek": 2204060, "start": 22044.68, "end": 22048.68, "text": " complex things in machine learning, it's very difficult to kind of grasp and understand all", "tokens": [50568, 3997, 721, 294, 3479, 2539, 11, 309, 311, 588, 2252, 281, 733, 295, 21743, 293, 1223, 439, 50768], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4966, "seek": 2204060, "start": 22048.68, "end": 22053.32, "text": " these concepts in an hour of me just explaining them. What I try to do in these videos is introduce", "tokens": [50768, 613, 10392, 294, 364, 1773, 295, 385, 445, 13468, 552, 13, 708, 286, 853, 281, 360, 294, 613, 2145, 307, 5366, 51000], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4967, "seek": 2204060, "start": 22053.32, "end": 22057.719999999998, "text": " you to the syntax show you how to get a working, you know, kind of prototype and hopefully give", "tokens": [51000, 291, 281, 264, 28431, 855, 291, 577, 281, 483, 257, 1364, 11, 291, 458, 11, 733, 295, 19475, 293, 4696, 976, 51220], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4968, "seek": 2204060, "start": 22057.719999999998, "end": 22061.559999999998, "text": " you enough knowledge to the fact where if you're confused by something that I said, you can go", "tokens": [51220, 291, 1547, 3601, 281, 264, 1186, 689, 498, 291, 434, 9019, 538, 746, 300, 286, 848, 11, 291, 393, 352, 51412], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4969, "seek": 2204060, "start": 22061.559999999998, "end": 22066.199999999997, "text": " and you can look that up and you can figure out kind of the more important details for yourself,", "tokens": [51412, 293, 291, 393, 574, 300, 493, 293, 291, 393, 2573, 484, 733, 295, 264, 544, 1021, 4365, 337, 1803, 11, 51644], "temperature": 0.0, "avg_logprob": -0.06342993135805483, "compression_ratio": 1.756923076923077, "no_speech_prob": 0.8393764495849609}, {"id": 4970, "seek": 2206620, "start": 22066.2, "end": 22070.52, "text": " because I really just I can't go into all, you know, the extremes in these videos. So anyways,", "tokens": [50364, 570, 286, 534, 445, 286, 393, 380, 352, 666, 439, 11, 291, 458, 11, 264, 41119, 294, 613, 2145, 13, 407, 13448, 11, 50580], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4971, "seek": 2206620, "start": 22070.52, "end": 22074.52, "text": " that has been this section. I hope you guys enjoyed doing this. I thought this was pretty cool.", "tokens": [50580, 300, 575, 668, 341, 3541, 13, 286, 1454, 291, 1074, 4626, 884, 341, 13, 286, 1194, 341, 390, 1238, 1627, 13, 50780], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4972, "seek": 2206620, "start": 22074.52, "end": 22077.4, "text": " And in the next section, we're going to be talking about reinforcement learning.", "tokens": [50780, 400, 294, 264, 958, 3541, 11, 321, 434, 516, 281, 312, 1417, 466, 29280, 2539, 13, 50924], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4973, "seek": 2206620, "start": 22080.600000000002, "end": 22085.48, "text": " Hello, everyone, and welcome to the next module in this course on reinforcement learning. So what", "tokens": [51084, 2425, 11, 1518, 11, 293, 2928, 281, 264, 958, 10088, 294, 341, 1164, 322, 29280, 2539, 13, 407, 437, 51328], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4974, "seek": 2206620, "start": 22085.48, "end": 22088.920000000002, "text": " we're going to be doing in this module is talking about another technique in machine learning called", "tokens": [51328, 321, 434, 516, 281, 312, 884, 294, 341, 10088, 307, 1417, 466, 1071, 6532, 294, 3479, 2539, 1219, 51500], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4975, "seek": 2206620, "start": 22088.920000000002, "end": 22092.920000000002, "text": " reinforcement learning. Now, if you remember at the very beginning of this course, which I know", "tokens": [51500, 29280, 2539, 13, 823, 11, 498, 291, 1604, 412, 264, 588, 2863, 295, 341, 1164, 11, 597, 286, 458, 51700], "temperature": 0.0, "avg_logprob": -0.07447304358849159, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.016913099214434624}, {"id": 4976, "seek": 2209292, "start": 22093.0, "end": 22098.199999999997, "text": " for you guys is probably at like six hours ago at this point, we did briefly discuss what reinforcement", "tokens": [50368, 337, 291, 1074, 307, 1391, 412, 411, 2309, 2496, 2057, 412, 341, 935, 11, 321, 630, 10515, 2248, 437, 29280, 50628], "temperature": 0.0, "avg_logprob": -0.07102026981590069, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.03845679759979248}, {"id": 4977, "seek": 2209292, "start": 22098.199999999997, "end": 22102.359999999997, "text": " learning was. Now I'll go through a recap here just to make sure everyone's clear on it. But", "tokens": [50628, 2539, 390, 13, 823, 286, 603, 352, 807, 257, 20928, 510, 445, 281, 652, 988, 1518, 311, 1850, 322, 309, 13, 583, 50836], "temperature": 0.0, "avg_logprob": -0.07102026981590069, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.03845679759979248}, {"id": 4978, "seek": 2209292, "start": 22102.359999999997, "end": 22107.0, "text": " essentially, reinforcement learning is kind of the strategy in machine learning where rather", "tokens": [50836, 4476, 11, 29280, 2539, 307, 733, 295, 264, 5206, 294, 3479, 2539, 689, 2831, 51068], "temperature": 0.0, "avg_logprob": -0.07102026981590069, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.03845679759979248}, {"id": 4979, "seek": 2209292, "start": 22107.0, "end": 22112.839999999997, "text": " than feeding a ton of data and a ton of examples to our model, we let the model or in this case,", "tokens": [51068, 813, 12919, 257, 2952, 295, 1412, 293, 257, 2952, 295, 5110, 281, 527, 2316, 11, 321, 718, 264, 2316, 420, 294, 341, 1389, 11, 51360], "temperature": 0.0, "avg_logprob": -0.07102026981590069, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.03845679759979248}, {"id": 4980, "seek": 2209292, "start": 22112.839999999997, "end": 22117.8, "text": " we're going to call it agent actually come up with these examples itself. And we do this by letting", "tokens": [51360, 321, 434, 516, 281, 818, 309, 9461, 767, 808, 493, 365, 613, 5110, 2564, 13, 400, 321, 360, 341, 538, 8295, 51608], "temperature": 0.0, "avg_logprob": -0.07102026981590069, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.03845679759979248}, {"id": 4981, "seek": 2211780, "start": 22117.88, "end": 22123.48, "text": " an agent explore an environment. Now, essentially, the concept here is just like humans, the way that", "tokens": [50368, 364, 9461, 6839, 364, 2823, 13, 823, 11, 4476, 11, 264, 3410, 510, 307, 445, 411, 6255, 11, 264, 636, 300, 50648], "temperature": 0.0, "avg_logprob": -0.07200524163624597, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.31055986881256104}, {"id": 4982, "seek": 2211780, "start": 22123.48, "end": 22128.52, "text": " we learn to do something say like play a game is by actually doing it, we get put in the environment,", "tokens": [50648, 321, 1466, 281, 360, 746, 584, 411, 862, 257, 1216, 307, 538, 767, 884, 309, 11, 321, 483, 829, 294, 264, 2823, 11, 50900], "temperature": 0.0, "avg_logprob": -0.07200524163624597, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.31055986881256104}, {"id": 4983, "seek": 2211780, "start": 22128.52, "end": 22132.44, "text": " we try to do it. And then, you know, we'll make mistakes, we'll encounter different things,", "tokens": [50900, 321, 853, 281, 360, 309, 13, 400, 550, 11, 291, 458, 11, 321, 603, 652, 8038, 11, 321, 603, 8593, 819, 721, 11, 51096], "temperature": 0.0, "avg_logprob": -0.07200524163624597, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.31055986881256104}, {"id": 4984, "seek": 2211780, "start": 22132.44, "end": 22137.719999999998, "text": " we'll see what goes correctly. And based on those experiences, we learn and we figure out the correct", "tokens": [51096, 321, 603, 536, 437, 1709, 8944, 13, 400, 2361, 322, 729, 5235, 11, 321, 1466, 293, 321, 2573, 484, 264, 3006, 51360], "temperature": 0.0, "avg_logprob": -0.07200524163624597, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.31055986881256104}, {"id": 4985, "seek": 2211780, "start": 22137.719999999998, "end": 22143.559999999998, "text": " things to do a very basic example is, you know, say we play a game. And when we go left, we fell", "tokens": [51360, 721, 281, 360, 257, 588, 3875, 1365, 307, 11, 291, 458, 11, 584, 321, 862, 257, 1216, 13, 400, 562, 321, 352, 1411, 11, 321, 5696, 51652], "temperature": 0.0, "avg_logprob": -0.07200524163624597, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.31055986881256104}, {"id": 4986, "seek": 2214356, "start": 22143.640000000003, "end": 22147.88, "text": " off a cliff or something, right? Next time we play that game, and we get to that point, we're", "tokens": [50368, 766, 257, 22316, 420, 746, 11, 558, 30, 3087, 565, 321, 862, 300, 1216, 11, 293, 321, 483, 281, 300, 935, 11, 321, 434, 50580], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4987, "seek": 2214356, "start": 22147.88, "end": 22152.600000000002, "text": " probably not going to go left, because we're going to remember the fact that that was bad, and hence", "tokens": [50580, 1391, 406, 516, 281, 352, 1411, 11, 570, 321, 434, 516, 281, 1604, 264, 1186, 300, 300, 390, 1578, 11, 293, 16678, 50816], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4988, "seek": 2214356, "start": 22152.600000000002, "end": 22157.48, "text": " learned from our mistakes. So that's kind of the idea here with reinforcement learning. I'm going", "tokens": [50816, 3264, 490, 527, 8038, 13, 407, 300, 311, 733, 295, 264, 1558, 510, 365, 29280, 2539, 13, 286, 478, 516, 51060], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4989, "seek": 2214356, "start": 22157.48, "end": 22161.88, "text": " to go through exactly how this works and give some better examples and some math behind one of the", "tokens": [51060, 281, 352, 807, 2293, 577, 341, 1985, 293, 976, 512, 1101, 5110, 293, 512, 5221, 2261, 472, 295, 264, 51280], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4990, "seek": 2214356, "start": 22161.88, "end": 22165.16, "text": " implementations we're going to use. But I just want to make this clear that there's a lot of", "tokens": [51280, 4445, 763, 321, 434, 516, 281, 764, 13, 583, 286, 445, 528, 281, 652, 341, 1850, 300, 456, 311, 257, 688, 295, 51444], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4991, "seek": 2214356, "start": 22165.16, "end": 22169.16, "text": " different types of reinforcement learning. In this example, we're just going to be talking", "tokens": [51444, 819, 3467, 295, 29280, 2539, 13, 682, 341, 1365, 11, 321, 434, 445, 516, 281, 312, 1417, 51644], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4992, "seek": 2214356, "start": 22169.16, "end": 22173.08, "text": " about something called q learning. And I'm going to keep this module shorter compared to the other", "tokens": [51644, 466, 746, 1219, 9505, 2539, 13, 400, 286, 478, 516, 281, 1066, 341, 10088, 11639, 5347, 281, 264, 661, 51840], "temperature": 0.0, "avg_logprob": -0.049215838594256704, "compression_ratio": 1.9039548022598871, "no_speech_prob": 0.009412196464836597}, {"id": 4993, "seek": 2217308, "start": 22173.08, "end": 22179.0, "text": " ones. Because this field of AI machine learning is pretty complex and can get pretty difficult", "tokens": [50364, 2306, 13, 1436, 341, 2519, 295, 7318, 3479, 2539, 307, 1238, 3997, 293, 393, 483, 1238, 2252, 50660], "temperature": 0.0, "avg_logprob": -0.07186296008048801, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.014954703859984875}, {"id": 4994, "seek": 2217308, "start": 22179.0, "end": 22183.640000000003, "text": " pretty quickly. So it's something that's maybe a more advanced topic for some of you guys. Alright,", "tokens": [50660, 1238, 2661, 13, 407, 309, 311, 746, 300, 311, 1310, 257, 544, 7339, 4829, 337, 512, 295, 291, 1074, 13, 2798, 11, 50892], "temperature": 0.0, "avg_logprob": -0.07186296008048801, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.014954703859984875}, {"id": 4995, "seek": 2217308, "start": 22183.640000000003, "end": 22188.280000000002, "text": " so anyways, now we need to define some terminology before I can even start really explaining the", "tokens": [50892, 370, 13448, 11, 586, 321, 643, 281, 6964, 512, 27575, 949, 286, 393, 754, 722, 534, 13468, 264, 51124], "temperature": 0.0, "avg_logprob": -0.07186296008048801, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.014954703859984875}, {"id": 4996, "seek": 2217308, "start": 22188.280000000002, "end": 22192.760000000002, "text": " technique we're going to use and how this works. So we have something called an environment,", "tokens": [51124, 6532, 321, 434, 516, 281, 764, 293, 577, 341, 1985, 13, 407, 321, 362, 746, 1219, 364, 2823, 11, 51348], "temperature": 0.0, "avg_logprob": -0.07186296008048801, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.014954703859984875}, {"id": 4997, "seek": 2217308, "start": 22192.760000000002, "end": 22197.640000000003, "text": " agent, state action and reward. And I'm hoping that some of you guys will remember this from the", "tokens": [51348, 9461, 11, 1785, 3069, 293, 7782, 13, 400, 286, 478, 7159, 300, 512, 295, 291, 1074, 486, 1604, 341, 490, 264, 51592], "temperature": 0.0, "avg_logprob": -0.07186296008048801, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.014954703859984875}, {"id": 4998, "seek": 2219764, "start": 22197.64, "end": 22203.64, "text": " very beginning. But environment is essentially what we're trying to solve or what we're trying to", "tokens": [50364, 588, 2863, 13, 583, 2823, 307, 4476, 437, 321, 434, 1382, 281, 5039, 420, 437, 321, 434, 1382, 281, 50664], "temperature": 0.0, "avg_logprob": -0.06780064733404863, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.1688215136528015}, {"id": 4999, "seek": 2219764, "start": 22203.64, "end": 22208.6, "text": " do. So in reinforcement learning, we have this notion of an agent. And the agent is what's going", "tokens": [50664, 360, 13, 407, 294, 29280, 2539, 11, 321, 362, 341, 10710, 295, 364, 9461, 13, 400, 264, 9461, 307, 437, 311, 516, 50912], "temperature": 0.0, "avg_logprob": -0.06780064733404863, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.1688215136528015}, {"id": 5000, "seek": 2219764, "start": 22208.6, "end": 22213.0, "text": " to explore the environment. So if we're thinking about reinforcement learning, when it comes to", "tokens": [50912, 281, 6839, 264, 2823, 13, 407, 498, 321, 434, 1953, 466, 29280, 2539, 11, 562, 309, 1487, 281, 51132], "temperature": 0.0, "avg_logprob": -0.06780064733404863, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.1688215136528015}, {"id": 5001, "seek": 2219764, "start": 22213.0, "end": 22218.44, "text": " say training an AI to play a game, well, in that instance, say we're talking about Mario, the agent", "tokens": [51132, 584, 3097, 364, 7318, 281, 862, 257, 1216, 11, 731, 11, 294, 300, 5197, 11, 584, 321, 434, 1417, 466, 9343, 11, 264, 9461, 51404], "temperature": 0.0, "avg_logprob": -0.06780064733404863, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.1688215136528015}, {"id": 5002, "seek": 2219764, "start": 22218.44, "end": 22223.32, "text": " would be Mario as that is the thing that's moving around and exploring our environment. And the", "tokens": [51404, 576, 312, 9343, 382, 300, 307, 264, 551, 300, 311, 2684, 926, 293, 12736, 527, 2823, 13, 400, 264, 51648], "temperature": 0.0, "avg_logprob": -0.06780064733404863, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.1688215136528015}, {"id": 5003, "seek": 2222332, "start": 22223.4, "end": 22228.6, "text": " environment would be the level in which we're playing in. So you know, in another example,", "tokens": [50368, 2823, 576, 312, 264, 1496, 294, 597, 321, 434, 2433, 294, 13, 407, 291, 458, 11, 294, 1071, 1365, 11, 50628], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5004, "seek": 2222332, "start": 22228.6, "end": 22232.04, "text": " maybe in the example we're going to use below, we're actually going to be kind of in almost a", "tokens": [50628, 1310, 294, 264, 1365, 321, 434, 516, 281, 764, 2507, 11, 321, 434, 767, 516, 281, 312, 733, 295, 294, 1920, 257, 50800], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5005, "seek": 2222332, "start": 22232.04, "end": 22237.32, "text": " maze. So the environment is going to be the maze. And the agent is going to be the character or the", "tokens": [50800, 33032, 13, 407, 264, 2823, 307, 516, 281, 312, 264, 33032, 13, 400, 264, 9461, 307, 516, 281, 312, 264, 2517, 420, 264, 51064], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5006, "seek": 2222332, "start": 22237.32, "end": 22242.04, "text": " entity or whatever you want to call it, that's exploring that maze. So it's pretty, it's usually", "tokens": [51064, 13977, 420, 2035, 291, 528, 281, 818, 309, 11, 300, 311, 12736, 300, 33032, 13, 407, 309, 311, 1238, 11, 309, 311, 2673, 51300], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5007, "seek": 2222332, "start": 22242.04, "end": 22246.04, "text": " pretty intuitive to come up with what the environment and the agent are, although in some", "tokens": [51300, 1238, 21769, 281, 808, 493, 365, 437, 264, 2823, 293, 264, 9461, 366, 11, 4878, 294, 512, 51500], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5008, "seek": 2222332, "start": 22246.04, "end": 22250.68, "text": " more complex examples, it might not always be clear. But just understand that reinforcement", "tokens": [51500, 544, 3997, 5110, 11, 309, 1062, 406, 1009, 312, 1850, 13, 583, 445, 1223, 300, 29280, 51732], "temperature": 0.0, "avg_logprob": -0.0652931637234158, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.0050600855611264706}, {"id": 5009, "seek": 2225068, "start": 22250.760000000002, "end": 22255.32, "text": " learning deals with an agent, something exploring an environment and a very common", "tokens": [50368, 2539, 11215, 365, 364, 9461, 11, 746, 12736, 364, 2823, 293, 257, 588, 2689, 50596], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5010, "seek": 2225068, "start": 22255.88, "end": 22259.96, "text": " application of reinforcement learning is in training AI is on how to play games. And it's", "tokens": [50624, 3861, 295, 29280, 2539, 307, 294, 3097, 7318, 307, 322, 577, 281, 862, 2813, 13, 400, 309, 311, 50828], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5011, "seek": 2225068, "start": 22259.96, "end": 22263.96, "text": " actually very interesting what they've been able to do in that field recently. Okay, so we have", "tokens": [50828, 767, 588, 1880, 437, 436, 600, 668, 1075, 281, 360, 294, 300, 2519, 3938, 13, 1033, 11, 370, 321, 362, 51028], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5012, "seek": 2225068, "start": 22263.96, "end": 22268.36, "text": " environments and agent, hopefully that makes sense. The next thing to talk about is state. So", "tokens": [51028, 12388, 293, 9461, 11, 4696, 300, 1669, 2020, 13, 440, 958, 551, 281, 751, 466, 307, 1785, 13, 407, 51248], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5013, "seek": 2225068, "start": 22268.36, "end": 22275.08, "text": " essentially, the state is where you are in the environment. So obviously, inside of the environment,", "tokens": [51248, 4476, 11, 264, 1785, 307, 689, 291, 366, 294, 264, 2823, 13, 407, 2745, 11, 1854, 295, 264, 2823, 11, 51584], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5014, "seek": 2225068, "start": 22275.08, "end": 22279.88, "text": " we can have many different states. And a state could also be associated with the, you know,", "tokens": [51584, 321, 393, 362, 867, 819, 4368, 13, 400, 257, 1785, 727, 611, 312, 6615, 365, 264, 11, 291, 458, 11, 51824], "temperature": 0.0, "avg_logprob": -0.08007802581787109, "compression_ratio": 1.750788643533123, "no_speech_prob": 0.01450242381542921}, {"id": 5015, "seek": 2227988, "start": 22279.88, "end": 22285.88, "text": " agent itself. So we're going to say the agent is in a specific state, whenever it is in some", "tokens": [50364, 9461, 2564, 13, 407, 321, 434, 516, 281, 584, 264, 9461, 307, 294, 257, 2685, 1785, 11, 5699, 309, 307, 294, 512, 50664], "temperature": 0.0, "avg_logprob": -0.09366093741522895, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.01798432134091854}, {"id": 5016, "seek": 2227988, "start": 22285.88, "end": 22291.16, "text": " part of the environment. Now, in the case of our game, the state that an agent would be in", "tokens": [50664, 644, 295, 264, 2823, 13, 823, 11, 294, 264, 1389, 295, 527, 1216, 11, 264, 1785, 300, 364, 9461, 576, 312, 294, 50928], "temperature": 0.0, "avg_logprob": -0.09366093741522895, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.01798432134091854}, {"id": 5017, "seek": 2227988, "start": 22291.16, "end": 22296.68, "text": " would be their position in the level, say if they're at, you know, x y coordinates, like 1020,", "tokens": [50928, 576, 312, 641, 2535, 294, 264, 1496, 11, 584, 498, 436, 434, 412, 11, 291, 458, 11, 2031, 288, 21056, 11, 411, 1266, 2009, 11, 51204], "temperature": 0.0, "avg_logprob": -0.09366093741522895, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.01798432134091854}, {"id": 5018, "seek": 2227988, "start": 22296.68, "end": 22303.32, "text": " they would be at state or in state 1020. That's kind of how we think about states. Now, obviously,", "tokens": [51204, 436, 576, 312, 412, 1785, 420, 294, 1785, 1266, 2009, 13, 663, 311, 733, 295, 577, 321, 519, 466, 4368, 13, 823, 11, 2745, 11, 51536], "temperature": 0.0, "avg_logprob": -0.09366093741522895, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.01798432134091854}, {"id": 5019, "seek": 2227988, "start": 22303.32, "end": 22307.56, "text": " state could be applied in some different instances as well. We're playing say, maybe a turn based", "tokens": [51536, 1785, 727, 312, 6456, 294, 512, 819, 14519, 382, 731, 13, 492, 434, 2433, 584, 11, 1310, 257, 1261, 2361, 51748], "temperature": 0.0, "avg_logprob": -0.09366093741522895, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.01798432134091854}, {"id": 5020, "seek": 2230756, "start": 22307.56, "end": 22312.2, "text": " game. You know, actually, that's not really a great example. I'm trying to think of something", "tokens": [50364, 1216, 13, 509, 458, 11, 767, 11, 300, 311, 406, 534, 257, 869, 1365, 13, 286, 478, 1382, 281, 519, 295, 746, 50596], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5021, "seek": 2230756, "start": 22312.2, "end": 22316.440000000002, "text": " where the state wouldn't necessarily be a position, maybe if you're playing a game where you have", "tokens": [50596, 689, 264, 1785, 2759, 380, 4725, 312, 257, 2535, 11, 1310, 498, 291, 434, 2433, 257, 1216, 689, 291, 362, 50808], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5022, "seek": 2230756, "start": 22316.440000000002, "end": 22321.4, "text": " like health or something like that. And part of the state might be the health of the character.", "tokens": [50808, 411, 1585, 420, 746, 411, 300, 13, 400, 644, 295, 264, 1785, 1062, 312, 264, 1585, 295, 264, 2517, 13, 51056], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5023, "seek": 2230756, "start": 22321.960000000003, "end": 22325.48, "text": " This can get complicated, depending on what you're trying to do. But just understand the notion", "tokens": [51084, 639, 393, 483, 6179, 11, 5413, 322, 437, 291, 434, 1382, 281, 360, 13, 583, 445, 1223, 264, 10710, 51260], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5024, "seek": 2230756, "start": 22325.48, "end": 22329.56, "text": " that for most of our example, state is simply going to be in location, although it really is", "tokens": [51260, 300, 337, 881, 295, 527, 1365, 11, 1785, 307, 2935, 516, 281, 312, 294, 4914, 11, 4878, 309, 534, 307, 51464], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5025, "seek": 2230756, "start": 22329.56, "end": 22334.04, "text": " just kind of telling us information about where the agent is, and its status in the environment.", "tokens": [51464, 445, 733, 295, 3585, 505, 1589, 466, 689, 264, 9461, 307, 11, 293, 1080, 6558, 294, 264, 2823, 13, 51688], "temperature": 0.0, "avg_logprob": -0.07548833249220208, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013221532106399536}, {"id": 5026, "seek": 2233404, "start": 22334.84, "end": 22340.280000000002, "text": " So next, we have this notion of an action. So in reinforcement learning, our agent is exploring", "tokens": [50404, 407, 958, 11, 321, 362, 341, 10710, 295, 364, 3069, 13, 407, 294, 29280, 2539, 11, 527, 9461, 307, 12736, 50676], "temperature": 0.0, "avg_logprob": -0.07365886514837092, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.005729914177209139}, {"id": 5027, "seek": 2233404, "start": 22340.280000000002, "end": 22344.440000000002, "text": " the environment, it's trying to figure out the best way or how to accomplish some kind of goal", "tokens": [50676, 264, 2823, 11, 309, 311, 1382, 281, 2573, 484, 264, 1151, 636, 420, 577, 281, 9021, 512, 733, 295, 3387, 50884], "temperature": 0.0, "avg_logprob": -0.07365886514837092, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.005729914177209139}, {"id": 5028, "seek": 2233404, "start": 22344.440000000002, "end": 22348.68, "text": " in the environment. And the way that it interacts with the environment is with something called", "tokens": [50884, 294, 264, 2823, 13, 400, 264, 636, 300, 309, 43582, 365, 264, 2823, 307, 365, 746, 1219, 51096], "temperature": 0.0, "avg_logprob": -0.07365886514837092, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.005729914177209139}, {"id": 5029, "seek": 2233404, "start": 22348.68, "end": 22354.04, "text": " actions. Now, actions could be say, moving the left arrow key, right, moving to the left in", "tokens": [51096, 5909, 13, 823, 11, 5909, 727, 312, 584, 11, 2684, 264, 1411, 11610, 2141, 11, 558, 11, 2684, 281, 264, 1411, 294, 51364], "temperature": 0.0, "avg_logprob": -0.07365886514837092, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.005729914177209139}, {"id": 5030, "seek": 2233404, "start": 22354.04, "end": 22358.280000000002, "text": " the environment, moving to the right, it could be something like jumping in an action can actually", "tokens": [51364, 264, 2823, 11, 2684, 281, 264, 558, 11, 309, 727, 312, 746, 411, 11233, 294, 364, 3069, 393, 767, 51576], "temperature": 0.0, "avg_logprob": -0.07365886514837092, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.005729914177209139}, {"id": 5031, "seek": 2235828, "start": 22358.28, "end": 22364.36, "text": " be not doing something at all. So when we say, you know, agent performed action, that could", "tokens": [50364, 312, 406, 884, 746, 412, 439, 13, 407, 562, 321, 584, 11, 291, 458, 11, 9461, 10332, 3069, 11, 300, 727, 50668], "temperature": 0.0, "avg_logprob": -0.08564995867865426, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.26274898648262024}, {"id": 5032, "seek": 2235828, "start": 22364.36, "end": 22368.68, "text": " really mean that the action and that maybe time step was that they didn't do something, right,", "tokens": [50668, 534, 914, 300, 264, 3069, 293, 300, 1310, 565, 1823, 390, 300, 436, 994, 380, 360, 746, 11, 558, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08564995867865426, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.26274898648262024}, {"id": 5033, "seek": 2235828, "start": 22368.68, "end": 22373.559999999998, "text": " that they didn't do anything that was their action. So that's kind of the idea of action.", "tokens": [50884, 300, 436, 994, 380, 360, 1340, 300, 390, 641, 3069, 13, 407, 300, 311, 733, 295, 264, 1558, 295, 3069, 13, 51128], "temperature": 0.0, "avg_logprob": -0.08564995867865426, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.26274898648262024}, {"id": 5034, "seek": 2235828, "start": 22374.12, "end": 22378.76, "text": " In the example of our Mario one, which I keep going back to an action would be something like", "tokens": [51156, 682, 264, 1365, 295, 527, 9343, 472, 11, 597, 286, 1066, 516, 646, 281, 364, 3069, 576, 312, 746, 411, 51388], "temperature": 0.0, "avg_logprob": -0.08564995867865426, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.26274898648262024}, {"id": 5035, "seek": 2235828, "start": 22378.76, "end": 22384.68, "text": " jumping. And typically actions will change the state of our entity or our agent, although they", "tokens": [51388, 11233, 13, 400, 5850, 5909, 486, 1319, 264, 1785, 295, 527, 13977, 420, 527, 9461, 11, 4878, 436, 51684], "temperature": 0.0, "avg_logprob": -0.08564995867865426, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.26274898648262024}, {"id": 5036, "seek": 2238468, "start": 22384.68, "end": 22389.8, "text": " might not necessarily do that. In fact, we will observe with a lot of the different actions that", "tokens": [50364, 1062, 406, 4725, 360, 300, 13, 682, 1186, 11, 321, 486, 11441, 365, 257, 688, 295, 264, 819, 5909, 300, 50620], "temperature": 0.0, "avg_logprob": -0.051851094326126244, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.006487828213721514}, {"id": 5037, "seek": 2238468, "start": 22389.8, "end": 22394.760000000002, "text": " we could actually be in the same state after performing that action. Alright, so now we're", "tokens": [50620, 321, 727, 767, 312, 294, 264, 912, 1785, 934, 10205, 300, 3069, 13, 2798, 11, 370, 586, 321, 434, 50868], "temperature": 0.0, "avg_logprob": -0.051851094326126244, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.006487828213721514}, {"id": 5038, "seek": 2238468, "start": 22394.760000000002, "end": 22401.24, "text": " on to the last part, which is actually the most important to understand. And this is reward. So", "tokens": [50868, 322, 281, 264, 1036, 644, 11, 597, 307, 767, 264, 881, 1021, 281, 1223, 13, 400, 341, 307, 7782, 13, 407, 51192], "temperature": 0.0, "avg_logprob": -0.051851094326126244, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.006487828213721514}, {"id": 5039, "seek": 2238468, "start": 22401.24, "end": 22406.52, "text": " reward is actually what our agent is trying to maximize while it is in the environment. So the", "tokens": [51192, 7782, 307, 767, 437, 527, 9461, 307, 1382, 281, 19874, 1339, 309, 307, 294, 264, 2823, 13, 407, 264, 51456], "temperature": 0.0, "avg_logprob": -0.051851094326126244, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.006487828213721514}, {"id": 5040, "seek": 2238468, "start": 22406.52, "end": 22412.6, "text": " goal of reinforcement learning is to have this agent navigate this environment, go through a", "tokens": [51456, 3387, 295, 29280, 2539, 307, 281, 362, 341, 9461, 12350, 341, 2823, 11, 352, 807, 257, 51760], "temperature": 0.0, "avg_logprob": -0.051851094326126244, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.006487828213721514}, {"id": 5041, "seek": 2241260, "start": 22412.6, "end": 22418.76, "text": " bunch of the different states of it and determine which actions maximize the reward at every given", "tokens": [50364, 3840, 295, 264, 819, 4368, 295, 309, 293, 6997, 597, 5909, 19874, 264, 7782, 412, 633, 2212, 50672], "temperature": 0.0, "avg_logprob": -0.054945427074766996, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.005384685937315226}, {"id": 5042, "seek": 2241260, "start": 22418.76, "end": 22425.32, "text": " state. So essentially, the goal of our agent is to maximize a reward. But what is a reward? Well,", "tokens": [50672, 1785, 13, 407, 4476, 11, 264, 3387, 295, 527, 9461, 307, 281, 19874, 257, 7782, 13, 583, 437, 307, 257, 7782, 30, 1042, 11, 51000], "temperature": 0.0, "avg_logprob": -0.054945427074766996, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.005384685937315226}, {"id": 5043, "seek": 2241260, "start": 22425.32, "end": 22431.32, "text": " after every action that's taken, the agent will receive a reward. Now this reward is something", "tokens": [51000, 934, 633, 3069, 300, 311, 2726, 11, 264, 9461, 486, 4774, 257, 7782, 13, 823, 341, 7782, 307, 746, 51300], "temperature": 0.0, "avg_logprob": -0.054945427074766996, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.005384685937315226}, {"id": 5044, "seek": 2241260, "start": 22431.32, "end": 22436.519999999997, "text": " that us as the programmer need to come up with. The reason we need to do this is because we need", "tokens": [51300, 300, 505, 382, 264, 32116, 643, 281, 808, 493, 365, 13, 440, 1778, 321, 643, 281, 360, 341, 307, 570, 321, 643, 51560], "temperature": 0.0, "avg_logprob": -0.054945427074766996, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.005384685937315226}, {"id": 5045, "seek": 2241260, "start": 22436.519999999997, "end": 22441.0, "text": " to tell the agent when it's performing well and when it's performing poorly. And just like we", "tokens": [51560, 281, 980, 264, 9461, 562, 309, 311, 10205, 731, 293, 562, 309, 311, 10205, 22271, 13, 400, 445, 411, 321, 51784], "temperature": 0.0, "avg_logprob": -0.054945427074766996, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.005384685937315226}, {"id": 5046, "seek": 2244100, "start": 22441.0, "end": 22446.2, "text": " had like a loss function in neural networks, when we're using those before, this is almost like", "tokens": [50364, 632, 411, 257, 4470, 2445, 294, 18161, 9590, 11, 562, 321, 434, 1228, 729, 949, 11, 341, 307, 1920, 411, 50624], "temperature": 0.0, "avg_logprob": -0.08095277150472005, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.007815279066562653}, {"id": 5047, "seek": 2244100, "start": 22446.2, "end": 22451.48, "text": " our loss function, you know, the higher this number is, the more reward the agent gets, the", "tokens": [50624, 527, 4470, 2445, 11, 291, 458, 11, 264, 2946, 341, 1230, 307, 11, 264, 544, 7782, 264, 9461, 2170, 11, 264, 50888], "temperature": 0.0, "avg_logprob": -0.08095277150472005, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.007815279066562653}, {"id": 5048, "seek": 2244100, "start": 22451.48, "end": 22457.24, "text": " better, the lower the reward, you know, it's not as good, it's not doing as well. So that's how we", "tokens": [50888, 1101, 11, 264, 3126, 264, 7782, 11, 291, 458, 11, 309, 311, 406, 382, 665, 11, 309, 311, 406, 884, 382, 731, 13, 407, 300, 311, 577, 321, 51176], "temperature": 0.0, "avg_logprob": -0.08095277150472005, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.007815279066562653}, {"id": 5049, "seek": 2244100, "start": 22457.24, "end": 22462.84, "text": " kind of monitor and assess performance for our agents is by determining the almost average amount", "tokens": [51176, 733, 295, 6002, 293, 5877, 3389, 337, 527, 12554, 307, 538, 23751, 264, 1920, 4274, 2372, 51456], "temperature": 0.0, "avg_logprob": -0.08095277150472005, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.007815279066562653}, {"id": 5050, "seek": 2244100, "start": 22462.84, "end": 22467.4, "text": " of reward that they're able to achieve. And their goal is really to, you know, it's almost an", "tokens": [51456, 295, 7782, 300, 436, 434, 1075, 281, 4584, 13, 400, 641, 3387, 307, 534, 281, 11, 291, 458, 11, 309, 311, 1920, 364, 51684], "temperature": 0.0, "avg_logprob": -0.08095277150472005, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.007815279066562653}, {"id": 5051, "seek": 2246740, "start": 22467.4, "end": 22472.120000000003, "text": " optimization problem where they're trying to maximize this reward. So what we're going to do", "tokens": [50364, 19618, 1154, 689, 436, 434, 1382, 281, 19874, 341, 7782, 13, 407, 437, 321, 434, 516, 281, 360, 50600], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5052, "seek": 2246740, "start": 22472.120000000003, "end": 22476.120000000003, "text": " in reinforcement learning is have this agent exploring the environment, going through these", "tokens": [50600, 294, 29280, 2539, 307, 362, 341, 9461, 12736, 264, 2823, 11, 516, 807, 613, 50800], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5053, "seek": 2246740, "start": 22476.120000000003, "end": 22480.760000000002, "text": " different states and performing these different actions, trying to maximize its reward. And", "tokens": [50800, 819, 4368, 293, 10205, 613, 819, 5909, 11, 1382, 281, 19874, 1080, 7782, 13, 400, 51032], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5054, "seek": 2246740, "start": 22480.760000000002, "end": 22485.08, "text": " obviously, if we're trying to get the agent to say finish a level or, you know, complete the game,", "tokens": [51032, 2745, 11, 498, 321, 434, 1382, 281, 483, 264, 9461, 281, 584, 2413, 257, 1496, 420, 11, 291, 458, 11, 3566, 264, 1216, 11, 51248], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5055, "seek": 2246740, "start": 22485.640000000003, "end": 22491.480000000003, "text": " then the maximum maximum reward will be achieved once it's completed the level or completed the", "tokens": [51276, 550, 264, 6674, 6674, 7782, 486, 312, 11042, 1564, 309, 311, 7365, 264, 1496, 420, 7365, 264, 51568], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5056, "seek": 2246740, "start": 22491.480000000003, "end": 22496.600000000002, "text": " game. And if it does things that we don't like, say like dying or like jumping in the wrong spot,", "tokens": [51568, 1216, 13, 400, 498, 309, 775, 721, 300, 321, 500, 380, 411, 11, 584, 411, 8639, 420, 411, 11233, 294, 264, 2085, 4008, 11, 51824], "temperature": 0.0, "avg_logprob": -0.06494983794197204, "compression_ratio": 1.9030100334448161, "no_speech_prob": 0.03963308781385422}, {"id": 5057, "seek": 2249660, "start": 22496.68, "end": 22501.079999999998, "text": " we could give it a negative reward to try to influence it to not do that. And our goal,", "tokens": [50368, 321, 727, 976, 309, 257, 3671, 7782, 281, 853, 281, 6503, 309, 281, 406, 360, 300, 13, 400, 527, 3387, 11, 50588], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5058, "seek": 2249660, "start": 22501.079999999998, "end": 22505.399999999998, "text": " you know, when we train these agents is for them to get the most reward. And we hope that", "tokens": [50588, 291, 458, 11, 562, 321, 3847, 613, 12554, 307, 337, 552, 281, 483, 264, 881, 7782, 13, 400, 321, 1454, 300, 50804], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5059, "seek": 2249660, "start": 22505.399999999998, "end": 22508.92, "text": " they're going to learn the optimal route through a level or through some environment that will", "tokens": [50804, 436, 434, 516, 281, 1466, 264, 16252, 7955, 807, 257, 1496, 420, 807, 512, 2823, 300, 486, 50980], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5060, "seek": 2249660, "start": 22508.92, "end": 22513.8, "text": " maximize that reward for them. Okay, so now I'm going to talk about a technique called Q learning,", "tokens": [50980, 19874, 300, 7782, 337, 552, 13, 1033, 11, 370, 586, 286, 478, 516, 281, 751, 466, 257, 6532, 1219, 1249, 2539, 11, 51224], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5061, "seek": 2249660, "start": 22513.8, "end": 22518.12, "text": " which is actually just an algorithm that we're going to use to implement this idea of reinforcement", "tokens": [51224, 597, 307, 767, 445, 364, 9284, 300, 321, 434, 516, 281, 764, 281, 4445, 341, 1558, 295, 29280, 51440], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5062, "seek": 2249660, "start": 22518.12, "end": 22522.519999999997, "text": " learning. We're not going to get into anything too crazy in this last module, because this is meant", "tokens": [51440, 2539, 13, 492, 434, 406, 516, 281, 483, 666, 1340, 886, 3219, 294, 341, 1036, 10088, 11, 570, 341, 307, 4140, 51660], "temperature": 0.0, "avg_logprob": -0.053381019563817265, "compression_ratio": 1.7732919254658386, "no_speech_prob": 0.004331307951360941}, {"id": 5063, "seek": 2252252, "start": 22522.52, "end": 22527.72, "text": " to be more of an introduction into the kind of field of reinforcement learning than anything else.", "tokens": [50364, 281, 312, 544, 295, 364, 9339, 666, 264, 733, 295, 2519, 295, 29280, 2539, 813, 1340, 1646, 13, 50624], "temperature": 0.0, "avg_logprob": -0.09540029005570845, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.20683196187019348}, {"id": 5064, "seek": 2252252, "start": 22527.72, "end": 22532.2, "text": " But Q learning is the most basic way to implement reinforcement learning, at least that I have", "tokens": [50624, 583, 1249, 2539, 307, 264, 881, 3875, 636, 281, 4445, 29280, 2539, 11, 412, 1935, 300, 286, 362, 50848], "temperature": 0.0, "avg_logprob": -0.09540029005570845, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.20683196187019348}, {"id": 5065, "seek": 2252252, "start": 22532.2, "end": 22537.8, "text": " discovered. And essentially, what Q learning is, and I don't actually really know why they call it", "tokens": [50848, 6941, 13, 400, 4476, 11, 437, 1249, 2539, 307, 11, 293, 286, 500, 380, 767, 534, 458, 983, 436, 818, 309, 51128], "temperature": 0.0, "avg_logprob": -0.09540029005570845, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.20683196187019348}, {"id": 5066, "seek": 2252252, "start": 22537.8, "end": 22543.88, "text": " Q, although I should probably know that is creating some kind of table or matrix likes", "tokens": [51128, 1249, 11, 4878, 286, 820, 1391, 458, 300, 307, 4084, 512, 733, 295, 3199, 420, 8141, 5902, 51432], "temperature": 0.0, "avg_logprob": -0.09540029005570845, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.20683196187019348}, {"id": 5067, "seek": 2252252, "start": 22543.88, "end": 22549.72, "text": " data structure, that's going to contain as the, what is it, I guess the rows, every single state,", "tokens": [51432, 1412, 3877, 11, 300, 311, 516, 281, 5304, 382, 264, 11, 437, 307, 309, 11, 286, 2041, 264, 13241, 11, 633, 2167, 1785, 11, 51724], "temperature": 0.0, "avg_logprob": -0.09540029005570845, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.20683196187019348}, {"id": 5068, "seek": 2254972, "start": 22549.72, "end": 22554.760000000002, "text": " and as the columns, every single action that could be taken in all of those different states.", "tokens": [50364, 293, 382, 264, 13766, 11, 633, 2167, 3069, 300, 727, 312, 2726, 294, 439, 295, 729, 819, 4368, 13, 50616], "temperature": 0.0, "avg_logprob": -0.07734952339759239, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.010013101622462273}, {"id": 5069, "seek": 2254972, "start": 22554.760000000002, "end": 22559.64, "text": " So for an example here, and we'll do one on kind of the whiteboard later on, if we can get there.", "tokens": [50616, 407, 337, 364, 1365, 510, 11, 293, 321, 603, 360, 472, 322, 733, 295, 264, 2418, 3787, 1780, 322, 11, 498, 321, 393, 483, 456, 13, 50860], "temperature": 0.0, "avg_logprob": -0.07734952339759239, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.010013101622462273}, {"id": 5070, "seek": 2254972, "start": 22560.68, "end": 22566.280000000002, "text": " But here, we can see that this is kind of my Q table. And what I'm saying is that we have a one,", "tokens": [50912, 583, 510, 11, 321, 393, 536, 300, 341, 307, 733, 295, 452, 1249, 3199, 13, 400, 437, 286, 478, 1566, 307, 300, 321, 362, 257, 472, 11, 51192], "temperature": 0.0, "avg_logprob": -0.07734952339759239, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.010013101622462273}, {"id": 5071, "seek": 2254972, "start": 22566.280000000002, "end": 22572.280000000002, "text": " a two, a three, a four, as all of the possible actions that could be performed in any given state.", "tokens": [51192, 257, 732, 11, 257, 1045, 11, 257, 1451, 11, 382, 439, 295, 264, 1944, 5909, 300, 727, 312, 10332, 294, 604, 2212, 1785, 13, 51492], "temperature": 0.0, "avg_logprob": -0.07734952339759239, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.010013101622462273}, {"id": 5072, "seek": 2254972, "start": 22572.280000000002, "end": 22577.72, "text": " And we have three states denoted by the fact that we have three rows. And the numbers in this,", "tokens": [51492, 400, 321, 362, 1045, 4368, 1441, 23325, 538, 264, 1186, 300, 321, 362, 1045, 13241, 13, 400, 264, 3547, 294, 341, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07734952339759239, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.010013101622462273}, {"id": 5073, "seek": 2257772, "start": 22578.440000000002, "end": 22583.32, "text": " this table with this Q, what do they call it, Q matrix Q table, whatever you want to call it,", "tokens": [50400, 341, 3199, 365, 341, 1249, 11, 437, 360, 436, 818, 309, 11, 1249, 8141, 1249, 3199, 11, 2035, 291, 528, 281, 818, 309, 11, 50644], "temperature": 0.0, "avg_logprob": -0.1091756820678711, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.004755053203552961}, {"id": 5074, "seek": 2257772, "start": 22583.32, "end": 22589.72, "text": " the numbers that are present here, represent what the predicted reward will be, given that we take", "tokens": [50644, 264, 3547, 300, 366, 1974, 510, 11, 2906, 437, 264, 19147, 7782, 486, 312, 11, 2212, 300, 321, 747, 50964], "temperature": 0.0, "avg_logprob": -0.1091756820678711, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.004755053203552961}, {"id": 5075, "seek": 2257772, "start": 22589.72, "end": 22595.72, "text": " an action, whatever this action is in this state. So I'm not sure if this is making sense to you", "tokens": [50964, 364, 3069, 11, 2035, 341, 3069, 307, 294, 341, 1785, 13, 407, 286, 478, 406, 988, 498, 341, 307, 1455, 2020, 281, 291, 51264], "temperature": 0.0, "avg_logprob": -0.1091756820678711, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.004755053203552961}, {"id": 5076, "seek": 2257772, "start": 22595.72, "end": 22601.800000000003, "text": " guys, but essentially, if we're saying that row zero is state zero, action two, a two, this value", "tokens": [51264, 1074, 11, 457, 4476, 11, 498, 321, 434, 1566, 300, 5386, 4018, 307, 1785, 4018, 11, 3069, 732, 11, 257, 732, 11, 341, 2158, 51568], "temperature": 0.0, "avg_logprob": -0.1091756820678711, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.004755053203552961}, {"id": 5077, "seek": 2260180, "start": 22601.8, "end": 22608.84, "text": " tells us what reward we should expect to get. If we take this action while we're in this state,", "tokens": [50364, 5112, 505, 437, 7782, 321, 820, 2066, 281, 483, 13, 759, 321, 747, 341, 3069, 1339, 321, 434, 294, 341, 1785, 11, 50716], "temperature": 0.0, "avg_logprob": -0.06508769591649373, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.2068655639886856}, {"id": 5078, "seek": 2260180, "start": 22608.84, "end": 22613.399999999998, "text": " that's what that is trying to tell us. That's what that means. Same thing here in, you know,", "tokens": [50716, 300, 311, 437, 300, 307, 1382, 281, 980, 505, 13, 663, 311, 437, 300, 1355, 13, 10635, 551, 510, 294, 11, 291, 458, 11, 50944], "temperature": 0.0, "avg_logprob": -0.06508769591649373, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.2068655639886856}, {"id": 5079, "seek": 2260180, "start": 22613.399999999998, "end": 22619.8, "text": " state two, we can see that the optimal action to take would be action two, because that has the", "tokens": [50944, 1785, 732, 11, 321, 393, 536, 300, 264, 16252, 3069, 281, 747, 576, 312, 3069, 732, 11, 570, 300, 575, 264, 51264], "temperature": 0.0, "avg_logprob": -0.06508769591649373, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.2068655639886856}, {"id": 5080, "seek": 2260180, "start": 22619.8, "end": 22625.0, "text": " highest reward for this state. And that's what this table is that we're going to try to generate", "tokens": [51264, 6343, 7782, 337, 341, 1785, 13, 400, 300, 311, 437, 341, 3199, 307, 300, 321, 434, 516, 281, 853, 281, 8460, 51524], "temperature": 0.0, "avg_logprob": -0.06508769591649373, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.2068655639886856}, {"id": 5081, "seek": 2260180, "start": 22625.0, "end": 22631.16, "text": " with this technique called Q learning, a table that can tell us given any state, what the predicted", "tokens": [51524, 365, 341, 6532, 1219, 1249, 2539, 11, 257, 3199, 300, 393, 980, 505, 2212, 604, 1785, 11, 437, 264, 19147, 51832], "temperature": 0.0, "avg_logprob": -0.06508769591649373, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.2068655639886856}, {"id": 5082, "seek": 2263116, "start": 22631.16, "end": 22635.88, "text": " reward will be for any action that we take. And we're going to generate this table by exploring", "tokens": [50364, 7782, 486, 312, 337, 604, 3069, 300, 321, 747, 13, 400, 321, 434, 516, 281, 8460, 341, 3199, 538, 12736, 50600], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5083, "seek": 2263116, "start": 22635.88, "end": 22641.4, "text": " the environment many different times, and updating these values according to what we kind of see", "tokens": [50600, 264, 2823, 867, 819, 1413, 11, 293, 25113, 613, 4190, 4650, 281, 437, 321, 733, 295, 536, 50876], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5084, "seek": 2263116, "start": 22641.4, "end": 22646.04, "text": " or what the agent sees in the environment and the rewards that it receives for any given action in", "tokens": [50876, 420, 437, 264, 9461, 8194, 294, 264, 2823, 293, 264, 17203, 300, 309, 20717, 337, 604, 2212, 3069, 294, 51108], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5085, "seek": 2263116, "start": 22646.04, "end": 22650.52, "text": " any given state. And we'll talk about how we're going to update that later. But this is the basic", "tokens": [51108, 604, 2212, 1785, 13, 400, 321, 603, 751, 466, 577, 321, 434, 516, 281, 5623, 300, 1780, 13, 583, 341, 307, 264, 3875, 51332], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5086, "seek": 2263116, "start": 22650.52, "end": 22655.48, "text": " premise. So that is kind of Q learning, we're going to hop on the whiteboard now, and we'll do a", "tokens": [51332, 22045, 13, 407, 300, 307, 733, 295, 1249, 2539, 11, 321, 434, 516, 281, 3818, 322, 264, 2418, 3787, 586, 11, 293, 321, 603, 360, 257, 51580], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5087, "seek": 2263116, "start": 22655.48, "end": 22659.88, "text": " more in depth example, but then we're going to talk about how we actually learned this Q table", "tokens": [51580, 544, 294, 7161, 1365, 11, 457, 550, 321, 434, 516, 281, 751, 466, 577, 321, 767, 3264, 341, 1249, 3199, 51800], "temperature": 0.0, "avg_logprob": -0.06910641073322982, "compression_ratio": 1.9694915254237289, "no_speech_prob": 0.0003353466163389385}, {"id": 5088, "seek": 2265988, "start": 22659.960000000003, "end": 22664.68, "text": " that I just discussed. Okay, so I've drawn a pretty basic example right now that I'm going to try", "tokens": [50368, 300, 286, 445, 7152, 13, 1033, 11, 370, 286, 600, 10117, 257, 1238, 3875, 1365, 558, 586, 300, 286, 478, 516, 281, 853, 50604], "temperature": 0.0, "avg_logprob": -0.08667031272512968, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.007576856296509504}, {"id": 5089, "seek": 2265988, "start": 22664.68, "end": 22670.2, "text": " to use to illustrate the idea of Q learning and talk about some problems with it and how we can", "tokens": [50604, 281, 764, 281, 23221, 264, 1558, 295, 1249, 2539, 293, 751, 466, 512, 2740, 365, 309, 293, 577, 321, 393, 50880], "temperature": 0.0, "avg_logprob": -0.08667031272512968, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.007576856296509504}, {"id": 5090, "seek": 2265988, "start": 22670.2, "end": 22674.920000000002, "text": " kind of combat those as we learn more about how Q learning works. But the idea here is that we", "tokens": [50880, 733, 295, 8361, 729, 382, 321, 1466, 544, 466, 577, 1249, 2539, 1985, 13, 583, 264, 1558, 510, 307, 300, 321, 51116], "temperature": 0.0, "avg_logprob": -0.08667031272512968, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.007576856296509504}, {"id": 5091, "seek": 2265988, "start": 22674.920000000002, "end": 22679.08, "text": " currently have three states and why, what is happening? Why was that happening up at the top?", "tokens": [51116, 4362, 362, 1045, 4368, 293, 983, 11, 437, 307, 2737, 30, 1545, 390, 300, 2737, 493, 412, 264, 1192, 30, 51324], "temperature": 0.0, "avg_logprob": -0.08667031272512968, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.007576856296509504}, {"id": 5092, "seek": 2265988, "start": 22679.08, "end": 22684.920000000002, "text": " I don't know. Anyways, the idea is we have three states as one s two and s three. And at each state", "tokens": [51324, 286, 500, 380, 458, 13, 15585, 11, 264, 1558, 307, 321, 362, 1045, 4368, 382, 472, 262, 732, 293, 262, 1045, 13, 400, 412, 1184, 1785, 51616], "temperature": 0.0, "avg_logprob": -0.08667031272512968, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.007576856296509504}, {"id": 5093, "seek": 2268492, "start": 22684.92, "end": 22690.28, "text": " we have two possible actions that can be taken, we can either stay in this state or we can move.", "tokens": [50364, 321, 362, 732, 1944, 5909, 300, 393, 312, 2726, 11, 321, 393, 2139, 1754, 294, 341, 1785, 420, 321, 393, 1286, 13, 50632], "temperature": 0.0, "avg_logprob": -0.06274818181991577, "compression_ratio": 1.86328125, "no_speech_prob": 0.14411932229995728}, {"id": 5094, "seek": 2268492, "start": 22690.839999999997, "end": 22695.96, "text": " Now, what I've done is kind of just written some integers here that represent the reward that we're", "tokens": [50660, 823, 11, 437, 286, 600, 1096, 307, 733, 295, 445, 3720, 512, 41674, 510, 300, 2906, 264, 7782, 300, 321, 434, 50916], "temperature": 0.0, "avg_logprob": -0.06274818181991577, "compression_ratio": 1.86328125, "no_speech_prob": 0.14411932229995728}, {"id": 5095, "seek": 2268492, "start": 22695.96, "end": 22701.64, "text": " going to get or that the agent is going to get such that it takes that action in a given state.", "tokens": [50916, 516, 281, 483, 420, 300, 264, 9461, 307, 516, 281, 483, 1270, 300, 309, 2516, 300, 3069, 294, 257, 2212, 1785, 13, 51200], "temperature": 0.0, "avg_logprob": -0.06274818181991577, "compression_ratio": 1.86328125, "no_speech_prob": 0.14411932229995728}, {"id": 5096, "seek": 2268492, "start": 22701.64, "end": 22709.079999999998, "text": " So if we take the action here in s one, right of moving, then we will receive a reward of one", "tokens": [51200, 407, 498, 321, 747, 264, 3069, 510, 294, 262, 472, 11, 558, 295, 2684, 11, 550, 321, 486, 4774, 257, 7782, 295, 472, 51572], "temperature": 0.0, "avg_logprob": -0.06274818181991577, "compression_ratio": 1.86328125, "no_speech_prob": 0.14411932229995728}, {"id": 5097, "seek": 2268492, "start": 22709.079999999998, "end": 22712.839999999997, "text": " because that's what we've written here is the reward that we get for moving. Whereas if we", "tokens": [51572, 570, 300, 311, 437, 321, 600, 3720, 510, 307, 264, 7782, 300, 321, 483, 337, 2684, 13, 13813, 498, 321, 51760], "temperature": 0.0, "avg_logprob": -0.06274818181991577, "compression_ratio": 1.86328125, "no_speech_prob": 0.14411932229995728}, {"id": 5098, "seek": 2271284, "start": 22712.920000000002, "end": 22717.48, "text": " stay, we'll get a reward of three, you know, same concept here, if we stay, we get two,", "tokens": [50368, 1754, 11, 321, 603, 483, 257, 7782, 295, 1045, 11, 291, 458, 11, 912, 3410, 510, 11, 498, 321, 1754, 11, 321, 483, 732, 11, 50596], "temperature": 0.0, "avg_logprob": -0.07102646786942442, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.004331238102167845}, {"id": 5099, "seek": 2271284, "start": 22717.48, "end": 22722.84, "text": " if we move, we get one, and I think you understand the point. So the goal of our agent to remember", "tokens": [50596, 498, 321, 1286, 11, 321, 483, 472, 11, 293, 286, 519, 291, 1223, 264, 935, 13, 407, 264, 3387, 295, 527, 9461, 281, 1604, 50864], "temperature": 0.0, "avg_logprob": -0.07102646786942442, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.004331238102167845}, {"id": 5100, "seek": 2271284, "start": 22722.84, "end": 22727.4, "text": " is to maximize its reward in the environment. And what we're going to call the environment", "tokens": [50864, 307, 281, 19874, 1080, 7782, 294, 264, 2823, 13, 400, 437, 321, 434, 516, 281, 818, 264, 2823, 51092], "temperature": 0.0, "avg_logprob": -0.07102646786942442, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.004331238102167845}, {"id": 5101, "seek": 2271284, "start": 22727.4, "end": 22733.08, "text": " is this right here, the environment is essentially defines the number of states, the number of", "tokens": [51092, 307, 341, 558, 510, 11, 264, 2823, 307, 4476, 23122, 264, 1230, 295, 4368, 11, 264, 1230, 295, 51376], "temperature": 0.0, "avg_logprob": -0.07102646786942442, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.004331238102167845}, {"id": 5102, "seek": 2271284, "start": 22733.08, "end": 22738.920000000002, "text": " actions, and you know, the way that the agent can interact with these states and these actions.", "tokens": [51376, 5909, 11, 293, 291, 458, 11, 264, 636, 300, 264, 9461, 393, 4648, 365, 613, 4368, 293, 613, 5909, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07102646786942442, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.004331238102167845}, {"id": 5103, "seek": 2273892, "start": 22738.92, "end": 22743.32, "text": " So in this case, the agent can interact with the states by taking actions that change its state,", "tokens": [50364, 407, 294, 341, 1389, 11, 264, 9461, 393, 4648, 365, 264, 4368, 538, 1940, 5909, 300, 1319, 1080, 1785, 11, 50584], "temperature": 0.0, "avg_logprob": -0.0782039579281137, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.001700685708783567}, {"id": 5104, "seek": 2273892, "start": 22743.32, "end": 22747.48, "text": " right? So that's where we're getting out with this. Now, what I want to do is show you how we", "tokens": [50584, 558, 30, 407, 300, 311, 689, 321, 434, 1242, 484, 365, 341, 13, 823, 11, 437, 286, 528, 281, 360, 307, 855, 291, 577, 321, 50792], "temperature": 0.0, "avg_logprob": -0.0782039579281137, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.001700685708783567}, {"id": 5105, "seek": 2273892, "start": 22747.48, "end": 22754.679999999997, "text": " use this queue table, or learn this queue table to come up with kind of the almost, you know,", "tokens": [50792, 764, 341, 18639, 3199, 11, 420, 1466, 341, 18639, 3199, 281, 808, 493, 365, 733, 295, 264, 1920, 11, 291, 458, 11, 51152], "temperature": 0.0, "avg_logprob": -0.0782039579281137, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.001700685708783567}, {"id": 5106, "seek": 2273892, "start": 22754.679999999997, "end": 22759.079999999998, "text": " the model, like the machine learning model that we're going to use. So essentially, what we would", "tokens": [51152, 264, 2316, 11, 411, 264, 3479, 2539, 2316, 300, 321, 434, 516, 281, 764, 13, 407, 4476, 11, 437, 321, 576, 51372], "temperature": 0.0, "avg_logprob": -0.0782039579281137, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.001700685708783567}, {"id": 5107, "seek": 2273892, "start": 22759.079999999998, "end": 22767.399999999998, "text": " want to have here is we want to have a kind of pattern in this table that allows our agent to", "tokens": [51372, 528, 281, 362, 510, 307, 321, 528, 281, 362, 257, 733, 295, 5102, 294, 341, 3199, 300, 4045, 527, 9461, 281, 51788], "temperature": 0.0, "avg_logprob": -0.0782039579281137, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.001700685708783567}, {"id": 5108, "seek": 2276740, "start": 22767.480000000003, "end": 22772.440000000002, "text": " receive the maximum reward. So in this case, we're going to say that our agent will start at", "tokens": [50368, 4774, 264, 6674, 7782, 13, 407, 294, 341, 1389, 11, 321, 434, 516, 281, 584, 300, 527, 9461, 486, 722, 412, 50616], "temperature": 0.0, "avg_logprob": -0.07173856099446614, "compression_ratio": 1.936, "no_speech_prob": 0.11592384427785873}, {"id": 5109, "seek": 2276740, "start": 22772.440000000002, "end": 22776.760000000002, "text": " state s one. And obviously, whenever we're doing this reinforcement learning, we need to have some", "tokens": [50616, 1785, 262, 472, 13, 400, 2745, 11, 5699, 321, 434, 884, 341, 29280, 2539, 11, 321, 643, 281, 362, 512, 50832], "temperature": 0.0, "avg_logprob": -0.07173856099446614, "compression_ratio": 1.936, "no_speech_prob": 0.11592384427785873}, {"id": 5110, "seek": 2276740, "start": 22776.760000000002, "end": 22781.24, "text": " kind of start state that the agent will start in this could be a random state, it could change,", "tokens": [50832, 733, 295, 722, 1785, 300, 264, 9461, 486, 722, 294, 341, 727, 312, 257, 4974, 1785, 11, 309, 727, 1319, 11, 51056], "temperature": 0.0, "avg_logprob": -0.07173856099446614, "compression_ratio": 1.936, "no_speech_prob": 0.11592384427785873}, {"id": 5111, "seek": 2276740, "start": 22781.24, "end": 22785.08, "text": " but it doesn't just start in some state. So in this case, we're going to say it starts at s one.", "tokens": [51056, 457, 309, 1177, 380, 445, 722, 294, 512, 1785, 13, 407, 294, 341, 1389, 11, 321, 434, 516, 281, 584, 309, 3719, 412, 262, 472, 13, 51248], "temperature": 0.0, "avg_logprob": -0.07173856099446614, "compression_ratio": 1.936, "no_speech_prob": 0.11592384427785873}, {"id": 5112, "seek": 2276740, "start": 22785.640000000003, "end": 22791.32, "text": " Now, when we're in s one, the agent has two things that it can do. It can stay in the current state", "tokens": [51276, 823, 11, 562, 321, 434, 294, 262, 472, 11, 264, 9461, 575, 732, 721, 300, 309, 393, 360, 13, 467, 393, 1754, 294, 264, 2190, 1785, 51560], "temperature": 0.0, "avg_logprob": -0.07173856099446614, "compression_ratio": 1.936, "no_speech_prob": 0.11592384427785873}, {"id": 5113, "seek": 2279132, "start": 22791.32, "end": 22797.56, "text": " and receive a reward of three, or it can move and receive a reward of one, right? If we get to s", "tokens": [50364, 293, 4774, 257, 7782, 295, 1045, 11, 420, 309, 393, 1286, 293, 4774, 257, 7782, 295, 472, 11, 558, 30, 759, 321, 483, 281, 262, 50676], "temperature": 0.0, "avg_logprob": -0.06498560976626268, "compression_ratio": 2.1627906976744184, "no_speech_prob": 0.08755102753639221}, {"id": 5114, "seek": 2279132, "start": 22797.56, "end": 22802.68, "text": " two, in this state, what can we do? We can stay, which means we receive a reward of two, or we", "tokens": [50676, 732, 11, 294, 341, 1785, 11, 437, 393, 321, 360, 30, 492, 393, 1754, 11, 597, 1355, 321, 4774, 257, 7782, 295, 732, 11, 420, 321, 50932], "temperature": 0.0, "avg_logprob": -0.06498560976626268, "compression_ratio": 2.1627906976744184, "no_speech_prob": 0.08755102753639221}, {"id": 5115, "seek": 2279132, "start": 22802.68, "end": 22807.32, "text": " can move, which means we get a reward of one. And same thing for s three, we can stay, we get a", "tokens": [50932, 393, 1286, 11, 597, 1355, 321, 483, 257, 7782, 295, 472, 13, 400, 912, 551, 337, 262, 1045, 11, 321, 393, 1754, 11, 321, 483, 257, 51164], "temperature": 0.0, "avg_logprob": -0.06498560976626268, "compression_ratio": 2.1627906976744184, "no_speech_prob": 0.08755102753639221}, {"id": 5116, "seek": 2279132, "start": 22807.32, "end": 22813.8, "text": " reward of four, and we can move, we get a reward of one. Now, right now, if we had just ran this", "tokens": [51164, 7782, 295, 1451, 11, 293, 321, 393, 1286, 11, 321, 483, 257, 7782, 295, 472, 13, 823, 11, 558, 586, 11, 498, 321, 632, 445, 5872, 341, 51488], "temperature": 0.0, "avg_logprob": -0.06498560976626268, "compression_ratio": 2.1627906976744184, "no_speech_prob": 0.08755102753639221}, {"id": 5117, "seek": 2279132, "start": 22814.36, "end": 22819.4, "text": " one time and have the agent stay in each state, like start in each unique state,", "tokens": [51516, 472, 565, 293, 362, 264, 9461, 1754, 294, 1184, 1785, 11, 411, 722, 294, 1184, 3845, 1785, 11, 51768], "temperature": 0.0, "avg_logprob": -0.06498560976626268, "compression_ratio": 2.1627906976744184, "no_speech_prob": 0.08755102753639221}, {"id": 5118, "seek": 2281940, "start": 22819.4, "end": 22824.2, "text": " this is what the queue table we would get would look like. Because after looking at this, just", "tokens": [50364, 341, 307, 437, 264, 18639, 3199, 321, 576, 483, 576, 574, 411, 13, 1436, 934, 1237, 412, 341, 11, 445, 50604], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5119, "seek": 2281940, "start": 22824.2, "end": 22829.16, "text": " one time starting in each state, what the agent would be able to, or I guess, two times, because", "tokens": [50604, 472, 565, 2891, 294, 1184, 1785, 11, 437, 264, 9461, 576, 312, 1075, 281, 11, 420, 286, 2041, 11, 732, 1413, 11, 570, 50852], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5120, "seek": 2281940, "start": 22829.16, "end": 22834.68, "text": " it would have to try each action. Let's say we had the agent start in each state twice. So it started", "tokens": [50852, 309, 576, 362, 281, 853, 1184, 3069, 13, 961, 311, 584, 321, 632, 264, 9461, 722, 294, 1184, 1785, 6091, 13, 407, 309, 1409, 51128], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5121, "seek": 2281940, "start": 22834.68, "end": 22839.08, "text": " an s one twice, it started s two twice, and it started an s three twice. And every time it started", "tokens": [51128, 364, 262, 472, 6091, 11, 309, 1409, 262, 732, 6091, 11, 293, 309, 1409, 364, 262, 1045, 6091, 13, 400, 633, 565, 309, 1409, 51348], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5122, "seek": 2281940, "start": 22839.08, "end": 22843.480000000003, "text": " there, it tried one of the different actions. So when it started in s one, it tried moving once,", "tokens": [51348, 456, 11, 309, 3031, 472, 295, 264, 819, 5909, 13, 407, 562, 309, 1409, 294, 262, 472, 11, 309, 3031, 2684, 1564, 11, 51568], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5123, "seek": 2281940, "start": 22843.480000000003, "end": 22847.4, "text": " and then it tried staying once, we would have a queue table that looks like this. Because what", "tokens": [51568, 293, 550, 309, 3031, 7939, 1564, 11, 321, 576, 362, 257, 18639, 3199, 300, 1542, 411, 341, 13, 1436, 437, 51764], "temperature": 0.0, "avg_logprob": -0.11438690649496543, "compression_ratio": 2.108303249097473, "no_speech_prob": 0.0032729031518101692}, {"id": 5124, "seek": 2284740, "start": 22847.480000000003, "end": 22852.84, "text": " would happen is we would update values in our queue table to represent the reward we received", "tokens": [50368, 576, 1051, 307, 321, 576, 5623, 4190, 294, 527, 18639, 3199, 281, 2906, 264, 7782, 321, 4613, 50636], "temperature": 0.0, "avg_logprob": -0.09002494812011719, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.004468251019716263}, {"id": 5125, "seek": 2284740, "start": 22852.84, "end": 22858.36, "text": " when we took that action from that state. So we can see here that when we're in state s one,", "tokens": [50636, 562, 321, 1890, 300, 3069, 490, 300, 1785, 13, 407, 321, 393, 536, 510, 300, 562, 321, 434, 294, 1785, 262, 472, 11, 50912], "temperature": 0.0, "avg_logprob": -0.09002494812011719, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.004468251019716263}, {"id": 5126, "seek": 2284740, "start": 22858.36, "end": 22864.52, "text": " and we decide to stay, what we did is we wrote a three inside of the stay column, because that is", "tokens": [50912, 293, 321, 4536, 281, 1754, 11, 437, 321, 630, 307, 321, 4114, 257, 1045, 1854, 295, 264, 1754, 7738, 11, 570, 300, 307, 51220], "temperature": 0.0, "avg_logprob": -0.09002494812011719, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.004468251019716263}, {"id": 5127, "seek": 2284740, "start": 22864.52, "end": 22870.84, "text": " how much reward we received when we moved, right? Same thing for state two, when we moved for state", "tokens": [51220, 577, 709, 7782, 321, 4613, 562, 321, 4259, 11, 558, 30, 10635, 551, 337, 1785, 732, 11, 562, 321, 4259, 337, 1785, 51536], "temperature": 0.0, "avg_logprob": -0.09002494812011719, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.004468251019716263}, {"id": 5128, "seek": 2284740, "start": 22870.84, "end": 22876.52, "text": " two or have I guess, sorry, stayed when we stayed in state two, we received a reward of two, same", "tokens": [51536, 732, 420, 362, 286, 2041, 11, 2597, 11, 9181, 562, 321, 9181, 294, 1785, 732, 11, 321, 4613, 257, 7782, 295, 732, 11, 912, 51820], "temperature": 0.0, "avg_logprob": -0.09002494812011719, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.004468251019716263}, {"id": 5129, "seek": 2287652, "start": 22876.52, "end": 22883.08, "text": " thing for four. Now, this is okay, right? This tells us kind of, you know, the optimal move to", "tokens": [50364, 551, 337, 1451, 13, 823, 11, 341, 307, 1392, 11, 558, 30, 639, 5112, 505, 733, 295, 11, 291, 458, 11, 264, 16252, 1286, 281, 50692], "temperature": 0.0, "avg_logprob": -0.05853879640972803, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0052198744378983974}, {"id": 5130, "seek": 2287652, "start": 22883.08, "end": 22888.12, "text": " make in any state to receive the maximum reward. But what if we introduce the idea that, you know,", "tokens": [50692, 652, 294, 604, 1785, 281, 4774, 264, 6674, 7782, 13, 583, 437, 498, 321, 5366, 264, 1558, 300, 11, 291, 458, 11, 50944], "temperature": 0.0, "avg_logprob": -0.05853879640972803, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0052198744378983974}, {"id": 5131, "seek": 2287652, "start": 22888.12, "end": 22895.16, "text": " our agent, we want it to receive the maximum total reward possible, right? So if it's in state one,", "tokens": [50944, 527, 9461, 11, 321, 528, 309, 281, 4774, 264, 6674, 3217, 7782, 1944, 11, 558, 30, 407, 498, 309, 311, 294, 1785, 472, 11, 51296], "temperature": 0.0, "avg_logprob": -0.05853879640972803, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0052198744378983974}, {"id": 5132, "seek": 2287652, "start": 22895.16, "end": 22899.8, "text": " ideally, we'd like it to move to state two, and then move to states three, and then just stay in", "tokens": [51296, 22915, 11, 321, 1116, 411, 309, 281, 1286, 281, 1785, 732, 11, 293, 550, 1286, 281, 4368, 1045, 11, 293, 550, 445, 1754, 294, 51528], "temperature": 0.0, "avg_logprob": -0.05853879640972803, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0052198744378983974}, {"id": 5133, "seek": 2287652, "start": 22899.8, "end": 22903.8, "text": " state three, because it will receive the most amount of reward. Well, with the current table", "tokens": [51528, 1785, 1045, 11, 570, 309, 486, 4774, 264, 881, 2372, 295, 7782, 13, 1042, 11, 365, 264, 2190, 3199, 51728], "temperature": 0.0, "avg_logprob": -0.05853879640972803, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0052198744378983974}, {"id": 5134, "seek": 2290380, "start": 22903.8, "end": 22908.2, "text": " that we've developed, if we just follow this, and we look at the table, we say, okay, if we want to", "tokens": [50364, 300, 321, 600, 4743, 11, 498, 321, 445, 1524, 341, 11, 293, 321, 574, 412, 264, 3199, 11, 321, 584, 11, 1392, 11, 498, 321, 528, 281, 50584], "temperature": 0.0, "avg_logprob": -0.0811501578461352, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.051842235028743744}, {"id": 5135, "seek": 2290380, "start": 22908.2, "end": 22912.92, "text": " use this Q learning table now to, you know, move an agent around our level, what we'll do is we'll", "tokens": [50584, 764, 341, 1249, 2539, 3199, 586, 281, 11, 291, 458, 11, 1286, 364, 9461, 926, 527, 1496, 11, 437, 321, 603, 360, 307, 321, 603, 50820], "temperature": 0.0, "avg_logprob": -0.0811501578461352, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.051842235028743744}, {"id": 5136, "seek": 2290380, "start": 22912.92, "end": 22918.6, "text": " say, okay, what state is it in? If it's in state two, we'll do stay because that's the highest reward", "tokens": [50820, 584, 11, 1392, 11, 437, 1785, 307, 309, 294, 30, 759, 309, 311, 294, 1785, 732, 11, 321, 603, 360, 1754, 570, 300, 311, 264, 6343, 7782, 51104], "temperature": 0.0, "avg_logprob": -0.0811501578461352, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.051842235028743744}, {"id": 5137, "seek": 2290380, "start": 22918.6, "end": 22924.12, "text": " that we have in this table. If that's the approach we use, then we could see that if our, you know,", "tokens": [51104, 300, 321, 362, 294, 341, 3199, 13, 759, 300, 311, 264, 3109, 321, 764, 11, 550, 321, 727, 536, 300, 498, 527, 11, 291, 458, 11, 51380], "temperature": 0.0, "avg_logprob": -0.0811501578461352, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.051842235028743744}, {"id": 5138, "seek": 2290380, "start": 22924.12, "end": 22929.64, "text": " agent start in state one or state two, it would stay in what we call a local minima, because", "tokens": [51380, 9461, 722, 294, 1785, 472, 420, 1785, 732, 11, 309, 576, 1754, 294, 437, 321, 818, 257, 2654, 4464, 64, 11, 570, 51656], "temperature": 0.0, "avg_logprob": -0.0811501578461352, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.051842235028743744}, {"id": 5139, "seek": 2292964, "start": 22929.64, "end": 22935.32, "text": " it's not able to kind of realize from this state that it can move any further and receive a much", "tokens": [50364, 309, 311, 406, 1075, 281, 733, 295, 4325, 490, 341, 1785, 300, 309, 393, 1286, 604, 3052, 293, 4774, 257, 709, 50648], "temperature": 0.0, "avg_logprob": -0.05479796036430027, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.031140625476837158}, {"id": 5140, "seek": 2292964, "start": 22935.32, "end": 22939.96, "text": " greater reward, right? And that's kind of the concept we're going to talk about as we implement and,", "tokens": [50648, 5044, 7782, 11, 558, 30, 400, 300, 311, 733, 295, 264, 3410, 321, 434, 516, 281, 751, 466, 382, 321, 4445, 293, 11, 50880], "temperature": 0.0, "avg_logprob": -0.05479796036430027, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.031140625476837158}, {"id": 5141, "seek": 2292964, "start": 22939.96, "end": 22945.399999999998, "text": " you know, discuss further how Q learning works. But hopefully this gives you a little bit of insight", "tokens": [50880, 291, 458, 11, 2248, 3052, 577, 1249, 2539, 1985, 13, 583, 4696, 341, 2709, 291, 257, 707, 857, 295, 11269, 51152], "temperature": 0.0, "avg_logprob": -0.05479796036430027, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.031140625476837158}, {"id": 5142, "seek": 2292964, "start": 22945.399999999998, "end": 22951.559999999998, "text": " into what we do with this table. Essentially, when we're updating these table values is when", "tokens": [51152, 666, 437, 321, 360, 365, 341, 3199, 13, 23596, 11, 562, 321, 434, 25113, 613, 3199, 4190, 307, 562, 51460], "temperature": 0.0, "avg_logprob": -0.05479796036430027, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.031140625476837158}, {"id": 5143, "seek": 2292964, "start": 22951.559999999998, "end": 22956.68, "text": " we're exploring this environment. So when we explore this environment, and we start in a state,", "tokens": [51460, 321, 434, 12736, 341, 2823, 13, 407, 562, 321, 6839, 341, 2823, 11, 293, 321, 722, 294, 257, 1785, 11, 51716], "temperature": 0.0, "avg_logprob": -0.05479796036430027, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.031140625476837158}, {"id": 5144, "seek": 2295668, "start": 22956.68, "end": 22961.72, "text": " when we take an action to another state, we observe the reward that we got from going there,", "tokens": [50364, 562, 321, 747, 364, 3069, 281, 1071, 1785, 11, 321, 11441, 264, 7782, 300, 321, 658, 490, 516, 456, 11, 50616], "temperature": 0.0, "avg_logprob": -0.048514432054225023, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0012842746218666434}, {"id": 5145, "seek": 2295668, "start": 22961.72, "end": 22966.2, "text": " and we observe the state that we change to, right? So we observe the fact that in state one,", "tokens": [50616, 293, 321, 11441, 264, 1785, 300, 321, 1319, 281, 11, 558, 30, 407, 321, 11441, 264, 1186, 300, 294, 1785, 472, 11, 50840], "temperature": 0.0, "avg_logprob": -0.048514432054225023, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0012842746218666434}, {"id": 5146, "seek": 2295668, "start": 22966.2, "end": 22971.48, "text": " when we go to state two, we receive the reward of one. And what we do is we take that observation", "tokens": [50840, 562, 321, 352, 281, 1785, 732, 11, 321, 4774, 264, 7782, 295, 472, 13, 400, 437, 321, 360, 307, 321, 747, 300, 14816, 51104], "temperature": 0.0, "avg_logprob": -0.048514432054225023, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0012842746218666434}, {"id": 5147, "seek": 2295668, "start": 22971.48, "end": 22977.64, "text": " and we use it to update this Q table. And the goal is that at the end of all of these observations,", "tokens": [51104, 293, 321, 764, 309, 281, 5623, 341, 1249, 3199, 13, 400, 264, 3387, 307, 300, 412, 264, 917, 295, 439, 295, 613, 18163, 11, 51412], "temperature": 0.0, "avg_logprob": -0.048514432054225023, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0012842746218666434}, {"id": 5148, "seek": 2295668, "start": 22977.64, "end": 22983.8, "text": " and there could be millions of them, that we have a Q table that tells us the optimal action to take", "tokens": [51412, 293, 456, 727, 312, 6803, 295, 552, 11, 300, 321, 362, 257, 1249, 3199, 300, 5112, 505, 264, 16252, 3069, 281, 747, 51720], "temperature": 0.0, "avg_logprob": -0.048514432054225023, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0012842746218666434}, {"id": 5149, "seek": 2298380, "start": 22983.88, "end": 22989.88, "text": " in any single state. So we're actually hard coding, this kind of mapping that essentially", "tokens": [50368, 294, 604, 2167, 1785, 13, 407, 321, 434, 767, 1152, 17720, 11, 341, 733, 295, 18350, 300, 4476, 50668], "temperature": 0.0, "avg_logprob": -0.06181095027122177, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.008061658591032028}, {"id": 5150, "seek": 2298380, "start": 22989.88, "end": 22994.76, "text": " just tells us given any state, all you have to do is look up in this table, look at all of the", "tokens": [50668, 445, 5112, 505, 2212, 604, 1785, 11, 439, 291, 362, 281, 360, 307, 574, 493, 294, 341, 3199, 11, 574, 412, 439, 295, 264, 50912], "temperature": 0.0, "avg_logprob": -0.06181095027122177, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.008061658591032028}, {"id": 5151, "seek": 2298380, "start": 22994.76, "end": 22999.719999999998, "text": " actions that could be taken, and just take the maximum action or the reward that's supposed to", "tokens": [50912, 5909, 300, 727, 312, 2726, 11, 293, 445, 747, 264, 6674, 3069, 420, 264, 7782, 300, 311, 3442, 281, 51160], "temperature": 0.0, "avg_logprob": -0.06181095027122177, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.008061658591032028}, {"id": 5152, "seek": 2298380, "start": 22999.719999999998, "end": 23004.44, "text": " give, I guess, the action that's supposed to give the maximum reward. And if we were to follow", "tokens": [51160, 976, 11, 286, 2041, 11, 264, 3069, 300, 311, 3442, 281, 976, 264, 6674, 7782, 13, 400, 498, 321, 645, 281, 1524, 51396], "temperature": 0.0, "avg_logprob": -0.06181095027122177, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.008061658591032028}, {"id": 5153, "seek": 2298380, "start": 23004.44, "end": 23008.12, "text": " that on this, we could see we get stuck in the local minima, which is why we're going to introduce", "tokens": [51396, 300, 322, 341, 11, 321, 727, 536, 321, 483, 5541, 294, 264, 2654, 4464, 64, 11, 597, 307, 983, 321, 434, 516, 281, 5366, 51580], "temperature": 0.0, "avg_logprob": -0.06181095027122177, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.008061658591032028}, {"id": 5154, "seek": 2300812, "start": 23008.12, "end": 23015.0, "text": " a lot of other concepts. So our reinforcement learning model in Q learning, we have to implement", "tokens": [50364, 257, 688, 295, 661, 10392, 13, 407, 527, 29280, 2539, 2316, 294, 1249, 2539, 11, 321, 362, 281, 4445, 50708], "temperature": 0.0, "avg_logprob": -0.0831578830340961, "compression_ratio": 1.8226415094339623, "no_speech_prob": 0.15200041234493256}, {"id": 5155, "seek": 2300812, "start": 23015.0, "end": 23021.32, "text": " the concept of being able to explore the environment, not based on previous experiences,", "tokens": [50708, 264, 3410, 295, 885, 1075, 281, 6839, 264, 2823, 11, 406, 2361, 322, 3894, 5235, 11, 51024], "temperature": 0.0, "avg_logprob": -0.0831578830340961, "compression_ratio": 1.8226415094339623, "no_speech_prob": 0.15200041234493256}, {"id": 5156, "seek": 2300812, "start": 23021.32, "end": 23026.12, "text": " right? Because if we just tell our model, okay, what we're going to do is we're going to start in", "tokens": [51024, 558, 30, 1436, 498, 321, 445, 980, 527, 2316, 11, 1392, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 722, 294, 51264], "temperature": 0.0, "avg_logprob": -0.0831578830340961, "compression_ratio": 1.8226415094339623, "no_speech_prob": 0.15200041234493256}, {"id": 5157, "seek": 2300812, "start": 23026.12, "end": 23030.12, "text": " all these different states, we're going to start in the start state and just start navigating around.", "tokens": [51264, 439, 613, 819, 4368, 11, 321, 434, 516, 281, 722, 294, 264, 722, 1785, 293, 445, 722, 32054, 926, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0831578830340961, "compression_ratio": 1.8226415094339623, "no_speech_prob": 0.15200041234493256}, {"id": 5158, "seek": 2300812, "start": 23030.12, "end": 23035.0, "text": " If we update our model immediately, or update our Q table immediately and put this three here for", "tokens": [51464, 759, 321, 5623, 527, 2316, 4258, 11, 420, 5623, 527, 1249, 3199, 4258, 293, 829, 341, 1045, 510, 337, 51708], "temperature": 0.0, "avg_logprob": -0.0831578830340961, "compression_ratio": 1.8226415094339623, "no_speech_prob": 0.15200041234493256}, {"id": 5159, "seek": 2303500, "start": 23035.08, "end": 23041.24, "text": " stay, we can almost guarantee that since this three is here, when our model is training, right,", "tokens": [50368, 1754, 11, 321, 393, 1920, 10815, 300, 1670, 341, 1045, 307, 510, 11, 562, 527, 2316, 307, 3097, 11, 558, 11, 50676], "temperature": 0.0, "avg_logprob": -0.06657388568979448, "compression_ratio": 1.72, "no_speech_prob": 0.0069036022759974}, {"id": 5160, "seek": 2303500, "start": 23041.24, "end": 23045.48, "text": " if it's using this Q table to determine what state to move to next, when it's training and", "tokens": [50676, 498, 309, 311, 1228, 341, 1249, 3199, 281, 6997, 437, 1785, 281, 1286, 281, 958, 11, 562, 309, 311, 3097, 293, 50888], "temperature": 0.0, "avg_logprob": -0.06657388568979448, "compression_ratio": 1.72, "no_speech_prob": 0.0069036022759974}, {"id": 5161, "seek": 2303500, "start": 23045.48, "end": 23049.48, "text": " determining what to do, it's just always going to stay, which means we'll never get a chance to", "tokens": [50888, 23751, 437, 281, 360, 11, 309, 311, 445, 1009, 516, 281, 1754, 11, 597, 1355, 321, 603, 1128, 483, 257, 2931, 281, 51088], "temperature": 0.0, "avg_logprob": -0.06657388568979448, "compression_ratio": 1.72, "no_speech_prob": 0.0069036022759974}, {"id": 5162, "seek": 2303500, "start": 23049.48, "end": 23055.48, "text": " even see what we could have gotten to at S three. So we need to kind of introduce some concept", "tokens": [51088, 754, 536, 437, 321, 727, 362, 5768, 281, 412, 318, 1045, 13, 407, 321, 643, 281, 733, 295, 5366, 512, 3410, 51388], "temperature": 0.0, "avg_logprob": -0.06657388568979448, "compression_ratio": 1.72, "no_speech_prob": 0.0069036022759974}, {"id": 5163, "seek": 2303500, "start": 23055.48, "end": 23061.56, "text": " of taking random actions, and being able to explore the environment more freely before starting", "tokens": [51388, 295, 1940, 4974, 5909, 11, 293, 885, 1075, 281, 6839, 264, 2823, 544, 16433, 949, 2891, 51692], "temperature": 0.0, "avg_logprob": -0.06657388568979448, "compression_ratio": 1.72, "no_speech_prob": 0.0069036022759974}, {"id": 5164, "seek": 2306156, "start": 23061.56, "end": 23065.800000000003, "text": " to look at these Q values, and use that for the training. So I'm actually going to go back", "tokens": [50364, 281, 574, 412, 613, 1249, 4190, 11, 293, 764, 300, 337, 264, 3097, 13, 407, 286, 478, 767, 516, 281, 352, 646, 50576], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5165, "seek": 2306156, "start": 23066.600000000002, "end": 23070.280000000002, "text": " to my slides now to make sure I don't get lost, because I think I was starting to ramble a little", "tokens": [50616, 281, 452, 9788, 586, 281, 652, 988, 286, 500, 380, 483, 2731, 11, 570, 286, 519, 286, 390, 2891, 281, 10211, 638, 257, 707, 50800], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5166, "seek": 2306156, "start": 23070.280000000002, "end": 23075.48, "text": " bit there. So we're going to now talk about learning the Q table. So essentially, I showed", "tokens": [50800, 857, 456, 13, 407, 321, 434, 516, 281, 586, 751, 466, 2539, 264, 1249, 3199, 13, 407, 4476, 11, 286, 4712, 51060], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5167, "seek": 2306156, "start": 23075.48, "end": 23081.32, "text": " you how we use that Q table, which is given some state, we just look that state up in the Q table,", "tokens": [51060, 291, 577, 321, 764, 300, 1249, 3199, 11, 597, 307, 2212, 512, 1785, 11, 321, 445, 574, 300, 1785, 493, 294, 264, 1249, 3199, 11, 51352], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5168, "seek": 2306156, "start": 23081.32, "end": 23085.960000000003, "text": " and then determine what the maximum reward we could get by taking, you know, some action is and", "tokens": [51352, 293, 550, 6997, 437, 264, 6674, 7782, 321, 727, 483, 538, 1940, 11, 291, 458, 11, 512, 3069, 307, 293, 51584], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5169, "seek": 2306156, "start": 23085.960000000003, "end": 23090.68, "text": " then take that action. And that's how we would use the Q table later on when we're actually using the", "tokens": [51584, 550, 747, 300, 3069, 13, 400, 300, 311, 577, 321, 576, 764, 264, 1249, 3199, 1780, 322, 562, 321, 434, 767, 1228, 264, 51820], "temperature": 0.0, "avg_logprob": -0.04746634528140894, "compression_ratio": 1.8520900321543408, "no_speech_prob": 0.07584583759307861}, {"id": 5170, "seek": 2309068, "start": 23090.68, "end": 23096.04, "text": " model. But when we're learning the Q table, that's not necessarily what we want to do. We don't want", "tokens": [50364, 2316, 13, 583, 562, 321, 434, 2539, 264, 1249, 3199, 11, 300, 311, 406, 4725, 437, 321, 528, 281, 360, 13, 492, 500, 380, 528, 50632], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5171, "seek": 2309068, "start": 23096.04, "end": 23101.32, "text": " to explore the environment by just taking the maximum reward we've seen so far and just always", "tokens": [50632, 281, 6839, 264, 2823, 538, 445, 1940, 264, 6674, 7782, 321, 600, 1612, 370, 1400, 293, 445, 1009, 50896], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5172, "seek": 2309068, "start": 23101.32, "end": 23105.48, "text": " going that direction, we need to make sure that we're exploring in a different way and learning", "tokens": [50896, 516, 300, 3513, 11, 321, 643, 281, 652, 988, 300, 321, 434, 12736, 294, 257, 819, 636, 293, 2539, 51104], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5173, "seek": 2309068, "start": 23105.48, "end": 23110.28, "text": " the correct values for the Q table. So essentially, our agent learns by exploring the environment", "tokens": [51104, 264, 3006, 4190, 337, 264, 1249, 3199, 13, 407, 4476, 11, 527, 9461, 27152, 538, 12736, 264, 2823, 51344], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5174, "seek": 2309068, "start": 23110.28, "end": 23114.28, "text": " and observing the outcome slash reward from each action it takes in a given state, which we've", "tokens": [51344, 293, 22107, 264, 9700, 17330, 7782, 490, 1184, 3069, 309, 2516, 294, 257, 2212, 1785, 11, 597, 321, 600, 51544], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5175, "seek": 2309068, "start": 23114.28, "end": 23118.52, "text": " already said. But how does it know what action to take in each state when it's learning? That's", "tokens": [51544, 1217, 848, 13, 583, 577, 775, 309, 458, 437, 3069, 281, 747, 294, 1184, 1785, 562, 309, 311, 2539, 30, 663, 311, 51756], "temperature": 0.0, "avg_logprob": -0.04984747568766276, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0013249621260911226}, {"id": 5176, "seek": 2311852, "start": 23118.52, "end": 23123.24, "text": " the question I need to answer for you now. Well, there's two ways of doing this. Our agent can", "tokens": [50364, 264, 1168, 286, 643, 281, 1867, 337, 291, 586, 13, 1042, 11, 456, 311, 732, 2098, 295, 884, 341, 13, 2621, 9461, 393, 50600], "temperature": 0.0, "avg_logprob": -0.04444067818777902, "compression_ratio": 1.6875, "no_speech_prob": 0.006692606955766678}, {"id": 5177, "seek": 2311852, "start": 23123.24, "end": 23128.2, "text": " essentially, you know, use the current Q table to find the best action, which is kind of what I", "tokens": [50600, 4476, 11, 291, 458, 11, 764, 264, 2190, 1249, 3199, 281, 915, 264, 1151, 3069, 11, 597, 307, 733, 295, 437, 286, 50848], "temperature": 0.0, "avg_logprob": -0.04444067818777902, "compression_ratio": 1.6875, "no_speech_prob": 0.006692606955766678}, {"id": 5178, "seek": 2311852, "start": 23128.2, "end": 23132.52, "text": " just discussed. So taking looking at the Q table, looking at the state and just taking the highest", "tokens": [50848, 445, 7152, 13, 407, 1940, 1237, 412, 264, 1249, 3199, 11, 1237, 412, 264, 1785, 293, 445, 1940, 264, 6343, 51064], "temperature": 0.0, "avg_logprob": -0.04444067818777902, "compression_ratio": 1.6875, "no_speech_prob": 0.006692606955766678}, {"id": 5179, "seek": 2311852, "start": 23132.52, "end": 23138.920000000002, "text": " reward, or it can randomly pick a valid action. And our goal is going to be when we create this Q", "tokens": [51064, 7782, 11, 420, 309, 393, 16979, 1888, 257, 7363, 3069, 13, 400, 527, 3387, 307, 516, 281, 312, 562, 321, 1884, 341, 1249, 51384], "temperature": 0.0, "avg_logprob": -0.04444067818777902, "compression_ratio": 1.6875, "no_speech_prob": 0.006692606955766678}, {"id": 5180, "seek": 2311852, "start": 23138.920000000002, "end": 23145.4, "text": " learning algorithm to have a really great balance of these two, where sometimes we use the Q table", "tokens": [51384, 2539, 9284, 281, 362, 257, 534, 869, 4772, 295, 613, 732, 11, 689, 2171, 321, 764, 264, 1249, 3199, 51708], "temperature": 0.0, "avg_logprob": -0.04444067818777902, "compression_ratio": 1.6875, "no_speech_prob": 0.006692606955766678}, {"id": 5181, "seek": 2314540, "start": 23145.4, "end": 23152.280000000002, "text": " to find the best action, and sometimes we take a random action. So that is one thing. But now", "tokens": [50364, 281, 915, 264, 1151, 3069, 11, 293, 2171, 321, 747, 257, 4974, 3069, 13, 407, 300, 307, 472, 551, 13, 583, 586, 50708], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5182, "seek": 2314540, "start": 23152.280000000002, "end": 23156.600000000002, "text": " I'm just going to talk about this formula for how we actually update Q values. So obviously, what's", "tokens": [50708, 286, 478, 445, 516, 281, 751, 466, 341, 8513, 337, 577, 321, 767, 5623, 1249, 4190, 13, 407, 2745, 11, 437, 311, 50924], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5183, "seek": 2314540, "start": 23156.600000000002, "end": 23161.08, "text": " going to end up happening in our Q learning is we're going to have an agent that's going to be in", "tokens": [50924, 516, 281, 917, 493, 2737, 294, 527, 1249, 2539, 307, 321, 434, 516, 281, 362, 364, 9461, 300, 311, 516, 281, 312, 294, 51148], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5184, "seek": 2314540, "start": 23161.08, "end": 23165.88, "text": " the learning stage, exploring the environment and having all these actions and all these rewards", "tokens": [51148, 264, 2539, 3233, 11, 12736, 264, 2823, 293, 1419, 439, 613, 5909, 293, 439, 613, 17203, 51388], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5185, "seek": 2314540, "start": 23165.88, "end": 23169.480000000003, "text": " and all these observations happening. And it's going to be moving around the environment by", "tokens": [51388, 293, 439, 613, 18163, 2737, 13, 400, 309, 311, 516, 281, 312, 2684, 926, 264, 2823, 538, 51568], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5186, "seek": 2314540, "start": 23169.480000000003, "end": 23173.480000000003, "text": " following one of these two kind of principles, randomly picking a valid action or using the", "tokens": [51568, 3480, 472, 295, 613, 732, 733, 295, 9156, 11, 16979, 8867, 257, 7363, 3069, 420, 1228, 264, 51768], "temperature": 0.0, "avg_logprob": -0.05685879801975862, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.004609204828739166}, {"id": 5187, "seek": 2317348, "start": 23173.56, "end": 23178.92, "text": " current Q table to find the best action. When it gets into a net, a new state, and it, you know,", "tokens": [50368, 2190, 1249, 3199, 281, 915, 264, 1151, 3069, 13, 1133, 309, 2170, 666, 257, 2533, 11, 257, 777, 1785, 11, 293, 309, 11, 291, 458, 11, 50636], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5188, "seek": 2317348, "start": 23178.92, "end": 23182.84, "text": " moves from state to state, it's going to keep updating this Q table, telling it, you know,", "tokens": [50636, 6067, 490, 1785, 281, 1785, 11, 309, 311, 516, 281, 1066, 25113, 341, 1249, 3199, 11, 3585, 309, 11, 291, 458, 11, 50832], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5189, "seek": 2317348, "start": 23182.84, "end": 23186.2, "text": " this is what I've learned about the environment, I think this is a better move, we're going to", "tokens": [50832, 341, 307, 437, 286, 600, 3264, 466, 264, 2823, 11, 286, 519, 341, 307, 257, 1101, 1286, 11, 321, 434, 516, 281, 51000], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5190, "seek": 2317348, "start": 23186.2, "end": 23190.36, "text": " update this value. But how does it do that in a way that's going to make sense? Because we can't", "tokens": [51000, 5623, 341, 2158, 13, 583, 577, 775, 309, 360, 300, 294, 257, 636, 300, 311, 516, 281, 652, 2020, 30, 1436, 321, 393, 380, 51208], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5191, "seek": 2317348, "start": 23190.36, "end": 23195.079999999998, "text": " just put, you know, the maximum value we got from moving, otherwise, we're going to run into that", "tokens": [51208, 445, 829, 11, 291, 458, 11, 264, 6674, 2158, 321, 658, 490, 2684, 11, 5911, 11, 321, 434, 516, 281, 1190, 666, 300, 51444], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5192, "seek": 2317348, "start": 23195.079999999998, "end": 23199.8, "text": " issue, which I just talked about, where we get stuck in that local maxima, right? I'm not sure", "tokens": [51444, 2734, 11, 597, 286, 445, 2825, 466, 11, 689, 321, 483, 5541, 294, 300, 2654, 5138, 64, 11, 558, 30, 286, 478, 406, 988, 51680], "temperature": 0.0, "avg_logprob": -0.07614839615360383, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.005060174968093634}, {"id": 5193, "seek": 2319980, "start": 23199.8, "end": 23204.76, "text": " if I called it minimum before, but anyways, it's local maxima, where we see this high reward,", "tokens": [50364, 498, 286, 1219, 309, 7285, 949, 11, 457, 13448, 11, 309, 311, 2654, 5138, 64, 11, 689, 321, 536, 341, 1090, 7782, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1013597940143786, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.007345486432313919}, {"id": 5194, "seek": 2319980, "start": 23204.76, "end": 23209.559999999998, "text": " but that's preventing us if we keep taking that action from reaching a potentially high reward", "tokens": [50612, 457, 300, 311, 19965, 505, 498, 321, 1066, 1940, 300, 3069, 490, 9906, 257, 7263, 1090, 7782, 50852], "temperature": 0.0, "avg_logprob": -0.1013597940143786, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.007345486432313919}, {"id": 5195, "seek": 2319980, "start": 23209.559999999998, "end": 23215.0, "text": " in a different state. So the formula that we actually use to update the Q table is this. So Q", "tokens": [50852, 294, 257, 819, 1785, 13, 407, 264, 8513, 300, 321, 767, 764, 281, 5623, 264, 1249, 3199, 307, 341, 13, 407, 1249, 51124], "temperature": 0.0, "avg_logprob": -0.1013597940143786, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.007345486432313919}, {"id": 5196, "seek": 2319980, "start": 23215.0, "end": 23220.6, "text": " state action equals Q state action, and a state action is just referencing first the rows for the", "tokens": [51124, 1785, 3069, 6915, 1249, 1785, 3069, 11, 293, 257, 1785, 3069, 307, 445, 40582, 700, 264, 13241, 337, 264, 51404], "temperature": 0.0, "avg_logprob": -0.1013597940143786, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.007345486432313919}, {"id": 5197, "seek": 2319980, "start": 23220.6, "end": 23227.239999999998, "text": " state and then the action as the column, plus alpha times, and then this is all in brackets,", "tokens": [51404, 1785, 293, 550, 264, 3069, 382, 264, 7738, 11, 1804, 8961, 1413, 11, 293, 550, 341, 307, 439, 294, 26179, 11, 51736], "temperature": 0.0, "avg_logprob": -0.1013597940143786, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.007345486432313919}, {"id": 5198, "seek": 2322724, "start": 23227.24, "end": 23235.88, "text": " right? Reward plus, I believe this is gamma times max Q of new states minus Q state action. So what", "tokens": [50364, 558, 30, 1300, 1007, 1804, 11, 286, 1697, 341, 307, 15546, 1413, 11469, 1249, 295, 777, 4368, 3175, 1249, 1785, 3069, 13, 407, 437, 50796], "temperature": 0.0, "avg_logprob": -0.107671937634868, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.006488137412816286}, {"id": 5199, "seek": 2322724, "start": 23235.88, "end": 23239.72, "text": " the heck does this mean? What are these constants? What is all this? We're going to talk about the", "tokens": [50796, 264, 12872, 775, 341, 914, 30, 708, 366, 613, 35870, 30, 708, 307, 439, 341, 30, 492, 434, 516, 281, 751, 466, 264, 50988], "temperature": 0.0, "avg_logprob": -0.107671937634868, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.006488137412816286}, {"id": 5200, "seek": 2322724, "start": 23239.72, "end": 23245.16, "text": " constants in a minute. But I want to, yeah, I want to explain this formula actually. So let's,", "tokens": [50988, 35870, 294, 257, 3456, 13, 583, 286, 528, 281, 11, 1338, 11, 286, 528, 281, 2903, 341, 8513, 767, 13, 407, 718, 311, 11, 51260], "temperature": 0.0, "avg_logprob": -0.107671937634868, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.006488137412816286}, {"id": 5201, "seek": 2322724, "start": 23245.16, "end": 23249.16, "text": " okay, I guess we'll go through the constants, it's hard to go through a complicated math formula.", "tokens": [51260, 1392, 11, 286, 2041, 321, 603, 352, 807, 264, 35870, 11, 309, 311, 1152, 281, 352, 807, 257, 6179, 5221, 8513, 13, 51460], "temperature": 0.0, "avg_logprob": -0.107671937634868, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.006488137412816286}, {"id": 5202, "seek": 2322724, "start": 23249.16, "end": 23254.36, "text": " So a stands for the learning rate, and gamma stands for the discount factor. So alpha learning", "tokens": [51460, 407, 257, 7382, 337, 264, 2539, 3314, 11, 293, 15546, 7382, 337, 264, 11635, 5952, 13, 407, 8961, 2539, 51720], "temperature": 0.0, "avg_logprob": -0.107671937634868, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.006488137412816286}, {"id": 5203, "seek": 2325436, "start": 23254.36, "end": 23258.52, "text": " rate, gamma discount factor. Now, what is the learning rate? Well, this is a little blurb on", "tokens": [50364, 3314, 11, 15546, 11635, 5952, 13, 823, 11, 437, 307, 264, 2539, 3314, 30, 1042, 11, 341, 307, 257, 707, 14257, 65, 322, 50572], "temperature": 0.0, "avg_logprob": -0.0861030932395689, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.013221962377429008}, {"id": 5204, "seek": 2325436, "start": 23258.52, "end": 23264.36, "text": " what this is. But essentially, the learning rate ensures that we don't update our Q table too much", "tokens": [50572, 437, 341, 307, 13, 583, 4476, 11, 264, 2539, 3314, 28111, 300, 321, 500, 380, 5623, 527, 1249, 3199, 886, 709, 50864], "temperature": 0.0, "avg_logprob": -0.0861030932395689, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.013221962377429008}, {"id": 5205, "seek": 2325436, "start": 23265.32, "end": 23271.16, "text": " on every observation. So before, right, when I was showing you like this, if we can go back", "tokens": [50912, 322, 633, 14816, 13, 407, 949, 11, 558, 11, 562, 286, 390, 4099, 291, 411, 341, 11, 498, 321, 393, 352, 646, 51204], "temperature": 0.0, "avg_logprob": -0.0861030932395689, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.013221962377429008}, {"id": 5206, "seek": 2325436, "start": 23271.16, "end": 23276.68, "text": " to my windows ink, why is this not working? I guess I'm just not patient enough. Before when I was", "tokens": [51204, 281, 452, 9309, 11276, 11, 983, 307, 341, 406, 1364, 30, 286, 2041, 286, 478, 445, 406, 4537, 1547, 13, 4546, 562, 286, 390, 51480], "temperature": 0.0, "avg_logprob": -0.0861030932395689, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.013221962377429008}, {"id": 5207, "seek": 2325436, "start": 23276.68, "end": 23281.88, "text": " showing you all I did when I took an action was I looked at the reward that I got from taking that", "tokens": [51480, 4099, 291, 439, 286, 630, 562, 286, 1890, 364, 3069, 390, 286, 2956, 412, 264, 7782, 300, 286, 658, 490, 1940, 300, 51740], "temperature": 0.0, "avg_logprob": -0.0861030932395689, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.013221962377429008}, {"id": 5208, "seek": 2328188, "start": 23281.88, "end": 23287.08, "text": " action. And I just put that in my Q table, right? Now, obviously, that is not an optimal approach", "tokens": [50364, 3069, 13, 400, 286, 445, 829, 300, 294, 452, 1249, 3199, 11, 558, 30, 823, 11, 2745, 11, 300, 307, 406, 364, 16252, 3109, 50624], "temperature": 0.0, "avg_logprob": -0.05427108332514763, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.005219935439527035}, {"id": 5209, "seek": 2328188, "start": 23287.08, "end": 23291.64, "text": " to do this, because that means that in the instance where we hit state one, well, I'm not going to", "tokens": [50624, 281, 360, 341, 11, 570, 300, 1355, 300, 294, 264, 5197, 689, 321, 2045, 1785, 472, 11, 731, 11, 286, 478, 406, 516, 281, 50852], "temperature": 0.0, "avg_logprob": -0.05427108332514763, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.005219935439527035}, {"id": 5210, "seek": 2328188, "start": 23291.64, "end": 23295.24, "text": " be able to get to this reward of four, because I'm going to throw that, you know, three in here,", "tokens": [50852, 312, 1075, 281, 483, 281, 341, 7782, 295, 1451, 11, 570, 286, 478, 516, 281, 3507, 300, 11, 291, 458, 11, 1045, 294, 510, 11, 51032], "temperature": 0.0, "avg_logprob": -0.05427108332514763, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.005219935439527035}, {"id": 5211, "seek": 2328188, "start": 23295.24, "end": 23301.8, "text": " and I'm just going to keep taking that action. We need to, you know, hopefully make this move", "tokens": [51032, 293, 286, 478, 445, 516, 281, 1066, 1940, 300, 3069, 13, 492, 643, 281, 11, 291, 458, 11, 4696, 652, 341, 1286, 51360], "temperature": 0.0, "avg_logprob": -0.05427108332514763, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.005219935439527035}, {"id": 5212, "seek": 2328188, "start": 23301.8, "end": 23307.48, "text": " action actually have a higher value than stay. So that next time we're in state one, we consider", "tokens": [51360, 3069, 767, 362, 257, 2946, 2158, 813, 1754, 13, 407, 300, 958, 565, 321, 434, 294, 1785, 472, 11, 321, 1949, 51644], "temperature": 0.0, "avg_logprob": -0.05427108332514763, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.005219935439527035}, {"id": 5213, "seek": 2330748, "start": 23307.48, "end": 23312.52, "text": " the fact that we could move to state two, and then move to state three to optimize our reward.", "tokens": [50364, 264, 1186, 300, 321, 727, 1286, 281, 1785, 732, 11, 293, 550, 1286, 281, 1785, 1045, 281, 19719, 527, 7782, 13, 50616], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5214, "seek": 2330748, "start": 23312.52, "end": 23316.04, "text": " So how do we do that? Well, the learning rate is one thing that helps us kind of accomplish", "tokens": [50616, 407, 577, 360, 321, 360, 300, 30, 1042, 11, 264, 2539, 3314, 307, 472, 551, 300, 3665, 505, 733, 295, 9021, 50792], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5215, "seek": 2330748, "start": 23316.04, "end": 23321.079999999998, "text": " this behavior. Essentially, what is telling us, and this is usually a decimal value, right,", "tokens": [50792, 341, 5223, 13, 23596, 11, 437, 307, 3585, 505, 11, 293, 341, 307, 2673, 257, 26601, 2158, 11, 558, 11, 51044], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5216, "seek": 2330748, "start": 23321.079999999998, "end": 23327.64, "text": " is how much we're allowed to update every single Q value by on every single action or every single", "tokens": [51044, 307, 577, 709, 321, 434, 4350, 281, 5623, 633, 2167, 1249, 2158, 538, 322, 633, 2167, 3069, 420, 633, 2167, 51372], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5217, "seek": 2330748, "start": 23327.64, "end": 23332.36, "text": " observation. So if we just use the approach before, then we're only going to need to observe,", "tokens": [51372, 14816, 13, 407, 498, 321, 445, 764, 264, 3109, 949, 11, 550, 321, 434, 787, 516, 281, 643, 281, 11441, 11, 51608], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5218, "seek": 2330748, "start": 23332.36, "end": 23335.96, "text": " given the amount of states and the amount of actions, and we'll be able to completely fill", "tokens": [51608, 2212, 264, 2372, 295, 4368, 293, 264, 2372, 295, 5909, 11, 293, 321, 603, 312, 1075, 281, 2584, 2836, 51788], "temperature": 0.0, "avg_logprob": -0.04840737230637494, "compression_ratio": 1.8070739549839228, "no_speech_prob": 0.029309209436178207}, {"id": 5219, "seek": 2333596, "start": 23335.96, "end": 23339.8, "text": " in the Q table. So in our case, if we had like three states and three actions, we could, you", "tokens": [50364, 294, 264, 1249, 3199, 13, 407, 294, 527, 1389, 11, 498, 321, 632, 411, 1045, 4368, 293, 1045, 5909, 11, 321, 727, 11, 291, 50556], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5220, "seek": 2333596, "start": 23339.8, "end": 23344.44, "text": " know, nine iterations, we'd be able to fill the entire Q table. The learning rate means that", "tokens": [50556, 458, 11, 4949, 36540, 11, 321, 1116, 312, 1075, 281, 2836, 264, 2302, 1249, 3199, 13, 440, 2539, 3314, 1355, 300, 50788], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5221, "seek": 2333596, "start": 23344.44, "end": 23350.04, "text": " it's going to just update a little bit slower and essentially change the value in the Q table very", "tokens": [50788, 309, 311, 516, 281, 445, 5623, 257, 707, 857, 14009, 293, 4476, 1319, 264, 2158, 294, 264, 1249, 3199, 588, 51068], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5222, "seek": 2333596, "start": 23350.04, "end": 23354.44, "text": " slightly. So you can see that what we're doing is taking the current value of the Q table. So", "tokens": [51068, 4748, 13, 407, 291, 393, 536, 300, 437, 321, 434, 884, 307, 1940, 264, 2190, 2158, 295, 264, 1249, 3199, 13, 407, 51288], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5223, "seek": 2333596, "start": 23354.44, "end": 23360.44, "text": " whatever is already there. And then what we're going to do is add some value here. And this value", "tokens": [51288, 2035, 307, 1217, 456, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 909, 512, 2158, 510, 13, 400, 341, 2158, 51588], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5224, "seek": 2333596, "start": 23360.44, "end": 23364.92, "text": " that we add is either going to be positive or negative, essentially telling us, you know,", "tokens": [51588, 300, 321, 909, 307, 2139, 516, 281, 312, 3353, 420, 3671, 11, 4476, 3585, 505, 11, 291, 458, 11, 51812], "temperature": 0.0, "avg_logprob": -0.06422056950313944, "compression_ratio": 1.9186440677966101, "no_speech_prob": 0.006487990729510784}, {"id": 5225, "seek": 2336492, "start": 23364.92, "end": 23369.239999999998, "text": " whether we should take this new action or whether we shouldn't take this new action.", "tokens": [50364, 1968, 321, 820, 747, 341, 777, 3069, 420, 1968, 321, 4659, 380, 747, 341, 777, 3069, 13, 50580], "temperature": 0.0, "avg_logprob": -0.06776310290609087, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001206560293212533}, {"id": 5226, "seek": 2336492, "start": 23369.239999999998, "end": 23374.92, "text": " Now, the way that this kind of value is calculated, right, is obviously our alpha is", "tokens": [50580, 823, 11, 264, 636, 300, 341, 733, 295, 2158, 307, 15598, 11, 558, 11, 307, 2745, 527, 8961, 307, 50864], "temperature": 0.0, "avg_logprob": -0.06776310290609087, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001206560293212533}, {"id": 5227, "seek": 2336492, "start": 23374.92, "end": 23381.0, "text": " multiplied this by this, but we have the reward plus, in this case, gamma, which is just going to", "tokens": [50864, 17207, 341, 538, 341, 11, 457, 321, 362, 264, 7782, 1804, 11, 294, 341, 1389, 11, 15546, 11, 597, 307, 445, 516, 281, 51168], "temperature": 0.0, "avg_logprob": -0.06776310290609087, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001206560293212533}, {"id": 5228, "seek": 2336492, "start": 23381.0, "end": 23386.28, "text": " actually be the discount factor. And I'll talk about how that works in a second of the maximum", "tokens": [51168, 767, 312, 264, 11635, 5952, 13, 400, 286, 603, 751, 466, 577, 300, 1985, 294, 257, 1150, 295, 264, 6674, 51432], "temperature": 0.0, "avg_logprob": -0.06776310290609087, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001206560293212533}, {"id": 5229, "seek": 2336492, "start": 23386.28, "end": 23393.96, "text": " of the new state we moved into. Now, what this means is find the maximum reward that we could", "tokens": [51432, 295, 264, 777, 1785, 321, 4259, 666, 13, 823, 11, 437, 341, 1355, 307, 915, 264, 6674, 7782, 300, 321, 727, 51816], "temperature": 0.0, "avg_logprob": -0.06776310290609087, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001206560293212533}, {"id": 5230, "seek": 2339396, "start": 23393.96, "end": 23401.399999999998, "text": " receive in the new state by taking any action and multiply that by what we call the discount factor.", "tokens": [50364, 4774, 294, 264, 777, 1785, 538, 1940, 604, 3069, 293, 12972, 300, 538, 437, 321, 818, 264, 11635, 5952, 13, 50736], "temperature": 0.0, "avg_logprob": -0.08554546649639423, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.01001307088881731}, {"id": 5231, "seek": 2339396, "start": 23401.399999999998, "end": 23405.879999999997, "text": " With this part of the formulas trying to do is exactly what I've kind of been talking about.", "tokens": [50736, 2022, 341, 644, 295, 264, 30546, 1382, 281, 360, 307, 2293, 437, 286, 600, 733, 295, 668, 1417, 466, 13, 50960], "temperature": 0.0, "avg_logprob": -0.08554546649639423, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.01001307088881731}, {"id": 5232, "seek": 2339396, "start": 23406.52, "end": 23412.04, "text": " Try to look forward and say, okay, so I know if I take this action in this state, I receive", "tokens": [50992, 6526, 281, 574, 2128, 293, 584, 11, 1392, 11, 370, 286, 458, 498, 286, 747, 341, 3069, 294, 341, 1785, 11, 286, 4774, 51268], "temperature": 0.0, "avg_logprob": -0.08554546649639423, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.01001307088881731}, {"id": 5233, "seek": 2339396, "start": 23412.04, "end": 23417.879999999997, "text": " this amount of reward. But I need to factor in the reward I could receive in the next state,", "tokens": [51268, 341, 2372, 295, 7782, 13, 583, 286, 643, 281, 5952, 294, 264, 7782, 286, 727, 4774, 294, 264, 958, 1785, 11, 51560], "temperature": 0.0, "avg_logprob": -0.08554546649639423, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.01001307088881731}, {"id": 5234, "seek": 2339396, "start": 23417.879999999997, "end": 23423.8, "text": " so that I can determine the best place to move to. That's kind of what this max and this", "tokens": [51560, 370, 300, 286, 393, 6997, 264, 1151, 1081, 281, 1286, 281, 13, 663, 311, 733, 295, 437, 341, 11469, 293, 341, 51856], "temperature": 0.0, "avg_logprob": -0.08554546649639423, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.01001307088881731}, {"id": 5235, "seek": 2342380, "start": 23424.36, "end": 23428.76, "text": " gamma are trying to do for us. So this discount factor, whatever you want to call it. It's trying", "tokens": [50392, 15546, 366, 1382, 281, 360, 337, 505, 13, 407, 341, 11635, 5952, 11, 2035, 291, 528, 281, 818, 309, 13, 467, 311, 1382, 50612], "temperature": 0.0, "avg_logprob": -0.09018472145343649, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0013249772600829601}, {"id": 5236, "seek": 2342380, "start": 23428.76, "end": 23435.16, "text": " to factor in a little bit about what we could get from the next state into this equation so that", "tokens": [50612, 281, 5952, 294, 257, 707, 857, 466, 437, 321, 727, 483, 490, 264, 958, 1785, 666, 341, 5367, 370, 300, 50932], "temperature": 0.0, "avg_logprob": -0.09018472145343649, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0013249772600829601}, {"id": 5237, "seek": 2342380, "start": 23435.16, "end": 23441.64, "text": " hopefully our kind of agent can learn a little bit more about the transition states. So states that", "tokens": [50932, 4696, 527, 733, 295, 9461, 393, 1466, 257, 707, 857, 544, 466, 264, 6034, 4368, 13, 407, 4368, 300, 51256], "temperature": 0.0, "avg_logprob": -0.09018472145343649, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0013249772600829601}, {"id": 5238, "seek": 2342380, "start": 23441.64, "end": 23446.2, "text": " maybe are actions that maybe don't give us an immediate reward, but lead to a larger reward", "tokens": [51256, 1310, 366, 5909, 300, 1310, 500, 380, 976, 505, 364, 11629, 7782, 11, 457, 1477, 281, 257, 4833, 7782, 51484], "temperature": 0.0, "avg_logprob": -0.09018472145343649, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0013249772600829601}, {"id": 5239, "seek": 2342380, "start": 23446.2, "end": 23450.92, "text": " in the future. That's what this Y and max are trying to do. Then what we do is we subtract", "tokens": [51484, 294, 264, 2027, 13, 663, 311, 437, 341, 398, 293, 11469, 366, 1382, 281, 360, 13, 1396, 437, 321, 360, 307, 321, 16390, 51720], "temperature": 0.0, "avg_logprob": -0.09018472145343649, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0013249772600829601}, {"id": 5240, "seek": 2345092, "start": 23451.559999999998, "end": 23457.559999999998, "text": " from this, the state and action. This is just to make sure that we're adding what the difference", "tokens": [50396, 490, 341, 11, 264, 1785, 293, 3069, 13, 639, 307, 445, 281, 652, 988, 300, 321, 434, 5127, 437, 264, 2649, 50696], "temperature": 0.0, "avg_logprob": -0.09673881530761719, "compression_ratio": 1.6982456140350877, "no_speech_prob": 0.02368617057800293}, {"id": 5241, "seek": 2345092, "start": 23457.559999999998, "end": 23465.239999999998, "text": " was in, you know, what we get from this versus what the current value is, and not like multiplying", "tokens": [50696, 390, 294, 11, 291, 458, 11, 437, 321, 483, 490, 341, 5717, 437, 264, 2190, 2158, 307, 11, 293, 406, 411, 30955, 51080], "temperature": 0.0, "avg_logprob": -0.09673881530761719, "compression_ratio": 1.6982456140350877, "no_speech_prob": 0.02368617057800293}, {"id": 5242, "seek": 2345092, "start": 23465.239999999998, "end": 23469.64, "text": " these values crazily. I mean, you can look into more of the math here and plug in like some values", "tokens": [51080, 613, 4190, 46348, 953, 13, 286, 914, 11, 291, 393, 574, 666, 544, 295, 264, 5221, 510, 293, 5452, 294, 411, 512, 4190, 51300], "temperature": 0.0, "avg_logprob": -0.09673881530761719, "compression_ratio": 1.6982456140350877, "no_speech_prob": 0.02368617057800293}, {"id": 5243, "seek": 2345092, "start": 23469.64, "end": 23472.679999999997, "text": " later, and you'll see how this kind of works. But this is the basic formula. And I feel like I", "tokens": [51300, 1780, 11, 293, 291, 603, 536, 577, 341, 733, 295, 1985, 13, 583, 341, 307, 264, 3875, 8513, 13, 400, 286, 841, 411, 286, 51452], "temperature": 0.0, "avg_logprob": -0.09673881530761719, "compression_ratio": 1.6982456140350877, "no_speech_prob": 0.02368617057800293}, {"id": 5244, "seek": 2345092, "start": 23472.679999999997, "end": 23478.28, "text": " explain that in depth enough. Okay, so now that we've done that, and we've updated this, we've", "tokens": [51452, 2903, 300, 294, 7161, 1547, 13, 1033, 11, 370, 586, 300, 321, 600, 1096, 300, 11, 293, 321, 600, 10588, 341, 11, 321, 600, 51732], "temperature": 0.0, "avg_logprob": -0.09673881530761719, "compression_ratio": 1.6982456140350877, "no_speech_prob": 0.02368617057800293}, {"id": 5245, "seek": 2347828, "start": 23478.36, "end": 23484.199999999997, "text": " learned kind of how we update the cells and how this works. I could go back to the whiteboard and", "tokens": [50368, 3264, 733, 295, 577, 321, 5623, 264, 5438, 293, 577, 341, 1985, 13, 286, 727, 352, 646, 281, 264, 2418, 3787, 293, 50660], "temperature": 0.0, "avg_logprob": -0.07475595392732538, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.001700695022009313}, {"id": 5246, "seek": 2347828, "start": 23484.199999999997, "end": 23488.36, "text": " draw it out. But I feel like that makes enough sense. We're going to look at what the next state is,", "tokens": [50660, 2642, 309, 484, 13, 583, 286, 841, 411, 300, 1669, 1547, 2020, 13, 492, 434, 516, 281, 574, 412, 437, 264, 958, 1785, 307, 11, 50868], "temperature": 0.0, "avg_logprob": -0.07475595392732538, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.001700695022009313}, {"id": 5247, "seek": 2347828, "start": 23488.36, "end": 23492.44, "text": " we're going to factor that into our calculation, we have this learning rate, which tells us essentially", "tokens": [50868, 321, 434, 516, 281, 5952, 300, 666, 527, 17108, 11, 321, 362, 341, 2539, 3314, 11, 597, 5112, 505, 4476, 51072], "temperature": 0.0, "avg_logprob": -0.07475595392732538, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.001700695022009313}, {"id": 5248, "seek": 2347828, "start": 23492.44, "end": 23498.52, "text": " how much we can update each cell value by. And we have this, what do you call it here discount", "tokens": [51072, 577, 709, 321, 393, 5623, 1184, 2815, 2158, 538, 13, 400, 321, 362, 341, 11, 437, 360, 291, 818, 309, 510, 11635, 51376], "temperature": 0.0, "avg_logprob": -0.07475595392732538, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.001700695022009313}, {"id": 5249, "seek": 2347828, "start": 23498.52, "end": 23503.879999999997, "text": " factor, which essentially tries to kind of define the balance between finding really good rewards", "tokens": [51376, 5952, 11, 597, 4476, 9898, 281, 733, 295, 6964, 264, 4772, 1296, 5006, 534, 665, 17203, 51644], "temperature": 0.0, "avg_logprob": -0.07475595392732538, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.001700695022009313}, {"id": 5250, "seek": 2350388, "start": 23503.88, "end": 23509.960000000003, "text": " in our current state, and finding the rewards in the future state. So the higher this value is,", "tokens": [50364, 294, 527, 2190, 1785, 11, 293, 5006, 264, 17203, 294, 264, 2027, 1785, 13, 407, 264, 2946, 341, 2158, 307, 11, 50668], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5251, "seek": 2350388, "start": 23509.960000000003, "end": 23513.960000000003, "text": " the more we're going to look towards the future, the lower it is, the more we're going to focus", "tokens": [50668, 264, 544, 321, 434, 516, 281, 574, 3030, 264, 2027, 11, 264, 3126, 309, 307, 11, 264, 544, 321, 434, 516, 281, 1879, 50868], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5252, "seek": 2350388, "start": 23513.960000000003, "end": 23517.48, "text": " completely on our current reward, right? And obviously, that makes sense, because we're going", "tokens": [50868, 2584, 322, 527, 2190, 7782, 11, 558, 30, 400, 2745, 11, 300, 1669, 2020, 11, 570, 321, 434, 516, 51044], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5253, "seek": 2350388, "start": 23517.48, "end": 23521.24, "text": " to add the maximum value. And if we're multiplying that by a lower number, that means we're going", "tokens": [51044, 281, 909, 264, 6674, 2158, 13, 400, 498, 321, 434, 30955, 300, 538, 257, 3126, 1230, 11, 300, 1355, 321, 434, 516, 51232], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5254, "seek": 2350388, "start": 23521.24, "end": 23527.48, "text": " to consider that less than if that was greater. Awesome. Okay. So now that we've kind of understand", "tokens": [51232, 281, 1949, 300, 1570, 813, 498, 300, 390, 5044, 13, 10391, 13, 1033, 13, 407, 586, 300, 321, 600, 733, 295, 1223, 51544], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5255, "seek": 2350388, "start": 23527.48, "end": 23531.0, "text": " that I want to move on to a Q learning example. And what we're going to do for this example is", "tokens": [51544, 300, 286, 528, 281, 1286, 322, 281, 257, 1249, 2539, 1365, 13, 400, 437, 321, 434, 516, 281, 360, 337, 341, 1365, 307, 51720], "temperature": 0.0, "avg_logprob": -0.06321235497792561, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.009708119556307793}, {"id": 5256, "seek": 2353100, "start": 23531.08, "end": 23536.6, "text": " actually use something called the open AI gym. I just need to throw my drawing tablet away", "tokens": [50368, 767, 764, 746, 1219, 264, 1269, 7318, 9222, 13, 286, 445, 643, 281, 3507, 452, 6316, 14136, 1314, 50644], "temperature": 0.0, "avg_logprob": -0.10026231876089553, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.07584627717733383}, {"id": 5257, "seek": 2353100, "start": 23536.6, "end": 23541.96, "text": " right there so that we can get started. But open AI gym is actually a really interesting kind of", "tokens": [50644, 558, 456, 370, 300, 321, 393, 483, 1409, 13, 583, 1269, 7318, 9222, 307, 767, 257, 534, 1880, 733, 295, 50912], "temperature": 0.0, "avg_logprob": -0.10026231876089553, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.07584627717733383}, {"id": 5258, "seek": 2353100, "start": 23541.96, "end": 23547.24, "text": " module. I don't even actually, I don't even really know the way to describe it almost tool. There's", "tokens": [50912, 10088, 13, 286, 500, 380, 754, 767, 11, 286, 500, 380, 754, 534, 458, 264, 636, 281, 6786, 309, 1920, 2290, 13, 821, 311, 51176], "temperature": 0.0, "avg_logprob": -0.10026231876089553, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.07584627717733383}, {"id": 5259, "seek": 2353100, "start": 23547.24, "end": 23553.08, "text": " actually developed by open AI, you know, coincidentally by the name, which is founded by Elon Musk", "tokens": [51176, 767, 4743, 538, 1269, 7318, 11, 291, 458, 11, 13001, 36578, 538, 264, 1315, 11, 597, 307, 13234, 538, 28498, 26019, 51468], "temperature": 0.0, "avg_logprob": -0.10026231876089553, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.07584627717733383}, {"id": 5260, "seek": 2353100, "start": 23553.08, "end": 23558.12, "text": " and someone else. So he's actually, you know, made this kind of, I don't really don't know the word", "tokens": [51468, 293, 1580, 1646, 13, 407, 415, 311, 767, 11, 291, 458, 11, 1027, 341, 733, 295, 11, 286, 500, 380, 534, 500, 380, 458, 264, 1349, 51720], "temperature": 0.0, "avg_logprob": -0.10026231876089553, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.07584627717733383}, {"id": 5261, "seek": 2355812, "start": 23558.12, "end": 23563.719999999998, "text": " to describe it. I almost want to say tool that allows programmers to work with these really cool", "tokens": [50364, 281, 6786, 309, 13, 286, 1920, 528, 281, 584, 2290, 300, 4045, 41504, 281, 589, 365, 613, 534, 1627, 50644], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5262, "seek": 2355812, "start": 23563.719999999998, "end": 23568.6, "text": " gym environments and train reinforcement learning models. So you'll see how this works in a second,", "tokens": [50644, 9222, 12388, 293, 3847, 29280, 2539, 5245, 13, 407, 291, 603, 536, 577, 341, 1985, 294, 257, 1150, 11, 50888], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5263, "seek": 2355812, "start": 23568.6, "end": 23572.84, "text": " but essentially, there's a ton of graphical environments that have very easy interfaces", "tokens": [50888, 457, 4476, 11, 456, 311, 257, 2952, 295, 35942, 12388, 300, 362, 588, 1858, 28416, 51100], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5264, "seek": 2355812, "start": 23572.84, "end": 23577.32, "text": " to use. So like moving characters around them, that you're allowed to experiment with completely", "tokens": [51100, 281, 764, 13, 407, 411, 2684, 4342, 926, 552, 11, 300, 291, 434, 4350, 281, 5120, 365, 2584, 51324], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5265, "seek": 2355812, "start": 23577.32, "end": 23581.64, "text": " for free as a programmer to try to, you know, make some cool reinforcement learning models.", "tokens": [51324, 337, 1737, 382, 257, 32116, 281, 853, 281, 11, 291, 458, 11, 652, 512, 1627, 29280, 2539, 5245, 13, 51540], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5266, "seek": 2355812, "start": 23581.64, "end": 23585.32, "text": " That's what open AI gym is. And you can look at it. I mean, we'll click on it here actually to see", "tokens": [51540, 663, 311, 437, 1269, 7318, 9222, 307, 13, 400, 291, 393, 574, 412, 309, 13, 286, 914, 11, 321, 603, 2052, 322, 309, 510, 767, 281, 536, 51724], "temperature": 0.0, "avg_logprob": -0.06334411096936873, "compression_ratio": 1.76, "no_speech_prob": 0.0408409908413887}, {"id": 5267, "seek": 2358532, "start": 23585.32, "end": 23589.079999999998, "text": " what it is. You can see gym, there's all these different Atari environments, and it's just a", "tokens": [50364, 437, 309, 307, 13, 509, 393, 536, 9222, 11, 456, 311, 439, 613, 819, 41381, 12388, 11, 293, 309, 311, 445, 257, 50552], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5268, "seek": 2358532, "start": 23589.079999999998, "end": 23594.36, "text": " way to kind of train reinforcement learning models. All right. So now we're going to start by just", "tokens": [50552, 636, 281, 733, 295, 3847, 29280, 2539, 5245, 13, 1057, 558, 13, 407, 586, 321, 434, 516, 281, 722, 538, 445, 50816], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5269, "seek": 2358532, "start": 23594.36, "end": 23598.84, "text": " importing gym. If you're in Collaboratory, there's nothing you need to do here. If you're in your", "tokens": [50816, 43866, 9222, 13, 759, 291, 434, 294, 44483, 4745, 11, 456, 311, 1825, 291, 643, 281, 360, 510, 13, 759, 291, 434, 294, 428, 51040], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5270, "seek": 2358532, "start": 23598.84, "end": 23602.84, "text": " own thing, you're going to have to pip install gym. And then what we're going to do is make this", "tokens": [51040, 1065, 551, 11, 291, 434, 516, 281, 362, 281, 8489, 3625, 9222, 13, 400, 550, 437, 321, 434, 516, 281, 360, 307, 652, 341, 51240], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5271, "seek": 2358532, "start": 23602.84, "end": 23608.52, "text": " frozen lake v zero gym. So essentially, what this does is just set up the environment that we're", "tokens": [51240, 12496, 11001, 371, 4018, 9222, 13, 407, 4476, 11, 437, 341, 775, 307, 445, 992, 493, 264, 2823, 300, 321, 434, 51524], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5272, "seek": 2358532, "start": 23608.52, "end": 23612.92, "text": " going to use. Now, I'll talk more about what this environment is later, but I want to talk about how", "tokens": [51524, 516, 281, 764, 13, 823, 11, 286, 603, 751, 544, 466, 437, 341, 2823, 307, 1780, 11, 457, 286, 528, 281, 751, 466, 577, 51744], "temperature": 0.0, "avg_logprob": -0.08401475270589193, "compression_ratio": 1.8422712933753944, "no_speech_prob": 0.031139517202973366}, {"id": 5273, "seek": 2361292, "start": 23612.92, "end": 23617.8, "text": " gym works, because we are going to be using this throughout the thing. So the open AI gym", "tokens": [50364, 9222, 1985, 11, 570, 321, 366, 516, 281, 312, 1228, 341, 3710, 264, 551, 13, 407, 264, 1269, 7318, 9222, 50608], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5274, "seek": 2361292, "start": 23618.44, "end": 23623.559999999998, "text": " is meant for reinforcement learning. And essentially what it has is an observation space", "tokens": [50640, 307, 4140, 337, 29280, 2539, 13, 400, 4476, 437, 309, 575, 307, 364, 14816, 1901, 50896], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5275, "seek": 2361292, "start": 23623.559999999998, "end": 23628.519999999997, "text": " and an action space for every environment. Now the observation space is what we call our", "tokens": [50896, 293, 364, 3069, 1901, 337, 633, 2823, 13, 823, 264, 14816, 1901, 307, 437, 321, 818, 527, 51144], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5276, "seek": 2361292, "start": 23628.519999999997, "end": 23633.879999999997, "text": " environment, right? And that will tell us the amount of states that exist in this environment.", "tokens": [51144, 2823, 11, 558, 30, 400, 300, 486, 980, 505, 264, 2372, 295, 4368, 300, 2514, 294, 341, 2823, 13, 51412], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5277, "seek": 2361292, "start": 23633.879999999997, "end": 23637.079999999998, "text": " Now, in our case, we're going to be using kind of like a maze like thing, which I'll show you in", "tokens": [51412, 823, 11, 294, 527, 1389, 11, 321, 434, 516, 281, 312, 1228, 733, 295, 411, 257, 33032, 411, 551, 11, 597, 286, 603, 855, 291, 294, 51572], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5278, "seek": 2361292, "start": 23637.079999999998, "end": 23641.8, "text": " a second. So you'll understand why we get the values we do. Action space tells us how many", "tokens": [51572, 257, 1150, 13, 407, 291, 603, 1223, 983, 321, 483, 264, 4190, 321, 360, 13, 16261, 1901, 5112, 505, 577, 867, 51808], "temperature": 0.0, "avg_logprob": -0.0653992493947347, "compression_ratio": 1.8456375838926173, "no_speech_prob": 0.003075200133025646}, {"id": 5279, "seek": 2364180, "start": 23641.8, "end": 23647.32, "text": " actions we can take when we do the dot n, at any given state. So if we print this out,", "tokens": [50364, 5909, 321, 393, 747, 562, 321, 360, 264, 5893, 297, 11, 412, 604, 2212, 1785, 13, 407, 498, 321, 4482, 341, 484, 11, 50640], "temperature": 0.0, "avg_logprob": -0.119058624903361, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.004468079656362534}, {"id": 5280, "seek": 2364180, "start": 23647.88, "end": 23652.68, "text": " we get 16 and four, representing the observation space. In other words, the number of states is", "tokens": [50668, 321, 483, 3165, 293, 1451, 11, 13460, 264, 14816, 1901, 13, 682, 661, 2283, 11, 264, 1230, 295, 4368, 307, 50908], "temperature": 0.0, "avg_logprob": -0.119058624903361, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.004468079656362534}, {"id": 5281, "seek": 2364180, "start": 23652.68, "end": 23658.2, "text": " 16. And the amount of actions we can take in every single state is four. Now in this case,", "tokens": [50908, 3165, 13, 400, 264, 2372, 295, 5909, 321, 393, 747, 294, 633, 2167, 1785, 307, 1451, 13, 823, 294, 341, 1389, 11, 51184], "temperature": 0.0, "avg_logprob": -0.119058624903361, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.004468079656362534}, {"id": 5282, "seek": 2364180, "start": 23658.2, "end": 23664.12, "text": " these actions are going to be left down up and right. But yes, now env dot reset. So essentially,", "tokens": [51184, 613, 5909, 366, 516, 281, 312, 1411, 760, 493, 293, 558, 13, 583, 2086, 11, 586, 465, 85, 5893, 14322, 13, 407, 4476, 11, 51480], "temperature": 0.0, "avg_logprob": -0.119058624903361, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.004468079656362534}, {"id": 5283, "seek": 2364180, "start": 23664.12, "end": 23669.079999999998, "text": " we have some commands that allow us to move around the environment, which are actually down here.", "tokens": [51480, 321, 362, 512, 16901, 300, 2089, 505, 281, 1286, 926, 264, 2823, 11, 597, 366, 767, 760, 510, 13, 51728], "temperature": 0.0, "avg_logprob": -0.119058624903361, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.004468079656362534}, {"id": 5284, "seek": 2366908, "start": 23669.08, "end": 23673.640000000003, "text": " If we want to reset the environment and start back in the beginning state, then we do env", "tokens": [50364, 759, 321, 528, 281, 14322, 264, 2823, 293, 722, 646, 294, 264, 2863, 1785, 11, 550, 321, 360, 465, 85, 50592], "temperature": 0.0, "avg_logprob": -0.08132812894623855, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.002472511027008295}, {"id": 5285, "seek": 2366908, "start": 23673.640000000003, "end": 23677.72, "text": " dot reset, you can see this actually returns to us the starting state, which obviously is going to", "tokens": [50592, 5893, 14322, 11, 291, 393, 536, 341, 767, 11247, 281, 505, 264, 2891, 1785, 11, 597, 2745, 307, 516, 281, 50796], "temperature": 0.0, "avg_logprob": -0.08132812894623855, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.002472511027008295}, {"id": 5286, "seek": 2366908, "start": 23677.72, "end": 23684.440000000002, "text": " be zero. Now we also have the ability to take a random action, or select a random action from", "tokens": [50796, 312, 4018, 13, 823, 321, 611, 362, 264, 3485, 281, 747, 257, 4974, 3069, 11, 420, 3048, 257, 4974, 3069, 490, 51132], "temperature": 0.0, "avg_logprob": -0.08132812894623855, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.002472511027008295}, {"id": 5287, "seek": 2366908, "start": 23684.440000000002, "end": 23689.24, "text": " the action space. So what this line does right here is say of the action space, so of all the", "tokens": [51132, 264, 3069, 1901, 13, 407, 437, 341, 1622, 775, 558, 510, 307, 584, 295, 264, 3069, 1901, 11, 370, 295, 439, 264, 51372], "temperature": 0.0, "avg_logprob": -0.08132812894623855, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.002472511027008295}, {"id": 5288, "seek": 2366908, "start": 23689.24, "end": 23694.440000000002, "text": " commands that are there, or all the actions we could take, pick a random one and return that.", "tokens": [51372, 16901, 300, 366, 456, 11, 420, 439, 264, 5909, 321, 727, 747, 11, 1888, 257, 4974, 472, 293, 2736, 300, 13, 51632], "temperature": 0.0, "avg_logprob": -0.08132812894623855, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.002472511027008295}, {"id": 5289, "seek": 2369444, "start": 23694.52, "end": 23701.559999999998, "text": " So if you do that, actually, let's just print action and see what this is. You see we get zero", "tokens": [50368, 407, 498, 291, 360, 300, 11, 767, 11, 718, 311, 445, 4482, 3069, 293, 536, 437, 341, 307, 13, 509, 536, 321, 483, 4018, 50720], "temperature": 0.0, "avg_logprob": -0.10460576057434082, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0018101363675668836}, {"id": 5290, "seek": 2369444, "start": 23701.559999999998, "end": 23707.96, "text": " to right, it just gives us a random action that is valid from the action space. All right. Next,", "tokens": [50720, 281, 558, 11, 309, 445, 2709, 505, 257, 4974, 3069, 300, 307, 7363, 490, 264, 3069, 1901, 13, 1057, 558, 13, 3087, 11, 51040], "temperature": 0.0, "avg_logprob": -0.10460576057434082, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0018101363675668836}, {"id": 5291, "seek": 2369444, "start": 23707.96, "end": 23714.6, "text": " what we have is this env dot step in action. Now what this does is take whatever action we have,", "tokens": [51040, 437, 321, 362, 307, 341, 465, 85, 5893, 1823, 294, 3069, 13, 823, 437, 341, 775, 307, 747, 2035, 3069, 321, 362, 11, 51372], "temperature": 0.0, "avg_logprob": -0.10460576057434082, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0018101363675668836}, {"id": 5292, "seek": 2369444, "start": 23714.6, "end": 23719.879999999997, "text": " which in this case is three, and perform that in the environment. So tell our agent to take", "tokens": [51372, 597, 294, 341, 1389, 307, 1045, 11, 293, 2042, 300, 294, 264, 2823, 13, 407, 980, 527, 9461, 281, 747, 51636], "temperature": 0.0, "avg_logprob": -0.10460576057434082, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0018101363675668836}, {"id": 5293, "seek": 2371988, "start": 23719.88, "end": 23725.16, "text": " this action in the environment and return to us a bunch of information. So the first thing is the", "tokens": [50364, 341, 3069, 294, 264, 2823, 293, 2736, 281, 505, 257, 3840, 295, 1589, 13, 407, 264, 700, 551, 307, 264, 50628], "temperature": 0.0, "avg_logprob": -0.10731153320847896, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.039633918553590775}, {"id": 5294, "seek": 2371988, "start": 23725.16, "end": 23730.2, "text": " observation, which essentially means what state do we move into next? So I could call this", "tokens": [50628, 14816, 11, 597, 4476, 1355, 437, 1785, 360, 321, 1286, 666, 958, 30, 407, 286, 727, 818, 341, 50880], "temperature": 0.0, "avg_logprob": -0.10731153320847896, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.039633918553590775}, {"id": 5295, "seek": 2371988, "start": 23731.64, "end": 23737.960000000003, "text": " new underserved state reward is what reward did we receive by taking that action? So this will", "tokens": [50952, 777, 16692, 6913, 1785, 7782, 307, 437, 7782, 630, 321, 4774, 538, 1940, 300, 3069, 30, 407, 341, 486, 51268], "temperature": 0.0, "avg_logprob": -0.10731153320847896, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.039633918553590775}, {"id": 5296, "seek": 2371988, "start": 23737.960000000003, "end": 23743.16, "text": " be some value right in our in this case, the reward is either one or zero. But that's not", "tokens": [51268, 312, 512, 2158, 558, 294, 527, 294, 341, 1389, 11, 264, 7782, 307, 2139, 472, 420, 4018, 13, 583, 300, 311, 406, 51528], "temperature": 0.0, "avg_logprob": -0.10731153320847896, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.039633918553590775}, {"id": 5297, "seek": 2371988, "start": 23743.16, "end": 23748.760000000002, "text": " that important to understand. And then we have a bool of done, which tells us did we lose the game", "tokens": [51528, 300, 1021, 281, 1223, 13, 400, 550, 321, 362, 257, 748, 401, 295, 1096, 11, 597, 5112, 505, 630, 321, 3624, 264, 1216, 51808], "temperature": 0.0, "avg_logprob": -0.10731153320847896, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.039633918553590775}, {"id": 5298, "seek": 2374876, "start": 23748.76, "end": 23754.12, "text": " or did we win the game? Yes or no. So true. So if this is true, what this means is we need to", "tokens": [50364, 420, 630, 321, 1942, 264, 1216, 30, 1079, 420, 572, 13, 407, 2074, 13, 407, 498, 341, 307, 2074, 11, 437, 341, 1355, 307, 321, 643, 281, 50632], "temperature": 0.0, "avg_logprob": -0.09671986004537787, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.010986131615936756}, {"id": 5299, "seek": 2374876, "start": 23754.12, "end": 23759.64, "text": " reset the environment because our agent either lost or won and is no longer in a valid state in", "tokens": [50632, 14322, 264, 2823, 570, 527, 9461, 2139, 2731, 420, 1582, 293, 307, 572, 2854, 294, 257, 7363, 1785, 294, 50908], "temperature": 0.0, "avg_logprob": -0.09671986004537787, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.010986131615936756}, {"id": 5300, "seek": 2374876, "start": 23759.64, "end": 23764.519999999997, "text": " the environment. Info gives us a little bit of information. It's not showing me anything here.", "tokens": [50908, 264, 2823, 13, 11537, 78, 2709, 505, 257, 707, 857, 295, 1589, 13, 467, 311, 406, 4099, 385, 1340, 510, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09671986004537787, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.010986131615936756}, {"id": 5301, "seek": 2374876, "start": 23764.519999999997, "end": 23768.92, "text": " We're not going to use info throughout this, but figured I'd let you know that now in VDOT", "tokens": [51152, 492, 434, 406, 516, 281, 764, 13614, 3710, 341, 11, 457, 8932, 286, 1116, 718, 291, 458, 300, 586, 294, 691, 35, 5068, 51372], "temperature": 0.0, "avg_logprob": -0.09671986004537787, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.010986131615936756}, {"id": 5302, "seek": 2374876, "start": 23768.92, "end": 23773.879999999997, "text": " render, I'll actually render this for you and show you renders a graphical user interface that", "tokens": [51372, 15529, 11, 286, 603, 767, 15529, 341, 337, 291, 293, 855, 291, 6125, 433, 257, 35942, 4195, 9226, 300, 51620], "temperature": 0.0, "avg_logprob": -0.09671986004537787, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.010986131615936756}, {"id": 5303, "seek": 2377388, "start": 23773.88, "end": 23778.36, "text": " shows you the environment. Now, if you use this while you're training, so you actually watch", "tokens": [50364, 3110, 291, 264, 2823, 13, 823, 11, 498, 291, 764, 341, 1339, 291, 434, 3097, 11, 370, 291, 767, 1159, 50588], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5304, "seek": 2377388, "start": 23778.36, "end": 23782.68, "text": " the agent do the training, which is what you can do with this, it slows it down drastically,", "tokens": [50588, 264, 9461, 360, 264, 3097, 11, 597, 307, 437, 291, 393, 360, 365, 341, 11, 309, 35789, 309, 760, 29673, 11, 50804], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5305, "seek": 2377388, "start": 23782.68, "end": 23786.52, "text": " like probably by, you know, 10 or 20 times, because it actually needs to draw the stuff on", "tokens": [50804, 411, 1391, 538, 11, 291, 458, 11, 1266, 420, 945, 1413, 11, 570, 309, 767, 2203, 281, 2642, 264, 1507, 322, 50996], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5306, "seek": 2377388, "start": 23786.52, "end": 23790.280000000002, "text": " the screen. But you know, you can use it if you want. So this is what our frozen lake example", "tokens": [50996, 264, 2568, 13, 583, 291, 458, 11, 291, 393, 764, 309, 498, 291, 528, 13, 407, 341, 307, 437, 527, 12496, 11001, 1365, 51184], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5307, "seek": 2377388, "start": 23790.280000000002, "end": 23794.36, "text": " looks like. You can see that the highlighted square is where our agent is. And in this case,", "tokens": [51184, 1542, 411, 13, 509, 393, 536, 300, 264, 17173, 3732, 307, 689, 527, 9461, 307, 13, 400, 294, 341, 1389, 11, 51388], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5308, "seek": 2377388, "start": 23794.36, "end": 23802.2, "text": " we have four different blocks. We have SFH and G. So S stands for start F stands for frozen,", "tokens": [51388, 321, 362, 1451, 819, 8474, 13, 492, 362, 31095, 39, 293, 460, 13, 407, 318, 7382, 337, 722, 479, 7382, 337, 12496, 11, 51780], "temperature": 0.0, "avg_logprob": -0.09223809942498908, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.28134268522262573}, {"id": 5309, "seek": 2380220, "start": 23802.2, "end": 23806.760000000002, "text": " because this is a frozen lake. And the goal is to navigate to the goal without falling in one", "tokens": [50364, 570, 341, 307, 257, 12496, 11001, 13, 400, 264, 3387, 307, 281, 12350, 281, 264, 3387, 1553, 7440, 294, 472, 50592], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5310, "seek": 2380220, "start": 23806.760000000002, "end": 23812.04, "text": " of the holes, which is represented by H. And this here tells us the action that we just took. Now,", "tokens": [50592, 295, 264, 8118, 11, 597, 307, 10379, 538, 389, 13, 400, 341, 510, 5112, 505, 264, 3069, 300, 321, 445, 1890, 13, 823, 11, 50856], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5311, "seek": 2380220, "start": 23812.04, "end": 23818.280000000002, "text": " I guess the starting action is up because that's zero, I believe. But yes, so if we run this a", "tokens": [50856, 286, 2041, 264, 2891, 3069, 307, 493, 570, 300, 311, 4018, 11, 286, 1697, 13, 583, 2086, 11, 370, 498, 321, 1190, 341, 257, 51168], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5312, "seek": 2380220, "start": 23818.280000000002, "end": 23822.280000000002, "text": " bunch of times, we'll see this updating. Unfortunately, this doesn't work very well in", "tokens": [51168, 3840, 295, 1413, 11, 321, 603, 536, 341, 25113, 13, 8590, 11, 341, 1177, 380, 589, 588, 731, 294, 51368], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5313, "seek": 2380220, "start": 23822.280000000002, "end": 23827.08, "text": " Google Collaboratory, the the GUIs. But if you did this in your own command line, and you like", "tokens": [51368, 3329, 44483, 4745, 11, 264, 264, 17917, 6802, 13, 583, 498, 291, 630, 341, 294, 428, 1065, 5622, 1622, 11, 293, 291, 411, 51608], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5314, "seek": 2380220, "start": 23827.08, "end": 23831.4, "text": " did some different steps and rounded it all out, you would see this working properly. Okay,", "tokens": [51608, 630, 512, 819, 4439, 293, 23382, 309, 439, 484, 11, 291, 576, 536, 341, 1364, 6108, 13, 1033, 11, 51824], "temperature": 0.0, "avg_logprob": -0.09648985295862585, "compression_ratio": 1.6746268656716419, "no_speech_prob": 0.01168580912053585}, {"id": 5315, "seek": 2383140, "start": 23831.480000000003, "end": 23835.0, "text": " so now we're on to talking about the frozen lake environment, which is kind of what I just did.", "tokens": [50368, 370, 586, 321, 434, 322, 281, 1417, 466, 264, 12496, 11001, 2823, 11, 597, 307, 733, 295, 437, 286, 445, 630, 13, 50544], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5316, "seek": 2383140, "start": 23835.0, "end": 23838.84, "text": " So now we're just going to move to the example where we actually implement Q learning to", "tokens": [50544, 407, 586, 321, 434, 445, 516, 281, 1286, 281, 264, 1365, 689, 321, 767, 4445, 1249, 2539, 281, 50736], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5317, "seek": 2383140, "start": 23838.84, "end": 23843.4, "text": " essentially solve the problem. How can we train an AI to navigate this environment and get to the", "tokens": [50736, 4476, 5039, 264, 1154, 13, 1012, 393, 321, 3847, 364, 7318, 281, 12350, 341, 2823, 293, 483, 281, 264, 50964], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5318, "seek": 2383140, "start": 23843.4, "end": 23847.960000000003, "text": " start to the goal? How can we do that? Well, we're going to use Q learning. So let's start. So the", "tokens": [50964, 722, 281, 264, 3387, 30, 1012, 393, 321, 360, 300, 30, 1042, 11, 321, 434, 516, 281, 764, 1249, 2539, 13, 407, 718, 311, 722, 13, 407, 264, 51192], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5319, "seek": 2383140, "start": 23847.960000000003, "end": 23852.68, "text": " first thing we need to do is import gym, import numpy, and then create some constants here. So", "tokens": [51192, 700, 551, 321, 643, 281, 360, 307, 974, 9222, 11, 974, 1031, 8200, 11, 293, 550, 1884, 512, 35870, 510, 13, 407, 51428], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5320, "seek": 2383140, "start": 23852.68, "end": 23856.36, "text": " we'll do that. We're going to say the amount of states is equal to the line I showed you before.", "tokens": [51428, 321, 603, 360, 300, 13, 492, 434, 516, 281, 584, 264, 2372, 295, 4368, 307, 2681, 281, 264, 1622, 286, 4712, 291, 949, 13, 51612], "temperature": 0.0, "avg_logprob": -0.07822869748485331, "compression_ratio": 1.8248407643312101, "no_speech_prob": 0.0033763765823096037}, {"id": 5321, "seek": 2385636, "start": 23856.440000000002, "end": 23863.24, "text": " So env dot observation, space dot n, actions is equal to env dot action space n. And then we're", "tokens": [50368, 407, 465, 85, 5893, 14816, 11, 1901, 5893, 297, 11, 5909, 307, 2681, 281, 465, 85, 5893, 3069, 1901, 297, 13, 400, 550, 321, 434, 50708], "temperature": 0.0, "avg_logprob": -0.14747231582115436, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0953250303864479}, {"id": 5322, "seek": 2385636, "start": 23863.24, "end": 23868.68, "text": " going to say Q is equal to NP dot zeros, states and actions. So something I guess I forgot to", "tokens": [50708, 516, 281, 584, 1249, 307, 2681, 281, 38611, 5893, 35193, 11, 4368, 293, 5909, 13, 407, 746, 286, 2041, 286, 5298, 281, 50980], "temperature": 0.0, "avg_logprob": -0.14747231582115436, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0953250303864479}, {"id": 5323, "seek": 2385636, "start": 23868.68, "end": 23873.8, "text": " mention is when we initialize the Q table, we just initialize all blank values or zero values,", "tokens": [50980, 2152, 307, 562, 321, 5883, 1125, 264, 1249, 3199, 11, 321, 445, 5883, 1125, 439, 8247, 4190, 420, 4018, 4190, 11, 51236], "temperature": 0.0, "avg_logprob": -0.14747231582115436, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0953250303864479}, {"id": 5324, "seek": 2385636, "start": 23873.8, "end": 23878.2, "text": " because obviously, at the beginning of our learning, our model or agent doesn't know", "tokens": [51236, 570, 2745, 11, 412, 264, 2863, 295, 527, 2539, 11, 527, 2316, 420, 9461, 1177, 380, 458, 51456], "temperature": 0.0, "avg_logprob": -0.14747231582115436, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0953250303864479}, {"id": 5325, "seek": 2385636, "start": 23878.2, "end": 23881.8, "text": " anything about the environment yet. So we just leave those all blank, which means we're going", "tokens": [51456, 1340, 466, 264, 2823, 1939, 13, 407, 321, 445, 1856, 729, 439, 8247, 11, 597, 1355, 321, 434, 516, 51636], "temperature": 0.0, "avg_logprob": -0.14747231582115436, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0953250303864479}, {"id": 5326, "seek": 2388180, "start": 23881.8, "end": 23886.52, "text": " to more likely be taking random actions at the beginning of our training, trying to explore", "tokens": [50364, 281, 544, 3700, 312, 1940, 4974, 5909, 412, 264, 2863, 295, 527, 3097, 11, 1382, 281, 6839, 50600], "temperature": 0.0, "avg_logprob": -0.09097712726916297, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.013635430485010147}, {"id": 5327, "seek": 2388180, "start": 23886.52, "end": 23891.16, "text": " the environment space more. And then as we get further on and learn more about the environment,", "tokens": [50600, 264, 2823, 1901, 544, 13, 400, 550, 382, 321, 483, 3052, 322, 293, 1466, 544, 466, 264, 2823, 11, 50832], "temperature": 0.0, "avg_logprob": -0.09097712726916297, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.013635430485010147}, {"id": 5328, "seek": 2388180, "start": 23891.16, "end": 23896.84, "text": " those actions will likely be more calculated based on the Q table values. So we print this out,", "tokens": [50832, 729, 5909, 486, 3700, 312, 544, 15598, 2361, 322, 264, 1249, 3199, 4190, 13, 407, 321, 4482, 341, 484, 11, 51116], "temperature": 0.0, "avg_logprob": -0.09097712726916297, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.013635430485010147}, {"id": 5329, "seek": 2388180, "start": 23896.84, "end": 23903.0, "text": " we can see this is the array that we get, we've had to be build a 16 by four, I guess not array,", "tokens": [51116, 321, 393, 536, 341, 307, 264, 10225, 300, 321, 483, 11, 321, 600, 632, 281, 312, 1322, 257, 3165, 538, 1451, 11, 286, 2041, 406, 10225, 11, 51424], "temperature": 0.0, "avg_logprob": -0.09097712726916297, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.013635430485010147}, {"id": 5330, "seek": 2388180, "start": 23903.0, "end": 23907.719999999998, "text": " well, I guess this technically is an array, we'll call it matrix 16 by four. So every single row", "tokens": [51424, 731, 11, 286, 2041, 341, 12120, 307, 364, 10225, 11, 321, 603, 818, 309, 8141, 3165, 538, 1451, 13, 407, 633, 2167, 5386, 51660], "temperature": 0.0, "avg_logprob": -0.09097712726916297, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.013635430485010147}, {"id": 5331, "seek": 2390772, "start": 23907.72, "end": 23911.64, "text": " represents a state, and every single column represents an action that could be taken in", "tokens": [50364, 8855, 257, 1785, 11, 293, 633, 2167, 7738, 8855, 364, 3069, 300, 727, 312, 2726, 294, 50560], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5332, "seek": 2390772, "start": 23911.64, "end": 23915.88, "text": " that state. Alright, so we're going to find some constants here, which we talked about before.", "tokens": [50560, 300, 1785, 13, 2798, 11, 370, 321, 434, 516, 281, 915, 512, 35870, 510, 11, 597, 321, 2825, 466, 949, 13, 50772], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5333, "seek": 2390772, "start": 23915.88, "end": 23920.760000000002, "text": " So we have the gamma, the learning rate, the max amount of steps and the number of episodes. So the", "tokens": [50772, 407, 321, 362, 264, 15546, 11, 264, 2539, 3314, 11, 264, 11469, 2372, 295, 4439, 293, 264, 1230, 295, 9313, 13, 407, 264, 51016], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5334, "seek": 2390772, "start": 23920.760000000002, "end": 23926.120000000003, "text": " number of episodes is actually, how many episodes do you want to train your agent on? So how many", "tokens": [51016, 1230, 295, 9313, 307, 767, 11, 577, 867, 9313, 360, 291, 528, 281, 3847, 428, 9461, 322, 30, 407, 577, 867, 51284], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5335, "seek": 2390772, "start": 23926.120000000003, "end": 23931.56, "text": " times do you want it to run around and explore the environment? That's what episode stands for.", "tokens": [51284, 1413, 360, 291, 528, 309, 281, 1190, 926, 293, 6839, 264, 2823, 30, 663, 311, 437, 3500, 7382, 337, 13, 51556], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5336, "seek": 2390772, "start": 23932.280000000002, "end": 23937.48, "text": " Max steps essentially says, Okay, so if we're in the environment, and we're kind of navigating", "tokens": [51592, 7402, 4439, 4476, 1619, 11, 1033, 11, 370, 498, 321, 434, 294, 264, 2823, 11, 293, 321, 434, 733, 295, 32054, 51852], "temperature": 0.0, "avg_logprob": -0.07636562515707578, "compression_ratio": 1.8012618296529967, "no_speech_prob": 0.014955978840589523}, {"id": 5337, "seek": 2393748, "start": 23937.56, "end": 23941.399999999998, "text": " and moving around, and we haven't died yet, how many steps are we going to let the agent take", "tokens": [50368, 293, 2684, 926, 11, 293, 321, 2378, 380, 4539, 1939, 11, 577, 867, 4439, 366, 321, 516, 281, 718, 264, 9461, 747, 50560], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5338, "seek": 2393748, "start": 23941.399999999998, "end": 23945.8, "text": " before we cut it off? Because what could happen is we could just bounce in between two different", "tokens": [50560, 949, 321, 1723, 309, 766, 30, 1436, 437, 727, 1051, 307, 321, 727, 445, 15894, 294, 1296, 732, 819, 50780], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5339, "seek": 2393748, "start": 23945.8, "end": 23950.76, "text": " states indefinitely. So we need to make sure we have a max steps so that at some point,", "tokens": [50780, 4368, 24162, 10925, 13, 407, 321, 643, 281, 652, 988, 321, 362, 257, 11469, 4439, 370, 300, 412, 512, 935, 11, 51028], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5340, "seek": 2393748, "start": 23950.76, "end": 23955.0, "text": " if the agent is just doing the same thing, we can, you know, end that or if it's like going in", "tokens": [51028, 498, 264, 9461, 307, 445, 884, 264, 912, 551, 11, 321, 393, 11, 291, 458, 11, 917, 300, 420, 498, 309, 311, 411, 516, 294, 51240], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5341, "seek": 2393748, "start": 23955.0, "end": 23961.48, "text": " circles, we can end that and start again with different, you know, Q values. Alright, so episodes,", "tokens": [51240, 13040, 11, 321, 393, 917, 300, 293, 722, 797, 365, 819, 11, 291, 458, 11, 1249, 4190, 13, 2798, 11, 370, 9313, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5342, "seek": 2393748, "start": 23961.48, "end": 23964.84, "text": " yeah, we already talked about that learning rate, we know what that is gamma, we know what that is", "tokens": [51564, 1338, 11, 321, 1217, 2825, 466, 300, 2539, 3314, 11, 321, 458, 437, 300, 307, 15546, 11, 321, 458, 437, 300, 307, 51732], "temperature": 0.0, "avg_logprob": -0.07290122933583716, "compression_ratio": 1.7899686520376175, "no_speech_prob": 0.002631520852446556}, {"id": 5343, "seek": 2396484, "start": 23965.4, "end": 23969.4, "text": " mess with these values as we go through and you'll see the difference it makes in our training.", "tokens": [50392, 2082, 365, 613, 4190, 382, 321, 352, 807, 293, 291, 603, 536, 264, 2649, 309, 1669, 294, 527, 3097, 13, 50592], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5344, "seek": 2396484, "start": 23969.4, "end": 23973.56, "text": " I've actually included a graph down below. So we'll talk about that kind of show us the outcome", "tokens": [50592, 286, 600, 767, 5556, 257, 4295, 760, 2507, 13, 407, 321, 603, 751, 466, 300, 733, 295, 855, 505, 264, 9700, 50800], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5345, "seek": 2396484, "start": 23973.56, "end": 23981.4, "text": " of our training. But learning rate, the higher this is, the faster I believe that it learns. Yes,", "tokens": [50800, 295, 527, 3097, 13, 583, 2539, 3314, 11, 264, 2946, 341, 307, 11, 264, 4663, 286, 1697, 300, 309, 27152, 13, 1079, 11, 51192], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5346, "seek": 2396484, "start": 23981.4, "end": 23985.8, "text": " so a high learning rate means that each update will introduce larger change to the current state.", "tokens": [51192, 370, 257, 1090, 2539, 3314, 1355, 300, 1184, 5623, 486, 5366, 4833, 1319, 281, 264, 2190, 1785, 13, 51412], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5347, "seek": 2396484, "start": 23985.8, "end": 23989.48, "text": " So yeah, so that makes sense based on the equation as well. Just wanted to make sure that I wasn't", "tokens": [51412, 407, 1338, 11, 370, 300, 1669, 2020, 2361, 322, 264, 5367, 382, 731, 13, 1449, 1415, 281, 652, 988, 300, 286, 2067, 380, 51596], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5348, "seek": 2396484, "start": 23989.48, "end": 23994.04, "text": " going crazy there. So let's run this constant block to make sure. And now we're going to talk", "tokens": [51596, 516, 3219, 456, 13, 407, 718, 311, 1190, 341, 5754, 3461, 281, 652, 988, 13, 400, 586, 321, 434, 516, 281, 751, 51824], "temperature": 0.0, "avg_logprob": -0.09738185557913273, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.00555475614964962}, {"id": 5349, "seek": 2399404, "start": 23994.04, "end": 23998.52, "text": " about picking an action. So remember how I said, and I actually wrote them down here,", "tokens": [50364, 466, 8867, 364, 3069, 13, 407, 1604, 577, 286, 848, 11, 293, 286, 767, 4114, 552, 760, 510, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09216070983369472, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.03514022380113602}, {"id": 5350, "seek": 2399404, "start": 23998.52, "end": 24004.52, "text": " there's essentially two things we can do at every, what do we call it, step, right? We can", "tokens": [50588, 456, 311, 4476, 732, 721, 321, 393, 360, 412, 633, 11, 437, 360, 321, 818, 309, 11, 1823, 11, 558, 30, 492, 393, 50888], "temperature": 0.0, "avg_logprob": -0.09216070983369472, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.03514022380113602}, {"id": 5351, "seek": 2399404, "start": 24004.52, "end": 24009.56, "text": " randomly pick a valid action, or we can use the current Q table to find the best action. So how", "tokens": [50888, 16979, 1888, 257, 7363, 3069, 11, 420, 321, 393, 764, 264, 2190, 1249, 3199, 281, 915, 264, 1151, 3069, 13, 407, 577, 51140], "temperature": 0.0, "avg_logprob": -0.09216070983369472, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.03514022380113602}, {"id": 5352, "seek": 2399404, "start": 24009.56, "end": 24013.48, "text": " do we actually implement that into our open AI gym? Well, I just wanted to write a little", "tokens": [51140, 360, 321, 767, 4445, 300, 666, 527, 1269, 7318, 9222, 30, 1042, 11, 286, 445, 1415, 281, 2464, 257, 707, 51336], "temperature": 0.0, "avg_logprob": -0.09216070983369472, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.03514022380113602}, {"id": 5353, "seek": 2399404, "start": 24013.48, "end": 24018.2, "text": " code block here to show you the exact code that will do this for us. So we're going to introduce", "tokens": [51336, 3089, 3461, 510, 281, 855, 291, 264, 1900, 3089, 300, 486, 360, 341, 337, 505, 13, 407, 321, 434, 516, 281, 5366, 51572], "temperature": 0.0, "avg_logprob": -0.09216070983369472, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.03514022380113602}, {"id": 5354, "seek": 2401820, "start": 24018.2, "end": 24025.16, "text": " this new concept or this new, I can almost call it constant, called epsilon. And I think epsilon,", "tokens": [50364, 341, 777, 3410, 420, 341, 777, 11, 286, 393, 1920, 818, 309, 5754, 11, 1219, 17889, 13, 400, 286, 519, 17889, 11, 50712], "temperature": 0.0, "avg_logprob": -0.12526451110839842, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0164021085947752}, {"id": 5355, "seek": 2401820, "start": 24025.16, "end": 24030.52, "text": " I think I spelt this wrong, ep salon. Yeah, that should be how you spell it. So we're going to start", "tokens": [50712, 286, 519, 286, 637, 2018, 341, 2085, 11, 2388, 27768, 13, 865, 11, 300, 820, 312, 577, 291, 9827, 309, 13, 407, 321, 434, 516, 281, 722, 50980], "temperature": 0.0, "avg_logprob": -0.12526451110839842, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0164021085947752}, {"id": 5356, "seek": 2401820, "start": 24030.52, "end": 24034.12, "text": " the epsilon value essentially tells us the percentage chance that we're going to pick a", "tokens": [50980, 264, 17889, 2158, 4476, 5112, 505, 264, 9668, 2931, 300, 321, 434, 516, 281, 1888, 257, 51160], "temperature": 0.0, "avg_logprob": -0.12526451110839842, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0164021085947752}, {"id": 5357, "seek": 2401820, "start": 24034.12, "end": 24039.48, "text": " random action. So here, we're going to use a 90% epsilon, which essentially means that every time", "tokens": [51160, 4974, 3069, 13, 407, 510, 11, 321, 434, 516, 281, 764, 257, 4289, 4, 17889, 11, 597, 4476, 1355, 300, 633, 565, 51428], "temperature": 0.0, "avg_logprob": -0.12526451110839842, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0164021085947752}, {"id": 5358, "seek": 2401820, "start": 24039.48, "end": 24043.96, "text": " we take an action, there's going to be a 90% chance that it's random and 10% chance that we look at", "tokens": [51428, 321, 747, 364, 3069, 11, 456, 311, 516, 281, 312, 257, 4289, 4, 2931, 300, 309, 311, 4974, 293, 1266, 4, 2931, 300, 321, 574, 412, 51652], "temperature": 0.0, "avg_logprob": -0.12526451110839842, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0164021085947752}, {"id": 5359, "seek": 2404396, "start": 24043.96, "end": 24049.719999999998, "text": " the Q table to make that action. Now, we'll reduce this epsilon value as we train, so that", "tokens": [50364, 264, 1249, 3199, 281, 652, 300, 3069, 13, 823, 11, 321, 603, 5407, 341, 17889, 2158, 382, 321, 3847, 11, 370, 300, 50652], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5360, "seek": 2404396, "start": 24049.719999999998, "end": 24053.96, "text": " our model will start being able to explore, you know, as much as it possibly can in the", "tokens": [50652, 527, 2316, 486, 722, 885, 1075, 281, 6839, 11, 291, 458, 11, 382, 709, 382, 309, 6264, 393, 294, 264, 50864], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5361, "seek": 2404396, "start": 24053.96, "end": 24059.16, "text": " environment by just taking random actions. And then after we have enough observations,", "tokens": [50864, 2823, 538, 445, 1940, 4974, 5909, 13, 400, 550, 934, 321, 362, 1547, 18163, 11, 51124], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5362, "seek": 2404396, "start": 24059.16, "end": 24062.92, "text": " and we've explored the environment enough, we'll start to slowly decrease the epsilon,", "tokens": [51124, 293, 321, 600, 24016, 264, 2823, 1547, 11, 321, 603, 722, 281, 5692, 11514, 264, 17889, 11, 51312], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5363, "seek": 2404396, "start": 24062.92, "end": 24068.12, "text": " so that it hopefully finds a more optimal route for things to do. Now, the way we do this is we", "tokens": [51312, 370, 300, 309, 4696, 10704, 257, 544, 16252, 7955, 337, 721, 281, 360, 13, 823, 11, 264, 636, 321, 360, 341, 307, 321, 51572], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5364, "seek": 2404396, "start": 24068.12, "end": 24072.36, "text": " save NP dot random dot uniform zero one, which essentially means pick a random value between", "tokens": [51572, 3155, 38611, 5893, 4974, 5893, 9452, 4018, 472, 11, 597, 4476, 1355, 1888, 257, 4974, 2158, 1296, 51784], "temperature": 0.0, "avg_logprob": -0.07765524089336395, "compression_ratio": 1.750809061488673, "no_speech_prob": 0.02368778921663761}, {"id": 5365, "seek": 2407236, "start": 24072.36, "end": 24079.88, "text": " zero and one is less than epsilon and epsilon like that. I think I'm going to have to change", "tokens": [50364, 4018, 293, 472, 307, 1570, 813, 17889, 293, 17889, 411, 300, 13, 286, 519, 286, 478, 516, 281, 362, 281, 1319, 50740], "temperature": 0.0, "avg_logprob": -0.1342662442632082, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.01098619494587183}, {"id": 5366, "seek": 2407236, "start": 24079.88, "end": 24085.0, "text": " some other stuff, but we'll see, then action equals ENV dot action space dot sample. So", "tokens": [50740, 512, 661, 1507, 11, 457, 321, 603, 536, 11, 550, 3069, 6915, 15244, 53, 5893, 3069, 1901, 5893, 6889, 13, 407, 50996], "temperature": 0.0, "avg_logprob": -0.1342662442632082, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.01098619494587183}, {"id": 5367, "seek": 2407236, "start": 24085.0, "end": 24089.88, "text": " take a random action. That's what this means store what that action is in here. Otherwise,", "tokens": [50996, 747, 257, 4974, 3069, 13, 663, 311, 437, 341, 1355, 3531, 437, 300, 3069, 307, 294, 510, 13, 10328, 11, 51240], "temperature": 0.0, "avg_logprob": -0.1342662442632082, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.01098619494587183}, {"id": 5368, "seek": 2407236, "start": 24089.88, "end": 24097.48, "text": " we're going to take the argument max of the state row in the Q table. So what this means is find", "tokens": [51240, 321, 434, 516, 281, 747, 264, 6770, 11469, 295, 264, 1785, 5386, 294, 264, 1249, 3199, 13, 407, 437, 341, 1355, 307, 915, 51620], "temperature": 0.0, "avg_logprob": -0.1342662442632082, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.01098619494587183}, {"id": 5369, "seek": 2407236, "start": 24097.48, "end": 24101.8, "text": " the maximum value in the Q table and tell us what row it's in. So that way we know what", "tokens": [51620, 264, 6674, 2158, 294, 264, 1249, 3199, 293, 980, 505, 437, 5386, 309, 311, 294, 13, 407, 300, 636, 321, 458, 437, 51836], "temperature": 0.0, "avg_logprob": -0.1342662442632082, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.01098619494587183}, {"id": 5370, "seek": 2410180, "start": 24101.8, "end": 24106.68, "text": " action to take. So if we're in row, I guess, not sorry, not row column for in column one,", "tokens": [50364, 3069, 281, 747, 13, 407, 498, 321, 434, 294, 5386, 11, 286, 2041, 11, 406, 2597, 11, 406, 5386, 7738, 337, 294, 7738, 472, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5371, "seek": 2410180, "start": 24106.68, "end": 24110.28, "text": " you know, that's maximum value, take action one, that's what this is saying. So using the Q table", "tokens": [50608, 291, 458, 11, 300, 311, 6674, 2158, 11, 747, 3069, 472, 11, 300, 311, 437, 341, 307, 1566, 13, 407, 1228, 264, 1249, 3199, 50788], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5372, "seek": 2410180, "start": 24110.28, "end": 24115.079999999998, "text": " to pick the best action. Alright, so we don't need to run this because this is just going to be", "tokens": [50788, 281, 1888, 264, 1151, 3069, 13, 2798, 11, 370, 321, 500, 380, 643, 281, 1190, 341, 570, 341, 307, 445, 516, 281, 312, 51028], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5373, "seek": 2410180, "start": 24115.079999999998, "end": 24119.64, "text": " which I just wrote that to show you. Now, how do we update the Q values? Well, this is just", "tokens": [51028, 597, 286, 445, 4114, 300, 281, 855, 291, 13, 823, 11, 577, 360, 321, 5623, 264, 1249, 4190, 30, 1042, 11, 341, 307, 445, 51256], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5374, "seek": 2410180, "start": 24119.64, "end": 24124.04, "text": " following the equation that I showed above. So this is the line of code that does this, I just", "tokens": [51256, 3480, 264, 5367, 300, 286, 4712, 3673, 13, 407, 341, 307, 264, 1622, 295, 3089, 300, 775, 341, 11, 286, 445, 51476], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5375, "seek": 2410180, "start": 24124.04, "end": 24128.04, "text": " want to write it out so you guys could see exactly what each line is doing and kind of explore it", "tokens": [51476, 528, 281, 2464, 309, 484, 370, 291, 1074, 727, 536, 2293, 437, 1184, 1622, 307, 884, 293, 733, 295, 6839, 309, 51676], "temperature": 0.0, "avg_logprob": -0.1045830249786377, "compression_ratio": 1.775, "no_speech_prob": 0.004331186413764954}, {"id": 5376, "seek": 2412804, "start": 24128.04, "end": 24131.96, "text": " for yourself. But essentially, you get the point, you know, you have your learning rate, reward,", "tokens": [50364, 337, 1803, 13, 583, 4476, 11, 291, 483, 264, 935, 11, 291, 458, 11, 291, 362, 428, 2539, 3314, 11, 7782, 11, 50560], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5377, "seek": 2412804, "start": 24131.96, "end": 24137.16, "text": " gamma, take the max, so NP dot max does the same thing as a max function in Python. This is going", "tokens": [50560, 15546, 11, 747, 264, 11469, 11, 370, 38611, 5893, 11469, 775, 264, 912, 551, 382, 257, 11469, 2445, 294, 15329, 13, 639, 307, 516, 50820], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5378, "seek": 2412804, "start": 24137.16, "end": 24142.68, "text": " to take the max value, not the argument max from the next state, right, the new state that we moved", "tokens": [50820, 281, 747, 264, 11469, 2158, 11, 406, 264, 6770, 11469, 490, 264, 958, 1785, 11, 558, 11, 264, 777, 1785, 300, 321, 4259, 51096], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5379, "seek": 2412804, "start": 24142.68, "end": 24148.2, "text": " into. And then subtracting obviously Q state action. Alright, so putting it all together. So", "tokens": [51096, 666, 13, 400, 550, 16390, 278, 2745, 1249, 1785, 3069, 13, 2798, 11, 370, 3372, 309, 439, 1214, 13, 407, 51372], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5380, "seek": 2412804, "start": 24148.2, "end": 24152.36, "text": " now we're actually going to show how we can train and create this Q table and then use that Q table.", "tokens": [51372, 586, 321, 434, 767, 516, 281, 855, 577, 321, 393, 3847, 293, 1884, 341, 1249, 3199, 293, 550, 764, 300, 1249, 3199, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5381, "seek": 2412804, "start": 24153.0, "end": 24157.32, "text": " So this is the pretty much all this code that I have, we've already actually", "tokens": [51612, 407, 341, 307, 264, 1238, 709, 439, 341, 3089, 300, 286, 362, 11, 321, 600, 1217, 767, 51828], "temperature": 0.0, "avg_logprob": -0.09153276056676478, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.050323083996772766}, {"id": 5382, "seek": 2415732, "start": 24157.32, "end": 24161.4, "text": " written at least this block here, that's why I put it in its own block. So just all the constants,", "tokens": [50364, 3720, 412, 1935, 341, 3461, 510, 11, 300, 311, 983, 286, 829, 309, 294, 1080, 1065, 3461, 13, 407, 445, 439, 264, 35870, 11, 50568], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5383, "seek": 2415732, "start": 24161.4, "end": 24165.0, "text": " I've included this render constant to tell us whether we want to draw the environment or not.", "tokens": [50568, 286, 600, 5556, 341, 15529, 5754, 281, 980, 505, 1968, 321, 528, 281, 2642, 264, 2823, 420, 406, 13, 50748], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5384, "seek": 2415732, "start": 24165.0, "end": 24167.88, "text": " In this case, I'm going to leave it false, but you can make it true if you want.", "tokens": [50748, 682, 341, 1389, 11, 286, 478, 516, 281, 1856, 309, 7908, 11, 457, 291, 393, 652, 309, 2074, 498, 291, 528, 13, 50892], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5385, "seek": 2415732, "start": 24167.88, "end": 24172.36, "text": " Episodes, I've left at 1500 for this, if you want to make your model better, typically you", "tokens": [50892, 9970, 271, 4789, 11, 286, 600, 1411, 412, 22671, 337, 341, 11, 498, 291, 528, 281, 652, 428, 2316, 1101, 11, 5850, 291, 51116], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5386, "seek": 2415732, "start": 24172.36, "end": 24177.0, "text": " train it on more episodes, but that's up to you. And now we're going to get into the big chunk", "tokens": [51116, 3847, 309, 322, 544, 9313, 11, 457, 300, 311, 493, 281, 291, 13, 400, 586, 321, 434, 516, 281, 483, 666, 264, 955, 16635, 51348], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5387, "seek": 2415732, "start": 24177.0, "end": 24182.2, "text": " of code, which I'm going to talk about. So what this is going to do, we're going to have a rewards", "tokens": [51348, 295, 3089, 11, 597, 286, 478, 516, 281, 751, 466, 13, 407, 437, 341, 307, 516, 281, 360, 11, 321, 434, 516, 281, 362, 257, 17203, 51608], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5388, "seek": 2415732, "start": 24182.2, "end": 24186.28, "text": " list, which is actually just going to store all the rewards we see, just so I can graph that later", "tokens": [51608, 1329, 11, 597, 307, 767, 445, 516, 281, 3531, 439, 264, 17203, 321, 536, 11, 445, 370, 286, 393, 4295, 300, 1780, 51812], "temperature": 0.0, "avg_logprob": -0.08330631256103516, "compression_ratio": 1.8403361344537814, "no_speech_prob": 0.01032722182571888}, {"id": 5389, "seek": 2418628, "start": 24186.28, "end": 24191.079999999998, "text": " for you guys. Then we're going to say for episode in range episodes. So this is just telling us,", "tokens": [50364, 337, 291, 1074, 13, 1396, 321, 434, 516, 281, 584, 337, 3500, 294, 3613, 9313, 13, 407, 341, 307, 445, 3585, 505, 11, 50604], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5390, "seek": 2418628, "start": 24191.079999999998, "end": 24196.199999999997, "text": " you know, for every episode, let's do the steps I'm about to do. So maximum amount of episodes,", "tokens": [50604, 291, 458, 11, 337, 633, 3500, 11, 718, 311, 360, 264, 4439, 286, 478, 466, 281, 360, 13, 407, 6674, 2372, 295, 9313, 11, 50860], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5391, "seek": 2418628, "start": 24196.199999999997, "end": 24200.12, "text": " which is our training length, essentially, we're going to reset the state, obviously,", "tokens": [50860, 597, 307, 527, 3097, 4641, 11, 4476, 11, 321, 434, 516, 281, 14322, 264, 1785, 11, 2745, 11, 51056], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5392, "seek": 2418628, "start": 24200.12, "end": 24204.44, "text": " which makes sense. So state equals in V dot reset, which will give us the starting state.", "tokens": [51056, 597, 1669, 2020, 13, 407, 1785, 6915, 294, 691, 5893, 14322, 11, 597, 486, 976, 505, 264, 2891, 1785, 13, 51272], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5393, "seek": 2418628, "start": 24205.079999999998, "end": 24209.399999999998, "text": " We're going to say for underscore in range, max steps, which means, okay, we're going to do,", "tokens": [51304, 492, 434, 516, 281, 584, 337, 37556, 294, 3613, 11, 11469, 4439, 11, 597, 1355, 11, 1392, 11, 321, 434, 516, 281, 360, 11, 51520], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5394, "seek": 2418628, "start": 24209.399999999998, "end": 24214.039999999997, "text": " you know, we're going to explore the environment up to maximum steps, we do have a done here,", "tokens": [51520, 291, 458, 11, 321, 434, 516, 281, 6839, 264, 2823, 493, 281, 6674, 4439, 11, 321, 360, 362, 257, 1096, 510, 11, 51752], "temperature": 0.0, "avg_logprob": -0.0970828549615268, "compression_ratio": 1.96113074204947, "no_speech_prob": 0.03513965755701065}, {"id": 5395, "seek": 2421404, "start": 24214.120000000003, "end": 24218.04, "text": " which will actually break the loop if we've reached the goal, which we'll talk about further.", "tokens": [50368, 597, 486, 767, 1821, 264, 6367, 498, 321, 600, 6488, 264, 3387, 11, 597, 321, 603, 751, 466, 3052, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5396, "seek": 2421404, "start": 24218.600000000002, "end": 24222.280000000002, "text": " So the first thing we're going to do is say, if render, you know, render the environment,", "tokens": [50592, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 584, 11, 498, 15529, 11, 291, 458, 11, 15529, 264, 2823, 11, 50776], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5397, "seek": 2421404, "start": 24222.280000000002, "end": 24227.64, "text": " that's pretty straightforward. Otherwise, let's take an action. So for each time step, we need to", "tokens": [50776, 300, 311, 1238, 15325, 13, 10328, 11, 718, 311, 747, 364, 3069, 13, 407, 337, 1184, 565, 1823, 11, 321, 643, 281, 51044], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5398, "seek": 2421404, "start": 24227.64, "end": 24232.2, "text": " take an action. So epsilon, I think is spelled correctly here. Yeah, believe that's right. So", "tokens": [51044, 747, 364, 3069, 13, 407, 17889, 11, 286, 519, 307, 34388, 8944, 510, 13, 865, 11, 1697, 300, 311, 558, 13, 407, 51272], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5399, "seek": 2421404, "start": 24232.2, "end": 24236.280000000002, "text": " I'm going to say action equals in V dot action space, this is already the code we've looked at.", "tokens": [51272, 286, 478, 516, 281, 584, 3069, 6915, 294, 691, 5893, 3069, 1901, 11, 341, 307, 1217, 264, 3089, 321, 600, 2956, 412, 13, 51476], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5400, "seek": 2421404, "start": 24236.280000000002, "end": 24241.16, "text": " And then what we're going to say is next state reward done underscore equals in V dot step", "tokens": [51476, 400, 550, 437, 321, 434, 516, 281, 584, 307, 958, 1785, 7782, 1096, 37556, 6915, 294, 691, 5893, 1823, 51720], "temperature": 0.0, "avg_logprob": -0.11598609870588275, "compression_ratio": 1.7841269841269842, "no_speech_prob": 0.004755003377795219}, {"id": 5401, "seek": 2424116, "start": 24241.16, "end": 24245.72, "text": " action, we've put an underscore here, because we don't really care about this info value. So", "tokens": [50364, 3069, 11, 321, 600, 829, 364, 37556, 510, 11, 570, 321, 500, 380, 534, 1127, 466, 341, 13614, 2158, 13, 407, 50592], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5402, "seek": 2424116, "start": 24245.72, "end": 24249.4, "text": " I'm not going to store it, but we do care about what the next state will be the reward from that", "tokens": [50592, 286, 478, 406, 516, 281, 3531, 309, 11, 457, 321, 360, 1127, 466, 437, 264, 958, 1785, 486, 312, 264, 7782, 490, 300, 50776], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5403, "seek": 2424116, "start": 24249.4, "end": 24254.84, "text": " action. And if we were done or not. So we take that action, that's what does this EMB dot step.", "tokens": [50776, 3069, 13, 400, 498, 321, 645, 1096, 420, 406, 13, 407, 321, 747, 300, 3069, 11, 300, 311, 437, 775, 341, 16237, 33, 5893, 1823, 13, 51048], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5404, "seek": 2424116, "start": 24255.48, "end": 24261.24, "text": " And then what we do is say Q state action, we just update the Q value using the formula that", "tokens": [51080, 400, 550, 437, 321, 360, 307, 584, 1249, 1785, 3069, 11, 321, 445, 5623, 264, 1249, 2158, 1228, 264, 8513, 300, 51368], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5405, "seek": 2424116, "start": 24261.24, "end": 24265.48, "text": " we've talked about. So this is the formula, you can look at it more in depth if you want.", "tokens": [51368, 321, 600, 2825, 466, 13, 407, 341, 307, 264, 8513, 11, 291, 393, 574, 412, 309, 544, 294, 7161, 498, 291, 528, 13, 51580], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5406, "seek": 2424116, "start": 24265.48, "end": 24269.079999999998, "text": " But based on whatever the reward is, you know, that's how we're going to update those Q values.", "tokens": [51580, 583, 2361, 322, 2035, 264, 7782, 307, 11, 291, 458, 11, 300, 311, 577, 321, 434, 516, 281, 5623, 729, 1249, 4190, 13, 51760], "temperature": 0.0, "avg_logprob": -0.07759498444614031, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.004198670387268066}, {"id": 5407, "seek": 2426908, "start": 24269.08, "end": 24274.600000000002, "text": " And after a lot of training, we should have some decent Q values in there. Alright, so then we", "tokens": [50364, 400, 934, 257, 688, 295, 3097, 11, 321, 820, 362, 512, 8681, 1249, 4190, 294, 456, 13, 2798, 11, 370, 550, 321, 50640], "temperature": 0.0, "avg_logprob": -0.09213615285939183, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.0035935116466134787}, {"id": 5408, "seek": 2426908, "start": 24274.600000000002, "end": 24278.68, "text": " set the current state to be the next state. So that when we run this time step again,", "tokens": [50640, 992, 264, 2190, 1785, 281, 312, 264, 958, 1785, 13, 407, 300, 562, 321, 1190, 341, 565, 1823, 797, 11, 50844], "temperature": 0.0, "avg_logprob": -0.09213615285939183, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.0035935116466134787}, {"id": 5409, "seek": 2426908, "start": 24279.320000000003, "end": 24283.4, "text": " now our agent is in the next state, and can start exploring the environment again,", "tokens": [50876, 586, 527, 9461, 307, 294, 264, 958, 1785, 11, 293, 393, 722, 12736, 264, 2823, 797, 11, 51080], "temperature": 0.0, "avg_logprob": -0.09213615285939183, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.0035935116466134787}, {"id": 5410, "seek": 2426908, "start": 24284.04, "end": 24289.0, "text": " in this current, you know, iteration, almost, if that makes sense. So then we say if done,", "tokens": [51112, 294, 341, 2190, 11, 291, 458, 11, 24784, 11, 1920, 11, 498, 300, 1669, 2020, 13, 407, 550, 321, 584, 498, 1096, 11, 51360], "temperature": 0.0, "avg_logprob": -0.09213615285939183, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.0035935116466134787}, {"id": 5411, "seek": 2426908, "start": 24289.0, "end": 24293.640000000003, "text": " so essentially, if the agent died, or if they lost or whatever it was, we're going to append", "tokens": [51360, 370, 4476, 11, 498, 264, 9461, 4539, 11, 420, 498, 436, 2731, 420, 2035, 309, 390, 11, 321, 434, 516, 281, 34116, 51592], "temperature": 0.0, "avg_logprob": -0.09213615285939183, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.0035935116466134787}, {"id": 5412, "seek": 2429364, "start": 24293.64, "end": 24300.6, "text": " whatever reward they got from their last step into the rewards up here. And it's worthy of", "tokens": [50364, 2035, 7782, 436, 658, 490, 641, 1036, 1823, 666, 264, 17203, 493, 510, 13, 400, 309, 311, 14829, 295, 50712], "temperature": 0.0, "avg_logprob": -0.06640359389880472, "compression_ratio": 1.828, "no_speech_prob": 0.0803477093577385}, {"id": 5413, "seek": 2429364, "start": 24300.6, "end": 24306.2, "text": " noting that the way the rewards work here is you get one reward, if you move to a valid block,", "tokens": [50712, 26801, 300, 264, 636, 264, 17203, 589, 510, 307, 291, 483, 472, 7782, 11, 498, 291, 1286, 281, 257, 7363, 3461, 11, 50992], "temperature": 0.0, "avg_logprob": -0.06640359389880472, "compression_ratio": 1.828, "no_speech_prob": 0.0803477093577385}, {"id": 5414, "seek": 2429364, "start": 24306.2, "end": 24311.0, "text": " and you get zero reward, if you die. So every time we move to a valid spot, we get one,", "tokens": [50992, 293, 291, 483, 4018, 7782, 11, 498, 291, 978, 13, 407, 633, 565, 321, 1286, 281, 257, 7363, 4008, 11, 321, 483, 472, 11, 51232], "temperature": 0.0, "avg_logprob": -0.06640359389880472, "compression_ratio": 1.828, "no_speech_prob": 0.0803477093577385}, {"id": 5415, "seek": 2429364, "start": 24311.0, "end": 24315.64, "text": " otherwise we get zero. I'm pretty sure that's the way it works at least. But that's something", "tokens": [51232, 5911, 321, 483, 4018, 13, 286, 478, 1238, 988, 300, 311, 264, 636, 309, 1985, 412, 1935, 13, 583, 300, 311, 746, 51464], "temperature": 0.0, "avg_logprob": -0.06640359389880472, "compression_ratio": 1.828, "no_speech_prob": 0.0803477093577385}, {"id": 5416, "seek": 2429364, "start": 24315.64, "end": 24320.12, "text": " that's important to know. So then what we're going to do is reduce the epsilon if we die,", "tokens": [51464, 300, 311, 1021, 281, 458, 13, 407, 550, 437, 321, 434, 516, 281, 360, 307, 5407, 264, 17889, 498, 321, 978, 11, 51688], "temperature": 0.0, "avg_logprob": -0.06640359389880472, "compression_ratio": 1.828, "no_speech_prob": 0.0803477093577385}, {"id": 5417, "seek": 2432012, "start": 24320.12, "end": 24325.399999999998, "text": " but just a fraction of an amount, you know, 0.001, just so we slowly start decreasing the epsilon", "tokens": [50364, 457, 445, 257, 14135, 295, 364, 2372, 11, 291, 458, 11, 1958, 13, 628, 16, 11, 445, 370, 321, 5692, 722, 23223, 264, 17889, 50628], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5418, "seek": 2432012, "start": 24325.399999999998, "end": 24329.559999999998, "text": " moving in the correct direction. And then we're going to break because we've reached the goals,", "tokens": [50628, 2684, 294, 264, 3006, 3513, 13, 400, 550, 321, 434, 516, 281, 1821, 570, 321, 600, 6488, 264, 5493, 11, 50836], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5419, "seek": 2432012, "start": 24329.559999999998, "end": 24333.32, "text": " print the Q table, and then print the average reward. Now this takes a second to train,", "tokens": [50836, 4482, 264, 1249, 3199, 11, 293, 550, 4482, 264, 4274, 7782, 13, 823, 341, 2516, 257, 1150, 281, 3847, 11, 51024], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5420, "seek": 2432012, "start": 24334.12, "end": 24339.16, "text": " like, you know, a few seconds, really. That one is pretty fast, because I've set this at", "tokens": [51064, 411, 11, 291, 458, 11, 257, 1326, 3949, 11, 534, 13, 663, 472, 307, 1238, 2370, 11, 570, 286, 600, 992, 341, 412, 51316], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5421, "seek": 2432012, "start": 24339.16, "end": 24343.879999999997, "text": " was it 1500. But if you want, you can set this at say 10,000, wait another, you know,", "tokens": [51316, 390, 309, 22671, 13, 583, 498, 291, 528, 11, 291, 393, 992, 341, 412, 584, 1266, 11, 1360, 11, 1699, 1071, 11, 291, 458, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5422, "seek": 2432012, "start": 24343.879999999997, "end": 24348.84, "text": " few minutes or whatever, and then see how much better you can do. So we can see that after that,", "tokens": [51552, 1326, 2077, 420, 2035, 11, 293, 550, 536, 577, 709, 1101, 291, 393, 360, 13, 407, 321, 393, 536, 300, 934, 300, 11, 51800], "temperature": 0.0, "avg_logprob": -0.10941856179461383, "compression_ratio": 1.7389937106918238, "no_speech_prob": 0.008315281011164188}, {"id": 5423, "seek": 2434884, "start": 24348.920000000002, "end": 24356.04, "text": " I received an average reward of 0.28886667. This is actually what the Q table values look like.", "tokens": [50368, 286, 4613, 364, 4274, 7782, 295, 1958, 13, 11205, 16919, 15237, 22452, 13, 639, 307, 767, 437, 264, 1249, 3199, 4190, 574, 411, 13, 50724], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5424, "seek": 2434884, "start": 24356.04, "end": 24360.04, "text": " So all these decimal values after all these updates, I just decided to print them out.", "tokens": [50724, 407, 439, 613, 26601, 4190, 934, 439, 613, 9205, 11, 286, 445, 3047, 281, 4482, 552, 484, 13, 50924], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5425, "seek": 2434884, "start": 24360.04, "end": 24364.04, "text": " And I just want to show you the average reward so that we can compare that to what we can get", "tokens": [50924, 400, 286, 445, 528, 281, 855, 291, 264, 4274, 7782, 370, 300, 321, 393, 6794, 300, 281, 437, 321, 393, 483, 51124], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5426, "seek": 2434884, "start": 24364.04, "end": 24368.12, "text": " from testing or this graph. So now I'm just going to graph this. And we're going to see this is", "tokens": [51124, 490, 4997, 420, 341, 4295, 13, 407, 586, 286, 478, 445, 516, 281, 4295, 341, 13, 400, 321, 434, 516, 281, 536, 341, 307, 51328], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5427, "seek": 2434884, "start": 24368.12, "end": 24371.16, "text": " what the graph so you don't have to really understand this code if you don't want to. But", "tokens": [51328, 437, 264, 4295, 370, 291, 500, 380, 362, 281, 534, 1223, 341, 3089, 498, 291, 500, 380, 528, 281, 13, 583, 51480], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5428, "seek": 2434884, "start": 24371.16, "end": 24377.48, "text": " this is just graphing the average reward over 100 steps from the beginning to the end. So", "tokens": [51480, 341, 307, 445, 1295, 79, 571, 264, 4274, 7782, 670, 2319, 4439, 490, 264, 2863, 281, 264, 917, 13, 407, 51796], "temperature": 0.0, "avg_logprob": -0.08845128475780219, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.010986169800162315}, {"id": 5429, "seek": 2437748, "start": 24377.48, "end": 24382.36, "text": " essentially, I've been, I've calculated the average of every 100 episodes, and then just", "tokens": [50364, 4476, 11, 286, 600, 668, 11, 286, 600, 15598, 264, 4274, 295, 633, 2319, 9313, 11, 293, 550, 445, 50608], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5430, "seek": 2437748, "start": 24382.36, "end": 24386.84, "text": " graph this on here. We can see that we start off very poorly in terms of reward, because the", "tokens": [50608, 4295, 341, 322, 510, 13, 492, 393, 536, 300, 321, 722, 766, 588, 22271, 294, 2115, 295, 7782, 11, 570, 264, 50832], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5431, "seek": 2437748, "start": 24386.84, "end": 24391.48, "text": " epsilon value is quite high, which means that we're taking, you know, random actions pretty", "tokens": [50832, 17889, 2158, 307, 1596, 1090, 11, 597, 1355, 300, 321, 434, 1940, 11, 291, 458, 11, 4974, 5909, 1238, 51064], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5432, "seek": 2437748, "start": 24391.48, "end": 24395.079999999998, "text": " much all the time. So if we're taking a bunch of random actions, obviously, chances are,", "tokens": [51064, 709, 439, 264, 565, 13, 407, 498, 321, 434, 1940, 257, 3840, 295, 4974, 5909, 11, 2745, 11, 10486, 366, 11, 51244], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5433, "seek": 2437748, "start": 24395.079999999998, "end": 24398.92, "text": " we're probably going to die a lot, we're probably going to get rewards of zeros quite frequently.", "tokens": [51244, 321, 434, 1391, 516, 281, 978, 257, 688, 11, 321, 434, 1391, 516, 281, 483, 17203, 295, 35193, 1596, 10374, 13, 51436], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5434, "seek": 2437748, "start": 24398.92, "end": 24403.56, "text": " And then after we get to about 600 episodes, you can see that six actually represents 600,", "tokens": [51436, 400, 550, 934, 321, 483, 281, 466, 11849, 9313, 11, 291, 393, 536, 300, 2309, 767, 8855, 11849, 11, 51668], "temperature": 0.0, "avg_logprob": -0.06500639844296584, "compression_ratio": 1.7831715210355987, "no_speech_prob": 0.0009399231639690697}, {"id": 5435, "seek": 2440356, "start": 24403.640000000003, "end": 24407.72, "text": " because this is in hundreds, we start to slowly increase. And then actually, we go on a crazy", "tokens": [50368, 570, 341, 307, 294, 6779, 11, 321, 722, 281, 5692, 3488, 13, 400, 550, 767, 11, 321, 352, 322, 257, 3219, 50572], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5436, "seek": 2440356, "start": 24407.72, "end": 24413.960000000003, "text": " increase here, when we start to take values more frequently. So the epsilon is increasing,", "tokens": [50572, 3488, 510, 11, 562, 321, 722, 281, 747, 4190, 544, 10374, 13, 407, 264, 17889, 307, 5662, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5437, "seek": 2440356, "start": 24413.960000000003, "end": 24419.0, "text": " right. And then after we get here, we kind of level off. And this does show a slight decline.", "tokens": [50884, 558, 13, 400, 550, 934, 321, 483, 510, 11, 321, 733, 295, 1496, 766, 13, 400, 341, 775, 855, 257, 4036, 15635, 13, 51136], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5438, "seek": 2440356, "start": 24419.0, "end": 24422.84, "text": " But I guarantee you if we ran this for, you know, like 15,000, it would just go up and down and", "tokens": [51136, 583, 286, 10815, 291, 498, 321, 5872, 341, 337, 11, 291, 458, 11, 411, 2119, 11, 1360, 11, 309, 576, 445, 352, 493, 293, 760, 293, 51328], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5439, "seek": 2440356, "start": 24422.84, "end": 24427.32, "text": " bob up and down. And that's just because even though we have increased the epsilon, there is", "tokens": [51328, 27292, 493, 293, 760, 13, 400, 300, 311, 445, 570, 754, 1673, 321, 362, 6505, 264, 17889, 11, 456, 307, 51552], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5440, "seek": 2440356, "start": 24427.32, "end": 24431.88, "text": " still a chance that we take a random action and you know, gets your reward. So that is pretty", "tokens": [51552, 920, 257, 2931, 300, 321, 747, 257, 4974, 3069, 293, 291, 458, 11, 2170, 428, 7782, 13, 407, 300, 307, 1238, 51780], "temperature": 0.0, "avg_logprob": -0.09485683574543133, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.07584890723228455}, {"id": 5441, "seek": 2443188, "start": 24431.88, "end": 24436.280000000002, "text": " much it for this Q learning example. You know, I mean, that's pretty straightforward", "tokens": [50364, 709, 309, 337, 341, 1249, 2539, 1365, 13, 509, 458, 11, 286, 914, 11, 300, 311, 1238, 15325, 50584], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5442, "seek": 2443188, "start": 24436.84, "end": 24441.56, "text": " to use the Q table. If you actually wanted to say, you know, watch the agent move around the", "tokens": [50612, 281, 764, 264, 1249, 3199, 13, 759, 291, 767, 1415, 281, 584, 11, 291, 458, 11, 1159, 264, 9461, 1286, 926, 264, 50848], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5443, "seek": 2443188, "start": 24441.56, "end": 24446.04, "text": " thing, I'm going to leave that to you guys, because if you can follow what I've just done in here", "tokens": [50848, 551, 11, 286, 478, 516, 281, 1856, 300, 281, 291, 1074, 11, 570, 498, 291, 393, 1524, 437, 286, 600, 445, 1096, 294, 510, 51072], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5444, "seek": 2443188, "start": 24446.04, "end": 24450.600000000002, "text": " and understand this, it's actually quite easy to use the Q table. And I think as like a final,", "tokens": [51072, 293, 1223, 341, 11, 309, 311, 767, 1596, 1858, 281, 764, 264, 1249, 3199, 13, 400, 286, 519, 382, 411, 257, 2572, 11, 51300], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5445, "seek": 2443188, "start": 24450.600000000002, "end": 24455.72, "text": " almost like, you know, trust in you guys, you can figure out how to do that. The hint is essentially", "tokens": [51300, 1920, 411, 11, 291, 458, 11, 3361, 294, 291, 1074, 11, 291, 393, 2573, 484, 577, 281, 360, 300, 13, 440, 12075, 307, 4476, 51556], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5446, "seek": 2443188, "start": 24455.72, "end": 24460.600000000002, "text": " do exactly what I've done in here, except don't update the Q table values, just use the Q table", "tokens": [51556, 360, 2293, 437, 286, 600, 1096, 294, 510, 11, 3993, 500, 380, 5623, 264, 1249, 3199, 4190, 11, 445, 764, 264, 1249, 3199, 51800], "temperature": 0.0, "avg_logprob": -0.06843659684464738, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.05339499190449715}, {"id": 5447, "seek": 2446060, "start": 24460.6, "end": 24465.8, "text": " values already. And that's, you know, pretty much all there is to Q learning. So this has", "tokens": [50364, 4190, 1217, 13, 400, 300, 311, 11, 291, 458, 11, 1238, 709, 439, 456, 307, 281, 1249, 2539, 13, 407, 341, 575, 50624], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5448, "seek": 2446060, "start": 24465.8, "end": 24470.68, "text": " been the reinforcement learning module for this TensorFlow course, which actually is the last", "tokens": [50624, 668, 264, 29280, 2539, 10088, 337, 341, 37624, 1164, 11, 597, 767, 307, 264, 1036, 50868], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5449, "seek": 2446060, "start": 24470.68, "end": 24474.76, "text": " module in this series. Now, I hope you guys have enjoyed up until this point, just an emphasis", "tokens": [50868, 10088, 294, 341, 2638, 13, 823, 11, 286, 1454, 291, 1074, 362, 4626, 493, 1826, 341, 935, 11, 445, 364, 16271, 51072], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5450, "seek": 2446060, "start": 24474.76, "end": 24479.719999999998, "text": " again, this was really just an introduction to reinforcement learning. This technique and this", "tokens": [51072, 797, 11, 341, 390, 534, 445, 364, 9339, 281, 29280, 2539, 13, 639, 6532, 293, 341, 51320], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5451, "seek": 2446060, "start": 24479.719999999998, "end": 24484.44, "text": " problem itself is not very interesting and not, you know, the best way to do things is not the", "tokens": [51320, 1154, 2564, 307, 406, 588, 1880, 293, 406, 11, 291, 458, 11, 264, 1151, 636, 281, 360, 721, 307, 406, 264, 51556], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5452, "seek": 2446060, "start": 24484.44, "end": 24488.44, "text": " most powerful. It's just to get you thinking about how reinforcement learning works. And", "tokens": [51556, 881, 4005, 13, 467, 311, 445, 281, 483, 291, 1953, 466, 577, 29280, 2539, 1985, 13, 400, 51756], "temperature": 0.0, "avg_logprob": -0.06639409443688771, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.009124942123889923}, {"id": 5453, "seek": 2448844, "start": 24488.52, "end": 24492.28, "text": " potentially, if you'd like to look into that more, there's a ton of different resources and,", "tokens": [50368, 7263, 11, 498, 291, 1116, 411, 281, 574, 666, 300, 544, 11, 456, 311, 257, 2952, 295, 819, 3593, 293, 11, 50556], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5454, "seek": 2448844, "start": 24492.28, "end": 24495.719999999998, "text": " you know, things you can look at in terms of reinforcement learning. So that being said,", "tokens": [50556, 291, 458, 11, 721, 291, 393, 574, 412, 294, 2115, 295, 29280, 2539, 13, 407, 300, 885, 848, 11, 50728], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5455, "seek": 2448844, "start": 24495.719999999998, "end": 24498.84, "text": " that has been this module. And now we're going to move into the conclusion, we'll talk about", "tokens": [50728, 300, 575, 668, 341, 10088, 13, 400, 586, 321, 434, 516, 281, 1286, 666, 264, 10063, 11, 321, 603, 751, 466, 50884], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5456, "seek": 2448844, "start": 24498.84, "end": 24502.92, "text": " some next steps and some more things that you guys can look at to improve your machine learning skills.", "tokens": [50884, 512, 958, 4439, 293, 512, 544, 721, 300, 291, 1074, 393, 574, 412, 281, 3470, 428, 3479, 2539, 3942, 13, 51088], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5457, "seek": 2448844, "start": 24506.28, "end": 24512.28, "text": " So finally, after about seven hours of course content, we have reached the conclusion of this", "tokens": [51256, 407, 2721, 11, 934, 466, 3407, 2496, 295, 1164, 2701, 11, 321, 362, 6488, 264, 10063, 295, 341, 51556], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5458, "seek": 2448844, "start": 24512.28, "end": 24517.48, "text": " course. Now what I'm going to do in this last brief short section is just explain to you where", "tokens": [51556, 1164, 13, 823, 437, 286, 478, 516, 281, 360, 294, 341, 1036, 5353, 2099, 3541, 307, 445, 2903, 281, 291, 689, 51816], "temperature": 0.0, "avg_logprob": -0.07162361002680082, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.00028684770222753286}, {"id": 5459, "seek": 2451748, "start": 24517.56, "end": 24522.44, "text": " you can go for some next steps and some further learning with TensorFlow and machine learning", "tokens": [50368, 291, 393, 352, 337, 512, 958, 4439, 293, 512, 3052, 2539, 365, 37624, 293, 3479, 2539, 50612], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5460, "seek": 2451748, "start": 24522.44, "end": 24526.52, "text": " artificial intelligence in general. Now what I'm going to be recommending to you guys is that we", "tokens": [50612, 11677, 7599, 294, 2674, 13, 823, 437, 286, 478, 516, 281, 312, 30559, 281, 291, 1074, 307, 300, 321, 50816], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5461, "seek": 2451748, "start": 24526.52, "end": 24531.56, "text": " look at the TensorFlow website, because they have some amazing guides and resources on here. And in", "tokens": [50816, 574, 412, 264, 37624, 3144, 11, 570, 436, 362, 512, 2243, 17007, 293, 3593, 322, 510, 13, 400, 294, 51068], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5462, "seek": 2451748, "start": 24531.56, "end": 24537.32, "text": " fact, a lot of the examples that we used in our notebooks were based off of or exactly the same", "tokens": [51068, 1186, 11, 257, 688, 295, 264, 5110, 300, 321, 1143, 294, 527, 43782, 645, 2361, 766, 295, 420, 2293, 264, 912, 51356], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5463, "seek": 2451748, "start": 24537.32, "end": 24541.96, "text": " as the original TensorFlow guide. And that's because the code that they have is just very good.", "tokens": [51356, 382, 264, 3380, 37624, 5934, 13, 400, 300, 311, 570, 264, 3089, 300, 436, 362, 307, 445, 588, 665, 13, 51588], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5464, "seek": 2451748, "start": 24541.96, "end": 24547.079999999998, "text": " They're very good and easy to understand examples. And in terms of learning, I find that these", "tokens": [51588, 814, 434, 588, 665, 293, 1858, 281, 1223, 5110, 13, 400, 294, 2115, 295, 2539, 11, 286, 915, 300, 613, 51844], "temperature": 0.0, "avg_logprob": -0.05438832719196645, "compression_ratio": 1.8259493670886076, "no_speech_prob": 0.033079784363508224}, {"id": 5465, "seek": 2454708, "start": 24547.08, "end": 24551.24, "text": " guides are great for people that want to get in quickly, see the examples and then go and do some", "tokens": [50364, 17007, 366, 869, 337, 561, 300, 528, 281, 483, 294, 2661, 11, 536, 264, 5110, 293, 550, 352, 293, 360, 512, 50572], "temperature": 0.0, "avg_logprob": -0.04701509475708008, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.001987637020647526}, {"id": 5466, "seek": 2454708, "start": 24551.24, "end": 24556.52, "text": " research on their own time and understand why they work. So if you're looking for some further steps,", "tokens": [50572, 2132, 322, 641, 1065, 565, 293, 1223, 983, 436, 589, 13, 407, 498, 291, 434, 1237, 337, 512, 3052, 4439, 11, 50836], "temperature": 0.0, "avg_logprob": -0.04701509475708008, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.001987637020647526}, {"id": 5467, "seek": 2454708, "start": 24556.52, "end": 24561.72, "text": " at this point in time, you have gained a very general and broad knowledge of machine learning", "tokens": [50836, 412, 341, 935, 294, 565, 11, 291, 362, 12634, 257, 588, 2674, 293, 4152, 3601, 295, 3479, 2539, 51096], "temperature": 0.0, "avg_logprob": -0.04701509475708008, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.001987637020647526}, {"id": 5468, "seek": 2454708, "start": 24561.72, "end": 24567.24, "text": " and AI, you have some basic skills in a lot of the different areas. And hopefully this has", "tokens": [51096, 293, 7318, 11, 291, 362, 512, 3875, 3942, 294, 257, 688, 295, 264, 819, 3179, 13, 400, 4696, 341, 575, 51372], "temperature": 0.0, "avg_logprob": -0.04701509475708008, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.001987637020647526}, {"id": 5469, "seek": 2454708, "start": 24567.24, "end": 24573.160000000003, "text": " introduced you to a bunch of different concepts and the possibilities of what you are able to do", "tokens": [51372, 7268, 291, 281, 257, 3840, 295, 819, 10392, 293, 264, 12178, 295, 437, 291, 366, 1075, 281, 360, 51668], "temperature": 0.0, "avg_logprob": -0.04701509475708008, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.001987637020647526}, {"id": 5470, "seek": 2457316, "start": 24573.16, "end": 24578.36, "text": " using modules like TensorFlow. Now what I'm going to suggest to all of you is that if you find a", "tokens": [50364, 1228, 16679, 411, 37624, 13, 823, 437, 286, 478, 516, 281, 3402, 281, 439, 295, 291, 307, 300, 498, 291, 915, 257, 50624], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5471, "seek": 2457316, "start": 24578.36, "end": 24583.64, "text": " specific area of machine learning AI that you are very interested in, that you would dial in on", "tokens": [50624, 2685, 1859, 295, 3479, 2539, 7318, 300, 291, 366, 588, 3102, 294, 11, 300, 291, 576, 5502, 294, 322, 50888], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5472, "seek": 2457316, "start": 24583.64, "end": 24588.84, "text": " that area and focus most of your time into learning that, that is because when you get to a point in", "tokens": [50888, 300, 1859, 293, 1879, 881, 295, 428, 565, 666, 2539, 300, 11, 300, 307, 570, 562, 291, 483, 281, 257, 935, 294, 51148], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5473, "seek": 2457316, "start": 24588.84, "end": 24593.32, "text": " machine learning and AI, where you really get specific and pick one kind of strain or one", "tokens": [51148, 3479, 2539, 293, 7318, 11, 689, 291, 534, 483, 2685, 293, 1888, 472, 733, 295, 14249, 420, 472, 51372], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5474, "seek": 2457316, "start": 24593.32, "end": 24597.96, "text": " kind of area, it gets very interesting very quickly. And you can devote most of your time", "tokens": [51372, 733, 295, 1859, 11, 309, 2170, 588, 1880, 588, 2661, 13, 400, 291, 393, 23184, 881, 295, 428, 565, 51604], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5475, "seek": 2457316, "start": 24597.96, "end": 24602.12, "text": " to getting as deep as possible and not specific topic. And that's something that's really cool.", "tokens": [51604, 281, 1242, 382, 2452, 382, 1944, 293, 406, 2685, 4829, 13, 400, 300, 311, 746, 300, 311, 534, 1627, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07889951876739958, "compression_ratio": 1.928813559322034, "no_speech_prob": 0.030210433527827263}, {"id": 5476, "seek": 2460212, "start": 24602.12, "end": 24607.32, "text": " And most people that are experts in AI or machine learning field typically have one area of", "tokens": [50364, 400, 881, 561, 300, 366, 8572, 294, 7318, 420, 3479, 2539, 2519, 5850, 362, 472, 1859, 295, 50624], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5477, "seek": 2460212, "start": 24607.32, "end": 24611.719999999998, "text": " specialization. Now, if you're someone who doesn't care to specialize an area or you just want to", "tokens": [50624, 2121, 2144, 13, 823, 11, 498, 291, 434, 1580, 567, 1177, 380, 1127, 281, 37938, 364, 1859, 420, 291, 445, 528, 281, 50844], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5478, "seek": 2460212, "start": 24611.719999999998, "end": 24616.36, "text": " play around and see some different things, the TensorFlow website is great to really get kind", "tokens": [50844, 862, 926, 293, 536, 512, 819, 721, 11, 264, 37624, 3144, 307, 869, 281, 534, 483, 733, 51076], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5479, "seek": 2460212, "start": 24616.36, "end": 24621.16, "text": " of a general introduction to a lot of different areas and be able to kind of use this code tweak", "tokens": [51076, 295, 257, 2674, 9339, 281, 257, 688, 295, 819, 3179, 293, 312, 1075, 281, 733, 295, 764, 341, 3089, 29879, 51316], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5480, "seek": 2460212, "start": 24621.16, "end": 24625.48, "text": " it a little bit on your own, and implement it into your own projects. And in fact, the next kind", "tokens": [51316, 309, 257, 707, 857, 322, 428, 1065, 11, 293, 4445, 309, 666, 428, 1065, 4455, 13, 400, 294, 1186, 11, 264, 958, 733, 51532], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5481, "seek": 2460212, "start": 24625.48, "end": 24630.28, "text": " of steps and resources I'm going to be showing you here, and involve simply going to the TensorFlow", "tokens": [51532, 295, 4439, 293, 3593, 286, 478, 516, 281, 312, 4099, 291, 510, 11, 293, 9494, 2935, 516, 281, 264, 37624, 51772], "temperature": 0.0, "avg_logprob": -0.07710292644070503, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.009125079959630966}, {"id": 5482, "seek": 2463028, "start": 24630.36, "end": 24634.84, "text": " website, going to the tutorial page, this is very easy to find, I don't even need to link it,", "tokens": [50368, 3144, 11, 516, 281, 264, 7073, 3028, 11, 341, 307, 588, 1858, 281, 915, 11, 286, 500, 380, 754, 643, 281, 2113, 309, 11, 50592], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5483, "seek": 2463028, "start": 24634.84, "end": 24639.16, "text": " you can just search TensorFlow, and you'll find this online. And looking at some more advanced", "tokens": [50592, 291, 393, 445, 3164, 37624, 11, 293, 291, 603, 915, 341, 2950, 13, 400, 1237, 412, 512, 544, 7339, 50808], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5484, "seek": 2463028, "start": 24639.16, "end": 24644.84, "text": " topics that we haven't covered. So we've covered a few of the topics and tutorials that are here,", "tokens": [50808, 8378, 300, 321, 2378, 380, 5343, 13, 407, 321, 600, 5343, 257, 1326, 295, 264, 8378, 293, 17616, 300, 366, 510, 11, 51092], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5485, "seek": 2463028, "start": 24644.84, "end": 24649.239999999998, "text": " I've just kind of modified their version, and thrown out in the notebook and explained it in", "tokens": [51092, 286, 600, 445, 733, 295, 15873, 641, 3037, 11, 293, 11732, 484, 294, 264, 21060, 293, 8825, 309, 294, 51312], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5486, "seek": 2463028, "start": 24649.239999999998, "end": 24653.8, "text": " wars and video content. But if you'd like to move on to say a next step or something very cool,", "tokens": [51312, 13718, 293, 960, 2701, 13, 583, 498, 291, 1116, 411, 281, 1286, 322, 281, 584, 257, 958, 1823, 420, 746, 588, 1627, 11, 51540], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5487, "seek": 2463028, "start": 24653.8, "end": 24659.32, "text": " something I would recommend is doing the deep dream in the generic generative neural network", "tokens": [51540, 746, 286, 576, 2748, 307, 884, 264, 2452, 3055, 294, 264, 19577, 1337, 1166, 18161, 3209, 51816], "temperature": 0.0, "avg_logprob": -0.07997711209485131, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.020329797640442848}, {"id": 5488, "seek": 2465932, "start": 24659.32, "end": 24663.56, "text": " section on the TensorFlow website, being able to make something like this, I think is very cool.", "tokens": [50364, 3541, 322, 264, 37624, 3144, 11, 885, 1075, 281, 652, 746, 411, 341, 11, 286, 519, 307, 588, 1627, 13, 50576], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5489, "seek": 2465932, "start": 24663.56, "end": 24669.239999999998, "text": " And this is an example where you can tweak this a ton by yourself and get some really cool results.", "tokens": [50576, 400, 341, 307, 364, 1365, 689, 291, 393, 29879, 341, 257, 2952, 538, 1803, 293, 483, 512, 534, 1627, 3542, 13, 50860], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5490, "seek": 2465932, "start": 24669.239999999998, "end": 24673.88, "text": " So some things like this are definitely next steps, there's tons and tons of guides and tutorials", "tokens": [50860, 407, 512, 721, 411, 341, 366, 2138, 958, 4439, 11, 456, 311, 9131, 293, 9131, 295, 17007, 293, 17616, 51092], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5491, "seek": 2465932, "start": 24673.88, "end": 24678.36, "text": " on this website, they make it very easy for anyone to get started. And with these guides,", "tokens": [51092, 322, 341, 3144, 11, 436, 652, 309, 588, 1858, 337, 2878, 281, 483, 1409, 13, 400, 365, 613, 17007, 11, 51316], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5492, "seek": 2465932, "start": 24678.36, "end": 24682.2, "text": " what I will say is typically what will end up happening is they just give you the code and", "tokens": [51316, 437, 286, 486, 584, 307, 5850, 437, 486, 917, 493, 2737, 307, 436, 445, 976, 291, 264, 3089, 293, 51508], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5493, "seek": 2465932, "start": 24682.2, "end": 24687.48, "text": " brief explanations of why things work. You should really be researching and looking up some more,", "tokens": [51508, 5353, 28708, 295, 983, 721, 589, 13, 509, 820, 534, 312, 24176, 293, 1237, 493, 512, 544, 11, 51772], "temperature": 0.0, "avg_logprob": -0.05632628011339493, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.013221542350947857}, {"id": 5494, "seek": 2468748, "start": 24687.56, "end": 24691.96, "text": " you know, deep level explanations of why some of these things work as you go through, if you", "tokens": [50368, 291, 458, 11, 2452, 1496, 28708, 295, 983, 512, 295, 613, 721, 589, 382, 291, 352, 807, 11, 498, 291, 50588], "temperature": 0.0, "avg_logprob": -0.058450226090912126, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.01971602626144886}, {"id": 5495, "seek": 2468748, "start": 24691.96, "end": 24697.239999999998, "text": " want to have a firm and great understanding of why the model performs the way that it does.", "tokens": [50588, 528, 281, 362, 257, 6174, 293, 869, 3701, 295, 983, 264, 2316, 26213, 264, 636, 300, 309, 775, 13, 50852], "temperature": 0.0, "avg_logprob": -0.058450226090912126, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.01971602626144886}, {"id": 5496, "seek": 2468748, "start": 24697.239999999998, "end": 24702.36, "text": " So with that being said, I believe I'm going to wrap up the course now. I know you guys can imagine", "tokens": [50852, 407, 365, 300, 885, 848, 11, 286, 1697, 286, 478, 516, 281, 7019, 493, 264, 1164, 586, 13, 286, 458, 291, 1074, 393, 3811, 51108], "temperature": 0.0, "avg_logprob": -0.058450226090912126, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.01971602626144886}, {"id": 5497, "seek": 2468748, "start": 24702.36, "end": 24707.399999999998, "text": " how much work I put into this. So please do leave a like, subscribe to the channel, leave a content,", "tokens": [51108, 577, 709, 589, 286, 829, 666, 341, 13, 407, 1767, 360, 1856, 257, 411, 11, 3022, 281, 264, 2269, 11, 1856, 257, 2701, 11, 51360], "temperature": 0.0, "avg_logprob": -0.058450226090912126, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.01971602626144886}, {"id": 5498, "seek": 2468748, "start": 24707.399999999998, "end": 24712.36, "text": " show your support. This I believe is the largest open source machine learning course in the world", "tokens": [51360, 855, 428, 1406, 13, 639, 286, 1697, 307, 264, 6443, 1269, 4009, 3479, 2539, 1164, 294, 264, 1002, 51608], "temperature": 0.0, "avg_logprob": -0.058450226090912126, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.01971602626144886}, {"id": 5499, "seek": 2471236, "start": 24712.36, "end": 24717.96, "text": " that deals completely with TensorFlow and Python. And I hope that this gave you a lot of knowledge.", "tokens": [50364, 300, 11215, 2584, 365, 37624, 293, 15329, 13, 400, 286, 1454, 300, 341, 2729, 291, 257, 688, 295, 3601, 13, 50644], "temperature": 0.0, "avg_logprob": -0.087373178396652, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.21198345720767975}, {"id": 5500, "seek": 2471236, "start": 24717.96, "end": 24721.96, "text": " So please do give me your feedback down below in the comments. With that being said, again,", "tokens": [50644, 407, 1767, 360, 976, 385, 428, 5824, 760, 2507, 294, 264, 3053, 13, 2022, 300, 885, 848, 11, 797, 11, 50844], "temperature": 0.0, "avg_logprob": -0.087373178396652, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.21198345720767975}, {"id": 5501, "seek": 2471236, "start": 24721.96, "end": 24727.4, "text": " I hope you enjoyed. And I hopefully I will see you again in another tutorial guide or series.", "tokens": [50844, 286, 1454, 291, 4626, 13, 400, 286, 4696, 286, 486, 536, 291, 797, 294, 1071, 7073, 5934, 420, 2638, 13, 51116], "temperature": 0.0, "avg_logprob": -0.087373178396652, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.21198345720767975}], "language": "en"}