start	end	text
0	4320	Hello, everybody, and welcome to an absolutely massive TensorFlow
4320	7800	slash machine learning slash artificial intelligence course.
8000	11000	Now, please stick with me for this short introduction, as I am going to give you a
11000	15040	lot of important information regarding the course concept, the resources for the
15040	18120	course and what you can expect after going through this.
18440	21840	Now, first, I will tell you who this course is aimed for.
22080	25440	So this course is aimed for people that are beginners in machine learning
25440	28440	and artificial intelligence, or maybe have a little bit of understanding
28440	32200	that are trying to get better, but do have a basic fundamental knowledge
32200	34040	of programming and Python.
34320	36880	So this is not a course you're going to take if you haven't done any
36880	40280	programming before, or if you don't know any Python syntax in general.
40640	43840	It's going to be highly advised that you understand the basic syntax
43840	46920	behind Python, as I'm not going to be explaining that throughout this course.
47280	50320	Now, in terms of your instructor for this course, that is going to be me.
50360	51280	My name is Tim.
51280	54960	Some of you may know me as Tech with Tim from my YouTube channel, where I teach
54960	56880	all kinds of different programming topics.
57000	59720	And I've actually been working with Free Code Camp and posted some of my
59720	61480	series on their channel as well.
61880	64720	Now, let's get into the course breakdown and talk about exactly what you're
64720	66880	going to learn and what you can expect from this course.
67120	69920	So as this course is geared towards beginners and people just getting
69920	73040	started in the machine learning and AI world, we're going to start by
73040	76600	breaking down exactly what machine learning and artificial intelligence is.
76720	80040	So talking about what the differences are between them, the different types
80040	83760	of machine learning, reinforcement learning, for example, versus neural
83760	86400	networks versus simple machine learning.
86640	88480	We're going to go through all those different differences.
88680	91720	And then we're going to get into a general introduction of TensorFlow.
92280	94800	Now, for those of you that don't know, TensorFlow is a module
94800	98440	developed and maintained by Google, which can be used within Python to do
98440	101400	a ton of different scientific computing, machine learning and
101400	103160	artificial intelligence applications.
103160	105840	We're going to be working with that through the entire tutorial series.
106080	108800	And after we do that general introduction to TensorFlow, we're going
108800	111040	to get into our core learning algorithms.
111360	114120	Now, these are the learning algorithms that you need to know before we can get
114120	115640	further into machine learning.
115880	117680	They build a really strong foundation.
117840	121280	They're pretty easy to understand and implement, and they're extremely powerful.
121680	124760	After we do that, we're going to get into neural networks, discuss all the
124760	127920	different things that go into how neural networks work, how we can use them
127920	129480	and then do a bunch of different examples.
129760	132560	And then we're going to get into some more complex aspects of machine
132560	135840	learning and artificial intelligence and get to convolutional neural networks,
135840	138600	which can do things like image recognition and detection.
138800	140960	And then we're going to get into recurrent neural networks, which are
140960	145000	going to do things like natural language processing, chatbots, text
145040	147080	processing, all those different kinds of things.
147280	149760	And finally ended off with reinforcement learning.
150040	153320	Now, in terms of resources for this course, there are a ton.
153520	156720	And what we're going to be doing to make this really easy for you and for me
156760	159120	is doing everything through Google Collaboratory.
159280	162200	Now, if you haven't heard of Google Collaboratory, essentially it's a
162200	166720	collaborative coding environment that runs an iPython notebook in the cloud
166960	170560	on a Google machine where you can do all of your machine learning for free.
170800	172600	So you don't need to install any packages.
172600	173760	You don't need to use PIP.
173960	175560	You don't need to get your environment set up.
175640	178840	All you need to do is open a new Google Collaboratory window and you can
178840	179920	start writing code.
180120	181720	And that's what we're going to be doing in this series.
181920	184880	If you look in the description right now, you will see links to all of the
184880	187080	notebooks that I use throughout this guide.
187200	190120	So if there's anything that you want to be cleared up, if you want the code
190120	193320	for yourself, if you want just text based descriptions of the things that I'm
193320	195960	saying, you can click those links and gain access to them.
196160	198440	So with that being said, I'm very excited to get started.
198440	200000	I hope you guys are as well.
200200	202800	And let's go ahead and get into the content.
204640	209760	So in this first section, I'm going to spend a few minutes discussing the
209760	214280	difference between artificial intelligence, neural networks and machine learning.
214560	216840	Now, the reason we need to go into this is because we're going to be covering
216840	218560	all of these topics throughout this course.
218760	221760	So it's vital that you guys understand what these actually mean.
221760	223720	And you can kind of differentiate between them.
223720	225240	So that's what we're going to focus on now.
225520	228240	Now, quick disclaimer here, just so everyone's aware, I'm using something
228240	229440	called Windows, Inc.
229480	231200	This just default comes with Windows.
231400	232880	I have a drawing tabled down here.
232880	235840	And this is what I'm going to be using for some of the explanatory parts where
235840	240000	there's no real coding, just to kind of illustrate some concepts and topics to you.
240200	242200	Now, I have very horrible handwriting.
242240	243920	I'm not artistic whatsoever.
243920	249520	Programming is definitely more of my thing than drawing and doing diagrams and stuff.
249760	250760	But I'm going to try my best.
250760	254360	And this is just the way that I find I can convey information the best to you guys.
254800	259280	So anyways, let's get started and discuss the first topic here, which is artificial intelligence.
259640	262160	Now, artificial intelligence is a huge hype nowadays.
262440	265640	And it's funny because a lot of people actually don't know what this means, or
265640	269360	they try to tell people that what they've created is not artificial intelligence,
269600	271320	when in reality, it actually is.
271920	275480	Now, the kind of formal definition of AI, and I'm just going to read it off
275480	278720	of my slide here to make sure that I'm not messing this up, is the effort
278720	282400	to automate intellectual tasks normally performed by humans.
282760	285120	Now, that's a fairly big definition, right?
285160	287560	What is considered an intellectual task?
287600	290760	And, you know, really, that doesn't help us too much.
290760	294120	So what I'm going to do is bring us back to when AI was created, first created
294280	298200	to kind of explain to you how AI has evolved and what it really started out being.
298600	302640	So back in 1950, there was kind of the question being asked by scientists
302640	306800	and researchers, can computers think, can we get them to figure things out?
306800	308760	Can we get away from just hard coding?
309000	312800	And, you know, having like, can we get a computer to think and it do its own thing?
313440	315000	So that was kind of the question that was asked.
315200	319000	And that's when the term artificial intelligence was kind of coined and created.
319320	323400	Now, back then, AI was simply a predefined set of rules.
323440	327720	So if you're thinking about an AI for maybe like tic-tac-toe or an AI for chess,
327920	332040	all they would have had back then is predefined rules that humans had come up
332040	335920	with and typed into the computer in code, and the computer would simply execute
335920	338200	those set of rules and follow those instructions.
338440	341960	So there was no deep learning, machine learning, crazy algorithms happening.
342040	345480	It was simply if you wanted the computer to do something, you would have to tell
345480	349280	it beforehand, say you're in this position and this happens, do this.
349320	350640	And that's what AI was.
350880	355400	And very good AI was simply just a very good set of rules or a ton of different
355400	358200	rules that humans had implemented into some program.
358440	361800	You can have AI programs that are stretching, you know, half a million lines
361800	365400	of code, just with tons and tons and tons of different rules that have been
365400	366960	created for that AI.
367600	372360	So just be aware that AI does not necessarily mean anything crazy, complex
372360	374080	or super complicated.
374080	377760	But essentially, if you're trying to simulate some intellectual task, like
377760	381680	playing a game that a human would do with a computer, that is considered AI.
381840	385680	So even a very basic artificial intelligence for a tic-tac-toe game
385680	388360	where it plays against you, that is still considered AI.
388400	391360	And if we think of something like Pac-Man, right, where we have, you know,
391360	395040	our little ghost, and this will be my rough sketch of a ghost, and we have our
395040	396720	Pac-Man guy who will just be this.
397080	399720	Well, would we consider this ghost AI?
400320	404360	What it does is it attempts to find and kind of simulate how it would get
404360	405560	to Pac-Man, right?
405760	409120	And the way this works is just using a very basic path finding algorithm.
409280	412520	This is nothing to do with deep learning or machine learning or anything crazy.
412640	415040	But this is still considered artificial intelligence.
415160	418840	The computer is figuring out how it can kind of play and do something
418840	420360	by following an algorithm.
420360	424200	So we don't necessarily need to have anything crazy, stupid, complex to be
424200	428720	considered AI, it simply needs to just be simulating some intellectual human
428720	432200	behavior. That's kind of the definition of artificial intelligence.
432720	436640	Now, obviously today, AI has evolved into a much more complex field where we
436640	439280	now have machine learning and deep learning and all these other techniques,
439520	440920	which is what we're going to talk about now.
441200	444000	So what I want to start by doing is just drawing a circle here.
444480	448240	And I want to label this circle and say AI like that.
448480	451440	So this is going to define AI because everything I'm going to put inside of
451440	453600	here is considered artificial intelligence.
454160	456280	So now let's get into machine learning.
456760	458920	So what I'm going to do is draw another circle inside of here.
459800	463240	And we're going to label this circle ML for machine learning.
463600	466360	Now notice I put this inside of the artificial intelligence circle.
466480	470440	This is because machine learning is a part of artificial intelligence.
471000	472840	Now, what is machine learning?
473320	477880	Well, what we talked about previously was kind of the idea that AI used to just
477880	480040	be a predefined set of rules, right?
480560	484800	Where what would happen is we would feed some data, we would go through the rules
484800	487000	by and then analyze the data with the rules.
487000	489760	And then we'd spit out some output, which would be, you know, what we're going to do.
490080	494400	So in the classic example of chess, say we're in check, well, we pass that board
494400	498080	information to the computer, it looks at its sets of rules, it determines we're in
498080	499880	check, and then it moves us somewhere else.
500280	502680	Now, what is machine learning in contrast to that?
503000	506920	Well, machine learning is kind of the first field that's actually figuring out
506920	508080	the rules for us.
508400	512280	So rather than us hard coding the rules into the computer, what machine learning
512320	516920	attempts to do is take the data and take what the output should be and figure
516920	518040	out the rules for us.
518240	521520	So you'll often hear that, you know, machine learning requires a lot of data
521520	526520	and you need ton of examples and, you know, input data to really train a good
526520	529880	model. Well, the reason for that is because the way that machine learning
529880	532200	works is it generates the rules for us.
532440	535880	We give it some input data, we give it what the output data should be.
536120	540360	And then it looks at that information and figures out what rules can we generate
540560	544760	so that when we look at new data, we can have the best possible output for that.
544960	548440	Now, that's also why a lot of the times machine learning models do not have
548440	552520	a hundred percent accuracy, which means that they may not necessarily get the
552520	554400	correct answer every single time.
554680	558240	And our goal when we create machine learning models is to raise our accuracy
558240	562320	as high as possible, which means it's going to make the fewest mistakes possible.
562360	565360	Because just like a human, you know, our machine learning models, which are
565360	568800	trying to simulate, you know, human behavior can make mistakes.
568920	571920	But to summarize that, essentially, machine learning, the difference
571920	576600	between that and kind of, you know, algorithms and basic artificial intelligence
576880	581000	is the fact that rather get that rather than us, the programmer giving it the
581000	583720	rules, it figures out the rules for us.
583880	587520	And we might not necessarily know explicitly what those rules are when we
587520	589840	look at machine learning and create machine learning models.
590120	593640	But we know that we're giving some input data, we're giving the expected
593640	597640	output data, and then it looks at all of that information, does some algorithms,
597680	602040	which we'll talk about later on that, and figures out the rules for us so that
602040	605520	later when we give it some input data, and we don't know the output data, it
605520	608640	can use those rules that it's figured out from our examples and all that
608640	611280	training data that we gave it to generate some output.
612000	613560	Okay, so that's machine learning.
613960	615800	Now we've covered AI and machine learning.
616040	618960	And now it's time to cover neural networks or deep learning.
619480	622840	Now this circle gets to go right inside of the machine learning right here.
623080	626400	I'm just going to label this one NN, which stands for neural networks.
626840	628920	Now neural networks get a big hype.
628960	631640	They're usually what the first, you know, when you get into machine learning,
631640	634480	you want to learn neural networks, you're kind of like neural networks are
634480	635920	cool, they're capable of a lot.
636520	638160	But let's discuss what these really are.
638400	642400	So the easiest way to define a neural network is it is a form of machine
642400	645920	learning that uses a layered representation of data.
646240	648960	Now we're not going to really understand this completely right now.
648960	652520	But as we get further in that should start to make more sense as a definition.
653000	656600	But what I need to kind of illustrate to you is that in the previous example,
656600	659240	where we just talked about machine learning, essentially what we had is we
659240	661640	had some input bubbles, which I'm going to define as these.
661960	664640	We had some set of rules that is going to be in between here.
664680	665840	And then we had some output.
666160	669280	And what would happen is we feed this input to this set of rules.
670080	671400	Something happens in here.
671560	672720	And then we get some output.
673040	675320	And then that is what, you know, our program does.
675320	676680	So that's what we get from the model.
676720	678600	We pretty much just have two layers.
678640	681560	We have kind of the input layer, the output layer.
681760	684720	And the rules are kind of just what connects those two layers together.
685280	690600	Now in neural networks and what we call deep learning, we have more than two layers.
690600	693560	Now I'm just trying to erase all this quickly so I can show you that.
693880	696840	So let's say, and I'll draw this one another color, because why not?
696880	701000	If we're talking about neural networks, what we might have, and this will vary.
701000	704520	And I'll talk about this in a second, is the fact that we have an input layer,
704520	706040	which will be our first layer of data.
706200	710600	We could have some layers in between this layer that are all connected together.
711160	713240	And then we could have some output layer.
713520	718360	So essentially, what happens is our data is going to be transformed
718360	721480	through different layers, and different things are going to happen.
721480	724680	There's going to be different connections between these layers.
725000	727080	And then eventually we'll reach an output.
727280	731120	Now it's very difficult to explain neural networks without going completely in depth.
731120	733160	So I'll cover a few more notes that I have here.
733640	736840	Essentially, in neural networks, we just have multiple layers.
736840	738320	That's kind of the way to think of them.
738600	742160	And as we see machine learning, you guys should start to understand this more.
742640	745480	But just understand that we're dealing with multiple layers.
745480	750440	And a lot of people actually call this a multi stage information extraction process.
750680	752040	Now, I did not come up with that term.
752040	753720	I think that's from a book or something.
753720	757600	But essentially what ends up happening is we have our data at this first layer,
757600	760920	which is that input information, which we're going to be passing to the model
760920	762440	that we're going to do something with.
762440	765720	It then goes to another layer where it will be transformed.
765720	769880	It will change into something else using a predefined kind of set of
770520	772680	rules and weights that we'll talk about later.
773080	776120	Then it will pass through all of these different layers where different
776120	779280	kind of features of the data, which again, we'll discuss in a second,
779600	783560	will be extracted, will be figured out, will be found until eventually
783560	786560	we reach an output layer where we can kind of combine everything
786560	791040	we've discovered about the data into some kind of output that's meaningful to our program.
791480	794440	So that's kind of the best that I can do to explain neural networks
794440	796160	without going on to a deeper level.
796160	799200	I understand that a lot of you probably don't understand what they are right now.
799200	800480	And that's totally fine.
800480	803520	But just know that there are layered representation of data.
803560	807840	We have multiple layers of information, whereas in standard machine learning,
807840	812000	we only have, you know, one or two layers and an artificial intelligence.
812000	816400	In general, we don't necessarily have to have like a predefined set of layers.
817080	820320	OK, so that is pretty much it for neural networks.
820320	823480	There's one last thing I will say about them is that they're actually not
823480	824920	modeled after the brain.
824920	828400	So a lot of people seem to think that neural networks are modeled after the brain
828400	832880	and the fact that you have neurons firing in your brain, and that can relate to neural networks.
832960	837000	Now, there is a biological inspiration for the name neural networks
837000	840960	in the way that they work from, you know, human biology, but it is not
840960	843320	necessarily modeled about the way that our brain works.
843320	847160	And in fact, we actually don't really know how a lot of the things in our brain
847160	848120	operate and work.
848120	851040	So it would be impossible for us to say that neural networks are modeled
851040	855160	after the brain, because we actually don't know how information is kind of
855160	858880	happens and occurs and transfers through our brain, or at least we don't know
858880	862280	enough to be able to say this is exactly what it is a neural network.
862280	864520	So anyways, that was kind of the last point there.
864520	866920	OK, so now we need to talk about data.
866920	870520	Now, data is the most important part of machine learning and artificial
870520	872400	intelligence neural networks as well.
872400	876960	And it's very important that we understand how important data is and what
876960	879800	the different kind of parts of it are, because they're going to be referenced
879800	882480	a lot in any of the resources that we're using.
882480	885320	Now, what I want to do is just create an example here where I'm going to make a
885320	890600	data set that is about students final grades in like a school system.
890600	893480	So essentially, we're going to make this a very easy example where all we're
893480	896960	going to have for this data set is we're going to have information about students.
896960	901360	So we're going to have their midterm one grade, their midterm two grade, and then
901360	902760	we're going to have their final grade.
902760	906920	So I'm just going to say midterm one.
906920	908720	And again, excuse my handwriting here.
908760	911680	It's not the easiest thing to write with this drawing tablet.
911680	913920	And then I'll just do final.
913920	915560	So this is going to be our data set.
915560	919120	And we'll actually see some similar data sets to this as we go through and do
919120	920760	some examples later on.
920760	925080	So for student one, which we'll just put some students here, we're going to have
925080	928800	their midterm one grade, maybe that's a 70, their midterm two grade, maybe that
928800	929440	was an 80.
929440	933760	And then let's say their final was like their final term grade, not just the
933760	935320	mark on the final exam.
935320	937480	Let's give them a 77.
937520	939720	Now, for midterm one, we can give someone a 60.
939720	941120	Maybe we give them a 90.
941120	945840	And then we determined that the final grade on their exam was let's say an 84.
946200	948440	And then we could do something with maybe a lower grade here.
948440	954560	So 40, 50, and then maybe they got a 38 or something in the final grade.
954880	958880	Now, obviously, we could have some other information here that we're omitting.
958880	961880	Like maybe there was some exam, some assignments, whatever, some other things
961880	963560	they did that contributed to their grade.
963800	967840	But the problem that I want to consider here is the fact that given our midterm
967840	971400	one grade and our midterm two grade and our final grade, how can I use this
971400	974600	information to predict any one of these three columns?
974840	979080	So if I were given a student's midterm one grade, and I were given a student's
979160	982160	final grade, how could I predict their midterm two grade?
983120	986560	So this is where we're going to talk about features and labels.
986760	990680	Now, whatever information we have, that is the input information, which is the
990680	994040	information we will always have that we need to give to the model to get
994040	996320	some output is what we call our features.
996480	999960	So in the example where we're trying to predict midterm two, and let's just do
999960	1001800	this and highlight this in red.
1001800	1006600	So we understand what we would have as our features, our input information
1006600	1010440	are going to be midterm one and final, because this is the information
1010440	1012920	we are going to use to predict something.
1012960	1015800	It is the input, it is what we need to give the model.
1015960	1019440	And if we're training a model to look at midterm one and final grade, whenever
1019440	1022800	we want to make a new prediction, we need to have that information to do so.
1023160	1024720	Now, what's highlighted in red.
1024720	1029320	So this midterm two here is what we would call the label or the output.
1029600	1033800	Now, the label is simply what we are trying to look for or predict.
1033960	1038200	So when we talk about features versus labels, features is our input information,
1038240	1041480	the information that we have that we need to use to make a prediction.
1041760	1045240	And our label is that output information that is just representing, you know,
1045240	1046360	what we're looking for.
1046600	1051160	So when we feed our features to a model, it will give to us a label.
1051280	1053240	And that is kind of the point that we need to understand.
1053520	1054760	So that was the basic here.
1055400	1058240	And now I'm just going to talk a little bit more about data, because we will
1058240	1061720	get into this more as we continue going and about the importance of it.
1062000	1066120	So the reason why data is so important is this is kind of the key thing
1066120	1068120	that we use to create models.
1068320	1072320	So whenever we're doing AI and machine learning, we need data pretty much.
1072320	1075600	Unless you're doing a very specific type of machine learning and artificial
1075640	1077240	intelligence, which we'll talk about later.
1077760	1080240	Now, for most of these models, we need tons of different data.
1080280	1081960	We need tons of different examples.
1082160	1085560	And that's because we know how machine learning works now, which is essentially
1085840	1088960	we're trying to come up with rules for a data set.
1089160	1090360	We have some input information.
1090360	1093400	We have some output information or some features and some labels.
1093640	1096560	We can give that to a model and tell it to start training.
1096680	1100320	And what it will do is come up with rules such that we can just give some
1100320	1102000	features to the model in the future.
1102160	1105520	And then it should be able to give us a pretty good estimate of what the output
1105520	1110240	should be. So when we're training, we have a set of training data.
1110440	1114560	And that is data where we have all of the features and all of the labels.
1114560	1116320	So we have all of this information.
1116800	1120840	Then when we're going to test the model or use the model later on, we would not
1120840	1122440	have this midterm to information.
1122440	1123960	We wouldn't pass this to the model.
1124160	1127440	We would just pass our features, which is midterm one and final.
1127440	1129400	And then we would get the output of midterm two.
1129800	1130960	So I hope that makes sense.
1131200	1133400	That just means data is extremely important.
1133400	1137360	If we're feeding incorrect data or data that we shouldn't be using to the model,
1137520	1139840	that could definitely result in a lot of mistakes.
1140040	1143160	And if we have incorrect output information or incorrect input
1143160	1146080	information, that is going to cause a lot of mistakes as well, because that is
1146080	1150000	essentially what the model is using to learn and to kind of develop and figure
1150000	1152160	out what it's going to do with new input information.
1152520	1154200	So that means that is enough of data.
1154240	1156880	Now let's talk about the different types of machine learning.
1157360	1159760	OK, so now that we've discussed the difference between artificial
1159760	1163840	intelligence, machine learning and neural networks, we have a kind of decent idea
1163840	1167000	about what data is in the difference between features and labels.
1167320	1171400	It's time to talk about the different types of machine learning specifically,
1171760	1176320	which are unsupervised learning, supervised learning and reinforcement learning.
1176600	1179560	Now, these are just the different types of learning, the different types
1179560	1181080	of figuring things out.
1181080	1184520	Now, different kind of algorithms fit into these different categories
1184520	1188440	from within artificial intelligence, within machine learning and within neural networks.
1188880	1191760	So the first one we're going to talk about is supervised learning,
1191760	1193680	which is kind of what we've already discussed.
1193920	1198520	So I'll just write supervised up here again, excuse the handwriting.
1199200	1201000	So supervised learning.
1201000	1202360	Now, what is this?
1202360	1204640	Well, supervised learning is kind of everything we've already learned,
1204640	1207240	which is we have some features.
1207280	1209760	So we'll write our features like this, right?
1210000	1214200	We have some features and those features correspond to some label
1214200	1215640	or potentially labels.
1215640	1217760	Sometimes we might predict more than one information.
1218160	1221840	So when we have this information, we have the features and we have the labels.
1222040	1225360	What we do is we pass this information to some machine learning model.
1225600	1227120	It figures out the rules for us.
1227120	1231040	And then later on, all we need is the features and it will give us some labels
1231040	1232120	using those rules.
1232120	1236280	But essentially, what supervised learning is, is when we have both of this information.
1236680	1240040	The reason it's called supervised is because what ends up happening
1240040	1244200	when we train our machine learning model is we pass the input information.
1244520	1248280	It makes some arbitrary prediction using the rules it already knows.
1248440	1251640	And then it compares that prediction that it made to what the actual
1251640	1253880	prediction is, which is this label.
1254160	1258720	So we supervise the model and we say, OK, so you predicted that the color was red,
1258920	1261720	but really the color of whatever we passed in should have been blue.
1261880	1265240	So we need to tweak you just a little bit so that you get a little bit better
1265240	1266960	and you move in the correct direction.
1266960	1268840	And that's kind of the way that this works.
1268840	1271680	For example, say we're predicting, you know, students final grade.
1271960	1276840	Well, if we predict that the final grade is 76, but the actual grade is 77,
1277040	1279160	we were pretty close, but we're not quite there.
1279320	1282320	So we supervise the model and we say, hey, we're going to tweak you
1282320	1284520	just a little bit, move you in the correct direction.
1284720	1286360	And hopefully we get you to 77.
1286920	1289520	And that is kind of the way to explain this, right?
1289520	1291360	You have the features, you have the labels.
1291520	1295080	When you pass the features, the model has some rules that it's already built.
1295120	1296320	It makes a prediction.
1296480	1300120	And then it compares that prediction to the label and then re tweaks the model
1300200	1304920	and continues doing this with thousands upon thousands upon thousands of pieces
1304920	1308520	of data, until eventually it gets so good that we can stop training it.
1308680	1310600	And that is what supervised learning is.
1310800	1312520	It's the most common type of learning.
1312520	1315240	It's definitely the most applicable in a lot of instances.
1315440	1318600	And most machine learning algorithms that are actually used
1318600	1320560	use a form of supervised machine learning.
1320880	1324240	A lot of people seem to think that this is, you know, a less complicated,
1324240	1327800	less advanced way of doing things that is definitely not true.
1327840	1329920	All of the different methods I'm going to tell you have different
1329920	1331520	advantages and disadvantages.
1331720	1335600	And this has a massive advantage when you have a ton of information
1335600	1338360	and you have the output of that information as well.
1338560	1340600	But sometimes we don't have the luxury of doing that.
1340600	1342760	And that's where we talk about unsupervised learning.
1343120	1345600	So hopefully that made sense for supervised learning.
1346080	1347440	Tried my best to explain that.
1347640	1350200	And now let's go into or sorry for supervised learning.
1350240	1352520	Now let's go into unsupervised learning.
1353200	1356040	So if we know the definition of supervised learning,
1356280	1359400	we should hopefully be able to come up with a definition of unsupervised
1359400	1362400	learning, which is when we only have features.
1362760	1367760	So given a bunch of features like this and absolutely no labels,
1367760	1372400	no output for these features, what we want to do is have the model
1372400	1374320	come up with those labels for us.
1374520	1375480	Now, this is kind of weird.
1375480	1377440	You're kind of like, wait, how does that work?
1377440	1379000	Why would we even want to do that?
1379240	1380840	Well, let's take this for an example.
1381320	1384360	We have some access, some axes of data.
1384360	1387040	Okay, and we have like a two dimensional data point.
1387040	1390600	So I'm just going to call this, let's say X and let's say Y.
1390800	1391200	Okay.
1391600	1394400	And I'm going to just put a bunch of dots on the screen that kind of
1394400	1397360	represents like maybe a scatter plot of some of our different data.
1398360	1401840	And I'm just going to put some dots specifically closer to other ones.
1401840	1404640	Just so you guys kind of get the point of what we're trying to do here.
1404920	1406360	So let's do that.
1406800	1407000	Okay.
1407000	1410520	So let's say I have this data set, this here is what we're working with.
1410560	1412240	And we have these features.
1412600	1415880	The features in this instance are going to be X and Y, right?
1415880	1418320	So X and Y are my features.
1418520	1421880	Now, we don't have any output specifically for these data points.
1422160	1425520	What we actually want to do is we want to create some kind of model
1426160	1430400	that can cluster these data points, which means figure out kind of, you know,
1430440	1434600	unique groups of data and say, okay, so you're in group one, you're in group two,
1434800	1436480	you're in group three, and you're in group four.
1437120	1441360	We may not necessarily know how many groups we have, although sometimes we do.
1441920	1445360	But what we want to do is just group them and kind of say, okay, we want to
1445360	1448680	figure out which ones are similar and we want to combine those together.
1449000	1452160	So hopefully what we would do with an unsupervised machine learning model
1452200	1456000	is pass all of these features and then have the model create kind of these
1456000	1461400	groupings. So like maybe this is a group, maybe this is a group, maybe this is a
1461400	1464600	group, if we were having four groupings, and maybe if we had two groupings,
1464800	1467360	we might get groupings that look something like this, right?
1467760	1472760	And then when we pass a new data point in, that could, we could figure out what
1472760	1476080	group that was a part of by determining, you know, which one it is closer to.
1476720	1478560	Now, this is kind of a rough example.
1478560	1482160	It's hard to again, explain all of these without going very in depth into
1482160	1486000	the specific algorithms, but unsupervised machine learning or just learning
1486000	1489800	in general is when you don't have some output information, you actually want
1489840	1492080	the model to figure out the output for you.
1492320	1495640	You don't really care how it gets there, you just want it to get there.
1495800	1499560	And again, a good example is clustering data points, and we'll talk about some
1499560	1503400	specific applications of when we might even want to use that later on, just
1503400	1506880	understand you have the features, you don't have the labels, and you get the
1506920	1509720	unsupervised model to kind of figure it out for you.
1510120	1513640	Okay, so now our last type, which is very different than the two types I just
1513640	1515760	explained is called reinforcement learning.
1516120	1519080	Now personally, reinforcement learning, and I don't even know if I want to
1519080	1522520	spell this because I feel like I'm going to mess it up.
1523240	1527440	Reinforcement learning is the coolest type of machine learning, in my opinion.
1527480	1531800	And this is when you actually don't have any data, you have what you call an
1531840	1534640	agent, an environment and a reward.
1534880	1539440	I'm going to explain this very briefly with a very, very, very simple example
1539440	1540840	because it's hard to get too far.
1541160	1544320	So let's say we have a very basic game, you know, maybe we made this game
1544320	1548080	ourselves, and essentially, the objective of the game is to get to the flag.
1548320	1549600	Okay, that's all it is.
1549600	1553360	We have some ground, you can move left or right, and we want to get to this
1553360	1558000	flag. Well, we want to train some artificial intelligence, some machine
1558000	1560400	learning model that can figure out how to do this.
1560800	1563640	So what we do is we call this our agent.
1564840	1566640	We call this entire thing.
1566640	1569160	So this whole thing here, the environment.
1569440	1571200	So I guess I could write that here.
1571200	1575600	So n by our meant think I spelled that correctly.
1576040	1577800	And then we have something called a reward.
1578200	1581640	And a reward is essentially what the agent gets when it does something
1581640	1585520	correctly. So let's say the agent takes one step over this way.
1585520	1586920	So let's say he's a new position is here.
1586920	1587880	I just don't want to keep drawing him.
1587880	1588760	So I'm just going to use a dot.
1589400	1591640	Well, he got closer to the flag.
1592000	1595400	So what I'm actually going to do is give him a plus two reward.
1596240	1598240	So let's say he moves again closer to the flag.
1598880	1601960	Maybe I give him now plus one this time he got even closer.
1602760	1605800	And as he gets closer, I give him more and more reward.
1606640	1608720	Now what happens if he moves backwards?
1609200	1611800	So let's erase this and let's say that at some point in time,
1612000	1615680	rather than moving closer to the threat, the flag, he moves backwards.
1616320	1618280	Well, he might get a negative reward.
1619000	1622520	Now, essentially, what the objective of this agent is to do
1623080	1625080	is to maximize its reward.
1625080	1627880	So if you give it a negative reward for moving backwards,
1628160	1629560	it's going to remember that.
1629560	1633080	And it's going to say, OK, at this position here, where I was standing,
1633080	1635880	when I moved backwards, I got a negative reward.
1636280	1640080	So if I get to this position again, I don't want to go backwards anymore.
1640400	1644640	I want to go forwards because that should give me a positive reward.
1645240	1647920	And the whole point of this is we have this agent
1648160	1651840	that starts off with absolutely no idea, no kind of, you know,
1651880	1653800	knowledge of the environment.
1653800	1656080	And what it does is it starts exploring.
1656080	1659160	And it's a mixture of randomly exploring and exploring
1659160	1661720	using kind of some of the things that's figured out so far
1661880	1664000	to try to maximize its reward.
1664240	1667040	So eventually, when the agent gets to the flag,
1667120	1670520	it will have the most the highest possible reward that it can have.
1670840	1674480	And then next time that we plug this agent into the environment,
1674640	1676920	it will know how to get to the flag immediately
1676920	1678600	because it's kind of figured that out.
1678600	1680840	It's determined that in all of these different positions,
1680840	1683480	if I move here, this is the best place to move.
1683480	1685680	So if I get in this position, move there.
1686080	1689200	Now, this is, again, hard to explain without more detailed examples
1689200	1691200	and going more mathematically and all of that.
1691200	1693120	But essentially, just understand we have the agent,
1693120	1697320	which is kind of what the thing is that's moving around in our environment.
1697600	1701880	We have this environment, which is just what the agent can move around in.
1702080	1703400	And then we have a reward.
1703400	1706160	And the reward is what we need to figure out as the programmer,
1706160	1709920	a way to reward the agent correctly so that it gets to the objective
1710240	1712480	in the best possible way.
1712480	1715040	But the agent simply maximizes that reward.
1715120	1717760	So it just figures out where I need to go to maximize that reward.
1717880	1721080	It starts at the beginning, kind of randomly exploring the environment
1721080	1724120	because it doesn't know any of the rewards it gets at any of the positions.
1724360	1726480	And then as it explores some more different areas,
1726480	1729680	it kind of figures out the rules and the way that the environment works
1729880	1732840	and then will determine how to reach the objective,
1732840	1734560	which is whatever it is that it is.
1734560	1735720	This is a very simple example.
1735720	1739120	You could train a reinforcement model to do this and, you know, like half a second, right?
1739360	1741400	But there is way more advanced examples
1741400	1743880	and there's been examples of reinforcement learning,
1744080	1747760	like of AI is pretty much figuring out how to play games together.
1747760	1749200	How to it's it's actually pretty cool.
1749200	1751280	Some of the stuff that reinforcement learning is doing.
1751480	1754240	And it's a really awesome kind of advancement in the field,
1754240	1756400	because it means we don't need all this data anymore.
1756600	1759800	We can just get this to kind of figure out how to do things for us
1759800	1761880	and explore the environment and learn on its own.
1762160	1763760	Now, this can take a really long time.
1763760	1766800	This can take a very short amount of time, really depends on the environment.
1767000	1770280	But a real application of this is training AIs to play games,
1770280	1772840	as you might be able to tell by kind of what I was explaining here.
1773120	1775840	And yeah, so that is kind of the fundamental differences
1775840	1779160	between supervised, unsupervised and reinforcement learning.
1779160	1781960	We're going to cover all three of these topics throughout this course.
1782240	1785560	And it's really interesting to see some of the applications we can actually do with this.
1785800	1789240	So with that being said, I'm going to kind of end what I'm going to call module one,
1789240	1792160	which is just a general overview of the different topics,
1792160	1794640	some definitions and getting a fundamental knowledge.
1794920	1798880	And in the next one, what we're going to be talking about is what TensorFlow is.
1798880	1801800	We're going to get into coding a little bit and we're going to discuss
1802160	1804640	some different aspects of TensorFlow and things we need to know
1804640	1806680	to be able to move forward and do some more advanced things.
1809840	1812560	So now in module two of this course, what we're going to be doing
1812560	1816760	is getting a general introduction to TensorFlow, understanding what a tensor is,
1816960	1819280	understanding shapes and data representation,
1819280	1823000	and then how TensorFlow actually works on a bit of a lower level.
1823240	1826280	This is very important because you can definitely go through and learn
1826280	1829720	how to do machine learning without kind of gaining this information and knowledge.
1829960	1832320	But it makes it a lot more difficult to tweak your models
1832320	1834920	and really understand what's going on if you don't, you know,
1834920	1839080	have that fundamental lower level knowledge of how TensorFlow actually works
1839080	1841760	and operates. So that's exactly what we're going to cover here.
1842080	1844320	Now, for those of you that don't know what TensorFlow is,
1844320	1847400	essentially, this is an open source machine learning library.
1847560	1849240	It's one of the largest ones in the world.
1849240	1853360	It's one of the most well known and it's maintained and supported by Google.
1853720	1858440	Now, TensorFlow essentially allows us to do and create machine learning models
1858440	1861600	and neural networks and all of that without having to have a very complex
1861600	1865800	math background. Now, as we get further in and we start discussing more in detail,
1865800	1868840	how neural networks work in machine learning algorithms actually function,
1869080	1871560	you'll realize there's a lot of math that goes into this.
1871920	1875640	Now, it starts off being very kind of fundamental, like basic calculus
1875640	1879360	and basic linear algebra, and then it gets much more advanced into things
1879360	1883280	like gradient descent and some more regression techniques and classification.
1883680	1888080	And essentially, you know, a lot of us don't know that and we don't really need to know that.
1888280	1890560	So long as we have a basic understanding of it,
1890840	1894560	then we can use the tools that TensorFlow provides for us to create models.
1894560	1896280	And that's exactly what TensorFlow does.
1896640	1900080	Now, what I'm in right now is what I call Google Collaboratory.
1900160	1902080	I'm going to talk about this more in depth in a second.
1902080	1906680	But what I've done for this whole course is I've transcribed very detailed
1907000	1909440	everything that I'm going to be covering through each module.
1909600	1914600	So this is kind of the transcription of module one, which is the introduction to TensorFlow.
1914840	1918200	You can see it's not crazy long, but I wanted to do this so that any of you
1918240	1922640	can follow along with kind of the text base and kind of my lecture notes.
1922640	1925040	I almost want to call them as I go through the different content.
1925320	1928720	So in the description, there will be links to all of these different notebooks.
1928760	1932520	This is in something called Google Collaboratory, which again, we're going to discuss in a second.
1932800	1937240	But you can see here that I have a bunch of text and it gets down to some different coding aspects.
1937480	1941160	And what I'm going to be doing to make sure that I stay on track is simply following along
1941160	1944480	through this, I might deviate slightly, I might go into some other examples.
1944720	1948240	This will be kind of everything that I'm going to be covering through each module.
1948640	1951720	So again, to follow along, click the link in the description.
1952280	1955160	All right. So what can we do with TensorFlow?
1955440	1957840	Well, these are some of the different things I've listed them here.
1957840	1962320	So I don't forget we can do image classification, data clustering, regression,
1962640	1966560	reinforcement learning, natural language processing, and pretty much anything
1966560	1968640	that you can imagine with machine learning.
1969280	1972760	Essentially, what TensorFlow does is gives us a library of tools
1972880	1976800	that allow us to omit having to do these very complicated math operations.
1977280	1978720	It just does them for us.
1978720	1981680	Now, there is a bit that we need to know about them, but nothing too complex.
1982200	1985160	Now, let's talk about how TensorFlow actually works.
1985640	1989400	So TensorFlow has two main components that we need to understand
1989760	1992840	to figure out how operations and math are actually performed.
1993080	1995720	Now, we have something called graphs and sessions.
1996200	2001880	Now, the way that TensorFlow works is it creates a graph of partial computations.
2002200	2004200	Now, I know this is going to sound a little bit complicated.
2004200	2007720	Some of you guys just try to kind of forget about the complex vocabulary
2007720	2012200	and follow along. But essentially, what we do when we write code in TensorFlow
2012200	2014040	is we create a graph.
2014040	2018120	So if I were to create some variable, that variable gets added to the graph.
2018400	2022680	And maybe that variable is the sum or the summation of two other variables.
2023000	2026400	What the graph will define now is say, you know, we have variable one,
2026760	2031000	which is equal to the sum of variable two and variable three.
2031560	2035560	But what we need to understand is that it doesn't actually evaluate that.
2035560	2039360	It simply states that that is the computation that we've defined.
2039760	2044320	So it's almost like writing down an equation without actually performing any math.
2044560	2047240	We kind of just, you know, have that equation there.
2047440	2050840	We know that this is the value, but we haven't evaluated it.
2050840	2053280	So we don't know that the value is like seven per se.
2053440	2057200	We just know that it's the sum of, you know, vector one and vector two.
2057240	2060840	Or it's the sum of this or it's the cross product or the dot product.
2060840	2064080	We just define all of the different partial computations
2064320	2066680	because we haven't evaluated those computation yet.
2066880	2068840	And that is what is stored in the graph.
2069680	2072400	Now, the reason it's called a graph is because different
2072400	2074600	computations can be related to each other.
2074920	2078720	For example, if I want to figure out the value of vector one,
2078920	2082800	but vector one is equal to the value of vector three plus vector four,
2083000	2085840	I need to determine the value of vector three and vector four
2086320	2088240	before I can do that computation.
2088240	2089800	So they're kind of linked together.
2089800	2091840	And I hope that makes a little bit of sense.
2092440	2094200	Now, what is a session?
2094200	2098920	Well, session is essentially a way to execute part or the entire graph.
2099280	2103840	So when we start a session, what we do is we start executing different aspects
2103840	2106960	of the graph. So we start at the lowest level of the graph
2106960	2108880	where nothing is dependent on anything else.
2108880	2112040	We have maybe constant values or something like that.
2112280	2114360	And then we move our way through the graph
2114360	2117960	and start doing all of the different partial computations that we've defined.
2118520	2120200	Now, I hope that this isn't too confusing.
2120200	2121640	I know this is kind of a lot of lingo.
2121640	2123640	You guys will understand this as we go through.
2123840	2126280	And again, you can read through some of these components here
2126280	2129000	that I have in collaboratory, if I'm kind of skipping through anything,
2129000	2131080	or you don't truly understand.
2131320	2133520	But that is the way that graphs and sessions work.
2133880	2135480	We won't go too in depth with them.
2135480	2138160	We do need to understand that that is the way TensorFlow works.
2138160	2142480	And there's some times where we can't use a specific value in our code yet
2142680	2145000	because we haven't evaluated the graph.
2145000	2147640	We haven't created a session and gotten the values yet.
2147840	2150200	Which we might need to do before we can actually, you know,
2150200	2151800	use some specific value.
2151800	2153800	So that's just something to consider.
2153800	2156160	All right, so now we're actually going to get into coding,
2156160	2158560	importing and installing TensorFlow.
2158880	2161720	Now, this is where I'm going to introduce you to Google Collaboratory
2161720	2163520	and explain how you guys can follow along
2163520	2166360	without having to install anything on your computer.
2166520	2169160	And it doesn't matter if you have like a really crappy computer
2169160	2172560	or even if you're on like an iPhone per se, you can actually do this,
2172560	2173760	which is amazing.
2173760	2177120	So all you need to do is Google, Google Collaboratory
2177360	2179600	and create a new notebook.
2179600	2182720	Now, what Google Collaboratory is, is essentially a free
2182720	2185040	Jupyter notebook in the cloud for you.
2185320	2187920	The way this works is you can open up this notebook.
2187920	2190480	You can see this is called I pi NB.
2191200	2192120	I yeah, what is that?
2192120	2195400	I pi NB, which I think just stands for I Python notebook.
2195680	2199760	And what you can do in here is actually write code and write text as well.
2200040	2203760	So this in here is what it's called, you know, Google Collaboratory Notebook.
2203960	2205920	And essentially why it's called a notebook
2205920	2209320	is because not only can you put code, but you can also put notes,
2209320	2212400	which is what I've done here with these specific titles.
2212680	2215280	So you can actually use Markdown inside of this.
2215280	2218720	So if I open up one of these, you can see that I've used Markdown text
2219280	2221280	to actually kind of create these sections.
2221640	2224760	And yeah, that is kind of how Collaboratory works.
2225040	2228680	But what you can do in Collaboratory is forget about having to install
2228680	2231840	all of these modules, they're already installed for you.
2232080	2235400	So what you're actually going to do when you open a Collaboratory window
2235400	2238680	is Google is going to automatically connect you to one of their servers
2238680	2242680	or one of their machines that has all of this stuff done and set up for you.
2242880	2246200	And you can start writing code and executing it off their machine
2246200	2247800	and seeing the result.
2247800	2251760	So for example, if I want to print hello like this,
2251760	2253960	and I'll zoom in a little bit so you guys can read this.
2253960	2257880	All I do is I create a new code block, which I can do by clicking code.
2258480	2260800	Like that, I can delete one like that as well.
2260800	2262920	And I hit run.
2262920	2266040	Now notice, give it a second, it does take longer than typically on your own
2266040	2268720	machine, and we get hello popping up here.
2269000	2272360	So the great thing about Collaboratory is the fact that we can have multiple
2272360	2275760	code blocks and we can run them in whatever sequence we want.
2276040	2278680	So to create another code block, you can just, you know, do another
2278680	2281840	code block from up here or by just by looking down here, you get code
2281840	2284800	and you get text and I can run this in whatever order I want.
2284800	2286840	So I could do like print.
2286840	2290680	Yes, for example, I could run yes, and we'll see the output of yes.
2290680	2294040	And then I could print hello one more time and notice that it's showing me
2294040	2298040	the number on this left hand side here on which these kind of code blocks were
2298040	2302040	run. Now, all of these code blocks can kind of access each other.
2302040	2306320	So for example, I do define funk and we'll just take some parameter H.
2306360	2310880	And all we'll do is just print H. Well, if I create another code block down here,
2310880	2317080	so let's go code, I can call funk with say, hello, make sure I run this block
2317080	2321240	first, so we define the function. Now I'll run funk and notice we get the output
2321240	2324840	hello, so we can access all of the variables, all the functions, anything
2324840	2328240	we've defined in other code blocks from code blocks that are below it or code
2328240	2331600	blocks that have executed after it. Now, another thing that's great about
2331600	2334880	collaboratory is the fact that we can import pretty much any module we can
2334880	2338360	imagine, and we don't need to install it. So I'm not actually going to be going
2338360	2342600	through how to install TensorFlow completely. There is a little bit on how
2342600	2346320	to install TensorFlow on your local machine inside of this notebook, which
2346360	2349280	I'll refer you to. But essentially, if you know how to use pip, it's pretty
2349280	2353000	straightforward, you can pip install TensorFlow, or pip install TensorFlow
2353000	2356720	GPU, if you have a compatible GPU, which you can check from the link that's in
2356720	2361120	this notebook. Now, if I want to import something, what I can do is literally
2361120	2364800	just write the import. So I can say import numpy like this. And usually numpy
2364800	2368320	is a module that you need to install. But we don't need to do that here. It's
2368360	2371240	already installed on the machine. So again, we hook up to those Google
2371240	2375200	servers, we can use their hardware to perform machine learning. And this is
2375200	2378680	awesome. This is amazing. And it gives you performance benefits when you're
2378680	2382240	running on like a lower kind of crappier machine, right? So we can have a look
2382240	2385680	at the RAM in the disk space of our computer, we can see we have 12 gigs of
2385680	2390080	RAM, we're dealing with 107 gigabytes of data on our disk space. And we can
2390080	2394080	obviously, you know, look at that if we want, we can connect to our local
2394080	2396960	runtime, which I believe connects to your local machine. But I'm not going to go
2396960	2400080	through all of that. I just want to show you guys some basic components of
2400080	2403680	collaboratory. Now, some other things that are important to understand is this
2403680	2408840	runtime tab, which you might see me use. So restart runtime essentially clears
2408840	2412800	all of your output, and just restarts whatever's happened. Because the great
2412800	2416640	thing with collaboratory is since I can run specific code blocks, I don't need
2416640	2421080	to execute the entire thing of code every time I want to run something. If I've
2421080	2425520	just made a minor change in one code block, I can just run that code. Sorry, I
2425520	2429160	can just run that code block. I don't need to run everything before it or even
2429160	2433200	everything after it, right? But sometimes you want to restart everything and just
2433240	2437440	rerun everything. So to do that, you click restart runtime, that's just going to
2437440	2441480	clear everything you have. And then restart and run all will restart the
2441480	2446480	runtime as well as run every single block of code you have in sequential order in
2446480	2450200	which it shows up in the thing. So I recommend you guys open up one of these
2450200	2453480	windows. You can obviously follow along with this notebook if you want. But if
2453480	2456880	you want to type it out on your own and kind of mess with it, open up a notebook,
2457120	2461040	save it. It's very easy. And these are again, extremely similar to Jupiter
2461040	2466160	notebooks, Jupiter notebooks, they're pretty much the same. Okay, so that is
2466160	2471200	kind of the Google Collaboratory aspect how to use that. Let's get into importing
2471200	2475160	TensorFlow. Now this is going to be kind of specific to Google Collaboratory. So
2475160	2478320	you can see here, these are kind of the steps we need to follow to import
2478320	2482400	TensorFlow. So since we're working in Google Collaboratory, they have
2482400	2485520	multiple versions of TensorFlow, they have the original version of TensorFlow,
2485520	2490360	which is 1.0, and the 2.0 version. Now to define the fact that we want to use
2490360	2494840	TensorFlow 2.0, just because we're in this notebook, we need to write this line
2494840	2499440	of code at the very beginning of all of our notebooks. So percent TensorFlow
2499440	2503800	underscore version 2.x. Now this is simply just saying we need to use
2503800	2508000	TensorFlow 2.x. So whatever version that is, and this is only required in a
2508000	2511240	notebook, if you're doing this on your local machine in a text editor, you're
2511240	2515080	not going to need to write this. Now once we do that, we typically import
2515120	2520000	TensorFlow as an alias name of TF. Now to do that, we simply import the
2520000	2524040	TensorFlow module, and then we write as TF. If you're on your local machine,
2524040	2527280	again, you're going to need to install TensorFlow first to make sure that
2527280	2530040	you're able to do this. But since we're in Collaboratory, we don't need to do
2530040	2534600	that. Now, since we've defined the fact we're using version 2.x, when we
2534600	2539520	print the TensorFlow version, we can see here that it says version two, which is
2539520	2543680	exactly what we're looking for. And then it says TensorFlow 2.1.0. So make
2543680	2547320	sure that you print your version, you're using version 2.0, because there is a
2547320	2551640	lot of what I'm using in this series that is kind of, if you're in TensorFlow
2551640	2555440	1.0, it's not going to work. So it's new in TensorFlow 2.0, or it's been
2555440	2559160	refactored and the names have been changed. Okay, so now that we've done
2559160	2562280	that, we've imported TensorFlow, we've got this here, and I'm actually going to
2562280	2565600	go to my fresh notebook and just do this. So we'll just copy these lines over
2565600	2569000	just so we have some fresh code, and I don't have all this text that we have to
2569040	2575200	deal with. So let's do this TensorFlow, let's import TensorFlow as TF, and then
2575200	2581840	we can print the TF dot version and have a look at that. So version. Okay, so
2581840	2585160	let's run our code here, we can see TensorFlow is already loaded. Oh, it says
2585160	2588680	1.0. So if you get this error, it's actually good, I ran into this where
2588680	2591920	TensorFlow has already been loaded. All you need to do is just restart your
2591920	2595560	runtime. So I'm going to restart and run all just click Yes. And now we should
2595600	2599880	see that we get that version 2.0. Once this starts running, give it a second
2599880	2605200	TensorFlow 2.0 selected, we're going to import that module. And there we go, we
2605200	2610800	have version 2.0. Okay, so now it's time to talk about tensors. Now, what is a
2610840	2614400	tensor? Now, tensor just immediately seems kind of like a complicated name, you're
2614400	2618840	like, All right, tensor, like this is confusing. But what is it? Well, obviously
2618840	2622600	this is going to be a primary aspect of TensorFlow, considering the name
2622600	2627720	similarities. And essentially, all it is is a vector generalized to higher
2627720	2632120	dimensions. Now, what is a vector? Well, if you've ever done any linear algebra
2632120	2635480	or even some basic kind of vector calculus, you should hopefully know what
2635480	2640000	that is. But essentially, it is kind of a data point is kind of the way that I
2640000	2643920	like to describe it. And the reason we call it a vector is because it doesn't
2643920	2648760	necessarily have a certain coordinate. So like if you're talking about a two
2648760	2652840	dimensional data point, you have, you know, maybe an x and a y value, or like an
2652840	2658000	x one value and an x two value. Now a vector can have any amount of dimensions
2658040	2662000	in it, it could have one dimension, which simply means it's just one number, could
2662000	2665480	have two dimensions, which means we're having two numbers. So like an x and a
2665480	2669600	y value, if we're thinking about a two dimensional graph, we'd have three
2669600	2672960	dimensions, if we're thinking about a three dimensional graph, so that would be
2672960	2676440	three data points, we could have four dimensions, if we're talking about
2676480	2680360	sometimes some image data and some video data, five dimensions, and we can
2680360	2685120	keep going, going, going with vectors. So essentially, what a tensor is, and I'll
2685120	2688160	just read this formal definition to make sure I haven't butchered anything
2688160	2692160	that's from the actual TensorFlow website. A tensor is a generalization of
2692160	2696480	vectors and matrices to potentially higher dimensions, internally TensorFlow
2696480	2700400	represents tensors as n dimensional arrays of base data types. Now we'll
2700400	2705400	understand what that means in a second, but hopefully that makes sense. Now, since
2705400	2709120	tensors are so important to TensorFlow, they're kind of the main object that
2709120	2713040	we're going to be working with, manipulating and viewing. And it's the main
2713040	2717520	object that's passed around through our program. Now, what we can see here is
2717520	2721480	each tensor represents a partially defined computation that will eventually
2721480	2725880	produce a value. So just like we talked about in the graphs and sessions, what
2725880	2729280	we're going to do is when we create our program, we're going to be creating a
2729280	2732520	bunch of tensors and TensorFlow is going to be creating them as well. And those
2732520	2737640	are going to store partially defined computations in the graph. Later, when we
2737640	2741520	actually build the graph and have the session running, we will run different
2741520	2744800	parts of the graph, which means we'll execute different tensors, and be able
2744800	2748720	to get different results from our tensors. Now each tensor has what we call a
2748720	2753640	data type and a shape, and that's we're going to get into now. So a data type is
2753640	2757680	simply what kind of information is stored in the tensor. Now it's very rare that
2757680	2761440	we see any data types different than numbers, although there is the data type
2761480	2764560	of strings and a few others as well. But I haven't included all of them here
2764560	2768840	because they're not that important. But some examples we can see our float 32 in
2768840	2774720	32 string and others. Now the shape is simply the representation of the
2774720	2778600	tensor in terms of what dimension it is. And we'll get some examples because I
2778600	2781600	don't want to explain the shape until we can see some examples to really dial
2781600	2786360	in. But here are some examples of how we would create different tensors. So what
2786360	2792560	you can do is you can simply do TF dot variable. And then you can do the value
2792560	2796920	and the data type that your tensor is. So in this case, we've created a string
2796920	2801960	tensor which stores one string. And it is TF dot strings, we define the data type
2801960	2807360	second, we have a number tensor which stores some integer value. And then that
2807360	2812480	is up type TF int 16. And we have a floating point tensor, which stores a
2812520	2818240	simple floating point. Now these tensors have a shape of I believe it's going to
2818240	2823120	be one, which simply means they are a scalar. Now a scalar value and you might
2823120	2828160	hear me say this a lot simply means just one value. That's all it means. When we
2828160	2832720	talk about like vector values, that typically means more than one value. And
2832720	2836840	we talk about matrices, we're having different it just it goes up but scalar
2836840	2842240	simply means one number. So yeah, that is what we get for the different data
2842280	2845320	types and creating tensors, we're not really going to do this very much in our
2845320	2849440	program. But just for some examples here, that's how we do it. So we've imported
2849440	2851960	them. So I can actually run these. And I mean, we're not going to really get any
2851960	2856280	output by running this code because well, there's nothing to see. But now we're
2856280	2861040	going to talk about the rank slash degree of tensors. So another word for rank is
2861040	2865440	agree. So these are interchangeably. And again, this simply means the the number
2865440	2870840	of dimensions involved in the tensor. So when we create a tensor of rank zero,
2870880	2874800	which is what we've done up here, we call that a scalar. Now the reason this has
2874800	2879760	rank zero is because it's simply one thing, we don't have any dimensions to
2879760	2884320	this, there's like zero dimensionality of that. It was even a word, it's just one
2884320	2890400	value. Whereas here, we have an array. Now when we have an array or a list, we
2890400	2895360	immediately have at least rank one. Now the reason for that is because this
2895360	2899120	array can store more than one value in one dimension, right? So I can do
2899120	2904760	something like test, I could do okay, I could do Tim, which is my name, and we
2904760	2908280	can run this and we're not going to get any output obviously here. But this is
2908280	2914080	what we would call a rank one tensor, because it is simply one list, one array,
2914280	2918840	which means one dimension. And again, you know, that's also like a vector. Now
2918840	2923280	this, what we're looking at here is a rank to tensor. The reason this is a rank
2923320	2927680	to tensor is because we have a list inside of a list, or in this case,
2927720	2932000	multiple lists inside of a list. So the way that you can actually determine the
2932000	2937760	rank of a tensor is the deepest level of a nested list, at least in Python with
2937760	2942600	our representation, that's what that is. So here we can see we have a list inside
2942600	2946320	of a list, and then another list inside of this upper list. So this would give us
2946480	2951240	rank two. And this is what we typically call a matrices. And this again, is going
2951240	2956560	to be of TF dot strings. So that's the data type for this tensor variable. So all
2956600	2960120	of these that we've created are tensors, they have a data type, and they have some
2960120	2963880	rank and some shape, and we're going to talk about the shape in a second. So to
2963880	2968840	determine the rank of a tensor, we can simply use the method TF dot rank. So
2968840	2974160	notice when I run this, we get the shape which is blank of rank to tensor. That's
2974160	2979560	fine. And then we get num pi two, which simply means that this is of rank two. Now
2979560	2984160	if I go for that rank one tensor, and I print this out. So let's have a look at
2984160	2988840	it, we get num pi one here, which is telling us that this is simply of rank
2988920	2992320	one. Now if I want to use one of these ones up here and see what it is, so let's
2992320	2997320	try it, we can do numbers. So TF dot ring numbers. So we'll print that here. And
2997320	3000680	we get num pi zero, because that's rank zero, right? So we'll go back to what we
3000680	3003920	had, which was ranked to tensor. But again, those are kind of the examples we
3003920	3008080	want to look at. Okay, so shapes of a tensor. So this is a little bit different
3008080	3013520	now. What a shape simply tells us is how many items we have in each dimension. So
3013520	3018720	in this case, when we're looking at rank two, tensor dot shape, so we have dot
3018720	3022960	shape here, that's an attribute of all of our tensors, we get two two. Now let's
3022960	3028400	look up here. What we have is Whoa, look at this two, and two. So we have two
3028400	3031120	elements in the first dimension, right, and then two elements in the second
3031120	3035200	dimension. That's pretty much what this is telling us. Now let's look at the rank
3035480	3040960	for the shape of rank one tensor, we get three. So because we only have a rank
3040960	3046560	one, notice we only get one number. Whereas when we had rank two, we got two
3046560	3049880	numbers, and it told us how many elements were in each of these lists, right? So if
3049880	3054480	I go and I add another one here, like that, and we have a look now at the shape.
3055080	3060400	Oops, I got to run this first. So that's something can convert non square to
3060440	3064320	tensor. Ah, sorry, so I need to have a uniform amount of elements in each one
3064320	3068560	here, I can't just do what I did there. So add a third element here. Now what we
3068560	3074120	can do is run this shouldn't get any issues. Let's have a look at the shape and
3074120	3079880	notice we get now two three. So we have two lists, and each of those lists have
3079880	3084000	three elements inside of them. So that's how the shape works. Now I could go ahead
3084040	3089600	and add another list in here if I wanted to and I could say like, okay, okay,
3090960	3095400	okay, so let's run this hopefully no errors. Looks like we're good. Now let's
3095400	3098400	look at the shape again. And now we get a shape of three, three, because we have
3098440	3102640	three interior lists. And in each of those lists, we have three elements. And
3102640	3106880	that is pretty much how that works. Now again, we could go even further here and
3106880	3110760	we could put another list inside of here that would give us a rank three tensor.
3110760	3114520	And we'd have to do that inside of all of these lists. And then what that would
3114520	3118680	give us now would be three numbers representing how many elements we have in
3118720	3124600	each of those different dimensions. Okay, so changing shape. Alright, so this is
3124640	3128000	what we need to do a lot of times when we're dealing with tensors and tensor
3128000	3132360	flow. So essentially, there is many different shapes that can represent the
3132360	3138160	same number of elements. So up here, we have three elements in a rank one
3138160	3143400	tensor. And then here we have nine elements in a rank two tensor. Now there's
3143400	3147600	ways that we can reshape this data so that we have the same amount of
3147600	3151840	elements, but in a different shape. For example, I could flatten this, right,
3151840	3156400	take all of these elements and throw them into a rank one tensor that simply is
3156400	3160840	a length of nine elements. So how do we do that? Well, let me just run this code
3160840	3163520	for us here and have a look at this. So what we've done is we've created tensor
3163520	3168000	one, that is TF dot ones, what this stands for is we're going to create a
3168000	3174880	tensor that simply is populated completely with ones of this shape. So shape one,
3174880	3178560	two, three, which means, you know, that's the shape we're going to get. So let's
3178560	3183520	print this out and look at tensor one, just so I can better illustrate this. So
3183520	3189640	tensor one, look at the shape that we have one, two, three, right? So we have one
3189640	3193440	interior list, which we're looking at here. And then we have two lists inside
3193440	3197200	of that list. And then each of those lists, we have three elements. So that's
3197200	3201800	the shape we just defined. Now we have six elements inside of here. So there
3201800	3205680	must be a way that we can reshape this data to have six elements, but in a
3205720	3210560	different shape. In fact, what we can do is reshape this into a two, three, one
3210560	3214200	shape, where we're going to have two lists, right? We're going to have three
3214200	3216800	inside of those. And then inside of each of those, we're going to have one
3216800	3220600	element. So let's have a look at that one. So let's have a look at tensor two.
3220600	3223360	Actually, what am I doing? We print all we can print all of them here. So let's
3223360	3226680	just print them and have a look at them. So when we look at tensor one, we saw
3226680	3230840	this was a shape. And now we look at this tensor two. And we can see that we
3230840	3235240	have two lists, right? Inside of each of those lists, we have three lists. And
3235240	3239720	inside of each of those lists, we have one element. Now, finally, our tensor
3239720	3245160	three is a shape of three negative one. Well, what is negative one? When we put
3245160	3249840	negative one here, what this does is infer what this number actually needs to
3249840	3254880	be. So if we define an initial shape of three, what this does is say, Okay, we're
3254880	3259640	going to have three lists. That's our first level. And then we need to figure
3259640	3263240	out based on how many elements we have in this reshape, which is the method we're
3263240	3266440	using, which I didn't even talk about, which we'll go into a second, what this
3266440	3270840	next dimension should be. Now, obviously, this is going to need to be three. So three
3270840	3274240	three, right, because we're going to have three lists inside of each of those lists
3274240	3277000	we need to have. Or actually, is that correct? Let's see if that's even the
3277000	3281400	shape, three, two, my bad. So this actually needs to change to three, two, I
3281400	3285000	don't know why I wrote three, three there. But you get the point, right? So what
3285000	3288160	this does is we have three lists, we have six elements, this number obviously needs
3288200	3291720	to be two, because well, three times two is going to give us six. And that is
3291720	3295360	essentially how you can determine how many elements are actually in a tensor by
3295360	3299600	just looking at its shape. Now, this is the reshape method, where all we need to
3299600	3303680	do is call tf dot reshape, give the tensor and give the shape we want to change
3303680	3308120	it to. So long as that's a valid shape. And when we multiply all of the numbers
3308120	3311920	in here, it's equal to the number of elements in this tensor that will reshape
3311920	3316280	it for us and give us that new shaped data. This is very useful. We'll use this
3316320	3319680	actually a lot as we go through TensorFlow. So make sure you're kind of
3319680	3323880	familiar with how that works. All right. So now we're moving on to types of
3323920	3328680	tensors. So there is a bunch of different types of tensors that we can use. So
3328680	3333400	far, the only one we've looked at is variable. So we've created tf dot
3333400	3336680	variables and kind of just hard coded our own tensors. We're not really going to
3336680	3340960	do that very much. But just for that example. So we have these different
3340960	3345640	types, we have constant placeholder sparse tensor variable. And there's actually
3345640	3349960	a few other ones as well. Now, we're not going to really talk about these two
3350000	3353600	that much, although constant and variable are important to understand the
3353600	3357760	difference between. So we can read this says with the exception of variable, all
3357760	3361080	of these tensors are immutable, meaning their value may not change during
3361120	3365480	execution. So essentially, all of these when we create a tensor mean we have
3365480	3369520	some constant value, which means that whatever we've defined here, it's not
3369520	3374160	going to change. Whereas the variable tensor could change. So that's just
3374160	3377560	something to keep in mind when we use variable, that's because we think we
3377560	3380760	might need to change the value of that tensor later on. Whereas if we're using
3380760	3384080	a constant value tensor, we cannot change it. So that's just something to keep
3384080	3388440	in mind, we can obviously copy it, but we can't change it. Okay, so evaluating
3388440	3391080	tensors, we're almost at the end of the section, I know, and then we'll get into
3391080	3395280	some more kind of deeper code. So there will be some times for this guide, we
3395280	3398960	need to evaluate a tense, of course, so what we need to do to evaluate a tensor
3398960	3403880	is create a session. Now, this isn't really like, we're not going to do this that
3403880	3407000	much. But I just figured I'd mention it to make sure that you guys are aware of
3407000	3411000	what I'm doing. If I start kind of typing this later on. Essentially, sometimes
3411000	3414720	we have some tensor object. And throughout our code, we actually need to
3414720	3419520	evaluate it to be able to do something else. So to do that, all we need to do
3419560	3424240	is literally just use this kind of default template, a block of code. Well, we
3424400	3428440	say with TF dot session, as some kind of session doesn't really matter what we
3428440	3433760	put here, then we can just do whatever the tensor name is dot eval. And calling
3433760	3437240	that will actually have TensorFlow just figure out what it needs to do to find
3437240	3440600	the value of this tensor, it will evaluate it, and then it will allow us to
3440600	3443640	actually use that value. So I put this in here, you guys can obviously read
3443640	3446960	through this if you want to understand some more in depth on how that works. And
3446960	3450080	the source for this is straight from the TensorFlow website. A lot of this is
3450080	3454200	straight up copied from there. And I've just kind of added my own spin to it and
3454200	3457760	made it a little bit easier to understand. Okay, so we've done all that. So let's
3457760	3460880	just go in here and do a few examples of reshaping just to make sure that
3460880	3463800	everyone's kind of on the same page. And then we'll move on to actually talking
3463800	3467840	about some simple learning algorithms. So I want to create a tensor that we can
3467840	3471960	kind of mess with in reshape. So what I'm going to do is just say t equals and
3471960	3477560	we'll say TF dot ones. Now what TF dot ones does is just create again, all of
3477560	3481480	the values to be ones that we're going to have and whatever shape. Now we can
3481480	3485160	also do zeros and zeros is just going to give us a bunch of zeros. And let's
3485160	3488120	create some like crazy shape and just visualize this. Let's see like a five
3488120	3491760	by five by five. So obviously, if we want to figure out how many elements are
3491760	3494720	going to be in here, we need to multiply this value. So I believe this is going to
3494720	3498600	be 625 because that should be five to the power of four. So five times five times
3498600	3503160	five times five. And let's actually print T and have a look at that and see
3503160	3506760	what this is. So we run this now. And you can see this is the output we're
3506760	3510600	getting. So obviously, this is a pretty crazy looking tensor, but you get the
3510600	3515480	point, right? And it tells us the shape is 55555. Now watch what happens when I
3515520	3520160	reshape this tensor. So if I want to take all of these elements and flatten them
3520160	3528160	out, what I could do is simply say, we'll say T equals TF dot reshape like
3528160	3535040	that. And we'll reshape the tensor T to just the shape 625. Now if we do this
3535080	3541240	and we run here, oops, I got a print T at the bottom after we've done that if I
3541280	3546240	could spell the print statement correctly, you can see that now we just get this
3546240	3552080	massive list that just has 625 zeros. And again, if we wanted to reshape this to
3552080	3555360	something like 125, and maybe we weren't that good at math and couldn't figure out
3555360	3559320	that this last value should be five, we could put a negative one, this would mean
3559320	3563200	that TensorFlow would infer now what the shape needs to be. And now when we look
3563200	3567200	at it, we can see that we're what we're going to get is well, just simply five
3567200	3571600	kind of sets of these, I don't know, matrices, whatever you want to call them in
3571600	3577000	our shape is 125 five. So that is essentially how that works. So that's how
3577000	3581480	we reshape. That's how we kind of deal with tensors create variables, how that
3581480	3584640	works in terms of sessions and graphs. And hopefully with that, that gives you
3584640	3590600	enough of an understanding of tensors of shapes of ranks of value so that when we
3590600	3593440	move into the next part of the tutorial, where we're actually writing code, and I
3593440	3596440	promise we're going to be writing some more advanced code, you'll understand how
3596480	3603480	that works. So with that being said, let's get into the next section. So welcome
3603480	3607080	to module three of this course. Now what we're going to be doing in this module is
3607080	3611320	learning the core machine learning algorithms that come with TensorFlow. Now
3611320	3614480	these algorithms are not specific to TensorFlow, but they are used within
3614480	3617640	there and we'll use some tools from TensorFlow to kind of implement them. But
3617640	3620800	essentially, these are the building blocks before moving on to things like
3620800	3624280	neural networks and more advanced machine learning techniques. You really need to
3624320	3627960	understand how these work because they're kind of used in a lot of different
3627960	3630960	techniques and combined together. And one of them but to show you is actually
3630960	3635080	very powerful if you use it in the right way. A lot of what machine learning
3635080	3638200	actually is in a lot of machine learning algorithms and implementations and
3638200	3642800	businesses and applications and stuff like that, actually just use pretty basic
3643120	3646680	models, because these models are capable of actually doing, you know, very
3646680	3650080	powerful things. When you're not dealing with anything that's crazy complicated,
3650080	3653680	you just need some basic machine learning, some basic classification, you can
3653680	3657680	use these kind of fundamental core learning algorithms. Now the first one
3657680	3660120	we're going to go through is a linear regression, but we will cover
3660120	3664280	classification, clustering and hidden Markov models. And those are kind of
3664280	3668720	going to give us a good spread of the different core algorithms. Now there is
3668760	3672840	a ton, ton, like thousands of different machine learning algorithms. These are
3672840	3676320	kind of the main categories that you'll cover. But within these categories,
3676320	3679400	there is more specific algorithms that you can get into. I just feel like I
3679400	3682680	need to mention that because I know a lot of you will have maybe seen some
3682680	3685400	different ways of doing things in this course might show you, you know, a
3685400	3688640	different perspective on that. So let me just quickly talk about how I'm going
3688640	3692040	to go through this. It's very similar to before I have this notebook, as I've
3692040	3695040	kind of talked about, there is a link in the description, I would recommend that
3695040	3698440	you guys hit that and follow along with what I'm doing and read through the
3698440	3701440	notebook, but I will just be going through the notebook. And then occasionally
3701440	3705640	what I will actually do, oops, I need to open this up here is go to this kind
3705640	3709200	of untitled tab I have here and write some code in here. Because most of what
3709240	3713000	I'm going to do is just copy code over into here so we can see it all in kind
3713000	3717200	of one block. And then we'll be good to go. And the last note before we really
3717200	3720000	get into it, and I'm sorry I'm talking a lot, but it is important to make you
3720000	3723200	guys aware of this, you're going to see that we use a lot of complicated
3723200	3727120	syntax throughout this kind of series and the rest of the course in general. I
3727120	3731080	just want to make it extremely clear that you should not have to memorize or
3731080	3735520	even feel obligated to memorize any of the syntax that you see, everything that
3735520	3739160	you see here, I personally don't even have memorized is a lot of what's in here
3739160	3742640	that I can't just come up with on the top of my head. When we're dealing with
3742640	3746960	kind of a library and modules so big that like TensorFlow, it's hard to
3746960	3750200	memorize all those different components. So just make sure you understand what's
3750200	3753240	happening, but you don't need to memorize it. If you're ever going to need to use
3753240	3755880	any of these tools, you're going to look them up, you're going to see what it is
3755880	3758280	you're going to be like, okay, I've used this before, you're going to understand
3758280	3761040	it, and then you can go ahead and you know, copy that code in and use it in
3761040	3764760	whatever way you need to, you don't need to memorize anything that we do. All
3764760	3768720	right, so let's go ahead and get started with linear regression. So what is
3768760	3772520	linear regression? What's one of those basic forms of machine learning? And
3772520	3776640	essentially, what we try to do is have a linear correspondence between data
3776640	3779960	points. So I'm just going to scroll down here, do a good example. So what I've
3779960	3783760	done is use map plot live just to plot a little graph here. So we can see this
3783760	3787360	one right here. And essentially, this is kind of our data set. This is what we'll
3787360	3791800	call your data set. What we want to do is use linear regression to come up with
3791840	3795400	a model that can give us some good predictions for our data points. So in
3795400	3799320	this instance, maybe what we want to do is given some x value for a data point,
3799320	3803720	we want to predict the y value. Now, in this case, we can see there is kind of
3803720	3808360	some correspondence linearly for these data points. Now, what that means is we
3808360	3812360	can draw something called a line of best fit through these data points that can
3812400	3816320	kind of accurately classify them, if that makes any sense. So I'm going to
3816320	3819920	scroll down here and look at what our line of best fit for this data set
3819920	3823880	actually is, you can see this blue line, a pretty much, I mean, it is the
3823880	3828800	perfect line of best fit for this data set. And using this line, we can actually
3828800	3833520	predict future values in our data set. So essentially, linear regression is used
3833520	3837560	when you have data points that correlate in kind of a linear fashion. Now, this is
3837560	3842160	a very basic example, because we're doing this in two dimensions with x and y. But
3842160	3845720	oftentimes, what you'll have is you'll have data points that have, you know, eight
3845720	3850160	or nine kind of input values. So that gives us, you know, a nine dimensional kind
3850200	3853680	of data set. What we'll do is predict one of the different values. So in the
3853680	3856280	instance where we were talking about students before, maybe we have a
3856280	3860000	student, what is it midterm grade, and their second midterm grade, and then we
3860000	3863920	want to predict their final grade, what we can do is use linear regression to do
3863920	3867440	that, where our kind of input values are going to be the two midterm grades and
3867440	3871960	the output value is going to be that final grade that we're looking to predict. So
3871960	3875800	if we were to plot that, we would plot that on a three dimensional graph, and we
3875800	3879800	would draw a three dimensional line that would represent the line of best fit for
3879800	3883200	that data set. Now, for any of you that don't know what line of best fit stands
3883200	3886920	for, it says line, or this is just the definition I got from this website here,
3887120	3890440	line of best fit refers to a line through a scatter plot of data points that
3890440	3894080	best expresses the relationship between those points. So exactly what I've kind
3894080	3898320	of been trying to explain, when we have data that correlates linearly, and I
3898360	3902320	always butcher that word, what we can do is draw a line through it, and then we
3902320	3906160	can use that line to predict new data points, because if that line is good,
3906200	3910480	it's a good line of best fit for the data set, then hopefully we would assume
3910480	3914360	that we can just, you know, pick some point, find where it would be on that
3914360	3918160	line, and that'll be kind of our predicted value. So I'm going to go into an
3918160	3920800	example now where I start drawing and going into a little bit of math. So we
3920800	3923960	understand how this works on a deeper level. But that should give you a
3923960	3926800	surface level understanding. So actually, I'll leave this up because I was
3926960	3931400	messing with this beforehand. This is kind of a data set that I've drawn on
3931400	3936000	here. So we have our x, and we have our y, and we have our line of best fit. Now,
3936000	3939640	what I want to do is I want to use this line of best fit to predict a new
3939640	3942920	data point. So all these red data points are ones that we've trained our model
3942920	3946320	with their information that we gave to the model so that it could create this
3946320	3950600	line of best fit. Because essentially, all linear regression really does is look
3950600	3955200	at all of these data points and create a line of best fit for them. That's all it
3955200	3959720	does. It's pretty, I don't know the word for it. It's pretty easy to actually do
3959720	3962600	this. This algorithm is not that complicated. It's not that advanced. And
3962640	3966360	that's why we start with it here, because it just makes sense to explain. So I
3966360	3970520	hope that a lot of you would know in two dimensions, a line can be defined as
3970520	3976680	follows. So with the equation y equals mx plus b. Now b stands for the y
3976680	3980280	intercept, which means somewhere on this line. So essentially, where the line
3980280	3985120	starts. So in this instance, our b value is going to be right here. So this is
3985120	3989120	going to be b, because that is the y intercept. So we could say that that's
3989160	3994440	like maybe, you know, we go on, we'll do this, we'll say this is like 123, we
3994440	3999200	might say b is something like 0.4, right? So I could just pencil that into 0.4.
4000120	4005160	And then what is mx and y? Well, x and y stand for the coordinates of this
4005160	4010360	data point. So this would have, you know, some x, y value. In this case, we might
4010360	4015200	call it, you know, something like, what do you want to say to 2.7, that might be
4015200	4020400	the value of this data point. So that's our x and y. And then our m stands for
4020400	4025080	the slope, which is probably the most important part. Now slope simply defines
4025280	4029200	the steepness of this line of best fit that we've done here. Now the way we
4029200	4033840	calculate slope is using rise over run. Now rise over run essentially just
4033840	4037480	means how much we went up versus how much we went across. So if you want to
4037480	4040800	calculate the slope of a line, what you can actually do is just draw a triangle.
4041440	4045840	So a right angle triangle anywhere on the line. So just pick two data points. And
4045840	4049720	what you can do is calculate this distance, and this distance. And then you
4049720	4053800	can simply divide the distance up by the distance across. And that gives you the
4053800	4056600	slope. I'm not going to go too far into slope because I feel like you guys
4056600	4060320	probably understand what that is. But let's just pick some values for this line.
4060320	4063200	And I want to actually show you some real examples of math and how we're going
4063200	4066800	to do this. So let's say that our linear regression algorithm, you know, comes up
4066800	4069840	with this line, I'm not going to discuss really how it does that, although it
4069840	4073360	just pretty much looks at all these data points, and finds a line that you know,
4073400	4078600	goes, it splits these data points evenly. So essentially, you want to be as close
4078600	4082680	to every data point as possible. And you want to have as many data points, you
4082680	4085600	want to have like the same amount of data points on the left side and the right
4085600	4088600	side of the line. So in this example, we have, you know, a data point on the
4088600	4091840	left, a data point on the left, we have what two that are pretty much on the
4091840	4095040	line. And then we have two that are on the right. So this is a pretty good line
4095040	4099400	of best fit, because all of the points are very close to the line. And they
4099440	4103680	split them evenly. So that's kind of how you come up with a line of best fit. So
4103680	4107640	let's say that the equation for this line is something like y equals, let's just
4107640	4115440	give it 1.5 and x plus and let's say that value is just 0.5 to make it easy. So
4115440	4119200	this is going to be the equation of our line. Now notice that x and y don't have
4119200	4122960	a value, that's because we need to give the value to come up with one of the
4122960	4127360	other ones. So what we can do is we can say if we have either the y value or we
4127360	4131560	have the x value of some point, and we want to figure out, you know, where it
4131560	4136320	is on the line, what we can do is just feed one in, do a calculation, and that
4136320	4139520	will actually give us the other value. So in this instance, let's say that you
4139520	4143320	know, I'm trying to predict something and I'm given the that the fact that x
4143360	4147760	equals two, I know that x equals two, and I want to figure out what y would be
4147800	4152280	if x equals two. Well, I can use this line to do so. So what I would do is I'm
4152280	4160560	going to say y equals 1.5 times two plus 0.5. Now, all of you quick math majors
4160560	4165480	out there give me the value of 3.5, which means that if x was at two, then I
4165480	4169920	would have my data point as a prediction here on this line. And I would say, okay,
4169920	4173400	so if you're telling me x is two, my prediction is that y is going to be
4173400	4177560	equal to 3.5, because given the line of best fit for this data set, that's where
4177560	4182480	this point will lie on that line. So I hope that makes sense. You can actually
4182480	4186680	do this the reverse way as well. So if I'm just given some y values to say, I
4186680	4191840	know that, you know, my y value is at like 2.7 or something, I can plug that in,
4191840	4195440	just rearrange the numbers in this equation and then solve for x. Now,
4195440	4198720	obviously, this is a very basic example, because we're just doing all of this in
4198720	4202560	two dimensions. But you can do this in higher dimensions as well. So actually,
4202560	4205160	most times, what's going to end up happening is you're going to have, you
4205200	4208680	know, like eight or nine input variables. And then you're going to have one output
4208680	4212960	variable that you're predicting. Now, so long as our data points are correlated
4212960	4216600	linearly in three dimensions, we can still do this. So I'm going to attempt to
4216600	4219880	show you this actually, in three dimensions, just to hopefully clear some
4219880	4224280	things up, because it is important to kind of get a grasp and perspective of the
4224280	4228720	different dimensions. So let's say we have a bunch of data points that are kind
4228720	4234040	of like this, now I'm trying my best to kind of draw them in some linear fashion
4234120	4238680	using like all the dimensions here. But it is hard because drawing in three
4238680	4242280	dimensions on a two dimensional screen is not easy. Okay, so let's say this is
4242280	4245680	kind of like what our data points look like. Now, I would say that these
4245680	4250160	correlate linearly, like pretty, pretty well, they kind of go up in one fashion,
4250160	4253440	and we don't know the scale of this. So this is probably fun. So the line of
4253440	4257720	best fit for this data set, and I'll just put my kind of thickness up might be
4257720	4263080	something like this, right? Now notice that this line is in three dimensions,
4263080	4268560	right? This is going to cross our, I guess this is our x, y, and z axes. So we
4268560	4271440	have a three dimensional line. Now the equation for this line is a little bit
4271440	4274920	more complicated. I'm not going to talk about exactly what it is. But essentially
4274920	4278960	what we do is we make this line, and then we say, Okay, what value do I want to
4278960	4285760	predict? Do I want to predict y, x or z? Now, so long as I have two values, so two
4285800	4289240	values, I can always predict the other one. So if I have, you know, the x, y of
4289280	4295120	the data point, that will give me the z. And if I have the z, y, that will give
4295120	4300520	me the x. So so long as you have, you know, all of the data points, except one,
4300680	4304320	you can always find what that point is, based on the fact that, you know, we have
4304320	4307680	this line, and we're using that to predict. So I think I'm going to leave it at
4307680	4312440	that for the explanation. I hope that makes sense. Again, just understand that we
4312440	4316200	use linear regression when our data points are correlated linearly. Now some
4316200	4319480	good examples of linear regression were, you know, that kind of student
4319480	4323520	predicting the grade kind of thing, you would assume that if someone has, you
4323520	4327080	know, a low grade, then they would finish with a lower grade, and you would
4327080	4330720	assume if they have a higher grade, they would finish with a higher grade. Now you
4330720	4334680	could also do something like predicting, you know, future life expectancy. Now this
4334680	4338360	is kind of a darker example. But essentially, what you could think of here is
4338360	4343080	if someone is older, they're expected to live, you know, like not as long. Or you
4343080	4346520	could look at health conditions, if someone is in critical illness condition,
4346560	4350240	they have a critical illness, then chances are their life expectancy is lower. So
4350240	4353400	that's an example of something that is correlated linearly. Essentially,
4353400	4356040	something goes up, and something goes down, or something goes up, the other
4356040	4358840	thing goes up. That's kind of what you need to think of when you think of a
4358840	4362880	linear correlation. Now the magnitude of that correlation, so you know, how much
4362880	4366320	does one go up versus how much one goes down is exactly what our algorithm
4366320	4369960	figures out for us, we just need to know to pick linear regression when we think
4370000	4373720	things are going to be correlated in that sense. Okay, so that is enough of the
4373720	4376760	explanation of linear regression. Now we're going to get into actually coding
4376760	4379840	and creating a model. But we first need to talk about the data set that we're
4379840	4382760	going to use in the example we're going to kind of illustrate linear regression
4382760	4386680	with. Okay, so I'm here and I'm back in the notebook. Now these are the imports
4386680	4389840	we need to start with to actually start programming and getting some stuff done.
4390160	4394160	Now the first thing we need to do is actually install SK learn. Now even if
4394160	4397240	you're in a notebook, you actually need to do this because for some reason it
4397240	4400000	doesn't come by default with the notebook. So to do this, we just did an
4400000	4404680	exclamation point, pip install hyphen q SK learn. Now if you're going to be working
4404680	4408000	on your own machine, again, you can use pip to install this. And I'm assuming
4408000	4411320	that you know to use pip if you're going to be going along in that direction. Now
4411320	4413960	as before, since we're in the notebook, we need to define we're going to use
4413960	4418680	TensorFlow version to point x. So to do that, we're going to just, you know, do
4418680	4421920	that up here with the percent sign. And then we have all these imports, which
4421920	4425400	we're going to be using throughout here. So from future import, absolutely import
4425440	4428800	division, print function, Unicode literals, and then obviously the big one. So
4428800	4434200	NumPy, pandas, map plot lib, we're gonna be using I Python, we're gonna be using
4434200	4437560	TensorFlow. And yeah, so I'm actually just gonna explain what some of these
4437560	4441560	modules are, because I feel like some of you may actually not know. NumPy is
4441560	4447360	essentially a very optimized version of arrays in Python. So what this allows
4447360	4451680	us to do is lots of kind of multi dimensional calculations. So essentially
4451680	4455040	if you have a multi dimensional array, which we've talked about before, right
4455040	4459200	when we had, you know, those crazy shapes like 5555, NumPy allows us to
4459200	4462880	represent data in that form, and then very quickly manipulate and perform
4462880	4467240	operations on it. So we can do things like cross product, dot product, matrix
4467240	4471600	addition, matrix subtraction, element wise addition, subtraction, you know,
4471600	4475320	vector operations, that's what this does for us. It's pretty complex, but we're
4475320	4479400	going to be using it a fair amount. Pandas. Now what pandas does is it's kind
4479400	4483480	of a data analytics tool, I almost want to say, I don't know the formal
4483480	4487640	definition of what pandas is. But it allows us to very easily manipulate
4487640	4492800	data. So you know, load in data sets, view data sets, cut off specific columns
4492800	4496440	or cut out rows from our data sets, visualize the data sets. That's what
4496440	4500520	pandas does for us. Now map plot lib is actually a visualization of kind of
4500520	4504080	graphs and charts. So we'll use that a little bit lower when I actually
4504440	4509120	graph some different aspects of our data set. The IPython display, this is
4509120	4511920	just specific for this notebook, it's just to clear the output, there's
4511920	4515400	nothing crazy with that. And then obviously, we know what TensorFlow is,
4515600	4520000	this crazy import for TensorFlow here. So compact v to feature column as FC,
4520000	4523560	we'll talk about later, but we need something called a feature column when
4523560	4527600	we create a linear regression algorithm or model and TensorFlow. So we're going
4527600	4531640	to use that. Okay. So now that we've gone through all that, we need to start
4531640	4534280	talking about the data set that we're going to use for linear regression. And
4534280	4537200	for this example, because what we're going to do is, you know, actually create
4537200	4541040	this model, and start using it to predict values. So the data set that we're
4541080	4544080	going to use, actually, I need to read this, because I forget exactly what the
4544080	4548440	name of it is, is the Titanic data set, that's what it is. So essentially, what
4548440	4552320	this does is aim to predict who's going to survive or the likelihood that
4552320	4557200	someone will survive, being on the Titanic, given a bunch of information. So
4557200	4559760	what we need to do is load in this data set. Now, I know this seems like a
4559760	4562880	bunch of gibberish, but this is how we need to load it. So we're going to use
4562880	4569480	pandas. So PD dot read CSV from this URL. So what this is going to do is take
4569520	4573480	this CSV file, which stands for comma, separated values. And we can actually
4573480	4577400	look at this if we want, I think so I said, it said control click, let's see
4577400	4580920	if this pops up. So let's actually download this. And let's open this up
4580920	4583800	ourselves and have a look at what it is in Excel. So I'm going to bring this
4583800	4588400	up here. You can see that link. And this is what our data set is. So we have
4588400	4592520	our columns, which just stand for, you know, what is it the different
4592520	4595160	attributes in our data set of the different features and labels of our
4595160	4598760	data set, we have survived. So this is what we're actually going to be aiming
4598800	4602040	to predict. So we're going to call this our label, right, or our output
4602040	4607560	information. So here, a zero stands for the fact that someone did not survive.
4607600	4611360	And one stands for the fact that someone did survive. Now, just thinking about
4611360	4614720	it on your own for a second, and looking at some of the categories we have up
4614720	4618640	here, can you think about why linear regression would be a good algorithm for
4618640	4623480	something like this? Well, for example, if someone is a female, we can kind of
4623480	4625960	assume that they're going to have a higher chance of surviving on the
4625960	4629680	Titanic, just because of, you know, the kind of the way that our culture works,
4629720	4632280	you know, saving women and children first, right? And if we look through this
4632280	4636520	data set, we'll see that when we see females, it's pretty rare that they
4636520	4639760	don't survive. Although as I go through, there is quite a few that didn't
4639760	4642440	survive. But if we look at it compared to males, you know, there's definitely
4642440	4646960	strong correlation that being a female results in a stronger survival rate. Now,
4646960	4650080	if we look at age, right, can we think of how age might affect this? Well, I
4650080	4653240	would assume if someone's way younger, they probably have a higher chance of
4653280	4658320	surviving, because they would be, you know, prioritized in terms of lifeboats or
4658320	4661040	whatever it was. I don't know much about the Titanic. So I can't talk about that
4661040	4663680	specifically. But I'm just trying to go through the categories and explain to
4663680	4667160	you why we picked this algorithm. Now, number of siblings, that one might not
4667160	4670880	be as, you know, influential, in my opinion, parched. I don't actually
4670880	4676760	remember what parched stands for. I think it is like what parched, I don't know
4676760	4679600	exactly what this column stands for. So unfortunately, I can't tell you guys
4679640	4684640	that one. But we'll talk about some more of the second fair. Again, not exactly
4684640	4687640	sure what fair stands for. I'm going to look on the TensorFlow website after
4687640	4691560	this and get back to you guys. And we have a class. So class is what class they
4691560	4694800	were on the boat, right? So first class, second class, third class. So you might
4694800	4698080	think someone that's in a higher class might have a higher chance of surviving.
4698480	4702400	We have decks, this is what deck they were on when it crashed. So unknown is
4702400	4706160	pretty common. And then we have all these other decks, you know, if someone got
4706200	4710200	hit, if someone was standing on the deck that had the initial impact, we might
4710200	4714000	assume that they would have a lower chance of survival. embark to is where
4714000	4717320	they were going. And then are they alone? Yes or no. And this one, you know, this
4717320	4720520	is interesting, we're going to see does this make an effect? If someone is alone,
4720520	4723920	is that a higher chance of survival? Is that a lower chance of survival? So this
4723920	4726200	is kind of interesting. And this is what I want you guys to think about is that
4726200	4730560	when we have information and data like this, we don't necessarily know what
4730560	4734400	correlations there might be. But we can kind of assume there's some linear
4734480	4738000	thing that we're looking for some kind of pattern, right? Whereas if something is
4738000	4741520	true, then you know, maybe it's more likely someone will survive. Whereas like,
4741520	4745080	if they're not alone, maybe it's less likely. And maybe there's no correlation
4745080	4748840	whatsoever. But that's where we're going to find out as we do this model. So let
4748840	4752280	me look actually, on the TensorFlow website and see if I can remember what
4752280	4756600	parched and I guess what fair was. So let's go up to the top here. Again, a lot
4756600	4760000	of this stuff is just straight up copied from the TensorFlow website. I've just
4760000	4763680	added my own stuff to it. You can see like, I just copied all this, we're just
4763720	4767880	bringing it in there. Let's see what it says about the different columns, if it
4767880	4773440	gives us any exact explanations. Okay, so I couldn't find what parts were fair
4773440	4776320	stands for. For some reason, it's not on the TensorFlow website, either. I
4776320	4779040	couldn't really find any information about it. If you guys know, you know,
4779040	4781760	leave a comment down below, but it's not that important, we just want to use
4781760	4786320	this data to do a test. So what I've done here, if I've loaded in my data set,
4786320	4790560	and notice that I've loaded a training data set in a testing data set. Now we'll
4790600	4794160	talk about this more later. This is important, I have two different data
4794160	4798640	sets, one to train the model with, and one to test the model with. Now kind of the
4798640	4802080	basic reason we would do this is because when we test our model for accuracy to
4802080	4805800	see how well it's doing, it doesn't make sense to test it on data, it's already
4805800	4809760	seen, it needs to see fresh data, so we can make sure there's no bias, and it
4809760	4814840	hasn't simply just memorize the data, you know, that we had. Now what I'm doing
4814840	4819800	here, at the bottom with this y train in this y eval, is I'm essentially popping
4819880	4824560	a column off of this data set. So if I print out the data set here, and I'm
4824560	4827120	actually I'm going to show you a cool trick with pandas that we can use to
4827120	4833520	look at this. So I can say D F train dot head. So if I look at this, by just
4833520	4836800	looking at the head, and we'll print this out, oh, I might need to import some
4836800	4840880	stuff above. We'll see if this works or not. Yeah, so I need to just do these
4840880	4844640	imports. So let's install. And let's do these imports. I'll wait for the
4844640	4848080	surrounding. Okay, so I've just selected TensorFlow 2.0. We're just importing
4848080	4851920	this now should be done in one second. And now what we'll do is we'll print
4851960	4856200	out the the data frame here. So essentially what this does is load this
4856200	4860600	into a pandas data frame. This is a specific type of object. Now we're not
4860600	4864240	going to go into this specifically, but a data frame allows us to view a lot of
4864240	4868400	different aspects about the data and kind of store it in a nice form, as opposed
4868400	4871960	to just loading it in and storing it in like a list or a NumPy array, which we
4871960	4875440	might do if we didn't know how to use pandas. This is a really nice way to do
4875520	4879680	it read CSV, load it into a data frame object, which actually means we can
4879680	4883800	reference specific columns and specific rows in the data frame. So let's run this
4883800	4890760	and just have a look at it. Yeah, I got need to print dftrain.head. So let's do
4890760	4895920	that. And there we go. So this is what our data frame head looks like. Now head
4895920	4900800	what that does is show us the first five entries in our data set, as well as show
4900800	4904760	us a lot of the different columns that are in it. Now since we have more than
4904760	4907520	you know, we have a few different columns, it's not showing us all of them, it's
4907520	4910920	just giving us the dot dot dot. But we can see this is what the data frame
4910920	4914960	looks like. And this is kind of the representation internally. So we have
4915160	4920040	entry zero, survived zero, survived one, we have male, female, all that. Now
4920040	4923800	notice that this has the survived column. Okay. Because what I'm going to do is
4923800	4930440	I'm going to print the data frame head again. So dftrain.head after we run
4930440	4935840	these two lines. Now what this line does is takes this entire survived column, so
4935840	4940400	all these zeros and ones, and removes it from this data frame, so the head data
4940400	4944720	frame, and stores it in the variable y train. The reason we need to do that is
4944720	4947480	because we need to separate the data, we're going to be classifying from the
4947480	4951480	data that is kind of our input information or our initial data set, right? So
4951480	4955080	since we're looking for the survived information, we're going to put that in
4955080	4959240	its own, you know, kind of variable store here. Now we'll do the same thing for
4959240	4964240	the evaluation data set, which is DF evaluation or testing data. And notice
4964240	4968840	that here this was trained on CSV, and this one was eval.csv. Now these have
4968840	4973320	the exact same form, they look the like completely identical, it's just that
4973320	4976400	you know, some entries, we've just kind of arbitrarily split them. So we're going
4976400	4979120	to have a lot of entries in this training set, and we'll have a few in the
4979120	4983000	testing set that we'll just use to do an evaluation on the model later on. So we
4983000	4987800	pop them off by doing this pop removes and returns this column. So if I print
4987840	4991160	out why train, which are actually let's look at this one first, just to show
4991160	4995200	you how it's been removed, we can see that we have the survived column here, we
4995200	4998600	popped and now the survived column is removed from that data set. So that's
4998600	5002120	just important to understand. Now we can print out some other stuff too. So we can
5002120	5005960	look at the why train and see what that is. Just to make sure we really
5005960	5009360	understand this data. So let's look at why train. And you can see that we have
5009360	5014960	626 or 627 entries and address, you know, zeros or ones representing whether
5014960	5019720	someone survived or whether they did not. Now the corresponding indexes in this
5019800	5024160	kind of list or data frame correspond to the indexes in the testing and
5024200	5028200	training data frame. What I mean by that is, you know, entry zero in this
5028240	5033960	specific data frame corresponds to entry zero in our why train variable. So if
5033960	5038240	someone survived, you know, at entry zero, it would say one here, right? Or in
5038240	5042360	this case, entry zero did not survive. Now, I hope that's clear. I hope I'm not
5042400	5046320	confusing you with that. But I just want to show one more example to make sure. So
5046320	5049880	we'll say D F train zero, I'm going to print that and I'm going to print why
5049880	5054880	train at index zero. Oops, if I didn't mess up my brackets, and we'll have a
5054880	5058240	look at it. Okay, so I've just looked up the documentation because I totally
5058240	5062280	forgot that I couldn't do that. If I want to find one specific row in my data
5062280	5067600	frame, what I can do is print dot loc. So I do my data frame and then dot loc and
5067600	5071800	then whatever index I want. So in this case, I'm locating row zero, which is
5071800	5075480	this. And then on the why train, I'm doing the same thing, I'm locating row
5075480	5080280	zero. Now what I had before, right, if I did D F train, and I put square brackets
5080520	5084000	inside here, what I can actually do is reference a specific column. So if I
5084000	5087960	wanted to look at, you know, say the column for age, right, so we have a
5087960	5092840	column for age, what I can do is do D F train age. And then I can print this out
5092840	5096440	like this. And it gives me all of the different age values. So that's kind of
5096440	5099800	how we use a data frame, we'll see that as we go further on. Now let's go back to
5099800	5103520	the other example I had, because I just erased it, where I want to show you the
5103920	5108960	row zero in the data frame that's training, and then in the why train, you
5108960	5113080	know, output, whatever that is. So the survival. So you can see here that this
5113080	5117520	is what we get from printing D F train loc zero. So row zero, this is all the
5117520	5121200	information. And then here, this corresponds to the fact that they did not
5121200	5125400	survive at row zero, because it's simply just the output is value zero. Now I
5125440	5129320	know this is weird, it's saying like name, zero, d type, object, zero, don't
5129320	5131560	worry about that. It's just because it's trying to print it with some
5131560	5135880	information. But essentially, this just means this person who was male 22 and had
5135880	5140480	one sibling did not survive. Okay, so let's get out of this. Now we can close
5140480	5144360	this and let's go to Oh, we've pretty much already done what I've just have
5144360	5147800	down here. But we can look at the data frame head. This is a little bit of a
5147800	5151880	nicer output when we just have D F train dot head, we can see that we get kind of
5151880	5155280	a nice outputted little graph. We've already looked at this information. So we
5155320	5159200	know kind of some of the attributes of the data set. Now we want to describe the
5159200	5163520	data set sometimes. What describe does is just give us some overall information. So
5163520	5168480	let's have a look at it here. We can see that we have 627 entries, the mean of
5168520	5173360	age is 29, the standard deviation is you know, 12 point whatever. And then we get
5173360	5177040	the same information about all of these other different attributes. So for example,
5177040	5180320	it gives us you know, the mean fair, the minimum fair, and just some statistics,
5180360	5183760	because understand this great, if you don't doesn't really matter. The important
5183760	5186560	thing to look at typically is just how many entries we have is sometimes we need
5186560	5189880	that information. And sometimes the mean can be helpful as well, because you can
5189880	5194080	kind of get an average of like what the average value is in the data set. So if
5194080	5197520	there's any bias later on, you can figure that out. But it's not crazy important.
5197920	5201680	Okay, so let's have a look at the shape. So just like NumPy arrays and tensors
5201680	5205080	have a shape attribute, so do data frames. So we want to look at the shape, you
5205080	5209320	know, we can just print out D F train dot shape, we get 627 by nine, which
5209320	5215880	essentially means we have 627 rows, and nine columns or nine attributes. So yeah,
5215880	5220480	that's what it says here, you know, 627 entries, nine features, we can interchange
5220480	5224360	attributes and features. And we can look at the head information for why so we can
5224360	5227240	see that here, which we've already looked at before. And that gives us the name,
5227280	5231440	which was survived. Okay, so now what we can actually do is make some kind of
5231440	5234520	graphs about this data. Now I've just stolen this code, you know, straight up
5234520	5238320	from the TensorFlow website, I wouldn't expect you guys to do any of this, you
5238320	5241760	know, like output any of these values. What we're going to do is create a few
5241760	5245360	histograms and some plots just to look at kind of some correlations in the data.
5245360	5248680	So that when we start creating this model, we have some intuition on what we
5248680	5253600	might expect. So let's look at age. So this gives us a histogram of the age. So
5253600	5258560	we can see that there's about 25 people that are kind of between zero and five.
5258760	5263600	There is you know, maybe like five people that are in between five and 10. And
5263600	5267120	then the most amount of people are kind of in between their 20s and 30s. So in the
5267120	5271480	mid 20s, this is good information to know, because that's going to introduce a
5271480	5275920	little bit of bias into kind of our linear correlation graph, right? So just
5276160	5279720	understanding, you know, that we have like a large subset, there's some outliers
5279720	5282760	here, like there's one person that's 80, right over here, a few people that are
5282760	5286080	70, some important things to kind of understand before we move on to the
5286080	5290960	algorithm. So let's look at the sex values now. So this is how many female and
5290960	5295360	how many male, we can see that there's many more males than there is females. We
5295400	5298560	can have a look at the class. So we can see if they're in first, second or third
5298560	5303800	class, most people are in third, then followed by first and then second. And
5303800	5307240	then lastly, we can look at what is this that we're doing? Oh, the percentage
5307240	5312320	survival by sex. So we can see how likely a specific person or a specific sex is
5312320	5316240	to survive just by plotting this. So we can see that males have about a 20%
5316240	5320640	survival rate, whereas females are all the way up to about 78%. So that's
5320640	5323880	important to understand that kind of confirms that what we were looking at
5323880	5326920	before in the data set when we were exploring it. And you don't need to do
5326920	5330160	this every time that you're looking at a data set, but it is good to kind of get
5330160	5333000	some intuition about it. So this is what we've learned so far, majority
5333000	5336720	passengers are in their 20s or 30s, then majority passengers are male, they're in
5336720	5339960	third class, and females have a much higher chance of survival, kind of
5339960	5343040	already knew that. Alright, so training and testing data sets. Now we already
5343040	5346040	kind of went through this all skim through it quickly. Essentially, what we
5346040	5350600	did above is load in two different data sets. The first data set was that
5350600	5354920	training data set which had the shape of 627 by nine. What I'm actually going to
5354920	5360640	do is create a code block here, and just have a look at what was this Df eval dot
5360640	5364440	shape to show you how many entries we have in here. So here in our testing
5364440	5369680	data set, you can see we have significantly less at 264 entries, or rows,
5369680	5373400	whatever you want to call them. So that's how many things we have to actually
5373400	5377080	test our model. So what we do is we use that training data to create the model,
5377120	5379880	and then the testing data to evaluate it and make sure that it's working
5379880	5383160	properly. So these things are important whenever we're doing machine learning
5383160	5388120	models, we typically have testing and training data. And yeah, that is pretty
5388120	5391440	much it. Now I'm just going to take one second to copy over a lot of this code
5391720	5395440	into the kind of other notebook I have, just so we can see all of it at once. And
5395440	5398200	then I'll be back and we'll get into actually making the model. Okay, so I've
5398200	5401080	copied in some code here. I know this seems like a lot of kind of gibberish
5401080	5404080	right now, but I'm going to break down line by line what all this is doing and
5404080	5407520	why we have this here. But we first need to discuss something called feature
5407520	5413040	columns and the difference between categorical and numeric data. So categorical
5413040	5416240	data is actually fairly common. Now when we're looking at our data set, and
5416240	5419920	actually I can open I don't have it open in Excel anymore, but let's open this
5419920	5425640	from my downloads. So let's go downloads. Where is this train? Okay, awesome. So we
5425640	5430480	have this Excel data sheet here. And we can see what a categorical data or what
5430480	5435200	categorical data is is something that's not numeric. So for example, unknown, C
5435320	5441040	first, third, city, and why right so anything that has different categories,
5441720	5444800	there's going to be like a specific set of different categories there could be. So
5444800	5448480	for example, for age, kind of the set of values we get out for age, well, this is
5448480	5451280	numeric. So that's different. But for categorical, we can have male or we
5451280	5454640	can have female. And I suppose we could have other but in this data set, we just
5454640	5458560	have male and we just have female. For class, we can have first, second, third.
5458800	5462320	For deck, we can have unknown CA, I'm sure through all the letters of the
5462320	5464720	alphabet, but that is still considered categorical.
5465320	5469520	Now, what do we do with categorical data? Well, we always need to transform this
5469520	5474160	data into numbers somehow. So what we actually end up doing is we encode
5474160	5478640	this data using an integer value. So for the example of male and female, what
5478640	5481960	we might say, and this is what we're going to do in a second is that female is
5481960	5486120	represented by zero and male is represented by one. We do this because
5486120	5490280	although it's interesting to know what the actual class is, the model doesn't
5490280	5492960	care, right, female and male, it doesn't make a difference to it. It just needs
5492960	5496080	to know that those values are the different are different or those values
5496080	5499640	are the same. So rather than using strings and trying to find some way to
5499640	5503280	pass that in and do math with that, we need to turn those into integers, we
5503280	5507800	turn those into zeros and ones, right? Now for class, right, so first, second,
5507800	5510320	third, you know, you guys can probably assume what we're going to encode this
5510320	5513720	with, we're going to encode it with zero, one, two. Now, again, this doesn't
5513720	5517680	necessarily need to be in order. So third could be represented by one and
5517680	5521360	first could be represented by two, right? It doesn't need to be in order, it
5521360	5525000	doesn't matter. So long as every third has the same number, every first has the
5525000	5527800	same number and every second has the same number. And then same thing with
5527800	5531520	that same thing with embark and same thing with alone. Now we could have an
5531520	5534800	instance where you know, we've encoded every single one of these values with a
5534800	5539000	different value. So in the, you know, rare occasion where there's one category
5539000	5543640	that's categorical, and every single value in that category is different, then
5543640	5548600	we will have, you know, 627 in this instance, different encoding labels that
5548640	5551680	are going to be numbers, that's fine, we can do that. And actually, we don't
5551680	5555320	really need to do that, because you're going to see how TensorFlow can handle
5555320	5559280	that for us. So that is categorical data, numeric columns are pretty
5559280	5562080	straightforward. They're anything that just have integer or float values
5562080	5566760	already. So in this case, age and fair. And yeah, so that's what we've done. We've
5566760	5570680	just defined our categorical columns here, and our numeric columns here. This is
5570680	5573360	important because we're going to loop through them, which we're doing here to
5573360	5576640	create something called feature columns, feature columns are nothing special.
5576680	5580760	They're just what we need to feed to our linear estimator or linear model to
5580760	5584200	actually make predictions. So kind of our steps here that we've gone through so
5584200	5589200	far is import, load the data set, explore the data set, make sure we
5589200	5593200	understand it, create our categorical columns and our numeric columns. So I've
5593200	5597760	just hard coded these in right like sex, parts, class, deck, alone, all these
5597760	5601040	ones. And then same thing with the numeric columns. And then for a linear
5601040	5604480	estimator, we need to create these as feature columns using some kind of
5604480	5608240	advanced syntax, which we're going to look at here. So we create a blank list,
5608240	5610760	which is our feature columns, which will just store our different feature
5610760	5616120	columns. We loop through each feature name in the categorical columns. And what
5616120	5621120	we do is we define a vocabulary, which is equal to the data frame at that
5621120	5624640	feature name. So first, we would start with sex, then we go and siblings, then
5624640	5629760	we go parts, then we go class. And we get all of the different unique values. So
5629760	5633360	that's actually what this does dot unique, gets a list of all unique values
5633400	5637840	from the feature call. And I can print this out. She'll print this in a
5637840	5640320	different line, we'll just take this value and have a look at actually what
5640320	5646520	this is, right? So if I run, I guess we'll have to run all these in order. And
5646520	5650720	then we'll create a new code block while we wait for that to happen. Let's see if
5650720	5662360	we can get this installing fast enough. Run, run, run. Okay, now we go to df
5662360	5665400	train. And we can see this is what this looks like. So these are all the
5665400	5668960	different unique values that we had in that specific feature name. Now that
5668960	5673880	feature name was what categorical columns? Oh, what I do feature name up,
5673880	5676920	sorry, that's going to be the unique one. Let's just put rather than feature
5676920	5681120	name. Let's put sex, right? And let's have a look at what this is. So we can
5681120	5684920	see that the two unique values are male and female. Now I actually want to do
5685480	5688800	what is it embark town? And I want to see what this one is. So how many
5688800	5691920	different values we have. So we'll copy that in. And we can see we have
5691960	5696520	Southampton cannot pronounce that and then the other cities and unknown. And
5696520	5699360	that is kind of how we get the unique value. So that's what that method is
5699360	5702160	doing there. Let's actually delete this code block because we don't need
5702160	5706200	anymore. Alright, so that's what we do. And then what we do down here is we say
5706200	5710600	feature columns dot append. So just add to this list, the TensorFlow feature
5710600	5715120	column dot categorical column with vocabulary list. Now I know this is a
5715120	5717760	mouthful, but this is kind of something again, you're just going to look up
5717760	5720480	when you need to use it, right? So understand you need to make feature
5720480	5723600	columns for linear regression, you don't really need to completely understand
5723600	5726360	how, but you just need to know that that's something you need to do. And then
5726360	5729960	you can look up the syntax and understand. So this is what this does. This is
5729960	5734200	actually going to create for us a column, it's going to be in the form of a
5734200	5738400	like NumPy array kind of that has the feature name. So whatever one we've
5738400	5742760	looped through, and then all the different vocabulary associated with it. Now
5742760	5746800	we need this because we just need to create this column so that we can create
5746800	5750280	our model using those different columns, if that makes any sense. So our linear
5750280	5753080	model needs to have, you know, all the different columns we're going to use, it
5753080	5756400	needs to know all of the different entries that could be in that column, and
5756400	5760280	needs to know whether this is a categorical column or a numeric column. In
5760280	5763320	previous examples, what we might have done is actually change the data set
5763320	5767320	manually, so encoded it manually. TensorFlow just can do this for us now
5767320	5771440	in TensorFlow 2.0. So we'll just use that too. Okay, so that's what we did with
5771440	5774960	these feature columns. Now for the numeric columns, a little bit different. It's
5774960	5777800	actually easier. All we need to do is give the feature name and whatever the
5777840	5781520	data type is and create a column with that. So notice we don't, we can omit
5781520	5784040	this unique value, because we know when it's numeric, that you know, there
5784040	5786840	could be an infinite amount of values. And then I've just printed out the
5786840	5789640	feature columns, you can see what this looks like. So vocabulary list,
5789640	5793280	categorical column gives us the number of siblings. And then the vocabulary
5793280	5797120	list is these are all the different encoding values that is created. And
5797120	5800400	then same thing, you know, we can go down here, parts, these are different
5800400	5803720	encodings. So they're not necessarily in order is like what I was talking about
5803760	5810560	before. Let's go do a numeric one. What do we have here? Yeah, so for a numeric
5810560	5814400	column, just as is the key, that's the shape we're expecting. And this is the
5814400	5819000	data type. So that is pretty much it. We're actually loading these in. So now
5819000	5822760	it's almost time to create the model. So what we're going to do to create the
5822760	5827120	model now is talk about first the training process and training some kind
5827120	5831640	of, you know, machine learning model. Okay, so the training process. Now the
5831680	5834880	training process of our model is actually fairly simple, at least for a
5834880	5839400	linear model. Now, the way that we train the model is we feed it information,
5839400	5844040	right? So we feed it that those data points from our data set. But how do we
5844040	5846880	do that? Right? Like how do we feed that to the model? Do we just give it all at
5846880	5851200	once? Well, in our case, we only have 627 rows, which isn't really that much
5851200	5854360	data, like we can fit that in RAM in our computer, right? But what if we're
5854360	5858680	training a crazy machine learning model, and we have, you know, 25 terabytes of
5858680	5862240	data that we need to pass it, we can't load that into RAM, at least I don't
5862240	5865360	know any RAM that's that large. So we need to find a way that we can kind of
5865360	5869720	load it in what's called batches. So the way that we actually load this model is
5869720	5873080	we load it in batches. Now we don't need to understand really kind of how this
5873080	5878200	process works and how batching kind of occurs. What we do is give 32 entries at
5878200	5882640	once to the model. Now the reason we don't just feed one at a time is because
5882640	5887440	that's a lot slower, we can load, you know, a small batch size of 32, that can
5887480	5890520	increase our speed dramatically. And that's kind of a lower level
5890520	5894080	understanding. So I'm not going to go too far into that. Now that we understand, we
5894080	5897280	kind of load it in batches, right? So we don't load it entirely all at once, we
5897280	5902720	just load a specific set of kind of elements as we go. What we have is
5902720	5908080	called epochs. Now what are epochs? Well, epochs are essentially how many times the
5908080	5912400	model is going to see the same data. So we might be the case, right? And when we
5912400	5916160	pass the data to our model, the first time, it's pretty bad, like it looks at the
5916160	5919320	model creates our line of best fit, but it's not great, it's not working
5919320	5922600	perfectly. So we need to use something called an epoch, which means we're just
5922600	5926560	going to feed the model, feed the data again, but in a different order. So we
5926560	5930840	do this multiple times, so that the model will look at a data, look at the data in
5930840	5934720	a different way, then kind of a different form, and see the same data a few
5934720	5938080	different times and pick up on patterns. Because the first time it sees a new
5938080	5941240	data point, it's probably not going to have a good idea how to make a prediction
5941240	5945600	for that. So if we can feed it more and more and more, then you know, we can get
5945640	5948440	a better prediction. Now, this is where we talk about something called
5948480	5953720	overfitting, though, sometimes we can see the data too much, we can pass too much
5953720	5957640	data to our model, to the point where it just straight up memorizes those data
5957640	5961720	points. And it's, it's really good at classifying for those data points. But
5961720	5965520	when we pass it some new data points, like our testing data, for example, it's
5965520	5970120	horrible at kind of, you know, classifying those. So what we do to kind of
5970120	5973480	prevent this from happening, is we just make sure that we start with like a
5973520	5976360	lower amount of epochs, and then we can work our way up and kind of
5976360	5980440	incrementally change that if we need to, you know, go higher, right, we need more
5980440	5984960	epochs. So yeah, so that's kind of it for epochs. Now, I will say that this
5984960	5989000	training process kind of applies to all the different, what is it, machine learning
5989000	5991960	models that we're going to look at, we have epochs, we have batches, we have a
5991960	5996760	batch size, and now we have something called an input function. Now, this is
5997080	6001000	pretty complicated. This is the code for the input function. I don't like that we
6001000	6005880	need to do this. But it's necessary. So essentially, what an input function is
6005920	6010320	is the way that we define how our data is going to be broke into epochs and into
6010320	6014720	batches to feed to our model. Now, these, you probably aren't ever going to
6014720	6018720	really need to code like from scratch by yourself. But this is the one I've just
6018720	6021360	stolen from the TensorFlow website, pretty much like everything else that's
6021360	6028280	in the series. And what this does is it takes our data and encodes it in a tf
6028320	6034080	dot data dot data set object. Now, this is because our model needs this specific
6034120	6038040	object to be able to work, it needs to see a data set object to be able to use
6038040	6042320	that data to create the model. So what we need to do is take this pandas data
6042320	6046200	frame, we need to turn it into that object. And the way we do that is with
6046200	6050320	the input function. So we can see that what this is doing here. So this is make
6050360	6054280	input function, we actually have a function defined inside of another function. I
6054280	6057640	know this is kind of complicated for some of you guys. But and what I'm
6057640	6060160	actually gonna do, sorry, I'm gonna just copy this into the other page, because I
6060160	6063280	think it's easier to explain without all the text around. So let's create a new
6063280	6068360	code block. Let's paste this in. And let's have a look at what this does. So
6068360	6073240	actually, let me just tab down. Okay, so make input function, we have our
6073240	6077080	parameters data data frame, which is our pandas data frame, our label data
6077080	6082640	frame, which stands for those labels. So that y train or that eval y eval, right,
6082840	6085680	we have a number of epochs, which is how many epochs we're going to do, we set
6085720	6090760	the default 10 shuffle, which means are we going to shuffle our data and mix it
6090760	6095080	up before we pass it to the model, and batch size, which is how many elements
6095120	6100000	are we going to give to the data to the model? Well, it's training at once. Now,
6100000	6104720	what this does is we have an input function defined inside of this function. And
6104720	6111120	we say data set equals tensor frame dot data dot data set from tensor slices,
6111360	6115320	dict data frame, a label data frame. Now, what this does, and we can read the
6115320	6119240	comment, I mean, create a tf dot data dot data set object with the data and
6119240	6123360	its label. Now, I can't explain to you like how this works on a lower level. But
6123360	6127840	essentially, we pass a dictionary representation of our data frame, which is
6127840	6131840	whatever we passed in here. And then we pass the label data frame, which is
6131840	6136120	going to be, you know, all those y values. And we create this object. And
6136120	6140560	that's what this line of code does. So tf data dot data set from tensor slices,
6140680	6143320	which is just what you're going to use. I mean, we can read this documentation,
6143320	6147320	create a data set whose elements are slices of the given tensors, the given
6147320	6151000	tensors are sliced along their first dimension, this operation preserves the
6151000	6154600	structure of the input tensors, removing the first dimension of each tensor and
6154600	6157760	using it as the data set dimension. So I mean, you guys can look at that, like
6157760	6160960	read through the documentation, if you want. But essentially, what it does is
6160960	6168120	create the data set object for us. Now, if shuffle DS equals DS dot shuffle 1000,
6168280	6170880	what this does is just shuffle the data set, you don't really need to
6170880	6175040	understand more than that. And then what we do is we say data set equals data set
6175040	6179240	dot batch, the batch size, which is going to be 32, and then repeat for the
6179240	6182760	number of epochs. So what this is going to do is essentially take our data set
6182760	6188280	and split it into the number of, I don't want to, what do I want to call it? Like
6188280	6192480	blocks that are going to be passed to our model. So we can do this by knowing
6192480	6195800	the batch size, it obviously knows how many elements because that's the data set
6195800	6200040	object itself, and then repeat number of epochs. So this can figure out, you know,
6200080	6206480	how many one how many blocks do I need to split it into to feed it to my model. Now
6206480	6210560	return data set, simply from this function here, we'll return that data set
6210600	6214880	object. And then on the outside return, we actually return this function. So
6214880	6218440	what this out exterior function does, and I'm really just trying to break this
6218440	6222840	down. So you guys understand is make an input function, it literally makes a
6222840	6228320	function and returns the function object to wherever we call it from. So that's
6228320	6232920	how that works. Now we have a train input function and an eval input function. And
6232920	6236160	what we need to do to create these, it's just use this function that we've
6236160	6241320	defined above. So we say make input function, df train, y train, so our data
6241320	6244960	frame for training and our data frame for the labels of that. So we can see the
6244960	6248320	comment, you know, here, we will call the input function, right? And then eval
6248320	6252200	train, so it's going to be the same thing, except for the evaluation, we don't
6252240	6255680	need to shuffle the data because we're not training it, we only need one epoch.
6255720	6259720	Because again, we're just training it. And we'll pass the evaluation data set and
6259720	6265440	the evaluation value from Y. Okay, so that's it for making the input function. Now
6265440	6269600	I know this is complicated, but that's the way we have to do it. And unfortunately,
6269600	6272520	if you don't understand after that, there's not much more I can do, you might
6272520	6276320	just have to read through some of the documentation. Alright, creating the
6276320	6279240	model, we are finally here, I know this has been a while, but I need to get
6279280	6282360	through everything. So linear estimate, so we're going to copy this, and I'm just
6282360	6286200	going to put it in here and we'll talk about what this does. So linear underscore
6286200	6292520	EST equals tf dot estimator dot linear classifier, and we're giving it the feature
6292520	6295640	columns that we created up here. So this work was not for nothing, we have this
6295640	6300360	feature column, which defines, you know, what is in every single, like what should
6300360	6305400	we expect for our input data, we pass that to a linear classifier object from
6305400	6310320	the estimator module from TensorFlow. And then that creates the model for us. Now,
6310440	6313160	this again is syntax that you don't need to memorize, you just need to understand
6313160	6316800	how it works. What we're doing is creating an estimator, all of these kind of core
6316800	6320480	learning algorithms use what's called estimators, which are just basic
6320480	6324040	implementations of algorithms and TensorFlow. And again, pass the feature
6324040	6328880	columns. That's how that works. Alright, so now let's go to training the model. Okay,
6328880	6331360	so I'm just going to copy this again, I know you guys think I'm just copying the
6331360	6334360	code back and forth, but I'm not going to memorize the syntax, I just want to
6334360	6337480	explain to you how all this works. And again, you guys will have all this code,
6337480	6341840	you can mess with it, play with it, and learn on your own that way. So to train
6341880	6346400	is really easy. All we need to do, I say linear EST dot train, and then just give
6346400	6350400	that input function. So that input function that we created up here, right,
6350400	6354920	which was returned from make input function, like this train input function
6354920	6359600	here is actually equal to a function, it's equal to a function object itself. If I
6359640	6365160	were to call train underscore input function like this, this would actually
6365200	6369120	call this function. That's how this works in Python. It's a little bit of a
6369120	6373920	complicated syntax, but that's how it works. We pass that function here. And then
6373920	6377680	this will use the function to grab all of the input that we need and train the
6377680	6381880	model. Now the result is going to be rather than trained, we're going to
6381880	6385240	evaluate right and notice that we didn't store this one in a variable, but we're
6385240	6389360	storing the result in a variable so that we can look at it. Now clear output is
6389400	6392520	just from what we import above just going to clear the console output, because
6392520	6395880	there will be some output while we're training. And then we can present print
6395920	6400440	the accuracy of this model. So let's actually run this and see how this
6400440	6403640	works. This will take a second. So I'll be back once this is done. Okay, so we're
6403640	6409680	back, and we've got a 73.8% accuracy. So essentially what we've done right is
6409680	6412560	we've trained the model, you might have seen a bunch of output while you were
6412560	6416640	doing this on your screen. And then we printed out the accuracy after
6416680	6420480	evaluating the model. This accuracy isn't very good. But for our first shot,
6420480	6424680	this okay, we're going to talk about how to improve this in a second. Okay, so
6424680	6428040	we've evaluated the data set, we stored that in result. I want to actually look
6428040	6432120	at what result is, because obviously you can see we've referenced the accuracy
6432120	6435800	part, like you know, as if this was a Python dictionary. So let's run this one
6435800	6439440	more time. Oh, this is going to take a second again. So okay, so we printed out
6439440	6442520	result here, and we can see that we have actually a bunch of different values. So
6442520	6446560	we have accuracy, accuracy baseline, AUC, and all these different kinds of
6446560	6450240	statistical values. Now, these aren't really going to mean much to you guys, but
6450240	6453360	I just want to show you that we do have those statistics and to access any
6453360	6456480	specific one, this is really just a dictionary object. So we can just
6456480	6460520	reference the key that we want, which is what we did with accuracy. Now, notice
6460760	6465520	our accuracy actually changed here. We went to 76. The reason for this is like
6465520	6468680	I said, you know, our data is getting shuffled, it's getting put in a different
6468680	6472400	order. And based on the order in which we see data, our model will, you know,
6472440	6476360	make different predictions and be trained differently. So if we had, you know,
6476720	6481360	another epoch, right, if I change epochs to say 11 or 15, our accuracy will
6481360	6484160	change. Now it might go up, it might go down. That's something we have to play
6484160	6487480	with as you know, our machine, a machine learning developer, right? That's what
6487480	6491120	your goal is, is to get the most accurate model. Okay, so now it's time to
6491120	6494240	actually use the model to make predictions. So up until this point, we've
6494240	6497600	just been doing a lot of work to understand how to create the model, you
6497600	6501560	know, what the model is, how we make an input function, training, testing data, I
6501560	6506160	know a lot, a lot, a lot of stuff. Now to actually use this model and like make
6506240	6510000	accurate predictions with it is somewhat difficult, but I'm going to show you
6510000	6514840	how. So essentially, TensorFlow models are built to make predictions on a lot
6514840	6519320	of things at once. They're not great at making predictions on like one piece of
6519320	6523040	data, you just want like one passenger to make a prediction for, they're much
6523040	6526600	better at working in like large batches of data. Now you can definitely do it
6526600	6530160	with one, but I'm just going to show you how we can make a prediction for every
6530200	6534600	single point that's in that evaluation data set. So right now we looked at the
6534600	6538440	accuracy, and the way we determine the accuracy was by essentially comparing
6538600	6542840	the results that the predictions gave from our model versus what the actual
6542840	6546400	results were for every single one of those passengers. And that's how we came
6546400	6551800	up with an accuracy of 76%. Now if we want to actually check and get predictions
6551800	6555400	from the model and see what those actual predictions are, what we can do is use
6555440	6559760	a method called dot predict. So what I'm going to do is I'm going to say, I
6559760	6565240	guess, results like this equals, and in this case, we're going to do the model
6565240	6571520	name, which is linear EST dot predict. And then inside here, what we're going to
6571520	6576520	pass is that input function we use for the evaluation. So just like, you know, we
6576520	6580160	need to pass an input function to actually train the model, we also need to
6580160	6583680	pass an input function to make a prediction. Now this input function could
6583680	6586800	be a little bit different, we can modify this a bit if we wanted to, but to keep
6586800	6590280	things simple, we use the same one for now. So what I'm going to do is just use
6590280	6593360	this eval input function. So the one we've already created where we did, you
6593360	6597480	know, one epoch, we don't need to shuffle because it's just the evaluation set. So
6597480	6601640	inside here, we're going to eval input function. Now what we need to do though is
6601640	6605400	convert this to a list, just because we're going to loop through it. And I'm
6605400	6608480	actually going to print out this value so we can see what it is before we go to
6608480	6613440	the next step. So let's run this and have a look at what we get. Okay, so we get
6613480	6618320	logistics array, we can see all these different values. So we have, you know,
6618320	6623840	this array with this value, we have probabilities, this value. And this is
6623840	6626440	kind of what we're getting. So we're getting logistic, all classes, like
6626440	6629720	there's all this random stuff. What you hopefully should notice, and I know I'm
6629720	6633440	just like whizzing through is that we have a dictionary that represents the
6633440	6637240	predictions. And I'll see if I can find the end of the dictionary here for every
6637240	6643800	single, what is it prediction? So since we've passed, you know, 267 input data
6643800	6648360	from this, you know, eval input function, what was returned to us is a list of all
6648360	6651880	of these different dictionaries that represent each prediction. So what we
6651880	6656680	need to do is look at each dictionary so that we can determine what the actual
6656680	6662440	prediction was. So what I'm going to do is actually just present to result. I'm
6662440	6665120	wondering to result zero, because this is a list. So that should mean we can
6665120	6669200	index it. So we can actually look at one prediction. Okay, so this is the
6669280	6673440	dictionary of one prediction. So I know this seems like a lot. But this is what
6673440	6676840	we have. This is our prediction. So logistics, we get some array, we have
6676840	6681440	logistic in here in this dictionary, and then we have probabilities. So what I
6681440	6686240	actually want is probability. Now, since what we ended up having was a
6686240	6690120	prediction of two classes, right, either zero or one, we're predicting either
6690120	6693800	someone survived, or they didn't survive, or what their percentage should be. We
6693800	6699040	can see that the percentage of survival here is actually 96%. And the percentage
6699080	6703520	that it thinks that it won't survive is, you know, 3.3%. So if we want to
6703560	6709680	access this, what we need to do is click do result at some index. So whatever,
6709680	6713080	you know, one we want. So we're gonna say result. And then here, we're going to
6713080	6716680	put probabilities. So I'm just going to print that like that. And then we can
6716680	6720680	see the probabilities. So let's run this. And now we see our probabilities are
6720720	6726880	96 and 33. Now, if we want the probability of survival, so I think I
6726880	6730640	actually might have messed this up, I'm pretty sure the survival probability is
6730640	6734400	actually the last one, whereas like the non survival is the first one, because
6734400	6737400	zero means you didn't survive and one means you did survive. So that's my
6737400	6740960	bad, I messed that up. So I actually want their chance of survival, I'll index
6740960	6746720	one. So if I index one, you see we get 3.3%. But if I wanted their chance of
6746720	6750800	survival, not surviving, I would index zero. And that makes sense because zero
6750800	6754400	is you know, what we're looking like zero represents they didn't survive, whereas
6754400	6758440	one represents they did survive. So that's kind of how we do that. So that's
6758440	6761960	how we get them. Now, if we wanted to loop through all of these, we could we
6761960	6765000	could loop through every dictionary, we could print every single probability of
6765000	6768560	each person. We can also look at that person's stats and then look at their
6768560	6773000	probability. So let's see the probability of surviving is in this case, you
6773000	6777320	know, 3%, or whatever it was 3.3%. But let's look at the person that we were
6777320	6781880	actually predicting them and see if that makes sense. So if I go eval, or what
6781880	6789240	was it df eval dot loc, zero, we print that and then we print the result. What
6789240	6793840	we can see is that for the person who was male and 35 that had no siblings, their
6793840	6796680	fair was this, they're in third class, we don't know what deck they were on and
6796680	6801560	they were alone. They have a 3.3% chance of survive. Now, if we change this, we
6801600	6806000	could go like to to let's have a look at this second person and see what their
6806000	6809800	chance of survival is. Okay, so they have a higher percent chance of 38% chance
6809840	6812840	they're female, they're a little bit older. So that might be a reason why their
6812840	6816840	survival rates a bit lower. I mean, we can keep doing this and look through and
6816840	6821880	see what it is, right? If we want to get the actual value, like if this person
6821880	6828040	survived, or if they didn't survive, and what I can do is I can print df eval.
6828720	6833160	Actually, it's not going to be eval, it's going to be y underscore eval. Yeah. And
6833160	6838240	that's going to be loc three. Now this will give us if they survived or not. So
6838240	6842040	actually, in this case, that person did survive, but we're only predicting a 32%.
6842320	6845200	So you can see that that's, you know, represented in the fact that we only have
6845200	6849720	about a 76% accuracy, because this model is not perfect. And in this instance, it
6849720	6853560	was pretty bad. It's saying they have a 32% chance of surviving, but they actually
6853560	6856400	did survive. So maybe that should be higher, right? So we could change this
6856400	6861560	number, go for four, I'm just messing around and showing you guys, you know, how
6861560	6865920	we use this. So in this one, you know, same thing, this person survived, although
6866800	6870640	what is it, they only were given a 14% chance of survival. So anyways, that is
6870640	6874000	how that works. This is how we actually make predictions and look at the
6874000	6877440	predictions, you understand that now what's happening is I've converted this to
6877440	6880640	a list just because this is actually a generator object, which means it's meant
6880640	6883560	to just be looped through rather than just look at it with a list, but that's
6883560	6887680	fine, we'll use a list. And then we can just print out, you know, result at
6887680	6891080	whatever index probabilities, and then one to represent their chance of
6891080	6895000	survival. Okay, so that has been it for linear regression. Now let's get into
6895000	6899200	classification. And now we are on to classification. So essentially,
6899200	6904680	classification is differentiating between, you know, data points and separating
6904680	6908400	them into classes. So rather than predicting a numeric value, which we did
6908400	6911840	with regression earlier, so linear regression, and you know, the percentage
6911880	6916520	survival chance, which is a numeric value, we actually want to predict classes. So
6916520	6921160	what we're going to end up doing is predicting the probability that a specific
6921160	6924880	data point or a specific entry or whatever we're going to call it is within
6924880	6927960	all of the different classes it could be. So for the example here, we're going to
6927960	6932560	use flowers. So it's called the iris. I think it's the iris flower data set or
6932560	6935360	something like that. And we're going to use some different properties of flowers
6935360	6938480	to predict what species of flower it is. So that's the difference between
6938480	6942000	classification and regression. Now I'm not going to talk about the specific
6942000	6945200	algorithm we're going to use here for classification, because there's just so
6945240	6950640	many different ones you can use. But yeah, I mean, if you really care about how
6950640	6954000	they work on a lower mathematical level, I'm not going to be explaining that
6954000	6957120	because it doesn't make sense to explain it for one algorithm when there's like
6957160	6960840	hundreds and they all work a little bit differently. So you guys can kind of look
6960840	6964880	that up. And I'll tell you some resources and where you can find that. I'm also
6964880	6967880	going to go faster through this example, just because I've already covered kind
6967880	6970960	of a lot of the fundamental stuff in linear regression. So hopefully we should
6970960	6974360	get this one done a little bit quicker and move on to the next kind of aspects
6974360	6978720	in this series. Alright, so first steps, load tensor flow, import tensor flow, we've
6978720	6982440	done that already, data set, we need to talk about this. So the data that we're
6982440	6986560	using is that iris flowers data set like I talked about. And this specific data set
6986560	6989680	separates flowers into three different species. So we have these different
6989680	6993560	species. This is the information we have. So septal length width, petal length,
6993560	6996480	petal width, we're going to use that information obviously to make the
6996480	7000840	predictions. So given this information, you know, in our final model, can it tell
7000840	7005040	us which one of these flowers it's most likely to be? Okay, so what we're going to
7005040	7009320	do now is define the CSV column names and species. So the column names are just
7009320	7012640	going to define what we're going to have in our data set is like the headers for
7012640	7016240	the columns, species obviously is just the species and we'll throw them there.
7017240	7020240	Alright, so now we're going to load in our data sets. So this is going to be
7020240	7023040	different every time you're kind of working with models, depending on where
7023040	7025440	you're getting your data from. In our example, we're going to get it from
7025480	7030280	Keras, which is kind of in sub module of TensorFlow. It has a lot of useful
7030280	7035040	data sets and tools that we'll be using throughout the series. But keras.utils.get
7035040	7038400	file, again, don't really focus on this, just understand what this is going to
7038400	7043360	do is save this file onto our computer as iris training dot CSV, grab it from
7043360	7047120	this link. And then what we're going to do down here is load the train and test
7047120	7051280	and again, notice this training and this is testing into two separate data
7051280	7055040	frames. So here we're going to use the names of the columns as the CSV column
7055040	7058240	names, we're going to use the path as whatever we loaded here, header equals
7058240	7063320	zero, which just means row zero is the header. Alright, so now we will move down
7063320	7066560	and we'll have a look at our data set. So like we've done before, oh, I've got
7066560	7071200	to run this code first. CSV column names. Okay, so we've just we're just running
7071200	7075200	things in the wrong order here, apparently. Okay, so let's look at the head. So
7075200	7078360	we can see this is kind of what our data frame looks like. And notice that our
7078360	7083080	species here are actually defined numerically. So rather than before, when
7083080	7086080	we had to do that thing where you know, we made those feature columns, and we
7086080	7089800	converted the categorical data into numeric data with those kind of weird
7089800	7094360	TensorFlow tools. This is actually already encoded for us. Now zero stands
7094360	7099200	for SITOSA. And then one and two obviously stand for these ones respectively. And
7099200	7104080	that's how that works. Now these I believe are in centimeters, the septal length,
7104320	7108240	petal length, petal width, that's not super important. But sometimes you do
7108240	7112400	want to know that information. Okay, so now we're going to pop up those columns
7112400	7116480	for the species like we did before and separate that into train white test, why
7116480	7121040	and then have a look at the head again. So let's do that and run this notice that
7121040	7125200	is gone. Again, we've talked about how that works. And then these, if we want to
7125200	7129200	have a look at them, and actually, let's do this, by just having a new block,
7129200	7137880	let's say train underscore y dot, what is it dot head? If I could spell head
7137880	7141720	correctly. Okay, so we run head, and we can see this is what it looks like
7141720	7145400	nothing special. That's what we're getting. Alright, so let's delete that. Let's
7145400	7148440	look at the shape of our training data. I mean, we can probably guess what it is
7148440	7152000	already, right? We're going to have shape four, because we have four features. And
7152000	7156040	then how many entries do we have? Well, I'm sure this will tell us so 120 entries
7156200	7160520	in shape four. Awesome. That's our shape. Okay, input function. So we're moving
7160520	7162920	fast here already, we're getting into a lot of the coding. So what I'm actually
7162920	7166360	going to do is again, copy this over into a separate document, and I'll be back in
7166360	7169840	a second with all that. Okay, so input function time, we already know what the
7169880	7174040	input function does because we used it previously. Now this input function is a
7174040	7178360	little bit different than before, just because we're kind of changing things
7178360	7183640	slightly. So here, we don't actually have any, what do you call it, we don't have
7183640	7187240	any epochs, and our batch size is different. So what we've done here is
7187240	7192280	rather than actually, you know, defining like make input function, we just have
7192400	7196880	input function like this. And what we're going to do is a little bit different
7196880	7199560	when we pass this input function, I'll kind of show you it's a little bit more
7199560	7202680	complicated. But you can see that we've cleaned this up a little bit. So exactly
7202680	7205640	we're doing what we did do before, we're converting this data, which is our
7205640	7209360	features, which we're passing in here into a data set. And then we're passing
7209360	7213560	those labels as well. And then if we're training, so if training is true, what
7213560	7217440	we're going to do is say data set is equal to the data set dot shuffle. So we're
7217440	7221280	going to shuffle that information, and then repeat that. And that is all we
7221280	7225920	really need to do. We can do data set dot batch at the batch size 256 return that.
7226240	7229520	And we're good to go. So this is our input function. Again, these are kind of
7229520	7233040	complicated. You kind of have to just get experience seeing a bunch of
7233040	7236320	different ones to understand how to actually make one on your own. For now
7236320	7239360	on, don't worry about it too much. You can pretty much just copy the input
7239360	7242880	functions you've created before, and modify them very slightly if you're
7242880	7245480	going to be doing your own models. But by the end of this, you should have a
7245480	7248520	good idea of how these input functions work. We will have seen like four or
7248520	7251760	five different ones. And then, you know, we can kind of mess with them and tweak
7251760	7255800	them as we go on, but don't focus on it too much. Okay, so input function, this
7255840	7259000	is our input function, I'm not really going to go in too much more detail with
7259000	7263200	that. And now our feature columns. So this is again, pretty straightforward for
7263200	7266080	the feature columns. All we need to do for this is since our all numeric feature
7266080	7269120	columns is rather than having two for loops where we were separating the
7269120	7273960	numeric and categorical feature columns before, we can just loop through all of
7273960	7278240	the keys in our training data set. And then we can append to my feature
7278240	7283240	columns blank list, the feature column, the numeric column, and the key is
7283240	7286200	equal to whatever key we've looped through here. Now I'm going to show you
7286200	7289560	what this means in case anyone's confused. Again, you can see when I print my
7289560	7294120	feature columns, we get key equals septal length, we get our shape, and we get
7294160	7297640	all of that other nice information. So let's copy this into the other one and
7297640	7302840	have a look at our output after this. Okay, so my feature columns for key and
7302840	7308360	train dot keys. So notice train is here, train dot keys. What that does is
7308360	7312360	actually give us all the columns. So this was a really quick and easy way to
7312360	7315280	kind of loop through all the different columns. Although I could have looped
7315280	7319280	through CSV column names and just remove the species column to do that. But
7319800	7323160	again, we don't really need to. So for key and train dot keys, my feature
7323160	7326720	columns dot append tf feature column, numeric column, key equals key, this
7326720	7329080	was just going to create those feature columns, we don't need to do that
7329080	7332320	vocabulary thing and that dot unique because again, these are all already
7332320	7336280	encoded for us. Okay, awesome. So that was the next step. So let's go back
7336280	7339480	here, building the model. Okay, so this is where we need to talk a bit more
7339480	7342920	in depth of what we're actually going to build. So the model for this is a
7342920	7346920	classification model. Now there is like hundreds of different classification
7346920	7350800	models we can use that are pre made in TensorFlow. And so far, what we've done
7350800	7355000	with that linear classifier is that's a pre made model that we kind of just
7355000	7358920	feed a little bit of information to and it just works for us. Now here we have
7358920	7363120	two kind of main choices that we can use for this kind of classification task
7363120	7367120	that are pre built in TensorFlow, we have a DNN classifier, which stands for a
7367160	7371200	deep neural network, which we've talked about very vaguely, very briefly. And we
7371200	7374920	have a linear classifier. Now a linear classifier works very similarly to
7374920	7378760	linear regression, except it does classification, rather than regression. So
7378760	7382480	we get actually numeric value, or we get, sorry, you know, the labels like
7382480	7387520	probability of being a specific label, rather than a numeric value. But in this
7387520	7390680	instance, we're actually going to go with deep neural network. Now that's simply
7390680	7394760	because TensorFlow on their website, like this is all of this is kind of
7394800	7398200	building off of TensorFlow website, just all the code is very similar. And I've
7398200	7402280	just added my own spin and explain things very in depth. They've recommended
7402280	7406240	using that deep neural network for this is a better kind of choice. But typically
7406240	7409040	when you're creating machine learning apps, you'll mess around with different
7409040	7412480	models and kind of tweak them. And you'll notice that it's not that difficult to
7412480	7415400	change models, because most of the work comes from loading and kind of
7415400	7420400	pre processing our data. Okay, so what we need to do is build a deep neural
7420400	7424600	network with two hidden later, two hidden layers with 30 nodes and 10 hidden
7424600	7428320	nodes each. Now I'm going to draw out the architecture of this neural network in
7428320	7431080	just one second. But I want to show you what we've done here. So we said
7431080	7436160	classifier equals tf dot estimator. So this estimator module just stores a
7436160	7440200	bunch of pre made models from TensorFlow. So in this case, DNN classifier is
7440200	7443680	one of those. What we need to do is pass our feature columns just like we did to
7443680	7448160	our linear classifier. And now we need to define the hidden units. Now hidden
7448200	7452360	units is essentially us a building the architecture of the neural network. So
7452360	7455840	like you saw before, we had an input layer, we had some like middle layers
7455840	7458480	called our hidden layers in a neural network. And then we had our output
7458480	7462080	layer. I'm going to explain neural networks in the next module. So this will
7462080	7466120	all kind of click and make sense. For now, we've arbitrarily decided 30 nodes in
7466120	7470520	the first hidden layer, 10 in the second, and the number of classes is going to
7470520	7473320	be three. Now that's something that we need to decide. We know there's three
7473320	7477200	classes for the flowers. So that's what we've defined. Okay, so let's copy this
7477200	7482440	in. Go back to the other page here. And that is now our model. And now it is
7482440	7485920	time to talk about how we can actually train the model, which is coming down
7485920	7489280	here. Okay, so I'm going to copy this, I'm going to paste it over here and
7489280	7492480	let's just dig through this because this is a bit more of a complicated piece of
7492480	7496040	code than we usually used to work with. I'm also going to remove these comments
7496040	7499880	just to clean things up in here. So we've defined the classifier, which is a
7499880	7503040	deep neural network classifier, we have our feature columns hidden units
7503040	7508440	classes. Now to train the classifier. So we have this input function here. This
7508440	7511600	input function is different than the one we created previously. Remember when we
7511600	7515880	had previously was like make input, whatever function, I won't continue
7515880	7519720	typing in the inside it to find another function. And it actually returned that
7519720	7523840	function from this function. I know, complicated. If you're not a Python kind
7523840	7527720	of pro, I don't expect that to make perfect sense. But here, we just have a
7527720	7530960	function, right? We do not returning a function from another function, it's just
7531000	7537520	one function. So when we want to use this to train our model, what we do is
7537520	7541640	create something called a lambda. Now a lambda is an anonymous function that can
7541640	7545960	be defined in one line. When you write lambda, what that means is essentially
7545960	7550520	this is a function. So this is a function. And whatever's after the colon is what
7550520	7554960	this function does. Now this is a one line function. So like, if I create a
7554960	7562840	lambda here, right, and I say lambda, print, hi, and I said, x equals lambda,
7562880	7568040	and I called x like that, this works, this is a valid line of syntax. Actually,
7568040	7570920	I want to make sure that I'm not just like messing with you. And I say that
7570920	7575040	and then this is actually correct. Okay, so sorry, I just accidentally trained
7575040	7577960	the model. So I just commented that out. You can see we're printing high, right?
7577960	7581200	At the bottom of the screen, I know it's kind of small, but does say hi. That's
7581200	7583840	how this works. Okay, so this is a cool thing. If you haven't seen this in
7583880	7587840	Python before, that's what a lambda does allows you to define a function in one
7587840	7591760	line. Now the thing that's great about this is that we can say, like, you know,
7591800	7595000	x equals lambda, and here put another function, which is exactly what will be
7595000	7598960	done with this print function. And that means when we call x, it will, you know,
7599000	7602320	execute this function, which will just execute the other function. So it's kind
7602320	7606200	of like a chain where you call x, x is a function. And inside that function, it
7606200	7608920	does another function, right? It just like calling a function from inside a
7608920	7615080	function. So what is lambda doing here? Well, since we need the actual function
7615120	7621200	object, what we do is we define a function that returns to us a function. So this
7621200	7626120	actually just like it calls this function, when you put this here. Now there's no
7626160	7629480	I can't it's it's very difficult to explain this if you don't really
7629480	7633600	understand the concept of lambdas, and you don't understand the input functions. But
7633600	7637560	just know we're doing this because of the fact that we didn't embed another
7637600	7642160	function and return the function object. If we had done that, if we had done that,
7642200	7646080	you know, input function that we had created before where we had the interior
7646080	7649720	function, then we wouldn't need to do this because what would happen is we would
7649720	7656000	return the input function, right, like that, which means when we passed it into
7656000	7660920	here, it could just call that directly. It didn't need to have a lambda. Whereas
7660920	7666120	here, though, since we need to just put a lambda, we need to define what this is
7666120	7669280	and then and then this works. That's just there's no other way to really explain
7669280	7673640	this. So yeah, what we do is we create this input function. So we pass we have
7673640	7678160	train, we have train y and we have training equals true. And then we do steps
7678160	7682880	equals 5000. So this is similar to an epoch, except this is just defining a set
7682880	7685760	amount of steps we're going to go through. So rather than saying like we'll go
7685760	7688840	through the data set 10 times, we're just going to say we'll go through the data
7688840	7693720	set until we fit 5000 numbers, like 5000 things that have been looked at. So
7693760	7696960	that's what this does with that train. Let's run this and just look at the
7696960	7701240	training output from our model, it gives us some like, things here, we can kind of
7701240	7705160	see how this is working. Notice that if I can stop here for a second, it tells us
7705160	7710080	the current step, it tells us the loss, the lowest, the lower this number, the
7710080	7714200	better. And then it tells us global steps per second. So how many steps we're
7714200	7720120	completing per second. Now at the end here, we get final step loss of 39, which
7720120	7725240	is pretty high, which means this is pretty bad. But that's fine. This is kind
7725240	7728560	of just our first test at training in neural network. So this is just giving
7728560	7731800	us output while it's training to kind of save what's happening. Now, in our case,
7731800	7735160	we don't really care because this is a very small model. When you're training
7735160	7738720	models that are massive and take terabytes of data, you kind of care about the
7738720	7743080	progress of them. So that's when you would use kind of that output, right? And
7743080	7745760	you would actually look at that. Okay, so now that we've trained the model, let's
7745760	7749520	actually do an evaluation on the model. So we're just going to say classifier dot
7749560	7753240	evaluate. And what we're going to do is a very similar thing to what we've done
7753240	7758080	here is just pass this input function, right, like here with a lambda once
7758080	7762840	again, and reason we add the lambda when we don't have this like, double
7762840	7766200	function going on, like a nested function, we need the lambda. And then in
7766200	7770200	here, what we do is rather than passing train and train y, we're going to pass
7770240	7775720	test, I believe. And I think it's I just called it test y. Okay, and then for
7775720	7780360	training, obviously, this is false. So we can just set that false like that. I'm
7780360	7782760	just going to look at the other screen and make sure I didn't mess this up. Because
7782760	7786720	again, I don't remember the syntax. Yes, a cluster classifier dot evaluate test
7786720	7790520	test y looks good to me. We'll take this print statement just so we get a nice
7790560	7795280	output for our accuracy. Okay, so let's look at this. Again, we're going to have
7795280	7799040	to wait for this to train. But I will show you a way that we don't need to wait
7799040	7803840	for this to train every time in one second. And I'll be right back. Okay, so
7803880	7806800	what I'm actually going to do, and I've just kind of pause like the execution of
7806800	7811720	this code is throw this in the next block under, because the nice thing about
7811720	7815800	Google Collaboratory is that I can run this block of code, right, I can train
7815800	7819160	all this stuff, which is what I'll run now while we're talking just so it happens.
7819440	7823560	And then I can have another code block kind of below it, which I have here. And
7823600	7826800	it doesn't matter. I don't need to rerun that block every time I change
7826800	7829720	something here. So if I change something in any lower blocks, I don't need to
7829720	7832720	change the upper block, which means I don't need to wait for this to train every
7832720	7836600	time I want to do an evaluation on it. Anyways, so we've done this, we got test,
7836600	7840880	we got test why I just need to change this instead of eval result. Actually, I
7840880	7846600	need to say eval underscore result equals classifier dot evaluate so that we can
7846600	7849960	actually store this somewhere and get the answer. And now we'll print this and
7849960	7854520	notice this happens much, much faster. We get a test accuracy of 80%. So if I were
7854520	7858440	to retrain the model, chances are this accuracy would change again, because of
7858440	7861640	the order in which we're seeing different flowers. But this is pretty decent
7861680	7866000	considering we don't have that much test data. And we don't really know what
7866000	7868440	we're doing, right? We're kind of just messing around and experimenting for
7868440	7872120	right now. So to get 80% is pretty good. Okay, so actually, what am I doing? We
7872120	7875200	need to go back now and do predictions. So how am I going to predict this for
7875200	7879440	specific flowers? So let's go back to our core learning algorithms. And let's go
7879440	7884120	to predictions. Now, I've written a script already, just to save a bit of time
7884120	7889480	that allows us to do a prediction on any given flower. So what I'm going to do is
7889520	7893320	create a new block down here, code block and copy this function in. Now we're
7893320	7896400	going to digest this and kind of go through this on our own to make sure
7896400	7900720	this makes sense. But what this little script does is allow the user to type in
7900720	7905200	some numbers. So the septal length width, and I guess petal length and width, and
7905200	7909840	then it will spit out to you what the predicted class of that flower is. So we
7909840	7914200	could do a prediction on every single one of our data points like we did
7914200	7916800	previously. And we already know how to do that. I showed you that with linear
7916840	7921760	regression. But here I just wanted to do it on one entry. So what do we do? So I
7921760	7926360	start by creating a input function, it's very basic, we have batch size 256. All
7926360	7930280	we do is we give some features, and we created data set from those features
7930320	7935120	that's a dict and then dot batch and the batch size. So what this is doing is
7935120	7939080	notice we don't give any y value, right? We don't give any labels. The reason we
7939080	7942880	do we don't do that is because when we're making a prediction, we don't know
7942880	7947080	the label, right? Like we actually want that the model to give us the answer. So
7947080	7950480	here I wrote down the features, I created a predict dictionary, just because I'm
7950480	7953880	going to add things to it. And then I just prompted here with a print statement,
7953880	7959360	please type numeric values as prompted. So for feature and feature, valid equals
7959360	7963280	true, well valid, valid equals input feature colon. So this just means what
7963280	7966120	we're going to do is for each feature, we're going to wait to get some valid
7966120	7970960	response. Once we get some valid response, what we're going to do is add that to
7971000	7975000	our dictionary. So we're going to say predict feature. So whatever that feature
7975000	7980280	was, so septal length, septal width, petal length or pep, petal width is equal
7980280	7985560	to a list that has in this instance, whatever that value was. Now the reason
7985560	7989360	we need to do this is because again, the predict method from TensorFlow works
7989360	7993280	on predicting for multiple things, not just one value. So even if we only have
7993280	7996800	one value we want to predict for it, we need to put it inside of a list because
7996800	8000320	it's expecting the fact that we will probably have more than one value in
8000360	8003920	which we would have multiple values in the list, right, each representing a
8003920	8008320	different row or a new flower to make a prediction for. Okay, now we say
8008320	8011720	predictions equals classifier dot predict. And then in this case, we have input
8011720	8016240	function lambda input function predict, which is this input function up here. And
8016240	8020280	then we say for prediction dictionaries, because remember, every prediction comes
8020280	8024200	back as a dictionary in predictions, we'll say the class ID is equal to
8024240	8029160	whatever the class IDs of the prediction dictionary at zero. And these are
8029160	8034840	simply what I don't know exactly how to explain this. We'll look at in a second,
8034840	8038080	and I'll go through that. And then we have the probability is equal to the
8038080	8043640	prediction dictionary probabilities of class ID. Okay, then we're going to say
8043640	8047960	print prediction is we're going to do this weird format thing, I just stole
8047960	8052360	this from TensorFlow. And it's going to be the species at the class ID, and then
8052360	8055640	100 times probability, which will give us actual integer value, we're going to
8055640	8058600	digest this, but let's run this right now and have a look. So please type
8058640	8064160	numeric values as prompted septal length, let's type like 2.4, septal width 2.6,
8064600	8070320	petal width, let's just say that's like 6.5. And yeah, petal width like 6.3. Okay,
8070360	8074520	so then it calls this and it says prediction is virginica, I guess that's
8074560	8079840	the the class we're going with. And it says that's an 83 or 86.3% chance that
8079840	8083920	that is the prediction. So yeah, that is how that works. So that's what this
8083920	8087000	does. I wanted to give a little script, I wrote most of this, I mean, I stole
8087040	8090760	some of this from TensorFlow. But just to show you how we actually predict on
8090760	8095360	one value. So let's look at these prediction dictionary, because I just
8095360	8099080	want to show you what one of them actually is. So I'm going to say print,
8100240	8103760	pred underscore dict. And then this will allow me to actually walk through what
8103760	8108400	class IDs are probabilities are and how I've kind of done this. So let's run
8108400	8113640	this up to length. Okay, let's just go like 1.4, 2.3. I don't know what these
8113640	8119840	values are going to end up being. And we get prediction is same one with 77.1%,
8119840	8123680	which makes sense, because these values are similar kind of in difference to what
8123680	8127360	I did before. Okay, so this is the dictionary. So let's look for what we
8127360	8131480	were looking for. So probabilities notice we get three probabilities, one for
8131520	8135320	each of the different classes. So we can actually say what, you know, the
8135880	8140760	percentages for every single one of the predictions. Then what we have is class
8140800	8146920	IDs. Now class IDs, what this does is tell us what class ID, it predicts is
8146960	8150600	actually the flower, right? So here it says two, which means that this
8150600	8155000	probability at 77%. That's that index two in this array, right? So that's why
8155000	8159680	this value is two. So it's saying that that class is to it thinks it's class
8159680	8165280	two, like that's whatever was encoded in our system is two. And that's how that
8165280	8170400	works. So that's how I know, which one to print out is because this tells me
8170400	8175160	it's class two. And I know for making this list all the way back up here, if I
8175160	8180920	can get rid of this output. Where is it? When I say species, that number two is
8180920	8185200	virginica, or I guess that's how you say it. So that is what the classification
8185200	8187840	is. So that's what the prediction is. So that's how I do that. And that's how
8187840	8193840	that works. Okay, so I think that is pretty much it for actually classification. So
8193840	8196800	it was pretty basic. I'm going to go and see if there's anything else that I did
8196840	8201160	for classification in here. Okay, so here, I just put some examples. So here's
8201160	8204280	some example input and expected classes. So you guys could try to do these if you
8204280	8211920	want. So for example, this one, septal length, septal width. So for 5.1, 3.3, 1.7
8211920	8218280	and 0.5, the output should be Satosa. For 5.9, 3.0, 4.2, 1.5, it should be this
8218280	8222720	one. And then obviously this for this, just so you guys can mess with them if you
8222720	8227080	want. But that's pretty much it for classification. And now on to clustering.
8228680	8233160	Okay, so now we're moving on to clustering. Now clustering is the first unsupervised
8233520	8237600	learning algorithm that we're going to see in this series. And it's very powerful. Now
8237600	8242720	clustering only works for a very specific set of problems. And you use clustering
8242720	8246760	when you have a bunch of input information or features, you don't have any labels or
8246760	8259440	open information. Essentially, what clustering does is finds clusters of like data points
8259640	8263800	and tells you the location of those clusters. So you give a bunch of training data, you
8263800	8268200	can pick how many clusters you want to find. So maybe we're going to be classifying digits
8268200	8272080	right handwritten digits using k means clustering. In that instance, we would have 10
8272080	8276760	different clusters for the digits zero through nine. And you pass all this information and
8276760	8281160	the algorithm actually finds those clusters in the data set for you. We're going to walk
8281160	8285520	through an example that'll make sense. But I just want to quickly explain the basic
8285560	8290600	algorithm behind k means essentially the set of steps. These are going to walk you through
8290600	8296040	them and with a visual example. So we're going to start by randomly picking k points to
8296040	8301720	place k centroids. Now a centroid stands for where our current cluster is kind of
8301760	8305880	defined. And we'll see it in a second. The next step is we're going to assign all of the
8305880	8310640	data points to the centroids by distance. So actually, now that I'm talking about this,
8310640	8313920	I think it just makes more sense to get right into the example, because if I keep talking
8313920	8316480	about this, you guys are probably just going to be confused, although I might come back
8316480	8321760	to this just to reference those points. Okay, so let's create a little graph like this in
8321760	8326240	two dimensions for our basic example. And let's make some data points here. So I'm just
8326240	8329520	going to make them all read. And you're going to notice that I'm going to make this kind
8329520	8333960	of easier for ourselves by putting them in like their own unique little groups, right?
8333960	8340120	So actually, we'll add one up here. Then we can add some down here and down here. Now
8340120	8344600	the algorithm starts for k means clustering. And you guys understand how this works as
8344600	8351040	we continue by randomly picking k centroids. Now I'm going to denote a centroid by a little
8351040	8357320	filled in triangle like this. And essentially what these are is where these different clusters
8357360	8362280	currently exist. So we start by randomly picking k, which is what we've defined. So like me
8362280	8367360	in this instance, that we're going to say k equals three, k centroid, wherever. So maybe
8367360	8371680	we put one, you know, somewhere like here, you know, I might not bother filling these
8371680	8375480	in because they're going to take a while. Maybe we pull in here, maybe we end up putting
8375480	8380920	one over here. Now, I've kind of put them close to where clusters are, but these are
8380920	8387080	going to be completely random. Now what happens next is each group, or each data point is
8387080	8392520	assigned to a cluster by distance. So essentially, what we do is for every single data point
8392520	8396640	that we have, we find what's known as the Euclidean distance, or it actually could be
8396640	8400760	a different distance, you'd use like Manhattan distance, if you guys know what that is, to
8400760	8404720	all of the centroids. So let's say we're looking at this data point here, what we do is find
8404720	8410800	the distance to all of these different centroids. And we assign this data point to the closest
8410800	8414680	centroid. So the closest one by distance. Now in this instance is looking like it's
8414680	8418000	going to be a bit of a tie between this centroid and this centroid. But I'm going to give it
8418000	8423720	to the one on the left. So what we do is we're going to say this is now a part of this centroid.
8423720	8428000	So if I'm calling this like, let's just say this is centroid one, this is centroid two,
8428000	8431800	and this is centroid three, then this now is going to be a part of centroid one, because
8431800	8435640	it's closest to centroid one. And we can go through and we do this for every single data
8435640	8440120	point. So obviously, we know all of these are going to be our ones, right? And we know
8440160	8444360	these are going to be our two. So two, two, two. And then these are obviously going to
8444360	8448840	be our three. Now I'm actually just going to add a few other data points, because I want
8448840	8455000	to make this a little bit more sophisticated, almost, if that makes any sense. So add those
8455000	8458920	data points here, we've been add one here, and that will give these labels. So these
8458920	8462080	ones are close. So I'm going to say this one's one, I'm going to say this one's two, I know
8462080	8466200	it's not closest to it. But just because I want to do that for now, we'll say two for
8466200	8470120	that. And we'll say three here. Okay, so now that we've done that, we've labeled all these
8470120	8475760	points, what we do is we now move these centroids that we've defined into the middle of all
8475760	8481800	of their data points. So what I do is I essentially find it's called center of mass, the center
8481800	8486120	of mass between all of the data points that are labeled the same. So in this case, these
8486120	8489360	will be all the ones that are labeled the same. And I take this centroid, which I'm
8489360	8493640	going to have to erase, get rid of it here. And I put it right in the middle. So let's
8493680	8498080	go back to blue. And let's say the middle of these data points ends up being somewhere
8498080	8502920	around here. So we put it in here. And this is what we call center of mass. And this again,
8502920	8507840	it'd be centroid two. So let's just erase this. And there we go. Now we do the same
8507840	8513120	thing with the other centroid. So let's remove these ones, to remove these ones. So for three,
8513120	8518680	I'm saying it's probably going to be somewhere in here. And then for one, our center of mass
8518760	8525760	is probably going to be located somewhere about here. Now what I do is I repeat the process
8525760	8530760	that I just did. And I reassign all the points now to the closest centroid. So all these
8530760	8534960	points are labeled one to all that, you know, we can kind of remove their labels. And this
8534960	8539440	is just going to be great. Me trying to erase the labels, I shouldn't have wrote them on
8539440	8543040	top. But essentially, what we do is we're just going to be like reassigning them. So
8543040	8546440	I'm going to say, okay, so this is two, and we just do the same thing as before, find
8546440	8550600	the closest distance. So we'll say, you know, these can stay in the same cluster, maybe
8550600	8556280	this one actually here gets changed to one now, because it's closest to centroid one.
8556280	8559440	And we just reassigned all these points. And maybe, you know, this one now, if it was two
8559440	8564200	before, let's say like this one's one, and we just reassigned them. Now we repeat this
8564200	8570920	process of finding the closest or assigning all the points that are closest centroid,
8570920	8575520	moving the centroid into the center of mass, and we keep doing this until eventually we
8575600	8580320	reach a point where none of these points are changing which centroid they're a part of.
8580320	8584000	So eventually we reach a point where I'm just going to erase this and draw like a new graph
8584000	8587320	because it'll be a little bit cleaner. But what we have is, you know, like a bunch of
8587320	8593080	data points. So we have some over here, some over here, maybe we'll just put some here,
8593080	8597440	and maybe we'll do like a K equals four example for this one. And we have all these centroids
8597440	8601760	and I'll just draw these centroids with blue again, that are directly in the middle of
8601760	8605240	all of their data points, they're like as in the middle as they can get, none of our
8605280	8610920	data points have moved. And we call this now our cluster. So now we have these clusters,
8610920	8614280	we have these centroids, right, we know where they are. And what we do is when we have a
8614280	8618720	new data point that we want to make a prediction for or figure out what cluster it's a part
8618720	8623360	of, what we do is we will plot that data point. So let's say it's this new data point here,
8623360	8628200	we find the distance to all of the clusters that exist. And then we assign it to the closest
8628200	8632360	one. So obviously it would be assigned to that one. And we can do this for any data point,
8632360	8636560	right? So even if I put a data point all the way over here, well, it's closest cluster
8636560	8642160	is this, so it gets assigned to this cluster. And my output will be whatever this label
8642160	8646200	of this cluster is. And that's essentially how this works, you're just clustering data
8646200	8649680	points, figuring out which ones are similar. And this is a pretty basic algorithm, I mean,
8649680	8653040	you draw your little triangle, you find the distance from every point of the triangle,
8653040	8657480	or to all of the triangles, actually. And then what you do is just simply assign those
8657480	8661840	values to that centroid, you move that centroid to the center of mass, and you repeat this
8661860	8665760	process constantly, until eventually you get to a point where none of your data points
8665760	8670240	are moving. That means you found the best clusters that you can, essentially. Now, the
8670240	8674440	only thing with this is you do need to know how many clusters you want for k means clustering,
8674440	8678280	because k is a variable that you need to define. Although there is some algorithms that can
8678280	8682600	actually determine the best amount of clusters for a specific data set. But that's a little
8682600	8686120	bit beyond what we're going to be focused on focusing on right now. So that is pretty
8686120	8690280	much clustering. There's not really much more to talk about it, especially because we can't
8690280	8695720	really code anything for it now. So we're going to move on to hidden Markov models. Now
8695720	8699920	hidden Markov models are way different than what we've seen so far, we've been using kind
8699920	8704560	of algorithms that rely on data. So like k means clustering, we gave a lot of data,
8704560	8708800	and we know clustered all those data points found those centroids, use those centroids
8708800	8714000	to find where new data points should be. Same thing with linear regression and classification.
8714000	8718680	Whereas hidden Markov models, we actually deal with probability distributions. Now,
8718680	8721800	the example we're going to go into here, and it's kind of I have to do a lot of examples
8721800	8727080	for this because it's a very abstract concept is a basic weather model. So what we actually
8727080	8734320	want to do is predict the weather on any given day, given the probability of different events
8734320	8738640	occurring. So let's say we know, you know, maybe in like a simulated environment or something
8738640	8744560	like that, this might be an application, that we have some specific things about our environment,
8744640	8748920	we know if it's sunny, there's an 80% chance that the next day, it's going to be sunny
8748920	8753320	again, and a 20% chance that it's going to rain. Maybe we know some information about
8753320	8757780	sunny days and about cold days. And we also know some information about the average temperature
8757780	8763160	on those days. Using this information, we can create a hidden Markov model that will
8763160	8768600	allow us to make a prediction for the weather in future days, given kind of that probability
8768600	8772360	that we've discovered. Now you might be like, Well, how do we know this? Like how do I know
8772400	8776840	this probability? A lot of the times you actually do know the probability of certain events
8776840	8780480	occurring or certain things happening, which makes these models really good. But there's
8780480	8784600	some times where what you actually do is you have a huge data set, and you calculate the
8784600	8789240	probability of things occurring based on that data set. So we're not going to do that part
8789240	8791760	because that's just kind of going a little bit too far. And the whole point of this is
8791760	8796200	just to introduce you to some different models. But in this example, what we will do is use
8796200	8800720	some predefined probability distributions. So let me just read out the exact definition
8800720	8804200	of a hidden Markov model will start going more in depth. So the hidden Markov model
8804200	8808640	is a finite set of states, each of which is associated with a generally multi dimensional
8808640	8813160	probability distribution, transitions among the states are governed by a set of probabilities
8813160	8819040	called transition probabilities. So in a hidden Markov model, we have a bunch of states. Now
8819040	8822200	in the example that I was just talking about with this weather model, the states we would
8822200	8829520	have is hot day and cold day. Now, these are what we call hidden, because never do we actually
8829560	8834880	access or look at these states, while we interact with the model. In fact, what we look at is
8834880	8839320	something called observations. Now at each state, we have an observation, I'll give you
8839320	8845200	an example of an observation. If it is hot outside, Tim has an 80% chance of being happy.
8845200	8850600	If it is cold outside, Tim has a 20% chance of being happy. That is an observation. So
8850600	8856720	at that state, we can observe the probability of something happening during that state is
8856760	8862800	x, right, or is y or whatever it is. So we don't actually care about the states in particular,
8862800	8866440	we care about the observations we get from that state. Now in our example, what we're
8866440	8870880	actually going to do is we're going to look at the weather as an observation for the state. So
8870880	8875760	for example, on a sunny day, the weather has, you know, the probability of being between
8875760	8880840	five and 15 degrees Celsius, with an average temperature of 11 degrees. That's like, that's
8880840	8885720	a probability we can use. Now I know this is slightly abstract, but I just want to talk
8885720	8889440	about the data we're going to work with here. I'm going to draw out a little example, go
8889440	8892840	through it and then we'll actually get into the code. So let's start by discussing the
8892840	8897120	type of data we're going to use. So typically in previous ones, right, we use like hundreds,
8897120	8902240	if not like thousands of entries or rows or data points for our models to train. For this,
8902240	8906960	we don't need any of that. In fact, all we need is just constant values for probability
8906960	8912280	and or what is it transition distributions and observation distributions. Now what I'm
8912320	8916720	going to do is go in here and talk about states observations and transitions. So we have a
8916720	8922000	certain amount of states. Now we will define how many states we have, we don't really care
8922000	8927280	what that state is. So we could have states, for example, like warm, cold, high, low, red,
8927280	8931480	green, blue, you have as many states as we want, we could have one state to be honest,
8931480	8934760	although that would be kind of strange to have that. And these are called hidden because
8934760	8939640	we don't directly observe. Now observations. So each state has a particular outcome or
8939720	8944200	observation associated with it based on a probability distribution. So it could be the
8944200	8950440	fact that during a hot day, it is 100% true that Tim is happy. Although, in a hot day, we
8950440	8956080	could observe that 80% of the time Tim is happy. And 20% of the time, he is sad, right? Those
8956080	8960640	are observations we make about each state. And each state will have their different observations
8960640	8966360	and different probabilities of those observations occurring. So if we were just going to have
8966440	8970400	like an outcome for the state, that means it's always the same, there's no probability that
8970400	8974800	something happens. And in that case, that's just called an outcome, because the probability
8974800	8980600	of the event occurring will be 100%. Okay, then we have transitions. So each state will
8980600	8984880	have a probability defining the likelihood of transitioning to a different state. So
8984880	8989840	for example, if we have a hot day, there will be a percentage chance the next day will be
8989840	8993160	a cold day. And if we have a cold day, there will be a percentage chance that the next
8993160	8997680	day is either a hot day or a cold day. So we're going to go through like the exact what
8997680	9002160	we have for our specific model below. But just understand there's a probability that
9002160	9006400	we could transition into a different state. And from each state, we can transition into
9006400	9012280	every other state or a defined set of states given a certain probability. So I know it's
9012280	9017360	a mouthful, I know it's a lot. But let's go into a basic drawing example, because I just
9017360	9021320	want to illustrate like graphically a little bit kind of how this works. In case these
9021360	9026120	are ideas are a little bit too abstract for any of you. Okay, I'm just pulling out the
9026120	9032040	drawing tablet, just one second here, and let's do this basic weather model. So what
9032040	9036840	I'm going to do is just simply draw two states. Actually, let's do it with some colors because
9036840	9040960	why not? So we're going to use yellow. And this is going to be our hot day. Okay, this
9040960	9045640	is going to be our sun. And then I'm just going to make a cloud. We'll just do like
9045640	9050520	a gray cloud. This will be my cloud. And we'll just say it's going to be raining over here.
9050520	9055360	Okay, so these are my two states. Now, in each state, there's a probability of transitioning
9055360	9062440	to the other state. So for example, in a hot day, we have a let's say 20% chance of transitioning
9062440	9068720	to a cold day. And we have a 80% chance of transitioning to another hot day, like the
9068720	9074480	next day, right? Now in a cold day, we have let's say a 30% chance of transitioning to
9074520	9080760	a hot day. And we have in this case, what is that going to be a 70% chance of transitioning
9080760	9085360	to another cold day. Now, on each of these days, we have a list of observations. So these
9085360	9089280	are what we call states, right? So this could be s one, and this could be s two, it doesn't
9089280	9092960	really matter, like if we name them or anything, we just we have two states, that's what we
9092960	9097600	know. We know the transition probability, that's what we've just defined. Now we want
9097600	9102880	the observation probability or distribution for that. So essentially, on a hot day, our
9102920	9110000	observation is going to be that the temperature could be between 15 and 25 degrees Celsius
9110000	9117600	with an average temperature of let's say 20. So we can say observation, right? So say observation,
9117600	9123280	and we'll say that the mean so the average temperature is going to be 20. And then the
9123280	9129200	distribution for that will be like the minimum value is going to be 15. And the max is going
9129200	9134120	to be 25. So this is what we call actually like a standard deviation. I'm not really
9134120	9137200	going to explain exactly what standard deviation is, although you can kind of think of it as
9137200	9142560	something like this. So essentially, there's a mean, which is the middle point, the most
9142560	9147480	common event that could occur. And at different levels of standard deviation, which is going
9147480	9150640	into statistics, which I don't really want to mention that much, because I'm definitely
9150640	9155760	non expert. We have a probability of hitting different temperatures as we move to the left
9155800	9161160	and right of this value. So on this curve somewhere, we have 15. And on this curve to the
9161160	9164880	right somewhere, we have 25. Now, we're just defining the fact that this is where we're
9164880	9169960	going to kind of end our curve. So we're going to say that like the probability is in between
9169960	9174280	these numbers, it's going to be in between 15 and 25 with an average of 20. And then our
9174280	9178760	model will kind of figure out some things to do with that. That's as far as I really
9178760	9183320	want to go in standard deviation. And I'm sure that's like a really horrible explanation.
9183360	9186040	That's kind of the best I'm going to give you guys for right now. Okay, so that's our
9186040	9189360	observation here. Now our observation over here is going to be similar. So we're going
9189360	9194160	to say mean on a cold day, temperature is going to be five degrees. We'll say the minimum
9194160	9198480	temperature maybe is going to be something like negative five and the max could be something
9198480	9204760	like 15 or like, yeah, we can say 15. So we'll have some distribution, not just what we want
9204760	9208720	to understand, right? And this is kind of a strange distribution because we're dealing
9208760	9214080	with what is it standard deviation, although we can just deal with like straight percentage
9214080	9218480	observations. So for example, you know, there's a 20% chance that Tim is happy, or there's an
9218480	9224800	80% chance that he is sad, like those are probabilities that we can have as our observation
9224800	9229840	probabilities in the model. Okay, so there's a lot of lingo. There's a lot going on, we're
9229840	9233560	going to get into like a concrete example now. So hopefully this should make more sense. But
9233560	9237680	again, just understand states, transitions, observations, we don't actually ever look at
9237680	9241800	the states, we just have to know how many we have, and the transition probability and
9241800	9247880	observation probability in each of them. Okay, so what I want to say now, though, is what
9247880	9251960	do we even do with this model? So once I make this right, once I make this hidden markup
9251960	9255560	model, what's the point of it? Well, the point of it is to predict future events based on
9255560	9260320	past events. So we know that probability distribution. And I want to predict the weather for the
9260320	9264440	next week, I can use that model to do that, because I can say, well, if the current day
9264520	9269000	today is warm, then what is the likelihood that the next day tomorrow is going to be
9269000	9273480	cold, right? And that's what we're kind of doing with this model, we're making predictions
9273480	9280160	for the future based on probability of past events occur. Okay, so imports and so let's
9280160	9286000	just run this already loaded import tensorflow. And notice that here I've imported tensorflow
9286000	9292680	probability is TFP. This is because this is a separate module from TensorFlow that
9292720	9298200	deals with probability. Now, we also need tensorflow two. But for this hidden markup
9298200	9302560	model, we're going to use the tensorflow probability module, not a huge deal. Okay, so
9302560	9306960	weather model. So this is just going to define what our model actually is so the different
9306960	9311440	parts of it. So this is taken directly from the documentation of tensorflow. You guys can
9311440	9315280	see you know, where I have all this information from like I've sourced all of it. But essentially
9315280	9318480	what the model we're going to try to create is that cold days are encoded by zero and
9318520	9323200	hot days are encoded by one. The first day in our sequence has an 80% chance of being
9323200	9326680	cold. So whatever day we're starting out at has an 80% chance of being cold, which would
9326680	9332000	mean 20% chance of being warm. A cold day has a 30% chance of being followed by hot day.
9332000	9335360	And a hot day has a 20% chance of being followed by a cold day, which would mean you know,
9335360	9340880	70% cold to cold and 80% hot to hot. On each day, the temperature is normally distributed
9340880	9345120	with mean and standard deviation zero and five on a cold day and mean and standard deviation
9345160	9349760	15 and 10 on a hot day. Now what that means standard deviation is essentially I mean, we
9349760	9354520	can read this thing here is that on a hot day, the average temperature is 15 that's mean
9354520	9359840	and ranges from five to 25 because the standard deviation is 10 of that, which just means 10
9359840	9364680	on each side kind of the min max value. Again, I'm not in statistics. So please don't quote
9364680	9368480	me on any definitions of standard deviation. I just trying to explain it enough so that
9368480	9373200	you guys can understand what we're doing. Okay, so what we're going to do to model this
9373200	9377160	and I'm just kind of going through this fairly quickly because it's pretty easy to really do
9377160	9383920	this is I'm going to load the TensorFlow probability distributions kind of module and just save
9383920	9389400	that as TFD. And I'm just going to do that so I don't need to write TFP dot distributions dot
9389400	9393400	all of this, I can just kind of shortcut it. So you'll notice I'm referencing TFD here,
9393400	9398560	which just stands for TFP dot distributions and TFP is TensorFlow probability. Okay,
9398720	9405200	so my initial distribution is TensorFlow probability distributions dot categorical. This
9405200	9410560	is probability of 80% and 20%. Now this refers to point two. So let's look at point two. The
9410560	9415040	first day in our sequence has an 80% chance of being cold. So we're saying that that's
9415040	9420640	essentially what this is the initial distribution of being cold is 80%. And then 20% after categorical
9420640	9426440	is just a way that we can do this distribution. Okay, so transition distribution. What is
9426440	9434520	it TensorFlow probability categorical, the probability is 70% and 30% and 20% 80%. Now
9434520	9439880	notice that since we have two states, we've defined two probabilities. Notice since we have two
9439880	9444600	states, we have defined two probabilities, the probability of landing on each of these states
9444600	9448920	at the very beginning of our sequence. This is the transition probability referred to points three
9448920	9456120	and four above. So this is what we have here. So cold is 30% chance 20% chance for a hot day. And
9456120	9460680	that's what we've defined. So we say this is going to be cold day state one, we have 70% chance
9460680	9464840	of being cold day again, we have 30% chance of going hot day and then you know, reverse here.
9465720	9471400	Okay, so observation distribution. Now this one is a little bit different, but essentially we do
9471400	9476680	tfd dot normal. Now I don't know, I'm not going to explain exactly what all this is, but when you're
9476680	9480520	doing standard deviation, you're going to do it like this, where you're going to say, look, which
9480520	9485000	stands for your average or your mean, right? So that was our average temperature is going to be
9485000	9490600	zero on a hot day, 15 on a cold day. The standard deviation on the cold days five, which means we
9490600	9497400	range from five, or negative five to five degrees. And on a hot day, it's 10. So that is going to be
9497400	9502360	we go range from five to 25 degrees, and our average temperature is 15. Now the reason we've
9502360	9507240	added dot here is because these just need to be float values. So rather than inserting integers
9507240	9512680	here and having potentially type errors later on, we just have floats. Okay, so the low argument
9512680	9515800	represents the mean and the scales of standard deviation. Yeah, exactly what we just defined
9515800	9520520	there. Alright, so let's run this, I think we actually already did. And now we can create our
9520520	9524760	model. So to create the models pretty easy. I mean, all we do is say model equals TensorFlow
9524760	9528920	distribution dot hidden Markov model, give it the initial distribution, which is equal to
9528920	9533960	initial distribution, transition distribution, observation, distribution and steps. Now what
9533960	9539720	is steps? Well, steps is how many days we want to predict for. So the number of steps is how many
9539720	9545240	times we're going to step through this probability cycle, and run the model essentially. Now remember,
9545240	9549240	what we want to do is we want to predict the average temperature on each day, right? Like
9549240	9553640	that's what the goal of our example is is to predict the average temperature. So given this
9553640	9559240	information, using these observations and using these transitions, what we'll do is predict that.
9559240	9565240	So I'm going to run this model. What is the issue here? tensor is on hash of tensor is
9565400	9569480	okay, give me one sec, I'll have a look here, though I haven't had this issue before. Okay,
9569480	9574520	so after a painful amount of searching on stack overflow and Google and actually just reading
9574520	9579320	through more documentation on TensorFlow, I have determined the issue. So remember the error was
9579320	9583640	we were getting on actually this line here, I think I can see what the output is. Oh, this
9583640	9587000	okay, well, this is a different error. But anyways, there was an error at this line. Essentially,
9587000	9591800	what was happening is we have a mismatch between the two versions here. So the most recent version
9592440	9598120	of TensorFlow is not compatible with the older version of TensorFlow probability, at least in
9598120	9601720	the sense that the things that we're trying to do with it. So I just need to make sure that I
9601720	9607160	installed the most recent version of TensorFlow probability. So what you need to do if this
9607160	9611960	is in your notebook, and this should actually work fine for you guys, because this will be updated
9611960	9615960	by the time you get there. But in case you run into the issue, I'll, you know, deal with it.
9615960	9620360	But essentially, we're going to select version 2.x of TensorFlow, you're going to run this
9620440	9625160	install commands, you're going to install TensorFlow probability, just run this command.
9625160	9630120	Then after you run this command, you're going to need to restart your run times, go to run time,
9630120	9634280	and then restart run time. And then you can just continue on with the script, select TensorFlow
9634280	9639000	2.x again, do your imports, and then you know, we'll test if this is actually going to work for
9639000	9644840	us here, run our distributions, create the model without any issues this time, notice no red text,
9644840	9649640	and then run this final line, which will give you the output. Now, this is what I wanted to
9649640	9653880	talk about here that we didn't quite get to because we were having some bugs. But this is how we
9653880	9659480	can actually kind of run our model and see the output. So what you can do is do model dot mean,
9659480	9664680	so you say mean equals model dot mean. And what this is going to do is essentially just calculate
9664680	9670680	the probability is going to essentially take that from the model. Now, when we have model dot mean,
9670680	9675160	this is what we call, you know, a partially defined tensor. So remember our tensors were like
9675240	9679720	partially defined computations. Well, that's what model dot mean actually is. That's what
9679720	9685240	this method is. So if we want to get the value of that, what we actually need to do is create a
9685240	9690440	new session in TensorFlow, run this part of the graph, which we're going to get by doing mean
9690440	9694520	dot numpy, and then we can print that out. So I know this might seem a little bit confusing,
9694520	9700040	but essentially to run a session in the new version of TensorFlow, so 2.x, or 2.1 or whatever
9700040	9707400	it is, you're going to type with TF dot compact dot v one dot session as sesh. And then I mean,
9707400	9710760	this doesn't really matter what you have here, but whatever you want. And then what I'm doing is
9710760	9715480	just printing mean dot numpy. So to actually get the value from this here, this variable,
9715480	9720280	I call dot numpy. And then what it does is print out this array that gives me the expected
9720280	9727000	temperatures on each day. So we have, you know, three, six, essentially 7.5, 8.25. And you can
9727080	9732120	see these are the temperatures, based on the fact that we start with an initial probability of
9732120	9736360	starting on a cold day. So we kind of get that here, right, we're starting at three degrees.
9736360	9740680	That's what it's determined, we're going to start at. And then we have all of these other
9740680	9745960	temperatures is predicting for the next days. Now notice if we recreate this model, so just
9745960	9750760	rerun the distributions, rerun them and go model dot mean again, this stays the same, right? Well,
9750760	9754360	because our probabilities are the same, this model is going to do the calculation the exact
9754360	9758760	same, there's not really any training that goes into this. So we get, you know, very similar,
9758760	9762680	if not the exact same values, I can't remember if these are identical, but that's what it looks
9762680	9766600	like to me. I mean, we can run this again, see, we get the same one, and we'll create the model
9766600	9770120	one more time. And let me just check these values here to make sure I'm not lying to you as yes,
9770120	9773960	they are the exact same. Okay, so let's start messing with a few probabilities and see what we
9773960	9780520	can do to this temperature and see what changes we can cause. So if I do 0.5 here, and I do 0.5
9780600	9785560	for the categorical probability, remember this refers to points three and four above. So it's
9785560	9789880	a cold day has a 30% chance of being followed by hot day and then a hot day has a 20% chance of
9789880	9793960	being followed by cold day. So what I've just done now is change the probability to be 50%
9794760	9799240	so that a cold day now has a 50% chance of being followed by hot day and a 50% chance of
9799240	9804360	being followed by cold day. And let's recreate this model. Let's rerun this and let's see if
9804360	9809720	we get a difference. But we do notice this, the temperature now has been a is going a little bit
9809720	9814440	higher. Now notice that we get the same starting temperature because that's just the average
9814440	9818760	based on this probability that we have here. But if we wanted to potentially start, you know,
9818760	9824840	hotter, we could reverse these numbers, we go 0.2 0.8. Let's rerun all of this. And now look at
9824840	9830040	this what our temperatures are, we start at 12. And then we actually drop our temperature down to 10.
9830040	9834120	So that's how this hidden Markov model works. Now this is nice, because you can just tweak the
9834120	9838520	probabilities. This happens pretty well instantly. And we can have a look at our output very nicely.
9838520	9843320	So obviously, this is representing the temperature on our like the first day, this would be the
9843320	9848200	second day, third day, fourth day, fifth, sixth, seventh, and obviously, like the more days you
9848200	9853000	go on, the least accurate, this is probably going to be because it just runs off probability. And
9853000	9856520	if you're going to try to predict, you know, a year in advance, and you're using the weather that you
9856520	9860600	have from I guess the previous year, you're probably not going to get a very accurate prediction.
9860600	9864440	But anyways, these are hidden Markov models. They're not like extremely useful. There's
9864440	9867800	some situations where you might want to use something like this. So that's why we're
9867800	9871480	implementing them in kind of in this course and showing you how they work. It's also another
9871480	9875560	feature of TensorFlow that a lot of people don't talk about or see. And you know, personally,
9875560	9879720	I hadn't really heard of hidden Markov models until I started developing this course. So anyways,
9879720	9885240	that has been it for this module. Now I hope that this kind of gave you guys a little bit of an idea
9885240	9888840	of how we can actually implement some of these machine learning algorithms, a little bit of
9888840	9893640	idea of how to work with data, how we can feed that to a model, the importance between testing
9893640	9897800	and training data. And then obviously, linear regression is one we focused a lot on. So I
9897800	9901800	hope you guys are very comfortable with that algorithm. And then what was the last the second
9901800	9905640	one we did, I got to go up to remember exactly the sequence we had here. So classification,
9905640	9910760	that one was important as well. So I hope you guys really understood that clustering, we didn't go
9910760	9914920	too far into that. But again, this is an interesting algorithm. And if you need to do some kind of
9914920	9919320	clustering, you now know of one algorithm to do that called K means clustering, and you understand
9919320	9923080	how that works. And now you know, hidden Markov models. So in the next module, we're going to
9923080	9926920	start covering neural networks, we now have the knowledge we need to really dive in there and
9926920	9930600	start doing some cool stuff. And then in the future modules, we're going to do deep computer
9930600	9934760	vision, I believe we're going to do chatbots with recurrent neural networks, and then some form
9934760	9939320	of reinforcement learning at the end. So with that being said, let's go to the next module.
9942680	9947240	Hello, everybody, and welcome to module four. Now in this module of this course, we're going to be
9947240	9951960	talking about neural networks, discussing how neural networks work, a little bit of the math
9951960	9957400	behind them, talking about gradient descent and back propagation, and how information actually
9957400	9960920	flows through the neural network, and then getting into an example where we use a neural
9960920	9965400	network to classify articles of clothing. So I know that was a lot, but that's what we're
9965400	9970040	going to be covering here. Now neural networks are complex. There's kind of a lot of components
9970040	9973800	that go into them. And I'm going to apologize right now, because it's very difficult to explain
9973800	9978200	it all at once. What I'm going to be trying to do is kind of piece things together and explain
9978280	9983320	them in blocks. And then at the end, you know, kind of combine everything together. Now I will
9983320	9987480	say in case any of you didn't watch the beginning of this course, I do have very horrible handwriting,
9987480	9991880	but this is the easiest way to explain things to you guys. So bear with me, you know, I'm sure
9991880	9995880	you'll be able to understand what I'm saying, but it might just be painful to read some of it.
9995880	9999640	All right, so let's get into it right away and start discussing what neural networks are and
9999640	10004680	how they work. Well, the whole point of a neural network is to provide, you know, classification
10004680	10009800	or predictions for us. So we have some input information, we feed it to the neural network,
10009800	10013960	and then we want it to give us some output. So if we think of the neural network as this black box,
10013960	10017640	we have all this input, right, we give all this data to the neural network, maybe we're talking
10017640	10022440	about an image, maybe we're talking about just some random data points, maybe we're talking about a
10022440	10027880	data set, and then we get some meaningful output. This is what we're looking at. So if we're just
10027880	10031560	looking at a neural network from kind of the outside, we think of it as this magical black
10031560	10036200	box, we give some input, it gives us some output. And I mean, we could call this black box just some
10036200	10040520	function, right, where it's a function of the input maps it to some output. And that's exactly
10040520	10045800	what a neural network does. It takes input and maps that input to some output, just like any
10045800	10051640	other function, right, just like if you had a straight line like this, this is a function,
10051640	10056520	you know, this is your line, you know, whatever it is, you're going to say y equals like four x,
10056600	10061720	maybe that's your line, you give some input x, and it gives you some value y, this is a mapping
10061720	10068040	of your input to your output. Alright, so now that we have that down, what is a neural network
10068040	10072920	made up of? Well, a neural network is made up of layers. And remember, we talked about the layered
10072920	10077880	representation of data when we talked about neural networks. So I'm going to draw a very basic
10077880	10084360	neural network, we're going to start with the input layer. Now the input layer is always the
10084360	10089720	first layer in our neural network. And it is what is going to accept our raw data. Now what I mean
10089720	10095400	by raw data is whatever data we like want to give to the network, whatever we want to classify
10095400	10100520	whatever our input information is, that's what this layer is going to receive in the neural
10100520	10105480	network. So we can say, you know, these arrows represent our input, and they come to our first
10105480	10110840	input layer. So this means, for example, if you had an image, and this image, and I'll just draw
10110920	10114920	like one like this, let's say this our image, and it has all these different pixels, right,
10114920	10118280	all these different pixels in the image, and you want to make a classification on this image.
10119000	10124360	Well, maybe it has a width and a height and a classic width and height example is 28 by 28.
10124360	10128520	If you had 28 by 28 pixels, and you want to make a classification on this image,
10129080	10133160	how many input neurons you think you would need in your neural network to do this?
10134120	10137560	Well, this is kind of, you know, a tough question if you don't know a lot about neural networks.
10138520	10142360	If you're predicting for the image, if you're going to be looking at the entire image to
10142360	10148200	make a prediction, you're going to need every single one of those pixels, which is 28 times 28
10148200	10154120	pixels, which I believe is something like 784. I could be wrong on that number, but I believe
10154120	10160440	that's what it is. So you would need 784 input input neurons. Now, that's totally fine. That
10160440	10164120	might seem like a big number, but we deal with massive numbers when it comes to computers. So
10164200	10168680	this really isn't that many. But that's an example of, you know, how you would use a neural network
10168680	10175080	input layer to represent an image, you would have 784 input neurons, and you would pass
10175080	10179240	one pixel to every single one of those neurons. Now, if we're doing an example where maybe we
10179240	10184600	just have one piece of input information, maybe it's literally just one number. Well, then all
10184600	10190920	we need is one input nerve. If we have an example where we have four pieces of information, we would
10190920	10196040	need four input neurons, right? Now, this can get a little bit more complicated. But that's
10196040	10199720	the basis that I want you to understand is, you know, the pieces of input you're going to have
10199720	10204200	regardless of what they are, you need one input neuron for each piece of that information, unless
10204200	10208040	you're going to be reshaping or putting that information in different form. Okay, so let's
10208040	10213640	just actually skip ahead and go to now our output layer. So this is going to be our output. Now,
10213640	10218440	what is our output layer? Well, our output layer is going to have as many neurons. And again,
10218440	10224680	the neurons are just representing like a node in the layer as output pieces that we want. Now,
10224680	10229800	let's say we're doing a classification for images, right? And maybe there's two classes
10229800	10234840	that we could represent. Well, there's a few different ways we could design our output layer.
10234840	10239720	What we could do is say, okay, we're going to use one output neuron. This output neuron is going to
10239720	10246920	give us some value. We want this value to be between zero and one. And we'll say that's inclusive.
10247560	10253000	Now, what we can do now if we're predicting two classes say, Okay, so if my output neuron is
10253000	10258120	going to give me some value, if that value is closer to zero, then that's going to be class zero.
10258120	10262920	If this value is closer to one, it's going to be class one, right? And that would mean
10263640	10267400	when we have our training data, right, and we talked about training and testing data,
10267400	10273160	we'd give our input and our output would need to be the value zero or one, because it's either
10273160	10276520	the correct class, which is zero, right, or the correct class, which is one. So like our
10276920	10281800	what am I saying, our labels for our training data set would be zero and one. And then this value
10281800	10286040	on our output neuron will be guaranteed to be between zero and one, based on something that
10286040	10289640	I'm going to talk about a little bit later. That's one way to approach it, right? We have a single
10289640	10295160	value, we look at that value. And based on what that value is, we can determine, you know, what
10295160	10300040	class we predicted, not work sometimes. But in other instances, when we're doing classification,
10300040	10305720	what makes more sense is to have as many output neurons as classes you're looking to predict for.
10305720	10309320	So let's say we're going to have, you know, like five classes that we're predicting for maybe
10309320	10314680	these three pieces of input information are enough to make that prediction. Well, we'd actually have
10314680	10321000	five output neurons. And each of these neurons would have a value between zero and one. And the
10321000	10327160	combination, so the sum of every single one of these values would be equal to one. Now, can you
10327160	10332440	think of what this means? If every single one of these neurons is a value between zero and one,
10332440	10337000	and their sum is one, what does this look like to you? Well, to me, this looks like a probability
10337000	10341240	distribution. And essentially, what's going to happen is we're going to make predictions for how
10341240	10347480	strongly we think each input information is each class. So if we think that it's like class one,
10347480	10352920	maybe we'll just label these like this, then what we would do is say, okay, this is going to be
10352920	10362920	0.9 representing 90%. Maybe this is like 0.001, maybe this is 0.05, 0.003, you get the point,
10362920	10366600	it's going to add up to one, and this is a probability distribution for our output layer.
10367160	10371480	So that's a way to do it as well. And then obviously, if we're doing some kind of regression task,
10371480	10375720	we can just have one neuron and that will just predict some value. And we'll define, you know,
10375720	10381160	what we want that value to be. Okay, so that's my example for my output. Now let's erase this
10381160	10384200	and let's actually just go back to one output neuron, because that's what I want to use for
10384200	10390200	this example. Now, we have something in between these layers, because obviously, you know, we
10390200	10394680	can't just go from input to output with nothing else. What we have here is called a hidden layer.
10395400	10398520	Now, in neural networks, we can have many different hidden layers, we can have, you know,
10398520	10402920	hidden layers that are connecting to other hidden layers, and like we could have hundreds,
10402920	10408360	thousands, if we wanted to, for this basic example, we'll use one. And I'll write this as hidden.
10409320	10413320	So now we have our three layers. Now, why is this called hidden? The reason this is called
10413320	10417720	hidden is because we don't observe it when we're using the neural network, we pass information
10417720	10421800	to the input layer, we get information from the output layer, we don't know what happens
10421800	10426600	in this hidden layer or in these hidden layers. Now, how are these layers connected to each other?
10426600	10430200	How do we get from this input layer to the hidden layer to the output layer and get some
10430200	10436200	meaningful output? Well, every single layer is connected to another layer with something called
10436280	10440120	weights. Now, we can have different kind of architectures of connections, which means I
10440120	10445080	could have something like this one connects to this, this connects to this, this connects to this,
10445080	10449080	and that could be like my connection kind of architecture, right? We could have another one
10449080	10454920	where this one goes here. And you know, maybe this one goes here. And actually, after I've drawn
10454920	10459800	this line, now we get what we're going to be talking about a lot, which is called a densely
10459800	10465160	connected neural network. Now a densely connected neural network or a densely connected layer,
10465240	10470120	essentially means that is connected to every node from the previous layer. So in this case,
10470120	10475720	you can see every single node in the input layer is connected to every single node in the output
10475720	10480760	layer or in the hidden layer, my bad. And these connections are what we call weights. Now these
10480760	10486360	weights are actually what the neural network is going to change and optimize to determine the
10486360	10490360	mapping from our input to our output. Because again, remember, that's what we're trying to do.
10490360	10494520	We have some kind of function, we give some input, it gives us some output. How do we get that input
10494520	10498680	and output? Well, by modifying these weights, it's a little bit more complex, but this is the
10498680	10502920	starting. So these lines that I've drawn are really just numbers. And every single one of these
10502920	10507560	lines is some numeric value. Typically, these numeric values are between zero and one, but
10507560	10512280	they can be large, they can be negative. It really depends on what kind of network you're doing and
10512280	10517240	how you've designed it. Now, let's just write some random numbers, we have like 0.1, this could
10517240	10521800	be like 0.7, you get the point, right? We just have numbers for every single one of these lines.
10522600	10526440	And these are what we call the trainable parameters that our neural network will
10526440	10532280	actually tweak and change as we train to get the best possible result. So we have these
10532280	10535800	connections. Now our hidden layer is connected to our output layer as well. This is again,
10535800	10541800	another densely connected layer, because every layer or every nor neuron from the previous layer
10541800	10545960	is connected to every neuron from the next layer, you would like to determine how many
10545960	10549880	connections you have, what you can do is say there's three neurons here, there's two neurons
10549880	10555240	here, three times two equals six connections. That's how that works from layers. And then
10555240	10559880	obviously, you can just multiply all of the neurons together as you go through and determine
10560680	10565960	what that's going to be. Okay, so that is how we connect these layers, we have these weights. So
10565960	10570120	let's just write a w on here. So we remember that those are weights. Now, we also have something
10570120	10576520	called biases. So let's add a bias here, I'm going to label this B. Now biases are a little bit
10576600	10582040	different than these nodes we have regularly. There's only one bias, and a bias exists in
10582040	10587320	the previous layer to the layer that it affects. So in this case, what we actually have is a bias
10587320	10593160	that connects to each neuron in the next layer from this layer, right? So it's still densely connected.
10594200	10598600	But it's just a little bit different. Now notice that this bias doesn't have an arrow beside it
10598600	10604040	because this doesn't take any input information. This is another trainable parameter for the
10604040	10610520	network. And this bias is just some constant numeric value that we're going to connect to the
10610520	10615640	hidden layer. So we can do a few things with it. Now these weights always have a value of one.
10616280	10620040	We're going to talk about why they have a value of one in a second. But just know that whenever
10620040	10625720	a bias is connected to another layer or to another neuron, its weight is typically one.
10626680	10631080	Okay, so we have that connected, we have our bias, and that actually means we have a bias
10631080	10636520	here as well. And this bias connects to this. Notice that our biases do not connect with each
10636520	10639960	other. The reason for this, again, is they're just some constant value, and they're just something
10639960	10645160	we're kind of adding into the network is another trainable parameter that we can use. Now let's
10645160	10649480	talk about how we actually pass information through the network and why we even use these
10649480	10654920	weights and biases of what they do. So let's say we have, I can't really think of a good example,
10654920	10660360	so we're just going to do some arbitrary stuff. Let's say we have like data points, right? X, Y, Z,
10661480	10665640	and all of these data points have some mapped value, right? There's some value that we're
10665640	10668920	looking for for them, or there's some class we're trying to put them in, maybe we're clustering
10668920	10676280	them between like, red dots and blue dots. So let's do that. Let's say an XYZ is either a part of
10676280	10682360	the red class, or the blue class, let's just do that. So what we want this output neuron to give
10682360	10687080	us is red or blue. So what I'm going to do is say since it's just one class, we'll get this
10687160	10692520	output neuron in between the range is your own one, we'll say, okay, if it's closer to zero,
10692520	10697160	that's red, if it's closer to one, that's blue. And that's what we'll do for this network. And for
10697160	10704200	this example, now our input neurons are going to obviously be X, Y, and Z. So let's pick some
10704200	10708600	data point. And let's say we have, you know, the value two, two, two, that's our data point. And
10708600	10714120	we want to predict whether it's red or blue. How do we pass it through? Well, what we need to do
10714120	10720200	is determine how we can, you know, find the value of this hidden layer node, we already know the
10720200	10724840	value of these input node, but now we need to go to the next layer using these connections and find
10724840	10729320	what the value of these nodes are. Well, the way we determine these values is I'm going to say,
10729320	10734920	and I've just said n one, just to represent like this is a node, like this is node one, maybe this
10734920	10741880	one should be node two, is equal to what we call the weighted sum of all of the previous nodes that
10741880	10747480	are connected to it. If that makes any sense to you guys. So a weighted sum is something like this.
10747480	10751000	So I'm just going to write the equation, I'll explain it, I'm going to say n one is equal to
10751000	10759320	the sum of, let's not say n equals zero, let's say, I equals zero to n of, in this case, we're
10759320	10766920	going to say w i times x i plus b. Now, I know this equation looks really mathy and complicated,
10766920	10772440	it's really not what this symbol and this equation here means is take the weighted sum
10772440	10777480	of all the neurons that are connected to this neuron. So in this case, we have a neuron x neuron
10777480	10783480	y and neuron z connected to n one. So when we take the weighted sum, or we calculate this,
10783480	10789960	what this is really equal to is the weight at neuron x, we can say w x times the value at
10789960	10796600	neuron x, which in this case, is just equal to two, right, plus whatever the weight is at neuron y.
10796600	10803080	So in this case, this is w y. And then times two, and then you get the point where we have
10803080	10808280	w z and I'm trying on the edge of my drawing tablet to write this times two. Now, obviously,
10808280	10812280	these weights have some numeric value. Now, when we start our neural network, these weights are
10812280	10817160	just completely random. They don't make any sense or just some random values that we can use. As the
10817160	10821640	neural network gets better, these weights are updated and changed to make more sense in our
10821640	10826440	network. So right now, we'll just leave them as w x, w y, w z. But no, these are some numeric
10826440	10832280	values. So this returns to a sum value, right, some value, let's just call this value v. And
10832280	10838040	that's what this is equal to. So v. Then what we do is we add the bias. Now remember, the bias
10838040	10844760	was connected with a weight of one, which means if we take the weighted sum of the bias, right,
10844760	10849640	all we're doing is adding whatever that biases value was. So if this bias value was 100,
10849640	10854840	then what we do is we add 100. Now, I've just written the plus B to explicitly state the fact
10854840	10859160	that we're adding the bias, although it could really be considered as a part of the summation
10859160	10864840	equation, because it's another connection to the neural. Now, let's just talk about what
10864840	10869720	this symbol means for anyone that's confused about that. Essentially, this stands for sum,
10869720	10876120	I stands for an index, and n stands for what index will go up to now n means how many neurons we
10876120	10881320	had in the previous layer. And then what we're doing here is saying wi xi. So we're going to say
10881320	10887400	weight zero x zero plus weight one x one plus weight two x two, it's almost like a for loop where
10887400	10891880	we're just adding them all together. And then we add the B. And I hope that makes enough sense
10891880	10896520	so that we understand that. So that is our weighted sum and our bias. So essentially, what we do is
10896520	10900120	we go through and we calculate these values. So this gets some value, maybe this value is like
10900120	10905720	0.3, maybe this value seven, whatever it is, and we do the same thing now at our output neuron.
10905720	10911080	So we take the weighted sum of this value times its weight. And then we take the weighted sum,
10911080	10917080	so this value times its weight, plus the bias, this is given some value here. And then we can
10917080	10922280	look at that value and determine what the output of our neural network is. So that is pretty much
10922280	10926600	how that works in terms of the weighted sums, the weights and the biases. Now, let's talk about the
10926600	10930920	kind of the training process and another thing called an activation function. So I've lied to
10930920	10934200	a little bit because I've said I'm just going to start erasing some stuff. So we have a little bit
10934200	10939000	more room on here. So I've lied to you and I've said that this is completely how this works.
10939000	10943080	Well, we're missing one key feature that I want to talk about, which is called an activation
10943080	10949320	function. Now remember how we want this value to be in between zero and one right at our output layer.
10949320	10953400	Well, right now, we can't really guarantee that that's going to happen. I mean, especially for
10953400	10958440	starting with random weights and random biases in our neural network, we're passing this information
10958440	10964120	through, we could get to this, you know, point here, we could have like 700 as our value.
10964840	10968600	That's kind of crazy to me, right? We have this huge value, how do we look at 700 and
10968600	10972120	determine whether this is red or whether this is blue? Well, we can use something called an
10972120	10976280	activation function. Now, I'm going to go back to my slides here, whatever you want to call this
10976280	10980200	this notebook, just to talk about what an activation function is. And you guys can see here, you can
10980200	10985800	follow along, I have all the equations kind of written out here as well. So let's go to activation
10985880	10990360	function, which is right here. Okay. So these are some examples of an activation function. And I
10990360	10994680	just want you to look at what they do. So this first one is called rectified linear unit. Now
10994680	11000200	notice that essentially what this activation function does is take any values that are less
11000200	11005080	than zero and just make them zero. So any x values that are, you know, in the negative, it just makes
11005080	11010440	their y zero. And then any values that are positive, it's just equal to whatever their positive value
11010440	11015160	is. So if it's 10, it's 10. This allows us to just pretty much eliminate any negative numbers,
11015160	11020760	right? That's kind of what rectified linear unit does. Now 10 h or hyperbolic tangent.
11021400	11026520	What does this do? This actually squishes our values between negative one and one. So it takes
11026520	11031720	whatever values we have, and the more positive they are, the closer to one they are, the more
11031720	11036360	negative they are, the closer to negative one they are. So when we see why this might be useful,
11036360	11040200	right for a neural network, and then last one is sigmoid, what this does is squish our values
11040200	11045400	between zero and one, a lot of people call it like the squishifier function. Because all it does
11045400	11049960	is take any extremely negative numbers and put them closer to zero and any extremely positive
11049960	11053560	numbers and put them close to one, any values in between, you're going to get some number that's
11053560	11058760	kind of in between that, based on the equation one over one plus e to the negative z. And this is
11058760	11063640	theta z, I guess, is equal to that. Okay, so that's how that works. Those are some activation
11063640	11067240	functions. Now I hope that's not too much math for you. But let's talk about how we use them,
11067240	11072120	right? So essentially, what we do is at each of our neurons, we're going to have an activation
11072120	11077560	function that is applied to the output of that neuron. So we take this this weighted sum plus
11077560	11083720	the bias, and then we apply an activation function to it before we send that value to the next neuron.
11083720	11090680	So in this case, n one isn't actually just equal to this, what n one is equal to is n one is equal
11090680	11096760	to f, which stands for activation function of this equation, right? So we say I equals zero,
11098040	11106520	w i x i plus B. And that's what n one's value is equal to when it comes to this output neuron.
11107160	11111400	So each of these have an activation function on them. And two has the same activation function
11111400	11116280	as n one. And we can define what activation function we want to apply at each neuron.
11116920	11120840	Now at our output neuron, the activation function is very important, because we need to determine
11120840	11124040	what we want our value to look like. Do we want it between negative one on one? Do we want it
11124040	11129960	between zero and one? Or do we want it to be some massively large number? Do we want it between zero
11129960	11135000	and positive infinity? What do we want? Right? So what we do is we pick some activation function
11135000	11141000	for our output neuron. And based on what I said, where we want our values between zero and one,
11141080	11148040	I'm going to be picking the sigmoid function. So sigmoid, recall, squishes our values between
11148040	11155160	zero and one. So what we'll do here is we'll take n one, right? So n one times whatever the weight
11155160	11163240	is there. So weight zero, plus n two times weight one, plus a bias, and apply sigmoid.
11164280	11170120	And then this will give us some value between zero and one, then we can look at that value and we
11170120	11174520	can determine what the output of this network is. So that's great. And that makes sense. Why
11174520	11179400	we would use that on the output neuron, right? So we can squish our value in between some kind
11179400	11182440	of value. So we can actually look at it and determine, you know, what to do with it, rather
11182440	11187080	than just having these crazy, and I want to see if I can make this eraser any bigger. Ah, that's
11187080	11192680	much better. Okay. So there we go. Let's just erase some of this. And now let's talk about why we
11192680	11197480	would use the activation function on like an intermediate layer like this. Well, the whole
11197480	11203480	point of an activation function is to introduce complexity into our neural network. So essentially,
11203480	11207720	you know, we just have these basic weights and these biases. And this is kind of just,
11207720	11211000	you know, like a complex function at this point, we have a bunch of weights, we have a bunch of
11211000	11214600	biases. And those are the only things that we're training. And the only things that we're changing
11214600	11219880	to make our network better. Now, what an activation function can do is, for example,
11219880	11223000	take a bunch of points that are on the same like plane, right? So let's just say,
11223880	11229560	these are in some plane. If we can apply an activation function of these, where we
11230520	11234920	introduce a higher dimensionality, so an activation function like sigmoid that is like a
11234920	11241800	higher dimension function, we can hopefully spread these points out and move them up or down off the
11241800	11248760	plane in a hopes of extracting kind of some different features. Now, it's hard to explain
11248760	11253800	this until we get into the training process of the neural network. But I'm hoping this is maybe
11253800	11259000	giving you a little bit of idea, if we can introduce a complex activation function into this kind of
11259000	11263560	process, then it allows us to make some more complex predictions, we can pick up on some
11263560	11268360	different patterns. If I can see that, you know, when sigmoid or rectify linear unit is applied
11268360	11273000	to this output, it moves my point up or it moves it down or moves it in like whatever direction
11273000	11277560	and n dimensional space, then I can determine specific patterns I couldn't determine in the
11277560	11281960	previous dimension. That's just like if we're looking at something in two dimensions, if I can
11281960	11285880	move that into three dimensions, I immediately see more detail, there's more things that I can
11285880	11290360	look at, right? And I'll try to do a good example of why we might use it like this. So let's say
11290360	11294280	we have a square, right, like this, right? And I ask you, I'm like, tell me some information
11294280	11297080	about the square. Well, what you can tell me immediately is you can tell me the width, you
11297080	11301160	can tell me the height, and I guess you could tell me the color, right? You can tell me it has
11301160	11304920	one face, you can tell me it has four vertexes, you can tell me a fair amount about the square,
11304920	11309400	you can tell me its area. Now what happens as soon as I extend the square and I make it into a cube?
11310280	11314760	Well, now you can immediately tell me a lot more information, you can tell me, you know, the height,
11315640	11320600	or I guess the depth with height depth, yeah, whatever you want to call it there. You can tell
11320600	11324600	me how many faces it has, you can tell me what color each of the faces are, you can tell me how
11324600	11329880	many vertexes you can tell me if this cube or the square, this rectangle is uniform or not,
11329880	11333880	and you can pick up on a lot more information. So that's kind of I mean, this is a very over
11333880	11339160	simplification of what this actually does. But this is kind of the concept, right, is that if we
11339160	11343720	are in two dimensions, if we can somehow move our data points into a higher dimension by applying
11343720	11348360	some function to them, then what we can do is get more information and extract more information
11348360	11353320	about the data points, which will lead to better predictions. Okay, so now that we've talked about
11353320	11356360	all this, it's time to talk about how neural networks train. And I think you guys are ready
11356360	11361400	for this. This is a little bit more complicated. But again, it's not that crazy. Alright, so we
11361480	11365400	talked about these weights and biases. And these weights and biases are what our network will
11365400	11371000	come up with and determine to, you know, like make the network better. So essentially, what we're
11371000	11376120	going to do now is talk about something called a loss function. So as our network starts, right,
11376120	11381000	the way that we train it, just like we've trained other networks, or other machine learning models
11381000	11385960	is we give it some information, we give it what the expected output is. And then we just see what
11385960	11390440	the expected output or what the output was from the network, compare it to the expected output
11390440	11395240	and modify it like that. So essentially, what we start with is we say, okay, 222, we say this
11395240	11400840	class is red, which I forget what I labeled that was as but let's just say, like that was a zero,
11400840	11407000	okay. So this class is zero. So I want this network to give me a zero for the point 222. Now,
11407000	11412360	this network starts with completely random weights and completely random biases. So chances are,
11412360	11416840	when we get to this output here, we're not going to get zero, maybe we get some value after applying
11416840	11424040	the sigmoid function, that's like 0.7. Well, this is pretty far away from red. But how far away is
11424040	11429400	it? Well, this is where we use something called a loss function. Now, what a loss function does is
11429400	11436440	calculate how far away our output was from our expected output. So if our expected output is
11436440	11442200	zero, and our output was 0.7, the loss function is going to give us some value that represents
11442200	11447800	like how bad or how good this network was. Now, if it tells us this network was really bad,
11447800	11452520	it gives us like a really high loss, then that tells us that we need to tweak the weights and
11452520	11458040	biases more and move the network in a different direction. We're starting to get into gradient
11458040	11462120	descent. But let's understand the loss function first. So it's going to say, if it was really
11462120	11465800	bad, let's move it more, let's change the weights more drastically, let's change the biases more
11465800	11470840	drastically. Whereas, if it was really good, it'll be like, Okay, so that one was actually decent,
11470840	11474600	you know, you only need to tweak a little bit, and you only need to move this, this and this.
11474600	11478680	So that's good. And that's the point of this loss function, it just calculates some value,
11478680	11482600	the higher the value, the worse our network was a few examples of loss function.
11483320	11490520	Let's go down here, because I think I had a few optimizer loss here, mean squared error,
11490520	11496200	mean absolute error and hinge loss. Now mean absolute error, you know, let's actually just
11496280	11504120	look one up here. So mean, absolute error, and have a look at what this is. So images,
11504120	11511560	let's pick something. This is mean absolute error. This is the equation for mean absolute error.
11511560	11519720	Okay, so the summation of the absolute value of yi minus lambda of xi over n. Now, this is kind of
11519720	11523400	complicated. I'm not going to go into it too much. I was expecting I was hoping I was going to get
11523400	11533960	like a better example for mean squared error. Okay, so these are the three loss functions here.
11533960	11537960	So mean squared error, mean absolute error, hinge loss, obviously, there's a ton more that we could
11537960	11542280	use. I'm not going to talk about which how each of these work specifically, I mean, you can look
11542280	11546920	them up pretty easily. And also, so you know, these are also referenced as cost functions,
11546920	11552680	so cost or loss, you might just hear these, these terms kind of interchanged cost and loss
11552680	11557000	essentially mean the same thing, you want your network to cost the least, you want your network
11557000	11561720	to have the least amount of loss. Okay, so now that we have talked about the loss function,
11562360	11567800	we need to talk about how we actually update these weights and biases. Now, actually, let's
11567800	11571800	go back to here, because I think I had some notes on it. This is what we call gradient descent.
11572520	11576840	So essentially, the parameters for our network are weights and biases. And by changing these
11576840	11581640	weights and biases, we will, you know, either make the network better or make the network worse,
11581720	11585880	the loss function will determine if the network is getting better, if it's getting worse, and then
11585880	11591240	we can determine how we're going to move the network to change that. So this is now gradient
11591240	11595880	descent, where the math gets a little bit more complicated. So this is an example of what your
11595880	11602360	neural network function might look like. Now, as you have higher dimensional math, you have,
11602360	11606600	you know, a lot more dimensions, a lot more space to explore when it comes to creating
11606600	11611160	different parameters and creating different biases and activation functions and all of that.
11611240	11615240	So as we apply our activation functions, we're kind of spreading our network into higher
11615240	11619240	dimensions, which just makes things much more complicated. Now, essentially, what we're trying
11619240	11624280	to do with the neural network is optimize this loss function. This loss function is telling us
11624280	11628520	how good it is or how bad it is. So if we can get this loss function as low as possible,
11628520	11632920	then that means we should technically have the best neural network. So this is our kind of
11632920	11637800	loss functions, like mapping or whatever, what we're looking for is something called a global
11637800	11643560	minimum, we're looking for the minimum point where we get the least possible loss from our
11643560	11647880	neural network. So if we start where these red circles are, right, and I've just stole this
11647880	11654120	image off Google images, what we're trying to do is move downwards into this global global
11654120	11658920	minimum. And this is with a process of called gradient descent. So we calculate this loss,
11658920	11664440	and we use an algorithm called gradient descent, which tells us what direction we need to move
11664520	11669400	our function to determine or to get to this global minimum. So it essentially looks where
11669400	11673400	we are. It says this was the loss. And it says, okay, I'm going to calculate what's called a
11673400	11678040	gradient, which is literally just a steepness or a direction. And we're going to move in that
11678040	11683000	direction. And then the algorithm called brought back propagation, we'll go backwards through
11683000	11688040	the network and update the weights and biases so that we move in that direction. Now, I think this
11688040	11692040	is as far as I really want to go, because I know this is getting more complicated already,
11692040	11696760	then some of you guys probably can handle and that I can probably explain. But that's kind of
11696760	11700760	the basic principle. We'll go back to the drawing board and we'll do a very quick recap before we
11700760	11706760	get into some of the other stuff, neural networks, input, output hidden layers connected with weights,
11706760	11711720	there's biases that connect to each layer. These biases can be thought of as y intercepts, they'll
11711720	11717480	simply move completely up or move completely down that entire, you know, activation function,
11717480	11721960	right, we're shifting things left or right, because this will allow us to get a better
11721960	11726600	prediction and have another parameter that we can train and add a little bit of complexity
11726600	11732120	to our neural network model. Now, the way that information is passed through these layers is
11732120	11737800	we take the weighted sum at a neuron of all of the connected neurons to it, we then add this
11737800	11743720	bias neuron, and we apply some activation function that's going to put this, you know, these values
11743720	11748600	in between two set values. So for example, when we talk about sigmoid, that's going to squish our
11748600	11752600	values between zero and one, when we talk about hyperbolic tangent, that's going to squish our
11752600	11756760	values between negative one and one. And when we talk about rectifier linear unit, that's going to
11756760	11761320	squish our values between zero and positive infinity. So we apply those activation functions,
11761320	11766040	and then we continue the process. So n one gets its value and two gets its value. And then finally,
11766040	11769480	we make our way to our output layer, we might have passed through some other hidden layers
11769480	11774200	before that. And then we do the same thing, we take the weighted sum, we add the bias,
11774200	11779880	we apply an activation function, we look at the output, and we determine whether we know we are
11779880	11784440	a class y or we are class z or whether this is the value we're looking for. And and that's how
11784440	11789880	it works. Now we're at the training process, right? So we're doing this now, that's kind of how this
11789880	11794280	worked when we were making a prediction. So when we're training, essentially, what happens is we
11794280	11801000	just make predictions, we compare those predictions to whatever these expected value should be using
11801000	11806840	this loss function. Then we calculate what's called a gradient, a gradient is the direction we need
11806840	11811560	to move to minimize this loss function. And this is where the advanced math happens and why I'm
11811560	11816760	kind of skimming over this aspect. And then we use an algorithm called back propagation, where we
11816760	11821720	step backwards through the network, and update the weights and biases, according to the gradient
11821800	11827640	that we calculated. Now that is pretty much how this works. So you know, the more info we have,
11828440	11832600	likely unless we're overfitting, but you know, if we have a lot of data, if we can keep feeding
11832600	11837240	the network, it starts off being really horrible, having no idea what's going on. And then as more
11837240	11841960	and more information comes in, it updates these weights and biases gets better and better sees
11841960	11846440	more examples. And after you know, a certain amount of epochs or certain amount of pieces of
11846440	11850760	information, our network is making better and better predictions and having a lower and lower
11850760	11855320	loss. And the way we will calculate how well our network is doing is by passing it, you know,
11855320	11862680	our validation data set, where it can say, okay, so we got an 85% accuracy on this data set, we're
11862680	11867240	doing okay, you know, let's tweak this, let's tweak that, let's do this. So the loss function,
11867240	11872680	the lower this is the better, also known as the cost function. And that is kind of neural networks
11872680	11877480	in a nutshell. Now I know this wasn't really in a nutshell, because it was 30 minutes long. But that
11877480	11881720	is, you know, as much of an explanation as I can really give you without going too far into the
11881720	11886040	mathematics behind everything. And again, remember, the activation function is to move us up in
11886040	11890760	dimensionality. The bias is another layer of complexity and a trainable parameter for our
11890760	11897000	network allows us to shift this kind of activation function left, right up, down. And yeah, that
11897000	11903400	is how that works. Okay, so now we have an optimizer. This is kind of the last thing on
11903480	11907880	how neural networks work. optimizer is literally just the algorithm that does the gradient descent
11907880	11912520	and back propagation for us. So I mean, you guys can read through some of them here, we'll be using
11913320	11917560	probably the atom optimizer for most of our examples, although there's, you know, lots of
11917560	11922200	different ones that we can pick from. Now this optimization technique, again, is just a different
11922200	11925560	algorithm. There's some of them are faster, some of them are slower, some of them work a little bit
11925560	11930120	differently. And we're not really going to get into picking optimizers in this course, because
11930200	11934600	that's more of an advanced machine learning technique. All right, so enough explaining,
11934600	11941480	enough math, enough drawings, enough talking. Now it is time to create our first official
11941480	11946200	neural network. Now these are the imports we're going to need. So import TensorFlow is TF from
11946200	11950200	TensorFlow import Keras again, so this does actually come with TensorFlow. I forget if I
11950200	11955800	said you need to install that before. My apologies and then import numpy as NP, import map plot
11955880	11961000	live dot pi plot as PLT. Alright, so I'm going to do actually similar thing to what I did before
11961000	11965160	where I'm kind of just going to copy some of this code into another notebook, just to make sure
11965160	11969880	that we can look at everything at the end, and then kind of step through the code step by step
11969880	11975960	rather than all of the text kind of happening here. Alright, so the data set, and the problem
11975960	11980920	we are going to consider for our first neural network is the fashion MNIST data set. Now the
11981000	11986680	fashion MNIST data set contains 60,000 images for training and 10,000 images for validating and
11986680	11994600	testing 70,000 images. And it is essentially pixel data of clothing articles. So what we're
11994600	11999880	going to do to load in this data set from Keras, this actually built into Keras, it's meant as
11999880	12006280	like a beginner, like testing training data set, we're going to say fashion underscore MNIST
12006280	12012920	equals Keras dot data sets dot fashion MNIST. Now this will get the data set object, and then we
12012920	12018920	can load that object by doing fashion MNIST dot load data. Now by doing this by having the tuples
12018920	12025480	train images train labels, test images test labels equals this, this will automatically split our
12025480	12030760	data into the sets that we need. So we need the training, and we need the testing. And again,
12030760	12035160	we've talked about all of that. So I'm going to kind of skim through that. And now we have it in
12035160	12039800	all of these kind of tuples here. Alright, so let's have a look at this data set to see what
12039800	12042920	we're working with. Okay, so let's run some of this code, let's get this import going,
12043640	12049880	if it doesn't take forever. Okay, let's get the data sets. Yeah, this will take a second to
12049880	12054360	download for you guys, if you don't already have it cached. And then we'll go train images dot
12054360	12059880	shape. And let's look at what one of the images looks like. Or sorry, what our data set looks
12059960	12066200	like. So we have 60,000 images that are 28 by 28. Now what that means is we have 28 pixels or 28
12066200	12071080	rows of 28 pixels, right? So that's kind of what our, you know, information is. So we're going to
12071080	12079240	have in total 784 pixels, which I've denoted here. So let's have a look at one pixel. So to reference
12079240	12084680	one pixel, this is what I what I'm doing, this comes in as a, actually, I'm not sure what type of
12084680	12089400	data frame this is. But let's have a look at it. So let's say type of train underscore images,
12089400	12095720	because I want to see that. So that's an numpy array. So to reference the different indexes in
12095720	12100760	this is similar to pandas, we're just going to do zero comma 23 comma 23, which stands for, you
12100760	12108360	know, image zero 23, and then 23. And this gives us one pixel. So row 23 column 23, which will be
12108360	12115480	that. Okay, so let's run this. And let's see this value is 194. Okay, so that's kind of interesting.
12115480	12119800	That's what one pixel looks like. So let's look at what multiple pixels look like. So we'll print
12120440	12128040	train underscore images. And okay, so we get all these zeros, let's print train images, zero,
12128040	12133880	colon, that should work for us. And we're getting all these zeros. Okay, so that's the border of
12133880	12139160	the picture. That's okay, I can't show you what I wanted to show you. Anyways, one pixel. And I
12139160	12144680	wanted to have you guys guess it is simply just represented by a number between zero and 255.
12145240	12149560	Now what this stands for is the grayscale value of this pixel. So we're dealing with grayscale
12149560	12155720	images, although we can deal with, you know, 3d, 4d, 5d images as well, or not 5d images,
12155720	12160440	but we can deal with images that have like RGB values. First, so for example, we could have
12160440	12165720	a number between zero 255, another number between zero and 255 and another number between zero and
12165720	12171960	255 for every single pixel, right? Whereas this one is just one simple static value. Okay, so it
12171960	12177640	says that here, a pixel values between zero and 255, zero being black and 255 being white. So
12177640	12182360	essentially, you know, if it's 255, that means that this is white, if it's zero, that means that
12182360	12187320	it is black. Alright, so let's have a look at the first 10 training labels. So that was our
12187320	12192200	training images. Now what are the training labels? Okay, so we have an array, and we get values from
12192200	12199160	zero to nine. Now this is because we have 10 different classes that we could have for our
12199240	12204040	dataset. So there's 10 different articles of clothing that are represented. I don't know what
12204040	12209320	all of them are, although they are right here. So t shirt, trouser, pullover, dress, coat, sandals,
12209320	12215400	shirt, sneaker, bag, ankle boot. Okay, so let's run this class names, just so that we have that
12215400	12220200	saved. And now what I'm going to do is use matplotlib to show you what one of the images looks like.
12220200	12224600	So in this case, this is a shirt. I know this is printing out kind of weird, but I'm just showing
12224600	12228280	the image. I know it's like different colors, but that's because if we don't define that we're
12228280	12232520	drawing a grayscale, it's going to do this. But anyways, that is what we get for the shirt. So
12232520	12237160	let's go to another image and let's have a look at what this one is. I actually don't know what
12237160	12244920	that is. So we'll skip that maybe that's a what is it t shirt or top. This I guess is going to be
12244920	12250840	like a dress. Yeah, so we do have dressed there. Let's go for have a look at this. And some of
12250840	12256040	these are like hard to even make out when I'm looking at them myself. And then I guess this
12256120	12259320	will be like a hoodie or something. I'm trying to get one of the sandals to show you guys a
12259320	12263720	few different ones. There we go. So that is a sandal or a sneaker. Okay, so that is kind of
12263720	12267880	how we do that and how we look at the different images. So if you wanted to draw it out, all you
12267880	12272600	do is just make a figure, you just show the image, do the color bar, which is just giving you this,
12272600	12275560	then you're going to say, I don't want to grid and then you can just show the image, right? Because
12275560	12281000	if you don't have this line here, and you show with the grid, oh, it's actually not showing the grid
12281000	12285000	that's interesting. Although I thought it was going to show me those pixelated grid. So I guess
12285080	12290040	you don't need that line. Alright, so data pre processing. Alright, so this is an important
12290040	12294920	step in neural networks. And a lot of times when we have our data, we have it in these like random
12294920	12299240	forms, or we're missing data, or there's information we don't know, or that we haven't seen. And
12299240	12303800	typically what we need to do is pre process it. Now, what I'm going to do here is squish all my
12303800	12308840	values between zero and one. Typically, it's a good idea to get all of your input values in a
12308840	12313560	neural network in between, like that range in between, I would say negative one and one is what
12313640	12317480	you're trying to do, you're trying to make your numbers as small as possible to feed to the neural
12317480	12321880	network. The reason for this is your neural network starts out with random weights and biases
12321880	12327240	that are in between the range zero and one, unless you change that value. So if you have massive
12327240	12331400	input information and tiny weights, then you're kind of having a bit of a mismatch, and you're
12331400	12336040	going to make it much more difficult for your network to actually classify your information,
12336040	12340040	because it's going to have to work harder to update those weights and biases to reduce how
12340040	12345800	large those values are going to be, if that makes any sense. So it usually is a good idea to
12345800	12351160	pre process these and make them in between the value of zero and one. Now, since we know that
12351160	12355960	we're just going to have pixel values that are in the range of 255, we can just divide by 255,
12355960	12361640	and that will automatically scale it down for us. Although it is extremely important that we do this
12361640	12367640	to not only the training images, but the testing images as well. If you just pre process your
12367640	12372760	training images, and then you pass in, you know, new data that's not pre processed, that's going to
12372760	12377000	be a huge issue. You need to make sure that your data comes in the same form. And that means when
12377000	12382360	we're using the model to to make predictions, whatever, you know, I guess it pixel data we
12382360	12386920	have, we need to pre process in the same way that we pre processed our other data. Okay,
12386920	12391800	so let's pre process that so train images and test images. And I'm just going to actually steal
12392760	12396680	some of this stuff here, and throw it in my other one before we get too far. So let's get this
12396760	12402520	data sets. And let's throw it in here, just so we can come back and reference all this together.
12402520	12409080	Let's go class names. We don't actually need the figures, a few things I can skip, we do need
12409080	12416040	this pre processing step. Like that, if I could go over here. And then what else do we need,
12416040	12419480	we're going to need this model. Okay, so let's actually just copy the model into this and just
12419480	12425160	make it a little bit cleaner. We can have a look at it. So new code block model. Okay, so model,
12425880	12429720	creating our model. Now creating our models actually really easy. I'm hoping what you guys
12429720	12433960	have realized so far is that data is usually the hardest part of machine learning and neural
12433960	12438040	networks, getting your data in the right form, the right shape and you know, pre processed
12438040	12442120	correctly, building the models usually pretty easy because we have tools like TensorFlow and Keras
12442120	12447320	that can do it for us. So we're going to say model equals Keras dot sequential. Now sequential
12447320	12451160	simply stands for the most basic form of neural network, which we've talked about so far, which
12451160	12455880	is just information going from the left side to the right side, passing through the layers,
12455880	12460680	sequentially, right, called sequential, we have not talked about recurrent or convolutional
12460680	12466920	neural networks yet. Now what we're going to do here is go Keras dot layers dot flat. So sorry,
12466920	12471240	inside here, we're going to define the layers that we want in our neural network. This first
12471240	12477720	layer is our input layer. And what flatten does is allows us to take in a shape of 28 by 28,
12478360	12485640	which we've defined here, and flatten all of the pixels into 784 pixels. So we take this 28 by 28
12485640	12491480	kind of matrix like structure, and just flatten it out. And Keras will do that for us, we don't
12491480	12497080	actually need to take our you know, matrix data and transform before passing. So we've done that.
12497800	12506280	Next, we have Keras dot layers dot dense 128 activation equals rectify linear unit. So this
12506360	12512760	is our first hidden layer, layer two, right, that's what I've denoted here. And this is a dense layer.
12512760	12518680	Now dense, again, means that all of the, what is it, the neurons in the previous layer are connected
12518680	12524680	to every neuron in this layer. So we have 828 neurons here. How do we pick that number? We
12524680	12528440	don't know, we kind of just came up with it. Usually, it's a good idea that you're going to do
12528440	12533160	this as like a little bit smaller than what your input layer is, although sometimes it's going to
12533160	12536760	be bigger, you know, sometimes it's going to be half the size, it really depends on the problem,
12536760	12540840	I can't really give you a straight answer for that. And then our activation function will
12540840	12545880	define as rectify linear unit. Now we could pick a bunch of different activation functions, there's
12545880	12551400	time we can pick sigmoid, we could pick 10 h, which is hyperbolic tangent, doesn't really matter.
12551960	12556200	And then we're going to define our last layer, which is our output layer, which is a dense layer
12556200	12562200	of 10 output neurons with the activation of softmax. Okay, so can we think of why we would have picked
12562280	12566440	10 here, right? I'll give you guys a second to think about it, based on the fact that our output
12566440	12571480	layer, you know, is supposed to have as many neurons as classes we're going to predict for.
12571480	12577560	So that is exactly what we have 10. If we look, we have 10 classes here. So we're going to have 10
12577560	12583480	output neurons in our output layer. And again, we're going to have this probability distribution.
12583480	12589160	And the way we do that is using the activation function softmax. So softmax will make sure
12589240	12594040	that all of the values of our neurons add up to one, and that they're between zero and one.
12594760	12599000	So that is our, our model, we've created the model now. So let's actually run this.
12600840	12605560	See, are we going to get any errors here? Is this going to run? And then we'll run the model,
12605560	12609320	and then we'll go on to the next step, which actually going to be training and testing the
12609320	12614040	model. Okay, so let's create the model now, shouldn't get any issues, and we're good. And now let's
12614040	12619080	move on to the next step. I'm forgetting what it is, though, which is training the model. Oh,
12619080	12624440	sorry, compiling the model. Okay, so compiling the model. So we've built now what we call the
12624440	12628920	architecture of our neural network, right, we've defined the amount of neurons in each layer,
12628920	12633880	we've defined the activation function, and we define the type of layer and the type of connections.
12633880	12638840	The next thing we need to pick is the optimizer, the loss and the metrics we're going to be looking
12638840	12643880	at. So the optimizer we're going to use is Adam. This is again, just the algorithm that performs
12643880	12648520	the gradient descent, you don't really need to look at these too much, you can read up on some
12648520	12653640	different activation functions or sorry, optimizers, if you want to kind of see the difference between
12653640	12659560	them, but that's not crazy, we're going to pick a loss. So in this case, sparse categorical cross
12659560	12663240	entropy, again, not going to go into depth about that, you guys can look that up if you want to
12663240	12667800	see how it works. And then metrics. So what we're looking for the output that we want to see from
12667800	12673800	the network, which is accuracy. Now from, you know, kind of right now with our current knowledge,
12673800	12678360	we're just going to stick with this as what we're going to compile our neural networks with,
12678360	12683720	we can pick different values if we want. And these are what we call, what is it hyper parameter
12683720	12688680	tuning. So the parameters that are inside here, so like the weights and the biases are things that
12688680	12694120	we can't manually change. But these are things that we can change, right, the optimizer, the loss,
12694120	12698840	the metrics, the activation function, we can change that. So these are called hyper parameters,
12698840	12704520	same thing with the number of neurons in each layer. So hyper parameter tuning is a process of
12704520	12709960	changing all of these values and looking at how models perform with different hyper parameters
12709960	12713720	change. So I'm not really going to talk about that too much. But that is something to note,
12713720	12718520	because you'll probably hear that, you know, this hyper parameter kind of idea. Okay, so we've
12718520	12722280	compiled the model now using this, which just means we've picked all the different things that
12722280	12727080	we need to use for it. And now on to training the model. So I'm just going to copy this in.
12728040	12733640	Again, remember this, these parts are pretty syntaxually heavy, but fairly easy to actually do.
12733640	12738120	So we're going to fit the model. So fit just means we're fitting it to the training data. It's
12738120	12741880	another word for training, essentially. So we're going to pass it the training images,
12741880	12746600	the training labels, and notice how much easier it is to pass this. Now, we don't need to do this
12746600	12750280	input function, we don't need to do all of that, because Keras can handle it for us. And we define
12750280	12756120	our epochs as 10 epochs is another hyper parameter that you could tune and change if you wanted to.
12756680	12761240	All right, so that will actually fit our model. So what I'm going to do is put this in another
12761240	12766360	code block. So I don't need to keep retraining this. So we'll go like that. And let's actually
12766360	12770840	look at this training process. So we've run the model, this should compile. And now let's fit
12770840	12776120	it and let's see what we actually end up getting. Alright, so epoch one, and we can see that we're
12776120	12780600	getting a loss and we're getting accuracy printing out on the side here. Now, this is going to take
12780600	12785400	a second, like this is going to take a few minutes, as opposed to our other models that we made are
12785400	12790440	not a few minutes, but you know, a few seconds, when you have 60,000 images, and you have a network
12790440	12796680	that's comprised of 784 neurons, 128 neurons, and then 10 neurons, you have a lot of weights and
12796680	12801160	biases and a lot of math that needs to go on. So this will take a few seconds to run. Now,
12801160	12804760	if you're on a much faster computer, you'll probably be faster than this. But this is why I
12804760	12808600	like Google Collaboratory, because you know, this isn't using any of my computer's resources
12808600	12815160	to train. It's using this. And we can see, like the RAM and the disk. How do I look at this?
12816120	12820680	In this network? Oh, is it going to let me look at this now? Okay, I don't know why it's not letting
12820680	12825400	me click this, but usually you can have a look at it. And now we've trained and we've fit the
12825400	12831240	model. So we can see that we have an accuracy of 91%. But the thing is, this is the accuracy
12831960	12837480	on or testing or our training data. So now if we want to find what the true accuracy is,
12838040	12842040	what we need to do is actually test it on our testing data. So I'm going to steal
12842040	12846520	this line of code here. This is how we test our model. Pretty straightforward. I'll just
12846520	12852120	close this. So let's go into code block. So we have test loss test accuracy is model dot
12852120	12858920	evaluate test images test labels verbose equals one. Now what is verbose? I was hoping it was
12858920	12863160	going to give me the thing so I could just read it to you guys. But verbose essentially is just
12863160	12868120	are we looking at output or not? So like how much information are we seeing as this model evaluates?
12868760	12873080	It's like how much is printing out to the console? That's what that means. And yes,
12873080	12877560	this will just split up kind of the metrics that are returned to this into test loss and test accuracy
12877560	12883640	so we can have a look at it. Now you will notice when I run this, that the accuracy will likely
12883640	12888840	be lower on this than it was on our model. So actually, the accuracy we had from this
12888840	12894680	was about 91. And now we're only getting 88.5. So this is an example of something we call
12894680	12900760	overfitting. Our model seemed like it was doing really well on the testing data or sorry,
12900760	12906440	the training data. But that's because it was seeing that data so often, right with 10 epochs,
12906440	12912280	it started to just kind of memorize that data and get good at seeing that data. Whereas now
12912280	12918040	when we pass it new data that it's never seen before, it's only 88.5% accurate, which means
12918040	12923160	we overfit our model. And it's not as good at generalizing for other data sets, which is usually
12923160	12927560	the goal, right? When we create a model, we want the highest accuracy possible, but we want the
12927560	12934440	highest accuracy possible on new data. So we need to make sure our model generalizes properly.
12934440	12938760	Now in this instance, you know, like, it's, it's hard to figure out how do we do that because
12938760	12943080	we don't know that much about neural networks. But this is the idea of overfitting and of
12943080	12947640	hyper parameter tuning, right? So if we can start changing some of this architecture, and we can
12947640	12952440	change maybe the optimizer, the loss function, maybe we go epochs eight, let's see if this
12952440	12957800	does any better, right? So let's now fit the model with eight epochs, we'll have a look at what this
12957800	12962840	accuracy is. And then we'll test it and see if we get a higher accuracy on the testing data set.
12962840	12967240	And this is kind of the idea of that hyper parameter tuning, right? Well, we just look at
12967240	12972680	each epoch, or not each epoch, we look at each parameter, we tweak them a little bit. And usually
12972680	12977960	we'll like write some code that automates this for us. But that's the idea is we want to get the most
12977960	12983240	generalized accuracy that we can. So I'll wait for this to train. We're actually almost done. So I
12983240	12988040	won't even bother cutting the video. And then we'll run at this evaluation. And we'll see now if we
12988040	12991640	got a better accuracy. Now I'm getting a little bit scared because the accuracy is getting very
12991640	12996040	high here. And sometimes, you know, like you want the accuracy to be high on your training data.
12996120	12999720	But when it gets to a point where it's very high, you're in a situation where it's likely
12999720	13005240	that you've overfit. So let's look at this now. And let's see what we get. So 88.4. So we actually
13005240	13009160	dropped down a little bit. And it seemed like those epochs didn't make a big difference. So maybe
13009160	13014920	if I train it on one epoch, let's have an idea and see what this does. You know, make your prediction,
13014920	13018440	you think we're going to be better, do you think we're going to be worse? It's only seen the training
13018440	13027080	data one time. Let's run this. And let's see 89.34. So in this situation, less epochs was actually
13027080	13032120	better. So that's something to consider. You know, a lot of people I see just go like 100 epochs and
13032120	13036760	just think their model is going to be great. That's actually not good to do. A lot of the
13036760	13040040	times you're going to have a worse model because what's going to end up happening is it's going to
13040040	13046520	be seeing the same information so much tweaking so specifically to that information that it's seen
13046600	13050920	that when you show it new data, it can't actually, you know, classify and generalize on that.
13051640	13055560	All right. So let's go back and let's see what else we're doing now with this. Okay,
13055560	13059320	so now that we've done that, we need to make predictions. So to make predictions is actually
13059320	13064520	pretty easy. So I'm actually just going to copy this line in, we'll go into a new code block down
13064520	13069880	here. So all you have to do is say model that predict, and then you're going to give it an array
13069880	13074520	of images that you want to predict on. So in this case, if we look at test images shape, so actually
13074520	13081480	let's make a new code block and let's go here. So let's say test underscore images dot shape.
13083640	13090040	All right, give me a second. So we have 10,000 by 28 by 28. So this is an array of 10,000 entries
13090040	13095720	of images. Now, if I just wanted to predict on one image, what I could do is say test images,
13095720	13101480	zero and then put that inside of an array. The reason I need to do that is because the data
13101480	13106840	that this model is used to see in is an array of images to make a prediction on, that's what this
13106840	13111720	predict method needs. And it's much better at making predictions on many things at once than
13111720	13117560	just one specific item. So if you are predicting one item only, you do need to put it in an array,
13117560	13121320	because it's used to seeing that form. So we could do this. I'm, I mean, I'm just going to leave it.
13121320	13124920	So we're just going to predict on every single one of the test images, because then we can have
13124920	13129160	a look at a cool function I've kind of made. So let's actually do this predictions equals model
13129240	13135640	dot predict test images. I mean, let's print predictions. And look at actually what it is.
13136280	13141320	Where is my autocomplete? There it is. Okay. So let's have a look. Is this some object? Whoa,
13141320	13147800	okay. So this is a raise of arrays that looks like we have some like really tiny numbers in them.
13147800	13153800	So what this is, is essentially every single, you know, prediction or every single image has
13153800	13157400	a list that represents the prediction for it, just like we've done with kind of the linear
13157400	13162360	models and stuff like that. So if I want to see the prediction for test image zero, I would say
13162360	13167720	prediction zero, right? Let's print this out. And this is the array that we're getting. These
13167720	13173960	this is the probability distribution that was calculated on our output layer for, you know,
13173960	13179080	these, what is it for that image? So if we want to figure out what class we actually think that
13179080	13184840	this is predicting for, we can use a cool function from NumPy called arg max, which essentially is
13184920	13190200	just going to take the index, this is going to return to us the index of the maximum value in
13190200	13195000	this list. So let's say that it was I'm looking for the least negative, which I believe is this,
13195000	13200760	so this should be nine, this should return to us nine, because this is the index of the highest
13200760	13206280	value in this list. Unless I'm just wrong when I'm looking at the negatives here. So nine, that's
13206280	13212200	what we got. Okay, so now if we want to see what the actual classes, while we have our class names
13212200	13216840	up here, so we know class nine is actually ankle boot. So let's see if this is actually an ankle
13216840	13223320	boot. So I'm just going to do class underscore names, I think that's what I called it, like this,
13223320	13228520	so that should print out what it thinks it is. Yeah, class underscore names. But now let's actually
13228520	13233560	show the image of this prediction. So to do that, I'm just going to steal some code from here because
13234360	13237960	I don't remember all the syntax off the top of my head. So this
13238760	13243640	is what it looks like. So let's steal this figure. Let's show this and let's see if it actually looks
13243640	13249640	like an ankle boot. So to do that, we're going to say test underscore images zero, because obviously
13249640	13255240	image zero corresponds to predict prediction zero. And that will show this and see what we get. Okay,
13255240	13260600	so ankle boot, and we'll be looking at the image is actually an ankle boot. And we can do this for
13260600	13266120	any of the images that we want, right? So if I do prediction one, prediction one, now let's have a
13266120	13270760	look pull over kind of looks like a pull over to me. I mean, I don't know if it actually is,
13270760	13279080	but that's what it looks like. You do to to have a look here. Okay, trouser. Yep, looks like trousers
13279080	13283160	to me. And we can see that that is how we get the predictions from our model, we use model dot
13283160	13289400	predict. Alright, so let's move down here now to the next thing that we did. Alright, so we've
13289400	13293720	already done that. So verifying predictions. Okay, so this is actually a cool kind of script
13293720	13299160	that I wrote, I'll zoom out a little bit so we can read it. What this does is let us use our model
13299160	13304520	to actually make. And I've stolen some of this from TensorFlow to make predictions on any
13304520	13308840	entry that we want. So what it's going to do is ask us to type in some number, we're going to type
13308840	13313240	in that number, it's going to find that image in the test data set, it's going to make your
13313240	13318600	prediction on that from the model, and then show us what it actually is versus what it was predicted
13318600	13323560	being. Now, I just need to actually run. Actually, let's just steal this code and bring it in the
13323560	13326840	other one, because I've already trained the model there. So we don't have to wait again. So let's
13326840	13335240	go f 11, f 11, let's go to a new code block, and run that. So let's run this script. Have a look
13335240	13339880	down here. So pick a number, we'll pick some number, let's go 45. And then what it's going to do is
13339880	13345320	say expected sneaker, guess sneaker, and actually show us the image that's there. So we can see
13345320	13350360	this is what you know, our pixel kind of data looks like. And this is what the expected was,
13350360	13354120	and this is what the guess was from the neural network. Now we can do the same thing if we run
13354120	13361000	it again, pick a number 34. Let's see here, expected bag, guess bag. So that's kind of showing you
13361000	13367480	how we can actually use this model. So anyways, that has been it for this kind of module on neural
13367480	13372120	networks. Now I did this in about an hour, I'm hoping I explained a good amount that you guys
13372120	13376760	understand now how neural networks work. In the next module, we're going to move on to convolutional
13376760	13380760	neural networks, which again should help, you know, kind of get your understanding of neural
13380760	13385480	networks up as well as learn how we can do deep computer vision, object recognition and detection
13385480	13389640	using convolutional neural networks. So that being said, let's get into the next module.
13392920	13397720	Hello, everyone, and welcome to the next module in this TensorFlow course. So what we're going
13397720	13402200	to be doing here is talking about deep computer vision, which is very exciting, very cool. This
13402200	13406520	has been used for all kinds of things you ever seen the self driving cars, for example, Tesla,
13406520	13412760	they actually use a TensorFlow deep learning model, obviously very complicated, more than I can
13412760	13417240	really explain here to do a lot of their computer vision for self driving, we've used computer vision
13417240	13421800	in the medicine field, computer vision is actually used in sports a lot for things like goal line
13421800	13426600	technology and even detecting images and players on the field doing analysis, there's lots of cool
13426680	13430440	things are doing with it nowadays. And for our purposes, what we're going to be doing is using
13430440	13435960	this for to perform classification, although it can be used for object detection and recognition,
13435960	13440360	as well as facial detection and recognition as well. So all kinds of applications, in my opinion,
13440360	13445160	one of the cooler things in deep learning that we're doing right now. And let's go ahead and talk
13445160	13448680	about what we're actually going to be focusing on here. So we're going to start by discussing what
13448680	13453560	a convolutional neural network is, which is essentially the way that we do deep learning,
13453640	13457080	we're going to learn about image data. So what's the difference between image data and other
13457080	13461800	regular data? We're going to talk about convolutional layers and pooling layers and how stacks of those
13461800	13466760	work together as what we call a convolutional base for our convolutional neural network. We're
13466760	13471800	going to talk about CNN architectures and get into actually using pre trained models that have been
13471800	13476040	developed by companies such as Google and TensorFlow themselves to perform classification
13476040	13480840	tasks for us. So that is pretty much the breakdown of what we're about to learn. There's quite a
13480840	13484520	bit in this module, it's probably the more difficult one or the most difficult one we've
13484520	13488200	been doing so far. So if you do get lost at any point, or you don't understand some of it,
13488200	13492360	don't feel bad, this stuff is very difficult. And I would obviously recommend reading through
13492360	13495960	some of the descriptions I have here in this notebook, which again, you can find from the
13495960	13500360	link in the description or looking up some things that maybe I don't go into enough enough depth
13500360	13505560	about in your own time, as I can't really spend, you know, 10, 11 hours explaining a convolutional
13505640	13511000	neural network. So let's now talk about image data, which is the first thing we need to understand.
13511000	13516200	So in our previous examples, what we did with when we had a neural network is we had two dimensional
13516200	13520360	data, right, we had a width and height when we were trying to classify some kind of images using
13520360	13524920	a dense neural network. And well, that's what we use two dimensions. Well, with an image, we
13524920	13530200	actually have three dimensions. And what makes up those dimensions? Well, we have a height, and we
13530200	13534280	have a width, and then we have something called a color channels. Now, it's very important to
13534280	13538440	understand this, because we're going to see this a lot as we get into convolutional networks, that
13538440	13544200	the same image is really represented by three specific layers, right? We have the first layer,
13544200	13549080	which tells us all of the red values of the pixels, the second layer, which tells us all the green
13549080	13553480	values, and the third layer, which tells us all the blue values. So in this case, those are the
13553480	13558040	covered channels. And we're going to be talking about channels in depth quite a bit in this series.
13558040	13563080	So just understand that although you think of an image as a two dimensional kind of thing,
13563160	13567800	and our computer, it's really represented by three dimensions where these channels are telling us
13567800	13573240	the color of each pixel. Because remember, in red, green, blue, you have three values for each pixel,
13573240	13578120	which means that you're going to need three layers to represent that pixel, right? So this is what
13578120	13583160	we can kind of think of it as a stack of layers. And in this case, a stack of pixels, right, or
13583160	13587400	stack of colors really telling us the value for each pixel. So if we were to draw this to the
13587400	13593880	screen, we would get the blue, green and red values of each pixel, determine the color of it,
13593880	13599000	and then draw the two dimensional image right based on the width and the height. Okay, so now
13599000	13602520	we're going to talk about a convolutional neural network and the difference between that in a dense
13602520	13606680	neural network. So in our previous examples, when we use the dense neural network to do some kind
13606680	13612280	of image classification, like that fashion, and this data set, what it essentially did was look
13612280	13618520	at the entire image at once and determined based on finding features in specific areas of the image,
13618520	13623240	what that image was, right? Maybe it found an edge here, a line here, maybe it found a shape,
13623240	13627560	maybe it found a horizontal diagonal line. The important thing to understand, though, is that
13627560	13632200	when it found these patterns and learned the patterns that made up specific shapes, it learned
13632200	13637400	them in specific areas. It knew that if we're in between, for example, looking at this cat image,
13637480	13642120	we're going to classify this as a cat, if an eye exists on, you know, the left side of the screen
13642120	13647480	where the eyes are here, then that's a cat. It doesn't necessarily know that if we flipped this
13647480	13652680	cat, we did a horizontal flip of this cat, and the eyes were over here, that that is a pattern that
13652680	13657720	makes up a cat. So the idea is that the dense network looks at things globally, it looks at the
13657720	13663640	entire image and learns patterns in specific areas. That's why we need things to be centered, we need
13663640	13668680	things to be very similar when we use a dense neural network to actually perform image classification,
13669320	13674280	because it cannot learn local patterns, and apply those to different areas of the image.
13674280	13678600	So for example, some patterns we might look for, when we're looking at an image like a cat here
13678600	13683000	would be something like this, right, we would hope that maybe we could find a few ears, we could find
13683000	13690200	the eyes, the nose, and you know, the paws here. And those features would tell us that this makes
13690280	13695400	up a cat. Now with a dense neural network, it would find these features, it would learn them,
13695400	13700120	learn these patterns, we only learn them in this specific area where they're boxed off,
13700120	13703960	which means if I horizontally flip this image, right, and I go like that,
13703960	13707960	then it's not going to know that that's a cat, because it learned that pattern in a specific
13707960	13713080	area, it'll need to relearn that pattern in the other area. Now a convolutional neural network,
13713080	13718120	on the other hand, learns local patterns. So rather than learning that the ear exists in,
13718120	13722680	you know, this specific location, it just learns that this is what an ear looks like,
13722680	13727480	and it can find that anywhere in the image. And we'll talk about how we do that as we get to the
13727480	13731880	explanation. But the whole point is that our convolutional neural network will scan through
13731880	13737720	our entire image, it will pick up features and find features in the image. And then based on the
13737720	13742920	features that exist in that image will pass that actually to a dense neural network or a dense
13742920	13747720	classifier, it will look at the presence of these features and determine, you know, the combination
13747800	13752280	of these presences of features that make up specific classes or make up specific objects.
13752280	13756520	So that's kind of the point. I hope that makes sense. The main thing to remember is that dense
13756520	13761400	neural networks work on a global scale, meaning they learn global patterns, which are specific and
13761400	13767320	are found in specific areas. Whereas convolutional neural networks or convolutional layers will
13767320	13771800	find patterns that exist anywhere in the image, because they know what the pattern looks like,
13771800	13777560	not that it just exists in a specific area. Alright, so how they work, right? So let's see
13777560	13781720	when a neural network, regular neural network looks at this dog image, this is a good example,
13781720	13787160	I should have been using this before, it will find that there's two eyes that exist here, right?
13787160	13791800	And we'll say, okay, so I found that these eyes make up a dog. This is its training image, for
13791800	13795880	example, and it's like, okay, so this pattern makes up the dog, the IR is in this location.
13796680	13801880	Now, what happens when we do this? And we flip the image to the other side. Well, our neural
13801880	13805640	network starts looking for these eyes, right on the left side of the image where it found them
13805640	13810040	previously and where it was trained on. It obviously doesn't find them there. And so it says
13810040	13814280	that our image isn't a dog, although it clearly is a dog, it's just a dog that's orientated
13814280	13818200	differently. In fact, it's just flipped horizontally, right? Or actually, I guess I would say
13818200	13823640	vertically, flip vertically. So since it doesn't find the eyes in this location, and it can only
13823640	13828200	look at patterns that it's learned in specific locations, it knows that this, or it's going
13828200	13833080	to say this isn't a dog, even though it is. Whereas our convolutional layer will find the eyes
13833080	13837720	regardless of where they are in the image, and still tell us that this is a dog, because even
13837720	13841480	though the dogs moved over, it knows what an eye looks like, so it can find the eye anywhere in
13841480	13846760	the image. So that's kind of the point of the convolutional neural network and the convolutional
13846760	13852040	layer. And what the convolutional layer does is look at our image and essentially feedback to us,
13852040	13857080	what we call an output feature map that tells us about the presence of specific features,
13857080	13862040	or what we're going to call filters in our image. So that is kind of the way that works.
13862760	13867160	Now, essentially, the thing we have to remember is that our dense neural networks output just a
13867160	13871640	bunch of numeric values. Whereas what our convolutional layers are actually going to be
13871640	13876040	doing is outputting what we call a feature map. Now I'm going to scroll down here to show you
13876040	13880840	this example. What we're actually going to do is run what we call a filter over our image,
13880840	13884280	we're going to sample the image at all these different areas. And then we're going to create
13884280	13889960	what we call an output feature map that quantifies the presence of the filters pattern at different
13889960	13895880	locations. And we'll run many, many, many different filters over our image at a time.
13895880	13899800	So we have all these different feature maps telling us about the presence of all these
13899800	13904920	different features. So one convolutional layer, we'll start by doing that with very small,
13904920	13910200	simple filters such as straight lines like this. And then other convolutional layers on top of
13910200	13914840	that, right, because it's going to return a map that looks something like this out of the layer,
13914840	13919480	we'll take this map in now, the one that was created from the previous layer, and say, okay,
13919480	13923320	what this map is representing to me, for example, the presence of these diagonal lines,
13923880	13928280	let me try to look for curves, right, or let me try to look for edges. So it will look at the
13928280	13932520	presence of the features from the previous convolutional layer, and then say, okay, well,
13932520	13937080	if I have all these lines combined together, that makes up an edge, and it will look for that,
13937080	13941320	right? And that's kind of the way that a convolutional neural network works and why we
13941320	13945400	stack these different layers. Now, we also use something called pooling, and there's a few
13945400	13949560	other things that we're going to get into. But that is the basics, I'm going to go into a drawing
13949560	13954200	example and show you exactly how that works. But hopefully this makes a little bit of sense
13954200	13959240	that the convolutional layer returns a feature map that quantifies the presence of a filter
13959240	13964200	at a specific location. And this filter, the advantage of it is that we slide it across
13964200	13969240	the entire image. So if this filter or this feature is presence anywhere in the image,
13969240	13972760	we will know about it rather than in our dense network, where it had to learn that pattern
13972760	13978120	in a specific global location. Okay, so let's get on the drawing tablet and do a few examples.
13978120	13981480	All right, so I'm here on my drawing tablet, and we're going to explain exactly how a
13981480	13987160	convolutional layer works, and how the network kind of works together. So this is an image I've
13987160	13991880	drawn on the left side of our screen here. I know this is very basic, you know, this is just an X,
13991880	13995080	right? This is what our images, we're just going to assume this is grayscale, we're going to avoid
13995080	14000120	doing anything with color channels the second just because they're not that important. But
14000120	14004040	just understand that what I'm going to show you does apply to color channels as well and to
14004040	14009080	multiple kind of layers and depth. And then if we can understand it on a simple level,
14009080	14014840	we should be able to understand it more thoroughly. So what we want essentially is our convolutional
14014840	14020120	layer to give us some output. It's meaningful about this image. So we're going to assume this is
14020120	14025400	the first convolutional layer. And what it needs to do essentially is return to us some feature
14025400	14031560	map that tells us about the presence of specific what we call filters in this image. So each
14031560	14037320	convolutional layer has a few properties to it. The first one is going to be the input size.
14038360	14045400	So what can we expect? Wow, what is that that was as the as the input size, how many filters
14045400	14050760	are we going to have so filters like this? And what's the sample size of our filters?
14051720	14057400	That's what we need to know for each of our convolutional neural networks. So essentially,
14057400	14061080	what is a filter? Well, a filter is just some pattern of pixels. And we saw them before,
14061080	14066440	we'll do a pretty basic one here, as the filter we're going to look for, which will look something
14066440	14072280	like this. This will be the first filter we're going to look for just to illustrate how this works.
14072280	14077480	But the idea is that at each convolutional layer, we look for many different filters. And in fact,
14077480	14084360	the number we're typically looking for is actually about times 32 filters. Sometimes we have 64
14084360	14090120	filters as well. And sometimes even 128. So we can do as many filters as we want, as few filters
14090120	14095800	as we want. But the filters are what is going to be trained. So this filter is actually what is
14095800	14101320	going to be found by the neural network. It's what's going to change. It's, you know, this is
14101320	14105000	essentially what we're looking for. This is what's created in the program. And that's kind of like
14105000	14111080	the trainable parameter of a convolutional neural network is the filter. So the amount of filters
14111080	14116680	and what they are will change as the program goes on, as we're learning more and figuring out what
14116680	14121480	features that make up, you know, a specific image. So I'm going to get rid of this stuff right now,
14121480	14126520	just so we can draw and do a basic example. But I want to show you how we look for a filter in the
14126520	14131480	image. So we have filters, right, they'll come up with them, they're gonna start completely random,
14131480	14135240	but they'll change as we go on. So let's say the filter we're looking for is that one I drew
14135240	14138360	before, I'm just going to redraw it at the top here a little bit smaller. And we'll just say
14138360	14143800	it's a diagonal line, right? But another filter we could look for might be something like, you know,
14143800	14148840	a straight line, just like that all across, we could have a horizontal line. And in fact, we'll
14148840	14153400	have 32 of them. And when we're doing just, you know, three by three grids of filters, well,
14154040	14157640	there's not that many, you know, combinations, we're going to do at least grayscale wise.
14158200	14162680	So what we'll do is we'll define the sample size, which is how big our filter is going to be three
14162680	14166600	by three, which we know right now, which means that what we're going to do is we're going to look
14166600	14174040	at three by three spots in our image, and look at the pixels, and try to find how closely these
14174040	14179000	filters match with the pixels we're looking at on each sample. So what this is going to do,
14179000	14183880	this convolutional layer is going to output us what we call a feature map, which can be a little
14183880	14188840	bit smaller than the original image. And you'll see why in a second, but that tells us about the
14188840	14194120	presence of specific features in areas of the image. So since we're looking for two filters here,
14194120	14197800	actually, we'll do two filters, which means that we're actually going to have a depth to
14198920	14202920	feature map being returned to us, right? Because for two filters, that means we need two maps,
14203560	14208760	quantifying the presence of both of those filters. So for this green box that we're looking on at
14208760	14213880	the left side here, we'll look for this first filter here. And what do we get? Well, the way we
14213880	14218680	actually do this, the way we look at this filter, is we take the cross product, or actually not the
14218680	14223480	cross product, the dot product, sorry, between this little green box and this filter, right,
14223480	14228040	because they're both pixels, they're both actually numeric values down at the bottom. So what we do
14228040	14233880	is we take that dot product, which essentially means we're element wise, adding, or what is it
14233880	14238920	element wise, multiplying all of these pixels by each other. So if this pixel values is zero,
14238920	14242840	right, because it's white, or it could be the other way around, we could say white is one,
14242840	14247720	black is zero, it doesn't really matter, right? If this is a zero, and this is a one, these are
14247720	14252680	obviously very different. And when we do the dot product of those two, so we multiply them together,
14252680	14256920	then in our output feature, we would have a zero, right, that's kind of the way it works. So we do
14256920	14262040	this dot product of this entire thing. If you don't know the dot product is I'm not really going to
14262040	14265560	go into that. But we do the dot product, and that gives us some value essentially telling us how
14265560	14270680	similar these two blocks are. So how similar this sample is that we're taking of the image and the
14270680	14275160	filter that we're looking for. They're very similar, we're going to likely put a one or something telling
14275160	14279240	us, you know, they're very close together. They're not similar at all, we're going to put a zero. So
14279240	14283400	in this case, for our first filter, we're probably going to have a value because this middle pixel
14283400	14288600	is the same as something like 0.12, right? But the all the other values are different. So it's
14288600	14294440	not going to be very similar whatsoever. So then what we're going to do now is we'll look at the
14294440	14299400	actually second filter, which is this horizontal line. And in fact, we're going to get a very
14299400	14303960	similar output response here, probably something like, you know, 0.12, that's going to go in the
14303960	14309720	top left. And again, these are both maps representing each filter, right? So now we'll move our green
14309720	14317560	box over one, like this. So just shift that over one. And now we'll start looking at the next section.
14317560	14321720	And in fact, I'm going to see if I can erase this just to make it a little bit cleaner here.
14324120	14328360	Get rid of the green, there we go. Okay, so we'll move this box over like this. And now we'll
14328360	14332520	start looking at this one, we'll do the exact same thing we did again before. So we're going to say,
14332520	14336520	alright, how similar are these? Well, they're not similar at all. So we're going to get a zero for
14336520	14341960	that first filter. How similar the other ones? Oh, actually, they're like a little bit similar.
14341960	14345480	There's a lot of white that's kind of in the same space, like, you know, stuff like that. So we'll
14345480	14350760	say maybe this is like 0.7, right? I'm just randomly picking these numbers. They are going to be much
14350760	14355000	different than what I'm putting in here. But I'm just trying to get you to understand what's kind
14355000	14358520	of happening, right? And this is completely random, the way I'm making the numbers to just make sure
14358520	14361880	you understand that because this is not exactly what it would look like. Okay, so then we're going
14361880	14365160	to move the box over one more time. Let's just erase this to keep this clean. This will be the
14365160	14369800	last time we do this for the purpose of this example. And now what we're going to have is wow,
14369800	14374040	we have a perfect match for the first filter. So we put one, the other ones like ads kind of
14374120	14378040	similar as a few things that are different. So maybe this gets like a 0.4 or something, right?
14378040	14381160	Whatever they are, we end up getting some value. So we'll fill in all these values, let's just
14381160	14386280	put some arbitrary values here for now, just so we can do something with the example 0.7, 0,
14387000	14400040	0.12, 0.42, 0.3, 0.9, 0.1, again, completely random, 0.4, 0.6. Alright, so this is now what
14400040	14406120	we've gotten our response map from looking at two filters on our original image of five by five.
14406120	14410280	Now notice that the size of these is three by three. And obviously, the reason for that is
14410280	14415160	because in a five by five image, when we're taking three by three samples, well, we can only take
14415160	14420040	nine three by three samples, because when we go down a row, right, we're going to move down one,
14420040	14423240	and we're going to do the same thing we did before of this, these three by three samples. And if we
14423240	14428280	add the amount of times we can do that, well, we just get three by three, which is not. So this
14428280	14433560	now is kind of telling us of the presence of features in this original image map. Now the
14433560	14439400	thing is, though, we're going to do this 64 times, right, for 64 filters or 32 filters or the amount
14439400	14443960	of filters that we have. So we're going to have a lot of layers, like a ton of different layers,
14443960	14448200	which means that we're going to be constantly expanding as we go through the convolutional
14448200	14454360	layers, the depth of this, this kind of output feature map. And that means that there's a lot
14454360	14457560	of computations that need to be done. And essentially, that means that this can be very
14457560	14462600	slow. So now we need to talk about an operation called pooling. So I'll backtrack a little bit,
14462600	14466600	but we will talk about pooling in a second. What's going to happen, right, is when we have all these
14466600	14472040	layers that are generated, so this is called the output feature map, right, from this original
14472040	14476760	image. What we're going to do is the next convolutional layer in the network is now going
14476760	14482040	to do the process we just talked about, except on this output feature map, which means that
14482120	14487960	since this one was picking up things like lines and edges, right, the next convolutional layer,
14487960	14494120	we'll pick up combinations of lines and edges and maybe find what a curve is, right, we'll slowly
14494120	14499640	work our way up from very, very small amount of pixels, defining more and more, almost I want to
14499640	14505720	say abstract, different features that exist in the image. And this is what really allows us to
14505720	14509160	do some amazing things with a convolutional neural network, when we have a ton of different
14509160	14513480	layers stacking up on each other, we can pick out all the small little edges, which are pretty
14513480	14519160	easy to find. And with all these combinations of layers working together, we can even find things
14519160	14527160	like, say, eyes, right, or feet, or heads or face, right, we can find very complicated structures,
14527160	14531880	because we slowly work our way up starting by solving very easy problem, which are like finding
14531880	14536840	lines, and then finding combinations of lines, combination of edges, shapes and very abstract
14536920	14542040	things. That's how this convolutional network works. So we've done that now, it's now time to
14542040	14546840	talk about pooling. And we'll also talk about pat actually, we'll go padding first before we go
14546840	14550840	pooling, I just, it doesn't really matter what order we talk about this in. But I just think
14550840	14555960	padding makes sense based on the way we're going right now. So sometimes we want to make sure that
14555960	14563720	the output feature map from our original image here is the same dimensions or same size as this,
14563720	14568680	right? So this was five by five, obviously. And this is three by three. So if we want this to be
14568680	14573320	five by five, as an output, what we need to do is add something called padding to our original
14573320	14578120	image. So padding is essentially just adding an extra row and column on each side of our image
14578120	14583320	here. So that when we, and we just fill in all these pixels in like kind of the padded pixels
14583320	14588440	here, I just blank random pixels, they don't mean anything. Essentially, why we do that is so that
14588520	14595080	when we do our three by three sample size here like this, we can take a three by three sample
14595080	14600840	where every single pixel is in the center of that sample. Because right now, this pixel is not in
14600840	14605880	the center. This pixel can never be in the center, this pixel can never be in the center, only,
14605880	14610840	you know, a few pixels get to be in the center. And what this allows us to do is generate an output
14610840	14615880	map that is the same size as our original input, and allows us to look at features that are maybe
14615880	14620680	right on the edges of images that we might not have been able to see before. Now, this isn't
14620680	14624280	super important when you go to like very large images, but it just something to consider you
14624280	14629240	can add padding, we may do this as we get through our examples. And there's also something called
14629240	14634360	stride, which I want to talk about as well. So what a stride is is essentially how much we move
14634360	14640360	this sample box, every time that we're about to move it, right? So before, like, so let's say
14640360	14644440	we're doing example with padding here, right, we are first sample, we would take here. And again,
14644440	14649720	these pixels are just added, we added them in to make this work better for us. You would assume
14649720	14653640	that the next time we move the box, we're going to move it one pixel over, that's called a stride of
14653640	14658760	one, we can do that. But we also can employ stride of two, which means we'll move over by two.
14659400	14663560	Obviously, the larger your stride, the smaller your output feature map is going to be. So you
14663560	14667000	might want to add more padding. Well, you don't want to add too much padding, but it's just
14667000	14672360	something to consider. And we will use a stride in different instances. Okay, so that's great.
14672360	14676200	That hopefully makes sense. Let's erase this. Now we don't need this anymore. We talked about
14676200	14679800	padding, we talked about the stride. And now we're going to talk about a pooling operation,
14679800	14685560	which is very important. So kind of the idea is that we're going to have a ton of layers, right,
14685560	14689320	for all these filters. And we're just going to have a lot of numbers, a lot of computations. And
14689320	14692680	there must be some way to make these a little bit simpler, a little bit easier to use. Well,
14692680	14697240	yes, that's true. And there is a way to do that. And that's called pooling. So there's three different
14697240	14704040	types of pooling. Well, it is more but the basic ones are min, max, and average. And essentially,
14704040	14711160	a pooling operation is just taking specific values from a sample of the output feature map. So
14711160	14716280	once we generate this output feature map, what we do to reduce its dimensionality and just make it
14716280	14723240	a little bit easier to work with is what we sample typically two by two areas of this output feature
14723240	14729640	map. And just take either the min, max or average value of all the values inside of here and map
14729640	14735480	these, we're going to go back this way to a new feature map that's twice, like one times the size
14735480	14739240	essentially, or not, what am I saying, two times smaller than this original map. And it's kind of
14739240	14742840	hard with three, like three by three, to really show you this. But essentially, what's going to
14742840	14747640	end up happening is we're going to have something like this. So we're going to take the sample here,
14747640	14751320	we're going to say, okay, what are we doing min, max or average pooling, if we're doing min pooling,
14751400	14754920	we're going to take the smallest value, which means we'll take zero. If we're doing max pooling,
14754920	14759320	we'll take the maximum value, which means we'll take 0.3. If we're doing average, we're probably
14759320	14764440	going to get an average value of close to what 0.2, maybe. So let's say 0.2, we'll go there.
14765000	14770200	That's how we do that with pooling. Again, just to make this feature map smaller. So we'll do that
14770200	14776120	for both of the filters. But let's just say this is 0.2. Let's say this here that I'm blocking off
14776120	14783160	is, I don't know, what is this going to be 0.6? It's hard to do this average with four numbers,
14783160	14791240	let's say this one down here is going to be 0. I don't know, let's just do two one or something.
14791240	14795240	And then this last one here, we okay, we got some bigger values, maybe this will be like 0.4.
14796360	14801000	Okay, so that's one, the one down here will have some values of its own, we'll just do squiggles
14801000	14805800	to represent that it has something. And we've effectively done a max pooling operation on
14805800	14811160	this, we've reduced the size of it by about half. And that is kind of how that works. Now,
14811160	14817160	typically what we do is we use a two by two pooling or like sample size like that, with a
14817160	14822200	stride of two, which actually means that we would straw it like this. But since we're not going to
14822200	14827320	do padding on this layer right now, we'll just do a stride of one. And this is how we pull it.
14827320	14830600	Now the different kinds of pooling are used for different kind of things. The reason we would
14830600	14837400	use a max pooling operation is to pretty much tell us about the maximum presence of a feature in that
14837400	14843240	kind of local area. We really only care if the feature exists, where if it doesn't exist, an
14843240	14847640	average pooling is not very often used, although in this case, we did use an average pooling.
14848360	14851800	But you know, it's just different kinds of pooling and average tells you about the average
14851800	14856920	presence of the feature in that area. Max tells you about is that feature present in that area
14856920	14860920	at all and men tells you does it not exist. If it doesn't exist, right, we're just going to have a
14860920	14866040	zero if there's even one zero in that area. So that's the point of pooling. That's the point of
14866040	14869480	convolutional layers. I think I'm done with the white boarding for now, we're actually going to
14869480	14873960	start getting into a little bit of code and talking about creating our own convolutional networks,
14873960	14878040	which hopefully will make this a lot more clear. So let's go ahead and get into that. All right,
14878040	14881960	so now it is time to create our first convolutional neural network. Now we're going to be using
14881960	14887480	Keras to do this. And we're also going to be using the CI FAR image data set that contains 60,000
14887480	14892760	images of 10 different classes of everyday objects. Now these images are 32 by 32, which
14892760	14898760	essentially means they are blurs, and they are colorful. Now, I just want to emphasize as we
14898760	14903080	get into this, that the reason I'm not typing all of these lines out and I just have them in here
14903080	14907400	already is because this is likely what you guys will be using or doing when you actually make
14907400	14912280	your own models. Chances are that you are not going to sit unless you're a pro at TensorFlow,
14912280	14917000	and I am not even there yet either with my knowledge of it, and have all of the lines memorized and
14917000	14921720	not have to go reference the syntax. So the point is here, so long as you can understand why this
14921720	14926200	works and what these lines are doing, you're going to be fine, you don't need to memorize them and I
14926200	14930360	have not memorized them. And I don't I look up for the documentation, I copy and paste what I need,
14930360	14934120	I alter them, I write a little bit of my own code. But that's kind of what you're going to end up
14934120	14939400	doing. So that's what I'm doing here. So this is the image data set. We have truck, horse, ship,
14939400	14943720	airplane, you know, just some everyday, regular objects, there is 60,000 images, as we said,
14943720	14949880	and 6000 images of each class. So we don't have too many images of just one specific class. So
14949880	14955160	we'll start by importing our modules. So TensorFlow, we're going to import TensorFlow dot Keras,
14955160	14960040	we're going to use the data set built into Keras for this. So that's the CI FR image data set,
14960040	14963720	which you can actually look at just by clicking at this, it'll bring you and give the information
14963720	14968200	about the data set. Although we don't need that right now, because I already know the information
14968200	14972920	about it. And now we're just going to load our images in. So again, this stuff, the way this
14972920	14979080	works is you're gonna say data sets dot CI FR 10 dot load data. Now this loads it in as like a very
14979080	14983960	strange TensorFlow object, that's like a data set object. So this is different from what we've
14983960	14989400	used before, where some of our objects have actually been like in NumPy arrays, where we can look at
14989400	14993320	them better. This is not going to be in that. So just something to keep in mind here. And we're
14993320	14998680	going to normalize this data into train images and test images, but just dividing both of them by
14998680	15003240	255. Now, again, we're doing that because we want to make sure that our values are between zero and
15003240	15007000	one, because that's just a lot better to work with in our neural networks, rather than large
15007000	15011960	integer values, just causes, you know, some things to mess up sometimes. Now class names,
15011960	15015400	we're just going to find a list here. So we have all the class names so that zero represents
15015400	15021880	airplane one auto avail so far in tilt truck, run that block of code here, we'll download this
15021880	15026120	data set, although I don't think it takes that long to do that. So okay, so wait, I guess,
15026120	15030520	yeah, I guess that's good. I think we're okay there. And now let's just have a look at actually
15030520	15035160	some of the images here by running this script. So we can see this is a truck can change the
15035160	15041560	image index to be two, we can see this is another truck. Let's go to say six, we get a bird. And
15041560	15044920	you can see these are really blurry. But that's fine. For this example, we're just trying to get
15044920	15049400	something that works all right. Okay, so that's a horse, you know, you get the point. All right.
15049400	15054440	So now CNN architecture. So essentially, we've already talked about how a convolutional neural
15054440	15058520	network works, we haven't talked about the architecture and how we actually make one.
15058520	15063160	Essentially, what we do is we stack a bunch of convolutional layers and max pooling,
15063160	15068040	min pooling or average pooling layers together in something like this, right. So after each
15068040	15072680	convolutional layer, we have a max pooling layer, some kind of pooling layer typically to reduce
15072680	15076680	the dimensionality, although you don't need that, you could just go straight into three
15076760	15082520	convolutional layers. And on our first layer, what we do is we define the amount of filters
15082520	15088120	just like here, we define the sample size. So how big are those filters and activation function,
15088120	15093720	which essentially means after we apply that, what is it that cross not cross product dot
15093720	15098280	product operation that we talked about, we'll apply rectifier linear unit to that and then
15098280	15102360	put that in the output feature map. Again, we've talked about activations functions before. So I
15102360	15106040	won't go too far into depth with them. And then we define the input shape, which essentially
15106040	15110520	means what can we expect in this first layer? Well, 32 by 32 by three, these ones, we don't
15110520	15113720	need to do that, because they're going to figure out what that is based on the input from the
15113720	15118520	previous layer. Alright, so these are just a breakdown of the layers. The convolution or the
15118520	15122200	max pooling layers here, two by two, essentially means that what we're going to do is we're going
15122200	15126760	to have a two by two sample size with actually a stride of two. Again, the whole point of this
15126760	15132760	is to actually divide or, you know, shrink it by a factor of two, how large each of these layers
15132840	15137720	are. Alright, so now let's have a summary. It's already printed out here. We can see that we
15137720	15144520	have, oh, wait, is this correct? mobile net v two, I don't think that's correct. That's because I
15144520	15149400	haven't run this one. My apologies on that guys, this is from something later in the tutorial,
15149400	15153640	we can see that we have calm 2d as our first layer. This is the output shape of that layer.
15153640	15161080	Notice that it is not 32 by 32 by 32. It is 30 by 30 by 32, because when we do that sampling
15161080	15165160	without padding, right, that's what we're going to get. We're going to get two pixels less,
15165960	15171880	because the amount of samples we can take. Alright, next, we have the max pooling 2d layer.
15171880	15177640	So this now says the output shape is 15 by 15 by 32, which means we've shrunk this shape by a
15177640	15183880	factor of two, we do a convolution on this, which means that now we get 1313 and we're doing 64,
15183880	15189640	because we're going to take 64 filters this time. And then max pooling again, we go six by six by
15190200	15194760	64, because we're going to divide this again by factor of two. Notice that it just rounded,
15194760	15199560	right? And then calm 2d. So another layer here, we get four by four by 64, again, because of the
15199560	15205960	way we take those values. So this is what we've defined so far. But this is not the end of our
15205960	15211320	convolutional neural network. In fact, this doesn't really mean much to us, right? This just tells us
15211320	15215960	about the presence of specific features, as we've gone through this convolution base, which is what
15215960	15221720	this is called the stack of convolution and max pooling layers. So what we actually need to do
15221720	15227560	is now pass this information into some kind of dense layer classifier, which is actually going to
15227560	15233160	take this pixel data that we've kind of calculated and found. So the almost extraction of features
15233160	15238360	that exist in the image, and tell us which combination of these features map to either,
15238360	15243320	you know, what one of these 10 classes are. So that's kind of the point you do this convolution
15243320	15248920	base, which extracts all of the features out of your image. And then you use the dense network
15248920	15253880	to say, Okay, well, if these combination of features exist, then that means this image is this,
15253880	15258920	otherwise, it's this and that and so on. So that's what we're doing here. Alright, so let's say adding
15258920	15262920	the dense layers. So to add the dense layers pretty easy model dot add is just how we add them,
15262920	15268680	right? So we're going to flatten all of those pixels was which essentially means take the four
15268680	15273080	by four by 64, and just put those all into a straight line, like we've done before. So just
15273080	15279000	one dimensional, then we're going to have a 64 neuron dense layer that connects all of those
15279000	15283480	things to it with an activation function of rectifier linear unit, then our output layer of
15283480	15288360	a dense layer with 10 neurons, obviously 10, because that's the amount of classes we have for
15288360	15292040	this problem. So let's run this here, we'll add those layers, let's look at a summary and see
15292040	15298360	how things have changed now. So we go from four by four by 64 to 2024. Notice that that is precisely
15298360	15303320	the calculation of four times four times 64. That's how we get that number here. Then we have a
15303320	15307000	dense layer and another dense layer. And this is our output layer. Finally, this is what we're
15307000	15311160	getting is we're going to get 10 neurons out. So essentially, just a list of values. And that's
15311160	15317480	how we can determine which class is predicted. So this up to here is the convolutional base,
15317480	15321960	this is what we call the classifier, and they work together to essentially extract the features,
15321960	15327000	and then look at the features and predict the actual object or whatever it is the class. Alright,
15327640	15330360	so that's how that works. Now it's time to train again, we'll go through this quickly.
15331000	15335000	I believe I've already trained this this takes a long time to train. So I'm actually going to
15335000	15341800	reduce the epochs here to just be for I'd recommend you guys train this on higher. So like 10, if
15341800	15345560	you're going to do it, it does take a while. So for our purposes, and for my time, we'll leave
15345560	15349400	a little bit shorter right now, but you should be getting about 70% accuracy. And you can see I've
15349400	15353720	trained this previously, if you train it on 10 epochs, but I'm just going to train up to four,
15353800	15358040	we get our 6768%. And that should be fine. So we'll be back once this is trained,
15358040	15361880	then we'll talk about how some of this works. Okay, so the model is finally finished training,
15361880	15367800	we did about four epochs, you can see we got an accuracy about 67% on the evaluation data.
15367800	15374120	To quickly go over this stuff. optimizers, Adam talked about that before. loss function is sparse
15374120	15378440	categorical cross entropy. That one, I mean, you can read this if you want computes the cross
15378440	15382920	entropy loss between the labels and predictions. And I'm not going to go into that. But these
15382920	15386360	kind of things are things that you can look up if you really understand why they work.
15386920	15390840	For most problems, you can just if you want to figure out what, you know, loss function
15390840	15396040	or optimizer to use, just use the basics, like use Adam, use a categorical cross entropy,
15396040	15400040	using a classification task, you want to do something like this, there's just you can go
15400040	15404280	up and look kind of all of the different loss functions, and it'll tell you when to use which
15404280	15409480	one and you can kind of mess with them and tweak them if you want. Now history equals model dot
15409480	15414040	fit. This is just so we can access some of the statistics from this model dot fit. Obviously,
15414040	15418840	it's just training the data to this test images, test labels and train images and train labels
15418840	15423560	where this is the validation data suite. So evaluating the model, we want to evaluate the
15423560	15427080	model, we can evaluate it now on the test images and test labels, we're obviously going to get
15427080	15431480	the same thing because the valuation is test images and test labels. So we should get the
15431480	15437480	same accuracy as 6735, which we do right here. Alright, so there we go, we get about 70%. If
15437560	15441080	you guys train this on 10 epochs, you should get close to 70. I'm a little bit lower just
15441080	15445400	because I didn't want to go that high. And that is now the model. I mean, we could use this if
15445400	15449400	we want, we could use predict, we could pass in some image, and we could see the prediction for
15449400	15453000	it. I'm not going to do that just because we've already talked about that enough. And I want to
15453000	15458120	get into some of the cooler stuff when we're working with smaller data sets. So the basic idea
15458120	15462040	here is this is actually a pretty small data set, right? We use about 60,000 images. And if you
15462040	15467880	think about the amount of different patterns we need to pick up to classify, you know, things like
15467880	15473480	horses versus trucks, that's a pretty difficult task to do, which means that we need a lot of data.
15473480	15478200	And in fact, some of the best convolutional networks that are out there are trained on millions
15478200	15482680	of pieces of, you know, sample information or data. So obviously, we don't have that kind of data.
15482680	15487480	So how can we work with, you know, a few images, maybe like a few 1000 images, and still get a
15487480	15492520	decent model. Well, the thing is, you can't unless we use some of the techniques that I have to show
15492520	15498120	you. So working with small data sets. So just like I mentioned, it's difficult to create a very good
15498120	15503320	convolutional neural network from scratch, if you're using a small amount of data, that is why we
15503320	15508840	can actually employ these techniques, the first one data augmentation, but also using pre trained
15508840	15512440	models to kind of accomplish what we need to do. And that's what we're going to be talking about
15512440	15515480	now in the second part of the tutorial, we're going to create another convolutional neural
15515560	15519320	network. So just to clarify, this is created, we've made the model up here already. This is
15519320	15523000	all we need to do to do it. This is the architecture. And this was just to get you familiar with the
15523000	15530520	idea. So data augmentation. So this is basically the idea, if you have one image, we can turn that
15530520	15537560	image into several different images, and train and pass all those images to our, our model. So
15537560	15542520	essentially, if we can rotate the image, if we can flip it, if we can stretch it, compress it,
15543080	15547560	you know, shift it, zoom it, whatever it is, and pass that to our model, it should be better at
15547560	15553720	generalizing, because we'll see the same image, but modified and augmented multiple times, which
15553720	15560840	means that we can turn a data set say of 10,000 images into 40,000 images, by doing four augmentations
15560840	15566600	on every single image. Now, obviously, you still want a lot of unique images, but this technique
15566600	15571560	can help a lot and is used quite a bit, because that allows our kind of model to be able to pick
15571560	15575400	up images that maybe are orientated differently or zoomed in a bit or stretch something different,
15575400	15579800	right, just better at generalizing, which is the whole point. So I'm not going to go through this
15579800	15584280	in too depth, too much depth, but this is essentially a script that does data augmentation
15584280	15590440	for you. We're gonna use this image data generator from the Keras dot preprocessing dot image module,
15591080	15595320	we're going to create an image data generator object. Now essentially, what this allows us to
15595320	15599720	do is specify some parameters on how we want to modify our image. In this case, we have the
15599720	15606680	rotation range, some shifts, shear, zoom horizontal flip and the mode. Now I'm not going to go into
15606680	15610440	how this works, you can look at the documentation if you'd like. But essentially, this will just
15610440	15616280	allow us to augment our images. Now what I'm going to do is pick one arbitrary image from the test
15616280	15621160	image data set, just our test image, I guess, group of photos, whatever you want to call it.
15621160	15625320	I'm going to convert that to an image array, which essentially takes it from the weird data set
15625320	15630520	object that it kind of is and turns it into a NumPy array. Then we're going to reshape this.
15630520	15634600	So that's in the form one comma, which essentially means one, and then this will figure out what
15634600	15639720	the rest of the shape should be. Oh, sorry, one and then plus the image shape, which is whatever
15639720	15645960	this shape is. So we'll reshape that. And then what we're going to do is we're going to say for batch
15645960	15651720	in data flow gen dot flow. Talk about how that works in a second. Essentially, this is just going
15651800	15656120	to augment the image for us and actually save it onto our drive. So in this instance, what's
15656120	15660840	going to happen is this data gen dot flow is going to take the image which we've created here, right?
15660840	15665080	And we formatted it correctly by doing these two steps, which you need to do beforehand,
15665080	15669400	it's going to save this image as test dot jpeg. And this will be the prefix, which means there'll
15669400	15674840	be some information after. And it will do this as many times until we break. So essentially,
15674840	15679640	given an image, it will do test one, test two, test three, test four, test five, with random
15679640	15685160	augmentations using this, until eventually we decided to break out of this. Now what I'm doing
15685160	15689480	is just showing the image by doing this and batch zero is just showing us the you know,
15689480	15695080	that first image in there. And that's kind of how this works. So you can mess with the script
15695080	15697960	and figure out a way to use it. But I would recommend if you want to do data augmentation,
15697960	15702360	just look into image data generator. This is something that I just want to show you so
15702360	15705960	you're aware of and I'll just run it so you can see exactly how this works. So essentially,
15705960	15709320	given an image of a truck, what it will do is augmented in these different ways.
15709880	15715160	You can see kind of the shifts, the translations, the rotations, all of that. And we'll do
15716040	15718920	actually a different image here to see what one looks like. Let's just do image say 20.
15719800	15724040	See if we get something different. So in this case, I believe this is maybe like a deer or
15724040	15728120	rabbit or a dog or something. I don't really know exactly what it is because it's so blurry.
15728120	15731560	But you can see that's kind of the shifts we're getting. And it makes sense because you want
15731560	15735560	to have images in different areas so that we have a better generalization. All right,
15735640	15740760	let's close that. Okay, so now we're going to talk about using or sorry, what is it pre trained
15740760	15745320	models? Okay, so we talked about data augmentation. That's a great technique if you want to increase
15745320	15749480	the size of your data set. But what if even after that, we still don't have enough images in our
15749480	15754360	data set? Well, what we can do is use something called a pre trained model. Now companies like
15754360	15759560	Google and you know, TensorFlow, which is owned by Google, make their own amazing convolutional
15759560	15763320	neural networks that are completely open source that we can use. So what we're going to do is
15763320	15767960	actually use part of a convolutional neural network that they've trained already on, I believe
15767960	15774520	1.4 million images. And we're just going to use part of that model as kind of the base of our
15774520	15779480	models that we have a really good starting point. And all we need to do is what's called fine tune,
15779480	15784520	the last few layers of that network, so that they work a little bit better for our purposes.
15785240	15789960	So what we're going to do essentially say, All right, we have this model that Google's trained,
15789960	15794280	they've trained it on 1.4 million images, it's capable of classifying, let's say 1000 different
15794280	15799480	classes, which is actually the example we'll look at later. So obviously, the beginning of that model
15800120	15805400	is what's picking up on the smaller edges, and you know, kind of the very general things that
15805400	15810600	appear in all of our images. So if we can use the base of that model, so kind of the beginning of
15810600	15815160	it, that does a really good job picking up on edges and general things that will apply to any
15815160	15820200	images. Then what we can do is just change the top layers of that model a tiny bit or add our own
15820200	15825240	layers to it to classify for the problem that we want. And that should be a very effective way
15825240	15829960	to use this pre trained model. We're saying we're going to use the beginning part that's really
15829960	15834840	good at kind of the generalization step, then we'll pass it into our own layers that we'll do
15834840	15839480	whatever we need to do specifically for our problem. That's what's like the fine tuning
15839480	15842680	step. And then we should have a model that works pretty well. And in fact, that's what we're going
15842760	15846920	to do in this example now. So that's kind of the point of what I'm talking about here is using
15846920	15851240	part of a model that already exists, that's very good at generalizing, and it's been trained on
15851240	15856760	so many different images. And then we'll pass our own training data in, we won't modify the
15856760	15861400	beginning aspect of our neural network, because it already works really well, we'll just modify the
15861400	15866760	last few layers that are really good at classifying, for example, just cats and dogs, which is exactly
15866760	15870040	the example we're actually going to do here. So I hope that makes sense as we get through this
15870120	15874200	should be cleared up a little bit. But using a pretrained model is now the section we're
15874200	15877960	getting into. So this is based on this documentation, as always, I'm referencing
15877960	15881640	everything. So you guys can go see that if you'd like, and do our imports like this,
15881640	15885160	we're going to load a data set that actually takes a second to load the data set, I believe,
15885160	15890600	oh, maybe not. And essentially, the problem we're doing is trying to classify dogs versus cats with
15890600	15896040	a fair degree of accuracy. In fact, we'd like to get above 90%. So this is the data set we're
15896040	15901800	loading in from TensorFlow data sets as TFDS. This is kind of a weird way to load it in again,
15901800	15905880	stuff like this, you just have to reference the documentation, I can explain it to you, but
15905880	15908840	it's not really going to help when the next example is going to be a different way of
15908840	15912600	loading the data, right? So so long as you know how to get the data in the correct form,
15912600	15916440	you can get it into some kind of NumPy array, you can split it into training, testing and
15916440	15920600	validation data, you should be okay. And if you're using a TensorFlow data set, it should
15920600	15925160	tell you in the documentation how to load it in properly. So we'll load it in here, we're training
15925160	15931400	80% train will go 10% for what is it raw validation and 10% for the testing data.
15932120	15936440	So we've loaded that. And now what we're doing here is just we're going to look at a few images.
15936440	15940680	So this actually creates a function, I know this is a weird thing, this is pretty unique to this
15940680	15947000	example, that allows us to call this function with some integer, essentially, and get what the
15947000	15951240	actual string representation of that is to the label for it. And what I'm doing here is just
15951240	15955720	taking two images from our raw training data set, and just displaying them. And you can see
15955720	15961320	that's where we're getting here dog and dog. If I go ahead and take five, we'll see, these are
15961320	15967400	what our images look like. Right? So here's example of a dog, we have a cat, right? And so on so forth,
15967400	15971960	you kind of you get the you get the point there. Now, notice, though, that these images are
15971960	15975320	different dimensions. In fact, none of these images other than these two actually are the same
15975320	15979640	dimension at all. Oh, actually, I don't think these ones are either. So obviously, there's a
15979640	15983720	step that we need to do, which is we need to scale all these images to be the same size.
15984440	15988840	So to do that, what we're going to do is write a little function like this, and essentially,
15988840	15994280	we'll return an image that is reshaped. So I guess that is reshaped to the image size, which I'm
15994280	15999160	going to set at 160 by 160. Now, we can make this bigger if we want. But the problem sometimes is
15999160	16004680	if you make an image that is bigger than like you want to make your image bigger than most of your
16004680	16008520	data set examples, and that means you're going to be really stretching a lot of the examples out
16008520	16012760	and you're losing a lot of detail. So it's much better to make the image size smaller rather
16012760	16016920	than bigger. You might say, well, if you make it smaller, you're going to lose detail too. But
16016920	16021800	it's just it's better to compress it smaller than it is to go really big, even just when it comes
16021800	16025880	to the amount of training time and how complex networks going to be. So that's something to
16025880	16029400	consider. You can mess around with those when you're making your own networks. But again,
16029400	16033480	smaller is typically better in my opinion, you don't want to go too small, but something that's
16033480	16038200	like, you know, half the size of what an average image would be. Alright, so we're going to go
16038200	16042360	format example. So we're going to just take an image and a label. And what this will do is return
16042360	16046840	to us just the reshaped image and label. So in this case, we're going to cast, which means convert
16046840	16051880	every single pixel in our image to be a float 32 value, because it could be integers, we're then
16051880	16059400	going to divide that by 127.5, which taken is exactly half of 255. And then subtract one,
16059480	16064360	then we're going to resize this image to be the image size. So sorry, the image will be
16064360	16069480	resized to the image size of 160 by 160 and we'll return the new image and the label. So now we
16069480	16073320	can apply this function to all of our images using map, if you don't know what map is, essentially,
16073320	16078360	it takes every single example in in this case, going to be raw train and applies the function
16078360	16084040	to it, which will mean that it will convert raw train into images that are all resized to 160
16084040	16089320	by 160. And we'll do the same thing for validation and test. So run that no issue there. Now
16089320	16093880	let's have a look at our images and see what we get. And there we are. Now I've just messed up
16093880	16100520	the color because I didn't add a CMAP thing, which I think I needed. Where was the CMAP?
16102040	16104920	Anyways, you know what, that's fine for now. This is what our images look like. This is the
16104920	16111400	resize. Now we get all images 160 by 160. And we are good to go. Alright, so
16112680	16116360	now let's have a look at the shape of an original image versus our new image. So I mean, this was
16116440	16121960	just to prove that essentially our original shapes were like 262 409 by some random values,
16121960	16126840	and they're all reshaped now to 160 160 by three, three obviously is the color channel of the images.
16127480	16131400	Alright, so picking a pre trained model. So this is the next step, this is probably one of the
16131400	16135560	harder steps is picking a model that you would actually like to use the base up. Now we're going
16135560	16139720	to use one called mobile net v two, which is actually from Google, it's built into TensorFlow
16139720	16143640	itself. That's why I've picked it. And all we're going to do is set this. So essentially, we're
16143640	16149240	going to say the base model in our code is equal to TF dot keras dot applications dot mobile net
16149240	16153960	v two, which is just telling us the architecture of the model that we want. And we'll have a look
16153960	16158360	at it down below here. In just a second, we'll define the input shape, which is important,
16158360	16162680	because this can take any input shape that we want. So we'll change it to 160 160 by three,
16162680	16168280	which we've defined up here, include top, very important means do we include the
16168280	16172840	classifier that comes with this network already or not. Now in our case, we're going to be
16172840	16178600	retraining parts of this network so that it works specifically for dogs and cats, and not for 1000
16178600	16183480	different classes, which is what this model was actually aimed to do is train a 1.4 million images
16183480	16188280	for 1000 different classes of everyday objects. So we're going to not include the top, which means
16188280	16193400	I don't include the classifier for these 1000 classes. And we're going to load the weights
16193400	16198200	from what's called image net, which is just a specific save of the weights. So this is the
16198200	16202120	architecture. And this is kind of the data that we're filling in for that architecture. So the
16202120	16206920	weights, and we'll load that in which we have here. So base model, now let's look at it. So
16206920	16211160	let's have a summary. You can see this is a pretty crazy model. I mean, we would never be
16211160	16214840	expected to create something like this by ourselves. This is, you know, teams of data
16214840	16219080	scientists, PhD students, engineers, would I write the experts in the field that have created a
16219080	16223160	network like this. So that's why we're going to use it because it works so effectively for
16223160	16227800	the generalization at the beginning, which is what we want. And then we can take those features
16227800	16232680	that this takes out. So in five by five by 1280, which is what I want us to focus on the output
16232680	16238120	of this actual network here. So really, you can see this last layer, we're going to take this and
16238120	16244200	using this information, pass that to some more convolutional layers and actually our own classifier,
16244200	16250360	I believe, and use that to predict versus dogs versus cats. So at this point, the base model
16250360	16255000	will simply output a shape 32 by five by five by 1280. That's the tensor that we're going to get
16255000	16259080	out of this, that's the shape, you can watch how this kind of works as you go through it.
16260200	16264840	And yes, all right. So we can just have a look at this here. This what I wanted to do essentially
16264840	16269720	was just look at what the actual shape was going to be. So 32 five by five by 1280, just because
16269720	16274120	this gives us none until it knows what the input is. And now it's time to talk about freezing the
16274120	16278920	base. So essentially, the point is, we want to use this as the base of our network, which means
16278920	16283640	we don't want to change it. If we just put this network in right now is the base to our neural
16283640	16288520	network. Well, what's going to happen is, it's going to start retraining all these weights and
16288520	16294600	biases. And in fact, it's going to train 2.257 million more weights and biases, when in fact,
16294600	16297960	we don't want to change these because these have already been defined, they've been set. And we
16297960	16301960	know that they work well for the problem already, right, they worked well for classifying 1000
16301960	16305480	classes. Why are we going to touch this now? And if we were going to touch this, what's the point
16305480	16309240	of even using this base, right, we don't want to train this, we want to leave it the same.
16309320	16313320	So to do that, we're just going to freeze it. Now, freezing is a pretty, I mean, it just
16313320	16318760	essentially means turning the trainable attribute of a layer off or of the model off. So what we do
16318760	16322120	is you just say base model dot trainable equals false, which essentially means that we are no
16322120	16326680	longer going to be training any aspect of that, I want to say model, although we'll just call it
16326680	16331240	the base layer for now, or the base model. So now if we look at the summary, we can see when we
16331240	16337480	scroll down to the bottom, if we get there any day soon, that now the trainable parameters is
16337480	16343160	zero instead of 2.257 million, which it was before. And now it's time to add our own classifier on
16343160	16347240	top of this. So essentially, we've got a pretty good network, right, five by five by 1280s,
16347240	16353320	our last output. And what we want to do now is take that. And we want to use it to classify
16353320	16358520	either cat or either dog, right? So what we're going to do is add a global average layer,
16358520	16364760	which essentially is going to take the entire average of every single so 1280 different layers
16364840	16370680	that are five by five, and put that into a 1d tensor, which is kind of flattening that for us.
16370680	16375000	So we do that global average pooling. And then we're just going to add the prediction layer,
16375000	16379800	which essentially is going to just be one dense node. And since we're only classifying two different
16379800	16384680	classes, right, dogs and cats, we only need one, then we're going to add all these models together.
16384680	16389080	So the base model, and I guess layers, the global average layer that we define there, and then the
16389080	16394920	prediction layer to create our final model. So let's do this global average layer, prediction
16394920	16400760	layer model, give that a second to kind of run there. Now when we look at the summary, we can see
16400760	16405720	we have mobile net v2, which is actually a model, but that is our base layer. And that's fine,
16405720	16411640	because the output shape is that then global average pooling, which again, just takes this
16411640	16416600	flattens it out does the average for us. And then finally, our dense layer, which is going to
16416600	16422680	simply have one neuron, which is going to be our output. Now notice that we have 2.25 and nine
16422680	16428920	million parameters in total, and only 1281 of them are trainable. That's because we have 1280
16428920	16435000	connections from this layer to this layer, which means 1280 weights and one bias. So that is what
16435000	16438760	we're doing. This is what we have created now, this base, the majority of the network has been
16438760	16443560	done for us. And we just add our own little classifier on top of this. And now we're going to
16443560	16448600	feed some training samples and data to this. Remember, we're not training this base layer
16448600	16452840	whatsoever. So the only thing that needs to be learned is the weights and biases on these two
16452840	16458040	layers here. Once we have that, we should have a decent model ready to go. So let's actually train
16458040	16463400	this now. I'm going to compile this here, I'm picking a learning rate that's very slow, what
16463400	16467800	essentially what the learning rate means is how much am I allowed to modify the weights and biases
16467800	16472040	of this network, which is what I've done, just made that very low, because we don't want to make
16472040	16477800	any major changes if we don't have to, because we're already using a base model that exists,
16477800	16481160	right? So we'll set the learning rate, I'm not going to talk about what this does specifically,
16481160	16485400	you can look that up if you'd like to. And then the loss function will use binary cross entropy,
16485400	16490280	just because we're using two classes, if you're using more than more than two classes, you would
16490280	16495560	just have cross entropy, or some other type of cross entropy. And then what we're going to do is
16495560	16501640	actually evaluate the model right now, before we even train it. So I've compiled it, I've just set
16501720	16507560	what we'll end up using. But I want to evaluate the model currently, without training it whatsoever,
16507560	16513640	on our validation data or validation batches, and see what it actually looks like, what it
16513640	16518120	actually, you know, what we're getting right now, with the current base model being the way it is,
16518120	16522040	and not having changed the weights and biases that completely random from the global average
16522040	16526920	pooling in the dense layer. So let's evaluate. Let's see what we get as an accuracy. Okay,
16526920	16531320	so we can actually see that with the random weights and biases for those last layer that we added,
16531720	16536040	we're getting an accuracy of 56%, which pretty much means that it's guessing, right? It's,
16536040	16540760	you know, 50% is only two classes. So if we got anything lower than 50, like 50 should have been
16540760	16546040	our guess, which is what we're getting. So now what we're going to do. And actually, I've trained
16546040	16553000	this already, I think so I might not have to do it again, is train this model on all of our images.
16553000	16557560	So all of our images and cats and cats and dogs that we've loaded in before, which will allow us
16557640	16563160	now to modify these weights and biases of this layer. So hopefully it can determine what features
16563160	16568040	need to be present for a dog to be a dog and for a cat to be a cat, right? And then it can make a
16568040	16571960	pretty good prediction. In fact, I'm not going to train this in front of us right now, because
16571960	16576440	it actually takes close to an hour to train just because there is a lot of images that it needs
16576440	16581960	to look at and a lot of calculations that need to happen. But when you do end up training this,
16581960	16587560	you end up getting an accuracy of a close to 92 or 93%, which is pretty good,
16587560	16593160	considering the fact that all we did was use an original layer, like base layer that classified
16593160	16598520	up to 1000 different images, so very general, and applied that just to cats and dogs by adding
16598520	16603240	our dense layer classifier on top. So you can see this was kind of the accuracy I had from
16603240	16607000	training this previously, I don't want to train again, because it takes so long. But I did want
16607080	16613480	to show that you can save a model and load a model by doing this syntax. So essentially,
16613480	16618360	on your model object, you can call model dot save, save it as whatever name you like dot h5,
16618360	16623960	which is just a format for saving models and Keras is specific to Keras, not TensorFlow.
16623960	16628600	And then you can load the model by doing this. So this is useful because after you train this
16628600	16632600	for an hour, obviously, you don't want to retrain this if you don't have to to actually use it to
16632600	16638280	make predictions. So you can just load the model. Now, I'm not going to go into using the model
16638280	16641880	specifically, you guys can look up the documentation to do that. We're at the point now where I've
16641880	16646280	showed you so much syntax on predicting and how we actually use the models. But the basic idea
16646280	16650120	would be to do model dot predict, right? And then you can see that it's even giving me the input
16650120	16655080	here. So model dot predict, give it some x batch size or both, right, because it will predict on
16655080	16658920	multiple things. And that will spit back to you a class, which then you can figure out, okay,
16658920	16662920	this is a cat, or this is a dog, you're going to pass this obviously the same input information
16662920	16667320	we have before, which is 160 by 160 by three. And that will make the prediction for you.
16667960	16672600	So that's kind of the thing there. I was getting an OS error just because I hadn't saved this
16672600	16676040	previously. But that's how you save and load models, which I think is important when you're
16676040	16680680	doing very large models. So when you fit this, feel free to change the epochs to be something
16680680	16686200	slower if you'd like, again, right, this takes a long time to actually end up running. But you
16686200	16690920	can see that the accuracy increases pretty well, exponentially, exponentially from when we didn't
16690920	16695560	even have that classifier on it. Now, the last thing that I want to talk about is object detection,
16695560	16698920	I'm just going to load up a page, we're not going to do any examples, I'm just going to give you a
16698920	16703000	brief introduction, because we're kind of running out of time for this module, because you can use
16703000	16707640	TensorFlow to do object detection and recognition, which is kind of cool. So let's get into that
16707640	16712440	now. Okay, so right now, I'm on a GitHub page that's built by TensorFlow here, I'm going to leave
16712440	16716600	that link in the notebook where it said object detection, so you guys can look at that. But
16716600	16720600	essentially, there is an API for TensorFlow that does object detection for you. And in fact,
16720600	16724280	it works very well, and even gives you confidence scores. So you can see this is what you'll
16724280	16728200	actually end up getting if you end up using this API. Now, unfortunately, we don't have time to go
16728200	16732280	through this because this will take a good amount of time to talk about the setup and how to actually
16732280	16736520	use this project properly. But if you go through this documentation, you should be able to figure
16736520	16739960	it out. And now you guys are familiar with TensorFlow, and you understand some of the
16739960	16744920	concepts here. This runs a very different model than what we've discussed before. Unfortunately,
16744920	16747960	again, we don't have time to get into it. But just something I wanted to make clear is that you
16747960	16752360	can do something like this with TensorFlow. And I will leave that resource so that if you'd like
16752360	16756680	to check this out, you can use it. There's also a great module in Python called facial recognition,
16756680	16761000	it's not a part of TensorFlow. But it does use some kind of convolutional neural network to do
16761000	16765800	facial detection and recognition, which is pretty cool as well. So I'll put that link in here.
16765880	16770680	But for that, for now, that's going to be our, what is it convolutional neural network kind of
16770680	16776200	module. So I hope that says cleared some things up on how deep vision works and how convolutional
16776200	16780680	neural networks work. I know I haven't gone into crazy examples of what I've shown you some different
16780680	16786360	techniques that hopefully you'll go look up kind of on your own and really dive into. Because now
16786360	16790360	you have that base kind of domain knowledge where you're going to be able to follow along with the
16790360	16794760	tutorial and understand exactly what to do. And if you want to create your own model, so long as
16794760	16799480	you can get enough sufficient training data, you can load that training data into your computer,
16799480	16804760	put that in a NumPy array. Then what you can do is create a model like we've just done using even
16805640	16810440	something like the mobile nets, what was it v two that we talked about previously, if I could even
16810440	16815480	get up and need to close this output. Oh my gosh, this is this is massive output here. Where is
16815480	16820280	this begin to pre train model? Yeah, mobile net v two, you can use the base of that, and then add
16820280	16824920	your own classifier on do a similar thing to what I've done with that dense neuron and that global
16824920	16828680	average layer. And hopefully you should get a decent result from that. So this is just showing
16828680	16833720	you what you can do. Obviously, you can pick a different base layer, depending on what kind of
16833720	16837560	problem you're trying to solve. So anyways, that has been convolutional neural networks. I hope
16837560	16841480	you enjoyed that module. Now we're on to recurrent neural networks, which is actually going to be
16841480	16848840	pretty interesting. So I'll see you in that module. Hello, everyone, and welcome to the next module
16848840	16852920	in this course, which is covering natural language processing with recurrent neural
16852920	16858440	networks. Now what we're going to be doing in this module here is first of all, first off discussing
16858440	16862680	what natural language processing is, which I guess I'll start with here. Essentially, for those of
16862680	16868520	you that don't know, natural language processing or NLP for short, is the field or discipline in
16868520	16874200	computing or machine learning that deals with trying to understand natural or human languages.
16874200	16878280	Now, the reason we call them natural is because these are not computer languages or
16878280	16884280	programming languages, per se. And actually, computers are quite bad at understanding textual
16884280	16889400	information and human languages. And that's why we've come up with this entire discipline focused
16889400	16893880	on how they can do that. So we're going to do that using something called recurrent neural
16893880	16898920	networks. But some examples of natural language processing would be something like spell check,
16898920	16905400	autocomplete voice assistance, translation between languages, there's all different kinds of things,
16905480	16910920	chatbots, but essentially anything that deals with textual data. So you like paragraphs,
16910920	16916360	sentences, even words, that is probably going to be classified under natural language processing
16916360	16921000	in terms of doing some kind of machine learning stuff with it. Now, we are going to be talking
16921000	16925000	about a different kind of neural network in this series called recurrent neural networks.
16925000	16929720	Now, these are very good at classifying and understanding textual data. And that's why we'll
16929720	16934440	be using them. But they are fairly complex. And there's a lot of stuff that goes into them.
16934440	16938840	Now, in the interest of time, and just not knowing a lot of your math background, I'm not
16938840	16943880	going to be getting into the exact details of how this works on a lower level, like I did when I
16943880	16949000	explained kind of our, I guess, fundamental learning algorithms, which are a bit easier to grasp,
16949000	16953080	and even just regular neural networks in general, we're going to be kind of skipping over that and
16953080	16959480	really focusing on why this works the way it does, rather than how and when you should use this.
16960200	16963560	And then maybe understanding a few of the different kinds of layers that have to do with
16963640	16967480	recurrent neural networks. But again, we're not going to get into the math. If you'd like to
16967480	16971160	learn about that, there will be some sources at the bottom of the guide. And you can also just
16971160	16975320	look up recurrent neural networks, and you'll find lots of resources that explain all of the
16975320	16980840	fancy math that goes on behind them. Now, the exact applications and kind of things will be working
16980840	16986120	towards here is sentiment analysis. That's the first kind of task or thing we're going to do.
16986120	16990200	We're actually going to use movie reviews and try to determine whether these movie reviews are
16990280	16995720	positive or negative by performing sentiment analysis on them. Now, if you're unfamiliar
16995720	16999320	with sentiment analysis, we'll talk about it more later, but essentially means trying to
16999320	17004120	determine how positive or negative a sentence or piece of text is, which you can see why that would
17004120	17010120	be useful for movie reviews. Next, we're going to do character slash text generation. So essentially,
17010120	17014920	we're going to use a natural language processing model, I guess, if you want to call it that,
17015560	17020920	to generate the next character in a sequence of text for us. And we're going to use that model a
17020920	17026120	bunch of times to actually generate an entire play. Now, I know this seems a little bit ridiculous
17026120	17030520	compared to some of the trivial examples we've done before. This will be quite a bit more code
17030520	17033800	than anything we've really looked at yet. But this is very cool, because we're going to actually
17033800	17038680	going to train a model to learn how to write a play. That's literally what it's going to do.
17038680	17042760	It's going to read through a play, I believe it's Romeo and Juliet. And then we're going to give it
17042840	17047480	a little prompt when we're actually using the model and say, okay, this is the first part of
17047480	17051640	the play, write the rest of it. And then it will actually go and write the rest of the characters
17051640	17055560	in the play. And we'll see that we can get something that's pretty good using the techniques that
17055560	17060760	we'll talk about. So the first thing that I want to do is talk about data. So I'm going to hop onto
17060760	17066040	my drawing tablet here. And we're going to compare the difference between textual data and numeric
17066040	17070360	data, like we've seen before, and why we're going to have to employ some pretty complex and
17070360	17074760	different steps to turn something like this, you know, a block of text into some meaningful
17074760	17079560	information that our neural networks actually going to be actually going to be able to understand
17079560	17083720	and process. So let's go ahead and get over to that. Okay, so now we're going to get into the
17083720	17089960	problem of how we can turn some textual data into numeric data that we can feed to our neural
17089960	17094680	network. Now, this is a pretty interesting problem. And we'll kind of go through as we start going
17094680	17098360	through it, you should see why this is interesting and why there's a lot of difficulties with the
17098360	17102200	different methods that we pick. But the first method that I want to talk about is something
17102200	17107800	called bag of words, in terms of how we can kind of encode and pre process text into integers.
17107800	17112920	Now, obviously, I'm not the first person to come up with this bag of words is a very famous,
17112920	17118200	almost I want to say algorithm or method of converting textual data to numeric data, although
17118200	17123400	it is pretty flawed and only really works for simple tasks. And we're going to understand why
17123400	17128280	in a second. So we're going to call this bag of words. Essentially, what bag of words says is
17128280	17131880	what we're going to do is we're going to look at our entire training data set, right, because we're
17131880	17136120	going to be turning our training data set into a form the network can understand. And we're going
17136120	17142280	to create a dictionary lookup of the vocabulary. Now, what I mean by that is we're going to say
17142280	17149080	that every single unique word in our data set is the vocabulary, right? That's the amount of words
17149080	17153320	that the model is expected to understand, because we're, you know, going to show all those words
17153320	17157400	to the model. And we're going to say that every single one of these words. So every single one
17157400	17162440	of these words in the vocabulary is going to be placed in a dictionary. And beside that, we're
17162440	17167320	going to have some integer that represents it. So for example, maybe the vocabulary of our data
17167320	17175960	set is the words, you know, I, a, maybe Tim, maybe day, me, right, we're gonna have a bunch of arbitrary
17175960	17180520	words, I'll just put dot dot dot to show that this kind of goes to the length of the vocabulary.
17180520	17183800	And every single one of these words will be placed in a dictionary, which we're going to just
17183800	17189320	going to call kind of our lookup table or word index table. And we're going to have a number
17189320	17195160	that represents every single one of them. So you can imagine that in very large data sets, we're
17195160	17200280	going to have, you know, tens of thousands of hundreds of thousands, sometimes even maybe millions
17200280	17204680	of different words, and they're all going to be encoded by different integers. Now, the reason
17204760	17209960	we call this bag of words is because what we're actually going to do when we look at a sentence
17209960	17215480	is we're only going to keep track of the words that are present and the frequency of those words.
17215480	17221720	And in fact, what we'll do well is we'll create what we call a bag. And whenever we see a word
17221720	17228040	appears, we'll simply add its number into the bag. So if I have a sentence like, you know, I
17229000	17236360	am Tim day, day, I'm just going to do like a random sentence like that. Then what we're going
17236360	17241000	to do is every time we see a word, we're going to take its number and throw it into the bag. So
17241000	17248360	we're going to say, all right, I, that's zero, and that's one, Tim, that's two, day, that's three,
17249160	17253960	that's three again. And notice that what's happening here is we're losing the ordering
17254040	17258920	of these words, but we're just keeping track of the frequency. Now, there's lots of different
17258920	17264200	ways to kind of format how we want to do bag of words. But this is the basic idea, I'm not going
17264200	17267880	to go too far in because we're not actually really going to use this technique. But essentially,
17267880	17272840	you lose the ordering in which words appear, but you just keep track of the frequency and what
17272840	17277640	words appear. So this could be very useful when you're looking, you know, you're doing very simple
17277640	17283400	tasks where the presence of a certain word will really influence the kind of type of sentence
17283480	17287240	that it is, or the meaning that you're going to get from it. But when we're looking at more
17287240	17292040	complex input, where, you know, different words have different meanings, depending on where they
17292040	17298360	are in a sentence, this is a pretty flawed way to encode this data. Now, I won't go much further
17298360	17303480	into this. This is not the exact way the bag of words works. But I just wanted to show you
17303480	17308840	kind of an idea here, which is we just encode every single unique word by an integer. And then we
17308840	17313800	don't even really care about where these words are. We just throw them into a bag and we say,
17313800	17318040	All right, you know, this is our bag right here that I'm doing the arrow to, we'll just throw in
17318040	17322680	three as many times as you know, the word day appears, we'll throw in one as many times as the
17322680	17329640	word, I guess, am appears, and so on and so forth. And then what will happen is we'll feed this bag
17329640	17333720	to our neural network in some form, depending on the network that we're using. And it will just
17333720	17337560	look at and say, Okay, so I have all these different numbers, that means these words are present
17337640	17341640	and try to do something with it. Now, I'm going to show you a few examples of where this kind of
17341640	17345320	breaks down, but just understand that this is how this works. This is the first technique called
17345320	17350360	bag of words, which again, we will not be using. So what happens when we have a sentence where
17350360	17356200	the same word conveys a very different meaning, right? And I'm actually, I think I have an example
17356200	17362600	on the slides here that I'll go into. Yes, select this. Okay, so for our bag of words technique,
17362600	17367160	which we can kind of see here, maybe we'll go through it. Let's consider the two sentences
17367560	17371800	where are they here? I thought the movie was going to be bad, but it was actually amazing.
17371800	17376200	And I thought the movie was going to be amazing, but it was actually bad, right? So consider these
17376200	17379560	two sentences. Now, I know you guys already know what I'm going to get at. But essentially, these
17379560	17385320	sentences use the exact same words. In fact, they use the exact same number of words, the exact same
17385320	17391800	words in total. And well, they have a very different meaning. With our bag of words technique,
17391800	17397080	we're actually going to encode these two sentences using the exact same representation. Because
17397160	17402440	remember, all we do is we care about the frequency and what words appear, but we don't care about
17402440	17408680	where they appear. So we end up losing that meaning from the sentence, because the sentence I thought
17408680	17412920	the movie was going to be bad, but it was actually amazing is encoded and represented by the same
17412920	17417640	thing as this sentence is. So that, you know, obviously is an issue that's a flaw. And that's
17417640	17422760	one of the reasons why bag of words is not very good to use, because we lose the context of the
17422760	17427560	words within the sentence, we just pick up the frequency and the fact that these words exist.
17427560	17431080	So that's the first technique that's called bag of words. I've actually written a little
17431080	17435560	function here that does this for us. This is not really the exact way that we would write a bag
17435560	17441720	of words function. But you kind of get the idea that when I have a text, this is a test to see
17441720	17447560	if this test will work is test a I just did a bunch of random stuff. So we can see what I'm
17447560	17451560	doing is printing out the bag, which I get from this function. And you guys can look at this if
17451560	17455400	you kind of want to see how this works. And essentially, what it tells us is the word one
17455400	17462200	appears two times. Yes, the word two appears three times the word three appears three times word
17462200	17467320	four appears three times five ones, six, one, seven ones. So on, that's the information we get
17467320	17472040	from our bag, right from that encoding. And then if we look up here, this is our vocabulary. So
17472040	17476440	this stands for one is is two is three so on. And you can kind of get the idea from that.
17477240	17482120	So that is how we would use bag of words, right? If we did an encoding kind of like this, that's
17482120	17486920	what that does. And that's one way of encoding it. Now, I'm going to go back and we'll talk about
17486920	17491160	another method here as well, actually, a few more methods before we get into anything further.
17491160	17495400	All right, so I'm sure a lot of you were looking at the previous example I did. And you saw the
17495400	17501480	fact that what I did was completely remove the idea of kind of sequence or ordering of words,
17501480	17504760	right? And what I did was just throw everything in a bag. And I said, All right, we're just going
17504760	17511160	to keep track of the fact that we have, you know, three A's, or we have four, those or seven, Tim's
17511160	17514920	right. And we're going to just going to lose the fact that, you know, words come after one
17514920	17518760	each other, we're going to lose their ordering in the sentence. And that's how we're going to
17518760	17523320	encode it. And I'm sure a lot of you were saying, Well, why don't we just not lose the ordering of
17523320	17527800	those words? We'll just encode every single word with an integer and just leave it in its space
17527800	17531560	where it would have been in the original string. Okay, good idea. So what you're telling me to
17531560	17537560	do is something like this, you know, Tim is here will be our sentence. Let's say we encode the word
17537560	17542520	Tim with zero is as one, here's two. And then that means our translation goes zero, one, two.
17543160	17548360	And that means, right, if we have a translation, say, like two, one, zero, even though these use
17548360	17554520	the exact same number of words and exact same representation for all these words. Well, this
17554520	17559080	is a different sentence. And our model should be able to tell that because these words come in a
17559080	17563080	different order. And to you, good point, if you made that point, but I'm going to discuss where
17563080	17568200	this falls apart as well. And why we're not going to use this method. So although this does solve
17568200	17573080	the problem I talked about previously, where we're going to kind of lose out on the context of a word,
17573080	17576920	there's still a lot of issues with this and they come, especially when you're dealing with very
17576920	17581960	large vocabularies. Now, let's take an example where we actually have a vocabulary of say 100,000
17581960	17587800	words. And we know that that means we're going to have to have 100,000 unique mappings from words
17587800	17594440	to integers. So let's say our mappings are something like this, one maps to the string happy,
17595160	17604200	the word happy, right, to maps to sad. And let's say that the string 100,000 or the number 100,000
17604200	17610120	maps to the word, I don't know, let's say good. Now, we know as humans, by kind of just thinking
17610120	17614440	about, let's consider the fact that we're going to try to classify sentences as a positive or
17614440	17619000	negative. So sentiment analysis, that the words happy and good in that regard, you know, sentiment
17619000	17623000	analysis are probably pretty similar words, right? And then if we were going to group these words,
17623000	17627000	we'd probably put them in a similar group, we'd classify them as similar words, we get probably
17627000	17631560	interchange them in a sentence, and it wouldn't change the meaning a whole ton. I mean, it might,
17631560	17636600	but it might not as well. And that we could say these are kind of similar. But our model or our
17636600	17643400	encoder, right, whatever we're doing to translate our text into integers here, has decided that 100,000
17643480	17647080	is going to represent good, and one is going to represent happy. And well, there's an issue with
17647080	17652360	that, because that means when we pass in something like one, or 100,000 to our model, it's going to
17652360	17657800	have a very difficult time determining the fact that one and 100,000, although they're 99,999,
17659640	17664200	kind of units apart, are actually very similar words. And that's the issue we get into when we
17664200	17668920	do something like this is that the numbers we decide to pick to represent each word are very
17668920	17674600	important. And we don't really have a way of being able to look at words group them and saying,
17674600	17679720	okay, well, we need to put all of the happy words and the range like zero to 100, all of the like
17679720	17684440	adjectives in this range, we don't really have a way to do that. And this gets even harder for our
17684440	17688680	model when we have these arbitrary mappings, right? And then we have something like two in
17688680	17694040	between where two is very close to one, right? Yet these words are complete opposites. In fact,
17694040	17698280	I'd say they're probably polar opposites, our model trying to learn that the difference between one
17698360	17703320	and two is actually way larger than the difference between one and 100,000 is going to be very
17703320	17708840	difficult. And say it's even able to do that, as soon as we throw in the mapping 900, right,
17708840	17715000	the 99,900, we put that as bad. Well, now it gets even more difficult, because it's now like, okay,
17715000	17719080	what the range is this big, then that means these words are actually very similar. But then you
17719080	17723320	throw another word in here like this, and it messes up the entire system. So that's kind of
17723320	17727000	what I want to show is that that's where this breaks apart on these large vocabularies. And
17727080	17731800	that's why I'm going to introduce us now to another concept called word embeddings. Now,
17731800	17736680	what word embeddings does is essentially try to find a way to represent words that are similar
17736680	17742360	using very similar numbers. And in fact, what a word embedding is actually going to do, I'll talk
17742360	17749560	about this more in detail as we go on, is classify or translate every single one of our words into
17749560	17755320	a vector. And that vector is going to have some, you know, n amount of dimensions. Usually, we're
17755320	17761320	going to use something like 64, maybe 128 dimensions for each vector. And every single component of
17761320	17767160	that vector will kind of tell us what group it belongs to or how similar it is to other words.
17767160	17770680	So let me give you an idea of what I mean. So we're going to create something called the word
17770680	17775560	embeddings. Now, don't ask why it's called embeddings, I don't know the exact reason, but I
17775560	17779960	believe it's to have has to do something with the fact that they're vectors. And let's just say we
17779960	17783800	have a 3d plane like this. And we've already kind of looked at what vectors are before. So I'll skip
17783880	17789800	over explaining them. And what we're going to do is take some word. So let's say we have the word
17789800	17794840	good. And instead of picking some integer to represent it, we're going to pick some vector,
17794840	17798680	which means we're going to draw some vector in this 3d space. Actually, let's make this a different
17798680	17806360	color. Let's make this vector say red, like this. And this vector represents this word good. And in
17806440	17813240	this case, we'll say we have x one, x two, x three is our dimensions, which means that every single
17813240	17818120	word in our data set will be represented by three coordinates. So one vector with three different
17818120	17823960	dimensions, where we have x one x two and x three. And our hope is that by using this word
17823960	17828680	embeddings layer, and we'll talk about how it accomplishes this in a second, is that we can
17828680	17833640	have vectors that represent very similar words being very similar, which means that you know,
17833640	17839960	if we have the vector good here, we would hope the vector happy from our previous example, right,
17839960	17845160	will be a vector that points in a similar direction to it. That is kind of a similar looking thing
17845160	17850360	where the angle between these two vectors, right, and maybe I'll draw it here so we can see is small
17850360	17855480	so that we know that these words are similar. And then we would hope that if we had a word that
17855480	17859800	was much different, maybe say like the word bad, that that would point in a different direction,
17859800	17864680	the vector that represents it. And that that would tell our model, because the angle between
17864680	17869000	these two vectors is so big, that these are very different words, right? Now, in theory,
17869000	17873320	does the embedding work layer work like this, you know, not always, but this is what it's
17873320	17879960	trying to do is essentially pick some representation in a vector form for each word. And then these
17879960	17883880	vectors, we hope if they're similar words, they're going to be pointing in a very similar
17883880	17888920	direction. And that's kind of the best explanation of a word embeddings layer I can give you.
17888920	17893400	Now, how do we do this, though, how do we actually, you know, go from word to vector,
17894440	17899160	and have that be meaningful? Well, this is actually what we call a layer. So word embeddings
17899160	17903560	is actually a layer, and it's something we're going to add to our model. And that means that
17903560	17908920	this actually learns the embeddings for our words. And the way it does that is by trying to kind of
17908920	17914760	pick out context in the sentence and determine based on where a word is in a sentence, kind of what
17914840	17920680	it means, and then encodes it doing that. Now, I know that's kind of a rough explanation to give
17920680	17924760	to you guys, I don't want to go too far into word embeddings in terms of the math, because I don't
17924760	17929080	want to get, you know, waste our time or get too complicated if we don't need to. But just understand
17929080	17933400	that our word embeddings are actually trained, and that the model actually learns these word
17933400	17939160	embeddings as it goes. And we hope that by the time it's looked at enough training data, it's
17939160	17944600	determined really good ways to represent all of our different words, so that they make sense to
17944600	17949160	our model in the further layers. And we can use pre trained word embedding layers, if we'd like,
17949160	17954360	just like we use that pre trained convolutional base in the previous section. And we might actually
17954360	17957800	end up doing that. Actually, probably not in this tutorial, but it is something to consider that
17957800	17962280	you can do that. So that's how word embeddings work. This is how we encode textual data. And this
17962280	17966920	is why it's so important that we kind of consider the way that we pass information to our neural
17966920	17971960	network, because it makes a huge difference. Okay, so now that we've talked about kind of the form
17972040	17975960	that we need to get our data in before we can pass it further in the neural network, right,
17975960	17980600	before it can get past that embedding layer, before it can get put in, put into any dense
17980600	17985000	neurons, before we can even really do any math with it, we need to turn it into numbers, right,
17985000	17990040	our textual data. So now that we know that it's time to talk about recurrent neural networks.
17990040	17994280	Now recurrent neural networks are the type of networks we use when we process textual data,
17994280	17998600	typically, you don't always have to use these, but they are just the best for natural language
17998600	18003080	processing. And that's why they're kind of their own class, right? Now, the fundamental difference
18003080	18007320	between a recurrence neural network and something like a dense neural network, or a convolutional
18007320	18013400	neural network, is the fact that it contains an internal loop. Now, what this really means is
18013400	18018680	that the recurrent neural network does not process our entire data at once. So it doesn't
18018680	18024360	process the entire training example, or the entire input to the model at once, what it does is
18024440	18030920	processes it at different time steps, and maintains what we call an internal memory, and kind of an
18030920	18037240	internal state, so that when it looks at a new input, it will remember what it's seen previously
18037240	18042600	and treat that input based on kind of the context or the understanding it's already developed.
18042600	18047240	Now, I understand that this doesn't make any sense right now. But with a dense neural network,
18047240	18052680	or the neural networks we looked at so far, we call those something called feed forward neural
18052680	18058280	networks. What that means is we give all of our data to it at once, and we pass that data from
18058280	18063880	left to right, or I guess for you guys from left to right. So we give all of the information, you
18063880	18067960	know, we would pass those through the convolutional layer to start, maybe we have passed them through
18067960	18073000	dense neurons, but they get given all of the info. And then that information gets translated
18073000	18078200	through the network to the very end, again, from left to right. Whereas here, with recurrent neural
18078200	18083400	networks, we actually have a loop, which means that we don't feed the entire textual data at
18083400	18089960	once, we actually feed one word at a time, it processes that word, generate some output based
18089960	18095800	on that word, and uses the internal memory state that it's keeping track of to do that as part of
18095800	18100920	the calculation. So essentially, the reason we do this is because just like humans, when we, you
18100920	18107240	know, look at text, we don't just take a photo of this text and process it all at once, we read it
18107320	18112920	left to right, word to word. And based on the words that we've already read, we start to slowly
18112920	18119160	develop an understanding of what we're reading, right? If I just read the word now, that doesn't
18119160	18124520	mean much to me, if I just read the word in code, that doesn't mean much. Whereas if I read the entire
18124520	18129480	sentence, now that we've learned a little bit about how we can encode text, I start to develop
18129480	18134680	an understanding about what this next word means, based on the previous words before it, right?
18134760	18138200	And that's kind of the point here is that this is what a recurrent neural network is going to do
18138200	18144440	for us. It's going to read one word at a time, and slowly start building up its understanding
18144440	18149960	of what the entire textual data means. And this works in kind of a more complicated sense than
18149960	18154840	that will draw it out a little bit. But this is kind of what would happen if we on, I guess,
18154840	18159240	unraveled a recurrent layer, because recurrent neural network, yes, it has a loop in it. But
18159240	18163480	really the recurrent aspect of a neural network is the layer that implements this
18164040	18169000	recurrent functionality with a loop. Essentially, what we can see here is that if we're saying x
18169000	18175960	is our input and h is our output, x t is going to be our input at time t, whereas h t is going to
18175960	18181880	be our output at time t. If we had a text of say length four, so four words, like we've encoded
18181880	18187320	them into integers now at this point, the first input at time zero will be the first word into
18187320	18192680	our network, right, or the first word that this layer is going to see. And the output at that time
18192680	18197720	is going to be our current understanding of the entire text after looking at just that one word.
18198600	18205080	Next, what we're going to do is process input one, which will be the next word in the sentence.
18205080	18210680	But we're going to use the output from the previous kind of computation or the previous iteration
18211400	18216360	to do this. So we're going to process this word in combination with what we've already seen,
18216360	18220040	and then have a new output, which hopefully should now give us an understanding of what
18220120	18226280	those two words mean. Next, we'll go to the third word. And so forth, and slowly start building
18226280	18231720	our understanding of what the entire textual data means by building it up one by one. The reason
18231720	18236360	we don't pass the entire sequence at once is because it's very, very difficult to just kind
18236360	18241880	of look at this huge blob of integers and figure out what the entire thing means. If we can do it
18241880	18246840	one by one and understand the meaning of specific words based on the words that have came before
18246920	18251320	it and start learning those patterns, that's going to be a lot easier for a neural network to deal
18251320	18256120	with than just passing it all at once, looking at it and trying to get some output. And that's
18256120	18260600	why we have these recurrent layers, there's a few different types of them. And I'm going to go
18260600	18265080	through them, and then we'll talk a little bit more in depth of how they work. So the first one
18265080	18270520	is called long short term memory. And actually, in fact, before we get into this, let's, let's
18270520	18275000	talk about just a first like a simple layer so that we kind of have a reference point before
18275000	18280280	going here. Okay, so this is kind of the example I want to use here to illustrate however current
18280280	18285960	neural network works and a more teaching style rather than what I was doing before. So essentially,
18285960	18291080	the way that this works is that this whole thing that I'm drawing here, right, all of this circle
18291080	18296760	stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing
18296760	18303000	you kind of how this works in a series of steps. So rather than passing all the information at once,
18303080	18307640	we're going to pass it as a sequence, which means that we're going to have all these different words
18307640	18312520	and we're going to pass them one at a time to the kind of to the layer, right, to this recurrent
18312520	18317400	layer. So we're going to start from this left side over here. So this right, you know, start over
18317400	18323320	here at time step zero, that's what zero means. So time step is just, you know, the order. In this
18323320	18328440	case, this is the first word. So let's say we have the sentence Hi, I am Tim, right, we've broken
18328440	18331960	these down into vectors, they've been turned into their numbers, I'm just writing them here so we
18331960	18337720	can kind of see what I mean in like a natural language. And they are the input to this recurrent
18337720	18343320	layer. So all of our different words, right, that's how many kind of little cells we're going to draw
18343320	18346920	here is how many words we have in this sequence that we're talking about. So in this case, we have
18346920	18352120	four, right, four words. So that's why I've drawn four cells to illustrate that. Now what we do is
18352120	18358760	that time step zero, the internal state of this layer is nothing, there's no previous output,
18358760	18364760	we haven't seen anything yet, which means that this first kind of cell, which is what I'm looking
18364760	18369960	at right here, what I'm drawing in this first cell is only going to look and consider this first word
18369960	18374760	and kind of make some prediction about it and do something with it. We're going to pass high to
18374760	18379560	this cell, some math's going to go on in here. And then what it's going to do is it's going to output
18379560	18385400	some value, which, you know, tells us something about the word high, right, some numeric value,
18385400	18388280	we're not going to talk about what that is, but it's going to do is going to be some output.
18389080	18394200	Now, what happens is after this cell has finished processing this, so right, so this one's done,
18394200	18399080	this has completed h zero, the outputs there, we'll do a check mark to say that that's done,
18399080	18404840	it's finished processing, this output gets fed into actually the same thing again, we're kind of
18404840	18412120	just keeping track of it. And now what we do is we process the next input, which is I, and we use
18412120	18417960	the output from the previous cell to process this and understand what it means. So now, technically,
18417960	18423240	we should have some output from the previous cell. So from whatever high was, right, we do some
18423240	18429560	analysis on the word I, we kind of combine these things together. And that's the output of this
18429560	18436040	cell is our understanding of not only the current input, but the previous input with the current
18436040	18441240	input. So we're slowly kind of building up our understanding of what this word I means,
18441240	18445800	based on the words we saw before. And that's the point I'm trying to get at is that
18445800	18450920	this network uses what it's seen previously to understand the next thing that it sees,
18450920	18456440	it's building a context is trying to understand not only the word, but what the word means,
18456440	18462120	you know, in relation to what's come before it. So that's what's happening here. So then
18462120	18467560	this output here, right, we get some output, we finish this, we get some output h one,
18467560	18473240	h one is passed into here. And now we have the understanding of what high and I means,
18473240	18479240	and we add am like that, we do some kind of computations, we build an understanding of what
18479240	18486120	this sentence is. And then we get the output h two, that passes to h three. And now finally,
18486120	18489720	we have this final output h three, which is going to understand hopefully,
18490520	18496600	what this entire thing means. Now, this is good, this works fairly well. And this is called a
18496600	18502680	simple RNN layer, which means that all we do is we take the output from the previous cell or the
18502680	18507880	previous iteration, because really, all of these cells is just an iteration almost in a for loop,
18507880	18513240	right, based on all the different words in our sequence. And we slowly start building to that
18513240	18519720	understanding as we go through the entire sequence. Now, the only issue with this is that as we have
18519720	18526520	a very long sequence, so sequences of length, say 100 or 150, the beginning of those sequences
18526520	18531880	starts to kind of get lost. As we go through this, because remember, all we're doing, right,
18531880	18537320	is the output from h two is really a combination of the output from h zero and h one, and then
18537320	18542200	there's a new word that we've looked at. And h three is now a combination of everything before it,
18542200	18546440	and this new word. So it becomes increasingly difficult for our model to actually
18547000	18552040	build a really good understanding of the text in general, when the sequence gets long, because
18552040	18556920	it's hard for it to remember what it's seen at the very beginning, because that is now so insignificant,
18556920	18561800	there's been so many outputs tacked on to that, that it's hard for it to go back and see that if
18561800	18566520	that makes any sense. Okay, so what I'm going to do now is try to explain the next layer we're
18566520	18572120	going to look at, which is called LSTM. So the previous layer we just looked at the recurrent
18572120	18576680	layer was called a simple RNN layer. So simple recurrent neural network layer, whatever you
18576680	18581240	want to call it, right, simple recurrent layer. Now we're going to talk about the layer, which is
18581240	18587640	LSTM, which stands for long, short term memory. Now, long and short are hyphenated together.
18587640	18591320	But essentially, what we're doing, and it just gets a little bit more complex, but I won't go
18591320	18598120	into the math, is we add another component that keeps track of the internal state. So right now,
18598120	18603320	the only thing that we were tracking as kind of our internal state as the memory for this model
18603320	18609640	was the previous output. So whatever the previous output was. So for example, at time zero here,
18610280	18616120	there was no previous output. So there was nothing being kept in this model. But at time one, the
18616120	18623320	output from this cell right here was what we were storing. And then at cell two, the only thing we
18623400	18630440	were storing was the output at time one, right? And we've lost now the output from time zero.
18631080	18637000	What we're adding in long, short term memory is an ability to access the output from any
18637000	18642040	previous state at any point in the future when we want it. Now, what this means is that rather
18642040	18647800	than just keeping track of the previous output, we'll add all of the outputs that we've seen so far
18647800	18651480	into what I'm going to call my little kind of conveyor belt, it's going to run at the top
18651480	18654920	up here. I know it's kind of hard to see, but it's just what I'm highlighting. It's almost just
18654920	18661080	like a lookup table that can tell us the output at any previous cell that we want. So we can kind
18661080	18665080	of add things to this conveyor belt, we can pull things off, we can look at them. And this just
18665080	18670280	adds a little bit of complexity to the model. It allows us to not just remember the last state,
18670280	18676760	but look anywhere at any point in time, which can be useful. Now, I don't want to go into much
18676760	18681240	more depth about exactly how this works. But essentially, you know, just think about the
18681240	18686680	idea that as the sequence gets very long, it's pretty easy to forget the things we saw at the
18686680	18690920	beginning. So if we can keep track of some of the things we've seen at the beginning, and some of
18690920	18695320	the things in between on this little conveyor belt, and we can access them whenever we want,
18695320	18699080	then that's going to make this probably a much more useful layer, right? We could look at the
18699080	18705240	first sentence and the last sentence of a big piece of text at any point that we want and say,
18705240	18710520	okay, you know, this tells us X about the meaning of this text, right? So that's what this LSTM
18710520	18715000	does. Again, I don't want to go too far. We've already spent a lot of time kind of covering,
18715000	18718840	you know, recurrent layers and how all this works. Anyways, if you do want to look it up,
18718840	18722920	some great mathematical definitions, again, I will source everything at the bottom of this
18722920	18727400	document so you can go there. But again, that's LSTM, long short term memory, that's what we're
18727400	18732520	going to use for some of our examples, although simple RNN does work fairly well for shorter
18732520	18737080	length sequences. And again, remember, we're treating our text as a sequence now, where we're
18737080	18741720	going to feed each word into the recurrent layer, and it's going to slowly start to develop an
18741720	18745960	understanding as it reads through each word, right and processes that. Okay, so now we are
18745960	18751000	on to our first example, where we're going to be performing sentiment analysis on movie reviews
18751000	18755560	to determine whether they are positive reviews or negative reviews. Now, we already know what
18755560	18759160	sentiment means. That's essentially what I just described. So picking up, you know, whether a
18759800	18764600	block of text is considered positive or negative. And for this example, we're going to be using the
18764600	18769960	movie review data sets. Now, as per usual, this is based off of this TensorFlow tutorial slash
18769960	18775320	guide. I found this one kind of confusing to follow in the TensorFlow website, but obviously
18775320	18779640	you can follow along with that if you don't prefer that version over mine. But anyways,
18779640	18783800	we're going to be talking about the movie review data set. So this data set is straight from Keras,
18783880	18789960	and it contains 25,000 reviews, which are already pre processed and labeled. Now, what that means
18789960	18794760	for us is that every single word is actually already encoded by an integer. And in fact,
18794760	18800360	they've done kind of a clever encoding system where what they've done is said, if a character is
18800360	18807320	encoded by say integer zero, that represents how common that word is in the entire data set. So
18807320	18811480	if an integer was encoded, but are not interested, a word was encoded by integer three, that would
18811480	18815720	mean that it is the third most common word in the data set. And in this specific data set,
18815720	18821720	we have a vocabulary size of 88,584 unique words, which means that something that was
18821720	18827640	classified as this. So 88,584 would be the least common word in the data set. So something to
18827640	18831720	keep in mind, we're going to load in the data set and do our imports just by hitting run here.
18831720	18836040	And as I've mentioned previously, you know, I'm not going to be typing this stuff out. It's just
18836040	18840520	it's kind of a waste of time. I don't have all the syntax memorized. I would never expect you
18840520	18845560	guys to memorize this either. But what I will do is obviously walk through the code step by step,
18845560	18851640	and make sure you understand why it is that we have what we have here. Okay, so what we've done
18851640	18857640	is to find the vocabulary size, the max length of a review, and the batch size. Now what we've
18857640	18862440	done is just loaded in our data set by defining the vocabulary size. So this is just the words
18862440	18867080	it will include. So in this case, all of them, then we have trained data, trained labels, test
18867160	18871640	data, test labels. And we can look at a review and see what it looks like by doing something
18871640	18876120	like this. So this is an example of our first review, we can see kind of the different encodings
18876120	18880840	for all of these words. And this is what it looks like, they're already in integer form.
18880840	18885560	Now, just something to note here is that the length of our reviews are not unique. So if I do
18885560	18890840	the length of trained data, I guess I wouldn't say unique, but I mean, they're just all different.
18890840	18894280	So the length of trained data is zero is different than the length of trained data one,
18894280	18897880	right? So that's something to consider as we go through this and something we're actually going
18897880	18902680	to have to handle. Okay, so more pre processing. So this is what I was talking about. If you have
18902680	18906280	a look at our loaded interviews, we'll notice there are different lengths, this is an issue,
18906280	18910520	we cannot pass different length data into our neural network, which is true. Therefore, we
18910520	18914760	must make each review the same length. Okay, so what we're going to do for now is we're actually
18914760	18919800	going to pad our sequences. Now what that means is we're going to follow this kind of step that
18919880	18925880	I've talked about here. So if the review is greater than 250 words, we will trim off extra words,
18925880	18931640	if the review is less than 250 words, we'll add the necessary amount of this should actually be
18931640	18938360	zeros in here, let's fix this of zeros to make it equal to 250. So what that means is we're
18938360	18942040	essentially going to add some kind of padding to our review. So in this case, I believe we're
18942040	18946280	actually going to pad to the left side, which means that say we have a review of length, you know,
18946360	18951640	200, we're going to add 50, just kind of blank words, which will represent with the index zero
18951640	18957640	to the left side of the review to make it the necessary length. So that's, that's good, we'll
18957640	18962200	do that. So if we look at train data and test data, what this does is we're just going to use
18962200	18966680	something from Keras, which we've imported above. So we're saying from Keras dot pre processing
18966680	18971000	import sequence, again, we're treating our text data as a sequence, as we've talked about,
18971000	18975960	we're going to say sequence dot pad sequences, train data, and then we define the length that
18975960	18981080	we want to pad it to. So that's what this will do. It will perform these steps that we've already
18981080	18985560	talked about. And again, we're just going to assign test data and train data to, you know,
18985560	18989880	whatever this does for us, we can pass the entire thing, it'll pad all of them for us at once.
18990600	18996360	Okay, so let's run that. And then let's just have a look at say train data one now, because
18996360	19002200	remember, this was like 189, right? So if we look at train data, so train underscore data one,
19003000	19009000	like that, we can see that as an array with a bunch of zeros before, because that is the padding
19009000	19013960	that we've employed to make it the correct length. Okay, so that's padding, that's something that
19013960	19018440	we're probably going to have to do most of the time, when we feed something to our neural networks.
19018440	19021640	All right, so the next step is actually to create the model. Now this model is pretty
19021640	19027240	straightforward. We have an embedding layer and LSTM in a dense layer here. So the reason we've
19027240	19031560	done dense with the activation function of sigmoid at the end is because we're trying to
19031560	19036760	pretty much predict the sentiment of this, right? Which means that if we have the sentiment between
19036760	19042840	zero and one, then if a number is greater than 0.5, we could classify that as a positive review.
19042840	19046680	And if it's less than 0.5 or equal, you know, whatever you want to set the bounds at,
19046680	19051240	then we could say that's a negative review. So sigmoid, as we probably might recall,
19051240	19055640	squishes our values between zero and one. So whatever the value is at the end of the network
19055640	19059960	will be between zero and one, which means that, you know, we can make the accurate prediction.
19060760	19065160	Now here, the reason we have the embedding layer, like, well, we've already pre processed our review
19065160	19069560	is even though we've pre processed this with these integers, and they are a bit more meaningful than
19069560	19074040	just our random lookup table that we've talked about before, we still want to pass that to an
19074040	19079640	embedding layer, which is going to find a way more meaningful representation for those numbers
19079640	19083720	than just their integer values already. So it's going to create those vectors for us. And this
19083720	19089560	32 is denoting the fact that we're going to make the output of every single one of our embeddings
19089640	19095480	or vectors that are created 32 dimensions, which means that when we pass them to the LSTM layer,
19095480	19100200	we need to tell the LSTM layer, it's going to have 32 dimensions for every single word,
19100200	19104680	which is what we're doing. And this will implement that long short term memory process we talked
19104680	19112280	about before, and output the final output to TF dot cares dot layers dot dense, which will tell us,
19112280	19118520	you know, that's what this is, right? It'll make the prediction. So that's what this model is.
19119080	19123720	We can see, give us a second to run here, the model summary, which is already printed out,
19123720	19127560	we can look at the fact that the embedding layer actually has the most amount of parameters,
19127560	19131000	because essentially, it's trying to figure out, you know, all these different numbers,
19131000	19136120	how can we convert that into a tensor of 32 dimensions, which is not that easy to do. And
19136120	19140200	this is going to be the major aspect that's being trained. And then we have our LSTM layer,
19140200	19144920	we can see the parameters there. And our final dense layer, which is eight getting 33 parameters,
19144920	19150680	that's because the output from every single one of these dimensions 32 plus a bias node, right,
19150680	19154440	that we need. So that's what we'll get there. You can see model dot summary.
19155560	19160040	We get the sequential model. Okay, so training. Alright, so now it's time to compile and train
19160040	19164120	the model, you can see I've already trained mine. What I'm going to say here is if you want to speed
19164120	19167480	up your training, because this will actually take a second, and we'll talk about why we pick these
19167480	19175640	things in a minute is go to runtime, change runtime type, and add a hardware accelerator of GPU.
19176360	19180280	What this will allow you to do is utilize a GPU while you're training, which should speed up your
19180280	19185560	training by about 10 to 20 times. So I probably should have mentioned that beforehand. But you
19185560	19191720	can do that. And please do for these examples. So model dot compile. Alright, so we're compiling
19191720	19196280	our model, we're picking the loss function as binary cross entropy. The reason we're picking
19196280	19200920	this is because this is going to essentially tell us how far away we are from the correct
19201720	19206040	probability, right, because we have two different things we could be predicting. So you know, either
19206040	19212200	zero or one, so positive or negative. So this will give us a correct loss for that kind of
19212200	19216360	problem that we've talked about before. The optimizer, we're going to use rms prop. Again,
19216360	19219480	I'm not going to discuss all the different optimizers, you can look them up if you care
19219480	19224280	that much about what they do. And we're going to use metrics as ACC. One thing I will say is
19224280	19228920	the optimizer is not crazy important. For this one, you could use Adam if you wanted to, and it
19228920	19234040	would still work fine. My usual go to is just use the atom optimizer unless you think there's a better
19234040	19238520	one to use. But anyways, that's something to mention. Okay, so finally, we will fit the model,
19238520	19242440	we've looked at the syntax a lot before. So model that fit, we'll give the training data,
19242440	19247720	the training labels, the epochs, and we'll do a validation split of 20%. So that's what 0.2 stands
19247720	19253160	for, which means that what we're going to be doing is using 20% of the training data to actually
19253160	19257880	evaluate and validate the model as we go through. And we can see that after training, which I've
19257880	19261880	already done, and you guys are welcome to obviously do on your own computer, we kind of stall at an
19261880	19268440	evaluation accuracy of about 88%. Whereas the model actually gets overfit to about 97 98%.
19269320	19273400	So what this is telling us essentially is that we don't have enough training data, and that
19273400	19278120	after we've even done just one epoch, we're pretty much stuck on the same validation accuracy,
19278120	19281480	and that there's something that needs to change in the model to make it better. But for now,
19281480	19285720	that's fine, we'll leave it the way that it is. Okay, so now we can look at the results. I've
19285720	19290760	already did the results here, just to again, speed up some time, but we'll do the evaluation on our
19290760	19296440	test data and test labels to get a more accurate kind of result here. And that tells us we have
19296440	19302600	an accuracy of about 85.5%, which you know, isn't great, but it's decent considering that we didn't
19302600	19307240	really write that much code to get to the point that we're at right now. Okay, so that's what
19307240	19310920	we're getting. The model has been trained. Again, it's not too complicated. And now we're
19310920	19316040	on to making predictions. So the idea is that now we've trained our model, and we want to actually
19316040	19322280	use it to make a prediction on some kind of movie review. So since our data was pre processed, when
19322280	19327160	we gave it to the model, that means we actually need to process anything we want to make a prediction
19327160	19331720	on in the exact same way, we need to use the same lookup table, we need to encode it, you know,
19331720	19336280	precisely the same. Otherwise, when we give it to the model, it's going to think that the words
19336280	19340760	are different, and it's not going to make an accurate prediction. So what I've done here is
19340760	19347960	I've made a function that will encode any text into what do you call the proper pre processed
19347960	19353000	kind of integers, right, just like our training data was pre processed. That's what this function
19353000	19357000	is going to do for us is pre processed some line of text. So what I've done is actually
19357000	19365160	gotten the lookup table. So essentially, the mappings from IBM, I be IMDB, I could read that
19365160	19369800	properly. From that data set that we loaded earlier. So let me go see if I can find where I
19369800	19376360	defined IMDB, you can see up here. So keras dot data sets import IMDB, just like we loaded it in,
19376360	19381160	we can also actually get all of the word indexes or that map, we can actually print this out if
19381160	19385720	we want to look at what it is after. But anyways, we have that mapping, which means that all we need
19385720	19393480	to do is keras dot preprocessing dot text dot text, two word sequence, what this means is give
19393720	19399000	in some text convert all of that text into what we call tokens, which are just the individual
19399000	19403800	words themselves. And then what we're going to do is just use a kind of for loop inside of here
19403800	19410600	that says word index at word, if word in word index, L zero for word in tokens. Now what this
19410600	19418040	means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary
19418040	19424040	of 88,000 words, then what we'll do is replace its location in the list with that specific word,
19424760	19429560	or with that specific integer that represents it, otherwise we'll put zero just to stand for,
19429560	19434840	you know, we don't know what this character is. And then what we'll do is return sequence dot pad
19434840	19441160	sequences, and we'll pad this token sequence, and just return actually the first index here.
19441160	19446200	The reason we're doing that is because this pad sequences works on a list of sequences,
19446280	19450760	so multiple sequences. So we need to put this inside a list, which means that this is going
19450760	19455480	to return to us a list of lists. So we just obviously want the first entry, because we only
19455480	19460120	want, you know, that one sequence that we padded. So that's how this works. Sorry, that's a bit of
19460120	19463480	a mouthful to explain, but you guys can run through and print the stuff out if you want to see how
19463480	19467960	all of it works specifically. But yeah, so we can run this cell and have a look at what this
19467960	19472600	actually does for us on some sample text. So that maybe was just amazing. So amazing, we can see
19472600	19476840	we get the output that we were kind of expecting. So integer encoded words down here, and then a
19476840	19481560	bunch of zeros just for all the padding. Now, while we're at it, I decided why not we why don't we
19481560	19486360	make a decode function so that if we have any movie review like this, that's in the integer
19486360	19490840	form, we can decode that into the text value. So the way we're going to do that is start by
19490840	19495880	reversing the word index that we just created. Now the reason for that is the word index we
19495880	19501720	looked at, which is this right, goes from word to integer. But we actually now want to go from
19501720	19506520	integer to word so that we can actually translate a sentence, right? So what I've done is made this
19506520	19511880	decode integers function, we've set the padding key as zero, which means that if we see zero,
19511880	19516040	that's really just means you know, nothing's there. We're going to create a text string,
19516040	19521000	which we're going to add to. And I'm just gonna say for num in integers, integers is our input,
19521000	19525240	which will be a list that looks something like this or an array, whatever you want to call it,
19525240	19529480	we're gonna say if number does not equal pad. So essentially, if the number is not zero, right,
19529480	19534840	it's not padding, then what we'll do is add the lookup of reverse word index num. So whatever
19534840	19541080	that number is, into this new string plus a space, and then just return text colon negative one,
19541080	19545000	which means return everything except the last space that we would have added. And then if I
19545000	19552280	print the decode integers, we can see that this encoded thing that we have before, which looks
19552280	19557880	like this gets encoded by the string that movie was just amazing. So maybe sorry, not encoded,
19557880	19563000	decoded, because this was the encoded form. So that's how that works. Okay, so now it's
19563000	19567400	time to actually make a prediction. So I've written a function here that will make a prediction on
19567400	19572200	some piece of text as the movie review for us. And I'll just walk us through quickly how this
19572200	19576360	works. And then I'll show us the actual output from our model, you know, making predictions like
19576360	19581640	this. So what we say is we'll take some parameter text, which will be our movie review. And we're
19581640	19586840	going to encode that text using the encode text function we've created above. So just this one
19586840	19592120	right here that essentially takes our sequence of, you know, words, we get the pre processing,
19592120	19596360	so turn that into a sequence, remove all the spaces, whatnot, you know, get the words,
19596360	19602440	then we turn those into the integers, we have that we return that. So here we have our proper
19602440	19609960	pre processed text. Then what we do is we create a blank NumPy array that is just a bunch of zeros
19610600	19615320	that's in the form one to 50 or in that shape. Now the reason I'm putting in that in that shape
19615320	19622440	is because the shape that our model expects is something 250, which means some number of entries,
19622440	19628040	and then 250 integers representing each word, right? Because that's the length of movie review
19628680	19632760	is what we've told the model is the length 250. So that's the length of the review.
19633400	19638920	Then what we do is we put pred zero. So that's what's up here, equals the encoded text. So we
19638920	19646520	just essentially insert our one entry into this, this array we've created. Then what we do is say
19646520	19653320	modeled up predict on that array, and just return and print the result zero. Now, that's pretty
19653320	19657480	much all there is to it. I mean, that's how it works. The reason we're doing result zero is
19657480	19663080	because again, model is optimized to predict on multiple things, which means like I would have
19663080	19668280	to do, you know, list of encoded text, which is kind of what I've done by just doing this
19668280	19673720	prediction lines here, which means it's going to return to me an array of arrays. So if I want
19673720	19678120	the first prediction, I need to index zero, because that will give me the prediction for
19678120	19683240	our first and only entry. Alright, so I hope that makes sense. Now we have a positive review I've
19683240	19687080	written and a negative review, and we're just going to compare the analysis on both of them.
19687080	19690360	So that movie was so awesome. I really loved it and would watch it again, because it was
19690360	19694040	amazingly great. And then that movie sucked, I hated it and wouldn't watch it again, was one
19694120	19698280	of the worst things I've ever watched. So let's look at this now. And we can see the first one
19698280	19704440	gets predicted at 72% positive, whereas the other one is 23% positive. So essentially what that
19704440	19708280	means is that, you know, if the lower the number, the more negative we're predicting it is, the
19708280	19712600	higher the number, the more positive we're predicting it is. If we wanted to not just print
19712600	19717720	out this value, and instead what we wanted to do was print out, you know, positive or negative,
19717720	19722600	we could just make a little if statement that says if this number is greater than 0.5, say positive,
19722600	19728200	otherwise say not say negative, right? And I just want to show you that changing these reviews
19728200	19733240	ever so slightly actually makes a big difference. So if I remove the word awesome, so that movie
19733240	19738680	was so and then I run this, you can see that Oh, wow, this actually increases and goes up to 84%.
19739640	19744120	So the presence of certain words in certain locations actually makes a big difference. And
19744120	19749720	especially when we have a shorter length review, right, if we have a longer length review, it
19749800	19753720	won't make that big of a difference. But even the removal of a few words here. And let's see,
19753720	19759560	so the removing the word awesome changed it by almost like 10%. Right. Now if I move, so let's
19759560	19763640	see if that makes a bigger difference. It makes a very little difference because it's learned,
19763640	19768120	at least the model, right, that the word so doesn't really make a huge impact into
19768120	19773080	the type of review. Whereas if I remove the word I, let's see if that makes a big impact,
19773080	19777320	probably not right now, it goes back up to 84. So that's cool. And that's something to play with
19777320	19781960	is removing certain words and seeing how much impact those actually carry. And even if I just
19781960	19785800	add the word great, like would great to watch it again, just in the middle of the sentence,
19785800	19789720	doesn't have to make any sense. Let's look at this here. Oh, boom, we increase like a little
19789720	19794520	bit, right. And let's say if I add this movie, you really suck. Let's see if that makes a difference.
19795320	19799720	No, that just reduces it like a tiny bit. So something cool, something to play with.
19799720	19804280	Anyways, now let's move on to the next example. So now we're on to our last and final example,
19804360	19809400	which is going to be creating a recurrent neural network play generator. Now, this is going to
19809400	19813320	be the first kind of neural network we've done, that's actually going to be creating something
19813320	19818680	for us. But essentially, what we're going to do is make a model that's capable of predicting the
19818680	19823800	next character in a sequence. So we're going to give it some sequence as an input. And what it's
19823800	19828200	going to do is just simply predict the most likely next character. Now, there's quite a bit
19828200	19831880	that's going to go into this. But the way we're going to use this to predict a play is we're
19831880	19837400	going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then
19837400	19841800	we're going to have it. So that we'll ask the model will give it some starting prompt, some
19841800	19846840	string to start with. And that'll be the first thing we pass to it, it will predict to us what
19846840	19852120	the most likely next character for that sequence is. And we'll take the output from the model
19852120	19857400	and feed it as the input again to the model and keep predicting sequence of characters. So
19857400	19862280	keep predicting the next character from the previous output as many times as we want to
19862280	19866760	generate an entire play. So we're going to have this neural network that's capable of predicting
19866760	19872920	one letter at a time actually end up generating an entire play for us by running it multiple
19872920	19878280	times on the previous output from the last iteration. Now, that's kind of the problem.
19878280	19881640	That's what we're trying to solve. So let's go ahead and get into it and talk about what's
19881640	19885800	involved in doing this. So the first thing we're going to do obviously is our imports. So from
19885800	19893080	Keras dot preprocessing import sequence, import Keras, we need TensorFlow NumPy and OS. So we'll
19893080	19898040	load that in. And now what we're going to do is download the file. So the data set for Romeo and
19898040	19903720	Juliet, which we can get by using this line here. So Keras has this utils thing, which will allow
19903720	19908920	us to get a file, save it as whatever we want. In this case, we're going to save it as Shakespeare
19908920	19913960	dot txt. And we're going to get that from this link. Now, I believe this is just some like shared
19913960	19919000	drive that we have access to from Keras. So we'll load that in here. And then this will
19919000	19924520	simply give us the path on this machine. Because remember, this is Google Collaboratory to this
19924520	19929480	text file. Now, if you want, you can actually load in your own text data. So we don't necessarily
19929480	19933720	need to use the Shakespeare play, we could use anything we want. In fact, an example that I'll
19933720	19938920	show later is using the B movie script. But the way you do that is run this block of code here.
19939880	19944280	And you'll see that it pops up this thing for choose files, just choose a file from your
19945800	19951400	local computer. And then what that will do is just save this on Google Collaboratory. And
19951400	19955400	then that will allow you to actually use that. So make sure that's a text file that you're loading
19955400	19961000	in there. But regardless, that should work. And then from there, you'll be good to go. So if you,
19961000	19964440	you know, you don't need to do that, you can just run this block of code here, if you want to load
19964600	19969960	in the Shakespeare txt, but otherwise, you can load in your own file. Now, after we do that,
19969960	19974120	what we want to do is actually open this file. So remember, that was just saving the path to it.
19974120	19979240	So we'll open that file in RB mode, which is read bytes mode, I believe. And then we're going to
19979240	19984200	say dot read, so we're going to read that in as an entire string, we're going to decode that into
19984200	19989160	utf a format. And then we're just printing the length of the text or the amount of characters
19989240	19994200	in the text. So if we do that, we can see we have the length of the text is 1.1 million
19994200	19998920	characters, approximately. And then we can have a look at the first 250 characters by doing this.
19999720	20003800	So we can see that this is kind of what the plate looks like, we have whoever's speaking,
20003800	20009640	colon, then some line, whoever's speaking, colon, some line, and there's all these break lines.
20009640	20014040	So backslash ends, which are telling us, you know, go to the next line, right? So it's going
20014040	20017880	to be important because we're going to hope that our neural network will be able to predict
20017880	20023240	things like break lines and spaces, and even this kind of format as we teach it more and get
20023240	20029400	further in. But now it's time to talk about encoding. So obviously, all of this text is in
20029400	20034520	text form, it's not pre processed for us, which means we need to pre process it and encode it
20034520	20038920	as integers before we can move forward. Now, fortunately, for us, this problem is actually
20038920	20043480	a little bit easier than the problem we discussed earlier with encoding words, because what we're
20043480	20048440	going to do is simply encode each character in the text with an integer. Now, you can imagine
20048440	20053720	why this makes this easier, because there really is a finite set of characters, whereas there's
20053720	20059080	kind of indefinite or, you know, I guess, infinite amount of words that could be created. So we're
20059080	20064360	not really going to run into the problem where, you know, two words are encoded with such different
20064360	20068680	or two characters are encoded with such different integers, that it makes it difficult for the model
20068680	20074120	to understand. Because I mean, and we can look at what the value of vocab is here, we're only
20074120	20078440	going to have so many characters in the text. And for characters, it just doesn't matter as much,
20078440	20083560	because you know, an R isn't like super meaningful compared to an A. So we can kind of encode in a
20083560	20087400	simple format, which is what we're going to do. So essentially, we need to figure out how many
20087400	20092680	unique characters are in our vocabulary. So to do that, we're going to say vocab equals sorted,
20092680	20097800	set text, this will sort all of the unique characters in the text. And then what we're
20097800	20102840	going to do is create a mapping from unique characters to indices indices. So essentially,
20102840	20108360	we're going to say UI, for IU in a new, a numerator vocabulary, what this will do is give us
20109240	20114680	essentially zero, whatever the string is, one, whatever the string is, two, whatever the string
20114680	20119480	is for every single letter or character in our vocabulary, which will allow us to create this
20119480	20126680	mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array,
20126760	20131960	so that we can just use the index at which a letter appears as the reverse mapping. So going
20131960	20136360	from index to letter, rather than lettered index, which is what this one's doing here.
20137000	20142680	Next, I've just written a function that takes some text and converts that to an int or the int
20142680	20147400	representation for it, just to make a little bit easier for us as we get later on in the tutorial.
20147400	20152360	So we're just going to say NP dot array of in this case, and we're just going to convert every single
20152440	20158600	character in our text into its integer representation by just referencing that character and putting
20158600	20163080	that in a list here, and then obviously converting that to NumPy array. So then if we wanted to have
20163080	20169480	a look at how this works, we can say text as int equals text to int text. So remember text is
20169480	20174520	that entire loaded file that we had above here. So we're just going to convert that to its integer
20174520	20180200	representation entirely using this function. And now we can look at how this works down here. So
20180200	20188520	we can see that the text for citizen, which is the first 13 letters is encoded by 1847 5657 581.
20188520	20192600	And obviously each character has its own encoding, and you can go through and kind of figure out what
20192600	20198040	they are based on the ones that are repeated, right? So that is how that works. Now I figured
20198040	20201400	while we were at it, we might as well write a function that goes the other way. So into text.
20202360	20207480	Reason I'm trying to convert this to a NumPy array first is just because we're going to be passing
20207480	20212120	in different objects potentially in here. So if it's not already a NumPy array, it needs to be
20212120	20217720	a NumPy array, which is kind of what this is doing. Otherwise, we're just going to pass on that we
20217720	20222840	don't need to convert it to NumPy array, if it already is one, we can just join all of the characters
20222840	20229000	from this list into here. So that's essentially what this is doing for us. It's just joining
20229000	20235800	into text. And then we can see if we go into text, text is in colon 13, that translates that back to
20235800	20239640	us for citizen, I mean, you can look more into this function if you want, but it's not that
20239640	20245000	complicated. Okay, so now that we have all this text encoded as integers, what we need to do is
20245000	20250600	create some training examples. It's not really feasible to just pass the entire you know, 1.1
20250600	20255640	million characters to our model at once for training, we need to split that up into something
20255640	20260360	that's meaningful. So what we're actually going to be doing is creating training examples where we
20260360	20267640	have the first, where the training input, right, so the input value is going to be some sequence
20267640	20272520	of some length, we'll pick the sequence length, in this case, we're actually going to pick 100.
20272520	20277800	And then the output or the expected output, so I guess like the label for that training example
20277800	20283480	is going to be the exact same sequence shifted right by one character. So essentially, I put a
20283480	20288600	good example here, our input will be something like hell, right? Now our output will be E L L O.
20288600	20293240	So what it's going to do is predict this last character, essentially. And these are what our
20293240	20298120	training examples are going to look like. So the entire beginning sequence, and then the output
20298120	20303640	sequence should be that beginning sequence minus the first letter, but tack on what the last letter
20303640	20308760	should be. So that this way, we can look at some input sequence and then predict that output sequence
20308760	20314120	that you know, plus a character, right? Okay, so that's how that works. So now we're going to do is
20314120	20318840	define a sequence length of 100. We're going to say the amount of examples per epoch is going to be
20318840	20323800	the length of the text divided by the sequence length plus one. The reason we're doing this is
20323800	20329480	because for every training example, we need to create a sequence input that's 100 characters long,
20329480	20334280	and we need to create a sequence output that's 100 characters long, which means that we need to have
20334280	20340760	101 characters that we use for every training example, right? Hopefully that would make sense.
20340840	20348440	So what this next line here is going to do is convert our entire string data set into characters.
20348440	20352680	And it's actually going to allow us to have a stream of characters, which means that it's
20352680	20359160	going to essentially contain, you know, 1.1 million characters inside of this TF dot data set
20359880	20364760	object from tensor slices. That's what that's doing. Next, so let's run this and make sure this
20364760	20370920	works. All right, what we're going to do is say sequences is equal to char data set dot batch
20370920	20376680	sequence length is the length of each batch. So in this case, 101 and then drop remainder means
20376680	20383720	let's say that we have, you know, 105 characters in our text, well, since we need sequences of
20383720	20389240	length 101, we'll just drop the last four characters of our text, because we can't even put those into
20389240	20394040	a batch. So that's what this is doing for us is going to take our entire character data set here
20394120	20399160	that we've created and batch it into length of 101, and then just drop the remainder. So that's
20399160	20406040	what we're going to do here. So sequences does now split input target. What this is going to do
20406040	20411400	essentially is just create those training examples that we needed. So taking this, these sequences
20411400	20416920	of 101 length and converting them into the input and target text, and I'll show you how they work
20416920	20423480	in a second, we can do this convert the sequences to that by just mapping them to this function.
20423480	20428280	So that's what this function does. So if we say sequences dot map, and we put this function here,
20428280	20433640	that means every single sequence will have this operation applied to it. And that will be stored
20433640	20438840	inside this data set object. Or I guess you'd say object, but we'll also just say that's it's
20438840	20443400	going to be, you know, the variable, right? So if we want to look at an example of how this works,
20444120	20448840	we can kind of see. So it just says example, the input will be first citizen, before we proceed
20448840	20454040	any further here, me speak, all speak, speak, first citizen, you and the output notice the first
20454040	20460120	character is gone, starts at I. And the last character is actually just a space here. Whereas
20460120	20463960	here, it didn't have a space, or you can see there's no space. Here, there is a space. That's
20463960	20469000	kind of what I'm trying to highlight for you. The next example, we get our all resolved rather
20469000	20472520	to die rather than famine, whatever it goes to here, right? And then you can see here, we omit
20472600	20478600	that a and the next letter is actually a K, right? That's added in there. So that's how that
20478600	20483800	works. Okay, so next, we need to make training batches. So we're going to say the batch size
20483800	20489480	equals 64. The vocabulary size is the length of the vocabulary, which if you remember all the way
20489480	20495160	back up to the top of the code, was the set or the sorted set of the text, which essentially
20495160	20500680	told us how many unique characters are in there. The embedding dimension is 256. The RNN units
20500760	20506680	is 1024. And the buffered size is 10,000. What we're going to do now is create a data set that
20506680	20511480	shuffled, we're going to switch around all these sequences, so they don't get shown in the proper
20511480	20516440	order, which we actually don't want. And then we're going to batch them by the batch size. So
20516440	20520520	if we haven't kind of gone over what batching and all this does before, I mean, you can read
20520520	20524360	these comments, this is straight from the TensorFlow documentation, what we want to do is
20524360	20529960	feed our model 64 batches of data at a time. So what we're going to do is shuffle all of the data,
20530520	20534440	batch it into that size, and then again, drop the remainder, if there's not enough batches,
20534440	20538920	which is what we'll do. We're going to define the embedding dimension, which is essentially
20539480	20544520	how big we want every single vector to represent our words are in the embedding layer. And then
20544520	20549960	the RNN units, I won't really discuss what that is right now. But that's essentially how many
20551480	20555000	it's hard to really just, I'm just going to omit describing at for right now, because I don't want
20555000	20561320	to butcher an explanation. It's not that important. Anyways, okay, so now we're going to go down to
20561320	20565560	building the model. So we've kind of set these parameters up here. Remember what those are,
20565560	20569640	we've batched and we've shuffled the data set. And again, that's how this works. You can print
20569640	20574680	it out if you want to see what a batch actually looks like. But essentially, it's just 64 entries
20574680	20579560	of those sequences, right? So 64 different training examples is what a batch that is.
20580360	20585880	All right. So now we go down here, we're going to say build model, we're actually making a function
20585880	20590520	that's going to return to us a built model. The reason for this is because
20591480	20596840	right now, we're going to pass the model batches of size 64 for training, right? But what we're
20596840	20602280	going to do later is save this model. And then we're going to patch pass it batches of one pieces
20602280	20607400	of, you know, training whatever data, so that it can actually make a prediction on just one
20608120	20612440	piece of data. Because for right now, what it's going to do is takes a batch size of 64, it's
20612440	20617800	going to take 64 training examples, and return to us 64 outputs. That's what this model is going
20617800	20623720	to be built to do the way we build it now to start. But later on, we're going to rebuild the model
20623720	20628120	using the same parameters that we've saved and trained for the model, but change it to just be
20628120	20633640	a batch size of one, so that that way we can get one prediction for one input sequence, right?
20634280	20638600	So that's why I'm creating this build model function. Now in here, it's going to have the
20638600	20644440	vocabulary sizes, first argument, the embedding dimension, which remember was 256 as a second
20644440	20649640	argument, but also these are the parameters up here, right? And then we're going to find the batch
20649640	20655880	size as you know, batch size, none, what this none means is we don't know how long the sequences
20655880	20661880	are going to be in each batch. All we know is that we're going to have 64 entries in each batch.
20661880	20667400	And then of those 64 entries, so training examples, right, we don't know how long each one
20667400	20671320	will be. Although in our case, we're going to use ones that are length 100. But when we actually
20671320	20675720	use the model to make predictions, we don't know how long the sequence is going to be that we input
20675720	20681000	so we leave this none. Next, we'll make an LSTM layer, which is a long short term memory RNN
20681000	20685800	units, which is 1024, which again, I don't really want to explain, but you can look up if you want
20685880	20694760	return sequences means return the intermediate stage at every step. The reason we're doing this
20695800	20701560	is because we want to look at what the model seeing at the intermediate steps and not just
20701560	20706680	the final stage. So if you leave this as false, and you don't set this to true, what happens is
20706680	20713480	this LSTM just returns one output that tells us what the model kind of found at the very last
20713480	20718600	time step. But we actually want the output at every single time step for this specific model.
20718600	20723880	And that's why we're setting this true, stateful, not going to talk about that one right now,
20723880	20727240	that's something you can look up if you want. And then recurrent initializer is just what
20727240	20731960	these values are going to start at in the LSTM. We're just picking this because this is what
20731960	20737400	TensorFlow has kind of said is a good default to pick. I won't go into more depth about that
20737400	20742680	again, things that you can look up more if you want. Finally, we have a dense layer, which is
20742680	20749000	going to contain the amount of vocabulary size nodes. The reason we're doing this is because we
20749000	20754200	want the final layer to have the amount of nodes in it equal to the amount of characters in the
20754200	20759800	vocabulary. This way, every single one of those nodes can represent a probability distribution
20759800	20766200	that that character comes next. So all of those nodes value some sum together should give us the
20766200	20771960	value of one. And that's going to allow us to look at that last layer as a predictive layer,
20771960	20776120	where it's telling us the probability that these characters come next, and we've discussed how
20776120	20782360	that's worked previously with other neural networks. So let's run this now. Name embedding
20782360	20787800	dim is not defined, which I mean, believes I have not ran this yet. So now we run that,
20787800	20792120	and we should be good. So if we look at the model summary, we can see we have our initial
20792120	20797400	embedding layer, we have our LSTM, and then we have our dense layer at the end. Now notice
20797400	20803160	64 is the batch size, right? That's the initial shape. None is the length of the sequence,
20803160	20809800	which we don't know. And then this is going to be just the output dimension or sorry, this is
20810440	20816120	the amount of values in the vector, right? So we're going to start with 256. We'll just do
20816120	20821160	1,024 units in the LSTM and then 65 stands for the amount of nodes, because that is the
20821160	20826440	length of the vocabulary. Alright, so combined, that's how many trainable parameters we get.
20827080	20831400	You can see each of them for each layer. And now it's time to move on to the next section.
20831400	20835320	Okay, so now we're moving on to the next step of the tutorial, which is creating a loss function
20835320	20839560	to compile our model with. Now, I'll talk about why we need to do this in a second,
20839560	20845560	but I first want to explore the output shape of our model. So remember, the input to our model
20845560	20851480	is something that is of length 64, because we're going to have batches of 64 training examples,
20851480	20857240	right? So every time we feed our model, we're going to give it 64 training examples. Now what
20857240	20862920	those training examples are, are sequences of length 100. That's what I want you to remember.
20862920	20870200	We're passing 64 entries that are all of length 100 into the model as its training data, right?
20870840	20875400	But sometimes, and when we make predictions with the model later on, we'll be passing it
20875400	20881400	just one entry that is of some variable length, right? And that's why we've created
20882120	20887400	this build model function, so that we can build this model using the parameters that we've saved
20887400	20894520	later on, once we train the model, and it can expect a different input shape, right? Because
20894520	20897560	when we're training it, it's going to be given a different shape, and we're actually testing with
20897560	20903800	it. Now what I want to do is explore the output of this model, though, at the current point in time.
20903800	20910600	So we've created a model that accepts a batch of 64 training examples that are length 100. So
20910600	20915640	let's just look at what the output is from the final layer. Give this a second to run.
20916440	20924360	We get 64, 165. And that represents the batch size, the sequence length, and the vocabulary
20924360	20929320	size. Now the reason for this is we have to remember that when we create a dense layer as our last
20929320	20936600	layer that has 65 nodes, every prediction is going to contain 65 numbers. And that's going
20936600	20942440	to be the probability of every one of those characters occurring, right? That's what that
20942440	20947480	does at the last one for us. So obviously, our last dimension is going to be 65 for the vocabulary
20947480	20951480	size. This is a sequence length, and that's a batch, I just want to make sure this is really
20951480	20954840	clear before we keep going. Otherwise, it's going to get very confusing very quickly.
20955480	20961160	So what I want to do now is actually look at the length of the example batch predictions,
20961160	20965000	and just print them out and look at what they actually are. So example batch predictions
20965000	20971560	is what happens when I use my model on some random input example, actually, well, the first one
20971560	20976840	from my data set with when it's not trained. So I can actually use my model before it's
20976840	20983000	trained with random weights and random biases and parameters, by simply using model, and then I can
20983000	20987480	put the little brackets like this and just pass in some example that I want to get a prediction
20987480	20991480	for. So that's what I'm going to do, I'm going to give it the first batch, and it can even it shows
20991480	20996200	me the shape of this batch 64 100, I'm going to pass that to the model, and it's going to give us a
20996200	21001960	prediction for that. And in fact, it's actually going to give us a prediction for every single
21001960	21006040	element in the batch, right, every single training example in the batch, it's going to give us a
21006040	21011640	prediction for. So let's look at what those predictions are. So this is what we get, we get
21011720	21019240	a length 64 tensor, right? And then inside of here, we get a list inside of a list or an array
21019240	21024840	inside of an array with all of these different predictions. So we'll stop there for this, like
21024840	21029000	explaining this aspect here. But you can see we're getting 64 different predictions because
21029000	21035080	there's 64 elements in the batch. Now, let's look at one prediction. So let's look at the very first
21035080	21041560	prediction for say the first element in the batch, right? So let's do that here. And we see now that
21041560	21048600	we get a length 100 tensor. And that this is what it looks like, there's still another layer inside.
21048600	21053240	And in fact, we can see that there's another nested layer here, right, another nested array
21053240	21060360	inside of this array. So the reason for this is because at every single time step, which means
21060360	21063640	the length of the sequence, right, because remember, our recurrent neural network is going to feed
21063640	21068840	one at a time every word in the sequence. In this case, our sequences are like the 100 at every
21068840	21075560	time step, we're actually saving that output as a, as a prediction, right? And we're passing
21075560	21081320	that back. So we can see that for one batch, one training, sorry, not one batch, one training
21081320	21086120	example, we get 100 outputs. And these outputs are in some shape, we'll talk about what those are
21086120	21091080	in a second. So that's something to remember that for every single training example, we get
21091080	21095720	whatever the length of that training example was outputs, because that's the way that this
21095720	21101560	model works. And then finally, we look at the prediction at just the very first time step. So
21101560	21106600	this is 100 different time steps. So let's look at the first time step and see what that prediction
21106600	21113240	is. And we can see that now we get a tensor of length 65. And this is telling us the probability
21113240	21119080	of every single character occurring next at the first time step. So that's what I wanted to walk
21119080	21123960	through is showing you what's actually outputted from the model, the current way that it works.
21124040	21130920	And that's why we need to actually make our own loss function to be able to determine how, you
21130920	21135640	know, good our models performing, when it outputs something ridiculous that looks like this, because
21135640	21140760	there is no just built in loss function and tensor flow that can look at a three dimensional
21140760	21146440	nested array of probabilities over, you know, the vocabulary size, and tell us how different the
21146440	21151720	two things are. So we need to make our own loss function. So if we want to determine the predicted
21151720	21159560	character from this array, so we'll go there now. What we can do is get the categorical, what's
21159560	21165800	this called, we can sample the categorical distribution. And that will tell us the predicted
21165800	21170280	character. So what I mean is, let's just look at this. And then we'll explain this. So since our
21170280	21174520	model works on random weights and biases right now, we haven't trained yet. This is actually
21174520	21179720	all of the predicted characters that it had. So at every time step, at the first time step,
21179800	21185320	it predicted h, then it predicted hyphen, then h, then g, then you, and so on so forth, you get
21185320	21192760	the point, right? So what we're doing to get this value is we're going to sample the prediction. So
21192760	21197960	at this, this is just the first time step, actually, we're sampled the prediction. Actually, no,
21197960	21202680	sorry, we're sampling every time stamp, my bad there. We're going to say sampled indices equals
21202680	21207000	NP dot reshapes, we're just reshaping this just changing the shape of it. We're going to say
21207080	21214200	predicted characters equals int to text sampled indices. So it's, I really, it's hard to explain
21214200	21218280	all this if you guys don't have a statistics kind of background a little bit to talk about why we're
21218280	21224120	sampling and not just taking the argument max value of like this array, because you would think
21224120	21227560	that what we'll do is just take the one that has the highest probability out of here, and that will
21227560	21232600	be the index of the next predicted character. There's some issues with doing that for the loss
21232680	21237960	function. Just because if we do that, then what that means is we're going to kind of get stuck in
21239000	21243080	an infinite loop almost where we just keep accepting the biggest character. So what we'll
21243080	21250040	do is pick a character based on this probability distribution, kind of, yeah, again, it's hard.
21250040	21253960	It's called sampling the distribution. You can look that up if you don't know what that means,
21253960	21258680	but sampling is just like trying to pick a character based on a probability distribution.
21258680	21263080	It doesn't guarantee that the character with the highest probability is going to be picked. It
21263080	21268520	just uses those probabilities to pick it. I hope that makes sense. I know that was like a really
21268520	21273880	rambly definition, but that's the best I can do. So here, we reshape the array and convert all the
21273880	21277880	integers to numbers to see the actual characters. So that's what these two lines are doing here.
21277880	21282280	And then I'm just showing the predicted characters by showing you this. And you know,
21282280	21287320	the character here is what was predicted at time step zero to be the next character and so on.
21288120	21293320	Okay, so now we can create a loss function that actually handles this for us. So this is the
21293320	21299160	loss function that we have. Keras has like a built in one that we can utilize, which is what
21299160	21304040	we're doing. But what this is going to do is take all of the labels and all of the probability
21304040	21308680	distributions, which is what this is, logits, I'm not going to talk about that really. And we'll
21308680	21314280	compute a loss on those. So how different or how similar those two things are. Remember the goal
21314280	21318920	of our algorithm in the neural network is to reduce the loss, right? Okay, so next,
21318920	21322040	we're going to compile the model, which we'll do here. So we're going to compile the model with
21322040	21328040	the atom optimizer and the loss function as loss, which we defined here. And now we're going to
21328040	21331320	set up some checkpoints. I'm not going to talk about how these work, you can kind of just read
21331320	21338280	through this if you want. And then we're going to train the model. Remember to start your GPU
21338280	21344600	hardware accelerator under runtime, change runtime type GPU, because if you do not, then this is
21344600	21349960	going to be very slow. But once you do that, you can train the model. I've already trained it. But
21349960	21355000	if we go through this training, we can see it's going to say train for 172 steps. It's going to
21355000	21359160	take about, you know, 30 seconds per epoch, probably maybe a little bit less than that. And
21359160	21363720	the more epochs you run this for, the better it will get. This is a different we're not likely
21363720	21369560	going to overfit here. So we can run this for like, say 100 epochs, if we wanted to. For our case,
21369560	21374440	let's actually start by just training this on, let's say two epochs, just to see how it does.
21374440	21380280	And then we'll train it on like 1020, 4050 and compare the results. But you'll notice the more
21380280	21383960	epochs, the better it's going to get. But just like for our case, we'll start with two, and then
21383960	21389400	we'll work our way up. So while that trains, we'll actually explain the next aspect of this
21389480	21394520	without running the code. So essentially, what we need to do, after we've trained the model,
21394520	21401640	we've initially the weights and biases, if we need to rebuild it using a new batch size of one.
21401640	21407800	So remember, the initial batch size was 64, which means that we'd have to pass it 64 inputs or
21407800	21412760	sequences for it to work properly. But now what I've done is I'm going to rebuild the model and
21412760	21417560	change it to a batch size of one, so that we can just pass it some sequence of whatever length we
21417560	21423080	want. And it will work. So if we run this, we've rebuilt the model with batch size one, that's
21423080	21428520	the only thing we've changed. And now what I can do is load the weights by saying model dot load
21428520	21435320	weights tf dot train dot latest checkpoint checkpoint directory, and then build the model
21436680	21443720	using the tensor shape one none. I know sounds strange. This is how we do this rebuild the model.
21443720	21448200	One none is just saying expect the input one and then none means we don't know what the next
21448200	21455640	dimension length will be. But here, checkpoint directory is just we've defined where on our
21455640	21460600	computer, we're going to save these TensorFlow checkpoints. This is just saying this is the
21461400	21465000	was it the prefix we're going to save the checkpoint with. So we're going to do the
21465000	21470040	checkpoint directory. And then checkpoint epoch where epoch will stand for obviously,
21470680	21475160	whatever epoch we're on. So we'll save checkpoint here, we'll save a checkpoint at epoch one,
21475160	21480440	a checkpoint at epoch two, to get the latest checkpoint, we do this. And then if we wanted
21480440	21485720	to load any intermediate checkpoint, say like checkpoint 10, which is what I've defined here,
21485720	21490280	we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading
21490280	21494680	by saying tf dot train dot load checkpoint, whereas this one just gets the most recent. So
21494680	21498280	we'll get the most recent, which should be checkpoint two for me. And then what we're going
21498280	21503640	to do is generate the text. So this function, I'll dig into it in a second, but I just want
21503640	21507160	to run and show you how this works, because I feel like we've done a lot of work for not
21507160	21511880	very many results right now. And I'm just going to type in the string Romeo, and just show you
21511880	21516920	that when I do this, we give it a second. And it will actually generate an output sequence
21516920	21523240	like this. So we have Romeo us give this is the beginning of our sequence that says lady
21523240	21530040	Capulet food, Marathon father, gnomes come to those shall, right? So it's like pseudo English,
21530040	21534360	most of it are like kind of proper words. But again, this is because we trained it on just
21534360	21540120	two epochs. So I'll talk about how we build this in a second. But if you wanted a better output
21540120	21544840	for this part, then you would train this on more epochs. So now let's talk about how I actually
21544840	21550360	generated that output. So we rebuilt the model to accept a batch size of one, which means that I
21550360	21555320	can pass it a sequence of any length. And in fact, what I start by doing is passing the
21555320	21561480	sequence that I've typed in here, which was Romeo, then what that does is we run this function
21561480	21565320	generate text, I just stole this from TensorFlow's website, like I've stolen almost all of this
21565320	21571800	code. And then we say the number of characters to generate is 800, the input evaluation, which
21571800	21577400	is now like we need to pre process this text again, so that this works properly, we could
21577400	21582040	use my little function, or we can just write this line of code here, which does with the function
21582040	21587880	that I wrote does for us. So char to IDX S for S and start string, start string is what we typed
21587880	21593080	in in that case, Romeo, then what we're going to do is expand the dimensions. So essentially turn
21593640	21600200	just a list like this that has all these numbers, nine, eight, seven into a double list like this,
21600200	21604520	or just a nested list, because that's what it's expecting as the input one batch, one entry.
21605160	21609240	Then what we do is we're going to say the string that we want to store, because we want to print
21609240	21615320	this out at the end, right, we'll put in this text generated list, temperature equals 1.0.
21615320	21620040	What this will allow us to do is if we change this value to be higher, I mean, you can read
21620040	21624120	the comment here, right, low temperature results in more predictable text, higher temperature results
21624120	21627720	in more surprising text. So this is just a parameter to mess with if you want, you don't
21627720	21632760	necessarily need it. And I would like, I've just left mine at one for now, we're going to start
21632760	21637800	by resetting the states of the model. This is because when we rebuild the model, it's going to
21637800	21643400	have stored the last state that it remembered when it was training. So we need to clear that
21643400	21648520	before we pass new input text to it. And we say for I and range num generate, which means however
21648520	21653080	many characters we want to generate, which is 800 here, what we're going to do is say predictions
21653080	21658680	equals model input a vowel, that's going to start as the start string that's encoded, right.
21659480	21664440	And then what we're going to do is say predictions equals TF dot squeeze prediction zero. What this
21664440	21669320	does is take our predictions, which is going to be in a nested list, and just removes that
21669320	21674040	exterior dimension. So we just have the predictions that we want, we don't have that extra dimension
21674040	21678040	that we need to index again. And then we're going to say using a categorical distribution to predict
21678040	21682440	the character returned by the model, that's what it writes here. We'll divide by the temperature,
21682440	21688200	if it's one, that's not going to do anything. We'll say predicted ID equals we'll sample
21688200	21693560	whatever the output was from the model, which is what this is doing. And then we're going to
21693560	21700040	take that output. So the predicted ID, and we are going to add that to the input evaluation.
21700840	21705080	And then what we're going to say is text generated dot append, and we're going to convert the text
21705080	21712360	that are integers now, back into a string, and return all of this. Now I know this seems like
21712360	21717160	a lot. Again, this is just given to us by TensorFlow to, you know, create this aspect,
21717160	21720760	you can read through the comments yourself, if you want to understand it more. But I think that
21720760	21725640	was a decent explanation of what this is doing. So yeah, that is how we can generate, you know,
21725640	21731960	sequences using recurrent neural network. Now what I'm going to do is go to my other window
21731960	21736120	here where I've actually typed all of the code, just in full and do a quick summary of everything
21736120	21739480	that we've done, just because there was a lot that went on. And then from there, I'm actually
21739480	21743960	going to train this on a B movie script and show you kind of how that works in comparison to the
21743960	21749160	Romeo and Juliet. Okay, so what I'm in now is just the exact same notebook we have before,
21749160	21754040	but I've just pretty much copied all the text in here. Or it's the exact same code we had before.
21754040	21757560	So we just don't have all that other text in between. So I can kind of do a short summary
21757560	21762920	of what we did, as well as show you how this worked when I trained it on the B movie script.
21762920	21766520	So I did mention I was going to show you that I'm not lying, I will show you can see I've
21766520	21772280	got B movie dot txt loaded in here. And in fact, actually, I'm going to show you this script first
21772280	21776920	to show you what it looks like. So this is what the B movie script looks like. You can see it
21776920	21782200	just like a long, you know, script of text, I just downloaded this for free off the internet.
21782200	21786280	And it's actually not as long as the Romeo and Juliet play. So we're not going to get as good
21786280	21791160	of results from our model. But it should hopefully be okay. So we just start and I'm just going to
21791160	21794280	do a brief summary. And then I'll show you the results from the B movie script, just so that
21794280	21798120	people that are confused, maybe have something that wraps it up here. We're doing our imports.
21798120	21802520	I don't think I need to explain that this part up here is just loading in your file. Again,
21802520	21807240	I don't think I need to explain that. Then we're actually going to read the file. So open it from
21807240	21814680	our directory, decode it into utf eight, we're going to create a vocabulary and encode all of
21814680	21819320	the text that's inside of this file. Then what we're going to do is turn all of that text up
21819320	21823480	into you know, the encoded version, we're writing a function here that goes the other way around.
21823480	21827960	So from int to text, not from text to int, we're going to define the sequence length that we
21827960	21832680	want to train with, which will be sequence length of 100. You can decrease this value if you want,
21832680	21837240	you go 50, go 20, it doesn't really matter. It's up to you. It just that's going to determine
21837240	21841400	how many training examples you're going to have right is the sequence length. Next, what we're
21841400	21846680	going to do is create a character data set from tensor slices from text as int. What this is going
21846680	21853160	to do is just convert our entire text that's now an integer array into a bunch of slices of
21853160	21857240	characters. And so that's what this is doing here. So or not slices, what am I saying,
21857960	21863240	you're just going to convert, like, split that entire array into just characters, like that's
21863240	21867480	pretty much what it's doing. And then what we're going to say sequences equals char data set dot
21867480	21872200	batch, which now is going to take all those characters and batch them in lengths of 101.
21872200	21877560	What we're going to do then is split all of that into the training examples. So like this, right,
21877560	21884120	he ll and then yellow, we're going to map this function to sequences, which means we're going
21884120	21889800	to apply this to every single sequence and store that in data set. Then we're going to find the
21889800	21894760	parameters for our initial network. We're going to shuffle the data set and batch that into now
21894760	21899960	64 training examples. Then we're going to make the function that builds the model, which I've
21899960	21905560	already discussed, we're going to actually build the model starting with a batch size of 64. We're
21905560	21912280	going to create our loss function, compile the model, set our checkpoints for saving, and then
21912280	21918120	train the model and make sure that we say checkpoint callback as the checkpoint callback
21918120	21922280	for the model, which means it's going to save every epoch, the weights that the model had
21922280	21927880	computed at that epoch. So after we do that, then our models train so we've trained the model,
21927880	21932360	you can see I train this on 50 epochs for the B movie script. And then what we're going to do is
21932360	21939000	build the model now with a batch size of one. So we can pass one example to it and get a prediction,
21939000	21943000	we're going to load the most recent weights into our model from the checkpoint directory
21943000	21947320	that we defined above. And then what we're going to do is build the model and tell it to
21947320	21953960	expect the shape one, none as its initial input. Now none just means we don't know what that value
21953960	21957640	is going to be, but we know we're going to have one entry. Alright, so now we have this generate
21957720	21962280	text method, or function here, which I've already kind of went through how that works. And then
21962920	21967640	we can see, if I type in input string, so we type, you know, input string, let's say,
21968680	21975480	of hello, and hit enter, we'll watch and we can see that the B movie, you know, trained model
21975480	21980600	comes up with its output here. Now, unfortunately, the B movie script does not work as well as Romeo
21980600	21985800	and Juliet. That's just because Romeo and Juliet is a much longer piece of text. It's much better
21986440	21991560	it's format a lot nicer and a lot more predictable. But yeah, you kind of get the idea here and it's
21991560	21995960	kind of cool to see how this performs on different data. So I would highly recommend that you guys
21995960	22000920	find some training data that you could give this other than just the Romeo and Juliet or maybe
22000920	22005640	even try another play or something and see what you can get out of it. Also, quick side note,
22005640	22010040	to make your model better, increase the amount of epochs here. Ideally, you want this loss to
22010040	22015240	be as low as possible, you can see mine was still actually moving down at epoch 50. You will
22015240	22019720	reach a point where the amount of epochs won't make a difference. Although, with models like this,
22019720	22024520	the more epochs typically the better, because it's difficult for it to kind of overfit, because all
22024520	22030680	you want it to do really is just kind of learn how the language works and then be able to replicate
22030680	22035560	that to you almost, right? So that's kind of the idea here. And with that being said, I'm going to
22035560	22040600	say that this section is probably done. Now, I know this was a long, probably confusing section
22040600	22044680	for a lot of you. But this is, you know, what happens when you start getting into some more
22044680	22048680	complex things in machine learning, it's very difficult to kind of grasp and understand all
22048680	22053320	these concepts in an hour of me just explaining them. What I try to do in these videos is introduce
22053320	22057720	you to the syntax show you how to get a working, you know, kind of prototype and hopefully give
22057720	22061560	you enough knowledge to the fact where if you're confused by something that I said, you can go
22061560	22066200	and you can look that up and you can figure out kind of the more important details for yourself,
22066200	22070520	because I really just I can't go into all, you know, the extremes in these videos. So anyways,
22070520	22074520	that has been this section. I hope you guys enjoyed doing this. I thought this was pretty cool.
22074520	22077400	And in the next section, we're going to be talking about reinforcement learning.
22080600	22085480	Hello, everyone, and welcome to the next module in this course on reinforcement learning. So what
22085480	22088920	we're going to be doing in this module is talking about another technique in machine learning called
22088920	22092920	reinforcement learning. Now, if you remember at the very beginning of this course, which I know
22093000	22098200	for you guys is probably at like six hours ago at this point, we did briefly discuss what reinforcement
22098200	22102360	learning was. Now I'll go through a recap here just to make sure everyone's clear on it. But
22102360	22107000	essentially, reinforcement learning is kind of the strategy in machine learning where rather
22107000	22112840	than feeding a ton of data and a ton of examples to our model, we let the model or in this case,
22112840	22117800	we're going to call it agent actually come up with these examples itself. And we do this by letting
22117880	22123480	an agent explore an environment. Now, essentially, the concept here is just like humans, the way that
22123480	22128520	we learn to do something say like play a game is by actually doing it, we get put in the environment,
22128520	22132440	we try to do it. And then, you know, we'll make mistakes, we'll encounter different things,
22132440	22137720	we'll see what goes correctly. And based on those experiences, we learn and we figure out the correct
22137720	22143560	things to do a very basic example is, you know, say we play a game. And when we go left, we fell
22143640	22147880	off a cliff or something, right? Next time we play that game, and we get to that point, we're
22147880	22152600	probably not going to go left, because we're going to remember the fact that that was bad, and hence
22152600	22157480	learned from our mistakes. So that's kind of the idea here with reinforcement learning. I'm going
22157480	22161880	to go through exactly how this works and give some better examples and some math behind one of the
22161880	22165160	implementations we're going to use. But I just want to make this clear that there's a lot of
22165160	22169160	different types of reinforcement learning. In this example, we're just going to be talking
22169160	22173080	about something called q learning. And I'm going to keep this module shorter compared to the other
22173080	22179000	ones. Because this field of AI machine learning is pretty complex and can get pretty difficult
22179000	22183640	pretty quickly. So it's something that's maybe a more advanced topic for some of you guys. Alright,
22183640	22188280	so anyways, now we need to define some terminology before I can even start really explaining the
22188280	22192760	technique we're going to use and how this works. So we have something called an environment,
22192760	22197640	agent, state action and reward. And I'm hoping that some of you guys will remember this from the
22197640	22203640	very beginning. But environment is essentially what we're trying to solve or what we're trying to
22203640	22208600	do. So in reinforcement learning, we have this notion of an agent. And the agent is what's going
22208600	22213000	to explore the environment. So if we're thinking about reinforcement learning, when it comes to
22213000	22218440	say training an AI to play a game, well, in that instance, say we're talking about Mario, the agent
22218440	22223320	would be Mario as that is the thing that's moving around and exploring our environment. And the
22223400	22228600	environment would be the level in which we're playing in. So you know, in another example,
22228600	22232040	maybe in the example we're going to use below, we're actually going to be kind of in almost a
22232040	22237320	maze. So the environment is going to be the maze. And the agent is going to be the character or the
22237320	22242040	entity or whatever you want to call it, that's exploring that maze. So it's pretty, it's usually
22242040	22246040	pretty intuitive to come up with what the environment and the agent are, although in some
22246040	22250680	more complex examples, it might not always be clear. But just understand that reinforcement
22250760	22255320	learning deals with an agent, something exploring an environment and a very common
22255880	22259960	application of reinforcement learning is in training AI is on how to play games. And it's
22259960	22263960	actually very interesting what they've been able to do in that field recently. Okay, so we have
22263960	22268360	environments and agent, hopefully that makes sense. The next thing to talk about is state. So
22268360	22275080	essentially, the state is where you are in the environment. So obviously, inside of the environment,
22275080	22279880	we can have many different states. And a state could also be associated with the, you know,
22279880	22285880	agent itself. So we're going to say the agent is in a specific state, whenever it is in some
22285880	22291160	part of the environment. Now, in the case of our game, the state that an agent would be in
22291160	22296680	would be their position in the level, say if they're at, you know, x y coordinates, like 1020,
22296680	22303320	they would be at state or in state 1020. That's kind of how we think about states. Now, obviously,
22303320	22307560	state could be applied in some different instances as well. We're playing say, maybe a turn based
22307560	22312200	game. You know, actually, that's not really a great example. I'm trying to think of something
22312200	22316440	where the state wouldn't necessarily be a position, maybe if you're playing a game where you have
22316440	22321400	like health or something like that. And part of the state might be the health of the character.
22321960	22325480	This can get complicated, depending on what you're trying to do. But just understand the notion
22325480	22329560	that for most of our example, state is simply going to be in location, although it really is
22329560	22334040	just kind of telling us information about where the agent is, and its status in the environment.
22334840	22340280	So next, we have this notion of an action. So in reinforcement learning, our agent is exploring
22340280	22344440	the environment, it's trying to figure out the best way or how to accomplish some kind of goal
22344440	22348680	in the environment. And the way that it interacts with the environment is with something called
22348680	22354040	actions. Now, actions could be say, moving the left arrow key, right, moving to the left in
22354040	22358280	the environment, moving to the right, it could be something like jumping in an action can actually
22358280	22364360	be not doing something at all. So when we say, you know, agent performed action, that could
22364360	22368680	really mean that the action and that maybe time step was that they didn't do something, right,
22368680	22373560	that they didn't do anything that was their action. So that's kind of the idea of action.
22374120	22378760	In the example of our Mario one, which I keep going back to an action would be something like
22378760	22384680	jumping. And typically actions will change the state of our entity or our agent, although they
22384680	22389800	might not necessarily do that. In fact, we will observe with a lot of the different actions that
22389800	22394760	we could actually be in the same state after performing that action. Alright, so now we're
22394760	22401240	on to the last part, which is actually the most important to understand. And this is reward. So
22401240	22406520	reward is actually what our agent is trying to maximize while it is in the environment. So the
22406520	22412600	goal of reinforcement learning is to have this agent navigate this environment, go through a
22412600	22418760	bunch of the different states of it and determine which actions maximize the reward at every given
22418760	22425320	state. So essentially, the goal of our agent is to maximize a reward. But what is a reward? Well,
22425320	22431320	after every action that's taken, the agent will receive a reward. Now this reward is something
22431320	22436520	that us as the programmer need to come up with. The reason we need to do this is because we need
22436520	22441000	to tell the agent when it's performing well and when it's performing poorly. And just like we
22441000	22446200	had like a loss function in neural networks, when we're using those before, this is almost like
22446200	22451480	our loss function, you know, the higher this number is, the more reward the agent gets, the
22451480	22457240	better, the lower the reward, you know, it's not as good, it's not doing as well. So that's how we
22457240	22462840	kind of monitor and assess performance for our agents is by determining the almost average amount
22462840	22467400	of reward that they're able to achieve. And their goal is really to, you know, it's almost an
22467400	22472120	optimization problem where they're trying to maximize this reward. So what we're going to do
22472120	22476120	in reinforcement learning is have this agent exploring the environment, going through these
22476120	22480760	different states and performing these different actions, trying to maximize its reward. And
22480760	22485080	obviously, if we're trying to get the agent to say finish a level or, you know, complete the game,
22485640	22491480	then the maximum maximum reward will be achieved once it's completed the level or completed the
22491480	22496600	game. And if it does things that we don't like, say like dying or like jumping in the wrong spot,
22496680	22501080	we could give it a negative reward to try to influence it to not do that. And our goal,
22501080	22505400	you know, when we train these agents is for them to get the most reward. And we hope that
22505400	22508920	they're going to learn the optimal route through a level or through some environment that will
22508920	22513800	maximize that reward for them. Okay, so now I'm going to talk about a technique called Q learning,
22513800	22518120	which is actually just an algorithm that we're going to use to implement this idea of reinforcement
22518120	22522520	learning. We're not going to get into anything too crazy in this last module, because this is meant
22522520	22527720	to be more of an introduction into the kind of field of reinforcement learning than anything else.
22527720	22532200	But Q learning is the most basic way to implement reinforcement learning, at least that I have
22532200	22537800	discovered. And essentially, what Q learning is, and I don't actually really know why they call it
22537800	22543880	Q, although I should probably know that is creating some kind of table or matrix likes
22543880	22549720	data structure, that's going to contain as the, what is it, I guess the rows, every single state,
22549720	22554760	and as the columns, every single action that could be taken in all of those different states.
22554760	22559640	So for an example here, and we'll do one on kind of the whiteboard later on, if we can get there.
22560680	22566280	But here, we can see that this is kind of my Q table. And what I'm saying is that we have a one,
22566280	22572280	a two, a three, a four, as all of the possible actions that could be performed in any given state.
22572280	22577720	And we have three states denoted by the fact that we have three rows. And the numbers in this,
22578440	22583320	this table with this Q, what do they call it, Q matrix Q table, whatever you want to call it,
22583320	22589720	the numbers that are present here, represent what the predicted reward will be, given that we take
22589720	22595720	an action, whatever this action is in this state. So I'm not sure if this is making sense to you
22595720	22601800	guys, but essentially, if we're saying that row zero is state zero, action two, a two, this value
22601800	22608840	tells us what reward we should expect to get. If we take this action while we're in this state,
22608840	22613400	that's what that is trying to tell us. That's what that means. Same thing here in, you know,
22613400	22619800	state two, we can see that the optimal action to take would be action two, because that has the
22619800	22625000	highest reward for this state. And that's what this table is that we're going to try to generate
22625000	22631160	with this technique called Q learning, a table that can tell us given any state, what the predicted
22631160	22635880	reward will be for any action that we take. And we're going to generate this table by exploring
22635880	22641400	the environment many different times, and updating these values according to what we kind of see
22641400	22646040	or what the agent sees in the environment and the rewards that it receives for any given action in
22646040	22650520	any given state. And we'll talk about how we're going to update that later. But this is the basic
22650520	22655480	premise. So that is kind of Q learning, we're going to hop on the whiteboard now, and we'll do a
22655480	22659880	more in depth example, but then we're going to talk about how we actually learned this Q table
22659960	22664680	that I just discussed. Okay, so I've drawn a pretty basic example right now that I'm going to try
22664680	22670200	to use to illustrate the idea of Q learning and talk about some problems with it and how we can
22670200	22674920	kind of combat those as we learn more about how Q learning works. But the idea here is that we
22674920	22679080	currently have three states and why, what is happening? Why was that happening up at the top?
22679080	22684920	I don't know. Anyways, the idea is we have three states as one s two and s three. And at each state
22684920	22690280	we have two possible actions that can be taken, we can either stay in this state or we can move.
22690840	22695960	Now, what I've done is kind of just written some integers here that represent the reward that we're
22695960	22701640	going to get or that the agent is going to get such that it takes that action in a given state.
22701640	22709080	So if we take the action here in s one, right of moving, then we will receive a reward of one
22709080	22712840	because that's what we've written here is the reward that we get for moving. Whereas if we
22712920	22717480	stay, we'll get a reward of three, you know, same concept here, if we stay, we get two,
22717480	22722840	if we move, we get one, and I think you understand the point. So the goal of our agent to remember
22722840	22727400	is to maximize its reward in the environment. And what we're going to call the environment
22727400	22733080	is this right here, the environment is essentially defines the number of states, the number of
22733080	22738920	actions, and you know, the way that the agent can interact with these states and these actions.
22738920	22743320	So in this case, the agent can interact with the states by taking actions that change its state,
22743320	22747480	right? So that's where we're getting out with this. Now, what I want to do is show you how we
22747480	22754680	use this queue table, or learn this queue table to come up with kind of the almost, you know,
22754680	22759080	the model, like the machine learning model that we're going to use. So essentially, what we would
22759080	22767400	want to have here is we want to have a kind of pattern in this table that allows our agent to
22767480	22772440	receive the maximum reward. So in this case, we're going to say that our agent will start at
22772440	22776760	state s one. And obviously, whenever we're doing this reinforcement learning, we need to have some
22776760	22781240	kind of start state that the agent will start in this could be a random state, it could change,
22781240	22785080	but it doesn't just start in some state. So in this case, we're going to say it starts at s one.
22785640	22791320	Now, when we're in s one, the agent has two things that it can do. It can stay in the current state
22791320	22797560	and receive a reward of three, or it can move and receive a reward of one, right? If we get to s
22797560	22802680	two, in this state, what can we do? We can stay, which means we receive a reward of two, or we
22802680	22807320	can move, which means we get a reward of one. And same thing for s three, we can stay, we get a
22807320	22813800	reward of four, and we can move, we get a reward of one. Now, right now, if we had just ran this
22814360	22819400	one time and have the agent stay in each state, like start in each unique state,
22819400	22824200	this is what the queue table we would get would look like. Because after looking at this, just
22824200	22829160	one time starting in each state, what the agent would be able to, or I guess, two times, because
22829160	22834680	it would have to try each action. Let's say we had the agent start in each state twice. So it started
22834680	22839080	an s one twice, it started s two twice, and it started an s three twice. And every time it started
22839080	22843480	there, it tried one of the different actions. So when it started in s one, it tried moving once,
22843480	22847400	and then it tried staying once, we would have a queue table that looks like this. Because what
22847480	22852840	would happen is we would update values in our queue table to represent the reward we received
22852840	22858360	when we took that action from that state. So we can see here that when we're in state s one,
22858360	22864520	and we decide to stay, what we did is we wrote a three inside of the stay column, because that is
22864520	22870840	how much reward we received when we moved, right? Same thing for state two, when we moved for state
22870840	22876520	two or have I guess, sorry, stayed when we stayed in state two, we received a reward of two, same
22876520	22883080	thing for four. Now, this is okay, right? This tells us kind of, you know, the optimal move to
22883080	22888120	make in any state to receive the maximum reward. But what if we introduce the idea that, you know,
22888120	22895160	our agent, we want it to receive the maximum total reward possible, right? So if it's in state one,
22895160	22899800	ideally, we'd like it to move to state two, and then move to states three, and then just stay in
22899800	22903800	state three, because it will receive the most amount of reward. Well, with the current table
22903800	22908200	that we've developed, if we just follow this, and we look at the table, we say, okay, if we want to
22908200	22912920	use this Q learning table now to, you know, move an agent around our level, what we'll do is we'll
22912920	22918600	say, okay, what state is it in? If it's in state two, we'll do stay because that's the highest reward
22918600	22924120	that we have in this table. If that's the approach we use, then we could see that if our, you know,
22924120	22929640	agent start in state one or state two, it would stay in what we call a local minima, because
22929640	22935320	it's not able to kind of realize from this state that it can move any further and receive a much
22935320	22939960	greater reward, right? And that's kind of the concept we're going to talk about as we implement and,
22939960	22945400	you know, discuss further how Q learning works. But hopefully this gives you a little bit of insight
22945400	22951560	into what we do with this table. Essentially, when we're updating these table values is when
22951560	22956680	we're exploring this environment. So when we explore this environment, and we start in a state,
22956680	22961720	when we take an action to another state, we observe the reward that we got from going there,
22961720	22966200	and we observe the state that we change to, right? So we observe the fact that in state one,
22966200	22971480	when we go to state two, we receive the reward of one. And what we do is we take that observation
22971480	22977640	and we use it to update this Q table. And the goal is that at the end of all of these observations,
22977640	22983800	and there could be millions of them, that we have a Q table that tells us the optimal action to take
22983880	22989880	in any single state. So we're actually hard coding, this kind of mapping that essentially
22989880	22994760	just tells us given any state, all you have to do is look up in this table, look at all of the
22994760	22999720	actions that could be taken, and just take the maximum action or the reward that's supposed to
22999720	23004440	give, I guess, the action that's supposed to give the maximum reward. And if we were to follow
23004440	23008120	that on this, we could see we get stuck in the local minima, which is why we're going to introduce
23008120	23015000	a lot of other concepts. So our reinforcement learning model in Q learning, we have to implement
23015000	23021320	the concept of being able to explore the environment, not based on previous experiences,
23021320	23026120	right? Because if we just tell our model, okay, what we're going to do is we're going to start in
23026120	23030120	all these different states, we're going to start in the start state and just start navigating around.
23030120	23035000	If we update our model immediately, or update our Q table immediately and put this three here for
23035080	23041240	stay, we can almost guarantee that since this three is here, when our model is training, right,
23041240	23045480	if it's using this Q table to determine what state to move to next, when it's training and
23045480	23049480	determining what to do, it's just always going to stay, which means we'll never get a chance to
23049480	23055480	even see what we could have gotten to at S three. So we need to kind of introduce some concept
23055480	23061560	of taking random actions, and being able to explore the environment more freely before starting
23061560	23065800	to look at these Q values, and use that for the training. So I'm actually going to go back
23066600	23070280	to my slides now to make sure I don't get lost, because I think I was starting to ramble a little
23070280	23075480	bit there. So we're going to now talk about learning the Q table. So essentially, I showed
23075480	23081320	you how we use that Q table, which is given some state, we just look that state up in the Q table,
23081320	23085960	and then determine what the maximum reward we could get by taking, you know, some action is and
23085960	23090680	then take that action. And that's how we would use the Q table later on when we're actually using the
23090680	23096040	model. But when we're learning the Q table, that's not necessarily what we want to do. We don't want
23096040	23101320	to explore the environment by just taking the maximum reward we've seen so far and just always
23101320	23105480	going that direction, we need to make sure that we're exploring in a different way and learning
23105480	23110280	the correct values for the Q table. So essentially, our agent learns by exploring the environment
23110280	23114280	and observing the outcome slash reward from each action it takes in a given state, which we've
23114280	23118520	already said. But how does it know what action to take in each state when it's learning? That's
23118520	23123240	the question I need to answer for you now. Well, there's two ways of doing this. Our agent can
23123240	23128200	essentially, you know, use the current Q table to find the best action, which is kind of what I
23128200	23132520	just discussed. So taking looking at the Q table, looking at the state and just taking the highest
23132520	23138920	reward, or it can randomly pick a valid action. And our goal is going to be when we create this Q
23138920	23145400	learning algorithm to have a really great balance of these two, where sometimes we use the Q table
23145400	23152280	to find the best action, and sometimes we take a random action. So that is one thing. But now
23152280	23156600	I'm just going to talk about this formula for how we actually update Q values. So obviously, what's
23156600	23161080	going to end up happening in our Q learning is we're going to have an agent that's going to be in
23161080	23165880	the learning stage, exploring the environment and having all these actions and all these rewards
23165880	23169480	and all these observations happening. And it's going to be moving around the environment by
23169480	23173480	following one of these two kind of principles, randomly picking a valid action or using the
23173560	23178920	current Q table to find the best action. When it gets into a net, a new state, and it, you know,
23178920	23182840	moves from state to state, it's going to keep updating this Q table, telling it, you know,
23182840	23186200	this is what I've learned about the environment, I think this is a better move, we're going to
23186200	23190360	update this value. But how does it do that in a way that's going to make sense? Because we can't
23190360	23195080	just put, you know, the maximum value we got from moving, otherwise, we're going to run into that
23195080	23199800	issue, which I just talked about, where we get stuck in that local maxima, right? I'm not sure
23199800	23204760	if I called it minimum before, but anyways, it's local maxima, where we see this high reward,
23204760	23209560	but that's preventing us if we keep taking that action from reaching a potentially high reward
23209560	23215000	in a different state. So the formula that we actually use to update the Q table is this. So Q
23215000	23220600	state action equals Q state action, and a state action is just referencing first the rows for the
23220600	23227240	state and then the action as the column, plus alpha times, and then this is all in brackets,
23227240	23235880	right? Reward plus, I believe this is gamma times max Q of new states minus Q state action. So what
23235880	23239720	the heck does this mean? What are these constants? What is all this? We're going to talk about the
23239720	23245160	constants in a minute. But I want to, yeah, I want to explain this formula actually. So let's,
23245160	23249160	okay, I guess we'll go through the constants, it's hard to go through a complicated math formula.
23249160	23254360	So a stands for the learning rate, and gamma stands for the discount factor. So alpha learning
23254360	23258520	rate, gamma discount factor. Now, what is the learning rate? Well, this is a little blurb on
23258520	23264360	what this is. But essentially, the learning rate ensures that we don't update our Q table too much
23265320	23271160	on every observation. So before, right, when I was showing you like this, if we can go back
23271160	23276680	to my windows ink, why is this not working? I guess I'm just not patient enough. Before when I was
23276680	23281880	showing you all I did when I took an action was I looked at the reward that I got from taking that
23281880	23287080	action. And I just put that in my Q table, right? Now, obviously, that is not an optimal approach
23287080	23291640	to do this, because that means that in the instance where we hit state one, well, I'm not going to
23291640	23295240	be able to get to this reward of four, because I'm going to throw that, you know, three in here,
23295240	23301800	and I'm just going to keep taking that action. We need to, you know, hopefully make this move
23301800	23307480	action actually have a higher value than stay. So that next time we're in state one, we consider
23307480	23312520	the fact that we could move to state two, and then move to state three to optimize our reward.
23312520	23316040	So how do we do that? Well, the learning rate is one thing that helps us kind of accomplish
23316040	23321080	this behavior. Essentially, what is telling us, and this is usually a decimal value, right,
23321080	23327640	is how much we're allowed to update every single Q value by on every single action or every single
23327640	23332360	observation. So if we just use the approach before, then we're only going to need to observe,
23332360	23335960	given the amount of states and the amount of actions, and we'll be able to completely fill
23335960	23339800	in the Q table. So in our case, if we had like three states and three actions, we could, you
23339800	23344440	know, nine iterations, we'd be able to fill the entire Q table. The learning rate means that
23344440	23350040	it's going to just update a little bit slower and essentially change the value in the Q table very
23350040	23354440	slightly. So you can see that what we're doing is taking the current value of the Q table. So
23354440	23360440	whatever is already there. And then what we're going to do is add some value here. And this value
23360440	23364920	that we add is either going to be positive or negative, essentially telling us, you know,
23364920	23369240	whether we should take this new action or whether we shouldn't take this new action.
23369240	23374920	Now, the way that this kind of value is calculated, right, is obviously our alpha is
23374920	23381000	multiplied this by this, but we have the reward plus, in this case, gamma, which is just going to
23381000	23386280	actually be the discount factor. And I'll talk about how that works in a second of the maximum
23386280	23393960	of the new state we moved into. Now, what this means is find the maximum reward that we could
23393960	23401400	receive in the new state by taking any action and multiply that by what we call the discount factor.
23401400	23405880	With this part of the formulas trying to do is exactly what I've kind of been talking about.
23406520	23412040	Try to look forward and say, okay, so I know if I take this action in this state, I receive
23412040	23417880	this amount of reward. But I need to factor in the reward I could receive in the next state,
23417880	23423800	so that I can determine the best place to move to. That's kind of what this max and this
23424360	23428760	gamma are trying to do for us. So this discount factor, whatever you want to call it. It's trying
23428760	23435160	to factor in a little bit about what we could get from the next state into this equation so that
23435160	23441640	hopefully our kind of agent can learn a little bit more about the transition states. So states that
23441640	23446200	maybe are actions that maybe don't give us an immediate reward, but lead to a larger reward
23446200	23450920	in the future. That's what this Y and max are trying to do. Then what we do is we subtract
23451560	23457560	from this, the state and action. This is just to make sure that we're adding what the difference
23457560	23465240	was in, you know, what we get from this versus what the current value is, and not like multiplying
23465240	23469640	these values crazily. I mean, you can look into more of the math here and plug in like some values
23469640	23472680	later, and you'll see how this kind of works. But this is the basic formula. And I feel like I
23472680	23478280	explain that in depth enough. Okay, so now that we've done that, and we've updated this, we've
23478360	23484200	learned kind of how we update the cells and how this works. I could go back to the whiteboard and
23484200	23488360	draw it out. But I feel like that makes enough sense. We're going to look at what the next state is,
23488360	23492440	we're going to factor that into our calculation, we have this learning rate, which tells us essentially
23492440	23498520	how much we can update each cell value by. And we have this, what do you call it here discount
23498520	23503880	factor, which essentially tries to kind of define the balance between finding really good rewards
23503880	23509960	in our current state, and finding the rewards in the future state. So the higher this value is,
23509960	23513960	the more we're going to look towards the future, the lower it is, the more we're going to focus
23513960	23517480	completely on our current reward, right? And obviously, that makes sense, because we're going
23517480	23521240	to add the maximum value. And if we're multiplying that by a lower number, that means we're going
23521240	23527480	to consider that less than if that was greater. Awesome. Okay. So now that we've kind of understand
23527480	23531000	that I want to move on to a Q learning example. And what we're going to do for this example is
23531080	23536600	actually use something called the open AI gym. I just need to throw my drawing tablet away
23536600	23541960	right there so that we can get started. But open AI gym is actually a really interesting kind of
23541960	23547240	module. I don't even actually, I don't even really know the way to describe it almost tool. There's
23547240	23553080	actually developed by open AI, you know, coincidentally by the name, which is founded by Elon Musk
23553080	23558120	and someone else. So he's actually, you know, made this kind of, I don't really don't know the word
23558120	23563720	to describe it. I almost want to say tool that allows programmers to work with these really cool
23563720	23568600	gym environments and train reinforcement learning models. So you'll see how this works in a second,
23568600	23572840	but essentially, there's a ton of graphical environments that have very easy interfaces
23572840	23577320	to use. So like moving characters around them, that you're allowed to experiment with completely
23577320	23581640	for free as a programmer to try to, you know, make some cool reinforcement learning models.
23581640	23585320	That's what open AI gym is. And you can look at it. I mean, we'll click on it here actually to see
23585320	23589080	what it is. You can see gym, there's all these different Atari environments, and it's just a
23589080	23594360	way to kind of train reinforcement learning models. All right. So now we're going to start by just
23594360	23598840	importing gym. If you're in Collaboratory, there's nothing you need to do here. If you're in your
23598840	23602840	own thing, you're going to have to pip install gym. And then what we're going to do is make this
23602840	23608520	frozen lake v zero gym. So essentially, what this does is just set up the environment that we're
23608520	23612920	going to use. Now, I'll talk more about what this environment is later, but I want to talk about how
23612920	23617800	gym works, because we are going to be using this throughout the thing. So the open AI gym
23618440	23623560	is meant for reinforcement learning. And essentially what it has is an observation space
23623560	23628520	and an action space for every environment. Now the observation space is what we call our
23628520	23633880	environment, right? And that will tell us the amount of states that exist in this environment.
23633880	23637080	Now, in our case, we're going to be using kind of like a maze like thing, which I'll show you in
23637080	23641800	a second. So you'll understand why we get the values we do. Action space tells us how many
23641800	23647320	actions we can take when we do the dot n, at any given state. So if we print this out,
23647880	23652680	we get 16 and four, representing the observation space. In other words, the number of states is
23652680	23658200	16. And the amount of actions we can take in every single state is four. Now in this case,
23658200	23664120	these actions are going to be left down up and right. But yes, now env dot reset. So essentially,
23664120	23669080	we have some commands that allow us to move around the environment, which are actually down here.
23669080	23673640	If we want to reset the environment and start back in the beginning state, then we do env
23673640	23677720	dot reset, you can see this actually returns to us the starting state, which obviously is going to
23677720	23684440	be zero. Now we also have the ability to take a random action, or select a random action from
23684440	23689240	the action space. So what this line does right here is say of the action space, so of all the
23689240	23694440	commands that are there, or all the actions we could take, pick a random one and return that.
23694520	23701560	So if you do that, actually, let's just print action and see what this is. You see we get zero
23701560	23707960	to right, it just gives us a random action that is valid from the action space. All right. Next,
23707960	23714600	what we have is this env dot step in action. Now what this does is take whatever action we have,
23714600	23719880	which in this case is three, and perform that in the environment. So tell our agent to take
23719880	23725160	this action in the environment and return to us a bunch of information. So the first thing is the
23725160	23730200	observation, which essentially means what state do we move into next? So I could call this
23731640	23737960	new underserved state reward is what reward did we receive by taking that action? So this will
23737960	23743160	be some value right in our in this case, the reward is either one or zero. But that's not
23743160	23748760	that important to understand. And then we have a bool of done, which tells us did we lose the game
23748760	23754120	or did we win the game? Yes or no. So true. So if this is true, what this means is we need to
23754120	23759640	reset the environment because our agent either lost or won and is no longer in a valid state in
23759640	23764520	the environment. Info gives us a little bit of information. It's not showing me anything here.
23764520	23768920	We're not going to use info throughout this, but figured I'd let you know that now in VDOT
23768920	23773880	render, I'll actually render this for you and show you renders a graphical user interface that
23773880	23778360	shows you the environment. Now, if you use this while you're training, so you actually watch
23778360	23782680	the agent do the training, which is what you can do with this, it slows it down drastically,
23782680	23786520	like probably by, you know, 10 or 20 times, because it actually needs to draw the stuff on
23786520	23790280	the screen. But you know, you can use it if you want. So this is what our frozen lake example
23790280	23794360	looks like. You can see that the highlighted square is where our agent is. And in this case,
23794360	23802200	we have four different blocks. We have SFH and G. So S stands for start F stands for frozen,
23802200	23806760	because this is a frozen lake. And the goal is to navigate to the goal without falling in one
23806760	23812040	of the holes, which is represented by H. And this here tells us the action that we just took. Now,
23812040	23818280	I guess the starting action is up because that's zero, I believe. But yes, so if we run this a
23818280	23822280	bunch of times, we'll see this updating. Unfortunately, this doesn't work very well in
23822280	23827080	Google Collaboratory, the the GUIs. But if you did this in your own command line, and you like
23827080	23831400	did some different steps and rounded it all out, you would see this working properly. Okay,
23831480	23835000	so now we're on to talking about the frozen lake environment, which is kind of what I just did.
23835000	23838840	So now we're just going to move to the example where we actually implement Q learning to
23838840	23843400	essentially solve the problem. How can we train an AI to navigate this environment and get to the
23843400	23847960	start to the goal? How can we do that? Well, we're going to use Q learning. So let's start. So the
23847960	23852680	first thing we need to do is import gym, import numpy, and then create some constants here. So
23852680	23856360	we'll do that. We're going to say the amount of states is equal to the line I showed you before.
23856440	23863240	So env dot observation, space dot n, actions is equal to env dot action space n. And then we're
23863240	23868680	going to say Q is equal to NP dot zeros, states and actions. So something I guess I forgot to
23868680	23873800	mention is when we initialize the Q table, we just initialize all blank values or zero values,
23873800	23878200	because obviously, at the beginning of our learning, our model or agent doesn't know
23878200	23881800	anything about the environment yet. So we just leave those all blank, which means we're going
23881800	23886520	to more likely be taking random actions at the beginning of our training, trying to explore
23886520	23891160	the environment space more. And then as we get further on and learn more about the environment,
23891160	23896840	those actions will likely be more calculated based on the Q table values. So we print this out,
23896840	23903000	we can see this is the array that we get, we've had to be build a 16 by four, I guess not array,
23903000	23907720	well, I guess this technically is an array, we'll call it matrix 16 by four. So every single row
23907720	23911640	represents a state, and every single column represents an action that could be taken in
23911640	23915880	that state. Alright, so we're going to find some constants here, which we talked about before.
23915880	23920760	So we have the gamma, the learning rate, the max amount of steps and the number of episodes. So the
23920760	23926120	number of episodes is actually, how many episodes do you want to train your agent on? So how many
23926120	23931560	times do you want it to run around and explore the environment? That's what episode stands for.
23932280	23937480	Max steps essentially says, Okay, so if we're in the environment, and we're kind of navigating
23937560	23941400	and moving around, and we haven't died yet, how many steps are we going to let the agent take
23941400	23945800	before we cut it off? Because what could happen is we could just bounce in between two different
23945800	23950760	states indefinitely. So we need to make sure we have a max steps so that at some point,
23950760	23955000	if the agent is just doing the same thing, we can, you know, end that or if it's like going in
23955000	23961480	circles, we can end that and start again with different, you know, Q values. Alright, so episodes,
23961480	23964840	yeah, we already talked about that learning rate, we know what that is gamma, we know what that is
23965400	23969400	mess with these values as we go through and you'll see the difference it makes in our training.
23969400	23973560	I've actually included a graph down below. So we'll talk about that kind of show us the outcome
23973560	23981400	of our training. But learning rate, the higher this is, the faster I believe that it learns. Yes,
23981400	23985800	so a high learning rate means that each update will introduce larger change to the current state.
23985800	23989480	So yeah, so that makes sense based on the equation as well. Just wanted to make sure that I wasn't
23989480	23994040	going crazy there. So let's run this constant block to make sure. And now we're going to talk
23994040	23998520	about picking an action. So remember how I said, and I actually wrote them down here,
23998520	24004520	there's essentially two things we can do at every, what do we call it, step, right? We can
24004520	24009560	randomly pick a valid action, or we can use the current Q table to find the best action. So how
24009560	24013480	do we actually implement that into our open AI gym? Well, I just wanted to write a little
24013480	24018200	code block here to show you the exact code that will do this for us. So we're going to introduce
24018200	24025160	this new concept or this new, I can almost call it constant, called epsilon. And I think epsilon,
24025160	24030520	I think I spelt this wrong, ep salon. Yeah, that should be how you spell it. So we're going to start
24030520	24034120	the epsilon value essentially tells us the percentage chance that we're going to pick a
24034120	24039480	random action. So here, we're going to use a 90% epsilon, which essentially means that every time
24039480	24043960	we take an action, there's going to be a 90% chance that it's random and 10% chance that we look at
24043960	24049720	the Q table to make that action. Now, we'll reduce this epsilon value as we train, so that
24049720	24053960	our model will start being able to explore, you know, as much as it possibly can in the
24053960	24059160	environment by just taking random actions. And then after we have enough observations,
24059160	24062920	and we've explored the environment enough, we'll start to slowly decrease the epsilon,
24062920	24068120	so that it hopefully finds a more optimal route for things to do. Now, the way we do this is we
24068120	24072360	save NP dot random dot uniform zero one, which essentially means pick a random value between
24072360	24079880	zero and one is less than epsilon and epsilon like that. I think I'm going to have to change
24079880	24085000	some other stuff, but we'll see, then action equals ENV dot action space dot sample. So
24085000	24089880	take a random action. That's what this means store what that action is in here. Otherwise,
24089880	24097480	we're going to take the argument max of the state row in the Q table. So what this means is find
24097480	24101800	the maximum value in the Q table and tell us what row it's in. So that way we know what
24101800	24106680	action to take. So if we're in row, I guess, not sorry, not row column for in column one,
24106680	24110280	you know, that's maximum value, take action one, that's what this is saying. So using the Q table
24110280	24115080	to pick the best action. Alright, so we don't need to run this because this is just going to be
24115080	24119640	which I just wrote that to show you. Now, how do we update the Q values? Well, this is just
24119640	24124040	following the equation that I showed above. So this is the line of code that does this, I just
24124040	24128040	want to write it out so you guys could see exactly what each line is doing and kind of explore it
24128040	24131960	for yourself. But essentially, you get the point, you know, you have your learning rate, reward,
24131960	24137160	gamma, take the max, so NP dot max does the same thing as a max function in Python. This is going
24137160	24142680	to take the max value, not the argument max from the next state, right, the new state that we moved
24142680	24148200	into. And then subtracting obviously Q state action. Alright, so putting it all together. So
24148200	24152360	now we're actually going to show how we can train and create this Q table and then use that Q table.
24153000	24157320	So this is the pretty much all this code that I have, we've already actually
24157320	24161400	written at least this block here, that's why I put it in its own block. So just all the constants,
24161400	24165000	I've included this render constant to tell us whether we want to draw the environment or not.
24165000	24167880	In this case, I'm going to leave it false, but you can make it true if you want.
24167880	24172360	Episodes, I've left at 1500 for this, if you want to make your model better, typically you
24172360	24177000	train it on more episodes, but that's up to you. And now we're going to get into the big chunk
24177000	24182200	of code, which I'm going to talk about. So what this is going to do, we're going to have a rewards
24182200	24186280	list, which is actually just going to store all the rewards we see, just so I can graph that later
24186280	24191080	for you guys. Then we're going to say for episode in range episodes. So this is just telling us,
24191080	24196200	you know, for every episode, let's do the steps I'm about to do. So maximum amount of episodes,
24196200	24200120	which is our training length, essentially, we're going to reset the state, obviously,
24200120	24204440	which makes sense. So state equals in V dot reset, which will give us the starting state.
24205080	24209400	We're going to say for underscore in range, max steps, which means, okay, we're going to do,
24209400	24214040	you know, we're going to explore the environment up to maximum steps, we do have a done here,
24214120	24218040	which will actually break the loop if we've reached the goal, which we'll talk about further.
24218600	24222280	So the first thing we're going to do is say, if render, you know, render the environment,
24222280	24227640	that's pretty straightforward. Otherwise, let's take an action. So for each time step, we need to
24227640	24232200	take an action. So epsilon, I think is spelled correctly here. Yeah, believe that's right. So
24232200	24236280	I'm going to say action equals in V dot action space, this is already the code we've looked at.
24236280	24241160	And then what we're going to say is next state reward done underscore equals in V dot step
24241160	24245720	action, we've put an underscore here, because we don't really care about this info value. So
24245720	24249400	I'm not going to store it, but we do care about what the next state will be the reward from that
24249400	24254840	action. And if we were done or not. So we take that action, that's what does this EMB dot step.
24255480	24261240	And then what we do is say Q state action, we just update the Q value using the formula that
24261240	24265480	we've talked about. So this is the formula, you can look at it more in depth if you want.
24265480	24269080	But based on whatever the reward is, you know, that's how we're going to update those Q values.
24269080	24274600	And after a lot of training, we should have some decent Q values in there. Alright, so then we
24274600	24278680	set the current state to be the next state. So that when we run this time step again,
24279320	24283400	now our agent is in the next state, and can start exploring the environment again,
24284040	24289000	in this current, you know, iteration, almost, if that makes sense. So then we say if done,
24289000	24293640	so essentially, if the agent died, or if they lost or whatever it was, we're going to append
24293640	24300600	whatever reward they got from their last step into the rewards up here. And it's worthy of
24300600	24306200	noting that the way the rewards work here is you get one reward, if you move to a valid block,
24306200	24311000	and you get zero reward, if you die. So every time we move to a valid spot, we get one,
24311000	24315640	otherwise we get zero. I'm pretty sure that's the way it works at least. But that's something
24315640	24320120	that's important to know. So then what we're going to do is reduce the epsilon if we die,
24320120	24325400	but just a fraction of an amount, you know, 0.001, just so we slowly start decreasing the epsilon
24325400	24329560	moving in the correct direction. And then we're going to break because we've reached the goals,
24329560	24333320	print the Q table, and then print the average reward. Now this takes a second to train,
24334120	24339160	like, you know, a few seconds, really. That one is pretty fast, because I've set this at
24339160	24343880	was it 1500. But if you want, you can set this at say 10,000, wait another, you know,
24343880	24348840	few minutes or whatever, and then see how much better you can do. So we can see that after that,
24348920	24356040	I received an average reward of 0.28886667. This is actually what the Q table values look like.
24356040	24360040	So all these decimal values after all these updates, I just decided to print them out.
24360040	24364040	And I just want to show you the average reward so that we can compare that to what we can get
24364040	24368120	from testing or this graph. So now I'm just going to graph this. And we're going to see this is
24368120	24371160	what the graph so you don't have to really understand this code if you don't want to. But
24371160	24377480	this is just graphing the average reward over 100 steps from the beginning to the end. So
24377480	24382360	essentially, I've been, I've calculated the average of every 100 episodes, and then just
24382360	24386840	graph this on here. We can see that we start off very poorly in terms of reward, because the
24386840	24391480	epsilon value is quite high, which means that we're taking, you know, random actions pretty
24391480	24395080	much all the time. So if we're taking a bunch of random actions, obviously, chances are,
24395080	24398920	we're probably going to die a lot, we're probably going to get rewards of zeros quite frequently.
24398920	24403560	And then after we get to about 600 episodes, you can see that six actually represents 600,
24403640	24407720	because this is in hundreds, we start to slowly increase. And then actually, we go on a crazy
24407720	24413960	increase here, when we start to take values more frequently. So the epsilon is increasing,
24413960	24419000	right. And then after we get here, we kind of level off. And this does show a slight decline.
24419000	24422840	But I guarantee you if we ran this for, you know, like 15,000, it would just go up and down and
24422840	24427320	bob up and down. And that's just because even though we have increased the epsilon, there is
24427320	24431880	still a chance that we take a random action and you know, gets your reward. So that is pretty
24431880	24436280	much it for this Q learning example. You know, I mean, that's pretty straightforward
24436840	24441560	to use the Q table. If you actually wanted to say, you know, watch the agent move around the
24441560	24446040	thing, I'm going to leave that to you guys, because if you can follow what I've just done in here
24446040	24450600	and understand this, it's actually quite easy to use the Q table. And I think as like a final,
24450600	24455720	almost like, you know, trust in you guys, you can figure out how to do that. The hint is essentially
24455720	24460600	do exactly what I've done in here, except don't update the Q table values, just use the Q table
24460600	24465800	values already. And that's, you know, pretty much all there is to Q learning. So this has
24465800	24470680	been the reinforcement learning module for this TensorFlow course, which actually is the last
24470680	24474760	module in this series. Now, I hope you guys have enjoyed up until this point, just an emphasis
24474760	24479720	again, this was really just an introduction to reinforcement learning. This technique and this
24479720	24484440	problem itself is not very interesting and not, you know, the best way to do things is not the
24484440	24488440	most powerful. It's just to get you thinking about how reinforcement learning works. And
24488520	24492280	potentially, if you'd like to look into that more, there's a ton of different resources and,
24492280	24495720	you know, things you can look at in terms of reinforcement learning. So that being said,
24495720	24498840	that has been this module. And now we're going to move into the conclusion, we'll talk about
24498840	24502920	some next steps and some more things that you guys can look at to improve your machine learning skills.
24506280	24512280	So finally, after about seven hours of course content, we have reached the conclusion of this
24512280	24517480	course. Now what I'm going to do in this last brief short section is just explain to you where
24517560	24522440	you can go for some next steps and some further learning with TensorFlow and machine learning
24522440	24526520	artificial intelligence in general. Now what I'm going to be recommending to you guys is that we
24526520	24531560	look at the TensorFlow website, because they have some amazing guides and resources on here. And in
24531560	24537320	fact, a lot of the examples that we used in our notebooks were based off of or exactly the same
24537320	24541960	as the original TensorFlow guide. And that's because the code that they have is just very good.
24541960	24547080	They're very good and easy to understand examples. And in terms of learning, I find that these
24547080	24551240	guides are great for people that want to get in quickly, see the examples and then go and do some
24551240	24556520	research on their own time and understand why they work. So if you're looking for some further steps,
24556520	24561720	at this point in time, you have gained a very general and broad knowledge of machine learning
24561720	24567240	and AI, you have some basic skills in a lot of the different areas. And hopefully this has
24567240	24573160	introduced you to a bunch of different concepts and the possibilities of what you are able to do
24573160	24578360	using modules like TensorFlow. Now what I'm going to suggest to all of you is that if you find a
24578360	24583640	specific area of machine learning AI that you are very interested in, that you would dial in on
24583640	24588840	that area and focus most of your time into learning that, that is because when you get to a point in
24588840	24593320	machine learning and AI, where you really get specific and pick one kind of strain or one
24593320	24597960	kind of area, it gets very interesting very quickly. And you can devote most of your time
24597960	24602120	to getting as deep as possible and not specific topic. And that's something that's really cool.
24602120	24607320	And most people that are experts in AI or machine learning field typically have one area of
24607320	24611720	specialization. Now, if you're someone who doesn't care to specialize an area or you just want to
24611720	24616360	play around and see some different things, the TensorFlow website is great to really get kind
24616360	24621160	of a general introduction to a lot of different areas and be able to kind of use this code tweak
24621160	24625480	it a little bit on your own, and implement it into your own projects. And in fact, the next kind
24625480	24630280	of steps and resources I'm going to be showing you here, and involve simply going to the TensorFlow
24630360	24634840	website, going to the tutorial page, this is very easy to find, I don't even need to link it,
24634840	24639160	you can just search TensorFlow, and you'll find this online. And looking at some more advanced
24639160	24644840	topics that we haven't covered. So we've covered a few of the topics and tutorials that are here,
24644840	24649240	I've just kind of modified their version, and thrown out in the notebook and explained it in
24649240	24653800	wars and video content. But if you'd like to move on to say a next step or something very cool,
24653800	24659320	something I would recommend is doing the deep dream in the generic generative neural network
24659320	24663560	section on the TensorFlow website, being able to make something like this, I think is very cool.
24663560	24669240	And this is an example where you can tweak this a ton by yourself and get some really cool results.
24669240	24673880	So some things like this are definitely next steps, there's tons and tons of guides and tutorials
24673880	24678360	on this website, they make it very easy for anyone to get started. And with these guides,
24678360	24682200	what I will say is typically what will end up happening is they just give you the code and
24682200	24687480	brief explanations of why things work. You should really be researching and looking up some more,
24687560	24691960	you know, deep level explanations of why some of these things work as you go through, if you
24691960	24697240	want to have a firm and great understanding of why the model performs the way that it does.
24697240	24702360	So with that being said, I believe I'm going to wrap up the course now. I know you guys can imagine
24702360	24707400	how much work I put into this. So please do leave a like, subscribe to the channel, leave a content,
24707400	24712360	show your support. This I believe is the largest open source machine learning course in the world
24712360	24717960	that deals completely with TensorFlow and Python. And I hope that this gave you a lot of knowledge.
24717960	24721960	So please do give me your feedback down below in the comments. With that being said, again,
24721960	24727400	I hope you enjoyed. And I hopefully I will see you again in another tutorial guide or series.
