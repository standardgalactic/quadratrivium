WEBVTT

00:00.000 --> 00:04.320
Hello, everybody, and welcome to an absolutely massive TensorFlow

00:04.320 --> 00:07.800
slash machine learning slash artificial intelligence course.

00:08.000 --> 00:11.000
Now, please stick with me for this short introduction, as I am going to give you a

00:11.000 --> 00:15.040
lot of important information regarding the course concept, the resources for the

00:15.040 --> 00:18.120
course and what you can expect after going through this.

00:18.440 --> 00:21.840
Now, first, I will tell you who this course is aimed for.

00:22.080 --> 00:25.440
So this course is aimed for people that are beginners in machine learning

00:25.440 --> 00:28.440
and artificial intelligence, or maybe have a little bit of understanding

00:28.440 --> 00:32.200
that are trying to get better, but do have a basic fundamental knowledge

00:32.200 --> 00:34.040
of programming and Python.

00:34.320 --> 00:36.880
So this is not a course you're going to take if you haven't done any

00:36.880 --> 00:40.280
programming before, or if you don't know any Python syntax in general.

00:40.640 --> 00:43.840
It's going to be highly advised that you understand the basic syntax

00:43.840 --> 00:46.920
behind Python, as I'm not going to be explaining that throughout this course.

00:47.280 --> 00:50.320
Now, in terms of your instructor for this course, that is going to be me.

00:50.360 --> 00:51.280
My name is Tim.

00:51.280 --> 00:54.960
Some of you may know me as Tech with Tim from my YouTube channel, where I teach

00:54.960 --> 00:56.880
all kinds of different programming topics.

00:57.000 --> 00:59.720
And I've actually been working with Free Code Camp and posted some of my

00:59.720 --> 01:01.480
series on their channel as well.

01:01.880 --> 01:04.720
Now, let's get into the course breakdown and talk about exactly what you're

01:04.720 --> 01:06.880
going to learn and what you can expect from this course.

01:07.120 --> 01:09.920
So as this course is geared towards beginners and people just getting

01:09.920 --> 01:13.040
started in the machine learning and AI world, we're going to start by

01:13.040 --> 01:16.600
breaking down exactly what machine learning and artificial intelligence is.

01:16.720 --> 01:20.040
So talking about what the differences are between them, the different types

01:20.040 --> 01:23.760
of machine learning, reinforcement learning, for example, versus neural

01:23.760 --> 01:26.400
networks versus simple machine learning.

01:26.640 --> 01:28.480
We're going to go through all those different differences.

01:28.680 --> 01:31.720
And then we're going to get into a general introduction of TensorFlow.

01:32.280 --> 01:34.800
Now, for those of you that don't know, TensorFlow is a module

01:34.800 --> 01:38.440
developed and maintained by Google, which can be used within Python to do

01:38.440 --> 01:41.400
a ton of different scientific computing, machine learning and

01:41.400 --> 01:43.160
artificial intelligence applications.

01:43.160 --> 01:45.840
We're going to be working with that through the entire tutorial series.

01:46.080 --> 01:48.800
And after we do that general introduction to TensorFlow, we're going

01:48.800 --> 01:51.040
to get into our core learning algorithms.

01:51.360 --> 01:54.120
Now, these are the learning algorithms that you need to know before we can get

01:54.120 --> 01:55.640
further into machine learning.

01:55.880 --> 01:57.680
They build a really strong foundation.

01:57.840 --> 02:01.280
They're pretty easy to understand and implement, and they're extremely powerful.

02:01.680 --> 02:04.760
After we do that, we're going to get into neural networks, discuss all the

02:04.760 --> 02:07.920
different things that go into how neural networks work, how we can use them

02:07.920 --> 02:09.480
and then do a bunch of different examples.

02:09.760 --> 02:12.560
And then we're going to get into some more complex aspects of machine

02:12.560 --> 02:15.840
learning and artificial intelligence and get to convolutional neural networks,

02:15.840 --> 02:18.600
which can do things like image recognition and detection.

02:18.800 --> 02:20.960
And then we're going to get into recurrent neural networks, which are

02:20.960 --> 02:25.000
going to do things like natural language processing, chatbots, text

02:25.040 --> 02:27.080
processing, all those different kinds of things.

02:27.280 --> 02:29.760
And finally ended off with reinforcement learning.

02:30.040 --> 02:33.320
Now, in terms of resources for this course, there are a ton.

02:33.520 --> 02:36.720
And what we're going to be doing to make this really easy for you and for me

02:36.760 --> 02:39.120
is doing everything through Google Collaboratory.

02:39.280 --> 02:42.200
Now, if you haven't heard of Google Collaboratory, essentially it's a

02:42.200 --> 02:46.720
collaborative coding environment that runs an iPython notebook in the cloud

02:46.960 --> 02:50.560
on a Google machine where you can do all of your machine learning for free.

02:50.800 --> 02:52.600
So you don't need to install any packages.

02:52.600 --> 02:53.760
You don't need to use PIP.

02:53.960 --> 02:55.560
You don't need to get your environment set up.

02:55.640 --> 02:58.840
All you need to do is open a new Google Collaboratory window and you can

02:58.840 --> 02:59.920
start writing code.

03:00.120 --> 03:01.720
And that's what we're going to be doing in this series.

03:01.920 --> 03:04.880
If you look in the description right now, you will see links to all of the

03:04.880 --> 03:07.080
notebooks that I use throughout this guide.

03:07.200 --> 03:10.120
So if there's anything that you want to be cleared up, if you want the code

03:10.120 --> 03:13.320
for yourself, if you want just text based descriptions of the things that I'm

03:13.320 --> 03:15.960
saying, you can click those links and gain access to them.

03:16.160 --> 03:18.440
So with that being said, I'm very excited to get started.

03:18.440 --> 03:20.000
I hope you guys are as well.

03:20.200 --> 03:22.800
And let's go ahead and get into the content.

03:24.640 --> 03:29.760
So in this first section, I'm going to spend a few minutes discussing the

03:29.760 --> 03:34.280
difference between artificial intelligence, neural networks and machine learning.

03:34.560 --> 03:36.840
Now, the reason we need to go into this is because we're going to be covering

03:36.840 --> 03:38.560
all of these topics throughout this course.

03:38.760 --> 03:41.760
So it's vital that you guys understand what these actually mean.

03:41.760 --> 03:43.720
And you can kind of differentiate between them.

03:43.720 --> 03:45.240
So that's what we're going to focus on now.

03:45.520 --> 03:48.240
Now, quick disclaimer here, just so everyone's aware, I'm using something

03:48.240 --> 03:49.440
called Windows, Inc.

03:49.480 --> 03:51.200
This just default comes with Windows.

03:51.400 --> 03:52.880
I have a drawing tabled down here.

03:52.880 --> 03:55.840
And this is what I'm going to be using for some of the explanatory parts where

03:55.840 --> 04:00.000
there's no real coding, just to kind of illustrate some concepts and topics to you.

04:00.200 --> 04:02.200
Now, I have very horrible handwriting.

04:02.240 --> 04:03.920
I'm not artistic whatsoever.

04:03.920 --> 04:09.520
Programming is definitely more of my thing than drawing and doing diagrams and stuff.

04:09.760 --> 04:10.760
But I'm going to try my best.

04:10.760 --> 04:14.360
And this is just the way that I find I can convey information the best to you guys.

04:14.800 --> 04:19.280
So anyways, let's get started and discuss the first topic here, which is artificial intelligence.

04:19.640 --> 04:22.160
Now, artificial intelligence is a huge hype nowadays.

04:22.440 --> 04:25.640
And it's funny because a lot of people actually don't know what this means, or

04:25.640 --> 04:29.360
they try to tell people that what they've created is not artificial intelligence,

04:29.600 --> 04:31.320
when in reality, it actually is.

04:31.920 --> 04:35.480
Now, the kind of formal definition of AI, and I'm just going to read it off

04:35.480 --> 04:38.720
of my slide here to make sure that I'm not messing this up, is the effort

04:38.720 --> 04:42.400
to automate intellectual tasks normally performed by humans.

04:42.760 --> 04:45.120
Now, that's a fairly big definition, right?

04:45.160 --> 04:47.560
What is considered an intellectual task?

04:47.600 --> 04:50.760
And, you know, really, that doesn't help us too much.

04:50.760 --> 04:54.120
So what I'm going to do is bring us back to when AI was created, first created

04:54.280 --> 04:58.200
to kind of explain to you how AI has evolved and what it really started out being.

04:58.600 --> 05:02.640
So back in 1950, there was kind of the question being asked by scientists

05:02.640 --> 05:06.800
and researchers, can computers think, can we get them to figure things out?

05:06.800 --> 05:08.760
Can we get away from just hard coding?

05:09.000 --> 05:12.800
And, you know, having like, can we get a computer to think and it do its own thing?

05:13.440 --> 05:15.000
So that was kind of the question that was asked.

05:15.200 --> 05:19.000
And that's when the term artificial intelligence was kind of coined and created.

05:19.320 --> 05:23.400
Now, back then, AI was simply a predefined set of rules.

05:23.440 --> 05:27.720
So if you're thinking about an AI for maybe like tic-tac-toe or an AI for chess,

05:27.920 --> 05:32.040
all they would have had back then is predefined rules that humans had come up

05:32.040 --> 05:35.920
with and typed into the computer in code, and the computer would simply execute

05:35.920 --> 05:38.200
those set of rules and follow those instructions.

05:38.440 --> 05:41.960
So there was no deep learning, machine learning, crazy algorithms happening.

05:42.040 --> 05:45.480
It was simply if you wanted the computer to do something, you would have to tell

05:45.480 --> 05:49.280
it beforehand, say you're in this position and this happens, do this.

05:49.320 --> 05:50.640
And that's what AI was.

05:50.880 --> 05:55.400
And very good AI was simply just a very good set of rules or a ton of different

05:55.400 --> 05:58.200
rules that humans had implemented into some program.

05:58.440 --> 06:01.800
You can have AI programs that are stretching, you know, half a million lines

06:01.800 --> 06:05.400
of code, just with tons and tons and tons of different rules that have been

06:05.400 --> 06:06.960
created for that AI.

06:07.600 --> 06:12.360
So just be aware that AI does not necessarily mean anything crazy, complex

06:12.360 --> 06:14.080
or super complicated.

06:14.080 --> 06:17.760
But essentially, if you're trying to simulate some intellectual task, like

06:17.760 --> 06:21.680
playing a game that a human would do with a computer, that is considered AI.

06:21.840 --> 06:25.680
So even a very basic artificial intelligence for a tic-tac-toe game

06:25.680 --> 06:28.360
where it plays against you, that is still considered AI.

06:28.400 --> 06:31.360
And if we think of something like Pac-Man, right, where we have, you know,

06:31.360 --> 06:35.040
our little ghost, and this will be my rough sketch of a ghost, and we have our

06:35.040 --> 06:36.720
Pac-Man guy who will just be this.

06:37.080 --> 06:39.720
Well, would we consider this ghost AI?

06:40.320 --> 06:44.360
What it does is it attempts to find and kind of simulate how it would get

06:44.360 --> 06:45.560
to Pac-Man, right?

06:45.760 --> 06:49.120
And the way this works is just using a very basic path finding algorithm.

06:49.280 --> 06:52.520
This is nothing to do with deep learning or machine learning or anything crazy.

06:52.640 --> 06:55.040
But this is still considered artificial intelligence.

06:55.160 --> 06:58.840
The computer is figuring out how it can kind of play and do something

06:58.840 --> 07:00.360
by following an algorithm.

07:00.360 --> 07:04.200
So we don't necessarily need to have anything crazy, stupid, complex to be

07:04.200 --> 07:08.720
considered AI, it simply needs to just be simulating some intellectual human

07:08.720 --> 07:12.200
behavior. That's kind of the definition of artificial intelligence.

07:12.720 --> 07:16.640
Now, obviously today, AI has evolved into a much more complex field where we

07:16.640 --> 07:19.280
now have machine learning and deep learning and all these other techniques,

07:19.520 --> 07:20.920
which is what we're going to talk about now.

07:21.200 --> 07:24.000
So what I want to start by doing is just drawing a circle here.

07:24.480 --> 07:28.240
And I want to label this circle and say AI like that.

07:28.480 --> 07:31.440
So this is going to define AI because everything I'm going to put inside of

07:31.440 --> 07:33.600
here is considered artificial intelligence.

07:34.160 --> 07:36.280
So now let's get into machine learning.

07:36.760 --> 07:38.920
So what I'm going to do is draw another circle inside of here.

07:39.800 --> 07:43.240
And we're going to label this circle ML for machine learning.

07:43.600 --> 07:46.360
Now notice I put this inside of the artificial intelligence circle.

07:46.480 --> 07:50.440
This is because machine learning is a part of artificial intelligence.

07:51.000 --> 07:52.840
Now, what is machine learning?

07:53.320 --> 07:57.880
Well, what we talked about previously was kind of the idea that AI used to just

07:57.880 --> 08:00.040
be a predefined set of rules, right?

08:00.560 --> 08:04.800
Where what would happen is we would feed some data, we would go through the rules

08:04.800 --> 08:07.000
by and then analyze the data with the rules.

08:07.000 --> 08:09.760
And then we'd spit out some output, which would be, you know, what we're going to do.

08:10.080 --> 08:14.400
So in the classic example of chess, say we're in check, well, we pass that board

08:14.400 --> 08:18.080
information to the computer, it looks at its sets of rules, it determines we're in

08:18.080 --> 08:19.880
check, and then it moves us somewhere else.

08:20.280 --> 08:22.680
Now, what is machine learning in contrast to that?

08:23.000 --> 08:26.920
Well, machine learning is kind of the first field that's actually figuring out

08:26.920 --> 08:28.080
the rules for us.

08:28.400 --> 08:32.280
So rather than us hard coding the rules into the computer, what machine learning

08:32.320 --> 08:36.920
attempts to do is take the data and take what the output should be and figure

08:36.920 --> 08:38.040
out the rules for us.

08:38.240 --> 08:41.520
So you'll often hear that, you know, machine learning requires a lot of data

08:41.520 --> 08:46.520
and you need ton of examples and, you know, input data to really train a good

08:46.520 --> 08:49.880
model. Well, the reason for that is because the way that machine learning

08:49.880 --> 08:52.200
works is it generates the rules for us.

08:52.440 --> 08:55.880
We give it some input data, we give it what the output data should be.

08:56.120 --> 09:00.360
And then it looks at that information and figures out what rules can we generate

09:00.560 --> 09:04.760
so that when we look at new data, we can have the best possible output for that.

09:04.960 --> 09:08.440
Now, that's also why a lot of the times machine learning models do not have

09:08.440 --> 09:12.520
a hundred percent accuracy, which means that they may not necessarily get the

09:12.520 --> 09:14.400
correct answer every single time.

09:14.680 --> 09:18.240
And our goal when we create machine learning models is to raise our accuracy

09:18.240 --> 09:22.320
as high as possible, which means it's going to make the fewest mistakes possible.

09:22.360 --> 09:25.360
Because just like a human, you know, our machine learning models, which are

09:25.360 --> 09:28.800
trying to simulate, you know, human behavior can make mistakes.

09:28.920 --> 09:31.920
But to summarize that, essentially, machine learning, the difference

09:31.920 --> 09:36.600
between that and kind of, you know, algorithms and basic artificial intelligence

09:36.880 --> 09:41.000
is the fact that rather get that rather than us, the programmer giving it the

09:41.000 --> 09:43.720
rules, it figures out the rules for us.

09:43.880 --> 09:47.520
And we might not necessarily know explicitly what those rules are when we

09:47.520 --> 09:49.840
look at machine learning and create machine learning models.

09:50.120 --> 09:53.640
But we know that we're giving some input data, we're giving the expected

09:53.640 --> 09:57.640
output data, and then it looks at all of that information, does some algorithms,

09:57.680 --> 10:02.040
which we'll talk about later on that, and figures out the rules for us so that

10:02.040 --> 10:05.520
later when we give it some input data, and we don't know the output data, it

10:05.520 --> 10:08.640
can use those rules that it's figured out from our examples and all that

10:08.640 --> 10:11.280
training data that we gave it to generate some output.

10:12.000 --> 10:13.560
Okay, so that's machine learning.

10:13.960 --> 10:15.800
Now we've covered AI and machine learning.

10:16.040 --> 10:18.960
And now it's time to cover neural networks or deep learning.

10:19.480 --> 10:22.840
Now this circle gets to go right inside of the machine learning right here.

10:23.080 --> 10:26.400
I'm just going to label this one NN, which stands for neural networks.

10:26.840 --> 10:28.920
Now neural networks get a big hype.

10:28.960 --> 10:31.640
They're usually what the first, you know, when you get into machine learning,

10:31.640 --> 10:34.480
you want to learn neural networks, you're kind of like neural networks are

10:34.480 --> 10:35.920
cool, they're capable of a lot.

10:36.520 --> 10:38.160
But let's discuss what these really are.

10:38.400 --> 10:42.400
So the easiest way to define a neural network is it is a form of machine

10:42.400 --> 10:45.920
learning that uses a layered representation of data.

10:46.240 --> 10:48.960
Now we're not going to really understand this completely right now.

10:48.960 --> 10:52.520
But as we get further in that should start to make more sense as a definition.

10:53.000 --> 10:56.600
But what I need to kind of illustrate to you is that in the previous example,

10:56.600 --> 10:59.240
where we just talked about machine learning, essentially what we had is we

10:59.240 --> 11:01.640
had some input bubbles, which I'm going to define as these.

11:01.960 --> 11:04.640
We had some set of rules that is going to be in between here.

11:04.680 --> 11:05.840
And then we had some output.

11:06.160 --> 11:09.280
And what would happen is we feed this input to this set of rules.

11:10.080 --> 11:11.400
Something happens in here.

11:11.560 --> 11:12.720
And then we get some output.

11:13.040 --> 11:15.320
And then that is what, you know, our program does.

11:15.320 --> 11:16.680
So that's what we get from the model.

11:16.720 --> 11:18.600
We pretty much just have two layers.

11:18.640 --> 11:21.560
We have kind of the input layer, the output layer.

11:21.760 --> 11:24.720
And the rules are kind of just what connects those two layers together.

11:25.280 --> 11:30.600
Now in neural networks and what we call deep learning, we have more than two layers.

11:30.600 --> 11:33.560
Now I'm just trying to erase all this quickly so I can show you that.

11:33.880 --> 11:36.840
So let's say, and I'll draw this one another color, because why not?

11:36.880 --> 11:41.000
If we're talking about neural networks, what we might have, and this will vary.

11:41.000 --> 11:44.520
And I'll talk about this in a second, is the fact that we have an input layer,

11:44.520 --> 11:46.040
which will be our first layer of data.

11:46.200 --> 11:50.600
We could have some layers in between this layer that are all connected together.

11:51.160 --> 11:53.240
And then we could have some output layer.

11:53.520 --> 11:58.360
So essentially, what happens is our data is going to be transformed

11:58.360 --> 12:01.480
through different layers, and different things are going to happen.

12:01.480 --> 12:04.680
There's going to be different connections between these layers.

12:05.000 --> 12:07.080
And then eventually we'll reach an output.

12:07.280 --> 12:11.120
Now it's very difficult to explain neural networks without going completely in depth.

12:11.120 --> 12:13.160
So I'll cover a few more notes that I have here.

12:13.640 --> 12:16.840
Essentially, in neural networks, we just have multiple layers.

12:16.840 --> 12:18.320
That's kind of the way to think of them.

12:18.600 --> 12:22.160
And as we see machine learning, you guys should start to understand this more.

12:22.640 --> 12:25.480
But just understand that we're dealing with multiple layers.

12:25.480 --> 12:30.440
And a lot of people actually call this a multi stage information extraction process.

12:30.680 --> 12:32.040
Now, I did not come up with that term.

12:32.040 --> 12:33.720
I think that's from a book or something.

12:33.720 --> 12:37.600
But essentially what ends up happening is we have our data at this first layer,

12:37.600 --> 12:40.920
which is that input information, which we're going to be passing to the model

12:40.920 --> 12:42.440
that we're going to do something with.

12:42.440 --> 12:45.720
It then goes to another layer where it will be transformed.

12:45.720 --> 12:49.880
It will change into something else using a predefined kind of set of

12:50.520 --> 12:52.680
rules and weights that we'll talk about later.

12:53.080 --> 12:56.120
Then it will pass through all of these different layers where different

12:56.120 --> 12:59.280
kind of features of the data, which again, we'll discuss in a second,

12:59.600 --> 13:03.560
will be extracted, will be figured out, will be found until eventually

13:03.560 --> 13:06.560
we reach an output layer where we can kind of combine everything

13:06.560 --> 13:11.040
we've discovered about the data into some kind of output that's meaningful to our program.

13:11.480 --> 13:14.440
So that's kind of the best that I can do to explain neural networks

13:14.440 --> 13:16.160
without going on to a deeper level.

13:16.160 --> 13:19.200
I understand that a lot of you probably don't understand what they are right now.

13:19.200 --> 13:20.480
And that's totally fine.

13:20.480 --> 13:23.520
But just know that there are layered representation of data.

13:23.560 --> 13:27.840
We have multiple layers of information, whereas in standard machine learning,

13:27.840 --> 13:32.000
we only have, you know, one or two layers and an artificial intelligence.

13:32.000 --> 13:36.400
In general, we don't necessarily have to have like a predefined set of layers.

13:37.080 --> 13:40.320
OK, so that is pretty much it for neural networks.

13:40.320 --> 13:43.480
There's one last thing I will say about them is that they're actually not

13:43.480 --> 13:44.920
modeled after the brain.

13:44.920 --> 13:48.400
So a lot of people seem to think that neural networks are modeled after the brain

13:48.400 --> 13:52.880
and the fact that you have neurons firing in your brain, and that can relate to neural networks.

13:52.960 --> 13:57.000
Now, there is a biological inspiration for the name neural networks

13:57.000 --> 14:00.960
in the way that they work from, you know, human biology, but it is not

14:00.960 --> 14:03.320
necessarily modeled about the way that our brain works.

14:03.320 --> 14:07.160
And in fact, we actually don't really know how a lot of the things in our brain

14:07.160 --> 14:08.120
operate and work.

14:08.120 --> 14:11.040
So it would be impossible for us to say that neural networks are modeled

14:11.040 --> 14:15.160
after the brain, because we actually don't know how information is kind of

14:15.160 --> 14:18.880
happens and occurs and transfers through our brain, or at least we don't know

14:18.880 --> 14:22.280
enough to be able to say this is exactly what it is a neural network.

14:22.280 --> 14:24.520
So anyways, that was kind of the last point there.

14:24.520 --> 14:26.920
OK, so now we need to talk about data.

14:26.920 --> 14:30.520
Now, data is the most important part of machine learning and artificial

14:30.520 --> 14:32.400
intelligence neural networks as well.

14:32.400 --> 14:36.960
And it's very important that we understand how important data is and what

14:36.960 --> 14:39.800
the different kind of parts of it are, because they're going to be referenced

14:39.800 --> 14:42.480
a lot in any of the resources that we're using.

14:42.480 --> 14:45.320
Now, what I want to do is just create an example here where I'm going to make a

14:45.320 --> 14:50.600
data set that is about students final grades in like a school system.

14:50.600 --> 14:53.480
So essentially, we're going to make this a very easy example where all we're

14:53.480 --> 14:56.960
going to have for this data set is we're going to have information about students.

14:56.960 --> 15:01.360
So we're going to have their midterm one grade, their midterm two grade, and then

15:01.360 --> 15:02.760
we're going to have their final grade.

15:02.760 --> 15:06.920
So I'm just going to say midterm one.

15:06.920 --> 15:08.720
And again, excuse my handwriting here.

15:08.760 --> 15:11.680
It's not the easiest thing to write with this drawing tablet.

15:11.680 --> 15:13.920
And then I'll just do final.

15:13.920 --> 15:15.560
So this is going to be our data set.

15:15.560 --> 15:19.120
And we'll actually see some similar data sets to this as we go through and do

15:19.120 --> 15:20.760
some examples later on.

15:20.760 --> 15:25.080
So for student one, which we'll just put some students here, we're going to have

15:25.080 --> 15:28.800
their midterm one grade, maybe that's a 70, their midterm two grade, maybe that

15:28.800 --> 15:29.440
was an 80.

15:29.440 --> 15:33.760
And then let's say their final was like their final term grade, not just the

15:33.760 --> 15:35.320
mark on the final exam.

15:35.320 --> 15:37.480
Let's give them a 77.

15:37.520 --> 15:39.720
Now, for midterm one, we can give someone a 60.

15:39.720 --> 15:41.120
Maybe we give them a 90.

15:41.120 --> 15:45.840
And then we determined that the final grade on their exam was let's say an 84.

15:46.200 --> 15:48.440
And then we could do something with maybe a lower grade here.

15:48.440 --> 15:54.560
So 40, 50, and then maybe they got a 38 or something in the final grade.

15:54.880 --> 15:58.880
Now, obviously, we could have some other information here that we're omitting.

15:58.880 --> 16:01.880
Like maybe there was some exam, some assignments, whatever, some other things

16:01.880 --> 16:03.560
they did that contributed to their grade.

16:03.800 --> 16:07.840
But the problem that I want to consider here is the fact that given our midterm

16:07.840 --> 16:11.400
one grade and our midterm two grade and our final grade, how can I use this

16:11.400 --> 16:14.600
information to predict any one of these three columns?

16:14.840 --> 16:19.080
So if I were given a student's midterm one grade, and I were given a student's

16:19.160 --> 16:22.160
final grade, how could I predict their midterm two grade?

16:23.120 --> 16:26.560
So this is where we're going to talk about features and labels.

16:26.760 --> 16:30.680
Now, whatever information we have, that is the input information, which is the

16:30.680 --> 16:34.040
information we will always have that we need to give to the model to get

16:34.040 --> 16:36.320
some output is what we call our features.

16:36.480 --> 16:39.960
So in the example where we're trying to predict midterm two, and let's just do

16:39.960 --> 16:41.800
this and highlight this in red.

16:41.800 --> 16:46.600
So we understand what we would have as our features, our input information

16:46.600 --> 16:50.440
are going to be midterm one and final, because this is the information

16:50.440 --> 16:52.920
we are going to use to predict something.

16:52.960 --> 16:55.800
It is the input, it is what we need to give the model.

16:55.960 --> 16:59.440
And if we're training a model to look at midterm one and final grade, whenever

16:59.440 --> 17:02.800
we want to make a new prediction, we need to have that information to do so.

17:03.160 --> 17:04.720
Now, what's highlighted in red.

17:04.720 --> 17:09.320
So this midterm two here is what we would call the label or the output.

17:09.600 --> 17:13.800
Now, the label is simply what we are trying to look for or predict.

17:13.960 --> 17:18.200
So when we talk about features versus labels, features is our input information,

17:18.240 --> 17:21.480
the information that we have that we need to use to make a prediction.

17:21.760 --> 17:25.240
And our label is that output information that is just representing, you know,

17:25.240 --> 17:26.360
what we're looking for.

17:26.600 --> 17:31.160
So when we feed our features to a model, it will give to us a label.

17:31.280 --> 17:33.240
And that is kind of the point that we need to understand.

17:33.520 --> 17:34.760
So that was the basic here.

17:35.400 --> 17:38.240
And now I'm just going to talk a little bit more about data, because we will

17:38.240 --> 17:41.720
get into this more as we continue going and about the importance of it.

17:42.000 --> 17:46.120
So the reason why data is so important is this is kind of the key thing

17:46.120 --> 17:48.120
that we use to create models.

17:48.320 --> 17:52.320
So whenever we're doing AI and machine learning, we need data pretty much.

17:52.320 --> 17:55.600
Unless you're doing a very specific type of machine learning and artificial

17:55.640 --> 17:57.240
intelligence, which we'll talk about later.

17:57.760 --> 18:00.240
Now, for most of these models, we need tons of different data.

18:00.280 --> 18:01.960
We need tons of different examples.

18:02.160 --> 18:05.560
And that's because we know how machine learning works now, which is essentially

18:05.840 --> 18:08.960
we're trying to come up with rules for a data set.

18:09.160 --> 18:10.360
We have some input information.

18:10.360 --> 18:13.400
We have some output information or some features and some labels.

18:13.640 --> 18:16.560
We can give that to a model and tell it to start training.

18:16.680 --> 18:20.320
And what it will do is come up with rules such that we can just give some

18:20.320 --> 18:22.000
features to the model in the future.

18:22.160 --> 18:25.520
And then it should be able to give us a pretty good estimate of what the output

18:25.520 --> 18:30.240
should be. So when we're training, we have a set of training data.

18:30.440 --> 18:34.560
And that is data where we have all of the features and all of the labels.

18:34.560 --> 18:36.320
So we have all of this information.

18:36.800 --> 18:40.840
Then when we're going to test the model or use the model later on, we would not

18:40.840 --> 18:42.440
have this midterm to information.

18:42.440 --> 18:43.960
We wouldn't pass this to the model.

18:44.160 --> 18:47.440
We would just pass our features, which is midterm one and final.

18:47.440 --> 18:49.400
And then we would get the output of midterm two.

18:49.800 --> 18:50.960
So I hope that makes sense.

18:51.200 --> 18:53.400
That just means data is extremely important.

18:53.400 --> 18:57.360
If we're feeding incorrect data or data that we shouldn't be using to the model,

18:57.520 --> 18:59.840
that could definitely result in a lot of mistakes.

19:00.040 --> 19:03.160
And if we have incorrect output information or incorrect input

19:03.160 --> 19:06.080
information, that is going to cause a lot of mistakes as well, because that is

19:06.080 --> 19:10.000
essentially what the model is using to learn and to kind of develop and figure

19:10.000 --> 19:12.160
out what it's going to do with new input information.

19:12.520 --> 19:14.200
So that means that is enough of data.

19:14.240 --> 19:16.880
Now let's talk about the different types of machine learning.

19:17.360 --> 19:19.760
OK, so now that we've discussed the difference between artificial

19:19.760 --> 19:23.840
intelligence, machine learning and neural networks, we have a kind of decent idea

19:23.840 --> 19:27.000
about what data is in the difference between features and labels.

19:27.320 --> 19:31.400
It's time to talk about the different types of machine learning specifically,

19:31.760 --> 19:36.320
which are unsupervised learning, supervised learning and reinforcement learning.

19:36.600 --> 19:39.560
Now, these are just the different types of learning, the different types

19:39.560 --> 19:41.080
of figuring things out.

19:41.080 --> 19:44.520
Now, different kind of algorithms fit into these different categories

19:44.520 --> 19:48.440
from within artificial intelligence, within machine learning and within neural networks.

19:48.880 --> 19:51.760
So the first one we're going to talk about is supervised learning,

19:51.760 --> 19:53.680
which is kind of what we've already discussed.

19:53.920 --> 19:58.520
So I'll just write supervised up here again, excuse the handwriting.

19:59.200 --> 20:01.000
So supervised learning.

20:01.000 --> 20:02.360
Now, what is this?

20:02.360 --> 20:04.640
Well, supervised learning is kind of everything we've already learned,

20:04.640 --> 20:07.240
which is we have some features.

20:07.280 --> 20:09.760
So we'll write our features like this, right?

20:10.000 --> 20:14.200
We have some features and those features correspond to some label

20:14.200 --> 20:15.640
or potentially labels.

20:15.640 --> 20:17.760
Sometimes we might predict more than one information.

20:18.160 --> 20:21.840
So when we have this information, we have the features and we have the labels.

20:22.040 --> 20:25.360
What we do is we pass this information to some machine learning model.

20:25.600 --> 20:27.120
It figures out the rules for us.

20:27.120 --> 20:31.040
And then later on, all we need is the features and it will give us some labels

20:31.040 --> 20:32.120
using those rules.

20:32.120 --> 20:36.280
But essentially, what supervised learning is, is when we have both of this information.

20:36.680 --> 20:40.040
The reason it's called supervised is because what ends up happening

20:40.040 --> 20:44.200
when we train our machine learning model is we pass the input information.

20:44.520 --> 20:48.280
It makes some arbitrary prediction using the rules it already knows.

20:48.440 --> 20:51.640
And then it compares that prediction that it made to what the actual

20:51.640 --> 20:53.880
prediction is, which is this label.

20:54.160 --> 20:58.720
So we supervise the model and we say, OK, so you predicted that the color was red,

20:58.920 --> 21:01.720
but really the color of whatever we passed in should have been blue.

21:01.880 --> 21:05.240
So we need to tweak you just a little bit so that you get a little bit better

21:05.240 --> 21:06.960
and you move in the correct direction.

21:06.960 --> 21:08.840
And that's kind of the way that this works.

21:08.840 --> 21:11.680
For example, say we're predicting, you know, students final grade.

21:11.960 --> 21:16.840
Well, if we predict that the final grade is 76, but the actual grade is 77,

21:17.040 --> 21:19.160
we were pretty close, but we're not quite there.

21:19.320 --> 21:22.320
So we supervise the model and we say, hey, we're going to tweak you

21:22.320 --> 21:24.520
just a little bit, move you in the correct direction.

21:24.720 --> 21:26.360
And hopefully we get you to 77.

21:26.920 --> 21:29.520
And that is kind of the way to explain this, right?

21:29.520 --> 21:31.360
You have the features, you have the labels.

21:31.520 --> 21:35.080
When you pass the features, the model has some rules that it's already built.

21:35.120 --> 21:36.320
It makes a prediction.

21:36.480 --> 21:40.120
And then it compares that prediction to the label and then re tweaks the model

21:40.200 --> 21:44.920
and continues doing this with thousands upon thousands upon thousands of pieces

21:44.920 --> 21:48.520
of data, until eventually it gets so good that we can stop training it.

21:48.680 --> 21:50.600
And that is what supervised learning is.

21:50.800 --> 21:52.520
It's the most common type of learning.

21:52.520 --> 21:55.240
It's definitely the most applicable in a lot of instances.

21:55.440 --> 21:58.600
And most machine learning algorithms that are actually used

21:58.600 --> 22:00.560
use a form of supervised machine learning.

22:00.880 --> 22:04.240
A lot of people seem to think that this is, you know, a less complicated,

22:04.240 --> 22:07.800
less advanced way of doing things that is definitely not true.

22:07.840 --> 22:09.920
All of the different methods I'm going to tell you have different

22:09.920 --> 22:11.520
advantages and disadvantages.

22:11.720 --> 22:15.600
And this has a massive advantage when you have a ton of information

22:15.600 --> 22:18.360
and you have the output of that information as well.

22:18.560 --> 22:20.600
But sometimes we don't have the luxury of doing that.

22:20.600 --> 22:22.760
And that's where we talk about unsupervised learning.

22:23.120 --> 22:25.600
So hopefully that made sense for supervised learning.

22:26.080 --> 22:27.440
Tried my best to explain that.

22:27.640 --> 22:30.200
And now let's go into or sorry for supervised learning.

22:30.240 --> 22:32.520
Now let's go into unsupervised learning.

22:33.200 --> 22:36.040
So if we know the definition of supervised learning,

22:36.280 --> 22:39.400
we should hopefully be able to come up with a definition of unsupervised

22:39.400 --> 22:42.400
learning, which is when we only have features.

22:42.760 --> 22:47.760
So given a bunch of features like this and absolutely no labels,

22:47.760 --> 22:52.400
no output for these features, what we want to do is have the model

22:52.400 --> 22:54.320
come up with those labels for us.

22:54.520 --> 22:55.480
Now, this is kind of weird.

22:55.480 --> 22:57.440
You're kind of like, wait, how does that work?

22:57.440 --> 22:59.000
Why would we even want to do that?

22:59.240 --> 23:00.840
Well, let's take this for an example.

23:01.320 --> 23:04.360
We have some access, some axes of data.

23:04.360 --> 23:07.040
Okay, and we have like a two dimensional data point.

23:07.040 --> 23:10.600
So I'm just going to call this, let's say X and let's say Y.

23:10.800 --> 23:11.200
Okay.

23:11.600 --> 23:14.400
And I'm going to just put a bunch of dots on the screen that kind of

23:14.400 --> 23:17.360
represents like maybe a scatter plot of some of our different data.

23:18.360 --> 23:21.840
And I'm just going to put some dots specifically closer to other ones.

23:21.840 --> 23:24.640
Just so you guys kind of get the point of what we're trying to do here.

23:24.920 --> 23:26.360
So let's do that.

23:26.800 --> 23:27.000
Okay.

23:27.000 --> 23:30.520
So let's say I have this data set, this here is what we're working with.

23:30.560 --> 23:32.240
And we have these features.

23:32.600 --> 23:35.880
The features in this instance are going to be X and Y, right?

23:35.880 --> 23:38.320
So X and Y are my features.

23:38.520 --> 23:41.880
Now, we don't have any output specifically for these data points.

23:42.160 --> 23:45.520
What we actually want to do is we want to create some kind of model

23:46.160 --> 23:50.400
that can cluster these data points, which means figure out kind of, you know,

23:50.440 --> 23:54.600
unique groups of data and say, okay, so you're in group one, you're in group two,

23:54.800 --> 23:56.480
you're in group three, and you're in group four.

23:57.120 --> 24:01.360
We may not necessarily know how many groups we have, although sometimes we do.

24:01.920 --> 24:05.360
But what we want to do is just group them and kind of say, okay, we want to

24:05.360 --> 24:08.680
figure out which ones are similar and we want to combine those together.

24:09.000 --> 24:12.160
So hopefully what we would do with an unsupervised machine learning model

24:12.200 --> 24:16.000
is pass all of these features and then have the model create kind of these

24:16.000 --> 24:21.400
groupings. So like maybe this is a group, maybe this is a group, maybe this is a

24:21.400 --> 24:24.600
group, if we were having four groupings, and maybe if we had two groupings,

24:24.800 --> 24:27.360
we might get groupings that look something like this, right?

24:27.760 --> 24:32.760
And then when we pass a new data point in, that could, we could figure out what

24:32.760 --> 24:36.080
group that was a part of by determining, you know, which one it is closer to.

24:36.720 --> 24:38.560
Now, this is kind of a rough example.

24:38.560 --> 24:42.160
It's hard to again, explain all of these without going very in depth into

24:42.160 --> 24:46.000
the specific algorithms, but unsupervised machine learning or just learning

24:46.000 --> 24:49.800
in general is when you don't have some output information, you actually want

24:49.840 --> 24:52.080
the model to figure out the output for you.

24:52.320 --> 24:55.640
You don't really care how it gets there, you just want it to get there.

24:55.800 --> 24:59.560
And again, a good example is clustering data points, and we'll talk about some

24:59.560 --> 25:03.400
specific applications of when we might even want to use that later on, just

25:03.400 --> 25:06.880
understand you have the features, you don't have the labels, and you get the

25:06.920 --> 25:09.720
unsupervised model to kind of figure it out for you.

25:10.120 --> 25:13.640
Okay, so now our last type, which is very different than the two types I just

25:13.640 --> 25:15.760
explained is called reinforcement learning.

25:16.120 --> 25:19.080
Now personally, reinforcement learning, and I don't even know if I want to

25:19.080 --> 25:22.520
spell this because I feel like I'm going to mess it up.

25:23.240 --> 25:27.440
Reinforcement learning is the coolest type of machine learning, in my opinion.

25:27.480 --> 25:31.800
And this is when you actually don't have any data, you have what you call an

25:31.840 --> 25:34.640
agent, an environment and a reward.

25:34.880 --> 25:39.440
I'm going to explain this very briefly with a very, very, very simple example

25:39.440 --> 25:40.840
because it's hard to get too far.

25:41.160 --> 25:44.320
So let's say we have a very basic game, you know, maybe we made this game

25:44.320 --> 25:48.080
ourselves, and essentially, the objective of the game is to get to the flag.

25:48.320 --> 25:49.600
Okay, that's all it is.

25:49.600 --> 25:53.360
We have some ground, you can move left or right, and we want to get to this

25:53.360 --> 25:58.000
flag. Well, we want to train some artificial intelligence, some machine

25:58.000 --> 26:00.400
learning model that can figure out how to do this.

26:00.800 --> 26:03.640
So what we do is we call this our agent.

26:04.840 --> 26:06.640
We call this entire thing.

26:06.640 --> 26:09.160
So this whole thing here, the environment.

26:09.440 --> 26:11.200
So I guess I could write that here.

26:11.200 --> 26:15.600
So n by our meant think I spelled that correctly.

26:16.040 --> 26:17.800
And then we have something called a reward.

26:18.200 --> 26:21.640
And a reward is essentially what the agent gets when it does something

26:21.640 --> 26:25.520
correctly. So let's say the agent takes one step over this way.

26:25.520 --> 26:26.920
So let's say he's a new position is here.

26:26.920 --> 26:27.880
I just don't want to keep drawing him.

26:27.880 --> 26:28.760
So I'm just going to use a dot.

26:29.400 --> 26:31.640
Well, he got closer to the flag.

26:32.000 --> 26:35.400
So what I'm actually going to do is give him a plus two reward.

26:36.240 --> 26:38.240
So let's say he moves again closer to the flag.

26:38.880 --> 26:41.960
Maybe I give him now plus one this time he got even closer.

26:42.760 --> 26:45.800
And as he gets closer, I give him more and more reward.

26:46.640 --> 26:48.720
Now what happens if he moves backwards?

26:49.200 --> 26:51.800
So let's erase this and let's say that at some point in time,

26:52.000 --> 26:55.680
rather than moving closer to the threat, the flag, he moves backwards.

26:56.320 --> 26:58.280
Well, he might get a negative reward.

26:59.000 --> 27:02.520
Now, essentially, what the objective of this agent is to do

27:03.080 --> 27:05.080
is to maximize its reward.

27:05.080 --> 27:07.880
So if you give it a negative reward for moving backwards,

27:08.160 --> 27:09.560
it's going to remember that.

27:09.560 --> 27:13.080
And it's going to say, OK, at this position here, where I was standing,

27:13.080 --> 27:15.880
when I moved backwards, I got a negative reward.

27:16.280 --> 27:20.080
So if I get to this position again, I don't want to go backwards anymore.

27:20.400 --> 27:24.640
I want to go forwards because that should give me a positive reward.

27:25.240 --> 27:27.920
And the whole point of this is we have this agent

27:28.160 --> 27:31.840
that starts off with absolutely no idea, no kind of, you know,

27:31.880 --> 27:33.800
knowledge of the environment.

27:33.800 --> 27:36.080
And what it does is it starts exploring.

27:36.080 --> 27:39.160
And it's a mixture of randomly exploring and exploring

27:39.160 --> 27:41.720
using kind of some of the things that's figured out so far

27:41.880 --> 27:44.000
to try to maximize its reward.

27:44.240 --> 27:47.040
So eventually, when the agent gets to the flag,

27:47.120 --> 27:50.520
it will have the most the highest possible reward that it can have.

27:50.840 --> 27:54.480
And then next time that we plug this agent into the environment,

27:54.640 --> 27:56.920
it will know how to get to the flag immediately

27:56.920 --> 27:58.600
because it's kind of figured that out.

27:58.600 --> 28:00.840
It's determined that in all of these different positions,

28:00.840 --> 28:03.480
if I move here, this is the best place to move.

28:03.480 --> 28:05.680
So if I get in this position, move there.

28:06.080 --> 28:09.200
Now, this is, again, hard to explain without more detailed examples

28:09.200 --> 28:11.200
and going more mathematically and all of that.

28:11.200 --> 28:13.120
But essentially, just understand we have the agent,

28:13.120 --> 28:17.320
which is kind of what the thing is that's moving around in our environment.

28:17.600 --> 28:21.880
We have this environment, which is just what the agent can move around in.

28:22.080 --> 28:23.400
And then we have a reward.

28:23.400 --> 28:26.160
And the reward is what we need to figure out as the programmer,

28:26.160 --> 28:29.920
a way to reward the agent correctly so that it gets to the objective

28:30.240 --> 28:32.480
in the best possible way.

28:32.480 --> 28:35.040
But the agent simply maximizes that reward.

28:35.120 --> 28:37.760
So it just figures out where I need to go to maximize that reward.

28:37.880 --> 28:41.080
It starts at the beginning, kind of randomly exploring the environment

28:41.080 --> 28:44.120
because it doesn't know any of the rewards it gets at any of the positions.

28:44.360 --> 28:46.480
And then as it explores some more different areas,

28:46.480 --> 28:49.680
it kind of figures out the rules and the way that the environment works

28:49.880 --> 28:52.840
and then will determine how to reach the objective,

28:52.840 --> 28:54.560
which is whatever it is that it is.

28:54.560 --> 28:55.720
This is a very simple example.

28:55.720 --> 28:59.120
You could train a reinforcement model to do this and, you know, like half a second, right?

28:59.360 --> 29:01.400
But there is way more advanced examples

29:01.400 --> 29:03.880
and there's been examples of reinforcement learning,

29:04.080 --> 29:07.760
like of AI is pretty much figuring out how to play games together.

29:07.760 --> 29:09.200
How to it's it's actually pretty cool.

29:09.200 --> 29:11.280
Some of the stuff that reinforcement learning is doing.

29:11.480 --> 29:14.240
And it's a really awesome kind of advancement in the field,

29:14.240 --> 29:16.400
because it means we don't need all this data anymore.

29:16.600 --> 29:19.800
We can just get this to kind of figure out how to do things for us

29:19.800 --> 29:21.880
and explore the environment and learn on its own.

29:22.160 --> 29:23.760
Now, this can take a really long time.

29:23.760 --> 29:26.800
This can take a very short amount of time, really depends on the environment.

29:27.000 --> 29:30.280
But a real application of this is training AIs to play games,

29:30.280 --> 29:32.840
as you might be able to tell by kind of what I was explaining here.

29:33.120 --> 29:35.840
And yeah, so that is kind of the fundamental differences

29:35.840 --> 29:39.160
between supervised, unsupervised and reinforcement learning.

29:39.160 --> 29:41.960
We're going to cover all three of these topics throughout this course.

29:42.240 --> 29:45.560
And it's really interesting to see some of the applications we can actually do with this.

29:45.800 --> 29:49.240
So with that being said, I'm going to kind of end what I'm going to call module one,

29:49.240 --> 29:52.160
which is just a general overview of the different topics,

29:52.160 --> 29:54.640
some definitions and getting a fundamental knowledge.

29:54.920 --> 29:58.880
And in the next one, what we're going to be talking about is what TensorFlow is.

29:58.880 --> 30:01.800
We're going to get into coding a little bit and we're going to discuss

30:02.160 --> 30:04.640
some different aspects of TensorFlow and things we need to know

30:04.640 --> 30:06.680
to be able to move forward and do some more advanced things.

30:09.840 --> 30:12.560
So now in module two of this course, what we're going to be doing

30:12.560 --> 30:16.760
is getting a general introduction to TensorFlow, understanding what a tensor is,

30:16.960 --> 30:19.280
understanding shapes and data representation,

30:19.280 --> 30:23.000
and then how TensorFlow actually works on a bit of a lower level.

30:23.240 --> 30:26.280
This is very important because you can definitely go through and learn

30:26.280 --> 30:29.720
how to do machine learning without kind of gaining this information and knowledge.

30:29.960 --> 30:32.320
But it makes it a lot more difficult to tweak your models

30:32.320 --> 30:34.920
and really understand what's going on if you don't, you know,

30:34.920 --> 30:39.080
have that fundamental lower level knowledge of how TensorFlow actually works

30:39.080 --> 30:41.760
and operates. So that's exactly what we're going to cover here.

30:42.080 --> 30:44.320
Now, for those of you that don't know what TensorFlow is,

30:44.320 --> 30:47.400
essentially, this is an open source machine learning library.

30:47.560 --> 30:49.240
It's one of the largest ones in the world.

30:49.240 --> 30:53.360
It's one of the most well known and it's maintained and supported by Google.

30:53.720 --> 30:58.440
Now, TensorFlow essentially allows us to do and create machine learning models

30:58.440 --> 31:01.600
and neural networks and all of that without having to have a very complex

31:01.600 --> 31:05.800
math background. Now, as we get further in and we start discussing more in detail,

31:05.800 --> 31:08.840
how neural networks work in machine learning algorithms actually function,

31:09.080 --> 31:11.560
you'll realize there's a lot of math that goes into this.

31:11.920 --> 31:15.640
Now, it starts off being very kind of fundamental, like basic calculus

31:15.640 --> 31:19.360
and basic linear algebra, and then it gets much more advanced into things

31:19.360 --> 31:23.280
like gradient descent and some more regression techniques and classification.

31:23.680 --> 31:28.080
And essentially, you know, a lot of us don't know that and we don't really need to know that.

31:28.280 --> 31:30.560
So long as we have a basic understanding of it,

31:30.840 --> 31:34.560
then we can use the tools that TensorFlow provides for us to create models.

31:34.560 --> 31:36.280
And that's exactly what TensorFlow does.

31:36.640 --> 31:40.080
Now, what I'm in right now is what I call Google Collaboratory.

31:40.160 --> 31:42.080
I'm going to talk about this more in depth in a second.

31:42.080 --> 31:46.680
But what I've done for this whole course is I've transcribed very detailed

31:47.000 --> 31:49.440
everything that I'm going to be covering through each module.

31:49.600 --> 31:54.600
So this is kind of the transcription of module one, which is the introduction to TensorFlow.

31:54.840 --> 31:58.200
You can see it's not crazy long, but I wanted to do this so that any of you

31:58.240 --> 32:02.640
can follow along with kind of the text base and kind of my lecture notes.

32:02.640 --> 32:05.040
I almost want to call them as I go through the different content.

32:05.320 --> 32:08.720
So in the description, there will be links to all of these different notebooks.

32:08.760 --> 32:12.520
This is in something called Google Collaboratory, which again, we're going to discuss in a second.

32:12.800 --> 32:17.240
But you can see here that I have a bunch of text and it gets down to some different coding aspects.

32:17.480 --> 32:21.160
And what I'm going to be doing to make sure that I stay on track is simply following along

32:21.160 --> 32:24.480
through this, I might deviate slightly, I might go into some other examples.

32:24.720 --> 32:28.240
This will be kind of everything that I'm going to be covering through each module.

32:28.640 --> 32:31.720
So again, to follow along, click the link in the description.

32:32.280 --> 32:35.160
All right. So what can we do with TensorFlow?

32:35.440 --> 32:37.840
Well, these are some of the different things I've listed them here.

32:37.840 --> 32:42.320
So I don't forget we can do image classification, data clustering, regression,

32:42.640 --> 32:46.560
reinforcement learning, natural language processing, and pretty much anything

32:46.560 --> 32:48.640
that you can imagine with machine learning.

32:49.280 --> 32:52.760
Essentially, what TensorFlow does is gives us a library of tools

32:52.880 --> 32:56.800
that allow us to omit having to do these very complicated math operations.

32:57.280 --> 32:58.720
It just does them for us.

32:58.720 --> 33:01.680
Now, there is a bit that we need to know about them, but nothing too complex.

33:02.200 --> 33:05.160
Now, let's talk about how TensorFlow actually works.

33:05.640 --> 33:09.400
So TensorFlow has two main components that we need to understand

33:09.760 --> 33:12.840
to figure out how operations and math are actually performed.

33:13.080 --> 33:15.720
Now, we have something called graphs and sessions.

33:16.200 --> 33:21.880
Now, the way that TensorFlow works is it creates a graph of partial computations.

33:22.200 --> 33:24.200
Now, I know this is going to sound a little bit complicated.

33:24.200 --> 33:27.720
Some of you guys just try to kind of forget about the complex vocabulary

33:27.720 --> 33:32.200
and follow along. But essentially, what we do when we write code in TensorFlow

33:32.200 --> 33:34.040
is we create a graph.

33:34.040 --> 33:38.120
So if I were to create some variable, that variable gets added to the graph.

33:38.400 --> 33:42.680
And maybe that variable is the sum or the summation of two other variables.

33:43.000 --> 33:46.400
What the graph will define now is say, you know, we have variable one,

33:46.760 --> 33:51.000
which is equal to the sum of variable two and variable three.

33:51.560 --> 33:55.560
But what we need to understand is that it doesn't actually evaluate that.

33:55.560 --> 33:59.360
It simply states that that is the computation that we've defined.

33:59.760 --> 34:04.320
So it's almost like writing down an equation without actually performing any math.

34:04.560 --> 34:07.240
We kind of just, you know, have that equation there.

34:07.440 --> 34:10.840
We know that this is the value, but we haven't evaluated it.

34:10.840 --> 34:13.280
So we don't know that the value is like seven per se.

34:13.440 --> 34:17.200
We just know that it's the sum of, you know, vector one and vector two.

34:17.240 --> 34:20.840
Or it's the sum of this or it's the cross product or the dot product.

34:20.840 --> 34:24.080
We just define all of the different partial computations

34:24.320 --> 34:26.680
because we haven't evaluated those computation yet.

34:26.880 --> 34:28.840
And that is what is stored in the graph.

34:29.680 --> 34:32.400
Now, the reason it's called a graph is because different

34:32.400 --> 34:34.600
computations can be related to each other.

34:34.920 --> 34:38.720
For example, if I want to figure out the value of vector one,

34:38.920 --> 34:42.800
but vector one is equal to the value of vector three plus vector four,

34:43.000 --> 34:45.840
I need to determine the value of vector three and vector four

34:46.320 --> 34:48.240
before I can do that computation.

34:48.240 --> 34:49.800
So they're kind of linked together.

34:49.800 --> 34:51.840
And I hope that makes a little bit of sense.

34:52.440 --> 34:54.200
Now, what is a session?

34:54.200 --> 34:58.920
Well, session is essentially a way to execute part or the entire graph.

34:59.280 --> 35:03.840
So when we start a session, what we do is we start executing different aspects

35:03.840 --> 35:06.960
of the graph. So we start at the lowest level of the graph

35:06.960 --> 35:08.880
where nothing is dependent on anything else.

35:08.880 --> 35:12.040
We have maybe constant values or something like that.

35:12.280 --> 35:14.360
And then we move our way through the graph

35:14.360 --> 35:17.960
and start doing all of the different partial computations that we've defined.

35:18.520 --> 35:20.200
Now, I hope that this isn't too confusing.

35:20.200 --> 35:21.640
I know this is kind of a lot of lingo.

35:21.640 --> 35:23.640
You guys will understand this as we go through.

35:23.840 --> 35:26.280
And again, you can read through some of these components here

35:26.280 --> 35:29.000
that I have in collaboratory, if I'm kind of skipping through anything,

35:29.000 --> 35:31.080
or you don't truly understand.

35:31.320 --> 35:33.520
But that is the way that graphs and sessions work.

35:33.880 --> 35:35.480
We won't go too in depth with them.

35:35.480 --> 35:38.160
We do need to understand that that is the way TensorFlow works.

35:38.160 --> 35:42.480
And there's some times where we can't use a specific value in our code yet

35:42.680 --> 35:45.000
because we haven't evaluated the graph.

35:45.000 --> 35:47.640
We haven't created a session and gotten the values yet.

35:47.840 --> 35:50.200
Which we might need to do before we can actually, you know,

35:50.200 --> 35:51.800
use some specific value.

35:51.800 --> 35:53.800
So that's just something to consider.

35:53.800 --> 35:56.160
All right, so now we're actually going to get into coding,

35:56.160 --> 35:58.560
importing and installing TensorFlow.

35:58.880 --> 36:01.720
Now, this is where I'm going to introduce you to Google Collaboratory

36:01.720 --> 36:03.520
and explain how you guys can follow along

36:03.520 --> 36:06.360
without having to install anything on your computer.

36:06.520 --> 36:09.160
And it doesn't matter if you have like a really crappy computer

36:09.160 --> 36:12.560
or even if you're on like an iPhone per se, you can actually do this,

36:12.560 --> 36:13.760
which is amazing.

36:13.760 --> 36:17.120
So all you need to do is Google, Google Collaboratory

36:17.360 --> 36:19.600
and create a new notebook.

36:19.600 --> 36:22.720
Now, what Google Collaboratory is, is essentially a free

36:22.720 --> 36:25.040
Jupyter notebook in the cloud for you.

36:25.320 --> 36:27.920
The way this works is you can open up this notebook.

36:27.920 --> 36:30.480
You can see this is called I pi NB.

36:31.200 --> 36:32.120
I yeah, what is that?

36:32.120 --> 36:35.400
I pi NB, which I think just stands for I Python notebook.

36:35.680 --> 36:39.760
And what you can do in here is actually write code and write text as well.

36:40.040 --> 36:43.760
So this in here is what it's called, you know, Google Collaboratory Notebook.

36:43.960 --> 36:45.920
And essentially why it's called a notebook

36:45.920 --> 36:49.320
is because not only can you put code, but you can also put notes,

36:49.320 --> 36:52.400
which is what I've done here with these specific titles.

36:52.680 --> 36:55.280
So you can actually use Markdown inside of this.

36:55.280 --> 36:58.720
So if I open up one of these, you can see that I've used Markdown text

36:59.280 --> 37:01.280
to actually kind of create these sections.

37:01.640 --> 37:04.760
And yeah, that is kind of how Collaboratory works.

37:05.040 --> 37:08.680
But what you can do in Collaboratory is forget about having to install

37:08.680 --> 37:11.840
all of these modules, they're already installed for you.

37:12.080 --> 37:15.400
So what you're actually going to do when you open a Collaboratory window

37:15.400 --> 37:18.680
is Google is going to automatically connect you to one of their servers

37:18.680 --> 37:22.680
or one of their machines that has all of this stuff done and set up for you.

37:22.880 --> 37:26.200
And you can start writing code and executing it off their machine

37:26.200 --> 37:27.800
and seeing the result.

37:27.800 --> 37:31.760
So for example, if I want to print hello like this,

37:31.760 --> 37:33.960
and I'll zoom in a little bit so you guys can read this.

37:33.960 --> 37:37.880
All I do is I create a new code block, which I can do by clicking code.

37:38.480 --> 37:40.800
Like that, I can delete one like that as well.

37:40.800 --> 37:42.920
And I hit run.

37:42.920 --> 37:46.040
Now notice, give it a second, it does take longer than typically on your own

37:46.040 --> 37:48.720
machine, and we get hello popping up here.

37:49.000 --> 37:52.360
So the great thing about Collaboratory is the fact that we can have multiple

37:52.360 --> 37:55.760
code blocks and we can run them in whatever sequence we want.

37:56.040 --> 37:58.680
So to create another code block, you can just, you know, do another

37:58.680 --> 38:01.840
code block from up here or by just by looking down here, you get code

38:01.840 --> 38:04.800
and you get text and I can run this in whatever order I want.

38:04.800 --> 38:06.840
So I could do like print.

38:06.840 --> 38:10.680
Yes, for example, I could run yes, and we'll see the output of yes.

38:10.680 --> 38:14.040
And then I could print hello one more time and notice that it's showing me

38:14.040 --> 38:18.040
the number on this left hand side here on which these kind of code blocks were

38:18.040 --> 38:22.040
run. Now, all of these code blocks can kind of access each other.

38:22.040 --> 38:26.320
So for example, I do define funk and we'll just take some parameter H.

38:26.360 --> 38:30.880
And all we'll do is just print H. Well, if I create another code block down here,

38:30.880 --> 38:37.080
so let's go code, I can call funk with say, hello, make sure I run this block

38:37.080 --> 38:41.240
first, so we define the function. Now I'll run funk and notice we get the output

38:41.240 --> 38:44.840
hello, so we can access all of the variables, all the functions, anything

38:44.840 --> 38:48.240
we've defined in other code blocks from code blocks that are below it or code

38:48.240 --> 38:51.600
blocks that have executed after it. Now, another thing that's great about

38:51.600 --> 38:54.880
collaboratory is the fact that we can import pretty much any module we can

38:54.880 --> 38:58.360
imagine, and we don't need to install it. So I'm not actually going to be going

38:58.360 --> 39:02.600
through how to install TensorFlow completely. There is a little bit on how

39:02.600 --> 39:06.320
to install TensorFlow on your local machine inside of this notebook, which

39:06.360 --> 39:09.280
I'll refer you to. But essentially, if you know how to use pip, it's pretty

39:09.280 --> 39:13.000
straightforward, you can pip install TensorFlow, or pip install TensorFlow

39:13.000 --> 39:16.720
GPU, if you have a compatible GPU, which you can check from the link that's in

39:16.720 --> 39:21.120
this notebook. Now, if I want to import something, what I can do is literally

39:21.120 --> 39:24.800
just write the import. So I can say import numpy like this. And usually numpy

39:24.800 --> 39:28.320
is a module that you need to install. But we don't need to do that here. It's

39:28.360 --> 39:31.240
already installed on the machine. So again, we hook up to those Google

39:31.240 --> 39:35.200
servers, we can use their hardware to perform machine learning. And this is

39:35.200 --> 39:38.680
awesome. This is amazing. And it gives you performance benefits when you're

39:38.680 --> 39:42.240
running on like a lower kind of crappier machine, right? So we can have a look

39:42.240 --> 39:45.680
at the RAM in the disk space of our computer, we can see we have 12 gigs of

39:45.680 --> 39:50.080
RAM, we're dealing with 107 gigabytes of data on our disk space. And we can

39:50.080 --> 39:54.080
obviously, you know, look at that if we want, we can connect to our local

39:54.080 --> 39:56.960
runtime, which I believe connects to your local machine. But I'm not going to go

39:56.960 --> 40:00.080
through all of that. I just want to show you guys some basic components of

40:00.080 --> 40:03.680
collaboratory. Now, some other things that are important to understand is this

40:03.680 --> 40:08.840
runtime tab, which you might see me use. So restart runtime essentially clears

40:08.840 --> 40:12.800
all of your output, and just restarts whatever's happened. Because the great

40:12.800 --> 40:16.640
thing with collaboratory is since I can run specific code blocks, I don't need

40:16.640 --> 40:21.080
to execute the entire thing of code every time I want to run something. If I've

40:21.080 --> 40:25.520
just made a minor change in one code block, I can just run that code. Sorry, I

40:25.520 --> 40:29.160
can just run that code block. I don't need to run everything before it or even

40:29.160 --> 40:33.200
everything after it, right? But sometimes you want to restart everything and just

40:33.240 --> 40:37.440
rerun everything. So to do that, you click restart runtime, that's just going to

40:37.440 --> 40:41.480
clear everything you have. And then restart and run all will restart the

40:41.480 --> 40:46.480
runtime as well as run every single block of code you have in sequential order in

40:46.480 --> 40:50.200
which it shows up in the thing. So I recommend you guys open up one of these

40:50.200 --> 40:53.480
windows. You can obviously follow along with this notebook if you want. But if

40:53.480 --> 40:56.880
you want to type it out on your own and kind of mess with it, open up a notebook,

40:57.120 --> 41:01.040
save it. It's very easy. And these are again, extremely similar to Jupiter

41:01.040 --> 41:06.160
notebooks, Jupiter notebooks, they're pretty much the same. Okay, so that is

41:06.160 --> 41:11.200
kind of the Google Collaboratory aspect how to use that. Let's get into importing

41:11.200 --> 41:15.160
TensorFlow. Now this is going to be kind of specific to Google Collaboratory. So

41:15.160 --> 41:18.320
you can see here, these are kind of the steps we need to follow to import

41:18.320 --> 41:22.400
TensorFlow. So since we're working in Google Collaboratory, they have

41:22.400 --> 41:25.520
multiple versions of TensorFlow, they have the original version of TensorFlow,

41:25.520 --> 41:30.360
which is 1.0, and the 2.0 version. Now to define the fact that we want to use

41:30.360 --> 41:34.840
TensorFlow 2.0, just because we're in this notebook, we need to write this line

41:34.840 --> 41:39.440
of code at the very beginning of all of our notebooks. So percent TensorFlow

41:39.440 --> 41:43.800
underscore version 2.x. Now this is simply just saying we need to use

41:43.800 --> 41:48.000
TensorFlow 2.x. So whatever version that is, and this is only required in a

41:48.000 --> 41:51.240
notebook, if you're doing this on your local machine in a text editor, you're

41:51.240 --> 41:55.080
not going to need to write this. Now once we do that, we typically import

41:55.120 --> 42:00.000
TensorFlow as an alias name of TF. Now to do that, we simply import the

42:00.000 --> 42:04.040
TensorFlow module, and then we write as TF. If you're on your local machine,

42:04.040 --> 42:07.280
again, you're going to need to install TensorFlow first to make sure that

42:07.280 --> 42:10.040
you're able to do this. But since we're in Collaboratory, we don't need to do

42:10.040 --> 42:14.600
that. Now, since we've defined the fact we're using version 2.x, when we

42:14.600 --> 42:19.520
print the TensorFlow version, we can see here that it says version two, which is

42:19.520 --> 42:23.680
exactly what we're looking for. And then it says TensorFlow 2.1.0. So make

42:23.680 --> 42:27.320
sure that you print your version, you're using version 2.0, because there is a

42:27.320 --> 42:31.640
lot of what I'm using in this series that is kind of, if you're in TensorFlow

42:31.640 --> 42:35.440
1.0, it's not going to work. So it's new in TensorFlow 2.0, or it's been

42:35.440 --> 42:39.160
refactored and the names have been changed. Okay, so now that we've done

42:39.160 --> 42:42.280
that, we've imported TensorFlow, we've got this here, and I'm actually going to

42:42.280 --> 42:45.600
go to my fresh notebook and just do this. So we'll just copy these lines over

42:45.600 --> 42:49.000
just so we have some fresh code, and I don't have all this text that we have to

42:49.040 --> 42:55.200
deal with. So let's do this TensorFlow, let's import TensorFlow as TF, and then

42:55.200 --> 43:01.840
we can print the TF dot version and have a look at that. So version. Okay, so

43:01.840 --> 43:05.160
let's run our code here, we can see TensorFlow is already loaded. Oh, it says

43:05.160 --> 43:08.680
1.0. So if you get this error, it's actually good, I ran into this where

43:08.680 --> 43:11.920
TensorFlow has already been loaded. All you need to do is just restart your

43:11.920 --> 43:15.560
runtime. So I'm going to restart and run all just click Yes. And now we should

43:15.600 --> 43:19.880
see that we get that version 2.0. Once this starts running, give it a second

43:19.880 --> 43:25.200
TensorFlow 2.0 selected, we're going to import that module. And there we go, we

43:25.200 --> 43:30.800
have version 2.0. Okay, so now it's time to talk about tensors. Now, what is a

43:30.840 --> 43:34.400
tensor? Now, tensor just immediately seems kind of like a complicated name, you're

43:34.400 --> 43:38.840
like, All right, tensor, like this is confusing. But what is it? Well, obviously

43:38.840 --> 43:42.600
this is going to be a primary aspect of TensorFlow, considering the name

43:42.600 --> 43:47.720
similarities. And essentially, all it is is a vector generalized to higher

43:47.720 --> 43:52.120
dimensions. Now, what is a vector? Well, if you've ever done any linear algebra

43:52.120 --> 43:55.480
or even some basic kind of vector calculus, you should hopefully know what

43:55.480 --> 44:00.000
that is. But essentially, it is kind of a data point is kind of the way that I

44:00.000 --> 44:03.920
like to describe it. And the reason we call it a vector is because it doesn't

44:03.920 --> 44:08.760
necessarily have a certain coordinate. So like if you're talking about a two

44:08.760 --> 44:12.840
dimensional data point, you have, you know, maybe an x and a y value, or like an

44:12.840 --> 44:18.000
x one value and an x two value. Now a vector can have any amount of dimensions

44:18.040 --> 44:22.000
in it, it could have one dimension, which simply means it's just one number, could

44:22.000 --> 44:25.480
have two dimensions, which means we're having two numbers. So like an x and a

44:25.480 --> 44:29.600
y value, if we're thinking about a two dimensional graph, we'd have three

44:29.600 --> 44:32.960
dimensions, if we're thinking about a three dimensional graph, so that would be

44:32.960 --> 44:36.440
three data points, we could have four dimensions, if we're talking about

44:36.480 --> 44:40.360
sometimes some image data and some video data, five dimensions, and we can

44:40.360 --> 44:45.120
keep going, going, going with vectors. So essentially, what a tensor is, and I'll

44:45.120 --> 44:48.160
just read this formal definition to make sure I haven't butchered anything

44:48.160 --> 44:52.160
that's from the actual TensorFlow website. A tensor is a generalization of

44:52.160 --> 44:56.480
vectors and matrices to potentially higher dimensions, internally TensorFlow

44:56.480 --> 45:00.400
represents tensors as n dimensional arrays of base data types. Now we'll

45:00.400 --> 45:05.400
understand what that means in a second, but hopefully that makes sense. Now, since

45:05.400 --> 45:09.120
tensors are so important to TensorFlow, they're kind of the main object that

45:09.120 --> 45:13.040
we're going to be working with, manipulating and viewing. And it's the main

45:13.040 --> 45:17.520
object that's passed around through our program. Now, what we can see here is

45:17.520 --> 45:21.480
each tensor represents a partially defined computation that will eventually

45:21.480 --> 45:25.880
produce a value. So just like we talked about in the graphs and sessions, what

45:25.880 --> 45:29.280
we're going to do is when we create our program, we're going to be creating a

45:29.280 --> 45:32.520
bunch of tensors and TensorFlow is going to be creating them as well. And those

45:32.520 --> 45:37.640
are going to store partially defined computations in the graph. Later, when we

45:37.640 --> 45:41.520
actually build the graph and have the session running, we will run different

45:41.520 --> 45:44.800
parts of the graph, which means we'll execute different tensors, and be able

45:44.800 --> 45:48.720
to get different results from our tensors. Now each tensor has what we call a

45:48.720 --> 45:53.640
data type and a shape, and that's we're going to get into now. So a data type is

45:53.640 --> 45:57.680
simply what kind of information is stored in the tensor. Now it's very rare that

45:57.680 --> 46:01.440
we see any data types different than numbers, although there is the data type

46:01.480 --> 46:04.560
of strings and a few others as well. But I haven't included all of them here

46:04.560 --> 46:08.840
because they're not that important. But some examples we can see our float 32 in

46:08.840 --> 46:14.720
32 string and others. Now the shape is simply the representation of the

46:14.720 --> 46:18.600
tensor in terms of what dimension it is. And we'll get some examples because I

46:18.600 --> 46:21.600
don't want to explain the shape until we can see some examples to really dial

46:21.600 --> 46:26.360
in. But here are some examples of how we would create different tensors. So what

46:26.360 --> 46:32.560
you can do is you can simply do TF dot variable. And then you can do the value

46:32.560 --> 46:36.920
and the data type that your tensor is. So in this case, we've created a string

46:36.920 --> 46:41.960
tensor which stores one string. And it is TF dot strings, we define the data type

46:41.960 --> 46:47.360
second, we have a number tensor which stores some integer value. And then that

46:47.360 --> 46:52.480
is up type TF int 16. And we have a floating point tensor, which stores a

46:52.520 --> 46:58.240
simple floating point. Now these tensors have a shape of I believe it's going to

46:58.240 --> 47:03.120
be one, which simply means they are a scalar. Now a scalar value and you might

47:03.120 --> 47:08.160
hear me say this a lot simply means just one value. That's all it means. When we

47:08.160 --> 47:12.720
talk about like vector values, that typically means more than one value. And

47:12.720 --> 47:16.840
we talk about matrices, we're having different it just it goes up but scalar

47:16.840 --> 47:22.240
simply means one number. So yeah, that is what we get for the different data

47:22.280 --> 47:25.320
types and creating tensors, we're not really going to do this very much in our

47:25.320 --> 47:29.440
program. But just for some examples here, that's how we do it. So we've imported

47:29.440 --> 47:31.960
them. So I can actually run these. And I mean, we're not going to really get any

47:31.960 --> 47:36.280
output by running this code because well, there's nothing to see. But now we're

47:36.280 --> 47:41.040
going to talk about the rank slash degree of tensors. So another word for rank is

47:41.040 --> 47:45.440
agree. So these are interchangeably. And again, this simply means the the number

47:45.440 --> 47:50.840
of dimensions involved in the tensor. So when we create a tensor of rank zero,

47:50.880 --> 47:54.800
which is what we've done up here, we call that a scalar. Now the reason this has

47:54.800 --> 47:59.760
rank zero is because it's simply one thing, we don't have any dimensions to

47:59.760 --> 48:04.320
this, there's like zero dimensionality of that. It was even a word, it's just one

48:04.320 --> 48:10.400
value. Whereas here, we have an array. Now when we have an array or a list, we

48:10.400 --> 48:15.360
immediately have at least rank one. Now the reason for that is because this

48:15.360 --> 48:19.120
array can store more than one value in one dimension, right? So I can do

48:19.120 --> 48:24.760
something like test, I could do okay, I could do Tim, which is my name, and we

48:24.760 --> 48:28.280
can run this and we're not going to get any output obviously here. But this is

48:28.280 --> 48:34.080
what we would call a rank one tensor, because it is simply one list, one array,

48:34.280 --> 48:38.840
which means one dimension. And again, you know, that's also like a vector. Now

48:38.840 --> 48:43.280
this, what we're looking at here is a rank to tensor. The reason this is a rank

48:43.320 --> 48:47.680
to tensor is because we have a list inside of a list, or in this case,

48:47.720 --> 48:52.000
multiple lists inside of a list. So the way that you can actually determine the

48:52.000 --> 48:57.760
rank of a tensor is the deepest level of a nested list, at least in Python with

48:57.760 --> 49:02.600
our representation, that's what that is. So here we can see we have a list inside

49:02.600 --> 49:06.320
of a list, and then another list inside of this upper list. So this would give us

49:06.480 --> 49:11.240
rank two. And this is what we typically call a matrices. And this again, is going

49:11.240 --> 49:16.560
to be of TF dot strings. So that's the data type for this tensor variable. So all

49:16.600 --> 49:20.120
of these that we've created are tensors, they have a data type, and they have some

49:20.120 --> 49:23.880
rank and some shape, and we're going to talk about the shape in a second. So to

49:23.880 --> 49:28.840
determine the rank of a tensor, we can simply use the method TF dot rank. So

49:28.840 --> 49:34.160
notice when I run this, we get the shape which is blank of rank to tensor. That's

49:34.160 --> 49:39.560
fine. And then we get num pi two, which simply means that this is of rank two. Now

49:39.560 --> 49:44.160
if I go for that rank one tensor, and I print this out. So let's have a look at

49:44.160 --> 49:48.840
it, we get num pi one here, which is telling us that this is simply of rank

49:48.920 --> 49:52.320
one. Now if I want to use one of these ones up here and see what it is, so let's

49:52.320 --> 49:57.320
try it, we can do numbers. So TF dot ring numbers. So we'll print that here. And

49:57.320 --> 50:00.680
we get num pi zero, because that's rank zero, right? So we'll go back to what we

50:00.680 --> 50:03.920
had, which was ranked to tensor. But again, those are kind of the examples we

50:03.920 --> 50:08.080
want to look at. Okay, so shapes of a tensor. So this is a little bit different

50:08.080 --> 50:13.520
now. What a shape simply tells us is how many items we have in each dimension. So

50:13.520 --> 50:18.720
in this case, when we're looking at rank two, tensor dot shape, so we have dot

50:18.720 --> 50:22.960
shape here, that's an attribute of all of our tensors, we get two two. Now let's

50:22.960 --> 50:28.400
look up here. What we have is Whoa, look at this two, and two. So we have two

50:28.400 --> 50:31.120
elements in the first dimension, right, and then two elements in the second

50:31.120 --> 50:35.200
dimension. That's pretty much what this is telling us. Now let's look at the rank

50:35.480 --> 50:40.960
for the shape of rank one tensor, we get three. So because we only have a rank

50:40.960 --> 50:46.560
one, notice we only get one number. Whereas when we had rank two, we got two

50:46.560 --> 50:49.880
numbers, and it told us how many elements were in each of these lists, right? So if

50:49.880 --> 50:54.480
I go and I add another one here, like that, and we have a look now at the shape.

50:55.080 --> 51:00.400
Oops, I got to run this first. So that's something can convert non square to

51:00.440 --> 51:04.320
tensor. Ah, sorry, so I need to have a uniform amount of elements in each one

51:04.320 --> 51:08.560
here, I can't just do what I did there. So add a third element here. Now what we

51:08.560 --> 51:14.120
can do is run this shouldn't get any issues. Let's have a look at the shape and

51:14.120 --> 51:19.880
notice we get now two three. So we have two lists, and each of those lists have

51:19.880 --> 51:24.000
three elements inside of them. So that's how the shape works. Now I could go ahead

51:24.040 --> 51:29.600
and add another list in here if I wanted to and I could say like, okay, okay,

51:30.960 --> 51:35.400
okay, so let's run this hopefully no errors. Looks like we're good. Now let's

51:35.400 --> 51:38.400
look at the shape again. And now we get a shape of three, three, because we have

51:38.440 --> 51:42.640
three interior lists. And in each of those lists, we have three elements. And

51:42.640 --> 51:46.880
that is pretty much how that works. Now again, we could go even further here and

51:46.880 --> 51:50.760
we could put another list inside of here that would give us a rank three tensor.

51:50.760 --> 51:54.520
And we'd have to do that inside of all of these lists. And then what that would

51:54.520 --> 51:58.680
give us now would be three numbers representing how many elements we have in

51:58.720 --> 52:04.600
each of those different dimensions. Okay, so changing shape. Alright, so this is

52:04.640 --> 52:08.000
what we need to do a lot of times when we're dealing with tensors and tensor

52:08.000 --> 52:12.360
flow. So essentially, there is many different shapes that can represent the

52:12.360 --> 52:18.160
same number of elements. So up here, we have three elements in a rank one

52:18.160 --> 52:23.400
tensor. And then here we have nine elements in a rank two tensor. Now there's

52:23.400 --> 52:27.600
ways that we can reshape this data so that we have the same amount of

52:27.600 --> 52:31.840
elements, but in a different shape. For example, I could flatten this, right,

52:31.840 --> 52:36.400
take all of these elements and throw them into a rank one tensor that simply is

52:36.400 --> 52:40.840
a length of nine elements. So how do we do that? Well, let me just run this code

52:40.840 --> 52:43.520
for us here and have a look at this. So what we've done is we've created tensor

52:43.520 --> 52:48.000
one, that is TF dot ones, what this stands for is we're going to create a

52:48.000 --> 52:54.880
tensor that simply is populated completely with ones of this shape. So shape one,

52:54.880 --> 52:58.560
two, three, which means, you know, that's the shape we're going to get. So let's

52:58.560 --> 53:03.520
print this out and look at tensor one, just so I can better illustrate this. So

53:03.520 --> 53:09.640
tensor one, look at the shape that we have one, two, three, right? So we have one

53:09.640 --> 53:13.440
interior list, which we're looking at here. And then we have two lists inside

53:13.440 --> 53:17.200
of that list. And then each of those lists, we have three elements. So that's

53:17.200 --> 53:21.800
the shape we just defined. Now we have six elements inside of here. So there

53:21.800 --> 53:25.680
must be a way that we can reshape this data to have six elements, but in a

53:25.720 --> 53:30.560
different shape. In fact, what we can do is reshape this into a two, three, one

53:30.560 --> 53:34.200
shape, where we're going to have two lists, right? We're going to have three

53:34.200 --> 53:36.800
inside of those. And then inside of each of those, we're going to have one

53:36.800 --> 53:40.600
element. So let's have a look at that one. So let's have a look at tensor two.

53:40.600 --> 53:43.360
Actually, what am I doing? We print all we can print all of them here. So let's

53:43.360 --> 53:46.680
just print them and have a look at them. So when we look at tensor one, we saw

53:46.680 --> 53:50.840
this was a shape. And now we look at this tensor two. And we can see that we

53:50.840 --> 53:55.240
have two lists, right? Inside of each of those lists, we have three lists. And

53:55.240 --> 53:59.720
inside of each of those lists, we have one element. Now, finally, our tensor

53:59.720 --> 54:05.160
three is a shape of three negative one. Well, what is negative one? When we put

54:05.160 --> 54:09.840
negative one here, what this does is infer what this number actually needs to

54:09.840 --> 54:14.880
be. So if we define an initial shape of three, what this does is say, Okay, we're

54:14.880 --> 54:19.640
going to have three lists. That's our first level. And then we need to figure

54:19.640 --> 54:23.240
out based on how many elements we have in this reshape, which is the method we're

54:23.240 --> 54:26.440
using, which I didn't even talk about, which we'll go into a second, what this

54:26.440 --> 54:30.840
next dimension should be. Now, obviously, this is going to need to be three. So three

54:30.840 --> 54:34.240
three, right, because we're going to have three lists inside of each of those lists

54:34.240 --> 54:37.000
we need to have. Or actually, is that correct? Let's see if that's even the

54:37.000 --> 54:41.400
shape, three, two, my bad. So this actually needs to change to three, two, I

54:41.400 --> 54:45.000
don't know why I wrote three, three there. But you get the point, right? So what

54:45.000 --> 54:48.160
this does is we have three lists, we have six elements, this number obviously needs

54:48.200 --> 54:51.720
to be two, because well, three times two is going to give us six. And that is

54:51.720 --> 54:55.360
essentially how you can determine how many elements are actually in a tensor by

54:55.360 --> 54:59.600
just looking at its shape. Now, this is the reshape method, where all we need to

54:59.600 --> 55:03.680
do is call tf dot reshape, give the tensor and give the shape we want to change

55:03.680 --> 55:08.120
it to. So long as that's a valid shape. And when we multiply all of the numbers

55:08.120 --> 55:11.920
in here, it's equal to the number of elements in this tensor that will reshape

55:11.920 --> 55:16.280
it for us and give us that new shaped data. This is very useful. We'll use this

55:16.320 --> 55:19.680
actually a lot as we go through TensorFlow. So make sure you're kind of

55:19.680 --> 55:23.880
familiar with how that works. All right. So now we're moving on to types of

55:23.920 --> 55:28.680
tensors. So there is a bunch of different types of tensors that we can use. So

55:28.680 --> 55:33.400
far, the only one we've looked at is variable. So we've created tf dot

55:33.400 --> 55:36.680
variables and kind of just hard coded our own tensors. We're not really going to

55:36.680 --> 55:40.960
do that very much. But just for that example. So we have these different

55:40.960 --> 55:45.640
types, we have constant placeholder sparse tensor variable. And there's actually

55:45.640 --> 55:49.960
a few other ones as well. Now, we're not going to really talk about these two

55:50.000 --> 55:53.600
that much, although constant and variable are important to understand the

55:53.600 --> 55:57.760
difference between. So we can read this says with the exception of variable, all

55:57.760 --> 56:01.080
of these tensors are immutable, meaning their value may not change during

56:01.120 --> 56:05.480
execution. So essentially, all of these when we create a tensor mean we have

56:05.480 --> 56:09.520
some constant value, which means that whatever we've defined here, it's not

56:09.520 --> 56:14.160
going to change. Whereas the variable tensor could change. So that's just

56:14.160 --> 56:17.560
something to keep in mind when we use variable, that's because we think we

56:17.560 --> 56:20.760
might need to change the value of that tensor later on. Whereas if we're using

56:20.760 --> 56:24.080
a constant value tensor, we cannot change it. So that's just something to keep

56:24.080 --> 56:28.440
in mind, we can obviously copy it, but we can't change it. Okay, so evaluating

56:28.440 --> 56:31.080
tensors, we're almost at the end of the section, I know, and then we'll get into

56:31.080 --> 56:35.280
some more kind of deeper code. So there will be some times for this guide, we

56:35.280 --> 56:38.960
need to evaluate a tense, of course, so what we need to do to evaluate a tensor

56:38.960 --> 56:43.880
is create a session. Now, this isn't really like, we're not going to do this that

56:43.880 --> 56:47.000
much. But I just figured I'd mention it to make sure that you guys are aware of

56:47.000 --> 56:51.000
what I'm doing. If I start kind of typing this later on. Essentially, sometimes

56:51.000 --> 56:54.720
we have some tensor object. And throughout our code, we actually need to

56:54.720 --> 56:59.520
evaluate it to be able to do something else. So to do that, all we need to do

56:59.560 --> 57:04.240
is literally just use this kind of default template, a block of code. Well, we

57:04.400 --> 57:08.440
say with TF dot session, as some kind of session doesn't really matter what we

57:08.440 --> 57:13.760
put here, then we can just do whatever the tensor name is dot eval. And calling

57:13.760 --> 57:17.240
that will actually have TensorFlow just figure out what it needs to do to find

57:17.240 --> 57:20.600
the value of this tensor, it will evaluate it, and then it will allow us to

57:20.600 --> 57:23.640
actually use that value. So I put this in here, you guys can obviously read

57:23.640 --> 57:26.960
through this if you want to understand some more in depth on how that works. And

57:26.960 --> 57:30.080
the source for this is straight from the TensorFlow website. A lot of this is

57:30.080 --> 57:34.200
straight up copied from there. And I've just kind of added my own spin to it and

57:34.200 --> 57:37.760
made it a little bit easier to understand. Okay, so we've done all that. So let's

57:37.760 --> 57:40.880
just go in here and do a few examples of reshaping just to make sure that

57:40.880 --> 57:43.800
everyone's kind of on the same page. And then we'll move on to actually talking

57:43.800 --> 57:47.840
about some simple learning algorithms. So I want to create a tensor that we can

57:47.840 --> 57:51.960
kind of mess with in reshape. So what I'm going to do is just say t equals and

57:51.960 --> 57:57.560
we'll say TF dot ones. Now what TF dot ones does is just create again, all of

57:57.560 --> 58:01.480
the values to be ones that we're going to have and whatever shape. Now we can

58:01.480 --> 58:05.160
also do zeros and zeros is just going to give us a bunch of zeros. And let's

58:05.160 --> 58:08.120
create some like crazy shape and just visualize this. Let's see like a five

58:08.120 --> 58:11.760
by five by five. So obviously, if we want to figure out how many elements are

58:11.760 --> 58:14.720
going to be in here, we need to multiply this value. So I believe this is going to

58:14.720 --> 58:18.600
be 625 because that should be five to the power of four. So five times five times

58:18.600 --> 58:23.160
five times five. And let's actually print T and have a look at that and see

58:23.160 --> 58:26.760
what this is. So we run this now. And you can see this is the output we're

58:26.760 --> 58:30.600
getting. So obviously, this is a pretty crazy looking tensor, but you get the

58:30.600 --> 58:35.480
point, right? And it tells us the shape is 55555. Now watch what happens when I

58:35.520 --> 58:40.160
reshape this tensor. So if I want to take all of these elements and flatten them

58:40.160 --> 58:48.160
out, what I could do is simply say, we'll say T equals TF dot reshape like

58:48.160 --> 58:55.040
that. And we'll reshape the tensor T to just the shape 625. Now if we do this

58:55.080 --> 59:01.240
and we run here, oops, I got a print T at the bottom after we've done that if I

59:01.280 --> 59:06.240
could spell the print statement correctly, you can see that now we just get this

59:06.240 --> 59:12.080
massive list that just has 625 zeros. And again, if we wanted to reshape this to

59:12.080 --> 59:15.360
something like 125, and maybe we weren't that good at math and couldn't figure out

59:15.360 --> 59:19.320
that this last value should be five, we could put a negative one, this would mean

59:19.320 --> 59:23.200
that TensorFlow would infer now what the shape needs to be. And now when we look

59:23.200 --> 59:27.200
at it, we can see that we're what we're going to get is well, just simply five

59:27.200 --> 59:31.600
kind of sets of these, I don't know, matrices, whatever you want to call them in

59:31.600 --> 59:37.000
our shape is 125 five. So that is essentially how that works. So that's how

59:37.000 --> 59:41.480
we reshape. That's how we kind of deal with tensors create variables, how that

59:41.480 --> 59:44.640
works in terms of sessions and graphs. And hopefully with that, that gives you

59:44.640 --> 59:50.600
enough of an understanding of tensors of shapes of ranks of value so that when we

59:50.600 --> 59:53.440
move into the next part of the tutorial, where we're actually writing code, and I

59:53.440 --> 59:56.440
promise we're going to be writing some more advanced code, you'll understand how

59:56.480 --> 01:00:03.480
that works. So with that being said, let's get into the next section. So welcome

01:00:03.480 --> 01:00:07.080
to module three of this course. Now what we're going to be doing in this module is

01:00:07.080 --> 01:00:11.320
learning the core machine learning algorithms that come with TensorFlow. Now

01:00:11.320 --> 01:00:14.480
these algorithms are not specific to TensorFlow, but they are used within

01:00:14.480 --> 01:00:17.640
there and we'll use some tools from TensorFlow to kind of implement them. But

01:00:17.640 --> 01:00:20.800
essentially, these are the building blocks before moving on to things like

01:00:20.800 --> 01:00:24.280
neural networks and more advanced machine learning techniques. You really need to

01:00:24.320 --> 01:00:27.960
understand how these work because they're kind of used in a lot of different

01:00:27.960 --> 01:00:30.960
techniques and combined together. And one of them but to show you is actually

01:00:30.960 --> 01:00:35.080
very powerful if you use it in the right way. A lot of what machine learning

01:00:35.080 --> 01:00:38.200
actually is in a lot of machine learning algorithms and implementations and

01:00:38.200 --> 01:00:42.800
businesses and applications and stuff like that, actually just use pretty basic

01:00:43.120 --> 01:00:46.680
models, because these models are capable of actually doing, you know, very

01:00:46.680 --> 01:00:50.080
powerful things. When you're not dealing with anything that's crazy complicated,

01:00:50.080 --> 01:00:53.680
you just need some basic machine learning, some basic classification, you can

01:00:53.680 --> 01:00:57.680
use these kind of fundamental core learning algorithms. Now the first one

01:00:57.680 --> 01:01:00.120
we're going to go through is a linear regression, but we will cover

01:01:00.120 --> 01:01:04.280
classification, clustering and hidden Markov models. And those are kind of

01:01:04.280 --> 01:01:08.720
going to give us a good spread of the different core algorithms. Now there is

01:01:08.760 --> 01:01:12.840
a ton, ton, like thousands of different machine learning algorithms. These are

01:01:12.840 --> 01:01:16.320
kind of the main categories that you'll cover. But within these categories,

01:01:16.320 --> 01:01:19.400
there is more specific algorithms that you can get into. I just feel like I

01:01:19.400 --> 01:01:22.680
need to mention that because I know a lot of you will have maybe seen some

01:01:22.680 --> 01:01:25.400
different ways of doing things in this course might show you, you know, a

01:01:25.400 --> 01:01:28.640
different perspective on that. So let me just quickly talk about how I'm going

01:01:28.640 --> 01:01:32.040
to go through this. It's very similar to before I have this notebook, as I've

01:01:32.040 --> 01:01:35.040
kind of talked about, there is a link in the description, I would recommend that

01:01:35.040 --> 01:01:38.440
you guys hit that and follow along with what I'm doing and read through the

01:01:38.440 --> 01:01:41.440
notebook, but I will just be going through the notebook. And then occasionally

01:01:41.440 --> 01:01:45.640
what I will actually do, oops, I need to open this up here is go to this kind

01:01:45.640 --> 01:01:49.200
of untitled tab I have here and write some code in here. Because most of what

01:01:49.240 --> 01:01:53.000
I'm going to do is just copy code over into here so we can see it all in kind

01:01:53.000 --> 01:01:57.200
of one block. And then we'll be good to go. And the last note before we really

01:01:57.200 --> 01:02:00.000
get into it, and I'm sorry I'm talking a lot, but it is important to make you

01:02:00.000 --> 01:02:03.200
guys aware of this, you're going to see that we use a lot of complicated

01:02:03.200 --> 01:02:07.120
syntax throughout this kind of series and the rest of the course in general. I

01:02:07.120 --> 01:02:11.080
just want to make it extremely clear that you should not have to memorize or

01:02:11.080 --> 01:02:15.520
even feel obligated to memorize any of the syntax that you see, everything that

01:02:15.520 --> 01:02:19.160
you see here, I personally don't even have memorized is a lot of what's in here

01:02:19.160 --> 01:02:22.640
that I can't just come up with on the top of my head. When we're dealing with

01:02:22.640 --> 01:02:26.960
kind of a library and modules so big that like TensorFlow, it's hard to

01:02:26.960 --> 01:02:30.200
memorize all those different components. So just make sure you understand what's

01:02:30.200 --> 01:02:33.240
happening, but you don't need to memorize it. If you're ever going to need to use

01:02:33.240 --> 01:02:35.880
any of these tools, you're going to look them up, you're going to see what it is

01:02:35.880 --> 01:02:38.280
you're going to be like, okay, I've used this before, you're going to understand

01:02:38.280 --> 01:02:41.040
it, and then you can go ahead and you know, copy that code in and use it in

01:02:41.040 --> 01:02:44.760
whatever way you need to, you don't need to memorize anything that we do. All

01:02:44.760 --> 01:02:48.720
right, so let's go ahead and get started with linear regression. So what is

01:02:48.760 --> 01:02:52.520
linear regression? What's one of those basic forms of machine learning? And

01:02:52.520 --> 01:02:56.640
essentially, what we try to do is have a linear correspondence between data

01:02:56.640 --> 01:02:59.960
points. So I'm just going to scroll down here, do a good example. So what I've

01:02:59.960 --> 01:03:03.760
done is use map plot live just to plot a little graph here. So we can see this

01:03:03.760 --> 01:03:07.360
one right here. And essentially, this is kind of our data set. This is what we'll

01:03:07.360 --> 01:03:11.800
call your data set. What we want to do is use linear regression to come up with

01:03:11.840 --> 01:03:15.400
a model that can give us some good predictions for our data points. So in

01:03:15.400 --> 01:03:19.320
this instance, maybe what we want to do is given some x value for a data point,

01:03:19.320 --> 01:03:23.720
we want to predict the y value. Now, in this case, we can see there is kind of

01:03:23.720 --> 01:03:28.360
some correspondence linearly for these data points. Now, what that means is we

01:03:28.360 --> 01:03:32.360
can draw something called a line of best fit through these data points that can

01:03:32.400 --> 01:03:36.320
kind of accurately classify them, if that makes any sense. So I'm going to

01:03:36.320 --> 01:03:39.920
scroll down here and look at what our line of best fit for this data set

01:03:39.920 --> 01:03:43.880
actually is, you can see this blue line, a pretty much, I mean, it is the

01:03:43.880 --> 01:03:48.800
perfect line of best fit for this data set. And using this line, we can actually

01:03:48.800 --> 01:03:53.520
predict future values in our data set. So essentially, linear regression is used

01:03:53.520 --> 01:03:57.560
when you have data points that correlate in kind of a linear fashion. Now, this is

01:03:57.560 --> 01:04:02.160
a very basic example, because we're doing this in two dimensions with x and y. But

01:04:02.160 --> 01:04:05.720
oftentimes, what you'll have is you'll have data points that have, you know, eight

01:04:05.720 --> 01:04:10.160
or nine kind of input values. So that gives us, you know, a nine dimensional kind

01:04:10.200 --> 01:04:13.680
of data set. What we'll do is predict one of the different values. So in the

01:04:13.680 --> 01:04:16.280
instance where we were talking about students before, maybe we have a

01:04:16.280 --> 01:04:20.000
student, what is it midterm grade, and their second midterm grade, and then we

01:04:20.000 --> 01:04:23.920
want to predict their final grade, what we can do is use linear regression to do

01:04:23.920 --> 01:04:27.440
that, where our kind of input values are going to be the two midterm grades and

01:04:27.440 --> 01:04:31.960
the output value is going to be that final grade that we're looking to predict. So

01:04:31.960 --> 01:04:35.800
if we were to plot that, we would plot that on a three dimensional graph, and we

01:04:35.800 --> 01:04:39.800
would draw a three dimensional line that would represent the line of best fit for

01:04:39.800 --> 01:04:43.200
that data set. Now, for any of you that don't know what line of best fit stands

01:04:43.200 --> 01:04:46.920
for, it says line, or this is just the definition I got from this website here,

01:04:47.120 --> 01:04:50.440
line of best fit refers to a line through a scatter plot of data points that

01:04:50.440 --> 01:04:54.080
best expresses the relationship between those points. So exactly what I've kind

01:04:54.080 --> 01:04:58.320
of been trying to explain, when we have data that correlates linearly, and I

01:04:58.360 --> 01:05:02.320
always butcher that word, what we can do is draw a line through it, and then we

01:05:02.320 --> 01:05:06.160
can use that line to predict new data points, because if that line is good,

01:05:06.200 --> 01:05:10.480
it's a good line of best fit for the data set, then hopefully we would assume

01:05:10.480 --> 01:05:14.360
that we can just, you know, pick some point, find where it would be on that

01:05:14.360 --> 01:05:18.160
line, and that'll be kind of our predicted value. So I'm going to go into an

01:05:18.160 --> 01:05:20.800
example now where I start drawing and going into a little bit of math. So we

01:05:20.800 --> 01:05:23.960
understand how this works on a deeper level. But that should give you a

01:05:23.960 --> 01:05:26.800
surface level understanding. So actually, I'll leave this up because I was

01:05:26.960 --> 01:05:31.400
messing with this beforehand. This is kind of a data set that I've drawn on

01:05:31.400 --> 01:05:36.000
here. So we have our x, and we have our y, and we have our line of best fit. Now,

01:05:36.000 --> 01:05:39.640
what I want to do is I want to use this line of best fit to predict a new

01:05:39.640 --> 01:05:42.920
data point. So all these red data points are ones that we've trained our model

01:05:42.920 --> 01:05:46.320
with their information that we gave to the model so that it could create this

01:05:46.320 --> 01:05:50.600
line of best fit. Because essentially, all linear regression really does is look

01:05:50.600 --> 01:05:55.200
at all of these data points and create a line of best fit for them. That's all it

01:05:55.200 --> 01:05:59.720
does. It's pretty, I don't know the word for it. It's pretty easy to actually do

01:05:59.720 --> 01:06:02.600
this. This algorithm is not that complicated. It's not that advanced. And

01:06:02.640 --> 01:06:06.360
that's why we start with it here, because it just makes sense to explain. So I

01:06:06.360 --> 01:06:10.520
hope that a lot of you would know in two dimensions, a line can be defined as

01:06:10.520 --> 01:06:16.680
follows. So with the equation y equals mx plus b. Now b stands for the y

01:06:16.680 --> 01:06:20.280
intercept, which means somewhere on this line. So essentially, where the line

01:06:20.280 --> 01:06:25.120
starts. So in this instance, our b value is going to be right here. So this is

01:06:25.120 --> 01:06:29.120
going to be b, because that is the y intercept. So we could say that that's

01:06:29.160 --> 01:06:34.440
like maybe, you know, we go on, we'll do this, we'll say this is like 123, we

01:06:34.440 --> 01:06:39.200
might say b is something like 0.4, right? So I could just pencil that into 0.4.

01:06:40.120 --> 01:06:45.160
And then what is mx and y? Well, x and y stand for the coordinates of this

01:06:45.160 --> 01:06:50.360
data point. So this would have, you know, some x, y value. In this case, we might

01:06:50.360 --> 01:06:55.200
call it, you know, something like, what do you want to say to 2.7, that might be

01:06:55.200 --> 01:07:00.400
the value of this data point. So that's our x and y. And then our m stands for

01:07:00.400 --> 01:07:05.080
the slope, which is probably the most important part. Now slope simply defines

01:07:05.280 --> 01:07:09.200
the steepness of this line of best fit that we've done here. Now the way we

01:07:09.200 --> 01:07:13.840
calculate slope is using rise over run. Now rise over run essentially just

01:07:13.840 --> 01:07:17.480
means how much we went up versus how much we went across. So if you want to

01:07:17.480 --> 01:07:20.800
calculate the slope of a line, what you can actually do is just draw a triangle.

01:07:21.440 --> 01:07:25.840
So a right angle triangle anywhere on the line. So just pick two data points. And

01:07:25.840 --> 01:07:29.720
what you can do is calculate this distance, and this distance. And then you

01:07:29.720 --> 01:07:33.800
can simply divide the distance up by the distance across. And that gives you the

01:07:33.800 --> 01:07:36.600
slope. I'm not going to go too far into slope because I feel like you guys

01:07:36.600 --> 01:07:40.320
probably understand what that is. But let's just pick some values for this line.

01:07:40.320 --> 01:07:43.200
And I want to actually show you some real examples of math and how we're going

01:07:43.200 --> 01:07:46.800
to do this. So let's say that our linear regression algorithm, you know, comes up

01:07:46.800 --> 01:07:49.840
with this line, I'm not going to discuss really how it does that, although it

01:07:49.840 --> 01:07:53.360
just pretty much looks at all these data points, and finds a line that you know,

01:07:53.400 --> 01:07:58.600
goes, it splits these data points evenly. So essentially, you want to be as close

01:07:58.600 --> 01:08:02.680
to every data point as possible. And you want to have as many data points, you

01:08:02.680 --> 01:08:05.600
want to have like the same amount of data points on the left side and the right

01:08:05.600 --> 01:08:08.600
side of the line. So in this example, we have, you know, a data point on the

01:08:08.600 --> 01:08:11.840
left, a data point on the left, we have what two that are pretty much on the

01:08:11.840 --> 01:08:15.040
line. And then we have two that are on the right. So this is a pretty good line

01:08:15.040 --> 01:08:19.400
of best fit, because all of the points are very close to the line. And they

01:08:19.440 --> 01:08:23.680
split them evenly. So that's kind of how you come up with a line of best fit. So

01:08:23.680 --> 01:08:27.640
let's say that the equation for this line is something like y equals, let's just

01:08:27.640 --> 01:08:35.440
give it 1.5 and x plus and let's say that value is just 0.5 to make it easy. So

01:08:35.440 --> 01:08:39.200
this is going to be the equation of our line. Now notice that x and y don't have

01:08:39.200 --> 01:08:42.960
a value, that's because we need to give the value to come up with one of the

01:08:42.960 --> 01:08:47.360
other ones. So what we can do is we can say if we have either the y value or we

01:08:47.360 --> 01:08:51.560
have the x value of some point, and we want to figure out, you know, where it

01:08:51.560 --> 01:08:56.320
is on the line, what we can do is just feed one in, do a calculation, and that

01:08:56.320 --> 01:08:59.520
will actually give us the other value. So in this instance, let's say that you

01:08:59.520 --> 01:09:03.320
know, I'm trying to predict something and I'm given the that the fact that x

01:09:03.360 --> 01:09:07.760
equals two, I know that x equals two, and I want to figure out what y would be

01:09:07.800 --> 01:09:12.280
if x equals two. Well, I can use this line to do so. So what I would do is I'm

01:09:12.280 --> 01:09:20.560
going to say y equals 1.5 times two plus 0.5. Now, all of you quick math majors

01:09:20.560 --> 01:09:25.480
out there give me the value of 3.5, which means that if x was at two, then I

01:09:25.480 --> 01:09:29.920
would have my data point as a prediction here on this line. And I would say, okay,

01:09:29.920 --> 01:09:33.400
so if you're telling me x is two, my prediction is that y is going to be

01:09:33.400 --> 01:09:37.560
equal to 3.5, because given the line of best fit for this data set, that's where

01:09:37.560 --> 01:09:42.480
this point will lie on that line. So I hope that makes sense. You can actually

01:09:42.480 --> 01:09:46.680
do this the reverse way as well. So if I'm just given some y values to say, I

01:09:46.680 --> 01:09:51.840
know that, you know, my y value is at like 2.7 or something, I can plug that in,

01:09:51.840 --> 01:09:55.440
just rearrange the numbers in this equation and then solve for x. Now,

01:09:55.440 --> 01:09:58.720
obviously, this is a very basic example, because we're just doing all of this in

01:09:58.720 --> 01:10:02.560
two dimensions. But you can do this in higher dimensions as well. So actually,

01:10:02.560 --> 01:10:05.160
most times, what's going to end up happening is you're going to have, you

01:10:05.200 --> 01:10:08.680
know, like eight or nine input variables. And then you're going to have one output

01:10:08.680 --> 01:10:12.960
variable that you're predicting. Now, so long as our data points are correlated

01:10:12.960 --> 01:10:16.600
linearly in three dimensions, we can still do this. So I'm going to attempt to

01:10:16.600 --> 01:10:19.880
show you this actually, in three dimensions, just to hopefully clear some

01:10:19.880 --> 01:10:24.280
things up, because it is important to kind of get a grasp and perspective of the

01:10:24.280 --> 01:10:28.720
different dimensions. So let's say we have a bunch of data points that are kind

01:10:28.720 --> 01:10:34.040
of like this, now I'm trying my best to kind of draw them in some linear fashion

01:10:34.120 --> 01:10:38.680
using like all the dimensions here. But it is hard because drawing in three

01:10:38.680 --> 01:10:42.280
dimensions on a two dimensional screen is not easy. Okay, so let's say this is

01:10:42.280 --> 01:10:45.680
kind of like what our data points look like. Now, I would say that these

01:10:45.680 --> 01:10:50.160
correlate linearly, like pretty, pretty well, they kind of go up in one fashion,

01:10:50.160 --> 01:10:53.440
and we don't know the scale of this. So this is probably fun. So the line of

01:10:53.440 --> 01:10:57.720
best fit for this data set, and I'll just put my kind of thickness up might be

01:10:57.720 --> 01:11:03.080
something like this, right? Now notice that this line is in three dimensions,

01:11:03.080 --> 01:11:08.560
right? This is going to cross our, I guess this is our x, y, and z axes. So we

01:11:08.560 --> 01:11:11.440
have a three dimensional line. Now the equation for this line is a little bit

01:11:11.440 --> 01:11:14.920
more complicated. I'm not going to talk about exactly what it is. But essentially

01:11:14.920 --> 01:11:18.960
what we do is we make this line, and then we say, Okay, what value do I want to

01:11:18.960 --> 01:11:25.760
predict? Do I want to predict y, x or z? Now, so long as I have two values, so two

01:11:25.800 --> 01:11:29.240
values, I can always predict the other one. So if I have, you know, the x, y of

01:11:29.280 --> 01:11:35.120
the data point, that will give me the z. And if I have the z, y, that will give

01:11:35.120 --> 01:11:40.520
me the x. So so long as you have, you know, all of the data points, except one,

01:11:40.680 --> 01:11:44.320
you can always find what that point is, based on the fact that, you know, we have

01:11:44.320 --> 01:11:47.680
this line, and we're using that to predict. So I think I'm going to leave it at

01:11:47.680 --> 01:11:52.440
that for the explanation. I hope that makes sense. Again, just understand that we

01:11:52.440 --> 01:11:56.200
use linear regression when our data points are correlated linearly. Now some

01:11:56.200 --> 01:11:59.480
good examples of linear regression were, you know, that kind of student

01:11:59.480 --> 01:12:03.520
predicting the grade kind of thing, you would assume that if someone has, you

01:12:03.520 --> 01:12:07.080
know, a low grade, then they would finish with a lower grade, and you would

01:12:07.080 --> 01:12:10.720
assume if they have a higher grade, they would finish with a higher grade. Now you

01:12:10.720 --> 01:12:14.680
could also do something like predicting, you know, future life expectancy. Now this

01:12:14.680 --> 01:12:18.360
is kind of a darker example. But essentially, what you could think of here is

01:12:18.360 --> 01:12:23.080
if someone is older, they're expected to live, you know, like not as long. Or you

01:12:23.080 --> 01:12:26.520
could look at health conditions, if someone is in critical illness condition,

01:12:26.560 --> 01:12:30.240
they have a critical illness, then chances are their life expectancy is lower. So

01:12:30.240 --> 01:12:33.400
that's an example of something that is correlated linearly. Essentially,

01:12:33.400 --> 01:12:36.040
something goes up, and something goes down, or something goes up, the other

01:12:36.040 --> 01:12:38.840
thing goes up. That's kind of what you need to think of when you think of a

01:12:38.840 --> 01:12:42.880
linear correlation. Now the magnitude of that correlation, so you know, how much

01:12:42.880 --> 01:12:46.320
does one go up versus how much one goes down is exactly what our algorithm

01:12:46.320 --> 01:12:49.960
figures out for us, we just need to know to pick linear regression when we think

01:12:50.000 --> 01:12:53.720
things are going to be correlated in that sense. Okay, so that is enough of the

01:12:53.720 --> 01:12:56.760
explanation of linear regression. Now we're going to get into actually coding

01:12:56.760 --> 01:12:59.840
and creating a model. But we first need to talk about the data set that we're

01:12:59.840 --> 01:13:02.760
going to use in the example we're going to kind of illustrate linear regression

01:13:02.760 --> 01:13:06.680
with. Okay, so I'm here and I'm back in the notebook. Now these are the imports

01:13:06.680 --> 01:13:09.840
we need to start with to actually start programming and getting some stuff done.

01:13:10.160 --> 01:13:14.160
Now the first thing we need to do is actually install SK learn. Now even if

01:13:14.160 --> 01:13:17.240
you're in a notebook, you actually need to do this because for some reason it

01:13:17.240 --> 01:13:20.000
doesn't come by default with the notebook. So to do this, we just did an

01:13:20.000 --> 01:13:24.680
exclamation point, pip install hyphen q SK learn. Now if you're going to be working

01:13:24.680 --> 01:13:28.000
on your own machine, again, you can use pip to install this. And I'm assuming

01:13:28.000 --> 01:13:31.320
that you know to use pip if you're going to be going along in that direction. Now

01:13:31.320 --> 01:13:33.960
as before, since we're in the notebook, we need to define we're going to use

01:13:33.960 --> 01:13:38.680
TensorFlow version to point x. So to do that, we're going to just, you know, do

01:13:38.680 --> 01:13:41.920
that up here with the percent sign. And then we have all these imports, which

01:13:41.920 --> 01:13:45.400
we're going to be using throughout here. So from future import, absolutely import

01:13:45.440 --> 01:13:48.800
division, print function, Unicode literals, and then obviously the big one. So

01:13:48.800 --> 01:13:54.200
NumPy, pandas, map plot lib, we're gonna be using I Python, we're gonna be using

01:13:54.200 --> 01:13:57.560
TensorFlow. And yeah, so I'm actually just gonna explain what some of these

01:13:57.560 --> 01:14:01.560
modules are, because I feel like some of you may actually not know. NumPy is

01:14:01.560 --> 01:14:07.360
essentially a very optimized version of arrays in Python. So what this allows

01:14:07.360 --> 01:14:11.680
us to do is lots of kind of multi dimensional calculations. So essentially

01:14:11.680 --> 01:14:15.040
if you have a multi dimensional array, which we've talked about before, right

01:14:15.040 --> 01:14:19.200
when we had, you know, those crazy shapes like 5555, NumPy allows us to

01:14:19.200 --> 01:14:22.880
represent data in that form, and then very quickly manipulate and perform

01:14:22.880 --> 01:14:27.240
operations on it. So we can do things like cross product, dot product, matrix

01:14:27.240 --> 01:14:31.600
addition, matrix subtraction, element wise addition, subtraction, you know,

01:14:31.600 --> 01:14:35.320
vector operations, that's what this does for us. It's pretty complex, but we're

01:14:35.320 --> 01:14:39.400
going to be using it a fair amount. Pandas. Now what pandas does is it's kind

01:14:39.400 --> 01:14:43.480
of a data analytics tool, I almost want to say, I don't know the formal

01:14:43.480 --> 01:14:47.640
definition of what pandas is. But it allows us to very easily manipulate

01:14:47.640 --> 01:14:52.800
data. So you know, load in data sets, view data sets, cut off specific columns

01:14:52.800 --> 01:14:56.440
or cut out rows from our data sets, visualize the data sets. That's what

01:14:56.440 --> 01:15:00.520
pandas does for us. Now map plot lib is actually a visualization of kind of

01:15:00.520 --> 01:15:04.080
graphs and charts. So we'll use that a little bit lower when I actually

01:15:04.440 --> 01:15:09.120
graph some different aspects of our data set. The IPython display, this is

01:15:09.120 --> 01:15:11.920
just specific for this notebook, it's just to clear the output, there's

01:15:11.920 --> 01:15:15.400
nothing crazy with that. And then obviously, we know what TensorFlow is,

01:15:15.600 --> 01:15:20.000
this crazy import for TensorFlow here. So compact v to feature column as FC,

01:15:20.000 --> 01:15:23.560
we'll talk about later, but we need something called a feature column when

01:15:23.560 --> 01:15:27.600
we create a linear regression algorithm or model and TensorFlow. So we're going

01:15:27.600 --> 01:15:31.640
to use that. Okay. So now that we've gone through all that, we need to start

01:15:31.640 --> 01:15:34.280
talking about the data set that we're going to use for linear regression. And

01:15:34.280 --> 01:15:37.200
for this example, because what we're going to do is, you know, actually create

01:15:37.200 --> 01:15:41.040
this model, and start using it to predict values. So the data set that we're

01:15:41.080 --> 01:15:44.080
going to use, actually, I need to read this, because I forget exactly what the

01:15:44.080 --> 01:15:48.440
name of it is, is the Titanic data set, that's what it is. So essentially, what

01:15:48.440 --> 01:15:52.320
this does is aim to predict who's going to survive or the likelihood that

01:15:52.320 --> 01:15:57.200
someone will survive, being on the Titanic, given a bunch of information. So

01:15:57.200 --> 01:15:59.760
what we need to do is load in this data set. Now, I know this seems like a

01:15:59.760 --> 01:16:02.880
bunch of gibberish, but this is how we need to load it. So we're going to use

01:16:02.880 --> 01:16:09.480
pandas. So PD dot read CSV from this URL. So what this is going to do is take

01:16:09.520 --> 01:16:13.480
this CSV file, which stands for comma, separated values. And we can actually

01:16:13.480 --> 01:16:17.400
look at this if we want, I think so I said, it said control click, let's see

01:16:17.400 --> 01:16:20.920
if this pops up. So let's actually download this. And let's open this up

01:16:20.920 --> 01:16:23.800
ourselves and have a look at what it is in Excel. So I'm going to bring this

01:16:23.800 --> 01:16:28.400
up here. You can see that link. And this is what our data set is. So we have

01:16:28.400 --> 01:16:32.520
our columns, which just stand for, you know, what is it the different

01:16:32.520 --> 01:16:35.160
attributes in our data set of the different features and labels of our

01:16:35.160 --> 01:16:38.760
data set, we have survived. So this is what we're actually going to be aiming

01:16:38.800 --> 01:16:42.040
to predict. So we're going to call this our label, right, or our output

01:16:42.040 --> 01:16:47.560
information. So here, a zero stands for the fact that someone did not survive.

01:16:47.600 --> 01:16:51.360
And one stands for the fact that someone did survive. Now, just thinking about

01:16:51.360 --> 01:16:54.720
it on your own for a second, and looking at some of the categories we have up

01:16:54.720 --> 01:16:58.640
here, can you think about why linear regression would be a good algorithm for

01:16:58.640 --> 01:17:03.480
something like this? Well, for example, if someone is a female, we can kind of

01:17:03.480 --> 01:17:05.960
assume that they're going to have a higher chance of surviving on the

01:17:05.960 --> 01:17:09.680
Titanic, just because of, you know, the kind of the way that our culture works,

01:17:09.720 --> 01:17:12.280
you know, saving women and children first, right? And if we look through this

01:17:12.280 --> 01:17:16.520
data set, we'll see that when we see females, it's pretty rare that they

01:17:16.520 --> 01:17:19.760
don't survive. Although as I go through, there is quite a few that didn't

01:17:19.760 --> 01:17:22.440
survive. But if we look at it compared to males, you know, there's definitely

01:17:22.440 --> 01:17:26.960
strong correlation that being a female results in a stronger survival rate. Now,

01:17:26.960 --> 01:17:30.080
if we look at age, right, can we think of how age might affect this? Well, I

01:17:30.080 --> 01:17:33.240
would assume if someone's way younger, they probably have a higher chance of

01:17:33.280 --> 01:17:38.320
surviving, because they would be, you know, prioritized in terms of lifeboats or

01:17:38.320 --> 01:17:41.040
whatever it was. I don't know much about the Titanic. So I can't talk about that

01:17:41.040 --> 01:17:43.680
specifically. But I'm just trying to go through the categories and explain to

01:17:43.680 --> 01:17:47.160
you why we picked this algorithm. Now, number of siblings, that one might not

01:17:47.160 --> 01:17:50.880
be as, you know, influential, in my opinion, parched. I don't actually

01:17:50.880 --> 01:17:56.760
remember what parched stands for. I think it is like what parched, I don't know

01:17:56.760 --> 01:17:59.600
exactly what this column stands for. So unfortunately, I can't tell you guys

01:17:59.640 --> 01:18:04.640
that one. But we'll talk about some more of the second fair. Again, not exactly

01:18:04.640 --> 01:18:07.640
sure what fair stands for. I'm going to look on the TensorFlow website after

01:18:07.640 --> 01:18:11.560
this and get back to you guys. And we have a class. So class is what class they

01:18:11.560 --> 01:18:14.800
were on the boat, right? So first class, second class, third class. So you might

01:18:14.800 --> 01:18:18.080
think someone that's in a higher class might have a higher chance of surviving.

01:18:18.480 --> 01:18:22.400
We have decks, this is what deck they were on when it crashed. So unknown is

01:18:22.400 --> 01:18:26.160
pretty common. And then we have all these other decks, you know, if someone got

01:18:26.200 --> 01:18:30.200
hit, if someone was standing on the deck that had the initial impact, we might

01:18:30.200 --> 01:18:34.000
assume that they would have a lower chance of survival. embark to is where

01:18:34.000 --> 01:18:37.320
they were going. And then are they alone? Yes or no. And this one, you know, this

01:18:37.320 --> 01:18:40.520
is interesting, we're going to see does this make an effect? If someone is alone,

01:18:40.520 --> 01:18:43.920
is that a higher chance of survival? Is that a lower chance of survival? So this

01:18:43.920 --> 01:18:46.200
is kind of interesting. And this is what I want you guys to think about is that

01:18:46.200 --> 01:18:50.560
when we have information and data like this, we don't necessarily know what

01:18:50.560 --> 01:18:54.400
correlations there might be. But we can kind of assume there's some linear

01:18:54.480 --> 01:18:58.000
thing that we're looking for some kind of pattern, right? Whereas if something is

01:18:58.000 --> 01:19:01.520
true, then you know, maybe it's more likely someone will survive. Whereas like,

01:19:01.520 --> 01:19:05.080
if they're not alone, maybe it's less likely. And maybe there's no correlation

01:19:05.080 --> 01:19:08.840
whatsoever. But that's where we're going to find out as we do this model. So let

01:19:08.840 --> 01:19:12.280
me look actually, on the TensorFlow website and see if I can remember what

01:19:12.280 --> 01:19:16.600
parched and I guess what fair was. So let's go up to the top here. Again, a lot

01:19:16.600 --> 01:19:20.000
of this stuff is just straight up copied from the TensorFlow website. I've just

01:19:20.000 --> 01:19:23.680
added my own stuff to it. You can see like, I just copied all this, we're just

01:19:23.720 --> 01:19:27.880
bringing it in there. Let's see what it says about the different columns, if it

01:19:27.880 --> 01:19:33.440
gives us any exact explanations. Okay, so I couldn't find what parts were fair

01:19:33.440 --> 01:19:36.320
stands for. For some reason, it's not on the TensorFlow website, either. I

01:19:36.320 --> 01:19:39.040
couldn't really find any information about it. If you guys know, you know,

01:19:39.040 --> 01:19:41.760
leave a comment down below, but it's not that important, we just want to use

01:19:41.760 --> 01:19:46.320
this data to do a test. So what I've done here, if I've loaded in my data set,

01:19:46.320 --> 01:19:50.560
and notice that I've loaded a training data set in a testing data set. Now we'll

01:19:50.600 --> 01:19:54.160
talk about this more later. This is important, I have two different data

01:19:54.160 --> 01:19:58.640
sets, one to train the model with, and one to test the model with. Now kind of the

01:19:58.640 --> 01:20:02.080
basic reason we would do this is because when we test our model for accuracy to

01:20:02.080 --> 01:20:05.800
see how well it's doing, it doesn't make sense to test it on data, it's already

01:20:05.800 --> 01:20:09.760
seen, it needs to see fresh data, so we can make sure there's no bias, and it

01:20:09.760 --> 01:20:14.840
hasn't simply just memorize the data, you know, that we had. Now what I'm doing

01:20:14.840 --> 01:20:19.800
here, at the bottom with this y train in this y eval, is I'm essentially popping

01:20:19.880 --> 01:20:24.560
a column off of this data set. So if I print out the data set here, and I'm

01:20:24.560 --> 01:20:27.120
actually I'm going to show you a cool trick with pandas that we can use to

01:20:27.120 --> 01:20:33.520
look at this. So I can say D F train dot head. So if I look at this, by just

01:20:33.520 --> 01:20:36.800
looking at the head, and we'll print this out, oh, I might need to import some

01:20:36.800 --> 01:20:40.880
stuff above. We'll see if this works or not. Yeah, so I need to just do these

01:20:40.880 --> 01:20:44.640
imports. So let's install. And let's do these imports. I'll wait for the

01:20:44.640 --> 01:20:48.080
surrounding. Okay, so I've just selected TensorFlow 2.0. We're just importing

01:20:48.080 --> 01:20:51.920
this now should be done in one second. And now what we'll do is we'll print

01:20:51.960 --> 01:20:56.200
out the the data frame here. So essentially what this does is load this

01:20:56.200 --> 01:21:00.600
into a pandas data frame. This is a specific type of object. Now we're not

01:21:00.600 --> 01:21:04.240
going to go into this specifically, but a data frame allows us to view a lot of

01:21:04.240 --> 01:21:08.400
different aspects about the data and kind of store it in a nice form, as opposed

01:21:08.400 --> 01:21:11.960
to just loading it in and storing it in like a list or a NumPy array, which we

01:21:11.960 --> 01:21:15.440
might do if we didn't know how to use pandas. This is a really nice way to do

01:21:15.520 --> 01:21:19.680
it read CSV, load it into a data frame object, which actually means we can

01:21:19.680 --> 01:21:23.800
reference specific columns and specific rows in the data frame. So let's run this

01:21:23.800 --> 01:21:30.760
and just have a look at it. Yeah, I got need to print dftrain.head. So let's do

01:21:30.760 --> 01:21:35.920
that. And there we go. So this is what our data frame head looks like. Now head

01:21:35.920 --> 01:21:40.800
what that does is show us the first five entries in our data set, as well as show

01:21:40.800 --> 01:21:44.760
us a lot of the different columns that are in it. Now since we have more than

01:21:44.760 --> 01:21:47.520
you know, we have a few different columns, it's not showing us all of them, it's

01:21:47.520 --> 01:21:50.920
just giving us the dot dot dot. But we can see this is what the data frame

01:21:50.920 --> 01:21:54.960
looks like. And this is kind of the representation internally. So we have

01:21:55.160 --> 01:22:00.040
entry zero, survived zero, survived one, we have male, female, all that. Now

01:22:00.040 --> 01:22:03.800
notice that this has the survived column. Okay. Because what I'm going to do is

01:22:03.800 --> 01:22:10.440
I'm going to print the data frame head again. So dftrain.head after we run

01:22:10.440 --> 01:22:15.840
these two lines. Now what this line does is takes this entire survived column, so

01:22:15.840 --> 01:22:20.400
all these zeros and ones, and removes it from this data frame, so the head data

01:22:20.400 --> 01:22:24.720
frame, and stores it in the variable y train. The reason we need to do that is

01:22:24.720 --> 01:22:27.480
because we need to separate the data, we're going to be classifying from the

01:22:27.480 --> 01:22:31.480
data that is kind of our input information or our initial data set, right? So

01:22:31.480 --> 01:22:35.080
since we're looking for the survived information, we're going to put that in

01:22:35.080 --> 01:22:39.240
its own, you know, kind of variable store here. Now we'll do the same thing for

01:22:39.240 --> 01:22:44.240
the evaluation data set, which is DF evaluation or testing data. And notice

01:22:44.240 --> 01:22:48.840
that here this was trained on CSV, and this one was eval.csv. Now these have

01:22:48.840 --> 01:22:53.320
the exact same form, they look the like completely identical, it's just that

01:22:53.320 --> 01:22:56.400
you know, some entries, we've just kind of arbitrarily split them. So we're going

01:22:56.400 --> 01:22:59.120
to have a lot of entries in this training set, and we'll have a few in the

01:22:59.120 --> 01:23:03.000
testing set that we'll just use to do an evaluation on the model later on. So we

01:23:03.000 --> 01:23:07.800
pop them off by doing this pop removes and returns this column. So if I print

01:23:07.840 --> 01:23:11.160
out why train, which are actually let's look at this one first, just to show

01:23:11.160 --> 01:23:15.200
you how it's been removed, we can see that we have the survived column here, we

01:23:15.200 --> 01:23:18.600
popped and now the survived column is removed from that data set. So that's

01:23:18.600 --> 01:23:22.120
just important to understand. Now we can print out some other stuff too. So we can

01:23:22.120 --> 01:23:25.960
look at the why train and see what that is. Just to make sure we really

01:23:25.960 --> 01:23:29.360
understand this data. So let's look at why train. And you can see that we have

01:23:29.360 --> 01:23:34.960
626 or 627 entries and address, you know, zeros or ones representing whether

01:23:34.960 --> 01:23:39.720
someone survived or whether they did not. Now the corresponding indexes in this

01:23:39.800 --> 01:23:44.160
kind of list or data frame correspond to the indexes in the testing and

01:23:44.200 --> 01:23:48.200
training data frame. What I mean by that is, you know, entry zero in this

01:23:48.240 --> 01:23:53.960
specific data frame corresponds to entry zero in our why train variable. So if

01:23:53.960 --> 01:23:58.240
someone survived, you know, at entry zero, it would say one here, right? Or in

01:23:58.240 --> 01:24:02.360
this case, entry zero did not survive. Now, I hope that's clear. I hope I'm not

01:24:02.400 --> 01:24:06.320
confusing you with that. But I just want to show one more example to make sure. So

01:24:06.320 --> 01:24:09.880
we'll say D F train zero, I'm going to print that and I'm going to print why

01:24:09.880 --> 01:24:14.880
train at index zero. Oops, if I didn't mess up my brackets, and we'll have a

01:24:14.880 --> 01:24:18.240
look at it. Okay, so I've just looked up the documentation because I totally

01:24:18.240 --> 01:24:22.280
forgot that I couldn't do that. If I want to find one specific row in my data

01:24:22.280 --> 01:24:27.600
frame, what I can do is print dot loc. So I do my data frame and then dot loc and

01:24:27.600 --> 01:24:31.800
then whatever index I want. So in this case, I'm locating row zero, which is

01:24:31.800 --> 01:24:35.480
this. And then on the why train, I'm doing the same thing, I'm locating row

01:24:35.480 --> 01:24:40.280
zero. Now what I had before, right, if I did D F train, and I put square brackets

01:24:40.520 --> 01:24:44.000
inside here, what I can actually do is reference a specific column. So if I

01:24:44.000 --> 01:24:47.960
wanted to look at, you know, say the column for age, right, so we have a

01:24:47.960 --> 01:24:52.840
column for age, what I can do is do D F train age. And then I can print this out

01:24:52.840 --> 01:24:56.440
like this. And it gives me all of the different age values. So that's kind of

01:24:56.440 --> 01:24:59.800
how we use a data frame, we'll see that as we go further on. Now let's go back to

01:24:59.800 --> 01:25:03.520
the other example I had, because I just erased it, where I want to show you the

01:25:03.920 --> 01:25:08.960
row zero in the data frame that's training, and then in the why train, you

01:25:08.960 --> 01:25:13.080
know, output, whatever that is. So the survival. So you can see here that this

01:25:13.080 --> 01:25:17.520
is what we get from printing D F train loc zero. So row zero, this is all the

01:25:17.520 --> 01:25:21.200
information. And then here, this corresponds to the fact that they did not

01:25:21.200 --> 01:25:25.400
survive at row zero, because it's simply just the output is value zero. Now I

01:25:25.440 --> 01:25:29.320
know this is weird, it's saying like name, zero, d type, object, zero, don't

01:25:29.320 --> 01:25:31.560
worry about that. It's just because it's trying to print it with some

01:25:31.560 --> 01:25:35.880
information. But essentially, this just means this person who was male 22 and had

01:25:35.880 --> 01:25:40.480
one sibling did not survive. Okay, so let's get out of this. Now we can close

01:25:40.480 --> 01:25:44.360
this and let's go to Oh, we've pretty much already done what I've just have

01:25:44.360 --> 01:25:47.800
down here. But we can look at the data frame head. This is a little bit of a

01:25:47.800 --> 01:25:51.880
nicer output when we just have D F train dot head, we can see that we get kind of

01:25:51.880 --> 01:25:55.280
a nice outputted little graph. We've already looked at this information. So we

01:25:55.320 --> 01:25:59.200
know kind of some of the attributes of the data set. Now we want to describe the

01:25:59.200 --> 01:26:03.520
data set sometimes. What describe does is just give us some overall information. So

01:26:03.520 --> 01:26:08.480
let's have a look at it here. We can see that we have 627 entries, the mean of

01:26:08.520 --> 01:26:13.360
age is 29, the standard deviation is you know, 12 point whatever. And then we get

01:26:13.360 --> 01:26:17.040
the same information about all of these other different attributes. So for example,

01:26:17.040 --> 01:26:20.320
it gives us you know, the mean fair, the minimum fair, and just some statistics,

01:26:20.360 --> 01:26:23.760
because understand this great, if you don't doesn't really matter. The important

01:26:23.760 --> 01:26:26.560
thing to look at typically is just how many entries we have is sometimes we need

01:26:26.560 --> 01:26:29.880
that information. And sometimes the mean can be helpful as well, because you can

01:26:29.880 --> 01:26:34.080
kind of get an average of like what the average value is in the data set. So if

01:26:34.080 --> 01:26:37.520
there's any bias later on, you can figure that out. But it's not crazy important.

01:26:37.920 --> 01:26:41.680
Okay, so let's have a look at the shape. So just like NumPy arrays and tensors

01:26:41.680 --> 01:26:45.080
have a shape attribute, so do data frames. So we want to look at the shape, you

01:26:45.080 --> 01:26:49.320
know, we can just print out D F train dot shape, we get 627 by nine, which

01:26:49.320 --> 01:26:55.880
essentially means we have 627 rows, and nine columns or nine attributes. So yeah,

01:26:55.880 --> 01:27:00.480
that's what it says here, you know, 627 entries, nine features, we can interchange

01:27:00.480 --> 01:27:04.360
attributes and features. And we can look at the head information for why so we can

01:27:04.360 --> 01:27:07.240
see that here, which we've already looked at before. And that gives us the name,

01:27:07.280 --> 01:27:11.440
which was survived. Okay, so now what we can actually do is make some kind of

01:27:11.440 --> 01:27:14.520
graphs about this data. Now I've just stolen this code, you know, straight up

01:27:14.520 --> 01:27:18.320
from the TensorFlow website, I wouldn't expect you guys to do any of this, you

01:27:18.320 --> 01:27:21.760
know, like output any of these values. What we're going to do is create a few

01:27:21.760 --> 01:27:25.360
histograms and some plots just to look at kind of some correlations in the data.

01:27:25.360 --> 01:27:28.680
So that when we start creating this model, we have some intuition on what we

01:27:28.680 --> 01:27:33.600
might expect. So let's look at age. So this gives us a histogram of the age. So

01:27:33.600 --> 01:27:38.560
we can see that there's about 25 people that are kind of between zero and five.

01:27:38.760 --> 01:27:43.600
There is you know, maybe like five people that are in between five and 10. And

01:27:43.600 --> 01:27:47.120
then the most amount of people are kind of in between their 20s and 30s. So in the

01:27:47.120 --> 01:27:51.480
mid 20s, this is good information to know, because that's going to introduce a

01:27:51.480 --> 01:27:55.920
little bit of bias into kind of our linear correlation graph, right? So just

01:27:56.160 --> 01:27:59.720
understanding, you know, that we have like a large subset, there's some outliers

01:27:59.720 --> 01:28:02.760
here, like there's one person that's 80, right over here, a few people that are

01:28:02.760 --> 01:28:06.080
70, some important things to kind of understand before we move on to the

01:28:06.080 --> 01:28:10.960
algorithm. So let's look at the sex values now. So this is how many female and

01:28:10.960 --> 01:28:15.360
how many male, we can see that there's many more males than there is females. We

01:28:15.400 --> 01:28:18.560
can have a look at the class. So we can see if they're in first, second or third

01:28:18.560 --> 01:28:23.800
class, most people are in third, then followed by first and then second. And

01:28:23.800 --> 01:28:27.240
then lastly, we can look at what is this that we're doing? Oh, the percentage

01:28:27.240 --> 01:28:32.320
survival by sex. So we can see how likely a specific person or a specific sex is

01:28:32.320 --> 01:28:36.240
to survive just by plotting this. So we can see that males have about a 20%

01:28:36.240 --> 01:28:40.640
survival rate, whereas females are all the way up to about 78%. So that's

01:28:40.640 --> 01:28:43.880
important to understand that kind of confirms that what we were looking at

01:28:43.880 --> 01:28:46.920
before in the data set when we were exploring it. And you don't need to do

01:28:46.920 --> 01:28:50.160
this every time that you're looking at a data set, but it is good to kind of get

01:28:50.160 --> 01:28:53.000
some intuition about it. So this is what we've learned so far, majority

01:28:53.000 --> 01:28:56.720
passengers are in their 20s or 30s, then majority passengers are male, they're in

01:28:56.720 --> 01:28:59.960
third class, and females have a much higher chance of survival, kind of

01:28:59.960 --> 01:29:03.040
already knew that. Alright, so training and testing data sets. Now we already

01:29:03.040 --> 01:29:06.040
kind of went through this all skim through it quickly. Essentially, what we

01:29:06.040 --> 01:29:10.600
did above is load in two different data sets. The first data set was that

01:29:10.600 --> 01:29:14.920
training data set which had the shape of 627 by nine. What I'm actually going to

01:29:14.920 --> 01:29:20.640
do is create a code block here, and just have a look at what was this Df eval dot

01:29:20.640 --> 01:29:24.440
shape to show you how many entries we have in here. So here in our testing

01:29:24.440 --> 01:29:29.680
data set, you can see we have significantly less at 264 entries, or rows,

01:29:29.680 --> 01:29:33.400
whatever you want to call them. So that's how many things we have to actually

01:29:33.400 --> 01:29:37.080
test our model. So what we do is we use that training data to create the model,

01:29:37.120 --> 01:29:39.880
and then the testing data to evaluate it and make sure that it's working

01:29:39.880 --> 01:29:43.160
properly. So these things are important whenever we're doing machine learning

01:29:43.160 --> 01:29:48.120
models, we typically have testing and training data. And yeah, that is pretty

01:29:48.120 --> 01:29:51.440
much it. Now I'm just going to take one second to copy over a lot of this code

01:29:51.720 --> 01:29:55.440
into the kind of other notebook I have, just so we can see all of it at once. And

01:29:55.440 --> 01:29:58.200
then I'll be back and we'll get into actually making the model. Okay, so I've

01:29:58.200 --> 01:30:01.080
copied in some code here. I know this seems like a lot of kind of gibberish

01:30:01.080 --> 01:30:04.080
right now, but I'm going to break down line by line what all this is doing and

01:30:04.080 --> 01:30:07.520
why we have this here. But we first need to discuss something called feature

01:30:07.520 --> 01:30:13.040
columns and the difference between categorical and numeric data. So categorical

01:30:13.040 --> 01:30:16.240
data is actually fairly common. Now when we're looking at our data set, and

01:30:16.240 --> 01:30:19.920
actually I can open I don't have it open in Excel anymore, but let's open this

01:30:19.920 --> 01:30:25.640
from my downloads. So let's go downloads. Where is this train? Okay, awesome. So we

01:30:25.640 --> 01:30:30.480
have this Excel data sheet here. And we can see what a categorical data or what

01:30:30.480 --> 01:30:35.200
categorical data is is something that's not numeric. So for example, unknown, C

01:30:35.320 --> 01:30:41.040
first, third, city, and why right so anything that has different categories,

01:30:41.720 --> 01:30:44.800
there's going to be like a specific set of different categories there could be. So

01:30:44.800 --> 01:30:48.480
for example, for age, kind of the set of values we get out for age, well, this is

01:30:48.480 --> 01:30:51.280
numeric. So that's different. But for categorical, we can have male or we

01:30:51.280 --> 01:30:54.640
can have female. And I suppose we could have other but in this data set, we just

01:30:54.640 --> 01:30:58.560
have male and we just have female. For class, we can have first, second, third.

01:30:58.800 --> 01:31:02.320
For deck, we can have unknown CA, I'm sure through all the letters of the

01:31:02.320 --> 01:31:04.720
alphabet, but that is still considered categorical.

01:31:05.320 --> 01:31:09.520
Now, what do we do with categorical data? Well, we always need to transform this

01:31:09.520 --> 01:31:14.160
data into numbers somehow. So what we actually end up doing is we encode

01:31:14.160 --> 01:31:18.640
this data using an integer value. So for the example of male and female, what

01:31:18.640 --> 01:31:21.960
we might say, and this is what we're going to do in a second is that female is

01:31:21.960 --> 01:31:26.120
represented by zero and male is represented by one. We do this because

01:31:26.120 --> 01:31:30.280
although it's interesting to know what the actual class is, the model doesn't

01:31:30.280 --> 01:31:32.960
care, right, female and male, it doesn't make a difference to it. It just needs

01:31:32.960 --> 01:31:36.080
to know that those values are the different are different or those values

01:31:36.080 --> 01:31:39.640
are the same. So rather than using strings and trying to find some way to

01:31:39.640 --> 01:31:43.280
pass that in and do math with that, we need to turn those into integers, we

01:31:43.280 --> 01:31:47.800
turn those into zeros and ones, right? Now for class, right, so first, second,

01:31:47.800 --> 01:31:50.320
third, you know, you guys can probably assume what we're going to encode this

01:31:50.320 --> 01:31:53.720
with, we're going to encode it with zero, one, two. Now, again, this doesn't

01:31:53.720 --> 01:31:57.680
necessarily need to be in order. So third could be represented by one and

01:31:57.680 --> 01:32:01.360
first could be represented by two, right? It doesn't need to be in order, it

01:32:01.360 --> 01:32:05.000
doesn't matter. So long as every third has the same number, every first has the

01:32:05.000 --> 01:32:07.800
same number and every second has the same number. And then same thing with

01:32:07.800 --> 01:32:11.520
that same thing with embark and same thing with alone. Now we could have an

01:32:11.520 --> 01:32:14.800
instance where you know, we've encoded every single one of these values with a

01:32:14.800 --> 01:32:19.000
different value. So in the, you know, rare occasion where there's one category

01:32:19.000 --> 01:32:23.640
that's categorical, and every single value in that category is different, then

01:32:23.640 --> 01:32:28.600
we will have, you know, 627 in this instance, different encoding labels that

01:32:28.640 --> 01:32:31.680
are going to be numbers, that's fine, we can do that. And actually, we don't

01:32:31.680 --> 01:32:35.320
really need to do that, because you're going to see how TensorFlow can handle

01:32:35.320 --> 01:32:39.280
that for us. So that is categorical data, numeric columns are pretty

01:32:39.280 --> 01:32:42.080
straightforward. They're anything that just have integer or float values

01:32:42.080 --> 01:32:46.760
already. So in this case, age and fair. And yeah, so that's what we've done. We've

01:32:46.760 --> 01:32:50.680
just defined our categorical columns here, and our numeric columns here. This is

01:32:50.680 --> 01:32:53.360
important because we're going to loop through them, which we're doing here to

01:32:53.360 --> 01:32:56.640
create something called feature columns, feature columns are nothing special.

01:32:56.680 --> 01:33:00.760
They're just what we need to feed to our linear estimator or linear model to

01:33:00.760 --> 01:33:04.200
actually make predictions. So kind of our steps here that we've gone through so

01:33:04.200 --> 01:33:09.200
far is import, load the data set, explore the data set, make sure we

01:33:09.200 --> 01:33:13.200
understand it, create our categorical columns and our numeric columns. So I've

01:33:13.200 --> 01:33:17.760
just hard coded these in right like sex, parts, class, deck, alone, all these

01:33:17.760 --> 01:33:21.040
ones. And then same thing with the numeric columns. And then for a linear

01:33:21.040 --> 01:33:24.480
estimator, we need to create these as feature columns using some kind of

01:33:24.480 --> 01:33:28.240
advanced syntax, which we're going to look at here. So we create a blank list,

01:33:28.240 --> 01:33:30.760
which is our feature columns, which will just store our different feature

01:33:30.760 --> 01:33:36.120
columns. We loop through each feature name in the categorical columns. And what

01:33:36.120 --> 01:33:41.120
we do is we define a vocabulary, which is equal to the data frame at that

01:33:41.120 --> 01:33:44.640
feature name. So first, we would start with sex, then we go and siblings, then

01:33:44.640 --> 01:33:49.760
we go parts, then we go class. And we get all of the different unique values. So

01:33:49.760 --> 01:33:53.360
that's actually what this does dot unique, gets a list of all unique values

01:33:53.400 --> 01:33:57.840
from the feature call. And I can print this out. She'll print this in a

01:33:57.840 --> 01:34:00.320
different line, we'll just take this value and have a look at actually what

01:34:00.320 --> 01:34:06.520
this is, right? So if I run, I guess we'll have to run all these in order. And

01:34:06.520 --> 01:34:10.720
then we'll create a new code block while we wait for that to happen. Let's see if

01:34:10.720 --> 01:34:22.360
we can get this installing fast enough. Run, run, run. Okay, now we go to df

01:34:22.360 --> 01:34:25.400
train. And we can see this is what this looks like. So these are all the

01:34:25.400 --> 01:34:28.960
different unique values that we had in that specific feature name. Now that

01:34:28.960 --> 01:34:33.880
feature name was what categorical columns? Oh, what I do feature name up,

01:34:33.880 --> 01:34:36.920
sorry, that's going to be the unique one. Let's just put rather than feature

01:34:36.920 --> 01:34:41.120
name. Let's put sex, right? And let's have a look at what this is. So we can

01:34:41.120 --> 01:34:44.920
see that the two unique values are male and female. Now I actually want to do

01:34:45.480 --> 01:34:48.800
what is it embark town? And I want to see what this one is. So how many

01:34:48.800 --> 01:34:51.920
different values we have. So we'll copy that in. And we can see we have

01:34:51.960 --> 01:34:56.520
Southampton cannot pronounce that and then the other cities and unknown. And

01:34:56.520 --> 01:34:59.360
that is kind of how we get the unique value. So that's what that method is

01:34:59.360 --> 01:35:02.160
doing there. Let's actually delete this code block because we don't need

01:35:02.160 --> 01:35:06.200
anymore. Alright, so that's what we do. And then what we do down here is we say

01:35:06.200 --> 01:35:10.600
feature columns dot append. So just add to this list, the TensorFlow feature

01:35:10.600 --> 01:35:15.120
column dot categorical column with vocabulary list. Now I know this is a

01:35:15.120 --> 01:35:17.760
mouthful, but this is kind of something again, you're just going to look up

01:35:17.760 --> 01:35:20.480
when you need to use it, right? So understand you need to make feature

01:35:20.480 --> 01:35:23.600
columns for linear regression, you don't really need to completely understand

01:35:23.600 --> 01:35:26.360
how, but you just need to know that that's something you need to do. And then

01:35:26.360 --> 01:35:29.960
you can look up the syntax and understand. So this is what this does. This is

01:35:29.960 --> 01:35:34.200
actually going to create for us a column, it's going to be in the form of a

01:35:34.200 --> 01:35:38.400
like NumPy array kind of that has the feature name. So whatever one we've

01:35:38.400 --> 01:35:42.760
looped through, and then all the different vocabulary associated with it. Now

01:35:42.760 --> 01:35:46.800
we need this because we just need to create this column so that we can create

01:35:46.800 --> 01:35:50.280
our model using those different columns, if that makes any sense. So our linear

01:35:50.280 --> 01:35:53.080
model needs to have, you know, all the different columns we're going to use, it

01:35:53.080 --> 01:35:56.400
needs to know all of the different entries that could be in that column, and

01:35:56.400 --> 01:36:00.280
needs to know whether this is a categorical column or a numeric column. In

01:36:00.280 --> 01:36:03.320
previous examples, what we might have done is actually change the data set

01:36:03.320 --> 01:36:07.320
manually, so encoded it manually. TensorFlow just can do this for us now

01:36:07.320 --> 01:36:11.440
in TensorFlow 2.0. So we'll just use that too. Okay, so that's what we did with

01:36:11.440 --> 01:36:14.960
these feature columns. Now for the numeric columns, a little bit different. It's

01:36:14.960 --> 01:36:17.800
actually easier. All we need to do is give the feature name and whatever the

01:36:17.840 --> 01:36:21.520
data type is and create a column with that. So notice we don't, we can omit

01:36:21.520 --> 01:36:24.040
this unique value, because we know when it's numeric, that you know, there

01:36:24.040 --> 01:36:26.840
could be an infinite amount of values. And then I've just printed out the

01:36:26.840 --> 01:36:29.640
feature columns, you can see what this looks like. So vocabulary list,

01:36:29.640 --> 01:36:33.280
categorical column gives us the number of siblings. And then the vocabulary

01:36:33.280 --> 01:36:37.120
list is these are all the different encoding values that is created. And

01:36:37.120 --> 01:36:40.400
then same thing, you know, we can go down here, parts, these are different

01:36:40.400 --> 01:36:43.720
encodings. So they're not necessarily in order is like what I was talking about

01:36:43.760 --> 01:36:50.560
before. Let's go do a numeric one. What do we have here? Yeah, so for a numeric

01:36:50.560 --> 01:36:54.400
column, just as is the key, that's the shape we're expecting. And this is the

01:36:54.400 --> 01:36:59.000
data type. So that is pretty much it. We're actually loading these in. So now

01:36:59.000 --> 01:37:02.760
it's almost time to create the model. So what we're going to do to create the

01:37:02.760 --> 01:37:07.120
model now is talk about first the training process and training some kind

01:37:07.120 --> 01:37:11.640
of, you know, machine learning model. Okay, so the training process. Now the

01:37:11.680 --> 01:37:14.880
training process of our model is actually fairly simple, at least for a

01:37:14.880 --> 01:37:19.400
linear model. Now, the way that we train the model is we feed it information,

01:37:19.400 --> 01:37:24.040
right? So we feed it that those data points from our data set. But how do we

01:37:24.040 --> 01:37:26.880
do that? Right? Like how do we feed that to the model? Do we just give it all at

01:37:26.880 --> 01:37:31.200
once? Well, in our case, we only have 627 rows, which isn't really that much

01:37:31.200 --> 01:37:34.360
data, like we can fit that in RAM in our computer, right? But what if we're

01:37:34.360 --> 01:37:38.680
training a crazy machine learning model, and we have, you know, 25 terabytes of

01:37:38.680 --> 01:37:42.240
data that we need to pass it, we can't load that into RAM, at least I don't

01:37:42.240 --> 01:37:45.360
know any RAM that's that large. So we need to find a way that we can kind of

01:37:45.360 --> 01:37:49.720
load it in what's called batches. So the way that we actually load this model is

01:37:49.720 --> 01:37:53.080
we load it in batches. Now we don't need to understand really kind of how this

01:37:53.080 --> 01:37:58.200
process works and how batching kind of occurs. What we do is give 32 entries at

01:37:58.200 --> 01:38:02.640
once to the model. Now the reason we don't just feed one at a time is because

01:38:02.640 --> 01:38:07.440
that's a lot slower, we can load, you know, a small batch size of 32, that can

01:38:07.480 --> 01:38:10.520
increase our speed dramatically. And that's kind of a lower level

01:38:10.520 --> 01:38:14.080
understanding. So I'm not going to go too far into that. Now that we understand, we

01:38:14.080 --> 01:38:17.280
kind of load it in batches, right? So we don't load it entirely all at once, we

01:38:17.280 --> 01:38:22.720
just load a specific set of kind of elements as we go. What we have is

01:38:22.720 --> 01:38:28.080
called epochs. Now what are epochs? Well, epochs are essentially how many times the

01:38:28.080 --> 01:38:32.400
model is going to see the same data. So we might be the case, right? And when we

01:38:32.400 --> 01:38:36.160
pass the data to our model, the first time, it's pretty bad, like it looks at the

01:38:36.160 --> 01:38:39.320
model creates our line of best fit, but it's not great, it's not working

01:38:39.320 --> 01:38:42.600
perfectly. So we need to use something called an epoch, which means we're just

01:38:42.600 --> 01:38:46.560
going to feed the model, feed the data again, but in a different order. So we

01:38:46.560 --> 01:38:50.840
do this multiple times, so that the model will look at a data, look at the data in

01:38:50.840 --> 01:38:54.720
a different way, then kind of a different form, and see the same data a few

01:38:54.720 --> 01:38:58.080
different times and pick up on patterns. Because the first time it sees a new

01:38:58.080 --> 01:39:01.240
data point, it's probably not going to have a good idea how to make a prediction

01:39:01.240 --> 01:39:05.600
for that. So if we can feed it more and more and more, then you know, we can get

01:39:05.640 --> 01:39:08.440
a better prediction. Now, this is where we talk about something called

01:39:08.480 --> 01:39:13.720
overfitting, though, sometimes we can see the data too much, we can pass too much

01:39:13.720 --> 01:39:17.640
data to our model, to the point where it just straight up memorizes those data

01:39:17.640 --> 01:39:21.720
points. And it's, it's really good at classifying for those data points. But

01:39:21.720 --> 01:39:25.520
when we pass it some new data points, like our testing data, for example, it's

01:39:25.520 --> 01:39:30.120
horrible at kind of, you know, classifying those. So what we do to kind of

01:39:30.120 --> 01:39:33.480
prevent this from happening, is we just make sure that we start with like a

01:39:33.520 --> 01:39:36.360
lower amount of epochs, and then we can work our way up and kind of

01:39:36.360 --> 01:39:40.440
incrementally change that if we need to, you know, go higher, right, we need more

01:39:40.440 --> 01:39:44.960
epochs. So yeah, so that's kind of it for epochs. Now, I will say that this

01:39:44.960 --> 01:39:49.000
training process kind of applies to all the different, what is it, machine learning

01:39:49.000 --> 01:39:51.960
models that we're going to look at, we have epochs, we have batches, we have a

01:39:51.960 --> 01:39:56.760
batch size, and now we have something called an input function. Now, this is

01:39:57.080 --> 01:40:01.000
pretty complicated. This is the code for the input function. I don't like that we

01:40:01.000 --> 01:40:05.880
need to do this. But it's necessary. So essentially, what an input function is

01:40:05.920 --> 01:40:10.320
is the way that we define how our data is going to be broke into epochs and into

01:40:10.320 --> 01:40:14.720
batches to feed to our model. Now, these, you probably aren't ever going to

01:40:14.720 --> 01:40:18.720
really need to code like from scratch by yourself. But this is the one I've just

01:40:18.720 --> 01:40:21.360
stolen from the TensorFlow website, pretty much like everything else that's

01:40:21.360 --> 01:40:28.280
in the series. And what this does is it takes our data and encodes it in a tf

01:40:28.320 --> 01:40:34.080
dot data dot data set object. Now, this is because our model needs this specific

01:40:34.120 --> 01:40:38.040
object to be able to work, it needs to see a data set object to be able to use

01:40:38.040 --> 01:40:42.320
that data to create the model. So what we need to do is take this pandas data

01:40:42.320 --> 01:40:46.200
frame, we need to turn it into that object. And the way we do that is with

01:40:46.200 --> 01:40:50.320
the input function. So we can see that what this is doing here. So this is make

01:40:50.360 --> 01:40:54.280
input function, we actually have a function defined inside of another function. I

01:40:54.280 --> 01:40:57.640
know this is kind of complicated for some of you guys. But and what I'm

01:40:57.640 --> 01:41:00.160
actually gonna do, sorry, I'm gonna just copy this into the other page, because I

01:41:00.160 --> 01:41:03.280
think it's easier to explain without all the text around. So let's create a new

01:41:03.280 --> 01:41:08.360
code block. Let's paste this in. And let's have a look at what this does. So

01:41:08.360 --> 01:41:13.240
actually, let me just tab down. Okay, so make input function, we have our

01:41:13.240 --> 01:41:17.080
parameters data data frame, which is our pandas data frame, our label data

01:41:17.080 --> 01:41:22.640
frame, which stands for those labels. So that y train or that eval y eval, right,

01:41:22.840 --> 01:41:25.680
we have a number of epochs, which is how many epochs we're going to do, we set

01:41:25.720 --> 01:41:30.760
the default 10 shuffle, which means are we going to shuffle our data and mix it

01:41:30.760 --> 01:41:35.080
up before we pass it to the model, and batch size, which is how many elements

01:41:35.120 --> 01:41:40.000
are we going to give to the data to the model? Well, it's training at once. Now,

01:41:40.000 --> 01:41:44.720
what this does is we have an input function defined inside of this function. And

01:41:44.720 --> 01:41:51.120
we say data set equals tensor frame dot data dot data set from tensor slices,

01:41:51.360 --> 01:41:55.320
dict data frame, a label data frame. Now, what this does, and we can read the

01:41:55.320 --> 01:41:59.240
comment, I mean, create a tf dot data dot data set object with the data and

01:41:59.240 --> 01:42:03.360
its label. Now, I can't explain to you like how this works on a lower level. But

01:42:03.360 --> 01:42:07.840
essentially, we pass a dictionary representation of our data frame, which is

01:42:07.840 --> 01:42:11.840
whatever we passed in here. And then we pass the label data frame, which is

01:42:11.840 --> 01:42:16.120
going to be, you know, all those y values. And we create this object. And

01:42:16.120 --> 01:42:20.560
that's what this line of code does. So tf data dot data set from tensor slices,

01:42:20.680 --> 01:42:23.320
which is just what you're going to use. I mean, we can read this documentation,

01:42:23.320 --> 01:42:27.320
create a data set whose elements are slices of the given tensors, the given

01:42:27.320 --> 01:42:31.000
tensors are sliced along their first dimension, this operation preserves the

01:42:31.000 --> 01:42:34.600
structure of the input tensors, removing the first dimension of each tensor and

01:42:34.600 --> 01:42:37.760
using it as the data set dimension. So I mean, you guys can look at that, like

01:42:37.760 --> 01:42:40.960
read through the documentation, if you want. But essentially, what it does is

01:42:40.960 --> 01:42:48.120
create the data set object for us. Now, if shuffle DS equals DS dot shuffle 1000,

01:42:48.280 --> 01:42:50.880
what this does is just shuffle the data set, you don't really need to

01:42:50.880 --> 01:42:55.040
understand more than that. And then what we do is we say data set equals data set

01:42:55.040 --> 01:42:59.240
dot batch, the batch size, which is going to be 32, and then repeat for the

01:42:59.240 --> 01:43:02.760
number of epochs. So what this is going to do is essentially take our data set

01:43:02.760 --> 01:43:08.280
and split it into the number of, I don't want to, what do I want to call it? Like

01:43:08.280 --> 01:43:12.480
blocks that are going to be passed to our model. So we can do this by knowing

01:43:12.480 --> 01:43:15.800
the batch size, it obviously knows how many elements because that's the data set

01:43:15.800 --> 01:43:20.040
object itself, and then repeat number of epochs. So this can figure out, you know,

01:43:20.080 --> 01:43:26.480
how many one how many blocks do I need to split it into to feed it to my model. Now

01:43:26.480 --> 01:43:30.560
return data set, simply from this function here, we'll return that data set

01:43:30.600 --> 01:43:34.880
object. And then on the outside return, we actually return this function. So

01:43:34.880 --> 01:43:38.440
what this out exterior function does, and I'm really just trying to break this

01:43:38.440 --> 01:43:42.840
down. So you guys understand is make an input function, it literally makes a

01:43:42.840 --> 01:43:48.320
function and returns the function object to wherever we call it from. So that's

01:43:48.320 --> 01:43:52.920
how that works. Now we have a train input function and an eval input function. And

01:43:52.920 --> 01:43:56.160
what we need to do to create these, it's just use this function that we've

01:43:56.160 --> 01:44:01.320
defined above. So we say make input function, df train, y train, so our data

01:44:01.320 --> 01:44:04.960
frame for training and our data frame for the labels of that. So we can see the

01:44:04.960 --> 01:44:08.320
comment, you know, here, we will call the input function, right? And then eval

01:44:08.320 --> 01:44:12.200
train, so it's going to be the same thing, except for the evaluation, we don't

01:44:12.240 --> 01:44:15.680
need to shuffle the data because we're not training it, we only need one epoch.

01:44:15.720 --> 01:44:19.720
Because again, we're just training it. And we'll pass the evaluation data set and

01:44:19.720 --> 01:44:25.440
the evaluation value from Y. Okay, so that's it for making the input function. Now

01:44:25.440 --> 01:44:29.600
I know this is complicated, but that's the way we have to do it. And unfortunately,

01:44:29.600 --> 01:44:32.520
if you don't understand after that, there's not much more I can do, you might

01:44:32.520 --> 01:44:36.320
just have to read through some of the documentation. Alright, creating the

01:44:36.320 --> 01:44:39.240
model, we are finally here, I know this has been a while, but I need to get

01:44:39.280 --> 01:44:42.360
through everything. So linear estimate, so we're going to copy this, and I'm just

01:44:42.360 --> 01:44:46.200
going to put it in here and we'll talk about what this does. So linear underscore

01:44:46.200 --> 01:44:52.520
EST equals tf dot estimator dot linear classifier, and we're giving it the feature

01:44:52.520 --> 01:44:55.640
columns that we created up here. So this work was not for nothing, we have this

01:44:55.640 --> 01:45:00.360
feature column, which defines, you know, what is in every single, like what should

01:45:00.360 --> 01:45:05.400
we expect for our input data, we pass that to a linear classifier object from

01:45:05.400 --> 01:45:10.320
the estimator module from TensorFlow. And then that creates the model for us. Now,

01:45:10.440 --> 01:45:13.160
this again is syntax that you don't need to memorize, you just need to understand

01:45:13.160 --> 01:45:16.800
how it works. What we're doing is creating an estimator, all of these kind of core

01:45:16.800 --> 01:45:20.480
learning algorithms use what's called estimators, which are just basic

01:45:20.480 --> 01:45:24.040
implementations of algorithms and TensorFlow. And again, pass the feature

01:45:24.040 --> 01:45:28.880
columns. That's how that works. Alright, so now let's go to training the model. Okay,

01:45:28.880 --> 01:45:31.360
so I'm just going to copy this again, I know you guys think I'm just copying the

01:45:31.360 --> 01:45:34.360
code back and forth, but I'm not going to memorize the syntax, I just want to

01:45:34.360 --> 01:45:37.480
explain to you how all this works. And again, you guys will have all this code,

01:45:37.480 --> 01:45:41.840
you can mess with it, play with it, and learn on your own that way. So to train

01:45:41.880 --> 01:45:46.400
is really easy. All we need to do, I say linear EST dot train, and then just give

01:45:46.400 --> 01:45:50.400
that input function. So that input function that we created up here, right,

01:45:50.400 --> 01:45:54.920
which was returned from make input function, like this train input function

01:45:54.920 --> 01:45:59.600
here is actually equal to a function, it's equal to a function object itself. If I

01:45:59.640 --> 01:46:05.160
were to call train underscore input function like this, this would actually

01:46:05.200 --> 01:46:09.120
call this function. That's how this works in Python. It's a little bit of a

01:46:09.120 --> 01:46:13.920
complicated syntax, but that's how it works. We pass that function here. And then

01:46:13.920 --> 01:46:17.680
this will use the function to grab all of the input that we need and train the

01:46:17.680 --> 01:46:21.880
model. Now the result is going to be rather than trained, we're going to

01:46:21.880 --> 01:46:25.240
evaluate right and notice that we didn't store this one in a variable, but we're

01:46:25.240 --> 01:46:29.360
storing the result in a variable so that we can look at it. Now clear output is

01:46:29.400 --> 01:46:32.520
just from what we import above just going to clear the console output, because

01:46:32.520 --> 01:46:35.880
there will be some output while we're training. And then we can present print

01:46:35.920 --> 01:46:40.440
the accuracy of this model. So let's actually run this and see how this

01:46:40.440 --> 01:46:43.640
works. This will take a second. So I'll be back once this is done. Okay, so we're

01:46:43.640 --> 01:46:49.680
back, and we've got a 73.8% accuracy. So essentially what we've done right is

01:46:49.680 --> 01:46:52.560
we've trained the model, you might have seen a bunch of output while you were

01:46:52.560 --> 01:46:56.640
doing this on your screen. And then we printed out the accuracy after

01:46:56.680 --> 01:47:00.480
evaluating the model. This accuracy isn't very good. But for our first shot,

01:47:00.480 --> 01:47:04.680
this okay, we're going to talk about how to improve this in a second. Okay, so

01:47:04.680 --> 01:47:08.040
we've evaluated the data set, we stored that in result. I want to actually look

01:47:08.040 --> 01:47:12.120
at what result is, because obviously you can see we've referenced the accuracy

01:47:12.120 --> 01:47:15.800
part, like you know, as if this was a Python dictionary. So let's run this one

01:47:15.800 --> 01:47:19.440
more time. Oh, this is going to take a second again. So okay, so we printed out

01:47:19.440 --> 01:47:22.520
result here, and we can see that we have actually a bunch of different values. So

01:47:22.520 --> 01:47:26.560
we have accuracy, accuracy baseline, AUC, and all these different kinds of

01:47:26.560 --> 01:47:30.240
statistical values. Now, these aren't really going to mean much to you guys, but

01:47:30.240 --> 01:47:33.360
I just want to show you that we do have those statistics and to access any

01:47:33.360 --> 01:47:36.480
specific one, this is really just a dictionary object. So we can just

01:47:36.480 --> 01:47:40.520
reference the key that we want, which is what we did with accuracy. Now, notice

01:47:40.760 --> 01:47:45.520
our accuracy actually changed here. We went to 76. The reason for this is like

01:47:45.520 --> 01:47:48.680
I said, you know, our data is getting shuffled, it's getting put in a different

01:47:48.680 --> 01:47:52.400
order. And based on the order in which we see data, our model will, you know,

01:47:52.440 --> 01:47:56.360
make different predictions and be trained differently. So if we had, you know,

01:47:56.720 --> 01:48:01.360
another epoch, right, if I change epochs to say 11 or 15, our accuracy will

01:48:01.360 --> 01:48:04.160
change. Now it might go up, it might go down. That's something we have to play

01:48:04.160 --> 01:48:07.480
with as you know, our machine, a machine learning developer, right? That's what

01:48:07.480 --> 01:48:11.120
your goal is, is to get the most accurate model. Okay, so now it's time to

01:48:11.120 --> 01:48:14.240
actually use the model to make predictions. So up until this point, we've

01:48:14.240 --> 01:48:17.600
just been doing a lot of work to understand how to create the model, you

01:48:17.600 --> 01:48:21.560
know, what the model is, how we make an input function, training, testing data, I

01:48:21.560 --> 01:48:26.160
know a lot, a lot, a lot of stuff. Now to actually use this model and like make

01:48:26.240 --> 01:48:30.000
accurate predictions with it is somewhat difficult, but I'm going to show you

01:48:30.000 --> 01:48:34.840
how. So essentially, TensorFlow models are built to make predictions on a lot

01:48:34.840 --> 01:48:39.320
of things at once. They're not great at making predictions on like one piece of

01:48:39.320 --> 01:48:43.040
data, you just want like one passenger to make a prediction for, they're much

01:48:43.040 --> 01:48:46.600
better at working in like large batches of data. Now you can definitely do it

01:48:46.600 --> 01:48:50.160
with one, but I'm just going to show you how we can make a prediction for every

01:48:50.200 --> 01:48:54.600
single point that's in that evaluation data set. So right now we looked at the

01:48:54.600 --> 01:48:58.440
accuracy, and the way we determine the accuracy was by essentially comparing

01:48:58.600 --> 01:49:02.840
the results that the predictions gave from our model versus what the actual

01:49:02.840 --> 01:49:06.400
results were for every single one of those passengers. And that's how we came

01:49:06.400 --> 01:49:11.800
up with an accuracy of 76%. Now if we want to actually check and get predictions

01:49:11.800 --> 01:49:15.400
from the model and see what those actual predictions are, what we can do is use

01:49:15.440 --> 01:49:19.760
a method called dot predict. So what I'm going to do is I'm going to say, I

01:49:19.760 --> 01:49:25.240
guess, results like this equals, and in this case, we're going to do the model

01:49:25.240 --> 01:49:31.520
name, which is linear EST dot predict. And then inside here, what we're going to

01:49:31.520 --> 01:49:36.520
pass is that input function we use for the evaluation. So just like, you know, we

01:49:36.520 --> 01:49:40.160
need to pass an input function to actually train the model, we also need to

01:49:40.160 --> 01:49:43.680
pass an input function to make a prediction. Now this input function could

01:49:43.680 --> 01:49:46.800
be a little bit different, we can modify this a bit if we wanted to, but to keep

01:49:46.800 --> 01:49:50.280
things simple, we use the same one for now. So what I'm going to do is just use

01:49:50.280 --> 01:49:53.360
this eval input function. So the one we've already created where we did, you

01:49:53.360 --> 01:49:57.480
know, one epoch, we don't need to shuffle because it's just the evaluation set. So

01:49:57.480 --> 01:50:01.640
inside here, we're going to eval input function. Now what we need to do though is

01:50:01.640 --> 01:50:05.400
convert this to a list, just because we're going to loop through it. And I'm

01:50:05.400 --> 01:50:08.480
actually going to print out this value so we can see what it is before we go to

01:50:08.480 --> 01:50:13.440
the next step. So let's run this and have a look at what we get. Okay, so we get

01:50:13.480 --> 01:50:18.320
logistics array, we can see all these different values. So we have, you know,

01:50:18.320 --> 01:50:23.840
this array with this value, we have probabilities, this value. And this is

01:50:23.840 --> 01:50:26.440
kind of what we're getting. So we're getting logistic, all classes, like

01:50:26.440 --> 01:50:29.720
there's all this random stuff. What you hopefully should notice, and I know I'm

01:50:29.720 --> 01:50:33.440
just like whizzing through is that we have a dictionary that represents the

01:50:33.440 --> 01:50:37.240
predictions. And I'll see if I can find the end of the dictionary here for every

01:50:37.240 --> 01:50:43.800
single, what is it prediction? So since we've passed, you know, 267 input data

01:50:43.800 --> 01:50:48.360
from this, you know, eval input function, what was returned to us is a list of all

01:50:48.360 --> 01:50:51.880
of these different dictionaries that represent each prediction. So what we

01:50:51.880 --> 01:50:56.680
need to do is look at each dictionary so that we can determine what the actual

01:50:56.680 --> 01:51:02.440
prediction was. So what I'm going to do is actually just present to result. I'm

01:51:02.440 --> 01:51:05.120
wondering to result zero, because this is a list. So that should mean we can

01:51:05.120 --> 01:51:09.200
index it. So we can actually look at one prediction. Okay, so this is the

01:51:09.280 --> 01:51:13.440
dictionary of one prediction. So I know this seems like a lot. But this is what

01:51:13.440 --> 01:51:16.840
we have. This is our prediction. So logistics, we get some array, we have

01:51:16.840 --> 01:51:21.440
logistic in here in this dictionary, and then we have probabilities. So what I

01:51:21.440 --> 01:51:26.240
actually want is probability. Now, since what we ended up having was a

01:51:26.240 --> 01:51:30.120
prediction of two classes, right, either zero or one, we're predicting either

01:51:30.120 --> 01:51:33.800
someone survived, or they didn't survive, or what their percentage should be. We

01:51:33.800 --> 01:51:39.040
can see that the percentage of survival here is actually 96%. And the percentage

01:51:39.080 --> 01:51:43.520
that it thinks that it won't survive is, you know, 3.3%. So if we want to

01:51:43.560 --> 01:51:49.680
access this, what we need to do is click do result at some index. So whatever,

01:51:49.680 --> 01:51:53.080
you know, one we want. So we're gonna say result. And then here, we're going to

01:51:53.080 --> 01:51:56.680
put probabilities. So I'm just going to print that like that. And then we can

01:51:56.680 --> 01:52:00.680
see the probabilities. So let's run this. And now we see our probabilities are

01:52:00.720 --> 01:52:06.880
96 and 33. Now, if we want the probability of survival, so I think I

01:52:06.880 --> 01:52:10.640
actually might have messed this up, I'm pretty sure the survival probability is

01:52:10.640 --> 01:52:14.400
actually the last one, whereas like the non survival is the first one, because

01:52:14.400 --> 01:52:17.400
zero means you didn't survive and one means you did survive. So that's my

01:52:17.400 --> 01:52:20.960
bad, I messed that up. So I actually want their chance of survival, I'll index

01:52:20.960 --> 01:52:26.720
one. So if I index one, you see we get 3.3%. But if I wanted their chance of

01:52:26.720 --> 01:52:30.800
survival, not surviving, I would index zero. And that makes sense because zero

01:52:30.800 --> 01:52:34.400
is you know, what we're looking like zero represents they didn't survive, whereas

01:52:34.400 --> 01:52:38.440
one represents they did survive. So that's kind of how we do that. So that's

01:52:38.440 --> 01:52:41.960
how we get them. Now, if we wanted to loop through all of these, we could we

01:52:41.960 --> 01:52:45.000
could loop through every dictionary, we could print every single probability of

01:52:45.000 --> 01:52:48.560
each person. We can also look at that person's stats and then look at their

01:52:48.560 --> 01:52:53.000
probability. So let's see the probability of surviving is in this case, you

01:52:53.000 --> 01:52:57.320
know, 3%, or whatever it was 3.3%. But let's look at the person that we were

01:52:57.320 --> 01:53:01.880
actually predicting them and see if that makes sense. So if I go eval, or what

01:53:01.880 --> 01:53:09.240
was it df eval dot loc, zero, we print that and then we print the result. What

01:53:09.240 --> 01:53:13.840
we can see is that for the person who was male and 35 that had no siblings, their

01:53:13.840 --> 01:53:16.680
fair was this, they're in third class, we don't know what deck they were on and

01:53:16.680 --> 01:53:21.560
they were alone. They have a 3.3% chance of survive. Now, if we change this, we

01:53:21.600 --> 01:53:26.000
could go like to to let's have a look at this second person and see what their

01:53:26.000 --> 01:53:29.800
chance of survival is. Okay, so they have a higher percent chance of 38% chance

01:53:29.840 --> 01:53:32.840
they're female, they're a little bit older. So that might be a reason why their

01:53:32.840 --> 01:53:36.840
survival rates a bit lower. I mean, we can keep doing this and look through and

01:53:36.840 --> 01:53:41.880
see what it is, right? If we want to get the actual value, like if this person

01:53:41.880 --> 01:53:48.040
survived, or if they didn't survive, and what I can do is I can print df eval.

01:53:48.720 --> 01:53:53.160
Actually, it's not going to be eval, it's going to be y underscore eval. Yeah. And

01:53:53.160 --> 01:53:58.240
that's going to be loc three. Now this will give us if they survived or not. So

01:53:58.240 --> 01:54:02.040
actually, in this case, that person did survive, but we're only predicting a 32%.

01:54:02.320 --> 01:54:05.200
So you can see that that's, you know, represented in the fact that we only have

01:54:05.200 --> 01:54:09.720
about a 76% accuracy, because this model is not perfect. And in this instance, it

01:54:09.720 --> 01:54:13.560
was pretty bad. It's saying they have a 32% chance of surviving, but they actually

01:54:13.560 --> 01:54:16.400
did survive. So maybe that should be higher, right? So we could change this

01:54:16.400 --> 01:54:21.560
number, go for four, I'm just messing around and showing you guys, you know, how

01:54:21.560 --> 01:54:25.920
we use this. So in this one, you know, same thing, this person survived, although

01:54:26.800 --> 01:54:30.640
what is it, they only were given a 14% chance of survival. So anyways, that is

01:54:30.640 --> 01:54:34.000
how that works. This is how we actually make predictions and look at the

01:54:34.000 --> 01:54:37.440
predictions, you understand that now what's happening is I've converted this to

01:54:37.440 --> 01:54:40.640
a list just because this is actually a generator object, which means it's meant

01:54:40.640 --> 01:54:43.560
to just be looped through rather than just look at it with a list, but that's

01:54:43.560 --> 01:54:47.680
fine, we'll use a list. And then we can just print out, you know, result at

01:54:47.680 --> 01:54:51.080
whatever index probabilities, and then one to represent their chance of

01:54:51.080 --> 01:54:55.000
survival. Okay, so that has been it for linear regression. Now let's get into

01:54:55.000 --> 01:54:59.200
classification. And now we are on to classification. So essentially,

01:54:59.200 --> 01:55:04.680
classification is differentiating between, you know, data points and separating

01:55:04.680 --> 01:55:08.400
them into classes. So rather than predicting a numeric value, which we did

01:55:08.400 --> 01:55:11.840
with regression earlier, so linear regression, and you know, the percentage

01:55:11.880 --> 01:55:16.520
survival chance, which is a numeric value, we actually want to predict classes. So

01:55:16.520 --> 01:55:21.160
what we're going to end up doing is predicting the probability that a specific

01:55:21.160 --> 01:55:24.880
data point or a specific entry or whatever we're going to call it is within

01:55:24.880 --> 01:55:27.960
all of the different classes it could be. So for the example here, we're going to

01:55:27.960 --> 01:55:32.560
use flowers. So it's called the iris. I think it's the iris flower data set or

01:55:32.560 --> 01:55:35.360
something like that. And we're going to use some different properties of flowers

01:55:35.360 --> 01:55:38.480
to predict what species of flower it is. So that's the difference between

01:55:38.480 --> 01:55:42.000
classification and regression. Now I'm not going to talk about the specific

01:55:42.000 --> 01:55:45.200
algorithm we're going to use here for classification, because there's just so

01:55:45.240 --> 01:55:50.640
many different ones you can use. But yeah, I mean, if you really care about how

01:55:50.640 --> 01:55:54.000
they work on a lower mathematical level, I'm not going to be explaining that

01:55:54.000 --> 01:55:57.120
because it doesn't make sense to explain it for one algorithm when there's like

01:55:57.160 --> 01:56:00.840
hundreds and they all work a little bit differently. So you guys can kind of look

01:56:00.840 --> 01:56:04.880
that up. And I'll tell you some resources and where you can find that. I'm also

01:56:04.880 --> 01:56:07.880
going to go faster through this example, just because I've already covered kind

01:56:07.880 --> 01:56:10.960
of a lot of the fundamental stuff in linear regression. So hopefully we should

01:56:10.960 --> 01:56:14.360
get this one done a little bit quicker and move on to the next kind of aspects

01:56:14.360 --> 01:56:18.720
in this series. Alright, so first steps, load tensor flow, import tensor flow, we've

01:56:18.720 --> 01:56:22.440
done that already, data set, we need to talk about this. So the data that we're

01:56:22.440 --> 01:56:26.560
using is that iris flowers data set like I talked about. And this specific data set

01:56:26.560 --> 01:56:29.680
separates flowers into three different species. So we have these different

01:56:29.680 --> 01:56:33.560
species. This is the information we have. So septal length width, petal length,

01:56:33.560 --> 01:56:36.480
petal width, we're going to use that information obviously to make the

01:56:36.480 --> 01:56:40.840
predictions. So given this information, you know, in our final model, can it tell

01:56:40.840 --> 01:56:45.040
us which one of these flowers it's most likely to be? Okay, so what we're going to

01:56:45.040 --> 01:56:49.320
do now is define the CSV column names and species. So the column names are just

01:56:49.320 --> 01:56:52.640
going to define what we're going to have in our data set is like the headers for

01:56:52.640 --> 01:56:56.240
the columns, species obviously is just the species and we'll throw them there.

01:56:57.240 --> 01:57:00.240
Alright, so now we're going to load in our data sets. So this is going to be

01:57:00.240 --> 01:57:03.040
different every time you're kind of working with models, depending on where

01:57:03.040 --> 01:57:05.440
you're getting your data from. In our example, we're going to get it from

01:57:05.480 --> 01:57:10.280
Keras, which is kind of in sub module of TensorFlow. It has a lot of useful

01:57:10.280 --> 01:57:15.040
data sets and tools that we'll be using throughout the series. But keras.utils.get

01:57:15.040 --> 01:57:18.400
file, again, don't really focus on this, just understand what this is going to

01:57:18.400 --> 01:57:23.360
do is save this file onto our computer as iris training dot CSV, grab it from

01:57:23.360 --> 01:57:27.120
this link. And then what we're going to do down here is load the train and test

01:57:27.120 --> 01:57:31.280
and again, notice this training and this is testing into two separate data

01:57:31.280 --> 01:57:35.040
frames. So here we're going to use the names of the columns as the CSV column

01:57:35.040 --> 01:57:38.240
names, we're going to use the path as whatever we loaded here, header equals

01:57:38.240 --> 01:57:43.320
zero, which just means row zero is the header. Alright, so now we will move down

01:57:43.320 --> 01:57:46.560
and we'll have a look at our data set. So like we've done before, oh, I've got

01:57:46.560 --> 01:57:51.200
to run this code first. CSV column names. Okay, so we've just we're just running

01:57:51.200 --> 01:57:55.200
things in the wrong order here, apparently. Okay, so let's look at the head. So

01:57:55.200 --> 01:57:58.360
we can see this is kind of what our data frame looks like. And notice that our

01:57:58.360 --> 01:58:03.080
species here are actually defined numerically. So rather than before, when

01:58:03.080 --> 01:58:06.080
we had to do that thing where you know, we made those feature columns, and we

01:58:06.080 --> 01:58:09.800
converted the categorical data into numeric data with those kind of weird

01:58:09.800 --> 01:58:14.360
TensorFlow tools. This is actually already encoded for us. Now zero stands

01:58:14.360 --> 01:58:19.200
for SITOSA. And then one and two obviously stand for these ones respectively. And

01:58:19.200 --> 01:58:24.080
that's how that works. Now these I believe are in centimeters, the septal length,

01:58:24.320 --> 01:58:28.240
petal length, petal width, that's not super important. But sometimes you do

01:58:28.240 --> 01:58:32.400
want to know that information. Okay, so now we're going to pop up those columns

01:58:32.400 --> 01:58:36.480
for the species like we did before and separate that into train white test, why

01:58:36.480 --> 01:58:41.040
and then have a look at the head again. So let's do that and run this notice that

01:58:41.040 --> 01:58:45.200
is gone. Again, we've talked about how that works. And then these, if we want to

01:58:45.200 --> 01:58:49.200
have a look at them, and actually, let's do this, by just having a new block,

01:58:49.200 --> 01:58:57.880
let's say train underscore y dot, what is it dot head? If I could spell head

01:58:57.880 --> 01:59:01.720
correctly. Okay, so we run head, and we can see this is what it looks like

01:59:01.720 --> 01:59:05.400
nothing special. That's what we're getting. Alright, so let's delete that. Let's

01:59:05.400 --> 01:59:08.440
look at the shape of our training data. I mean, we can probably guess what it is

01:59:08.440 --> 01:59:12.000
already, right? We're going to have shape four, because we have four features. And

01:59:12.000 --> 01:59:16.040
then how many entries do we have? Well, I'm sure this will tell us so 120 entries

01:59:16.200 --> 01:59:20.520
in shape four. Awesome. That's our shape. Okay, input function. So we're moving

01:59:20.520 --> 01:59:22.920
fast here already, we're getting into a lot of the coding. So what I'm actually

01:59:22.920 --> 01:59:26.360
going to do is again, copy this over into a separate document, and I'll be back in

01:59:26.360 --> 01:59:29.840
a second with all that. Okay, so input function time, we already know what the

01:59:29.880 --> 01:59:34.040
input function does because we used it previously. Now this input function is a

01:59:34.040 --> 01:59:38.360
little bit different than before, just because we're kind of changing things

01:59:38.360 --> 01:59:43.640
slightly. So here, we don't actually have any, what do you call it, we don't have

01:59:43.640 --> 01:59:47.240
any epochs, and our batch size is different. So what we've done here is

01:59:47.240 --> 01:59:52.280
rather than actually, you know, defining like make input function, we just have

01:59:52.400 --> 01:59:56.880
input function like this. And what we're going to do is a little bit different

01:59:56.880 --> 01:59:59.560
when we pass this input function, I'll kind of show you it's a little bit more

01:59:59.560 --> 02:00:02.680
complicated. But you can see that we've cleaned this up a little bit. So exactly

02:00:02.680 --> 02:00:05.640
we're doing what we did do before, we're converting this data, which is our

02:00:05.640 --> 02:00:09.360
features, which we're passing in here into a data set. And then we're passing

02:00:09.360 --> 02:00:13.560
those labels as well. And then if we're training, so if training is true, what

02:00:13.560 --> 02:00:17.440
we're going to do is say data set is equal to the data set dot shuffle. So we're

02:00:17.440 --> 02:00:21.280
going to shuffle that information, and then repeat that. And that is all we

02:00:21.280 --> 02:00:25.920
really need to do. We can do data set dot batch at the batch size 256 return that.

02:00:26.240 --> 02:00:29.520
And we're good to go. So this is our input function. Again, these are kind of

02:00:29.520 --> 02:00:33.040
complicated. You kind of have to just get experience seeing a bunch of

02:00:33.040 --> 02:00:36.320
different ones to understand how to actually make one on your own. For now

02:00:36.320 --> 02:00:39.360
on, don't worry about it too much. You can pretty much just copy the input

02:00:39.360 --> 02:00:42.880
functions you've created before, and modify them very slightly if you're

02:00:42.880 --> 02:00:45.480
going to be doing your own models. But by the end of this, you should have a

02:00:45.480 --> 02:00:48.520
good idea of how these input functions work. We will have seen like four or

02:00:48.520 --> 02:00:51.760
five different ones. And then, you know, we can kind of mess with them and tweak

02:00:51.760 --> 02:00:55.800
them as we go on, but don't focus on it too much. Okay, so input function, this

02:00:55.840 --> 02:00:59.000
is our input function, I'm not really going to go in too much more detail with

02:00:59.000 --> 02:01:03.200
that. And now our feature columns. So this is again, pretty straightforward for

02:01:03.200 --> 02:01:06.080
the feature columns. All we need to do for this is since our all numeric feature

02:01:06.080 --> 02:01:09.120
columns is rather than having two for loops where we were separating the

02:01:09.120 --> 02:01:13.960
numeric and categorical feature columns before, we can just loop through all of

02:01:13.960 --> 02:01:18.240
the keys in our training data set. And then we can append to my feature

02:01:18.240 --> 02:01:23.240
columns blank list, the feature column, the numeric column, and the key is

02:01:23.240 --> 02:01:26.200
equal to whatever key we've looped through here. Now I'm going to show you

02:01:26.200 --> 02:01:29.560
what this means in case anyone's confused. Again, you can see when I print my

02:01:29.560 --> 02:01:34.120
feature columns, we get key equals septal length, we get our shape, and we get

02:01:34.160 --> 02:01:37.640
all of that other nice information. So let's copy this into the other one and

02:01:37.640 --> 02:01:42.840
have a look at our output after this. Okay, so my feature columns for key and

02:01:42.840 --> 02:01:48.360
train dot keys. So notice train is here, train dot keys. What that does is

02:01:48.360 --> 02:01:52.360
actually give us all the columns. So this was a really quick and easy way to

02:01:52.360 --> 02:01:55.280
kind of loop through all the different columns. Although I could have looped

02:01:55.280 --> 02:01:59.280
through CSV column names and just remove the species column to do that. But

02:01:59.800 --> 02:02:03.160
again, we don't really need to. So for key and train dot keys, my feature

02:02:03.160 --> 02:02:06.720
columns dot append tf feature column, numeric column, key equals key, this

02:02:06.720 --> 02:02:09.080
was just going to create those feature columns, we don't need to do that

02:02:09.080 --> 02:02:12.320
vocabulary thing and that dot unique because again, these are all already

02:02:12.320 --> 02:02:16.280
encoded for us. Okay, awesome. So that was the next step. So let's go back

02:02:16.280 --> 02:02:19.480
here, building the model. Okay, so this is where we need to talk a bit more

02:02:19.480 --> 02:02:22.920
in depth of what we're actually going to build. So the model for this is a

02:02:22.920 --> 02:02:26.920
classification model. Now there is like hundreds of different classification

02:02:26.920 --> 02:02:30.800
models we can use that are pre made in TensorFlow. And so far, what we've done

02:02:30.800 --> 02:02:35.000
with that linear classifier is that's a pre made model that we kind of just

02:02:35.000 --> 02:02:38.920
feed a little bit of information to and it just works for us. Now here we have

02:02:38.920 --> 02:02:43.120
two kind of main choices that we can use for this kind of classification task

02:02:43.120 --> 02:02:47.120
that are pre built in TensorFlow, we have a DNN classifier, which stands for a

02:02:47.160 --> 02:02:51.200
deep neural network, which we've talked about very vaguely, very briefly. And we

02:02:51.200 --> 02:02:54.920
have a linear classifier. Now a linear classifier works very similarly to

02:02:54.920 --> 02:02:58.760
linear regression, except it does classification, rather than regression. So

02:02:58.760 --> 02:03:02.480
we get actually numeric value, or we get, sorry, you know, the labels like

02:03:02.480 --> 02:03:07.520
probability of being a specific label, rather than a numeric value. But in this

02:03:07.520 --> 02:03:10.680
instance, we're actually going to go with deep neural network. Now that's simply

02:03:10.680 --> 02:03:14.760
because TensorFlow on their website, like this is all of this is kind of

02:03:14.800 --> 02:03:18.200
building off of TensorFlow website, just all the code is very similar. And I've

02:03:18.200 --> 02:03:22.280
just added my own spin and explain things very in depth. They've recommended

02:03:22.280 --> 02:03:26.240
using that deep neural network for this is a better kind of choice. But typically

02:03:26.240 --> 02:03:29.040
when you're creating machine learning apps, you'll mess around with different

02:03:29.040 --> 02:03:32.480
models and kind of tweak them. And you'll notice that it's not that difficult to

02:03:32.480 --> 02:03:35.400
change models, because most of the work comes from loading and kind of

02:03:35.400 --> 02:03:40.400
pre processing our data. Okay, so what we need to do is build a deep neural

02:03:40.400 --> 02:03:44.600
network with two hidden later, two hidden layers with 30 nodes and 10 hidden

02:03:44.600 --> 02:03:48.320
nodes each. Now I'm going to draw out the architecture of this neural network in

02:03:48.320 --> 02:03:51.080
just one second. But I want to show you what we've done here. So we said

02:03:51.080 --> 02:03:56.160
classifier equals tf dot estimator. So this estimator module just stores a

02:03:56.160 --> 02:04:00.200
bunch of pre made models from TensorFlow. So in this case, DNN classifier is

02:04:00.200 --> 02:04:03.680
one of those. What we need to do is pass our feature columns just like we did to

02:04:03.680 --> 02:04:08.160
our linear classifier. And now we need to define the hidden units. Now hidden

02:04:08.200 --> 02:04:12.360
units is essentially us a building the architecture of the neural network. So

02:04:12.360 --> 02:04:15.840
like you saw before, we had an input layer, we had some like middle layers

02:04:15.840 --> 02:04:18.480
called our hidden layers in a neural network. And then we had our output

02:04:18.480 --> 02:04:22.080
layer. I'm going to explain neural networks in the next module. So this will

02:04:22.080 --> 02:04:26.120
all kind of click and make sense. For now, we've arbitrarily decided 30 nodes in

02:04:26.120 --> 02:04:30.520
the first hidden layer, 10 in the second, and the number of classes is going to

02:04:30.520 --> 02:04:33.320
be three. Now that's something that we need to decide. We know there's three

02:04:33.320 --> 02:04:37.200
classes for the flowers. So that's what we've defined. Okay, so let's copy this

02:04:37.200 --> 02:04:42.440
in. Go back to the other page here. And that is now our model. And now it is

02:04:42.440 --> 02:04:45.920
time to talk about how we can actually train the model, which is coming down

02:04:45.920 --> 02:04:49.280
here. Okay, so I'm going to copy this, I'm going to paste it over here and

02:04:49.280 --> 02:04:52.480
let's just dig through this because this is a bit more of a complicated piece of

02:04:52.480 --> 02:04:56.040
code than we usually used to work with. I'm also going to remove these comments

02:04:56.040 --> 02:04:59.880
just to clean things up in here. So we've defined the classifier, which is a

02:04:59.880 --> 02:05:03.040
deep neural network classifier, we have our feature columns hidden units

02:05:03.040 --> 02:05:08.440
classes. Now to train the classifier. So we have this input function here. This

02:05:08.440 --> 02:05:11.600
input function is different than the one we created previously. Remember when we

02:05:11.600 --> 02:05:15.880
had previously was like make input, whatever function, I won't continue

02:05:15.880 --> 02:05:19.720
typing in the inside it to find another function. And it actually returned that

02:05:19.720 --> 02:05:23.840
function from this function. I know, complicated. If you're not a Python kind

02:05:23.840 --> 02:05:27.720
of pro, I don't expect that to make perfect sense. But here, we just have a

02:05:27.720 --> 02:05:30.960
function, right? We do not returning a function from another function, it's just

02:05:31.000 --> 02:05:37.520
one function. So when we want to use this to train our model, what we do is

02:05:37.520 --> 02:05:41.640
create something called a lambda. Now a lambda is an anonymous function that can

02:05:41.640 --> 02:05:45.960
be defined in one line. When you write lambda, what that means is essentially

02:05:45.960 --> 02:05:50.520
this is a function. So this is a function. And whatever's after the colon is what

02:05:50.520 --> 02:05:54.960
this function does. Now this is a one line function. So like, if I create a

02:05:54.960 --> 02:06:02.840
lambda here, right, and I say lambda, print, hi, and I said, x equals lambda,

02:06:02.880 --> 02:06:08.040
and I called x like that, this works, this is a valid line of syntax. Actually,

02:06:08.040 --> 02:06:10.920
I want to make sure that I'm not just like messing with you. And I say that

02:06:10.920 --> 02:06:15.040
and then this is actually correct. Okay, so sorry, I just accidentally trained

02:06:15.040 --> 02:06:17.960
the model. So I just commented that out. You can see we're printing high, right?

02:06:17.960 --> 02:06:21.200
At the bottom of the screen, I know it's kind of small, but does say hi. That's

02:06:21.200 --> 02:06:23.840
how this works. Okay, so this is a cool thing. If you haven't seen this in

02:06:23.880 --> 02:06:27.840
Python before, that's what a lambda does allows you to define a function in one

02:06:27.840 --> 02:06:31.760
line. Now the thing that's great about this is that we can say, like, you know,

02:06:31.800 --> 02:06:35.000
x equals lambda, and here put another function, which is exactly what will be

02:06:35.000 --> 02:06:38.960
done with this print function. And that means when we call x, it will, you know,

02:06:39.000 --> 02:06:42.320
execute this function, which will just execute the other function. So it's kind

02:06:42.320 --> 02:06:46.200
of like a chain where you call x, x is a function. And inside that function, it

02:06:46.200 --> 02:06:48.920
does another function, right? It just like calling a function from inside a

02:06:48.920 --> 02:06:55.080
function. So what is lambda doing here? Well, since we need the actual function

02:06:55.120 --> 02:07:01.200
object, what we do is we define a function that returns to us a function. So this

02:07:01.200 --> 02:07:06.120
actually just like it calls this function, when you put this here. Now there's no

02:07:06.160 --> 02:07:09.480
I can't it's it's very difficult to explain this if you don't really

02:07:09.480 --> 02:07:13.600
understand the concept of lambdas, and you don't understand the input functions. But

02:07:13.600 --> 02:07:17.560
just know we're doing this because of the fact that we didn't embed another

02:07:17.600 --> 02:07:22.160
function and return the function object. If we had done that, if we had done that,

02:07:22.200 --> 02:07:26.080
you know, input function that we had created before where we had the interior

02:07:26.080 --> 02:07:29.720
function, then we wouldn't need to do this because what would happen is we would

02:07:29.720 --> 02:07:36.000
return the input function, right, like that, which means when we passed it into

02:07:36.000 --> 02:07:40.920
here, it could just call that directly. It didn't need to have a lambda. Whereas

02:07:40.920 --> 02:07:46.120
here, though, since we need to just put a lambda, we need to define what this is

02:07:46.120 --> 02:07:49.280
and then and then this works. That's just there's no other way to really explain

02:07:49.280 --> 02:07:53.640
this. So yeah, what we do is we create this input function. So we pass we have

02:07:53.640 --> 02:07:58.160
train, we have train y and we have training equals true. And then we do steps

02:07:58.160 --> 02:08:02.880
equals 5000. So this is similar to an epoch, except this is just defining a set

02:08:02.880 --> 02:08:05.760
amount of steps we're going to go through. So rather than saying like we'll go

02:08:05.760 --> 02:08:08.840
through the data set 10 times, we're just going to say we'll go through the data

02:08:08.840 --> 02:08:13.720
set until we fit 5000 numbers, like 5000 things that have been looked at. So

02:08:13.760 --> 02:08:16.960
that's what this does with that train. Let's run this and just look at the

02:08:16.960 --> 02:08:21.240
training output from our model, it gives us some like, things here, we can kind of

02:08:21.240 --> 02:08:25.160
see how this is working. Notice that if I can stop here for a second, it tells us

02:08:25.160 --> 02:08:30.080
the current step, it tells us the loss, the lowest, the lower this number, the

02:08:30.080 --> 02:08:34.200
better. And then it tells us global steps per second. So how many steps we're

02:08:34.200 --> 02:08:40.120
completing per second. Now at the end here, we get final step loss of 39, which

02:08:40.120 --> 02:08:45.240
is pretty high, which means this is pretty bad. But that's fine. This is kind

02:08:45.240 --> 02:08:48.560
of just our first test at training in neural network. So this is just giving

02:08:48.560 --> 02:08:51.800
us output while it's training to kind of save what's happening. Now, in our case,

02:08:51.800 --> 02:08:55.160
we don't really care because this is a very small model. When you're training

02:08:55.160 --> 02:08:58.720
models that are massive and take terabytes of data, you kind of care about the

02:08:58.720 --> 02:09:03.080
progress of them. So that's when you would use kind of that output, right? And

02:09:03.080 --> 02:09:05.760
you would actually look at that. Okay, so now that we've trained the model, let's

02:09:05.760 --> 02:09:09.520
actually do an evaluation on the model. So we're just going to say classifier dot

02:09:09.560 --> 02:09:13.240
evaluate. And what we're going to do is a very similar thing to what we've done

02:09:13.240 --> 02:09:18.080
here is just pass this input function, right, like here with a lambda once

02:09:18.080 --> 02:09:22.840
again, and reason we add the lambda when we don't have this like, double

02:09:22.840 --> 02:09:26.200
function going on, like a nested function, we need the lambda. And then in

02:09:26.200 --> 02:09:30.200
here, what we do is rather than passing train and train y, we're going to pass

02:09:30.240 --> 02:09:35.720
test, I believe. And I think it's I just called it test y. Okay, and then for

02:09:35.720 --> 02:09:40.360
training, obviously, this is false. So we can just set that false like that. I'm

02:09:40.360 --> 02:09:42.760
just going to look at the other screen and make sure I didn't mess this up. Because

02:09:42.760 --> 02:09:46.720
again, I don't remember the syntax. Yes, a cluster classifier dot evaluate test

02:09:46.720 --> 02:09:50.520
test y looks good to me. We'll take this print statement just so we get a nice

02:09:50.560 --> 02:09:55.280
output for our accuracy. Okay, so let's look at this. Again, we're going to have

02:09:55.280 --> 02:09:59.040
to wait for this to train. But I will show you a way that we don't need to wait

02:09:59.040 --> 02:10:03.840
for this to train every time in one second. And I'll be right back. Okay, so

02:10:03.880 --> 02:10:06.800
what I'm actually going to do, and I've just kind of pause like the execution of

02:10:06.800 --> 02:10:11.720
this code is throw this in the next block under, because the nice thing about

02:10:11.720 --> 02:10:15.800
Google Collaboratory is that I can run this block of code, right, I can train

02:10:15.800 --> 02:10:19.160
all this stuff, which is what I'll run now while we're talking just so it happens.

02:10:19.440 --> 02:10:23.560
And then I can have another code block kind of below it, which I have here. And

02:10:23.600 --> 02:10:26.800
it doesn't matter. I don't need to rerun that block every time I change

02:10:26.800 --> 02:10:29.720
something here. So if I change something in any lower blocks, I don't need to

02:10:29.720 --> 02:10:32.720
change the upper block, which means I don't need to wait for this to train every

02:10:32.720 --> 02:10:36.600
time I want to do an evaluation on it. Anyways, so we've done this, we got test,

02:10:36.600 --> 02:10:40.880
we got test why I just need to change this instead of eval result. Actually, I

02:10:40.880 --> 02:10:46.600
need to say eval underscore result equals classifier dot evaluate so that we can

02:10:46.600 --> 02:10:49.960
actually store this somewhere and get the answer. And now we'll print this and

02:10:49.960 --> 02:10:54.520
notice this happens much, much faster. We get a test accuracy of 80%. So if I were

02:10:54.520 --> 02:10:58.440
to retrain the model, chances are this accuracy would change again, because of

02:10:58.440 --> 02:11:01.640
the order in which we're seeing different flowers. But this is pretty decent

02:11:01.680 --> 02:11:06.000
considering we don't have that much test data. And we don't really know what

02:11:06.000 --> 02:11:08.440
we're doing, right? We're kind of just messing around and experimenting for

02:11:08.440 --> 02:11:12.120
right now. So to get 80% is pretty good. Okay, so actually, what am I doing? We

02:11:12.120 --> 02:11:15.200
need to go back now and do predictions. So how am I going to predict this for

02:11:15.200 --> 02:11:19.440
specific flowers? So let's go back to our core learning algorithms. And let's go

02:11:19.440 --> 02:11:24.120
to predictions. Now, I've written a script already, just to save a bit of time

02:11:24.120 --> 02:11:29.480
that allows us to do a prediction on any given flower. So what I'm going to do is

02:11:29.520 --> 02:11:33.320
create a new block down here, code block and copy this function in. Now we're

02:11:33.320 --> 02:11:36.400
going to digest this and kind of go through this on our own to make sure

02:11:36.400 --> 02:11:40.720
this makes sense. But what this little script does is allow the user to type in

02:11:40.720 --> 02:11:45.200
some numbers. So the septal length width, and I guess petal length and width, and

02:11:45.200 --> 02:11:49.840
then it will spit out to you what the predicted class of that flower is. So we

02:11:49.840 --> 02:11:54.200
could do a prediction on every single one of our data points like we did

02:11:54.200 --> 02:11:56.800
previously. And we already know how to do that. I showed you that with linear

02:11:56.840 --> 02:12:01.760
regression. But here I just wanted to do it on one entry. So what do we do? So I

02:12:01.760 --> 02:12:06.360
start by creating a input function, it's very basic, we have batch size 256. All

02:12:06.360 --> 02:12:10.280
we do is we give some features, and we created data set from those features

02:12:10.320 --> 02:12:15.120
that's a dict and then dot batch and the batch size. So what this is doing is

02:12:15.120 --> 02:12:19.080
notice we don't give any y value, right? We don't give any labels. The reason we

02:12:19.080 --> 02:12:22.880
do we don't do that is because when we're making a prediction, we don't know

02:12:22.880 --> 02:12:27.080
the label, right? Like we actually want that the model to give us the answer. So

02:12:27.080 --> 02:12:30.480
here I wrote down the features, I created a predict dictionary, just because I'm

02:12:30.480 --> 02:12:33.880
going to add things to it. And then I just prompted here with a print statement,

02:12:33.880 --> 02:12:39.360
please type numeric values as prompted. So for feature and feature, valid equals

02:12:39.360 --> 02:12:43.280
true, well valid, valid equals input feature colon. So this just means what

02:12:43.280 --> 02:12:46.120
we're going to do is for each feature, we're going to wait to get some valid

02:12:46.120 --> 02:12:50.960
response. Once we get some valid response, what we're going to do is add that to

02:12:51.000 --> 02:12:55.000
our dictionary. So we're going to say predict feature. So whatever that feature

02:12:55.000 --> 02:13:00.280
was, so septal length, septal width, petal length or pep, petal width is equal

02:13:00.280 --> 02:13:05.560
to a list that has in this instance, whatever that value was. Now the reason

02:13:05.560 --> 02:13:09.360
we need to do this is because again, the predict method from TensorFlow works

02:13:09.360 --> 02:13:13.280
on predicting for multiple things, not just one value. So even if we only have

02:13:13.280 --> 02:13:16.800
one value we want to predict for it, we need to put it inside of a list because

02:13:16.800 --> 02:13:20.320
it's expecting the fact that we will probably have more than one value in

02:13:20.360 --> 02:13:23.920
which we would have multiple values in the list, right, each representing a

02:13:23.920 --> 02:13:28.320
different row or a new flower to make a prediction for. Okay, now we say

02:13:28.320 --> 02:13:31.720
predictions equals classifier dot predict. And then in this case, we have input

02:13:31.720 --> 02:13:36.240
function lambda input function predict, which is this input function up here. And

02:13:36.240 --> 02:13:40.280
then we say for prediction dictionaries, because remember, every prediction comes

02:13:40.280 --> 02:13:44.200
back as a dictionary in predictions, we'll say the class ID is equal to

02:13:44.240 --> 02:13:49.160
whatever the class IDs of the prediction dictionary at zero. And these are

02:13:49.160 --> 02:13:54.840
simply what I don't know exactly how to explain this. We'll look at in a second,

02:13:54.840 --> 02:13:58.080
and I'll go through that. And then we have the probability is equal to the

02:13:58.080 --> 02:14:03.640
prediction dictionary probabilities of class ID. Okay, then we're going to say

02:14:03.640 --> 02:14:07.960
print prediction is we're going to do this weird format thing, I just stole

02:14:07.960 --> 02:14:12.360
this from TensorFlow. And it's going to be the species at the class ID, and then

02:14:12.360 --> 02:14:15.640
100 times probability, which will give us actual integer value, we're going to

02:14:15.640 --> 02:14:18.600
digest this, but let's run this right now and have a look. So please type

02:14:18.640 --> 02:14:24.160
numeric values as prompted septal length, let's type like 2.4, septal width 2.6,

02:14:24.600 --> 02:14:30.320
petal width, let's just say that's like 6.5. And yeah, petal width like 6.3. Okay,

02:14:30.360 --> 02:14:34.520
so then it calls this and it says prediction is virginica, I guess that's

02:14:34.560 --> 02:14:39.840
the the class we're going with. And it says that's an 83 or 86.3% chance that

02:14:39.840 --> 02:14:43.920
that is the prediction. So yeah, that is how that works. So that's what this

02:14:43.920 --> 02:14:47.000
does. I wanted to give a little script, I wrote most of this, I mean, I stole

02:14:47.040 --> 02:14:50.760
some of this from TensorFlow. But just to show you how we actually predict on

02:14:50.760 --> 02:14:55.360
one value. So let's look at these prediction dictionary, because I just

02:14:55.360 --> 02:14:59.080
want to show you what one of them actually is. So I'm going to say print,

02:15:00.240 --> 02:15:03.760
pred underscore dict. And then this will allow me to actually walk through what

02:15:03.760 --> 02:15:08.400
class IDs are probabilities are and how I've kind of done this. So let's run

02:15:08.400 --> 02:15:13.640
this up to length. Okay, let's just go like 1.4, 2.3. I don't know what these

02:15:13.640 --> 02:15:19.840
values are going to end up being. And we get prediction is same one with 77.1%,

02:15:19.840 --> 02:15:23.680
which makes sense, because these values are similar kind of in difference to what

02:15:23.680 --> 02:15:27.360
I did before. Okay, so this is the dictionary. So let's look for what we

02:15:27.360 --> 02:15:31.480
were looking for. So probabilities notice we get three probabilities, one for

02:15:31.520 --> 02:15:35.320
each of the different classes. So we can actually say what, you know, the

02:15:35.880 --> 02:15:40.760
percentages for every single one of the predictions. Then what we have is class

02:15:40.800 --> 02:15:46.920
IDs. Now class IDs, what this does is tell us what class ID, it predicts is

02:15:46.960 --> 02:15:50.600
actually the flower, right? So here it says two, which means that this

02:15:50.600 --> 02:15:55.000
probability at 77%. That's that index two in this array, right? So that's why

02:15:55.000 --> 02:15:59.680
this value is two. So it's saying that that class is to it thinks it's class

02:15:59.680 --> 02:16:05.280
two, like that's whatever was encoded in our system is two. And that's how that

02:16:05.280 --> 02:16:10.400
works. So that's how I know, which one to print out is because this tells me

02:16:10.400 --> 02:16:15.160
it's class two. And I know for making this list all the way back up here, if I

02:16:15.160 --> 02:16:20.920
can get rid of this output. Where is it? When I say species, that number two is

02:16:20.920 --> 02:16:25.200
virginica, or I guess that's how you say it. So that is what the classification

02:16:25.200 --> 02:16:27.840
is. So that's what the prediction is. So that's how I do that. And that's how

02:16:27.840 --> 02:16:33.840
that works. Okay, so I think that is pretty much it for actually classification. So

02:16:33.840 --> 02:16:36.800
it was pretty basic. I'm going to go and see if there's anything else that I did

02:16:36.840 --> 02:16:41.160
for classification in here. Okay, so here, I just put some examples. So here's

02:16:41.160 --> 02:16:44.280
some example input and expected classes. So you guys could try to do these if you

02:16:44.280 --> 02:16:51.920
want. So for example, this one, septal length, septal width. So for 5.1, 3.3, 1.7

02:16:51.920 --> 02:16:58.280
and 0.5, the output should be Satosa. For 5.9, 3.0, 4.2, 1.5, it should be this

02:16:58.280 --> 02:17:02.720
one. And then obviously this for this, just so you guys can mess with them if you

02:17:02.720 --> 02:17:07.080
want. But that's pretty much it for classification. And now on to clustering.

02:17:08.680 --> 02:17:13.160
Okay, so now we're moving on to clustering. Now clustering is the first unsupervised

02:17:13.520 --> 02:17:17.600
learning algorithm that we're going to see in this series. And it's very powerful. Now

02:17:17.600 --> 02:17:22.720
clustering only works for a very specific set of problems. And you use clustering

02:17:22.720 --> 02:17:26.760
when you have a bunch of input information or features, you don't have any labels or

02:17:26.760 --> 02:17:39.440
open information. Essentially, what clustering does is finds clusters of like data points

02:17:39.640 --> 02:17:43.800
and tells you the location of those clusters. So you give a bunch of training data, you

02:17:43.800 --> 02:17:48.200
can pick how many clusters you want to find. So maybe we're going to be classifying digits

02:17:48.200 --> 02:17:52.080
right handwritten digits using k means clustering. In that instance, we would have 10

02:17:52.080 --> 02:17:56.760
different clusters for the digits zero through nine. And you pass all this information and

02:17:56.760 --> 02:18:01.160
the algorithm actually finds those clusters in the data set for you. We're going to walk

02:18:01.160 --> 02:18:05.520
through an example that'll make sense. But I just want to quickly explain the basic

02:18:05.560 --> 02:18:10.600
algorithm behind k means essentially the set of steps. These are going to walk you through

02:18:10.600 --> 02:18:16.040
them and with a visual example. So we're going to start by randomly picking k points to

02:18:16.040 --> 02:18:21.720
place k centroids. Now a centroid stands for where our current cluster is kind of

02:18:21.760 --> 02:18:25.880
defined. And we'll see it in a second. The next step is we're going to assign all of the

02:18:25.880 --> 02:18:30.640
data points to the centroids by distance. So actually, now that I'm talking about this,

02:18:30.640 --> 02:18:33.920
I think it just makes more sense to get right into the example, because if I keep talking

02:18:33.920 --> 02:18:36.480
about this, you guys are probably just going to be confused, although I might come back

02:18:36.480 --> 02:18:41.760
to this just to reference those points. Okay, so let's create a little graph like this in

02:18:41.760 --> 02:18:46.240
two dimensions for our basic example. And let's make some data points here. So I'm just

02:18:46.240 --> 02:18:49.520
going to make them all read. And you're going to notice that I'm going to make this kind

02:18:49.520 --> 02:18:53.960
of easier for ourselves by putting them in like their own unique little groups, right?

02:18:53.960 --> 02:19:00.120
So actually, we'll add one up here. Then we can add some down here and down here. Now

02:19:00.120 --> 02:19:04.600
the algorithm starts for k means clustering. And you guys understand how this works as

02:19:04.600 --> 02:19:11.040
we continue by randomly picking k centroids. Now I'm going to denote a centroid by a little

02:19:11.040 --> 02:19:17.320
filled in triangle like this. And essentially what these are is where these different clusters

02:19:17.360 --> 02:19:22.280
currently exist. So we start by randomly picking k, which is what we've defined. So like me

02:19:22.280 --> 02:19:27.360
in this instance, that we're going to say k equals three, k centroid, wherever. So maybe

02:19:27.360 --> 02:19:31.680
we put one, you know, somewhere like here, you know, I might not bother filling these

02:19:31.680 --> 02:19:35.480
in because they're going to take a while. Maybe we pull in here, maybe we end up putting

02:19:35.480 --> 02:19:40.920
one over here. Now, I've kind of put them close to where clusters are, but these are

02:19:40.920 --> 02:19:47.080
going to be completely random. Now what happens next is each group, or each data point is

02:19:47.080 --> 02:19:52.520
assigned to a cluster by distance. So essentially, what we do is for every single data point

02:19:52.520 --> 02:19:56.640
that we have, we find what's known as the Euclidean distance, or it actually could be

02:19:56.640 --> 02:20:00.760
a different distance, you'd use like Manhattan distance, if you guys know what that is, to

02:20:00.760 --> 02:20:04.720
all of the centroids. So let's say we're looking at this data point here, what we do is find

02:20:04.720 --> 02:20:10.800
the distance to all of these different centroids. And we assign this data point to the closest

02:20:10.800 --> 02:20:14.680
centroid. So the closest one by distance. Now in this instance is looking like it's

02:20:14.680 --> 02:20:18.000
going to be a bit of a tie between this centroid and this centroid. But I'm going to give it

02:20:18.000 --> 02:20:23.720
to the one on the left. So what we do is we're going to say this is now a part of this centroid.

02:20:23.720 --> 02:20:28.000
So if I'm calling this like, let's just say this is centroid one, this is centroid two,

02:20:28.000 --> 02:20:31.800
and this is centroid three, then this now is going to be a part of centroid one, because

02:20:31.800 --> 02:20:35.640
it's closest to centroid one. And we can go through and we do this for every single data

02:20:35.640 --> 02:20:40.120
point. So obviously, we know all of these are going to be our ones, right? And we know

02:20:40.160 --> 02:20:44.360
these are going to be our two. So two, two, two. And then these are obviously going to

02:20:44.360 --> 02:20:48.840
be our three. Now I'm actually just going to add a few other data points, because I want

02:20:48.840 --> 02:20:55.000
to make this a little bit more sophisticated, almost, if that makes any sense. So add those

02:20:55.000 --> 02:20:58.920
data points here, we've been add one here, and that will give these labels. So these

02:20:58.920 --> 02:21:02.080
ones are close. So I'm going to say this one's one, I'm going to say this one's two, I know

02:21:02.080 --> 02:21:06.200
it's not closest to it. But just because I want to do that for now, we'll say two for

02:21:06.200 --> 02:21:10.120
that. And we'll say three here. Okay, so now that we've done that, we've labeled all these

02:21:10.120 --> 02:21:15.760
points, what we do is we now move these centroids that we've defined into the middle of all

02:21:15.760 --> 02:21:21.800
of their data points. So what I do is I essentially find it's called center of mass, the center

02:21:21.800 --> 02:21:26.120
of mass between all of the data points that are labeled the same. So in this case, these

02:21:26.120 --> 02:21:29.360
will be all the ones that are labeled the same. And I take this centroid, which I'm

02:21:29.360 --> 02:21:33.640
going to have to erase, get rid of it here. And I put it right in the middle. So let's

02:21:33.680 --> 02:21:38.080
go back to blue. And let's say the middle of these data points ends up being somewhere

02:21:38.080 --> 02:21:42.920
around here. So we put it in here. And this is what we call center of mass. And this again,

02:21:42.920 --> 02:21:47.840
it'd be centroid two. So let's just erase this. And there we go. Now we do the same

02:21:47.840 --> 02:21:53.120
thing with the other centroid. So let's remove these ones, to remove these ones. So for three,

02:21:53.120 --> 02:21:58.680
I'm saying it's probably going to be somewhere in here. And then for one, our center of mass

02:21:58.760 --> 02:22:05.760
is probably going to be located somewhere about here. Now what I do is I repeat the process

02:22:05.760 --> 02:22:10.760
that I just did. And I reassign all the points now to the closest centroid. So all these

02:22:10.760 --> 02:22:14.960
points are labeled one to all that, you know, we can kind of remove their labels. And this

02:22:14.960 --> 02:22:19.440
is just going to be great. Me trying to erase the labels, I shouldn't have wrote them on

02:22:19.440 --> 02:22:23.040
top. But essentially, what we do is we're just going to be like reassigning them. So

02:22:23.040 --> 02:22:26.440
I'm going to say, okay, so this is two, and we just do the same thing as before, find

02:22:26.440 --> 02:22:30.600
the closest distance. So we'll say, you know, these can stay in the same cluster, maybe

02:22:30.600 --> 02:22:36.280
this one actually here gets changed to one now, because it's closest to centroid one.

02:22:36.280 --> 02:22:39.440
And we just reassigned all these points. And maybe, you know, this one now, if it was two

02:22:39.440 --> 02:22:44.200
before, let's say like this one's one, and we just reassigned them. Now we repeat this

02:22:44.200 --> 02:22:50.920
process of finding the closest or assigning all the points that are closest centroid,

02:22:50.920 --> 02:22:55.520
moving the centroid into the center of mass, and we keep doing this until eventually we

02:22:55.600 --> 02:23:00.320
reach a point where none of these points are changing which centroid they're a part of.

02:23:00.320 --> 02:23:04.000
So eventually we reach a point where I'm just going to erase this and draw like a new graph

02:23:04.000 --> 02:23:07.320
because it'll be a little bit cleaner. But what we have is, you know, like a bunch of

02:23:07.320 --> 02:23:13.080
data points. So we have some over here, some over here, maybe we'll just put some here,

02:23:13.080 --> 02:23:17.440
and maybe we'll do like a K equals four example for this one. And we have all these centroids

02:23:17.440 --> 02:23:21.760
and I'll just draw these centroids with blue again, that are directly in the middle of

02:23:21.760 --> 02:23:25.240
all of their data points, they're like as in the middle as they can get, none of our

02:23:25.280 --> 02:23:30.920
data points have moved. And we call this now our cluster. So now we have these clusters,

02:23:30.920 --> 02:23:34.280
we have these centroids, right, we know where they are. And what we do is when we have a

02:23:34.280 --> 02:23:38.720
new data point that we want to make a prediction for or figure out what cluster it's a part

02:23:38.720 --> 02:23:43.360
of, what we do is we will plot that data point. So let's say it's this new data point here,

02:23:43.360 --> 02:23:48.200
we find the distance to all of the clusters that exist. And then we assign it to the closest

02:23:48.200 --> 02:23:52.360
one. So obviously it would be assigned to that one. And we can do this for any data point,

02:23:52.360 --> 02:23:56.560
right? So even if I put a data point all the way over here, well, it's closest cluster

02:23:56.560 --> 02:24:02.160
is this, so it gets assigned to this cluster. And my output will be whatever this label

02:24:02.160 --> 02:24:06.200
of this cluster is. And that's essentially how this works, you're just clustering data

02:24:06.200 --> 02:24:09.680
points, figuring out which ones are similar. And this is a pretty basic algorithm, I mean,

02:24:09.680 --> 02:24:13.040
you draw your little triangle, you find the distance from every point of the triangle,

02:24:13.040 --> 02:24:17.480
or to all of the triangles, actually. And then what you do is just simply assign those

02:24:17.480 --> 02:24:21.840
values to that centroid, you move that centroid to the center of mass, and you repeat this

02:24:21.860 --> 02:24:25.760
process constantly, until eventually you get to a point where none of your data points

02:24:25.760 --> 02:24:30.240
are moving. That means you found the best clusters that you can, essentially. Now, the

02:24:30.240 --> 02:24:34.440
only thing with this is you do need to know how many clusters you want for k means clustering,

02:24:34.440 --> 02:24:38.280
because k is a variable that you need to define. Although there is some algorithms that can

02:24:38.280 --> 02:24:42.600
actually determine the best amount of clusters for a specific data set. But that's a little

02:24:42.600 --> 02:24:46.120
bit beyond what we're going to be focused on focusing on right now. So that is pretty

02:24:46.120 --> 02:24:50.280
much clustering. There's not really much more to talk about it, especially because we can't

02:24:50.280 --> 02:24:55.720
really code anything for it now. So we're going to move on to hidden Markov models. Now

02:24:55.720 --> 02:24:59.920
hidden Markov models are way different than what we've seen so far, we've been using kind

02:24:59.920 --> 02:25:04.560
of algorithms that rely on data. So like k means clustering, we gave a lot of data,

02:25:04.560 --> 02:25:08.800
and we know clustered all those data points found those centroids, use those centroids

02:25:08.800 --> 02:25:14.000
to find where new data points should be. Same thing with linear regression and classification.

02:25:14.000 --> 02:25:18.680
Whereas hidden Markov models, we actually deal with probability distributions. Now,

02:25:18.680 --> 02:25:21.800
the example we're going to go into here, and it's kind of I have to do a lot of examples

02:25:21.800 --> 02:25:27.080
for this because it's a very abstract concept is a basic weather model. So what we actually

02:25:27.080 --> 02:25:34.320
want to do is predict the weather on any given day, given the probability of different events

02:25:34.320 --> 02:25:38.640
occurring. So let's say we know, you know, maybe in like a simulated environment or something

02:25:38.640 --> 02:25:44.560
like that, this might be an application, that we have some specific things about our environment,

02:25:44.640 --> 02:25:48.920
we know if it's sunny, there's an 80% chance that the next day, it's going to be sunny

02:25:48.920 --> 02:25:53.320
again, and a 20% chance that it's going to rain. Maybe we know some information about

02:25:53.320 --> 02:25:57.780
sunny days and about cold days. And we also know some information about the average temperature

02:25:57.780 --> 02:26:03.160
on those days. Using this information, we can create a hidden Markov model that will

02:26:03.160 --> 02:26:08.600
allow us to make a prediction for the weather in future days, given kind of that probability

02:26:08.600 --> 02:26:12.360
that we've discovered. Now you might be like, Well, how do we know this? Like how do I know

02:26:12.400 --> 02:26:16.840
this probability? A lot of the times you actually do know the probability of certain events

02:26:16.840 --> 02:26:20.480
occurring or certain things happening, which makes these models really good. But there's

02:26:20.480 --> 02:26:24.600
some times where what you actually do is you have a huge data set, and you calculate the

02:26:24.600 --> 02:26:29.240
probability of things occurring based on that data set. So we're not going to do that part

02:26:29.240 --> 02:26:31.760
because that's just kind of going a little bit too far. And the whole point of this is

02:26:31.760 --> 02:26:36.200
just to introduce you to some different models. But in this example, what we will do is use

02:26:36.200 --> 02:26:40.720
some predefined probability distributions. So let me just read out the exact definition

02:26:40.720 --> 02:26:44.200
of a hidden Markov model will start going more in depth. So the hidden Markov model

02:26:44.200 --> 02:26:48.640
is a finite set of states, each of which is associated with a generally multi dimensional

02:26:48.640 --> 02:26:53.160
probability distribution, transitions among the states are governed by a set of probabilities

02:26:53.160 --> 02:26:59.040
called transition probabilities. So in a hidden Markov model, we have a bunch of states. Now

02:26:59.040 --> 02:27:02.200
in the example that I was just talking about with this weather model, the states we would

02:27:02.200 --> 02:27:09.520
have is hot day and cold day. Now, these are what we call hidden, because never do we actually

02:27:09.560 --> 02:27:14.880
access or look at these states, while we interact with the model. In fact, what we look at is

02:27:14.880 --> 02:27:19.320
something called observations. Now at each state, we have an observation, I'll give you

02:27:19.320 --> 02:27:25.200
an example of an observation. If it is hot outside, Tim has an 80% chance of being happy.

02:27:25.200 --> 02:27:30.600
If it is cold outside, Tim has a 20% chance of being happy. That is an observation. So

02:27:30.600 --> 02:27:36.720
at that state, we can observe the probability of something happening during that state is

02:27:36.760 --> 02:27:42.800
x, right, or is y or whatever it is. So we don't actually care about the states in particular,

02:27:42.800 --> 02:27:46.440
we care about the observations we get from that state. Now in our example, what we're

02:27:46.440 --> 02:27:50.880
actually going to do is we're going to look at the weather as an observation for the state. So

02:27:50.880 --> 02:27:55.760
for example, on a sunny day, the weather has, you know, the probability of being between

02:27:55.760 --> 02:28:00.840
five and 15 degrees Celsius, with an average temperature of 11 degrees. That's like, that's

02:28:00.840 --> 02:28:05.720
a probability we can use. Now I know this is slightly abstract, but I just want to talk

02:28:05.720 --> 02:28:09.440
about the data we're going to work with here. I'm going to draw out a little example, go

02:28:09.440 --> 02:28:12.840
through it and then we'll actually get into the code. So let's start by discussing the

02:28:12.840 --> 02:28:17.120
type of data we're going to use. So typically in previous ones, right, we use like hundreds,

02:28:17.120 --> 02:28:22.240
if not like thousands of entries or rows or data points for our models to train. For this,

02:28:22.240 --> 02:28:26.960
we don't need any of that. In fact, all we need is just constant values for probability

02:28:26.960 --> 02:28:32.280
and or what is it transition distributions and observation distributions. Now what I'm

02:28:32.320 --> 02:28:36.720
going to do is go in here and talk about states observations and transitions. So we have a

02:28:36.720 --> 02:28:42.000
certain amount of states. Now we will define how many states we have, we don't really care

02:28:42.000 --> 02:28:47.280
what that state is. So we could have states, for example, like warm, cold, high, low, red,

02:28:47.280 --> 02:28:51.480
green, blue, you have as many states as we want, we could have one state to be honest,

02:28:51.480 --> 02:28:54.760
although that would be kind of strange to have that. And these are called hidden because

02:28:54.760 --> 02:28:59.640
we don't directly observe. Now observations. So each state has a particular outcome or

02:28:59.720 --> 02:29:04.200
observation associated with it based on a probability distribution. So it could be the

02:29:04.200 --> 02:29:10.440
fact that during a hot day, it is 100% true that Tim is happy. Although, in a hot day, we

02:29:10.440 --> 02:29:16.080
could observe that 80% of the time Tim is happy. And 20% of the time, he is sad, right? Those

02:29:16.080 --> 02:29:20.640
are observations we make about each state. And each state will have their different observations

02:29:20.640 --> 02:29:26.360
and different probabilities of those observations occurring. So if we were just going to have

02:29:26.440 --> 02:29:30.400
like an outcome for the state, that means it's always the same, there's no probability that

02:29:30.400 --> 02:29:34.800
something happens. And in that case, that's just called an outcome, because the probability

02:29:34.800 --> 02:29:40.600
of the event occurring will be 100%. Okay, then we have transitions. So each state will

02:29:40.600 --> 02:29:44.880
have a probability defining the likelihood of transitioning to a different state. So

02:29:44.880 --> 02:29:49.840
for example, if we have a hot day, there will be a percentage chance the next day will be

02:29:49.840 --> 02:29:53.160
a cold day. And if we have a cold day, there will be a percentage chance that the next

02:29:53.160 --> 02:29:57.680
day is either a hot day or a cold day. So we're going to go through like the exact what

02:29:57.680 --> 02:30:02.160
we have for our specific model below. But just understand there's a probability that

02:30:02.160 --> 02:30:06.400
we could transition into a different state. And from each state, we can transition into

02:30:06.400 --> 02:30:12.280
every other state or a defined set of states given a certain probability. So I know it's

02:30:12.280 --> 02:30:17.360
a mouthful, I know it's a lot. But let's go into a basic drawing example, because I just

02:30:17.360 --> 02:30:21.320
want to illustrate like graphically a little bit kind of how this works. In case these

02:30:21.360 --> 02:30:26.120
are ideas are a little bit too abstract for any of you. Okay, I'm just pulling out the

02:30:26.120 --> 02:30:32.040
drawing tablet, just one second here, and let's do this basic weather model. So what

02:30:32.040 --> 02:30:36.840
I'm going to do is just simply draw two states. Actually, let's do it with some colors because

02:30:36.840 --> 02:30:40.960
why not? So we're going to use yellow. And this is going to be our hot day. Okay, this

02:30:40.960 --> 02:30:45.640
is going to be our sun. And then I'm just going to make a cloud. We'll just do like

02:30:45.640 --> 02:30:50.520
a gray cloud. This will be my cloud. And we'll just say it's going to be raining over here.

02:30:50.520 --> 02:30:55.360
Okay, so these are my two states. Now, in each state, there's a probability of transitioning

02:30:55.360 --> 02:31:02.440
to the other state. So for example, in a hot day, we have a let's say 20% chance of transitioning

02:31:02.440 --> 02:31:08.720
to a cold day. And we have a 80% chance of transitioning to another hot day, like the

02:31:08.720 --> 02:31:14.480
next day, right? Now in a cold day, we have let's say a 30% chance of transitioning to

02:31:14.520 --> 02:31:20.760
a hot day. And we have in this case, what is that going to be a 70% chance of transitioning

02:31:20.760 --> 02:31:25.360
to another cold day. Now, on each of these days, we have a list of observations. So these

02:31:25.360 --> 02:31:29.280
are what we call states, right? So this could be s one, and this could be s two, it doesn't

02:31:29.280 --> 02:31:32.960
really matter, like if we name them or anything, we just we have two states, that's what we

02:31:32.960 --> 02:31:37.600
know. We know the transition probability, that's what we've just defined. Now we want

02:31:37.600 --> 02:31:42.880
the observation probability or distribution for that. So essentially, on a hot day, our

02:31:42.920 --> 02:31:50.000
observation is going to be that the temperature could be between 15 and 25 degrees Celsius

02:31:50.000 --> 02:31:57.600
with an average temperature of let's say 20. So we can say observation, right? So say observation,

02:31:57.600 --> 02:32:03.280
and we'll say that the mean so the average temperature is going to be 20. And then the

02:32:03.280 --> 02:32:09.200
distribution for that will be like the minimum value is going to be 15. And the max is going

02:32:09.200 --> 02:32:14.120
to be 25. So this is what we call actually like a standard deviation. I'm not really

02:32:14.120 --> 02:32:17.200
going to explain exactly what standard deviation is, although you can kind of think of it as

02:32:17.200 --> 02:32:22.560
something like this. So essentially, there's a mean, which is the middle point, the most

02:32:22.560 --> 02:32:27.480
common event that could occur. And at different levels of standard deviation, which is going

02:32:27.480 --> 02:32:30.640
into statistics, which I don't really want to mention that much, because I'm definitely

02:32:30.640 --> 02:32:35.760
non expert. We have a probability of hitting different temperatures as we move to the left

02:32:35.800 --> 02:32:41.160
and right of this value. So on this curve somewhere, we have 15. And on this curve to the

02:32:41.160 --> 02:32:44.880
right somewhere, we have 25. Now, we're just defining the fact that this is where we're

02:32:44.880 --> 02:32:49.960
going to kind of end our curve. So we're going to say that like the probability is in between

02:32:49.960 --> 02:32:54.280
these numbers, it's going to be in between 15 and 25 with an average of 20. And then our

02:32:54.280 --> 02:32:58.760
model will kind of figure out some things to do with that. That's as far as I really

02:32:58.760 --> 02:33:03.320
want to go in standard deviation. And I'm sure that's like a really horrible explanation.

02:33:03.360 --> 02:33:06.040
That's kind of the best I'm going to give you guys for right now. Okay, so that's our

02:33:06.040 --> 02:33:09.360
observation here. Now our observation over here is going to be similar. So we're going

02:33:09.360 --> 02:33:14.160
to say mean on a cold day, temperature is going to be five degrees. We'll say the minimum

02:33:14.160 --> 02:33:18.480
temperature maybe is going to be something like negative five and the max could be something

02:33:18.480 --> 02:33:24.760
like 15 or like, yeah, we can say 15. So we'll have some distribution, not just what we want

02:33:24.760 --> 02:33:28.720
to understand, right? And this is kind of a strange distribution because we're dealing

02:33:28.760 --> 02:33:34.080
with what is it standard deviation, although we can just deal with like straight percentage

02:33:34.080 --> 02:33:38.480
observations. So for example, you know, there's a 20% chance that Tim is happy, or there's an

02:33:38.480 --> 02:33:44.800
80% chance that he is sad, like those are probabilities that we can have as our observation

02:33:44.800 --> 02:33:49.840
probabilities in the model. Okay, so there's a lot of lingo. There's a lot going on, we're

02:33:49.840 --> 02:33:53.560
going to get into like a concrete example now. So hopefully this should make more sense. But

02:33:53.560 --> 02:33:57.680
again, just understand states, transitions, observations, we don't actually ever look at

02:33:57.680 --> 02:34:01.800
the states, we just have to know how many we have, and the transition probability and

02:34:01.800 --> 02:34:07.880
observation probability in each of them. Okay, so what I want to say now, though, is what

02:34:07.880 --> 02:34:11.960
do we even do with this model? So once I make this right, once I make this hidden markup

02:34:11.960 --> 02:34:15.560
model, what's the point of it? Well, the point of it is to predict future events based on

02:34:15.560 --> 02:34:20.320
past events. So we know that probability distribution. And I want to predict the weather for the

02:34:20.320 --> 02:34:24.440
next week, I can use that model to do that, because I can say, well, if the current day

02:34:24.520 --> 02:34:29.000
today is warm, then what is the likelihood that the next day tomorrow is going to be

02:34:29.000 --> 02:34:33.480
cold, right? And that's what we're kind of doing with this model, we're making predictions

02:34:33.480 --> 02:34:40.160
for the future based on probability of past events occur. Okay, so imports and so let's

02:34:40.160 --> 02:34:46.000
just run this already loaded import tensorflow. And notice that here I've imported tensorflow

02:34:46.000 --> 02:34:52.680
probability is TFP. This is because this is a separate module from TensorFlow that

02:34:52.720 --> 02:34:58.200
deals with probability. Now, we also need tensorflow two. But for this hidden markup

02:34:58.200 --> 02:35:02.560
model, we're going to use the tensorflow probability module, not a huge deal. Okay, so

02:35:02.560 --> 02:35:06.960
weather model. So this is just going to define what our model actually is so the different

02:35:06.960 --> 02:35:11.440
parts of it. So this is taken directly from the documentation of tensorflow. You guys can

02:35:11.440 --> 02:35:15.280
see you know, where I have all this information from like I've sourced all of it. But essentially

02:35:15.280 --> 02:35:18.480
what the model we're going to try to create is that cold days are encoded by zero and

02:35:18.520 --> 02:35:23.200
hot days are encoded by one. The first day in our sequence has an 80% chance of being

02:35:23.200 --> 02:35:26.680
cold. So whatever day we're starting out at has an 80% chance of being cold, which would

02:35:26.680 --> 02:35:32.000
mean 20% chance of being warm. A cold day has a 30% chance of being followed by hot day.

02:35:32.000 --> 02:35:35.360
And a hot day has a 20% chance of being followed by a cold day, which would mean you know,

02:35:35.360 --> 02:35:40.880
70% cold to cold and 80% hot to hot. On each day, the temperature is normally distributed

02:35:40.880 --> 02:35:45.120
with mean and standard deviation zero and five on a cold day and mean and standard deviation

02:35:45.160 --> 02:35:49.760
15 and 10 on a hot day. Now what that means standard deviation is essentially I mean, we

02:35:49.760 --> 02:35:54.520
can read this thing here is that on a hot day, the average temperature is 15 that's mean

02:35:54.520 --> 02:35:59.840
and ranges from five to 25 because the standard deviation is 10 of that, which just means 10

02:35:59.840 --> 02:36:04.680
on each side kind of the min max value. Again, I'm not in statistics. So please don't quote

02:36:04.680 --> 02:36:08.480
me on any definitions of standard deviation. I just trying to explain it enough so that

02:36:08.480 --> 02:36:13.200
you guys can understand what we're doing. Okay, so what we're going to do to model this

02:36:13.200 --> 02:36:17.160
and I'm just kind of going through this fairly quickly because it's pretty easy to really do

02:36:17.160 --> 02:36:23.920
this is I'm going to load the TensorFlow probability distributions kind of module and just save

02:36:23.920 --> 02:36:29.400
that as TFD. And I'm just going to do that so I don't need to write TFP dot distributions dot

02:36:29.400 --> 02:36:33.400
all of this, I can just kind of shortcut it. So you'll notice I'm referencing TFD here,

02:36:33.400 --> 02:36:38.560
which just stands for TFP dot distributions and TFP is TensorFlow probability. Okay,

02:36:38.720 --> 02:36:45.200
so my initial distribution is TensorFlow probability distributions dot categorical. This

02:36:45.200 --> 02:36:50.560
is probability of 80% and 20%. Now this refers to point two. So let's look at point two. The

02:36:50.560 --> 02:36:55.040
first day in our sequence has an 80% chance of being cold. So we're saying that that's

02:36:55.040 --> 02:37:00.640
essentially what this is the initial distribution of being cold is 80%. And then 20% after categorical

02:37:00.640 --> 02:37:06.440
is just a way that we can do this distribution. Okay, so transition distribution. What is

02:37:06.440 --> 02:37:14.520
it TensorFlow probability categorical, the probability is 70% and 30% and 20% 80%. Now

02:37:14.520 --> 02:37:19.880
notice that since we have two states, we've defined two probabilities. Notice since we have two

02:37:19.880 --> 02:37:24.600
states, we have defined two probabilities, the probability of landing on each of these states

02:37:24.600 --> 02:37:28.920
at the very beginning of our sequence. This is the transition probability referred to points three

02:37:28.920 --> 02:37:36.120
and four above. So this is what we have here. So cold is 30% chance 20% chance for a hot day. And

02:37:36.120 --> 02:37:40.680
that's what we've defined. So we say this is going to be cold day state one, we have 70% chance

02:37:40.680 --> 02:37:44.840
of being cold day again, we have 30% chance of going hot day and then you know, reverse here.

02:37:45.720 --> 02:37:51.400
Okay, so observation distribution. Now this one is a little bit different, but essentially we do

02:37:51.400 --> 02:37:56.680
tfd dot normal. Now I don't know, I'm not going to explain exactly what all this is, but when you're

02:37:56.680 --> 02:38:00.520
doing standard deviation, you're going to do it like this, where you're going to say, look, which

02:38:00.520 --> 02:38:05.000
stands for your average or your mean, right? So that was our average temperature is going to be

02:38:05.000 --> 02:38:10.600
zero on a hot day, 15 on a cold day. The standard deviation on the cold days five, which means we

02:38:10.600 --> 02:38:17.400
range from five, or negative five to five degrees. And on a hot day, it's 10. So that is going to be

02:38:17.400 --> 02:38:22.360
we go range from five to 25 degrees, and our average temperature is 15. Now the reason we've

02:38:22.360 --> 02:38:27.240
added dot here is because these just need to be float values. So rather than inserting integers

02:38:27.240 --> 02:38:32.680
here and having potentially type errors later on, we just have floats. Okay, so the low argument

02:38:32.680 --> 02:38:35.800
represents the mean and the scales of standard deviation. Yeah, exactly what we just defined

02:38:35.800 --> 02:38:40.520
there. Alright, so let's run this, I think we actually already did. And now we can create our

02:38:40.520 --> 02:38:44.760
model. So to create the models pretty easy. I mean, all we do is say model equals TensorFlow

02:38:44.760 --> 02:38:48.920
distribution dot hidden Markov model, give it the initial distribution, which is equal to

02:38:48.920 --> 02:38:53.960
initial distribution, transition distribution, observation, distribution and steps. Now what

02:38:53.960 --> 02:38:59.720
is steps? Well, steps is how many days we want to predict for. So the number of steps is how many

02:38:59.720 --> 02:39:05.240
times we're going to step through this probability cycle, and run the model essentially. Now remember,

02:39:05.240 --> 02:39:09.240
what we want to do is we want to predict the average temperature on each day, right? Like

02:39:09.240 --> 02:39:13.640
that's what the goal of our example is is to predict the average temperature. So given this

02:39:13.640 --> 02:39:19.240
information, using these observations and using these transitions, what we'll do is predict that.

02:39:19.240 --> 02:39:25.240
So I'm going to run this model. What is the issue here? tensor is on hash of tensor is

02:39:25.400 --> 02:39:29.480
okay, give me one sec, I'll have a look here, though I haven't had this issue before. Okay,

02:39:29.480 --> 02:39:34.520
so after a painful amount of searching on stack overflow and Google and actually just reading

02:39:34.520 --> 02:39:39.320
through more documentation on TensorFlow, I have determined the issue. So remember the error was

02:39:39.320 --> 02:39:43.640
we were getting on actually this line here, I think I can see what the output is. Oh, this

02:39:43.640 --> 02:39:47.000
okay, well, this is a different error. But anyways, there was an error at this line. Essentially,

02:39:47.000 --> 02:39:51.800
what was happening is we have a mismatch between the two versions here. So the most recent version

02:39:52.440 --> 02:39:58.120
of TensorFlow is not compatible with the older version of TensorFlow probability, at least in

02:39:58.120 --> 02:40:01.720
the sense that the things that we're trying to do with it. So I just need to make sure that I

02:40:01.720 --> 02:40:07.160
installed the most recent version of TensorFlow probability. So what you need to do if this

02:40:07.160 --> 02:40:11.960
is in your notebook, and this should actually work fine for you guys, because this will be updated

02:40:11.960 --> 02:40:15.960
by the time you get there. But in case you run into the issue, I'll, you know, deal with it.

02:40:15.960 --> 02:40:20.360
But essentially, we're going to select version 2.x of TensorFlow, you're going to run this

02:40:20.440 --> 02:40:25.160
install commands, you're going to install TensorFlow probability, just run this command.

02:40:25.160 --> 02:40:30.120
Then after you run this command, you're going to need to restart your run times, go to run time,

02:40:30.120 --> 02:40:34.280
and then restart run time. And then you can just continue on with the script, select TensorFlow

02:40:34.280 --> 02:40:39.000
2.x again, do your imports, and then you know, we'll test if this is actually going to work for

02:40:39.000 --> 02:40:44.840
us here, run our distributions, create the model without any issues this time, notice no red text,

02:40:44.840 --> 02:40:49.640
and then run this final line, which will give you the output. Now, this is what I wanted to

02:40:49.640 --> 02:40:53.880
talk about here that we didn't quite get to because we were having some bugs. But this is how we

02:40:53.880 --> 02:40:59.480
can actually kind of run our model and see the output. So what you can do is do model dot mean,

02:40:59.480 --> 02:41:04.680
so you say mean equals model dot mean. And what this is going to do is essentially just calculate

02:41:04.680 --> 02:41:10.680
the probability is going to essentially take that from the model. Now, when we have model dot mean,

02:41:10.680 --> 02:41:15.160
this is what we call, you know, a partially defined tensor. So remember our tensors were like

02:41:15.240 --> 02:41:19.720
partially defined computations. Well, that's what model dot mean actually is. That's what

02:41:19.720 --> 02:41:25.240
this method is. So if we want to get the value of that, what we actually need to do is create a

02:41:25.240 --> 02:41:30.440
new session in TensorFlow, run this part of the graph, which we're going to get by doing mean

02:41:30.440 --> 02:41:34.520
dot numpy, and then we can print that out. So I know this might seem a little bit confusing,

02:41:34.520 --> 02:41:40.040
but essentially to run a session in the new version of TensorFlow, so 2.x, or 2.1 or whatever

02:41:40.040 --> 02:41:47.400
it is, you're going to type with TF dot compact dot v one dot session as sesh. And then I mean,

02:41:47.400 --> 02:41:50.760
this doesn't really matter what you have here, but whatever you want. And then what I'm doing is

02:41:50.760 --> 02:41:55.480
just printing mean dot numpy. So to actually get the value from this here, this variable,

02:41:55.480 --> 02:42:00.280
I call dot numpy. And then what it does is print out this array that gives me the expected

02:42:00.280 --> 02:42:07.000
temperatures on each day. So we have, you know, three, six, essentially 7.5, 8.25. And you can

02:42:07.080 --> 02:42:12.120
see these are the temperatures, based on the fact that we start with an initial probability of

02:42:12.120 --> 02:42:16.360
starting on a cold day. So we kind of get that here, right, we're starting at three degrees.

02:42:16.360 --> 02:42:20.680
That's what it's determined, we're going to start at. And then we have all of these other

02:42:20.680 --> 02:42:25.960
temperatures is predicting for the next days. Now notice if we recreate this model, so just

02:42:25.960 --> 02:42:30.760
rerun the distributions, rerun them and go model dot mean again, this stays the same, right? Well,

02:42:30.760 --> 02:42:34.360
because our probabilities are the same, this model is going to do the calculation the exact

02:42:34.360 --> 02:42:38.760
same, there's not really any training that goes into this. So we get, you know, very similar,

02:42:38.760 --> 02:42:42.680
if not the exact same values, I can't remember if these are identical, but that's what it looks

02:42:42.680 --> 02:42:46.600
like to me. I mean, we can run this again, see, we get the same one, and we'll create the model

02:42:46.600 --> 02:42:50.120
one more time. And let me just check these values here to make sure I'm not lying to you as yes,

02:42:50.120 --> 02:42:53.960
they are the exact same. Okay, so let's start messing with a few probabilities and see what we

02:42:53.960 --> 02:43:00.520
can do to this temperature and see what changes we can cause. So if I do 0.5 here, and I do 0.5

02:43:00.600 --> 02:43:05.560
for the categorical probability, remember this refers to points three and four above. So it's

02:43:05.560 --> 02:43:09.880
a cold day has a 30% chance of being followed by hot day and then a hot day has a 20% chance of

02:43:09.880 --> 02:43:13.960
being followed by cold day. So what I've just done now is change the probability to be 50%

02:43:14.760 --> 02:43:19.240
so that a cold day now has a 50% chance of being followed by hot day and a 50% chance of

02:43:19.240 --> 02:43:24.360
being followed by cold day. And let's recreate this model. Let's rerun this and let's see if

02:43:24.360 --> 02:43:29.720
we get a difference. But we do notice this, the temperature now has been a is going a little bit

02:43:29.720 --> 02:43:34.440
higher. Now notice that we get the same starting temperature because that's just the average

02:43:34.440 --> 02:43:38.760
based on this probability that we have here. But if we wanted to potentially start, you know,

02:43:38.760 --> 02:43:44.840
hotter, we could reverse these numbers, we go 0.2 0.8. Let's rerun all of this. And now look at

02:43:44.840 --> 02:43:50.040
this what our temperatures are, we start at 12. And then we actually drop our temperature down to 10.

02:43:50.040 --> 02:43:54.120
So that's how this hidden Markov model works. Now this is nice, because you can just tweak the

02:43:54.120 --> 02:43:58.520
probabilities. This happens pretty well instantly. And we can have a look at our output very nicely.

02:43:58.520 --> 02:44:03.320
So obviously, this is representing the temperature on our like the first day, this would be the

02:44:03.320 --> 02:44:08.200
second day, third day, fourth day, fifth, sixth, seventh, and obviously, like the more days you

02:44:08.200 --> 02:44:13.000
go on, the least accurate, this is probably going to be because it just runs off probability. And

02:44:13.000 --> 02:44:16.520
if you're going to try to predict, you know, a year in advance, and you're using the weather that you

02:44:16.520 --> 02:44:20.600
have from I guess the previous year, you're probably not going to get a very accurate prediction.

02:44:20.600 --> 02:44:24.440
But anyways, these are hidden Markov models. They're not like extremely useful. There's

02:44:24.440 --> 02:44:27.800
some situations where you might want to use something like this. So that's why we're

02:44:27.800 --> 02:44:31.480
implementing them in kind of in this course and showing you how they work. It's also another

02:44:31.480 --> 02:44:35.560
feature of TensorFlow that a lot of people don't talk about or see. And you know, personally,

02:44:35.560 --> 02:44:39.720
I hadn't really heard of hidden Markov models until I started developing this course. So anyways,

02:44:39.720 --> 02:44:45.240
that has been it for this module. Now I hope that this kind of gave you guys a little bit of an idea

02:44:45.240 --> 02:44:48.840
of how we can actually implement some of these machine learning algorithms, a little bit of

02:44:48.840 --> 02:44:53.640
idea of how to work with data, how we can feed that to a model, the importance between testing

02:44:53.640 --> 02:44:57.800
and training data. And then obviously, linear regression is one we focused a lot on. So I

02:44:57.800 --> 02:45:01.800
hope you guys are very comfortable with that algorithm. And then what was the last the second

02:45:01.800 --> 02:45:05.640
one we did, I got to go up to remember exactly the sequence we had here. So classification,

02:45:05.640 --> 02:45:10.760
that one was important as well. So I hope you guys really understood that clustering, we didn't go

02:45:10.760 --> 02:45:14.920
too far into that. But again, this is an interesting algorithm. And if you need to do some kind of

02:45:14.920 --> 02:45:19.320
clustering, you now know of one algorithm to do that called K means clustering, and you understand

02:45:19.320 --> 02:45:23.080
how that works. And now you know, hidden Markov models. So in the next module, we're going to

02:45:23.080 --> 02:45:26.920
start covering neural networks, we now have the knowledge we need to really dive in there and

02:45:26.920 --> 02:45:30.600
start doing some cool stuff. And then in the future modules, we're going to do deep computer

02:45:30.600 --> 02:45:34.760
vision, I believe we're going to do chatbots with recurrent neural networks, and then some form

02:45:34.760 --> 02:45:39.320
of reinforcement learning at the end. So with that being said, let's go to the next module.

02:45:42.680 --> 02:45:47.240
Hello, everybody, and welcome to module four. Now in this module of this course, we're going to be

02:45:47.240 --> 02:45:51.960
talking about neural networks, discussing how neural networks work, a little bit of the math

02:45:51.960 --> 02:45:57.400
behind them, talking about gradient descent and back propagation, and how information actually

02:45:57.400 --> 02:46:00.920
flows through the neural network, and then getting into an example where we use a neural

02:46:00.920 --> 02:46:05.400
network to classify articles of clothing. So I know that was a lot, but that's what we're

02:46:05.400 --> 02:46:10.040
going to be covering here. Now neural networks are complex. There's kind of a lot of components

02:46:10.040 --> 02:46:13.800
that go into them. And I'm going to apologize right now, because it's very difficult to explain

02:46:13.800 --> 02:46:18.200
it all at once. What I'm going to be trying to do is kind of piece things together and explain

02:46:18.280 --> 02:46:23.320
them in blocks. And then at the end, you know, kind of combine everything together. Now I will

02:46:23.320 --> 02:46:27.480
say in case any of you didn't watch the beginning of this course, I do have very horrible handwriting,

02:46:27.480 --> 02:46:31.880
but this is the easiest way to explain things to you guys. So bear with me, you know, I'm sure

02:46:31.880 --> 02:46:35.880
you'll be able to understand what I'm saying, but it might just be painful to read some of it.

02:46:35.880 --> 02:46:39.640
All right, so let's get into it right away and start discussing what neural networks are and

02:46:39.640 --> 02:46:44.680
how they work. Well, the whole point of a neural network is to provide, you know, classification

02:46:44.680 --> 02:46:49.800
or predictions for us. So we have some input information, we feed it to the neural network,

02:46:49.800 --> 02:46:53.960
and then we want it to give us some output. So if we think of the neural network as this black box,

02:46:53.960 --> 02:46:57.640
we have all this input, right, we give all this data to the neural network, maybe we're talking

02:46:57.640 --> 02:47:02.440
about an image, maybe we're talking about just some random data points, maybe we're talking about a

02:47:02.440 --> 02:47:07.880
data set, and then we get some meaningful output. This is what we're looking at. So if we're just

02:47:07.880 --> 02:47:11.560
looking at a neural network from kind of the outside, we think of it as this magical black

02:47:11.560 --> 02:47:16.200
box, we give some input, it gives us some output. And I mean, we could call this black box just some

02:47:16.200 --> 02:47:20.520
function, right, where it's a function of the input maps it to some output. And that's exactly

02:47:20.520 --> 02:47:25.800
what a neural network does. It takes input and maps that input to some output, just like any

02:47:25.800 --> 02:47:31.640
other function, right, just like if you had a straight line like this, this is a function,

02:47:31.640 --> 02:47:36.520
you know, this is your line, you know, whatever it is, you're going to say y equals like four x,

02:47:36.600 --> 02:47:41.720
maybe that's your line, you give some input x, and it gives you some value y, this is a mapping

02:47:41.720 --> 02:47:48.040
of your input to your output. Alright, so now that we have that down, what is a neural network

02:47:48.040 --> 02:47:52.920
made up of? Well, a neural network is made up of layers. And remember, we talked about the layered

02:47:52.920 --> 02:47:57.880
representation of data when we talked about neural networks. So I'm going to draw a very basic

02:47:57.880 --> 02:48:04.360
neural network, we're going to start with the input layer. Now the input layer is always the

02:48:04.360 --> 02:48:09.720
first layer in our neural network. And it is what is going to accept our raw data. Now what I mean

02:48:09.720 --> 02:48:15.400
by raw data is whatever data we like want to give to the network, whatever we want to classify

02:48:15.400 --> 02:48:20.520
whatever our input information is, that's what this layer is going to receive in the neural

02:48:20.520 --> 02:48:25.480
network. So we can say, you know, these arrows represent our input, and they come to our first

02:48:25.480 --> 02:48:30.840
input layer. So this means, for example, if you had an image, and this image, and I'll just draw

02:48:30.920 --> 02:48:34.920
like one like this, let's say this our image, and it has all these different pixels, right,

02:48:34.920 --> 02:48:38.280
all these different pixels in the image, and you want to make a classification on this image.

02:48:39.000 --> 02:48:44.360
Well, maybe it has a width and a height and a classic width and height example is 28 by 28.

02:48:44.360 --> 02:48:48.520
If you had 28 by 28 pixels, and you want to make a classification on this image,

02:48:49.080 --> 02:48:53.160
how many input neurons you think you would need in your neural network to do this?

02:48:54.120 --> 02:48:57.560
Well, this is kind of, you know, a tough question if you don't know a lot about neural networks.

02:48:58.520 --> 02:49:02.360
If you're predicting for the image, if you're going to be looking at the entire image to

02:49:02.360 --> 02:49:08.200
make a prediction, you're going to need every single one of those pixels, which is 28 times 28

02:49:08.200 --> 02:49:14.120
pixels, which I believe is something like 784. I could be wrong on that number, but I believe

02:49:14.120 --> 02:49:20.440
that's what it is. So you would need 784 input input neurons. Now, that's totally fine. That

02:49:20.440 --> 02:49:24.120
might seem like a big number, but we deal with massive numbers when it comes to computers. So

02:49:24.200 --> 02:49:28.680
this really isn't that many. But that's an example of, you know, how you would use a neural network

02:49:28.680 --> 02:49:35.080
input layer to represent an image, you would have 784 input neurons, and you would pass

02:49:35.080 --> 02:49:39.240
one pixel to every single one of those neurons. Now, if we're doing an example where maybe we

02:49:39.240 --> 02:49:44.600
just have one piece of input information, maybe it's literally just one number. Well, then all

02:49:44.600 --> 02:49:50.920
we need is one input nerve. If we have an example where we have four pieces of information, we would

02:49:50.920 --> 02:49:56.040
need four input neurons, right? Now, this can get a little bit more complicated. But that's

02:49:56.040 --> 02:49:59.720
the basis that I want you to understand is, you know, the pieces of input you're going to have

02:49:59.720 --> 02:50:04.200
regardless of what they are, you need one input neuron for each piece of that information, unless

02:50:04.200 --> 02:50:08.040
you're going to be reshaping or putting that information in different form. Okay, so let's

02:50:08.040 --> 02:50:13.640
just actually skip ahead and go to now our output layer. So this is going to be our output. Now,

02:50:13.640 --> 02:50:18.440
what is our output layer? Well, our output layer is going to have as many neurons. And again,

02:50:18.440 --> 02:50:24.680
the neurons are just representing like a node in the layer as output pieces that we want. Now,

02:50:24.680 --> 02:50:29.800
let's say we're doing a classification for images, right? And maybe there's two classes

02:50:29.800 --> 02:50:34.840
that we could represent. Well, there's a few different ways we could design our output layer.

02:50:34.840 --> 02:50:39.720
What we could do is say, okay, we're going to use one output neuron. This output neuron is going to

02:50:39.720 --> 02:50:46.920
give us some value. We want this value to be between zero and one. And we'll say that's inclusive.

02:50:47.560 --> 02:50:53.000
Now, what we can do now if we're predicting two classes say, Okay, so if my output neuron is

02:50:53.000 --> 02:50:58.120
going to give me some value, if that value is closer to zero, then that's going to be class zero.

02:50:58.120 --> 02:51:02.920
If this value is closer to one, it's going to be class one, right? And that would mean

02:51:03.640 --> 02:51:07.400
when we have our training data, right, and we talked about training and testing data,

02:51:07.400 --> 02:51:13.160
we'd give our input and our output would need to be the value zero or one, because it's either

02:51:13.160 --> 02:51:16.520
the correct class, which is zero, right, or the correct class, which is one. So like our

02:51:16.920 --> 02:51:21.800
what am I saying, our labels for our training data set would be zero and one. And then this value

02:51:21.800 --> 02:51:26.040
on our output neuron will be guaranteed to be between zero and one, based on something that

02:51:26.040 --> 02:51:29.640
I'm going to talk about a little bit later. That's one way to approach it, right? We have a single

02:51:29.640 --> 02:51:35.160
value, we look at that value. And based on what that value is, we can determine, you know, what

02:51:35.160 --> 02:51:40.040
class we predicted, not work sometimes. But in other instances, when we're doing classification,

02:51:40.040 --> 02:51:45.720
what makes more sense is to have as many output neurons as classes you're looking to predict for.

02:51:45.720 --> 02:51:49.320
So let's say we're going to have, you know, like five classes that we're predicting for maybe

02:51:49.320 --> 02:51:54.680
these three pieces of input information are enough to make that prediction. Well, we'd actually have

02:51:54.680 --> 02:52:01.000
five output neurons. And each of these neurons would have a value between zero and one. And the

02:52:01.000 --> 02:52:07.160
combination, so the sum of every single one of these values would be equal to one. Now, can you

02:52:07.160 --> 02:52:12.440
think of what this means? If every single one of these neurons is a value between zero and one,

02:52:12.440 --> 02:52:17.000
and their sum is one, what does this look like to you? Well, to me, this looks like a probability

02:52:17.000 --> 02:52:21.240
distribution. And essentially, what's going to happen is we're going to make predictions for how

02:52:21.240 --> 02:52:27.480
strongly we think each input information is each class. So if we think that it's like class one,

02:52:27.480 --> 02:52:32.920
maybe we'll just label these like this, then what we would do is say, okay, this is going to be

02:52:32.920 --> 02:52:42.920
0.9 representing 90%. Maybe this is like 0.001, maybe this is 0.05, 0.003, you get the point,

02:52:42.920 --> 02:52:46.600
it's going to add up to one, and this is a probability distribution for our output layer.

02:52:47.160 --> 02:52:51.480
So that's a way to do it as well. And then obviously, if we're doing some kind of regression task,

02:52:51.480 --> 02:52:55.720
we can just have one neuron and that will just predict some value. And we'll define, you know,

02:52:55.720 --> 02:53:01.160
what we want that value to be. Okay, so that's my example for my output. Now let's erase this

02:53:01.160 --> 02:53:04.200
and let's actually just go back to one output neuron, because that's what I want to use for

02:53:04.200 --> 02:53:10.200
this example. Now, we have something in between these layers, because obviously, you know, we

02:53:10.200 --> 02:53:14.680
can't just go from input to output with nothing else. What we have here is called a hidden layer.

02:53:15.400 --> 02:53:18.520
Now, in neural networks, we can have many different hidden layers, we can have, you know,

02:53:18.520 --> 02:53:22.920
hidden layers that are connecting to other hidden layers, and like we could have hundreds,

02:53:22.920 --> 02:53:28.360
thousands, if we wanted to, for this basic example, we'll use one. And I'll write this as hidden.

02:53:29.320 --> 02:53:33.320
So now we have our three layers. Now, why is this called hidden? The reason this is called

02:53:33.320 --> 02:53:37.720
hidden is because we don't observe it when we're using the neural network, we pass information

02:53:37.720 --> 02:53:41.800
to the input layer, we get information from the output layer, we don't know what happens

02:53:41.800 --> 02:53:46.600
in this hidden layer or in these hidden layers. Now, how are these layers connected to each other?

02:53:46.600 --> 02:53:50.200
How do we get from this input layer to the hidden layer to the output layer and get some

02:53:50.200 --> 02:53:56.200
meaningful output? Well, every single layer is connected to another layer with something called

02:53:56.280 --> 02:54:00.120
weights. Now, we can have different kind of architectures of connections, which means I

02:54:00.120 --> 02:54:05.080
could have something like this one connects to this, this connects to this, this connects to this,

02:54:05.080 --> 02:54:09.080
and that could be like my connection kind of architecture, right? We could have another one

02:54:09.080 --> 02:54:14.920
where this one goes here. And you know, maybe this one goes here. And actually, after I've drawn

02:54:14.920 --> 02:54:19.800
this line, now we get what we're going to be talking about a lot, which is called a densely

02:54:19.800 --> 02:54:25.160
connected neural network. Now a densely connected neural network or a densely connected layer,

02:54:25.240 --> 02:54:30.120
essentially means that is connected to every node from the previous layer. So in this case,

02:54:30.120 --> 02:54:35.720
you can see every single node in the input layer is connected to every single node in the output

02:54:35.720 --> 02:54:40.760
layer or in the hidden layer, my bad. And these connections are what we call weights. Now these

02:54:40.760 --> 02:54:46.360
weights are actually what the neural network is going to change and optimize to determine the

02:54:46.360 --> 02:54:50.360
mapping from our input to our output. Because again, remember, that's what we're trying to do.

02:54:50.360 --> 02:54:54.520
We have some kind of function, we give some input, it gives us some output. How do we get that input

02:54:54.520 --> 02:54:58.680
and output? Well, by modifying these weights, it's a little bit more complex, but this is the

02:54:58.680 --> 02:55:02.920
starting. So these lines that I've drawn are really just numbers. And every single one of these

02:55:02.920 --> 02:55:07.560
lines is some numeric value. Typically, these numeric values are between zero and one, but

02:55:07.560 --> 02:55:12.280
they can be large, they can be negative. It really depends on what kind of network you're doing and

02:55:12.280 --> 02:55:17.240
how you've designed it. Now, let's just write some random numbers, we have like 0.1, this could

02:55:17.240 --> 02:55:21.800
be like 0.7, you get the point, right? We just have numbers for every single one of these lines.

02:55:22.600 --> 02:55:26.440
And these are what we call the trainable parameters that our neural network will

02:55:26.440 --> 02:55:32.280
actually tweak and change as we train to get the best possible result. So we have these

02:55:32.280 --> 02:55:35.800
connections. Now our hidden layer is connected to our output layer as well. This is again,

02:55:35.800 --> 02:55:41.800
another densely connected layer, because every layer or every nor neuron from the previous layer

02:55:41.800 --> 02:55:45.960
is connected to every neuron from the next layer, you would like to determine how many

02:55:45.960 --> 02:55:49.880
connections you have, what you can do is say there's three neurons here, there's two neurons

02:55:49.880 --> 02:55:55.240
here, three times two equals six connections. That's how that works from layers. And then

02:55:55.240 --> 02:55:59.880
obviously, you can just multiply all of the neurons together as you go through and determine

02:56:00.680 --> 02:56:05.960
what that's going to be. Okay, so that is how we connect these layers, we have these weights. So

02:56:05.960 --> 02:56:10.120
let's just write a w on here. So we remember that those are weights. Now, we also have something

02:56:10.120 --> 02:56:16.520
called biases. So let's add a bias here, I'm going to label this B. Now biases are a little bit

02:56:16.600 --> 02:56:22.040
different than these nodes we have regularly. There's only one bias, and a bias exists in

02:56:22.040 --> 02:56:27.320
the previous layer to the layer that it affects. So in this case, what we actually have is a bias

02:56:27.320 --> 02:56:33.160
that connects to each neuron in the next layer from this layer, right? So it's still densely connected.

02:56:34.200 --> 02:56:38.600
But it's just a little bit different. Now notice that this bias doesn't have an arrow beside it

02:56:38.600 --> 02:56:44.040
because this doesn't take any input information. This is another trainable parameter for the

02:56:44.040 --> 02:56:50.520
network. And this bias is just some constant numeric value that we're going to connect to the

02:56:50.520 --> 02:56:55.640
hidden layer. So we can do a few things with it. Now these weights always have a value of one.

02:56:56.280 --> 02:57:00.040
We're going to talk about why they have a value of one in a second. But just know that whenever

02:57:00.040 --> 02:57:05.720
a bias is connected to another layer or to another neuron, its weight is typically one.

02:57:06.680 --> 02:57:11.080
Okay, so we have that connected, we have our bias, and that actually means we have a bias

02:57:11.080 --> 02:57:16.520
here as well. And this bias connects to this. Notice that our biases do not connect with each

02:57:16.520 --> 02:57:19.960
other. The reason for this, again, is they're just some constant value, and they're just something

02:57:19.960 --> 02:57:25.160
we're kind of adding into the network is another trainable parameter that we can use. Now let's

02:57:25.160 --> 02:57:29.480
talk about how we actually pass information through the network and why we even use these

02:57:29.480 --> 02:57:34.920
weights and biases of what they do. So let's say we have, I can't really think of a good example,

02:57:34.920 --> 02:57:40.360
so we're just going to do some arbitrary stuff. Let's say we have like data points, right? X, Y, Z,

02:57:41.480 --> 02:57:45.640
and all of these data points have some mapped value, right? There's some value that we're

02:57:45.640 --> 02:57:48.920
looking for for them, or there's some class we're trying to put them in, maybe we're clustering

02:57:48.920 --> 02:57:56.280
them between like, red dots and blue dots. So let's do that. Let's say an XYZ is either a part of

02:57:56.280 --> 02:58:02.360
the red class, or the blue class, let's just do that. So what we want this output neuron to give

02:58:02.360 --> 02:58:07.080
us is red or blue. So what I'm going to do is say since it's just one class, we'll get this

02:58:07.160 --> 02:58:12.520
output neuron in between the range is your own one, we'll say, okay, if it's closer to zero,

02:58:12.520 --> 02:58:17.160
that's red, if it's closer to one, that's blue. And that's what we'll do for this network. And for

02:58:17.160 --> 02:58:24.200
this example, now our input neurons are going to obviously be X, Y, and Z. So let's pick some

02:58:24.200 --> 02:58:28.600
data point. And let's say we have, you know, the value two, two, two, that's our data point. And

02:58:28.600 --> 02:58:34.120
we want to predict whether it's red or blue. How do we pass it through? Well, what we need to do

02:58:34.120 --> 02:58:40.200
is determine how we can, you know, find the value of this hidden layer node, we already know the

02:58:40.200 --> 02:58:44.840
value of these input node, but now we need to go to the next layer using these connections and find

02:58:44.840 --> 02:58:49.320
what the value of these nodes are. Well, the way we determine these values is I'm going to say,

02:58:49.320 --> 02:58:54.920
and I've just said n one, just to represent like this is a node, like this is node one, maybe this

02:58:54.920 --> 02:59:01.880
one should be node two, is equal to what we call the weighted sum of all of the previous nodes that

02:59:01.880 --> 02:59:07.480
are connected to it. If that makes any sense to you guys. So a weighted sum is something like this.

02:59:07.480 --> 02:59:11.000
So I'm just going to write the equation, I'll explain it, I'm going to say n one is equal to

02:59:11.000 --> 02:59:19.320
the sum of, let's not say n equals zero, let's say, I equals zero to n of, in this case, we're

02:59:19.320 --> 02:59:26.920
going to say w i times x i plus b. Now, I know this equation looks really mathy and complicated,

02:59:26.920 --> 02:59:32.440
it's really not what this symbol and this equation here means is take the weighted sum

02:59:32.440 --> 02:59:37.480
of all the neurons that are connected to this neuron. So in this case, we have a neuron x neuron

02:59:37.480 --> 02:59:43.480
y and neuron z connected to n one. So when we take the weighted sum, or we calculate this,

02:59:43.480 --> 02:59:49.960
what this is really equal to is the weight at neuron x, we can say w x times the value at

02:59:49.960 --> 02:59:56.600
neuron x, which in this case, is just equal to two, right, plus whatever the weight is at neuron y.

02:59:56.600 --> 03:00:03.080
So in this case, this is w y. And then times two, and then you get the point where we have

03:00:03.080 --> 03:00:08.280
w z and I'm trying on the edge of my drawing tablet to write this times two. Now, obviously,

03:00:08.280 --> 03:00:12.280
these weights have some numeric value. Now, when we start our neural network, these weights are

03:00:12.280 --> 03:00:17.160
just completely random. They don't make any sense or just some random values that we can use. As the

03:00:17.160 --> 03:00:21.640
neural network gets better, these weights are updated and changed to make more sense in our

03:00:21.640 --> 03:00:26.440
network. So right now, we'll just leave them as w x, w y, w z. But no, these are some numeric

03:00:26.440 --> 03:00:32.280
values. So this returns to a sum value, right, some value, let's just call this value v. And

03:00:32.280 --> 03:00:38.040
that's what this is equal to. So v. Then what we do is we add the bias. Now remember, the bias

03:00:38.040 --> 03:00:44.760
was connected with a weight of one, which means if we take the weighted sum of the bias, right,

03:00:44.760 --> 03:00:49.640
all we're doing is adding whatever that biases value was. So if this bias value was 100,

03:00:49.640 --> 03:00:54.840
then what we do is we add 100. Now, I've just written the plus B to explicitly state the fact

03:00:54.840 --> 03:00:59.160
that we're adding the bias, although it could really be considered as a part of the summation

03:00:59.160 --> 03:01:04.840
equation, because it's another connection to the neural. Now, let's just talk about what

03:01:04.840 --> 03:01:09.720
this symbol means for anyone that's confused about that. Essentially, this stands for sum,

03:01:09.720 --> 03:01:16.120
I stands for an index, and n stands for what index will go up to now n means how many neurons we

03:01:16.120 --> 03:01:21.320
had in the previous layer. And then what we're doing here is saying wi xi. So we're going to say

03:01:21.320 --> 03:01:27.400
weight zero x zero plus weight one x one plus weight two x two, it's almost like a for loop where

03:01:27.400 --> 03:01:31.880
we're just adding them all together. And then we add the B. And I hope that makes enough sense

03:01:31.880 --> 03:01:36.520
so that we understand that. So that is our weighted sum and our bias. So essentially, what we do is

03:01:36.520 --> 03:01:40.120
we go through and we calculate these values. So this gets some value, maybe this value is like

03:01:40.120 --> 03:01:45.720
0.3, maybe this value seven, whatever it is, and we do the same thing now at our output neuron.

03:01:45.720 --> 03:01:51.080
So we take the weighted sum of this value times its weight. And then we take the weighted sum,

03:01:51.080 --> 03:01:57.080
so this value times its weight, plus the bias, this is given some value here. And then we can

03:01:57.080 --> 03:02:02.280
look at that value and determine what the output of our neural network is. So that is pretty much

03:02:02.280 --> 03:02:06.600
how that works in terms of the weighted sums, the weights and the biases. Now, let's talk about the

03:02:06.600 --> 03:02:10.920
kind of the training process and another thing called an activation function. So I've lied to

03:02:10.920 --> 03:02:14.200
a little bit because I've said I'm just going to start erasing some stuff. So we have a little bit

03:02:14.200 --> 03:02:19.000
more room on here. So I've lied to you and I've said that this is completely how this works.

03:02:19.000 --> 03:02:23.080
Well, we're missing one key feature that I want to talk about, which is called an activation

03:02:23.080 --> 03:02:29.320
function. Now remember how we want this value to be in between zero and one right at our output layer.

03:02:29.320 --> 03:02:33.400
Well, right now, we can't really guarantee that that's going to happen. I mean, especially for

03:02:33.400 --> 03:02:38.440
starting with random weights and random biases in our neural network, we're passing this information

03:02:38.440 --> 03:02:44.120
through, we could get to this, you know, point here, we could have like 700 as our value.

03:02:44.840 --> 03:02:48.600
That's kind of crazy to me, right? We have this huge value, how do we look at 700 and

03:02:48.600 --> 03:02:52.120
determine whether this is red or whether this is blue? Well, we can use something called an

03:02:52.120 --> 03:02:56.280
activation function. Now, I'm going to go back to my slides here, whatever you want to call this

03:02:56.280 --> 03:03:00.200
this notebook, just to talk about what an activation function is. And you guys can see here, you can

03:03:00.200 --> 03:03:05.800
follow along, I have all the equations kind of written out here as well. So let's go to activation

03:03:05.880 --> 03:03:10.360
function, which is right here. Okay. So these are some examples of an activation function. And I

03:03:10.360 --> 03:03:14.680
just want you to look at what they do. So this first one is called rectified linear unit. Now

03:03:14.680 --> 03:03:20.200
notice that essentially what this activation function does is take any values that are less

03:03:20.200 --> 03:03:25.080
than zero and just make them zero. So any x values that are, you know, in the negative, it just makes

03:03:25.080 --> 03:03:30.440
their y zero. And then any values that are positive, it's just equal to whatever their positive value

03:03:30.440 --> 03:03:35.160
is. So if it's 10, it's 10. This allows us to just pretty much eliminate any negative numbers,

03:03:35.160 --> 03:03:40.760
right? That's kind of what rectified linear unit does. Now 10 h or hyperbolic tangent.

03:03:41.400 --> 03:03:46.520
What does this do? This actually squishes our values between negative one and one. So it takes

03:03:46.520 --> 03:03:51.720
whatever values we have, and the more positive they are, the closer to one they are, the more

03:03:51.720 --> 03:03:56.360
negative they are, the closer to negative one they are. So when we see why this might be useful,

03:03:56.360 --> 03:04:00.200
right for a neural network, and then last one is sigmoid, what this does is squish our values

03:04:00.200 --> 03:04:05.400
between zero and one, a lot of people call it like the squishifier function. Because all it does

03:04:05.400 --> 03:04:09.960
is take any extremely negative numbers and put them closer to zero and any extremely positive

03:04:09.960 --> 03:04:13.560
numbers and put them close to one, any values in between, you're going to get some number that's

03:04:13.560 --> 03:04:18.760
kind of in between that, based on the equation one over one plus e to the negative z. And this is

03:04:18.760 --> 03:04:23.640
theta z, I guess, is equal to that. Okay, so that's how that works. Those are some activation

03:04:23.640 --> 03:04:27.240
functions. Now I hope that's not too much math for you. But let's talk about how we use them,

03:04:27.240 --> 03:04:32.120
right? So essentially, what we do is at each of our neurons, we're going to have an activation

03:04:32.120 --> 03:04:37.560
function that is applied to the output of that neuron. So we take this this weighted sum plus

03:04:37.560 --> 03:04:43.720
the bias, and then we apply an activation function to it before we send that value to the next neuron.

03:04:43.720 --> 03:04:50.680
So in this case, n one isn't actually just equal to this, what n one is equal to is n one is equal

03:04:50.680 --> 03:04:56.760
to f, which stands for activation function of this equation, right? So we say I equals zero,

03:04:58.040 --> 03:05:06.520
w i x i plus B. And that's what n one's value is equal to when it comes to this output neuron.

03:05:07.160 --> 03:05:11.400
So each of these have an activation function on them. And two has the same activation function

03:05:11.400 --> 03:05:16.280
as n one. And we can define what activation function we want to apply at each neuron.

03:05:16.920 --> 03:05:20.840
Now at our output neuron, the activation function is very important, because we need to determine

03:05:20.840 --> 03:05:24.040
what we want our value to look like. Do we want it between negative one on one? Do we want it

03:05:24.040 --> 03:05:29.960
between zero and one? Or do we want it to be some massively large number? Do we want it between zero

03:05:29.960 --> 03:05:35.000
and positive infinity? What do we want? Right? So what we do is we pick some activation function

03:05:35.000 --> 03:05:41.000
for our output neuron. And based on what I said, where we want our values between zero and one,

03:05:41.080 --> 03:05:48.040
I'm going to be picking the sigmoid function. So sigmoid, recall, squishes our values between

03:05:48.040 --> 03:05:55.160
zero and one. So what we'll do here is we'll take n one, right? So n one times whatever the weight

03:05:55.160 --> 03:06:03.240
is there. So weight zero, plus n two times weight one, plus a bias, and apply sigmoid.

03:06:04.280 --> 03:06:10.120
And then this will give us some value between zero and one, then we can look at that value and we

03:06:10.120 --> 03:06:14.520
can determine what the output of this network is. So that's great. And that makes sense. Why

03:06:14.520 --> 03:06:19.400
we would use that on the output neuron, right? So we can squish our value in between some kind

03:06:19.400 --> 03:06:22.440
of value. So we can actually look at it and determine, you know, what to do with it, rather

03:06:22.440 --> 03:06:27.080
than just having these crazy, and I want to see if I can make this eraser any bigger. Ah, that's

03:06:27.080 --> 03:06:32.680
much better. Okay. So there we go. Let's just erase some of this. And now let's talk about why we

03:06:32.680 --> 03:06:37.480
would use the activation function on like an intermediate layer like this. Well, the whole

03:06:37.480 --> 03:06:43.480
point of an activation function is to introduce complexity into our neural network. So essentially,

03:06:43.480 --> 03:06:47.720
you know, we just have these basic weights and these biases. And this is kind of just,

03:06:47.720 --> 03:06:51.000
you know, like a complex function at this point, we have a bunch of weights, we have a bunch of

03:06:51.000 --> 03:06:54.600
biases. And those are the only things that we're training. And the only things that we're changing

03:06:54.600 --> 03:06:59.880
to make our network better. Now, what an activation function can do is, for example,

03:06:59.880 --> 03:07:03.000
take a bunch of points that are on the same like plane, right? So let's just say,

03:07:03.880 --> 03:07:09.560
these are in some plane. If we can apply an activation function of these, where we

03:07:10.520 --> 03:07:14.920
introduce a higher dimensionality, so an activation function like sigmoid that is like a

03:07:14.920 --> 03:07:21.800
higher dimension function, we can hopefully spread these points out and move them up or down off the

03:07:21.800 --> 03:07:28.760
plane in a hopes of extracting kind of some different features. Now, it's hard to explain

03:07:28.760 --> 03:07:33.800
this until we get into the training process of the neural network. But I'm hoping this is maybe

03:07:33.800 --> 03:07:39.000
giving you a little bit of idea, if we can introduce a complex activation function into this kind of

03:07:39.000 --> 03:07:43.560
process, then it allows us to make some more complex predictions, we can pick up on some

03:07:43.560 --> 03:07:48.360
different patterns. If I can see that, you know, when sigmoid or rectify linear unit is applied

03:07:48.360 --> 03:07:53.000
to this output, it moves my point up or it moves it down or moves it in like whatever direction

03:07:53.000 --> 03:07:57.560
and n dimensional space, then I can determine specific patterns I couldn't determine in the

03:07:57.560 --> 03:08:01.960
previous dimension. That's just like if we're looking at something in two dimensions, if I can

03:08:01.960 --> 03:08:05.880
move that into three dimensions, I immediately see more detail, there's more things that I can

03:08:05.880 --> 03:08:10.360
look at, right? And I'll try to do a good example of why we might use it like this. So let's say

03:08:10.360 --> 03:08:14.280
we have a square, right, like this, right? And I ask you, I'm like, tell me some information

03:08:14.280 --> 03:08:17.080
about the square. Well, what you can tell me immediately is you can tell me the width, you

03:08:17.080 --> 03:08:21.160
can tell me the height, and I guess you could tell me the color, right? You can tell me it has

03:08:21.160 --> 03:08:24.920
one face, you can tell me it has four vertexes, you can tell me a fair amount about the square,

03:08:24.920 --> 03:08:29.400
you can tell me its area. Now what happens as soon as I extend the square and I make it into a cube?

03:08:30.280 --> 03:08:34.760
Well, now you can immediately tell me a lot more information, you can tell me, you know, the height,

03:08:35.640 --> 03:08:40.600
or I guess the depth with height depth, yeah, whatever you want to call it there. You can tell

03:08:40.600 --> 03:08:44.600
me how many faces it has, you can tell me what color each of the faces are, you can tell me how

03:08:44.600 --> 03:08:49.880
many vertexes you can tell me if this cube or the square, this rectangle is uniform or not,

03:08:49.880 --> 03:08:53.880
and you can pick up on a lot more information. So that's kind of I mean, this is a very over

03:08:53.880 --> 03:08:59.160
simplification of what this actually does. But this is kind of the concept, right, is that if we

03:08:59.160 --> 03:09:03.720
are in two dimensions, if we can somehow move our data points into a higher dimension by applying

03:09:03.720 --> 03:09:08.360
some function to them, then what we can do is get more information and extract more information

03:09:08.360 --> 03:09:13.320
about the data points, which will lead to better predictions. Okay, so now that we've talked about

03:09:13.320 --> 03:09:16.360
all this, it's time to talk about how neural networks train. And I think you guys are ready

03:09:16.360 --> 03:09:21.400
for this. This is a little bit more complicated. But again, it's not that crazy. Alright, so we

03:09:21.480 --> 03:09:25.400
talked about these weights and biases. And these weights and biases are what our network will

03:09:25.400 --> 03:09:31.000
come up with and determine to, you know, like make the network better. So essentially, what we're

03:09:31.000 --> 03:09:36.120
going to do now is talk about something called a loss function. So as our network starts, right,

03:09:36.120 --> 03:09:41.000
the way that we train it, just like we've trained other networks, or other machine learning models

03:09:41.000 --> 03:09:45.960
is we give it some information, we give it what the expected output is. And then we just see what

03:09:45.960 --> 03:09:50.440
the expected output or what the output was from the network, compare it to the expected output

03:09:50.440 --> 03:09:55.240
and modify it like that. So essentially, what we start with is we say, okay, 222, we say this

03:09:55.240 --> 03:10:00.840
class is red, which I forget what I labeled that was as but let's just say, like that was a zero,

03:10:00.840 --> 03:10:07.000
okay. So this class is zero. So I want this network to give me a zero for the point 222. Now,

03:10:07.000 --> 03:10:12.360
this network starts with completely random weights and completely random biases. So chances are,

03:10:12.360 --> 03:10:16.840
when we get to this output here, we're not going to get zero, maybe we get some value after applying

03:10:16.840 --> 03:10:24.040
the sigmoid function, that's like 0.7. Well, this is pretty far away from red. But how far away is

03:10:24.040 --> 03:10:29.400
it? Well, this is where we use something called a loss function. Now, what a loss function does is

03:10:29.400 --> 03:10:36.440
calculate how far away our output was from our expected output. So if our expected output is

03:10:36.440 --> 03:10:42.200
zero, and our output was 0.7, the loss function is going to give us some value that represents

03:10:42.200 --> 03:10:47.800
like how bad or how good this network was. Now, if it tells us this network was really bad,

03:10:47.800 --> 03:10:52.520
it gives us like a really high loss, then that tells us that we need to tweak the weights and

03:10:52.520 --> 03:10:58.040
biases more and move the network in a different direction. We're starting to get into gradient

03:10:58.040 --> 03:11:02.120
descent. But let's understand the loss function first. So it's going to say, if it was really

03:11:02.120 --> 03:11:05.800
bad, let's move it more, let's change the weights more drastically, let's change the biases more

03:11:05.800 --> 03:11:10.840
drastically. Whereas, if it was really good, it'll be like, Okay, so that one was actually decent,

03:11:10.840 --> 03:11:14.600
you know, you only need to tweak a little bit, and you only need to move this, this and this.

03:11:14.600 --> 03:11:18.680
So that's good. And that's the point of this loss function, it just calculates some value,

03:11:18.680 --> 03:11:22.600
the higher the value, the worse our network was a few examples of loss function.

03:11:23.320 --> 03:11:30.520
Let's go down here, because I think I had a few optimizer loss here, mean squared error,

03:11:30.520 --> 03:11:36.200
mean absolute error and hinge loss. Now mean absolute error, you know, let's actually just

03:11:36.280 --> 03:11:44.120
look one up here. So mean, absolute error, and have a look at what this is. So images,

03:11:44.120 --> 03:11:51.560
let's pick something. This is mean absolute error. This is the equation for mean absolute error.

03:11:51.560 --> 03:11:59.720
Okay, so the summation of the absolute value of yi minus lambda of xi over n. Now, this is kind of

03:11:59.720 --> 03:12:03.400
complicated. I'm not going to go into it too much. I was expecting I was hoping I was going to get

03:12:03.400 --> 03:12:13.960
like a better example for mean squared error. Okay, so these are the three loss functions here.

03:12:13.960 --> 03:12:17.960
So mean squared error, mean absolute error, hinge loss, obviously, there's a ton more that we could

03:12:17.960 --> 03:12:22.280
use. I'm not going to talk about which how each of these work specifically, I mean, you can look

03:12:22.280 --> 03:12:26.920
them up pretty easily. And also, so you know, these are also referenced as cost functions,

03:12:26.920 --> 03:12:32.680
so cost or loss, you might just hear these, these terms kind of interchanged cost and loss

03:12:32.680 --> 03:12:37.000
essentially mean the same thing, you want your network to cost the least, you want your network

03:12:37.000 --> 03:12:41.720
to have the least amount of loss. Okay, so now that we have talked about the loss function,

03:12:42.360 --> 03:12:47.800
we need to talk about how we actually update these weights and biases. Now, actually, let's

03:12:47.800 --> 03:12:51.800
go back to here, because I think I had some notes on it. This is what we call gradient descent.

03:12:52.520 --> 03:12:56.840
So essentially, the parameters for our network are weights and biases. And by changing these

03:12:56.840 --> 03:13:01.640
weights and biases, we will, you know, either make the network better or make the network worse,

03:13:01.720 --> 03:13:05.880
the loss function will determine if the network is getting better, if it's getting worse, and then

03:13:05.880 --> 03:13:11.240
we can determine how we're going to move the network to change that. So this is now gradient

03:13:11.240 --> 03:13:15.880
descent, where the math gets a little bit more complicated. So this is an example of what your

03:13:15.880 --> 03:13:22.360
neural network function might look like. Now, as you have higher dimensional math, you have,

03:13:22.360 --> 03:13:26.600
you know, a lot more dimensions, a lot more space to explore when it comes to creating

03:13:26.600 --> 03:13:31.160
different parameters and creating different biases and activation functions and all of that.

03:13:31.240 --> 03:13:35.240
So as we apply our activation functions, we're kind of spreading our network into higher

03:13:35.240 --> 03:13:39.240
dimensions, which just makes things much more complicated. Now, essentially, what we're trying

03:13:39.240 --> 03:13:44.280
to do with the neural network is optimize this loss function. This loss function is telling us

03:13:44.280 --> 03:13:48.520
how good it is or how bad it is. So if we can get this loss function as low as possible,

03:13:48.520 --> 03:13:52.920
then that means we should technically have the best neural network. So this is our kind of

03:13:52.920 --> 03:13:57.800
loss functions, like mapping or whatever, what we're looking for is something called a global

03:13:57.800 --> 03:14:03.560
minimum, we're looking for the minimum point where we get the least possible loss from our

03:14:03.560 --> 03:14:07.880
neural network. So if we start where these red circles are, right, and I've just stole this

03:14:07.880 --> 03:14:14.120
image off Google images, what we're trying to do is move downwards into this global global

03:14:14.120 --> 03:14:18.920
minimum. And this is with a process of called gradient descent. So we calculate this loss,

03:14:18.920 --> 03:14:24.440
and we use an algorithm called gradient descent, which tells us what direction we need to move

03:14:24.520 --> 03:14:29.400
our function to determine or to get to this global minimum. So it essentially looks where

03:14:29.400 --> 03:14:33.400
we are. It says this was the loss. And it says, okay, I'm going to calculate what's called a

03:14:33.400 --> 03:14:38.040
gradient, which is literally just a steepness or a direction. And we're going to move in that

03:14:38.040 --> 03:14:43.000
direction. And then the algorithm called brought back propagation, we'll go backwards through

03:14:43.000 --> 03:14:48.040
the network and update the weights and biases so that we move in that direction. Now, I think this

03:14:48.040 --> 03:14:52.040
is as far as I really want to go, because I know this is getting more complicated already,

03:14:52.040 --> 03:14:56.760
then some of you guys probably can handle and that I can probably explain. But that's kind of

03:14:56.760 --> 03:15:00.760
the basic principle. We'll go back to the drawing board and we'll do a very quick recap before we

03:15:00.760 --> 03:15:06.760
get into some of the other stuff, neural networks, input, output hidden layers connected with weights,

03:15:06.760 --> 03:15:11.720
there's biases that connect to each layer. These biases can be thought of as y intercepts, they'll

03:15:11.720 --> 03:15:17.480
simply move completely up or move completely down that entire, you know, activation function,

03:15:17.480 --> 03:15:21.960
right, we're shifting things left or right, because this will allow us to get a better

03:15:21.960 --> 03:15:26.600
prediction and have another parameter that we can train and add a little bit of complexity

03:15:26.600 --> 03:15:32.120
to our neural network model. Now, the way that information is passed through these layers is

03:15:32.120 --> 03:15:37.800
we take the weighted sum at a neuron of all of the connected neurons to it, we then add this

03:15:37.800 --> 03:15:43.720
bias neuron, and we apply some activation function that's going to put this, you know, these values

03:15:43.720 --> 03:15:48.600
in between two set values. So for example, when we talk about sigmoid, that's going to squish our

03:15:48.600 --> 03:15:52.600
values between zero and one, when we talk about hyperbolic tangent, that's going to squish our

03:15:52.600 --> 03:15:56.760
values between negative one and one. And when we talk about rectifier linear unit, that's going to

03:15:56.760 --> 03:16:01.320
squish our values between zero and positive infinity. So we apply those activation functions,

03:16:01.320 --> 03:16:06.040
and then we continue the process. So n one gets its value and two gets its value. And then finally,

03:16:06.040 --> 03:16:09.480
we make our way to our output layer, we might have passed through some other hidden layers

03:16:09.480 --> 03:16:14.200
before that. And then we do the same thing, we take the weighted sum, we add the bias,

03:16:14.200 --> 03:16:19.880
we apply an activation function, we look at the output, and we determine whether we know we are

03:16:19.880 --> 03:16:24.440
a class y or we are class z or whether this is the value we're looking for. And and that's how

03:16:24.440 --> 03:16:29.880
it works. Now we're at the training process, right? So we're doing this now, that's kind of how this

03:16:29.880 --> 03:16:34.280
worked when we were making a prediction. So when we're training, essentially, what happens is we

03:16:34.280 --> 03:16:41.000
just make predictions, we compare those predictions to whatever these expected value should be using

03:16:41.000 --> 03:16:46.840
this loss function. Then we calculate what's called a gradient, a gradient is the direction we need

03:16:46.840 --> 03:16:51.560
to move to minimize this loss function. And this is where the advanced math happens and why I'm

03:16:51.560 --> 03:16:56.760
kind of skimming over this aspect. And then we use an algorithm called back propagation, where we

03:16:56.760 --> 03:17:01.720
step backwards through the network, and update the weights and biases, according to the gradient

03:17:01.800 --> 03:17:07.640
that we calculated. Now that is pretty much how this works. So you know, the more info we have,

03:17:08.440 --> 03:17:12.600
likely unless we're overfitting, but you know, if we have a lot of data, if we can keep feeding

03:17:12.600 --> 03:17:17.240
the network, it starts off being really horrible, having no idea what's going on. And then as more

03:17:17.240 --> 03:17:21.960
and more information comes in, it updates these weights and biases gets better and better sees

03:17:21.960 --> 03:17:26.440
more examples. And after you know, a certain amount of epochs or certain amount of pieces of

03:17:26.440 --> 03:17:30.760
information, our network is making better and better predictions and having a lower and lower

03:17:30.760 --> 03:17:35.320
loss. And the way we will calculate how well our network is doing is by passing it, you know,

03:17:35.320 --> 03:17:42.680
our validation data set, where it can say, okay, so we got an 85% accuracy on this data set, we're

03:17:42.680 --> 03:17:47.240
doing okay, you know, let's tweak this, let's tweak that, let's do this. So the loss function,

03:17:47.240 --> 03:17:52.680
the lower this is the better, also known as the cost function. And that is kind of neural networks

03:17:52.680 --> 03:17:57.480
in a nutshell. Now I know this wasn't really in a nutshell, because it was 30 minutes long. But that

03:17:57.480 --> 03:18:01.720
is, you know, as much of an explanation as I can really give you without going too far into the

03:18:01.720 --> 03:18:06.040
mathematics behind everything. And again, remember, the activation function is to move us up in

03:18:06.040 --> 03:18:10.760
dimensionality. The bias is another layer of complexity and a trainable parameter for our

03:18:10.760 --> 03:18:17.000
network allows us to shift this kind of activation function left, right up, down. And yeah, that

03:18:17.000 --> 03:18:23.400
is how that works. Okay, so now we have an optimizer. This is kind of the last thing on

03:18:23.480 --> 03:18:27.880
how neural networks work. optimizer is literally just the algorithm that does the gradient descent

03:18:27.880 --> 03:18:32.520
and back propagation for us. So I mean, you guys can read through some of them here, we'll be using

03:18:33.320 --> 03:18:37.560
probably the atom optimizer for most of our examples, although there's, you know, lots of

03:18:37.560 --> 03:18:42.200
different ones that we can pick from. Now this optimization technique, again, is just a different

03:18:42.200 --> 03:18:45.560
algorithm. There's some of them are faster, some of them are slower, some of them work a little bit

03:18:45.560 --> 03:18:50.120
differently. And we're not really going to get into picking optimizers in this course, because

03:18:50.200 --> 03:18:54.600
that's more of an advanced machine learning technique. All right, so enough explaining,

03:18:54.600 --> 03:19:01.480
enough math, enough drawings, enough talking. Now it is time to create our first official

03:19:01.480 --> 03:19:06.200
neural network. Now these are the imports we're going to need. So import TensorFlow is TF from

03:19:06.200 --> 03:19:10.200
TensorFlow import Keras again, so this does actually come with TensorFlow. I forget if I

03:19:10.200 --> 03:19:15.800
said you need to install that before. My apologies and then import numpy as NP, import map plot

03:19:15.880 --> 03:19:21.000
live dot pi plot as PLT. Alright, so I'm going to do actually similar thing to what I did before

03:19:21.000 --> 03:19:25.160
where I'm kind of just going to copy some of this code into another notebook, just to make sure

03:19:25.160 --> 03:19:29.880
that we can look at everything at the end, and then kind of step through the code step by step

03:19:29.880 --> 03:19:35.960
rather than all of the text kind of happening here. Alright, so the data set, and the problem

03:19:35.960 --> 03:19:40.920
we are going to consider for our first neural network is the fashion MNIST data set. Now the

03:19:41.000 --> 03:19:46.680
fashion MNIST data set contains 60,000 images for training and 10,000 images for validating and

03:19:46.680 --> 03:19:54.600
testing 70,000 images. And it is essentially pixel data of clothing articles. So what we're

03:19:54.600 --> 03:19:59.880
going to do to load in this data set from Keras, this actually built into Keras, it's meant as

03:19:59.880 --> 03:20:06.280
like a beginner, like testing training data set, we're going to say fashion underscore MNIST

03:20:06.280 --> 03:20:12.920
equals Keras dot data sets dot fashion MNIST. Now this will get the data set object, and then we

03:20:12.920 --> 03:20:18.920
can load that object by doing fashion MNIST dot load data. Now by doing this by having the tuples

03:20:18.920 --> 03:20:25.480
train images train labels, test images test labels equals this, this will automatically split our

03:20:25.480 --> 03:20:30.760
data into the sets that we need. So we need the training, and we need the testing. And again,

03:20:30.760 --> 03:20:35.160
we've talked about all of that. So I'm going to kind of skim through that. And now we have it in

03:20:35.160 --> 03:20:39.800
all of these kind of tuples here. Alright, so let's have a look at this data set to see what

03:20:39.800 --> 03:20:42.920
we're working with. Okay, so let's run some of this code, let's get this import going,

03:20:43.640 --> 03:20:49.880
if it doesn't take forever. Okay, let's get the data sets. Yeah, this will take a second to

03:20:49.880 --> 03:20:54.360
download for you guys, if you don't already have it cached. And then we'll go train images dot

03:20:54.360 --> 03:20:59.880
shape. And let's look at what one of the images looks like. Or sorry, what our data set looks

03:20:59.960 --> 03:21:06.200
like. So we have 60,000 images that are 28 by 28. Now what that means is we have 28 pixels or 28

03:21:06.200 --> 03:21:11.080
rows of 28 pixels, right? So that's kind of what our, you know, information is. So we're going to

03:21:11.080 --> 03:21:19.240
have in total 784 pixels, which I've denoted here. So let's have a look at one pixel. So to reference

03:21:19.240 --> 03:21:24.680
one pixel, this is what I what I'm doing, this comes in as a, actually, I'm not sure what type of

03:21:24.680 --> 03:21:29.400
data frame this is. But let's have a look at it. So let's say type of train underscore images,

03:21:29.400 --> 03:21:35.720
because I want to see that. So that's an numpy array. So to reference the different indexes in

03:21:35.720 --> 03:21:40.760
this is similar to pandas, we're just going to do zero comma 23 comma 23, which stands for, you

03:21:40.760 --> 03:21:48.360
know, image zero 23, and then 23. And this gives us one pixel. So row 23 column 23, which will be

03:21:48.360 --> 03:21:55.480
that. Okay, so let's run this. And let's see this value is 194. Okay, so that's kind of interesting.

03:21:55.480 --> 03:21:59.800
That's what one pixel looks like. So let's look at what multiple pixels look like. So we'll print

03:22:00.440 --> 03:22:08.040
train underscore images. And okay, so we get all these zeros, let's print train images, zero,

03:22:08.040 --> 03:22:13.880
colon, that should work for us. And we're getting all these zeros. Okay, so that's the border of

03:22:13.880 --> 03:22:19.160
the picture. That's okay, I can't show you what I wanted to show you. Anyways, one pixel. And I

03:22:19.160 --> 03:22:24.680
wanted to have you guys guess it is simply just represented by a number between zero and 255.

03:22:25.240 --> 03:22:29.560
Now what this stands for is the grayscale value of this pixel. So we're dealing with grayscale

03:22:29.560 --> 03:22:35.720
images, although we can deal with, you know, 3d, 4d, 5d images as well, or not 5d images,

03:22:35.720 --> 03:22:40.440
but we can deal with images that have like RGB values. First, so for example, we could have

03:22:40.440 --> 03:22:45.720
a number between zero 255, another number between zero and 255 and another number between zero and

03:22:45.720 --> 03:22:51.960
255 for every single pixel, right? Whereas this one is just one simple static value. Okay, so it

03:22:51.960 --> 03:22:57.640
says that here, a pixel values between zero and 255, zero being black and 255 being white. So

03:22:57.640 --> 03:23:02.360
essentially, you know, if it's 255, that means that this is white, if it's zero, that means that

03:23:02.360 --> 03:23:07.320
it is black. Alright, so let's have a look at the first 10 training labels. So that was our

03:23:07.320 --> 03:23:12.200
training images. Now what are the training labels? Okay, so we have an array, and we get values from

03:23:12.200 --> 03:23:19.160
zero to nine. Now this is because we have 10 different classes that we could have for our

03:23:19.240 --> 03:23:24.040
dataset. So there's 10 different articles of clothing that are represented. I don't know what

03:23:24.040 --> 03:23:29.320
all of them are, although they are right here. So t shirt, trouser, pullover, dress, coat, sandals,

03:23:29.320 --> 03:23:35.400
shirt, sneaker, bag, ankle boot. Okay, so let's run this class names, just so that we have that

03:23:35.400 --> 03:23:40.200
saved. And now what I'm going to do is use matplotlib to show you what one of the images looks like.

03:23:40.200 --> 03:23:44.600
So in this case, this is a shirt. I know this is printing out kind of weird, but I'm just showing

03:23:44.600 --> 03:23:48.280
the image. I know it's like different colors, but that's because if we don't define that we're

03:23:48.280 --> 03:23:52.520
drawing a grayscale, it's going to do this. But anyways, that is what we get for the shirt. So

03:23:52.520 --> 03:23:57.160
let's go to another image and let's have a look at what this one is. I actually don't know what

03:23:57.160 --> 03:24:04.920
that is. So we'll skip that maybe that's a what is it t shirt or top. This I guess is going to be

03:24:04.920 --> 03:24:10.840
like a dress. Yeah, so we do have dressed there. Let's go for have a look at this. And some of

03:24:10.840 --> 03:24:16.040
these are like hard to even make out when I'm looking at them myself. And then I guess this

03:24:16.120 --> 03:24:19.320
will be like a hoodie or something. I'm trying to get one of the sandals to show you guys a

03:24:19.320 --> 03:24:23.720
few different ones. There we go. So that is a sandal or a sneaker. Okay, so that is kind of

03:24:23.720 --> 03:24:27.880
how we do that and how we look at the different images. So if you wanted to draw it out, all you

03:24:27.880 --> 03:24:32.600
do is just make a figure, you just show the image, do the color bar, which is just giving you this,

03:24:32.600 --> 03:24:35.560
then you're going to say, I don't want to grid and then you can just show the image, right? Because

03:24:35.560 --> 03:24:41.000
if you don't have this line here, and you show with the grid, oh, it's actually not showing the grid

03:24:41.000 --> 03:24:45.000
that's interesting. Although I thought it was going to show me those pixelated grid. So I guess

03:24:45.080 --> 03:24:50.040
you don't need that line. Alright, so data pre processing. Alright, so this is an important

03:24:50.040 --> 03:24:54.920
step in neural networks. And a lot of times when we have our data, we have it in these like random

03:24:54.920 --> 03:24:59.240
forms, or we're missing data, or there's information we don't know, or that we haven't seen. And

03:24:59.240 --> 03:25:03.800
typically what we need to do is pre process it. Now, what I'm going to do here is squish all my

03:25:03.800 --> 03:25:08.840
values between zero and one. Typically, it's a good idea to get all of your input values in a

03:25:08.840 --> 03:25:13.560
neural network in between, like that range in between, I would say negative one and one is what

03:25:13.640 --> 03:25:17.480
you're trying to do, you're trying to make your numbers as small as possible to feed to the neural

03:25:17.480 --> 03:25:21.880
network. The reason for this is your neural network starts out with random weights and biases

03:25:21.880 --> 03:25:27.240
that are in between the range zero and one, unless you change that value. So if you have massive

03:25:27.240 --> 03:25:31.400
input information and tiny weights, then you're kind of having a bit of a mismatch, and you're

03:25:31.400 --> 03:25:36.040
going to make it much more difficult for your network to actually classify your information,

03:25:36.040 --> 03:25:40.040
because it's going to have to work harder to update those weights and biases to reduce how

03:25:40.040 --> 03:25:45.800
large those values are going to be, if that makes any sense. So it usually is a good idea to

03:25:45.800 --> 03:25:51.160
pre process these and make them in between the value of zero and one. Now, since we know that

03:25:51.160 --> 03:25:55.960
we're just going to have pixel values that are in the range of 255, we can just divide by 255,

03:25:55.960 --> 03:26:01.640
and that will automatically scale it down for us. Although it is extremely important that we do this

03:26:01.640 --> 03:26:07.640
to not only the training images, but the testing images as well. If you just pre process your

03:26:07.640 --> 03:26:12.760
training images, and then you pass in, you know, new data that's not pre processed, that's going to

03:26:12.760 --> 03:26:17.000
be a huge issue. You need to make sure that your data comes in the same form. And that means when

03:26:17.000 --> 03:26:22.360
we're using the model to to make predictions, whatever, you know, I guess it pixel data we

03:26:22.360 --> 03:26:26.920
have, we need to pre process in the same way that we pre processed our other data. Okay,

03:26:26.920 --> 03:26:31.800
so let's pre process that so train images and test images. And I'm just going to actually steal

03:26:32.760 --> 03:26:36.680
some of this stuff here, and throw it in my other one before we get too far. So let's get this

03:26:36.760 --> 03:26:42.520
data sets. And let's throw it in here, just so we can come back and reference all this together.

03:26:42.520 --> 03:26:49.080
Let's go class names. We don't actually need the figures, a few things I can skip, we do need

03:26:49.080 --> 03:26:56.040
this pre processing step. Like that, if I could go over here. And then what else do we need,

03:26:56.040 --> 03:26:59.480
we're going to need this model. Okay, so let's actually just copy the model into this and just

03:26:59.480 --> 03:27:05.160
make it a little bit cleaner. We can have a look at it. So new code block model. Okay, so model,

03:27:05.880 --> 03:27:09.720
creating our model. Now creating our models actually really easy. I'm hoping what you guys

03:27:09.720 --> 03:27:13.960
have realized so far is that data is usually the hardest part of machine learning and neural

03:27:13.960 --> 03:27:18.040
networks, getting your data in the right form, the right shape and you know, pre processed

03:27:18.040 --> 03:27:22.120
correctly, building the models usually pretty easy because we have tools like TensorFlow and Keras

03:27:22.120 --> 03:27:27.320
that can do it for us. So we're going to say model equals Keras dot sequential. Now sequential

03:27:27.320 --> 03:27:31.160
simply stands for the most basic form of neural network, which we've talked about so far, which

03:27:31.160 --> 03:27:35.880
is just information going from the left side to the right side, passing through the layers,

03:27:35.880 --> 03:27:40.680
sequentially, right, called sequential, we have not talked about recurrent or convolutional

03:27:40.680 --> 03:27:46.920
neural networks yet. Now what we're going to do here is go Keras dot layers dot flat. So sorry,

03:27:46.920 --> 03:27:51.240
inside here, we're going to define the layers that we want in our neural network. This first

03:27:51.240 --> 03:27:57.720
layer is our input layer. And what flatten does is allows us to take in a shape of 28 by 28,

03:27:58.360 --> 03:28:05.640
which we've defined here, and flatten all of the pixels into 784 pixels. So we take this 28 by 28

03:28:05.640 --> 03:28:11.480
kind of matrix like structure, and just flatten it out. And Keras will do that for us, we don't

03:28:11.480 --> 03:28:17.080
actually need to take our you know, matrix data and transform before passing. So we've done that.

03:28:17.800 --> 03:28:26.280
Next, we have Keras dot layers dot dense 128 activation equals rectify linear unit. So this

03:28:26.360 --> 03:28:32.760
is our first hidden layer, layer two, right, that's what I've denoted here. And this is a dense layer.

03:28:32.760 --> 03:28:38.680
Now dense, again, means that all of the, what is it, the neurons in the previous layer are connected

03:28:38.680 --> 03:28:44.680
to every neuron in this layer. So we have 828 neurons here. How do we pick that number? We

03:28:44.680 --> 03:28:48.440
don't know, we kind of just came up with it. Usually, it's a good idea that you're going to do

03:28:48.440 --> 03:28:53.160
this as like a little bit smaller than what your input layer is, although sometimes it's going to

03:28:53.160 --> 03:28:56.760
be bigger, you know, sometimes it's going to be half the size, it really depends on the problem,

03:28:56.760 --> 03:29:00.840
I can't really give you a straight answer for that. And then our activation function will

03:29:00.840 --> 03:29:05.880
define as rectify linear unit. Now we could pick a bunch of different activation functions, there's

03:29:05.880 --> 03:29:11.400
time we can pick sigmoid, we could pick 10 h, which is hyperbolic tangent, doesn't really matter.

03:29:11.960 --> 03:29:16.200
And then we're going to define our last layer, which is our output layer, which is a dense layer

03:29:16.200 --> 03:29:22.200
of 10 output neurons with the activation of softmax. Okay, so can we think of why we would have picked

03:29:22.280 --> 03:29:26.440
10 here, right? I'll give you guys a second to think about it, based on the fact that our output

03:29:26.440 --> 03:29:31.480
layer, you know, is supposed to have as many neurons as classes we're going to predict for.

03:29:31.480 --> 03:29:37.560
So that is exactly what we have 10. If we look, we have 10 classes here. So we're going to have 10

03:29:37.560 --> 03:29:43.480
output neurons in our output layer. And again, we're going to have this probability distribution.

03:29:43.480 --> 03:29:49.160
And the way we do that is using the activation function softmax. So softmax will make sure

03:29:49.240 --> 03:29:54.040
that all of the values of our neurons add up to one, and that they're between zero and one.

03:29:54.760 --> 03:29:59.000
So that is our, our model, we've created the model now. So let's actually run this.

03:30:00.840 --> 03:30:05.560
See, are we going to get any errors here? Is this going to run? And then we'll run the model,

03:30:05.560 --> 03:30:09.320
and then we'll go on to the next step, which actually going to be training and testing the

03:30:09.320 --> 03:30:14.040
model. Okay, so let's create the model now, shouldn't get any issues, and we're good. And now let's

03:30:14.040 --> 03:30:19.080
move on to the next step. I'm forgetting what it is, though, which is training the model. Oh,

03:30:19.080 --> 03:30:24.440
sorry, compiling the model. Okay, so compiling the model. So we've built now what we call the

03:30:24.440 --> 03:30:28.920
architecture of our neural network, right, we've defined the amount of neurons in each layer,

03:30:28.920 --> 03:30:33.880
we've defined the activation function, and we define the type of layer and the type of connections.

03:30:33.880 --> 03:30:38.840
The next thing we need to pick is the optimizer, the loss and the metrics we're going to be looking

03:30:38.840 --> 03:30:43.880
at. So the optimizer we're going to use is Adam. This is again, just the algorithm that performs

03:30:43.880 --> 03:30:48.520
the gradient descent, you don't really need to look at these too much, you can read up on some

03:30:48.520 --> 03:30:53.640
different activation functions or sorry, optimizers, if you want to kind of see the difference between

03:30:53.640 --> 03:30:59.560
them, but that's not crazy, we're going to pick a loss. So in this case, sparse categorical cross

03:30:59.560 --> 03:31:03.240
entropy, again, not going to go into depth about that, you guys can look that up if you want to

03:31:03.240 --> 03:31:07.800
see how it works. And then metrics. So what we're looking for the output that we want to see from

03:31:07.800 --> 03:31:13.800
the network, which is accuracy. Now from, you know, kind of right now with our current knowledge,

03:31:13.800 --> 03:31:18.360
we're just going to stick with this as what we're going to compile our neural networks with,

03:31:18.360 --> 03:31:23.720
we can pick different values if we want. And these are what we call, what is it hyper parameter

03:31:23.720 --> 03:31:28.680
tuning. So the parameters that are inside here, so like the weights and the biases are things that

03:31:28.680 --> 03:31:34.120
we can't manually change. But these are things that we can change, right, the optimizer, the loss,

03:31:34.120 --> 03:31:38.840
the metrics, the activation function, we can change that. So these are called hyper parameters,

03:31:38.840 --> 03:31:44.520
same thing with the number of neurons in each layer. So hyper parameter tuning is a process of

03:31:44.520 --> 03:31:49.960
changing all of these values and looking at how models perform with different hyper parameters

03:31:49.960 --> 03:31:53.720
change. So I'm not really going to talk about that too much. But that is something to note,

03:31:53.720 --> 03:31:58.520
because you'll probably hear that, you know, this hyper parameter kind of idea. Okay, so we've

03:31:58.520 --> 03:32:02.280
compiled the model now using this, which just means we've picked all the different things that

03:32:02.280 --> 03:32:07.080
we need to use for it. And now on to training the model. So I'm just going to copy this in.

03:32:08.040 --> 03:32:13.640
Again, remember this, these parts are pretty syntaxually heavy, but fairly easy to actually do.

03:32:13.640 --> 03:32:18.120
So we're going to fit the model. So fit just means we're fitting it to the training data. It's

03:32:18.120 --> 03:32:21.880
another word for training, essentially. So we're going to pass it the training images,

03:32:21.880 --> 03:32:26.600
the training labels, and notice how much easier it is to pass this. Now, we don't need to do this

03:32:26.600 --> 03:32:30.280
input function, we don't need to do all of that, because Keras can handle it for us. And we define

03:32:30.280 --> 03:32:36.120
our epochs as 10 epochs is another hyper parameter that you could tune and change if you wanted to.

03:32:36.680 --> 03:32:41.240
All right, so that will actually fit our model. So what I'm going to do is put this in another

03:32:41.240 --> 03:32:46.360
code block. So I don't need to keep retraining this. So we'll go like that. And let's actually

03:32:46.360 --> 03:32:50.840
look at this training process. So we've run the model, this should compile. And now let's fit

03:32:50.840 --> 03:32:56.120
it and let's see what we actually end up getting. Alright, so epoch one, and we can see that we're

03:32:56.120 --> 03:33:00.600
getting a loss and we're getting accuracy printing out on the side here. Now, this is going to take

03:33:00.600 --> 03:33:05.400
a second, like this is going to take a few minutes, as opposed to our other models that we made are

03:33:05.400 --> 03:33:10.440
not a few minutes, but you know, a few seconds, when you have 60,000 images, and you have a network

03:33:10.440 --> 03:33:16.680
that's comprised of 784 neurons, 128 neurons, and then 10 neurons, you have a lot of weights and

03:33:16.680 --> 03:33:21.160
biases and a lot of math that needs to go on. So this will take a few seconds to run. Now,

03:33:21.160 --> 03:33:24.760
if you're on a much faster computer, you'll probably be faster than this. But this is why I

03:33:24.760 --> 03:33:28.600
like Google Collaboratory, because you know, this isn't using any of my computer's resources

03:33:28.600 --> 03:33:35.160
to train. It's using this. And we can see, like the RAM and the disk. How do I look at this?

03:33:36.120 --> 03:33:40.680
In this network? Oh, is it going to let me look at this now? Okay, I don't know why it's not letting

03:33:40.680 --> 03:33:45.400
me click this, but usually you can have a look at it. And now we've trained and we've fit the

03:33:45.400 --> 03:33:51.240
model. So we can see that we have an accuracy of 91%. But the thing is, this is the accuracy

03:33:51.960 --> 03:33:57.480
on or testing or our training data. So now if we want to find what the true accuracy is,

03:33:58.040 --> 03:34:02.040
what we need to do is actually test it on our testing data. So I'm going to steal

03:34:02.040 --> 03:34:06.520
this line of code here. This is how we test our model. Pretty straightforward. I'll just

03:34:06.520 --> 03:34:12.120
close this. So let's go into code block. So we have test loss test accuracy is model dot

03:34:12.120 --> 03:34:18.920
evaluate test images test labels verbose equals one. Now what is verbose? I was hoping it was

03:34:18.920 --> 03:34:23.160
going to give me the thing so I could just read it to you guys. But verbose essentially is just

03:34:23.160 --> 03:34:28.120
are we looking at output or not? So like how much information are we seeing as this model evaluates?

03:34:28.760 --> 03:34:33.080
It's like how much is printing out to the console? That's what that means. And yes,

03:34:33.080 --> 03:34:37.560
this will just split up kind of the metrics that are returned to this into test loss and test accuracy

03:34:37.560 --> 03:34:43.640
so we can have a look at it. Now you will notice when I run this, that the accuracy will likely

03:34:43.640 --> 03:34:48.840
be lower on this than it was on our model. So actually, the accuracy we had from this

03:34:48.840 --> 03:34:54.680
was about 91. And now we're only getting 88.5. So this is an example of something we call

03:34:54.680 --> 03:35:00.760
overfitting. Our model seemed like it was doing really well on the testing data or sorry,

03:35:00.760 --> 03:35:06.440
the training data. But that's because it was seeing that data so often, right with 10 epochs,

03:35:06.440 --> 03:35:12.280
it started to just kind of memorize that data and get good at seeing that data. Whereas now

03:35:12.280 --> 03:35:18.040
when we pass it new data that it's never seen before, it's only 88.5% accurate, which means

03:35:18.040 --> 03:35:23.160
we overfit our model. And it's not as good at generalizing for other data sets, which is usually

03:35:23.160 --> 03:35:27.560
the goal, right? When we create a model, we want the highest accuracy possible, but we want the

03:35:27.560 --> 03:35:34.440
highest accuracy possible on new data. So we need to make sure our model generalizes properly.

03:35:34.440 --> 03:35:38.760
Now in this instance, you know, like, it's, it's hard to figure out how do we do that because

03:35:38.760 --> 03:35:43.080
we don't know that much about neural networks. But this is the idea of overfitting and of

03:35:43.080 --> 03:35:47.640
hyper parameter tuning, right? So if we can start changing some of this architecture, and we can

03:35:47.640 --> 03:35:52.440
change maybe the optimizer, the loss function, maybe we go epochs eight, let's see if this

03:35:52.440 --> 03:35:57.800
does any better, right? So let's now fit the model with eight epochs, we'll have a look at what this

03:35:57.800 --> 03:36:02.840
accuracy is. And then we'll test it and see if we get a higher accuracy on the testing data set.

03:36:02.840 --> 03:36:07.240
And this is kind of the idea of that hyper parameter tuning, right? Well, we just look at

03:36:07.240 --> 03:36:12.680
each epoch, or not each epoch, we look at each parameter, we tweak them a little bit. And usually

03:36:12.680 --> 03:36:17.960
we'll like write some code that automates this for us. But that's the idea is we want to get the most

03:36:17.960 --> 03:36:23.240
generalized accuracy that we can. So I'll wait for this to train. We're actually almost done. So I

03:36:23.240 --> 03:36:28.040
won't even bother cutting the video. And then we'll run at this evaluation. And we'll see now if we

03:36:28.040 --> 03:36:31.640
got a better accuracy. Now I'm getting a little bit scared because the accuracy is getting very

03:36:31.640 --> 03:36:36.040
high here. And sometimes, you know, like you want the accuracy to be high on your training data.

03:36:36.120 --> 03:36:39.720
But when it gets to a point where it's very high, you're in a situation where it's likely

03:36:39.720 --> 03:36:45.240
that you've overfit. So let's look at this now. And let's see what we get. So 88.4. So we actually

03:36:45.240 --> 03:36:49.160
dropped down a little bit. And it seemed like those epochs didn't make a big difference. So maybe

03:36:49.160 --> 03:36:54.920
if I train it on one epoch, let's have an idea and see what this does. You know, make your prediction,

03:36:54.920 --> 03:36:58.440
you think we're going to be better, do you think we're going to be worse? It's only seen the training

03:36:58.440 --> 03:37:07.080
data one time. Let's run this. And let's see 89.34. So in this situation, less epochs was actually

03:37:07.080 --> 03:37:12.120
better. So that's something to consider. You know, a lot of people I see just go like 100 epochs and

03:37:12.120 --> 03:37:16.760
just think their model is going to be great. That's actually not good to do. A lot of the

03:37:16.760 --> 03:37:20.040
times you're going to have a worse model because what's going to end up happening is it's going to

03:37:20.040 --> 03:37:26.520
be seeing the same information so much tweaking so specifically to that information that it's seen

03:37:26.600 --> 03:37:30.920
that when you show it new data, it can't actually, you know, classify and generalize on that.

03:37:31.640 --> 03:37:35.560
All right. So let's go back and let's see what else we're doing now with this. Okay,

03:37:35.560 --> 03:37:39.320
so now that we've done that, we need to make predictions. So to make predictions is actually

03:37:39.320 --> 03:37:44.520
pretty easy. So I'm actually just going to copy this line in, we'll go into a new code block down

03:37:44.520 --> 03:37:49.880
here. So all you have to do is say model that predict, and then you're going to give it an array

03:37:49.880 --> 03:37:54.520
of images that you want to predict on. So in this case, if we look at test images shape, so actually

03:37:54.520 --> 03:38:01.480
let's make a new code block and let's go here. So let's say test underscore images dot shape.

03:38:03.640 --> 03:38:10.040
All right, give me a second. So we have 10,000 by 28 by 28. So this is an array of 10,000 entries

03:38:10.040 --> 03:38:15.720
of images. Now, if I just wanted to predict on one image, what I could do is say test images,

03:38:15.720 --> 03:38:21.480
zero and then put that inside of an array. The reason I need to do that is because the data

03:38:21.480 --> 03:38:26.840
that this model is used to see in is an array of images to make a prediction on, that's what this

03:38:26.840 --> 03:38:31.720
predict method needs. And it's much better at making predictions on many things at once than

03:38:31.720 --> 03:38:37.560
just one specific item. So if you are predicting one item only, you do need to put it in an array,

03:38:37.560 --> 03:38:41.320
because it's used to seeing that form. So we could do this. I'm, I mean, I'm just going to leave it.

03:38:41.320 --> 03:38:44.920
So we're just going to predict on every single one of the test images, because then we can have

03:38:44.920 --> 03:38:49.160
a look at a cool function I've kind of made. So let's actually do this predictions equals model

03:38:49.240 --> 03:38:55.640
dot predict test images. I mean, let's print predictions. And look at actually what it is.

03:38:56.280 --> 03:39:01.320
Where is my autocomplete? There it is. Okay. So let's have a look. Is this some object? Whoa,

03:39:01.320 --> 03:39:07.800
okay. So this is a raise of arrays that looks like we have some like really tiny numbers in them.

03:39:07.800 --> 03:39:13.800
So what this is, is essentially every single, you know, prediction or every single image has

03:39:13.800 --> 03:39:17.400
a list that represents the prediction for it, just like we've done with kind of the linear

03:39:17.400 --> 03:39:22.360
models and stuff like that. So if I want to see the prediction for test image zero, I would say

03:39:22.360 --> 03:39:27.720
prediction zero, right? Let's print this out. And this is the array that we're getting. These

03:39:27.720 --> 03:39:33.960
this is the probability distribution that was calculated on our output layer for, you know,

03:39:33.960 --> 03:39:39.080
these, what is it for that image? So if we want to figure out what class we actually think that

03:39:39.080 --> 03:39:44.840
this is predicting for, we can use a cool function from NumPy called arg max, which essentially is

03:39:44.920 --> 03:39:50.200
just going to take the index, this is going to return to us the index of the maximum value in

03:39:50.200 --> 03:39:55.000
this list. So let's say that it was I'm looking for the least negative, which I believe is this,

03:39:55.000 --> 03:40:00.760
so this should be nine, this should return to us nine, because this is the index of the highest

03:40:00.760 --> 03:40:06.280
value in this list. Unless I'm just wrong when I'm looking at the negatives here. So nine, that's

03:40:06.280 --> 03:40:12.200
what we got. Okay, so now if we want to see what the actual classes, while we have our class names

03:40:12.200 --> 03:40:16.840
up here, so we know class nine is actually ankle boot. So let's see if this is actually an ankle

03:40:16.840 --> 03:40:23.320
boot. So I'm just going to do class underscore names, I think that's what I called it, like this,

03:40:23.320 --> 03:40:28.520
so that should print out what it thinks it is. Yeah, class underscore names. But now let's actually

03:40:28.520 --> 03:40:33.560
show the image of this prediction. So to do that, I'm just going to steal some code from here because

03:40:34.360 --> 03:40:37.960
I don't remember all the syntax off the top of my head. So this

03:40:38.760 --> 03:40:43.640
is what it looks like. So let's steal this figure. Let's show this and let's see if it actually looks

03:40:43.640 --> 03:40:49.640
like an ankle boot. So to do that, we're going to say test underscore images zero, because obviously

03:40:49.640 --> 03:40:55.240
image zero corresponds to predict prediction zero. And that will show this and see what we get. Okay,

03:40:55.240 --> 03:41:00.600
so ankle boot, and we'll be looking at the image is actually an ankle boot. And we can do this for

03:41:00.600 --> 03:41:06.120
any of the images that we want, right? So if I do prediction one, prediction one, now let's have a

03:41:06.120 --> 03:41:10.760
look pull over kind of looks like a pull over to me. I mean, I don't know if it actually is,

03:41:10.760 --> 03:41:19.080
but that's what it looks like. You do to to have a look here. Okay, trouser. Yep, looks like trousers

03:41:19.080 --> 03:41:23.160
to me. And we can see that that is how we get the predictions from our model, we use model dot

03:41:23.160 --> 03:41:29.400
predict. Alright, so let's move down here now to the next thing that we did. Alright, so we've

03:41:29.400 --> 03:41:33.720
already done that. So verifying predictions. Okay, so this is actually a cool kind of script

03:41:33.720 --> 03:41:39.160
that I wrote, I'll zoom out a little bit so we can read it. What this does is let us use our model

03:41:39.160 --> 03:41:44.520
to actually make. And I've stolen some of this from TensorFlow to make predictions on any

03:41:44.520 --> 03:41:48.840
entry that we want. So what it's going to do is ask us to type in some number, we're going to type

03:41:48.840 --> 03:41:53.240
in that number, it's going to find that image in the test data set, it's going to make your

03:41:53.240 --> 03:41:58.600
prediction on that from the model, and then show us what it actually is versus what it was predicted

03:41:58.600 --> 03:42:03.560
being. Now, I just need to actually run. Actually, let's just steal this code and bring it in the

03:42:03.560 --> 03:42:06.840
other one, because I've already trained the model there. So we don't have to wait again. So let's

03:42:06.840 --> 03:42:15.240
go f 11, f 11, let's go to a new code block, and run that. So let's run this script. Have a look

03:42:15.240 --> 03:42:19.880
down here. So pick a number, we'll pick some number, let's go 45. And then what it's going to do is

03:42:19.880 --> 03:42:25.320
say expected sneaker, guess sneaker, and actually show us the image that's there. So we can see

03:42:25.320 --> 03:42:30.360
this is what you know, our pixel kind of data looks like. And this is what the expected was,

03:42:30.360 --> 03:42:34.120
and this is what the guess was from the neural network. Now we can do the same thing if we run

03:42:34.120 --> 03:42:41.000
it again, pick a number 34. Let's see here, expected bag, guess bag. So that's kind of showing you

03:42:41.000 --> 03:42:47.480
how we can actually use this model. So anyways, that has been it for this kind of module on neural

03:42:47.480 --> 03:42:52.120
networks. Now I did this in about an hour, I'm hoping I explained a good amount that you guys

03:42:52.120 --> 03:42:56.760
understand now how neural networks work. In the next module, we're going to move on to convolutional

03:42:56.760 --> 03:43:00.760
neural networks, which again should help, you know, kind of get your understanding of neural

03:43:00.760 --> 03:43:05.480
networks up as well as learn how we can do deep computer vision, object recognition and detection

03:43:05.480 --> 03:43:09.640
using convolutional neural networks. So that being said, let's get into the next module.

03:43:12.920 --> 03:43:17.720
Hello, everyone, and welcome to the next module in this TensorFlow course. So what we're going

03:43:17.720 --> 03:43:22.200
to be doing here is talking about deep computer vision, which is very exciting, very cool. This

03:43:22.200 --> 03:43:26.520
has been used for all kinds of things you ever seen the self driving cars, for example, Tesla,

03:43:26.520 --> 03:43:32.760
they actually use a TensorFlow deep learning model, obviously very complicated, more than I can

03:43:32.760 --> 03:43:37.240
really explain here to do a lot of their computer vision for self driving, we've used computer vision

03:43:37.240 --> 03:43:41.800
in the medicine field, computer vision is actually used in sports a lot for things like goal line

03:43:41.800 --> 03:43:46.600
technology and even detecting images and players on the field doing analysis, there's lots of cool

03:43:46.680 --> 03:43:50.440
things are doing with it nowadays. And for our purposes, what we're going to be doing is using

03:43:50.440 --> 03:43:55.960
this for to perform classification, although it can be used for object detection and recognition,

03:43:55.960 --> 03:44:00.360
as well as facial detection and recognition as well. So all kinds of applications, in my opinion,

03:44:00.360 --> 03:44:05.160
one of the cooler things in deep learning that we're doing right now. And let's go ahead and talk

03:44:05.160 --> 03:44:08.680
about what we're actually going to be focusing on here. So we're going to start by discussing what

03:44:08.680 --> 03:44:13.560
a convolutional neural network is, which is essentially the way that we do deep learning,

03:44:13.640 --> 03:44:17.080
we're going to learn about image data. So what's the difference between image data and other

03:44:17.080 --> 03:44:21.800
regular data? We're going to talk about convolutional layers and pooling layers and how stacks of those

03:44:21.800 --> 03:44:26.760
work together as what we call a convolutional base for our convolutional neural network. We're

03:44:26.760 --> 03:44:31.800
going to talk about CNN architectures and get into actually using pre trained models that have been

03:44:31.800 --> 03:44:36.040
developed by companies such as Google and TensorFlow themselves to perform classification

03:44:36.040 --> 03:44:40.840
tasks for us. So that is pretty much the breakdown of what we're about to learn. There's quite a

03:44:40.840 --> 03:44:44.520
bit in this module, it's probably the more difficult one or the most difficult one we've

03:44:44.520 --> 03:44:48.200
been doing so far. So if you do get lost at any point, or you don't understand some of it,

03:44:48.200 --> 03:44:52.360
don't feel bad, this stuff is very difficult. And I would obviously recommend reading through

03:44:52.360 --> 03:44:55.960
some of the descriptions I have here in this notebook, which again, you can find from the

03:44:55.960 --> 03:45:00.360
link in the description or looking up some things that maybe I don't go into enough enough depth

03:45:00.360 --> 03:45:05.560
about in your own time, as I can't really spend, you know, 10, 11 hours explaining a convolutional

03:45:05.640 --> 03:45:11.000
neural network. So let's now talk about image data, which is the first thing we need to understand.

03:45:11.000 --> 03:45:16.200
So in our previous examples, what we did with when we had a neural network is we had two dimensional

03:45:16.200 --> 03:45:20.360
data, right, we had a width and height when we were trying to classify some kind of images using

03:45:20.360 --> 03:45:24.920
a dense neural network. And well, that's what we use two dimensions. Well, with an image, we

03:45:24.920 --> 03:45:30.200
actually have three dimensions. And what makes up those dimensions? Well, we have a height, and we

03:45:30.200 --> 03:45:34.280
have a width, and then we have something called a color channels. Now, it's very important to

03:45:34.280 --> 03:45:38.440
understand this, because we're going to see this a lot as we get into convolutional networks, that

03:45:38.440 --> 03:45:44.200
the same image is really represented by three specific layers, right? We have the first layer,

03:45:44.200 --> 03:45:49.080
which tells us all of the red values of the pixels, the second layer, which tells us all the green

03:45:49.080 --> 03:45:53.480
values, and the third layer, which tells us all the blue values. So in this case, those are the

03:45:53.480 --> 03:45:58.040
covered channels. And we're going to be talking about channels in depth quite a bit in this series.

03:45:58.040 --> 03:46:03.080
So just understand that although you think of an image as a two dimensional kind of thing,

03:46:03.160 --> 03:46:07.800
and our computer, it's really represented by three dimensions where these channels are telling us

03:46:07.800 --> 03:46:13.240
the color of each pixel. Because remember, in red, green, blue, you have three values for each pixel,

03:46:13.240 --> 03:46:18.120
which means that you're going to need three layers to represent that pixel, right? So this is what

03:46:18.120 --> 03:46:23.160
we can kind of think of it as a stack of layers. And in this case, a stack of pixels, right, or

03:46:23.160 --> 03:46:27.400
stack of colors really telling us the value for each pixel. So if we were to draw this to the

03:46:27.400 --> 03:46:33.880
screen, we would get the blue, green and red values of each pixel, determine the color of it,

03:46:33.880 --> 03:46:39.000
and then draw the two dimensional image right based on the width and the height. Okay, so now

03:46:39.000 --> 03:46:42.520
we're going to talk about a convolutional neural network and the difference between that in a dense

03:46:42.520 --> 03:46:46.680
neural network. So in our previous examples, when we use the dense neural network to do some kind

03:46:46.680 --> 03:46:52.280
of image classification, like that fashion, and this data set, what it essentially did was look

03:46:52.280 --> 03:46:58.520
at the entire image at once and determined based on finding features in specific areas of the image,

03:46:58.520 --> 03:47:03.240
what that image was, right? Maybe it found an edge here, a line here, maybe it found a shape,

03:47:03.240 --> 03:47:07.560
maybe it found a horizontal diagonal line. The important thing to understand, though, is that

03:47:07.560 --> 03:47:12.200
when it found these patterns and learned the patterns that made up specific shapes, it learned

03:47:12.200 --> 03:47:17.400
them in specific areas. It knew that if we're in between, for example, looking at this cat image,

03:47:17.480 --> 03:47:22.120
we're going to classify this as a cat, if an eye exists on, you know, the left side of the screen

03:47:22.120 --> 03:47:27.480
where the eyes are here, then that's a cat. It doesn't necessarily know that if we flipped this

03:47:27.480 --> 03:47:32.680
cat, we did a horizontal flip of this cat, and the eyes were over here, that that is a pattern that

03:47:32.680 --> 03:47:37.720
makes up a cat. So the idea is that the dense network looks at things globally, it looks at the

03:47:37.720 --> 03:47:43.640
entire image and learns patterns in specific areas. That's why we need things to be centered, we need

03:47:43.640 --> 03:47:48.680
things to be very similar when we use a dense neural network to actually perform image classification,

03:47:49.320 --> 03:47:54.280
because it cannot learn local patterns, and apply those to different areas of the image.

03:47:54.280 --> 03:47:58.600
So for example, some patterns we might look for, when we're looking at an image like a cat here

03:47:58.600 --> 03:48:03.000
would be something like this, right, we would hope that maybe we could find a few ears, we could find

03:48:03.000 --> 03:48:10.200
the eyes, the nose, and you know, the paws here. And those features would tell us that this makes

03:48:10.280 --> 03:48:15.400
up a cat. Now with a dense neural network, it would find these features, it would learn them,

03:48:15.400 --> 03:48:20.120
learn these patterns, we only learn them in this specific area where they're boxed off,

03:48:20.120 --> 03:48:23.960
which means if I horizontally flip this image, right, and I go like that,

03:48:23.960 --> 03:48:27.960
then it's not going to know that that's a cat, because it learned that pattern in a specific

03:48:27.960 --> 03:48:33.080
area, it'll need to relearn that pattern in the other area. Now a convolutional neural network,

03:48:33.080 --> 03:48:38.120
on the other hand, learns local patterns. So rather than learning that the ear exists in,

03:48:38.120 --> 03:48:42.680
you know, this specific location, it just learns that this is what an ear looks like,

03:48:42.680 --> 03:48:47.480
and it can find that anywhere in the image. And we'll talk about how we do that as we get to the

03:48:47.480 --> 03:48:51.880
explanation. But the whole point is that our convolutional neural network will scan through

03:48:51.880 --> 03:48:57.720
our entire image, it will pick up features and find features in the image. And then based on the

03:48:57.720 --> 03:49:02.920
features that exist in that image will pass that actually to a dense neural network or a dense

03:49:02.920 --> 03:49:07.720
classifier, it will look at the presence of these features and determine, you know, the combination

03:49:07.800 --> 03:49:12.280
of these presences of features that make up specific classes or make up specific objects.

03:49:12.280 --> 03:49:16.520
So that's kind of the point. I hope that makes sense. The main thing to remember is that dense

03:49:16.520 --> 03:49:21.400
neural networks work on a global scale, meaning they learn global patterns, which are specific and

03:49:21.400 --> 03:49:27.320
are found in specific areas. Whereas convolutional neural networks or convolutional layers will

03:49:27.320 --> 03:49:31.800
find patterns that exist anywhere in the image, because they know what the pattern looks like,

03:49:31.800 --> 03:49:37.560
not that it just exists in a specific area. Alright, so how they work, right? So let's see

03:49:37.560 --> 03:49:41.720
when a neural network, regular neural network looks at this dog image, this is a good example,

03:49:41.720 --> 03:49:47.160
I should have been using this before, it will find that there's two eyes that exist here, right?

03:49:47.160 --> 03:49:51.800
And we'll say, okay, so I found that these eyes make up a dog. This is its training image, for

03:49:51.800 --> 03:49:55.880
example, and it's like, okay, so this pattern makes up the dog, the IR is in this location.

03:49:56.680 --> 03:50:01.880
Now, what happens when we do this? And we flip the image to the other side. Well, our neural

03:50:01.880 --> 03:50:05.640
network starts looking for these eyes, right on the left side of the image where it found them

03:50:05.640 --> 03:50:10.040
previously and where it was trained on. It obviously doesn't find them there. And so it says

03:50:10.040 --> 03:50:14.280
that our image isn't a dog, although it clearly is a dog, it's just a dog that's orientated

03:50:14.280 --> 03:50:18.200
differently. In fact, it's just flipped horizontally, right? Or actually, I guess I would say

03:50:18.200 --> 03:50:23.640
vertically, flip vertically. So since it doesn't find the eyes in this location, and it can only

03:50:23.640 --> 03:50:28.200
look at patterns that it's learned in specific locations, it knows that this, or it's going

03:50:28.200 --> 03:50:33.080
to say this isn't a dog, even though it is. Whereas our convolutional layer will find the eyes

03:50:33.080 --> 03:50:37.720
regardless of where they are in the image, and still tell us that this is a dog, because even

03:50:37.720 --> 03:50:41.480
though the dogs moved over, it knows what an eye looks like, so it can find the eye anywhere in

03:50:41.480 --> 03:50:46.760
the image. So that's kind of the point of the convolutional neural network and the convolutional

03:50:46.760 --> 03:50:52.040
layer. And what the convolutional layer does is look at our image and essentially feedback to us,

03:50:52.040 --> 03:50:57.080
what we call an output feature map that tells us about the presence of specific features,

03:50:57.080 --> 03:51:02.040
or what we're going to call filters in our image. So that is kind of the way that works.

03:51:02.760 --> 03:51:07.160
Now, essentially, the thing we have to remember is that our dense neural networks output just a

03:51:07.160 --> 03:51:11.640
bunch of numeric values. Whereas what our convolutional layers are actually going to be

03:51:11.640 --> 03:51:16.040
doing is outputting what we call a feature map. Now I'm going to scroll down here to show you

03:51:16.040 --> 03:51:20.840
this example. What we're actually going to do is run what we call a filter over our image,

03:51:20.840 --> 03:51:24.280
we're going to sample the image at all these different areas. And then we're going to create

03:51:24.280 --> 03:51:29.960
what we call an output feature map that quantifies the presence of the filters pattern at different

03:51:29.960 --> 03:51:35.880
locations. And we'll run many, many, many different filters over our image at a time.

03:51:35.880 --> 03:51:39.800
So we have all these different feature maps telling us about the presence of all these

03:51:39.800 --> 03:51:44.920
different features. So one convolutional layer, we'll start by doing that with very small,

03:51:44.920 --> 03:51:50.200
simple filters such as straight lines like this. And then other convolutional layers on top of

03:51:50.200 --> 03:51:54.840
that, right, because it's going to return a map that looks something like this out of the layer,

03:51:54.840 --> 03:51:59.480
we'll take this map in now, the one that was created from the previous layer, and say, okay,

03:51:59.480 --> 03:52:03.320
what this map is representing to me, for example, the presence of these diagonal lines,

03:52:03.880 --> 03:52:08.280
let me try to look for curves, right, or let me try to look for edges. So it will look at the

03:52:08.280 --> 03:52:12.520
presence of the features from the previous convolutional layer, and then say, okay, well,

03:52:12.520 --> 03:52:17.080
if I have all these lines combined together, that makes up an edge, and it will look for that,

03:52:17.080 --> 03:52:21.320
right? And that's kind of the way that a convolutional neural network works and why we

03:52:21.320 --> 03:52:25.400
stack these different layers. Now, we also use something called pooling, and there's a few

03:52:25.400 --> 03:52:29.560
other things that we're going to get into. But that is the basics, I'm going to go into a drawing

03:52:29.560 --> 03:52:34.200
example and show you exactly how that works. But hopefully this makes a little bit of sense

03:52:34.200 --> 03:52:39.240
that the convolutional layer returns a feature map that quantifies the presence of a filter

03:52:39.240 --> 03:52:44.200
at a specific location. And this filter, the advantage of it is that we slide it across

03:52:44.200 --> 03:52:49.240
the entire image. So if this filter or this feature is presence anywhere in the image,

03:52:49.240 --> 03:52:52.760
we will know about it rather than in our dense network, where it had to learn that pattern

03:52:52.760 --> 03:52:58.120
in a specific global location. Okay, so let's get on the drawing tablet and do a few examples.

03:52:58.120 --> 03:53:01.480
All right, so I'm here on my drawing tablet, and we're going to explain exactly how a

03:53:01.480 --> 03:53:07.160
convolutional layer works, and how the network kind of works together. So this is an image I've

03:53:07.160 --> 03:53:11.880
drawn on the left side of our screen here. I know this is very basic, you know, this is just an X,

03:53:11.880 --> 03:53:15.080
right? This is what our images, we're just going to assume this is grayscale, we're going to avoid

03:53:15.080 --> 03:53:20.120
doing anything with color channels the second just because they're not that important. But

03:53:20.120 --> 03:53:24.040
just understand that what I'm going to show you does apply to color channels as well and to

03:53:24.040 --> 03:53:29.080
multiple kind of layers and depth. And then if we can understand it on a simple level,

03:53:29.080 --> 03:53:34.840
we should be able to understand it more thoroughly. So what we want essentially is our convolutional

03:53:34.840 --> 03:53:40.120
layer to give us some output. It's meaningful about this image. So we're going to assume this is

03:53:40.120 --> 03:53:45.400
the first convolutional layer. And what it needs to do essentially is return to us some feature

03:53:45.400 --> 03:53:51.560
map that tells us about the presence of specific what we call filters in this image. So each

03:53:51.560 --> 03:53:57.320
convolutional layer has a few properties to it. The first one is going to be the input size.

03:53:58.360 --> 03:54:05.400
So what can we expect? Wow, what is that that was as the as the input size, how many filters

03:54:05.400 --> 03:54:10.760
are we going to have so filters like this? And what's the sample size of our filters?

03:54:11.720 --> 03:54:17.400
That's what we need to know for each of our convolutional neural networks. So essentially,

03:54:17.400 --> 03:54:21.080
what is a filter? Well, a filter is just some pattern of pixels. And we saw them before,

03:54:21.080 --> 03:54:26.440
we'll do a pretty basic one here, as the filter we're going to look for, which will look something

03:54:26.440 --> 03:54:32.280
like this. This will be the first filter we're going to look for just to illustrate how this works.

03:54:32.280 --> 03:54:37.480
But the idea is that at each convolutional layer, we look for many different filters. And in fact,

03:54:37.480 --> 03:54:44.360
the number we're typically looking for is actually about times 32 filters. Sometimes we have 64

03:54:44.360 --> 03:54:50.120
filters as well. And sometimes even 128. So we can do as many filters as we want, as few filters

03:54:50.120 --> 03:54:55.800
as we want. But the filters are what is going to be trained. So this filter is actually what is

03:54:55.800 --> 03:55:01.320
going to be found by the neural network. It's what's going to change. It's, you know, this is

03:55:01.320 --> 03:55:05.000
essentially what we're looking for. This is what's created in the program. And that's kind of like

03:55:05.000 --> 03:55:11.080
the trainable parameter of a convolutional neural network is the filter. So the amount of filters

03:55:11.080 --> 03:55:16.680
and what they are will change as the program goes on, as we're learning more and figuring out what

03:55:16.680 --> 03:55:21.480
features that make up, you know, a specific image. So I'm going to get rid of this stuff right now,

03:55:21.480 --> 03:55:26.520
just so we can draw and do a basic example. But I want to show you how we look for a filter in the

03:55:26.520 --> 03:55:31.480
image. So we have filters, right, they'll come up with them, they're gonna start completely random,

03:55:31.480 --> 03:55:35.240
but they'll change as we go on. So let's say the filter we're looking for is that one I drew

03:55:35.240 --> 03:55:38.360
before, I'm just going to redraw it at the top here a little bit smaller. And we'll just say

03:55:38.360 --> 03:55:43.800
it's a diagonal line, right? But another filter we could look for might be something like, you know,

03:55:43.800 --> 03:55:48.840
a straight line, just like that all across, we could have a horizontal line. And in fact, we'll

03:55:48.840 --> 03:55:53.400
have 32 of them. And when we're doing just, you know, three by three grids of filters, well,

03:55:54.040 --> 03:55:57.640
there's not that many, you know, combinations, we're going to do at least grayscale wise.

03:55:58.200 --> 03:56:02.680
So what we'll do is we'll define the sample size, which is how big our filter is going to be three

03:56:02.680 --> 03:56:06.600
by three, which we know right now, which means that what we're going to do is we're going to look

03:56:06.600 --> 03:56:14.040
at three by three spots in our image, and look at the pixels, and try to find how closely these

03:56:14.040 --> 03:56:19.000
filters match with the pixels we're looking at on each sample. So what this is going to do,

03:56:19.000 --> 03:56:23.880
this convolutional layer is going to output us what we call a feature map, which can be a little

03:56:23.880 --> 03:56:28.840
bit smaller than the original image. And you'll see why in a second, but that tells us about the

03:56:28.840 --> 03:56:34.120
presence of specific features in areas of the image. So since we're looking for two filters here,

03:56:34.120 --> 03:56:37.800
actually, we'll do two filters, which means that we're actually going to have a depth to

03:56:38.920 --> 03:56:42.920
feature map being returned to us, right? Because for two filters, that means we need two maps,

03:56:43.560 --> 03:56:48.760
quantifying the presence of both of those filters. So for this green box that we're looking on at

03:56:48.760 --> 03:56:53.880
the left side here, we'll look for this first filter here. And what do we get? Well, the way we

03:56:53.880 --> 03:56:58.680
actually do this, the way we look at this filter, is we take the cross product, or actually not the

03:56:58.680 --> 03:57:03.480
cross product, the dot product, sorry, between this little green box and this filter, right,

03:57:03.480 --> 03:57:08.040
because they're both pixels, they're both actually numeric values down at the bottom. So what we do

03:57:08.040 --> 03:57:13.880
is we take that dot product, which essentially means we're element wise, adding, or what is it

03:57:13.880 --> 03:57:18.920
element wise, multiplying all of these pixels by each other. So if this pixel values is zero,

03:57:18.920 --> 03:57:22.840
right, because it's white, or it could be the other way around, we could say white is one,

03:57:22.840 --> 03:57:27.720
black is zero, it doesn't really matter, right? If this is a zero, and this is a one, these are

03:57:27.720 --> 03:57:32.680
obviously very different. And when we do the dot product of those two, so we multiply them together,

03:57:32.680 --> 03:57:36.920
then in our output feature, we would have a zero, right, that's kind of the way it works. So we do

03:57:36.920 --> 03:57:42.040
this dot product of this entire thing. If you don't know the dot product is I'm not really going to

03:57:42.040 --> 03:57:45.560
go into that. But we do the dot product, and that gives us some value essentially telling us how

03:57:45.560 --> 03:57:50.680
similar these two blocks are. So how similar this sample is that we're taking of the image and the

03:57:50.680 --> 03:57:55.160
filter that we're looking for. They're very similar, we're going to likely put a one or something telling

03:57:55.160 --> 03:57:59.240
us, you know, they're very close together. They're not similar at all, we're going to put a zero. So

03:57:59.240 --> 03:58:03.400
in this case, for our first filter, we're probably going to have a value because this middle pixel

03:58:03.400 --> 03:58:08.600
is the same as something like 0.12, right? But the all the other values are different. So it's

03:58:08.600 --> 03:58:14.440
not going to be very similar whatsoever. So then what we're going to do now is we'll look at the

03:58:14.440 --> 03:58:19.400
actually second filter, which is this horizontal line. And in fact, we're going to get a very

03:58:19.400 --> 03:58:23.960
similar output response here, probably something like, you know, 0.12, that's going to go in the

03:58:23.960 --> 03:58:29.720
top left. And again, these are both maps representing each filter, right? So now we'll move our green

03:58:29.720 --> 03:58:37.560
box over one, like this. So just shift that over one. And now we'll start looking at the next section.

03:58:37.560 --> 03:58:41.720
And in fact, I'm going to see if I can erase this just to make it a little bit cleaner here.

03:58:44.120 --> 03:58:48.360
Get rid of the green, there we go. Okay, so we'll move this box over like this. And now we'll

03:58:48.360 --> 03:58:52.520
start looking at this one, we'll do the exact same thing we did again before. So we're going to say,

03:58:52.520 --> 03:58:56.520
alright, how similar are these? Well, they're not similar at all. So we're going to get a zero for

03:58:56.520 --> 03:59:01.960
that first filter. How similar the other ones? Oh, actually, they're like a little bit similar.

03:59:01.960 --> 03:59:05.480
There's a lot of white that's kind of in the same space, like, you know, stuff like that. So we'll

03:59:05.480 --> 03:59:10.760
say maybe this is like 0.7, right? I'm just randomly picking these numbers. They are going to be much

03:59:10.760 --> 03:59:15.000
different than what I'm putting in here. But I'm just trying to get you to understand what's kind

03:59:15.000 --> 03:59:18.520
of happening, right? And this is completely random, the way I'm making the numbers to just make sure

03:59:18.520 --> 03:59:21.880
you understand that because this is not exactly what it would look like. Okay, so then we're going

03:59:21.880 --> 03:59:25.160
to move the box over one more time. Let's just erase this to keep this clean. This will be the

03:59:25.160 --> 03:59:29.800
last time we do this for the purpose of this example. And now what we're going to have is wow,

03:59:29.800 --> 03:59:34.040
we have a perfect match for the first filter. So we put one, the other ones like ads kind of

03:59:34.120 --> 03:59:38.040
similar as a few things that are different. So maybe this gets like a 0.4 or something, right?

03:59:38.040 --> 03:59:41.160
Whatever they are, we end up getting some value. So we'll fill in all these values, let's just

03:59:41.160 --> 03:59:46.280
put some arbitrary values here for now, just so we can do something with the example 0.7, 0,

03:59:47.000 --> 04:00:00.040
0.12, 0.42, 0.3, 0.9, 0.1, again, completely random, 0.4, 0.6. Alright, so this is now what

04:00:00.040 --> 04:00:06.120
we've gotten our response map from looking at two filters on our original image of five by five.

04:00:06.120 --> 04:00:10.280
Now notice that the size of these is three by three. And obviously, the reason for that is

04:00:10.280 --> 04:00:15.160
because in a five by five image, when we're taking three by three samples, well, we can only take

04:00:15.160 --> 04:00:20.040
nine three by three samples, because when we go down a row, right, we're going to move down one,

04:00:20.040 --> 04:00:23.240
and we're going to do the same thing we did before of this, these three by three samples. And if we

04:00:23.240 --> 04:00:28.280
add the amount of times we can do that, well, we just get three by three, which is not. So this

04:00:28.280 --> 04:00:33.560
now is kind of telling us of the presence of features in this original image map. Now the

04:00:33.560 --> 04:00:39.400
thing is, though, we're going to do this 64 times, right, for 64 filters or 32 filters or the amount

04:00:39.400 --> 04:00:43.960
of filters that we have. So we're going to have a lot of layers, like a ton of different layers,

04:00:43.960 --> 04:00:48.200
which means that we're going to be constantly expanding as we go through the convolutional

04:00:48.200 --> 04:00:54.360
layers, the depth of this, this kind of output feature map. And that means that there's a lot

04:00:54.360 --> 04:00:57.560
of computations that need to be done. And essentially, that means that this can be very

04:00:57.560 --> 04:01:02.600
slow. So now we need to talk about an operation called pooling. So I'll backtrack a little bit,

04:01:02.600 --> 04:01:06.600
but we will talk about pooling in a second. What's going to happen, right, is when we have all these

04:01:06.600 --> 04:01:12.040
layers that are generated, so this is called the output feature map, right, from this original

04:01:12.040 --> 04:01:16.760
image. What we're going to do is the next convolutional layer in the network is now going

04:01:16.760 --> 04:01:22.040
to do the process we just talked about, except on this output feature map, which means that

04:01:22.120 --> 04:01:27.960
since this one was picking up things like lines and edges, right, the next convolutional layer,

04:01:27.960 --> 04:01:34.120
we'll pick up combinations of lines and edges and maybe find what a curve is, right, we'll slowly

04:01:34.120 --> 04:01:39.640
work our way up from very, very small amount of pixels, defining more and more, almost I want to

04:01:39.640 --> 04:01:45.720
say abstract, different features that exist in the image. And this is what really allows us to

04:01:45.720 --> 04:01:49.160
do some amazing things with a convolutional neural network, when we have a ton of different

04:01:49.160 --> 04:01:53.480
layers stacking up on each other, we can pick out all the small little edges, which are pretty

04:01:53.480 --> 04:01:59.160
easy to find. And with all these combinations of layers working together, we can even find things

04:01:59.160 --> 04:02:07.160
like, say, eyes, right, or feet, or heads or face, right, we can find very complicated structures,

04:02:07.160 --> 04:02:11.880
because we slowly work our way up starting by solving very easy problem, which are like finding

04:02:11.880 --> 04:02:16.840
lines, and then finding combinations of lines, combination of edges, shapes and very abstract

04:02:16.920 --> 04:02:22.040
things. That's how this convolutional network works. So we've done that now, it's now time to

04:02:22.040 --> 04:02:26.840
talk about pooling. And we'll also talk about pat actually, we'll go padding first before we go

04:02:26.840 --> 04:02:30.840
pooling, I just, it doesn't really matter what order we talk about this in. But I just think

04:02:30.840 --> 04:02:35.960
padding makes sense based on the way we're going right now. So sometimes we want to make sure that

04:02:35.960 --> 04:02:43.720
the output feature map from our original image here is the same dimensions or same size as this,

04:02:43.720 --> 04:02:48.680
right? So this was five by five, obviously. And this is three by three. So if we want this to be

04:02:48.680 --> 04:02:53.320
five by five, as an output, what we need to do is add something called padding to our original

04:02:53.320 --> 04:02:58.120
image. So padding is essentially just adding an extra row and column on each side of our image

04:02:58.120 --> 04:03:03.320
here. So that when we, and we just fill in all these pixels in like kind of the padded pixels

04:03:03.320 --> 04:03:08.440
here, I just blank random pixels, they don't mean anything. Essentially, why we do that is so that

04:03:08.520 --> 04:03:15.080
when we do our three by three sample size here like this, we can take a three by three sample

04:03:15.080 --> 04:03:20.840
where every single pixel is in the center of that sample. Because right now, this pixel is not in

04:03:20.840 --> 04:03:25.880
the center. This pixel can never be in the center, this pixel can never be in the center, only,

04:03:25.880 --> 04:03:30.840
you know, a few pixels get to be in the center. And what this allows us to do is generate an output

04:03:30.840 --> 04:03:35.880
map that is the same size as our original input, and allows us to look at features that are maybe

04:03:35.880 --> 04:03:40.680
right on the edges of images that we might not have been able to see before. Now, this isn't

04:03:40.680 --> 04:03:44.280
super important when you go to like very large images, but it just something to consider you

04:03:44.280 --> 04:03:49.240
can add padding, we may do this as we get through our examples. And there's also something called

04:03:49.240 --> 04:03:54.360
stride, which I want to talk about as well. So what a stride is is essentially how much we move

04:03:54.360 --> 04:04:00.360
this sample box, every time that we're about to move it, right? So before, like, so let's say

04:04:00.360 --> 04:04:04.440
we're doing example with padding here, right, we are first sample, we would take here. And again,

04:04:04.440 --> 04:04:09.720
these pixels are just added, we added them in to make this work better for us. You would assume

04:04:09.720 --> 04:04:13.640
that the next time we move the box, we're going to move it one pixel over, that's called a stride of

04:04:13.640 --> 04:04:18.760
one, we can do that. But we also can employ stride of two, which means we'll move over by two.

04:04:19.400 --> 04:04:23.560
Obviously, the larger your stride, the smaller your output feature map is going to be. So you

04:04:23.560 --> 04:04:27.000
might want to add more padding. Well, you don't want to add too much padding, but it's just

04:04:27.000 --> 04:04:32.360
something to consider. And we will use a stride in different instances. Okay, so that's great.

04:04:32.360 --> 04:04:36.200
That hopefully makes sense. Let's erase this. Now we don't need this anymore. We talked about

04:04:36.200 --> 04:04:39.800
padding, we talked about the stride. And now we're going to talk about a pooling operation,

04:04:39.800 --> 04:04:45.560
which is very important. So kind of the idea is that we're going to have a ton of layers, right,

04:04:45.560 --> 04:04:49.320
for all these filters. And we're just going to have a lot of numbers, a lot of computations. And

04:04:49.320 --> 04:04:52.680
there must be some way to make these a little bit simpler, a little bit easier to use. Well,

04:04:52.680 --> 04:04:57.240
yes, that's true. And there is a way to do that. And that's called pooling. So there's three different

04:04:57.240 --> 04:05:04.040
types of pooling. Well, it is more but the basic ones are min, max, and average. And essentially,

04:05:04.040 --> 04:05:11.160
a pooling operation is just taking specific values from a sample of the output feature map. So

04:05:11.160 --> 04:05:16.280
once we generate this output feature map, what we do to reduce its dimensionality and just make it

04:05:16.280 --> 04:05:23.240
a little bit easier to work with is what we sample typically two by two areas of this output feature

04:05:23.240 --> 04:05:29.640
map. And just take either the min, max or average value of all the values inside of here and map

04:05:29.640 --> 04:05:35.480
these, we're going to go back this way to a new feature map that's twice, like one times the size

04:05:35.480 --> 04:05:39.240
essentially, or not, what am I saying, two times smaller than this original map. And it's kind of

04:05:39.240 --> 04:05:42.840
hard with three, like three by three, to really show you this. But essentially, what's going to

04:05:42.840 --> 04:05:47.640
end up happening is we're going to have something like this. So we're going to take the sample here,

04:05:47.640 --> 04:05:51.320
we're going to say, okay, what are we doing min, max or average pooling, if we're doing min pooling,

04:05:51.400 --> 04:05:54.920
we're going to take the smallest value, which means we'll take zero. If we're doing max pooling,

04:05:54.920 --> 04:05:59.320
we'll take the maximum value, which means we'll take 0.3. If we're doing average, we're probably

04:05:59.320 --> 04:06:04.440
going to get an average value of close to what 0.2, maybe. So let's say 0.2, we'll go there.

04:06:05.000 --> 04:06:10.200
That's how we do that with pooling. Again, just to make this feature map smaller. So we'll do that

04:06:10.200 --> 04:06:16.120
for both of the filters. But let's just say this is 0.2. Let's say this here that I'm blocking off

04:06:16.120 --> 04:06:23.160
is, I don't know, what is this going to be 0.6? It's hard to do this average with four numbers,

04:06:23.160 --> 04:06:31.240
let's say this one down here is going to be 0. I don't know, let's just do two one or something.

04:06:31.240 --> 04:06:35.240
And then this last one here, we okay, we got some bigger values, maybe this will be like 0.4.

04:06:36.360 --> 04:06:41.000
Okay, so that's one, the one down here will have some values of its own, we'll just do squiggles

04:06:41.000 --> 04:06:45.800
to represent that it has something. And we've effectively done a max pooling operation on

04:06:45.800 --> 04:06:51.160
this, we've reduced the size of it by about half. And that is kind of how that works. Now,

04:06:51.160 --> 04:06:57.160
typically what we do is we use a two by two pooling or like sample size like that, with a

04:06:57.160 --> 04:07:02.200
stride of two, which actually means that we would straw it like this. But since we're not going to

04:07:02.200 --> 04:07:07.320
do padding on this layer right now, we'll just do a stride of one. And this is how we pull it.

04:07:07.320 --> 04:07:10.600
Now the different kinds of pooling are used for different kind of things. The reason we would

04:07:10.600 --> 04:07:17.400
use a max pooling operation is to pretty much tell us about the maximum presence of a feature in that

04:07:17.400 --> 04:07:23.240
kind of local area. We really only care if the feature exists, where if it doesn't exist, an

04:07:23.240 --> 04:07:27.640
average pooling is not very often used, although in this case, we did use an average pooling.

04:07:28.360 --> 04:07:31.800
But you know, it's just different kinds of pooling and average tells you about the average

04:07:31.800 --> 04:07:36.920
presence of the feature in that area. Max tells you about is that feature present in that area

04:07:36.920 --> 04:07:40.920
at all and men tells you does it not exist. If it doesn't exist, right, we're just going to have a

04:07:40.920 --> 04:07:46.040
zero if there's even one zero in that area. So that's the point of pooling. That's the point of

04:07:46.040 --> 04:07:49.480
convolutional layers. I think I'm done with the white boarding for now, we're actually going to

04:07:49.480 --> 04:07:53.960
start getting into a little bit of code and talking about creating our own convolutional networks,

04:07:53.960 --> 04:07:58.040
which hopefully will make this a lot more clear. So let's go ahead and get into that. All right,

04:07:58.040 --> 04:08:01.960
so now it is time to create our first convolutional neural network. Now we're going to be using

04:08:01.960 --> 04:08:07.480
Keras to do this. And we're also going to be using the CI FAR image data set that contains 60,000

04:08:07.480 --> 04:08:12.760
images of 10 different classes of everyday objects. Now these images are 32 by 32, which

04:08:12.760 --> 04:08:18.760
essentially means they are blurs, and they are colorful. Now, I just want to emphasize as we

04:08:18.760 --> 04:08:23.080
get into this, that the reason I'm not typing all of these lines out and I just have them in here

04:08:23.080 --> 04:08:27.400
already is because this is likely what you guys will be using or doing when you actually make

04:08:27.400 --> 04:08:32.280
your own models. Chances are that you are not going to sit unless you're a pro at TensorFlow,

04:08:32.280 --> 04:08:37.000
and I am not even there yet either with my knowledge of it, and have all of the lines memorized and

04:08:37.000 --> 04:08:41.720
not have to go reference the syntax. So the point is here, so long as you can understand why this

04:08:41.720 --> 04:08:46.200
works and what these lines are doing, you're going to be fine, you don't need to memorize them and I

04:08:46.200 --> 04:08:50.360
have not memorized them. And I don't I look up for the documentation, I copy and paste what I need,

04:08:50.360 --> 04:08:54.120
I alter them, I write a little bit of my own code. But that's kind of what you're going to end up

04:08:54.120 --> 04:08:59.400
doing. So that's what I'm doing here. So this is the image data set. We have truck, horse, ship,

04:08:59.400 --> 04:09:03.720
airplane, you know, just some everyday, regular objects, there is 60,000 images, as we said,

04:09:03.720 --> 04:09:09.880
and 6000 images of each class. So we don't have too many images of just one specific class. So

04:09:09.880 --> 04:09:15.160
we'll start by importing our modules. So TensorFlow, we're going to import TensorFlow dot Keras,

04:09:15.160 --> 04:09:20.040
we're going to use the data set built into Keras for this. So that's the CI FR image data set,

04:09:20.040 --> 04:09:23.720
which you can actually look at just by clicking at this, it'll bring you and give the information

04:09:23.720 --> 04:09:28.200
about the data set. Although we don't need that right now, because I already know the information

04:09:28.200 --> 04:09:32.920
about it. And now we're just going to load our images in. So again, this stuff, the way this

04:09:32.920 --> 04:09:39.080
works is you're gonna say data sets dot CI FR 10 dot load data. Now this loads it in as like a very

04:09:39.080 --> 04:09:43.960
strange TensorFlow object, that's like a data set object. So this is different from what we've

04:09:43.960 --> 04:09:49.400
used before, where some of our objects have actually been like in NumPy arrays, where we can look at

04:09:49.400 --> 04:09:53.320
them better. This is not going to be in that. So just something to keep in mind here. And we're

04:09:53.320 --> 04:09:58.680
going to normalize this data into train images and test images, but just dividing both of them by

04:09:58.680 --> 04:10:03.240
255. Now, again, we're doing that because we want to make sure that our values are between zero and

04:10:03.240 --> 04:10:07.000
one, because that's just a lot better to work with in our neural networks, rather than large

04:10:07.000 --> 04:10:11.960
integer values, just causes, you know, some things to mess up sometimes. Now class names,

04:10:11.960 --> 04:10:15.400
we're just going to find a list here. So we have all the class names so that zero represents

04:10:15.400 --> 04:10:21.880
airplane one auto avail so far in tilt truck, run that block of code here, we'll download this

04:10:21.880 --> 04:10:26.120
data set, although I don't think it takes that long to do that. So okay, so wait, I guess,

04:10:26.120 --> 04:10:30.520
yeah, I guess that's good. I think we're okay there. And now let's just have a look at actually

04:10:30.520 --> 04:10:35.160
some of the images here by running this script. So we can see this is a truck can change the

04:10:35.160 --> 04:10:41.560
image index to be two, we can see this is another truck. Let's go to say six, we get a bird. And

04:10:41.560 --> 04:10:44.920
you can see these are really blurry. But that's fine. For this example, we're just trying to get

04:10:44.920 --> 04:10:49.400
something that works all right. Okay, so that's a horse, you know, you get the point. All right.

04:10:49.400 --> 04:10:54.440
So now CNN architecture. So essentially, we've already talked about how a convolutional neural

04:10:54.440 --> 04:10:58.520
network works, we haven't talked about the architecture and how we actually make one.

04:10:58.520 --> 04:11:03.160
Essentially, what we do is we stack a bunch of convolutional layers and max pooling,

04:11:03.160 --> 04:11:08.040
min pooling or average pooling layers together in something like this, right. So after each

04:11:08.040 --> 04:11:12.680
convolutional layer, we have a max pooling layer, some kind of pooling layer typically to reduce

04:11:12.680 --> 04:11:16.680
the dimensionality, although you don't need that, you could just go straight into three

04:11:16.760 --> 04:11:22.520
convolutional layers. And on our first layer, what we do is we define the amount of filters

04:11:22.520 --> 04:11:28.120
just like here, we define the sample size. So how big are those filters and activation function,

04:11:28.120 --> 04:11:33.720
which essentially means after we apply that, what is it that cross not cross product dot

04:11:33.720 --> 04:11:38.280
product operation that we talked about, we'll apply rectifier linear unit to that and then

04:11:38.280 --> 04:11:42.360
put that in the output feature map. Again, we've talked about activations functions before. So I

04:11:42.360 --> 04:11:46.040
won't go too far into depth with them. And then we define the input shape, which essentially

04:11:46.040 --> 04:11:50.520
means what can we expect in this first layer? Well, 32 by 32 by three, these ones, we don't

04:11:50.520 --> 04:11:53.720
need to do that, because they're going to figure out what that is based on the input from the

04:11:53.720 --> 04:11:58.520
previous layer. Alright, so these are just a breakdown of the layers. The convolution or the

04:11:58.520 --> 04:12:02.200
max pooling layers here, two by two, essentially means that what we're going to do is we're going

04:12:02.200 --> 04:12:06.760
to have a two by two sample size with actually a stride of two. Again, the whole point of this

04:12:06.760 --> 04:12:12.760
is to actually divide or, you know, shrink it by a factor of two, how large each of these layers

04:12:12.840 --> 04:12:17.720
are. Alright, so now let's have a summary. It's already printed out here. We can see that we

04:12:17.720 --> 04:12:24.520
have, oh, wait, is this correct? mobile net v two, I don't think that's correct. That's because I

04:12:24.520 --> 04:12:29.400
haven't run this one. My apologies on that guys, this is from something later in the tutorial,

04:12:29.400 --> 04:12:33.640
we can see that we have calm 2d as our first layer. This is the output shape of that layer.

04:12:33.640 --> 04:12:41.080
Notice that it is not 32 by 32 by 32. It is 30 by 30 by 32, because when we do that sampling

04:12:41.080 --> 04:12:45.160
without padding, right, that's what we're going to get. We're going to get two pixels less,

04:12:45.960 --> 04:12:51.880
because the amount of samples we can take. Alright, next, we have the max pooling 2d layer.

04:12:51.880 --> 04:12:57.640
So this now says the output shape is 15 by 15 by 32, which means we've shrunk this shape by a

04:12:57.640 --> 04:13:03.880
factor of two, we do a convolution on this, which means that now we get 1313 and we're doing 64,

04:13:03.880 --> 04:13:09.640
because we're going to take 64 filters this time. And then max pooling again, we go six by six by

04:13:10.200 --> 04:13:14.760
64, because we're going to divide this again by factor of two. Notice that it just rounded,

04:13:14.760 --> 04:13:19.560
right? And then calm 2d. So another layer here, we get four by four by 64, again, because of the

04:13:19.560 --> 04:13:25.960
way we take those values. So this is what we've defined so far. But this is not the end of our

04:13:25.960 --> 04:13:31.320
convolutional neural network. In fact, this doesn't really mean much to us, right? This just tells us

04:13:31.320 --> 04:13:35.960
about the presence of specific features, as we've gone through this convolution base, which is what

04:13:35.960 --> 04:13:41.720
this is called the stack of convolution and max pooling layers. So what we actually need to do

04:13:41.720 --> 04:13:47.560
is now pass this information into some kind of dense layer classifier, which is actually going to

04:13:47.560 --> 04:13:53.160
take this pixel data that we've kind of calculated and found. So the almost extraction of features

04:13:53.160 --> 04:13:58.360
that exist in the image, and tell us which combination of these features map to either,

04:13:58.360 --> 04:14:03.320
you know, what one of these 10 classes are. So that's kind of the point you do this convolution

04:14:03.320 --> 04:14:08.920
base, which extracts all of the features out of your image. And then you use the dense network

04:14:08.920 --> 04:14:13.880
to say, Okay, well, if these combination of features exist, then that means this image is this,

04:14:13.880 --> 04:14:18.920
otherwise, it's this and that and so on. So that's what we're doing here. Alright, so let's say adding

04:14:18.920 --> 04:14:22.920
the dense layers. So to add the dense layers pretty easy model dot add is just how we add them,

04:14:22.920 --> 04:14:28.680
right? So we're going to flatten all of those pixels was which essentially means take the four

04:14:28.680 --> 04:14:33.080
by four by 64, and just put those all into a straight line, like we've done before. So just

04:14:33.080 --> 04:14:39.000
one dimensional, then we're going to have a 64 neuron dense layer that connects all of those

04:14:39.000 --> 04:14:43.480
things to it with an activation function of rectifier linear unit, then our output layer of

04:14:43.480 --> 04:14:48.360
a dense layer with 10 neurons, obviously 10, because that's the amount of classes we have for

04:14:48.360 --> 04:14:52.040
this problem. So let's run this here, we'll add those layers, let's look at a summary and see

04:14:52.040 --> 04:14:58.360
how things have changed now. So we go from four by four by 64 to 2024. Notice that that is precisely

04:14:58.360 --> 04:15:03.320
the calculation of four times four times 64. That's how we get that number here. Then we have a

04:15:03.320 --> 04:15:07.000
dense layer and another dense layer. And this is our output layer. Finally, this is what we're

04:15:07.000 --> 04:15:11.160
getting is we're going to get 10 neurons out. So essentially, just a list of values. And that's

04:15:11.160 --> 04:15:17.480
how we can determine which class is predicted. So this up to here is the convolutional base,

04:15:17.480 --> 04:15:21.960
this is what we call the classifier, and they work together to essentially extract the features,

04:15:21.960 --> 04:15:27.000
and then look at the features and predict the actual object or whatever it is the class. Alright,

04:15:27.640 --> 04:15:30.360
so that's how that works. Now it's time to train again, we'll go through this quickly.

04:15:31.000 --> 04:15:35.000
I believe I've already trained this this takes a long time to train. So I'm actually going to

04:15:35.000 --> 04:15:41.800
reduce the epochs here to just be for I'd recommend you guys train this on higher. So like 10, if

04:15:41.800 --> 04:15:45.560
you're going to do it, it does take a while. So for our purposes, and for my time, we'll leave

04:15:45.560 --> 04:15:49.400
a little bit shorter right now, but you should be getting about 70% accuracy. And you can see I've

04:15:49.400 --> 04:15:53.720
trained this previously, if you train it on 10 epochs, but I'm just going to train up to four,

04:15:53.800 --> 04:15:58.040
we get our 6768%. And that should be fine. So we'll be back once this is trained,

04:15:58.040 --> 04:16:01.880
then we'll talk about how some of this works. Okay, so the model is finally finished training,

04:16:01.880 --> 04:16:07.800
we did about four epochs, you can see we got an accuracy about 67% on the evaluation data.

04:16:07.800 --> 04:16:14.120
To quickly go over this stuff. optimizers, Adam talked about that before. loss function is sparse

04:16:14.120 --> 04:16:18.440
categorical cross entropy. That one, I mean, you can read this if you want computes the cross

04:16:18.440 --> 04:16:22.920
entropy loss between the labels and predictions. And I'm not going to go into that. But these

04:16:22.920 --> 04:16:26.360
kind of things are things that you can look up if you really understand why they work.

04:16:26.920 --> 04:16:30.840
For most problems, you can just if you want to figure out what, you know, loss function

04:16:30.840 --> 04:16:36.040
or optimizer to use, just use the basics, like use Adam, use a categorical cross entropy,

04:16:36.040 --> 04:16:40.040
using a classification task, you want to do something like this, there's just you can go

04:16:40.040 --> 04:16:44.280
up and look kind of all of the different loss functions, and it'll tell you when to use which

04:16:44.280 --> 04:16:49.480
one and you can kind of mess with them and tweak them if you want. Now history equals model dot

04:16:49.480 --> 04:16:54.040
fit. This is just so we can access some of the statistics from this model dot fit. Obviously,

04:16:54.040 --> 04:16:58.840
it's just training the data to this test images, test labels and train images and train labels

04:16:58.840 --> 04:17:03.560
where this is the validation data suite. So evaluating the model, we want to evaluate the

04:17:03.560 --> 04:17:07.080
model, we can evaluate it now on the test images and test labels, we're obviously going to get

04:17:07.080 --> 04:17:11.480
the same thing because the valuation is test images and test labels. So we should get the

04:17:11.480 --> 04:17:17.480
same accuracy as 6735, which we do right here. Alright, so there we go, we get about 70%. If

04:17:17.560 --> 04:17:21.080
you guys train this on 10 epochs, you should get close to 70. I'm a little bit lower just

04:17:21.080 --> 04:17:25.400
because I didn't want to go that high. And that is now the model. I mean, we could use this if

04:17:25.400 --> 04:17:29.400
we want, we could use predict, we could pass in some image, and we could see the prediction for

04:17:29.400 --> 04:17:33.000
it. I'm not going to do that just because we've already talked about that enough. And I want to

04:17:33.000 --> 04:17:38.120
get into some of the cooler stuff when we're working with smaller data sets. So the basic idea

04:17:38.120 --> 04:17:42.040
here is this is actually a pretty small data set, right? We use about 60,000 images. And if you

04:17:42.040 --> 04:17:47.880
think about the amount of different patterns we need to pick up to classify, you know, things like

04:17:47.880 --> 04:17:53.480
horses versus trucks, that's a pretty difficult task to do, which means that we need a lot of data.

04:17:53.480 --> 04:17:58.200
And in fact, some of the best convolutional networks that are out there are trained on millions

04:17:58.200 --> 04:18:02.680
of pieces of, you know, sample information or data. So obviously, we don't have that kind of data.

04:18:02.680 --> 04:18:07.480
So how can we work with, you know, a few images, maybe like a few 1000 images, and still get a

04:18:07.480 --> 04:18:12.520
decent model. Well, the thing is, you can't unless we use some of the techniques that I have to show

04:18:12.520 --> 04:18:18.120
you. So working with small data sets. So just like I mentioned, it's difficult to create a very good

04:18:18.120 --> 04:18:23.320
convolutional neural network from scratch, if you're using a small amount of data, that is why we

04:18:23.320 --> 04:18:28.840
can actually employ these techniques, the first one data augmentation, but also using pre trained

04:18:28.840 --> 04:18:32.440
models to kind of accomplish what we need to do. And that's what we're going to be talking about

04:18:32.440 --> 04:18:35.480
now in the second part of the tutorial, we're going to create another convolutional neural

04:18:35.560 --> 04:18:39.320
network. So just to clarify, this is created, we've made the model up here already. This is

04:18:39.320 --> 04:18:43.000
all we need to do to do it. This is the architecture. And this was just to get you familiar with the

04:18:43.000 --> 04:18:50.520
idea. So data augmentation. So this is basically the idea, if you have one image, we can turn that

04:18:50.520 --> 04:18:57.560
image into several different images, and train and pass all those images to our, our model. So

04:18:57.560 --> 04:19:02.520
essentially, if we can rotate the image, if we can flip it, if we can stretch it, compress it,

04:19:03.080 --> 04:19:07.560
you know, shift it, zoom it, whatever it is, and pass that to our model, it should be better at

04:19:07.560 --> 04:19:13.720
generalizing, because we'll see the same image, but modified and augmented multiple times, which

04:19:13.720 --> 04:19:20.840
means that we can turn a data set say of 10,000 images into 40,000 images, by doing four augmentations

04:19:20.840 --> 04:19:26.600
on every single image. Now, obviously, you still want a lot of unique images, but this technique

04:19:26.600 --> 04:19:31.560
can help a lot and is used quite a bit, because that allows our kind of model to be able to pick

04:19:31.560 --> 04:19:35.400
up images that maybe are orientated differently or zoomed in a bit or stretch something different,

04:19:35.400 --> 04:19:39.800
right, just better at generalizing, which is the whole point. So I'm not going to go through this

04:19:39.800 --> 04:19:44.280
in too depth, too much depth, but this is essentially a script that does data augmentation

04:19:44.280 --> 04:19:50.440
for you. We're gonna use this image data generator from the Keras dot preprocessing dot image module,

04:19:51.080 --> 04:19:55.320
we're going to create an image data generator object. Now essentially, what this allows us to

04:19:55.320 --> 04:19:59.720
do is specify some parameters on how we want to modify our image. In this case, we have the

04:19:59.720 --> 04:20:06.680
rotation range, some shifts, shear, zoom horizontal flip and the mode. Now I'm not going to go into

04:20:06.680 --> 04:20:10.440
how this works, you can look at the documentation if you'd like. But essentially, this will just

04:20:10.440 --> 04:20:16.280
allow us to augment our images. Now what I'm going to do is pick one arbitrary image from the test

04:20:16.280 --> 04:20:21.160
image data set, just our test image, I guess, group of photos, whatever you want to call it.

04:20:21.160 --> 04:20:25.320
I'm going to convert that to an image array, which essentially takes it from the weird data set

04:20:25.320 --> 04:20:30.520
object that it kind of is and turns it into a NumPy array. Then we're going to reshape this.

04:20:30.520 --> 04:20:34.600
So that's in the form one comma, which essentially means one, and then this will figure out what

04:20:34.600 --> 04:20:39.720
the rest of the shape should be. Oh, sorry, one and then plus the image shape, which is whatever

04:20:39.720 --> 04:20:45.960
this shape is. So we'll reshape that. And then what we're going to do is we're going to say for batch

04:20:45.960 --> 04:20:51.720
in data flow gen dot flow. Talk about how that works in a second. Essentially, this is just going

04:20:51.800 --> 04:20:56.120
to augment the image for us and actually save it onto our drive. So in this instance, what's

04:20:56.120 --> 04:21:00.840
going to happen is this data gen dot flow is going to take the image which we've created here, right?

04:21:00.840 --> 04:21:05.080
And we formatted it correctly by doing these two steps, which you need to do beforehand,

04:21:05.080 --> 04:21:09.400
it's going to save this image as test dot jpeg. And this will be the prefix, which means there'll

04:21:09.400 --> 04:21:14.840
be some information after. And it will do this as many times until we break. So essentially,

04:21:14.840 --> 04:21:19.640
given an image, it will do test one, test two, test three, test four, test five, with random

04:21:19.640 --> 04:21:25.160
augmentations using this, until eventually we decided to break out of this. Now what I'm doing

04:21:25.160 --> 04:21:29.480
is just showing the image by doing this and batch zero is just showing us the you know,

04:21:29.480 --> 04:21:35.080
that first image in there. And that's kind of how this works. So you can mess with the script

04:21:35.080 --> 04:21:37.960
and figure out a way to use it. But I would recommend if you want to do data augmentation,

04:21:37.960 --> 04:21:42.360
just look into image data generator. This is something that I just want to show you so

04:21:42.360 --> 04:21:45.960
you're aware of and I'll just run it so you can see exactly how this works. So essentially,

04:21:45.960 --> 04:21:49.320
given an image of a truck, what it will do is augmented in these different ways.

04:21:49.880 --> 04:21:55.160
You can see kind of the shifts, the translations, the rotations, all of that. And we'll do

04:21:56.040 --> 04:21:58.920
actually a different image here to see what one looks like. Let's just do image say 20.

04:21:59.800 --> 04:22:04.040
See if we get something different. So in this case, I believe this is maybe like a deer or

04:22:04.040 --> 04:22:08.120
rabbit or a dog or something. I don't really know exactly what it is because it's so blurry.

04:22:08.120 --> 04:22:11.560
But you can see that's kind of the shifts we're getting. And it makes sense because you want

04:22:11.560 --> 04:22:15.560
to have images in different areas so that we have a better generalization. All right,

04:22:15.640 --> 04:22:20.760
let's close that. Okay, so now we're going to talk about using or sorry, what is it pre trained

04:22:20.760 --> 04:22:25.320
models? Okay, so we talked about data augmentation. That's a great technique if you want to increase

04:22:25.320 --> 04:22:29.480
the size of your data set. But what if even after that, we still don't have enough images in our

04:22:29.480 --> 04:22:34.360
data set? Well, what we can do is use something called a pre trained model. Now companies like

04:22:34.360 --> 04:22:39.560
Google and you know, TensorFlow, which is owned by Google, make their own amazing convolutional

04:22:39.560 --> 04:22:43.320
neural networks that are completely open source that we can use. So what we're going to do is

04:22:43.320 --> 04:22:47.960
actually use part of a convolutional neural network that they've trained already on, I believe

04:22:47.960 --> 04:22:54.520
1.4 million images. And we're just going to use part of that model as kind of the base of our

04:22:54.520 --> 04:22:59.480
models that we have a really good starting point. And all we need to do is what's called fine tune,

04:22:59.480 --> 04:23:04.520
the last few layers of that network, so that they work a little bit better for our purposes.

04:23:05.240 --> 04:23:09.960
So what we're going to do essentially say, All right, we have this model that Google's trained,

04:23:09.960 --> 04:23:14.280
they've trained it on 1.4 million images, it's capable of classifying, let's say 1000 different

04:23:14.280 --> 04:23:19.480
classes, which is actually the example we'll look at later. So obviously, the beginning of that model

04:23:20.120 --> 04:23:25.400
is what's picking up on the smaller edges, and you know, kind of the very general things that

04:23:25.400 --> 04:23:30.600
appear in all of our images. So if we can use the base of that model, so kind of the beginning of

04:23:30.600 --> 04:23:35.160
it, that does a really good job picking up on edges and general things that will apply to any

04:23:35.160 --> 04:23:40.200
images. Then what we can do is just change the top layers of that model a tiny bit or add our own

04:23:40.200 --> 04:23:45.240
layers to it to classify for the problem that we want. And that should be a very effective way

04:23:45.240 --> 04:23:49.960
to use this pre trained model. We're saying we're going to use the beginning part that's really

04:23:49.960 --> 04:23:54.840
good at kind of the generalization step, then we'll pass it into our own layers that we'll do

04:23:54.840 --> 04:23:59.480
whatever we need to do specifically for our problem. That's what's like the fine tuning

04:23:59.480 --> 04:24:02.680
step. And then we should have a model that works pretty well. And in fact, that's what we're going

04:24:02.760 --> 04:24:06.920
to do in this example now. So that's kind of the point of what I'm talking about here is using

04:24:06.920 --> 04:24:11.240
part of a model that already exists, that's very good at generalizing, and it's been trained on

04:24:11.240 --> 04:24:16.760
so many different images. And then we'll pass our own training data in, we won't modify the

04:24:16.760 --> 04:24:21.400
beginning aspect of our neural network, because it already works really well, we'll just modify the

04:24:21.400 --> 04:24:26.760
last few layers that are really good at classifying, for example, just cats and dogs, which is exactly

04:24:26.760 --> 04:24:30.040
the example we're actually going to do here. So I hope that makes sense as we get through this

04:24:30.120 --> 04:24:34.200
should be cleared up a little bit. But using a pretrained model is now the section we're

04:24:34.200 --> 04:24:37.960
getting into. So this is based on this documentation, as always, I'm referencing

04:24:37.960 --> 04:24:41.640
everything. So you guys can go see that if you'd like, and do our imports like this,

04:24:41.640 --> 04:24:45.160
we're going to load a data set that actually takes a second to load the data set, I believe,

04:24:45.160 --> 04:24:50.600
oh, maybe not. And essentially, the problem we're doing is trying to classify dogs versus cats with

04:24:50.600 --> 04:24:56.040
a fair degree of accuracy. In fact, we'd like to get above 90%. So this is the data set we're

04:24:56.040 --> 04:25:01.800
loading in from TensorFlow data sets as TFDS. This is kind of a weird way to load it in again,

04:25:01.800 --> 04:25:05.880
stuff like this, you just have to reference the documentation, I can explain it to you, but

04:25:05.880 --> 04:25:08.840
it's not really going to help when the next example is going to be a different way of

04:25:08.840 --> 04:25:12.600
loading the data, right? So so long as you know how to get the data in the correct form,

04:25:12.600 --> 04:25:16.440
you can get it into some kind of NumPy array, you can split it into training, testing and

04:25:16.440 --> 04:25:20.600
validation data, you should be okay. And if you're using a TensorFlow data set, it should

04:25:20.600 --> 04:25:25.160
tell you in the documentation how to load it in properly. So we'll load it in here, we're training

04:25:25.160 --> 04:25:31.400
80% train will go 10% for what is it raw validation and 10% for the testing data.

04:25:32.120 --> 04:25:36.440
So we've loaded that. And now what we're doing here is just we're going to look at a few images.

04:25:36.440 --> 04:25:40.680
So this actually creates a function, I know this is a weird thing, this is pretty unique to this

04:25:40.680 --> 04:25:47.000
example, that allows us to call this function with some integer, essentially, and get what the

04:25:47.000 --> 04:25:51.240
actual string representation of that is to the label for it. And what I'm doing here is just

04:25:51.240 --> 04:25:55.720
taking two images from our raw training data set, and just displaying them. And you can see

04:25:55.720 --> 04:26:01.320
that's where we're getting here dog and dog. If I go ahead and take five, we'll see, these are

04:26:01.320 --> 04:26:07.400
what our images look like. Right? So here's example of a dog, we have a cat, right? And so on so forth,

04:26:07.400 --> 04:26:11.960
you kind of you get the you get the point there. Now, notice, though, that these images are

04:26:11.960 --> 04:26:15.320
different dimensions. In fact, none of these images other than these two actually are the same

04:26:15.320 --> 04:26:19.640
dimension at all. Oh, actually, I don't think these ones are either. So obviously, there's a

04:26:19.640 --> 04:26:23.720
step that we need to do, which is we need to scale all these images to be the same size.

04:26:24.440 --> 04:26:28.840
So to do that, what we're going to do is write a little function like this, and essentially,

04:26:28.840 --> 04:26:34.280
we'll return an image that is reshaped. So I guess that is reshaped to the image size, which I'm

04:26:34.280 --> 04:26:39.160
going to set at 160 by 160. Now, we can make this bigger if we want. But the problem sometimes is

04:26:39.160 --> 04:26:44.680
if you make an image that is bigger than like you want to make your image bigger than most of your

04:26:44.680 --> 04:26:48.520
data set examples, and that means you're going to be really stretching a lot of the examples out

04:26:48.520 --> 04:26:52.760
and you're losing a lot of detail. So it's much better to make the image size smaller rather

04:26:52.760 --> 04:26:56.920
than bigger. You might say, well, if you make it smaller, you're going to lose detail too. But

04:26:56.920 --> 04:27:01.800
it's just it's better to compress it smaller than it is to go really big, even just when it comes

04:27:01.800 --> 04:27:05.880
to the amount of training time and how complex networks going to be. So that's something to

04:27:05.880 --> 04:27:09.400
consider. You can mess around with those when you're making your own networks. But again,

04:27:09.400 --> 04:27:13.480
smaller is typically better in my opinion, you don't want to go too small, but something that's

04:27:13.480 --> 04:27:18.200
like, you know, half the size of what an average image would be. Alright, so we're going to go

04:27:18.200 --> 04:27:22.360
format example. So we're going to just take an image and a label. And what this will do is return

04:27:22.360 --> 04:27:26.840
to us just the reshaped image and label. So in this case, we're going to cast, which means convert

04:27:26.840 --> 04:27:31.880
every single pixel in our image to be a float 32 value, because it could be integers, we're then

04:27:31.880 --> 04:27:39.400
going to divide that by 127.5, which taken is exactly half of 255. And then subtract one,

04:27:39.480 --> 04:27:44.360
then we're going to resize this image to be the image size. So sorry, the image will be

04:27:44.360 --> 04:27:49.480
resized to the image size of 160 by 160 and we'll return the new image and the label. So now we

04:27:49.480 --> 04:27:53.320
can apply this function to all of our images using map, if you don't know what map is, essentially,

04:27:53.320 --> 04:27:58.360
it takes every single example in in this case, going to be raw train and applies the function

04:27:58.360 --> 04:28:04.040
to it, which will mean that it will convert raw train into images that are all resized to 160

04:28:04.040 --> 04:28:09.320
by 160. And we'll do the same thing for validation and test. So run that no issue there. Now

04:28:09.320 --> 04:28:13.880
let's have a look at our images and see what we get. And there we are. Now I've just messed up

04:28:13.880 --> 04:28:20.520
the color because I didn't add a CMAP thing, which I think I needed. Where was the CMAP?

04:28:22.040 --> 04:28:24.920
Anyways, you know what, that's fine for now. This is what our images look like. This is the

04:28:24.920 --> 04:28:31.400
resize. Now we get all images 160 by 160. And we are good to go. Alright, so

04:28:32.680 --> 04:28:36.360
now let's have a look at the shape of an original image versus our new image. So I mean, this was

04:28:36.440 --> 04:28:41.960
just to prove that essentially our original shapes were like 262 409 by some random values,

04:28:41.960 --> 04:28:46.840
and they're all reshaped now to 160 160 by three, three obviously is the color channel of the images.

04:28:47.480 --> 04:28:51.400
Alright, so picking a pre trained model. So this is the next step, this is probably one of the

04:28:51.400 --> 04:28:55.560
harder steps is picking a model that you would actually like to use the base up. Now we're going

04:28:55.560 --> 04:28:59.720
to use one called mobile net v two, which is actually from Google, it's built into TensorFlow

04:28:59.720 --> 04:29:03.640
itself. That's why I've picked it. And all we're going to do is set this. So essentially, we're

04:29:03.640 --> 04:29:09.240
going to say the base model in our code is equal to TF dot keras dot applications dot mobile net

04:29:09.240 --> 04:29:13.960
v two, which is just telling us the architecture of the model that we want. And we'll have a look

04:29:13.960 --> 04:29:18.360
at it down below here. In just a second, we'll define the input shape, which is important,

04:29:18.360 --> 04:29:22.680
because this can take any input shape that we want. So we'll change it to 160 160 by three,

04:29:22.680 --> 04:29:28.280
which we've defined up here, include top, very important means do we include the

04:29:28.280 --> 04:29:32.840
classifier that comes with this network already or not. Now in our case, we're going to be

04:29:32.840 --> 04:29:38.600
retraining parts of this network so that it works specifically for dogs and cats, and not for 1000

04:29:38.600 --> 04:29:43.480
different classes, which is what this model was actually aimed to do is train a 1.4 million images

04:29:43.480 --> 04:29:48.280
for 1000 different classes of everyday objects. So we're going to not include the top, which means

04:29:48.280 --> 04:29:53.400
I don't include the classifier for these 1000 classes. And we're going to load the weights

04:29:53.400 --> 04:29:58.200
from what's called image net, which is just a specific save of the weights. So this is the

04:29:58.200 --> 04:30:02.120
architecture. And this is kind of the data that we're filling in for that architecture. So the

04:30:02.120 --> 04:30:06.920
weights, and we'll load that in which we have here. So base model, now let's look at it. So

04:30:06.920 --> 04:30:11.160
let's have a summary. You can see this is a pretty crazy model. I mean, we would never be

04:30:11.160 --> 04:30:14.840
expected to create something like this by ourselves. This is, you know, teams of data

04:30:14.840 --> 04:30:19.080
scientists, PhD students, engineers, would I write the experts in the field that have created a

04:30:19.080 --> 04:30:23.160
network like this. So that's why we're going to use it because it works so effectively for

04:30:23.160 --> 04:30:27.800
the generalization at the beginning, which is what we want. And then we can take those features

04:30:27.800 --> 04:30:32.680
that this takes out. So in five by five by 1280, which is what I want us to focus on the output

04:30:32.680 --> 04:30:38.120
of this actual network here. So really, you can see this last layer, we're going to take this and

04:30:38.120 --> 04:30:44.200
using this information, pass that to some more convolutional layers and actually our own classifier,

04:30:44.200 --> 04:30:50.360
I believe, and use that to predict versus dogs versus cats. So at this point, the base model

04:30:50.360 --> 04:30:55.000
will simply output a shape 32 by five by five by 1280. That's the tensor that we're going to get

04:30:55.000 --> 04:30:59.080
out of this, that's the shape, you can watch how this kind of works as you go through it.

04:31:00.200 --> 04:31:04.840
And yes, all right. So we can just have a look at this here. This what I wanted to do essentially

04:31:04.840 --> 04:31:09.720
was just look at what the actual shape was going to be. So 32 five by five by 1280, just because

04:31:09.720 --> 04:31:14.120
this gives us none until it knows what the input is. And now it's time to talk about freezing the

04:31:14.120 --> 04:31:18.920
base. So essentially, the point is, we want to use this as the base of our network, which means

04:31:18.920 --> 04:31:23.640
we don't want to change it. If we just put this network in right now is the base to our neural

04:31:23.640 --> 04:31:28.520
network. Well, what's going to happen is, it's going to start retraining all these weights and

04:31:28.520 --> 04:31:34.600
biases. And in fact, it's going to train 2.257 million more weights and biases, when in fact,

04:31:34.600 --> 04:31:37.960
we don't want to change these because these have already been defined, they've been set. And we

04:31:37.960 --> 04:31:41.960
know that they work well for the problem already, right, they worked well for classifying 1000

04:31:41.960 --> 04:31:45.480
classes. Why are we going to touch this now? And if we were going to touch this, what's the point

04:31:45.480 --> 04:31:49.240
of even using this base, right, we don't want to train this, we want to leave it the same.

04:31:49.320 --> 04:31:53.320
So to do that, we're just going to freeze it. Now, freezing is a pretty, I mean, it just

04:31:53.320 --> 04:31:58.760
essentially means turning the trainable attribute of a layer off or of the model off. So what we do

04:31:58.760 --> 04:32:02.120
is you just say base model dot trainable equals false, which essentially means that we are no

04:32:02.120 --> 04:32:06.680
longer going to be training any aspect of that, I want to say model, although we'll just call it

04:32:06.680 --> 04:32:11.240
the base layer for now, or the base model. So now if we look at the summary, we can see when we

04:32:11.240 --> 04:32:17.480
scroll down to the bottom, if we get there any day soon, that now the trainable parameters is

04:32:17.480 --> 04:32:23.160
zero instead of 2.257 million, which it was before. And now it's time to add our own classifier on

04:32:23.160 --> 04:32:27.240
top of this. So essentially, we've got a pretty good network, right, five by five by 1280s,

04:32:27.240 --> 04:32:33.320
our last output. And what we want to do now is take that. And we want to use it to classify

04:32:33.320 --> 04:32:38.520
either cat or either dog, right? So what we're going to do is add a global average layer,

04:32:38.520 --> 04:32:44.760
which essentially is going to take the entire average of every single so 1280 different layers

04:32:44.840 --> 04:32:50.680
that are five by five, and put that into a 1d tensor, which is kind of flattening that for us.

04:32:50.680 --> 04:32:55.000
So we do that global average pooling. And then we're just going to add the prediction layer,

04:32:55.000 --> 04:32:59.800
which essentially is going to just be one dense node. And since we're only classifying two different

04:32:59.800 --> 04:33:04.680
classes, right, dogs and cats, we only need one, then we're going to add all these models together.

04:33:04.680 --> 04:33:09.080
So the base model, and I guess layers, the global average layer that we define there, and then the

04:33:09.080 --> 04:33:14.920
prediction layer to create our final model. So let's do this global average layer, prediction

04:33:14.920 --> 04:33:20.760
layer model, give that a second to kind of run there. Now when we look at the summary, we can see

04:33:20.760 --> 04:33:25.720
we have mobile net v2, which is actually a model, but that is our base layer. And that's fine,

04:33:25.720 --> 04:33:31.640
because the output shape is that then global average pooling, which again, just takes this

04:33:31.640 --> 04:33:36.600
flattens it out does the average for us. And then finally, our dense layer, which is going to

04:33:36.600 --> 04:33:42.680
simply have one neuron, which is going to be our output. Now notice that we have 2.25 and nine

04:33:42.680 --> 04:33:48.920
million parameters in total, and only 1281 of them are trainable. That's because we have 1280

04:33:48.920 --> 04:33:55.000
connections from this layer to this layer, which means 1280 weights and one bias. So that is what

04:33:55.000 --> 04:33:58.760
we're doing. This is what we have created now, this base, the majority of the network has been

04:33:58.760 --> 04:34:03.560
done for us. And we just add our own little classifier on top of this. And now we're going to

04:34:03.560 --> 04:34:08.600
feed some training samples and data to this. Remember, we're not training this base layer

04:34:08.600 --> 04:34:12.840
whatsoever. So the only thing that needs to be learned is the weights and biases on these two

04:34:12.840 --> 04:34:18.040
layers here. Once we have that, we should have a decent model ready to go. So let's actually train

04:34:18.040 --> 04:34:23.400
this now. I'm going to compile this here, I'm picking a learning rate that's very slow, what

04:34:23.400 --> 04:34:27.800
essentially what the learning rate means is how much am I allowed to modify the weights and biases

04:34:27.800 --> 04:34:32.040
of this network, which is what I've done, just made that very low, because we don't want to make

04:34:32.040 --> 04:34:37.800
any major changes if we don't have to, because we're already using a base model that exists,

04:34:37.800 --> 04:34:41.160
right? So we'll set the learning rate, I'm not going to talk about what this does specifically,

04:34:41.160 --> 04:34:45.400
you can look that up if you'd like to. And then the loss function will use binary cross entropy,

04:34:45.400 --> 04:34:50.280
just because we're using two classes, if you're using more than more than two classes, you would

04:34:50.280 --> 04:34:55.560
just have cross entropy, or some other type of cross entropy. And then what we're going to do is

04:34:55.560 --> 04:35:01.640
actually evaluate the model right now, before we even train it. So I've compiled it, I've just set

04:35:01.720 --> 04:35:07.560
what we'll end up using. But I want to evaluate the model currently, without training it whatsoever,

04:35:07.560 --> 04:35:13.640
on our validation data or validation batches, and see what it actually looks like, what it

04:35:13.640 --> 04:35:18.120
actually, you know, what we're getting right now, with the current base model being the way it is,

04:35:18.120 --> 04:35:22.040
and not having changed the weights and biases that completely random from the global average

04:35:22.040 --> 04:35:26.920
pooling in the dense layer. So let's evaluate. Let's see what we get as an accuracy. Okay,

04:35:26.920 --> 04:35:31.320
so we can actually see that with the random weights and biases for those last layer that we added,

04:35:31.720 --> 04:35:36.040
we're getting an accuracy of 56%, which pretty much means that it's guessing, right? It's,

04:35:36.040 --> 04:35:40.760
you know, 50% is only two classes. So if we got anything lower than 50, like 50 should have been

04:35:40.760 --> 04:35:46.040
our guess, which is what we're getting. So now what we're going to do. And actually, I've trained

04:35:46.040 --> 04:35:53.000
this already, I think so I might not have to do it again, is train this model on all of our images.

04:35:53.000 --> 04:35:57.560
So all of our images and cats and cats and dogs that we've loaded in before, which will allow us

04:35:57.640 --> 04:36:03.160
now to modify these weights and biases of this layer. So hopefully it can determine what features

04:36:03.160 --> 04:36:08.040
need to be present for a dog to be a dog and for a cat to be a cat, right? And then it can make a

04:36:08.040 --> 04:36:11.960
pretty good prediction. In fact, I'm not going to train this in front of us right now, because

04:36:11.960 --> 04:36:16.440
it actually takes close to an hour to train just because there is a lot of images that it needs

04:36:16.440 --> 04:36:21.960
to look at and a lot of calculations that need to happen. But when you do end up training this,

04:36:21.960 --> 04:36:27.560
you end up getting an accuracy of a close to 92 or 93%, which is pretty good,

04:36:27.560 --> 04:36:33.160
considering the fact that all we did was use an original layer, like base layer that classified

04:36:33.160 --> 04:36:38.520
up to 1000 different images, so very general, and applied that just to cats and dogs by adding

04:36:38.520 --> 04:36:43.240
our dense layer classifier on top. So you can see this was kind of the accuracy I had from

04:36:43.240 --> 04:36:47.000
training this previously, I don't want to train again, because it takes so long. But I did want

04:36:47.080 --> 04:36:53.480
to show that you can save a model and load a model by doing this syntax. So essentially,

04:36:53.480 --> 04:36:58.360
on your model object, you can call model dot save, save it as whatever name you like dot h5,

04:36:58.360 --> 04:37:03.960
which is just a format for saving models and Keras is specific to Keras, not TensorFlow.

04:37:03.960 --> 04:37:08.600
And then you can load the model by doing this. So this is useful because after you train this

04:37:08.600 --> 04:37:12.600
for an hour, obviously, you don't want to retrain this if you don't have to to actually use it to

04:37:12.600 --> 04:37:18.280
make predictions. So you can just load the model. Now, I'm not going to go into using the model

04:37:18.280 --> 04:37:21.880
specifically, you guys can look up the documentation to do that. We're at the point now where I've

04:37:21.880 --> 04:37:26.280
showed you so much syntax on predicting and how we actually use the models. But the basic idea

04:37:26.280 --> 04:37:30.120
would be to do model dot predict, right? And then you can see that it's even giving me the input

04:37:30.120 --> 04:37:35.080
here. So model dot predict, give it some x batch size or both, right, because it will predict on

04:37:35.080 --> 04:37:38.920
multiple things. And that will spit back to you a class, which then you can figure out, okay,

04:37:38.920 --> 04:37:42.920
this is a cat, or this is a dog, you're going to pass this obviously the same input information

04:37:42.920 --> 04:37:47.320
we have before, which is 160 by 160 by three. And that will make the prediction for you.

04:37:47.960 --> 04:37:52.600
So that's kind of the thing there. I was getting an OS error just because I hadn't saved this

04:37:52.600 --> 04:37:56.040
previously. But that's how you save and load models, which I think is important when you're

04:37:56.040 --> 04:38:00.680
doing very large models. So when you fit this, feel free to change the epochs to be something

04:38:00.680 --> 04:38:06.200
slower if you'd like, again, right, this takes a long time to actually end up running. But you

04:38:06.200 --> 04:38:10.920
can see that the accuracy increases pretty well, exponentially, exponentially from when we didn't

04:38:10.920 --> 04:38:15.560
even have that classifier on it. Now, the last thing that I want to talk about is object detection,

04:38:15.560 --> 04:38:18.920
I'm just going to load up a page, we're not going to do any examples, I'm just going to give you a

04:38:18.920 --> 04:38:23.000
brief introduction, because we're kind of running out of time for this module, because you can use

04:38:23.000 --> 04:38:27.640
TensorFlow to do object detection and recognition, which is kind of cool. So let's get into that

04:38:27.640 --> 04:38:32.440
now. Okay, so right now, I'm on a GitHub page that's built by TensorFlow here, I'm going to leave

04:38:32.440 --> 04:38:36.600
that link in the notebook where it said object detection, so you guys can look at that. But

04:38:36.600 --> 04:38:40.600
essentially, there is an API for TensorFlow that does object detection for you. And in fact,

04:38:40.600 --> 04:38:44.280
it works very well, and even gives you confidence scores. So you can see this is what you'll

04:38:44.280 --> 04:38:48.200
actually end up getting if you end up using this API. Now, unfortunately, we don't have time to go

04:38:48.200 --> 04:38:52.280
through this because this will take a good amount of time to talk about the setup and how to actually

04:38:52.280 --> 04:38:56.520
use this project properly. But if you go through this documentation, you should be able to figure

04:38:56.520 --> 04:38:59.960
it out. And now you guys are familiar with TensorFlow, and you understand some of the

04:38:59.960 --> 04:39:04.920
concepts here. This runs a very different model than what we've discussed before. Unfortunately,

04:39:04.920 --> 04:39:07.960
again, we don't have time to get into it. But just something I wanted to make clear is that you

04:39:07.960 --> 04:39:12.360
can do something like this with TensorFlow. And I will leave that resource so that if you'd like

04:39:12.360 --> 04:39:16.680
to check this out, you can use it. There's also a great module in Python called facial recognition,

04:39:16.680 --> 04:39:21.000
it's not a part of TensorFlow. But it does use some kind of convolutional neural network to do

04:39:21.000 --> 04:39:25.800
facial detection and recognition, which is pretty cool as well. So I'll put that link in here.

04:39:25.880 --> 04:39:30.680
But for that, for now, that's going to be our, what is it convolutional neural network kind of

04:39:30.680 --> 04:39:36.200
module. So I hope that says cleared some things up on how deep vision works and how convolutional

04:39:36.200 --> 04:39:40.680
neural networks work. I know I haven't gone into crazy examples of what I've shown you some different

04:39:40.680 --> 04:39:46.360
techniques that hopefully you'll go look up kind of on your own and really dive into. Because now

04:39:46.360 --> 04:39:50.360
you have that base kind of domain knowledge where you're going to be able to follow along with the

04:39:50.360 --> 04:39:54.760
tutorial and understand exactly what to do. And if you want to create your own model, so long as

04:39:54.760 --> 04:39:59.480
you can get enough sufficient training data, you can load that training data into your computer,

04:39:59.480 --> 04:40:04.760
put that in a NumPy array. Then what you can do is create a model like we've just done using even

04:40:05.640 --> 04:40:10.440
something like the mobile nets, what was it v two that we talked about previously, if I could even

04:40:10.440 --> 04:40:15.480
get up and need to close this output. Oh my gosh, this is this is massive output here. Where is

04:40:15.480 --> 04:40:20.280
this begin to pre train model? Yeah, mobile net v two, you can use the base of that, and then add

04:40:20.280 --> 04:40:24.920
your own classifier on do a similar thing to what I've done with that dense neuron and that global

04:40:24.920 --> 04:40:28.680
average layer. And hopefully you should get a decent result from that. So this is just showing

04:40:28.680 --> 04:40:33.720
you what you can do. Obviously, you can pick a different base layer, depending on what kind of

04:40:33.720 --> 04:40:37.560
problem you're trying to solve. So anyways, that has been convolutional neural networks. I hope

04:40:37.560 --> 04:40:41.480
you enjoyed that module. Now we're on to recurrent neural networks, which is actually going to be

04:40:41.480 --> 04:40:48.840
pretty interesting. So I'll see you in that module. Hello, everyone, and welcome to the next module

04:40:48.840 --> 04:40:52.920
in this course, which is covering natural language processing with recurrent neural

04:40:52.920 --> 04:40:58.440
networks. Now what we're going to be doing in this module here is first of all, first off discussing

04:40:58.440 --> 04:41:02.680
what natural language processing is, which I guess I'll start with here. Essentially, for those of

04:41:02.680 --> 04:41:08.520
you that don't know, natural language processing or NLP for short, is the field or discipline in

04:41:08.520 --> 04:41:14.200
computing or machine learning that deals with trying to understand natural or human languages.

04:41:14.200 --> 04:41:18.280
Now, the reason we call them natural is because these are not computer languages or

04:41:18.280 --> 04:41:24.280
programming languages, per se. And actually, computers are quite bad at understanding textual

04:41:24.280 --> 04:41:29.400
information and human languages. And that's why we've come up with this entire discipline focused

04:41:29.400 --> 04:41:33.880
on how they can do that. So we're going to do that using something called recurrent neural

04:41:33.880 --> 04:41:38.920
networks. But some examples of natural language processing would be something like spell check,

04:41:38.920 --> 04:41:45.400
autocomplete voice assistance, translation between languages, there's all different kinds of things,

04:41:45.480 --> 04:41:50.920
chatbots, but essentially anything that deals with textual data. So you like paragraphs,

04:41:50.920 --> 04:41:56.360
sentences, even words, that is probably going to be classified under natural language processing

04:41:56.360 --> 04:42:01.000
in terms of doing some kind of machine learning stuff with it. Now, we are going to be talking

04:42:01.000 --> 04:42:05.000
about a different kind of neural network in this series called recurrent neural networks.

04:42:05.000 --> 04:42:09.720
Now, these are very good at classifying and understanding textual data. And that's why we'll

04:42:09.720 --> 04:42:14.440
be using them. But they are fairly complex. And there's a lot of stuff that goes into them.

04:42:14.440 --> 04:42:18.840
Now, in the interest of time, and just not knowing a lot of your math background, I'm not

04:42:18.840 --> 04:42:23.880
going to be getting into the exact details of how this works on a lower level, like I did when I

04:42:23.880 --> 04:42:29.000
explained kind of our, I guess, fundamental learning algorithms, which are a bit easier to grasp,

04:42:29.000 --> 04:42:33.080
and even just regular neural networks in general, we're going to be kind of skipping over that and

04:42:33.080 --> 04:42:39.480
really focusing on why this works the way it does, rather than how and when you should use this.

04:42:40.200 --> 04:42:43.560
And then maybe understanding a few of the different kinds of layers that have to do with

04:42:43.640 --> 04:42:47.480
recurrent neural networks. But again, we're not going to get into the math. If you'd like to

04:42:47.480 --> 04:42:51.160
learn about that, there will be some sources at the bottom of the guide. And you can also just

04:42:51.160 --> 04:42:55.320
look up recurrent neural networks, and you'll find lots of resources that explain all of the

04:42:55.320 --> 04:43:00.840
fancy math that goes on behind them. Now, the exact applications and kind of things will be working

04:43:00.840 --> 04:43:06.120
towards here is sentiment analysis. That's the first kind of task or thing we're going to do.

04:43:06.120 --> 04:43:10.200
We're actually going to use movie reviews and try to determine whether these movie reviews are

04:43:10.280 --> 04:43:15.720
positive or negative by performing sentiment analysis on them. Now, if you're unfamiliar

04:43:15.720 --> 04:43:19.320
with sentiment analysis, we'll talk about it more later, but essentially means trying to

04:43:19.320 --> 04:43:24.120
determine how positive or negative a sentence or piece of text is, which you can see why that would

04:43:24.120 --> 04:43:30.120
be useful for movie reviews. Next, we're going to do character slash text generation. So essentially,

04:43:30.120 --> 04:43:34.920
we're going to use a natural language processing model, I guess, if you want to call it that,

04:43:35.560 --> 04:43:40.920
to generate the next character in a sequence of text for us. And we're going to use that model a

04:43:40.920 --> 04:43:46.120
bunch of times to actually generate an entire play. Now, I know this seems a little bit ridiculous

04:43:46.120 --> 04:43:50.520
compared to some of the trivial examples we've done before. This will be quite a bit more code

04:43:50.520 --> 04:43:53.800
than anything we've really looked at yet. But this is very cool, because we're going to actually

04:43:53.800 --> 04:43:58.680
going to train a model to learn how to write a play. That's literally what it's going to do.

04:43:58.680 --> 04:44:02.760
It's going to read through a play, I believe it's Romeo and Juliet. And then we're going to give it

04:44:02.840 --> 04:44:07.480
a little prompt when we're actually using the model and say, okay, this is the first part of

04:44:07.480 --> 04:44:11.640
the play, write the rest of it. And then it will actually go and write the rest of the characters

04:44:11.640 --> 04:44:15.560
in the play. And we'll see that we can get something that's pretty good using the techniques that

04:44:15.560 --> 04:44:20.760
we'll talk about. So the first thing that I want to do is talk about data. So I'm going to hop onto

04:44:20.760 --> 04:44:26.040
my drawing tablet here. And we're going to compare the difference between textual data and numeric

04:44:26.040 --> 04:44:30.360
data, like we've seen before, and why we're going to have to employ some pretty complex and

04:44:30.360 --> 04:44:34.760
different steps to turn something like this, you know, a block of text into some meaningful

04:44:34.760 --> 04:44:39.560
information that our neural networks actually going to be actually going to be able to understand

04:44:39.560 --> 04:44:43.720
and process. So let's go ahead and get over to that. Okay, so now we're going to get into the

04:44:43.720 --> 04:44:49.960
problem of how we can turn some textual data into numeric data that we can feed to our neural

04:44:49.960 --> 04:44:54.680
network. Now, this is a pretty interesting problem. And we'll kind of go through as we start going

04:44:54.680 --> 04:44:58.360
through it, you should see why this is interesting and why there's a lot of difficulties with the

04:44:58.360 --> 04:45:02.200
different methods that we pick. But the first method that I want to talk about is something

04:45:02.200 --> 04:45:07.800
called bag of words, in terms of how we can kind of encode and pre process text into integers.

04:45:07.800 --> 04:45:12.920
Now, obviously, I'm not the first person to come up with this bag of words is a very famous,

04:45:12.920 --> 04:45:18.200
almost I want to say algorithm or method of converting textual data to numeric data, although

04:45:18.200 --> 04:45:23.400
it is pretty flawed and only really works for simple tasks. And we're going to understand why

04:45:23.400 --> 04:45:28.280
in a second. So we're going to call this bag of words. Essentially, what bag of words says is

04:45:28.280 --> 04:45:31.880
what we're going to do is we're going to look at our entire training data set, right, because we're

04:45:31.880 --> 04:45:36.120
going to be turning our training data set into a form the network can understand. And we're going

04:45:36.120 --> 04:45:42.280
to create a dictionary lookup of the vocabulary. Now, what I mean by that is we're going to say

04:45:42.280 --> 04:45:49.080
that every single unique word in our data set is the vocabulary, right? That's the amount of words

04:45:49.080 --> 04:45:53.320
that the model is expected to understand, because we're, you know, going to show all those words

04:45:53.320 --> 04:45:57.400
to the model. And we're going to say that every single one of these words. So every single one

04:45:57.400 --> 04:46:02.440
of these words in the vocabulary is going to be placed in a dictionary. And beside that, we're

04:46:02.440 --> 04:46:07.320
going to have some integer that represents it. So for example, maybe the vocabulary of our data

04:46:07.320 --> 04:46:15.960
set is the words, you know, I, a, maybe Tim, maybe day, me, right, we're gonna have a bunch of arbitrary

04:46:15.960 --> 04:46:20.520
words, I'll just put dot dot dot to show that this kind of goes to the length of the vocabulary.

04:46:20.520 --> 04:46:23.800
And every single one of these words will be placed in a dictionary, which we're going to just

04:46:23.800 --> 04:46:29.320
going to call kind of our lookup table or word index table. And we're going to have a number

04:46:29.320 --> 04:46:35.160
that represents every single one of them. So you can imagine that in very large data sets, we're

04:46:35.160 --> 04:46:40.280
going to have, you know, tens of thousands of hundreds of thousands, sometimes even maybe millions

04:46:40.280 --> 04:46:44.680
of different words, and they're all going to be encoded by different integers. Now, the reason

04:46:44.760 --> 04:46:49.960
we call this bag of words is because what we're actually going to do when we look at a sentence

04:46:49.960 --> 04:46:55.480
is we're only going to keep track of the words that are present and the frequency of those words.

04:46:55.480 --> 04:47:01.720
And in fact, what we'll do well is we'll create what we call a bag. And whenever we see a word

04:47:01.720 --> 04:47:08.040
appears, we'll simply add its number into the bag. So if I have a sentence like, you know, I

04:47:09.000 --> 04:47:16.360
am Tim day, day, I'm just going to do like a random sentence like that. Then what we're going

04:47:16.360 --> 04:47:21.000
to do is every time we see a word, we're going to take its number and throw it into the bag. So

04:47:21.000 --> 04:47:28.360
we're going to say, all right, I, that's zero, and that's one, Tim, that's two, day, that's three,

04:47:29.160 --> 04:47:33.960
that's three again. And notice that what's happening here is we're losing the ordering

04:47:34.040 --> 04:47:38.920
of these words, but we're just keeping track of the frequency. Now, there's lots of different

04:47:38.920 --> 04:47:44.200
ways to kind of format how we want to do bag of words. But this is the basic idea, I'm not going

04:47:44.200 --> 04:47:47.880
to go too far in because we're not actually really going to use this technique. But essentially,

04:47:47.880 --> 04:47:52.840
you lose the ordering in which words appear, but you just keep track of the frequency and what

04:47:52.840 --> 04:47:57.640
words appear. So this could be very useful when you're looking, you know, you're doing very simple

04:47:57.640 --> 04:48:03.400
tasks where the presence of a certain word will really influence the kind of type of sentence

04:48:03.480 --> 04:48:07.240
that it is, or the meaning that you're going to get from it. But when we're looking at more

04:48:07.240 --> 04:48:12.040
complex input, where, you know, different words have different meanings, depending on where they

04:48:12.040 --> 04:48:18.360
are in a sentence, this is a pretty flawed way to encode this data. Now, I won't go much further

04:48:18.360 --> 04:48:23.480
into this. This is not the exact way the bag of words works. But I just wanted to show you

04:48:23.480 --> 04:48:28.840
kind of an idea here, which is we just encode every single unique word by an integer. And then we

04:48:28.840 --> 04:48:33.800
don't even really care about where these words are. We just throw them into a bag and we say,

04:48:33.800 --> 04:48:38.040
All right, you know, this is our bag right here that I'm doing the arrow to, we'll just throw in

04:48:38.040 --> 04:48:42.680
three as many times as you know, the word day appears, we'll throw in one as many times as the

04:48:42.680 --> 04:48:49.640
word, I guess, am appears, and so on and so forth. And then what will happen is we'll feed this bag

04:48:49.640 --> 04:48:53.720
to our neural network in some form, depending on the network that we're using. And it will just

04:48:53.720 --> 04:48:57.560
look at and say, Okay, so I have all these different numbers, that means these words are present

04:48:57.640 --> 04:49:01.640
and try to do something with it. Now, I'm going to show you a few examples of where this kind of

04:49:01.640 --> 04:49:05.320
breaks down, but just understand that this is how this works. This is the first technique called

04:49:05.320 --> 04:49:10.360
bag of words, which again, we will not be using. So what happens when we have a sentence where

04:49:10.360 --> 04:49:16.200
the same word conveys a very different meaning, right? And I'm actually, I think I have an example

04:49:16.200 --> 04:49:22.600
on the slides here that I'll go into. Yes, select this. Okay, so for our bag of words technique,

04:49:22.600 --> 04:49:27.160
which we can kind of see here, maybe we'll go through it. Let's consider the two sentences

04:49:27.560 --> 04:49:31.800
where are they here? I thought the movie was going to be bad, but it was actually amazing.

04:49:31.800 --> 04:49:36.200
And I thought the movie was going to be amazing, but it was actually bad, right? So consider these

04:49:36.200 --> 04:49:39.560
two sentences. Now, I know you guys already know what I'm going to get at. But essentially, these

04:49:39.560 --> 04:49:45.320
sentences use the exact same words. In fact, they use the exact same number of words, the exact same

04:49:45.320 --> 04:49:51.800
words in total. And well, they have a very different meaning. With our bag of words technique,

04:49:51.800 --> 04:49:57.080
we're actually going to encode these two sentences using the exact same representation. Because

04:49:57.160 --> 04:50:02.440
remember, all we do is we care about the frequency and what words appear, but we don't care about

04:50:02.440 --> 04:50:08.680
where they appear. So we end up losing that meaning from the sentence, because the sentence I thought

04:50:08.680 --> 04:50:12.920
the movie was going to be bad, but it was actually amazing is encoded and represented by the same

04:50:12.920 --> 04:50:17.640
thing as this sentence is. So that, you know, obviously is an issue that's a flaw. And that's

04:50:17.640 --> 04:50:22.760
one of the reasons why bag of words is not very good to use, because we lose the context of the

04:50:22.760 --> 04:50:27.560
words within the sentence, we just pick up the frequency and the fact that these words exist.

04:50:27.560 --> 04:50:31.080
So that's the first technique that's called bag of words. I've actually written a little

04:50:31.080 --> 04:50:35.560
function here that does this for us. This is not really the exact way that we would write a bag

04:50:35.560 --> 04:50:41.720
of words function. But you kind of get the idea that when I have a text, this is a test to see

04:50:41.720 --> 04:50:47.560
if this test will work is test a I just did a bunch of random stuff. So we can see what I'm

04:50:47.560 --> 04:50:51.560
doing is printing out the bag, which I get from this function. And you guys can look at this if

04:50:51.560 --> 04:50:55.400
you kind of want to see how this works. And essentially, what it tells us is the word one

04:50:55.400 --> 04:51:02.200
appears two times. Yes, the word two appears three times the word three appears three times word

04:51:02.200 --> 04:51:07.320
four appears three times five ones, six, one, seven ones. So on, that's the information we get

04:51:07.320 --> 04:51:12.040
from our bag, right from that encoding. And then if we look up here, this is our vocabulary. So

04:51:12.040 --> 04:51:16.440
this stands for one is is two is three so on. And you can kind of get the idea from that.

04:51:17.240 --> 04:51:22.120
So that is how we would use bag of words, right? If we did an encoding kind of like this, that's

04:51:22.120 --> 04:51:26.920
what that does. And that's one way of encoding it. Now, I'm going to go back and we'll talk about

04:51:26.920 --> 04:51:31.160
another method here as well, actually, a few more methods before we get into anything further.

04:51:31.160 --> 04:51:35.400
All right, so I'm sure a lot of you were looking at the previous example I did. And you saw the

04:51:35.400 --> 04:51:41.480
fact that what I did was completely remove the idea of kind of sequence or ordering of words,

04:51:41.480 --> 04:51:44.760
right? And what I did was just throw everything in a bag. And I said, All right, we're just going

04:51:44.760 --> 04:51:51.160
to keep track of the fact that we have, you know, three A's, or we have four, those or seven, Tim's

04:51:51.160 --> 04:51:54.920
right. And we're going to just going to lose the fact that, you know, words come after one

04:51:54.920 --> 04:51:58.760
each other, we're going to lose their ordering in the sentence. And that's how we're going to

04:51:58.760 --> 04:52:03.320
encode it. And I'm sure a lot of you were saying, Well, why don't we just not lose the ordering of

04:52:03.320 --> 04:52:07.800
those words? We'll just encode every single word with an integer and just leave it in its space

04:52:07.800 --> 04:52:11.560
where it would have been in the original string. Okay, good idea. So what you're telling me to

04:52:11.560 --> 04:52:17.560
do is something like this, you know, Tim is here will be our sentence. Let's say we encode the word

04:52:17.560 --> 04:52:22.520
Tim with zero is as one, here's two. And then that means our translation goes zero, one, two.

04:52:23.160 --> 04:52:28.360
And that means, right, if we have a translation, say, like two, one, zero, even though these use

04:52:28.360 --> 04:52:34.520
the exact same number of words and exact same representation for all these words. Well, this

04:52:34.520 --> 04:52:39.080
is a different sentence. And our model should be able to tell that because these words come in a

04:52:39.080 --> 04:52:43.080
different order. And to you, good point, if you made that point, but I'm going to discuss where

04:52:43.080 --> 04:52:48.200
this falls apart as well. And why we're not going to use this method. So although this does solve

04:52:48.200 --> 04:52:53.080
the problem I talked about previously, where we're going to kind of lose out on the context of a word,

04:52:53.080 --> 04:52:56.920
there's still a lot of issues with this and they come, especially when you're dealing with very

04:52:56.920 --> 04:53:01.960
large vocabularies. Now, let's take an example where we actually have a vocabulary of say 100,000

04:53:01.960 --> 04:53:07.800
words. And we know that that means we're going to have to have 100,000 unique mappings from words

04:53:07.800 --> 04:53:14.440
to integers. So let's say our mappings are something like this, one maps to the string happy,

04:53:15.160 --> 04:53:24.200
the word happy, right, to maps to sad. And let's say that the string 100,000 or the number 100,000

04:53:24.200 --> 04:53:30.120
maps to the word, I don't know, let's say good. Now, we know as humans, by kind of just thinking

04:53:30.120 --> 04:53:34.440
about, let's consider the fact that we're going to try to classify sentences as a positive or

04:53:34.440 --> 04:53:39.000
negative. So sentiment analysis, that the words happy and good in that regard, you know, sentiment

04:53:39.000 --> 04:53:43.000
analysis are probably pretty similar words, right? And then if we were going to group these words,

04:53:43.000 --> 04:53:47.000
we'd probably put them in a similar group, we'd classify them as similar words, we get probably

04:53:47.000 --> 04:53:51.560
interchange them in a sentence, and it wouldn't change the meaning a whole ton. I mean, it might,

04:53:51.560 --> 04:53:56.600
but it might not as well. And that we could say these are kind of similar. But our model or our

04:53:56.600 --> 04:54:03.400
encoder, right, whatever we're doing to translate our text into integers here, has decided that 100,000

04:54:03.480 --> 04:54:07.080
is going to represent good, and one is going to represent happy. And well, there's an issue with

04:54:07.080 --> 04:54:12.360
that, because that means when we pass in something like one, or 100,000 to our model, it's going to

04:54:12.360 --> 04:54:17.800
have a very difficult time determining the fact that one and 100,000, although they're 99,999,

04:54:19.640 --> 04:54:24.200
kind of units apart, are actually very similar words. And that's the issue we get into when we

04:54:24.200 --> 04:54:28.920
do something like this is that the numbers we decide to pick to represent each word are very

04:54:28.920 --> 04:54:34.600
important. And we don't really have a way of being able to look at words group them and saying,

04:54:34.600 --> 04:54:39.720
okay, well, we need to put all of the happy words and the range like zero to 100, all of the like

04:54:39.720 --> 04:54:44.440
adjectives in this range, we don't really have a way to do that. And this gets even harder for our

04:54:44.440 --> 04:54:48.680
model when we have these arbitrary mappings, right? And then we have something like two in

04:54:48.680 --> 04:54:54.040
between where two is very close to one, right? Yet these words are complete opposites. In fact,

04:54:54.040 --> 04:54:58.280
I'd say they're probably polar opposites, our model trying to learn that the difference between one

04:54:58.360 --> 04:55:03.320
and two is actually way larger than the difference between one and 100,000 is going to be very

04:55:03.320 --> 04:55:08.840
difficult. And say it's even able to do that, as soon as we throw in the mapping 900, right,

04:55:08.840 --> 04:55:15.000
the 99,900, we put that as bad. Well, now it gets even more difficult, because it's now like, okay,

04:55:15.000 --> 04:55:19.080
what the range is this big, then that means these words are actually very similar. But then you

04:55:19.080 --> 04:55:23.320
throw another word in here like this, and it messes up the entire system. So that's kind of

04:55:23.320 --> 04:55:27.000
what I want to show is that that's where this breaks apart on these large vocabularies. And

04:55:27.080 --> 04:55:31.800
that's why I'm going to introduce us now to another concept called word embeddings. Now,

04:55:31.800 --> 04:55:36.680
what word embeddings does is essentially try to find a way to represent words that are similar

04:55:36.680 --> 04:55:42.360
using very similar numbers. And in fact, what a word embedding is actually going to do, I'll talk

04:55:42.360 --> 04:55:49.560
about this more in detail as we go on, is classify or translate every single one of our words into

04:55:49.560 --> 04:55:55.320
a vector. And that vector is going to have some, you know, n amount of dimensions. Usually, we're

04:55:55.320 --> 04:56:01.320
going to use something like 64, maybe 128 dimensions for each vector. And every single component of

04:56:01.320 --> 04:56:07.160
that vector will kind of tell us what group it belongs to or how similar it is to other words.

04:56:07.160 --> 04:56:10.680
So let me give you an idea of what I mean. So we're going to create something called the word

04:56:10.680 --> 04:56:15.560
embeddings. Now, don't ask why it's called embeddings, I don't know the exact reason, but I

04:56:15.560 --> 04:56:19.960
believe it's to have has to do something with the fact that they're vectors. And let's just say we

04:56:19.960 --> 04:56:23.800
have a 3d plane like this. And we've already kind of looked at what vectors are before. So I'll skip

04:56:23.880 --> 04:56:29.800
over explaining them. And what we're going to do is take some word. So let's say we have the word

04:56:29.800 --> 04:56:34.840
good. And instead of picking some integer to represent it, we're going to pick some vector,

04:56:34.840 --> 04:56:38.680
which means we're going to draw some vector in this 3d space. Actually, let's make this a different

04:56:38.680 --> 04:56:46.360
color. Let's make this vector say red, like this. And this vector represents this word good. And in

04:56:46.440 --> 04:56:53.240
this case, we'll say we have x one, x two, x three is our dimensions, which means that every single

04:56:53.240 --> 04:56:58.120
word in our data set will be represented by three coordinates. So one vector with three different

04:56:58.120 --> 04:57:03.960
dimensions, where we have x one x two and x three. And our hope is that by using this word

04:57:03.960 --> 04:57:08.680
embeddings layer, and we'll talk about how it accomplishes this in a second, is that we can

04:57:08.680 --> 04:57:13.640
have vectors that represent very similar words being very similar, which means that you know,

04:57:13.640 --> 04:57:19.960
if we have the vector good here, we would hope the vector happy from our previous example, right,

04:57:19.960 --> 04:57:25.160
will be a vector that points in a similar direction to it. That is kind of a similar looking thing

04:57:25.160 --> 04:57:30.360
where the angle between these two vectors, right, and maybe I'll draw it here so we can see is small

04:57:30.360 --> 04:57:35.480
so that we know that these words are similar. And then we would hope that if we had a word that

04:57:35.480 --> 04:57:39.800
was much different, maybe say like the word bad, that that would point in a different direction,

04:57:39.800 --> 04:57:44.680
the vector that represents it. And that that would tell our model, because the angle between

04:57:44.680 --> 04:57:49.000
these two vectors is so big, that these are very different words, right? Now, in theory,

04:57:49.000 --> 04:57:53.320
does the embedding work layer work like this, you know, not always, but this is what it's

04:57:53.320 --> 04:57:59.960
trying to do is essentially pick some representation in a vector form for each word. And then these

04:57:59.960 --> 04:58:03.880
vectors, we hope if they're similar words, they're going to be pointing in a very similar

04:58:03.880 --> 04:58:08.920
direction. And that's kind of the best explanation of a word embeddings layer I can give you.

04:58:08.920 --> 04:58:13.400
Now, how do we do this, though, how do we actually, you know, go from word to vector,

04:58:14.440 --> 04:58:19.160
and have that be meaningful? Well, this is actually what we call a layer. So word embeddings

04:58:19.160 --> 04:58:23.560
is actually a layer, and it's something we're going to add to our model. And that means that

04:58:23.560 --> 04:58:28.920
this actually learns the embeddings for our words. And the way it does that is by trying to kind of

04:58:28.920 --> 04:58:34.760
pick out context in the sentence and determine based on where a word is in a sentence, kind of what

04:58:34.840 --> 04:58:40.680
it means, and then encodes it doing that. Now, I know that's kind of a rough explanation to give

04:58:40.680 --> 04:58:44.760
to you guys, I don't want to go too far into word embeddings in terms of the math, because I don't

04:58:44.760 --> 04:58:49.080
want to get, you know, waste our time or get too complicated if we don't need to. But just understand

04:58:49.080 --> 04:58:53.400
that our word embeddings are actually trained, and that the model actually learns these word

04:58:53.400 --> 04:58:59.160
embeddings as it goes. And we hope that by the time it's looked at enough training data, it's

04:58:59.160 --> 04:59:04.600
determined really good ways to represent all of our different words, so that they make sense to

04:59:04.600 --> 04:59:09.160
our model in the further layers. And we can use pre trained word embedding layers, if we'd like,

04:59:09.160 --> 04:59:14.360
just like we use that pre trained convolutional base in the previous section. And we might actually

04:59:14.360 --> 04:59:17.800
end up doing that. Actually, probably not in this tutorial, but it is something to consider that

04:59:17.800 --> 04:59:22.280
you can do that. So that's how word embeddings work. This is how we encode textual data. And this

04:59:22.280 --> 04:59:26.920
is why it's so important that we kind of consider the way that we pass information to our neural

04:59:26.920 --> 04:59:31.960
network, because it makes a huge difference. Okay, so now that we've talked about kind of the form

04:59:32.040 --> 04:59:35.960
that we need to get our data in before we can pass it further in the neural network, right,

04:59:35.960 --> 04:59:40.600
before it can get past that embedding layer, before it can get put in, put into any dense

04:59:40.600 --> 04:59:45.000
neurons, before we can even really do any math with it, we need to turn it into numbers, right,

04:59:45.000 --> 04:59:50.040
our textual data. So now that we know that it's time to talk about recurrent neural networks.

04:59:50.040 --> 04:59:54.280
Now recurrent neural networks are the type of networks we use when we process textual data,

04:59:54.280 --> 04:59:58.600
typically, you don't always have to use these, but they are just the best for natural language

04:59:58.600 --> 05:00:03.080
processing. And that's why they're kind of their own class, right? Now, the fundamental difference

05:00:03.080 --> 05:00:07.320
between a recurrence neural network and something like a dense neural network, or a convolutional

05:00:07.320 --> 05:00:13.400
neural network, is the fact that it contains an internal loop. Now, what this really means is

05:00:13.400 --> 05:00:18.680
that the recurrent neural network does not process our entire data at once. So it doesn't

05:00:18.680 --> 05:00:24.360
process the entire training example, or the entire input to the model at once, what it does is

05:00:24.440 --> 05:00:30.920
processes it at different time steps, and maintains what we call an internal memory, and kind of an

05:00:30.920 --> 05:00:37.240
internal state, so that when it looks at a new input, it will remember what it's seen previously

05:00:37.240 --> 05:00:42.600
and treat that input based on kind of the context or the understanding it's already developed.

05:00:42.600 --> 05:00:47.240
Now, I understand that this doesn't make any sense right now. But with a dense neural network,

05:00:47.240 --> 05:00:52.680
or the neural networks we looked at so far, we call those something called feed forward neural

05:00:52.680 --> 05:00:58.280
networks. What that means is we give all of our data to it at once, and we pass that data from

05:00:58.280 --> 05:01:03.880
left to right, or I guess for you guys from left to right. So we give all of the information, you

05:01:03.880 --> 05:01:07.960
know, we would pass those through the convolutional layer to start, maybe we have passed them through

05:01:07.960 --> 05:01:13.000
dense neurons, but they get given all of the info. And then that information gets translated

05:01:13.000 --> 05:01:18.200
through the network to the very end, again, from left to right. Whereas here, with recurrent neural

05:01:18.200 --> 05:01:23.400
networks, we actually have a loop, which means that we don't feed the entire textual data at

05:01:23.400 --> 05:01:29.960
once, we actually feed one word at a time, it processes that word, generate some output based

05:01:29.960 --> 05:01:35.800
on that word, and uses the internal memory state that it's keeping track of to do that as part of

05:01:35.800 --> 05:01:40.920
the calculation. So essentially, the reason we do this is because just like humans, when we, you

05:01:40.920 --> 05:01:47.240
know, look at text, we don't just take a photo of this text and process it all at once, we read it

05:01:47.320 --> 05:01:52.920
left to right, word to word. And based on the words that we've already read, we start to slowly

05:01:52.920 --> 05:01:59.160
develop an understanding of what we're reading, right? If I just read the word now, that doesn't

05:01:59.160 --> 05:02:04.520
mean much to me, if I just read the word in code, that doesn't mean much. Whereas if I read the entire

05:02:04.520 --> 05:02:09.480
sentence, now that we've learned a little bit about how we can encode text, I start to develop

05:02:09.480 --> 05:02:14.680
an understanding about what this next word means, based on the previous words before it, right?

05:02:14.760 --> 05:02:18.200
And that's kind of the point here is that this is what a recurrent neural network is going to do

05:02:18.200 --> 05:02:24.440
for us. It's going to read one word at a time, and slowly start building up its understanding

05:02:24.440 --> 05:02:29.960
of what the entire textual data means. And this works in kind of a more complicated sense than

05:02:29.960 --> 05:02:34.840
that will draw it out a little bit. But this is kind of what would happen if we on, I guess,

05:02:34.840 --> 05:02:39.240
unraveled a recurrent layer, because recurrent neural network, yes, it has a loop in it. But

05:02:39.240 --> 05:02:43.480
really the recurrent aspect of a neural network is the layer that implements this

05:02:44.040 --> 05:02:49.000
recurrent functionality with a loop. Essentially, what we can see here is that if we're saying x

05:02:49.000 --> 05:02:55.960
is our input and h is our output, x t is going to be our input at time t, whereas h t is going to

05:02:55.960 --> 05:03:01.880
be our output at time t. If we had a text of say length four, so four words, like we've encoded

05:03:01.880 --> 05:03:07.320
them into integers now at this point, the first input at time zero will be the first word into

05:03:07.320 --> 05:03:12.680
our network, right, or the first word that this layer is going to see. And the output at that time

05:03:12.680 --> 05:03:17.720
is going to be our current understanding of the entire text after looking at just that one word.

05:03:18.600 --> 05:03:25.080
Next, what we're going to do is process input one, which will be the next word in the sentence.

05:03:25.080 --> 05:03:30.680
But we're going to use the output from the previous kind of computation or the previous iteration

05:03:31.400 --> 05:03:36.360
to do this. So we're going to process this word in combination with what we've already seen,

05:03:36.360 --> 05:03:40.040
and then have a new output, which hopefully should now give us an understanding of what

05:03:40.120 --> 05:03:46.280
those two words mean. Next, we'll go to the third word. And so forth, and slowly start building

05:03:46.280 --> 05:03:51.720
our understanding of what the entire textual data means by building it up one by one. The reason

05:03:51.720 --> 05:03:56.360
we don't pass the entire sequence at once is because it's very, very difficult to just kind

05:03:56.360 --> 05:04:01.880
of look at this huge blob of integers and figure out what the entire thing means. If we can do it

05:04:01.880 --> 05:04:06.840
one by one and understand the meaning of specific words based on the words that have came before

05:04:06.920 --> 05:04:11.320
it and start learning those patterns, that's going to be a lot easier for a neural network to deal

05:04:11.320 --> 05:04:16.120
with than just passing it all at once, looking at it and trying to get some output. And that's

05:04:16.120 --> 05:04:20.600
why we have these recurrent layers, there's a few different types of them. And I'm going to go

05:04:20.600 --> 05:04:25.080
through them, and then we'll talk a little bit more in depth of how they work. So the first one

05:04:25.080 --> 05:04:30.520
is called long short term memory. And actually, in fact, before we get into this, let's, let's

05:04:30.520 --> 05:04:35.000
talk about just a first like a simple layer so that we kind of have a reference point before

05:04:35.000 --> 05:04:40.280
going here. Okay, so this is kind of the example I want to use here to illustrate however current

05:04:40.280 --> 05:04:45.960
neural network works and a more teaching style rather than what I was doing before. So essentially,

05:04:45.960 --> 05:04:51.080
the way that this works is that this whole thing that I'm drawing here, right, all of this circle

05:04:51.080 --> 05:04:56.760
stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing

05:04:56.760 --> 05:05:03.000
you kind of how this works in a series of steps. So rather than passing all the information at once,

05:05:03.080 --> 05:05:07.640
we're going to pass it as a sequence, which means that we're going to have all these different words

05:05:07.640 --> 05:05:12.520
and we're going to pass them one at a time to the kind of to the layer, right, to this recurrent

05:05:12.520 --> 05:05:17.400
layer. So we're going to start from this left side over here. So this right, you know, start over

05:05:17.400 --> 05:05:23.320
here at time step zero, that's what zero means. So time step is just, you know, the order. In this

05:05:23.320 --> 05:05:28.440
case, this is the first word. So let's say we have the sentence Hi, I am Tim, right, we've broken

05:05:28.440 --> 05:05:31.960
these down into vectors, they've been turned into their numbers, I'm just writing them here so we

05:05:31.960 --> 05:05:37.720
can kind of see what I mean in like a natural language. And they are the input to this recurrent

05:05:37.720 --> 05:05:43.320
layer. So all of our different words, right, that's how many kind of little cells we're going to draw

05:05:43.320 --> 05:05:46.920
here is how many words we have in this sequence that we're talking about. So in this case, we have

05:05:46.920 --> 05:05:52.120
four, right, four words. So that's why I've drawn four cells to illustrate that. Now what we do is

05:05:52.120 --> 05:05:58.760
that time step zero, the internal state of this layer is nothing, there's no previous output,

05:05:58.760 --> 05:06:04.760
we haven't seen anything yet, which means that this first kind of cell, which is what I'm looking

05:06:04.760 --> 05:06:09.960
at right here, what I'm drawing in this first cell is only going to look and consider this first word

05:06:09.960 --> 05:06:14.760
and kind of make some prediction about it and do something with it. We're going to pass high to

05:06:14.760 --> 05:06:19.560
this cell, some math's going to go on in here. And then what it's going to do is it's going to output

05:06:19.560 --> 05:06:25.400
some value, which, you know, tells us something about the word high, right, some numeric value,

05:06:25.400 --> 05:06:28.280
we're not going to talk about what that is, but it's going to do is going to be some output.

05:06:29.080 --> 05:06:34.200
Now, what happens is after this cell has finished processing this, so right, so this one's done,

05:06:34.200 --> 05:06:39.080
this has completed h zero, the outputs there, we'll do a check mark to say that that's done,

05:06:39.080 --> 05:06:44.840
it's finished processing, this output gets fed into actually the same thing again, we're kind of

05:06:44.840 --> 05:06:52.120
just keeping track of it. And now what we do is we process the next input, which is I, and we use

05:06:52.120 --> 05:06:57.960
the output from the previous cell to process this and understand what it means. So now, technically,

05:06:57.960 --> 05:07:03.240
we should have some output from the previous cell. So from whatever high was, right, we do some

05:07:03.240 --> 05:07:09.560
analysis on the word I, we kind of combine these things together. And that's the output of this

05:07:09.560 --> 05:07:16.040
cell is our understanding of not only the current input, but the previous input with the current

05:07:16.040 --> 05:07:21.240
input. So we're slowly kind of building up our understanding of what this word I means,

05:07:21.240 --> 05:07:25.800
based on the words we saw before. And that's the point I'm trying to get at is that

05:07:25.800 --> 05:07:30.920
this network uses what it's seen previously to understand the next thing that it sees,

05:07:30.920 --> 05:07:36.440
it's building a context is trying to understand not only the word, but what the word means,

05:07:36.440 --> 05:07:42.120
you know, in relation to what's come before it. So that's what's happening here. So then

05:07:42.120 --> 05:07:47.560
this output here, right, we get some output, we finish this, we get some output h one,

05:07:47.560 --> 05:07:53.240
h one is passed into here. And now we have the understanding of what high and I means,

05:07:53.240 --> 05:07:59.240
and we add am like that, we do some kind of computations, we build an understanding of what

05:07:59.240 --> 05:08:06.120
this sentence is. And then we get the output h two, that passes to h three. And now finally,

05:08:06.120 --> 05:08:09.720
we have this final output h three, which is going to understand hopefully,

05:08:10.520 --> 05:08:16.600
what this entire thing means. Now, this is good, this works fairly well. And this is called a

05:08:16.600 --> 05:08:22.680
simple RNN layer, which means that all we do is we take the output from the previous cell or the

05:08:22.680 --> 05:08:27.880
previous iteration, because really, all of these cells is just an iteration almost in a for loop,

05:08:27.880 --> 05:08:33.240
right, based on all the different words in our sequence. And we slowly start building to that

05:08:33.240 --> 05:08:39.720
understanding as we go through the entire sequence. Now, the only issue with this is that as we have

05:08:39.720 --> 05:08:46.520
a very long sequence, so sequences of length, say 100 or 150, the beginning of those sequences

05:08:46.520 --> 05:08:51.880
starts to kind of get lost. As we go through this, because remember, all we're doing, right,

05:08:51.880 --> 05:08:57.320
is the output from h two is really a combination of the output from h zero and h one, and then

05:08:57.320 --> 05:09:02.200
there's a new word that we've looked at. And h three is now a combination of everything before it,

05:09:02.200 --> 05:09:06.440
and this new word. So it becomes increasingly difficult for our model to actually

05:09:07.000 --> 05:09:12.040
build a really good understanding of the text in general, when the sequence gets long, because

05:09:12.040 --> 05:09:16.920
it's hard for it to remember what it's seen at the very beginning, because that is now so insignificant,

05:09:16.920 --> 05:09:21.800
there's been so many outputs tacked on to that, that it's hard for it to go back and see that if

05:09:21.800 --> 05:09:26.520
that makes any sense. Okay, so what I'm going to do now is try to explain the next layer we're

05:09:26.520 --> 05:09:32.120
going to look at, which is called LSTM. So the previous layer we just looked at the recurrent

05:09:32.120 --> 05:09:36.680
layer was called a simple RNN layer. So simple recurrent neural network layer, whatever you

05:09:36.680 --> 05:09:41.240
want to call it, right, simple recurrent layer. Now we're going to talk about the layer, which is

05:09:41.240 --> 05:09:47.640
LSTM, which stands for long, short term memory. Now, long and short are hyphenated together.

05:09:47.640 --> 05:09:51.320
But essentially, what we're doing, and it just gets a little bit more complex, but I won't go

05:09:51.320 --> 05:09:58.120
into the math, is we add another component that keeps track of the internal state. So right now,

05:09:58.120 --> 05:10:03.320
the only thing that we were tracking as kind of our internal state as the memory for this model

05:10:03.320 --> 05:10:09.640
was the previous output. So whatever the previous output was. So for example, at time zero here,

05:10:10.280 --> 05:10:16.120
there was no previous output. So there was nothing being kept in this model. But at time one, the

05:10:16.120 --> 05:10:23.320
output from this cell right here was what we were storing. And then at cell two, the only thing we

05:10:23.400 --> 05:10:30.440
were storing was the output at time one, right? And we've lost now the output from time zero.

05:10:31.080 --> 05:10:37.000
What we're adding in long, short term memory is an ability to access the output from any

05:10:37.000 --> 05:10:42.040
previous state at any point in the future when we want it. Now, what this means is that rather

05:10:42.040 --> 05:10:47.800
than just keeping track of the previous output, we'll add all of the outputs that we've seen so far

05:10:47.800 --> 05:10:51.480
into what I'm going to call my little kind of conveyor belt, it's going to run at the top

05:10:51.480 --> 05:10:54.920
up here. I know it's kind of hard to see, but it's just what I'm highlighting. It's almost just

05:10:54.920 --> 05:11:01.080
like a lookup table that can tell us the output at any previous cell that we want. So we can kind

05:11:01.080 --> 05:11:05.080
of add things to this conveyor belt, we can pull things off, we can look at them. And this just

05:11:05.080 --> 05:11:10.280
adds a little bit of complexity to the model. It allows us to not just remember the last state,

05:11:10.280 --> 05:11:16.760
but look anywhere at any point in time, which can be useful. Now, I don't want to go into much

05:11:16.760 --> 05:11:21.240
more depth about exactly how this works. But essentially, you know, just think about the

05:11:21.240 --> 05:11:26.680
idea that as the sequence gets very long, it's pretty easy to forget the things we saw at the

05:11:26.680 --> 05:11:30.920
beginning. So if we can keep track of some of the things we've seen at the beginning, and some of

05:11:30.920 --> 05:11:35.320
the things in between on this little conveyor belt, and we can access them whenever we want,

05:11:35.320 --> 05:11:39.080
then that's going to make this probably a much more useful layer, right? We could look at the

05:11:39.080 --> 05:11:45.240
first sentence and the last sentence of a big piece of text at any point that we want and say,

05:11:45.240 --> 05:11:50.520
okay, you know, this tells us X about the meaning of this text, right? So that's what this LSTM

05:11:50.520 --> 05:11:55.000
does. Again, I don't want to go too far. We've already spent a lot of time kind of covering,

05:11:55.000 --> 05:11:58.840
you know, recurrent layers and how all this works. Anyways, if you do want to look it up,

05:11:58.840 --> 05:12:02.920
some great mathematical definitions, again, I will source everything at the bottom of this

05:12:02.920 --> 05:12:07.400
document so you can go there. But again, that's LSTM, long short term memory, that's what we're

05:12:07.400 --> 05:12:12.520
going to use for some of our examples, although simple RNN does work fairly well for shorter

05:12:12.520 --> 05:12:17.080
length sequences. And again, remember, we're treating our text as a sequence now, where we're

05:12:17.080 --> 05:12:21.720
going to feed each word into the recurrent layer, and it's going to slowly start to develop an

05:12:21.720 --> 05:12:25.960
understanding as it reads through each word, right and processes that. Okay, so now we are

05:12:25.960 --> 05:12:31.000
on to our first example, where we're going to be performing sentiment analysis on movie reviews

05:12:31.000 --> 05:12:35.560
to determine whether they are positive reviews or negative reviews. Now, we already know what

05:12:35.560 --> 05:12:39.160
sentiment means. That's essentially what I just described. So picking up, you know, whether a

05:12:39.800 --> 05:12:44.600
block of text is considered positive or negative. And for this example, we're going to be using the

05:12:44.600 --> 05:12:49.960
movie review data sets. Now, as per usual, this is based off of this TensorFlow tutorial slash

05:12:49.960 --> 05:12:55.320
guide. I found this one kind of confusing to follow in the TensorFlow website, but obviously

05:12:55.320 --> 05:12:59.640
you can follow along with that if you don't prefer that version over mine. But anyways,

05:12:59.640 --> 05:13:03.800
we're going to be talking about the movie review data set. So this data set is straight from Keras,

05:13:03.880 --> 05:13:09.960
and it contains 25,000 reviews, which are already pre processed and labeled. Now, what that means

05:13:09.960 --> 05:13:14.760
for us is that every single word is actually already encoded by an integer. And in fact,

05:13:14.760 --> 05:13:20.360
they've done kind of a clever encoding system where what they've done is said, if a character is

05:13:20.360 --> 05:13:27.320
encoded by say integer zero, that represents how common that word is in the entire data set. So

05:13:27.320 --> 05:13:31.480
if an integer was encoded, but are not interested, a word was encoded by integer three, that would

05:13:31.480 --> 05:13:35.720
mean that it is the third most common word in the data set. And in this specific data set,

05:13:35.720 --> 05:13:41.720
we have a vocabulary size of 88,584 unique words, which means that something that was

05:13:41.720 --> 05:13:47.640
classified as this. So 88,584 would be the least common word in the data set. So something to

05:13:47.640 --> 05:13:51.720
keep in mind, we're going to load in the data set and do our imports just by hitting run here.

05:13:51.720 --> 05:13:56.040
And as I've mentioned previously, you know, I'm not going to be typing this stuff out. It's just

05:13:56.040 --> 05:14:00.520
it's kind of a waste of time. I don't have all the syntax memorized. I would never expect you

05:14:00.520 --> 05:14:05.560
guys to memorize this either. But what I will do is obviously walk through the code step by step,

05:14:05.560 --> 05:14:11.640
and make sure you understand why it is that we have what we have here. Okay, so what we've done

05:14:11.640 --> 05:14:17.640
is to find the vocabulary size, the max length of a review, and the batch size. Now what we've

05:14:17.640 --> 05:14:22.440
done is just loaded in our data set by defining the vocabulary size. So this is just the words

05:14:22.440 --> 05:14:27.080
it will include. So in this case, all of them, then we have trained data, trained labels, test

05:14:27.160 --> 05:14:31.640
data, test labels. And we can look at a review and see what it looks like by doing something

05:14:31.640 --> 05:14:36.120
like this. So this is an example of our first review, we can see kind of the different encodings

05:14:36.120 --> 05:14:40.840
for all of these words. And this is what it looks like, they're already in integer form.

05:14:40.840 --> 05:14:45.560
Now, just something to note here is that the length of our reviews are not unique. So if I do

05:14:45.560 --> 05:14:50.840
the length of trained data, I guess I wouldn't say unique, but I mean, they're just all different.

05:14:50.840 --> 05:14:54.280
So the length of trained data is zero is different than the length of trained data one,

05:14:54.280 --> 05:14:57.880
right? So that's something to consider as we go through this and something we're actually going

05:14:57.880 --> 05:15:02.680
to have to handle. Okay, so more pre processing. So this is what I was talking about. If you have

05:15:02.680 --> 05:15:06.280
a look at our loaded interviews, we'll notice there are different lengths, this is an issue,

05:15:06.280 --> 05:15:10.520
we cannot pass different length data into our neural network, which is true. Therefore, we

05:15:10.520 --> 05:15:14.760
must make each review the same length. Okay, so what we're going to do for now is we're actually

05:15:14.760 --> 05:15:19.800
going to pad our sequences. Now what that means is we're going to follow this kind of step that

05:15:19.880 --> 05:15:25.880
I've talked about here. So if the review is greater than 250 words, we will trim off extra words,

05:15:25.880 --> 05:15:31.640
if the review is less than 250 words, we'll add the necessary amount of this should actually be

05:15:31.640 --> 05:15:38.360
zeros in here, let's fix this of zeros to make it equal to 250. So what that means is we're

05:15:38.360 --> 05:15:42.040
essentially going to add some kind of padding to our review. So in this case, I believe we're

05:15:42.040 --> 05:15:46.280
actually going to pad to the left side, which means that say we have a review of length, you know,

05:15:46.360 --> 05:15:51.640
200, we're going to add 50, just kind of blank words, which will represent with the index zero

05:15:51.640 --> 05:15:57.640
to the left side of the review to make it the necessary length. So that's, that's good, we'll

05:15:57.640 --> 05:16:02.200
do that. So if we look at train data and test data, what this does is we're just going to use

05:16:02.200 --> 05:16:06.680
something from Keras, which we've imported above. So we're saying from Keras dot pre processing

05:16:06.680 --> 05:16:11.000
import sequence, again, we're treating our text data as a sequence, as we've talked about,

05:16:11.000 --> 05:16:15.960
we're going to say sequence dot pad sequences, train data, and then we define the length that

05:16:15.960 --> 05:16:21.080
we want to pad it to. So that's what this will do. It will perform these steps that we've already

05:16:21.080 --> 05:16:25.560
talked about. And again, we're just going to assign test data and train data to, you know,

05:16:25.560 --> 05:16:29.880
whatever this does for us, we can pass the entire thing, it'll pad all of them for us at once.

05:16:30.600 --> 05:16:36.360
Okay, so let's run that. And then let's just have a look at say train data one now, because

05:16:36.360 --> 05:16:42.200
remember, this was like 189, right? So if we look at train data, so train underscore data one,

05:16:43.000 --> 05:16:49.000
like that, we can see that as an array with a bunch of zeros before, because that is the padding

05:16:49.000 --> 05:16:53.960
that we've employed to make it the correct length. Okay, so that's padding, that's something that

05:16:53.960 --> 05:16:58.440
we're probably going to have to do most of the time, when we feed something to our neural networks.

05:16:58.440 --> 05:17:01.640
All right, so the next step is actually to create the model. Now this model is pretty

05:17:01.640 --> 05:17:07.240
straightforward. We have an embedding layer and LSTM in a dense layer here. So the reason we've

05:17:07.240 --> 05:17:11.560
done dense with the activation function of sigmoid at the end is because we're trying to

05:17:11.560 --> 05:17:16.760
pretty much predict the sentiment of this, right? Which means that if we have the sentiment between

05:17:16.760 --> 05:17:22.840
zero and one, then if a number is greater than 0.5, we could classify that as a positive review.

05:17:22.840 --> 05:17:26.680
And if it's less than 0.5 or equal, you know, whatever you want to set the bounds at,

05:17:26.680 --> 05:17:31.240
then we could say that's a negative review. So sigmoid, as we probably might recall,

05:17:31.240 --> 05:17:35.640
squishes our values between zero and one. So whatever the value is at the end of the network

05:17:35.640 --> 05:17:39.960
will be between zero and one, which means that, you know, we can make the accurate prediction.

05:17:40.760 --> 05:17:45.160
Now here, the reason we have the embedding layer, like, well, we've already pre processed our review

05:17:45.160 --> 05:17:49.560
is even though we've pre processed this with these integers, and they are a bit more meaningful than

05:17:49.560 --> 05:17:54.040
just our random lookup table that we've talked about before, we still want to pass that to an

05:17:54.040 --> 05:17:59.640
embedding layer, which is going to find a way more meaningful representation for those numbers

05:17:59.640 --> 05:18:03.720
than just their integer values already. So it's going to create those vectors for us. And this

05:18:03.720 --> 05:18:09.560
32 is denoting the fact that we're going to make the output of every single one of our embeddings

05:18:09.640 --> 05:18:15.480
or vectors that are created 32 dimensions, which means that when we pass them to the LSTM layer,

05:18:15.480 --> 05:18:20.200
we need to tell the LSTM layer, it's going to have 32 dimensions for every single word,

05:18:20.200 --> 05:18:24.680
which is what we're doing. And this will implement that long short term memory process we talked

05:18:24.680 --> 05:18:32.280
about before, and output the final output to TF dot cares dot layers dot dense, which will tell us,

05:18:32.280 --> 05:18:38.520
you know, that's what this is, right? It'll make the prediction. So that's what this model is.

05:18:39.080 --> 05:18:43.720
We can see, give us a second to run here, the model summary, which is already printed out,

05:18:43.720 --> 05:18:47.560
we can look at the fact that the embedding layer actually has the most amount of parameters,

05:18:47.560 --> 05:18:51.000
because essentially, it's trying to figure out, you know, all these different numbers,

05:18:51.000 --> 05:18:56.120
how can we convert that into a tensor of 32 dimensions, which is not that easy to do. And

05:18:56.120 --> 05:19:00.200
this is going to be the major aspect that's being trained. And then we have our LSTM layer,

05:19:00.200 --> 05:19:04.920
we can see the parameters there. And our final dense layer, which is eight getting 33 parameters,

05:19:04.920 --> 05:19:10.680
that's because the output from every single one of these dimensions 32 plus a bias node, right,

05:19:10.680 --> 05:19:14.440
that we need. So that's what we'll get there. You can see model dot summary.

05:19:15.560 --> 05:19:20.040
We get the sequential model. Okay, so training. Alright, so now it's time to compile and train

05:19:20.040 --> 05:19:24.120
the model, you can see I've already trained mine. What I'm going to say here is if you want to speed

05:19:24.120 --> 05:19:27.480
up your training, because this will actually take a second, and we'll talk about why we pick these

05:19:27.480 --> 05:19:35.640
things in a minute is go to runtime, change runtime type, and add a hardware accelerator of GPU.

05:19:36.360 --> 05:19:40.280
What this will allow you to do is utilize a GPU while you're training, which should speed up your

05:19:40.280 --> 05:19:45.560
training by about 10 to 20 times. So I probably should have mentioned that beforehand. But you

05:19:45.560 --> 05:19:51.720
can do that. And please do for these examples. So model dot compile. Alright, so we're compiling

05:19:51.720 --> 05:19:56.280
our model, we're picking the loss function as binary cross entropy. The reason we're picking

05:19:56.280 --> 05:20:00.920
this is because this is going to essentially tell us how far away we are from the correct

05:20:01.720 --> 05:20:06.040
probability, right, because we have two different things we could be predicting. So you know, either

05:20:06.040 --> 05:20:12.200
zero or one, so positive or negative. So this will give us a correct loss for that kind of

05:20:12.200 --> 05:20:16.360
problem that we've talked about before. The optimizer, we're going to use rms prop. Again,

05:20:16.360 --> 05:20:19.480
I'm not going to discuss all the different optimizers, you can look them up if you care

05:20:19.480 --> 05:20:24.280
that much about what they do. And we're going to use metrics as ACC. One thing I will say is

05:20:24.280 --> 05:20:28.920
the optimizer is not crazy important. For this one, you could use Adam if you wanted to, and it

05:20:28.920 --> 05:20:34.040
would still work fine. My usual go to is just use the atom optimizer unless you think there's a better

05:20:34.040 --> 05:20:38.520
one to use. But anyways, that's something to mention. Okay, so finally, we will fit the model,

05:20:38.520 --> 05:20:42.440
we've looked at the syntax a lot before. So model that fit, we'll give the training data,

05:20:42.440 --> 05:20:47.720
the training labels, the epochs, and we'll do a validation split of 20%. So that's what 0.2 stands

05:20:47.720 --> 05:20:53.160
for, which means that what we're going to be doing is using 20% of the training data to actually

05:20:53.160 --> 05:20:57.880
evaluate and validate the model as we go through. And we can see that after training, which I've

05:20:57.880 --> 05:21:01.880
already done, and you guys are welcome to obviously do on your own computer, we kind of stall at an

05:21:01.880 --> 05:21:08.440
evaluation accuracy of about 88%. Whereas the model actually gets overfit to about 97 98%.

05:21:09.320 --> 05:21:13.400
So what this is telling us essentially is that we don't have enough training data, and that

05:21:13.400 --> 05:21:18.120
after we've even done just one epoch, we're pretty much stuck on the same validation accuracy,

05:21:18.120 --> 05:21:21.480
and that there's something that needs to change in the model to make it better. But for now,

05:21:21.480 --> 05:21:25.720
that's fine, we'll leave it the way that it is. Okay, so now we can look at the results. I've

05:21:25.720 --> 05:21:30.760
already did the results here, just to again, speed up some time, but we'll do the evaluation on our

05:21:30.760 --> 05:21:36.440
test data and test labels to get a more accurate kind of result here. And that tells us we have

05:21:36.440 --> 05:21:42.600
an accuracy of about 85.5%, which you know, isn't great, but it's decent considering that we didn't

05:21:42.600 --> 05:21:47.240
really write that much code to get to the point that we're at right now. Okay, so that's what

05:21:47.240 --> 05:21:50.920
we're getting. The model has been trained. Again, it's not too complicated. And now we're

05:21:50.920 --> 05:21:56.040
on to making predictions. So the idea is that now we've trained our model, and we want to actually

05:21:56.040 --> 05:22:02.280
use it to make a prediction on some kind of movie review. So since our data was pre processed, when

05:22:02.280 --> 05:22:07.160
we gave it to the model, that means we actually need to process anything we want to make a prediction

05:22:07.160 --> 05:22:11.720
on in the exact same way, we need to use the same lookup table, we need to encode it, you know,

05:22:11.720 --> 05:22:16.280
precisely the same. Otherwise, when we give it to the model, it's going to think that the words

05:22:16.280 --> 05:22:20.760
are different, and it's not going to make an accurate prediction. So what I've done here is

05:22:20.760 --> 05:22:27.960
I've made a function that will encode any text into what do you call the proper pre processed

05:22:27.960 --> 05:22:33.000
kind of integers, right, just like our training data was pre processed. That's what this function

05:22:33.000 --> 05:22:37.000
is going to do for us is pre processed some line of text. So what I've done is actually

05:22:37.000 --> 05:22:45.160
gotten the lookup table. So essentially, the mappings from IBM, I be IMDB, I could read that

05:22:45.160 --> 05:22:49.800
properly. From that data set that we loaded earlier. So let me go see if I can find where I

05:22:49.800 --> 05:22:56.360
defined IMDB, you can see up here. So keras dot data sets import IMDB, just like we loaded it in,

05:22:56.360 --> 05:23:01.160
we can also actually get all of the word indexes or that map, we can actually print this out if

05:23:01.160 --> 05:23:05.720
we want to look at what it is after. But anyways, we have that mapping, which means that all we need

05:23:05.720 --> 05:23:13.480
to do is keras dot preprocessing dot text dot text, two word sequence, what this means is give

05:23:13.720 --> 05:23:19.000
in some text convert all of that text into what we call tokens, which are just the individual

05:23:19.000 --> 05:23:23.800
words themselves. And then what we're going to do is just use a kind of for loop inside of here

05:23:23.800 --> 05:23:30.600
that says word index at word, if word in word index, L zero for word in tokens. Now what this

05:23:30.600 --> 05:23:38.040
means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary

05:23:38.040 --> 05:23:44.040
of 88,000 words, then what we'll do is replace its location in the list with that specific word,

05:23:44.760 --> 05:23:49.560
or with that specific integer that represents it, otherwise we'll put zero just to stand for,

05:23:49.560 --> 05:23:54.840
you know, we don't know what this character is. And then what we'll do is return sequence dot pad

05:23:54.840 --> 05:24:01.160
sequences, and we'll pad this token sequence, and just return actually the first index here.

05:24:01.160 --> 05:24:06.200
The reason we're doing that is because this pad sequences works on a list of sequences,

05:24:06.280 --> 05:24:10.760
so multiple sequences. So we need to put this inside a list, which means that this is going

05:24:10.760 --> 05:24:15.480
to return to us a list of lists. So we just obviously want the first entry, because we only

05:24:15.480 --> 05:24:20.120
want, you know, that one sequence that we padded. So that's how this works. Sorry, that's a bit of

05:24:20.120 --> 05:24:23.480
a mouthful to explain, but you guys can run through and print the stuff out if you want to see how

05:24:23.480 --> 05:24:27.960
all of it works specifically. But yeah, so we can run this cell and have a look at what this

05:24:27.960 --> 05:24:32.600
actually does for us on some sample text. So that maybe was just amazing. So amazing, we can see

05:24:32.600 --> 05:24:36.840
we get the output that we were kind of expecting. So integer encoded words down here, and then a

05:24:36.840 --> 05:24:41.560
bunch of zeros just for all the padding. Now, while we're at it, I decided why not we why don't we

05:24:41.560 --> 05:24:46.360
make a decode function so that if we have any movie review like this, that's in the integer

05:24:46.360 --> 05:24:50.840
form, we can decode that into the text value. So the way we're going to do that is start by

05:24:50.840 --> 05:24:55.880
reversing the word index that we just created. Now the reason for that is the word index we

05:24:55.880 --> 05:25:01.720
looked at, which is this right, goes from word to integer. But we actually now want to go from

05:25:01.720 --> 05:25:06.520
integer to word so that we can actually translate a sentence, right? So what I've done is made this

05:25:06.520 --> 05:25:11.880
decode integers function, we've set the padding key as zero, which means that if we see zero,

05:25:11.880 --> 05:25:16.040
that's really just means you know, nothing's there. We're going to create a text string,

05:25:16.040 --> 05:25:21.000
which we're going to add to. And I'm just gonna say for num in integers, integers is our input,

05:25:21.000 --> 05:25:25.240
which will be a list that looks something like this or an array, whatever you want to call it,

05:25:25.240 --> 05:25:29.480
we're gonna say if number does not equal pad. So essentially, if the number is not zero, right,

05:25:29.480 --> 05:25:34.840
it's not padding, then what we'll do is add the lookup of reverse word index num. So whatever

05:25:34.840 --> 05:25:41.080
that number is, into this new string plus a space, and then just return text colon negative one,

05:25:41.080 --> 05:25:45.000
which means return everything except the last space that we would have added. And then if I

05:25:45.000 --> 05:25:52.280
print the decode integers, we can see that this encoded thing that we have before, which looks

05:25:52.280 --> 05:25:57.880
like this gets encoded by the string that movie was just amazing. So maybe sorry, not encoded,

05:25:57.880 --> 05:26:03.000
decoded, because this was the encoded form. So that's how that works. Okay, so now it's

05:26:03.000 --> 05:26:07.400
time to actually make a prediction. So I've written a function here that will make a prediction on

05:26:07.400 --> 05:26:12.200
some piece of text as the movie review for us. And I'll just walk us through quickly how this

05:26:12.200 --> 05:26:16.360
works. And then I'll show us the actual output from our model, you know, making predictions like

05:26:16.360 --> 05:26:21.640
this. So what we say is we'll take some parameter text, which will be our movie review. And we're

05:26:21.640 --> 05:26:26.840
going to encode that text using the encode text function we've created above. So just this one

05:26:26.840 --> 05:26:32.120
right here that essentially takes our sequence of, you know, words, we get the pre processing,

05:26:32.120 --> 05:26:36.360
so turn that into a sequence, remove all the spaces, whatnot, you know, get the words,

05:26:36.360 --> 05:26:42.440
then we turn those into the integers, we have that we return that. So here we have our proper

05:26:42.440 --> 05:26:49.960
pre processed text. Then what we do is we create a blank NumPy array that is just a bunch of zeros

05:26:50.600 --> 05:26:55.320
that's in the form one to 50 or in that shape. Now the reason I'm putting in that in that shape

05:26:55.320 --> 05:27:02.440
is because the shape that our model expects is something 250, which means some number of entries,

05:27:02.440 --> 05:27:08.040
and then 250 integers representing each word, right? Because that's the length of movie review

05:27:08.680 --> 05:27:12.760
is what we've told the model is the length 250. So that's the length of the review.

05:27:13.400 --> 05:27:18.920
Then what we do is we put pred zero. So that's what's up here, equals the encoded text. So we

05:27:18.920 --> 05:27:26.520
just essentially insert our one entry into this, this array we've created. Then what we do is say

05:27:26.520 --> 05:27:33.320
modeled up predict on that array, and just return and print the result zero. Now, that's pretty

05:27:33.320 --> 05:27:37.480
much all there is to it. I mean, that's how it works. The reason we're doing result zero is

05:27:37.480 --> 05:27:43.080
because again, model is optimized to predict on multiple things, which means like I would have

05:27:43.080 --> 05:27:48.280
to do, you know, list of encoded text, which is kind of what I've done by just doing this

05:27:48.280 --> 05:27:53.720
prediction lines here, which means it's going to return to me an array of arrays. So if I want

05:27:53.720 --> 05:27:58.120
the first prediction, I need to index zero, because that will give me the prediction for

05:27:58.120 --> 05:28:03.240
our first and only entry. Alright, so I hope that makes sense. Now we have a positive review I've

05:28:03.240 --> 05:28:07.080
written and a negative review, and we're just going to compare the analysis on both of them.

05:28:07.080 --> 05:28:10.360
So that movie was so awesome. I really loved it and would watch it again, because it was

05:28:10.360 --> 05:28:14.040
amazingly great. And then that movie sucked, I hated it and wouldn't watch it again, was one

05:28:14.120 --> 05:28:18.280
of the worst things I've ever watched. So let's look at this now. And we can see the first one

05:28:18.280 --> 05:28:24.440
gets predicted at 72% positive, whereas the other one is 23% positive. So essentially what that

05:28:24.440 --> 05:28:28.280
means is that, you know, if the lower the number, the more negative we're predicting it is, the

05:28:28.280 --> 05:28:32.600
higher the number, the more positive we're predicting it is. If we wanted to not just print

05:28:32.600 --> 05:28:37.720
out this value, and instead what we wanted to do was print out, you know, positive or negative,

05:28:37.720 --> 05:28:42.600
we could just make a little if statement that says if this number is greater than 0.5, say positive,

05:28:42.600 --> 05:28:48.200
otherwise say not say negative, right? And I just want to show you that changing these reviews

05:28:48.200 --> 05:28:53.240
ever so slightly actually makes a big difference. So if I remove the word awesome, so that movie

05:28:53.240 --> 05:28:58.680
was so and then I run this, you can see that Oh, wow, this actually increases and goes up to 84%.

05:28:59.640 --> 05:29:04.120
So the presence of certain words in certain locations actually makes a big difference. And

05:29:04.120 --> 05:29:09.720
especially when we have a shorter length review, right, if we have a longer length review, it

05:29:09.800 --> 05:29:13.720
won't make that big of a difference. But even the removal of a few words here. And let's see,

05:29:13.720 --> 05:29:19.560
so the removing the word awesome changed it by almost like 10%. Right. Now if I move, so let's

05:29:19.560 --> 05:29:23.640
see if that makes a bigger difference. It makes a very little difference because it's learned,

05:29:23.640 --> 05:29:28.120
at least the model, right, that the word so doesn't really make a huge impact into

05:29:28.120 --> 05:29:33.080
the type of review. Whereas if I remove the word I, let's see if that makes a big impact,

05:29:33.080 --> 05:29:37.320
probably not right now, it goes back up to 84. So that's cool. And that's something to play with

05:29:37.320 --> 05:29:41.960
is removing certain words and seeing how much impact those actually carry. And even if I just

05:29:41.960 --> 05:29:45.800
add the word great, like would great to watch it again, just in the middle of the sentence,

05:29:45.800 --> 05:29:49.720
doesn't have to make any sense. Let's look at this here. Oh, boom, we increase like a little

05:29:49.720 --> 05:29:54.520
bit, right. And let's say if I add this movie, you really suck. Let's see if that makes a difference.

05:29:55.320 --> 05:29:59.720
No, that just reduces it like a tiny bit. So something cool, something to play with.

05:29:59.720 --> 05:30:04.280
Anyways, now let's move on to the next example. So now we're on to our last and final example,

05:30:04.360 --> 05:30:09.400
which is going to be creating a recurrent neural network play generator. Now, this is going to

05:30:09.400 --> 05:30:13.320
be the first kind of neural network we've done, that's actually going to be creating something

05:30:13.320 --> 05:30:18.680
for us. But essentially, what we're going to do is make a model that's capable of predicting the

05:30:18.680 --> 05:30:23.800
next character in a sequence. So we're going to give it some sequence as an input. And what it's

05:30:23.800 --> 05:30:28.200
going to do is just simply predict the most likely next character. Now, there's quite a bit

05:30:28.200 --> 05:30:31.880
that's going to go into this. But the way we're going to use this to predict a play is we're

05:30:31.880 --> 05:30:37.400
going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then

05:30:37.400 --> 05:30:41.800
we're going to have it. So that we'll ask the model will give it some starting prompt, some

05:30:41.800 --> 05:30:46.840
string to start with. And that'll be the first thing we pass to it, it will predict to us what

05:30:46.840 --> 05:30:52.120
the most likely next character for that sequence is. And we'll take the output from the model

05:30:52.120 --> 05:30:57.400
and feed it as the input again to the model and keep predicting sequence of characters. So

05:30:57.400 --> 05:31:02.280
keep predicting the next character from the previous output as many times as we want to

05:31:02.280 --> 05:31:06.760
generate an entire play. So we're going to have this neural network that's capable of predicting

05:31:06.760 --> 05:31:12.920
one letter at a time actually end up generating an entire play for us by running it multiple

05:31:12.920 --> 05:31:18.280
times on the previous output from the last iteration. Now, that's kind of the problem.

05:31:18.280 --> 05:31:21.640
That's what we're trying to solve. So let's go ahead and get into it and talk about what's

05:31:21.640 --> 05:31:25.800
involved in doing this. So the first thing we're going to do obviously is our imports. So from

05:31:25.800 --> 05:31:33.080
Keras dot preprocessing import sequence, import Keras, we need TensorFlow NumPy and OS. So we'll

05:31:33.080 --> 05:31:38.040
load that in. And now what we're going to do is download the file. So the data set for Romeo and

05:31:38.040 --> 05:31:43.720
Juliet, which we can get by using this line here. So Keras has this utils thing, which will allow

05:31:43.720 --> 05:31:48.920
us to get a file, save it as whatever we want. In this case, we're going to save it as Shakespeare

05:31:48.920 --> 05:31:53.960
dot txt. And we're going to get that from this link. Now, I believe this is just some like shared

05:31:53.960 --> 05:31:59.000
drive that we have access to from Keras. So we'll load that in here. And then this will

05:31:59.000 --> 05:32:04.520
simply give us the path on this machine. Because remember, this is Google Collaboratory to this

05:32:04.520 --> 05:32:09.480
text file. Now, if you want, you can actually load in your own text data. So we don't necessarily

05:32:09.480 --> 05:32:13.720
need to use the Shakespeare play, we could use anything we want. In fact, an example that I'll

05:32:13.720 --> 05:32:18.920
show later is using the B movie script. But the way you do that is run this block of code here.

05:32:19.880 --> 05:32:24.280
And you'll see that it pops up this thing for choose files, just choose a file from your

05:32:25.800 --> 05:32:31.400
local computer. And then what that will do is just save this on Google Collaboratory. And

05:32:31.400 --> 05:32:35.400
then that will allow you to actually use that. So make sure that's a text file that you're loading

05:32:35.400 --> 05:32:41.000
in there. But regardless, that should work. And then from there, you'll be good to go. So if you,

05:32:41.000 --> 05:32:44.440
you know, you don't need to do that, you can just run this block of code here, if you want to load

05:32:44.600 --> 05:32:49.960
in the Shakespeare txt, but otherwise, you can load in your own file. Now, after we do that,

05:32:49.960 --> 05:32:54.120
what we want to do is actually open this file. So remember, that was just saving the path to it.

05:32:54.120 --> 05:32:59.240
So we'll open that file in RB mode, which is read bytes mode, I believe. And then we're going to

05:32:59.240 --> 05:33:04.200
say dot read, so we're going to read that in as an entire string, we're going to decode that into

05:33:04.200 --> 05:33:09.160
utf a format. And then we're just printing the length of the text or the amount of characters

05:33:09.240 --> 05:33:14.200
in the text. So if we do that, we can see we have the length of the text is 1.1 million

05:33:14.200 --> 05:33:18.920
characters, approximately. And then we can have a look at the first 250 characters by doing this.

05:33:19.720 --> 05:33:23.800
So we can see that this is kind of what the plate looks like, we have whoever's speaking,

05:33:23.800 --> 05:33:29.640
colon, then some line, whoever's speaking, colon, some line, and there's all these break lines.

05:33:29.640 --> 05:33:34.040
So backslash ends, which are telling us, you know, go to the next line, right? So it's going

05:33:34.040 --> 05:33:37.880
to be important because we're going to hope that our neural network will be able to predict

05:33:37.880 --> 05:33:43.240
things like break lines and spaces, and even this kind of format as we teach it more and get

05:33:43.240 --> 05:33:49.400
further in. But now it's time to talk about encoding. So obviously, all of this text is in

05:33:49.400 --> 05:33:54.520
text form, it's not pre processed for us, which means we need to pre process it and encode it

05:33:54.520 --> 05:33:58.920
as integers before we can move forward. Now, fortunately, for us, this problem is actually

05:33:58.920 --> 05:34:03.480
a little bit easier than the problem we discussed earlier with encoding words, because what we're

05:34:03.480 --> 05:34:08.440
going to do is simply encode each character in the text with an integer. Now, you can imagine

05:34:08.440 --> 05:34:13.720
why this makes this easier, because there really is a finite set of characters, whereas there's

05:34:13.720 --> 05:34:19.080
kind of indefinite or, you know, I guess, infinite amount of words that could be created. So we're

05:34:19.080 --> 05:34:24.360
not really going to run into the problem where, you know, two words are encoded with such different

05:34:24.360 --> 05:34:28.680
or two characters are encoded with such different integers, that it makes it difficult for the model

05:34:28.680 --> 05:34:34.120
to understand. Because I mean, and we can look at what the value of vocab is here, we're only

05:34:34.120 --> 05:34:38.440
going to have so many characters in the text. And for characters, it just doesn't matter as much,

05:34:38.440 --> 05:34:43.560
because you know, an R isn't like super meaningful compared to an A. So we can kind of encode in a

05:34:43.560 --> 05:34:47.400
simple format, which is what we're going to do. So essentially, we need to figure out how many

05:34:47.400 --> 05:34:52.680
unique characters are in our vocabulary. So to do that, we're going to say vocab equals sorted,

05:34:52.680 --> 05:34:57.800
set text, this will sort all of the unique characters in the text. And then what we're

05:34:57.800 --> 05:35:02.840
going to do is create a mapping from unique characters to indices indices. So essentially,

05:35:02.840 --> 05:35:08.360
we're going to say UI, for IU in a new, a numerator vocabulary, what this will do is give us

05:35:09.240 --> 05:35:14.680
essentially zero, whatever the string is, one, whatever the string is, two, whatever the string

05:35:14.680 --> 05:35:19.480
is for every single letter or character in our vocabulary, which will allow us to create this

05:35:19.480 --> 05:35:26.680
mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array,

05:35:26.760 --> 05:35:31.960
so that we can just use the index at which a letter appears as the reverse mapping. So going

05:35:31.960 --> 05:35:36.360
from index to letter, rather than lettered index, which is what this one's doing here.

05:35:37.000 --> 05:35:42.680
Next, I've just written a function that takes some text and converts that to an int or the int

05:35:42.680 --> 05:35:47.400
representation for it, just to make a little bit easier for us as we get later on in the tutorial.

05:35:47.400 --> 05:35:52.360
So we're just going to say NP dot array of in this case, and we're just going to convert every single

05:35:52.440 --> 05:35:58.600
character in our text into its integer representation by just referencing that character and putting

05:35:58.600 --> 05:36:03.080
that in a list here, and then obviously converting that to NumPy array. So then if we wanted to have

05:36:03.080 --> 05:36:09.480
a look at how this works, we can say text as int equals text to int text. So remember text is

05:36:09.480 --> 05:36:14.520
that entire loaded file that we had above here. So we're just going to convert that to its integer

05:36:14.520 --> 05:36:20.200
representation entirely using this function. And now we can look at how this works down here. So

05:36:20.200 --> 05:36:28.520
we can see that the text for citizen, which is the first 13 letters is encoded by 1847 5657 581.

05:36:28.520 --> 05:36:32.600
And obviously each character has its own encoding, and you can go through and kind of figure out what

05:36:32.600 --> 05:36:38.040
they are based on the ones that are repeated, right? So that is how that works. Now I figured

05:36:38.040 --> 05:36:41.400
while we were at it, we might as well write a function that goes the other way. So into text.

05:36:42.360 --> 05:36:47.480
Reason I'm trying to convert this to a NumPy array first is just because we're going to be passing

05:36:47.480 --> 05:36:52.120
in different objects potentially in here. So if it's not already a NumPy array, it needs to be

05:36:52.120 --> 05:36:57.720
a NumPy array, which is kind of what this is doing. Otherwise, we're just going to pass on that we

05:36:57.720 --> 05:37:02.840
don't need to convert it to NumPy array, if it already is one, we can just join all of the characters

05:37:02.840 --> 05:37:09.000
from this list into here. So that's essentially what this is doing for us. It's just joining

05:37:09.000 --> 05:37:15.800
into text. And then we can see if we go into text, text is in colon 13, that translates that back to

05:37:15.800 --> 05:37:19.640
us for citizen, I mean, you can look more into this function if you want, but it's not that

05:37:19.640 --> 05:37:25.000
complicated. Okay, so now that we have all this text encoded as integers, what we need to do is

05:37:25.000 --> 05:37:30.600
create some training examples. It's not really feasible to just pass the entire you know, 1.1

05:37:30.600 --> 05:37:35.640
million characters to our model at once for training, we need to split that up into something

05:37:35.640 --> 05:37:40.360
that's meaningful. So what we're actually going to be doing is creating training examples where we

05:37:40.360 --> 05:37:47.640
have the first, where the training input, right, so the input value is going to be some sequence

05:37:47.640 --> 05:37:52.520
of some length, we'll pick the sequence length, in this case, we're actually going to pick 100.

05:37:52.520 --> 05:37:57.800
And then the output or the expected output, so I guess like the label for that training example

05:37:57.800 --> 05:38:03.480
is going to be the exact same sequence shifted right by one character. So essentially, I put a

05:38:03.480 --> 05:38:08.600
good example here, our input will be something like hell, right? Now our output will be E L L O.

05:38:08.600 --> 05:38:13.240
So what it's going to do is predict this last character, essentially. And these are what our

05:38:13.240 --> 05:38:18.120
training examples are going to look like. So the entire beginning sequence, and then the output

05:38:18.120 --> 05:38:23.640
sequence should be that beginning sequence minus the first letter, but tack on what the last letter

05:38:23.640 --> 05:38:28.760
should be. So that this way, we can look at some input sequence and then predict that output sequence

05:38:28.760 --> 05:38:34.120
that you know, plus a character, right? Okay, so that's how that works. So now we're going to do is

05:38:34.120 --> 05:38:38.840
define a sequence length of 100. We're going to say the amount of examples per epoch is going to be

05:38:38.840 --> 05:38:43.800
the length of the text divided by the sequence length plus one. The reason we're doing this is

05:38:43.800 --> 05:38:49.480
because for every training example, we need to create a sequence input that's 100 characters long,

05:38:49.480 --> 05:38:54.280
and we need to create a sequence output that's 100 characters long, which means that we need to have

05:38:54.280 --> 05:39:00.760
101 characters that we use for every training example, right? Hopefully that would make sense.

05:39:00.840 --> 05:39:08.440
So what this next line here is going to do is convert our entire string data set into characters.

05:39:08.440 --> 05:39:12.680
And it's actually going to allow us to have a stream of characters, which means that it's

05:39:12.680 --> 05:39:19.160
going to essentially contain, you know, 1.1 million characters inside of this TF dot data set

05:39:19.880 --> 05:39:24.760
object from tensor slices. That's what that's doing. Next, so let's run this and make sure this

05:39:24.760 --> 05:39:30.920
works. All right, what we're going to do is say sequences is equal to char data set dot batch

05:39:30.920 --> 05:39:36.680
sequence length is the length of each batch. So in this case, 101 and then drop remainder means

05:39:36.680 --> 05:39:43.720
let's say that we have, you know, 105 characters in our text, well, since we need sequences of

05:39:43.720 --> 05:39:49.240
length 101, we'll just drop the last four characters of our text, because we can't even put those into

05:39:49.240 --> 05:39:54.040
a batch. So that's what this is doing for us is going to take our entire character data set here

05:39:54.120 --> 05:39:59.160
that we've created and batch it into length of 101, and then just drop the remainder. So that's

05:39:59.160 --> 05:40:06.040
what we're going to do here. So sequences does now split input target. What this is going to do

05:40:06.040 --> 05:40:11.400
essentially is just create those training examples that we needed. So taking this, these sequences

05:40:11.400 --> 05:40:16.920
of 101 length and converting them into the input and target text, and I'll show you how they work

05:40:16.920 --> 05:40:23.480
in a second, we can do this convert the sequences to that by just mapping them to this function.

05:40:23.480 --> 05:40:28.280
So that's what this function does. So if we say sequences dot map, and we put this function here,

05:40:28.280 --> 05:40:33.640
that means every single sequence will have this operation applied to it. And that will be stored

05:40:33.640 --> 05:40:38.840
inside this data set object. Or I guess you'd say object, but we'll also just say that's it's

05:40:38.840 --> 05:40:43.400
going to be, you know, the variable, right? So if we want to look at an example of how this works,

05:40:44.120 --> 05:40:48.840
we can kind of see. So it just says example, the input will be first citizen, before we proceed

05:40:48.840 --> 05:40:54.040
any further here, me speak, all speak, speak, first citizen, you and the output notice the first

05:40:54.040 --> 05:41:00.120
character is gone, starts at I. And the last character is actually just a space here. Whereas

05:41:00.120 --> 05:41:03.960
here, it didn't have a space, or you can see there's no space. Here, there is a space. That's

05:41:03.960 --> 05:41:09.000
kind of what I'm trying to highlight for you. The next example, we get our all resolved rather

05:41:09.000 --> 05:41:12.520
to die rather than famine, whatever it goes to here, right? And then you can see here, we omit

05:41:12.600 --> 05:41:18.600
that a and the next letter is actually a K, right? That's added in there. So that's how that

05:41:18.600 --> 05:41:23.800
works. Okay, so next, we need to make training batches. So we're going to say the batch size

05:41:23.800 --> 05:41:29.480
equals 64. The vocabulary size is the length of the vocabulary, which if you remember all the way

05:41:29.480 --> 05:41:35.160
back up to the top of the code, was the set or the sorted set of the text, which essentially

05:41:35.160 --> 05:41:40.680
told us how many unique characters are in there. The embedding dimension is 256. The RNN units

05:41:40.760 --> 05:41:46.680
is 1024. And the buffered size is 10,000. What we're going to do now is create a data set that

05:41:46.680 --> 05:41:51.480
shuffled, we're going to switch around all these sequences, so they don't get shown in the proper

05:41:51.480 --> 05:41:56.440
order, which we actually don't want. And then we're going to batch them by the batch size. So

05:41:56.440 --> 05:42:00.520
if we haven't kind of gone over what batching and all this does before, I mean, you can read

05:42:00.520 --> 05:42:04.360
these comments, this is straight from the TensorFlow documentation, what we want to do is

05:42:04.360 --> 05:42:09.960
feed our model 64 batches of data at a time. So what we're going to do is shuffle all of the data,

05:42:10.520 --> 05:42:14.440
batch it into that size, and then again, drop the remainder, if there's not enough batches,

05:42:14.440 --> 05:42:18.920
which is what we'll do. We're going to define the embedding dimension, which is essentially

05:42:19.480 --> 05:42:24.520
how big we want every single vector to represent our words are in the embedding layer. And then

05:42:24.520 --> 05:42:29.960
the RNN units, I won't really discuss what that is right now. But that's essentially how many

05:42:31.480 --> 05:42:35.000
it's hard to really just, I'm just going to omit describing at for right now, because I don't want

05:42:35.000 --> 05:42:41.320
to butcher an explanation. It's not that important. Anyways, okay, so now we're going to go down to

05:42:41.320 --> 05:42:45.560
building the model. So we've kind of set these parameters up here. Remember what those are,

05:42:45.560 --> 05:42:49.640
we've batched and we've shuffled the data set. And again, that's how this works. You can print

05:42:49.640 --> 05:42:54.680
it out if you want to see what a batch actually looks like. But essentially, it's just 64 entries

05:42:54.680 --> 05:42:59.560
of those sequences, right? So 64 different training examples is what a batch that is.

05:43:00.360 --> 05:43:05.880
All right. So now we go down here, we're going to say build model, we're actually making a function

05:43:05.880 --> 05:43:10.520
that's going to return to us a built model. The reason for this is because

05:43:11.480 --> 05:43:16.840
right now, we're going to pass the model batches of size 64 for training, right? But what we're

05:43:16.840 --> 05:43:22.280
going to do later is save this model. And then we're going to patch pass it batches of one pieces

05:43:22.280 --> 05:43:27.400
of, you know, training whatever data, so that it can actually make a prediction on just one

05:43:28.120 --> 05:43:32.440
piece of data. Because for right now, what it's going to do is takes a batch size of 64, it's

05:43:32.440 --> 05:43:37.800
going to take 64 training examples, and return to us 64 outputs. That's what this model is going

05:43:37.800 --> 05:43:43.720
to be built to do the way we build it now to start. But later on, we're going to rebuild the model

05:43:43.720 --> 05:43:48.120
using the same parameters that we've saved and trained for the model, but change it to just be

05:43:48.120 --> 05:43:53.640
a batch size of one, so that that way we can get one prediction for one input sequence, right?

05:43:54.280 --> 05:43:58.600
So that's why I'm creating this build model function. Now in here, it's going to have the

05:43:58.600 --> 05:44:04.440
vocabulary sizes, first argument, the embedding dimension, which remember was 256 as a second

05:44:04.440 --> 05:44:09.640
argument, but also these are the parameters up here, right? And then we're going to find the batch

05:44:09.640 --> 05:44:15.880
size as you know, batch size, none, what this none means is we don't know how long the sequences

05:44:15.880 --> 05:44:21.880
are going to be in each batch. All we know is that we're going to have 64 entries in each batch.

05:44:21.880 --> 05:44:27.400
And then of those 64 entries, so training examples, right, we don't know how long each one

05:44:27.400 --> 05:44:31.320
will be. Although in our case, we're going to use ones that are length 100. But when we actually

05:44:31.320 --> 05:44:35.720
use the model to make predictions, we don't know how long the sequence is going to be that we input

05:44:35.720 --> 05:44:41.000
so we leave this none. Next, we'll make an LSTM layer, which is a long short term memory RNN

05:44:41.000 --> 05:44:45.800
units, which is 1024, which again, I don't really want to explain, but you can look up if you want

05:44:45.880 --> 05:44:54.760
return sequences means return the intermediate stage at every step. The reason we're doing this

05:44:55.800 --> 05:45:01.560
is because we want to look at what the model seeing at the intermediate steps and not just

05:45:01.560 --> 05:45:06.680
the final stage. So if you leave this as false, and you don't set this to true, what happens is

05:45:06.680 --> 05:45:13.480
this LSTM just returns one output that tells us what the model kind of found at the very last

05:45:13.480 --> 05:45:18.600
time step. But we actually want the output at every single time step for this specific model.

05:45:18.600 --> 05:45:23.880
And that's why we're setting this true, stateful, not going to talk about that one right now,

05:45:23.880 --> 05:45:27.240
that's something you can look up if you want. And then recurrent initializer is just what

05:45:27.240 --> 05:45:31.960
these values are going to start at in the LSTM. We're just picking this because this is what

05:45:31.960 --> 05:45:37.400
TensorFlow has kind of said is a good default to pick. I won't go into more depth about that

05:45:37.400 --> 05:45:42.680
again, things that you can look up more if you want. Finally, we have a dense layer, which is

05:45:42.680 --> 05:45:49.000
going to contain the amount of vocabulary size nodes. The reason we're doing this is because we

05:45:49.000 --> 05:45:54.200
want the final layer to have the amount of nodes in it equal to the amount of characters in the

05:45:54.200 --> 05:45:59.800
vocabulary. This way, every single one of those nodes can represent a probability distribution

05:45:59.800 --> 05:46:06.200
that that character comes next. So all of those nodes value some sum together should give us the

05:46:06.200 --> 05:46:11.960
value of one. And that's going to allow us to look at that last layer as a predictive layer,

05:46:11.960 --> 05:46:16.120
where it's telling us the probability that these characters come next, and we've discussed how

05:46:16.120 --> 05:46:22.360
that's worked previously with other neural networks. So let's run this now. Name embedding

05:46:22.360 --> 05:46:27.800
dim is not defined, which I mean, believes I have not ran this yet. So now we run that,

05:46:27.800 --> 05:46:32.120
and we should be good. So if we look at the model summary, we can see we have our initial

05:46:32.120 --> 05:46:37.400
embedding layer, we have our LSTM, and then we have our dense layer at the end. Now notice

05:46:37.400 --> 05:46:43.160
64 is the batch size, right? That's the initial shape. None is the length of the sequence,

05:46:43.160 --> 05:46:49.800
which we don't know. And then this is going to be just the output dimension or sorry, this is

05:46:50.440 --> 05:46:56.120
the amount of values in the vector, right? So we're going to start with 256. We'll just do

05:46:56.120 --> 05:47:01.160
1,024 units in the LSTM and then 65 stands for the amount of nodes, because that is the

05:47:01.160 --> 05:47:06.440
length of the vocabulary. Alright, so combined, that's how many trainable parameters we get.

05:47:07.080 --> 05:47:11.400
You can see each of them for each layer. And now it's time to move on to the next section.

05:47:11.400 --> 05:47:15.320
Okay, so now we're moving on to the next step of the tutorial, which is creating a loss function

05:47:15.320 --> 05:47:19.560
to compile our model with. Now, I'll talk about why we need to do this in a second,

05:47:19.560 --> 05:47:25.560
but I first want to explore the output shape of our model. So remember, the input to our model

05:47:25.560 --> 05:47:31.480
is something that is of length 64, because we're going to have batches of 64 training examples,

05:47:31.480 --> 05:47:37.240
right? So every time we feed our model, we're going to give it 64 training examples. Now what

05:47:37.240 --> 05:47:42.920
those training examples are, are sequences of length 100. That's what I want you to remember.

05:47:42.920 --> 05:47:50.200
We're passing 64 entries that are all of length 100 into the model as its training data, right?

05:47:50.840 --> 05:47:55.400
But sometimes, and when we make predictions with the model later on, we'll be passing it

05:47:55.400 --> 05:48:01.400
just one entry that is of some variable length, right? And that's why we've created

05:48:02.120 --> 05:48:07.400
this build model function, so that we can build this model using the parameters that we've saved

05:48:07.400 --> 05:48:14.520
later on, once we train the model, and it can expect a different input shape, right? Because

05:48:14.520 --> 05:48:17.560
when we're training it, it's going to be given a different shape, and we're actually testing with

05:48:17.560 --> 05:48:23.800
it. Now what I want to do is explore the output of this model, though, at the current point in time.

05:48:23.800 --> 05:48:30.600
So we've created a model that accepts a batch of 64 training examples that are length 100. So

05:48:30.600 --> 05:48:35.640
let's just look at what the output is from the final layer. Give this a second to run.

05:48:36.440 --> 05:48:44.360
We get 64, 165. And that represents the batch size, the sequence length, and the vocabulary

05:48:44.360 --> 05:48:49.320
size. Now the reason for this is we have to remember that when we create a dense layer as our last

05:48:49.320 --> 05:48:56.600
layer that has 65 nodes, every prediction is going to contain 65 numbers. And that's going

05:48:56.600 --> 05:49:02.440
to be the probability of every one of those characters occurring, right? That's what that

05:49:02.440 --> 05:49:07.480
does at the last one for us. So obviously, our last dimension is going to be 65 for the vocabulary

05:49:07.480 --> 05:49:11.480
size. This is a sequence length, and that's a batch, I just want to make sure this is really

05:49:11.480 --> 05:49:14.840
clear before we keep going. Otherwise, it's going to get very confusing very quickly.

05:49:15.480 --> 05:49:21.160
So what I want to do now is actually look at the length of the example batch predictions,

05:49:21.160 --> 05:49:25.000
and just print them out and look at what they actually are. So example batch predictions

05:49:25.000 --> 05:49:31.560
is what happens when I use my model on some random input example, actually, well, the first one

05:49:31.560 --> 05:49:36.840
from my data set with when it's not trained. So I can actually use my model before it's

05:49:36.840 --> 05:49:43.000
trained with random weights and random biases and parameters, by simply using model, and then I can

05:49:43.000 --> 05:49:47.480
put the little brackets like this and just pass in some example that I want to get a prediction

05:49:47.480 --> 05:49:51.480
for. So that's what I'm going to do, I'm going to give it the first batch, and it can even it shows

05:49:51.480 --> 05:49:56.200
me the shape of this batch 64 100, I'm going to pass that to the model, and it's going to give us a

05:49:56.200 --> 05:50:01.960
prediction for that. And in fact, it's actually going to give us a prediction for every single

05:50:01.960 --> 05:50:06.040
element in the batch, right, every single training example in the batch, it's going to give us a

05:50:06.040 --> 05:50:11.640
prediction for. So let's look at what those predictions are. So this is what we get, we get

05:50:11.720 --> 05:50:19.240
a length 64 tensor, right? And then inside of here, we get a list inside of a list or an array

05:50:19.240 --> 05:50:24.840
inside of an array with all of these different predictions. So we'll stop there for this, like

05:50:24.840 --> 05:50:29.000
explaining this aspect here. But you can see we're getting 64 different predictions because

05:50:29.000 --> 05:50:35.080
there's 64 elements in the batch. Now, let's look at one prediction. So let's look at the very first

05:50:35.080 --> 05:50:41.560
prediction for say the first element in the batch, right? So let's do that here. And we see now that

05:50:41.560 --> 05:50:48.600
we get a length 100 tensor. And that this is what it looks like, there's still another layer inside.

05:50:48.600 --> 05:50:53.240
And in fact, we can see that there's another nested layer here, right, another nested array

05:50:53.240 --> 05:51:00.360
inside of this array. So the reason for this is because at every single time step, which means

05:51:00.360 --> 05:51:03.640
the length of the sequence, right, because remember, our recurrent neural network is going to feed

05:51:03.640 --> 05:51:08.840
one at a time every word in the sequence. In this case, our sequences are like the 100 at every

05:51:08.840 --> 05:51:15.560
time step, we're actually saving that output as a, as a prediction, right? And we're passing

05:51:15.560 --> 05:51:21.320
that back. So we can see that for one batch, one training, sorry, not one batch, one training

05:51:21.320 --> 05:51:26.120
example, we get 100 outputs. And these outputs are in some shape, we'll talk about what those are

05:51:26.120 --> 05:51:31.080
in a second. So that's something to remember that for every single training example, we get

05:51:31.080 --> 05:51:35.720
whatever the length of that training example was outputs, because that's the way that this

05:51:35.720 --> 05:51:41.560
model works. And then finally, we look at the prediction at just the very first time step. So

05:51:41.560 --> 05:51:46.600
this is 100 different time steps. So let's look at the first time step and see what that prediction

05:51:46.600 --> 05:51:53.240
is. And we can see that now we get a tensor of length 65. And this is telling us the probability

05:51:53.240 --> 05:51:59.080
of every single character occurring next at the first time step. So that's what I wanted to walk

05:51:59.080 --> 05:52:03.960
through is showing you what's actually outputted from the model, the current way that it works.

05:52:04.040 --> 05:52:10.920
And that's why we need to actually make our own loss function to be able to determine how, you

05:52:10.920 --> 05:52:15.640
know, good our models performing, when it outputs something ridiculous that looks like this, because

05:52:15.640 --> 05:52:20.760
there is no just built in loss function and tensor flow that can look at a three dimensional

05:52:20.760 --> 05:52:26.440
nested array of probabilities over, you know, the vocabulary size, and tell us how different the

05:52:26.440 --> 05:52:31.720
two things are. So we need to make our own loss function. So if we want to determine the predicted

05:52:31.720 --> 05:52:39.560
character from this array, so we'll go there now. What we can do is get the categorical, what's

05:52:39.560 --> 05:52:45.800
this called, we can sample the categorical distribution. And that will tell us the predicted

05:52:45.800 --> 05:52:50.280
character. So what I mean is, let's just look at this. And then we'll explain this. So since our

05:52:50.280 --> 05:52:54.520
model works on random weights and biases right now, we haven't trained yet. This is actually

05:52:54.520 --> 05:52:59.720
all of the predicted characters that it had. So at every time step, at the first time step,

05:52:59.800 --> 05:53:05.320
it predicted h, then it predicted hyphen, then h, then g, then you, and so on so forth, you get

05:53:05.320 --> 05:53:12.760
the point, right? So what we're doing to get this value is we're going to sample the prediction. So

05:53:12.760 --> 05:53:17.960
at this, this is just the first time step, actually, we're sampled the prediction. Actually, no,

05:53:17.960 --> 05:53:22.680
sorry, we're sampling every time stamp, my bad there. We're going to say sampled indices equals

05:53:22.680 --> 05:53:27.000
NP dot reshapes, we're just reshaping this just changing the shape of it. We're going to say

05:53:27.080 --> 05:53:34.200
predicted characters equals int to text sampled indices. So it's, I really, it's hard to explain

05:53:34.200 --> 05:53:38.280
all this if you guys don't have a statistics kind of background a little bit to talk about why we're

05:53:38.280 --> 05:53:44.120
sampling and not just taking the argument max value of like this array, because you would think

05:53:44.120 --> 05:53:47.560
that what we'll do is just take the one that has the highest probability out of here, and that will

05:53:47.560 --> 05:53:52.600
be the index of the next predicted character. There's some issues with doing that for the loss

05:53:52.680 --> 05:53:57.960
function. Just because if we do that, then what that means is we're going to kind of get stuck in

05:53:59.000 --> 05:54:03.080
an infinite loop almost where we just keep accepting the biggest character. So what we'll

05:54:03.080 --> 05:54:10.040
do is pick a character based on this probability distribution, kind of, yeah, again, it's hard.

05:54:10.040 --> 05:54:13.960
It's called sampling the distribution. You can look that up if you don't know what that means,

05:54:13.960 --> 05:54:18.680
but sampling is just like trying to pick a character based on a probability distribution.

05:54:18.680 --> 05:54:23.080
It doesn't guarantee that the character with the highest probability is going to be picked. It

05:54:23.080 --> 05:54:28.520
just uses those probabilities to pick it. I hope that makes sense. I know that was like a really

05:54:28.520 --> 05:54:33.880
rambly definition, but that's the best I can do. So here, we reshape the array and convert all the

05:54:33.880 --> 05:54:37.880
integers to numbers to see the actual characters. So that's what these two lines are doing here.

05:54:37.880 --> 05:54:42.280
And then I'm just showing the predicted characters by showing you this. And you know,

05:54:42.280 --> 05:54:47.320
the character here is what was predicted at time step zero to be the next character and so on.

05:54:48.120 --> 05:54:53.320
Okay, so now we can create a loss function that actually handles this for us. So this is the

05:54:53.320 --> 05:54:59.160
loss function that we have. Keras has like a built in one that we can utilize, which is what

05:54:59.160 --> 05:55:04.040
we're doing. But what this is going to do is take all of the labels and all of the probability

05:55:04.040 --> 05:55:08.680
distributions, which is what this is, logits, I'm not going to talk about that really. And we'll

05:55:08.680 --> 05:55:14.280
compute a loss on those. So how different or how similar those two things are. Remember the goal

05:55:14.280 --> 05:55:18.920
of our algorithm in the neural network is to reduce the loss, right? Okay, so next,

05:55:18.920 --> 05:55:22.040
we're going to compile the model, which we'll do here. So we're going to compile the model with

05:55:22.040 --> 05:55:28.040
the atom optimizer and the loss function as loss, which we defined here. And now we're going to

05:55:28.040 --> 05:55:31.320
set up some checkpoints. I'm not going to talk about how these work, you can kind of just read

05:55:31.320 --> 05:55:38.280
through this if you want. And then we're going to train the model. Remember to start your GPU

05:55:38.280 --> 05:55:44.600
hardware accelerator under runtime, change runtime type GPU, because if you do not, then this is

05:55:44.600 --> 05:55:49.960
going to be very slow. But once you do that, you can train the model. I've already trained it. But

05:55:49.960 --> 05:55:55.000
if we go through this training, we can see it's going to say train for 172 steps. It's going to

05:55:55.000 --> 05:55:59.160
take about, you know, 30 seconds per epoch, probably maybe a little bit less than that. And

05:55:59.160 --> 05:56:03.720
the more epochs you run this for, the better it will get. This is a different we're not likely

05:56:03.720 --> 05:56:09.560
going to overfit here. So we can run this for like, say 100 epochs, if we wanted to. For our case,

05:56:09.560 --> 05:56:14.440
let's actually start by just training this on, let's say two epochs, just to see how it does.

05:56:14.440 --> 05:56:20.280
And then we'll train it on like 1020, 4050 and compare the results. But you'll notice the more

05:56:20.280 --> 05:56:23.960
epochs, the better it's going to get. But just like for our case, we'll start with two, and then

05:56:23.960 --> 05:56:29.400
we'll work our way up. So while that trains, we'll actually explain the next aspect of this

05:56:29.480 --> 05:56:34.520
without running the code. So essentially, what we need to do, after we've trained the model,

05:56:34.520 --> 05:56:41.640
we've initially the weights and biases, if we need to rebuild it using a new batch size of one.

05:56:41.640 --> 05:56:47.800
So remember, the initial batch size was 64, which means that we'd have to pass it 64 inputs or

05:56:47.800 --> 05:56:52.760
sequences for it to work properly. But now what I've done is I'm going to rebuild the model and

05:56:52.760 --> 05:56:57.560
change it to a batch size of one, so that we can just pass it some sequence of whatever length we

05:56:57.560 --> 05:57:03.080
want. And it will work. So if we run this, we've rebuilt the model with batch size one, that's

05:57:03.080 --> 05:57:08.520
the only thing we've changed. And now what I can do is load the weights by saying model dot load

05:57:08.520 --> 05:57:15.320
weights tf dot train dot latest checkpoint checkpoint directory, and then build the model

05:57:16.680 --> 05:57:23.720
using the tensor shape one none. I know sounds strange. This is how we do this rebuild the model.

05:57:23.720 --> 05:57:28.200
One none is just saying expect the input one and then none means we don't know what the next

05:57:28.200 --> 05:57:35.640
dimension length will be. But here, checkpoint directory is just we've defined where on our

05:57:35.640 --> 05:57:40.600
computer, we're going to save these TensorFlow checkpoints. This is just saying this is the

05:57:41.400 --> 05:57:45.000
was it the prefix we're going to save the checkpoint with. So we're going to do the

05:57:45.000 --> 05:57:50.040
checkpoint directory. And then checkpoint epoch where epoch will stand for obviously,

05:57:50.680 --> 05:57:55.160
whatever epoch we're on. So we'll save checkpoint here, we'll save a checkpoint at epoch one,

05:57:55.160 --> 05:58:00.440
a checkpoint at epoch two, to get the latest checkpoint, we do this. And then if we wanted

05:58:00.440 --> 05:58:05.720
to load any intermediate checkpoint, say like checkpoint 10, which is what I've defined here,

05:58:05.720 --> 05:58:10.280
we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading

05:58:10.280 --> 05:58:14.680
by saying tf dot train dot load checkpoint, whereas this one just gets the most recent. So

05:58:14.680 --> 05:58:18.280
we'll get the most recent, which should be checkpoint two for me. And then what we're going

05:58:18.280 --> 05:58:23.640
to do is generate the text. So this function, I'll dig into it in a second, but I just want

05:58:23.640 --> 05:58:27.160
to run and show you how this works, because I feel like we've done a lot of work for not

05:58:27.160 --> 05:58:31.880
very many results right now. And I'm just going to type in the string Romeo, and just show you

05:58:31.880 --> 05:58:36.920
that when I do this, we give it a second. And it will actually generate an output sequence

05:58:36.920 --> 05:58:43.240
like this. So we have Romeo us give this is the beginning of our sequence that says lady

05:58:43.240 --> 05:58:50.040
Capulet food, Marathon father, gnomes come to those shall, right? So it's like pseudo English,

05:58:50.040 --> 05:58:54.360
most of it are like kind of proper words. But again, this is because we trained it on just

05:58:54.360 --> 05:59:00.120
two epochs. So I'll talk about how we build this in a second. But if you wanted a better output

05:59:00.120 --> 05:59:04.840
for this part, then you would train this on more epochs. So now let's talk about how I actually

05:59:04.840 --> 05:59:10.360
generated that output. So we rebuilt the model to accept a batch size of one, which means that I

05:59:10.360 --> 05:59:15.320
can pass it a sequence of any length. And in fact, what I start by doing is passing the

05:59:15.320 --> 05:59:21.480
sequence that I've typed in here, which was Romeo, then what that does is we run this function

05:59:21.480 --> 05:59:25.320
generate text, I just stole this from TensorFlow's website, like I've stolen almost all of this

05:59:25.320 --> 05:59:31.800
code. And then we say the number of characters to generate is 800, the input evaluation, which

05:59:31.800 --> 05:59:37.400
is now like we need to pre process this text again, so that this works properly, we could

05:59:37.400 --> 05:59:42.040
use my little function, or we can just write this line of code here, which does with the function

05:59:42.040 --> 05:59:47.880
that I wrote does for us. So char to IDX S for S and start string, start string is what we typed

05:59:47.880 --> 05:59:53.080
in in that case, Romeo, then what we're going to do is expand the dimensions. So essentially turn

05:59:53.640 --> 06:00:00.200
just a list like this that has all these numbers, nine, eight, seven into a double list like this,

06:00:00.200 --> 06:00:04.520
or just a nested list, because that's what it's expecting as the input one batch, one entry.

06:00:05.160 --> 06:00:09.240
Then what we do is we're going to say the string that we want to store, because we want to print

06:00:09.240 --> 06:00:15.320
this out at the end, right, we'll put in this text generated list, temperature equals 1.0.

06:00:15.320 --> 06:00:20.040
What this will allow us to do is if we change this value to be higher, I mean, you can read

06:00:20.040 --> 06:00:24.120
the comment here, right, low temperature results in more predictable text, higher temperature results

06:00:24.120 --> 06:00:27.720
in more surprising text. So this is just a parameter to mess with if you want, you don't

06:00:27.720 --> 06:00:32.760
necessarily need it. And I would like, I've just left mine at one for now, we're going to start

06:00:32.760 --> 06:00:37.800
by resetting the states of the model. This is because when we rebuild the model, it's going to

06:00:37.800 --> 06:00:43.400
have stored the last state that it remembered when it was training. So we need to clear that

06:00:43.400 --> 06:00:48.520
before we pass new input text to it. And we say for I and range num generate, which means however

06:00:48.520 --> 06:00:53.080
many characters we want to generate, which is 800 here, what we're going to do is say predictions

06:00:53.080 --> 06:00:58.680
equals model input a vowel, that's going to start as the start string that's encoded, right.

06:00:59.480 --> 06:01:04.440
And then what we're going to do is say predictions equals TF dot squeeze prediction zero. What this

06:01:04.440 --> 06:01:09.320
does is take our predictions, which is going to be in a nested list, and just removes that

06:01:09.320 --> 06:01:14.040
exterior dimension. So we just have the predictions that we want, we don't have that extra dimension

06:01:14.040 --> 06:01:18.040
that we need to index again. And then we're going to say using a categorical distribution to predict

06:01:18.040 --> 06:01:22.440
the character returned by the model, that's what it writes here. We'll divide by the temperature,

06:01:22.440 --> 06:01:28.200
if it's one, that's not going to do anything. We'll say predicted ID equals we'll sample

06:01:28.200 --> 06:01:33.560
whatever the output was from the model, which is what this is doing. And then we're going to

06:01:33.560 --> 06:01:40.040
take that output. So the predicted ID, and we are going to add that to the input evaluation.

06:01:40.840 --> 06:01:45.080
And then what we're going to say is text generated dot append, and we're going to convert the text

06:01:45.080 --> 06:01:52.360
that are integers now, back into a string, and return all of this. Now I know this seems like

06:01:52.360 --> 06:01:57.160
a lot. Again, this is just given to us by TensorFlow to, you know, create this aspect,

06:01:57.160 --> 06:02:00.760
you can read through the comments yourself, if you want to understand it more. But I think that

06:02:00.760 --> 06:02:05.640
was a decent explanation of what this is doing. So yeah, that is how we can generate, you know,

06:02:05.640 --> 06:02:11.960
sequences using recurrent neural network. Now what I'm going to do is go to my other window

06:02:11.960 --> 06:02:16.120
here where I've actually typed all of the code, just in full and do a quick summary of everything

06:02:16.120 --> 06:02:19.480
that we've done, just because there was a lot that went on. And then from there, I'm actually

06:02:19.480 --> 06:02:23.960
going to train this on a B movie script and show you kind of how that works in comparison to the

06:02:23.960 --> 06:02:29.160
Romeo and Juliet. Okay, so what I'm in now is just the exact same notebook we have before,

06:02:29.160 --> 06:02:34.040
but I've just pretty much copied all the text in here. Or it's the exact same code we had before.

06:02:34.040 --> 06:02:37.560
So we just don't have all that other text in between. So I can kind of do a short summary

06:02:37.560 --> 06:02:42.920
of what we did, as well as show you how this worked when I trained it on the B movie script.

06:02:42.920 --> 06:02:46.520
So I did mention I was going to show you that I'm not lying, I will show you can see I've

06:02:46.520 --> 06:02:52.280
got B movie dot txt loaded in here. And in fact, actually, I'm going to show you this script first

06:02:52.280 --> 06:02:56.920
to show you what it looks like. So this is what the B movie script looks like. You can see it

06:02:56.920 --> 06:03:02.200
just like a long, you know, script of text, I just downloaded this for free off the internet.

06:03:02.200 --> 06:03:06.280
And it's actually not as long as the Romeo and Juliet play. So we're not going to get as good

06:03:06.280 --> 06:03:11.160
of results from our model. But it should hopefully be okay. So we just start and I'm just going to

06:03:11.160 --> 06:03:14.280
do a brief summary. And then I'll show you the results from the B movie script, just so that

06:03:14.280 --> 06:03:18.120
people that are confused, maybe have something that wraps it up here. We're doing our imports.

06:03:18.120 --> 06:03:22.520
I don't think I need to explain that this part up here is just loading in your file. Again,

06:03:22.520 --> 06:03:27.240
I don't think I need to explain that. Then we're actually going to read the file. So open it from

06:03:27.240 --> 06:03:34.680
our directory, decode it into utf eight, we're going to create a vocabulary and encode all of

06:03:34.680 --> 06:03:39.320
the text that's inside of this file. Then what we're going to do is turn all of that text up

06:03:39.320 --> 06:03:43.480
into you know, the encoded version, we're writing a function here that goes the other way around.

06:03:43.480 --> 06:03:47.960
So from int to text, not from text to int, we're going to define the sequence length that we

06:03:47.960 --> 06:03:52.680
want to train with, which will be sequence length of 100. You can decrease this value if you want,

06:03:52.680 --> 06:03:57.240
you go 50, go 20, it doesn't really matter. It's up to you. It just that's going to determine

06:03:57.240 --> 06:04:01.400
how many training examples you're going to have right is the sequence length. Next, what we're

06:04:01.400 --> 06:04:06.680
going to do is create a character data set from tensor slices from text as int. What this is going

06:04:06.680 --> 06:04:13.160
to do is just convert our entire text that's now an integer array into a bunch of slices of

06:04:13.160 --> 06:04:17.240
characters. And so that's what this is doing here. So or not slices, what am I saying,

06:04:17.960 --> 06:04:23.240
you're just going to convert, like, split that entire array into just characters, like that's

06:04:23.240 --> 06:04:27.480
pretty much what it's doing. And then what we're going to say sequences equals char data set dot

06:04:27.480 --> 06:04:32.200
batch, which now is going to take all those characters and batch them in lengths of 101.

06:04:32.200 --> 06:04:37.560
What we're going to do then is split all of that into the training examples. So like this, right,

06:04:37.560 --> 06:04:44.120
he ll and then yellow, we're going to map this function to sequences, which means we're going

06:04:44.120 --> 06:04:49.800
to apply this to every single sequence and store that in data set. Then we're going to find the

06:04:49.800 --> 06:04:54.760
parameters for our initial network. We're going to shuffle the data set and batch that into now

06:04:54.760 --> 06:04:59.960
64 training examples. Then we're going to make the function that builds the model, which I've

06:04:59.960 --> 06:05:05.560
already discussed, we're going to actually build the model starting with a batch size of 64. We're

06:05:05.560 --> 06:05:12.280
going to create our loss function, compile the model, set our checkpoints for saving, and then

06:05:12.280 --> 06:05:18.120
train the model and make sure that we say checkpoint callback as the checkpoint callback

06:05:18.120 --> 06:05:22.280
for the model, which means it's going to save every epoch, the weights that the model had

06:05:22.280 --> 06:05:27.880
computed at that epoch. So after we do that, then our models train so we've trained the model,

06:05:27.880 --> 06:05:32.360
you can see I train this on 50 epochs for the B movie script. And then what we're going to do is

06:05:32.360 --> 06:05:39.000
build the model now with a batch size of one. So we can pass one example to it and get a prediction,

06:05:39.000 --> 06:05:43.000
we're going to load the most recent weights into our model from the checkpoint directory

06:05:43.000 --> 06:05:47.320
that we defined above. And then what we're going to do is build the model and tell it to

06:05:47.320 --> 06:05:53.960
expect the shape one, none as its initial input. Now none just means we don't know what that value

06:05:53.960 --> 06:05:57.640
is going to be, but we know we're going to have one entry. Alright, so now we have this generate

06:05:57.720 --> 06:06:02.280
text method, or function here, which I've already kind of went through how that works. And then

06:06:02.920 --> 06:06:07.640
we can see, if I type in input string, so we type, you know, input string, let's say,

06:06:08.680 --> 06:06:15.480
of hello, and hit enter, we'll watch and we can see that the B movie, you know, trained model

06:06:15.480 --> 06:06:20.600
comes up with its output here. Now, unfortunately, the B movie script does not work as well as Romeo

06:06:20.600 --> 06:06:25.800
and Juliet. That's just because Romeo and Juliet is a much longer piece of text. It's much better

06:06:26.440 --> 06:06:31.560
it's format a lot nicer and a lot more predictable. But yeah, you kind of get the idea here and it's

06:06:31.560 --> 06:06:35.960
kind of cool to see how this performs on different data. So I would highly recommend that you guys

06:06:35.960 --> 06:06:40.920
find some training data that you could give this other than just the Romeo and Juliet or maybe

06:06:40.920 --> 06:06:45.640
even try another play or something and see what you can get out of it. Also, quick side note,

06:06:45.640 --> 06:06:50.040
to make your model better, increase the amount of epochs here. Ideally, you want this loss to

06:06:50.040 --> 06:06:55.240
be as low as possible, you can see mine was still actually moving down at epoch 50. You will

06:06:55.240 --> 06:06:59.720
reach a point where the amount of epochs won't make a difference. Although, with models like this,

06:06:59.720 --> 06:07:04.520
the more epochs typically the better, because it's difficult for it to kind of overfit, because all

06:07:04.520 --> 06:07:10.680
you want it to do really is just kind of learn how the language works and then be able to replicate

06:07:10.680 --> 06:07:15.560
that to you almost, right? So that's kind of the idea here. And with that being said, I'm going to

06:07:15.560 --> 06:07:20.600
say that this section is probably done. Now, I know this was a long, probably confusing section

06:07:20.600 --> 06:07:24.680
for a lot of you. But this is, you know, what happens when you start getting into some more

06:07:24.680 --> 06:07:28.680
complex things in machine learning, it's very difficult to kind of grasp and understand all

06:07:28.680 --> 06:07:33.320
these concepts in an hour of me just explaining them. What I try to do in these videos is introduce

06:07:33.320 --> 06:07:37.720
you to the syntax show you how to get a working, you know, kind of prototype and hopefully give

06:07:37.720 --> 06:07:41.560
you enough knowledge to the fact where if you're confused by something that I said, you can go

06:07:41.560 --> 06:07:46.200
and you can look that up and you can figure out kind of the more important details for yourself,

06:07:46.200 --> 06:07:50.520
because I really just I can't go into all, you know, the extremes in these videos. So anyways,

06:07:50.520 --> 06:07:54.520
that has been this section. I hope you guys enjoyed doing this. I thought this was pretty cool.

06:07:54.520 --> 06:07:57.400
And in the next section, we're going to be talking about reinforcement learning.

06:08:00.600 --> 06:08:05.480
Hello, everyone, and welcome to the next module in this course on reinforcement learning. So what

06:08:05.480 --> 06:08:08.920
we're going to be doing in this module is talking about another technique in machine learning called

06:08:08.920 --> 06:08:12.920
reinforcement learning. Now, if you remember at the very beginning of this course, which I know

06:08:13.000 --> 06:08:18.200
for you guys is probably at like six hours ago at this point, we did briefly discuss what reinforcement

06:08:18.200 --> 06:08:22.360
learning was. Now I'll go through a recap here just to make sure everyone's clear on it. But

06:08:22.360 --> 06:08:27.000
essentially, reinforcement learning is kind of the strategy in machine learning where rather

06:08:27.000 --> 06:08:32.840
than feeding a ton of data and a ton of examples to our model, we let the model or in this case,

06:08:32.840 --> 06:08:37.800
we're going to call it agent actually come up with these examples itself. And we do this by letting

06:08:37.880 --> 06:08:43.480
an agent explore an environment. Now, essentially, the concept here is just like humans, the way that

06:08:43.480 --> 06:08:48.520
we learn to do something say like play a game is by actually doing it, we get put in the environment,

06:08:48.520 --> 06:08:52.440
we try to do it. And then, you know, we'll make mistakes, we'll encounter different things,

06:08:52.440 --> 06:08:57.720
we'll see what goes correctly. And based on those experiences, we learn and we figure out the correct

06:08:57.720 --> 06:09:03.560
things to do a very basic example is, you know, say we play a game. And when we go left, we fell

06:09:03.640 --> 06:09:07.880
off a cliff or something, right? Next time we play that game, and we get to that point, we're

06:09:07.880 --> 06:09:12.600
probably not going to go left, because we're going to remember the fact that that was bad, and hence

06:09:12.600 --> 06:09:17.480
learned from our mistakes. So that's kind of the idea here with reinforcement learning. I'm going

06:09:17.480 --> 06:09:21.880
to go through exactly how this works and give some better examples and some math behind one of the

06:09:21.880 --> 06:09:25.160
implementations we're going to use. But I just want to make this clear that there's a lot of

06:09:25.160 --> 06:09:29.160
different types of reinforcement learning. In this example, we're just going to be talking

06:09:29.160 --> 06:09:33.080
about something called q learning. And I'm going to keep this module shorter compared to the other

06:09:33.080 --> 06:09:39.000
ones. Because this field of AI machine learning is pretty complex and can get pretty difficult

06:09:39.000 --> 06:09:43.640
pretty quickly. So it's something that's maybe a more advanced topic for some of you guys. Alright,

06:09:43.640 --> 06:09:48.280
so anyways, now we need to define some terminology before I can even start really explaining the

06:09:48.280 --> 06:09:52.760
technique we're going to use and how this works. So we have something called an environment,

06:09:52.760 --> 06:09:57.640
agent, state action and reward. And I'm hoping that some of you guys will remember this from the

06:09:57.640 --> 06:10:03.640
very beginning. But environment is essentially what we're trying to solve or what we're trying to

06:10:03.640 --> 06:10:08.600
do. So in reinforcement learning, we have this notion of an agent. And the agent is what's going

06:10:08.600 --> 06:10:13.000
to explore the environment. So if we're thinking about reinforcement learning, when it comes to

06:10:13.000 --> 06:10:18.440
say training an AI to play a game, well, in that instance, say we're talking about Mario, the agent

06:10:18.440 --> 06:10:23.320
would be Mario as that is the thing that's moving around and exploring our environment. And the

06:10:23.400 --> 06:10:28.600
environment would be the level in which we're playing in. So you know, in another example,

06:10:28.600 --> 06:10:32.040
maybe in the example we're going to use below, we're actually going to be kind of in almost a

06:10:32.040 --> 06:10:37.320
maze. So the environment is going to be the maze. And the agent is going to be the character or the

06:10:37.320 --> 06:10:42.040
entity or whatever you want to call it, that's exploring that maze. So it's pretty, it's usually

06:10:42.040 --> 06:10:46.040
pretty intuitive to come up with what the environment and the agent are, although in some

06:10:46.040 --> 06:10:50.680
more complex examples, it might not always be clear. But just understand that reinforcement

06:10:50.760 --> 06:10:55.320
learning deals with an agent, something exploring an environment and a very common

06:10:55.880 --> 06:10:59.960
application of reinforcement learning is in training AI is on how to play games. And it's

06:10:59.960 --> 06:11:03.960
actually very interesting what they've been able to do in that field recently. Okay, so we have

06:11:03.960 --> 06:11:08.360
environments and agent, hopefully that makes sense. The next thing to talk about is state. So

06:11:08.360 --> 06:11:15.080
essentially, the state is where you are in the environment. So obviously, inside of the environment,

06:11:15.080 --> 06:11:19.880
we can have many different states. And a state could also be associated with the, you know,

06:11:19.880 --> 06:11:25.880
agent itself. So we're going to say the agent is in a specific state, whenever it is in some

06:11:25.880 --> 06:11:31.160
part of the environment. Now, in the case of our game, the state that an agent would be in

06:11:31.160 --> 06:11:36.680
would be their position in the level, say if they're at, you know, x y coordinates, like 1020,

06:11:36.680 --> 06:11:43.320
they would be at state or in state 1020. That's kind of how we think about states. Now, obviously,

06:11:43.320 --> 06:11:47.560
state could be applied in some different instances as well. We're playing say, maybe a turn based

06:11:47.560 --> 06:11:52.200
game. You know, actually, that's not really a great example. I'm trying to think of something

06:11:52.200 --> 06:11:56.440
where the state wouldn't necessarily be a position, maybe if you're playing a game where you have

06:11:56.440 --> 06:12:01.400
like health or something like that. And part of the state might be the health of the character.

06:12:01.960 --> 06:12:05.480
This can get complicated, depending on what you're trying to do. But just understand the notion

06:12:05.480 --> 06:12:09.560
that for most of our example, state is simply going to be in location, although it really is

06:12:09.560 --> 06:12:14.040
just kind of telling us information about where the agent is, and its status in the environment.

06:12:14.840 --> 06:12:20.280
So next, we have this notion of an action. So in reinforcement learning, our agent is exploring

06:12:20.280 --> 06:12:24.440
the environment, it's trying to figure out the best way or how to accomplish some kind of goal

06:12:24.440 --> 06:12:28.680
in the environment. And the way that it interacts with the environment is with something called

06:12:28.680 --> 06:12:34.040
actions. Now, actions could be say, moving the left arrow key, right, moving to the left in

06:12:34.040 --> 06:12:38.280
the environment, moving to the right, it could be something like jumping in an action can actually

06:12:38.280 --> 06:12:44.360
be not doing something at all. So when we say, you know, agent performed action, that could

06:12:44.360 --> 06:12:48.680
really mean that the action and that maybe time step was that they didn't do something, right,

06:12:48.680 --> 06:12:53.560
that they didn't do anything that was their action. So that's kind of the idea of action.

06:12:54.120 --> 06:12:58.760
In the example of our Mario one, which I keep going back to an action would be something like

06:12:58.760 --> 06:13:04.680
jumping. And typically actions will change the state of our entity or our agent, although they

06:13:04.680 --> 06:13:09.800
might not necessarily do that. In fact, we will observe with a lot of the different actions that

06:13:09.800 --> 06:13:14.760
we could actually be in the same state after performing that action. Alright, so now we're

06:13:14.760 --> 06:13:21.240
on to the last part, which is actually the most important to understand. And this is reward. So

06:13:21.240 --> 06:13:26.520
reward is actually what our agent is trying to maximize while it is in the environment. So the

06:13:26.520 --> 06:13:32.600
goal of reinforcement learning is to have this agent navigate this environment, go through a

06:13:32.600 --> 06:13:38.760
bunch of the different states of it and determine which actions maximize the reward at every given

06:13:38.760 --> 06:13:45.320
state. So essentially, the goal of our agent is to maximize a reward. But what is a reward? Well,

06:13:45.320 --> 06:13:51.320
after every action that's taken, the agent will receive a reward. Now this reward is something

06:13:51.320 --> 06:13:56.520
that us as the programmer need to come up with. The reason we need to do this is because we need

06:13:56.520 --> 06:14:01.000
to tell the agent when it's performing well and when it's performing poorly. And just like we

06:14:01.000 --> 06:14:06.200
had like a loss function in neural networks, when we're using those before, this is almost like

06:14:06.200 --> 06:14:11.480
our loss function, you know, the higher this number is, the more reward the agent gets, the

06:14:11.480 --> 06:14:17.240
better, the lower the reward, you know, it's not as good, it's not doing as well. So that's how we

06:14:17.240 --> 06:14:22.840
kind of monitor and assess performance for our agents is by determining the almost average amount

06:14:22.840 --> 06:14:27.400
of reward that they're able to achieve. And their goal is really to, you know, it's almost an

06:14:27.400 --> 06:14:32.120
optimization problem where they're trying to maximize this reward. So what we're going to do

06:14:32.120 --> 06:14:36.120
in reinforcement learning is have this agent exploring the environment, going through these

06:14:36.120 --> 06:14:40.760
different states and performing these different actions, trying to maximize its reward. And

06:14:40.760 --> 06:14:45.080
obviously, if we're trying to get the agent to say finish a level or, you know, complete the game,

06:14:45.640 --> 06:14:51.480
then the maximum maximum reward will be achieved once it's completed the level or completed the

06:14:51.480 --> 06:14:56.600
game. And if it does things that we don't like, say like dying or like jumping in the wrong spot,

06:14:56.680 --> 06:15:01.080
we could give it a negative reward to try to influence it to not do that. And our goal,

06:15:01.080 --> 06:15:05.400
you know, when we train these agents is for them to get the most reward. And we hope that

06:15:05.400 --> 06:15:08.920
they're going to learn the optimal route through a level or through some environment that will

06:15:08.920 --> 06:15:13.800
maximize that reward for them. Okay, so now I'm going to talk about a technique called Q learning,

06:15:13.800 --> 06:15:18.120
which is actually just an algorithm that we're going to use to implement this idea of reinforcement

06:15:18.120 --> 06:15:22.520
learning. We're not going to get into anything too crazy in this last module, because this is meant

06:15:22.520 --> 06:15:27.720
to be more of an introduction into the kind of field of reinforcement learning than anything else.

06:15:27.720 --> 06:15:32.200
But Q learning is the most basic way to implement reinforcement learning, at least that I have

06:15:32.200 --> 06:15:37.800
discovered. And essentially, what Q learning is, and I don't actually really know why they call it

06:15:37.800 --> 06:15:43.880
Q, although I should probably know that is creating some kind of table or matrix likes

06:15:43.880 --> 06:15:49.720
data structure, that's going to contain as the, what is it, I guess the rows, every single state,

06:15:49.720 --> 06:15:54.760
and as the columns, every single action that could be taken in all of those different states.

06:15:54.760 --> 06:15:59.640
So for an example here, and we'll do one on kind of the whiteboard later on, if we can get there.

06:16:00.680 --> 06:16:06.280
But here, we can see that this is kind of my Q table. And what I'm saying is that we have a one,

06:16:06.280 --> 06:16:12.280
a two, a three, a four, as all of the possible actions that could be performed in any given state.

06:16:12.280 --> 06:16:17.720
And we have three states denoted by the fact that we have three rows. And the numbers in this,

06:16:18.440 --> 06:16:23.320
this table with this Q, what do they call it, Q matrix Q table, whatever you want to call it,

06:16:23.320 --> 06:16:29.720
the numbers that are present here, represent what the predicted reward will be, given that we take

06:16:29.720 --> 06:16:35.720
an action, whatever this action is in this state. So I'm not sure if this is making sense to you

06:16:35.720 --> 06:16:41.800
guys, but essentially, if we're saying that row zero is state zero, action two, a two, this value

06:16:41.800 --> 06:16:48.840
tells us what reward we should expect to get. If we take this action while we're in this state,

06:16:48.840 --> 06:16:53.400
that's what that is trying to tell us. That's what that means. Same thing here in, you know,

06:16:53.400 --> 06:16:59.800
state two, we can see that the optimal action to take would be action two, because that has the

06:16:59.800 --> 06:17:05.000
highest reward for this state. And that's what this table is that we're going to try to generate

06:17:05.000 --> 06:17:11.160
with this technique called Q learning, a table that can tell us given any state, what the predicted

06:17:11.160 --> 06:17:15.880
reward will be for any action that we take. And we're going to generate this table by exploring

06:17:15.880 --> 06:17:21.400
the environment many different times, and updating these values according to what we kind of see

06:17:21.400 --> 06:17:26.040
or what the agent sees in the environment and the rewards that it receives for any given action in

06:17:26.040 --> 06:17:30.520
any given state. And we'll talk about how we're going to update that later. But this is the basic

06:17:30.520 --> 06:17:35.480
premise. So that is kind of Q learning, we're going to hop on the whiteboard now, and we'll do a

06:17:35.480 --> 06:17:39.880
more in depth example, but then we're going to talk about how we actually learned this Q table

06:17:39.960 --> 06:17:44.680
that I just discussed. Okay, so I've drawn a pretty basic example right now that I'm going to try

06:17:44.680 --> 06:17:50.200
to use to illustrate the idea of Q learning and talk about some problems with it and how we can

06:17:50.200 --> 06:17:54.920
kind of combat those as we learn more about how Q learning works. But the idea here is that we

06:17:54.920 --> 06:17:59.080
currently have three states and why, what is happening? Why was that happening up at the top?

06:17:59.080 --> 06:18:04.920
I don't know. Anyways, the idea is we have three states as one s two and s three. And at each state

06:18:04.920 --> 06:18:10.280
we have two possible actions that can be taken, we can either stay in this state or we can move.

06:18:10.840 --> 06:18:15.960
Now, what I've done is kind of just written some integers here that represent the reward that we're

06:18:15.960 --> 06:18:21.640
going to get or that the agent is going to get such that it takes that action in a given state.

06:18:21.640 --> 06:18:29.080
So if we take the action here in s one, right of moving, then we will receive a reward of one

06:18:29.080 --> 06:18:32.840
because that's what we've written here is the reward that we get for moving. Whereas if we

06:18:32.920 --> 06:18:37.480
stay, we'll get a reward of three, you know, same concept here, if we stay, we get two,

06:18:37.480 --> 06:18:42.840
if we move, we get one, and I think you understand the point. So the goal of our agent to remember

06:18:42.840 --> 06:18:47.400
is to maximize its reward in the environment. And what we're going to call the environment

06:18:47.400 --> 06:18:53.080
is this right here, the environment is essentially defines the number of states, the number of

06:18:53.080 --> 06:18:58.920
actions, and you know, the way that the agent can interact with these states and these actions.

06:18:58.920 --> 06:19:03.320
So in this case, the agent can interact with the states by taking actions that change its state,

06:19:03.320 --> 06:19:07.480
right? So that's where we're getting out with this. Now, what I want to do is show you how we

06:19:07.480 --> 06:19:14.680
use this queue table, or learn this queue table to come up with kind of the almost, you know,

06:19:14.680 --> 06:19:19.080
the model, like the machine learning model that we're going to use. So essentially, what we would

06:19:19.080 --> 06:19:27.400
want to have here is we want to have a kind of pattern in this table that allows our agent to

06:19:27.480 --> 06:19:32.440
receive the maximum reward. So in this case, we're going to say that our agent will start at

06:19:32.440 --> 06:19:36.760
state s one. And obviously, whenever we're doing this reinforcement learning, we need to have some

06:19:36.760 --> 06:19:41.240
kind of start state that the agent will start in this could be a random state, it could change,

06:19:41.240 --> 06:19:45.080
but it doesn't just start in some state. So in this case, we're going to say it starts at s one.

06:19:45.640 --> 06:19:51.320
Now, when we're in s one, the agent has two things that it can do. It can stay in the current state

06:19:51.320 --> 06:19:57.560
and receive a reward of three, or it can move and receive a reward of one, right? If we get to s

06:19:57.560 --> 06:20:02.680
two, in this state, what can we do? We can stay, which means we receive a reward of two, or we

06:20:02.680 --> 06:20:07.320
can move, which means we get a reward of one. And same thing for s three, we can stay, we get a

06:20:07.320 --> 06:20:13.800
reward of four, and we can move, we get a reward of one. Now, right now, if we had just ran this

06:20:14.360 --> 06:20:19.400
one time and have the agent stay in each state, like start in each unique state,

06:20:19.400 --> 06:20:24.200
this is what the queue table we would get would look like. Because after looking at this, just

06:20:24.200 --> 06:20:29.160
one time starting in each state, what the agent would be able to, or I guess, two times, because

06:20:29.160 --> 06:20:34.680
it would have to try each action. Let's say we had the agent start in each state twice. So it started

06:20:34.680 --> 06:20:39.080
an s one twice, it started s two twice, and it started an s three twice. And every time it started

06:20:39.080 --> 06:20:43.480
there, it tried one of the different actions. So when it started in s one, it tried moving once,

06:20:43.480 --> 06:20:47.400
and then it tried staying once, we would have a queue table that looks like this. Because what

06:20:47.480 --> 06:20:52.840
would happen is we would update values in our queue table to represent the reward we received

06:20:52.840 --> 06:20:58.360
when we took that action from that state. So we can see here that when we're in state s one,

06:20:58.360 --> 06:21:04.520
and we decide to stay, what we did is we wrote a three inside of the stay column, because that is

06:21:04.520 --> 06:21:10.840
how much reward we received when we moved, right? Same thing for state two, when we moved for state

06:21:10.840 --> 06:21:16.520
two or have I guess, sorry, stayed when we stayed in state two, we received a reward of two, same

06:21:16.520 --> 06:21:23.080
thing for four. Now, this is okay, right? This tells us kind of, you know, the optimal move to

06:21:23.080 --> 06:21:28.120
make in any state to receive the maximum reward. But what if we introduce the idea that, you know,

06:21:28.120 --> 06:21:35.160
our agent, we want it to receive the maximum total reward possible, right? So if it's in state one,

06:21:35.160 --> 06:21:39.800
ideally, we'd like it to move to state two, and then move to states three, and then just stay in

06:21:39.800 --> 06:21:43.800
state three, because it will receive the most amount of reward. Well, with the current table

06:21:43.800 --> 06:21:48.200
that we've developed, if we just follow this, and we look at the table, we say, okay, if we want to

06:21:48.200 --> 06:21:52.920
use this Q learning table now to, you know, move an agent around our level, what we'll do is we'll

06:21:52.920 --> 06:21:58.600
say, okay, what state is it in? If it's in state two, we'll do stay because that's the highest reward

06:21:58.600 --> 06:22:04.120
that we have in this table. If that's the approach we use, then we could see that if our, you know,

06:22:04.120 --> 06:22:09.640
agent start in state one or state two, it would stay in what we call a local minima, because

06:22:09.640 --> 06:22:15.320
it's not able to kind of realize from this state that it can move any further and receive a much

06:22:15.320 --> 06:22:19.960
greater reward, right? And that's kind of the concept we're going to talk about as we implement and,

06:22:19.960 --> 06:22:25.400
you know, discuss further how Q learning works. But hopefully this gives you a little bit of insight

06:22:25.400 --> 06:22:31.560
into what we do with this table. Essentially, when we're updating these table values is when

06:22:31.560 --> 06:22:36.680
we're exploring this environment. So when we explore this environment, and we start in a state,

06:22:36.680 --> 06:22:41.720
when we take an action to another state, we observe the reward that we got from going there,

06:22:41.720 --> 06:22:46.200
and we observe the state that we change to, right? So we observe the fact that in state one,

06:22:46.200 --> 06:22:51.480
when we go to state two, we receive the reward of one. And what we do is we take that observation

06:22:51.480 --> 06:22:57.640
and we use it to update this Q table. And the goal is that at the end of all of these observations,

06:22:57.640 --> 06:23:03.800
and there could be millions of them, that we have a Q table that tells us the optimal action to take

06:23:03.880 --> 06:23:09.880
in any single state. So we're actually hard coding, this kind of mapping that essentially

06:23:09.880 --> 06:23:14.760
just tells us given any state, all you have to do is look up in this table, look at all of the

06:23:14.760 --> 06:23:19.720
actions that could be taken, and just take the maximum action or the reward that's supposed to

06:23:19.720 --> 06:23:24.440
give, I guess, the action that's supposed to give the maximum reward. And if we were to follow

06:23:24.440 --> 06:23:28.120
that on this, we could see we get stuck in the local minima, which is why we're going to introduce

06:23:28.120 --> 06:23:35.000
a lot of other concepts. So our reinforcement learning model in Q learning, we have to implement

06:23:35.000 --> 06:23:41.320
the concept of being able to explore the environment, not based on previous experiences,

06:23:41.320 --> 06:23:46.120
right? Because if we just tell our model, okay, what we're going to do is we're going to start in

06:23:46.120 --> 06:23:50.120
all these different states, we're going to start in the start state and just start navigating around.

06:23:50.120 --> 06:23:55.000
If we update our model immediately, or update our Q table immediately and put this three here for

06:23:55.080 --> 06:24:01.240
stay, we can almost guarantee that since this three is here, when our model is training, right,

06:24:01.240 --> 06:24:05.480
if it's using this Q table to determine what state to move to next, when it's training and

06:24:05.480 --> 06:24:09.480
determining what to do, it's just always going to stay, which means we'll never get a chance to

06:24:09.480 --> 06:24:15.480
even see what we could have gotten to at S three. So we need to kind of introduce some concept

06:24:15.480 --> 06:24:21.560
of taking random actions, and being able to explore the environment more freely before starting

06:24:21.560 --> 06:24:25.800
to look at these Q values, and use that for the training. So I'm actually going to go back

06:24:26.600 --> 06:24:30.280
to my slides now to make sure I don't get lost, because I think I was starting to ramble a little

06:24:30.280 --> 06:24:35.480
bit there. So we're going to now talk about learning the Q table. So essentially, I showed

06:24:35.480 --> 06:24:41.320
you how we use that Q table, which is given some state, we just look that state up in the Q table,

06:24:41.320 --> 06:24:45.960
and then determine what the maximum reward we could get by taking, you know, some action is and

06:24:45.960 --> 06:24:50.680
then take that action. And that's how we would use the Q table later on when we're actually using the

06:24:50.680 --> 06:24:56.040
model. But when we're learning the Q table, that's not necessarily what we want to do. We don't want

06:24:56.040 --> 06:25:01.320
to explore the environment by just taking the maximum reward we've seen so far and just always

06:25:01.320 --> 06:25:05.480
going that direction, we need to make sure that we're exploring in a different way and learning

06:25:05.480 --> 06:25:10.280
the correct values for the Q table. So essentially, our agent learns by exploring the environment

06:25:10.280 --> 06:25:14.280
and observing the outcome slash reward from each action it takes in a given state, which we've

06:25:14.280 --> 06:25:18.520
already said. But how does it know what action to take in each state when it's learning? That's

06:25:18.520 --> 06:25:23.240
the question I need to answer for you now. Well, there's two ways of doing this. Our agent can

06:25:23.240 --> 06:25:28.200
essentially, you know, use the current Q table to find the best action, which is kind of what I

06:25:28.200 --> 06:25:32.520
just discussed. So taking looking at the Q table, looking at the state and just taking the highest

06:25:32.520 --> 06:25:38.920
reward, or it can randomly pick a valid action. And our goal is going to be when we create this Q

06:25:38.920 --> 06:25:45.400
learning algorithm to have a really great balance of these two, where sometimes we use the Q table

06:25:45.400 --> 06:25:52.280
to find the best action, and sometimes we take a random action. So that is one thing. But now

06:25:52.280 --> 06:25:56.600
I'm just going to talk about this formula for how we actually update Q values. So obviously, what's

06:25:56.600 --> 06:26:01.080
going to end up happening in our Q learning is we're going to have an agent that's going to be in

06:26:01.080 --> 06:26:05.880
the learning stage, exploring the environment and having all these actions and all these rewards

06:26:05.880 --> 06:26:09.480
and all these observations happening. And it's going to be moving around the environment by

06:26:09.480 --> 06:26:13.480
following one of these two kind of principles, randomly picking a valid action or using the

06:26:13.560 --> 06:26:18.920
current Q table to find the best action. When it gets into a net, a new state, and it, you know,

06:26:18.920 --> 06:26:22.840
moves from state to state, it's going to keep updating this Q table, telling it, you know,

06:26:22.840 --> 06:26:26.200
this is what I've learned about the environment, I think this is a better move, we're going to

06:26:26.200 --> 06:26:30.360
update this value. But how does it do that in a way that's going to make sense? Because we can't

06:26:30.360 --> 06:26:35.080
just put, you know, the maximum value we got from moving, otherwise, we're going to run into that

06:26:35.080 --> 06:26:39.800
issue, which I just talked about, where we get stuck in that local maxima, right? I'm not sure

06:26:39.800 --> 06:26:44.760
if I called it minimum before, but anyways, it's local maxima, where we see this high reward,

06:26:44.760 --> 06:26:49.560
but that's preventing us if we keep taking that action from reaching a potentially high reward

06:26:49.560 --> 06:26:55.000
in a different state. So the formula that we actually use to update the Q table is this. So Q

06:26:55.000 --> 06:27:00.600
state action equals Q state action, and a state action is just referencing first the rows for the

06:27:00.600 --> 06:27:07.240
state and then the action as the column, plus alpha times, and then this is all in brackets,

06:27:07.240 --> 06:27:15.880
right? Reward plus, I believe this is gamma times max Q of new states minus Q state action. So what

06:27:15.880 --> 06:27:19.720
the heck does this mean? What are these constants? What is all this? We're going to talk about the

06:27:19.720 --> 06:27:25.160
constants in a minute. But I want to, yeah, I want to explain this formula actually. So let's,

06:27:25.160 --> 06:27:29.160
okay, I guess we'll go through the constants, it's hard to go through a complicated math formula.

06:27:29.160 --> 06:27:34.360
So a stands for the learning rate, and gamma stands for the discount factor. So alpha learning

06:27:34.360 --> 06:27:38.520
rate, gamma discount factor. Now, what is the learning rate? Well, this is a little blurb on

06:27:38.520 --> 06:27:44.360
what this is. But essentially, the learning rate ensures that we don't update our Q table too much

06:27:45.320 --> 06:27:51.160
on every observation. So before, right, when I was showing you like this, if we can go back

06:27:51.160 --> 06:27:56.680
to my windows ink, why is this not working? I guess I'm just not patient enough. Before when I was

06:27:56.680 --> 06:28:01.880
showing you all I did when I took an action was I looked at the reward that I got from taking that

06:28:01.880 --> 06:28:07.080
action. And I just put that in my Q table, right? Now, obviously, that is not an optimal approach

06:28:07.080 --> 06:28:11.640
to do this, because that means that in the instance where we hit state one, well, I'm not going to

06:28:11.640 --> 06:28:15.240
be able to get to this reward of four, because I'm going to throw that, you know, three in here,

06:28:15.240 --> 06:28:21.800
and I'm just going to keep taking that action. We need to, you know, hopefully make this move

06:28:21.800 --> 06:28:27.480
action actually have a higher value than stay. So that next time we're in state one, we consider

06:28:27.480 --> 06:28:32.520
the fact that we could move to state two, and then move to state three to optimize our reward.

06:28:32.520 --> 06:28:36.040
So how do we do that? Well, the learning rate is one thing that helps us kind of accomplish

06:28:36.040 --> 06:28:41.080
this behavior. Essentially, what is telling us, and this is usually a decimal value, right,

06:28:41.080 --> 06:28:47.640
is how much we're allowed to update every single Q value by on every single action or every single

06:28:47.640 --> 06:28:52.360
observation. So if we just use the approach before, then we're only going to need to observe,

06:28:52.360 --> 06:28:55.960
given the amount of states and the amount of actions, and we'll be able to completely fill

06:28:55.960 --> 06:28:59.800
in the Q table. So in our case, if we had like three states and three actions, we could, you

06:28:59.800 --> 06:29:04.440
know, nine iterations, we'd be able to fill the entire Q table. The learning rate means that

06:29:04.440 --> 06:29:10.040
it's going to just update a little bit slower and essentially change the value in the Q table very

06:29:10.040 --> 06:29:14.440
slightly. So you can see that what we're doing is taking the current value of the Q table. So

06:29:14.440 --> 06:29:20.440
whatever is already there. And then what we're going to do is add some value here. And this value

06:29:20.440 --> 06:29:24.920
that we add is either going to be positive or negative, essentially telling us, you know,

06:29:24.920 --> 06:29:29.240
whether we should take this new action or whether we shouldn't take this new action.

06:29:29.240 --> 06:29:34.920
Now, the way that this kind of value is calculated, right, is obviously our alpha is

06:29:34.920 --> 06:29:41.000
multiplied this by this, but we have the reward plus, in this case, gamma, which is just going to

06:29:41.000 --> 06:29:46.280
actually be the discount factor. And I'll talk about how that works in a second of the maximum

06:29:46.280 --> 06:29:53.960
of the new state we moved into. Now, what this means is find the maximum reward that we could

06:29:53.960 --> 06:30:01.400
receive in the new state by taking any action and multiply that by what we call the discount factor.

06:30:01.400 --> 06:30:05.880
With this part of the formulas trying to do is exactly what I've kind of been talking about.

06:30:06.520 --> 06:30:12.040
Try to look forward and say, okay, so I know if I take this action in this state, I receive

06:30:12.040 --> 06:30:17.880
this amount of reward. But I need to factor in the reward I could receive in the next state,

06:30:17.880 --> 06:30:23.800
so that I can determine the best place to move to. That's kind of what this max and this

06:30:24.360 --> 06:30:28.760
gamma are trying to do for us. So this discount factor, whatever you want to call it. It's trying

06:30:28.760 --> 06:30:35.160
to factor in a little bit about what we could get from the next state into this equation so that

06:30:35.160 --> 06:30:41.640
hopefully our kind of agent can learn a little bit more about the transition states. So states that

06:30:41.640 --> 06:30:46.200
maybe are actions that maybe don't give us an immediate reward, but lead to a larger reward

06:30:46.200 --> 06:30:50.920
in the future. That's what this Y and max are trying to do. Then what we do is we subtract

06:30:51.560 --> 06:30:57.560
from this, the state and action. This is just to make sure that we're adding what the difference

06:30:57.560 --> 06:31:05.240
was in, you know, what we get from this versus what the current value is, and not like multiplying

06:31:05.240 --> 06:31:09.640
these values crazily. I mean, you can look into more of the math here and plug in like some values

06:31:09.640 --> 06:31:12.680
later, and you'll see how this kind of works. But this is the basic formula. And I feel like I

06:31:12.680 --> 06:31:18.280
explain that in depth enough. Okay, so now that we've done that, and we've updated this, we've

06:31:18.360 --> 06:31:24.200
learned kind of how we update the cells and how this works. I could go back to the whiteboard and

06:31:24.200 --> 06:31:28.360
draw it out. But I feel like that makes enough sense. We're going to look at what the next state is,

06:31:28.360 --> 06:31:32.440
we're going to factor that into our calculation, we have this learning rate, which tells us essentially

06:31:32.440 --> 06:31:38.520
how much we can update each cell value by. And we have this, what do you call it here discount

06:31:38.520 --> 06:31:43.880
factor, which essentially tries to kind of define the balance between finding really good rewards

06:31:43.880 --> 06:31:49.960
in our current state, and finding the rewards in the future state. So the higher this value is,

06:31:49.960 --> 06:31:53.960
the more we're going to look towards the future, the lower it is, the more we're going to focus

06:31:53.960 --> 06:31:57.480
completely on our current reward, right? And obviously, that makes sense, because we're going

06:31:57.480 --> 06:32:01.240
to add the maximum value. And if we're multiplying that by a lower number, that means we're going

06:32:01.240 --> 06:32:07.480
to consider that less than if that was greater. Awesome. Okay. So now that we've kind of understand

06:32:07.480 --> 06:32:11.000
that I want to move on to a Q learning example. And what we're going to do for this example is

06:32:11.080 --> 06:32:16.600
actually use something called the open AI gym. I just need to throw my drawing tablet away

06:32:16.600 --> 06:32:21.960
right there so that we can get started. But open AI gym is actually a really interesting kind of

06:32:21.960 --> 06:32:27.240
module. I don't even actually, I don't even really know the way to describe it almost tool. There's

06:32:27.240 --> 06:32:33.080
actually developed by open AI, you know, coincidentally by the name, which is founded by Elon Musk

06:32:33.080 --> 06:32:38.120
and someone else. So he's actually, you know, made this kind of, I don't really don't know the word

06:32:38.120 --> 06:32:43.720
to describe it. I almost want to say tool that allows programmers to work with these really cool

06:32:43.720 --> 06:32:48.600
gym environments and train reinforcement learning models. So you'll see how this works in a second,

06:32:48.600 --> 06:32:52.840
but essentially, there's a ton of graphical environments that have very easy interfaces

06:32:52.840 --> 06:32:57.320
to use. So like moving characters around them, that you're allowed to experiment with completely

06:32:57.320 --> 06:33:01.640
for free as a programmer to try to, you know, make some cool reinforcement learning models.

06:33:01.640 --> 06:33:05.320
That's what open AI gym is. And you can look at it. I mean, we'll click on it here actually to see

06:33:05.320 --> 06:33:09.080
what it is. You can see gym, there's all these different Atari environments, and it's just a

06:33:09.080 --> 06:33:14.360
way to kind of train reinforcement learning models. All right. So now we're going to start by just

06:33:14.360 --> 06:33:18.840
importing gym. If you're in Collaboratory, there's nothing you need to do here. If you're in your

06:33:18.840 --> 06:33:22.840
own thing, you're going to have to pip install gym. And then what we're going to do is make this

06:33:22.840 --> 06:33:28.520
frozen lake v zero gym. So essentially, what this does is just set up the environment that we're

06:33:28.520 --> 06:33:32.920
going to use. Now, I'll talk more about what this environment is later, but I want to talk about how

06:33:32.920 --> 06:33:37.800
gym works, because we are going to be using this throughout the thing. So the open AI gym

06:33:38.440 --> 06:33:43.560
is meant for reinforcement learning. And essentially what it has is an observation space

06:33:43.560 --> 06:33:48.520
and an action space for every environment. Now the observation space is what we call our

06:33:48.520 --> 06:33:53.880
environment, right? And that will tell us the amount of states that exist in this environment.

06:33:53.880 --> 06:33:57.080
Now, in our case, we're going to be using kind of like a maze like thing, which I'll show you in

06:33:57.080 --> 06:34:01.800
a second. So you'll understand why we get the values we do. Action space tells us how many

06:34:01.800 --> 06:34:07.320
actions we can take when we do the dot n, at any given state. So if we print this out,

06:34:07.880 --> 06:34:12.680
we get 16 and four, representing the observation space. In other words, the number of states is

06:34:12.680 --> 06:34:18.200
16. And the amount of actions we can take in every single state is four. Now in this case,

06:34:18.200 --> 06:34:24.120
these actions are going to be left down up and right. But yes, now env dot reset. So essentially,

06:34:24.120 --> 06:34:29.080
we have some commands that allow us to move around the environment, which are actually down here.

06:34:29.080 --> 06:34:33.640
If we want to reset the environment and start back in the beginning state, then we do env

06:34:33.640 --> 06:34:37.720
dot reset, you can see this actually returns to us the starting state, which obviously is going to

06:34:37.720 --> 06:34:44.440
be zero. Now we also have the ability to take a random action, or select a random action from

06:34:44.440 --> 06:34:49.240
the action space. So what this line does right here is say of the action space, so of all the

06:34:49.240 --> 06:34:54.440
commands that are there, or all the actions we could take, pick a random one and return that.

06:34:54.520 --> 06:35:01.560
So if you do that, actually, let's just print action and see what this is. You see we get zero

06:35:01.560 --> 06:35:07.960
to right, it just gives us a random action that is valid from the action space. All right. Next,

06:35:07.960 --> 06:35:14.600
what we have is this env dot step in action. Now what this does is take whatever action we have,

06:35:14.600 --> 06:35:19.880
which in this case is three, and perform that in the environment. So tell our agent to take

06:35:19.880 --> 06:35:25.160
this action in the environment and return to us a bunch of information. So the first thing is the

06:35:25.160 --> 06:35:30.200
observation, which essentially means what state do we move into next? So I could call this

06:35:31.640 --> 06:35:37.960
new underserved state reward is what reward did we receive by taking that action? So this will

06:35:37.960 --> 06:35:43.160
be some value right in our in this case, the reward is either one or zero. But that's not

06:35:43.160 --> 06:35:48.760
that important to understand. And then we have a bool of done, which tells us did we lose the game

06:35:48.760 --> 06:35:54.120
or did we win the game? Yes or no. So true. So if this is true, what this means is we need to

06:35:54.120 --> 06:35:59.640
reset the environment because our agent either lost or won and is no longer in a valid state in

06:35:59.640 --> 06:36:04.520
the environment. Info gives us a little bit of information. It's not showing me anything here.

06:36:04.520 --> 06:36:08.920
We're not going to use info throughout this, but figured I'd let you know that now in VDOT

06:36:08.920 --> 06:36:13.880
render, I'll actually render this for you and show you renders a graphical user interface that

06:36:13.880 --> 06:36:18.360
shows you the environment. Now, if you use this while you're training, so you actually watch

06:36:18.360 --> 06:36:22.680
the agent do the training, which is what you can do with this, it slows it down drastically,

06:36:22.680 --> 06:36:26.520
like probably by, you know, 10 or 20 times, because it actually needs to draw the stuff on

06:36:26.520 --> 06:36:30.280
the screen. But you know, you can use it if you want. So this is what our frozen lake example

06:36:30.280 --> 06:36:34.360
looks like. You can see that the highlighted square is where our agent is. And in this case,

06:36:34.360 --> 06:36:42.200
we have four different blocks. We have SFH and G. So S stands for start F stands for frozen,

06:36:42.200 --> 06:36:46.760
because this is a frozen lake. And the goal is to navigate to the goal without falling in one

06:36:46.760 --> 06:36:52.040
of the holes, which is represented by H. And this here tells us the action that we just took. Now,

06:36:52.040 --> 06:36:58.280
I guess the starting action is up because that's zero, I believe. But yes, so if we run this a

06:36:58.280 --> 06:37:02.280
bunch of times, we'll see this updating. Unfortunately, this doesn't work very well in

06:37:02.280 --> 06:37:07.080
Google Collaboratory, the the GUIs. But if you did this in your own command line, and you like

06:37:07.080 --> 06:37:11.400
did some different steps and rounded it all out, you would see this working properly. Okay,

06:37:11.480 --> 06:37:15.000
so now we're on to talking about the frozen lake environment, which is kind of what I just did.

06:37:15.000 --> 06:37:18.840
So now we're just going to move to the example where we actually implement Q learning to

06:37:18.840 --> 06:37:23.400
essentially solve the problem. How can we train an AI to navigate this environment and get to the

06:37:23.400 --> 06:37:27.960
start to the goal? How can we do that? Well, we're going to use Q learning. So let's start. So the

06:37:27.960 --> 06:37:32.680
first thing we need to do is import gym, import numpy, and then create some constants here. So

06:37:32.680 --> 06:37:36.360
we'll do that. We're going to say the amount of states is equal to the line I showed you before.

06:37:36.440 --> 06:37:43.240
So env dot observation, space dot n, actions is equal to env dot action space n. And then we're

06:37:43.240 --> 06:37:48.680
going to say Q is equal to NP dot zeros, states and actions. So something I guess I forgot to

06:37:48.680 --> 06:37:53.800
mention is when we initialize the Q table, we just initialize all blank values or zero values,

06:37:53.800 --> 06:37:58.200
because obviously, at the beginning of our learning, our model or agent doesn't know

06:37:58.200 --> 06:38:01.800
anything about the environment yet. So we just leave those all blank, which means we're going

06:38:01.800 --> 06:38:06.520
to more likely be taking random actions at the beginning of our training, trying to explore

06:38:06.520 --> 06:38:11.160
the environment space more. And then as we get further on and learn more about the environment,

06:38:11.160 --> 06:38:16.840
those actions will likely be more calculated based on the Q table values. So we print this out,

06:38:16.840 --> 06:38:23.000
we can see this is the array that we get, we've had to be build a 16 by four, I guess not array,

06:38:23.000 --> 06:38:27.720
well, I guess this technically is an array, we'll call it matrix 16 by four. So every single row

06:38:27.720 --> 06:38:31.640
represents a state, and every single column represents an action that could be taken in

06:38:31.640 --> 06:38:35.880
that state. Alright, so we're going to find some constants here, which we talked about before.

06:38:35.880 --> 06:38:40.760
So we have the gamma, the learning rate, the max amount of steps and the number of episodes. So the

06:38:40.760 --> 06:38:46.120
number of episodes is actually, how many episodes do you want to train your agent on? So how many

06:38:46.120 --> 06:38:51.560
times do you want it to run around and explore the environment? That's what episode stands for.

06:38:52.280 --> 06:38:57.480
Max steps essentially says, Okay, so if we're in the environment, and we're kind of navigating

06:38:57.560 --> 06:39:01.400
and moving around, and we haven't died yet, how many steps are we going to let the agent take

06:39:01.400 --> 06:39:05.800
before we cut it off? Because what could happen is we could just bounce in between two different

06:39:05.800 --> 06:39:10.760
states indefinitely. So we need to make sure we have a max steps so that at some point,

06:39:10.760 --> 06:39:15.000
if the agent is just doing the same thing, we can, you know, end that or if it's like going in

06:39:15.000 --> 06:39:21.480
circles, we can end that and start again with different, you know, Q values. Alright, so episodes,

06:39:21.480 --> 06:39:24.840
yeah, we already talked about that learning rate, we know what that is gamma, we know what that is

06:39:25.400 --> 06:39:29.400
mess with these values as we go through and you'll see the difference it makes in our training.

06:39:29.400 --> 06:39:33.560
I've actually included a graph down below. So we'll talk about that kind of show us the outcome

06:39:33.560 --> 06:39:41.400
of our training. But learning rate, the higher this is, the faster I believe that it learns. Yes,

06:39:41.400 --> 06:39:45.800
so a high learning rate means that each update will introduce larger change to the current state.

06:39:45.800 --> 06:39:49.480
So yeah, so that makes sense based on the equation as well. Just wanted to make sure that I wasn't

06:39:49.480 --> 06:39:54.040
going crazy there. So let's run this constant block to make sure. And now we're going to talk

06:39:54.040 --> 06:39:58.520
about picking an action. So remember how I said, and I actually wrote them down here,

06:39:58.520 --> 06:40:04.520
there's essentially two things we can do at every, what do we call it, step, right? We can

06:40:04.520 --> 06:40:09.560
randomly pick a valid action, or we can use the current Q table to find the best action. So how

06:40:09.560 --> 06:40:13.480
do we actually implement that into our open AI gym? Well, I just wanted to write a little

06:40:13.480 --> 06:40:18.200
code block here to show you the exact code that will do this for us. So we're going to introduce

06:40:18.200 --> 06:40:25.160
this new concept or this new, I can almost call it constant, called epsilon. And I think epsilon,

06:40:25.160 --> 06:40:30.520
I think I spelt this wrong, ep salon. Yeah, that should be how you spell it. So we're going to start

06:40:30.520 --> 06:40:34.120
the epsilon value essentially tells us the percentage chance that we're going to pick a

06:40:34.120 --> 06:40:39.480
random action. So here, we're going to use a 90% epsilon, which essentially means that every time

06:40:39.480 --> 06:40:43.960
we take an action, there's going to be a 90% chance that it's random and 10% chance that we look at

06:40:43.960 --> 06:40:49.720
the Q table to make that action. Now, we'll reduce this epsilon value as we train, so that

06:40:49.720 --> 06:40:53.960
our model will start being able to explore, you know, as much as it possibly can in the

06:40:53.960 --> 06:40:59.160
environment by just taking random actions. And then after we have enough observations,

06:40:59.160 --> 06:41:02.920
and we've explored the environment enough, we'll start to slowly decrease the epsilon,

06:41:02.920 --> 06:41:08.120
so that it hopefully finds a more optimal route for things to do. Now, the way we do this is we

06:41:08.120 --> 06:41:12.360
save NP dot random dot uniform zero one, which essentially means pick a random value between

06:41:12.360 --> 06:41:19.880
zero and one is less than epsilon and epsilon like that. I think I'm going to have to change

06:41:19.880 --> 06:41:25.000
some other stuff, but we'll see, then action equals ENV dot action space dot sample. So

06:41:25.000 --> 06:41:29.880
take a random action. That's what this means store what that action is in here. Otherwise,

06:41:29.880 --> 06:41:37.480
we're going to take the argument max of the state row in the Q table. So what this means is find

06:41:37.480 --> 06:41:41.800
the maximum value in the Q table and tell us what row it's in. So that way we know what

06:41:41.800 --> 06:41:46.680
action to take. So if we're in row, I guess, not sorry, not row column for in column one,

06:41:46.680 --> 06:41:50.280
you know, that's maximum value, take action one, that's what this is saying. So using the Q table

06:41:50.280 --> 06:41:55.080
to pick the best action. Alright, so we don't need to run this because this is just going to be

06:41:55.080 --> 06:41:59.640
which I just wrote that to show you. Now, how do we update the Q values? Well, this is just

06:41:59.640 --> 06:42:04.040
following the equation that I showed above. So this is the line of code that does this, I just

06:42:04.040 --> 06:42:08.040
want to write it out so you guys could see exactly what each line is doing and kind of explore it

06:42:08.040 --> 06:42:11.960
for yourself. But essentially, you get the point, you know, you have your learning rate, reward,

06:42:11.960 --> 06:42:17.160
gamma, take the max, so NP dot max does the same thing as a max function in Python. This is going

06:42:17.160 --> 06:42:22.680
to take the max value, not the argument max from the next state, right, the new state that we moved

06:42:22.680 --> 06:42:28.200
into. And then subtracting obviously Q state action. Alright, so putting it all together. So

06:42:28.200 --> 06:42:32.360
now we're actually going to show how we can train and create this Q table and then use that Q table.

06:42:33.000 --> 06:42:37.320
So this is the pretty much all this code that I have, we've already actually

06:42:37.320 --> 06:42:41.400
written at least this block here, that's why I put it in its own block. So just all the constants,

06:42:41.400 --> 06:42:45.000
I've included this render constant to tell us whether we want to draw the environment or not.

06:42:45.000 --> 06:42:47.880
In this case, I'm going to leave it false, but you can make it true if you want.

06:42:47.880 --> 06:42:52.360
Episodes, I've left at 1500 for this, if you want to make your model better, typically you

06:42:52.360 --> 06:42:57.000
train it on more episodes, but that's up to you. And now we're going to get into the big chunk

06:42:57.000 --> 06:43:02.200
of code, which I'm going to talk about. So what this is going to do, we're going to have a rewards

06:43:02.200 --> 06:43:06.280
list, which is actually just going to store all the rewards we see, just so I can graph that later

06:43:06.280 --> 06:43:11.080
for you guys. Then we're going to say for episode in range episodes. So this is just telling us,

06:43:11.080 --> 06:43:16.200
you know, for every episode, let's do the steps I'm about to do. So maximum amount of episodes,

06:43:16.200 --> 06:43:20.120
which is our training length, essentially, we're going to reset the state, obviously,

06:43:20.120 --> 06:43:24.440
which makes sense. So state equals in V dot reset, which will give us the starting state.

06:43:25.080 --> 06:43:29.400
We're going to say for underscore in range, max steps, which means, okay, we're going to do,

06:43:29.400 --> 06:43:34.040
you know, we're going to explore the environment up to maximum steps, we do have a done here,

06:43:34.120 --> 06:43:38.040
which will actually break the loop if we've reached the goal, which we'll talk about further.

06:43:38.600 --> 06:43:42.280
So the first thing we're going to do is say, if render, you know, render the environment,

06:43:42.280 --> 06:43:47.640
that's pretty straightforward. Otherwise, let's take an action. So for each time step, we need to

06:43:47.640 --> 06:43:52.200
take an action. So epsilon, I think is spelled correctly here. Yeah, believe that's right. So

06:43:52.200 --> 06:43:56.280
I'm going to say action equals in V dot action space, this is already the code we've looked at.

06:43:56.280 --> 06:44:01.160
And then what we're going to say is next state reward done underscore equals in V dot step

06:44:01.160 --> 06:44:05.720
action, we've put an underscore here, because we don't really care about this info value. So

06:44:05.720 --> 06:44:09.400
I'm not going to store it, but we do care about what the next state will be the reward from that

06:44:09.400 --> 06:44:14.840
action. And if we were done or not. So we take that action, that's what does this EMB dot step.

06:44:15.480 --> 06:44:21.240
And then what we do is say Q state action, we just update the Q value using the formula that

06:44:21.240 --> 06:44:25.480
we've talked about. So this is the formula, you can look at it more in depth if you want.

06:44:25.480 --> 06:44:29.080
But based on whatever the reward is, you know, that's how we're going to update those Q values.

06:44:29.080 --> 06:44:34.600
And after a lot of training, we should have some decent Q values in there. Alright, so then we

06:44:34.600 --> 06:44:38.680
set the current state to be the next state. So that when we run this time step again,

06:44:39.320 --> 06:44:43.400
now our agent is in the next state, and can start exploring the environment again,

06:44:44.040 --> 06:44:49.000
in this current, you know, iteration, almost, if that makes sense. So then we say if done,

06:44:49.000 --> 06:44:53.640
so essentially, if the agent died, or if they lost or whatever it was, we're going to append

06:44:53.640 --> 06:45:00.600
whatever reward they got from their last step into the rewards up here. And it's worthy of

06:45:00.600 --> 06:45:06.200
noting that the way the rewards work here is you get one reward, if you move to a valid block,

06:45:06.200 --> 06:45:11.000
and you get zero reward, if you die. So every time we move to a valid spot, we get one,

06:45:11.000 --> 06:45:15.640
otherwise we get zero. I'm pretty sure that's the way it works at least. But that's something

06:45:15.640 --> 06:45:20.120
that's important to know. So then what we're going to do is reduce the epsilon if we die,

06:45:20.120 --> 06:45:25.400
but just a fraction of an amount, you know, 0.001, just so we slowly start decreasing the epsilon

06:45:25.400 --> 06:45:29.560
moving in the correct direction. And then we're going to break because we've reached the goals,

06:45:29.560 --> 06:45:33.320
print the Q table, and then print the average reward. Now this takes a second to train,

06:45:34.120 --> 06:45:39.160
like, you know, a few seconds, really. That one is pretty fast, because I've set this at

06:45:39.160 --> 06:45:43.880
was it 1500. But if you want, you can set this at say 10,000, wait another, you know,

06:45:43.880 --> 06:45:48.840
few minutes or whatever, and then see how much better you can do. So we can see that after that,

06:45:48.920 --> 06:45:56.040
I received an average reward of 0.28886667. This is actually what the Q table values look like.

06:45:56.040 --> 06:46:00.040
So all these decimal values after all these updates, I just decided to print them out.

06:46:00.040 --> 06:46:04.040
And I just want to show you the average reward so that we can compare that to what we can get

06:46:04.040 --> 06:46:08.120
from testing or this graph. So now I'm just going to graph this. And we're going to see this is

06:46:08.120 --> 06:46:11.160
what the graph so you don't have to really understand this code if you don't want to. But

06:46:11.160 --> 06:46:17.480
this is just graphing the average reward over 100 steps from the beginning to the end. So

06:46:17.480 --> 06:46:22.360
essentially, I've been, I've calculated the average of every 100 episodes, and then just

06:46:22.360 --> 06:46:26.840
graph this on here. We can see that we start off very poorly in terms of reward, because the

06:46:26.840 --> 06:46:31.480
epsilon value is quite high, which means that we're taking, you know, random actions pretty

06:46:31.480 --> 06:46:35.080
much all the time. So if we're taking a bunch of random actions, obviously, chances are,

06:46:35.080 --> 06:46:38.920
we're probably going to die a lot, we're probably going to get rewards of zeros quite frequently.

06:46:38.920 --> 06:46:43.560
And then after we get to about 600 episodes, you can see that six actually represents 600,

06:46:43.640 --> 06:46:47.720
because this is in hundreds, we start to slowly increase. And then actually, we go on a crazy

06:46:47.720 --> 06:46:53.960
increase here, when we start to take values more frequently. So the epsilon is increasing,

06:46:53.960 --> 06:46:59.000
right. And then after we get here, we kind of level off. And this does show a slight decline.

06:46:59.000 --> 06:47:02.840
But I guarantee you if we ran this for, you know, like 15,000, it would just go up and down and

06:47:02.840 --> 06:47:07.320
bob up and down. And that's just because even though we have increased the epsilon, there is

06:47:07.320 --> 06:47:11.880
still a chance that we take a random action and you know, gets your reward. So that is pretty

06:47:11.880 --> 06:47:16.280
much it for this Q learning example. You know, I mean, that's pretty straightforward

06:47:16.840 --> 06:47:21.560
to use the Q table. If you actually wanted to say, you know, watch the agent move around the

06:47:21.560 --> 06:47:26.040
thing, I'm going to leave that to you guys, because if you can follow what I've just done in here

06:47:26.040 --> 06:47:30.600
and understand this, it's actually quite easy to use the Q table. And I think as like a final,

06:47:30.600 --> 06:47:35.720
almost like, you know, trust in you guys, you can figure out how to do that. The hint is essentially

06:47:35.720 --> 06:47:40.600
do exactly what I've done in here, except don't update the Q table values, just use the Q table

06:47:40.600 --> 06:47:45.800
values already. And that's, you know, pretty much all there is to Q learning. So this has

06:47:45.800 --> 06:47:50.680
been the reinforcement learning module for this TensorFlow course, which actually is the last

06:47:50.680 --> 06:47:54.760
module in this series. Now, I hope you guys have enjoyed up until this point, just an emphasis

06:47:54.760 --> 06:47:59.720
again, this was really just an introduction to reinforcement learning. This technique and this

06:47:59.720 --> 06:48:04.440
problem itself is not very interesting and not, you know, the best way to do things is not the

06:48:04.440 --> 06:48:08.440
most powerful. It's just to get you thinking about how reinforcement learning works. And

06:48:08.520 --> 06:48:12.280
potentially, if you'd like to look into that more, there's a ton of different resources and,

06:48:12.280 --> 06:48:15.720
you know, things you can look at in terms of reinforcement learning. So that being said,

06:48:15.720 --> 06:48:18.840
that has been this module. And now we're going to move into the conclusion, we'll talk about

06:48:18.840 --> 06:48:22.920
some next steps and some more things that you guys can look at to improve your machine learning skills.

06:48:26.280 --> 06:48:32.280
So finally, after about seven hours of course content, we have reached the conclusion of this

06:48:32.280 --> 06:48:37.480
course. Now what I'm going to do in this last brief short section is just explain to you where

06:48:37.560 --> 06:48:42.440
you can go for some next steps and some further learning with TensorFlow and machine learning

06:48:42.440 --> 06:48:46.520
artificial intelligence in general. Now what I'm going to be recommending to you guys is that we

06:48:46.520 --> 06:48:51.560
look at the TensorFlow website, because they have some amazing guides and resources on here. And in

06:48:51.560 --> 06:48:57.320
fact, a lot of the examples that we used in our notebooks were based off of or exactly the same

06:48:57.320 --> 06:49:01.960
as the original TensorFlow guide. And that's because the code that they have is just very good.

06:49:01.960 --> 06:49:07.080
They're very good and easy to understand examples. And in terms of learning, I find that these

06:49:07.080 --> 06:49:11.240
guides are great for people that want to get in quickly, see the examples and then go and do some

06:49:11.240 --> 06:49:16.520
research on their own time and understand why they work. So if you're looking for some further steps,

06:49:16.520 --> 06:49:21.720
at this point in time, you have gained a very general and broad knowledge of machine learning

06:49:21.720 --> 06:49:27.240
and AI, you have some basic skills in a lot of the different areas. And hopefully this has

06:49:27.240 --> 06:49:33.160
introduced you to a bunch of different concepts and the possibilities of what you are able to do

06:49:33.160 --> 06:49:38.360
using modules like TensorFlow. Now what I'm going to suggest to all of you is that if you find a

06:49:38.360 --> 06:49:43.640
specific area of machine learning AI that you are very interested in, that you would dial in on

06:49:43.640 --> 06:49:48.840
that area and focus most of your time into learning that, that is because when you get to a point in

06:49:48.840 --> 06:49:53.320
machine learning and AI, where you really get specific and pick one kind of strain or one

06:49:53.320 --> 06:49:57.960
kind of area, it gets very interesting very quickly. And you can devote most of your time

06:49:57.960 --> 06:50:02.120
to getting as deep as possible and not specific topic. And that's something that's really cool.

06:50:02.120 --> 06:50:07.320
And most people that are experts in AI or machine learning field typically have one area of

06:50:07.320 --> 06:50:11.720
specialization. Now, if you're someone who doesn't care to specialize an area or you just want to

06:50:11.720 --> 06:50:16.360
play around and see some different things, the TensorFlow website is great to really get kind

06:50:16.360 --> 06:50:21.160
of a general introduction to a lot of different areas and be able to kind of use this code tweak

06:50:21.160 --> 06:50:25.480
it a little bit on your own, and implement it into your own projects. And in fact, the next kind

06:50:25.480 --> 06:50:30.280
of steps and resources I'm going to be showing you here, and involve simply going to the TensorFlow

06:50:30.360 --> 06:50:34.840
website, going to the tutorial page, this is very easy to find, I don't even need to link it,

06:50:34.840 --> 06:50:39.160
you can just search TensorFlow, and you'll find this online. And looking at some more advanced

06:50:39.160 --> 06:50:44.840
topics that we haven't covered. So we've covered a few of the topics and tutorials that are here,

06:50:44.840 --> 06:50:49.240
I've just kind of modified their version, and thrown out in the notebook and explained it in

06:50:49.240 --> 06:50:53.800
wars and video content. But if you'd like to move on to say a next step or something very cool,

06:50:53.800 --> 06:50:59.320
something I would recommend is doing the deep dream in the generic generative neural network

06:50:59.320 --> 06:51:03.560
section on the TensorFlow website, being able to make something like this, I think is very cool.

06:51:03.560 --> 06:51:09.240
And this is an example where you can tweak this a ton by yourself and get some really cool results.

06:51:09.240 --> 06:51:13.880
So some things like this are definitely next steps, there's tons and tons of guides and tutorials

06:51:13.880 --> 06:51:18.360
on this website, they make it very easy for anyone to get started. And with these guides,

06:51:18.360 --> 06:51:22.200
what I will say is typically what will end up happening is they just give you the code and

06:51:22.200 --> 06:51:27.480
brief explanations of why things work. You should really be researching and looking up some more,

06:51:27.560 --> 06:51:31.960
you know, deep level explanations of why some of these things work as you go through, if you

06:51:31.960 --> 06:51:37.240
want to have a firm and great understanding of why the model performs the way that it does.

06:51:37.240 --> 06:51:42.360
So with that being said, I believe I'm going to wrap up the course now. I know you guys can imagine

06:51:42.360 --> 06:51:47.400
how much work I put into this. So please do leave a like, subscribe to the channel, leave a content,

06:51:47.400 --> 06:51:52.360
show your support. This I believe is the largest open source machine learning course in the world

06:51:52.360 --> 06:51:57.960
that deals completely with TensorFlow and Python. And I hope that this gave you a lot of knowledge.

06:51:57.960 --> 06:52:01.960
So please do give me your feedback down below in the comments. With that being said, again,

06:52:01.960 --> 06:52:07.400
I hope you enjoyed. And I hopefully I will see you again in another tutorial guide or series.

