WEBVTT

00:00.000 --> 00:07.520
Since the deep learning explosion started in 2012, the industry's biggest models have

00:07.520 --> 00:10.160
grown hundreds of thousands of times.

00:10.160 --> 00:18.520
Today, OpenAI's Dolly 2 has 3.5 billion parameters, Google's Imagine has 4.6 billion, and GPT-3

00:18.520 --> 00:22.360
has 175 billion parameters.

00:22.360 --> 00:27.040
Increasingly larger models are ahead, Google recently pre-trained a model with 1 trillion

00:27.040 --> 00:29.160
parameters.

00:29.160 --> 00:33.760
These increasingly large models strain the ability of our hardware to accommodate them,

00:33.760 --> 00:38.520
and many of these limitations tie back to memory and how we use it.

00:38.520 --> 00:42.520
In this video, we're going to look at deep learning's memory wall problem, and some

00:42.520 --> 00:47.480
of the memory-centric paradigms researchers are looking at to solve it.

00:47.480 --> 00:49.680
But first, the Asianometry Patreon.

00:49.680 --> 00:53.520
Early access members get to watch new videos and selected references for them before their

00:53.520 --> 00:55.000
release in the public.

00:55.000 --> 01:00.280
To help support the videos and appreciate every pledge, thanks, and on with the show.

01:00.280 --> 01:05.280
Virtually every modern computer runs what is called a von Neumann architecture, meaning

01:05.280 --> 01:10.280
that it stores both its instructions and data on the same memory bank.

01:10.280 --> 01:14.840
At its heart are your processing units, a CPU or a GPU.

01:14.840 --> 01:21.480
Those processing units access memory to execute its instructions and process its data.

01:21.480 --> 01:24.560
The von Neumann architecture has been really good for us.

01:24.560 --> 01:29.800
It helped make software as powerful as it is today, but it works nothing like a real

01:29.800 --> 01:31.520
human brain.

01:31.520 --> 01:36.200
The brain's compute ability is relatively low precision, but it tightly integrates

01:36.200 --> 01:41.200
that compute with memory and input-output communication.

01:41.200 --> 01:46.520
Computers on the other hand run on high precision, 32 or 64-bit floating-point arithmetic for

01:46.520 --> 01:51.600
instance, but separates that compute from memory and communication.

01:51.600 --> 01:56.040
This separation has consequences, especially for memory.

01:56.040 --> 02:00.880
The AI hardware industry is scaling up memory and processing unit performance as fast as

02:00.880 --> 02:01.880
they can.

02:01.880 --> 02:08.000
NVIDIA's V100 GPU released in 2017 had a 32 gigabyte offering.

02:08.000 --> 02:15.600
Today, the top of the line NVIDIA data center GPUs A100 and H100 sport 80 gigabytes of memory.

02:15.800 --> 02:20.160
Despite this bulking up, hardware performance is not keeping up with how fast these models

02:20.160 --> 02:24.040
are growing, especially when it comes to memory.

02:24.040 --> 02:29.280
Memory allocations for leading edge models can easily exceed hundreds of gigabytes.

02:29.280 --> 02:33.680
Even with the latest parallelization techniques, a trillion-parameter model is estimated to

02:33.680 --> 02:40.280
require 320 A100 GPUs each with 80 gigabytes of memory.

02:40.280 --> 02:45.240
These differences in processing and capacity mean that a processing unit wastes multiple

02:45.240 --> 02:50.280
processing cycles, waiting not only for all of the data to travel in and out of the memory,

02:50.280 --> 02:55.200
but also for the memory to perform its read-slash-write operation.

02:55.200 --> 02:59.800
This limitation is known as the memory wall, or memory capacity bottleneck.

02:59.800 --> 03:05.280
Alright then, the obvious solution would then be to add more memory to our GPUs, right?

03:05.280 --> 03:07.600
What is stopping us from doing that?

03:07.600 --> 03:11.840
There are practical and technology limits to how much extra memory you can add on, not

03:11.840 --> 03:15.120
to mention the issues of connections and wiring.

03:15.120 --> 03:19.600
Just think about how widening a highway does not much help with traffic.

03:19.600 --> 03:24.640
Additionally, there are very significant energy limitations associated with shuttling data

03:24.640 --> 03:27.200
between the chip and the memory.

03:27.200 --> 03:30.960
These electric connections have losses which cost energy.

03:30.960 --> 03:33.440
I mentioned this in previous videos.

03:33.440 --> 03:38.920
Accessing memory off chip uses 200 times more energy than a floating point operation.

03:39.480 --> 03:44.920
80% of the Google TPU's energy usage is from its electrical connections rather than its

03:44.920 --> 03:47.520
logic computational units.

03:47.520 --> 03:53.920
In some recent GPU and CPU systems, DRAM by itself accounts for 40% of the total system

03:53.920 --> 03:55.440
power.

03:55.440 --> 04:01.440
Energy makes up 40% of a data center's operating costs, so for this reason, storage and memory

04:01.440 --> 04:07.880
has come to be a significant factor in the data center's ongoing profitability.

04:07.880 --> 04:12.620
In addition to the significant operating costs of the energy, are the upfront capital

04:12.620 --> 04:16.320
costs of purchasing the AI hardware itself.

04:16.320 --> 04:22.440
As I mentioned earlier, a possible trillion parameter model would need 320 A100 GPUs,

04:22.440 --> 04:24.560
each with 80 gigabytes of memory.

04:24.560 --> 04:30.880
A100s cost $32,000 each at MSRP, so that's a clean $10 million.

04:30.880 --> 04:36.460
A 100 trillion parameter model might require over 6,000 such GPUs.

04:36.460 --> 04:40.740
That is just the cost of purchasing the hardware and does not even count the aforementioned

04:40.740 --> 04:45.900
energy costs of using these things to run inference on them, which is where 90% of a

04:45.900 --> 04:48.420
model's total costs are.

04:48.420 --> 04:55.700
It risks restricting the benefits of advanced AI only to the Uber-rich tech giants or governments.

04:55.700 --> 04:59.700
Much of these shortcomings are tied to historical and technological limits.

04:59.700 --> 05:05.620
In the 1960s and 70s, the industry adopted dynamic random access memory to form the basis

05:05.620 --> 05:07.380
of our computers.

05:07.380 --> 05:11.300
This adoption was largely made for technological and economic reasons.

05:11.300 --> 05:16.860
DRAM memories had relatively low latency and were cheap to manufacture in bulk.

05:16.860 --> 05:18.500
This worked fine for a while.

05:18.500 --> 05:25.020
As late as 1995, the memory industry was valued at $37 billion, with microprocessors at $20

05:25.020 --> 05:26.780
billion.

05:26.780 --> 05:31.540
But after 1980, compute scaling far outpaced memory scaling.

05:31.540 --> 05:36.620
This is because, generally speaking, the CPU or GPU industries have had just one metric

05:36.620 --> 05:40.220
to optimize towards, transistor density.

05:40.220 --> 05:44.900
The memory industry, on the other hand, not only has the scaled DRAM capacity but also

05:44.900 --> 05:48.140
bandwidth and latency at the same time.

05:48.140 --> 05:52.020
Something has to give and usually that has been latency.

05:52.020 --> 05:59.140
Over the past 20 years, memory capacity has improved 128 times and bandwidth 20 times.

05:59.140 --> 06:02.580
latency however, has improved by just 30%.

06:02.580 --> 06:08.100
Secondly, the memory industry realized that shrinking DRAM cells beyond a certain size

06:08.100 --> 06:09.860
gave you worse performance.

06:09.860 --> 06:16.580
Less reliability, less secure, worse latency, energy inefficiency, and so on.

06:16.580 --> 06:17.580
Here's why.

06:17.580 --> 06:22.620
A DRAM cell stores one bit of data in a form of a charge within a capacitor, a capacitor

06:22.620 --> 06:26.940
being a device that stores electrical energy within a field.

06:26.940 --> 06:31.620
That bit is accessed using an access transistor.

06:31.620 --> 06:37.420
As you scale the cell down to nanoscale sizes, that capacitor and its access transistor get

06:37.420 --> 06:41.700
leakier and more vulnerable to outside electrical noise.

06:41.700 --> 06:46.180
It also opens up new security vulnerabilities.

06:46.180 --> 06:50.780
These technical limitations and problems are fundamental to how the hardware works, which

06:50.780 --> 06:55.180
makes them extremely difficult to engineer our way around.

06:55.180 --> 07:00.980
The industry is going to grind out small solutions, but those will be small.

07:00.980 --> 07:06.300
So the problem also opens the door to brand new radical ideas that might give us a possible

07:06.300 --> 07:10.140
10x improvement over the current paradigm.

07:10.140 --> 07:14.340
In a previous video, I talked about the Silicon Photonic AI accelerator, where we tried to

07:14.340 --> 07:19.060
use light's properties to make data transfer more energy efficient.

07:19.060 --> 07:21.420
Alongside that, we have another idea.

07:21.420 --> 07:25.540
Let's alleviate or even possibly eliminate the von Neumann bottleneck and memory wall

07:25.540 --> 07:31.420
problems by making the memory do the computations themselves.

07:31.420 --> 07:35.860
Compute in memory refers to a random access memory with processing elements integrated

07:35.860 --> 07:36.860
together.

07:36.860 --> 07:42.140
The two might be very near each other or even be integrated onto the same die.

07:42.140 --> 07:45.860
I've seen the idea be called other things throughout the years.

07:45.860 --> 07:50.220
Processing and memory, computational RAM, near data computing, memory-centric computing,

07:50.220 --> 07:53.180
main memory computation, and so on.

07:53.180 --> 07:57.820
I'm aware that there are differences between these usages, but those differences are very

07:57.820 --> 07:58.820
subtle.

07:58.820 --> 08:03.300
I'm generally going to stick to saying compute in memory.

08:03.300 --> 08:07.900
The name has also been used to refer to concepts expanding on the SRAM idea.

08:07.900 --> 08:13.380
SRAM is often used for the memory cache that sits on chip with the CPU.

08:13.380 --> 08:17.980
But what we are more referring to here is bringing processing and compute ability to

08:17.980 --> 08:20.940
the computer main memory itself.

08:20.940 --> 08:23.380
The idea is well suited for deep learning.

08:23.380 --> 08:28.620
If you recall, running a neural network model is about calculating these massive matrices.

08:28.620 --> 08:35.780
The Google TPU had lots of circuits for running multiply and accumulate or MAC operations.

08:35.780 --> 08:38.220
The actual arithmetic is relatively simple.

08:38.220 --> 08:41.340
The problem is that there is so much of it that needs to be done.

08:41.340 --> 08:47.260
So in an ideal case, a compute in memory chip can execute MAC operations right inside the

08:47.260 --> 08:48.980
memory chip.

08:48.980 --> 08:53.860
This is especially helpful for running inference on the edge outside the data center.

08:53.860 --> 08:57.500
These use cases have energy size or heat restrictions.

08:57.500 --> 09:03.540
Being able to cut up to 80% of a neural network's energy usage is a game changer.

09:03.540 --> 09:07.700
The idea is decades old, dating back to the 1960s.

09:07.700 --> 09:13.540
Professor Harold Stone of Stanford University first notably explored the idea of logic and

09:13.540 --> 09:14.540
memory.

09:14.660 --> 09:19.780
Stone noted that the number of transistors in a microprocessor was growing very fast,

09:19.780 --> 09:23.820
but the processor's ability to communicate with its memory was limited by the number

09:23.820 --> 09:25.120
of pins.

09:25.120 --> 09:31.020
So he presented the idea of moving part of the computation into memory caches.

09:31.020 --> 09:34.420
The 1990s saw further explorations of the idea.

09:34.420 --> 09:39.060
In 1995, terraces produced what we would probably call the first processor in memory

09:39.060 --> 09:40.060
chip.

09:40.060 --> 09:45.060
It was a standard 4-bit memory with an integrated single-bit logical unit.

09:45.060 --> 09:50.580
This arithmetic logical unit can bring in data, apply some simple logic to it as dictated

09:50.580 --> 09:54.140
from a program, and then write it back to memory.

09:54.140 --> 09:59.660
And then in 1997, various professors at UC Berkeley, including Professor David Patterson,

09:59.660 --> 10:05.220
the inventor of risk, created the IRAM project, with the goal of putting a microprocessor and

10:05.220 --> 10:08.220
DRAM on the same chip.

10:08.220 --> 10:12.340
While other such proposals followed throughout the 1990s, but none of these ever caught

10:12.340 --> 10:14.900
on for a number of practical reasons.

10:14.900 --> 10:18.780
First, memory and logic are hard to manufacture together.

10:18.780 --> 10:22.220
Their fabrication have sort of opposing goals.

10:22.220 --> 10:28.380
Again, logic transistors are all about speed and performance, but memory transistors have

10:28.380 --> 10:33.220
to be about high density, low cost, and low leakage all at once.

10:33.980 --> 10:38.580
It is hard to build a logic process with a DRAM process and vice versa.

10:38.580 --> 10:42.380
DRAM designs are very regular, with lots of parallel wires.

10:42.380 --> 10:46.740
Logic designs, on the other hand, have much more complexity.

10:46.740 --> 10:50.820
Circuit elements in a chip have connections, referred to as metal layers.

10:50.820 --> 10:56.540
Having more metal layers allows for more complexity, but at the cost of current leakage and worse

10:56.540 --> 10:58.540
reliability.

10:58.540 --> 11:02.500
Contemporary DRAM processes use three to four metal layers.

11:02.500 --> 11:08.140
Contemporary logic processes use anywhere from 7 to 12 and even more metal layers.

11:08.140 --> 11:13.220
So it is estimated that if you tried to make logic circuits with a DRAM process, then the

11:13.220 --> 11:18.460
logic circuits would be 80% bigger and perform 22% worse.

11:18.460 --> 11:23.620
And if you were to try to make DRAM cells with a logic process, then you are essentially

11:23.620 --> 11:27.180
making embedded DRAM, or eDRAM.

11:27.220 --> 11:34.260
The cells use significantly more power, take up to 10 times more space, and are less reliable.

11:34.260 --> 11:39.020
The industry has since considered these manufacturing shortcomings and have come up with a variety

11:39.020 --> 11:40.500
of workarounds.

11:40.500 --> 11:47.580
Many proposals of compute and memory are on three levels, the device, circuit, and system.

11:47.580 --> 11:52.300
The device level leans on new types of memory hardware other than the conventional DRAM

11:52.300 --> 11:54.060
and SRAM memories.

11:54.060 --> 12:01.580
Notable examples include resistive random access memory, or rerAM, or rRAM, and spin

12:01.580 --> 12:08.820
transfer torque magneto resistive random access memory, or STT-MRAM.

12:08.820 --> 12:12.580
RerAM is one of the more promising emerging memory technologies.

12:12.580 --> 12:17.580
Like I mentioned earlier, conventional RAM memories store information using a charge

12:17.580 --> 12:20.420
stored within a capacitor.

12:20.420 --> 12:25.740
RerAM instead stores information by changing the electrical resistance of a certain material,

12:25.740 --> 12:31.380
referred to as a resistor, switching between a high and low resistance state.

12:31.380 --> 12:38.140
This structure allows rerAM to compute logical functions directly within the memory cells.

12:38.140 --> 12:42.740
Please don't ask me to try to explain it any further than that.

12:42.740 --> 12:47.420
RerAM is probably the emerging memory technology that is closest to commercialization due to

12:47.420 --> 12:53.580
it being compatible with silicon CMOS, however there remain substantial hurdles to overcome

12:53.580 --> 12:57.740
before we see products arrive on shelves.

12:57.740 --> 13:02.620
The circuit level is where we modify peripheral circuits to do the calculations right inside

13:02.620 --> 13:06.820
the SRAM or DRAM memory arrays themselves.

13:06.820 --> 13:13.300
The phrase I see a lot is in C2 computing, in C2 meaning locally or on site.

13:13.300 --> 13:17.180
Memories are particularly clever, but they also require an intimate knowledge of how

13:17.180 --> 13:21.980
memory works and still can be difficult to understand.

13:21.980 --> 13:27.300
One prominent example of this is Ombit, an in-memory accelerator proposed by people from

13:27.300 --> 13:33.300
Microsoft, NVIDIA, Intel, ETH, Zurich, and Carnegie Mellon.

13:33.300 --> 13:38.420
A DRAM memory contains sub arrays with many rows of DRAM cells.

13:38.420 --> 13:42.700
In normal use, the memory activates one row at a time.

13:42.700 --> 13:48.380
This system activates three rows at a time in order to implement an AND slash OR logic

13:48.380 --> 13:53.440
function, two rows for the inputs and one for the output.

13:53.440 --> 13:55.540
The concept is logically attractive.

13:55.540 --> 13:59.900
You can utilize the memory's internal bandwidth to do all the calculations.

13:59.900 --> 14:03.220
However, there are significant concerns.

14:03.220 --> 14:10.020
Ombit can perform basing AND or and NOT logic operations, but it takes multiple cycles.

14:10.020 --> 14:17.100
Furthermore, more complex logic implementations like XNOR remain challenging to utilize.

14:17.100 --> 14:22.180
So far, the big downside with these two compute and memory approaches is that their performance

14:22.180 --> 14:27.460
still falls short of what can be achieved with current Von Neumann GPU slash ASIC-centric

14:27.460 --> 14:29.020
systems.

14:29.020 --> 14:33.740
In other words, they suffer the same drawbacks people saw in the 1990s.

14:33.740 --> 14:40.180
Putting memory and logic together still makes a jack of all trades master of none situation.

14:40.180 --> 14:44.460
So the middle ground that the industry seems to be moving towards is implementing compute

14:44.460 --> 14:46.940
and memory at a system level.

14:46.940 --> 14:51.660
This is where we integrate together discrete processing units and memory at a very close

14:51.660 --> 14:53.580
level.

14:53.580 --> 15:00.220
This is enabled thanks to new packaging technologies like 2.5D or 3D memory die stacking where we

15:00.220 --> 15:04.700
stack a bunch of DRAM memory dies on top of a CPU die.

15:04.700 --> 15:09.120
The memories are then connected to the CPU using thousands of channels called through

15:09.120 --> 15:12.060
silicon vias or TSVs.

15:12.060 --> 15:15.100
This gives you immense amounts of internal bandwidth.

15:15.100 --> 15:20.580
AMD is working on something kind of like this with what they call 3DVcache, which is based

15:20.580 --> 15:24.300
on a TSMC 3D stacking packaging technology.

15:24.300 --> 15:28.460
They used it to add more memory cache to their processor chips.

15:28.460 --> 15:33.020
There is a future where we can use similarly advanced packaging technologies to add hundreds

15:33.020 --> 15:36.980
of gigabytes of memory to an AI ASIC.

15:36.980 --> 15:42.460
This lets us integrate together world-class memory and logic dies closer than ever before

15:42.460 --> 15:46.460
without needing to place them on the same die.

15:46.460 --> 15:48.300
Ideas in the laboratory are cheap.

15:48.300 --> 15:53.820
What is harder is to execute on those ideas in a way that performs well enough to replace

15:53.820 --> 15:56.580
what is already out there on the market.

15:56.580 --> 16:04.620
And the fact is that the current NVIDIA A100 and H100 AI GPUs are a very formidable competitor.

16:04.620 --> 16:08.740
But with leading edge semiconductor technology slowing down the way they are, we need new

16:08.740 --> 16:15.180
ways to leapfrog towards more powerful and robust hardware for running AI.

16:15.180 --> 16:17.740
Generally speaking, bigger models perform better.

16:17.740 --> 16:23.220
Today's best performing natural language processing and computer vision models are great, but they

16:23.220 --> 16:29.180
still have ways to go, which means they might have to get bigger so to get better.

16:29.180 --> 16:35.020
But unless we develop new systems and hardware that can overcome these aforementioned limits,

16:35.020 --> 16:39.620
it seems that deep learning might fall short of fulfilling its great expectations.

16:39.620 --> 16:42.020
Alright everyone, that's it for tonight.

16:42.020 --> 16:43.020
Thanks for watching.

16:43.020 --> 16:45.620
Subscribe to the channel, sign up for the newsletter, and I'll see you guys next time.

