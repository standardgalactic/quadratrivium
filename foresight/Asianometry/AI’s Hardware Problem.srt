1
00:00:00,000 --> 00:00:07,520
Since the deep learning explosion started in 2012, the industry's biggest models have

2
00:00:07,520 --> 00:00:10,160
grown hundreds of thousands of times.

3
00:00:10,160 --> 00:00:18,520
Today, OpenAI's Dolly 2 has 3.5 billion parameters, Google's Imagine has 4.6 billion, and GPT-3

4
00:00:18,520 --> 00:00:22,360
has 175 billion parameters.

5
00:00:22,360 --> 00:00:27,040
Increasingly larger models are ahead, Google recently pre-trained a model with 1 trillion

6
00:00:27,040 --> 00:00:29,160
parameters.

7
00:00:29,160 --> 00:00:33,760
These increasingly large models strain the ability of our hardware to accommodate them,

8
00:00:33,760 --> 00:00:38,520
and many of these limitations tie back to memory and how we use it.

9
00:00:38,520 --> 00:00:42,520
In this video, we're going to look at deep learning's memory wall problem, and some

10
00:00:42,520 --> 00:00:47,480
of the memory-centric paradigms researchers are looking at to solve it.

11
00:00:47,480 --> 00:00:49,680
But first, the Asianometry Patreon.

12
00:00:49,680 --> 00:00:53,520
Early access members get to watch new videos and selected references for them before their

13
00:00:53,520 --> 00:00:55,000
release in the public.

14
00:00:55,000 --> 00:01:00,280
To help support the videos and appreciate every pledge, thanks, and on with the show.

15
00:01:00,280 --> 00:01:05,280
Virtually every modern computer runs what is called a von Neumann architecture, meaning

16
00:01:05,280 --> 00:01:10,280
that it stores both its instructions and data on the same memory bank.

17
00:01:10,280 --> 00:01:14,840
At its heart are your processing units, a CPU or a GPU.

18
00:01:14,840 --> 00:01:21,480
Those processing units access memory to execute its instructions and process its data.

19
00:01:21,480 --> 00:01:24,560
The von Neumann architecture has been really good for us.

20
00:01:24,560 --> 00:01:29,800
It helped make software as powerful as it is today, but it works nothing like a real

21
00:01:29,800 --> 00:01:31,520
human brain.

22
00:01:31,520 --> 00:01:36,200
The brain's compute ability is relatively low precision, but it tightly integrates

23
00:01:36,200 --> 00:01:41,200
that compute with memory and input-output communication.

24
00:01:41,200 --> 00:01:46,520
Computers on the other hand run on high precision, 32 or 64-bit floating-point arithmetic for

25
00:01:46,520 --> 00:01:51,600
instance, but separates that compute from memory and communication.

26
00:01:51,600 --> 00:01:56,040
This separation has consequences, especially for memory.

27
00:01:56,040 --> 00:02:00,880
The AI hardware industry is scaling up memory and processing unit performance as fast as

28
00:02:00,880 --> 00:02:01,880
they can.

29
00:02:01,880 --> 00:02:08,000
NVIDIA's V100 GPU released in 2017 had a 32 gigabyte offering.

30
00:02:08,000 --> 00:02:15,600
Today, the top of the line NVIDIA data center GPUs A100 and H100 sport 80 gigabytes of memory.

31
00:02:15,800 --> 00:02:20,160
Despite this bulking up, hardware performance is not keeping up with how fast these models

32
00:02:20,160 --> 00:02:24,040
are growing, especially when it comes to memory.

33
00:02:24,040 --> 00:02:29,280
Memory allocations for leading edge models can easily exceed hundreds of gigabytes.

34
00:02:29,280 --> 00:02:33,680
Even with the latest parallelization techniques, a trillion-parameter model is estimated to

35
00:02:33,680 --> 00:02:40,280
require 320 A100 GPUs each with 80 gigabytes of memory.

36
00:02:40,280 --> 00:02:45,240
These differences in processing and capacity mean that a processing unit wastes multiple

37
00:02:45,240 --> 00:02:50,280
processing cycles, waiting not only for all of the data to travel in and out of the memory,

38
00:02:50,280 --> 00:02:55,200
but also for the memory to perform its read-slash-write operation.

39
00:02:55,200 --> 00:02:59,800
This limitation is known as the memory wall, or memory capacity bottleneck.

40
00:02:59,800 --> 00:03:05,280
Alright then, the obvious solution would then be to add more memory to our GPUs, right?

41
00:03:05,280 --> 00:03:07,600
What is stopping us from doing that?

42
00:03:07,600 --> 00:03:11,840
There are practical and technology limits to how much extra memory you can add on, not

43
00:03:11,840 --> 00:03:15,120
to mention the issues of connections and wiring.

44
00:03:15,120 --> 00:03:19,600
Just think about how widening a highway does not much help with traffic.

45
00:03:19,600 --> 00:03:24,640
Additionally, there are very significant energy limitations associated with shuttling data

46
00:03:24,640 --> 00:03:27,200
between the chip and the memory.

47
00:03:27,200 --> 00:03:30,960
These electric connections have losses which cost energy.

48
00:03:30,960 --> 00:03:33,440
I mentioned this in previous videos.

49
00:03:33,440 --> 00:03:38,920
Accessing memory off chip uses 200 times more energy than a floating point operation.

50
00:03:39,480 --> 00:03:44,920
80% of the Google TPU's energy usage is from its electrical connections rather than its

51
00:03:44,920 --> 00:03:47,520
logic computational units.

52
00:03:47,520 --> 00:03:53,920
In some recent GPU and CPU systems, DRAM by itself accounts for 40% of the total system

53
00:03:53,920 --> 00:03:55,440
power.

54
00:03:55,440 --> 00:04:01,440
Energy makes up 40% of a data center's operating costs, so for this reason, storage and memory

55
00:04:01,440 --> 00:04:07,880
has come to be a significant factor in the data center's ongoing profitability.

56
00:04:07,880 --> 00:04:12,620
In addition to the significant operating costs of the energy, are the upfront capital

57
00:04:12,620 --> 00:04:16,320
costs of purchasing the AI hardware itself.

58
00:04:16,320 --> 00:04:22,440
As I mentioned earlier, a possible trillion parameter model would need 320 A100 GPUs,

59
00:04:22,440 --> 00:04:24,560
each with 80 gigabytes of memory.

60
00:04:24,560 --> 00:04:30,880
A100s cost $32,000 each at MSRP, so that's a clean $10 million.

61
00:04:30,880 --> 00:04:36,460
A 100 trillion parameter model might require over 6,000 such GPUs.

62
00:04:36,460 --> 00:04:40,740
That is just the cost of purchasing the hardware and does not even count the aforementioned

63
00:04:40,740 --> 00:04:45,900
energy costs of using these things to run inference on them, which is where 90% of a

64
00:04:45,900 --> 00:04:48,420
model's total costs are.

65
00:04:48,420 --> 00:04:55,700
It risks restricting the benefits of advanced AI only to the Uber-rich tech giants or governments.

66
00:04:55,700 --> 00:04:59,700
Much of these shortcomings are tied to historical and technological limits.

67
00:04:59,700 --> 00:05:05,620
In the 1960s and 70s, the industry adopted dynamic random access memory to form the basis

68
00:05:05,620 --> 00:05:07,380
of our computers.

69
00:05:07,380 --> 00:05:11,300
This adoption was largely made for technological and economic reasons.

70
00:05:11,300 --> 00:05:16,860
DRAM memories had relatively low latency and were cheap to manufacture in bulk.

71
00:05:16,860 --> 00:05:18,500
This worked fine for a while.

72
00:05:18,500 --> 00:05:25,020
As late as 1995, the memory industry was valued at $37 billion, with microprocessors at $20

73
00:05:25,020 --> 00:05:26,780
billion.

74
00:05:26,780 --> 00:05:31,540
But after 1980, compute scaling far outpaced memory scaling.

75
00:05:31,540 --> 00:05:36,620
This is because, generally speaking, the CPU or GPU industries have had just one metric

76
00:05:36,620 --> 00:05:40,220
to optimize towards, transistor density.

77
00:05:40,220 --> 00:05:44,900
The memory industry, on the other hand, not only has the scaled DRAM capacity but also

78
00:05:44,900 --> 00:05:48,140
bandwidth and latency at the same time.

79
00:05:48,140 --> 00:05:52,020
Something has to give and usually that has been latency.

80
00:05:52,020 --> 00:05:59,140
Over the past 20 years, memory capacity has improved 128 times and bandwidth 20 times.

81
00:05:59,140 --> 00:06:02,580
latency however, has improved by just 30%.

82
00:06:02,580 --> 00:06:08,100
Secondly, the memory industry realized that shrinking DRAM cells beyond a certain size

83
00:06:08,100 --> 00:06:09,860
gave you worse performance.

84
00:06:09,860 --> 00:06:16,580
Less reliability, less secure, worse latency, energy inefficiency, and so on.

85
00:06:16,580 --> 00:06:17,580
Here's why.

86
00:06:17,580 --> 00:06:22,620
A DRAM cell stores one bit of data in a form of a charge within a capacitor, a capacitor

87
00:06:22,620 --> 00:06:26,940
being a device that stores electrical energy within a field.

88
00:06:26,940 --> 00:06:31,620
That bit is accessed using an access transistor.

89
00:06:31,620 --> 00:06:37,420
As you scale the cell down to nanoscale sizes, that capacitor and its access transistor get

90
00:06:37,420 --> 00:06:41,700
leakier and more vulnerable to outside electrical noise.

91
00:06:41,700 --> 00:06:46,180
It also opens up new security vulnerabilities.

92
00:06:46,180 --> 00:06:50,780
These technical limitations and problems are fundamental to how the hardware works, which

93
00:06:50,780 --> 00:06:55,180
makes them extremely difficult to engineer our way around.

94
00:06:55,180 --> 00:07:00,980
The industry is going to grind out small solutions, but those will be small.

95
00:07:00,980 --> 00:07:06,300
So the problem also opens the door to brand new radical ideas that might give us a possible

96
00:07:06,300 --> 00:07:10,140
10x improvement over the current paradigm.

97
00:07:10,140 --> 00:07:14,340
In a previous video, I talked about the Silicon Photonic AI accelerator, where we tried to

98
00:07:14,340 --> 00:07:19,060
use light's properties to make data transfer more energy efficient.

99
00:07:19,060 --> 00:07:21,420
Alongside that, we have another idea.

100
00:07:21,420 --> 00:07:25,540
Let's alleviate or even possibly eliminate the von Neumann bottleneck and memory wall

101
00:07:25,540 --> 00:07:31,420
problems by making the memory do the computations themselves.

102
00:07:31,420 --> 00:07:35,860
Compute in memory refers to a random access memory with processing elements integrated

103
00:07:35,860 --> 00:07:36,860
together.

104
00:07:36,860 --> 00:07:42,140
The two might be very near each other or even be integrated onto the same die.

105
00:07:42,140 --> 00:07:45,860
I've seen the idea be called other things throughout the years.

106
00:07:45,860 --> 00:07:50,220
Processing and memory, computational RAM, near data computing, memory-centric computing,

107
00:07:50,220 --> 00:07:53,180
main memory computation, and so on.

108
00:07:53,180 --> 00:07:57,820
I'm aware that there are differences between these usages, but those differences are very

109
00:07:57,820 --> 00:07:58,820
subtle.

110
00:07:58,820 --> 00:08:03,300
I'm generally going to stick to saying compute in memory.

111
00:08:03,300 --> 00:08:07,900
The name has also been used to refer to concepts expanding on the SRAM idea.

112
00:08:07,900 --> 00:08:13,380
SRAM is often used for the memory cache that sits on chip with the CPU.

113
00:08:13,380 --> 00:08:17,980
But what we are more referring to here is bringing processing and compute ability to

114
00:08:17,980 --> 00:08:20,940
the computer main memory itself.

115
00:08:20,940 --> 00:08:23,380
The idea is well suited for deep learning.

116
00:08:23,380 --> 00:08:28,620
If you recall, running a neural network model is about calculating these massive matrices.

117
00:08:28,620 --> 00:08:35,780
The Google TPU had lots of circuits for running multiply and accumulate or MAC operations.

118
00:08:35,780 --> 00:08:38,220
The actual arithmetic is relatively simple.

119
00:08:38,220 --> 00:08:41,340
The problem is that there is so much of it that needs to be done.

120
00:08:41,340 --> 00:08:47,260
So in an ideal case, a compute in memory chip can execute MAC operations right inside the

121
00:08:47,260 --> 00:08:48,980
memory chip.

122
00:08:48,980 --> 00:08:53,860
This is especially helpful for running inference on the edge outside the data center.

123
00:08:53,860 --> 00:08:57,500
These use cases have energy size or heat restrictions.

124
00:08:57,500 --> 00:09:03,540
Being able to cut up to 80% of a neural network's energy usage is a game changer.

125
00:09:03,540 --> 00:09:07,700
The idea is decades old, dating back to the 1960s.

126
00:09:07,700 --> 00:09:13,540
Professor Harold Stone of Stanford University first notably explored the idea of logic and

127
00:09:13,540 --> 00:09:14,540
memory.

128
00:09:14,660 --> 00:09:19,780
Stone noted that the number of transistors in a microprocessor was growing very fast,

129
00:09:19,780 --> 00:09:23,820
but the processor's ability to communicate with its memory was limited by the number

130
00:09:23,820 --> 00:09:25,120
of pins.

131
00:09:25,120 --> 00:09:31,020
So he presented the idea of moving part of the computation into memory caches.

132
00:09:31,020 --> 00:09:34,420
The 1990s saw further explorations of the idea.

133
00:09:34,420 --> 00:09:39,060
In 1995, terraces produced what we would probably call the first processor in memory

134
00:09:39,060 --> 00:09:40,060
chip.

135
00:09:40,060 --> 00:09:45,060
It was a standard 4-bit memory with an integrated single-bit logical unit.

136
00:09:45,060 --> 00:09:50,580
This arithmetic logical unit can bring in data, apply some simple logic to it as dictated

137
00:09:50,580 --> 00:09:54,140
from a program, and then write it back to memory.

138
00:09:54,140 --> 00:09:59,660
And then in 1997, various professors at UC Berkeley, including Professor David Patterson,

139
00:09:59,660 --> 00:10:05,220
the inventor of risk, created the IRAM project, with the goal of putting a microprocessor and

140
00:10:05,220 --> 00:10:08,220
DRAM on the same chip.

141
00:10:08,220 --> 00:10:12,340
While other such proposals followed throughout the 1990s, but none of these ever caught

142
00:10:12,340 --> 00:10:14,900
on for a number of practical reasons.

143
00:10:14,900 --> 00:10:18,780
First, memory and logic are hard to manufacture together.

144
00:10:18,780 --> 00:10:22,220
Their fabrication have sort of opposing goals.

145
00:10:22,220 --> 00:10:28,380
Again, logic transistors are all about speed and performance, but memory transistors have

146
00:10:28,380 --> 00:10:33,220
to be about high density, low cost, and low leakage all at once.

147
00:10:33,980 --> 00:10:38,580
It is hard to build a logic process with a DRAM process and vice versa.

148
00:10:38,580 --> 00:10:42,380
DRAM designs are very regular, with lots of parallel wires.

149
00:10:42,380 --> 00:10:46,740
Logic designs, on the other hand, have much more complexity.

150
00:10:46,740 --> 00:10:50,820
Circuit elements in a chip have connections, referred to as metal layers.

151
00:10:50,820 --> 00:10:56,540
Having more metal layers allows for more complexity, but at the cost of current leakage and worse

152
00:10:56,540 --> 00:10:58,540
reliability.

153
00:10:58,540 --> 00:11:02,500
Contemporary DRAM processes use three to four metal layers.

154
00:11:02,500 --> 00:11:08,140
Contemporary logic processes use anywhere from 7 to 12 and even more metal layers.

155
00:11:08,140 --> 00:11:13,220
So it is estimated that if you tried to make logic circuits with a DRAM process, then the

156
00:11:13,220 --> 00:11:18,460
logic circuits would be 80% bigger and perform 22% worse.

157
00:11:18,460 --> 00:11:23,620
And if you were to try to make DRAM cells with a logic process, then you are essentially

158
00:11:23,620 --> 00:11:27,180
making embedded DRAM, or eDRAM.

159
00:11:27,220 --> 00:11:34,260
The cells use significantly more power, take up to 10 times more space, and are less reliable.

160
00:11:34,260 --> 00:11:39,020
The industry has since considered these manufacturing shortcomings and have come up with a variety

161
00:11:39,020 --> 00:11:40,500
of workarounds.

162
00:11:40,500 --> 00:11:47,580
Many proposals of compute and memory are on three levels, the device, circuit, and system.

163
00:11:47,580 --> 00:11:52,300
The device level leans on new types of memory hardware other than the conventional DRAM

164
00:11:52,300 --> 00:11:54,060
and SRAM memories.

165
00:11:54,060 --> 00:12:01,580
Notable examples include resistive random access memory, or rerAM, or rRAM, and spin

166
00:12:01,580 --> 00:12:08,820
transfer torque magneto resistive random access memory, or STT-MRAM.

167
00:12:08,820 --> 00:12:12,580
RerAM is one of the more promising emerging memory technologies.

168
00:12:12,580 --> 00:12:17,580
Like I mentioned earlier, conventional RAM memories store information using a charge

169
00:12:17,580 --> 00:12:20,420
stored within a capacitor.

170
00:12:20,420 --> 00:12:25,740
RerAM instead stores information by changing the electrical resistance of a certain material,

171
00:12:25,740 --> 00:12:31,380
referred to as a resistor, switching between a high and low resistance state.

172
00:12:31,380 --> 00:12:38,140
This structure allows rerAM to compute logical functions directly within the memory cells.

173
00:12:38,140 --> 00:12:42,740
Please don't ask me to try to explain it any further than that.

174
00:12:42,740 --> 00:12:47,420
RerAM is probably the emerging memory technology that is closest to commercialization due to

175
00:12:47,420 --> 00:12:53,580
it being compatible with silicon CMOS, however there remain substantial hurdles to overcome

176
00:12:53,580 --> 00:12:57,740
before we see products arrive on shelves.

177
00:12:57,740 --> 00:13:02,620
The circuit level is where we modify peripheral circuits to do the calculations right inside

178
00:13:02,620 --> 00:13:06,820
the SRAM or DRAM memory arrays themselves.

179
00:13:06,820 --> 00:13:13,300
The phrase I see a lot is in C2 computing, in C2 meaning locally or on site.

180
00:13:13,300 --> 00:13:17,180
Memories are particularly clever, but they also require an intimate knowledge of how

181
00:13:17,180 --> 00:13:21,980
memory works and still can be difficult to understand.

182
00:13:21,980 --> 00:13:27,300
One prominent example of this is Ombit, an in-memory accelerator proposed by people from

183
00:13:27,300 --> 00:13:33,300
Microsoft, NVIDIA, Intel, ETH, Zurich, and Carnegie Mellon.

184
00:13:33,300 --> 00:13:38,420
A DRAM memory contains sub arrays with many rows of DRAM cells.

185
00:13:38,420 --> 00:13:42,700
In normal use, the memory activates one row at a time.

186
00:13:42,700 --> 00:13:48,380
This system activates three rows at a time in order to implement an AND slash OR logic

187
00:13:48,380 --> 00:13:53,440
function, two rows for the inputs and one for the output.

188
00:13:53,440 --> 00:13:55,540
The concept is logically attractive.

189
00:13:55,540 --> 00:13:59,900
You can utilize the memory's internal bandwidth to do all the calculations.

190
00:13:59,900 --> 00:14:03,220
However, there are significant concerns.

191
00:14:03,220 --> 00:14:10,020
Ombit can perform basing AND or and NOT logic operations, but it takes multiple cycles.

192
00:14:10,020 --> 00:14:17,100
Furthermore, more complex logic implementations like XNOR remain challenging to utilize.

193
00:14:17,100 --> 00:14:22,180
So far, the big downside with these two compute and memory approaches is that their performance

194
00:14:22,180 --> 00:14:27,460
still falls short of what can be achieved with current Von Neumann GPU slash ASIC-centric

195
00:14:27,460 --> 00:14:29,020
systems.

196
00:14:29,020 --> 00:14:33,740
In other words, they suffer the same drawbacks people saw in the 1990s.

197
00:14:33,740 --> 00:14:40,180
Putting memory and logic together still makes a jack of all trades master of none situation.

198
00:14:40,180 --> 00:14:44,460
So the middle ground that the industry seems to be moving towards is implementing compute

199
00:14:44,460 --> 00:14:46,940
and memory at a system level.

200
00:14:46,940 --> 00:14:51,660
This is where we integrate together discrete processing units and memory at a very close

201
00:14:51,660 --> 00:14:53,580
level.

202
00:14:53,580 --> 00:15:00,220
This is enabled thanks to new packaging technologies like 2.5D or 3D memory die stacking where we

203
00:15:00,220 --> 00:15:04,700
stack a bunch of DRAM memory dies on top of a CPU die.

204
00:15:04,700 --> 00:15:09,120
The memories are then connected to the CPU using thousands of channels called through

205
00:15:09,120 --> 00:15:12,060
silicon vias or TSVs.

206
00:15:12,060 --> 00:15:15,100
This gives you immense amounts of internal bandwidth.

207
00:15:15,100 --> 00:15:20,580
AMD is working on something kind of like this with what they call 3DVcache, which is based

208
00:15:20,580 --> 00:15:24,300
on a TSMC 3D stacking packaging technology.

209
00:15:24,300 --> 00:15:28,460
They used it to add more memory cache to their processor chips.

210
00:15:28,460 --> 00:15:33,020
There is a future where we can use similarly advanced packaging technologies to add hundreds

211
00:15:33,020 --> 00:15:36,980
of gigabytes of memory to an AI ASIC.

212
00:15:36,980 --> 00:15:42,460
This lets us integrate together world-class memory and logic dies closer than ever before

213
00:15:42,460 --> 00:15:46,460
without needing to place them on the same die.

214
00:15:46,460 --> 00:15:48,300
Ideas in the laboratory are cheap.

215
00:15:48,300 --> 00:15:53,820
What is harder is to execute on those ideas in a way that performs well enough to replace

216
00:15:53,820 --> 00:15:56,580
what is already out there on the market.

217
00:15:56,580 --> 00:16:04,620
And the fact is that the current NVIDIA A100 and H100 AI GPUs are a very formidable competitor.

218
00:16:04,620 --> 00:16:08,740
But with leading edge semiconductor technology slowing down the way they are, we need new

219
00:16:08,740 --> 00:16:15,180
ways to leapfrog towards more powerful and robust hardware for running AI.

220
00:16:15,180 --> 00:16:17,740
Generally speaking, bigger models perform better.

221
00:16:17,740 --> 00:16:23,220
Today's best performing natural language processing and computer vision models are great, but they

222
00:16:23,220 --> 00:16:29,180
still have ways to go, which means they might have to get bigger so to get better.

223
00:16:29,180 --> 00:16:35,020
But unless we develop new systems and hardware that can overcome these aforementioned limits,

224
00:16:35,020 --> 00:16:39,620
it seems that deep learning might fall short of fulfilling its great expectations.

225
00:16:39,620 --> 00:16:42,020
Alright everyone, that's it for tonight.

226
00:16:42,020 --> 00:16:43,020
Thanks for watching.

227
00:16:43,020 --> 00:16:45,620
Subscribe to the channel, sign up for the newsletter, and I'll see you guys next time.

