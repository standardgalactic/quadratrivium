{"text": " Since the deep learning explosion started in 2012, the industry's biggest models have grown hundreds of thousands of times. Today, OpenAI's Dolly 2 has 3.5 billion parameters, Google's Imagine has 4.6 billion, and GPT-3 has 175 billion parameters. Increasingly larger models are ahead, Google recently pre-trained a model with 1 trillion parameters. These increasingly large models strain the ability of our hardware to accommodate them, and many of these limitations tie back to memory and how we use it. In this video, we're going to look at deep learning's memory wall problem, and some of the memory-centric paradigms researchers are looking at to solve it. But first, the Asianometry Patreon. Early access members get to watch new videos and selected references for them before their release in the public. To help support the videos and appreciate every pledge, thanks, and on with the show. Virtually every modern computer runs what is called a von Neumann architecture, meaning that it stores both its instructions and data on the same memory bank. At its heart are your processing units, a CPU or a GPU. Those processing units access memory to execute its instructions and process its data. The von Neumann architecture has been really good for us. It helped make software as powerful as it is today, but it works nothing like a real human brain. The brain's compute ability is relatively low precision, but it tightly integrates that compute with memory and input-output communication. Computers on the other hand run on high precision, 32 or 64-bit floating-point arithmetic for instance, but separates that compute from memory and communication. This separation has consequences, especially for memory. The AI hardware industry is scaling up memory and processing unit performance as fast as they can. NVIDIA's V100 GPU released in 2017 had a 32 gigabyte offering. Today, the top of the line NVIDIA data center GPUs A100 and H100 sport 80 gigabytes of memory. Despite this bulking up, hardware performance is not keeping up with how fast these models are growing, especially when it comes to memory. Memory allocations for leading edge models can easily exceed hundreds of gigabytes. Even with the latest parallelization techniques, a trillion-parameter model is estimated to require 320 A100 GPUs each with 80 gigabytes of memory. These differences in processing and capacity mean that a processing unit wastes multiple processing cycles, waiting not only for all of the data to travel in and out of the memory, but also for the memory to perform its read-slash-write operation. This limitation is known as the memory wall, or memory capacity bottleneck. Alright then, the obvious solution would then be to add more memory to our GPUs, right? What is stopping us from doing that? There are practical and technology limits to how much extra memory you can add on, not to mention the issues of connections and wiring. Just think about how widening a highway does not much help with traffic. Additionally, there are very significant energy limitations associated with shuttling data between the chip and the memory. These electric connections have losses which cost energy. I mentioned this in previous videos. Accessing memory off chip uses 200 times more energy than a floating point operation. 80% of the Google TPU's energy usage is from its electrical connections rather than its logic computational units. In some recent GPU and CPU systems, DRAM by itself accounts for 40% of the total system power. Energy makes up 40% of a data center's operating costs, so for this reason, storage and memory has come to be a significant factor in the data center's ongoing profitability. In addition to the significant operating costs of the energy, are the upfront capital costs of purchasing the AI hardware itself. As I mentioned earlier, a possible trillion parameter model would need 320 A100 GPUs, each with 80 gigabytes of memory. A100s cost $32,000 each at MSRP, so that's a clean $10 million. A 100 trillion parameter model might require over 6,000 such GPUs. That is just the cost of purchasing the hardware and does not even count the aforementioned energy costs of using these things to run inference on them, which is where 90% of a model's total costs are. It risks restricting the benefits of advanced AI only to the Uber-rich tech giants or governments. Much of these shortcomings are tied to historical and technological limits. In the 1960s and 70s, the industry adopted dynamic random access memory to form the basis of our computers. This adoption was largely made for technological and economic reasons. DRAM memories had relatively low latency and were cheap to manufacture in bulk. This worked fine for a while. As late as 1995, the memory industry was valued at $37 billion, with microprocessors at $20 billion. But after 1980, compute scaling far outpaced memory scaling. This is because, generally speaking, the CPU or GPU industries have had just one metric to optimize towards, transistor density. The memory industry, on the other hand, not only has the scaled DRAM capacity but also bandwidth and latency at the same time. Something has to give and usually that has been latency. Over the past 20 years, memory capacity has improved 128 times and bandwidth 20 times. latency however, has improved by just 30%. Secondly, the memory industry realized that shrinking DRAM cells beyond a certain size gave you worse performance. Less reliability, less secure, worse latency, energy inefficiency, and so on. Here's why. A DRAM cell stores one bit of data in a form of a charge within a capacitor, a capacitor being a device that stores electrical energy within a field. That bit is accessed using an access transistor. As you scale the cell down to nanoscale sizes, that capacitor and its access transistor get leakier and more vulnerable to outside electrical noise. It also opens up new security vulnerabilities. These technical limitations and problems are fundamental to how the hardware works, which makes them extremely difficult to engineer our way around. The industry is going to grind out small solutions, but those will be small. So the problem also opens the door to brand new radical ideas that might give us a possible 10x improvement over the current paradigm. In a previous video, I talked about the Silicon Photonic AI accelerator, where we tried to use light's properties to make data transfer more energy efficient. Alongside that, we have another idea. Let's alleviate or even possibly eliminate the von Neumann bottleneck and memory wall problems by making the memory do the computations themselves. Compute in memory refers to a random access memory with processing elements integrated together. The two might be very near each other or even be integrated onto the same die. I've seen the idea be called other things throughout the years. Processing and memory, computational RAM, near data computing, memory-centric computing, main memory computation, and so on. I'm aware that there are differences between these usages, but those differences are very subtle. I'm generally going to stick to saying compute in memory. The name has also been used to refer to concepts expanding on the SRAM idea. SRAM is often used for the memory cache that sits on chip with the CPU. But what we are more referring to here is bringing processing and compute ability to the computer main memory itself. The idea is well suited for deep learning. If you recall, running a neural network model is about calculating these massive matrices. The Google TPU had lots of circuits for running multiply and accumulate or MAC operations. The actual arithmetic is relatively simple. The problem is that there is so much of it that needs to be done. So in an ideal case, a compute in memory chip can execute MAC operations right inside the memory chip. This is especially helpful for running inference on the edge outside the data center. These use cases have energy size or heat restrictions. Being able to cut up to 80% of a neural network's energy usage is a game changer. The idea is decades old, dating back to the 1960s. Professor Harold Stone of Stanford University first notably explored the idea of logic and memory. Stone noted that the number of transistors in a microprocessor was growing very fast, but the processor's ability to communicate with its memory was limited by the number of pins. So he presented the idea of moving part of the computation into memory caches. The 1990s saw further explorations of the idea. In 1995, terraces produced what we would probably call the first processor in memory chip. It was a standard 4-bit memory with an integrated single-bit logical unit. This arithmetic logical unit can bring in data, apply some simple logic to it as dictated from a program, and then write it back to memory. And then in 1997, various professors at UC Berkeley, including Professor David Patterson, the inventor of risk, created the IRAM project, with the goal of putting a microprocessor and DRAM on the same chip. While other such proposals followed throughout the 1990s, but none of these ever caught on for a number of practical reasons. First, memory and logic are hard to manufacture together. Their fabrication have sort of opposing goals. Again, logic transistors are all about speed and performance, but memory transistors have to be about high density, low cost, and low leakage all at once. It is hard to build a logic process with a DRAM process and vice versa. DRAM designs are very regular, with lots of parallel wires. Logic designs, on the other hand, have much more complexity. Circuit elements in a chip have connections, referred to as metal layers. Having more metal layers allows for more complexity, but at the cost of current leakage and worse reliability. Contemporary DRAM processes use three to four metal layers. Contemporary logic processes use anywhere from 7 to 12 and even more metal layers. So it is estimated that if you tried to make logic circuits with a DRAM process, then the logic circuits would be 80% bigger and perform 22% worse. And if you were to try to make DRAM cells with a logic process, then you are essentially making embedded DRAM, or eDRAM. The cells use significantly more power, take up to 10 times more space, and are less reliable. The industry has since considered these manufacturing shortcomings and have come up with a variety of workarounds. Many proposals of compute and memory are on three levels, the device, circuit, and system. The device level leans on new types of memory hardware other than the conventional DRAM and SRAM memories. Notable examples include resistive random access memory, or rerAM, or rRAM, and spin transfer torque magneto resistive random access memory, or STT-MRAM. RerAM is one of the more promising emerging memory technologies. Like I mentioned earlier, conventional RAM memories store information using a charge stored within a capacitor. RerAM instead stores information by changing the electrical resistance of a certain material, referred to as a resistor, switching between a high and low resistance state. This structure allows rerAM to compute logical functions directly within the memory cells. Please don't ask me to try to explain it any further than that. RerAM is probably the emerging memory technology that is closest to commercialization due to it being compatible with silicon CMOS, however there remain substantial hurdles to overcome before we see products arrive on shelves. The circuit level is where we modify peripheral circuits to do the calculations right inside the SRAM or DRAM memory arrays themselves. The phrase I see a lot is in C2 computing, in C2 meaning locally or on site. Memories are particularly clever, but they also require an intimate knowledge of how memory works and still can be difficult to understand. One prominent example of this is Ombit, an in-memory accelerator proposed by people from Microsoft, NVIDIA, Intel, ETH, Zurich, and Carnegie Mellon. A DRAM memory contains sub arrays with many rows of DRAM cells. In normal use, the memory activates one row at a time. This system activates three rows at a time in order to implement an AND slash OR logic function, two rows for the inputs and one for the output. The concept is logically attractive. You can utilize the memory's internal bandwidth to do all the calculations. However, there are significant concerns. Ombit can perform basing AND or and NOT logic operations, but it takes multiple cycles. Furthermore, more complex logic implementations like XNOR remain challenging to utilize. So far, the big downside with these two compute and memory approaches is that their performance still falls short of what can be achieved with current Von Neumann GPU slash ASIC-centric systems. In other words, they suffer the same drawbacks people saw in the 1990s. Putting memory and logic together still makes a jack of all trades master of none situation. So the middle ground that the industry seems to be moving towards is implementing compute and memory at a system level. This is where we integrate together discrete processing units and memory at a very close level. This is enabled thanks to new packaging technologies like 2.5D or 3D memory die stacking where we stack a bunch of DRAM memory dies on top of a CPU die. The memories are then connected to the CPU using thousands of channels called through silicon vias or TSVs. This gives you immense amounts of internal bandwidth. AMD is working on something kind of like this with what they call 3DVcache, which is based on a TSMC 3D stacking packaging technology. They used it to add more memory cache to their processor chips. There is a future where we can use similarly advanced packaging technologies to add hundreds of gigabytes of memory to an AI ASIC. This lets us integrate together world-class memory and logic dies closer than ever before without needing to place them on the same die. Ideas in the laboratory are cheap. What is harder is to execute on those ideas in a way that performs well enough to replace what is already out there on the market. And the fact is that the current NVIDIA A100 and H100 AI GPUs are a very formidable competitor. But with leading edge semiconductor technology slowing down the way they are, we need new ways to leapfrog towards more powerful and robust hardware for running AI. Generally speaking, bigger models perform better. Today's best performing natural language processing and computer vision models are great, but they still have ways to go, which means they might have to get bigger so to get better. But unless we develop new systems and hardware that can overcome these aforementioned limits, it seems that deep learning might fall short of fulfilling its great expectations. Alright everyone, that's it for tonight. Thanks for watching. Subscribe to the channel, sign up for the newsletter, and I'll see you guys next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.5200000000000005, "text": " Since the deep learning explosion started in 2012, the industry's biggest models have", "tokens": [50364, 4162, 264, 2452, 2539, 15673, 1409, 294, 9125, 11, 264, 3518, 311, 3880, 5245, 362, 50740], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 1, "seek": 0, "start": 7.5200000000000005, "end": 10.16, "text": " grown hundreds of thousands of times.", "tokens": [50740, 7709, 6779, 295, 5383, 295, 1413, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 2, "seek": 0, "start": 10.16, "end": 18.52, "text": " Today, OpenAI's Dolly 2 has 3.5 billion parameters, Google's Imagine has 4.6 billion, and GPT-3", "tokens": [50872, 2692, 11, 7238, 48698, 311, 1144, 13020, 568, 575, 805, 13, 20, 5218, 9834, 11, 3329, 311, 11739, 575, 1017, 13, 21, 5218, 11, 293, 26039, 51, 12, 18, 51290], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 3, "seek": 0, "start": 18.52, "end": 22.36, "text": " has 175 billion parameters.", "tokens": [51290, 575, 41165, 5218, 9834, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 4, "seek": 0, "start": 22.36, "end": 27.04, "text": " Increasingly larger models are ahead, Google recently pre-trained a model with 1 trillion", "tokens": [51482, 30367, 3349, 356, 4833, 5245, 366, 2286, 11, 3329, 3938, 659, 12, 17227, 2001, 257, 2316, 365, 502, 18723, 51716], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 5, "seek": 0, "start": 27.04, "end": 29.16, "text": " parameters.", "tokens": [51716, 9834, 13, 51822], "temperature": 0.0, "avg_logprob": -0.2046753181206001, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.0617842823266983}, {"id": 6, "seek": 2916, "start": 29.16, "end": 33.76, "text": " These increasingly large models strain the ability of our hardware to accommodate them,", "tokens": [50364, 1981, 12980, 2416, 5245, 14249, 264, 3485, 295, 527, 8837, 281, 21410, 552, 11, 50594], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 7, "seek": 2916, "start": 33.76, "end": 38.519999999999996, "text": " and many of these limitations tie back to memory and how we use it.", "tokens": [50594, 293, 867, 295, 613, 15705, 7582, 646, 281, 4675, 293, 577, 321, 764, 309, 13, 50832], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 8, "seek": 2916, "start": 38.519999999999996, "end": 42.519999999999996, "text": " In this video, we're going to look at deep learning's memory wall problem, and some", "tokens": [50832, 682, 341, 960, 11, 321, 434, 516, 281, 574, 412, 2452, 2539, 311, 4675, 2929, 1154, 11, 293, 512, 51032], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 9, "seek": 2916, "start": 42.519999999999996, "end": 47.480000000000004, "text": " of the memory-centric paradigms researchers are looking at to solve it.", "tokens": [51032, 295, 264, 4675, 12, 45300, 13480, 328, 2592, 10309, 366, 1237, 412, 281, 5039, 309, 13, 51280], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 10, "seek": 2916, "start": 47.480000000000004, "end": 49.68, "text": " But first, the Asianometry Patreon.", "tokens": [51280, 583, 700, 11, 264, 10645, 34730, 15692, 13, 51390], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 11, "seek": 2916, "start": 49.68, "end": 53.519999999999996, "text": " Early access members get to watch new videos and selected references for them before their", "tokens": [51390, 18344, 2105, 2679, 483, 281, 1159, 777, 2145, 293, 8209, 15400, 337, 552, 949, 641, 51582], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 12, "seek": 2916, "start": 53.519999999999996, "end": 55.0, "text": " release in the public.", "tokens": [51582, 4374, 294, 264, 1908, 13, 51656], "temperature": 0.0, "avg_logprob": -0.16700723436143664, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.004131315741688013}, {"id": 13, "seek": 5500, "start": 55.0, "end": 60.28, "text": " To help support the videos and appreciate every pledge, thanks, and on with the show.", "tokens": [50364, 1407, 854, 1406, 264, 2145, 293, 4449, 633, 26819, 11, 3231, 11, 293, 322, 365, 264, 855, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1554925147067295, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.006486471742391586}, {"id": 14, "seek": 5500, "start": 60.28, "end": 65.28, "text": " Virtually every modern computer runs what is called a von Neumann architecture, meaning", "tokens": [50628, 19447, 671, 633, 4363, 3820, 6676, 437, 307, 1219, 257, 2957, 1734, 449, 969, 9482, 11, 3620, 50878], "temperature": 0.0, "avg_logprob": -0.1554925147067295, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.006486471742391586}, {"id": 15, "seek": 5500, "start": 65.28, "end": 70.28, "text": " that it stores both its instructions and data on the same memory bank.", "tokens": [50878, 300, 309, 9512, 1293, 1080, 9415, 293, 1412, 322, 264, 912, 4675, 3765, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1554925147067295, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.006486471742391586}, {"id": 16, "seek": 5500, "start": 70.28, "end": 74.84, "text": " At its heart are your processing units, a CPU or a GPU.", "tokens": [51128, 1711, 1080, 1917, 366, 428, 9007, 6815, 11, 257, 13199, 420, 257, 18407, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1554925147067295, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.006486471742391586}, {"id": 17, "seek": 5500, "start": 74.84, "end": 81.48, "text": " Those processing units access memory to execute its instructions and process its data.", "tokens": [51356, 3950, 9007, 6815, 2105, 4675, 281, 14483, 1080, 9415, 293, 1399, 1080, 1412, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1554925147067295, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.006486471742391586}, {"id": 18, "seek": 8148, "start": 81.48, "end": 84.56, "text": " The von Neumann architecture has been really good for us.", "tokens": [50364, 440, 2957, 1734, 449, 969, 9482, 575, 668, 534, 665, 337, 505, 13, 50518], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 19, "seek": 8148, "start": 84.56, "end": 89.80000000000001, "text": " It helped make software as powerful as it is today, but it works nothing like a real", "tokens": [50518, 467, 4254, 652, 4722, 382, 4005, 382, 309, 307, 965, 11, 457, 309, 1985, 1825, 411, 257, 957, 50780], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 20, "seek": 8148, "start": 89.80000000000001, "end": 91.52000000000001, "text": " human brain.", "tokens": [50780, 1952, 3567, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 21, "seek": 8148, "start": 91.52000000000001, "end": 96.2, "text": " The brain's compute ability is relatively low precision, but it tightly integrates", "tokens": [50866, 440, 3567, 311, 14722, 3485, 307, 7226, 2295, 18356, 11, 457, 309, 21952, 3572, 1024, 51100], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 22, "seek": 8148, "start": 96.2, "end": 101.2, "text": " that compute with memory and input-output communication.", "tokens": [51100, 300, 14722, 365, 4675, 293, 4846, 12, 346, 2582, 6101, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 23, "seek": 8148, "start": 101.2, "end": 106.52000000000001, "text": " Computers on the other hand run on high precision, 32 or 64-bit floating-point arithmetic for", "tokens": [51350, 37804, 433, 322, 264, 661, 1011, 1190, 322, 1090, 18356, 11, 8858, 420, 12145, 12, 5260, 12607, 12, 6053, 42973, 337, 51616], "temperature": 0.0, "avg_logprob": -0.1241893165989926, "compression_ratio": 1.5375494071146245, "no_speech_prob": 0.4259033501148224}, {"id": 24, "seek": 10652, "start": 106.52, "end": 111.6, "text": " instance, but separates that compute from memory and communication.", "tokens": [50364, 5197, 11, 457, 34149, 300, 14722, 490, 4675, 293, 6101, 13, 50618], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 25, "seek": 10652, "start": 111.6, "end": 116.03999999999999, "text": " This separation has consequences, especially for memory.", "tokens": [50618, 639, 14634, 575, 10098, 11, 2318, 337, 4675, 13, 50840], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 26, "seek": 10652, "start": 116.03999999999999, "end": 120.88, "text": " The AI hardware industry is scaling up memory and processing unit performance as fast as", "tokens": [50840, 440, 7318, 8837, 3518, 307, 21589, 493, 4675, 293, 9007, 4985, 3389, 382, 2370, 382, 51082], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 27, "seek": 10652, "start": 120.88, "end": 121.88, "text": " they can.", "tokens": [51082, 436, 393, 13, 51132], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 28, "seek": 10652, "start": 121.88, "end": 128.0, "text": " NVIDIA's V100 GPU released in 2017 had a 32 gigabyte offering.", "tokens": [51132, 426, 3958, 6914, 311, 691, 6879, 18407, 4736, 294, 6591, 632, 257, 8858, 8741, 34529, 8745, 13, 51438], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 29, "seek": 10652, "start": 128.0, "end": 135.6, "text": " Today, the top of the line NVIDIA data center GPUs A100 and H100 sport 80 gigabytes of memory.", "tokens": [51438, 2692, 11, 264, 1192, 295, 264, 1622, 426, 3958, 6914, 1412, 3056, 18407, 82, 316, 6879, 293, 389, 6879, 7282, 4688, 42741, 295, 4675, 13, 51818], "temperature": 0.0, "avg_logprob": -0.18383482669262177, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.16017457842826843}, {"id": 30, "seek": 13560, "start": 135.79999999999998, "end": 140.16, "text": " Despite this bulking up, hardware performance is not keeping up with how fast these models", "tokens": [50374, 11334, 341, 6493, 5092, 493, 11, 8837, 3389, 307, 406, 5145, 493, 365, 577, 2370, 613, 5245, 50592], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 31, "seek": 13560, "start": 140.16, "end": 144.04, "text": " are growing, especially when it comes to memory.", "tokens": [50592, 366, 4194, 11, 2318, 562, 309, 1487, 281, 4675, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 32, "seek": 13560, "start": 144.04, "end": 149.28, "text": " Memory allocations for leading edge models can easily exceed hundreds of gigabytes.", "tokens": [50786, 38203, 12660, 763, 337, 5775, 4691, 5245, 393, 3612, 14048, 6779, 295, 42741, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 33, "seek": 13560, "start": 149.28, "end": 153.68, "text": " Even with the latest parallelization techniques, a trillion-parameter model is estimated to", "tokens": [51048, 2754, 365, 264, 6792, 8952, 2144, 7512, 11, 257, 18723, 12, 2181, 335, 2398, 2316, 307, 14109, 281, 51268], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 34, "seek": 13560, "start": 153.68, "end": 160.28, "text": " require 320 A100 GPUs each with 80 gigabytes of memory.", "tokens": [51268, 3651, 42429, 316, 6879, 18407, 82, 1184, 365, 4688, 42741, 295, 4675, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 35, "seek": 13560, "start": 160.28, "end": 165.24, "text": " These differences in processing and capacity mean that a processing unit wastes multiple", "tokens": [51598, 1981, 7300, 294, 9007, 293, 6042, 914, 300, 257, 9007, 4985, 390, 7269, 3866, 51846], "temperature": 0.0, "avg_logprob": -0.1071852207183838, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.012429436668753624}, {"id": 36, "seek": 16524, "start": 165.24, "end": 170.28, "text": " processing cycles, waiting not only for all of the data to travel in and out of the memory,", "tokens": [50364, 9007, 17796, 11, 3806, 406, 787, 337, 439, 295, 264, 1412, 281, 3147, 294, 293, 484, 295, 264, 4675, 11, 50616], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 37, "seek": 16524, "start": 170.28, "end": 175.20000000000002, "text": " but also for the memory to perform its read-slash-write operation.", "tokens": [50616, 457, 611, 337, 264, 4675, 281, 2042, 1080, 1401, 12, 10418, 1299, 12, 21561, 6916, 13, 50862], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 38, "seek": 16524, "start": 175.20000000000002, "end": 179.8, "text": " This limitation is known as the memory wall, or memory capacity bottleneck.", "tokens": [50862, 639, 27432, 307, 2570, 382, 264, 4675, 2929, 11, 420, 4675, 6042, 44641, 547, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 39, "seek": 16524, "start": 179.8, "end": 185.28, "text": " Alright then, the obvious solution would then be to add more memory to our GPUs, right?", "tokens": [51092, 2798, 550, 11, 264, 6322, 3827, 576, 550, 312, 281, 909, 544, 4675, 281, 527, 18407, 82, 11, 558, 30, 51366], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 40, "seek": 16524, "start": 185.28, "end": 187.60000000000002, "text": " What is stopping us from doing that?", "tokens": [51366, 708, 307, 12767, 505, 490, 884, 300, 30, 51482], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 41, "seek": 16524, "start": 187.60000000000002, "end": 191.84, "text": " There are practical and technology limits to how much extra memory you can add on, not", "tokens": [51482, 821, 366, 8496, 293, 2899, 10406, 281, 577, 709, 2857, 4675, 291, 393, 909, 322, 11, 406, 51694], "temperature": 0.0, "avg_logprob": -0.13904606212269177, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.003074964275583625}, {"id": 42, "seek": 19184, "start": 191.84, "end": 195.12, "text": " to mention the issues of connections and wiring.", "tokens": [50364, 281, 2152, 264, 2663, 295, 9271, 293, 27520, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 43, "seek": 19184, "start": 195.12, "end": 199.6, "text": " Just think about how widening a highway does not much help with traffic.", "tokens": [50528, 1449, 519, 466, 577, 32552, 278, 257, 17205, 775, 406, 709, 854, 365, 6419, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 44, "seek": 19184, "start": 199.6, "end": 204.64000000000001, "text": " Additionally, there are very significant energy limitations associated with shuttling data", "tokens": [50752, 19927, 11, 456, 366, 588, 4776, 2281, 15705, 6615, 365, 5309, 83, 1688, 1412, 51004], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 45, "seek": 19184, "start": 204.64000000000001, "end": 207.2, "text": " between the chip and the memory.", "tokens": [51004, 1296, 264, 11409, 293, 264, 4675, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 46, "seek": 19184, "start": 207.2, "end": 210.96, "text": " These electric connections have losses which cost energy.", "tokens": [51132, 1981, 5210, 9271, 362, 15352, 597, 2063, 2281, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 47, "seek": 19184, "start": 210.96, "end": 213.44, "text": " I mentioned this in previous videos.", "tokens": [51320, 286, 2835, 341, 294, 3894, 2145, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 48, "seek": 19184, "start": 213.44, "end": 218.92000000000002, "text": " Accessing memory off chip uses 200 times more energy than a floating point operation.", "tokens": [51444, 17166, 278, 4675, 766, 11409, 4960, 2331, 1413, 544, 2281, 813, 257, 12607, 935, 6916, 13, 51718], "temperature": 0.0, "avg_logprob": -0.1719639685846144, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.006903233006596565}, {"id": 49, "seek": 21892, "start": 219.48, "end": 224.92, "text": " 80% of the Google TPU's energy usage is from its electrical connections rather than its", "tokens": [50392, 4688, 4, 295, 264, 3329, 314, 8115, 311, 2281, 14924, 307, 490, 1080, 12147, 9271, 2831, 813, 1080, 50664], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 50, "seek": 21892, "start": 224.92, "end": 227.51999999999998, "text": " logic computational units.", "tokens": [50664, 9952, 28270, 6815, 13, 50794], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 51, "seek": 21892, "start": 227.51999999999998, "end": 233.92, "text": " In some recent GPU and CPU systems, DRAM by itself accounts for 40% of the total system", "tokens": [50794, 682, 512, 5162, 18407, 293, 13199, 3652, 11, 12118, 2865, 538, 2564, 9402, 337, 3356, 4, 295, 264, 3217, 1185, 51114], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 52, "seek": 21892, "start": 233.92, "end": 235.44, "text": " power.", "tokens": [51114, 1347, 13, 51190], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 53, "seek": 21892, "start": 235.44, "end": 241.44, "text": " Energy makes up 40% of a data center's operating costs, so for this reason, storage and memory", "tokens": [51190, 14939, 1669, 493, 3356, 4, 295, 257, 1412, 3056, 311, 7447, 5497, 11, 370, 337, 341, 1778, 11, 6725, 293, 4675, 51490], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 54, "seek": 21892, "start": 241.44, "end": 247.88, "text": " has come to be a significant factor in the data center's ongoing profitability.", "tokens": [51490, 575, 808, 281, 312, 257, 4776, 5952, 294, 264, 1412, 3056, 311, 10452, 46249, 13, 51812], "temperature": 0.0, "avg_logprob": -0.13498133801399392, "compression_ratio": 1.536, "no_speech_prob": 0.0038240684662014246}, {"id": 55, "seek": 24788, "start": 247.88, "end": 252.62, "text": " In addition to the significant operating costs of the energy, are the upfront capital", "tokens": [50364, 682, 4500, 281, 264, 4776, 7447, 5497, 295, 264, 2281, 11, 366, 264, 30264, 4238, 50601], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 56, "seek": 24788, "start": 252.62, "end": 256.32, "text": " costs of purchasing the AI hardware itself.", "tokens": [50601, 5497, 295, 20906, 264, 7318, 8837, 2564, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 57, "seek": 24788, "start": 256.32, "end": 262.44, "text": " As I mentioned earlier, a possible trillion parameter model would need 320 A100 GPUs,", "tokens": [50786, 1018, 286, 2835, 3071, 11, 257, 1944, 18723, 13075, 2316, 576, 643, 42429, 316, 6879, 18407, 82, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 58, "seek": 24788, "start": 262.44, "end": 264.56, "text": " each with 80 gigabytes of memory.", "tokens": [51092, 1184, 365, 4688, 42741, 295, 4675, 13, 51198], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 59, "seek": 24788, "start": 264.56, "end": 270.88, "text": " A100s cost $32,000 each at MSRP, so that's a clean $10 million.", "tokens": [51198, 316, 6879, 82, 2063, 1848, 11440, 11, 1360, 1184, 412, 7395, 28516, 11, 370, 300, 311, 257, 2541, 1848, 3279, 2459, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 60, "seek": 24788, "start": 270.88, "end": 276.46, "text": " A 100 trillion parameter model might require over 6,000 such GPUs.", "tokens": [51514, 316, 2319, 18723, 13075, 2316, 1062, 3651, 670, 1386, 11, 1360, 1270, 18407, 82, 13, 51793], "temperature": 0.0, "avg_logprob": -0.1783108181423611, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0018100995803251863}, {"id": 61, "seek": 27646, "start": 276.46, "end": 280.73999999999995, "text": " That is just the cost of purchasing the hardware and does not even count the aforementioned", "tokens": [50364, 663, 307, 445, 264, 2063, 295, 20906, 264, 8837, 293, 775, 406, 754, 1207, 264, 48927, 46842, 50578], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 62, "seek": 27646, "start": 280.73999999999995, "end": 285.9, "text": " energy costs of using these things to run inference on them, which is where 90% of a", "tokens": [50578, 2281, 5497, 295, 1228, 613, 721, 281, 1190, 38253, 322, 552, 11, 597, 307, 689, 4289, 4, 295, 257, 50836], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 63, "seek": 27646, "start": 285.9, "end": 288.41999999999996, "text": " model's total costs are.", "tokens": [50836, 2316, 311, 3217, 5497, 366, 13, 50962], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 64, "seek": 27646, "start": 288.41999999999996, "end": 295.7, "text": " It risks restricting the benefits of advanced AI only to the Uber-rich tech giants or governments.", "tokens": [50962, 467, 10888, 1472, 37714, 264, 5311, 295, 7339, 7318, 787, 281, 264, 21839, 12, 10794, 7553, 31894, 420, 11280, 13, 51326], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 65, "seek": 27646, "start": 295.7, "end": 299.7, "text": " Much of these shortcomings are tied to historical and technological limits.", "tokens": [51326, 12313, 295, 613, 2099, 49886, 366, 9601, 281, 8584, 293, 18439, 10406, 13, 51526], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 66, "seek": 27646, "start": 299.7, "end": 305.62, "text": " In the 1960s and 70s, the industry adopted dynamic random access memory to form the basis", "tokens": [51526, 682, 264, 16157, 82, 293, 5285, 82, 11, 264, 3518, 12175, 8546, 4974, 2105, 4675, 281, 1254, 264, 5143, 51822], "temperature": 0.0, "avg_logprob": -0.12619679945486564, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.0013669648906216025}, {"id": 67, "seek": 30562, "start": 305.62, "end": 307.38, "text": " of our computers.", "tokens": [50364, 295, 527, 10807, 13, 50452], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 68, "seek": 30562, "start": 307.38, "end": 311.3, "text": " This adoption was largely made for technological and economic reasons.", "tokens": [50452, 639, 19215, 390, 11611, 1027, 337, 18439, 293, 4836, 4112, 13, 50648], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 69, "seek": 30562, "start": 311.3, "end": 316.86, "text": " DRAM memories had relatively low latency and were cheap to manufacture in bulk.", "tokens": [50648, 12118, 2865, 8495, 632, 7226, 2295, 27043, 293, 645, 7084, 281, 27400, 294, 16139, 13, 50926], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 70, "seek": 30562, "start": 316.86, "end": 318.5, "text": " This worked fine for a while.", "tokens": [50926, 639, 2732, 2489, 337, 257, 1339, 13, 51008], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 71, "seek": 30562, "start": 318.5, "end": 325.02, "text": " As late as 1995, the memory industry was valued at $37 billion, with microprocessors at $20", "tokens": [51008, 1018, 3469, 382, 22601, 11, 264, 4675, 3518, 390, 22608, 412, 1848, 12851, 5218, 11, 365, 3123, 1513, 340, 45700, 412, 1848, 2009, 51334], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 72, "seek": 30562, "start": 325.02, "end": 326.78000000000003, "text": " billion.", "tokens": [51334, 5218, 13, 51422], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 73, "seek": 30562, "start": 326.78000000000003, "end": 331.54, "text": " But after 1980, compute scaling far outpaced memory scaling.", "tokens": [51422, 583, 934, 13626, 11, 14722, 21589, 1400, 484, 47038, 4675, 21589, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14433520634969074, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030751246958971024}, {"id": 74, "seek": 33154, "start": 331.54, "end": 336.62, "text": " This is because, generally speaking, the CPU or GPU industries have had just one metric", "tokens": [50364, 639, 307, 570, 11, 5101, 4124, 11, 264, 13199, 420, 18407, 13284, 362, 632, 445, 472, 20678, 50618], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 75, "seek": 33154, "start": 336.62, "end": 340.22, "text": " to optimize towards, transistor density.", "tokens": [50618, 281, 19719, 3030, 11, 34750, 10305, 13, 50798], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 76, "seek": 33154, "start": 340.22, "end": 344.90000000000003, "text": " The memory industry, on the other hand, not only has the scaled DRAM capacity but also", "tokens": [50798, 440, 4675, 3518, 11, 322, 264, 661, 1011, 11, 406, 787, 575, 264, 36039, 12118, 2865, 6042, 457, 611, 51032], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 77, "seek": 33154, "start": 344.90000000000003, "end": 348.14000000000004, "text": " bandwidth and latency at the same time.", "tokens": [51032, 23647, 293, 27043, 412, 264, 912, 565, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 78, "seek": 33154, "start": 348.14000000000004, "end": 352.02000000000004, "text": " Something has to give and usually that has been latency.", "tokens": [51194, 6595, 575, 281, 976, 293, 2673, 300, 575, 668, 27043, 13, 51388], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 79, "seek": 33154, "start": 352.02000000000004, "end": 359.14000000000004, "text": " Over the past 20 years, memory capacity has improved 128 times and bandwidth 20 times.", "tokens": [51388, 4886, 264, 1791, 945, 924, 11, 4675, 6042, 575, 9689, 29810, 1413, 293, 23647, 945, 1413, 13, 51744], "temperature": 0.0, "avg_logprob": -0.13794276534870106, "compression_ratio": 1.5770750988142292, "no_speech_prob": 0.06186136603355408}, {"id": 80, "seek": 35914, "start": 359.14, "end": 362.58, "text": " latency however, has improved by just 30%.", "tokens": [50364, 27043, 4461, 11, 575, 9689, 538, 445, 2217, 6856, 50536], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 81, "seek": 35914, "start": 362.58, "end": 368.09999999999997, "text": " Secondly, the memory industry realized that shrinking DRAM cells beyond a certain size", "tokens": [50536, 19483, 11, 264, 4675, 3518, 5334, 300, 41684, 12118, 2865, 5438, 4399, 257, 1629, 2744, 50812], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 82, "seek": 35914, "start": 368.09999999999997, "end": 369.86, "text": " gave you worse performance.", "tokens": [50812, 2729, 291, 5324, 3389, 13, 50900], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 83, "seek": 35914, "start": 369.86, "end": 376.58, "text": " Less reliability, less secure, worse latency, energy inefficiency, and so on.", "tokens": [50900, 18649, 24550, 11, 1570, 7144, 11, 5324, 27043, 11, 2281, 7167, 49086, 11, 293, 370, 322, 13, 51236], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 84, "seek": 35914, "start": 376.58, "end": 377.58, "text": " Here's why.", "tokens": [51236, 1692, 311, 983, 13, 51286], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 85, "seek": 35914, "start": 377.58, "end": 382.62, "text": " A DRAM cell stores one bit of data in a form of a charge within a capacitor, a capacitor", "tokens": [51286, 316, 12118, 2865, 2815, 9512, 472, 857, 295, 1412, 294, 257, 1254, 295, 257, 4602, 1951, 257, 29372, 11, 257, 29372, 51538], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 86, "seek": 35914, "start": 382.62, "end": 386.94, "text": " being a device that stores electrical energy within a field.", "tokens": [51538, 885, 257, 4302, 300, 9512, 12147, 2281, 1951, 257, 2519, 13, 51754], "temperature": 0.0, "avg_logprob": -0.19364283036212532, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.08505953103303909}, {"id": 87, "seek": 38694, "start": 386.94, "end": 391.62, "text": " That bit is accessed using an access transistor.", "tokens": [50364, 663, 857, 307, 34211, 1228, 364, 2105, 34750, 13, 50598], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 88, "seek": 38694, "start": 391.62, "end": 397.42, "text": " As you scale the cell down to nanoscale sizes, that capacitor and its access transistor get", "tokens": [50598, 1018, 291, 4373, 264, 2815, 760, 281, 14067, 10466, 1220, 11602, 11, 300, 29372, 293, 1080, 2105, 34750, 483, 50888], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 89, "seek": 38694, "start": 397.42, "end": 401.7, "text": " leakier and more vulnerable to outside electrical noise.", "tokens": [50888, 17143, 811, 293, 544, 10955, 281, 2380, 12147, 5658, 13, 51102], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 90, "seek": 38694, "start": 401.7, "end": 406.18, "text": " It also opens up new security vulnerabilities.", "tokens": [51102, 467, 611, 9870, 493, 777, 3825, 37633, 13, 51326], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 91, "seek": 38694, "start": 406.18, "end": 410.78, "text": " These technical limitations and problems are fundamental to how the hardware works, which", "tokens": [51326, 1981, 6191, 15705, 293, 2740, 366, 8088, 281, 577, 264, 8837, 1985, 11, 597, 51556], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 92, "seek": 38694, "start": 410.78, "end": 415.18, "text": " makes them extremely difficult to engineer our way around.", "tokens": [51556, 1669, 552, 4664, 2252, 281, 11403, 527, 636, 926, 13, 51776], "temperature": 0.0, "avg_logprob": -0.10955133892240979, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.03621324896812439}, {"id": 93, "seek": 41518, "start": 415.18, "end": 420.98, "text": " The industry is going to grind out small solutions, but those will be small.", "tokens": [50364, 440, 3518, 307, 516, 281, 16700, 484, 1359, 6547, 11, 457, 729, 486, 312, 1359, 13, 50654], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 94, "seek": 41518, "start": 420.98, "end": 426.3, "text": " So the problem also opens the door to brand new radical ideas that might give us a possible", "tokens": [50654, 407, 264, 1154, 611, 9870, 264, 2853, 281, 3360, 777, 12001, 3487, 300, 1062, 976, 505, 257, 1944, 50920], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 95, "seek": 41518, "start": 426.3, "end": 430.14, "text": " 10x improvement over the current paradigm.", "tokens": [50920, 1266, 87, 10444, 670, 264, 2190, 24709, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 96, "seek": 41518, "start": 430.14, "end": 434.34000000000003, "text": " In a previous video, I talked about the Silicon Photonic AI accelerator, where we tried to", "tokens": [51112, 682, 257, 3894, 960, 11, 286, 2825, 466, 264, 25351, 13919, 11630, 7318, 39889, 11, 689, 321, 3031, 281, 51322], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 97, "seek": 41518, "start": 434.34000000000003, "end": 439.06, "text": " use light's properties to make data transfer more energy efficient.", "tokens": [51322, 764, 1442, 311, 7221, 281, 652, 1412, 5003, 544, 2281, 7148, 13, 51558], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 98, "seek": 41518, "start": 439.06, "end": 441.42, "text": " Alongside that, we have another idea.", "tokens": [51558, 17457, 1812, 300, 11, 321, 362, 1071, 1558, 13, 51676], "temperature": 0.0, "avg_logprob": -0.16649150848388672, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.04144729673862457}, {"id": 99, "seek": 44142, "start": 441.42, "end": 445.54, "text": " Let's alleviate or even possibly eliminate the von Neumann bottleneck and memory wall", "tokens": [50364, 961, 311, 42701, 420, 754, 6264, 13819, 264, 2957, 1734, 449, 969, 44641, 547, 293, 4675, 2929, 50570], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 100, "seek": 44142, "start": 445.54, "end": 451.42, "text": " problems by making the memory do the computations themselves.", "tokens": [50570, 2740, 538, 1455, 264, 4675, 360, 264, 2807, 763, 2969, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 101, "seek": 44142, "start": 451.42, "end": 455.86, "text": " Compute in memory refers to a random access memory with processing elements integrated", "tokens": [50864, 6620, 1169, 294, 4675, 14942, 281, 257, 4974, 2105, 4675, 365, 9007, 4959, 10919, 51086], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 102, "seek": 44142, "start": 455.86, "end": 456.86, "text": " together.", "tokens": [51086, 1214, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 103, "seek": 44142, "start": 456.86, "end": 462.14, "text": " The two might be very near each other or even be integrated onto the same die.", "tokens": [51136, 440, 732, 1062, 312, 588, 2651, 1184, 661, 420, 754, 312, 10919, 3911, 264, 912, 978, 13, 51400], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 104, "seek": 44142, "start": 462.14, "end": 465.86, "text": " I've seen the idea be called other things throughout the years.", "tokens": [51400, 286, 600, 1612, 264, 1558, 312, 1219, 661, 721, 3710, 264, 924, 13, 51586], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 105, "seek": 44142, "start": 465.86, "end": 470.22, "text": " Processing and memory, computational RAM, near data computing, memory-centric computing,", "tokens": [51586, 31093, 278, 293, 4675, 11, 28270, 14561, 11, 2651, 1412, 15866, 11, 4675, 12, 45300, 15866, 11, 51804], "temperature": 0.0, "avg_logprob": -0.14889881989666234, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.12078205496072769}, {"id": 106, "seek": 47022, "start": 470.22, "end": 473.18, "text": " main memory computation, and so on.", "tokens": [50364, 2135, 4675, 24903, 11, 293, 370, 322, 13, 50512], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 107, "seek": 47022, "start": 473.18, "end": 477.82000000000005, "text": " I'm aware that there are differences between these usages, but those differences are very", "tokens": [50512, 286, 478, 3650, 300, 456, 366, 7300, 1296, 613, 505, 1660, 11, 457, 729, 7300, 366, 588, 50744], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 108, "seek": 47022, "start": 477.82000000000005, "end": 478.82000000000005, "text": " subtle.", "tokens": [50744, 13743, 13, 50794], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 109, "seek": 47022, "start": 478.82000000000005, "end": 483.3, "text": " I'm generally going to stick to saying compute in memory.", "tokens": [50794, 286, 478, 5101, 516, 281, 2897, 281, 1566, 14722, 294, 4675, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 110, "seek": 47022, "start": 483.3, "end": 487.90000000000003, "text": " The name has also been used to refer to concepts expanding on the SRAM idea.", "tokens": [51018, 440, 1315, 575, 611, 668, 1143, 281, 2864, 281, 10392, 14702, 322, 264, 20840, 2865, 1558, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 111, "seek": 47022, "start": 487.90000000000003, "end": 493.38000000000005, "text": " SRAM is often used for the memory cache that sits on chip with the CPU.", "tokens": [51248, 20840, 2865, 307, 2049, 1143, 337, 264, 4675, 19459, 300, 12696, 322, 11409, 365, 264, 13199, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 112, "seek": 47022, "start": 493.38000000000005, "end": 497.98, "text": " But what we are more referring to here is bringing processing and compute ability to", "tokens": [51522, 583, 437, 321, 366, 544, 13761, 281, 510, 307, 5062, 9007, 293, 14722, 3485, 281, 51752], "temperature": 0.0, "avg_logprob": -0.1379472567484929, "compression_ratio": 1.7, "no_speech_prob": 0.029301561415195465}, {"id": 113, "seek": 49798, "start": 497.98, "end": 500.94, "text": " the computer main memory itself.", "tokens": [50364, 264, 3820, 2135, 4675, 2564, 13, 50512], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 114, "seek": 49798, "start": 500.94, "end": 503.38, "text": " The idea is well suited for deep learning.", "tokens": [50512, 440, 1558, 307, 731, 24736, 337, 2452, 2539, 13, 50634], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 115, "seek": 49798, "start": 503.38, "end": 508.62, "text": " If you recall, running a neural network model is about calculating these massive matrices.", "tokens": [50634, 759, 291, 9901, 11, 2614, 257, 18161, 3209, 2316, 307, 466, 28258, 613, 5994, 32284, 13, 50896], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 116, "seek": 49798, "start": 508.62, "end": 515.78, "text": " The Google TPU had lots of circuits for running multiply and accumulate or MAC operations.", "tokens": [50896, 440, 3329, 314, 8115, 632, 3195, 295, 26354, 337, 2614, 12972, 293, 33384, 420, 27716, 7705, 13, 51254], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 117, "seek": 49798, "start": 515.78, "end": 518.22, "text": " The actual arithmetic is relatively simple.", "tokens": [51254, 440, 3539, 42973, 307, 7226, 2199, 13, 51376], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 118, "seek": 49798, "start": 518.22, "end": 521.34, "text": " The problem is that there is so much of it that needs to be done.", "tokens": [51376, 440, 1154, 307, 300, 456, 307, 370, 709, 295, 309, 300, 2203, 281, 312, 1096, 13, 51532], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 119, "seek": 49798, "start": 521.34, "end": 527.26, "text": " So in an ideal case, a compute in memory chip can execute MAC operations right inside the", "tokens": [51532, 407, 294, 364, 7157, 1389, 11, 257, 14722, 294, 4675, 11409, 393, 14483, 27716, 7705, 558, 1854, 264, 51828], "temperature": 0.0, "avg_logprob": -0.09203656514485677, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.012819391675293446}, {"id": 120, "seek": 52726, "start": 527.26, "end": 528.98, "text": " memory chip.", "tokens": [50364, 4675, 11409, 13, 50450], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 121, "seek": 52726, "start": 528.98, "end": 533.86, "text": " This is especially helpful for running inference on the edge outside the data center.", "tokens": [50450, 639, 307, 2318, 4961, 337, 2614, 38253, 322, 264, 4691, 2380, 264, 1412, 3056, 13, 50694], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 122, "seek": 52726, "start": 533.86, "end": 537.5, "text": " These use cases have energy size or heat restrictions.", "tokens": [50694, 1981, 764, 3331, 362, 2281, 2744, 420, 3738, 14191, 13, 50876], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 123, "seek": 52726, "start": 537.5, "end": 543.54, "text": " Being able to cut up to 80% of a neural network's energy usage is a game changer.", "tokens": [50876, 8891, 1075, 281, 1723, 493, 281, 4688, 4, 295, 257, 18161, 3209, 311, 2281, 14924, 307, 257, 1216, 22822, 13, 51178], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 124, "seek": 52726, "start": 543.54, "end": 547.7, "text": " The idea is decades old, dating back to the 1960s.", "tokens": [51178, 440, 1558, 307, 7878, 1331, 11, 10689, 646, 281, 264, 16157, 82, 13, 51386], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 125, "seek": 52726, "start": 547.7, "end": 553.54, "text": " Professor Harold Stone of Stanford University first notably explored the idea of logic and", "tokens": [51386, 8419, 36076, 15012, 295, 20374, 3535, 700, 31357, 24016, 264, 1558, 295, 9952, 293, 51678], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 126, "seek": 52726, "start": 553.54, "end": 554.54, "text": " memory.", "tokens": [51678, 4675, 13, 51728], "temperature": 0.0, "avg_logprob": -0.14651279039280388, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.017436742782592773}, {"id": 127, "seek": 55454, "start": 554.66, "end": 559.78, "text": " Stone noted that the number of transistors in a microprocessor was growing very fast,", "tokens": [50370, 15012, 12964, 300, 264, 1230, 295, 1145, 46976, 294, 257, 3123, 1513, 340, 25432, 390, 4194, 588, 2370, 11, 50626], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 128, "seek": 55454, "start": 559.78, "end": 563.8199999999999, "text": " but the processor's ability to communicate with its memory was limited by the number", "tokens": [50626, 457, 264, 15321, 311, 3485, 281, 7890, 365, 1080, 4675, 390, 5567, 538, 264, 1230, 50828], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 129, "seek": 55454, "start": 563.8199999999999, "end": 565.12, "text": " of pins.", "tokens": [50828, 295, 16392, 13, 50893], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 130, "seek": 55454, "start": 565.12, "end": 571.02, "text": " So he presented the idea of moving part of the computation into memory caches.", "tokens": [50893, 407, 415, 8212, 264, 1558, 295, 2684, 644, 295, 264, 24903, 666, 4675, 269, 13272, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 131, "seek": 55454, "start": 571.02, "end": 574.42, "text": " The 1990s saw further explorations of the idea.", "tokens": [51188, 440, 13384, 82, 1866, 3052, 24765, 763, 295, 264, 1558, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 132, "seek": 55454, "start": 574.42, "end": 579.06, "text": " In 1995, terraces produced what we would probably call the first processor in memory", "tokens": [51358, 682, 22601, 11, 7245, 2116, 7126, 437, 321, 576, 1391, 818, 264, 700, 15321, 294, 4675, 51590], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 133, "seek": 55454, "start": 579.06, "end": 580.06, "text": " chip.", "tokens": [51590, 11409, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1511523772259148, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.00012730751768685877}, {"id": 134, "seek": 58006, "start": 580.06, "end": 585.06, "text": " It was a standard 4-bit memory with an integrated single-bit logical unit.", "tokens": [50364, 467, 390, 257, 3832, 1017, 12, 5260, 4675, 365, 364, 10919, 2167, 12, 5260, 14978, 4985, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 135, "seek": 58006, "start": 585.06, "end": 590.5799999999999, "text": " This arithmetic logical unit can bring in data, apply some simple logic to it as dictated", "tokens": [50614, 639, 42973, 14978, 4985, 393, 1565, 294, 1412, 11, 3079, 512, 2199, 9952, 281, 309, 382, 12569, 770, 50890], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 136, "seek": 58006, "start": 590.5799999999999, "end": 594.14, "text": " from a program, and then write it back to memory.", "tokens": [50890, 490, 257, 1461, 11, 293, 550, 2464, 309, 646, 281, 4675, 13, 51068], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 137, "seek": 58006, "start": 594.14, "end": 599.66, "text": " And then in 1997, various professors at UC Berkeley, including Professor David Patterson,", "tokens": [51068, 400, 550, 294, 22383, 11, 3683, 15924, 412, 14079, 23684, 11, 3009, 8419, 4389, 34367, 3015, 11, 51344], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 138, "seek": 58006, "start": 599.66, "end": 605.2199999999999, "text": " the inventor of risk, created the IRAM project, with the goal of putting a microprocessor and", "tokens": [51344, 264, 41593, 295, 3148, 11, 2942, 264, 16486, 2865, 1716, 11, 365, 264, 3387, 295, 3372, 257, 3123, 1513, 340, 25432, 293, 51622], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 139, "seek": 58006, "start": 605.2199999999999, "end": 608.2199999999999, "text": " DRAM on the same chip.", "tokens": [51622, 12118, 2865, 322, 264, 912, 11409, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12306969188084112, "compression_ratio": 1.5592592592592593, "no_speech_prob": 0.00298081012442708}, {"id": 140, "seek": 60822, "start": 608.22, "end": 612.34, "text": " While other such proposals followed throughout the 1990s, but none of these ever caught", "tokens": [50364, 3987, 661, 1270, 20198, 6263, 3710, 264, 13384, 82, 11, 457, 6022, 295, 613, 1562, 5415, 50570], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 141, "seek": 60822, "start": 612.34, "end": 614.9, "text": " on for a number of practical reasons.", "tokens": [50570, 322, 337, 257, 1230, 295, 8496, 4112, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 142, "seek": 60822, "start": 614.9, "end": 618.78, "text": " First, memory and logic are hard to manufacture together.", "tokens": [50698, 2386, 11, 4675, 293, 9952, 366, 1152, 281, 27400, 1214, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 143, "seek": 60822, "start": 618.78, "end": 622.22, "text": " Their fabrication have sort of opposing goals.", "tokens": [50892, 6710, 44820, 362, 1333, 295, 27890, 5493, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 144, "seek": 60822, "start": 622.22, "end": 628.38, "text": " Again, logic transistors are all about speed and performance, but memory transistors have", "tokens": [51064, 3764, 11, 9952, 1145, 46976, 366, 439, 466, 3073, 293, 3389, 11, 457, 4675, 1145, 46976, 362, 51372], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 145, "seek": 60822, "start": 628.38, "end": 633.22, "text": " to be about high density, low cost, and low leakage all at once.", "tokens": [51372, 281, 312, 466, 1090, 10305, 11, 2295, 2063, 11, 293, 2295, 47799, 439, 412, 1564, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1769005987379286, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.013220689259469509}, {"id": 146, "seek": 63322, "start": 633.98, "end": 638.58, "text": " It is hard to build a logic process with a DRAM process and vice versa.", "tokens": [50402, 467, 307, 1152, 281, 1322, 257, 9952, 1399, 365, 257, 12118, 2865, 1399, 293, 11964, 25650, 13, 50632], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 147, "seek": 63322, "start": 638.58, "end": 642.38, "text": " DRAM designs are very regular, with lots of parallel wires.", "tokens": [50632, 12118, 2865, 11347, 366, 588, 3890, 11, 365, 3195, 295, 8952, 15537, 13, 50822], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 148, "seek": 63322, "start": 642.38, "end": 646.74, "text": " Logic designs, on the other hand, have much more complexity.", "tokens": [50822, 49898, 11347, 11, 322, 264, 661, 1011, 11, 362, 709, 544, 14024, 13, 51040], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 149, "seek": 63322, "start": 646.74, "end": 650.82, "text": " Circuit elements in a chip have connections, referred to as metal layers.", "tokens": [51040, 36939, 4959, 294, 257, 11409, 362, 9271, 11, 10839, 281, 382, 5760, 7914, 13, 51244], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 150, "seek": 63322, "start": 650.82, "end": 656.5400000000001, "text": " Having more metal layers allows for more complexity, but at the cost of current leakage and worse", "tokens": [51244, 10222, 544, 5760, 7914, 4045, 337, 544, 14024, 11, 457, 412, 264, 2063, 295, 2190, 47799, 293, 5324, 51530], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 151, "seek": 63322, "start": 656.5400000000001, "end": 658.5400000000001, "text": " reliability.", "tokens": [51530, 24550, 13, 51630], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 152, "seek": 63322, "start": 658.5400000000001, "end": 662.5, "text": " Contemporary DRAM processes use three to four metal layers.", "tokens": [51630, 4839, 11840, 822, 12118, 2865, 7555, 764, 1045, 281, 1451, 5760, 7914, 13, 51828], "temperature": 0.0, "avg_logprob": -0.15891724712443803, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.06185571849346161}, {"id": 153, "seek": 66250, "start": 662.5, "end": 668.14, "text": " Contemporary logic processes use anywhere from 7 to 12 and even more metal layers.", "tokens": [50364, 4839, 11840, 822, 9952, 7555, 764, 4992, 490, 1614, 281, 2272, 293, 754, 544, 5760, 7914, 13, 50646], "temperature": 0.0, "avg_logprob": -0.12114389046378758, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.008576801046729088}, {"id": 154, "seek": 66250, "start": 668.14, "end": 673.22, "text": " So it is estimated that if you tried to make logic circuits with a DRAM process, then the", "tokens": [50646, 407, 309, 307, 14109, 300, 498, 291, 3031, 281, 652, 9952, 26354, 365, 257, 12118, 2865, 1399, 11, 550, 264, 50900], "temperature": 0.0, "avg_logprob": -0.12114389046378758, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.008576801046729088}, {"id": 155, "seek": 66250, "start": 673.22, "end": 678.46, "text": " logic circuits would be 80% bigger and perform 22% worse.", "tokens": [50900, 9952, 26354, 576, 312, 4688, 4, 3801, 293, 2042, 5853, 4, 5324, 13, 51162], "temperature": 0.0, "avg_logprob": -0.12114389046378758, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.008576801046729088}, {"id": 156, "seek": 66250, "start": 678.46, "end": 683.62, "text": " And if you were to try to make DRAM cells with a logic process, then you are essentially", "tokens": [51162, 400, 498, 291, 645, 281, 853, 281, 652, 12118, 2865, 5438, 365, 257, 9952, 1399, 11, 550, 291, 366, 4476, 51420], "temperature": 0.0, "avg_logprob": -0.12114389046378758, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.008576801046729088}, {"id": 157, "seek": 66250, "start": 683.62, "end": 687.18, "text": " making embedded DRAM, or eDRAM.", "tokens": [51420, 1455, 16741, 12118, 2865, 11, 420, 308, 9301, 2865, 13, 51598], "temperature": 0.0, "avg_logprob": -0.12114389046378758, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.008576801046729088}, {"id": 158, "seek": 68718, "start": 687.2199999999999, "end": 694.26, "text": " The cells use significantly more power, take up to 10 times more space, and are less reliable.", "tokens": [50366, 440, 5438, 764, 10591, 544, 1347, 11, 747, 493, 281, 1266, 1413, 544, 1901, 11, 293, 366, 1570, 12924, 13, 50718], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 159, "seek": 68718, "start": 694.26, "end": 699.02, "text": " The industry has since considered these manufacturing shortcomings and have come up with a variety", "tokens": [50718, 440, 3518, 575, 1670, 4888, 613, 11096, 2099, 49886, 293, 362, 808, 493, 365, 257, 5673, 50956], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 160, "seek": 68718, "start": 699.02, "end": 700.5, "text": " of workarounds.", "tokens": [50956, 295, 589, 289, 4432, 13, 51030], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 161, "seek": 68718, "start": 700.5, "end": 707.5799999999999, "text": " Many proposals of compute and memory are on three levels, the device, circuit, and system.", "tokens": [51030, 5126, 20198, 295, 14722, 293, 4675, 366, 322, 1045, 4358, 11, 264, 4302, 11, 9048, 11, 293, 1185, 13, 51384], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 162, "seek": 68718, "start": 707.5799999999999, "end": 712.3, "text": " The device level leans on new types of memory hardware other than the conventional DRAM", "tokens": [51384, 440, 4302, 1496, 476, 599, 322, 777, 3467, 295, 4675, 8837, 661, 813, 264, 16011, 12118, 2865, 51620], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 163, "seek": 68718, "start": 712.3, "end": 714.06, "text": " and SRAM memories.", "tokens": [51620, 293, 20840, 2865, 8495, 13, 51708], "temperature": 0.0, "avg_logprob": -0.10679027438163757, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.0011694517452269793}, {"id": 164, "seek": 71406, "start": 714.06, "end": 721.5799999999999, "text": " Notable examples include resistive random access memory, or rerAM, or rRAM, and spin", "tokens": [50364, 1726, 712, 5110, 4090, 4597, 488, 4974, 2105, 4675, 11, 420, 43819, 2865, 11, 420, 367, 49, 2865, 11, 293, 6060, 50740], "temperature": 0.0, "avg_logprob": -0.2108893045564977, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.007344696670770645}, {"id": 165, "seek": 71406, "start": 721.5799999999999, "end": 728.8199999999999, "text": " transfer torque magneto resistive random access memory, or STT-MRAM.", "tokens": [50740, 5003, 16437, 15211, 78, 4597, 488, 4974, 2105, 4675, 11, 420, 4904, 51, 12, 44, 49, 2865, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2108893045564977, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.007344696670770645}, {"id": 166, "seek": 71406, "start": 728.8199999999999, "end": 732.5799999999999, "text": " RerAM is one of the more promising emerging memory technologies.", "tokens": [51102, 497, 260, 2865, 307, 472, 295, 264, 544, 20257, 14989, 4675, 7943, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2108893045564977, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.007344696670770645}, {"id": 167, "seek": 71406, "start": 732.5799999999999, "end": 737.5799999999999, "text": " Like I mentioned earlier, conventional RAM memories store information using a charge", "tokens": [51290, 1743, 286, 2835, 3071, 11, 16011, 14561, 8495, 3531, 1589, 1228, 257, 4602, 51540], "temperature": 0.0, "avg_logprob": -0.2108893045564977, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.007344696670770645}, {"id": 168, "seek": 71406, "start": 737.5799999999999, "end": 740.42, "text": " stored within a capacitor.", "tokens": [51540, 12187, 1951, 257, 29372, 13, 51682], "temperature": 0.0, "avg_logprob": -0.2108893045564977, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.007344696670770645}, {"id": 169, "seek": 74042, "start": 740.42, "end": 745.74, "text": " RerAM instead stores information by changing the electrical resistance of a certain material,", "tokens": [50364, 497, 260, 2865, 2602, 9512, 1589, 538, 4473, 264, 12147, 7335, 295, 257, 1629, 2527, 11, 50630], "temperature": 0.0, "avg_logprob": -0.12730604807535809, "compression_ratio": 1.624031007751938, "no_speech_prob": 0.018542535603046417}, {"id": 170, "seek": 74042, "start": 745.74, "end": 751.38, "text": " referred to as a resistor, switching between a high and low resistance state.", "tokens": [50630, 10839, 281, 382, 257, 37672, 11, 16493, 1296, 257, 1090, 293, 2295, 7335, 1785, 13, 50912], "temperature": 0.0, "avg_logprob": -0.12730604807535809, "compression_ratio": 1.624031007751938, "no_speech_prob": 0.018542535603046417}, {"id": 171, "seek": 74042, "start": 751.38, "end": 758.14, "text": " This structure allows rerAM to compute logical functions directly within the memory cells.", "tokens": [50912, 639, 3877, 4045, 43819, 2865, 281, 14722, 14978, 6828, 3838, 1951, 264, 4675, 5438, 13, 51250], "temperature": 0.0, "avg_logprob": -0.12730604807535809, "compression_ratio": 1.624031007751938, "no_speech_prob": 0.018542535603046417}, {"id": 172, "seek": 74042, "start": 758.14, "end": 762.74, "text": " Please don't ask me to try to explain it any further than that.", "tokens": [51250, 2555, 500, 380, 1029, 385, 281, 853, 281, 2903, 309, 604, 3052, 813, 300, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12730604807535809, "compression_ratio": 1.624031007751938, "no_speech_prob": 0.018542535603046417}, {"id": 173, "seek": 74042, "start": 762.74, "end": 767.42, "text": " RerAM is probably the emerging memory technology that is closest to commercialization due to", "tokens": [51480, 497, 260, 2865, 307, 1391, 264, 14989, 4675, 2899, 300, 307, 13699, 281, 6841, 2144, 3462, 281, 51714], "temperature": 0.0, "avg_logprob": -0.12730604807535809, "compression_ratio": 1.624031007751938, "no_speech_prob": 0.018542535603046417}, {"id": 174, "seek": 76742, "start": 767.42, "end": 773.5799999999999, "text": " it being compatible with silicon CMOS, however there remain substantial hurdles to overcome", "tokens": [50364, 309, 885, 18218, 365, 22848, 20424, 4367, 11, 4461, 456, 6222, 16726, 48387, 281, 10473, 50672], "temperature": 0.0, "avg_logprob": -0.12056497879970221, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.054981112480163574}, {"id": 175, "seek": 76742, "start": 773.5799999999999, "end": 777.74, "text": " before we see products arrive on shelves.", "tokens": [50672, 949, 321, 536, 3383, 8881, 322, 24349, 13, 50880], "temperature": 0.0, "avg_logprob": -0.12056497879970221, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.054981112480163574}, {"id": 176, "seek": 76742, "start": 777.74, "end": 782.62, "text": " The circuit level is where we modify peripheral circuits to do the calculations right inside", "tokens": [50880, 440, 9048, 1496, 307, 689, 321, 16927, 40235, 26354, 281, 360, 264, 20448, 558, 1854, 51124], "temperature": 0.0, "avg_logprob": -0.12056497879970221, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.054981112480163574}, {"id": 177, "seek": 76742, "start": 782.62, "end": 786.8199999999999, "text": " the SRAM or DRAM memory arrays themselves.", "tokens": [51124, 264, 20840, 2865, 420, 12118, 2865, 4675, 41011, 2969, 13, 51334], "temperature": 0.0, "avg_logprob": -0.12056497879970221, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.054981112480163574}, {"id": 178, "seek": 76742, "start": 786.8199999999999, "end": 793.3, "text": " The phrase I see a lot is in C2 computing, in C2 meaning locally or on site.", "tokens": [51334, 440, 9535, 286, 536, 257, 688, 307, 294, 383, 17, 15866, 11, 294, 383, 17, 3620, 16143, 420, 322, 3621, 13, 51658], "temperature": 0.0, "avg_logprob": -0.12056497879970221, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.054981112480163574}, {"id": 179, "seek": 79330, "start": 793.3, "end": 797.18, "text": " Memories are particularly clever, but they also require an intimate knowledge of how", "tokens": [50364, 8731, 2083, 366, 4098, 13494, 11, 457, 436, 611, 3651, 364, 20215, 3601, 295, 577, 50558], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 180, "seek": 79330, "start": 797.18, "end": 801.9799999999999, "text": " memory works and still can be difficult to understand.", "tokens": [50558, 4675, 1985, 293, 920, 393, 312, 2252, 281, 1223, 13, 50798], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 181, "seek": 79330, "start": 801.9799999999999, "end": 807.3, "text": " One prominent example of this is Ombit, an in-memory accelerator proposed by people from", "tokens": [50798, 1485, 17034, 1365, 295, 341, 307, 422, 2504, 270, 11, 364, 294, 12, 17886, 827, 39889, 10348, 538, 561, 490, 51064], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 182, "seek": 79330, "start": 807.3, "end": 813.3, "text": " Microsoft, NVIDIA, Intel, ETH, Zurich, and Carnegie Mellon.", "tokens": [51064, 8116, 11, 426, 3958, 6914, 11, 19762, 11, 462, 9620, 11, 30518, 480, 11, 293, 47301, 376, 898, 266, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 183, "seek": 79330, "start": 813.3, "end": 818.42, "text": " A DRAM memory contains sub arrays with many rows of DRAM cells.", "tokens": [51364, 316, 12118, 2865, 4675, 8306, 1422, 41011, 365, 867, 13241, 295, 12118, 2865, 5438, 13, 51620], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 184, "seek": 79330, "start": 818.42, "end": 822.6999999999999, "text": " In normal use, the memory activates one row at a time.", "tokens": [51620, 682, 2710, 764, 11, 264, 4675, 43869, 472, 5386, 412, 257, 565, 13, 51834], "temperature": 0.0, "avg_logprob": -0.12410861755085882, "compression_ratio": 1.4908424908424909, "no_speech_prob": 0.4957728385925293}, {"id": 185, "seek": 82270, "start": 822.7, "end": 828.38, "text": " This system activates three rows at a time in order to implement an AND slash OR logic", "tokens": [50364, 639, 1185, 43869, 1045, 13241, 412, 257, 565, 294, 1668, 281, 4445, 364, 8093, 17330, 19654, 9952, 50648], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 186, "seek": 82270, "start": 828.38, "end": 833.44, "text": " function, two rows for the inputs and one for the output.", "tokens": [50648, 2445, 11, 732, 13241, 337, 264, 15743, 293, 472, 337, 264, 5598, 13, 50901], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 187, "seek": 82270, "start": 833.44, "end": 835.5400000000001, "text": " The concept is logically attractive.", "tokens": [50901, 440, 3410, 307, 38887, 12609, 13, 51006], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 188, "seek": 82270, "start": 835.5400000000001, "end": 839.9000000000001, "text": " You can utilize the memory's internal bandwidth to do all the calculations.", "tokens": [51006, 509, 393, 16117, 264, 4675, 311, 6920, 23647, 281, 360, 439, 264, 20448, 13, 51224], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 189, "seek": 82270, "start": 839.9000000000001, "end": 843.22, "text": " However, there are significant concerns.", "tokens": [51224, 2908, 11, 456, 366, 4776, 7389, 13, 51390], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 190, "seek": 82270, "start": 843.22, "end": 850.0200000000001, "text": " Ombit can perform basing AND or and NOT logic operations, but it takes multiple cycles.", "tokens": [51390, 422, 2504, 270, 393, 2042, 987, 278, 8093, 420, 293, 12854, 9952, 7705, 11, 457, 309, 2516, 3866, 17796, 13, 51730], "temperature": 0.0, "avg_logprob": -0.19733649033766526, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0008040440734475851}, {"id": 191, "seek": 85002, "start": 850.02, "end": 857.1, "text": " Furthermore, more complex logic implementations like XNOR remain challenging to utilize.", "tokens": [50364, 23999, 11, 544, 3997, 9952, 4445, 763, 411, 1783, 45, 2483, 6222, 7595, 281, 16117, 13, 50718], "temperature": 0.0, "avg_logprob": -0.16115984462556385, "compression_ratio": 1.4257028112449799, "no_speech_prob": 0.02160682901740074}, {"id": 192, "seek": 85002, "start": 857.1, "end": 862.18, "text": " So far, the big downside with these two compute and memory approaches is that their performance", "tokens": [50718, 407, 1400, 11, 264, 955, 25060, 365, 613, 732, 14722, 293, 4675, 11587, 307, 300, 641, 3389, 50972], "temperature": 0.0, "avg_logprob": -0.16115984462556385, "compression_ratio": 1.4257028112449799, "no_speech_prob": 0.02160682901740074}, {"id": 193, "seek": 85002, "start": 862.18, "end": 867.46, "text": " still falls short of what can be achieved with current Von Neumann GPU slash ASIC-centric", "tokens": [50972, 920, 8804, 2099, 295, 437, 393, 312, 11042, 365, 2190, 20700, 1734, 449, 969, 18407, 17330, 7469, 2532, 12, 45300, 51236], "temperature": 0.0, "avg_logprob": -0.16115984462556385, "compression_ratio": 1.4257028112449799, "no_speech_prob": 0.02160682901740074}, {"id": 194, "seek": 85002, "start": 867.46, "end": 869.02, "text": " systems.", "tokens": [51236, 3652, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16115984462556385, "compression_ratio": 1.4257028112449799, "no_speech_prob": 0.02160682901740074}, {"id": 195, "seek": 85002, "start": 869.02, "end": 873.74, "text": " In other words, they suffer the same drawbacks people saw in the 1990s.", "tokens": [51314, 682, 661, 2283, 11, 436, 9753, 264, 912, 2642, 17758, 561, 1866, 294, 264, 13384, 82, 13, 51550], "temperature": 0.0, "avg_logprob": -0.16115984462556385, "compression_ratio": 1.4257028112449799, "no_speech_prob": 0.02160682901740074}, {"id": 196, "seek": 87374, "start": 873.74, "end": 880.1800000000001, "text": " Putting memory and logic together still makes a jack of all trades master of none situation.", "tokens": [50364, 31367, 4675, 293, 9952, 1214, 920, 1669, 257, 7109, 295, 439, 21287, 4505, 295, 6022, 2590, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 197, "seek": 87374, "start": 880.1800000000001, "end": 884.46, "text": " So the middle ground that the industry seems to be moving towards is implementing compute", "tokens": [50686, 407, 264, 2808, 2727, 300, 264, 3518, 2544, 281, 312, 2684, 3030, 307, 18114, 14722, 50900], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 198, "seek": 87374, "start": 884.46, "end": 886.94, "text": " and memory at a system level.", "tokens": [50900, 293, 4675, 412, 257, 1185, 1496, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 199, "seek": 87374, "start": 886.94, "end": 891.66, "text": " This is where we integrate together discrete processing units and memory at a very close", "tokens": [51024, 639, 307, 689, 321, 13365, 1214, 27706, 9007, 6815, 293, 4675, 412, 257, 588, 1998, 51260], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 200, "seek": 87374, "start": 891.66, "end": 893.58, "text": " level.", "tokens": [51260, 1496, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 201, "seek": 87374, "start": 893.58, "end": 900.22, "text": " This is enabled thanks to new packaging technologies like 2.5D or 3D memory die stacking where we", "tokens": [51356, 639, 307, 15172, 3231, 281, 777, 16836, 7943, 411, 568, 13, 20, 35, 420, 805, 35, 4675, 978, 41376, 689, 321, 51688], "temperature": 0.0, "avg_logprob": -0.1328447048480694, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.452804833650589}, {"id": 202, "seek": 90022, "start": 900.22, "end": 904.7, "text": " stack a bunch of DRAM memory dies on top of a CPU die.", "tokens": [50364, 8630, 257, 3840, 295, 12118, 2865, 4675, 2714, 322, 1192, 295, 257, 13199, 978, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 203, "seek": 90022, "start": 904.7, "end": 909.12, "text": " The memories are then connected to the CPU using thousands of channels called through", "tokens": [50588, 440, 8495, 366, 550, 4582, 281, 264, 13199, 1228, 5383, 295, 9235, 1219, 807, 50809], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 204, "seek": 90022, "start": 909.12, "end": 912.0600000000001, "text": " silicon vias or TSVs.", "tokens": [50809, 22848, 1932, 296, 420, 37645, 53, 82, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 205, "seek": 90022, "start": 912.0600000000001, "end": 915.1, "text": " This gives you immense amounts of internal bandwidth.", "tokens": [50956, 639, 2709, 291, 22920, 11663, 295, 6920, 23647, 13, 51108], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 206, "seek": 90022, "start": 915.1, "end": 920.58, "text": " AMD is working on something kind of like this with what they call 3DVcache, which is based", "tokens": [51108, 34808, 307, 1364, 322, 746, 733, 295, 411, 341, 365, 437, 436, 818, 805, 35, 53, 66, 6000, 11, 597, 307, 2361, 51382], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 207, "seek": 90022, "start": 920.58, "end": 924.3000000000001, "text": " on a TSMC 3D stacking packaging technology.", "tokens": [51382, 322, 257, 314, 26693, 34, 805, 35, 41376, 16836, 2899, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 208, "seek": 90022, "start": 924.3000000000001, "end": 928.46, "text": " They used it to add more memory cache to their processor chips.", "tokens": [51568, 814, 1143, 309, 281, 909, 544, 4675, 19459, 281, 641, 15321, 11583, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1673837944313332, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.07156985253095627}, {"id": 209, "seek": 92846, "start": 928.46, "end": 933.02, "text": " There is a future where we can use similarly advanced packaging technologies to add hundreds", "tokens": [50364, 821, 307, 257, 2027, 689, 321, 393, 764, 14138, 7339, 16836, 7943, 281, 909, 6779, 50592], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 210, "seek": 92846, "start": 933.02, "end": 936.98, "text": " of gigabytes of memory to an AI ASIC.", "tokens": [50592, 295, 42741, 295, 4675, 281, 364, 7318, 7469, 2532, 13, 50790], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 211, "seek": 92846, "start": 936.98, "end": 942.46, "text": " This lets us integrate together world-class memory and logic dies closer than ever before", "tokens": [50790, 639, 6653, 505, 13365, 1214, 1002, 12, 11665, 4675, 293, 9952, 2714, 4966, 813, 1562, 949, 51064], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 212, "seek": 92846, "start": 942.46, "end": 946.46, "text": " without needing to place them on the same die.", "tokens": [51064, 1553, 18006, 281, 1081, 552, 322, 264, 912, 978, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 213, "seek": 92846, "start": 946.46, "end": 948.3000000000001, "text": " Ideas in the laboratory are cheap.", "tokens": [51264, 13090, 296, 294, 264, 16523, 366, 7084, 13, 51356], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 214, "seek": 92846, "start": 948.3000000000001, "end": 953.82, "text": " What is harder is to execute on those ideas in a way that performs well enough to replace", "tokens": [51356, 708, 307, 6081, 307, 281, 14483, 322, 729, 3487, 294, 257, 636, 300, 26213, 731, 1547, 281, 7406, 51632], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 215, "seek": 92846, "start": 953.82, "end": 956.58, "text": " what is already out there on the market.", "tokens": [51632, 437, 307, 1217, 484, 456, 322, 264, 2142, 13, 51770], "temperature": 0.0, "avg_logprob": -0.08822942247577742, "compression_ratio": 1.6278195488721805, "no_speech_prob": 0.003376463195309043}, {"id": 216, "seek": 95658, "start": 956.58, "end": 964.62, "text": " And the fact is that the current NVIDIA A100 and H100 AI GPUs are a very formidable competitor.", "tokens": [50364, 400, 264, 1186, 307, 300, 264, 2190, 426, 3958, 6914, 316, 6879, 293, 389, 6879, 7318, 18407, 82, 366, 257, 588, 41246, 27266, 13, 50766], "temperature": 0.0, "avg_logprob": -0.1710296882377876, "compression_ratio": 1.5092250922509225, "no_speech_prob": 0.09531859308481216}, {"id": 217, "seek": 95658, "start": 964.62, "end": 968.74, "text": " But with leading edge semiconductor technology slowing down the way they are, we need new", "tokens": [50766, 583, 365, 5775, 4691, 45310, 2899, 26958, 760, 264, 636, 436, 366, 11, 321, 643, 777, 50972], "temperature": 0.0, "avg_logprob": -0.1710296882377876, "compression_ratio": 1.5092250922509225, "no_speech_prob": 0.09531859308481216}, {"id": 218, "seek": 95658, "start": 968.74, "end": 975.1800000000001, "text": " ways to leapfrog towards more powerful and robust hardware for running AI.", "tokens": [50972, 2098, 281, 19438, 69, 6675, 3030, 544, 4005, 293, 13956, 8837, 337, 2614, 7318, 13, 51294], "temperature": 0.0, "avg_logprob": -0.1710296882377876, "compression_ratio": 1.5092250922509225, "no_speech_prob": 0.09531859308481216}, {"id": 219, "seek": 95658, "start": 975.1800000000001, "end": 977.74, "text": " Generally speaking, bigger models perform better.", "tokens": [51294, 21082, 4124, 11, 3801, 5245, 2042, 1101, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1710296882377876, "compression_ratio": 1.5092250922509225, "no_speech_prob": 0.09531859308481216}, {"id": 220, "seek": 95658, "start": 977.74, "end": 983.22, "text": " Today's best performing natural language processing and computer vision models are great, but they", "tokens": [51422, 2692, 311, 1151, 10205, 3303, 2856, 9007, 293, 3820, 5201, 5245, 366, 869, 11, 457, 436, 51696], "temperature": 0.0, "avg_logprob": -0.1710296882377876, "compression_ratio": 1.5092250922509225, "no_speech_prob": 0.09531859308481216}, {"id": 221, "seek": 98322, "start": 983.22, "end": 989.1800000000001, "text": " still have ways to go, which means they might have to get bigger so to get better.", "tokens": [50364, 920, 362, 2098, 281, 352, 11, 597, 1355, 436, 1062, 362, 281, 483, 3801, 370, 281, 483, 1101, 13, 50662], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}, {"id": 222, "seek": 98322, "start": 989.1800000000001, "end": 995.02, "text": " But unless we develop new systems and hardware that can overcome these aforementioned limits,", "tokens": [50662, 583, 5969, 321, 1499, 777, 3652, 293, 8837, 300, 393, 10473, 613, 48927, 46842, 10406, 11, 50954], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}, {"id": 223, "seek": 98322, "start": 995.02, "end": 999.62, "text": " it seems that deep learning might fall short of fulfilling its great expectations.", "tokens": [50954, 309, 2544, 300, 2452, 2539, 1062, 2100, 2099, 295, 25800, 1080, 869, 9843, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}, {"id": 224, "seek": 98322, "start": 999.62, "end": 1002.02, "text": " Alright everyone, that's it for tonight.", "tokens": [51184, 2798, 1518, 11, 300, 311, 309, 337, 4440, 13, 51304], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}, {"id": 225, "seek": 98322, "start": 1002.02, "end": 1003.02, "text": " Thanks for watching.", "tokens": [51304, 2561, 337, 1976, 13, 51354], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}, {"id": 226, "seek": 98322, "start": 1003.02, "end": 1005.62, "text": " Subscribe to the channel, sign up for the newsletter, and I'll see you guys next time.", "tokens": [51354, 10611, 281, 264, 2269, 11, 1465, 493, 337, 264, 26469, 11, 293, 286, 603, 536, 291, 1074, 958, 565, 13, 51484], "temperature": 0.0, "avg_logprob": -0.15515802283036081, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.5229280591011047}], "language": "en"}