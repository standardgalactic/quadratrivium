start	end	text
0	7520	Since the deep learning explosion started in 2012, the industry's biggest models have
7520	10160	grown hundreds of thousands of times.
10160	18520	Today, OpenAI's Dolly 2 has 3.5 billion parameters, Google's Imagine has 4.6 billion, and GPT-3
18520	22360	has 175 billion parameters.
22360	27040	Increasingly larger models are ahead, Google recently pre-trained a model with 1 trillion
27040	29160	parameters.
29160	33760	These increasingly large models strain the ability of our hardware to accommodate them,
33760	38520	and many of these limitations tie back to memory and how we use it.
38520	42520	In this video, we're going to look at deep learning's memory wall problem, and some
42520	47480	of the memory-centric paradigms researchers are looking at to solve it.
47480	49680	But first, the Asianometry Patreon.
49680	53520	Early access members get to watch new videos and selected references for them before their
53520	55000	release in the public.
55000	60280	To help support the videos and appreciate every pledge, thanks, and on with the show.
60280	65280	Virtually every modern computer runs what is called a von Neumann architecture, meaning
65280	70280	that it stores both its instructions and data on the same memory bank.
70280	74840	At its heart are your processing units, a CPU or a GPU.
74840	81480	Those processing units access memory to execute its instructions and process its data.
81480	84560	The von Neumann architecture has been really good for us.
84560	89800	It helped make software as powerful as it is today, but it works nothing like a real
89800	91520	human brain.
91520	96200	The brain's compute ability is relatively low precision, but it tightly integrates
96200	101200	that compute with memory and input-output communication.
101200	106520	Computers on the other hand run on high precision, 32 or 64-bit floating-point arithmetic for
106520	111600	instance, but separates that compute from memory and communication.
111600	116040	This separation has consequences, especially for memory.
116040	120880	The AI hardware industry is scaling up memory and processing unit performance as fast as
120880	121880	they can.
121880	128000	NVIDIA's V100 GPU released in 2017 had a 32 gigabyte offering.
128000	135600	Today, the top of the line NVIDIA data center GPUs A100 and H100 sport 80 gigabytes of memory.
135800	140160	Despite this bulking up, hardware performance is not keeping up with how fast these models
140160	144040	are growing, especially when it comes to memory.
144040	149280	Memory allocations for leading edge models can easily exceed hundreds of gigabytes.
149280	153680	Even with the latest parallelization techniques, a trillion-parameter model is estimated to
153680	160280	require 320 A100 GPUs each with 80 gigabytes of memory.
160280	165240	These differences in processing and capacity mean that a processing unit wastes multiple
165240	170280	processing cycles, waiting not only for all of the data to travel in and out of the memory,
170280	175200	but also for the memory to perform its read-slash-write operation.
175200	179800	This limitation is known as the memory wall, or memory capacity bottleneck.
179800	185280	Alright then, the obvious solution would then be to add more memory to our GPUs, right?
185280	187600	What is stopping us from doing that?
187600	191840	There are practical and technology limits to how much extra memory you can add on, not
191840	195120	to mention the issues of connections and wiring.
195120	199600	Just think about how widening a highway does not much help with traffic.
199600	204640	Additionally, there are very significant energy limitations associated with shuttling data
204640	207200	between the chip and the memory.
207200	210960	These electric connections have losses which cost energy.
210960	213440	I mentioned this in previous videos.
213440	218920	Accessing memory off chip uses 200 times more energy than a floating point operation.
219480	224920	80% of the Google TPU's energy usage is from its electrical connections rather than its
224920	227520	logic computational units.
227520	233920	In some recent GPU and CPU systems, DRAM by itself accounts for 40% of the total system
233920	235440	power.
235440	241440	Energy makes up 40% of a data center's operating costs, so for this reason, storage and memory
241440	247880	has come to be a significant factor in the data center's ongoing profitability.
247880	252620	In addition to the significant operating costs of the energy, are the upfront capital
252620	256320	costs of purchasing the AI hardware itself.
256320	262440	As I mentioned earlier, a possible trillion parameter model would need 320 A100 GPUs,
262440	264560	each with 80 gigabytes of memory.
264560	270880	A100s cost $32,000 each at MSRP, so that's a clean $10 million.
270880	276460	A 100 trillion parameter model might require over 6,000 such GPUs.
276460	280740	That is just the cost of purchasing the hardware and does not even count the aforementioned
280740	285900	energy costs of using these things to run inference on them, which is where 90% of a
285900	288420	model's total costs are.
288420	295700	It risks restricting the benefits of advanced AI only to the Uber-rich tech giants or governments.
295700	299700	Much of these shortcomings are tied to historical and technological limits.
299700	305620	In the 1960s and 70s, the industry adopted dynamic random access memory to form the basis
305620	307380	of our computers.
307380	311300	This adoption was largely made for technological and economic reasons.
311300	316860	DRAM memories had relatively low latency and were cheap to manufacture in bulk.
316860	318500	This worked fine for a while.
318500	325020	As late as 1995, the memory industry was valued at $37 billion, with microprocessors at $20
325020	326780	billion.
326780	331540	But after 1980, compute scaling far outpaced memory scaling.
331540	336620	This is because, generally speaking, the CPU or GPU industries have had just one metric
336620	340220	to optimize towards, transistor density.
340220	344900	The memory industry, on the other hand, not only has the scaled DRAM capacity but also
344900	348140	bandwidth and latency at the same time.
348140	352020	Something has to give and usually that has been latency.
352020	359140	Over the past 20 years, memory capacity has improved 128 times and bandwidth 20 times.
359140	362580	latency however, has improved by just 30%.
362580	368100	Secondly, the memory industry realized that shrinking DRAM cells beyond a certain size
368100	369860	gave you worse performance.
369860	376580	Less reliability, less secure, worse latency, energy inefficiency, and so on.
376580	377580	Here's why.
377580	382620	A DRAM cell stores one bit of data in a form of a charge within a capacitor, a capacitor
382620	386940	being a device that stores electrical energy within a field.
386940	391620	That bit is accessed using an access transistor.
391620	397420	As you scale the cell down to nanoscale sizes, that capacitor and its access transistor get
397420	401700	leakier and more vulnerable to outside electrical noise.
401700	406180	It also opens up new security vulnerabilities.
406180	410780	These technical limitations and problems are fundamental to how the hardware works, which
410780	415180	makes them extremely difficult to engineer our way around.
415180	420980	The industry is going to grind out small solutions, but those will be small.
420980	426300	So the problem also opens the door to brand new radical ideas that might give us a possible
426300	430140	10x improvement over the current paradigm.
430140	434340	In a previous video, I talked about the Silicon Photonic AI accelerator, where we tried to
434340	439060	use light's properties to make data transfer more energy efficient.
439060	441420	Alongside that, we have another idea.
441420	445540	Let's alleviate or even possibly eliminate the von Neumann bottleneck and memory wall
445540	451420	problems by making the memory do the computations themselves.
451420	455860	Compute in memory refers to a random access memory with processing elements integrated
455860	456860	together.
456860	462140	The two might be very near each other or even be integrated onto the same die.
462140	465860	I've seen the idea be called other things throughout the years.
465860	470220	Processing and memory, computational RAM, near data computing, memory-centric computing,
470220	473180	main memory computation, and so on.
473180	477820	I'm aware that there are differences between these usages, but those differences are very
477820	478820	subtle.
478820	483300	I'm generally going to stick to saying compute in memory.
483300	487900	The name has also been used to refer to concepts expanding on the SRAM idea.
487900	493380	SRAM is often used for the memory cache that sits on chip with the CPU.
493380	497980	But what we are more referring to here is bringing processing and compute ability to
497980	500940	the computer main memory itself.
500940	503380	The idea is well suited for deep learning.
503380	508620	If you recall, running a neural network model is about calculating these massive matrices.
508620	515780	The Google TPU had lots of circuits for running multiply and accumulate or MAC operations.
515780	518220	The actual arithmetic is relatively simple.
518220	521340	The problem is that there is so much of it that needs to be done.
521340	527260	So in an ideal case, a compute in memory chip can execute MAC operations right inside the
527260	528980	memory chip.
528980	533860	This is especially helpful for running inference on the edge outside the data center.
533860	537500	These use cases have energy size or heat restrictions.
537500	543540	Being able to cut up to 80% of a neural network's energy usage is a game changer.
543540	547700	The idea is decades old, dating back to the 1960s.
547700	553540	Professor Harold Stone of Stanford University first notably explored the idea of logic and
553540	554540	memory.
554660	559780	Stone noted that the number of transistors in a microprocessor was growing very fast,
559780	563820	but the processor's ability to communicate with its memory was limited by the number
563820	565120	of pins.
565120	571020	So he presented the idea of moving part of the computation into memory caches.
571020	574420	The 1990s saw further explorations of the idea.
574420	579060	In 1995, terraces produced what we would probably call the first processor in memory
579060	580060	chip.
580060	585060	It was a standard 4-bit memory with an integrated single-bit logical unit.
585060	590580	This arithmetic logical unit can bring in data, apply some simple logic to it as dictated
590580	594140	from a program, and then write it back to memory.
594140	599660	And then in 1997, various professors at UC Berkeley, including Professor David Patterson,
599660	605220	the inventor of risk, created the IRAM project, with the goal of putting a microprocessor and
605220	608220	DRAM on the same chip.
608220	612340	While other such proposals followed throughout the 1990s, but none of these ever caught
612340	614900	on for a number of practical reasons.
614900	618780	First, memory and logic are hard to manufacture together.
618780	622220	Their fabrication have sort of opposing goals.
622220	628380	Again, logic transistors are all about speed and performance, but memory transistors have
628380	633220	to be about high density, low cost, and low leakage all at once.
633980	638580	It is hard to build a logic process with a DRAM process and vice versa.
638580	642380	DRAM designs are very regular, with lots of parallel wires.
642380	646740	Logic designs, on the other hand, have much more complexity.
646740	650820	Circuit elements in a chip have connections, referred to as metal layers.
650820	656540	Having more metal layers allows for more complexity, but at the cost of current leakage and worse
656540	658540	reliability.
658540	662500	Contemporary DRAM processes use three to four metal layers.
662500	668140	Contemporary logic processes use anywhere from 7 to 12 and even more metal layers.
668140	673220	So it is estimated that if you tried to make logic circuits with a DRAM process, then the
673220	678460	logic circuits would be 80% bigger and perform 22% worse.
678460	683620	And if you were to try to make DRAM cells with a logic process, then you are essentially
683620	687180	making embedded DRAM, or eDRAM.
687220	694260	The cells use significantly more power, take up to 10 times more space, and are less reliable.
694260	699020	The industry has since considered these manufacturing shortcomings and have come up with a variety
699020	700500	of workarounds.
700500	707580	Many proposals of compute and memory are on three levels, the device, circuit, and system.
707580	712300	The device level leans on new types of memory hardware other than the conventional DRAM
712300	714060	and SRAM memories.
714060	721580	Notable examples include resistive random access memory, or rerAM, or rRAM, and spin
721580	728820	transfer torque magneto resistive random access memory, or STT-MRAM.
728820	732580	RerAM is one of the more promising emerging memory technologies.
732580	737580	Like I mentioned earlier, conventional RAM memories store information using a charge
737580	740420	stored within a capacitor.
740420	745740	RerAM instead stores information by changing the electrical resistance of a certain material,
745740	751380	referred to as a resistor, switching between a high and low resistance state.
751380	758140	This structure allows rerAM to compute logical functions directly within the memory cells.
758140	762740	Please don't ask me to try to explain it any further than that.
762740	767420	RerAM is probably the emerging memory technology that is closest to commercialization due to
767420	773580	it being compatible with silicon CMOS, however there remain substantial hurdles to overcome
773580	777740	before we see products arrive on shelves.
777740	782620	The circuit level is where we modify peripheral circuits to do the calculations right inside
782620	786820	the SRAM or DRAM memory arrays themselves.
786820	793300	The phrase I see a lot is in C2 computing, in C2 meaning locally or on site.
793300	797180	Memories are particularly clever, but they also require an intimate knowledge of how
797180	801980	memory works and still can be difficult to understand.
801980	807300	One prominent example of this is Ombit, an in-memory accelerator proposed by people from
807300	813300	Microsoft, NVIDIA, Intel, ETH, Zurich, and Carnegie Mellon.
813300	818420	A DRAM memory contains sub arrays with many rows of DRAM cells.
818420	822700	In normal use, the memory activates one row at a time.
822700	828380	This system activates three rows at a time in order to implement an AND slash OR logic
828380	833440	function, two rows for the inputs and one for the output.
833440	835540	The concept is logically attractive.
835540	839900	You can utilize the memory's internal bandwidth to do all the calculations.
839900	843220	However, there are significant concerns.
843220	850020	Ombit can perform basing AND or and NOT logic operations, but it takes multiple cycles.
850020	857100	Furthermore, more complex logic implementations like XNOR remain challenging to utilize.
857100	862180	So far, the big downside with these two compute and memory approaches is that their performance
862180	867460	still falls short of what can be achieved with current Von Neumann GPU slash ASIC-centric
867460	869020	systems.
869020	873740	In other words, they suffer the same drawbacks people saw in the 1990s.
873740	880180	Putting memory and logic together still makes a jack of all trades master of none situation.
880180	884460	So the middle ground that the industry seems to be moving towards is implementing compute
884460	886940	and memory at a system level.
886940	891660	This is where we integrate together discrete processing units and memory at a very close
891660	893580	level.
893580	900220	This is enabled thanks to new packaging technologies like 2.5D or 3D memory die stacking where we
900220	904700	stack a bunch of DRAM memory dies on top of a CPU die.
904700	909120	The memories are then connected to the CPU using thousands of channels called through
909120	912060	silicon vias or TSVs.
912060	915100	This gives you immense amounts of internal bandwidth.
915100	920580	AMD is working on something kind of like this with what they call 3DVcache, which is based
920580	924300	on a TSMC 3D stacking packaging technology.
924300	928460	They used it to add more memory cache to their processor chips.
928460	933020	There is a future where we can use similarly advanced packaging technologies to add hundreds
933020	936980	of gigabytes of memory to an AI ASIC.
936980	942460	This lets us integrate together world-class memory and logic dies closer than ever before
942460	946460	without needing to place them on the same die.
946460	948300	Ideas in the laboratory are cheap.
948300	953820	What is harder is to execute on those ideas in a way that performs well enough to replace
953820	956580	what is already out there on the market.
956580	964620	And the fact is that the current NVIDIA A100 and H100 AI GPUs are a very formidable competitor.
964620	968740	But with leading edge semiconductor technology slowing down the way they are, we need new
968740	975180	ways to leapfrog towards more powerful and robust hardware for running AI.
975180	977740	Generally speaking, bigger models perform better.
977740	983220	Today's best performing natural language processing and computer vision models are great, but they
983220	989180	still have ways to go, which means they might have to get bigger so to get better.
989180	995020	But unless we develop new systems and hardware that can overcome these aforementioned limits,
995020	999620	it seems that deep learning might fall short of fulfilling its great expectations.
999620	1002020	Alright everyone, that's it for tonight.
1002020	1003020	Thanks for watching.
1003020	1005620	Subscribe to the channel, sign up for the newsletter, and I'll see you guys next time.
