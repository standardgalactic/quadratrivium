start	end	text
0	4800	Hi, everyone. Welcome to Foresight's existential podcast. We're really delighted to have Stuart
4800	11440	Buck here today. We actually met in person at a recent EAG before. I've had heard of you research
11440	16560	before and just booked a call with you or an in-person meeting there. We stumbled over a bunch
16560	21600	of really interesting metascience projects and problems, challenges, and possible solutions,
21600	26320	and then dove into a little bit of perhaps the new emerging landscapes of interesting orgs.
26960	30560	I'll talk to you a little bit about that in the introduction. I'd love to hear from you on
30560	34800	that. I guess we were just ships passing the night itself by Southwest too bad,
34800	39920	but I decided to hopefully next time in person connect you. All right, so maybe just to get
39920	44720	us started, would you want to share a little bit more about good sciences really up to and if you
44720	49920	can also get your journey into the organization? I think that usually helps people like map the
49920	55200	genealogy of your work a bit. Sure. I should start by reminding about 12 years or so.
56080	60080	Around 2012, I went to go work for a place called the Laura and John Arnold Foundation.
60080	65520	It's this major billion-dollar plus philanthropy that focuses a lot on evidence-based policy,
65520	68960	and it's grown a lot since I joined. When I joined, it was pretty new. There were like
68960	73680	eight or 10 people. There are now it's over 100. The scope and the scale of it is really
73680	78000	growing, but right from the start, the Arnold's, who themselves are around 50 years old,
78000	83040	they had retired at 38 and were devoting their wealth to philanthropy. They were mostly interested
83040	87840	in just evidence-based policy across lots of areas like education and criminal justice and health.
89200	94080	One thing that we initially started noticing was the what you call the reproducibility crisis,
94080	98800	the problem of trying to replicate research. We first noticed it in psychology, but I started
98800	103120	digging into it as director of research there. It's a problem in a lot of fields,
103120	108160	including medicine and cancer biology and economics. Pretty much any field you dig into
108160	112000	that turns out there are some issues with replication, sometimes outright fraud.
112960	118320	Just the publication process is often biased towards exciting positive results, which is
118320	123520	natural. We all want to have exciting positive results come out of the scientific world, but
123520	128080	when there's a bias towards that, then people feel compelled to sometimes stretch the truth,
128080	132400	push the boundaries of acceptable practices. The Arnold's decided that if you want to pursue
132400	136320	evidence-based policy, it's really difficult to do that if you're not sure how much you can trust
136320	140800	the evidence, or if you think the evidence has been biased in a positive direction.
141760	147200	Their initial vision of philanthropy had been that they would look to education, for example,
147200	152320	and it would be fairly simple just to find what are the best ideas supported by the best evidence,
152320	156240	and then just write a big check to the best idea, and then it's very simple.
156240	159760	But it turns out it's much more complicated than that if you really start digging into the evidence
159760	165920	as to what works. I started a grant-making program there with the Arnold's money, of
165920	172960	course, focused on open science and reproducibility and trying to improve science. I handed out
172960	179280	probably $60-plus million over several years, and then in the process of that endeavor,
179280	183920	I ran across a guy named Patrick Collison who runs a company called Stripe. I ran into him
183920	188240	several years ago at a conference, and he was very interested in trying to improve science,
188240	192640	but not just reproducibility, but improving innovation, improving the pace of innovation,
192640	197200	the freedom that the best scientists have to explore the universe and explore their best
197200	203440	ideas without having to cater to what funders most desire. So anyway, I introduced him to John
203440	209600	Earl, and we continued conversations, and then a couple of years ago, actually, time has flown.
209600	213920	Actually, it's going on two and a half, almost three years ago. I had some further conversations
213920	218480	with Patrick Collison that led to him being the initial funder for what I'm doing on my own now,
218480	222960	which is a good science project. It's a small, I guess you could say, think tank focused on
222960	228080	trying to improve federal science funding and policy so that we have faster innovation and,
228080	233600	hopefully, more breakthroughs and clean up reproducibility as well. So that's the journey
233600	238320	as to how I got to where I am now. Robert, I think it's always really interesting to hear,
238320	242480	and the individuals involved and so forth, and somewhat of the serendipity in it. Okay,
242480	247120	that's wonderful. Maybe let's dive into a few of the topics that you actually focus on to give
247120	251520	people a little bit of a taste of what you guys are working on. I know that you've published a
251520	255920	lot on Substack, but in other outlets, too, and you sent me a few really interesting docs,
255920	260560	so I just want to jump around here a bit, if you don't mind. And one that I thought was really
260560	264480	interesting and is, of course, has become, I think, a pretty prominent field recently,
264480	268880	is the field of meta science progress, and it's not really necessarily involved with individual
268880	273200	scientific fields. For example, Forsyte was a part specific researchers working on one technology,
273200	276480	but it's really also looking at a broader, with a broader lens of what could be improved
276480	280080	in the ecosystem. And you've written some really interesting stuff there. And so I'd just love
280080	284080	to know, including, for example, how much progress we've made in meta science, but where we could
284080	289600	still speed up progress. And I'd love to get your thoughts on the broader, if you think about
289600	293520	science from this meta lens, like you mentioned reproducibility as one, but what are a few of
293520	297200	the different areas that you think are really holding scientists back right now at producing
297200	300720	the research that we would all want from them? And then perhaps a few recommendations that you
300720	305920	have here. Sure, that's really broad. So I'll just pick one issue that I care about a lot. And
306000	310720	a lot of folks have focused on, and it's really tough to crack down. It's the issue of bureaucracy.
310720	315440	Everyone hates bureaucracy in the abstract. But the problem is that everyone loves bureaucracy
315440	321120	when you point to any particular feature of it. So to take us back, there have been multiple
321120	326560	surveys of federally funded scientists over the past couple of decades. And the most recent survey
326560	331600	surveyed thousands of federally funded scientists. And they said, on average, that they were spending
331600	338480	44% of their time on bureaucracy, basically filing reports and budgets and proposals and
338480	344080	just all the machinery that comes with getting a federal grant. And so everyone points to that
344080	348480	and says, that's a huge problem. We're scientists spending nearly half their time on bureaucracy.
348480	354480	That seems like we're paying people to dig a hole and fill it back in. And as you say,
354480	359200	that's an average. There are some scientists who are fortunate enough to have great administrative
359200	364240	help. And so they don't have personally have to spend as much on the other extreme. I talked to
364240	369920	one scientist at the University of North Carolina, who said that he's probably spent 70% of his time
369920	374960	on bureaucracy, because he said he does animal experiments. And he said that quite frankly,
374960	380080	his administrative help in the department wasn't very good. And so he had to do all the ethics
380080	385360	paperwork himself. And so he felt like his direct quote from him was, I just don't feel like there's
385360	389200	time to do science anymore. And so that seems quite paradoxical. What are we paying people to do
389200	393920	just to fill out reports about the money that we handed them? It just makes no sense. And it's
393920	398400	depressing. No one goes into science thinking they're going to spend 70% of their time filling out
398400	403680	reports and filling out paperwork and et cetera, right? They go into science because they love
403680	408160	a particular field and they want to learn more and they want to make discoveries and so forth.
408160	413120	And it just drains all the excitement out of science. But here's the problem. Every bureaucratic
413200	418480	requirement has some justification for it. There are ethics requirements as to animal
418480	421920	experiments. And those are there for good reasons, because animals can be abused and
421920	427600	can suffer horrifically in experimentation. We've developed a whole set of procedures to protect
427600	434160	animal safety and to protect against unnecessary deaths of animals and so forth.
434160	439920	The same goes for experiments involving human beings. And that's again, thanks to a kind of
439920	446640	horrific history of experimentation done in the 20th century on unsuspecting human subjects that
446640	450880	were mistreated. And so there's a whole set of ethical requirements there so that nobody wants
450880	455920	to get rid of that. It is federal money that's being spent. And so there's going to be some oversight
455920	460560	like the budget and how the money is spent and so forth. Right now, there's a lot of focus on
460560	467920	international security and focus on our researchers unwittingly passing the top technological secrets
467920	472320	to researchers in China. And there's probably that new focus on China and maybe some discrimination
472320	477680	involved. But yeah, it's still a fair consideration. But how much should we fund research that might
477680	482800	unwittingly be used to support a foreign adversary, let's say. So anyway, like any bureaucratic
482800	486480	requirement you point to, someone somewhere is going to say, there's a good reason for that,
486480	490720	right? We need to keep that one. So it's really hard. And the whole is like, that's
490720	495360	by a thousand cuts. But if you point to any one specific bureaucratic requirement, again,
496000	498960	there was a good reason for it. There was some scandal. There was some
498960	502480	problem that someone was trying to solve with this rule, with this procedure.
503200	508320	And so that's why there have been many efforts to get rid of or to try to limit bureaucracy,
508320	512480	but they haven't really gone anywhere. Because what you really need is to have some person,
513040	518880	like with almost, I hate to use this word, but almost dictatorial authority over an agency like
518960	525280	NIH or over an agency like NSF, to just go through the entire bureaucracy and take a red
525280	529840	pen and slash through the stuff that isn't necessary or that isn't the top priority.
529840	534560	And with the objective of, let's say, reducing the burden on scientists time to 20%, let's say,
534560	540000	rather than 44%. And you'd have to have someone who is willing to make some really difficult
540000	545120	tradeoffs and difficult choices and prioritize. We have a thousand things that everyone wants
545120	548720	researchers to do. They'd take up too much of their time. So you're going to have to slash
548720	552400	through some of them that even though they individually, they might sound like a good idea
552400	558880	because you just have to prioritize. So you need someone or some committee that has the power and
558880	566160	the will to actually get rid of some rules and regulations that maybe seem like a good idea.
566160	569520	And that's politically difficult. But I think it's still worth trying because otherwise we're
569520	574880	in a trajectory where ultimately we'll just be paying for 50%, 60% of science at this time.
576160	580240	And it's insanity. Scientists need to have more time to focus on their science.
580800	584320	So that's one of the issues that I've written. But I could dive into many more.
585040	588960	Just to attach on this for a little bit longer, who would be an org that could do this?
588960	592320	Or do you think it would be an individual at each scientific organization? Or could that be
592320	596960	something for the GOA? Or would that be something like scientists writing an open letter about
596960	601280	specific things that they get particularly up about in their research? In terms of thinking about
601280	607680	solutions, if it's not as easy as a science gone coming in and doing it, are there any
607680	614000	pathways that you think are worthwhile exploring? Another challenge is that these rules and regulations
614000	619200	arise from different places. They arise from to dig into the weeds of the American government,
619200	626000	like the Office of Management and Budget, or OMB, that has federal-wide authority to regulate how
626000	630160	federal monies are spent and accounted for. And so they have a lot of budgetary requirements
630160	634400	amongst others. So that's one source. But that's under the control of the White House.
634400	638320	And so the White House could do something about that. Some of the requirements come from agencies
638320	644240	like NIH or NSF themselves. Some requirements come directly from Congress. So Congress mandates
644240	649520	particular practices and says that agencies need to look into X, Y, or Z. Some requirements
649840	655680	honestly come from universities that want to behave conservatively, so to speak. They're
655680	660640	risk averse and they want to make sure that they cross every T and dot every I. And some universities
660640	665920	perhaps over-regulate their own researchers or over-stats their own departments that are in
665920	671920	charge of monitoring and evaluating and submitting budgets and all of that. So yeah, it comes from
671920	676640	many different places. So that's one challenge. I do think the White House could, in theory,
676640	681920	issue an executive order that asked the Office of Management and Budget to review all of its
681920	687520	practices and its rules with the guide towards slashing stuff that hasn't a time requirement
687520	692640	on researchers or the impact on researchers directly. The White House could also review
692640	696960	its past executive orders because some executive... So here's an example. Some of these requirements
696960	700480	come from the White House itself, from prior executive orders. So there was an executive order
700480	705680	in the 1990s signed by President Clinton that says that if you get federal money, you need to
705680	711280	certify that you make people wear seatbelts. And again, it's well-motivated. There was nobody's
711280	716480	really against seatbelts and it's a good idea to probably save some lives perhaps. But some 20,
717120	723120	30, almost 30 years later, with their seatbelt laws in various states and probably anyone who
723120	727200	wears it wants to wear a seatbelt or he does wear a seatbelt. And it's done clear that making
727200	733040	people check that box and on federal applications really does need good. And so you could go back
733040	736800	and look at it through prior executive orders like that and say, look, here's some executive orders
736800	742560	that may have been a nice idea at the time, but we don't need to make everybody at every university
742560	746240	certify that they do everything with a good idea in the world. We can prioritize and say, look,
746240	750880	the time that passed when we needed to investigate whether people use seatbelts. And so there are
750880	755280	probably any number of requirements like that that again, individually, sure, that's fine to make
755280	760400	people wear seatbelts and it's not that much time to check a box. But just in terms of priorities,
760400	763600	we should be able to streamline that. So the White House could, as I say,
763600	768080	go back and look at the requirements that it has come up with over decades and see where it does
768080	772000	streamline. So yeah, I think there are some opportunities that the White House could take
772000	776400	advantage of if they want to. Okay, if anyone relevant is hearing, that feels a little bit
776400	780880	about the action. I also guess like the problem here is almost just getting worse over time just
780880	785040	because like people rarely ever take things out, but they just add to the pile. Like you never
785040	790000	really know how large the pile in total becomes once you start adding stuff to it.
790000	793040	But your individual thing that you want to add to it is really important right now
793040	797360	without considering the entirety of it. So it's definitely, hopefully it's not getting much worse
797360	802880	on the long run. But yeah, okay, that's definitely really important one that I couldn't agree more.
802880	807120	Like riding grants is already complicated enough. And if that is a big one, that's that that could
807120	811040	be an easy one to perhaps cut down on. The other thing that you've also written about really
811040	815840	interestingly is I think on like patents in general. And I think the sub seg post was actually
815840	819680	titled like how we screwing over researchers or something like that. And it had a lead at least
819680	823840	a bit like patent component in there. And that's I guess university targeted to some extent also.
823840	827280	So perhaps you could share a little bit more about what you were addressing there and perhaps even
827280	832640	tell the quick story of Catalina Carrico, if you feel like it. Sure. Yeah. Yeah. I admittedly
832640	837360	chose a kind of provocative title for that article set to hopefully get people to read at least a
837360	843360	little bit to rewind a little bit. There was this famous act in 1980, the Bidol Act that was passed
843360	849440	in the United States. Prior to that, it's complicated. But basically, when NIH NSF, when the federal
849440	854800	government funded research, it would often end up taking control the government itself would end
854800	859440	up taking control of patents arising from that research. And there was this perception that
859440	864320	the government is not the most efficient user of patents. It doesn't know what to do with them.
864400	868720	They weren't being actually used very well or commercialized or turned into
868720	873440	something that was useful for the market, useful for medical patients and so forth.
874080	877680	There's this idea that instead of having the government take control of patents,
877680	881440	let's shift that and have universities take control of patents because universities
881440	886880	are technically the recipients of all the federal grants that come from NIH and NSF, etc.
886880	890640	So the money doesn't go straight to it. We talk about a researcher getting grant, but it doesn't.
891280	895040	The money goes straight to the researcher's pocket, right? It goes to the university. They're
895040	899040	the ones who handle the money, and they pay the researcher, right? So universities are the...
899040	906320	They have a lot of indirect costs. That's a whole separate issue. But yeah, the indirect costs at
906320	912080	top universities are often between 60 and 70%. So what that... And just to clarify what that means,
912080	916320	so if you get a... If you as a researcher get a grant and let's say it's $100 to be simple,
917280	921920	like NIH would give $100 designated for you, the researcher, and they would add
921920	926800	60% on top of that, $60, let's say, as indirect costs to the university. So the total grant would
926800	933360	have to be 160. So the 60% is on top of rather than taken out of. So it's not like... So anyways,
933360	937360	that's just how that works. And by the way, indirect costs are also supporting a lot of
937360	942880	bureaucracy that universities administer. So the bureaucracy issue is tied in to indirect costs.
942880	948560	So anyways, university started patenting. And the story since then has been like, this is a great
948560	953680	success. So until 1980, you had all these patents that were either didn't exist or went unused.
953680	958000	And then afterwards, you have this huge flourishing university-based patents.
958560	962640	And so around 20 years ago, there were a bunch of European countries that also started moving
962640	968000	in that direction and trying to give universities more control over patents. But here's the thing,
968160	974080	in some of those universities, in some of those European countries, the prior rule had not been
974080	977520	that the government controlled the patents. The prior rule had been that the professor or the
977520	981520	researcher controlled the patent. And in fact, they call it professor's privilege in some of
981520	986960	those countries. And so they were switching from in a different direction, right? They were switching
986960	992880	to the US regime that was perceived as successful. But they were switching from completely different
992960	998080	place where the professor or the researcher had more control. And so there's been some empirical
998080	1003440	research on that that has shown that when European countries moved in that direction,
1003440	1008720	the rate of patenting actually went down, which actually doesn't seem too surprising because
1008720	1015280	universities often are very diffused. They encompass many departments. They may not have
1015280	1020160	anyone who's a specialist in what one professor is doing and the commercialization of that
1020160	1026480	research. And so maybe they put less priority overall on trying to commercialize anyone given
1026480	1031600	discovery or possible patent than the researcher themselves who has more skin in the game, so
1031600	1035760	to speak. So that's where the empirical evidence seems to lie so far is that it would be better
1035760	1039600	to give better for innovation, better for patenting, better for commercialization,
1039600	1043920	to get professors more of a say and perhaps to control over the patents rather than the
1043920	1047920	universities themselves. Now, this came to a head with Katalin Kiriko, which is the story that I
1047920	1053200	talked about quite a bit. So she was a Hungarian researcher who came over to the U.S. and worked
1053200	1059840	at the University of Pennsylvania and got demoted repeatedly at Pennsylvania because she couldn't
1059840	1066320	get NIH grants to support the work that she was doing on early mRNA research, which at the time
1066320	1070720	it was not very popular. They weren't further of it now because it turned out to be the basis
1070720	1075760	of some COVID vaccines. But in the 90s, it wasn't very popular at all. And it was seen as the dead
1075760	1079840	end for whatever or something that's extremely difficult. It would never work. So she couldn't
1079840	1084800	get grants to support the work. And the NAS, by the way, opens up a whole other huge topic,
1084800	1090720	which is the role of NIH money and so-called soft money in universities, soft money, meaning
1090720	1094560	researchers like Kiriko who were expected to pay for their own salary. It's like they're
1095680	1100000	they're not given a salary directly by the university or not 100% of their salary. They're
1100000	1104640	expected to raise their own salary for themselves to their grants. And so that makes them very
1104640	1109680	dependent on appealing to whatever NIH wants to fund at any given point in time.
1109680	1114080	So Karen Kiriko was repeatedly demoted and eventually basically pushed out of academia
1114080	1120880	even after what became a paper that later won her and her co-author the Nobel Prize.
1120880	1124880	Now, of course, no one knew that at the time. Like Ben, the University of Pennsylvania couldn't
1124880	1129520	foresee the future. But the University of Pennsylvania did, as I found from reading some
1129520	1134640	student newspapers from Pennsylvania, they kept the patents on for work, even after pushing her
1134640	1140880	out of academia. And the student newspaper produced a chart like the universities across
1140880	1144960	the country and how much money they're making royalties from patents. And the University of
1144960	1150960	Pennsylvania was far and away, making many times more money than Stanford or other universities
1150960	1156160	that you might think of as hotbeds of discovery and technological advancement.
1156160	1161440	And so Pennsylvania made something like over a billion dollars in one recent year from the
1161440	1165920	Fed with vaccines and from the patents on apparently from the patents on Kiriko's research,
1165920	1169840	even though she's long gone from Penn and they not only didn't help with her research,
1169840	1173120	they drove her out of academia. So things like this kind of glaring
1173760	1179200	unfairness and on top of the inefficiency process. So yeah, I think that's an area that yeah, I
1179200	1183760	think definitely deserves some reform or at least some really detailed, I think Congress
1183760	1188720	definitely should fund or require some really detailed investigations of what is even happening
1188720	1192800	at these university offices that are supposed to be patenting, researching, commercializing it.
1192800	1198640	How many patents go unrealized or uncommercialized? How many take too long? There are lots of
1198640	1203120	anecdotal complaints at certain universities that the process takes too long and the university
1203120	1209680	demands too much of a cut. I think it'd be better to have a more systematic kind of investigation
1209680	1214400	to add to the anecdotal stories. But in any event, I think I do think that the story from
1214400	1218720	Europe or the empirical evidence from Europe shows that it might be better to move back in the
1218720	1223360	direction of giving the professor or the researcher more of a say in what happens to their own
1223360	1226720	discoveries. Yeah, it's crazy when you think about it, it's almost like all the incentives
1226720	1232640	that are wrong. It's a few, right? But like, why even, how to come up with that type of system
1232640	1237920	in the first place, I think. But yeah, I think I love that there is this almost not really an
1237920	1242640	A-B testing, but at least some precedent of how it used to possibly work better and that that was
1242640	1247760	useful to go back into that. You already mentioned one of the bits on funding and with the kind of
1247760	1253120	like soft support from the NIH. And so I think another really interesting, super detailed analysis
1253120	1258560	that you've done is specifically on NIH reform. You list a full laundry list there of things
1258560	1262880	that could be improved with the NIH. And I don't think we get through all of them. I think accounting
1263840	1268640	is almost 10. And we already touched on them individually. But when you think about the
1268640	1273920	the NIH, what are like a few kind of crucial areas that you yourself are perhaps really excited about
1273920	1278480	and promoting that there could be possibly a good reform applied to the NIH?
1279680	1284480	Sure. I think there are any number of ideas, as you say. I think one thing the NIH should
1284480	1290160	consider using more is an approach called basically fund the person, not the project.
1290800	1295840	Now, it's interesting. There is a program at one of the NIH institutes called the National
1295840	1300560	Institute for General Medical Sciences, NIGMS. And if you're wondering what that is,
1300560	1304400	because many folks might, I did when I first started it. It's basically the NIH Institute
1304400	1310880	that funds basic research as opposed to National Cancer Institute, focuses on cancer, the National
1310880	1316400	Heart, Lung and Blood Institute that focuses on the cardiovascular disease, etc. NIGMS is focused
1316400	1323280	on truly basic science that's not necessarily connected to any one specific disease like some
1323280	1329680	of the other NIH institutes are. So NIGMS has this program called Maximizing Investigators
1329680	1337280	Research Awards, M-I-R-A, and they pronounce it Mira. And this program is really intended to give
1337280	1341920	researchers more flexibility and freedom to give them funding for several years where they don't
1341920	1347360	have to spend as much time trying to pre-specify everything that they're going to do and then
1347360	1351360	report back on what they said they were going to do three years ago or four years ago,
1351360	1357040	instead it's intended to give them more freedom to follow the science and to follow their nose,
1357040	1363280	so to speak, as to what the best ideas are at any given point. And I think that there's some
1363280	1369840	evidence that the papers produced by that are performing equally as well or better in terms
1369840	1375040	of citations. That's only one very limited metric, I think longer term you would want to see a rate of
1375680	1379920	breakthrough discoveries. And that's hard to see because you can't expect a breakthrough every
1379920	1386000	day from everyone. That's impossible. And as the director of NIGMS, John Lorsch has said,
1386000	1389600	if I knew where the next breakthrough was going to come from, I would have already made that
1389600	1395040	discovery myself. So part of his idea of funding is that you want to spread the money widely
1395040	1399280	amongst talented scientists and give them the freedom and who knows where the next breakthrough
1399360	1404000	will come from. Especially with basic science, often there's any number of stories from basic
1404000	1408560	science where a discovery will be made in one decade and then three decades later it turns out
1408560	1414240	that it's amazingly useful or influential. But yeah, I think that's a good example of a program
1414240	1418720	that's experimental at NIH in the sense that it's a new thing. It's still fairly new. It's been
1418720	1424800	around for a few years. I think that program could be expanded to other institutes at NIH like
1424800	1428960	National Cancer Institute. And there are other institutes that have a version of this, but it's
1428960	1435840	much smaller. NIGMS funds this type of grant four times as much when I last reviewed the numbers.
1435840	1440720	Four times as much as the rest of NIH put together. Huge imbalance. NIGMS is very much focused on
1440720	1446320	this for basic science, but I think you could try that approach elsewhere at NIH. And again,
1446320	1451040	with the idea of giving scientists more flexibility and freedom. And here's another key idea that
1451040	1456080	was in the document I sent you, which is NIH should take a more deliberate approach to
1456080	1461040	experimenting and learning from what it does. Take a very meta science approach instead of just
1461040	1464800	starting up new programs and saying, all right, everybody's going to do this. Then you don't
1464800	1470160	have as much of a chance to learn and to iterate and to introduce deliberate experimentation.
1470160	1475680	NIH is big enough that you could do like the literal randomized experiments and how you hand
1475680	1482560	out money. They'd fund something like 90,000 grants at any given point in time. There's 90,000
1482560	1488000	grants and each grant often supports multiple researchers. So I'm not sure the exact total,
1488640	1493680	but it's hundreds of thousands of researchers that are supported by NIH. That's plenty of
1493680	1498160	opportunity. So you could do a randomized trial that involved a thousand researchers,
1498160	1504000	and that would be a drop in the bucket from what NIH supports. And you could randomize 500 to be
1504080	1510400	funded in one way, 500 to be funded in a different way, and they just fold over time and see what
1510400	1515840	happens. See what are the results from funding that offers more flexibility and freedom versus the
1515840	1520240	more usual way that NIH does things. And I think that kind of deliberate experimentation
1520240	1525280	is something that NIH should do at all. They've really ripped on that.
1525280	1529440	Yeah, I think on your last point, it's really interesting just to hop back on the kind of
1529440	1533920	funding people, not projects that is somewhat also pretty present right now, I guess in the
1533920	1537520	private space where I think, for example, the Lieberman Brothers, they're now launching a new
1537520	1541840	effort to actually fund individuals early on before basically making bets early on and almost
1541840	1546080	invest into individuals like you invest in companies. And I think that kind of at least that
1546080	1552400	meme or that new approach should also hopefully extrapolate outward through different and perhaps
1552400	1555680	even scientific funding organizations. I think that would be wonderful. I think.
1556240	1559840	I'd like the parallel to venture capital because I mean, I think there are a lot of,
1559840	1565360	any number of, again, statements or examples of venture capitalists who say, yeah, you're making
1565360	1571200	a bet on the person. There are many examples where a founder of a company ends up pivoting and doing
1571200	1576400	something slightly different or a lot different. And venture capitalists often are happy with that
1576400	1579920	because you're trying something and you realize it didn't work and now you found something that
1579920	1586480	did work. And if you would be unthinkable to go to, I don't know, to go to Mark Zuckerberg and say,
1586480	1590240	wait a minute, like your original proposal said that you were going to do X, Y, and Z and said
1590240	1595920	that you were going to spend $5,000 in year three on this line item and where it shows that you
1595920	1600560	spent the money that way. You would never want that level of micro management of a talented
1600560	1605200	entrepreneur. Okay, students want to give them a little more freedom and flexibility to adapt to
1605200	1610320	the market in which we should treat a lot more scientists the same way that you should give
1610320	1616800	them the same respect and autonomy that entrepreneurs routinely have and give them the freedom of
1616800	1622400	flexibility to say, wait a minute, I said I was going to do X, Y, and Z, but I tried it and it
1622400	1627440	didn't work and I came up with a better idea the next year and now I'm going to want to do the better
1627440	1632080	idea. Of course, we should all follow the better idea when that comes up. Yeah, I guess. Otherwise,
1632160	1635280	you just assume that people aren't learning as they're actually in the field and then
1635280	1639200	fermenting and dropping things. It's a crazy assumption to make in the first place, I think.
1640560	1645520	The process should be about learning and changing and adapting to new ideas of the MLA. That's
1645520	1649200	a bold point. If you already knew what you were going to do five years from now,
1649680	1655280	I think it's, I'm paraphrasing, but well, one scientist that I know at Pennsylvania as well,
1655280	1659040	he said, if I'm doing what I said I was going to do five years ago, I should be fired because I
1659040	1664000	should have warranted and adapted in that book. Yeah, I love it. We only got to a small part of
1664000	1667600	the laundry list that you have for the NIH, including other misopryptonies and stuff, but I
1667600	1671760	think I want to close at least my part of the podcast with a question perhaps a little bit on
1671760	1677040	a hopeful note to already lean into the extension whole part of the podcast after, but we have
1677040	1681520	briefly touched on when we met and you've also pointed out again afterwards that there's actually
1681520	1688160	now a really cool new landscape emerging of these alternative possible homes for researchers and
1688160	1693360	scientists. I think there's FROs, there's Future House, Park Institute, Astera, Spectac, there's
1693360	1698160	a few really interesting new orgs that have been popping up and we certainly had a few founders
1698160	1702240	and executive directors of these orgs already on here for podcasts and for individual topics,
1702240	1706240	but could you share a little bit of how you see that landscape changing and what these new organizations
1706240	1710160	are setting out to do and perhaps what we can hope for in the minutes? We don't have to cover all
1710160	1715040	of them, but just the general spirit, a great idea. And you could add to that some, there's
1715040	1720640	some government agency, surprisingly enough. There's a health version of DARPA, so it's called
1720640	1725280	ARPA-H, which I'm sure any listeners are familiar with, but yeah, it's intended to be like a
1726160	1731120	government agency that has the innovative approach that DARPA and it's taken for decades.
1731120	1735360	And there's a version of that in the UK as well called ARIA, also trying to be like the
1736320	1739920	imitator of DARPA in a way. I think that's all tremendously hopeful. I do think it's
1739920	1744640	hardly borne out of discontent with the current system, but as I say, it tends to be
1745520	1750720	very bureaucratic and very top-heavy and a system in which it's hard to get a
1750720	1756480	foothold, the average age for NIH, the first major NIH grant is so we're making people like
1756480	1762960	slave away in other people's labs for 15 years before they finally get a foothold as a researcher.
1762960	1766880	And so they're long past the age, which a lot of people in previous generations did some of
1766880	1770720	their best scientific work. Einstein made some of his greatest discoveries at age 25, and so
1770720	1774480	I think Newton, James Watson, there's any number of examples of that kind of thing.
1775600	1780000	So there's a lot of discontent with that system. And so there are people within government and
1780000	1784960	private philanthropists who say this is an opportunity to diversify the landscape,
1784960	1790080	to come up with new ways, hopefully better ways of doing science and of funding science and of
1790080	1796640	organizing science, all very important meta science issues. And so one thing that I really
1796640	1802480	hope we're able to do, we being just collective meta science community and policymakers over the
1802480	1809120	next five to 10 years, is just really deliberately learn from what all these new organizations
1809120	1815680	have produced and do it in a systematic way. I do think possibly one risk, like any new
1815680	1821120	organizations, like just the same as universities, their temptation is going to be to send out
1821120	1826240	press releases about all the amazing things they've done and brag about that. And that's fine,
1826240	1830880	that's to be expected, and often that's even true. But there will be cases in which they fail,
1830880	1835360	and I think it's important to learn from failure, and it's important to be public about that.
1835360	1840880	And so I think hopefully over time, we can have a meta science discussion that's able to have
1840880	1846800	just a truly honest appraisal of what's working, what has failed, and failure isn't a bad thing.
1846800	1850160	We should learn from not everything that's going to work for the first try. Let's learn
1850160	1855760	about how to design organizations more efficiently or with more perfectly going forwards.
1855840	1859680	And so I think that's where I hope the conversation goes for the next five to 10 years.
1859680	1865440	Yeah, thank you for that. So yeah, so this part is more like the philosophical part. We've been
1865440	1870640	talking about the meta science and everything, but it's diving into the sort of existential hope
1870640	1875200	aspects of where science and the progress of it can take us. You touched on it now,
1875200	1878800	but I would be really curious to hear, do you feel like things are changing? Are you gaining
1878800	1884400	traction with this? Yeah, I do think things are changing. Again, we just touched on that with all
1884400	1891200	the proliferation of new organizations, both inside government and outside government. I think
1891200	1900000	that's a really helpful sign. And again, my hope is that over time, there should really be a diversity
1900000	1905520	of approaches and organizations that probably something that will be bad for science is to
1905520	1913360	say everybody has to fit in the exact same box. That probably isn't good, even if the box makes
1913360	1918960	sense, so to speak. So we talked about fund the person, not the project. That seems like a good
1918960	1924960	idea. And we should use it sometimes. But I think if literally 100% of science funding was fund the
1924960	1928880	person, not the project, there probably be some failure points there as well. There probably be
1928880	1934080	some things that got missed. There are probably some examples where you should fund an organization,
1934080	1939120	like fund a team, not the person. So there are lots of different approaches one could use.
1939200	1943920	And in fact, in some areas of science, the Large Shader and Collider or big astrophysics
1943920	1948640	efforts, you're not funding one person, you're funding like a team of 1000 people. You know what
1948640	1953200	I mean? Yeah, if you want to approach this to science and to science funding, I think would
1953200	1959760	probably be suboptimal. But I do think that hopefully going forward, I think one thing that
1959760	1966560	could help improve the pace of innovation is having a more deliberately diverse approach
1966560	1971440	in like how we fund science and the source of people to get funded, and so forth. And
1971440	1975680	when I say the source of people to get funded, that's also an interesting point to emphasize.
1976320	1980720	There are lots and lots of examples from history where great scientific advances come from places
1980720	1985040	that sometimes you wouldn't necessarily have expected or they come from people that were
1985600	1990960	heretics at their time, Semmelweis and the germ theory of disease, like people despise them at
1990960	1995680	the time. There are tons of examples like that. It doesn't mean we should, again, we don't want
1995680	1999840	a science funding system that only funds people that are outcasts and heretics. That probably would
1999840	2004080	be in the wrong direction too. But I do think there should be some space within science,
2004080	2009440	like a National Institute for Oddball. We should have a deliberate approach to fund some things
2009440	2014800	that are outside the box. And now some of them will be crazy and won't work. We might end up
2014800	2018720	funding some of the greatest breakthroughs ever if we've made more space within the scientific
2018720	2023040	funding system or people with truly outside the box ideas and approaches that don't get funded
2023280	2030720	correctly. Yeah, I remember I was talking to an economist, I think, Johan Nurebe,
2031280	2037360	about he was talking about how technology often progresses. It comes from quite dirty areas or
2037360	2042880	not necessarily the most appreciated ones. I think his example was the technology of online
2042880	2048000	payment solutions coming from porn, like people wanting to watch porn online and that was like
2048000	2053360	what are online payment solutions? And now that's a very useful and established thing.
2053360	2057920	And I'm sure there are more exciting scientific innovations that came from the not the most
2057920	2065040	appreciated areas maybe. Yeah, but it sounds like you're positive and you're optimistic.
2065040	2067360	Would you say that you're optimistic about the future?
2068480	2073920	That is a big question. In general, yes, I think that there's lots of possibilities. There's
2073920	2078240	dangers as well. We've both kind of, I think I've met you in an effective algorithm conference.
2078240	2085360	There are certainly areas like nuclear proliferation or biosecurity or artificial
2085360	2092480	general intelligence. There are some areas of potential R&D advancement that do possibly have
2092480	2097600	some risk and some would say existential risk. But I think the theme of your podcast existential
2097920	2104240	right is that hopefully with the broad sweep of innovation and improvement that we can address
2104240	2109280	those existential risks and that we can hopefully still keep progressing and making life better
2109280	2114400	for everyone. And so I think that's where I guess my hopes and efforts would lie is like trying to
2114400	2120320	figure out what we're doing wrong, all the ways in which we're holding back scientists and science
2120320	2127040	itself and figuring out ways to hopefully speed up and accelerate the pace of innovation. I think
2127040	2134000	that does offer more hope for the future. Yeah. I'd be interested to hear in relation to the
2134000	2139280	like increasing pace of progress on like in relation to maybe AI in specific,
2139280	2142640	do you see the science landscape shifting due to that?
2144720	2153600	Due to AI in general, I haven't looked at it that much in depth. There are some really amazing
2153600	2157440	advances that have been made, for example, AlphaFold and protein folding. There are other
2157440	2163520	similar tools that are like that that offer the potential to speed up at least some components
2163520	2170640	of science. I'm less certain myself with what I've seen out of large language models so far. It
2170640	2175840	seems like they have, they sometimes show science a great sophistication, but then sometimes they
2176480	2181680	like just completely hallucinate, at least the ones to date. And so I'm a little nervous about
2181760	2185840	that. You could end up with kind of pollution of the scientific literature with people using
2185840	2190720	AI models to write papers thinking this will help them publish more and then it'll end up on
2190720	2196960	live somewhere and might be largely fake or largely just made up or hallucinated. Another
2196960	2201840	thing that I worry about, and this is born out of the work that I did while I was at the Arnold
2201840	2208080	Foundation, a lot of the published literature just isn't that good. Some of it's outright fraudulent
2208880	2213120	and there are more discoveries about academic broad that come out, it seems, every week.
2213120	2217760	A lot of it isn't that reproducible. And even the stuff that is reproducible, it often isn't
2217760	2222960	described that well in print. So one thing that I funded, for example, was called the
2222960	2228320	Reproducibility Project in Cancer Biology. And their original idea was to replicate
2228320	2234240	the experiments from 50 top-sided cancer biology papers. And ultimately, they could only complete
2234320	2240080	fewer than half of their intended experiments. And the reason was that in every single case,
2240080	2244320	you couldn't just go off the publish paper. You had to go back to the original lab and ask them
2244320	2248560	to fill in all the gaps and fill in all the details about what they had actually done.
2248560	2253040	And some of the original labs were not cooperative at all. In some cases, just were
2253040	2259440	hostile. Some cases just said, we're not sure. And when they worked for operative,
2259440	2264800	the answer was always that you need twice as much materials as you expected. And so it's
2264800	2269040	going to cost more and take longer. And they're like, worry about AI models that might be
2269680	2276960	trained on scientific literature as if what's written down on paper is the complete entry.
2276960	2282240	And that's often not the case. And you just worry that might spiral into even more
2282240	2287920	irreproducible work. So again, cautiously optimistic about some of the specialized tools
2287920	2294720	that are out there that are based on like really rigorous systematic databases. But then
2294720	2298560	there's a whole wealth or a whole broad millions and millions of scientific articles
2299200	2305200	that I would say probably aren't worth trying to use in an AI model in the first place or at
2305200	2309120	least with great skepticism and a lot less of corrections needed before you just
2309920	2313760	train an AI model and expect to get useful information out of it. So it's all over the
2313760	2318400	place. But yes, that's my answer for now. Yeah, that's very interesting. I hadn't really thought
2318400	2324720	about that in relation to that it would get access to like maybe incorrect data technically.
2324720	2328880	But yeah, I guess that I heard about the Center for Open Science and that's like
2329440	2334640	where they try to pre-register the studies that they're doing so that you can actually check if
2334640	2340800	they are correct in that they're reproducible or that that oftentimes you don't really publish
2341760	2345520	what's maybe not like an exciting result or something like that. But here you can see what
2345520	2350880	people have published or have been doing research on even if the results aren't like that exciting.
2350880	2356960	Any other like projects like that that you think are promising or seem like they could
2356960	2361920	actually make a difference in this field? Yeah, there's a lot of directions I could take this. So
2362960	2367440	the idea of pre-registration really came out of medicine from some recommendations that were
2367440	2372080	made. And I think it was early the late 1980s or definitely by the early 90s, there were some
2372080	2376640	folks writing about it, the idea in medicine. These folks in medicine, they were trying to do
2376640	2381760	meta research in medicine to try to summarize the whole body of literature. And as of the 90s,
2382800	2386880	probably still true somewhat today. They were very frustrated. And I remember an article
2386880	2391440	by a couple of folks at the time where they said that it's just really strange to them that you
2391440	2397040	can find much more information on baseball statistics and up to the minute information
2397040	2401120	about every baseball team, every baseball player, everything they've ever done. But yes,
2401120	2405200	it's impossible to find that information about clinical trials, that much wealth of information
2405200	2410960	about clinical trials in medicine, which is to them more important than baseball. So pre-registration
2410960	2416080	was in part a way to get access to information about the trials that are being conducted
2416960	2423280	on human beings and involving drugs or other types of treatments that might be used in medicine.
2423280	2428640	That was one motivation for me to still be able to see the kind of see the whole denominator,
2428640	2432560	so to speak. It's easy to see the published successes and the press releases of announcing
2432560	2438400	that a drug will cure in the US, the commercial that say try this drug. But it's harder to find
2438400	2443280	the failure since the idea of pre-registration in part was intended to address that. And then
2443280	2448720	like subsequently, I guess you're able to like what there was one famous article by kind of
2448720	2454960	Eric Turner and some colleagues where they reviewed something like 70 or 80 clinical trials
2454960	2460320	that have been done on antidepressants. And I'm going off a memory here, so I might get a number
2460320	2464880	slightly wrong, but roughly half of the trials showed that the antidepressants work and roughly
2464880	2469920	half of the trials showed no results. The antidepressant wasn't any better than placebo.
2469920	2475120	And they had access to all this because of both pre-registration and because they went to the FDA
2475120	2479440	and these were FDA-free drugs and the FDA demands to see all the evidence and not just
2479440	2483360	what got in the top journal. So you have the whole denominator there to look at.
2483360	2488000	And so what they found was that basically all of the positive trials, I think except one,
2488000	2494240	got published in a top journal. And the negative trials or the null trials mostly went unpublished,
2494240	2500000	but I think a few of them were published and then with a kind of spin or misrepresentation
2500000	2504480	as if they've been positive studies. And so yeah, if you look at the medical literature,
2504480	2509040	and this is also back to what I was saying about AI, if you looked at just the medical literature
2509040	2512880	on those antidepressants, you would say, wow, that'll work. Everything's amazing. But if you
2512880	2516480	look at the whole body of evidence that the FDA could look at, you would say sometimes they work,
2516480	2520320	sometimes they don't own or sometimes the one medicine is better than others. It's a lot more
2520320	2524480	complicated and messy when you have all the evidence sitting before you. Yeah, there have been
2524480	2529360	efforts like that in medicine. Pre-registration has been growing in other disciplines, psychology and
2529360	2535120	economics in particular over the past 10 years or so. The American Economics Association, like
2535120	2540560	the Center for International Science, started demanding pre-registration about 10 years ago,
2541360	2546960	and the rate at which that happened on a yearly basis that's gone up within economics alone.
2546960	2551680	Yeah, I think that's all kind of hopeful signs of progress in different fields, just being better
2551680	2557120	practices about being public about what kinds of studies you're doing. And then ultimately,
2557680	2563440	it's kind of all for not unless you publicize the failures and the nor results as well. Because
2563440	2567600	again, if you only publish the positive results and don't say anything publicly about the negative
2567600	2572000	results, it's very skewed. It'd be like if you flip a coin and then you only announce when you flip
2572000	2577520	the heads. Yeah, you would look like you'd flip 100% heads, but that wouldn't be true. So be very
2577520	2586240	misleading. Yeah, I'm going to make a bit of a turn with the next question now, which is it's a
2586240	2591440	question that we always ask in this podcast, which is in relation to the like AU catastrophe. So
2591440	2596640	it's a term that means basically the opposite of a catastrophe. So once it's happened, the value of
2596640	2603200	the world would be much higher. And feel free to relate this question to like your area of expertise
2603200	2609600	within science. And I think also think ambitiously when I ask you this question, which is if you
2609600	2616160	could envision like a specific you could have for the progress of science, what would that be?
2616160	2621680	What would be an existential hope scenario of science? If you like, if all the work that you're
2621680	2628560	doing now came true, what would happen? That's great. Yeah, I first came across that word,
2628560	2633360	you catastrophe and the writings of JR Volkian. Did he invent the word? I can't remember. Yeah,
2633360	2639760	I believe he did. Yeah, it's a long time. Yeah, that's that is a super interesting question.
2639760	2645120	The kind of revolutionary side of me says that in terms of meta science, what would be really cool
2645120	2651920	is if you could duplicate all federal funding of science, but with an entirely new set of
2651920	2658080	institutions. Imagine we spend 50 billion or so on NIH, what if we add in an extra 50 billion on
2658080	2662560	biomedical research? Or with the condition that it has to be run completely differently
2662640	2667200	from NIH, and it has to be in the hands of different people. And again, touching on these
2667200	2674160	ideas of exploiting the and proliferating the diversity of the approaches and the scientists
2674160	2678400	that get funded and the ways in which funding is handed out, because then you have two different
2678400	2683440	systems operating. And so thinking very meta here, you have two different systems operating
2683440	2688000	with equal amounts of funding. And now you can really see at a grand scale, hopefully,
2688000	2692800	what happens? What are the results? It would be a chance to test out lots of different meta
2692800	2698560	science ideas that people have discussed for years or for decades, but you just really need
2698560	2704880	a kind of the grand landscape in which to experiment with that. So again, that's very meta,
2704880	2711360	but my hope would be that we learn a lot about how to fund science and how to organize scientists
2711360	2716720	into organizations, into labs and universities. We need entirely different institutions that
2716720	2721040	are not don't look anything like universities, for example, maybe you should maybe you should
2721040	2724960	set up a whole national institute where the rule is you can only fund scientists who are
2724960	2729200	working out of their garage. I don't know, it just you need some kind of like wild ideas out
2729200	2734240	there to try out different approaches, different scientists, different ideas. And my hope would
2734240	2739360	be that at a minimum, we'd learn something from that. And but in terms of existential hope, I think
2739360	2745120	we we might end up creating a number of great breakthroughs that wouldn't have happened otherwise.
2745120	2752400	And today is hot, heavy, bureaucratic system that is so conformist and so focused on doing
2752400	2755920	whatever gets you approval from the kind of the existing bureaucracy.
2757920	2767520	Yeah, I really, really like that. It's if you like were to introduce someone who is entirely
2767520	2772640	new to this field, is there anything you'd recommend that they like read or watch? Maybe it's
2772640	2776640	just your own sub stack? Or is there anything else that you would recommend for an intro to the
2776640	2782640	field? Sure, the writings that Good Science Project has produced are good points to look at.
2782640	2786880	You mentioned the Center for Open Science, they've had a number of publications and projects that
2786880	2791520	are related to meta science. I would say that there's more stuff coming out from the Institute
2791520	2795600	for Progress and the Federation of American Scientists, they've had a series of papers on
2795600	2800800	open science, for example. Yeah, that might be that gives you plenty of reading to start with.
2801360	2808000	That sounds like a great place to start. And how can listeners best stay updated with your work
2808000	2812400	and the work of the Good Science Project? Sure, you mentioned sub stack, it's just
2812400	2819520	goodscience.substack.com. Also, the website is just goodscienceproject.org. And yeah,
2820560	2825440	we'll have to hear from folks who have ideas or enjoy the writings and want to learn more.
2825600	2832080	Great. Thank you so much, Stuart, for coming on this podcast. We really appreciate it and
2832080	2835840	we'll definitely keep an eye on all the work that you're doing with the Good Science Project
2835840	2838080	in the future. So thank you so much.
