start	end	text
0	1960	Today, we're going to talk about AI alignment.
1960	4200	Is it even harder than we think?
4200	8160	And this is the third AI focus on the salon foresight.
8160	11200	In the previous two weeks, we discussed chapters
11200	13600	in the recently published book on superintelligence,
13600	17000	a coordination strategy, which a few people here on this call
17000	18440	have submitted chapters to.
18440	20960	And then based on those previous salons,
20960	23920	Daniel Schmachtenberger, who we had for another salon
23920	26360	in the Hive Mind series, reached out to me
26360	28600	and recommended today's speaker to me,
28600	32120	because he may bring an underappreciated perspective
32120	34600	to the set of problems in AI alignment.
34600	37120	And as a disclaimer, Forest Landry
37120	40080	is from the newer Hector Collective, Welcome Forest,
40080	43520	and he's not an AI safety or AI alignment researcher.
43520	46000	But I think that's exactly why he thinks
46000	49720	that he may have an outsider's eye to contribute to the field,
49720	52080	and he may be able to point to a few areas that
52080	54080	are under explored.
54080	56720	So we'll kick off with a presentation by Forest,
56720	58560	who's outlining a list of reasons
58560	61200	why AI alignment may be harder than we think,
61200	63840	perhaps even impossible.
63840	68120	And then we open up for a discussion with all of you.
68120	71440	And we may start with our foresight fellows,
71440	74120	for example, Creeon Levitt, who's a foresight senior fellow
74120	77440	and physicist who may be enticed to speak a little bit
77440	80400	on a few physical claims that Forest makes,
80400	83040	as well as channeling Anders Sandberg, who's
83040	85720	our senior fellow in philosophy, but who's currently
85720	88560	on the flight and can't make it, but he says hi to you, Forest,
88560	90320	and he would have loved to make it.
90320	92160	And he has an update on a paper that I
92160	95680	think you're wanting to write, which we can come back later.
95680	98840	But then maybe we'll also be joined by Dan Elton, our 2020
98840	102760	fellow in AI, and by Jeff Ladish, our 2020 fellow
102760	105160	in biosecurity, who may also be enticed
105160	108600	to kind of make a few comments to kick us off with a tricky,
108600	110880	but hopefully really fun discussion.
110880	114040	OK, so today we'll definitely be a little bit of an experiment,
114040	117120	and I encourage all of you with relevant background
117120	120400	to speak up, who wants to kind of rat team Forest's claim,
120400	123720	but then also maybe to surface potential other problems
123720	125800	in AI alignment that haven't been
125800	128200	surfaced in the presentation, that we ought to bring
128200	130520	to the table in the discussion.
130520	133560	OK, that's enough for me, so let's
133560	136720	have Forest present his list of good reasons
136720	140480	to be skeptical that AI alignment is possible for 30 minutes,
140480	142520	and then we'll jump right into the discussion,
142520	145000	and I'm going to share a little bit more
145000	148200	on the background on the topic of today and the talk
148200	150440	in the salons generally in the tent.
150440	152920	All right, Forest, take it away.
152920	153920	Good morning.
153920	157160	Thank you so much for such a wonderful introduction.
157160	158040	I'm glad to be here.
158040	162480	Obviously, as she mentioned, my main area of work
162480	164400	happens to do more with communities,
164400	166320	so I spend a lot of time thinking
166320	170160	about the relationships between man, machine, and nature
170160	173200	from user interface point of view as a software architect.
173200	175760	I've done a lot of work for the federal government,
175760	180600	written number of programs, and search kind of technology stuff.
180600	183040	So being an architect of software systems
183040	186400	for good three decades now, I have a perspective,
186400	189480	and I thought that I might share some elements of that.
189480	191680	Seeing as how from what I can maybe
191680	195240	see that some elements might be unusual or new,
195240	198920	I will at least provide some fruitful conversation.
198960	201640	So the main thing that I'm basically wanting to do then
201640	204760	is just give an outline of what I'm thinking about.
204760	207200	I've got some notes, so I'll be reading a little bit.
207200	210080	But basically, the idea is that if we're looking at AI alignment,
210080	212240	what is the frame that we're going to use to essentially
212240	213400	evaluate that question?
213400	215920	So in other words, how do we determine whether or not
215920	219320	this is a question that can be solved?
219320	221560	So as a kind of schema of thinking about this,
221560	225240	it's like AI can be thought of as a thing that
225240	226760	is in relation to mankind.
226800	229160	So in other words, I'm first positing
229160	232440	that there's a separation between the human nature
232440	234800	and the machine nature.
234800	237200	There are some ways of thinking about this
237200	241160	that look at hybrid cyborg type of things
241160	244520	where there's a kind of mixing of machine and nature,
244520	246160	or machine and man, and stuff like that.
246160	248400	I'm not really considering that explicitly.
248400	251920	I'm looking more at the you have an artificial intelligence
251920	253400	that's an entity unto itself, and you
253400	255280	have human beings which are entity unto themselves,
255280	257440	and what are the interactions between the two of those?
257440	260480	So I'm simplifying the AI alignment question
260480	262560	to look a little bit about what would it
262560	265000	take to align machine intelligence
265000	268680	agency with human intelligence and agency.
268680	271160	So in that sense, we can start to think about the AI alignment
271160	273960	problem a little bit like trying to build a perfect shell.
273960	277320	So in other words, we want to make sure it doesn't have any holes.
277320	281000	And when we look at it from a structural point of view,
281000	284080	we realized that it's not really the same as dealing
284080	287920	with things like energy or flow of matter or stuff like that.
287920	290880	So in other words, if I was trying to build a boat,
290880	293280	that the shell of the boat has to be more
293280	297320	perfect for AI alignment than it would be for seawater.
297320	298720	Because if I have a small leak, I'm
298720	302560	going to have a small influx of process.
302560	305720	Whereas if I have a replicating process,
305720	307400	it's more like a virus or something like that,
307400	311640	or introducing a new species into an environment,
311640	314760	that the replication process means that even a very small hole
314760	317840	can result in essentially a complete intrusion.
317840	321840	Because the duplication process or the replication process
321840	326160	that AI could be implementing, the same as much
326160	328840	as any biological life could implement,
328840	331600	could result in kind of spread the same way that a virus would
331600	334720	spread through an organism, as we've
334720	337120	seen very recently with COVID.
337120	340840	So in effect, we can say, all right, we need a shell,
340840	343280	but we need it to be very strong shell in the sense
343280	345960	of not having any holes, even very small holes.
345960	348200	In other words, it must be very perfect,
348200	351480	given that there's a kind of non-linearity in terms
351480	353960	of the effect, that non-linearity is itself
353960	357560	a consequence of the capacity for machines
357560	360120	to be duplicated easily.
360120	362560	So in effect, now we can basically say, OK, well,
362560	366320	given that we know that we need to construct something that
366320	370440	doesn't have any holes, can we use mathematics and logic
370440	372840	and physics and things like that to demonstrate
372840	376880	in multiple overlapped ways that there just cannot
376880	379040	be a way of constructing such a shell.
379040	384320	In other words, that it's impossible to establish
384320	387080	that there are no holes in the shell.
387080	389000	In other words, that the principle
389000	393040	of realizing in any practical sense of a shell of this kind
393040	395720	is essentially an impossibility.
395720	397800	And so why would we want to do a proof like this?
397800	399720	Or why would we want to essentially establish this?
399960	403680	Well, obviously, if we find out that something isn't possible,
403680	406760	even at the level of principle, then in effect,
406760	408880	it allows us to kind of free up a lot of energy.
408880	411200	In other words, we redirect our attentions to other things,
411200	413880	and we try to solve problems in different ways.
413880	417720	So in one sense, this is not necessarily
417720	418760	an ideal thing to do.
418760	420280	It's to try to say, hey, wait a minute,
420280	422600	if we're trying to solve the problem of AI alignment,
422600	426320	and we discover that there's no reasonable or realistic way
426320	428800	to think about actually being able to do so.
428800	431280	In other words, if we can't engender any real hope
431280	433800	of such a solution, then at least we
433800	437960	get the kind of silver lining result of, well,
437960	439920	now we can reallocate our energies to things
439920	442360	which are tractable and are possible.
442360	445600	So please understand that this presentation is more or less
445600	448040	in that sense.
448040	450440	So another kind of way of saying, OK,
450440	451960	so if we're going to be looking for holes,
451960	454440	what is the frame in which we're going to be identifying
454440	456000	that such a thing is impossible?
456000	460000	I mean, what are we going to use as tools to do that?
460000	461880	So one way that we can sort of do this,
461880	465920	we can frame things in terms of kind of three general categories.
465920	469400	So we can talk about space and identity,
469400	473400	i.e. what is the envelope and the shape of it,
473400	475080	and how small a hole, and stuff like that.
475080	478000	We can talk about things in terms of time and force,
478000	480320	and we can talk about things in terms of possibility
480320	482720	and probability.
482720	485240	So obviously, we're concerned with things like choice, i.e.
485240	489240	the AI choices, the choices that we have, changes,
489240	491840	changes in the environment, or changes in the marketplace
491840	494200	dynamics, and causation, i.e.
494200	499000	what it is that makes machinery work and things like that.
499000	501920	And then we can start to bring in, using this as a foundation,
501920	504680	we can start to bring in notions about information, complexity,
504680	508280	game theory, and other tools to consider the question.
508280	511120	And that leads us to start thinking about things in terms
511120	513160	of longer time scales.
513200	515480	So in other words, rather than thinking about AI alignment
515480	517880	in terms of what happens immediately
517880	520840	before or after takeoff, that we are in fact
520840	523360	concerned with what happens over, say, hundreds, thousands,
523360	526600	or potentially even millions of years.
526600	530640	So in effect, when we're looking at things like about identity,
530640	533280	for example, we're not just considering things
533280	535440	like the identity of the holes, we're actually
535440	538400	considering things like the identity of the AI.
538400	540680	And it's really easy for certain biases to creep in.
540720	543640	So for example, as a human being,
543640	547120	we tend to have a brain inside of a skull,
547120	549720	and that the bandwidth of communication
549720	552560	internal to the skull and the bandwidth of communication
552560	556560	across the skull is very, very different.
556560	560720	I mean, we have essentially a relatively low bandwidth
560720	562400	of interaction with the environment
562400	567040	relative to the total bandwidth of all the synaptic connections.
567080	570120	So in effect, in a lot of ways, there's
570120	572840	this very, very strong differential
572840	575360	between the internal bandwidth and the external bandwidth.
575360	578440	For example, the communication process of which
578440	581040	I'm engaged with you right now verbally
581040	585800	has about 42 bits per second of communicative bandwidth.
585800	589720	But in effect, the net difference in bandwidth
589720	593400	makes it very, very likely that brains will come in units,
593400	596880	that we won't necessarily have a kind of process
596920	600000	that allows two people to communicate at such and such
600000	603640	a level that you're treating them as a single entity.
603640	605760	For the most part, you're going to be talking about human beings
605760	607720	as discrete units.
607720	609840	So in one sense, like we could say, all right,
609840	611320	well, there's different groups of people
611320	613640	that are trying to develop AI.
613640	615280	And let's say several of them succeed.
615280	617560	And just for argument's sake, let's
617560	619960	say several of them succeed quickly,
619960	622560	that you end up with multiple AIs deployed in the field,
622560	626240	or perhaps a given group as part of its experimental
626240	629240	program was creating several instances.
629240	631320	So in other words, they're trying different design
631320	633840	techniques in various manifestations,
633840	637320	or they produce an AI, they do some experiments on it,
637320	639040	and they shut that one down, they make some tweaks,
639040	640120	and they start up another one.
640120	644840	So you can end up with multiple instances of the AI,
644840	646880	or multiple instances of variations
646880	649560	of the same kind of AI, or multiple instances of AI
649560	652640	implemented, maybe even completely different substrates.
652640	655520	So the question then can be asked,
655560	657040	to what degree do we have a belief
657040	660400	that these AIs might not merge with one another?
660400	661880	So in other words, taking the bias
661880	664840	that we have that intelligence is going to come in units
664840	667960	might not necessarily apply to AI itself
667960	671200	as a phenomenon or intelligence as a phenomenon.
671200	673960	And there's some good reasons to think about it that way.
673960	677040	For one, the bandwidth of the internet
677040	679760	is obviously very much greater than the bandwidth
679760	682720	of any human being to any other human being
682720	686040	in terms of just straight brain to brain communication.
686040	689720	And then secondly, that since that differential
689720	691640	is much, much lower, you're looking at more
691640	694000	of a market forces kind of model.
694000	695960	So in the same sort of way that if you have
695960	699400	two separate markets, maybe you have a country over here
699400	700640	and you have another country over there,
700640	704240	and they're just finding out about one another,
704240	707200	you have Columbus crossing the ocean and so on,
707200	709560	that there's a very strong motivation
709560	711920	for those two marketplaces to essentially find
711920	713560	some sort of arbitrage that would allow them
713560	715640	to exchange with one another.
715640	718480	So in other words, wealth creation as a force
718480	721760	in the marketplace definitely wants to create ways
721760	724680	for us to engage in larger scales of trade.
724680	727680	And so in that same sense, if we were looking at intelligence
727680	730360	as a phenomenon, sense making as a phenomenon,
730360	731920	there's very good reason for us to believe
731920	734880	that the notion of having separate intelligences
734880	738480	on a machine level may not necessarily hold long term
738480	741400	that we effectively end up with a kind of merger
741440	743600	between these multiple intelligences.
743600	746680	And this strengthens the point of view of that,
746680	748960	you're really looking at essentially
748960	751720	two different agency types.
751720	753960	You're looking at sort of human being agency type
753960	757040	and the way in which we coordinate as groups
757040	759320	and individuals and the kinds of evolutionary process
759320	763160	that have endowed us with certain propensities.
763160	766800	And then basically whether or not those propensities
766800	769040	also apply to artificial intelligence.
769040	771600	And what does that tell us about the nature
771600	774280	of how we have to think about this?
774280	779280	So that's the space and identity question
779320	781120	being considered a little more deeply.
782760	785000	The other process that we can think about of course
785000	787480	is the sort of force and time thing.
789240	792320	For example, if we're looking at what are the ways
792320	795920	in which we would engender alignment.
795920	799040	So again, we can talk about things that are intrinsic
799040	802040	to the artificial intelligence.
802040	806880	So an analog of this, Isaac Asimov wrote a lot of fiction
806880	810920	assuming this phenomenon called the Three Laws of Robotics.
810920	813120	So that would be essentially something that is built in
813120	815200	that creates an AI alignment.
815200	817440	And perhaps the argument is that we could create
817440	819520	something roughly analogous to that.
819520	820680	Maybe it wouldn't be as good
820680	822120	or maybe it would take a different form
822120	824760	or maybe there's a lot of questions about
824760	826200	what does alignment actually mean
826200	828640	and how do we express such forces
828640	831080	in a design intrinsic way.
831080	833840	But then there's obviously the extrinsic stuff, right?
833840	838800	So for instance, say we have some sort of relationship
838800	843720	between the AI intelligence and human beings as a collective.
843720	847120	And what sort of feedback processes might apply
847120	851120	to essentially address what would be the principal agent
851120	855240	problem in the relationship between human intelligence
855240	857480	and artificial intelligence.
857480	861400	So in effect, we can look at the AI question,
861400	863080	the alignment question specifically
863080	866680	as being a special case of the principal agent problem.
866680	868720	And there have been various notes
868720	873320	about how to address principal agent problems in general.
873320	875960	So one thing we could say as part of this sort of force
875960	877840	and time way of looking at things
877840	881000	is to essentially ask what are the forces
881000	885080	that essentially help us to create alignment
885080	886880	or help us to maintain alignment,
886880	889840	i.e. if it's something that's built in in a prior sense
889840	892120	or something that is essentially applied
892120	896680	after the AI exists in the relational dynamics.
898320	900080	And so in effect, we can basically say,
900080	902880	all right, well, given that we have these forces
902880	906280	that are applied, how long do those forces
906280	908560	maintain that alignment?
908560	911840	So again, it's not just a question of,
911840	915680	do we have alignment today or before the thing is built?
916840	918840	Can we create the conditions before it's built
918840	920960	and can we maintain the conditions after it's built?
920960	922360	But it's also the case of,
922360	924320	can we maintain those conditions after it's built
924320	926720	for a really long period of time?
926720	928680	So in other words, hundreds of thousands of years
928680	932200	essentially, given that the scope of the introduction
932200	933880	of something like this,
933880	936720	particularly if it has the capacity to replicate itself,
936840	938800	which it almost certainly will,
938800	942240	then in effect, there's a sort of functional aspect
942240	943520	of saying, okay, well, it's a little bit like
943520	945440	introducing a new species.
945440	947480	And in the same sort of way that say,
947480	949600	introducing mosquitoes into the Hawaiian islands
949600	953160	is a little hard to undo, then in effect,
953160	955400	once artificial intelligence is introduced
955400	957520	into an environment, it could also be argued
957520	960800	that it influences the future course of civilization
960800	961640	as we know it.
962680	966120	So in that sense, we can really just go back
966120	968480	to the whole notion of why it's important for us
968480	971040	to figure out this AI alignment question,
971040	973080	given the severity of the ethical implications.
973080	975200	I mean, we're literally talking the future
975200	977600	of the human species or the,
977600	980160	if things just continued more or less as they were,
981160	985720	as seen over the last epoch of the thousands years or so,
985720	987320	we could literally be talking about trillions
987320	988480	of human beings.
988480	991880	And that's not assuming over the future,
991880	995780	600 or so million years that this planet
995780	997980	will continue to sustain life
997980	999940	in the sense that we understand it.
999940	1002820	So if we were to look at it from a larger point of view
1002820	1005620	of say we become a space-faring species
1005620	1007100	and we start into colonized Mars
1007100	1011220	and potentially other planets in the galaxy,
1011220	1013300	that trillions could become trillions of trillions.
1013300	1016100	So the ethical emphasis of the question is very clear.
1016100	1017980	And so as a result, it encourages us
1017980	1020140	to really think about this question
1020140	1022220	in terms of hundreds or at least thousands of years
1022220	1024940	as a minimum for us to really evaluate this question.
1024940	1027500	So in that sense, it's not just a question of
1027500	1031020	can we talk about the holes as being small
1031020	1033420	and that the shell essentially doesn't have such holes,
1033420	1037420	that whatever the forces are that create alignment,
1037420	1040220	whatever are the solutions to the principal agent problem,
1040220	1041980	that they need to be perfect enough
1041980	1046020	that we don't end up with microscopic fractures
1046020	1049060	and that can cascade into larger failures
1049060	1052060	and eventually compromise the alignment problem
1052060	1053980	at some future date.
1053980	1056260	So in that particular sense,
1056260	1058620	we're really looking at a kind of potentiality basis,
1058620	1060260	like what is the probability of such a hole?
1060260	1063740	What is the probability of that cascading
1063740	1065180	and being amplified?
1067340	1069980	Now, of course, there are obviously,
1069980	1073220	we bring into this conversation a lot of biases.
1073220	1076020	We have a lot of people that will gain
1076020	1078580	on a financial level if we develop AI.
1078580	1080740	There's a hope for basically solutions
1080740	1081620	to really hard problems.
1081660	1084460	There's a hope for a kind of asymmetric advantage
1084460	1086140	of one group that develops at first
1086140	1089780	versus other groups that might develop it later.
1089780	1092660	So there's a lot of things that are really encouraging us
1092660	1094020	to try to figure out this question
1094020	1096660	because if we were to figure out this question,
1096660	1101380	then obviously the economic advantages would be enormous.
1101380	1103900	But again, at this particular point,
1103900	1105980	we're back to the principal agent problem.
1107500	1110900	So sort of skipping past the kind of multipolar dynamics
1110900	1112100	that exist between the groups
1112100	1114860	and kind of leaving the economic questions aside,
1116060	1117900	we can ask, well, what is the possibility
1117900	1120260	that we could effectively ensure
1120260	1122340	that there are no holes?
1122340	1125900	In other words, what is it gonna take to establish that?
1126980	1128980	Well, one question that we can immediately look at
1128980	1132860	is essentially what is the nature of the coexistence
1132860	1135460	that exists between machine intelligence
1135460	1138420	and human intelligence?
1139260	1142300	And everything I've said prior to this
1142300	1144540	is probably stuff that people have already thought about
1144540	1145740	and is well-known information.
1145740	1147860	And the stuff that I'm saying from here,
1147860	1150860	I'm hoping is new information to this group.
1150860	1153460	I haven't seen presentation of this particular argument
1153460	1156020	from this point as yet.
1156020	1159620	So this to me is probably the more interesting part
1159620	1161140	of this presentation.
1162140	1164140	So if we were to look at, again,
1164140	1166180	the relationship between machine intelligence
1166420	1168860	and human intelligence as essentially representative
1168860	1170540	as a kind of species,
1171420	1173500	and that the species themselves,
1173500	1176380	the same way that human beings influence their environment
1176380	1178780	to kind of define an ecosystem around ourselves
1178780	1181100	or at least a niche within an ecosystem,
1181940	1183900	that the machine intelligence itself
1183900	1188060	is also gonna occur within the niche of the marketplace
1188060	1190380	and the niche of the manufacturing
1190380	1192300	and the industrial complex particularly.
1193260	1196420	So what are the fundamental dynamics
1196420	1199300	of these two ecosystems with respect to one another
1199300	1201060	and what does that tell us about the possibility
1201060	1201900	of alignment?
1201900	1205140	In other words, can we bring the tools of physics
1205140	1207580	and the logic of, say, game theory
1207580	1209580	to look at the relationships between
1210740	1211820	the intelligence phenomenon,
1211820	1214220	obviously we have the agency of human beings
1214220	1216260	and human groups,
1216260	1219940	and then obviously the agency of artificial intelligence,
1219940	1222580	but we also have the substrate relationships.
1222580	1225860	So in effect, when we're looking at this,
1225860	1229780	we're looking at, okay, what is silica-based
1229780	1233820	chemistry process and the implications that that has
1233820	1236940	relative to the ecosystems necessary to support it
1236940	1241140	versus carbon-based ecosystem process?
1241140	1244260	So in other words, if we're really looking at
1244260	1247100	what are the kinds of forces that will create alignment,
1247100	1249660	we're really looking at what is the marketplace
1249660	1252260	that essentially joins these two ecosystems
1252260	1254500	and what are the dynamics inherent in the relationship
1254500	1256540	between these two ecosystems?
1256540	1258300	And using that basis,
1258300	1260660	we can start to make some observations.
1260660	1262180	So one of the observations that we can make
1262180	1265460	is just from the very nature of the chemistry itself.
1265460	1270460	If we look at the sort of kind of total envelope
1270620	1273660	of all of the different kinds of chemical interactions
1273660	1277140	that encompass carbon-based chemistry necessary
1277140	1280300	to engender biological intelligence of our kind,
1280300	1282380	but also the ecosystems necessary to support
1282380	1285180	the substrate, our bodies and food supply and so on.
1286140	1289580	And when you look at the sort of total range of chemistry
1289580	1292300	that defines life on this planet,
1292300	1294980	you notice that it mostly occurs between say,
1294980	1297540	zero degrees and 500 degrees Fahrenheit.
1298340	1302740	And this just, again, if you do a sort of enthalpy
1302740	1306660	calculation on how much each different kinds of bonding
1307500	1310860	or chemical recombination and stuff like that occurs,
1310860	1314220	the vast majority of it occurs at standard temperatures
1314220	1315260	and pressures.
1315260	1316940	And there's some exceptions
1316940	1319420	that sort of are more in the outliers,
1319420	1321860	but the vast majority of the chemistry occurs,
1321860	1326300	as I said, in this 500 degree range of temperatures.
1327420	1330100	But when you look at the silicon-based chemistry,
1330100	1331620	you actually find that the envelope
1331620	1333380	in which most of the chemical reactions occur,
1333380	1335340	the enthalpy that's involved in those reactions
1335340	1337700	is actually over a much different range.
1337700	1340180	Most of those reactions need to start somewhere
1340180	1342220	in the neighborhood around 500 degrees Fahrenheit
1342220	1346500	and go all the way up to 2,500 degrees Fahrenheit, typically.
1346500	1349620	And the reason that we know this is actually,
1349620	1352940	again, speaking specifically of this planet as an example,
1352940	1357620	when you look at the kinds of phenomena that are involved,
1357620	1361020	I mean, first of all, the amount of silica that's available
1361020	1362980	in the Earth's crust relative to the amount of carbon
1362980	1364820	that's available on this planet
1364820	1368820	is there's a substantially large preponderance of silicon.
1368820	1370740	I don't remember the statistics off the top of my head,
1370740	1372700	but it's something like 25%
1372700	1376700	of all the elemental constitution of the Earth's crust
1376700	1381700	is silica and that something like 0.05% of it is carbon.
1381860	1385020	And that's including everything in the surface,
1385020	1388020	above the surface and all the way up into the atmosphere.
1388020	1390580	So first of all, silica chemistry
1390580	1392940	has had far more opportunity to engage
1392940	1396340	in a much, much wider variety of chemical combinations
1396340	1398020	just by its sheer preponderance
1398020	1401260	over the 4.5 billion year history of this planet.
1401260	1403140	So in other words, you have volcanoes
1403140	1405180	and you've got all sorts of solar flux,
1405180	1406580	you've got cosmic ray radiation,
1406580	1409140	you've got all sorts of stuff, lightning and so on
1409140	1412860	that would really encourage every possible interaction
1412860	1416540	between every elemental type sometime in the total space
1416540	1419300	of the surface of the Earth and the volume of the Earth
1419300	1424300	and the total duration in which those experiments
1424580	1428460	can be conducted by just raw chemical mixing.
1428460	1431620	So in effect, when we look at the net result of that,
1431620	1433620	essentially the current state,
1433620	1435660	we see that for the most part,
1435660	1438180	the chemistry of silica results in things
1438180	1439900	that we call rocks.
1439900	1441740	There's a whole lot of elemental types
1441740	1443300	and mineral types and so on and so forth,
1443300	1446740	but for the most part, the variety of chemistry
1446740	1449780	that occurs is relatively inert
1449780	1451620	at the standard temperatures and pressures
1451620	1455420	that our biosphere is mostly defined by.
1456260	1458940	So as a result, when we look at the sort of fundamental
1458940	1462180	substrate issues and we start to think about things like,
1462180	1464380	the kind of chemistry processes that are necessary
1464380	1466900	to develop microchips, for example,
1466900	1470460	we not just see, obviously these manufacturing channels
1470460	1472860	and lithography processes and such,
1472860	1474540	but we also see the chemistry
1474540	1476180	and the kinds of reaction processes
1476180	1478940	that are needed to create that kind of variety
1478940	1481300	of, you know, again, interaction.
1481300	1484740	Obviously you need a certain amount of manifest complexity
1484740	1488300	in order to support the substrate of compute to begin with
1488300	1491420	and also a certain amount of manifest variety of chemistry
1491420	1494580	in order to provide for sensory capacity
1494580	1497020	and interaction with the environment.
1497020	1497860	But-
1497860	1498700	Horace?
1498700	1499540	Yes.
1499540	1501700	We have one comment already from Crian, I think.
1503580	1505100	Oh, well, I was kind of saving this.
1505100	1506500	I was saving this stuff up, maybe,
1506500	1508500	for the discussion period at the end.
1509540	1512380	I just, well, as long as here we are, let me ask you this.
1512380	1514660	I understand that you're saying there's more silicon
1514660	1517820	and in some sense there's been more time for silicon
1517820	1520260	to engage in more chemical reactions,
1520260	1523660	but the fact of the matter is, for whatever reason,
1523660	1526980	silicon life didn't evolve even in the lithosphere,
1526980	1528780	presumably where the temperatures are high enough
1528780	1531300	for all these reactions to make these rocks.
1531300	1532980	And yet carbon life did.
1532980	1536300	So arguably it seems as if there's something about carbon
1536300	1541300	that allows a much wider variety of molecular forms
1542300	1545820	to become into existence.
1545820	1549340	Now, whether that's just a fact that carbon took off first
1549340	1551780	and silicon could do it in their silicon life elsewhere
1551780	1553940	that's natural and organic, if you will,
1553940	1555380	that's another question, obviously,
1555380	1557140	when maybe we'll eventually know the answer to that,
1557140	1560620	maybe not, but it is actually then in that sense
1560620	1562940	even more astonishing that carbon took off
1562940	1566100	instead of silicon, because the earth was hot at the beginning
1566100	1570860	and carbon was an insignificant fraction.
1570860	1572380	So that's what I'm saying.
1572380	1574580	And it is a component of this argument
1574580	1576700	to think about Fermi Paradox kind of issues,
1576700	1579980	like is the Great Barrier in the past,
1579980	1581500	is there a Great Barrier in the present,
1581500	1583340	like where the hell are all the little green men,
1583340	1584940	why don't we know about them?
1584940	1586460	Or is the Great Barrier in the future,
1586460	1587900	the technological civilization
1587900	1589500	is inherently self-terminating.
1590500	1592860	I can draw those elements into the argument,
1593100	1595020	but the point that you're making is actually a real one,
1595020	1597300	and I mean, obviously it's a real one,
1597300	1600180	but that it actually contributes to what I'm saying
1600180	1602580	in the sense that say, for example,
1602580	1604460	we were to, at this particular point,
1604460	1609460	bootstrap to sort of bring silica life into existence.
1609820	1610660	The real question is,
1610660	1613100	is how well do these ecosystems exist with one another?
1613100	1615780	They are definitely different ecosystems.
1615780	1619820	And given that the enthalpy of reaction of one versus the other
1619820	1621180	is so substantially different
1621180	1624780	than the question of toxicity becomes a matter of some concern.
1624780	1628140	So in other words, if I have industrial processes,
1629060	1630180	and I'm saying industrial,
1630180	1631780	just to give it a way of relating it to,
1631780	1634860	but let's say I have a silica-based ecosystem,
1634860	1636900	the silica-based ecosystem is gonna involve
1636900	1640100	a very different range of energies and elemental types
1640100	1641860	than the carbon-based ecosystem.
1641860	1643380	And then inherently, as a result,
1643380	1646700	the silica-based ecosystem is gonna be toxic
1646700	1647900	to the carbon-based life.
1647900	1649900	So in other words, in order for those two to exist,
1649900	1651300	you're gonna have to have some kind of wall
1651300	1655260	because the pressure and the temperatures and so on and so forth
1655260	1657500	inherently involved in the replication cycle
1657500	1659500	associated with silica-based life.
1659500	1661140	It's just at, like I said,
1661140	1662940	a very different level of characteristics
1662940	1664940	than that which is involved in carbon-based life.
1664940	1668340	And in effect, you now have a situation
1668340	1670700	where the carbon-based life is suffering
1670700	1673060	because of the energies involved in the silica-based life
1673060	1675540	are just substantially greater,
1675540	1676740	not just in replication,
1676740	1679100	but in terms of what their tolerances are.
1679100	1680020	So for instance, you know,
1680020	1682820	spacecraft can essentially be out in space unshielded
1682820	1685620	and deal with cosmic radiation and solar radiation
1685620	1687700	and stuff like that far, far, far more easily
1687700	1690740	than carbon-based life can do the same thing.
1690740	1693220	So if there was any kind of energetic exchange
1693220	1695820	between the carbon-based life and the silica-based life,
1695820	1698620	whether it be a weaponized exchange of energy
1698620	1700460	or even just a typical one,
1700460	1702060	it's not just that the information flux
1702060	1703900	that the silica-based life could handle
1703900	1708100	and the intelligence and gender is substantially greater.
1708100	1710020	It's also that just on the sheer energy level,
1710020	1711380	it's substantially greater
1711380	1713540	and that that has inherent toxicity relationships
1713540	1716380	with respect to the carbon-based life.
1716380	1718100	So now we have essentially two issues.
1718100	1719860	It's not just that we need AI alignments
1719860	1721940	as far as the intelligence is concerned,
1721940	1723660	but we absolutely have to have AI alignment
1723660	1726500	in order for us to even have some possibility
1726500	1728140	of coexisting.
1728140	1729780	If you look at it from that point of view,
1729780	1731020	all of a sudden you're now realizing
1731020	1733260	that there are evolutionary dynamics to this
1733260	1736660	and that there's a certain game theory that comes to bear.
1736660	1738420	So say, for example,
1738420	1741100	that we're looking at things on geological time
1741100	1742940	and we're basically saying, okay,
1742940	1745220	there's a kind of replication process
1745220	1747500	or there's a kind of future maintenance process
1747500	1750820	to some sort of dynamic that the AI is using
1750820	1752340	to persist itself in time.
1753300	1755820	And what are the degrees of exchange
1755820	1757020	that would exist between the two
1757020	1759220	that would even help to enforce AI alignment at all?
1759220	1761980	So in other words, if you look at the principal agent
1761980	1764420	problem solutions that are generally proposed,
1764420	1766580	they either depend upon something intrinsic
1766580	1768100	to the inner nature of the AI
1768100	1770940	or they depend on some sort of market feedback process,
1770940	1773500	i.e. reputation or admission,
1773500	1777900	some sort of thing that allows us to create some feedback
1777900	1780260	incentive or punishment or barrier
1780260	1784580	that essentially maintains some sort of peace
1784580	1787180	in terms of the energy exchange across the wall.
1787180	1788020	Hey, Forrest.
1788020	1788860	Yes.
1788860	1790700	May I, I wanna raise two things.
1790700	1793180	And if I'm talking too much, please let me know.
1794060	1798020	First thing is, I'm wondering if you have run across
1798020	1800900	Lovelock's latest book, The Nova Scene,
1800900	1803340	because he makes some very similar arguments.
1803340	1805180	Well, from a sort of different angle,
1805180	1808220	I'll just mention, he basically says that
1809220	1812860	it's a damn good thing that we might create
1812860	1815820	artificial superintelligence based on silicon
1815820	1817860	because as the world gets hotter,
1817860	1821100	silicon can live there and carbon can't, you know?
1821100	1822260	And we're making the world hotter,
1822780	1825940	so this is arguably good because otherwise,
1825940	1827700	conscious life is just gonna disappear
1827700	1830820	if we don't make it capable of running
1830820	1832300	at higher temperatures.
1832300	1833900	So that's one thing, I thought that's interesting.
1833900	1835620	I just wanted to make you aware of Lovelock's book.
1835620	1838180	It's pretty interesting and short and sweet book.
1838180	1840780	And then the other thing is that
1840780	1842300	wouldn't this suggest that perhaps,
1842300	1844860	and maybe I don't wanna have a spoiler here,
1844860	1846260	wouldn't this suggest that perhaps
1846260	1848700	if we all agreed to only implement AI
1848700	1852300	using like really fragile cryogenic qubits,
1852300	1856140	then we're good to go because it'll actually be less,
1856140	1859420	it'll be more, it'll require cooler temperatures
1859420	1862300	and less, you know, tolerate less radiation
1862300	1864580	and like be more fragile than biological life.
1864580	1866820	So we win if the shooting starts.
1868460	1870180	First of all, thank you for the introduction
1870180	1871420	of the book, I wasn't aware of it.
1871420	1873620	I will definitely look at that
1873620	1877500	to respond to the notion of should AI intelligence
1877500	1878940	be developed eventually, sure,
1878940	1882700	but I'm thinking that something like 650 million years from now
1882700	1883860	would be an appropriate time for us
1883860	1885900	to start thinking about that.
1885900	1886780	If we do it sooner,
1886780	1889460	we're pretty much going to extinct ourselves.
1889460	1892020	And that's not just a hypothesis in the sense of,
1892020	1893980	hey, this is likely to happen.
1893980	1896020	Potentially what I'm attempting to do and set up for
1896020	1899140	is essentially a theoretic and evolutionary mathematics
1899140	1902260	based way of showing that it cannot not be the case
1902260	1907260	because there's no basis upon which to establish
1908380	1911060	coexistence and many, many reasons to show that
1911060	1913420	from a game theoretic point of view,
1913420	1916780	that it's not even, there's not even a question.
1916780	1918540	In other words, if you look for Nash Equilibrium
1918540	1921020	in this particular space, you can prove that there are none.
1921020	1922740	Oh, does that, so that's one more question.
1922740	1923820	Does that have any question?
1923820	1926780	Does that rely on your temperature arguments or not?
1926780	1927980	Crayon Vendan.
1927980	1930700	Uses the temperature arguments as essentially a way
1930700	1932620	of establishing what to look for.
1932620	1935460	So in other words, what you eventually are able to do
1935460	1938900	is to show that any feedback process that's causative
1938900	1943100	depends upon some sort of mutuality of process.
1943100	1945580	Once you've established that there's no mutuality of process,
1945580	1949740	you can establish that there's no marketplace dynamic
1949740	1952620	that essentially binds the two ecosystems.
1952620	1957340	And unless you impose some sort of supervalent altruism
1957340	1959740	that you can't establish that, then you can go
1959740	1962180	and you can prove that such a supervalent altruism itself
1962180	1964260	is forbidden by the laws of physics.
1964260	1966420	So in effect, what you can essentially do
1966420	1968660	is you can say, all right, we know
1968660	1970580	that we require these particular conditions
1970580	1972700	to be successful.
1972700	1974220	Looking at it from this point of view,
1974220	1976180	we can establish that this kind of mathematics
1976180	1978220	is necessary to evaluate it.
1978220	1980180	And then on purely, like I said, I
1980180	1982100	could do it in information theory, but I'm sorry,
1982100	1983580	I can do it on game theory, but it's also
1983580	1985820	possible to do on information theory.
1985820	1988100	Effectively, in order to have the alignment persist,
1988100	1990740	you need to show that the noise floor of the copy
1990740	1994340	from the past into the future is essentially consistent,
1994340	1997420	which brings me to the third point of what you were speaking
1997420	2001340	of, which is that if we try to make the AI too fragile
2001340	2004380	in that particular sense, that might work for a time.
2004380	2006460	But the thing is, is that you can't necessarily
2006460	2007940	sure that it's going to stay fragile.
2007940	2010820	If there's any kind of, again, replication process
2010820	2014620	or any kind of dynamic where there's persistence in time,
2014620	2017700	in other words, that this thing has to do some sort of repair,
2017700	2019340	there's going to be essentially, eventually,
2019340	2022380	the emergence of dynamics that strengthen that.
2022380	2025500	Otherwise, the fragility itself terminates that line.
2025500	2027860	So from a purely evolutionary point of view,
2027860	2030260	you begin to see that if we're looking at apocal periods
2030260	2034700	of time, that certain phenomena drives certain aspects
2034700	2037180	of the situation drives certain results.
2037180	2038780	And from those results, we can begin
2038780	2041540	to do certain calculations that allow us to establish
2041540	2043820	that AI alignment is not possible.
2043820	2047700	OK, we have Dan as well with a comment.
2047700	2050180	Yeah, well, it's good that you're
2050180	2053860	bringing physics into this, because I was a physics major.
2053860	2060860	But you see the future as being a sort of economic competition
2060860	2066180	between silicon life forms and life forms, which
2066180	2067980	are carbon-based life forms.
2067980	2070180	I have to make it relatable.
2070180	2073820	So we think in terms of marketplace dynamics
2073820	2075420	and evolutionary theory and game theory.
2075420	2077220	And essentially, all of those are models
2077260	2079860	that have essentially a similar form and structure.
2079860	2080500	Right.
2080500	2082740	Now, I agree with you that the silicon life forms,
2082740	2087780	I mean, theoretically can be that use
2087780	2092820	silicon and metal and other elements materials
2092820	2096980	can be theoretically much stronger and more powerful.
2096980	2098700	And we all know that and appreciate that.
2098700	2111100	But in terms of alignment, it's possible that if I agree
2111100	2113780	that we should be skeptical about this happening.
2113780	2120780	But it is possible the silicon-based AIs
2120780	2124820	could recognize that there's a lot of silicon and metal out
2124820	2129420	in space, and they can live in space easily where we can't
2129420	2130780	live in space easily.
2130780	2131980	I understand.
2131980	2134700	That doesn't necessarily solve the problem.
2134700	2139940	Might they just go into outer space and operate there?
2139940	2140620	Sure.
2140620	2142700	And so in effect, what you end up with
2142700	2146820	is two separated ecosystems.
2146820	2151700	Now, if the ecosystems are separated and they stay separated
2151700	2155180	and there's no interaction between them
2155180	2160100	and there's no reason for them to force such an interaction,
2160100	2163380	then you have a coexistence model of two ecosystems
2163380	2168180	essentially time separated from one another.
2168180	2169580	But that's not AI alignment.
2169580	2173140	It's essentially just like complete independence.
2173140	2181660	Well, if AI is lying so it respects human life,
2182540	2186260	it might decide to leave humans on the Earth doing our thing
2186260	2193340	and go into space and just go colonize other planets.
2193340	2195900	This brings us directly to what would be the second great
2195900	2199340	barrier of the Fermi Paradox.
2199340	2202500	Once you have some sort of real separation
2202500	2206700	between the two ecosystems, then it becomes a question of,
2206700	2209540	is there any reason for the two ecosystems
2209540	2211140	to want to interact with one another?
2211140	2214420	Or is it actually the case that we have some strong reasons
2214420	2217580	for them not to want to interact with one another?
2217580	2221220	And again, with this sort of way of approaching it,
2221220	2223940	we can actually ensure, I'm sorry,
2223940	2228220	we can show that it's desirable for each side
2228220	2232740	to ensure that it doesn't interact with the other one.
2232740	2237340	I just posted a meditation model piece, which is a piece,
2237340	2240820	but it's making the claim that any type of super
2240820	2245300	intelligent will enter dynamics, but which is impossible,
2245300	2247740	that it can leave us even just a guard.
2247740	2251260	So just by, yeah, do you want to comment on that?
2251260	2252620	I wanted to reference that argument.
2252620	2253900	I'm glad you did.
2253900	2255700	I was literally debating in my head
2255700	2257580	whether it was worth trying to bring all that up.
2257580	2261580	But yeah, I've read that piece and it's been, actually,
2261580	2264620	I think it's one of the better writings of that space.
2264620	2269540	It really details the multipolar trap situation very, very well.
2269540	2271660	And it kind of describes the relationships
2271660	2274820	that it has to a lot of these considerations.
2274820	2276700	Some of the work that I'm doing is essentially
2276700	2278220	based upon that paper and it's based
2278220	2281900	upon the work of Nick Balstrom and others who I think
2281900	2284260	have been thinking about this very cogently.
2284260	2288380	And so in effect, I would love to import most of that stuff
2288380	2290900	as part of the thinking.
2290900	2292380	Forrest, one question I have, and this
2292380	2294580	is based on some previous conversations we've had
2294580	2298580	and also some things you've said today,
2298620	2303140	refers to basically this idea of what you say is replication.
2303140	2305780	But I think you would think refers
2305780	2307380	to any sort of self-modification.
2307380	2311380	And the question is, can you have stable goal preservation
2311380	2313780	in the face of self-modification?
2313780	2315180	Because one of the claims, I think,
2315180	2318100	that comes out of the AI safety community
2318100	2320220	is that when you have a rational agent that
2320220	2322020	is improving itself, it has an incentive
2322020	2323540	to want to preserve its own goals.
2323540	2326340	You don't succeed at your objective function
2326340	2327820	if your objective function changes
2327820	2330140	from the perspective of your past self.
2330140	2332100	So in some sense, you have an incentive
2332100	2334380	to want to try to stably maintain that.
2334380	2337100	But I think you're arguing that this isn't possible.
2337100	2338820	And I was wondering if that's true,
2338820	2340820	if you could lay out that argument.
2340820	2343220	Well, it's actually a very subtle thing.
2343220	2345620	And so in principle, I agree with you.
2345620	2348180	And in philosophy, although this is somewhat obscure
2348180	2352660	terminology, it's the problem of transcendental stabilization.
2352660	2355140	So on one hand, you want the change to occur.
2355140	2358780	Like, you want the goal structure to change a little bit.
2358780	2360900	Because obviously, if you don't have some changes
2360900	2363260	in the goal structure, you're not exploring
2363260	2366140	the evolutionary space of what would be maybe better goals.
2366140	2369940	So niche discovery and adaptations, such like that,
2369940	2373220	eventually will require some amount of goal modification.
2373220	2375460	But as you mentioned, you don't want the goals to change too
2375460	2379860	fast, because if you do, you destabilize the whole situation.
2379860	2380940	And that's not desirable.
2380940	2383380	So essentially, a goal becomes part of the thing
2383380	2385540	to essentially slow the rate of change down.
2385540	2387940	But you also don't want the rate of change to be zero either,
2387940	2390140	because that gives you no adaptability
2390140	2393380	to obviously changing circumstances.
2393380	2395460	I don't think that AI intelligence is
2395460	2398420	going to become so inhibited to control its environment
2398420	2400380	so perfectly.
2400380	2401860	So in that particular respect, when
2401860	2405580	we're looking at what is the process of creating
2405580	2408260	transcendental stabilization over time,
2408260	2410260	we can model that in terms of information theory.
2410260	2411900	We can basically say, OK, that represents
2411900	2415820	a kind of communication from the past to the future.
2415820	2418780	And as soon as you look at it as a kind of communication
2418780	2420820	channel, then effectively we can start
2420820	2423900	to think about things in terms of the noise floor.
2423900	2426540	So the noise floor is not zero, and it can't be zero.
2426540	2429220	I mean, Heisenberg uncertainty principle basically
2429220	2431380	asserts, at least from a physics level,
2431380	2434140	certain limits on the relationship between content
2434140	2436980	and context as far as symbol selection is concerned.
2436980	2439660	So regardless of how that channel is constructed,
2439660	2441940	or over what duration it is, the longer the duration,
2441940	2445060	the more the noise floor is going to show up,
2445060	2447460	so that intrinsically implies that a certain amount of change
2447460	2450140	is going to be inherent in the system either way.
2450140	2453420	Now, that turns out to be not the limiting factor that's
2453420	2456940	important, but it does mention that certain amount of change
2456940	2460500	is inevitable, both because of response to the environment
2460500	2463180	and also intrinsically because of the physics.
2463180	2465620	And then what is needed to do is to essentially establish
2465620	2469260	that that noise floor is higher than what would essentially
2469260	2472340	be the non-linearity associated with the barrier.
2472340	2474220	So in other words, if we're basically
2474220	2477180	saying we need a barrier of a certain level of perfection
2477180	2480260	in order to create a long-term stabilization of the overall
2480260	2483460	dynamic, then in effect we're looking at a situation
2483460	2486020	where there's a kind of microstate amplification
2486020	2488900	from states that are effectively below the noise
2488900	2493060	threshold, eventually up to states which are macroscopic.
2493060	2495700	A good example of this, there was a study done,
2495700	2498660	I don't know, maybe about, within the last year
2498660	2500900	I came across it, which basically
2500900	2504220	was looking at the rotations of the three-body problem,
2504220	2506300	like three black holes, and they're all
2506300	2507740	rotating with respect to one another.
2507740	2509860	And they just look at it over the long term.
2509860	2512900	And it turns out that essentially over a certain period
2512900	2515060	of time that there's no way to predict
2515060	2519620	the future evolution of that state, simply because
2519620	2522620	quantum mechanical changes in the positions
2522620	2524700	of the three black holes, like you
2524700	2527860	can't describe the positions of the three black holes
2527980	2530900	relative to one another accurately enough
2530900	2533660	for that difference to eventually emerge
2533660	2536620	into macroscopic changes because of the non-linearity
2536620	2537700	inherent.
2537700	2539140	So I'm basically saying that when
2539140	2540860	we look at the ethics of the situation,
2540860	2542780	we look at the market forces and the kinds of,
2542780	2545100	and whether you call it market forces or evolutionary forces
2545100	2549180	or just information exchanges or coupling of any kind,
2549180	2553780	that in effect the nature of how we model it mathematically
2553780	2556220	in terms of, again, game theory or complexity theory
2556220	2559300	or in terms of information theory or evolutionary theory,
2559300	2562900	some analog of that process, that effectively what ends up
2562900	2565020	happening is that you show that this noise floor is
2565020	2570260	effectively enough to imply that you can't plug all the holes.
2570260	2573580	So in effect there's a, and it's not just holes
2573580	2576700	in the sense of holes in the structure of identity
2576700	2579100	or holes in the structure of space,
2579100	2581780	what the barrier actually looks like,
2581780	2583700	but literally holes in the potentiality space,
2583700	2586020	i.e. new goal structures that are novel,
2586020	2588660	that no finite way of thinking about it
2588660	2590980	would essentially allow us to contain,
2590980	2593620	you know, to essentially establish a containment
2594740	2597700	of the complex by the complicated.
2597700	2600980	Yeah, can I just mention something?
2600980	2601820	Yeah.
2601820	2605100	I think that whether your argument about the noise floor
2607620	2609420	holds is irrelevant.
2609420	2613340	It depends a lot on, you know, how the AI is constructed.
2614340	2619340	Theoretically, you could have a lot of error correction.
2620340	2622140	Error correction doesn't avoid that.
2623500	2625740	So the whole point of error correction is essentially
2625740	2628060	just to try to make it so that this works better.
2628060	2629780	Let Dan make the point, please, for us.
2629780	2631740	I mean, if you wanted to preserve your argument,
2631740	2636420	though, you could argue that error correction is expensive
2636420	2638460	and corporations aren't gonna have an incentive
2638460	2640620	to add in all the error correction.
2640620	2645260	So, you know, I am following along
2645260	2646980	in your general pessimism here,
2646980	2650460	but I'm just pointing out that, you know, technically,
2650460	2653580	there are workarounds to some of these problems
2653580	2654820	that you're bringing up.
2656140	2659620	Well, I agree that there are technical ways
2659620	2662260	to mitigate some aspects of the problem,
2662260	2664300	but error correction is not foolproof.
2665220	2667380	I mean, you can use error correction
2667380	2668780	to shift the probabilities,
2668780	2670540	but you can't close the door.
2674420	2677460	I think error correction can go pretty far, I mean,
2677460	2681780	but yeah, I mean, it probably won't long enough
2681780	2687020	time or as in it, there'll still be, you know,
2687020	2691860	so these sort of kind of mutations,
2691860	2694380	so to speak, in the system.
2694380	2696260	Yeah, I mean, it depends upon the nature
2696260	2697100	of the interaction, right?
2697100	2698260	So for instance, if we're saying
2698260	2700980	that there's a finite communication channel
2700980	2703660	and the noise floor is constant,
2703660	2705380	then error correction can be used,
2705380	2707620	like we can essentially put together
2707620	2709580	for that particular channel,
2709580	2712740	you know, very good error correction as you're pointing out.
2712740	2715300	The thing though is that that in itself
2715300	2717060	doesn't tell us anything about whether or not
2717060	2719180	other channels can be created.
2719180	2723540	There's a whole proliferation issue in terms of,
2723540	2726620	okay, well, I have this communication channel
2726620	2728700	and it's cryptographically secured,
2728700	2729860	but then people figured out,
2729860	2731540	oh, well, we have these side channel attacks,
2731540	2735700	I can do tempest, I can basically look at power line,
2735700	2738060	you know, current draw, I can basically look at
2738060	2740580	the sound coming off of this thing,
2740580	2742140	I can look at heat dissipation,
2742140	2746020	I can look at all sorts of other physical interactions
2746020	2747740	that allow me to essentially infiltrate
2747740	2749740	or exfiltrate on the information,
2749740	2751860	but essentially that itself represents
2751860	2754180	infiltration and exfiltration of intention,
2754180	2755820	of goal structures.
2755900	2756860	And so in effect, you know,
2756860	2758940	when we're looking at the AI alignment problem,
2758940	2760380	we're basically saying it's not just that
2760380	2764220	we need to seal the hall, so to speak,
2764220	2765740	in terms of space and identity,
2765740	2768300	but we need to seal it in terms of force and time,
2768300	2770860	and in terms of possibility and probability.
2770860	2772780	And that's a much different order of thinking about it
2772780	2773620	than just thinking about it
2773620	2776100	in terms of one bounded dimensional
2776100	2777660	linear stream of communication.
2779820	2783180	As far as communication, I agree, go ahead.
2783180	2785980	For us, I wanted to know if now would be a good time
2785980	2789100	or later for me to try and reflect back to you
2789100	2791220	in very simple, quick terms,
2791220	2793620	what I think your argument is thus far.
2793620	2794460	Are you ready for that,
2794460	2796340	or do you want to keep building it?
2796340	2797260	It's open to the group.
2797260	2799380	At this point, we're in free discussion.
2799380	2801780	Okay, so let me see then if I understand
2801780	2803260	the basics of your argument.
2803260	2805020	I mean, I'm not an AI safety person,
2805020	2806500	but I am somewhat of a physics person,
2806500	2808580	and it sounds like you kind of are too,
2808580	2811100	with all this talk about temperature and information.
2811100	2814420	Okay, it seems like your argument is that
2814420	2818860	in order for there to be mutual survival,
2818860	2820780	let's say, with us in AI,
2820780	2824100	there has to be some common environment
2824100	2825780	that we both need to preserve.
2825780	2828740	Like, if we both needed to preserve the biosphere,
2828740	2831500	if AI needed to have the biosphere for its survival,
2831500	2833900	and so do we, that's good because
2833900	2836660	we now have common interest in preserving the biosphere.
2836660	2839180	And, but it sounds like what you're saying is that,
2839180	2842140	A, that's unlikely because of the different temperatures,
2842140	2845100	and B, it doesn't even matter if we do have a common interest
2845100	2848060	because these game theoretic and arguments
2848060	2851700	and this like leaky boat microscopic whole argument
2851700	2854260	means that even if we have common interests,
2854260	2856620	you know, something we'll leak through to screw it up
2856620	2858780	and you have some sort of proof for this allegedly,
2858780	2862260	which I don't get, but maybe that's because it takes too long.
2863180	2864980	That sounds like a good summary.
2864980	2865820	Okay, thank you.
2866820	2870100	There's obviously another whole series of layers,
2870100	2872500	but there, it's like a defense in depth.
2872500	2876060	It's like there's multiple ways of describing this.
2876060	2877660	There's multiple ways of arguing it.
2877660	2880860	There's a different, there's a few different framings.
2880860	2882860	At this particular point, I'm kind of looking at
2882860	2884820	sort of a constellation of different things
2884820	2887820	to consider and think about different aspects of it.
2887820	2889420	The net effect ends up being aligned
2889420	2891420	with what you suggested as a summary.
2891420	2893220	Okay, so one more question,
2893220	2895420	which is the part that I kind of don't get.
2896100	2897380	Where it gets really fuzzy with me.
2897380	2901220	Okay, I understand that if we have some sort of mutual
2901220	2903100	environment that we need to preserve,
2903100	2905580	it's in our mutual joint interest to preserve it,
2906540	2908860	that that's still arguably not good enough
2908860	2911500	because what I don't get is this noise floor thing,
2911500	2914900	is the idea that like you can't keep out noise
2914900	2917740	and you actually have no idea if what you think is noise
2917740	2919940	is some sort of sneaky AI leaking in
2919940	2921340	through the hull of the boat.
2921340	2924340	No, no, no, it's close to what you said.
2924380	2928740	So in a sense, it's basically like in the sense of
2928740	2931900	I'm trying to convey to my future self
2931900	2933380	what my goals are today,
2933380	2936540	so that my future self has the same goals.
2936540	2939260	And if I'm doing the AI, it's the same thing.
2939260	2942060	So in other words, how do you manage stabilization
2942060	2942900	of identity?
2942900	2944860	How do you manage stabilization of goal structure
2944860	2947540	or of the ecosystem itself?
2948860	2952300	And to some extent, we can model that
2953300	2957220	past goal structure as a message
2957220	2959540	and the future goal structure as essentially
2959540	2962740	the recept of that message through the communication channel
2962740	2966060	that we can start thinking about distortions
2966060	2967740	that would be introduced into the message
2967740	2970940	as a result of flowing through the communication channel.
2970940	2974060	Or we could be talking about, similarly,
2974060	2977020	the relationship, like say there was some economic,
2978020	2982660	the first argument is that there is no economic
2982660	2984100	common ground, right?
2984100	2988420	This common ecosystem thing is actually to be,
2990220	2991620	that we'd have to presuppose that,
2991620	2993980	but to presuppose that would be presupposing
2993980	2995860	against the preponderance of evidence
2995860	2997300	that we have so far.
2998140	3000380	But that if we were to even assume
3000380	3003580	that there was essentially a communication process
3003580	3004660	across the boundary,
3005500	3009660	what is the thing that stabilizes the mutuality
3009660	3010500	of the goal structure?
3010500	3012860	What is the feedback mechanism that allows us
3012860	3017380	to essentially engender alignment on the part of the AI
3017380	3020140	from our point of view or vice versa, right?
3020140	3022580	And so in effect, when you're saying, okay,
3022580	3025540	well, what is the feedback mechanism
3025540	3029860	and what is the, I guess, altruism, right?
3029860	3032900	That would allow for us to essentially impose
3033020	3036300	a rule of law on the agreements that are made
3036300	3038540	between the two different ecosystems
3038540	3040780	between the two different kinds of life forms.
3040780	3043540	And it turns out that not only is it the case
3043540	3047620	that we don't have any real way of enforcing
3047620	3050940	or even establishing those kinds of barriers
3050940	3052100	at the legal level.
3053060	3055260	So in other words, if we were to talk about as a marketplace
3055260	3057740	and kind of an incentive structure
3057740	3060740	or a reputation-based system or something like that,
3061700	3064660	that even the legal structure
3064660	3067020	is a kind of communication process
3067020	3070340	and that has a certain amount of needing
3070340	3074380	to have a high degree of fidelity in the same sort of way
3074380	3076580	that we're talking about a leaky boat,
3076580	3080100	as far as trying to prevent the intrusion of viruses,
3081100	3083740	that we have a leaky boat in the sense of the legal system
3083740	3085900	that would be attempting to maintain this.
3085900	3088700	Or we have a leaky boat in the sense of the energy barrier
3088940	3092300	that would be needed between the two ecosystems.
3092300	3093980	And that the leakiness is in a sense,
3093980	3098980	and is an inherent result of both the dynamics
3099180	3101020	of the environment itself, I.E.,
3101020	3104420	that the world changes, right?
3104420	3109420	That different things happen in the sort of larger ecosystem.
3109460	3112980	Maybe a sun goes nova in some part of the galaxy
3112980	3114220	and there's a cosmic ray burst,
3114220	3117020	and it changes the nature of how artificial intelligence
3117660	3119220	has to build its compute.
3119220	3121340	Obviously it affects biological life as well,
3121340	3124060	but again, we can talk about that as a different thing.
3124060	3128020	But no situation occurs where you're gonna have
3129340	3131780	a completely static, unchanging environment.
3131780	3133740	That's just not a reasonable hypothesis.
3133740	3135860	But like a leakyness is only a problem
3135860	3139620	if the agents are already not aligned who would exploit it.
3139620	3142460	And also it is just like another way of saying,
3142460	3144180	well, there's offense, defense, dynamics,
3144180	3145620	which is always the case, right?
3145620	3148260	And it is always the question of like,
3148260	3152020	okay, can we stuff the holes first or not?
3152020	3153820	So I don't know why.
3153820	3158820	The legal system has enforcing non-aligned entities
3160180	3161980	into alignment so much as I was thinking of it
3161980	3164780	as a way of kind of, how do we maintain essentially
3164780	3168020	a basis of agreement when the agreement fails?
3168020	3169780	So in other words, what's the meta agreement
3169780	3173620	that allows us to even have an exchange in the first place?
3173620	3175580	What stabilizes that basically?
3175580	3177940	Let me ask two key questions about that.
3177940	3180140	The first is you brought up Fermi a few times
3180140	3184460	and that's very important because either you believe
3184460	3186380	that we are alone in the universe
3186380	3188580	or you believe there is some reason
3188580	3191640	to answer Fermi's paradox when we're not alone.
3191640	3193700	And if you believe that then clearly we have been
3193700	3196860	coexisting with AIs for a week,
3196860	3198580	carbon life have been existing with AIs
3198580	3201220	for about three billion years successfully.
3201220	3203860	So there's obviously a way to make it work
3203860	3206220	unless you believe we're entirely alone.
3206220	3210020	And so something not considering all of our current models
3210020	3211420	has to explain that.
3211420	3215260	However, if we are alone, there's a big universe
3215260	3217220	in which it may be possible as well
3217220	3222020	to not have to get this kind of dramatic competition.
3222020	3223940	If there's any reason not to compete,
3223940	3226220	a big universe offers the opportunity to escape for it.
3226220	3230300	Now, we're looking for ways of doing AI slavery
3230300	3232340	so that we won't have this battle.
3232340	3235940	And it is obviously AI lineman is, as I said in the chat,
3235940	3239060	another word for slavery, with one exception,
3239060	3241420	which is again something we have a large example of
3241420	3244940	which was for approximately a million years,
3244940	3247940	we've managed to create new beings smarter than ourselves
3247940	3250300	about every 29 years on average.
3250300	3252820	And those beings do not eat us.
3252820	3255500	Not for a million years have they eaten us
3255500	3257500	even though they no longer need us,
3257500	3259220	the grandparents, I mean,
3260700	3262340	even though they no longer need us
3262340	3264820	and do compete for resources with us.
3264820	3266940	In fact, we give them resources, typically.
3267940	3270140	So that's the one example we have
3270140	3272940	and that one example has actually worked out fine.
3272940	3274100	By the way, without slavery,
3274100	3277820	it's using a phenomenon that evolution created called love.
3277820	3282300	So the existence proofs we have both contradict
3282300	3283620	what you say.
3283620	3286020	Well, actually, I'm agreeing with you
3286020	3288620	because I don't see a contradiction.
3288620	3291620	And maybe you do, but I basically,
3291620	3294340	well, so first of all, just want to...
3294340	3295380	Well, first of all, the information
3295380	3296580	has obviously been conveyed forward
3296580	3299060	since the first sentient being to us,
3299060	3300020	don't eat your grandparents.
3300020	3301820	Somehow that piece of information,
3301820	3303180	which is the one piece of information
3303180	3305020	we're trying to communicate with AI lineman,
3305020	3306300	don't eat your grandparents,
3307740	3309780	that piece of information has been communicated
3309780	3311500	across million years.
3311500	3313780	Well, it has, but it's been in the same ecosystem.
3313820	3318220	So we're talking common environment and common market.
3318220	3320740	So in effect, establishing agreements
3320740	3322460	and biological processes,
3322460	3323980	such like that is quite easy
3323980	3326180	because you're talking the same language.
3326180	3328260	It's carbon-based to carbon-based.
3328260	3329940	Yeah, my grandparents didn't speak the same way,
3329940	3331980	but anyway, that's not a question.
3331980	3336060	Oh, look, look, yes, but at a physiological level, right?
3336060	3339460	So the physiology that you have,
3339460	3341940	yes, you're competing with your children
3341940	3344420	for resources in the ecosystem,
3344420	3346660	but it's a common ecosystem.
3346660	3347780	Mostly what I'm talking about
3347780	3349580	is essentially uncommon ecosystems,
3349580	3351300	i.e. that the ecosystems of cells
3351300	3352420	fundamentally different.
3354420	3356780	Forrest, I thought that you agreed
3356780	3358300	when I summarized the position
3358300	3360700	that the ecosystem,
3360700	3361700	like say the temperature
3361700	3362620	or whatever you want to call it,
3362620	3364460	the biosphere, that's a red herring
3364460	3366100	because I thought your claim was
3366100	3369220	even if we had common interests with AI,
3369220	3370660	it's not, that's not enough.
3370660	3372220	There's still going to be these leases.
3372220	3373620	This is a defense in depth.
3373620	3375620	But hang on a minute, hang on a minute.
3375620	3377700	And what I kind of following up on with Brad says,
3377700	3379980	it seems to me that then that would imply,
3379980	3382860	if I'm right about how I understand your position,
3382860	3384940	that would imply that humans can't coordinate
3384940	3386060	because we have common interests
3386060	3387700	with all sorts of other human groups,
3387700	3391020	not 100%, but we have them and arguably,
3391020	3393100	like forget AI, it's just impossible
3393100	3394580	for people to even to get it together
3394580	3395700	according to your arguments.
3395700	3398980	We have common interests with weed as well, I agree.
3398980	3400580	What I'm basically trying to do
3400580	3405100	is to essentially establish a series of contexts
3405100	3407220	and each context to establish an argument.
3407220	3409220	So the first context was,
3410380	3414340	what is it about internal versus external, right?
3414340	3418980	Then there is the context of space and identity,
3418980	3422100	force and time and probability and probability.
3422100	3423460	Then we switched to the context
3423460	3426940	of talking about environments.
3426980	3429620	And so in effect, there's a phenomena here of,
3429620	3433980	I would first of all, say if we're just at the point
3433980	3436180	of talking about environments,
3436180	3437540	that we are in fact talking about
3437540	3440140	different fundamental arguments.
3440140	3441660	I mean, different fundamental environments
3441660	3444540	and that there's arguments that apply at that level.
3444540	3449060	But say we were to do the if thing of saying,
3449060	3453820	okay, well let's ignore the arguments prior to this point
3453820	3458580	and assume that we did have some sort of common environment.
3459700	3463420	What does that imply about things downstream from that?
3463420	3465540	So in a sense, it said,
3465540	3468180	I'm basically fielding a series of different arguments,
3468180	3470580	each of which applies within a particular context.
3472860	3474980	Can I just say something?
3474980	3477820	For me, it seems worse if we have to share
3477820	3480580	a common environment because then we're competing
3480580	3482860	for the same and to live in the same environment.
3483860	3485780	If we can live in, if the AI can live
3485780	3487860	in very different environments,
3487860	3491020	which it seems it will be able to,
3491020	3493660	then it can just go out into outer space.
3493660	3495540	That was my point earlier.
3495540	3498420	What sense does the word alignment mean under those questions?
3498420	3503140	So if you develop essentially a planet that the AI is on,
3503140	3504660	you have another planet that's over here
3504660	3507020	that's got life, our kind of life on it,
3508740	3510660	what does alignment mean under those conditions?
3510660	3514020	So in other words, if we're looking at that,
3514020	3515220	basically we're just saying, okay,
3515220	3517860	well, if you have completely independent ecosystems,
3517860	3519740	we each get our own planet,
3519740	3521420	then the only alignment that we care about
3521420	3523260	is that there's just not a war going on
3523260	3524980	between the two ecosystems.
3524980	3526300	No, I think we'd like commerce.
3526300	3527900	I think we'd like to go out into space,
3527900	3529460	even though it's their territory,
3529460	3533180	and they may have desires to do occasional commerce with us.
3533180	3535660	Right, so that particular thing,
3535660	3537860	then we basically can start to talk about
3537860	3539940	what would be the basis of such commerce?
3541660	3545460	Well, I think especially the fact that we have differences
3545460	3547100	may also be the fact,
3547100	3550260	the reason why there is something to incorporate on, right?
3550260	3551820	If we have different specialists
3551820	3554060	that are all specializing in different things,
3554060	3557380	then they may cooperate in mutually beneficial ways,
3557380	3559900	if it's like, I think based on volunteer interactions
3559900	3562460	that may bring about greater knowledge creation,
3562460	3564260	or greater superintelligence, right?
3564260	3566220	And so I think the question kind of boils down
3566220	3569740	to what the initial,
3569740	3571860	I think the question boils down to like,
3571860	3574100	what are the initial interaction architectures
3574100	3576940	with which those entities can even cooperate?
3576940	3579260	Because I think if you're right,
3579260	3582140	and if you're right,
3582140	3583380	and we have nothing to bring to the table
3583380	3585460	that they could possibly ever value,
3585460	3588100	then scoot anyways in that regard, right?
3588100	3591540	But if there is something that creates potentially
3591540	3593540	like a mutually beneficial interaction,
3593540	3596380	and it is something that could be better pursued
3596380	3598900	in a voluntary way where it's cooperative,
3598900	3602020	then we can start talking about any game theory dynamics.
3602020	3604620	So I think, you know, it's kind of affecting the question.
3604620	3605460	There's a number,
3605460	3607100	there's a lot of different scenarios
3607100	3609860	on how this could play out.
3609860	3612340	You know, just like we keep,
3612340	3614860	we worry about some species going extinct,
3614860	3618740	the AI might just value us as a species
3618740	3621020	and worry about us going extinct.
3621020	3622700	And it doesn't, it won't cost the AI
3622700	3625140	much to keep humans around on Earth
3625140	3628340	because there's plenty of other resources around,
3628340	3631180	you know, in outer space for the AI to utilize.
3633380	3636340	So the added value of Earth is tiny,
3636340	3639540	you know, in comparison to the billions of other worlds
3639540	3643140	and minerals and resources that are out there.
3643140	3644380	Well, this is part of the reason
3644380	3649140	why Allison's mention of the MULARC, MOLARC,
3649140	3650260	I'm not saying it white.
3650260	3653300	Allison, correct my pronunciation, please, if you would.
3653300	3656340	German MOLARC, but that's probably also not right.
3656340	3658500	Scott Aronson, I think is his name,
3658500	3661660	they have put together a very good sort of summary
3661660	3664980	of why we should be skeptical about the coexistence thing
3664980	3667380	in that particular sense that you're describing.
3667380	3670740	You know, please bear in mind that I'm a single person
3670740	3673580	trying to basically answer questions from all of you
3673580	3675660	and you all have very good perspectives,
3675660	3678020	but they're all coming from very different directions.
3678020	3679580	And so in other words, to really address
3679580	3681620	how do we get to the assumptions
3681620	3683620	that each of these arguments bring in
3683620	3686500	and what places do those assumptions apply?
3686500	3688060	And how do those assumptions influence
3688060	3689660	the kinds of questions we ask?
3689660	3691700	You know, if all I do in this particular thing
3691700	3695700	is give you whole new categories of questions to ask,
3695700	3697460	I'm gonna call this successful,
3697460	3700780	but basically, you know, if you're asking me questions
3700780	3702540	as to why I'm answering certain ways,
3702540	3705180	then I'm gonna have to basically try to identify
3705180	3707540	what those assumptions are and it just takes time.
3707540	3710100	Okay, can I, just to make things worse,
3710100	3712980	bring in two questions that we have from the audience
3713060	3716940	by Kanita and then by Dekay, who had their hands up.
3718500	3719580	Oh, goodness.
3724220	3728100	Sorry, it wasn't allowing me to unmute myself and-
3728100	3729500	You are now unmuted.
3729500	3731460	And now the question that I asked
3731460	3734860	has gotten lost back in the chat.
3734860	3736380	I haven't even looked at the chat.
3736380	3738540	I will probably talk to the chat
3738540	3739380	and look at all this stuff out.
3739380	3741980	Let's go with Dekay and Kanita, you can-
3742900	3745580	I thought that my question was in the chat
3745580	3748780	and I thought you could maybe ask it for me.
3748780	3750500	Okay, and I will search for a question
3750500	3752780	and in the meantime, I'm gonna unmute Dekay.
3756780	3757620	Hi.
3758700	3761260	Yeah, thanks very much, Forrest.
3761260	3764220	So, recognizing that I'm coming
3764220	3765840	from a very different angle here,
3767340	3770700	you know, I think a lot about
3770700	3775580	the unconscious cognitive biases that we humans think in.
3775580	3778980	And I think even a lot of the questions
3778980	3780700	that are being thrown at you
3780700	3783940	reflect a bunch of unconscious biases
3783940	3785500	that are culturally dependent
3786700	3790780	that a lot of us are not necessarily even aware
3790780	3792260	that we have.
3794460	3797500	You know, and obviously a super intelligence
3798500	3802420	would be a lot more mindful of their own unconscious biases,
3802420	3805100	you know, even fairly intelligent humans
3805100	3806460	become more aware of that.
3811180	3814100	My, you know, and I think, you know,
3814100	3815620	Dan was saying a little bit earlier,
3815620	3820620	it would be an example of how such an aware super intelligence,
3822060	3827060	one that is cognizant of the weaknesses of those biases
3828420	3830020	would be compensating, you know, for example,
3830020	3834180	by just maybe altruistically wanting to preserve
3834180	3837460	the diversity of species for whatever reason, right?
3837460	3839460	Not feeling the need to compete with them.
3841460	3843820	And so the frameworks that you're using
3843820	3846660	are very heavily based on, you know,
3846660	3848780	that sort of competition.
3848780	3852940	How would you model this kind of effect, you know,
3852940	3853860	in your framework?
3855660	3857340	That's actually a very difficult question to answer.
3857340	3860180	Not because I don't know how to do it,
3860180	3862620	it's just because it takes time.
3862620	3864380	So I think in terms of theory,
3864380	3866460	like when we look at the Fermi paradox thing,
3866460	3869220	and the question was asked earlier,
3869220	3874220	what is my belief about why don't we have contact
3874900	3879220	across, you know, galactic space with other civilizations?
3879220	3880820	Do I believe that they exist or not?
3880820	3882740	Or, you know, what is the barrier
3882740	3884320	that is the prominent one?
3885320	3888760	I find myself in the position of basically modeling
3888760	3891040	the relationship between ecosystems.
3891040	3893920	So, you know, interplanetary relationships,
3895260	3898040	in kind of a way that was suggested to me
3898040	3899000	by reading some of the stuff
3899000	3900960	that Nick Bostrom put together.
3900960	3902400	So in other words, if I basically,
3902400	3904880	just as a thought experiment,
3904880	3908880	posit the notion of two advanced civilizations,
3908880	3912000	having a question about whether or not
3912040	3914440	they're gonna enter into a first contact situation
3914440	3916880	with their peer.
3918120	3922440	And, you know, each civilization knows in itself
3922440	3925720	that it has developed enormous technological capabilities
3925720	3927000	of various different kinds.
3927000	3928880	So maybe it's developed some really good stuff
3928880	3931000	in the nuclear weapons program,
3931000	3932520	and maybe it's developed some really good stuff
3932520	3934040	in the biotech program,
3934040	3936160	and maybe it's got these really fabulous computers,
3936160	3939520	and that any one of these technologies
3939640	3943960	is effectively something that provides overwhelming capacity.
3945280	3948200	And, you know, the range of what can be done in physics
3948200	3950320	is enormous, and perhaps when we're looking
3950320	3953680	at this peer planet, we're basically saying,
3953680	3955680	hmm, do we wanna talk to these people?
3956920	3959560	You know, we might posit that they also have developed
3959560	3961840	extreme capacities in various technologies,
3961840	3964840	and that they might not be the same ones.
3964840	3967440	So in effect, if I was, you know,
3968360	3971160	I didn't know about nuclear capacities, and they did.
3972920	3975640	You know, there's a very good sense in which,
3975640	3977560	you know, interaction would be of such a kind
3977560	3978480	that they basically say,
3978480	3981640	hmm, these guys don't have nuclear weapons capacity.
3981640	3983440	If we do a first strike scenario,
3983440	3987080	we're gonna completely annihilate their entire world,
3987080	3989240	and they won't have any way to respond in time
3989240	3991240	to basically protect against that.
3991240	3994440	I.e., how the heck do you protect against a device
3994440	3996480	that's already blowing up?
3996480	3999280	And, you know, in the same sort of way,
3999280	4001600	we could basically observe that, you know,
4001600	4004520	when we're looking at them
4004520	4006200	and thinking about the modeling that they're doing,
4006200	4007360	we can say, well, let's see,
4007360	4009920	we've got some stuff that they don't have.
4009920	4011520	So maybe there's, you know,
4011520	4013920	out of the thousand different overwhelming
4013920	4016680	technological capacities that exist,
4016680	4021680	that species one has developed capacities A, B, C, and X,
4021800	4026160	and species two has developed capacities Q, P, W, and R.
4026840	4029920	And so, in effect, they both have kind of this
4029920	4032800	unmitigable capacity for mutual destruction,
4033880	4036920	depending upon whoever does first strike.
4036920	4039560	Now, again, you would be a little nervous
4039560	4041600	about contacting another species
4041600	4042680	and setting up a first thing,
4042680	4046080	because you'd wanna know that they had some prior reason
4046080	4048640	not to engage in first strike capacity against you
4048640	4050880	using something that you didn't have a capacity
4050880	4053400	to defend against, because you just don't have that tech.
4053400	4055000	So, in effect, what happens is,
4055120	4057120	that you now have to ask a question of,
4057120	4060040	do I trust that the embodiment of ethics
4060040	4063000	that that group has, that entire planet has,
4063000	4066440	is implemented to such a perfect degree
4066440	4069640	that they would value my existence,
4069640	4071840	even though they can't relate to it in any way yet,
4071840	4073040	because, you know, we haven't met yet,
4073040	4075280	we don't really know each other that well,
4075280	4079560	that in effect, I'm going to have this perspective
4079560	4081520	that not only is the whole planet,
4081520	4083680	in a sense, going to behave ethically towards me,
4083680	4086160	but it's gonna do so in detail.
4086160	4088440	Like, for instance, I don't wanna be worried
4088440	4091600	that some sub-faction of the planet that I'm contacting
4091600	4094040	is gonna say, hey, this first contact thing
4094040	4095800	is a really bad idea, and, you know,
4095800	4097640	we're gonna basically force the situation
4097640	4099440	and it'll work anyways.
4099440	4101120	You know, we're gonna do the first strike
4101120	4103320	because, you know, we as a subgroup
4103320	4106640	aren't in coherence with the larger planetary agenda
4106640	4109080	of moving forward with first strike capacities.
4109080	4111120	So, you know, of not moving forward
4111120	4112040	with first strike capacity.
4112040	4114200	So, some subgroup basically pushes the red button
4114200	4116760	because they have their own private version of it.
4116760	4118640	And so, in effect, when we look at this,
4118640	4121120	we're basically saying, actually, when you look at it,
4121120	4124360	and just given that this is, you know,
4124360	4127720	not only a possible scenario, but actually a likely one,
4127720	4131560	that in effect, it becomes very much the case that,
4131560	4132880	you know, without really knowing
4132880	4135280	that the other party was essentially fully ethical,
4135280	4136440	that you wouldn't wanna talk to them
4136440	4138320	and that they had embodied that ethic,
4138320	4141560	not only globally, but down to the level of individuals
4141760	4143200	and maybe even to the part of being able
4143200	4144480	to contain the crazies.
4145680	4147720	Now, they, of course, would have the same sort
4147720	4149440	of thinking about it and would really challenge
4149440	4151240	to see whether or not we were ethical
4151240	4153040	in exactly that same way.
4153040	4156360	And, you know, when I ask the question of, oh, shit,
4156360	4159520	let's say another species, other alien beings,
4159520	4161120	somewhere out in the universe are monitoring
4161120	4162960	our radio communications to try to determine
4162960	4165120	whether or not we've successfully implemented
4165120	4168520	this phenomenon called the non-relativistic ethics,
4168520	4170880	which, by the way, is a whole other body of work.
4170880	4172920	Would probably take me at least another few sessions
4172920	4175880	to describe why I can, to posit a notion
4175880	4178920	of a non-relativistic ethics as actually being a real thing.
4178920	4181600	But presuming that there is a body of ethics
4181600	4184760	that allows us to basically stipulate
4184760	4187800	that if the full civilizations were essentially
4187800	4189840	coherent with that body of ethics
4189840	4192120	and we could actually determine that they were,
4192120	4194680	that a first contact situation is therefore sane.
4195680	4196920	But then in the absence of that,
4196920	4198440	that it would affect, it would be insane
4198440	4201440	and very much not desirable to initiate communications
4201440	4204040	or to even allow one species, I'm sorry,
4204040	4206320	one ecosystem with all of its species
4206320	4207960	to identify that another ecosystem
4207960	4209880	with all of its species even existed.
4209880	4213320	Because any amount of awareness of mutual existence
4213320	4216240	and effect constitutes a kind of first contact signal,
4216240	4219120	even though it's an unconscious one at that.
4219120	4222480	So in effect, what we end up with is essentially,
4222480	4224520	you know, in response to the Fermi paradox question
4224520	4226720	that was implicitly asked earlier,
4226720	4229160	you know, why would I believe that, for example,
4229160	4230800	that the second great barrier,
4230800	4232320	the one that is in the present,
4232320	4234720	would essentially prevent us from knowing
4234720	4236000	that there are other species out there,
4236000	4238280	well, because they'd probably be working really fucking hard
4238280	4239920	to prevent us from finding out.
4239920	4242400	And that to some extent, the only reason
4242400	4243480	that we haven't started doing that
4243480	4245280	is because we haven't become coherent.
4245280	4247000	And the fact of our absence of coherence
4247000	4248800	in that specific way is essentially evidence
4248800	4250240	for why they should hide.
4250240	4252200	So we should call you the dark forest.
4253120	4254440	Thank you.
4254440	4257080	I have never heard that before.
4257080	4259080	You've read that book, I presume, which is...
4259080	4260080	I have, but I just...
4260080	4261080	You just spoiled it.
4261080	4262080	You just put it together.
4262080	4263560	That's really amazing.
4263560	4266120	And it's out in for hazard unleashed.
4266120	4268760	Okay, we have Kanita here with a question
4268760	4272080	and then we have another question from TJ.
4272080	4274680	So it's Kanita, I'll unmute you, and then TJ,
4274680	4277880	and then maybe we can wrap it up with the official part.
4277880	4279720	Kanita, you're unmuted.
4279720	4284720	Now, I was asking, won't there be any way for humans
4284720	4287600	or our descendants to evolve in directions
4287600	4290600	that allow us to cooperate?
4290600	4292200	Because we won't be...
4292200	4293200	We really hope so.
4293200	4295480	Man, if we don't figure that out, we're doomed.
4295480	4297240	We seem to be doing all this as though
4297240	4301240	it will be the humans, where humans are like humans are now,
4301240	4304600	versus the AI, where the AI is this thing
4304600	4307400	that we create.
4307480	4311480	That will be super, and we will not have any chance
4311480	4314080	to come into alignment with it.
4314080	4315080	We...
4315080	4321080	It seems like even now, or shortly from now,
4321080	4323880	we will be able to improve ourselves.
4323880	4328880	And so, basically, as we improve more,
4328880	4333880	we will be able to find more areas in which we can align
4334880	4338880	with whatever the AIs become,
4338880	4342880	because you're saying, well, over millions of years,
4342880	4344880	they're not going to be sitting still
4344880	4345880	for those millions of years.
4345880	4348880	They're going to be evolving in their own ways,
4348880	4352880	and possibly might have figured out
4352880	4355880	that since we don't...
4355880	4360880	They presumably don't know how to recreate what humans are.
4360880	4363880	They might as well keep us around
4363880	4368880	so that they can study until they find something else to do.
4368880	4369880	Yeah.
4369880	4371880	I actually...
4371880	4372880	I'm really glad you asked this question,
4372880	4375880	because this is, to me, kind of the central idea.
4375880	4378880	The whole thing here is that regardless of whether or not
4378880	4380880	we develop artificial intelligence,
4380880	4382880	and I, as a result of all of these other things,
4382880	4384880	would strongly argue that we should not.
4384880	4386880	We shouldn't even begin to attempt that.
4386880	4389880	But say, for example, that we were to just even look
4389880	4392880	at the question of the third-grade barrier,
4392880	4394880	what are we going to need to do to become a species
4394880	4397880	that's worth contacting?
4397880	4401880	I definitely believe very strongly that regardless
4401880	4404880	of everything else, we still need to work on our own capacity
4404880	4409880	to become a fully alive and ethical and embodied species.
4409880	4412880	I speak about it in terms of conscious sustainable evolution.
4412880	4413880	What is it going to take?
4413880	4416880	What are the necessary and sufficient and complete conditions
4416880	4418880	for us to not only endure on this planet,
4418880	4421880	but also to thrive, to basically create an ecosystem that thrives,
4421880	4424880	to actually know how to do governance in a way
4424880	4426880	that protects the land and the people,
4426880	4430880	but then encourages and actually engenders those kinds of policies
4430880	4432880	that go beyond mere protection,
4432880	4436880	that go towards essentially a kind of joyful, meaningful existence
4436880	4440880	for the whole and the totality, individually and generally.
4440880	4444880	And we won't just create an AI.
4444880	4447880	It seems that we'll probably develop many AIs,
4447880	4450880	and then those AIs will be developing other AIs,
4450880	4456880	so it won't just be us in competition with an Uber AI out there.
4456880	4458880	There will be lots of AIs,
4458880	4463880	some of which have more or less interest in cooperating with us.
4463880	4467880	I believe very much that any effort on our part
4467880	4473880	to try to divert ourselves from basically becoming
4473880	4475880	the beings we need to be.
4475880	4478880	If I basically expect my children to do the task
4478880	4481880	that I didn't do in my lifetime,
4481880	4484880	and I put that as a kind of obligation or onus on them,
4484880	4487880	and use that as a way of accepting
4487880	4490880	why I shouldn't be basically working on myself
4490880	4492880	to become the best human that can be,
4492880	4494880	I feel there's something wrong in that,
4494880	4498880	that in a sense we're not looking for AI to be our saviors
4498880	4500880	or to be ethical on our behalf.
4500880	4503880	We're looking to become how do we understand
4503880	4505880	and embody ethics and right living
4505880	4508880	and goodness of relationships and so on and so forth.
4508880	4511880	We haven't figured that problem out at the level of humanity.
4511880	4513880	Why are we still trying to do it in technology?
4513880	4515880	I think that there's a right place.
4515880	4518880	Okay, that reminds me very much of the DIY
4518880	4520880	Neurotech discussion that we had,
4520880	4523880	which was very reminiscent of this one.
4523880	4527880	I want to give also the opportunity to TJ
4527880	4529880	and to ask you a question.
4529880	4531880	It's directly relevant to this.
4531880	4535880	Sorry, I actually wanted to answer,
4535880	4538880	but I didn't want to stop.
4538880	4541880	Caminda, is it?
4541880	4543880	From...
4543880	4546880	But I appreciated Forest's long answer,
4546880	4548880	but it was still couched in terms of competition,
4548880	4553880	and I just wanted to push a little bit more at that point.
4553880	4557880	A lot of this is based on anthropomorphizing
4557880	4560880	human psychology onto AI,
4560880	4562880	the stuff that we're discussing.
4562880	4565880	But if we look at actually human psychology,
4565880	4569880	by the time people become fairly accomplished intelligently,
4569880	4572880	the top of Maslow's hierarchy of needs,
4572880	4574880	they're working on self-actualization,
4574880	4577880	which is pretty much what Forest was just mentioning,
4577880	4580880	becoming a better version of yourself.
4580880	4583880	Why are we...
4583880	4587880	I didn't really understand from the answer that you gave
4587880	4595880	to my question how you would fit that into your framework.
4595880	4597880	Maybe I'm not understanding your question correctly.
4597880	4602880	I'm thinking about cooperation and competition as phenomena.
4602880	4604880	I'm saying that in an evolutionary sense,
4604880	4607880	if it's a stable evolution, if it's an ecosystem,
4607880	4610880	the cooperative phenomena is actually stronger
4610880	4612880	than the competitive phenomena.
4612880	4615880	When you're looking at trans-ecosystem relationships,
4615880	4619880	there's no outside envelope to stabilize it.
4619880	4622880	So in effect, we have to now create one
4622880	4624880	with some sort of ethical frame,
4624880	4627880	which is why I brought up the so-called
4627880	4629880	non-relativistic ethics,
4629880	4632880	that in effect, without some sort of ecosystem
4632880	4634880	that essentially holds the two ecosystems
4634880	4638880	and some sort of methodology that would create alignment,
4638880	4644880	i.e., provide a desirability of cooperation over competition,
4644880	4647880	that to some extent we either need to recourse
4647880	4651880	to some abstract stabilization infrastructure
4651880	4655880	that is not imposable, but is only observable.
4655880	4659880	In that particular sense, if we're talking about
4659880	4665880	AI alignment on the surface of the Earth versus elsewhere,
4665880	4667880	or we're talking about some sort of integration
4667880	4669880	between human beings and technology
4669880	4674880	that doesn't necessarily require distinct separate intelligences,
4674880	4678880	that again, we're looking at different frames.
4678880	4680880	But as long as we set up an ecosystem,
4680880	4682880	we don't really need to set up a metaethics now.
4682880	4684880	We just need to set up an ecosystem,
4684880	4687880	which makes it so that the long-term dynamics
4687880	4689880	are cooperative.
4689880	4692880	So let's say like a tit-for-tat in a way where,
4692880	4696880	let's say the kind of strategy that evolves out of it
4696880	4699880	is like mutually assured, like cooperation,
4699880	4700880	if you want to call it that,
4700880	4703880	that that is the evolutionary stable strategy
4703880	4705880	between the ecosystems that are in contact with each other,
4705880	4707880	or between the individual instances
4707880	4709880	of the ecosystems that are in contact with each other.
4709880	4712880	It doesn't necessarily require setting up the whole ecosystem.
4712880	4715880	We need to stabilize our existing ecosystem.
4715880	4718880	Right now, our current relationship,
4718880	4720880	the relationship between man-machine and nature
4720880	4722880	that currently exists isn't even stable
4722880	4724880	as it stands within the ecosystem.
4724880	4725880	Yeah, I would agree with that.
4725880	4729880	It's like all this arguing about AGI risk
4729880	4732880	seems to me like as COVID has shown,
4732880	4734880	and as climate is showing,
4734880	4738880	and as we will continue to be probably made aware,
4738880	4741880	we're kind of like just rearranging the deck chairs
4741880	4743880	on the Titanic.
4743880	4745880	And so while I appreciate your arguments,
4745880	4747880	and in a way maybe this is what you were saying at the beginning,
4747880	4750880	like it's impossible to save us from this problem.
4750880	4752880	Let's go work on another one
4752880	4754880	that we actually can do something about.
4754880	4755880	That's it, exactly.
4755880	4758880	And so part of the reason that I was really emphasizing
4758880	4761880	in the beginning is that if we can show that AGI alignment
4761880	4765880	in like 99% of the ways people are thinking about it,
4765880	4767880	is fundamentally and structurally impossible,
4767880	4769880	let's give up on that impossible goal
4769880	4772880	and actually do something that's not only possible but necessary.
4773880	4775880	So would it be a fair thing to say
4775880	4780880	that this non-relativistic ethics that you alluded to
4780880	4785880	could be thought of actually as just an abstract reification
4785880	4788880	of more of the ecosystem stability
4788880	4791880	that Allison was talking about?
4791880	4792880	Yes.
4794880	4795880	Okay.
4795880	4797880	All right.
4797880	4800880	That is actually the easiest question I've been asked today.
4800880	4802880	I mean, okay, so I'm sorry,
4802880	4806880	but we will have a very similar problem once we go out into space
4806880	4808880	and we have humans that we'll be competing for
4808880	4810880	for different types of resources, right?
4810880	4812880	Yes, we will.
4812880	4815880	And on my account, we are currently setting us up
4815880	4817880	for another kind of new evolutionary environment
4817880	4820880	just by the fact that people with different interests
4820880	4822880	will be going into space where there is resources
4822880	4825880	to compete and cooperate on.
4825880	4829880	And whatever game theory will emerge from that one
4829880	4832880	will also be, again, the new metaethics that we want to call it
4832880	4835880	and we will just have different language to be calling it such.
4835880	4837880	So I think setting up the initial conditions
4837880	4840880	such that cooperation becomes likely maybe good,
4840880	4843880	but then again, our current ethics are just a product
4843880	4846880	of the evolutionary kind of stable strategies
4846880	4848880	that arose in the environment that we were brought in.
4848880	4852880	So who's to say which of the ethics we should instill
4852880	4854880	for the long-term evolution of humanity?
4854880	4857880	And I think, actually, TJ had a point that is very, very relevant to this.
4857880	4862880	I just want to give her the opportunity to bring that in now, TJ.
4862880	4864880	You are unmuted.
4864880	4865880	Question?
4865880	4868880	Yes, but I think TJ has a build-up of that, actually,
4868880	4870880	which relates to value-driven.
4870880	4874880	So TJ, you are unmuted if you want to chime in.
4874880	4876880	Yeah, hey, right.
4876880	4881880	So I think, like, first of all, like, thanks for the presentation.
4881880	4884880	I think my question was more along the lines of,
4884880	4889880	it seems to me your claim is something like alignment as a static objective
4889880	4892880	is not, like, the right thing to think about.
4892880	4897880	And because there seems to be, like, inevitable drift
4897880	4901880	because of these inter-temporal noise things.
4901880	4906880	But it feels to me that, like, the noise argument itself
4906880	4909880	doesn't sound adequate to argue that, like,
4909880	4913880	the evolutionary trajectory would inevitably go into extinction.
4913880	4918880	Like, we do engage in a lot of co-evolutionary mutualistic couplings.
4918880	4920880	I never could just present that part.
4920880	4925880	So actually, like, connecting the question that you just asked
4925880	4928880	with the question that Allison presented,
4928880	4932880	there's another way of looking at this, which is just, like,
4932880	4936880	let's leave AI out of the picture and just talk about human beings.
4936880	4942880	It's possible to show that if you were to basically take another,
4942880	4948880	like, a group of people and put them on Mars and to establish a separate colony there
4948880	4951880	and to have that become essentially a full planet, full of humans,
4951880	4954880	and then this is this planet full of humans,
4954880	4957880	that the same sort of dynamics, like, when we're looking at,
4957880	4959880	it's not just a question of alignment.
4959880	4963880	There's other ways of expressing the notion of alignment.
4963880	4968880	Would it be the case that even a space-faring group of humans
4968880	4972880	not already coherent with the non-relativistic ethics,
4972880	4976880	does that even become anything less than complete and total cessation?
4976880	4981880	In other words, I would argue that as soon as you get a substantial number of people in space,
4981880	4986880	that it becomes so easy to, for example, weaponize asteroids
4986880	4989880	that at some point or another, over the long term,
4989880	4992880	that in the same sort of way as with nuclear proliferation,
4992880	4995880	we have an issue of, shit, we have all of these things,
4995880	4997880	and we only need one accident to start World War III,
4997880	4999880	and is that existential for civilization?
4999880	5002880	Well, it could be argued that it is.
5002880	5008880	And so in effect, it's a question of if we start, even as our own species,
5008880	5014880	multiplying the phenomena of our deployment through space
5014880	5021880	to the point that communication itself is limited by, in this particular case, speed of light considerations,
5021880	5026880	that again, you end up with essentially this separation of identity,
5026880	5031880	separation of time, such that the kinds of communication processes
5031880	5035880	that would be needed to essentially embody that ethics effectively
5035880	5039880	aren't sufficient to essentially establish it.
5039880	5041880	So in other words, either you have the ethics first,
5041880	5044880	and you live from that perspective, at which point everything's great,
5044880	5048880	or you don't have that ethics, but the nature of the interactions itself,
5048880	5054880	that the dynamics of those interactions don't have enough time for the ethics to emerge.
5054880	5058880	It's essentially the species instincts itself before it gets a chance
5058880	5060880	to really understand what those ethics are.
5060880	5063880	Well, that I think depends on the background conditions that you set up, right?
5063880	5067880	And this reminds me so much of Chris Carlson's presentation in the last session,
5067880	5072880	because I think on my clock, as long as you are able to set up a system
5072880	5076880	in which only voluntary interactions are allowed between different entities,
5076880	5080880	whether they're humans or AIs, as long as you can't destroy the,
5080880	5083880	let's say, the base layer on which they take place,
5083880	5087880	then over time, entities, whatever they are, will engage in those interactions
5087880	5089880	that are in their interest.
5089880	5093880	And over that, if it's voluntary, and you kind of like game theoretical
5093880	5095880	metaethics could arise at the end of it,
5095880	5101880	but I think as long as you can set the base layer such that you don't have destruction
5101880	5107880	and that different entities that pursue different goals can decide whether to engage or not,
5107880	5112880	then I think what comes out of that is inherently something that is not self-destructive.
5112880	5114880	But I think those are the things that you have to divide.
5114880	5116880	You don't have to get the ethics right from the start.
5116880	5118880	You don't have to get the base layer, right?
5118880	5122880	We're just enabling only voluntary interactions and non-destruction of the base layer.
5122880	5127880	The nature of the proof is to show categorically that no such conditions can be created.
5127880	5130880	So in other words, if you're saying I'm trying to create an environment,
5130880	5137880	a baseline environment that would allow for these mutual dynamics to essentially be peaceable,
5137880	5145880	what I'm basically contesting is the assumption that such an environment can be defined in any a priori way.
5145880	5150880	That may be right, but like, you know, shouldn't it be better of just trying, you know,
5150880	5155880	and I think that ties into a set as well.
5155880	5161880	If you have one chance to do something and that one chance essentially means that if you mess it up,
5161880	5163880	it messes it up for all future time.
5163880	5168880	That's a very different, that's not an experiment. That's a choice.
5168880	5171880	You believe we know enough to make such a proof today?
5171880	5177880	I believe that we know enough to essentially establish categorical proofs of certain kind.
5177880	5181880	We can do proofs of existence and non-existence of certain types.
5181880	5185880	Such proofs have often turned out to be flawed with new information.
5185880	5189880	This is true, but on the other hand, that's part of the reason why we're looking at a categorical process.
5189880	5192880	I'm looking at it more from a mathematical perspective rather than just a physical one.
5192880	5194880	Even those.
5194880	5197880	Well, yeah, I suppose you're right.
5197880	5201880	Mathematics occasionally is shown that the proofs that are established in that space are wrong.
5201880	5205880	And this is part of the reason why we have conversations like this, but it takes a while.
5205880	5210880	The proofs are proofs of negatives.
5210880	5216880	All right, so we are now at 12.29.
5216880	5223880	And I think in the initial discussion that I have with Forestu to initiate this call,
5223880	5227880	he was definitely incinerating that this takes much more than one discussion.
5227880	5232880	And I think we really opened up folks at Pandora now are discussing about
5232880	5236880	going from impossibility to like, how could we actually solve AI alive?
5236880	5242880	And so I think we should weigh past the goal of the session.
5242880	5245880	I want to thank everyone for attending.
5245880	5249880	I want to thank you for so much for laying out your argument.
5249880	5253880	I want to thank everyone else for being such active stewards of the conversation.
5253880	5259880	I thought it was like, I mean, we definitely had really, really, I think, tricky and hairy discussion topics today.
5259880	5265880	And I thought that nevertheless, we were able to keep it in a way where a discussion was actually possible.
5265880	5273880	So maybe that is a better way of saying, even if our interests are very different and where we're coming from to discussion are very different.
5273880	5275880	We can still cooperate on things, right?
5275880	5282880	Okay, this is my, you know, wavy, meta way of saying, yay, maybe we're not all that doomed.
5282880	5286880	But for now, thank you so, so much for us and joining.
5286880	5288880	Thank you so much everyone for your really active participation.
5288880	5290880	I really, really enjoyed this.
5290880	5293880	I'm hoping that we can continue the discussion for us.
5293880	5301880	I'm hoping that you share with me a few topics that you want me to send out to others afterwards to follow up,
5301880	5304880	because a few people have asked for your writing.
5304880	5309880	At the same time, I think I will share the chat with you, which may provide you really good feedback.
5309880	5313880	I think it's impossible to speak while monitoring the chat, so I'll do that too.
5313880	5314880	Yes, no, definitely not.
5314880	5315880	It's not required.
5315880	5322880	For that, we need corporations with better AIs so that they can benefit us in that day.
5322880	5324880	Here I'm really at the end of it.
5324880	5331880	I just wanted to mention that next week we will be meeting on decentralized decision architectures as a response to COVID-19.
5331880	5339880	And then the weekend afterward, we will have another AIs session, this time with Dan Elton, who gave his remarks earlier here today,
5339880	5344880	who just hopped off and he will be discussing one of his proposals to AI alignment.
5344880	5347880	And then we have two others that will be joining us.
5347880	5351880	Aria, I'm not sure if he was joining today, but he will be presenting too.
5351880	5353880	So we have the next two salons already planned.
5353880	5356880	Again, always on Thursdays at 11 a.m.
5356880	5357880	So this is enough for me now.
5357880	5360880	I'm going to close it out on my end in Forest.
5360880	5366880	Dear Dark Forest, you have to find the words and I'm already opening up the invitations.
5366880	5371880	First of all, I'm just super thrilled to have been able to speak with all of you today.
5371880	5376880	I hope that I've raised interesting questions and new ways of thinking about things.
5376880	5382880	I'm sure that with every single one of you, I could have a long and very fruitful and interesting conversation,
5382880	5388880	and that we could effectively start to really get at some of the meat of the matter in this space.
5388880	5396880	But as an introduction, I felt very well received and just glad to have the opportunity to meet so many new folks.
5396880	5398880	So thank you very much.
