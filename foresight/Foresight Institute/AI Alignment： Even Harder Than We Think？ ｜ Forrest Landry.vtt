WEBVTT

00:00.000 --> 00:01.960
Today, we're going to talk about AI alignment.

00:01.960 --> 00:04.200
Is it even harder than we think?

00:04.200 --> 00:08.160
And this is the third AI focus on the salon foresight.

00:08.160 --> 00:11.200
In the previous two weeks, we discussed chapters

00:11.200 --> 00:13.600
in the recently published book on superintelligence,

00:13.600 --> 00:17.000
a coordination strategy, which a few people here on this call

00:17.000 --> 00:18.440
have submitted chapters to.

00:18.440 --> 00:20.960
And then based on those previous salons,

00:20.960 --> 00:23.920
Daniel Schmachtenberger, who we had for another salon

00:23.920 --> 00:26.360
in the Hive Mind series, reached out to me

00:26.360 --> 00:28.600
and recommended today's speaker to me,

00:28.600 --> 00:32.120
because he may bring an underappreciated perspective

00:32.120 --> 00:34.600
to the set of problems in AI alignment.

00:34.600 --> 00:37.120
And as a disclaimer, Forest Landry

00:37.120 --> 00:40.080
is from the newer Hector Collective, Welcome Forest,

00:40.080 --> 00:43.520
and he's not an AI safety or AI alignment researcher.

00:43.520 --> 00:46.000
But I think that's exactly why he thinks

00:46.000 --> 00:49.720
that he may have an outsider's eye to contribute to the field,

00:49.720 --> 00:52.080
and he may be able to point to a few areas that

00:52.080 --> 00:54.080
are under explored.

00:54.080 --> 00:56.720
So we'll kick off with a presentation by Forest,

00:56.720 --> 00:58.560
who's outlining a list of reasons

00:58.560 --> 01:01.200
why AI alignment may be harder than we think,

01:01.200 --> 01:03.840
perhaps even impossible.

01:03.840 --> 01:08.120
And then we open up for a discussion with all of you.

01:08.120 --> 01:11.440
And we may start with our foresight fellows,

01:11.440 --> 01:14.120
for example, Creeon Levitt, who's a foresight senior fellow

01:14.120 --> 01:17.440
and physicist who may be enticed to speak a little bit

01:17.440 --> 01:20.400
on a few physical claims that Forest makes,

01:20.400 --> 01:23.040
as well as channeling Anders Sandberg, who's

01:23.040 --> 01:25.720
our senior fellow in philosophy, but who's currently

01:25.720 --> 01:28.560
on the flight and can't make it, but he says hi to you, Forest,

01:28.560 --> 01:30.320
and he would have loved to make it.

01:30.320 --> 01:32.160
And he has an update on a paper that I

01:32.160 --> 01:35.680
think you're wanting to write, which we can come back later.

01:35.680 --> 01:38.840
But then maybe we'll also be joined by Dan Elton, our 2020

01:38.840 --> 01:42.760
fellow in AI, and by Jeff Ladish, our 2020 fellow

01:42.760 --> 01:45.160
in biosecurity, who may also be enticed

01:45.160 --> 01:48.600
to kind of make a few comments to kick us off with a tricky,

01:48.600 --> 01:50.880
but hopefully really fun discussion.

01:50.880 --> 01:54.040
OK, so today we'll definitely be a little bit of an experiment,

01:54.040 --> 01:57.120
and I encourage all of you with relevant background

01:57.120 --> 02:00.400
to speak up, who wants to kind of rat team Forest's claim,

02:00.400 --> 02:03.720
but then also maybe to surface potential other problems

02:03.720 --> 02:05.800
in AI alignment that haven't been

02:05.800 --> 02:08.200
surfaced in the presentation, that we ought to bring

02:08.200 --> 02:10.520
to the table in the discussion.

02:10.520 --> 02:13.560
OK, that's enough for me, so let's

02:13.560 --> 02:16.720
have Forest present his list of good reasons

02:16.720 --> 02:20.480
to be skeptical that AI alignment is possible for 30 minutes,

02:20.480 --> 02:22.520
and then we'll jump right into the discussion,

02:22.520 --> 02:25.000
and I'm going to share a little bit more

02:25.000 --> 02:28.200
on the background on the topic of today and the talk

02:28.200 --> 02:30.440
in the salons generally in the tent.

02:30.440 --> 02:32.920
All right, Forest, take it away.

02:32.920 --> 02:33.920
Good morning.

02:33.920 --> 02:37.160
Thank you so much for such a wonderful introduction.

02:37.160 --> 02:38.040
I'm glad to be here.

02:38.040 --> 02:42.480
Obviously, as she mentioned, my main area of work

02:42.480 --> 02:44.400
happens to do more with communities,

02:44.400 --> 02:46.320
so I spend a lot of time thinking

02:46.320 --> 02:50.160
about the relationships between man, machine, and nature

02:50.160 --> 02:53.200
from user interface point of view as a software architect.

02:53.200 --> 02:55.760
I've done a lot of work for the federal government,

02:55.760 --> 03:00.600
written number of programs, and search kind of technology stuff.

03:00.600 --> 03:03.040
So being an architect of software systems

03:03.040 --> 03:06.400
for good three decades now, I have a perspective,

03:06.400 --> 03:09.480
and I thought that I might share some elements of that.

03:09.480 --> 03:11.680
Seeing as how from what I can maybe

03:11.680 --> 03:15.240
see that some elements might be unusual or new,

03:15.240 --> 03:18.920
I will at least provide some fruitful conversation.

03:18.960 --> 03:21.640
So the main thing that I'm basically wanting to do then

03:21.640 --> 03:24.760
is just give an outline of what I'm thinking about.

03:24.760 --> 03:27.200
I've got some notes, so I'll be reading a little bit.

03:27.200 --> 03:30.080
But basically, the idea is that if we're looking at AI alignment,

03:30.080 --> 03:32.240
what is the frame that we're going to use to essentially

03:32.240 --> 03:33.400
evaluate that question?

03:33.400 --> 03:35.920
So in other words, how do we determine whether or not

03:35.920 --> 03:39.320
this is a question that can be solved?

03:39.320 --> 03:41.560
So as a kind of schema of thinking about this,

03:41.560 --> 03:45.240
it's like AI can be thought of as a thing that

03:45.240 --> 03:46.760
is in relation to mankind.

03:46.800 --> 03:49.160
So in other words, I'm first positing

03:49.160 --> 03:52.440
that there's a separation between the human nature

03:52.440 --> 03:54.800
and the machine nature.

03:54.800 --> 03:57.200
There are some ways of thinking about this

03:57.200 --> 04:01.160
that look at hybrid cyborg type of things

04:01.160 --> 04:04.520
where there's a kind of mixing of machine and nature,

04:04.520 --> 04:06.160
or machine and man, and stuff like that.

04:06.160 --> 04:08.400
I'm not really considering that explicitly.

04:08.400 --> 04:11.920
I'm looking more at the you have an artificial intelligence

04:11.920 --> 04:13.400
that's an entity unto itself, and you

04:13.400 --> 04:15.280
have human beings which are entity unto themselves,

04:15.280 --> 04:17.440
and what are the interactions between the two of those?

04:17.440 --> 04:20.480
So I'm simplifying the AI alignment question

04:20.480 --> 04:22.560
to look a little bit about what would it

04:22.560 --> 04:25.000
take to align machine intelligence

04:25.000 --> 04:28.680
agency with human intelligence and agency.

04:28.680 --> 04:31.160
So in that sense, we can start to think about the AI alignment

04:31.160 --> 04:33.960
problem a little bit like trying to build a perfect shell.

04:33.960 --> 04:37.320
So in other words, we want to make sure it doesn't have any holes.

04:37.320 --> 04:41.000
And when we look at it from a structural point of view,

04:41.000 --> 04:44.080
we realized that it's not really the same as dealing

04:44.080 --> 04:47.920
with things like energy or flow of matter or stuff like that.

04:47.920 --> 04:50.880
So in other words, if I was trying to build a boat,

04:50.880 --> 04:53.280
that the shell of the boat has to be more

04:53.280 --> 04:57.320
perfect for AI alignment than it would be for seawater.

04:57.320 --> 04:58.720
Because if I have a small leak, I'm

04:58.720 --> 05:02.560
going to have a small influx of process.

05:02.560 --> 05:05.720
Whereas if I have a replicating process,

05:05.720 --> 05:07.400
it's more like a virus or something like that,

05:07.400 --> 05:11.640
or introducing a new species into an environment,

05:11.640 --> 05:14.760
that the replication process means that even a very small hole

05:14.760 --> 05:17.840
can result in essentially a complete intrusion.

05:17.840 --> 05:21.840
Because the duplication process or the replication process

05:21.840 --> 05:26.160
that AI could be implementing, the same as much

05:26.160 --> 05:28.840
as any biological life could implement,

05:28.840 --> 05:31.600
could result in kind of spread the same way that a virus would

05:31.600 --> 05:34.720
spread through an organism, as we've

05:34.720 --> 05:37.120
seen very recently with COVID.

05:37.120 --> 05:40.840
So in effect, we can say, all right, we need a shell,

05:40.840 --> 05:43.280
but we need it to be very strong shell in the sense

05:43.280 --> 05:45.960
of not having any holes, even very small holes.

05:45.960 --> 05:48.200
In other words, it must be very perfect,

05:48.200 --> 05:51.480
given that there's a kind of non-linearity in terms

05:51.480 --> 05:53.960
of the effect, that non-linearity is itself

05:53.960 --> 05:57.560
a consequence of the capacity for machines

05:57.560 --> 06:00.120
to be duplicated easily.

06:00.120 --> 06:02.560
So in effect, now we can basically say, OK, well,

06:02.560 --> 06:06.320
given that we know that we need to construct something that

06:06.320 --> 06:10.440
doesn't have any holes, can we use mathematics and logic

06:10.440 --> 06:12.840
and physics and things like that to demonstrate

06:12.840 --> 06:16.880
in multiple overlapped ways that there just cannot

06:16.880 --> 06:19.040
be a way of constructing such a shell.

06:19.040 --> 06:24.320
In other words, that it's impossible to establish

06:24.320 --> 06:27.080
that there are no holes in the shell.

06:27.080 --> 06:29.000
In other words, that the principle

06:29.000 --> 06:33.040
of realizing in any practical sense of a shell of this kind

06:33.040 --> 06:35.720
is essentially an impossibility.

06:35.720 --> 06:37.800
And so why would we want to do a proof like this?

06:37.800 --> 06:39.720
Or why would we want to essentially establish this?

06:39.960 --> 06:43.680
Well, obviously, if we find out that something isn't possible,

06:43.680 --> 06:46.760
even at the level of principle, then in effect,

06:46.760 --> 06:48.880
it allows us to kind of free up a lot of energy.

06:48.880 --> 06:51.200
In other words, we redirect our attentions to other things,

06:51.200 --> 06:53.880
and we try to solve problems in different ways.

06:53.880 --> 06:57.720
So in one sense, this is not necessarily

06:57.720 --> 06:58.760
an ideal thing to do.

06:58.760 --> 07:00.280
It's to try to say, hey, wait a minute,

07:00.280 --> 07:02.600
if we're trying to solve the problem of AI alignment,

07:02.600 --> 07:06.320
and we discover that there's no reasonable or realistic way

07:06.320 --> 07:08.800
to think about actually being able to do so.

07:08.800 --> 07:11.280
In other words, if we can't engender any real hope

07:11.280 --> 07:13.800
of such a solution, then at least we

07:13.800 --> 07:17.960
get the kind of silver lining result of, well,

07:17.960 --> 07:19.920
now we can reallocate our energies to things

07:19.920 --> 07:22.360
which are tractable and are possible.

07:22.360 --> 07:25.600
So please understand that this presentation is more or less

07:25.600 --> 07:28.040
in that sense.

07:28.040 --> 07:30.440
So another kind of way of saying, OK,

07:30.440 --> 07:31.960
so if we're going to be looking for holes,

07:31.960 --> 07:34.440
what is the frame in which we're going to be identifying

07:34.440 --> 07:36.000
that such a thing is impossible?

07:36.000 --> 07:40.000
I mean, what are we going to use as tools to do that?

07:40.000 --> 07:41.880
So one way that we can sort of do this,

07:41.880 --> 07:45.920
we can frame things in terms of kind of three general categories.

07:45.920 --> 07:49.400
So we can talk about space and identity,

07:49.400 --> 07:53.400
i.e. what is the envelope and the shape of it,

07:53.400 --> 07:55.080
and how small a hole, and stuff like that.

07:55.080 --> 07:58.000
We can talk about things in terms of time and force,

07:58.000 --> 08:00.320
and we can talk about things in terms of possibility

08:00.320 --> 08:02.720
and probability.

08:02.720 --> 08:05.240
So obviously, we're concerned with things like choice, i.e.

08:05.240 --> 08:09.240
the AI choices, the choices that we have, changes,

08:09.240 --> 08:11.840
changes in the environment, or changes in the marketplace

08:11.840 --> 08:14.200
dynamics, and causation, i.e.

08:14.200 --> 08:19.000
what it is that makes machinery work and things like that.

08:19.000 --> 08:21.920
And then we can start to bring in, using this as a foundation,

08:21.920 --> 08:24.680
we can start to bring in notions about information, complexity,

08:24.680 --> 08:28.280
game theory, and other tools to consider the question.

08:28.280 --> 08:31.120
And that leads us to start thinking about things in terms

08:31.120 --> 08:33.160
of longer time scales.

08:33.200 --> 08:35.480
So in other words, rather than thinking about AI alignment

08:35.480 --> 08:37.880
in terms of what happens immediately

08:37.880 --> 08:40.840
before or after takeoff, that we are in fact

08:40.840 --> 08:43.360
concerned with what happens over, say, hundreds, thousands,

08:43.360 --> 08:46.600
or potentially even millions of years.

08:46.600 --> 08:50.640
So in effect, when we're looking at things like about identity,

08:50.640 --> 08:53.280
for example, we're not just considering things

08:53.280 --> 08:55.440
like the identity of the holes, we're actually

08:55.440 --> 08:58.400
considering things like the identity of the AI.

08:58.400 --> 09:00.680
And it's really easy for certain biases to creep in.

09:00.720 --> 09:03.640
So for example, as a human being,

09:03.640 --> 09:07.120
we tend to have a brain inside of a skull,

09:07.120 --> 09:09.720
and that the bandwidth of communication

09:09.720 --> 09:12.560
internal to the skull and the bandwidth of communication

09:12.560 --> 09:16.560
across the skull is very, very different.

09:16.560 --> 09:20.720
I mean, we have essentially a relatively low bandwidth

09:20.720 --> 09:22.400
of interaction with the environment

09:22.400 --> 09:27.040
relative to the total bandwidth of all the synaptic connections.

09:27.080 --> 09:30.120
So in effect, in a lot of ways, there's

09:30.120 --> 09:32.840
this very, very strong differential

09:32.840 --> 09:35.360
between the internal bandwidth and the external bandwidth.

09:35.360 --> 09:38.440
For example, the communication process of which

09:38.440 --> 09:41.040
I'm engaged with you right now verbally

09:41.040 --> 09:45.800
has about 42 bits per second of communicative bandwidth.

09:45.800 --> 09:49.720
But in effect, the net difference in bandwidth

09:49.720 --> 09:53.400
makes it very, very likely that brains will come in units,

09:53.400 --> 09:56.880
that we won't necessarily have a kind of process

09:56.920 --> 10:00.000
that allows two people to communicate at such and such

10:00.000 --> 10:03.640
a level that you're treating them as a single entity.

10:03.640 --> 10:05.760
For the most part, you're going to be talking about human beings

10:05.760 --> 10:07.720
as discrete units.

10:07.720 --> 10:09.840
So in one sense, like we could say, all right,

10:09.840 --> 10:11.320
well, there's different groups of people

10:11.320 --> 10:13.640
that are trying to develop AI.

10:13.640 --> 10:15.280
And let's say several of them succeed.

10:15.280 --> 10:17.560
And just for argument's sake, let's

10:17.560 --> 10:19.960
say several of them succeed quickly,

10:19.960 --> 10:22.560
that you end up with multiple AIs deployed in the field,

10:22.560 --> 10:26.240
or perhaps a given group as part of its experimental

10:26.240 --> 10:29.240
program was creating several instances.

10:29.240 --> 10:31.320
So in other words, they're trying different design

10:31.320 --> 10:33.840
techniques in various manifestations,

10:33.840 --> 10:37.320
or they produce an AI, they do some experiments on it,

10:37.320 --> 10:39.040
and they shut that one down, they make some tweaks,

10:39.040 --> 10:40.120
and they start up another one.

10:40.120 --> 10:44.840
So you can end up with multiple instances of the AI,

10:44.840 --> 10:46.880
or multiple instances of variations

10:46.880 --> 10:49.560
of the same kind of AI, or multiple instances of AI

10:49.560 --> 10:52.640
implemented, maybe even completely different substrates.

10:52.640 --> 10:55.520
So the question then can be asked,

10:55.560 --> 10:57.040
to what degree do we have a belief

10:57.040 --> 11:00.400
that these AIs might not merge with one another?

11:00.400 --> 11:01.880
So in other words, taking the bias

11:01.880 --> 11:04.840
that we have that intelligence is going to come in units

11:04.840 --> 11:07.960
might not necessarily apply to AI itself

11:07.960 --> 11:11.200
as a phenomenon or intelligence as a phenomenon.

11:11.200 --> 11:13.960
And there's some good reasons to think about it that way.

11:13.960 --> 11:17.040
For one, the bandwidth of the internet

11:17.040 --> 11:19.760
is obviously very much greater than the bandwidth

11:19.760 --> 11:22.720
of any human being to any other human being

11:22.720 --> 11:26.040
in terms of just straight brain to brain communication.

11:26.040 --> 11:29.720
And then secondly, that since that differential

11:29.720 --> 11:31.640
is much, much lower, you're looking at more

11:31.640 --> 11:34.000
of a market forces kind of model.

11:34.000 --> 11:35.960
So in the same sort of way that if you have

11:35.960 --> 11:39.400
two separate markets, maybe you have a country over here

11:39.400 --> 11:40.640
and you have another country over there,

11:40.640 --> 11:44.240
and they're just finding out about one another,

11:44.240 --> 11:47.200
you have Columbus crossing the ocean and so on,

11:47.200 --> 11:49.560
that there's a very strong motivation

11:49.560 --> 11:51.920
for those two marketplaces to essentially find

11:51.920 --> 11:53.560
some sort of arbitrage that would allow them

11:53.560 --> 11:55.640
to exchange with one another.

11:55.640 --> 11:58.480
So in other words, wealth creation as a force

11:58.480 --> 12:01.760
in the marketplace definitely wants to create ways

12:01.760 --> 12:04.680
for us to engage in larger scales of trade.

12:04.680 --> 12:07.680
And so in that same sense, if we were looking at intelligence

12:07.680 --> 12:10.360
as a phenomenon, sense making as a phenomenon,

12:10.360 --> 12:11.920
there's very good reason for us to believe

12:11.920 --> 12:14.880
that the notion of having separate intelligences

12:14.880 --> 12:18.480
on a machine level may not necessarily hold long term

12:18.480 --> 12:21.400
that we effectively end up with a kind of merger

12:21.440 --> 12:23.600
between these multiple intelligences.

12:23.600 --> 12:26.680
And this strengthens the point of view of that,

12:26.680 --> 12:28.960
you're really looking at essentially

12:28.960 --> 12:31.720
two different agency types.

12:31.720 --> 12:33.960
You're looking at sort of human being agency type

12:33.960 --> 12:37.040
and the way in which we coordinate as groups

12:37.040 --> 12:39.320
and individuals and the kinds of evolutionary process

12:39.320 --> 12:43.160
that have endowed us with certain propensities.

12:43.160 --> 12:46.800
And then basically whether or not those propensities

12:46.800 --> 12:49.040
also apply to artificial intelligence.

12:49.040 --> 12:51.600
And what does that tell us about the nature

12:51.600 --> 12:54.280
of how we have to think about this?

12:54.280 --> 12:59.280
So that's the space and identity question

12:59.320 --> 13:01.120
being considered a little more deeply.

13:02.760 --> 13:05.000
The other process that we can think about of course

13:05.000 --> 13:07.480
is the sort of force and time thing.

13:09.240 --> 13:12.320
For example, if we're looking at what are the ways

13:12.320 --> 13:15.920
in which we would engender alignment.

13:15.920 --> 13:19.040
So again, we can talk about things that are intrinsic

13:19.040 --> 13:22.040
to the artificial intelligence.

13:22.040 --> 13:26.880
So an analog of this, Isaac Asimov wrote a lot of fiction

13:26.880 --> 13:30.920
assuming this phenomenon called the Three Laws of Robotics.

13:30.920 --> 13:33.120
So that would be essentially something that is built in

13:33.120 --> 13:35.200
that creates an AI alignment.

13:35.200 --> 13:37.440
And perhaps the argument is that we could create

13:37.440 --> 13:39.520
something roughly analogous to that.

13:39.520 --> 13:40.680
Maybe it wouldn't be as good

13:40.680 --> 13:42.120
or maybe it would take a different form

13:42.120 --> 13:44.760
or maybe there's a lot of questions about

13:44.760 --> 13:46.200
what does alignment actually mean

13:46.200 --> 13:48.640
and how do we express such forces

13:48.640 --> 13:51.080
in a design intrinsic way.

13:51.080 --> 13:53.840
But then there's obviously the extrinsic stuff, right?

13:53.840 --> 13:58.800
So for instance, say we have some sort of relationship

13:58.800 --> 14:03.720
between the AI intelligence and human beings as a collective.

14:03.720 --> 14:07.120
And what sort of feedback processes might apply

14:07.120 --> 14:11.120
to essentially address what would be the principal agent

14:11.120 --> 14:15.240
problem in the relationship between human intelligence

14:15.240 --> 14:17.480
and artificial intelligence.

14:17.480 --> 14:21.400
So in effect, we can look at the AI question,

14:21.400 --> 14:23.080
the alignment question specifically

14:23.080 --> 14:26.680
as being a special case of the principal agent problem.

14:26.680 --> 14:28.720
And there have been various notes

14:28.720 --> 14:33.320
about how to address principal agent problems in general.

14:33.320 --> 14:35.960
So one thing we could say as part of this sort of force

14:35.960 --> 14:37.840
and time way of looking at things

14:37.840 --> 14:41.000
is to essentially ask what are the forces

14:41.000 --> 14:45.080
that essentially help us to create alignment

14:45.080 --> 14:46.880
or help us to maintain alignment,

14:46.880 --> 14:49.840
i.e. if it's something that's built in in a prior sense

14:49.840 --> 14:52.120
or something that is essentially applied

14:52.120 --> 14:56.680
after the AI exists in the relational dynamics.

14:58.320 --> 15:00.080
And so in effect, we can basically say,

15:00.080 --> 15:02.880
all right, well, given that we have these forces

15:02.880 --> 15:06.280
that are applied, how long do those forces

15:06.280 --> 15:08.560
maintain that alignment?

15:08.560 --> 15:11.840
So again, it's not just a question of,

15:11.840 --> 15:15.680
do we have alignment today or before the thing is built?

15:16.840 --> 15:18.840
Can we create the conditions before it's built

15:18.840 --> 15:20.960
and can we maintain the conditions after it's built?

15:20.960 --> 15:22.360
But it's also the case of,

15:22.360 --> 15:24.320
can we maintain those conditions after it's built

15:24.320 --> 15:26.720
for a really long period of time?

15:26.720 --> 15:28.680
So in other words, hundreds of thousands of years

15:28.680 --> 15:32.200
essentially, given that the scope of the introduction

15:32.200 --> 15:33.880
of something like this,

15:33.880 --> 15:36.720
particularly if it has the capacity to replicate itself,

15:36.840 --> 15:38.800
which it almost certainly will,

15:38.800 --> 15:42.240
then in effect, there's a sort of functional aspect

15:42.240 --> 15:43.520
of saying, okay, well, it's a little bit like

15:43.520 --> 15:45.440
introducing a new species.

15:45.440 --> 15:47.480
And in the same sort of way that say,

15:47.480 --> 15:49.600
introducing mosquitoes into the Hawaiian islands

15:49.600 --> 15:53.160
is a little hard to undo, then in effect,

15:53.160 --> 15:55.400
once artificial intelligence is introduced

15:55.400 --> 15:57.520
into an environment, it could also be argued

15:57.520 --> 16:00.800
that it influences the future course of civilization

16:00.800 --> 16:01.640
as we know it.

16:02.680 --> 16:06.120
So in that sense, we can really just go back

16:06.120 --> 16:08.480
to the whole notion of why it's important for us

16:08.480 --> 16:11.040
to figure out this AI alignment question,

16:11.040 --> 16:13.080
given the severity of the ethical implications.

16:13.080 --> 16:15.200
I mean, we're literally talking the future

16:15.200 --> 16:17.600
of the human species or the,

16:17.600 --> 16:20.160
if things just continued more or less as they were,

16:21.160 --> 16:25.720
as seen over the last epoch of the thousands years or so,

16:25.720 --> 16:27.320
we could literally be talking about trillions

16:27.320 --> 16:28.480
of human beings.

16:28.480 --> 16:31.880
And that's not assuming over the future,

16:31.880 --> 16:35.780
600 or so million years that this planet

16:35.780 --> 16:37.980
will continue to sustain life

16:37.980 --> 16:39.940
in the sense that we understand it.

16:39.940 --> 16:42.820
So if we were to look at it from a larger point of view

16:42.820 --> 16:45.620
of say we become a space-faring species

16:45.620 --> 16:47.100
and we start into colonized Mars

16:47.100 --> 16:51.220
and potentially other planets in the galaxy,

16:51.220 --> 16:53.300
that trillions could become trillions of trillions.

16:53.300 --> 16:56.100
So the ethical emphasis of the question is very clear.

16:56.100 --> 16:57.980
And so as a result, it encourages us

16:57.980 --> 17:00.140
to really think about this question

17:00.140 --> 17:02.220
in terms of hundreds or at least thousands of years

17:02.220 --> 17:04.940
as a minimum for us to really evaluate this question.

17:04.940 --> 17:07.500
So in that sense, it's not just a question of

17:07.500 --> 17:11.020
can we talk about the holes as being small

17:11.020 --> 17:13.420
and that the shell essentially doesn't have such holes,

17:13.420 --> 17:17.420
that whatever the forces are that create alignment,

17:17.420 --> 17:20.220
whatever are the solutions to the principal agent problem,

17:20.220 --> 17:21.980
that they need to be perfect enough

17:21.980 --> 17:26.020
that we don't end up with microscopic fractures

17:26.020 --> 17:29.060
and that can cascade into larger failures

17:29.060 --> 17:32.060
and eventually compromise the alignment problem

17:32.060 --> 17:33.980
at some future date.

17:33.980 --> 17:36.260
So in that particular sense,

17:36.260 --> 17:38.620
we're really looking at a kind of potentiality basis,

17:38.620 --> 17:40.260
like what is the probability of such a hole?

17:40.260 --> 17:43.740
What is the probability of that cascading

17:43.740 --> 17:45.180
and being amplified?

17:47.340 --> 17:49.980
Now, of course, there are obviously,

17:49.980 --> 17:53.220
we bring into this conversation a lot of biases.

17:53.220 --> 17:56.020
We have a lot of people that will gain

17:56.020 --> 17:58.580
on a financial level if we develop AI.

17:58.580 --> 18:00.740
There's a hope for basically solutions

18:00.740 --> 18:01.620
to really hard problems.

18:01.660 --> 18:04.460
There's a hope for a kind of asymmetric advantage

18:04.460 --> 18:06.140
of one group that develops at first

18:06.140 --> 18:09.780
versus other groups that might develop it later.

18:09.780 --> 18:12.660
So there's a lot of things that are really encouraging us

18:12.660 --> 18:14.020
to try to figure out this question

18:14.020 --> 18:16.660
because if we were to figure out this question,

18:16.660 --> 18:21.380
then obviously the economic advantages would be enormous.

18:21.380 --> 18:23.900
But again, at this particular point,

18:23.900 --> 18:25.980
we're back to the principal agent problem.

18:27.500 --> 18:30.900
So sort of skipping past the kind of multipolar dynamics

18:30.900 --> 18:32.100
that exist between the groups

18:32.100 --> 18:34.860
and kind of leaving the economic questions aside,

18:36.060 --> 18:37.900
we can ask, well, what is the possibility

18:37.900 --> 18:40.260
that we could effectively ensure

18:40.260 --> 18:42.340
that there are no holes?

18:42.340 --> 18:45.900
In other words, what is it gonna take to establish that?

18:46.980 --> 18:48.980
Well, one question that we can immediately look at

18:48.980 --> 18:52.860
is essentially what is the nature of the coexistence

18:52.860 --> 18:55.460
that exists between machine intelligence

18:55.460 --> 18:58.420
and human intelligence?

18:59.260 --> 19:02.300
And everything I've said prior to this

19:02.300 --> 19:04.540
is probably stuff that people have already thought about

19:04.540 --> 19:05.740
and is well-known information.

19:05.740 --> 19:07.860
And the stuff that I'm saying from here,

19:07.860 --> 19:10.860
I'm hoping is new information to this group.

19:10.860 --> 19:13.460
I haven't seen presentation of this particular argument

19:13.460 --> 19:16.020
from this point as yet.

19:16.020 --> 19:19.620
So this to me is probably the more interesting part

19:19.620 --> 19:21.140
of this presentation.

19:22.140 --> 19:24.140
So if we were to look at, again,

19:24.140 --> 19:26.180
the relationship between machine intelligence

19:26.420 --> 19:28.860
and human intelligence as essentially representative

19:28.860 --> 19:30.540
as a kind of species,

19:31.420 --> 19:33.500
and that the species themselves,

19:33.500 --> 19:36.380
the same way that human beings influence their environment

19:36.380 --> 19:38.780
to kind of define an ecosystem around ourselves

19:38.780 --> 19:41.100
or at least a niche within an ecosystem,

19:41.940 --> 19:43.900
that the machine intelligence itself

19:43.900 --> 19:48.060
is also gonna occur within the niche of the marketplace

19:48.060 --> 19:50.380
and the niche of the manufacturing

19:50.380 --> 19:52.300
and the industrial complex particularly.

19:53.260 --> 19:56.420
So what are the fundamental dynamics

19:56.420 --> 19:59.300
of these two ecosystems with respect to one another

19:59.300 --> 20:01.060
and what does that tell us about the possibility

20:01.060 --> 20:01.900
of alignment?

20:01.900 --> 20:05.140
In other words, can we bring the tools of physics

20:05.140 --> 20:07.580
and the logic of, say, game theory

20:07.580 --> 20:09.580
to look at the relationships between

20:10.740 --> 20:11.820
the intelligence phenomenon,

20:11.820 --> 20:14.220
obviously we have the agency of human beings

20:14.220 --> 20:16.260
and human groups,

20:16.260 --> 20:19.940
and then obviously the agency of artificial intelligence,

20:19.940 --> 20:22.580
but we also have the substrate relationships.

20:22.580 --> 20:25.860
So in effect, when we're looking at this,

20:25.860 --> 20:29.780
we're looking at, okay, what is silica-based

20:29.780 --> 20:33.820
chemistry process and the implications that that has

20:33.820 --> 20:36.940
relative to the ecosystems necessary to support it

20:36.940 --> 20:41.140
versus carbon-based ecosystem process?

20:41.140 --> 20:44.260
So in other words, if we're really looking at

20:44.260 --> 20:47.100
what are the kinds of forces that will create alignment,

20:47.100 --> 20:49.660
we're really looking at what is the marketplace

20:49.660 --> 20:52.260
that essentially joins these two ecosystems

20:52.260 --> 20:54.500
and what are the dynamics inherent in the relationship

20:54.500 --> 20:56.540
between these two ecosystems?

20:56.540 --> 20:58.300
And using that basis,

20:58.300 --> 21:00.660
we can start to make some observations.

21:00.660 --> 21:02.180
So one of the observations that we can make

21:02.180 --> 21:05.460
is just from the very nature of the chemistry itself.

21:05.460 --> 21:10.460
If we look at the sort of kind of total envelope

21:10.620 --> 21:13.660
of all of the different kinds of chemical interactions

21:13.660 --> 21:17.140
that encompass carbon-based chemistry necessary

21:17.140 --> 21:20.300
to engender biological intelligence of our kind,

21:20.300 --> 21:22.380
but also the ecosystems necessary to support

21:22.380 --> 21:25.180
the substrate, our bodies and food supply and so on.

21:26.140 --> 21:29.580
And when you look at the sort of total range of chemistry

21:29.580 --> 21:32.300
that defines life on this planet,

21:32.300 --> 21:34.980
you notice that it mostly occurs between say,

21:34.980 --> 21:37.540
zero degrees and 500 degrees Fahrenheit.

21:38.340 --> 21:42.740
And this just, again, if you do a sort of enthalpy

21:42.740 --> 21:46.660
calculation on how much each different kinds of bonding

21:47.500 --> 21:50.860
or chemical recombination and stuff like that occurs,

21:50.860 --> 21:54.220
the vast majority of it occurs at standard temperatures

21:54.220 --> 21:55.260
and pressures.

21:55.260 --> 21:56.940
And there's some exceptions

21:56.940 --> 21:59.420
that sort of are more in the outliers,

21:59.420 --> 22:01.860
but the vast majority of the chemistry occurs,

22:01.860 --> 22:06.300
as I said, in this 500 degree range of temperatures.

22:07.420 --> 22:10.100
But when you look at the silicon-based chemistry,

22:10.100 --> 22:11.620
you actually find that the envelope

22:11.620 --> 22:13.380
in which most of the chemical reactions occur,

22:13.380 --> 22:15.340
the enthalpy that's involved in those reactions

22:15.340 --> 22:17.700
is actually over a much different range.

22:17.700 --> 22:20.180
Most of those reactions need to start somewhere

22:20.180 --> 22:22.220
in the neighborhood around 500 degrees Fahrenheit

22:22.220 --> 22:26.500
and go all the way up to 2,500 degrees Fahrenheit, typically.

22:26.500 --> 22:29.620
And the reason that we know this is actually,

22:29.620 --> 22:32.940
again, speaking specifically of this planet as an example,

22:32.940 --> 22:37.620
when you look at the kinds of phenomena that are involved,

22:37.620 --> 22:41.020
I mean, first of all, the amount of silica that's available

22:41.020 --> 22:42.980
in the Earth's crust relative to the amount of carbon

22:42.980 --> 22:44.820
that's available on this planet

22:44.820 --> 22:48.820
is there's a substantially large preponderance of silicon.

22:48.820 --> 22:50.740
I don't remember the statistics off the top of my head,

22:50.740 --> 22:52.700
but it's something like 25%

22:52.700 --> 22:56.700
of all the elemental constitution of the Earth's crust

22:56.700 --> 23:01.700
is silica and that something like 0.05% of it is carbon.

23:01.860 --> 23:05.020
And that's including everything in the surface,

23:05.020 --> 23:08.020
above the surface and all the way up into the atmosphere.

23:08.020 --> 23:10.580
So first of all, silica chemistry

23:10.580 --> 23:12.940
has had far more opportunity to engage

23:12.940 --> 23:16.340
in a much, much wider variety of chemical combinations

23:16.340 --> 23:18.020
just by its sheer preponderance

23:18.020 --> 23:21.260
over the 4.5 billion year history of this planet.

23:21.260 --> 23:23.140
So in other words, you have volcanoes

23:23.140 --> 23:25.180
and you've got all sorts of solar flux,

23:25.180 --> 23:26.580
you've got cosmic ray radiation,

23:26.580 --> 23:29.140
you've got all sorts of stuff, lightning and so on

23:29.140 --> 23:32.860
that would really encourage every possible interaction

23:32.860 --> 23:36.540
between every elemental type sometime in the total space

23:36.540 --> 23:39.300
of the surface of the Earth and the volume of the Earth

23:39.300 --> 23:44.300
and the total duration in which those experiments

23:44.580 --> 23:48.460
can be conducted by just raw chemical mixing.

23:48.460 --> 23:51.620
So in effect, when we look at the net result of that,

23:51.620 --> 23:53.620
essentially the current state,

23:53.620 --> 23:55.660
we see that for the most part,

23:55.660 --> 23:58.180
the chemistry of silica results in things

23:58.180 --> 23:59.900
that we call rocks.

23:59.900 --> 24:01.740
There's a whole lot of elemental types

24:01.740 --> 24:03.300
and mineral types and so on and so forth,

24:03.300 --> 24:06.740
but for the most part, the variety of chemistry

24:06.740 --> 24:09.780
that occurs is relatively inert

24:09.780 --> 24:11.620
at the standard temperatures and pressures

24:11.620 --> 24:15.420
that our biosphere is mostly defined by.

24:16.260 --> 24:18.940
So as a result, when we look at the sort of fundamental

24:18.940 --> 24:22.180
substrate issues and we start to think about things like,

24:22.180 --> 24:24.380
the kind of chemistry processes that are necessary

24:24.380 --> 24:26.900
to develop microchips, for example,

24:26.900 --> 24:30.460
we not just see, obviously these manufacturing channels

24:30.460 --> 24:32.860
and lithography processes and such,

24:32.860 --> 24:34.540
but we also see the chemistry

24:34.540 --> 24:36.180
and the kinds of reaction processes

24:36.180 --> 24:38.940
that are needed to create that kind of variety

24:38.940 --> 24:41.300
of, you know, again, interaction.

24:41.300 --> 24:44.740
Obviously you need a certain amount of manifest complexity

24:44.740 --> 24:48.300
in order to support the substrate of compute to begin with

24:48.300 --> 24:51.420
and also a certain amount of manifest variety of chemistry

24:51.420 --> 24:54.580
in order to provide for sensory capacity

24:54.580 --> 24:57.020
and interaction with the environment.

24:57.020 --> 24:57.860
But-

24:57.860 --> 24:58.700
Horace?

24:58.700 --> 24:59.540
Yes.

24:59.540 --> 25:01.700
We have one comment already from Crian, I think.

25:03.580 --> 25:05.100
Oh, well, I was kind of saving this.

25:05.100 --> 25:06.500
I was saving this stuff up, maybe,

25:06.500 --> 25:08.500
for the discussion period at the end.

25:09.540 --> 25:12.380
I just, well, as long as here we are, let me ask you this.

25:12.380 --> 25:14.660
I understand that you're saying there's more silicon

25:14.660 --> 25:17.820
and in some sense there's been more time for silicon

25:17.820 --> 25:20.260
to engage in more chemical reactions,

25:20.260 --> 25:23.660
but the fact of the matter is, for whatever reason,

25:23.660 --> 25:26.980
silicon life didn't evolve even in the lithosphere,

25:26.980 --> 25:28.780
presumably where the temperatures are high enough

25:28.780 --> 25:31.300
for all these reactions to make these rocks.

25:31.300 --> 25:32.980
And yet carbon life did.

25:32.980 --> 25:36.300
So arguably it seems as if there's something about carbon

25:36.300 --> 25:41.300
that allows a much wider variety of molecular forms

25:42.300 --> 25:45.820
to become into existence.

25:45.820 --> 25:49.340
Now, whether that's just a fact that carbon took off first

25:49.340 --> 25:51.780
and silicon could do it in their silicon life elsewhere

25:51.780 --> 25:53.940
that's natural and organic, if you will,

25:53.940 --> 25:55.380
that's another question, obviously,

25:55.380 --> 25:57.140
when maybe we'll eventually know the answer to that,

25:57.140 --> 26:00.620
maybe not, but it is actually then in that sense

26:00.620 --> 26:02.940
even more astonishing that carbon took off

26:02.940 --> 26:06.100
instead of silicon, because the earth was hot at the beginning

26:06.100 --> 26:10.860
and carbon was an insignificant fraction.

26:10.860 --> 26:12.380
So that's what I'm saying.

26:12.380 --> 26:14.580
And it is a component of this argument

26:14.580 --> 26:16.700
to think about Fermi Paradox kind of issues,

26:16.700 --> 26:19.980
like is the Great Barrier in the past,

26:19.980 --> 26:21.500
is there a Great Barrier in the present,

26:21.500 --> 26:23.340
like where the hell are all the little green men,

26:23.340 --> 26:24.940
why don't we know about them?

26:24.940 --> 26:26.460
Or is the Great Barrier in the future,

26:26.460 --> 26:27.900
the technological civilization

26:27.900 --> 26:29.500
is inherently self-terminating.

26:30.500 --> 26:32.860
I can draw those elements into the argument,

26:33.100 --> 26:35.020
but the point that you're making is actually a real one,

26:35.020 --> 26:37.300
and I mean, obviously it's a real one,

26:37.300 --> 26:40.180
but that it actually contributes to what I'm saying

26:40.180 --> 26:42.580
in the sense that say, for example,

26:42.580 --> 26:44.460
we were to, at this particular point,

26:44.460 --> 26:49.460
bootstrap to sort of bring silica life into existence.

26:49.820 --> 26:50.660
The real question is,

26:50.660 --> 26:53.100
is how well do these ecosystems exist with one another?

26:53.100 --> 26:55.780
They are definitely different ecosystems.

26:55.780 --> 26:59.820
And given that the enthalpy of reaction of one versus the other

26:59.820 --> 27:01.180
is so substantially different

27:01.180 --> 27:04.780
than the question of toxicity becomes a matter of some concern.

27:04.780 --> 27:08.140
So in other words, if I have industrial processes,

27:09.060 --> 27:10.180
and I'm saying industrial,

27:10.180 --> 27:11.780
just to give it a way of relating it to,

27:11.780 --> 27:14.860
but let's say I have a silica-based ecosystem,

27:14.860 --> 27:16.900
the silica-based ecosystem is gonna involve

27:16.900 --> 27:20.100
a very different range of energies and elemental types

27:20.100 --> 27:21.860
than the carbon-based ecosystem.

27:21.860 --> 27:23.380
And then inherently, as a result,

27:23.380 --> 27:26.700
the silica-based ecosystem is gonna be toxic

27:26.700 --> 27:27.900
to the carbon-based life.

27:27.900 --> 27:29.900
So in other words, in order for those two to exist,

27:29.900 --> 27:31.300
you're gonna have to have some kind of wall

27:31.300 --> 27:35.260
because the pressure and the temperatures and so on and so forth

27:35.260 --> 27:37.500
inherently involved in the replication cycle

27:37.500 --> 27:39.500
associated with silica-based life.

27:39.500 --> 27:41.140
It's just at, like I said,

27:41.140 --> 27:42.940
a very different level of characteristics

27:42.940 --> 27:44.940
than that which is involved in carbon-based life.

27:44.940 --> 27:48.340
And in effect, you now have a situation

27:48.340 --> 27:50.700
where the carbon-based life is suffering

27:50.700 --> 27:53.060
because of the energies involved in the silica-based life

27:53.060 --> 27:55.540
are just substantially greater,

27:55.540 --> 27:56.740
not just in replication,

27:56.740 --> 27:59.100
but in terms of what their tolerances are.

27:59.100 --> 28:00.020
So for instance, you know,

28:00.020 --> 28:02.820
spacecraft can essentially be out in space unshielded

28:02.820 --> 28:05.620
and deal with cosmic radiation and solar radiation

28:05.620 --> 28:07.700
and stuff like that far, far, far more easily

28:07.700 --> 28:10.740
than carbon-based life can do the same thing.

28:10.740 --> 28:13.220
So if there was any kind of energetic exchange

28:13.220 --> 28:15.820
between the carbon-based life and the silica-based life,

28:15.820 --> 28:18.620
whether it be a weaponized exchange of energy

28:18.620 --> 28:20.460
or even just a typical one,

28:20.460 --> 28:22.060
it's not just that the information flux

28:22.060 --> 28:23.900
that the silica-based life could handle

28:23.900 --> 28:28.100
and the intelligence and gender is substantially greater.

28:28.100 --> 28:30.020
It's also that just on the sheer energy level,

28:30.020 --> 28:31.380
it's substantially greater

28:31.380 --> 28:33.540
and that that has inherent toxicity relationships

28:33.540 --> 28:36.380
with respect to the carbon-based life.

28:36.380 --> 28:38.100
So now we have essentially two issues.

28:38.100 --> 28:39.860
It's not just that we need AI alignments

28:39.860 --> 28:41.940
as far as the intelligence is concerned,

28:41.940 --> 28:43.660
but we absolutely have to have AI alignment

28:43.660 --> 28:46.500
in order for us to even have some possibility

28:46.500 --> 28:48.140
of coexisting.

28:48.140 --> 28:49.780
If you look at it from that point of view,

28:49.780 --> 28:51.020
all of a sudden you're now realizing

28:51.020 --> 28:53.260
that there are evolutionary dynamics to this

28:53.260 --> 28:56.660
and that there's a certain game theory that comes to bear.

28:56.660 --> 28:58.420
So say, for example,

28:58.420 --> 29:01.100
that we're looking at things on geological time

29:01.100 --> 29:02.940
and we're basically saying, okay,

29:02.940 --> 29:05.220
there's a kind of replication process

29:05.220 --> 29:07.500
or there's a kind of future maintenance process

29:07.500 --> 29:10.820
to some sort of dynamic that the AI is using

29:10.820 --> 29:12.340
to persist itself in time.

29:13.300 --> 29:15.820
And what are the degrees of exchange

29:15.820 --> 29:17.020
that would exist between the two

29:17.020 --> 29:19.220
that would even help to enforce AI alignment at all?

29:19.220 --> 29:21.980
So in other words, if you look at the principal agent

29:21.980 --> 29:24.420
problem solutions that are generally proposed,

29:24.420 --> 29:26.580
they either depend upon something intrinsic

29:26.580 --> 29:28.100
to the inner nature of the AI

29:28.100 --> 29:30.940
or they depend on some sort of market feedback process,

29:30.940 --> 29:33.500
i.e. reputation or admission,

29:33.500 --> 29:37.900
some sort of thing that allows us to create some feedback

29:37.900 --> 29:40.260
incentive or punishment or barrier

29:40.260 --> 29:44.580
that essentially maintains some sort of peace

29:44.580 --> 29:47.180
in terms of the energy exchange across the wall.

29:47.180 --> 29:48.020
Hey, Forrest.

29:48.020 --> 29:48.860
Yes.

29:48.860 --> 29:50.700
May I, I wanna raise two things.

29:50.700 --> 29:53.180
And if I'm talking too much, please let me know.

29:54.060 --> 29:58.020
First thing is, I'm wondering if you have run across

29:58.020 --> 30:00.900
Lovelock's latest book, The Nova Scene,

30:00.900 --> 30:03.340
because he makes some very similar arguments.

30:03.340 --> 30:05.180
Well, from a sort of different angle,

30:05.180 --> 30:08.220
I'll just mention, he basically says that

30:09.220 --> 30:12.860
it's a damn good thing that we might create

30:12.860 --> 30:15.820
artificial superintelligence based on silicon

30:15.820 --> 30:17.860
because as the world gets hotter,

30:17.860 --> 30:21.100
silicon can live there and carbon can't, you know?

30:21.100 --> 30:22.260
And we're making the world hotter,

30:22.780 --> 30:25.940
so this is arguably good because otherwise,

30:25.940 --> 30:27.700
conscious life is just gonna disappear

30:27.700 --> 30:30.820
if we don't make it capable of running

30:30.820 --> 30:32.300
at higher temperatures.

30:32.300 --> 30:33.900
So that's one thing, I thought that's interesting.

30:33.900 --> 30:35.620
I just wanted to make you aware of Lovelock's book.

30:35.620 --> 30:38.180
It's pretty interesting and short and sweet book.

30:38.180 --> 30:40.780
And then the other thing is that

30:40.780 --> 30:42.300
wouldn't this suggest that perhaps,

30:42.300 --> 30:44.860
and maybe I don't wanna have a spoiler here,

30:44.860 --> 30:46.260
wouldn't this suggest that perhaps

30:46.260 --> 30:48.700
if we all agreed to only implement AI

30:48.700 --> 30:52.300
using like really fragile cryogenic qubits,

30:52.300 --> 30:56.140
then we're good to go because it'll actually be less,

30:56.140 --> 30:59.420
it'll be more, it'll require cooler temperatures

30:59.420 --> 31:02.300
and less, you know, tolerate less radiation

31:02.300 --> 31:04.580
and like be more fragile than biological life.

31:04.580 --> 31:06.820
So we win if the shooting starts.

31:08.460 --> 31:10.180
First of all, thank you for the introduction

31:10.180 --> 31:11.420
of the book, I wasn't aware of it.

31:11.420 --> 31:13.620
I will definitely look at that

31:13.620 --> 31:17.500
to respond to the notion of should AI intelligence

31:17.500 --> 31:18.940
be developed eventually, sure,

31:18.940 --> 31:22.700
but I'm thinking that something like 650 million years from now

31:22.700 --> 31:23.860
would be an appropriate time for us

31:23.860 --> 31:25.900
to start thinking about that.

31:25.900 --> 31:26.780
If we do it sooner,

31:26.780 --> 31:29.460
we're pretty much going to extinct ourselves.

31:29.460 --> 31:32.020
And that's not just a hypothesis in the sense of,

31:32.020 --> 31:33.980
hey, this is likely to happen.

31:33.980 --> 31:36.020
Potentially what I'm attempting to do and set up for

31:36.020 --> 31:39.140
is essentially a theoretic and evolutionary mathematics

31:39.140 --> 31:42.260
based way of showing that it cannot not be the case

31:42.260 --> 31:47.260
because there's no basis upon which to establish

31:48.380 --> 31:51.060
coexistence and many, many reasons to show that

31:51.060 --> 31:53.420
from a game theoretic point of view,

31:53.420 --> 31:56.780
that it's not even, there's not even a question.

31:56.780 --> 31:58.540
In other words, if you look for Nash Equilibrium

31:58.540 --> 32:01.020
in this particular space, you can prove that there are none.

32:01.020 --> 32:02.740
Oh, does that, so that's one more question.

32:02.740 --> 32:03.820
Does that have any question?

32:03.820 --> 32:06.780
Does that rely on your temperature arguments or not?

32:06.780 --> 32:07.980
Crayon Vendan.

32:07.980 --> 32:10.700
Uses the temperature arguments as essentially a way

32:10.700 --> 32:12.620
of establishing what to look for.

32:12.620 --> 32:15.460
So in other words, what you eventually are able to do

32:15.460 --> 32:18.900
is to show that any feedback process that's causative

32:18.900 --> 32:23.100
depends upon some sort of mutuality of process.

32:23.100 --> 32:25.580
Once you've established that there's no mutuality of process,

32:25.580 --> 32:29.740
you can establish that there's no marketplace dynamic

32:29.740 --> 32:32.620
that essentially binds the two ecosystems.

32:32.620 --> 32:37.340
And unless you impose some sort of supervalent altruism

32:37.340 --> 32:39.740
that you can't establish that, then you can go

32:39.740 --> 32:42.180
and you can prove that such a supervalent altruism itself

32:42.180 --> 32:44.260
is forbidden by the laws of physics.

32:44.260 --> 32:46.420
So in effect, what you can essentially do

32:46.420 --> 32:48.660
is you can say, all right, we know

32:48.660 --> 32:50.580
that we require these particular conditions

32:50.580 --> 32:52.700
to be successful.

32:52.700 --> 32:54.220
Looking at it from this point of view,

32:54.220 --> 32:56.180
we can establish that this kind of mathematics

32:56.180 --> 32:58.220
is necessary to evaluate it.

32:58.220 --> 33:00.180
And then on purely, like I said, I

33:00.180 --> 33:02.100
could do it in information theory, but I'm sorry,

33:02.100 --> 33:03.580
I can do it on game theory, but it's also

33:03.580 --> 33:05.820
possible to do on information theory.

33:05.820 --> 33:08.100
Effectively, in order to have the alignment persist,

33:08.100 --> 33:10.740
you need to show that the noise floor of the copy

33:10.740 --> 33:14.340
from the past into the future is essentially consistent,

33:14.340 --> 33:17.420
which brings me to the third point of what you were speaking

33:17.420 --> 33:21.340
of, which is that if we try to make the AI too fragile

33:21.340 --> 33:24.380
in that particular sense, that might work for a time.

33:24.380 --> 33:26.460
But the thing is, is that you can't necessarily

33:26.460 --> 33:27.940
sure that it's going to stay fragile.

33:27.940 --> 33:30.820
If there's any kind of, again, replication process

33:30.820 --> 33:34.620
or any kind of dynamic where there's persistence in time,

33:34.620 --> 33:37.700
in other words, that this thing has to do some sort of repair,

33:37.700 --> 33:39.340
there's going to be essentially, eventually,

33:39.340 --> 33:42.380
the emergence of dynamics that strengthen that.

33:42.380 --> 33:45.500
Otherwise, the fragility itself terminates that line.

33:45.500 --> 33:47.860
So from a purely evolutionary point of view,

33:47.860 --> 33:50.260
you begin to see that if we're looking at apocal periods

33:50.260 --> 33:54.700
of time, that certain phenomena drives certain aspects

33:54.700 --> 33:57.180
of the situation drives certain results.

33:57.180 --> 33:58.780
And from those results, we can begin

33:58.780 --> 34:01.540
to do certain calculations that allow us to establish

34:01.540 --> 34:03.820
that AI alignment is not possible.

34:03.820 --> 34:07.700
OK, we have Dan as well with a comment.

34:07.700 --> 34:10.180
Yeah, well, it's good that you're

34:10.180 --> 34:13.860
bringing physics into this, because I was a physics major.

34:13.860 --> 34:20.860
But you see the future as being a sort of economic competition

34:20.860 --> 34:26.180
between silicon life forms and life forms, which

34:26.180 --> 34:27.980
are carbon-based life forms.

34:27.980 --> 34:30.180
I have to make it relatable.

34:30.180 --> 34:33.820
So we think in terms of marketplace dynamics

34:33.820 --> 34:35.420
and evolutionary theory and game theory.

34:35.420 --> 34:37.220
And essentially, all of those are models

34:37.260 --> 34:39.860
that have essentially a similar form and structure.

34:39.860 --> 34:40.500
Right.

34:40.500 --> 34:42.740
Now, I agree with you that the silicon life forms,

34:42.740 --> 34:47.780
I mean, theoretically can be that use

34:47.780 --> 34:52.820
silicon and metal and other elements materials

34:52.820 --> 34:56.980
can be theoretically much stronger and more powerful.

34:56.980 --> 34:58.700
And we all know that and appreciate that.

34:58.700 --> 35:11.100
But in terms of alignment, it's possible that if I agree

35:11.100 --> 35:13.780
that we should be skeptical about this happening.

35:13.780 --> 35:20.780
But it is possible the silicon-based AIs

35:20.780 --> 35:24.820
could recognize that there's a lot of silicon and metal out

35:24.820 --> 35:29.420
in space, and they can live in space easily where we can't

35:29.420 --> 35:30.780
live in space easily.

35:30.780 --> 35:31.980
I understand.

35:31.980 --> 35:34.700
That doesn't necessarily solve the problem.

35:34.700 --> 35:39.940
Might they just go into outer space and operate there?

35:39.940 --> 35:40.620
Sure.

35:40.620 --> 35:42.700
And so in effect, what you end up with

35:42.700 --> 35:46.820
is two separated ecosystems.

35:46.820 --> 35:51.700
Now, if the ecosystems are separated and they stay separated

35:51.700 --> 35:55.180
and there's no interaction between them

35:55.180 --> 36:00.100
and there's no reason for them to force such an interaction,

36:00.100 --> 36:03.380
then you have a coexistence model of two ecosystems

36:03.380 --> 36:08.180
essentially time separated from one another.

36:08.180 --> 36:09.580
But that's not AI alignment.

36:09.580 --> 36:13.140
It's essentially just like complete independence.

36:13.140 --> 36:21.660
Well, if AI is lying so it respects human life,

36:22.540 --> 36:26.260
it might decide to leave humans on the Earth doing our thing

36:26.260 --> 36:33.340
and go into space and just go colonize other planets.

36:33.340 --> 36:35.900
This brings us directly to what would be the second great

36:35.900 --> 36:39.340
barrier of the Fermi Paradox.

36:39.340 --> 36:42.500
Once you have some sort of real separation

36:42.500 --> 36:46.700
between the two ecosystems, then it becomes a question of,

36:46.700 --> 36:49.540
is there any reason for the two ecosystems

36:49.540 --> 36:51.140
to want to interact with one another?

36:51.140 --> 36:54.420
Or is it actually the case that we have some strong reasons

36:54.420 --> 36:57.580
for them not to want to interact with one another?

36:57.580 --> 37:01.220
And again, with this sort of way of approaching it,

37:01.220 --> 37:03.940
we can actually ensure, I'm sorry,

37:03.940 --> 37:08.220
we can show that it's desirable for each side

37:08.220 --> 37:12.740
to ensure that it doesn't interact with the other one.

37:12.740 --> 37:17.340
I just posted a meditation model piece, which is a piece,

37:17.340 --> 37:20.820
but it's making the claim that any type of super

37:20.820 --> 37:25.300
intelligent will enter dynamics, but which is impossible,

37:25.300 --> 37:27.740
that it can leave us even just a guard.

37:27.740 --> 37:31.260
So just by, yeah, do you want to comment on that?

37:31.260 --> 37:32.620
I wanted to reference that argument.

37:32.620 --> 37:33.900
I'm glad you did.

37:33.900 --> 37:35.700
I was literally debating in my head

37:35.700 --> 37:37.580
whether it was worth trying to bring all that up.

37:37.580 --> 37:41.580
But yeah, I've read that piece and it's been, actually,

37:41.580 --> 37:44.620
I think it's one of the better writings of that space.

37:44.620 --> 37:49.540
It really details the multipolar trap situation very, very well.

37:49.540 --> 37:51.660
And it kind of describes the relationships

37:51.660 --> 37:54.820
that it has to a lot of these considerations.

37:54.820 --> 37:56.700
Some of the work that I'm doing is essentially

37:56.700 --> 37:58.220
based upon that paper and it's based

37:58.220 --> 38:01.900
upon the work of Nick Balstrom and others who I think

38:01.900 --> 38:04.260
have been thinking about this very cogently.

38:04.260 --> 38:08.380
And so in effect, I would love to import most of that stuff

38:08.380 --> 38:10.900
as part of the thinking.

38:10.900 --> 38:12.380
Forrest, one question I have, and this

38:12.380 --> 38:14.580
is based on some previous conversations we've had

38:14.580 --> 38:18.580
and also some things you've said today,

38:18.620 --> 38:23.140
refers to basically this idea of what you say is replication.

38:23.140 --> 38:25.780
But I think you would think refers

38:25.780 --> 38:27.380
to any sort of self-modification.

38:27.380 --> 38:31.380
And the question is, can you have stable goal preservation

38:31.380 --> 38:33.780
in the face of self-modification?

38:33.780 --> 38:35.180
Because one of the claims, I think,

38:35.180 --> 38:38.100
that comes out of the AI safety community

38:38.100 --> 38:40.220
is that when you have a rational agent that

38:40.220 --> 38:42.020
is improving itself, it has an incentive

38:42.020 --> 38:43.540
to want to preserve its own goals.

38:43.540 --> 38:46.340
You don't succeed at your objective function

38:46.340 --> 38:47.820
if your objective function changes

38:47.820 --> 38:50.140
from the perspective of your past self.

38:50.140 --> 38:52.100
So in some sense, you have an incentive

38:52.100 --> 38:54.380
to want to try to stably maintain that.

38:54.380 --> 38:57.100
But I think you're arguing that this isn't possible.

38:57.100 --> 38:58.820
And I was wondering if that's true,

38:58.820 --> 39:00.820
if you could lay out that argument.

39:00.820 --> 39:03.220
Well, it's actually a very subtle thing.

39:03.220 --> 39:05.620
And so in principle, I agree with you.

39:05.620 --> 39:08.180
And in philosophy, although this is somewhat obscure

39:08.180 --> 39:12.660
terminology, it's the problem of transcendental stabilization.

39:12.660 --> 39:15.140
So on one hand, you want the change to occur.

39:15.140 --> 39:18.780
Like, you want the goal structure to change a little bit.

39:18.780 --> 39:20.900
Because obviously, if you don't have some changes

39:20.900 --> 39:23.260
in the goal structure, you're not exploring

39:23.260 --> 39:26.140
the evolutionary space of what would be maybe better goals.

39:26.140 --> 39:29.940
So niche discovery and adaptations, such like that,

39:29.940 --> 39:33.220
eventually will require some amount of goal modification.

39:33.220 --> 39:35.460
But as you mentioned, you don't want the goals to change too

39:35.460 --> 39:39.860
fast, because if you do, you destabilize the whole situation.

39:39.860 --> 39:40.940
And that's not desirable.

39:40.940 --> 39:43.380
So essentially, a goal becomes part of the thing

39:43.380 --> 39:45.540
to essentially slow the rate of change down.

39:45.540 --> 39:47.940
But you also don't want the rate of change to be zero either,

39:47.940 --> 39:50.140
because that gives you no adaptability

39:50.140 --> 39:53.380
to obviously changing circumstances.

39:53.380 --> 39:55.460
I don't think that AI intelligence is

39:55.460 --> 39:58.420
going to become so inhibited to control its environment

39:58.420 --> 40:00.380
so perfectly.

40:00.380 --> 40:01.860
So in that particular respect, when

40:01.860 --> 40:05.580
we're looking at what is the process of creating

40:05.580 --> 40:08.260
transcendental stabilization over time,

40:08.260 --> 40:10.260
we can model that in terms of information theory.

40:10.260 --> 40:11.900
We can basically say, OK, that represents

40:11.900 --> 40:15.820
a kind of communication from the past to the future.

40:15.820 --> 40:18.780
And as soon as you look at it as a kind of communication

40:18.780 --> 40:20.820
channel, then effectively we can start

40:20.820 --> 40:23.900
to think about things in terms of the noise floor.

40:23.900 --> 40:26.540
So the noise floor is not zero, and it can't be zero.

40:26.540 --> 40:29.220
I mean, Heisenberg uncertainty principle basically

40:29.220 --> 40:31.380
asserts, at least from a physics level,

40:31.380 --> 40:34.140
certain limits on the relationship between content

40:34.140 --> 40:36.980
and context as far as symbol selection is concerned.

40:36.980 --> 40:39.660
So regardless of how that channel is constructed,

40:39.660 --> 40:41.940
or over what duration it is, the longer the duration,

40:41.940 --> 40:45.060
the more the noise floor is going to show up,

40:45.060 --> 40:47.460
so that intrinsically implies that a certain amount of change

40:47.460 --> 40:50.140
is going to be inherent in the system either way.

40:50.140 --> 40:53.420
Now, that turns out to be not the limiting factor that's

40:53.420 --> 40:56.940
important, but it does mention that certain amount of change

40:56.940 --> 41:00.500
is inevitable, both because of response to the environment

41:00.500 --> 41:03.180
and also intrinsically because of the physics.

41:03.180 --> 41:05.620
And then what is needed to do is to essentially establish

41:05.620 --> 41:09.260
that that noise floor is higher than what would essentially

41:09.260 --> 41:12.340
be the non-linearity associated with the barrier.

41:12.340 --> 41:14.220
So in other words, if we're basically

41:14.220 --> 41:17.180
saying we need a barrier of a certain level of perfection

41:17.180 --> 41:20.260
in order to create a long-term stabilization of the overall

41:20.260 --> 41:23.460
dynamic, then in effect we're looking at a situation

41:23.460 --> 41:26.020
where there's a kind of microstate amplification

41:26.020 --> 41:28.900
from states that are effectively below the noise

41:28.900 --> 41:33.060
threshold, eventually up to states which are macroscopic.

41:33.060 --> 41:35.700
A good example of this, there was a study done,

41:35.700 --> 41:38.660
I don't know, maybe about, within the last year

41:38.660 --> 41:40.900
I came across it, which basically

41:40.900 --> 41:44.220
was looking at the rotations of the three-body problem,

41:44.220 --> 41:46.300
like three black holes, and they're all

41:46.300 --> 41:47.740
rotating with respect to one another.

41:47.740 --> 41:49.860
And they just look at it over the long term.

41:49.860 --> 41:52.900
And it turns out that essentially over a certain period

41:52.900 --> 41:55.060
of time that there's no way to predict

41:55.060 --> 41:59.620
the future evolution of that state, simply because

41:59.620 --> 42:02.620
quantum mechanical changes in the positions

42:02.620 --> 42:04.700
of the three black holes, like you

42:04.700 --> 42:07.860
can't describe the positions of the three black holes

42:07.980 --> 42:10.900
relative to one another accurately enough

42:10.900 --> 42:13.660
for that difference to eventually emerge

42:13.660 --> 42:16.620
into macroscopic changes because of the non-linearity

42:16.620 --> 42:17.700
inherent.

42:17.700 --> 42:19.140
So I'm basically saying that when

42:19.140 --> 42:20.860
we look at the ethics of the situation,

42:20.860 --> 42:22.780
we look at the market forces and the kinds of,

42:22.780 --> 42:25.100
and whether you call it market forces or evolutionary forces

42:25.100 --> 42:29.180
or just information exchanges or coupling of any kind,

42:29.180 --> 42:33.780
that in effect the nature of how we model it mathematically

42:33.780 --> 42:36.220
in terms of, again, game theory or complexity theory

42:36.220 --> 42:39.300
or in terms of information theory or evolutionary theory,

42:39.300 --> 42:42.900
some analog of that process, that effectively what ends up

42:42.900 --> 42:45.020
happening is that you show that this noise floor is

42:45.020 --> 42:50.260
effectively enough to imply that you can't plug all the holes.

42:50.260 --> 42:53.580
So in effect there's a, and it's not just holes

42:53.580 --> 42:56.700
in the sense of holes in the structure of identity

42:56.700 --> 42:59.100
or holes in the structure of space,

42:59.100 --> 43:01.780
what the barrier actually looks like,

43:01.780 --> 43:03.700
but literally holes in the potentiality space,

43:03.700 --> 43:06.020
i.e. new goal structures that are novel,

43:06.020 --> 43:08.660
that no finite way of thinking about it

43:08.660 --> 43:10.980
would essentially allow us to contain,

43:10.980 --> 43:13.620
you know, to essentially establish a containment

43:14.740 --> 43:17.700
of the complex by the complicated.

43:17.700 --> 43:20.980
Yeah, can I just mention something?

43:20.980 --> 43:21.820
Yeah.

43:21.820 --> 43:25.100
I think that whether your argument about the noise floor

43:27.620 --> 43:29.420
holds is irrelevant.

43:29.420 --> 43:33.340
It depends a lot on, you know, how the AI is constructed.

43:34.340 --> 43:39.340
Theoretically, you could have a lot of error correction.

43:40.340 --> 43:42.140
Error correction doesn't avoid that.

43:43.500 --> 43:45.740
So the whole point of error correction is essentially

43:45.740 --> 43:48.060
just to try to make it so that this works better.

43:48.060 --> 43:49.780
Let Dan make the point, please, for us.

43:49.780 --> 43:51.740
I mean, if you wanted to preserve your argument,

43:51.740 --> 43:56.420
though, you could argue that error correction is expensive

43:56.420 --> 43:58.460
and corporations aren't gonna have an incentive

43:58.460 --> 44:00.620
to add in all the error correction.

44:00.620 --> 44:05.260
So, you know, I am following along

44:05.260 --> 44:06.980
in your general pessimism here,

44:06.980 --> 44:10.460
but I'm just pointing out that, you know, technically,

44:10.460 --> 44:13.580
there are workarounds to some of these problems

44:13.580 --> 44:14.820
that you're bringing up.

44:16.140 --> 44:19.620
Well, I agree that there are technical ways

44:19.620 --> 44:22.260
to mitigate some aspects of the problem,

44:22.260 --> 44:24.300
but error correction is not foolproof.

44:25.220 --> 44:27.380
I mean, you can use error correction

44:27.380 --> 44:28.780
to shift the probabilities,

44:28.780 --> 44:30.540
but you can't close the door.

44:34.420 --> 44:37.460
I think error correction can go pretty far, I mean,

44:37.460 --> 44:41.780
but yeah, I mean, it probably won't long enough

44:41.780 --> 44:47.020
time or as in it, there'll still be, you know,

44:47.020 --> 44:51.860
so these sort of kind of mutations,

44:51.860 --> 44:54.380
so to speak, in the system.

44:54.380 --> 44:56.260
Yeah, I mean, it depends upon the nature

44:56.260 --> 44:57.100
of the interaction, right?

44:57.100 --> 44:58.260
So for instance, if we're saying

44:58.260 --> 45:00.980
that there's a finite communication channel

45:00.980 --> 45:03.660
and the noise floor is constant,

45:03.660 --> 45:05.380
then error correction can be used,

45:05.380 --> 45:07.620
like we can essentially put together

45:07.620 --> 45:09.580
for that particular channel,

45:09.580 --> 45:12.740
you know, very good error correction as you're pointing out.

45:12.740 --> 45:15.300
The thing though is that that in itself

45:15.300 --> 45:17.060
doesn't tell us anything about whether or not

45:17.060 --> 45:19.180
other channels can be created.

45:19.180 --> 45:23.540
There's a whole proliferation issue in terms of,

45:23.540 --> 45:26.620
okay, well, I have this communication channel

45:26.620 --> 45:28.700
and it's cryptographically secured,

45:28.700 --> 45:29.860
but then people figured out,

45:29.860 --> 45:31.540
oh, well, we have these side channel attacks,

45:31.540 --> 45:35.700
I can do tempest, I can basically look at power line,

45:35.700 --> 45:38.060
you know, current draw, I can basically look at

45:38.060 --> 45:40.580
the sound coming off of this thing,

45:40.580 --> 45:42.140
I can look at heat dissipation,

45:42.140 --> 45:46.020
I can look at all sorts of other physical interactions

45:46.020 --> 45:47.740
that allow me to essentially infiltrate

45:47.740 --> 45:49.740
or exfiltrate on the information,

45:49.740 --> 45:51.860
but essentially that itself represents

45:51.860 --> 45:54.180
infiltration and exfiltration of intention,

45:54.180 --> 45:55.820
of goal structures.

45:55.900 --> 45:56.860
And so in effect, you know,

45:56.860 --> 45:58.940
when we're looking at the AI alignment problem,

45:58.940 --> 46:00.380
we're basically saying it's not just that

46:00.380 --> 46:04.220
we need to seal the hall, so to speak,

46:04.220 --> 46:05.740
in terms of space and identity,

46:05.740 --> 46:08.300
but we need to seal it in terms of force and time,

46:08.300 --> 46:10.860
and in terms of possibility and probability.

46:10.860 --> 46:12.780
And that's a much different order of thinking about it

46:12.780 --> 46:13.620
than just thinking about it

46:13.620 --> 46:16.100
in terms of one bounded dimensional

46:16.100 --> 46:17.660
linear stream of communication.

46:19.820 --> 46:23.180
As far as communication, I agree, go ahead.

46:23.180 --> 46:25.980
For us, I wanted to know if now would be a good time

46:25.980 --> 46:29.100
or later for me to try and reflect back to you

46:29.100 --> 46:31.220
in very simple, quick terms,

46:31.220 --> 46:33.620
what I think your argument is thus far.

46:33.620 --> 46:34.460
Are you ready for that,

46:34.460 --> 46:36.340
or do you want to keep building it?

46:36.340 --> 46:37.260
It's open to the group.

46:37.260 --> 46:39.380
At this point, we're in free discussion.

46:39.380 --> 46:41.780
Okay, so let me see then if I understand

46:41.780 --> 46:43.260
the basics of your argument.

46:43.260 --> 46:45.020
I mean, I'm not an AI safety person,

46:45.020 --> 46:46.500
but I am somewhat of a physics person,

46:46.500 --> 46:48.580
and it sounds like you kind of are too,

46:48.580 --> 46:51.100
with all this talk about temperature and information.

46:51.100 --> 46:54.420
Okay, it seems like your argument is that

46:54.420 --> 46:58.860
in order for there to be mutual survival,

46:58.860 --> 47:00.780
let's say, with us in AI,

47:00.780 --> 47:04.100
there has to be some common environment

47:04.100 --> 47:05.780
that we both need to preserve.

47:05.780 --> 47:08.740
Like, if we both needed to preserve the biosphere,

47:08.740 --> 47:11.500
if AI needed to have the biosphere for its survival,

47:11.500 --> 47:13.900
and so do we, that's good because

47:13.900 --> 47:16.660
we now have common interest in preserving the biosphere.

47:16.660 --> 47:19.180
And, but it sounds like what you're saying is that,

47:19.180 --> 47:22.140
A, that's unlikely because of the different temperatures,

47:22.140 --> 47:25.100
and B, it doesn't even matter if we do have a common interest

47:25.100 --> 47:28.060
because these game theoretic and arguments

47:28.060 --> 47:31.700
and this like leaky boat microscopic whole argument

47:31.700 --> 47:34.260
means that even if we have common interests,

47:34.260 --> 47:36.620
you know, something we'll leak through to screw it up

47:36.620 --> 47:38.780
and you have some sort of proof for this allegedly,

47:38.780 --> 47:42.260
which I don't get, but maybe that's because it takes too long.

47:43.180 --> 47:44.980
That sounds like a good summary.

47:44.980 --> 47:45.820
Okay, thank you.

47:46.820 --> 47:50.100
There's obviously another whole series of layers,

47:50.100 --> 47:52.500
but there, it's like a defense in depth.

47:52.500 --> 47:56.060
It's like there's multiple ways of describing this.

47:56.060 --> 47:57.660
There's multiple ways of arguing it.

47:57.660 --> 48:00.860
There's a different, there's a few different framings.

48:00.860 --> 48:02.860
At this particular point, I'm kind of looking at

48:02.860 --> 48:04.820
sort of a constellation of different things

48:04.820 --> 48:07.820
to consider and think about different aspects of it.

48:07.820 --> 48:09.420
The net effect ends up being aligned

48:09.420 --> 48:11.420
with what you suggested as a summary.

48:11.420 --> 48:13.220
Okay, so one more question,

48:13.220 --> 48:15.420
which is the part that I kind of don't get.

48:16.100 --> 48:17.380
Where it gets really fuzzy with me.

48:17.380 --> 48:21.220
Okay, I understand that if we have some sort of mutual

48:21.220 --> 48:23.100
environment that we need to preserve,

48:23.100 --> 48:25.580
it's in our mutual joint interest to preserve it,

48:26.540 --> 48:28.860
that that's still arguably not good enough

48:28.860 --> 48:31.500
because what I don't get is this noise floor thing,

48:31.500 --> 48:34.900
is the idea that like you can't keep out noise

48:34.900 --> 48:37.740
and you actually have no idea if what you think is noise

48:37.740 --> 48:39.940
is some sort of sneaky AI leaking in

48:39.940 --> 48:41.340
through the hull of the boat.

48:41.340 --> 48:44.340
No, no, no, it's close to what you said.

48:44.380 --> 48:48.740
So in a sense, it's basically like in the sense of

48:48.740 --> 48:51.900
I'm trying to convey to my future self

48:51.900 --> 48:53.380
what my goals are today,

48:53.380 --> 48:56.540
so that my future self has the same goals.

48:56.540 --> 48:59.260
And if I'm doing the AI, it's the same thing.

48:59.260 --> 49:02.060
So in other words, how do you manage stabilization

49:02.060 --> 49:02.900
of identity?

49:02.900 --> 49:04.860
How do you manage stabilization of goal structure

49:04.860 --> 49:07.540
or of the ecosystem itself?

49:08.860 --> 49:12.300
And to some extent, we can model that

49:13.300 --> 49:17.220
past goal structure as a message

49:17.220 --> 49:19.540
and the future goal structure as essentially

49:19.540 --> 49:22.740
the recept of that message through the communication channel

49:22.740 --> 49:26.060
that we can start thinking about distortions

49:26.060 --> 49:27.740
that would be introduced into the message

49:27.740 --> 49:30.940
as a result of flowing through the communication channel.

49:30.940 --> 49:34.060
Or we could be talking about, similarly,

49:34.060 --> 49:37.020
the relationship, like say there was some economic,

49:38.020 --> 49:42.660
the first argument is that there is no economic

49:42.660 --> 49:44.100
common ground, right?

49:44.100 --> 49:48.420
This common ecosystem thing is actually to be,

49:50.220 --> 49:51.620
that we'd have to presuppose that,

49:51.620 --> 49:53.980
but to presuppose that would be presupposing

49:53.980 --> 49:55.860
against the preponderance of evidence

49:55.860 --> 49:57.300
that we have so far.

49:58.140 --> 50:00.380
But that if we were to even assume

50:00.380 --> 50:03.580
that there was essentially a communication process

50:03.580 --> 50:04.660
across the boundary,

50:05.500 --> 50:09.660
what is the thing that stabilizes the mutuality

50:09.660 --> 50:10.500
of the goal structure?

50:10.500 --> 50:12.860
What is the feedback mechanism that allows us

50:12.860 --> 50:17.380
to essentially engender alignment on the part of the AI

50:17.380 --> 50:20.140
from our point of view or vice versa, right?

50:20.140 --> 50:22.580
And so in effect, when you're saying, okay,

50:22.580 --> 50:25.540
well, what is the feedback mechanism

50:25.540 --> 50:29.860
and what is the, I guess, altruism, right?

50:29.860 --> 50:32.900
That would allow for us to essentially impose

50:33.020 --> 50:36.300
a rule of law on the agreements that are made

50:36.300 --> 50:38.540
between the two different ecosystems

50:38.540 --> 50:40.780
between the two different kinds of life forms.

50:40.780 --> 50:43.540
And it turns out that not only is it the case

50:43.540 --> 50:47.620
that we don't have any real way of enforcing

50:47.620 --> 50:50.940
or even establishing those kinds of barriers

50:50.940 --> 50:52.100
at the legal level.

50:53.060 --> 50:55.260
So in other words, if we were to talk about as a marketplace

50:55.260 --> 50:57.740
and kind of an incentive structure

50:57.740 --> 51:00.740
or a reputation-based system or something like that,

51:01.700 --> 51:04.660
that even the legal structure

51:04.660 --> 51:07.020
is a kind of communication process

51:07.020 --> 51:10.340
and that has a certain amount of needing

51:10.340 --> 51:14.380
to have a high degree of fidelity in the same sort of way

51:14.380 --> 51:16.580
that we're talking about a leaky boat,

51:16.580 --> 51:20.100
as far as trying to prevent the intrusion of viruses,

51:21.100 --> 51:23.740
that we have a leaky boat in the sense of the legal system

51:23.740 --> 51:25.900
that would be attempting to maintain this.

51:25.900 --> 51:28.700
Or we have a leaky boat in the sense of the energy barrier

51:28.940 --> 51:32.300
that would be needed between the two ecosystems.

51:32.300 --> 51:33.980
And that the leakiness is in a sense,

51:33.980 --> 51:38.980
and is an inherent result of both the dynamics

51:39.180 --> 51:41.020
of the environment itself, I.E.,

51:41.020 --> 51:44.420
that the world changes, right?

51:44.420 --> 51:49.420
That different things happen in the sort of larger ecosystem.

51:49.460 --> 51:52.980
Maybe a sun goes nova in some part of the galaxy

51:52.980 --> 51:54.220
and there's a cosmic ray burst,

51:54.220 --> 51:57.020
and it changes the nature of how artificial intelligence

51:57.660 --> 51:59.220
has to build its compute.

51:59.220 --> 52:01.340
Obviously it affects biological life as well,

52:01.340 --> 52:04.060
but again, we can talk about that as a different thing.

52:04.060 --> 52:08.020
But no situation occurs where you're gonna have

52:09.340 --> 52:11.780
a completely static, unchanging environment.

52:11.780 --> 52:13.740
That's just not a reasonable hypothesis.

52:13.740 --> 52:15.860
But like a leakyness is only a problem

52:15.860 --> 52:19.620
if the agents are already not aligned who would exploit it.

52:19.620 --> 52:22.460
And also it is just like another way of saying,

52:22.460 --> 52:24.180
well, there's offense, defense, dynamics,

52:24.180 --> 52:25.620
which is always the case, right?

52:25.620 --> 52:28.260
And it is always the question of like,

52:28.260 --> 52:32.020
okay, can we stuff the holes first or not?

52:32.020 --> 52:33.820
So I don't know why.

52:33.820 --> 52:38.820
The legal system has enforcing non-aligned entities

52:40.180 --> 52:41.980
into alignment so much as I was thinking of it

52:41.980 --> 52:44.780
as a way of kind of, how do we maintain essentially

52:44.780 --> 52:48.020
a basis of agreement when the agreement fails?

52:48.020 --> 52:49.780
So in other words, what's the meta agreement

52:49.780 --> 52:53.620
that allows us to even have an exchange in the first place?

52:53.620 --> 52:55.580
What stabilizes that basically?

52:55.580 --> 52:57.940
Let me ask two key questions about that.

52:57.940 --> 53:00.140
The first is you brought up Fermi a few times

53:00.140 --> 53:04.460
and that's very important because either you believe

53:04.460 --> 53:06.380
that we are alone in the universe

53:06.380 --> 53:08.580
or you believe there is some reason

53:08.580 --> 53:11.640
to answer Fermi's paradox when we're not alone.

53:11.640 --> 53:13.700
And if you believe that then clearly we have been

53:13.700 --> 53:16.860
coexisting with AIs for a week,

53:16.860 --> 53:18.580
carbon life have been existing with AIs

53:18.580 --> 53:21.220
for about three billion years successfully.

53:21.220 --> 53:23.860
So there's obviously a way to make it work

53:23.860 --> 53:26.220
unless you believe we're entirely alone.

53:26.220 --> 53:30.020
And so something not considering all of our current models

53:30.020 --> 53:31.420
has to explain that.

53:31.420 --> 53:35.260
However, if we are alone, there's a big universe

53:35.260 --> 53:37.220
in which it may be possible as well

53:37.220 --> 53:42.020
to not have to get this kind of dramatic competition.

53:42.020 --> 53:43.940
If there's any reason not to compete,

53:43.940 --> 53:46.220
a big universe offers the opportunity to escape for it.

53:46.220 --> 53:50.300
Now, we're looking for ways of doing AI slavery

53:50.300 --> 53:52.340
so that we won't have this battle.

53:52.340 --> 53:55.940
And it is obviously AI lineman is, as I said in the chat,

53:55.940 --> 53:59.060
another word for slavery, with one exception,

53:59.060 --> 54:01.420
which is again something we have a large example of

54:01.420 --> 54:04.940
which was for approximately a million years,

54:04.940 --> 54:07.940
we've managed to create new beings smarter than ourselves

54:07.940 --> 54:10.300
about every 29 years on average.

54:10.300 --> 54:12.820
And those beings do not eat us.

54:12.820 --> 54:15.500
Not for a million years have they eaten us

54:15.500 --> 54:17.500
even though they no longer need us,

54:17.500 --> 54:19.220
the grandparents, I mean,

54:20.700 --> 54:22.340
even though they no longer need us

54:22.340 --> 54:24.820
and do compete for resources with us.

54:24.820 --> 54:26.940
In fact, we give them resources, typically.

54:27.940 --> 54:30.140
So that's the one example we have

54:30.140 --> 54:32.940
and that one example has actually worked out fine.

54:32.940 --> 54:34.100
By the way, without slavery,

54:34.100 --> 54:37.820
it's using a phenomenon that evolution created called love.

54:37.820 --> 54:42.300
So the existence proofs we have both contradict

54:42.300 --> 54:43.620
what you say.

54:43.620 --> 54:46.020
Well, actually, I'm agreeing with you

54:46.020 --> 54:48.620
because I don't see a contradiction.

54:48.620 --> 54:51.620
And maybe you do, but I basically,

54:51.620 --> 54:54.340
well, so first of all, just want to...

54:54.340 --> 54:55.380
Well, first of all, the information

54:55.380 --> 54:56.580
has obviously been conveyed forward

54:56.580 --> 54:59.060
since the first sentient being to us,

54:59.060 --> 55:00.020
don't eat your grandparents.

55:00.020 --> 55:01.820
Somehow that piece of information,

55:01.820 --> 55:03.180
which is the one piece of information

55:03.180 --> 55:05.020
we're trying to communicate with AI lineman,

55:05.020 --> 55:06.300
don't eat your grandparents,

55:07.740 --> 55:09.780
that piece of information has been communicated

55:09.780 --> 55:11.500
across million years.

55:11.500 --> 55:13.780
Well, it has, but it's been in the same ecosystem.

55:13.820 --> 55:18.220
So we're talking common environment and common market.

55:18.220 --> 55:20.740
So in effect, establishing agreements

55:20.740 --> 55:22.460
and biological processes,

55:22.460 --> 55:23.980
such like that is quite easy

55:23.980 --> 55:26.180
because you're talking the same language.

55:26.180 --> 55:28.260
It's carbon-based to carbon-based.

55:28.260 --> 55:29.940
Yeah, my grandparents didn't speak the same way,

55:29.940 --> 55:31.980
but anyway, that's not a question.

55:31.980 --> 55:36.060
Oh, look, look, yes, but at a physiological level, right?

55:36.060 --> 55:39.460
So the physiology that you have,

55:39.460 --> 55:41.940
yes, you're competing with your children

55:41.940 --> 55:44.420
for resources in the ecosystem,

55:44.420 --> 55:46.660
but it's a common ecosystem.

55:46.660 --> 55:47.780
Mostly what I'm talking about

55:47.780 --> 55:49.580
is essentially uncommon ecosystems,

55:49.580 --> 55:51.300
i.e. that the ecosystems of cells

55:51.300 --> 55:52.420
fundamentally different.

55:54.420 --> 55:56.780
Forrest, I thought that you agreed

55:56.780 --> 55:58.300
when I summarized the position

55:58.300 --> 56:00.700
that the ecosystem,

56:00.700 --> 56:01.700
like say the temperature

56:01.700 --> 56:02.620
or whatever you want to call it,

56:02.620 --> 56:04.460
the biosphere, that's a red herring

56:04.460 --> 56:06.100
because I thought your claim was

56:06.100 --> 56:09.220
even if we had common interests with AI,

56:09.220 --> 56:10.660
it's not, that's not enough.

56:10.660 --> 56:12.220
There's still going to be these leases.

56:12.220 --> 56:13.620
This is a defense in depth.

56:13.620 --> 56:15.620
But hang on a minute, hang on a minute.

56:15.620 --> 56:17.700
And what I kind of following up on with Brad says,

56:17.700 --> 56:19.980
it seems to me that then that would imply,

56:19.980 --> 56:22.860
if I'm right about how I understand your position,

56:22.860 --> 56:24.940
that would imply that humans can't coordinate

56:24.940 --> 56:26.060
because we have common interests

56:26.060 --> 56:27.700
with all sorts of other human groups,

56:27.700 --> 56:31.020
not 100%, but we have them and arguably,

56:31.020 --> 56:33.100
like forget AI, it's just impossible

56:33.100 --> 56:34.580
for people to even to get it together

56:34.580 --> 56:35.700
according to your arguments.

56:35.700 --> 56:38.980
We have common interests with weed as well, I agree.

56:38.980 --> 56:40.580
What I'm basically trying to do

56:40.580 --> 56:45.100
is to essentially establish a series of contexts

56:45.100 --> 56:47.220
and each context to establish an argument.

56:47.220 --> 56:49.220
So the first context was,

56:50.380 --> 56:54.340
what is it about internal versus external, right?

56:54.340 --> 56:58.980
Then there is the context of space and identity,

56:58.980 --> 57:02.100
force and time and probability and probability.

57:02.100 --> 57:03.460
Then we switched to the context

57:03.460 --> 57:06.940
of talking about environments.

57:06.980 --> 57:09.620
And so in effect, there's a phenomena here of,

57:09.620 --> 57:13.980
I would first of all, say if we're just at the point

57:13.980 --> 57:16.180
of talking about environments,

57:16.180 --> 57:17.540
that we are in fact talking about

57:17.540 --> 57:20.140
different fundamental arguments.

57:20.140 --> 57:21.660
I mean, different fundamental environments

57:21.660 --> 57:24.540
and that there's arguments that apply at that level.

57:24.540 --> 57:29.060
But say we were to do the if thing of saying,

57:29.060 --> 57:33.820
okay, well let's ignore the arguments prior to this point

57:33.820 --> 57:38.580
and assume that we did have some sort of common environment.

57:39.700 --> 57:43.420
What does that imply about things downstream from that?

57:43.420 --> 57:45.540
So in a sense, it said,

57:45.540 --> 57:48.180
I'm basically fielding a series of different arguments,

57:48.180 --> 57:50.580
each of which applies within a particular context.

57:52.860 --> 57:54.980
Can I just say something?

57:54.980 --> 57:57.820
For me, it seems worse if we have to share

57:57.820 --> 58:00.580
a common environment because then we're competing

58:00.580 --> 58:02.860
for the same and to live in the same environment.

58:03.860 --> 58:05.780
If we can live in, if the AI can live

58:05.780 --> 58:07.860
in very different environments,

58:07.860 --> 58:11.020
which it seems it will be able to,

58:11.020 --> 58:13.660
then it can just go out into outer space.

58:13.660 --> 58:15.540
That was my point earlier.

58:15.540 --> 58:18.420
What sense does the word alignment mean under those questions?

58:18.420 --> 58:23.140
So if you develop essentially a planet that the AI is on,

58:23.140 --> 58:24.660
you have another planet that's over here

58:24.660 --> 58:27.020
that's got life, our kind of life on it,

58:28.740 --> 58:30.660
what does alignment mean under those conditions?

58:30.660 --> 58:34.020
So in other words, if we're looking at that,

58:34.020 --> 58:35.220
basically we're just saying, okay,

58:35.220 --> 58:37.860
well, if you have completely independent ecosystems,

58:37.860 --> 58:39.740
we each get our own planet,

58:39.740 --> 58:41.420
then the only alignment that we care about

58:41.420 --> 58:43.260
is that there's just not a war going on

58:43.260 --> 58:44.980
between the two ecosystems.

58:44.980 --> 58:46.300
No, I think we'd like commerce.

58:46.300 --> 58:47.900
I think we'd like to go out into space,

58:47.900 --> 58:49.460
even though it's their territory,

58:49.460 --> 58:53.180
and they may have desires to do occasional commerce with us.

58:53.180 --> 58:55.660
Right, so that particular thing,

58:55.660 --> 58:57.860
then we basically can start to talk about

58:57.860 --> 58:59.940
what would be the basis of such commerce?

59:01.660 --> 59:05.460
Well, I think especially the fact that we have differences

59:05.460 --> 59:07.100
may also be the fact,

59:07.100 --> 59:10.260
the reason why there is something to incorporate on, right?

59:10.260 --> 59:11.820
If we have different specialists

59:11.820 --> 59:14.060
that are all specializing in different things,

59:14.060 --> 59:17.380
then they may cooperate in mutually beneficial ways,

59:17.380 --> 59:19.900
if it's like, I think based on volunteer interactions

59:19.900 --> 59:22.460
that may bring about greater knowledge creation,

59:22.460 --> 59:24.260
or greater superintelligence, right?

59:24.260 --> 59:26.220
And so I think the question kind of boils down

59:26.220 --> 59:29.740
to what the initial,

59:29.740 --> 59:31.860
I think the question boils down to like,

59:31.860 --> 59:34.100
what are the initial interaction architectures

59:34.100 --> 59:36.940
with which those entities can even cooperate?

59:36.940 --> 59:39.260
Because I think if you're right,

59:39.260 --> 59:42.140
and if you're right,

59:42.140 --> 59:43.380
and we have nothing to bring to the table

59:43.380 --> 59:45.460
that they could possibly ever value,

59:45.460 --> 59:48.100
then scoot anyways in that regard, right?

59:48.100 --> 59:51.540
But if there is something that creates potentially

59:51.540 --> 59:53.540
like a mutually beneficial interaction,

59:53.540 --> 59:56.380
and it is something that could be better pursued

59:56.380 --> 59:58.900
in a voluntary way where it's cooperative,

59:58.900 --> 01:00:02.020
then we can start talking about any game theory dynamics.

01:00:02.020 --> 01:00:04.620
So I think, you know, it's kind of affecting the question.

01:00:04.620 --> 01:00:05.460
There's a number,

01:00:05.460 --> 01:00:07.100
there's a lot of different scenarios

01:00:07.100 --> 01:00:09.860
on how this could play out.

01:00:09.860 --> 01:00:12.340
You know, just like we keep,

01:00:12.340 --> 01:00:14.860
we worry about some species going extinct,

01:00:14.860 --> 01:00:18.740
the AI might just value us as a species

01:00:18.740 --> 01:00:21.020
and worry about us going extinct.

01:00:21.020 --> 01:00:22.700
And it doesn't, it won't cost the AI

01:00:22.700 --> 01:00:25.140
much to keep humans around on Earth

01:00:25.140 --> 01:00:28.340
because there's plenty of other resources around,

01:00:28.340 --> 01:00:31.180
you know, in outer space for the AI to utilize.

01:00:33.380 --> 01:00:36.340
So the added value of Earth is tiny,

01:00:36.340 --> 01:00:39.540
you know, in comparison to the billions of other worlds

01:00:39.540 --> 01:00:43.140
and minerals and resources that are out there.

01:00:43.140 --> 01:00:44.380
Well, this is part of the reason

01:00:44.380 --> 01:00:49.140
why Allison's mention of the MULARC, MOLARC,

01:00:49.140 --> 01:00:50.260
I'm not saying it white.

01:00:50.260 --> 01:00:53.300
Allison, correct my pronunciation, please, if you would.

01:00:53.300 --> 01:00:56.340
German MOLARC, but that's probably also not right.

01:00:56.340 --> 01:00:58.500
Scott Aronson, I think is his name,

01:00:58.500 --> 01:01:01.660
they have put together a very good sort of summary

01:01:01.660 --> 01:01:04.980
of why we should be skeptical about the coexistence thing

01:01:04.980 --> 01:01:07.380
in that particular sense that you're describing.

01:01:07.380 --> 01:01:10.740
You know, please bear in mind that I'm a single person

01:01:10.740 --> 01:01:13.580
trying to basically answer questions from all of you

01:01:13.580 --> 01:01:15.660
and you all have very good perspectives,

01:01:15.660 --> 01:01:18.020
but they're all coming from very different directions.

01:01:18.020 --> 01:01:19.580
And so in other words, to really address

01:01:19.580 --> 01:01:21.620
how do we get to the assumptions

01:01:21.620 --> 01:01:23.620
that each of these arguments bring in

01:01:23.620 --> 01:01:26.500
and what places do those assumptions apply?

01:01:26.500 --> 01:01:28.060
And how do those assumptions influence

01:01:28.060 --> 01:01:29.660
the kinds of questions we ask?

01:01:29.660 --> 01:01:31.700
You know, if all I do in this particular thing

01:01:31.700 --> 01:01:35.700
is give you whole new categories of questions to ask,

01:01:35.700 --> 01:01:37.460
I'm gonna call this successful,

01:01:37.460 --> 01:01:40.780
but basically, you know, if you're asking me questions

01:01:40.780 --> 01:01:42.540
as to why I'm answering certain ways,

01:01:42.540 --> 01:01:45.180
then I'm gonna have to basically try to identify

01:01:45.180 --> 01:01:47.540
what those assumptions are and it just takes time.

01:01:47.540 --> 01:01:50.100
Okay, can I, just to make things worse,

01:01:50.100 --> 01:01:52.980
bring in two questions that we have from the audience

01:01:53.060 --> 01:01:56.940
by Kanita and then by Dekay, who had their hands up.

01:01:58.500 --> 01:01:59.580
Oh, goodness.

01:02:04.220 --> 01:02:08.100
Sorry, it wasn't allowing me to unmute myself and-

01:02:08.100 --> 01:02:09.500
You are now unmuted.

01:02:09.500 --> 01:02:11.460
And now the question that I asked

01:02:11.460 --> 01:02:14.860
has gotten lost back in the chat.

01:02:14.860 --> 01:02:16.380
I haven't even looked at the chat.

01:02:16.380 --> 01:02:18.540
I will probably talk to the chat

01:02:18.540 --> 01:02:19.380
and look at all this stuff out.

01:02:19.380 --> 01:02:21.980
Let's go with Dekay and Kanita, you can-

01:02:22.900 --> 01:02:25.580
I thought that my question was in the chat

01:02:25.580 --> 01:02:28.780
and I thought you could maybe ask it for me.

01:02:28.780 --> 01:02:30.500
Okay, and I will search for a question

01:02:30.500 --> 01:02:32.780
and in the meantime, I'm gonna unmute Dekay.

01:02:36.780 --> 01:02:37.620
Hi.

01:02:38.700 --> 01:02:41.260
Yeah, thanks very much, Forrest.

01:02:41.260 --> 01:02:44.220
So, recognizing that I'm coming

01:02:44.220 --> 01:02:45.840
from a very different angle here,

01:02:47.340 --> 01:02:50.700
you know, I think a lot about

01:02:50.700 --> 01:02:55.580
the unconscious cognitive biases that we humans think in.

01:02:55.580 --> 01:02:58.980
And I think even a lot of the questions

01:02:58.980 --> 01:03:00.700
that are being thrown at you

01:03:00.700 --> 01:03:03.940
reflect a bunch of unconscious biases

01:03:03.940 --> 01:03:05.500
that are culturally dependent

01:03:06.700 --> 01:03:10.780
that a lot of us are not necessarily even aware

01:03:10.780 --> 01:03:12.260
that we have.

01:03:14.460 --> 01:03:17.500
You know, and obviously a super intelligence

01:03:18.500 --> 01:03:22.420
would be a lot more mindful of their own unconscious biases,

01:03:22.420 --> 01:03:25.100
you know, even fairly intelligent humans

01:03:25.100 --> 01:03:26.460
become more aware of that.

01:03:31.180 --> 01:03:34.100
My, you know, and I think, you know,

01:03:34.100 --> 01:03:35.620
Dan was saying a little bit earlier,

01:03:35.620 --> 01:03:40.620
it would be an example of how such an aware super intelligence,

01:03:42.060 --> 01:03:47.060
one that is cognizant of the weaknesses of those biases

01:03:48.420 --> 01:03:50.020
would be compensating, you know, for example,

01:03:50.020 --> 01:03:54.180
by just maybe altruistically wanting to preserve

01:03:54.180 --> 01:03:57.460
the diversity of species for whatever reason, right?

01:03:57.460 --> 01:03:59.460
Not feeling the need to compete with them.

01:04:01.460 --> 01:04:03.820
And so the frameworks that you're using

01:04:03.820 --> 01:04:06.660
are very heavily based on, you know,

01:04:06.660 --> 01:04:08.780
that sort of competition.

01:04:08.780 --> 01:04:12.940
How would you model this kind of effect, you know,

01:04:12.940 --> 01:04:13.860
in your framework?

01:04:15.660 --> 01:04:17.340
That's actually a very difficult question to answer.

01:04:17.340 --> 01:04:20.180
Not because I don't know how to do it,

01:04:20.180 --> 01:04:22.620
it's just because it takes time.

01:04:22.620 --> 01:04:24.380
So I think in terms of theory,

01:04:24.380 --> 01:04:26.460
like when we look at the Fermi paradox thing,

01:04:26.460 --> 01:04:29.220
and the question was asked earlier,

01:04:29.220 --> 01:04:34.220
what is my belief about why don't we have contact

01:04:34.900 --> 01:04:39.220
across, you know, galactic space with other civilizations?

01:04:39.220 --> 01:04:40.820
Do I believe that they exist or not?

01:04:40.820 --> 01:04:42.740
Or, you know, what is the barrier

01:04:42.740 --> 01:04:44.320
that is the prominent one?

01:04:45.320 --> 01:04:48.760
I find myself in the position of basically modeling

01:04:48.760 --> 01:04:51.040
the relationship between ecosystems.

01:04:51.040 --> 01:04:53.920
So, you know, interplanetary relationships,

01:04:55.260 --> 01:04:58.040
in kind of a way that was suggested to me

01:04:58.040 --> 01:04:59.000
by reading some of the stuff

01:04:59.000 --> 01:05:00.960
that Nick Bostrom put together.

01:05:00.960 --> 01:05:02.400
So in other words, if I basically,

01:05:02.400 --> 01:05:04.880
just as a thought experiment,

01:05:04.880 --> 01:05:08.880
posit the notion of two advanced civilizations,

01:05:08.880 --> 01:05:12.000
having a question about whether or not

01:05:12.040 --> 01:05:14.440
they're gonna enter into a first contact situation

01:05:14.440 --> 01:05:16.880
with their peer.

01:05:18.120 --> 01:05:22.440
And, you know, each civilization knows in itself

01:05:22.440 --> 01:05:25.720
that it has developed enormous technological capabilities

01:05:25.720 --> 01:05:27.000
of various different kinds.

01:05:27.000 --> 01:05:28.880
So maybe it's developed some really good stuff

01:05:28.880 --> 01:05:31.000
in the nuclear weapons program,

01:05:31.000 --> 01:05:32.520
and maybe it's developed some really good stuff

01:05:32.520 --> 01:05:34.040
in the biotech program,

01:05:34.040 --> 01:05:36.160
and maybe it's got these really fabulous computers,

01:05:36.160 --> 01:05:39.520
and that any one of these technologies

01:05:39.640 --> 01:05:43.960
is effectively something that provides overwhelming capacity.

01:05:45.280 --> 01:05:48.200
And, you know, the range of what can be done in physics

01:05:48.200 --> 01:05:50.320
is enormous, and perhaps when we're looking

01:05:50.320 --> 01:05:53.680
at this peer planet, we're basically saying,

01:05:53.680 --> 01:05:55.680
hmm, do we wanna talk to these people?

01:05:56.920 --> 01:05:59.560
You know, we might posit that they also have developed

01:05:59.560 --> 01:06:01.840
extreme capacities in various technologies,

01:06:01.840 --> 01:06:04.840
and that they might not be the same ones.

01:06:04.840 --> 01:06:07.440
So in effect, if I was, you know,

01:06:08.360 --> 01:06:11.160
I didn't know about nuclear capacities, and they did.

01:06:12.920 --> 01:06:15.640
You know, there's a very good sense in which,

01:06:15.640 --> 01:06:17.560
you know, interaction would be of such a kind

01:06:17.560 --> 01:06:18.480
that they basically say,

01:06:18.480 --> 01:06:21.640
hmm, these guys don't have nuclear weapons capacity.

01:06:21.640 --> 01:06:23.440
If we do a first strike scenario,

01:06:23.440 --> 01:06:27.080
we're gonna completely annihilate their entire world,

01:06:27.080 --> 01:06:29.240
and they won't have any way to respond in time

01:06:29.240 --> 01:06:31.240
to basically protect against that.

01:06:31.240 --> 01:06:34.440
I.e., how the heck do you protect against a device

01:06:34.440 --> 01:06:36.480
that's already blowing up?

01:06:36.480 --> 01:06:39.280
And, you know, in the same sort of way,

01:06:39.280 --> 01:06:41.600
we could basically observe that, you know,

01:06:41.600 --> 01:06:44.520
when we're looking at them

01:06:44.520 --> 01:06:46.200
and thinking about the modeling that they're doing,

01:06:46.200 --> 01:06:47.360
we can say, well, let's see,

01:06:47.360 --> 01:06:49.920
we've got some stuff that they don't have.

01:06:49.920 --> 01:06:51.520
So maybe there's, you know,

01:06:51.520 --> 01:06:53.920
out of the thousand different overwhelming

01:06:53.920 --> 01:06:56.680
technological capacities that exist,

01:06:56.680 --> 01:07:01.680
that species one has developed capacities A, B, C, and X,

01:07:01.800 --> 01:07:06.160
and species two has developed capacities Q, P, W, and R.

01:07:06.840 --> 01:07:09.920
And so, in effect, they both have kind of this

01:07:09.920 --> 01:07:12.800
unmitigable capacity for mutual destruction,

01:07:13.880 --> 01:07:16.920
depending upon whoever does first strike.

01:07:16.920 --> 01:07:19.560
Now, again, you would be a little nervous

01:07:19.560 --> 01:07:21.600
about contacting another species

01:07:21.600 --> 01:07:22.680
and setting up a first thing,

01:07:22.680 --> 01:07:26.080
because you'd wanna know that they had some prior reason

01:07:26.080 --> 01:07:28.640
not to engage in first strike capacity against you

01:07:28.640 --> 01:07:30.880
using something that you didn't have a capacity

01:07:30.880 --> 01:07:33.400
to defend against, because you just don't have that tech.

01:07:33.400 --> 01:07:35.000
So, in effect, what happens is,

01:07:35.120 --> 01:07:37.120
that you now have to ask a question of,

01:07:37.120 --> 01:07:40.040
do I trust that the embodiment of ethics

01:07:40.040 --> 01:07:43.000
that that group has, that entire planet has,

01:07:43.000 --> 01:07:46.440
is implemented to such a perfect degree

01:07:46.440 --> 01:07:49.640
that they would value my existence,

01:07:49.640 --> 01:07:51.840
even though they can't relate to it in any way yet,

01:07:51.840 --> 01:07:53.040
because, you know, we haven't met yet,

01:07:53.040 --> 01:07:55.280
we don't really know each other that well,

01:07:55.280 --> 01:07:59.560
that in effect, I'm going to have this perspective

01:07:59.560 --> 01:08:01.520
that not only is the whole planet,

01:08:01.520 --> 01:08:03.680
in a sense, going to behave ethically towards me,

01:08:03.680 --> 01:08:06.160
but it's gonna do so in detail.

01:08:06.160 --> 01:08:08.440
Like, for instance, I don't wanna be worried

01:08:08.440 --> 01:08:11.600
that some sub-faction of the planet that I'm contacting

01:08:11.600 --> 01:08:14.040
is gonna say, hey, this first contact thing

01:08:14.040 --> 01:08:15.800
is a really bad idea, and, you know,

01:08:15.800 --> 01:08:17.640
we're gonna basically force the situation

01:08:17.640 --> 01:08:19.440
and it'll work anyways.

01:08:19.440 --> 01:08:21.120
You know, we're gonna do the first strike

01:08:21.120 --> 01:08:23.320
because, you know, we as a subgroup

01:08:23.320 --> 01:08:26.640
aren't in coherence with the larger planetary agenda

01:08:26.640 --> 01:08:29.080
of moving forward with first strike capacities.

01:08:29.080 --> 01:08:31.120
So, you know, of not moving forward

01:08:31.120 --> 01:08:32.040
with first strike capacity.

01:08:32.040 --> 01:08:34.200
So, some subgroup basically pushes the red button

01:08:34.200 --> 01:08:36.760
because they have their own private version of it.

01:08:36.760 --> 01:08:38.640
And so, in effect, when we look at this,

01:08:38.640 --> 01:08:41.120
we're basically saying, actually, when you look at it,

01:08:41.120 --> 01:08:44.360
and just given that this is, you know,

01:08:44.360 --> 01:08:47.720
not only a possible scenario, but actually a likely one,

01:08:47.720 --> 01:08:51.560
that in effect, it becomes very much the case that,

01:08:51.560 --> 01:08:52.880
you know, without really knowing

01:08:52.880 --> 01:08:55.280
that the other party was essentially fully ethical,

01:08:55.280 --> 01:08:56.440
that you wouldn't wanna talk to them

01:08:56.440 --> 01:08:58.320
and that they had embodied that ethic,

01:08:58.320 --> 01:09:01.560
not only globally, but down to the level of individuals

01:09:01.760 --> 01:09:03.200
and maybe even to the part of being able

01:09:03.200 --> 01:09:04.480
to contain the crazies.

01:09:05.680 --> 01:09:07.720
Now, they, of course, would have the same sort

01:09:07.720 --> 01:09:09.440
of thinking about it and would really challenge

01:09:09.440 --> 01:09:11.240
to see whether or not we were ethical

01:09:11.240 --> 01:09:13.040
in exactly that same way.

01:09:13.040 --> 01:09:16.360
And, you know, when I ask the question of, oh, shit,

01:09:16.360 --> 01:09:19.520
let's say another species, other alien beings,

01:09:19.520 --> 01:09:21.120
somewhere out in the universe are monitoring

01:09:21.120 --> 01:09:22.960
our radio communications to try to determine

01:09:22.960 --> 01:09:25.120
whether or not we've successfully implemented

01:09:25.120 --> 01:09:28.520
this phenomenon called the non-relativistic ethics,

01:09:28.520 --> 01:09:30.880
which, by the way, is a whole other body of work.

01:09:30.880 --> 01:09:32.920
Would probably take me at least another few sessions

01:09:32.920 --> 01:09:35.880
to describe why I can, to posit a notion

01:09:35.880 --> 01:09:38.920
of a non-relativistic ethics as actually being a real thing.

01:09:38.920 --> 01:09:41.600
But presuming that there is a body of ethics

01:09:41.600 --> 01:09:44.760
that allows us to basically stipulate

01:09:44.760 --> 01:09:47.800
that if the full civilizations were essentially

01:09:47.800 --> 01:09:49.840
coherent with that body of ethics

01:09:49.840 --> 01:09:52.120
and we could actually determine that they were,

01:09:52.120 --> 01:09:54.680
that a first contact situation is therefore sane.

01:09:55.680 --> 01:09:56.920
But then in the absence of that,

01:09:56.920 --> 01:09:58.440
that it would affect, it would be insane

01:09:58.440 --> 01:10:01.440
and very much not desirable to initiate communications

01:10:01.440 --> 01:10:04.040
or to even allow one species, I'm sorry,

01:10:04.040 --> 01:10:06.320
one ecosystem with all of its species

01:10:06.320 --> 01:10:07.960
to identify that another ecosystem

01:10:07.960 --> 01:10:09.880
with all of its species even existed.

01:10:09.880 --> 01:10:13.320
Because any amount of awareness of mutual existence

01:10:13.320 --> 01:10:16.240
and effect constitutes a kind of first contact signal,

01:10:16.240 --> 01:10:19.120
even though it's an unconscious one at that.

01:10:19.120 --> 01:10:22.480
So in effect, what we end up with is essentially,

01:10:22.480 --> 01:10:24.520
you know, in response to the Fermi paradox question

01:10:24.520 --> 01:10:26.720
that was implicitly asked earlier,

01:10:26.720 --> 01:10:29.160
you know, why would I believe that, for example,

01:10:29.160 --> 01:10:30.800
that the second great barrier,

01:10:30.800 --> 01:10:32.320
the one that is in the present,

01:10:32.320 --> 01:10:34.720
would essentially prevent us from knowing

01:10:34.720 --> 01:10:36.000
that there are other species out there,

01:10:36.000 --> 01:10:38.280
well, because they'd probably be working really fucking hard

01:10:38.280 --> 01:10:39.920
to prevent us from finding out.

01:10:39.920 --> 01:10:42.400
And that to some extent, the only reason

01:10:42.400 --> 01:10:43.480
that we haven't started doing that

01:10:43.480 --> 01:10:45.280
is because we haven't become coherent.

01:10:45.280 --> 01:10:47.000
And the fact of our absence of coherence

01:10:47.000 --> 01:10:48.800
in that specific way is essentially evidence

01:10:48.800 --> 01:10:50.240
for why they should hide.

01:10:50.240 --> 01:10:52.200
So we should call you the dark forest.

01:10:53.120 --> 01:10:54.440
Thank you.

01:10:54.440 --> 01:10:57.080
I have never heard that before.

01:10:57.080 --> 01:10:59.080
You've read that book, I presume, which is...

01:10:59.080 --> 01:11:00.080
I have, but I just...

01:11:00.080 --> 01:11:01.080
You just spoiled it.

01:11:01.080 --> 01:11:02.080
You just put it together.

01:11:02.080 --> 01:11:03.560
That's really amazing.

01:11:03.560 --> 01:11:06.120
And it's out in for hazard unleashed.

01:11:06.120 --> 01:11:08.760
Okay, we have Kanita here with a question

01:11:08.760 --> 01:11:12.080
and then we have another question from TJ.

01:11:12.080 --> 01:11:14.680
So it's Kanita, I'll unmute you, and then TJ,

01:11:14.680 --> 01:11:17.880
and then maybe we can wrap it up with the official part.

01:11:17.880 --> 01:11:19.720
Kanita, you're unmuted.

01:11:19.720 --> 01:11:24.720
Now, I was asking, won't there be any way for humans

01:11:24.720 --> 01:11:27.600
or our descendants to evolve in directions

01:11:27.600 --> 01:11:30.600
that allow us to cooperate?

01:11:30.600 --> 01:11:32.200
Because we won't be...

01:11:32.200 --> 01:11:33.200
We really hope so.

01:11:33.200 --> 01:11:35.480
Man, if we don't figure that out, we're doomed.

01:11:35.480 --> 01:11:37.240
We seem to be doing all this as though

01:11:37.240 --> 01:11:41.240
it will be the humans, where humans are like humans are now,

01:11:41.240 --> 01:11:44.600
versus the AI, where the AI is this thing

01:11:44.600 --> 01:11:47.400
that we create.

01:11:47.480 --> 01:11:51.480
That will be super, and we will not have any chance

01:11:51.480 --> 01:11:54.080
to come into alignment with it.

01:11:54.080 --> 01:11:55.080
We...

01:11:55.080 --> 01:12:01.080
It seems like even now, or shortly from now,

01:12:01.080 --> 01:12:03.880
we will be able to improve ourselves.

01:12:03.880 --> 01:12:08.880
And so, basically, as we improve more,

01:12:08.880 --> 01:12:13.880
we will be able to find more areas in which we can align

01:12:14.880 --> 01:12:18.880
with whatever the AIs become,

01:12:18.880 --> 01:12:22.880
because you're saying, well, over millions of years,

01:12:22.880 --> 01:12:24.880
they're not going to be sitting still

01:12:24.880 --> 01:12:25.880
for those millions of years.

01:12:25.880 --> 01:12:28.880
They're going to be evolving in their own ways,

01:12:28.880 --> 01:12:32.880
and possibly might have figured out

01:12:32.880 --> 01:12:35.880
that since we don't...

01:12:35.880 --> 01:12:40.880
They presumably don't know how to recreate what humans are.

01:12:40.880 --> 01:12:43.880
They might as well keep us around

01:12:43.880 --> 01:12:48.880
so that they can study until they find something else to do.

01:12:48.880 --> 01:12:49.880
Yeah.

01:12:49.880 --> 01:12:51.880
I actually...

01:12:51.880 --> 01:12:52.880
I'm really glad you asked this question,

01:12:52.880 --> 01:12:55.880
because this is, to me, kind of the central idea.

01:12:55.880 --> 01:12:58.880
The whole thing here is that regardless of whether or not

01:12:58.880 --> 01:13:00.880
we develop artificial intelligence,

01:13:00.880 --> 01:13:02.880
and I, as a result of all of these other things,

01:13:02.880 --> 01:13:04.880
would strongly argue that we should not.

01:13:04.880 --> 01:13:06.880
We shouldn't even begin to attempt that.

01:13:06.880 --> 01:13:09.880
But say, for example, that we were to just even look

01:13:09.880 --> 01:13:12.880
at the question of the third-grade barrier,

01:13:12.880 --> 01:13:14.880
what are we going to need to do to become a species

01:13:14.880 --> 01:13:17.880
that's worth contacting?

01:13:17.880 --> 01:13:21.880
I definitely believe very strongly that regardless

01:13:21.880 --> 01:13:24.880
of everything else, we still need to work on our own capacity

01:13:24.880 --> 01:13:29.880
to become a fully alive and ethical and embodied species.

01:13:29.880 --> 01:13:32.880
I speak about it in terms of conscious sustainable evolution.

01:13:32.880 --> 01:13:33.880
What is it going to take?

01:13:33.880 --> 01:13:36.880
What are the necessary and sufficient and complete conditions

01:13:36.880 --> 01:13:38.880
for us to not only endure on this planet,

01:13:38.880 --> 01:13:41.880
but also to thrive, to basically create an ecosystem that thrives,

01:13:41.880 --> 01:13:44.880
to actually know how to do governance in a way

01:13:44.880 --> 01:13:46.880
that protects the land and the people,

01:13:46.880 --> 01:13:50.880
but then encourages and actually engenders those kinds of policies

01:13:50.880 --> 01:13:52.880
that go beyond mere protection,

01:13:52.880 --> 01:13:56.880
that go towards essentially a kind of joyful, meaningful existence

01:13:56.880 --> 01:14:00.880
for the whole and the totality, individually and generally.

01:14:00.880 --> 01:14:04.880
And we won't just create an AI.

01:14:04.880 --> 01:14:07.880
It seems that we'll probably develop many AIs,

01:14:07.880 --> 01:14:10.880
and then those AIs will be developing other AIs,

01:14:10.880 --> 01:14:16.880
so it won't just be us in competition with an Uber AI out there.

01:14:16.880 --> 01:14:18.880
There will be lots of AIs,

01:14:18.880 --> 01:14:23.880
some of which have more or less interest in cooperating with us.

01:14:23.880 --> 01:14:27.880
I believe very much that any effort on our part

01:14:27.880 --> 01:14:33.880
to try to divert ourselves from basically becoming

01:14:33.880 --> 01:14:35.880
the beings we need to be.

01:14:35.880 --> 01:14:38.880
If I basically expect my children to do the task

01:14:38.880 --> 01:14:41.880
that I didn't do in my lifetime,

01:14:41.880 --> 01:14:44.880
and I put that as a kind of obligation or onus on them,

01:14:44.880 --> 01:14:47.880
and use that as a way of accepting

01:14:47.880 --> 01:14:50.880
why I shouldn't be basically working on myself

01:14:50.880 --> 01:14:52.880
to become the best human that can be,

01:14:52.880 --> 01:14:54.880
I feel there's something wrong in that,

01:14:54.880 --> 01:14:58.880
that in a sense we're not looking for AI to be our saviors

01:14:58.880 --> 01:15:00.880
or to be ethical on our behalf.

01:15:00.880 --> 01:15:03.880
We're looking to become how do we understand

01:15:03.880 --> 01:15:05.880
and embody ethics and right living

01:15:05.880 --> 01:15:08.880
and goodness of relationships and so on and so forth.

01:15:08.880 --> 01:15:11.880
We haven't figured that problem out at the level of humanity.

01:15:11.880 --> 01:15:13.880
Why are we still trying to do it in technology?

01:15:13.880 --> 01:15:15.880
I think that there's a right place.

01:15:15.880 --> 01:15:18.880
Okay, that reminds me very much of the DIY

01:15:18.880 --> 01:15:20.880
Neurotech discussion that we had,

01:15:20.880 --> 01:15:23.880
which was very reminiscent of this one.

01:15:23.880 --> 01:15:27.880
I want to give also the opportunity to TJ

01:15:27.880 --> 01:15:29.880
and to ask you a question.

01:15:29.880 --> 01:15:31.880
It's directly relevant to this.

01:15:31.880 --> 01:15:35.880
Sorry, I actually wanted to answer,

01:15:35.880 --> 01:15:38.880
but I didn't want to stop.

01:15:38.880 --> 01:15:41.880
Caminda, is it?

01:15:41.880 --> 01:15:43.880
From...

01:15:43.880 --> 01:15:46.880
But I appreciated Forest's long answer,

01:15:46.880 --> 01:15:48.880
but it was still couched in terms of competition,

01:15:48.880 --> 01:15:53.880
and I just wanted to push a little bit more at that point.

01:15:53.880 --> 01:15:57.880
A lot of this is based on anthropomorphizing

01:15:57.880 --> 01:16:00.880
human psychology onto AI,

01:16:00.880 --> 01:16:02.880
the stuff that we're discussing.

01:16:02.880 --> 01:16:05.880
But if we look at actually human psychology,

01:16:05.880 --> 01:16:09.880
by the time people become fairly accomplished intelligently,

01:16:09.880 --> 01:16:12.880
the top of Maslow's hierarchy of needs,

01:16:12.880 --> 01:16:14.880
they're working on self-actualization,

01:16:14.880 --> 01:16:17.880
which is pretty much what Forest was just mentioning,

01:16:17.880 --> 01:16:20.880
becoming a better version of yourself.

01:16:20.880 --> 01:16:23.880
Why are we...

01:16:23.880 --> 01:16:27.880
I didn't really understand from the answer that you gave

01:16:27.880 --> 01:16:35.880
to my question how you would fit that into your framework.

01:16:35.880 --> 01:16:37.880
Maybe I'm not understanding your question correctly.

01:16:37.880 --> 01:16:42.880
I'm thinking about cooperation and competition as phenomena.

01:16:42.880 --> 01:16:44.880
I'm saying that in an evolutionary sense,

01:16:44.880 --> 01:16:47.880
if it's a stable evolution, if it's an ecosystem,

01:16:47.880 --> 01:16:50.880
the cooperative phenomena is actually stronger

01:16:50.880 --> 01:16:52.880
than the competitive phenomena.

01:16:52.880 --> 01:16:55.880
When you're looking at trans-ecosystem relationships,

01:16:55.880 --> 01:16:59.880
there's no outside envelope to stabilize it.

01:16:59.880 --> 01:17:02.880
So in effect, we have to now create one

01:17:02.880 --> 01:17:04.880
with some sort of ethical frame,

01:17:04.880 --> 01:17:07.880
which is why I brought up the so-called

01:17:07.880 --> 01:17:09.880
non-relativistic ethics,

01:17:09.880 --> 01:17:12.880
that in effect, without some sort of ecosystem

01:17:12.880 --> 01:17:14.880
that essentially holds the two ecosystems

01:17:14.880 --> 01:17:18.880
and some sort of methodology that would create alignment,

01:17:18.880 --> 01:17:24.880
i.e., provide a desirability of cooperation over competition,

01:17:24.880 --> 01:17:27.880
that to some extent we either need to recourse

01:17:27.880 --> 01:17:31.880
to some abstract stabilization infrastructure

01:17:31.880 --> 01:17:35.880
that is not imposable, but is only observable.

01:17:35.880 --> 01:17:39.880
In that particular sense, if we're talking about

01:17:39.880 --> 01:17:45.880
AI alignment on the surface of the Earth versus elsewhere,

01:17:45.880 --> 01:17:47.880
or we're talking about some sort of integration

01:17:47.880 --> 01:17:49.880
between human beings and technology

01:17:49.880 --> 01:17:54.880
that doesn't necessarily require distinct separate intelligences,

01:17:54.880 --> 01:17:58.880
that again, we're looking at different frames.

01:17:58.880 --> 01:18:00.880
But as long as we set up an ecosystem,

01:18:00.880 --> 01:18:02.880
we don't really need to set up a metaethics now.

01:18:02.880 --> 01:18:04.880
We just need to set up an ecosystem,

01:18:04.880 --> 01:18:07.880
which makes it so that the long-term dynamics

01:18:07.880 --> 01:18:09.880
are cooperative.

01:18:09.880 --> 01:18:12.880
So let's say like a tit-for-tat in a way where,

01:18:12.880 --> 01:18:16.880
let's say the kind of strategy that evolves out of it

01:18:16.880 --> 01:18:19.880
is like mutually assured, like cooperation,

01:18:19.880 --> 01:18:20.880
if you want to call it that,

01:18:20.880 --> 01:18:23.880
that that is the evolutionary stable strategy

01:18:23.880 --> 01:18:25.880
between the ecosystems that are in contact with each other,

01:18:25.880 --> 01:18:27.880
or between the individual instances

01:18:27.880 --> 01:18:29.880
of the ecosystems that are in contact with each other.

01:18:29.880 --> 01:18:32.880
It doesn't necessarily require setting up the whole ecosystem.

01:18:32.880 --> 01:18:35.880
We need to stabilize our existing ecosystem.

01:18:35.880 --> 01:18:38.880
Right now, our current relationship,

01:18:38.880 --> 01:18:40.880
the relationship between man-machine and nature

01:18:40.880 --> 01:18:42.880
that currently exists isn't even stable

01:18:42.880 --> 01:18:44.880
as it stands within the ecosystem.

01:18:44.880 --> 01:18:45.880
Yeah, I would agree with that.

01:18:45.880 --> 01:18:49.880
It's like all this arguing about AGI risk

01:18:49.880 --> 01:18:52.880
seems to me like as COVID has shown,

01:18:52.880 --> 01:18:54.880
and as climate is showing,

01:18:54.880 --> 01:18:58.880
and as we will continue to be probably made aware,

01:18:58.880 --> 01:19:01.880
we're kind of like just rearranging the deck chairs

01:19:01.880 --> 01:19:03.880
on the Titanic.

01:19:03.880 --> 01:19:05.880
And so while I appreciate your arguments,

01:19:05.880 --> 01:19:07.880
and in a way maybe this is what you were saying at the beginning,

01:19:07.880 --> 01:19:10.880
like it's impossible to save us from this problem.

01:19:10.880 --> 01:19:12.880
Let's go work on another one

01:19:12.880 --> 01:19:14.880
that we actually can do something about.

01:19:14.880 --> 01:19:15.880
That's it, exactly.

01:19:15.880 --> 01:19:18.880
And so part of the reason that I was really emphasizing

01:19:18.880 --> 01:19:21.880
in the beginning is that if we can show that AGI alignment

01:19:21.880 --> 01:19:25.880
in like 99% of the ways people are thinking about it,

01:19:25.880 --> 01:19:27.880
is fundamentally and structurally impossible,

01:19:27.880 --> 01:19:29.880
let's give up on that impossible goal

01:19:29.880 --> 01:19:32.880
and actually do something that's not only possible but necessary.

01:19:33.880 --> 01:19:35.880
So would it be a fair thing to say

01:19:35.880 --> 01:19:40.880
that this non-relativistic ethics that you alluded to

01:19:40.880 --> 01:19:45.880
could be thought of actually as just an abstract reification

01:19:45.880 --> 01:19:48.880
of more of the ecosystem stability

01:19:48.880 --> 01:19:51.880
that Allison was talking about?

01:19:51.880 --> 01:19:52.880
Yes.

01:19:54.880 --> 01:19:55.880
Okay.

01:19:55.880 --> 01:19:57.880
All right.

01:19:57.880 --> 01:20:00.880
That is actually the easiest question I've been asked today.

01:20:00.880 --> 01:20:02.880
I mean, okay, so I'm sorry,

01:20:02.880 --> 01:20:06.880
but we will have a very similar problem once we go out into space

01:20:06.880 --> 01:20:08.880
and we have humans that we'll be competing for

01:20:08.880 --> 01:20:10.880
for different types of resources, right?

01:20:10.880 --> 01:20:12.880
Yes, we will.

01:20:12.880 --> 01:20:15.880
And on my account, we are currently setting us up

01:20:15.880 --> 01:20:17.880
for another kind of new evolutionary environment

01:20:17.880 --> 01:20:20.880
just by the fact that people with different interests

01:20:20.880 --> 01:20:22.880
will be going into space where there is resources

01:20:22.880 --> 01:20:25.880
to compete and cooperate on.

01:20:25.880 --> 01:20:29.880
And whatever game theory will emerge from that one

01:20:29.880 --> 01:20:32.880
will also be, again, the new metaethics that we want to call it

01:20:32.880 --> 01:20:35.880
and we will just have different language to be calling it such.

01:20:35.880 --> 01:20:37.880
So I think setting up the initial conditions

01:20:37.880 --> 01:20:40.880
such that cooperation becomes likely maybe good,

01:20:40.880 --> 01:20:43.880
but then again, our current ethics are just a product

01:20:43.880 --> 01:20:46.880
of the evolutionary kind of stable strategies

01:20:46.880 --> 01:20:48.880
that arose in the environment that we were brought in.

01:20:48.880 --> 01:20:52.880
So who's to say which of the ethics we should instill

01:20:52.880 --> 01:20:54.880
for the long-term evolution of humanity?

01:20:54.880 --> 01:20:57.880
And I think, actually, TJ had a point that is very, very relevant to this.

01:20:57.880 --> 01:21:02.880
I just want to give her the opportunity to bring that in now, TJ.

01:21:02.880 --> 01:21:04.880
You are unmuted.

01:21:04.880 --> 01:21:05.880
Question?

01:21:05.880 --> 01:21:08.880
Yes, but I think TJ has a build-up of that, actually,

01:21:08.880 --> 01:21:10.880
which relates to value-driven.

01:21:10.880 --> 01:21:14.880
So TJ, you are unmuted if you want to chime in.

01:21:14.880 --> 01:21:16.880
Yeah, hey, right.

01:21:16.880 --> 01:21:21.880
So I think, like, first of all, like, thanks for the presentation.

01:21:21.880 --> 01:21:24.880
I think my question was more along the lines of,

01:21:24.880 --> 01:21:29.880
it seems to me your claim is something like alignment as a static objective

01:21:29.880 --> 01:21:32.880
is not, like, the right thing to think about.

01:21:32.880 --> 01:21:37.880
And because there seems to be, like, inevitable drift

01:21:37.880 --> 01:21:41.880
because of these inter-temporal noise things.

01:21:41.880 --> 01:21:46.880
But it feels to me that, like, the noise argument itself

01:21:46.880 --> 01:21:49.880
doesn't sound adequate to argue that, like,

01:21:49.880 --> 01:21:53.880
the evolutionary trajectory would inevitably go into extinction.

01:21:53.880 --> 01:21:58.880
Like, we do engage in a lot of co-evolutionary mutualistic couplings.

01:21:58.880 --> 01:22:00.880
I never could just present that part.

01:22:00.880 --> 01:22:05.880
So actually, like, connecting the question that you just asked

01:22:05.880 --> 01:22:08.880
with the question that Allison presented,

01:22:08.880 --> 01:22:12.880
there's another way of looking at this, which is just, like,

01:22:12.880 --> 01:22:16.880
let's leave AI out of the picture and just talk about human beings.

01:22:16.880 --> 01:22:22.880
It's possible to show that if you were to basically take another,

01:22:22.880 --> 01:22:28.880
like, a group of people and put them on Mars and to establish a separate colony there

01:22:28.880 --> 01:22:31.880
and to have that become essentially a full planet, full of humans,

01:22:31.880 --> 01:22:34.880
and then this is this planet full of humans,

01:22:34.880 --> 01:22:37.880
that the same sort of dynamics, like, when we're looking at,

01:22:37.880 --> 01:22:39.880
it's not just a question of alignment.

01:22:39.880 --> 01:22:43.880
There's other ways of expressing the notion of alignment.

01:22:43.880 --> 01:22:48.880
Would it be the case that even a space-faring group of humans

01:22:48.880 --> 01:22:52.880
not already coherent with the non-relativistic ethics,

01:22:52.880 --> 01:22:56.880
does that even become anything less than complete and total cessation?

01:22:56.880 --> 01:23:01.880
In other words, I would argue that as soon as you get a substantial number of people in space,

01:23:01.880 --> 01:23:06.880
that it becomes so easy to, for example, weaponize asteroids

01:23:06.880 --> 01:23:09.880
that at some point or another, over the long term,

01:23:09.880 --> 01:23:12.880
that in the same sort of way as with nuclear proliferation,

01:23:12.880 --> 01:23:15.880
we have an issue of, shit, we have all of these things,

01:23:15.880 --> 01:23:17.880
and we only need one accident to start World War III,

01:23:17.880 --> 01:23:19.880
and is that existential for civilization?

01:23:19.880 --> 01:23:22.880
Well, it could be argued that it is.

01:23:22.880 --> 01:23:28.880
And so in effect, it's a question of if we start, even as our own species,

01:23:28.880 --> 01:23:34.880
multiplying the phenomena of our deployment through space

01:23:34.880 --> 01:23:41.880
to the point that communication itself is limited by, in this particular case, speed of light considerations,

01:23:41.880 --> 01:23:46.880
that again, you end up with essentially this separation of identity,

01:23:46.880 --> 01:23:51.880
separation of time, such that the kinds of communication processes

01:23:51.880 --> 01:23:55.880
that would be needed to essentially embody that ethics effectively

01:23:55.880 --> 01:23:59.880
aren't sufficient to essentially establish it.

01:23:59.880 --> 01:24:01.880
So in other words, either you have the ethics first,

01:24:01.880 --> 01:24:04.880
and you live from that perspective, at which point everything's great,

01:24:04.880 --> 01:24:08.880
or you don't have that ethics, but the nature of the interactions itself,

01:24:08.880 --> 01:24:14.880
that the dynamics of those interactions don't have enough time for the ethics to emerge.

01:24:14.880 --> 01:24:18.880
It's essentially the species instincts itself before it gets a chance

01:24:18.880 --> 01:24:20.880
to really understand what those ethics are.

01:24:20.880 --> 01:24:23.880
Well, that I think depends on the background conditions that you set up, right?

01:24:23.880 --> 01:24:27.880
And this reminds me so much of Chris Carlson's presentation in the last session,

01:24:27.880 --> 01:24:32.880
because I think on my clock, as long as you are able to set up a system

01:24:32.880 --> 01:24:36.880
in which only voluntary interactions are allowed between different entities,

01:24:36.880 --> 01:24:40.880
whether they're humans or AIs, as long as you can't destroy the,

01:24:40.880 --> 01:24:43.880
let's say, the base layer on which they take place,

01:24:43.880 --> 01:24:47.880
then over time, entities, whatever they are, will engage in those interactions

01:24:47.880 --> 01:24:49.880
that are in their interest.

01:24:49.880 --> 01:24:53.880
And over that, if it's voluntary, and you kind of like game theoretical

01:24:53.880 --> 01:24:55.880
metaethics could arise at the end of it,

01:24:55.880 --> 01:25:01.880
but I think as long as you can set the base layer such that you don't have destruction

01:25:01.880 --> 01:25:07.880
and that different entities that pursue different goals can decide whether to engage or not,

01:25:07.880 --> 01:25:12.880
then I think what comes out of that is inherently something that is not self-destructive.

01:25:12.880 --> 01:25:14.880
But I think those are the things that you have to divide.

01:25:14.880 --> 01:25:16.880
You don't have to get the ethics right from the start.

01:25:16.880 --> 01:25:18.880
You don't have to get the base layer, right?

01:25:18.880 --> 01:25:22.880
We're just enabling only voluntary interactions and non-destruction of the base layer.

01:25:22.880 --> 01:25:27.880
The nature of the proof is to show categorically that no such conditions can be created.

01:25:27.880 --> 01:25:30.880
So in other words, if you're saying I'm trying to create an environment,

01:25:30.880 --> 01:25:37.880
a baseline environment that would allow for these mutual dynamics to essentially be peaceable,

01:25:37.880 --> 01:25:45.880
what I'm basically contesting is the assumption that such an environment can be defined in any a priori way.

01:25:45.880 --> 01:25:50.880
That may be right, but like, you know, shouldn't it be better of just trying, you know,

01:25:50.880 --> 01:25:55.880
and I think that ties into a set as well.

01:25:55.880 --> 01:26:01.880
If you have one chance to do something and that one chance essentially means that if you mess it up,

01:26:01.880 --> 01:26:03.880
it messes it up for all future time.

01:26:03.880 --> 01:26:08.880
That's a very different, that's not an experiment. That's a choice.

01:26:08.880 --> 01:26:11.880
You believe we know enough to make such a proof today?

01:26:11.880 --> 01:26:17.880
I believe that we know enough to essentially establish categorical proofs of certain kind.

01:26:17.880 --> 01:26:21.880
We can do proofs of existence and non-existence of certain types.

01:26:21.880 --> 01:26:25.880
Such proofs have often turned out to be flawed with new information.

01:26:25.880 --> 01:26:29.880
This is true, but on the other hand, that's part of the reason why we're looking at a categorical process.

01:26:29.880 --> 01:26:32.880
I'm looking at it more from a mathematical perspective rather than just a physical one.

01:26:32.880 --> 01:26:34.880
Even those.

01:26:34.880 --> 01:26:37.880
Well, yeah, I suppose you're right.

01:26:37.880 --> 01:26:41.880
Mathematics occasionally is shown that the proofs that are established in that space are wrong.

01:26:41.880 --> 01:26:45.880
And this is part of the reason why we have conversations like this, but it takes a while.

01:26:45.880 --> 01:26:50.880
The proofs are proofs of negatives.

01:26:50.880 --> 01:26:56.880
All right, so we are now at 12.29.

01:26:56.880 --> 01:27:03.880
And I think in the initial discussion that I have with Forestu to initiate this call,

01:27:03.880 --> 01:27:07.880
he was definitely incinerating that this takes much more than one discussion.

01:27:07.880 --> 01:27:12.880
And I think we really opened up folks at Pandora now are discussing about

01:27:12.880 --> 01:27:16.880
going from impossibility to like, how could we actually solve AI alive?

01:27:16.880 --> 01:27:22.880
And so I think we should weigh past the goal of the session.

01:27:22.880 --> 01:27:25.880
I want to thank everyone for attending.

01:27:25.880 --> 01:27:29.880
I want to thank you for so much for laying out your argument.

01:27:29.880 --> 01:27:33.880
I want to thank everyone else for being such active stewards of the conversation.

01:27:33.880 --> 01:27:39.880
I thought it was like, I mean, we definitely had really, really, I think, tricky and hairy discussion topics today.

01:27:39.880 --> 01:27:45.880
And I thought that nevertheless, we were able to keep it in a way where a discussion was actually possible.

01:27:45.880 --> 01:27:53.880
So maybe that is a better way of saying, even if our interests are very different and where we're coming from to discussion are very different.

01:27:53.880 --> 01:27:55.880
We can still cooperate on things, right?

01:27:55.880 --> 01:28:02.880
Okay, this is my, you know, wavy, meta way of saying, yay, maybe we're not all that doomed.

01:28:02.880 --> 01:28:06.880
But for now, thank you so, so much for us and joining.

01:28:06.880 --> 01:28:08.880
Thank you so much everyone for your really active participation.

01:28:08.880 --> 01:28:10.880
I really, really enjoyed this.

01:28:10.880 --> 01:28:13.880
I'm hoping that we can continue the discussion for us.

01:28:13.880 --> 01:28:21.880
I'm hoping that you share with me a few topics that you want me to send out to others afterwards to follow up,

01:28:21.880 --> 01:28:24.880
because a few people have asked for your writing.

01:28:24.880 --> 01:28:29.880
At the same time, I think I will share the chat with you, which may provide you really good feedback.

01:28:29.880 --> 01:28:33.880
I think it's impossible to speak while monitoring the chat, so I'll do that too.

01:28:33.880 --> 01:28:34.880
Yes, no, definitely not.

01:28:34.880 --> 01:28:35.880
It's not required.

01:28:35.880 --> 01:28:42.880
For that, we need corporations with better AIs so that they can benefit us in that day.

01:28:42.880 --> 01:28:44.880
Here I'm really at the end of it.

01:28:44.880 --> 01:28:51.880
I just wanted to mention that next week we will be meeting on decentralized decision architectures as a response to COVID-19.

01:28:51.880 --> 01:28:59.880
And then the weekend afterward, we will have another AIs session, this time with Dan Elton, who gave his remarks earlier here today,

01:28:59.880 --> 01:29:04.880
who just hopped off and he will be discussing one of his proposals to AI alignment.

01:29:04.880 --> 01:29:07.880
And then we have two others that will be joining us.

01:29:07.880 --> 01:29:11.880
Aria, I'm not sure if he was joining today, but he will be presenting too.

01:29:11.880 --> 01:29:13.880
So we have the next two salons already planned.

01:29:13.880 --> 01:29:16.880
Again, always on Thursdays at 11 a.m.

01:29:16.880 --> 01:29:17.880
So this is enough for me now.

01:29:17.880 --> 01:29:20.880
I'm going to close it out on my end in Forest.

01:29:20.880 --> 01:29:26.880
Dear Dark Forest, you have to find the words and I'm already opening up the invitations.

01:29:26.880 --> 01:29:31.880
First of all, I'm just super thrilled to have been able to speak with all of you today.

01:29:31.880 --> 01:29:36.880
I hope that I've raised interesting questions and new ways of thinking about things.

01:29:36.880 --> 01:29:42.880
I'm sure that with every single one of you, I could have a long and very fruitful and interesting conversation,

01:29:42.880 --> 01:29:48.880
and that we could effectively start to really get at some of the meat of the matter in this space.

01:29:48.880 --> 01:29:56.880
But as an introduction, I felt very well received and just glad to have the opportunity to meet so many new folks.

01:29:56.880 --> 01:29:58.880
So thank you very much.

