start	end	text
0	3200	Hi, Vern. Welcome to FOSAT's Intelligent Cooperation Group.
3200	5080	We're really excited about this seminar.
5080	7560	We have DC here from Welkheim.
7560	11160	Thank you so much for joining us to give an overview of ZKML,
11160	13560	which is a topic that has come up so much,
13560	16920	especially, I guess, data coming up already a few years ago
16920	18560	in this group, and then much, much more
18560	20640	to have really ramping up in the last year.
20640	22720	And so I'm really excited for you to share a little bit more
22720	24840	about it because we have not had a dedicated seminar
24840	25600	to this topic yet.
25600	26960	So thanks a lot for joining.
26960	28640	We're really excited about you guys' work.
28720	30120	Without further ado, please take it away.
30120	31760	I'll be in the chat monitoring questions,
31760	33760	and we're off to the races.
33760	35600	Awesome. Thank you for having me.
35600	38640	So today, I'll be talking about the zero-knowledge machine
38640	39120	learning.
39120	41600	I'll be giving a very brief introduction.
41600	45120	But first, maybe let me start by saying a little bit about myself.
45120	48200	I'm a research engineer at the World Coin Foundation,
48200	49640	and World Coin is this project that
49640	53640	is trying to build the largest identity and financial network.
53640	56720	And there is an interplay of various technologies
56720	59600	in the products and things that we're building at World Coin,
59600	63040	and some of which are AI, and some of which are cryptography.
63040	65160	So we've had expertise in both realms
65160	66840	from different teams internally.
66840	69840	And there have been some essentially experimentation
69840	70960	that naturally occurred.
70960	72840	We have some AI parts of the stack,
72840	75200	and there are some reasons why cryptography might
75200	77560	be useful in the AI sector of the stack.
77560	81200	And so this prompted us to think about this topic
81200	83000	about two years ago in August.
83000	85280	Almost two years ago, August of 2022.
85760	88240	One of my teammates was just playing around with cryptography
88240	90160	and trying to prove machine learning models.
90160	92640	But so this is a little bit of a background.
92640	94920	I mostly run our grants program, where
94920	96920	we give grants to people to help decentralize World
96920	98400	Coin and solve some more problems,
98400	101040	and also help with a bunch of different R&D efforts
101040	103080	as an individual contribute.
103080	106240	So without further ado, let me start with the presentation.
106240	108720	Usually, the way that I like to start with this presentation
108720	112400	is that I like to decompose the statement into its constituents
112400	115560	so that people have an understanding of which elements
115560	117040	or what is zero-knowledge machine learning?
117040	118280	What is the zero-knowledge part?
118280	120040	What is the machine learning part?
120040	123000	I like to decompose it into its fundamental compositions,
123000	124240	so competition.
124240	126280	And the first one that I want to talk about
126280	130280	is zero-knowledge cryptography, or called zero-knowledge.
130280	132720	I don't know if many of you understand the reference that
132720	135320	or get the reference that I put here, which is Waldo,
135320	136320	finding Waldo.
136320	140160	Waldo essentially is a character or one specific analogy,
140200	142320	which is very simple, very easy, very friendly
142320	144280	to explain what zero-knowledge cryptography is
144280	146320	to people who have never heard about it.
146320	148760	Because there is this essentially like poster
148760	151080	that many people know where there's just lots of characters
151080	153960	which are in the city, and there's one Waldo.
153960	156960	And it takes some amount of time to essentially find the Waldo,
156960	159720	and that's like the challenge of these specific games.
159720	161840	And so there is one specific analogy
161840	164360	which allows people to explain what zero-knowledge is,
164360	170120	which is that if I put a bigger like white paper
170240	173400	on top of the poster, defining Waldo game,
173400	176120	and if I create a little tiny cutout for the head of Waldo,
176120	179520	and I place the cutout just on top of Waldo's head,
179520	183960	but covering the entire game itself, or the poster itself,
183960	186960	then I'm able to prove to anyone that I know where Waldo is
186960	189400	without revealing his location within the poster.
189400	190680	So this is like the good analogy
190680	192640	for explaining what zero-knowledge cryptography is,
192640	195600	because I'm able to prove things that I know
195600	197880	to someone else, to an outside observer,
197880	200360	without them learning everything
200360	201800	about the things I'm making a proof of.
201800	204440	I can selectively prove specific statements
204440	205960	because I have information,
205960	207880	but I don't have to reveal everything
207880	209360	in order to be able to prove that statement.
209360	212240	So this is like a good analogy to explain DK.
212240	215520	So some of the properties that zero-knowledge cryptography has.
215520	217600	So the first one, which I think is the most important one,
217600	219200	especially in the context of blockchain,
219200	222800	I saw that many of you were like in previous presentations
222800	224720	of this specific group.
224720	226120	I saw that there's a few crypto people
226120	227240	that were talking about different things.
227240	229920	I'm sure that came along a few times.
229920	231960	So succinct list essentially just means
231960	235320	that in order to verify a proof of a statement,
235320	237680	it's a lot less computationally expensive
237680	239160	or a lot less expensive
239160	242160	than to actually prove the computation
242160	244520	or to just perform the computation yourself, right?
244520	247600	So essentially verifying that I know where Waldo is,
247600	248680	as an outside observer,
248680	251320	it's a lot easier than me finding Waldo myself.
251320	253280	So this is really important in the context of blockchains
253280	255160	because for example, for scalability solutions,
255160	257080	instead of everyone having to re-execute
257080	258720	the same transactions in a block,
258720	260000	I can just verify a proof
260000	261280	and I can just update my state
261280	263760	without having to secure it myself, for example.
263760	265640	So this property is important for the kid
265640	267720	because it allows us to very easily,
267720	270440	computationally easily verify things
270440	272000	without having to do computation ourselves.
272000	273600	This is a really important property.
273600	277240	The second one, arguably the one that is most known for,
277240	278200	is correctness.
278200	279640	So correctness essentially means
279640	282960	that I can have almost 100% certainty
282960	286400	that this statement that I'm proving is correct, right?
286440	287680	That I cannot lie.
287680	289840	There is no way that I as a prover
289840	292000	can lie to a verifier
292000	295000	unless if the cryptography is sound in this case.
295000	296520	There's two specific properties
296520	298040	that constitute correctness.
298040	299440	One is soundness.
299440	301760	So soundness means that I, if I'm a prover,
301760	304560	someone making this claim, someone making a statement,
304560	308720	I'm not able to fool a verifier with invalid proof
308720	311040	and completeness is another property
311040	314280	where I'm not essentially able to create a valid proof
314280	315220	unless I know the truth.
315220	316180	If I don't know the truth,
316180	320380	I cannot make a valid proof as a prover.
320380	323460	And the third one, which is name after, zero knowledge,
323460	326860	is this property where I can hide parts of the statement.
326860	328660	For example, let's say that I have,
328660	329820	I don't know, this is a good example.
329820	331260	Let's say I have my passport.
331260	334380	So I have my name, my nationality, my date of birth,
334380	336980	where I'm from, which country I was born in,
336980	339100	for example, the place of birth.
339100	340460	So something that would be useful
340460	343220	or like something that would constitute a zero knowledge proof
343220	344500	is that I can make the statement
344500	347500	that my age is over 18 years old
347500	349460	without revealing to anyone my age,
349460	353060	but anyone can just verify that I'm actually over 18.
353060	354540	The way that this is actually implemented
354540	355820	is that within a zero knowledge proof,
355820	359100	I can verify a signature from some issuing body
359100	360160	like the government.
360160	362020	And then I can make a statement that like A,
362020	363820	this age, which was attested to
363820	367420	or essentially committed to by a government is over 18
367420	368780	and you don't learn my age.
368780	370100	So this is the zero knowledge part
370100	371940	where I'm able to hide parts of the state
371940	374740	that I'm making a statement about or proof about
374740	377780	according to some constraint or some statement, right?
377780	379580	Like I can say greater than, less than,
379580	383020	equal to a bunch of other properties
383020	384540	that I think can put.
384540	387020	So the second part of the statement
387020	388340	of zero knowledge machine learning,
388340	389420	this machine learning, right?
389420	392020	I think that one is much more familiar to most of you
392020	395100	since it's been generating such a buzz everywhere,
395100	396860	like machine learning through a generative AI,
396860	399060	like in things like ChagYPT or Dali
399060	401300	or a lot of generative AI models
401340	403940	or natural language processing, categorizing models
403940	406180	like robots, the machine learning essentially,
406180	409540	the way that I think about it is that it's a tool
409540	413340	that allows us to give us not the non-deterministic solutions
413340	416220	or just estimates for short for problems
416220	418940	that don't really have a concrete solution, right?
418940	420860	There's usually there's some problems
420860	422820	which we can solve algorithmically
422820	424180	and we can just have a set of steps
424180	427140	that we can just execute in order to solve it
427140	429620	and we will have a perfect solution every time.
429620	431900	In the case of machine learning, however,
431900	433420	most of the problems that are being solved
433420	435340	by machine learning are not such problems.
435340	436820	Therefore, we need to,
436820	439500	because maybe like the space of solutions is too big
439500	442420	or the space of the steps that we can take is too big,
442420	444940	so it's really hard to navigate deterministically,
444940	447060	then we just have this sort of juristic.
447060	450620	A juristic essentially is a good enough approximation
450620	453460	to the real solution which we can work with
453460	455820	and which has some form of accuracy, right?
455820	457540	So in the context of machine learning,
457540	460300	we have some sort of juristic for some problem.
460300	462540	So let's say I want to categorize
462540	466060	whether an image that I see is the image of a dog or a cat.
466060	468340	This I can train a machine learning model
468340	469980	to essentially solve this task,
469980	472300	but the machine learning model will never be 100% correct.
472300	474460	It will have some accuracy, right?
474460	475780	It will have some that will fail on,
475780	478940	it will have, most of them it will get correct.
478940	481860	But essentially the way that the machine learning model works
481860	484380	is that it trains on some data.
484380	486620	So I feed some data to some model
486740	489340	or to some machine learning algorithm and it gets better.
489340	491940	And this juristic keep getting better and better
491940	494060	for things that it hasn't seen before.
494060	495820	It generalizes over the data
495820	498540	and it's able to make essentially like predictions
498540	501060	or classifications or all sorts of things.
501060	502300	So in the case, for example,
502300	505220	of foundational models for large language models,
505220	508620	they get better at creating like cohesive explanations
508620	510620	or at reasoning or at mathematics
510620	512620	or at all sorts of different things.
512620	514380	And we can have these benchmarks
514380	516860	and with more data, they get better at these benchmarks,
516860	519540	which essentially provide better juristic problems
519540	521060	that we're grading them on.
521060	524180	So another concept that I want to explain here
524180	527140	is that there's two specific parts within machine learning
527140	530580	or two specific things you can do usually,
530580	533380	is that when you have a model, you can train a model.
533380	536540	The act of training a model is the act of creating a function
536540	538860	which gets actually better and better
538860	540820	at giving you the juristics,
540820	542700	the more data you feed into it, right?
542700	544780	So I'm able to update the parameters of this function
544780	547820	by learning, this is what learning is, right?
547820	549340	I'm updating parameters of a function
549340	550980	in order to get a better jurist.
550980	552900	And this process is really expensive, right?
552900	555820	It's really hard, you have to co-locate a data center,
555820	559340	it's running lots of graphics cards in a big place
559340	561460	and consuming lots of electricity for months on end
561460	563500	in order to be able to create something useful
563500	565540	or meaningful, this is really expensive.
565540	567580	But the end product is very easy to run.
567580	569020	So once I've trained this function,
569020	570700	once I have my set of parameters,
570700	572620	evaluating this function at some input,
572620	575340	is usually inexpensive or very inexpensive
575340	577300	compared to actually training it,
577300	579500	several orders of magnitude less.
579500	581220	So these are like the,
581220	584300	how I usually explain machine learning just very briefly.
584300	585700	And so now I want to get,
585700	587660	what is zero knowledge machine learning, right?
587660	589620	We have some intuitions from the ZK side,
589620	591740	some intuitions from the machine learning side.
591740	595700	Now we can discuss what ZK machine learning can be
595700	598780	or is within the modern understanding of it.
598780	600780	So essentially what ZKML is,
600780	604060	is the creation of zero knowledge proofs
604060	605980	of machine learning algorithms, right?
605980	608460	So zero knowledge cryptography allows you
608460	611420	to create proofs of arbitrary computations.
611420	614620	It can be a proof that I've computed some specific thing.
614620	617500	It can be a proof that some variable is bigger than another.
617500	619300	Essentially it's making proofs about computation
619300	622140	and you know the person who's verifying that proof
622140	624540	knows that someone has executed that computation
624540	627860	on some inputs and has produced some output.
627860	629100	So in the context of machine learning,
629100	632260	usually you have some input, a function or a model,
632260	633500	and then some output, right?
633500	635460	So if it's let's say like charge EPT,
635460	637700	I have a prompt which I feed into the model.
637700	641380	The model just takes that prompt and evaluates its model
641380	642700	and it gives you an output,
642700	644660	which is the thing that you then read in the end,
644660	646060	which is the result, right?
646060	648780	So zero knowledge machine learning would be the art
648780	653060	or act of creating a proof that I have fed an input
653060	654940	to a model and I've produced some output.
654940	658060	And I can verify that this output came from a model
658100	661700	without essentially having to evaluate this myself personally.
661700	663900	I just know that this comes from a model
663900	667100	because I have a cryptographic proof that this indeed happened.
667100	669300	So something that many people in the space
669300	673140	actually use as a good analogy that accountable AI, right?
673140	675380	Usually AI or machine learning models,
675380	677540	you don't know that they're actually correct
677540	680180	unless you run them yourself, right?
680180	682260	If you run it yourself on your own local machine
682260	684940	or your own data center which you have privileged access to,
684940	687580	then you know that you've run the right thing.
687580	690660	But let's say that you're using some form of server or API, right?
690660	693860	If I go to chat GPT, like the website,
693860	696940	how do I know that OpenAI is actually serving me
696940	697860	the right model?
697860	700940	They claim, like in the UI, in the front end,
700940	703740	they claim that I am using GPT-4, but how do I know that?
703740	706540	There's no way of me of actually verifying that this is GPT-4.
706540	708180	They might be serving me a worse model,
708180	709540	which is cheaper to run
709540	712020	and just pocketing the difference, for example, right?
712020	712900	I don't know.
712900	715460	So one good thing that ZKML provides
715460	716780	is this form of accountability
716780	720660	where I, as the consumer of some API or some model,
720660	722900	I know that this actually came from something
722900	725100	because I can verify a cryptographic proof
725100	726020	that this indeed happened.
726020	727740	So we can make AIs accountable.
727740	729780	We can make anyone using AI accountable
729780	732620	because we can make proofs of computation.
732620	737020	Many of the framing for ZKML in the modern way
737020	740340	is that they want to essentially bring machine learning on-chain
740340	742220	or onto the blockchain in this case, right?
742220	744860	Like we have the blockchains where we have
744860	749100	very interconnected networks of low-end hardware, mostly.
749100	752220	It's like consumer hardware, which is available everywhere,
752220	754460	to run these decentralized networks.
754460	756700	And the problem with this is that every single computer
756700	759420	on this network needs to re-execute everything
759420	761620	that the network sees in order to validate
761620	763860	that the network is progressing correctly.
763860	765100	And this is a big problem
765100	767060	because now everything is really expensive.
767060	769940	If it's already expensive running it on your own machine,
769940	772100	if you have to run it on 1,000 machines
772100	773900	or 10,000 or 100,000 machines,
773940	776620	it's as many times more expensive.
776620	780300	So it's unfeasible to essentially do machine learning
780300	783500	on-chain right now because it's just too expensive.
783500	785140	And the computational environment
785140	787300	that these usually like blockchains have,
787300	789900	like virtual machines, let's say if they're about the EVM,
789900	792900	Solana has SVM, the Solana virtual machine,
792900	795060	that every single blockchain or most blockchains
795060	797460	do have some form of execution capabilities
797460	798740	or computing capabilities.
798740	800420	And these are very constrained.
800460	803940	So some of the things that blockchains are good at
803940	806580	is cryptography because they're usually subsidized
806580	810100	within the cost of execution in blockchain.
810100	813700	So I can verify a zero-knowledge proof on a blockchain
813700	816540	and I can bring something that I run off-chain
816540	818060	on-chain by providing a proof, right?
818060	821300	So for example, if I'm evaluating a model locally,
821300	822820	I'm able to create a zero-knowledge proof
822820	824900	that I've evaluated a model on some input
824900	827460	and I can just send the output to the chain
827460	828660	and verify a proof.
828660	830820	And then the chain or the smart contract
830820	833860	can know that I've actually run a model on some inputs
833860	837060	and I don't have to run that within the environment,
837060	838180	the computing environment, the blockchain,
838180	839660	so I can save a lot of cost.
839660	842340	And if I make ML more accessible on-chain,
842340	844820	I can actually bring it and I can build application
844820	847420	that leverage machine learning for lots of different things
847420	849820	which I'll get into a little bit later.
849820	852660	And the last one is the zero-knowledge part of things
852660	855660	where I'm able to hide specific parts of the computation
855660	858420	or specific parts of the data that I'm making proofs about
858420	861340	and therefore I can make machine learning private.
861340	862660	In order to make machine learning private,
862660	864540	there's other techniques as well
864540	866260	which is like fully homomorphic encryption
866260	867860	and multi-party computation.
867860	870340	Each of these other types of cryptography
870340	873700	or types of distributed systems engineering and compute it
873700	875460	usually have different trade-offs.
875460	877580	So for example, fully homomorphic encryption
877580	880140	does not give you this correctness assumption
880140	881500	or like probability.
881500	883620	I cannot verify that something happened correctly,
883620	887020	but I can, for example, make computations on Cyphertex.
887020	888500	So if I encrypt some data,
888500	891220	I'm able to perform computations on encrypted data.
891220	894380	And when I decrypt, I have the computation performed
894380	897020	on the original input, on the original plaintext
897020	899180	which is something that is quite fascinating.
899180	901140	However, I do lose this property of ZK
901140	904220	where I cannot verify that something happened correctly.
904220	907060	Multi-party computation is like the better of both worlds
907060	910380	but for example, these protocols require multiple parties
910380	912140	to work in unison
912140	914740	and this is really hard to manage, for example.
914740	917060	So yeah, so I want to talk a bit
917060	918620	about who is actually building
918620	921380	Zeronautic Machine Learning nowadays.
921380	923260	Like which startups, which teams,
923260	924700	what are they're focusing on?
925700	928500	So full disclosure, I am an investor in Giza and Modulus
928500	931980	so I don't want to put that out there, just so you know.
931980	935180	So essentially there's different avenues
935180	938980	on what is there to build on within the ZKML domain
938980	940820	in order to make these systems better
940820	942540	or to make this more interesting
942540	945140	or faster and all sorts of different things.
945140	948220	So the three main companies that usually go around
948220	950740	are Giza, Modulus and Ezekio.
950740	953300	On Giza, for example, they're building
953300	954860	on top of the StarkNet ecosystem
954860	957060	which is like one specific scalability solution
957060	958780	in the Ethereum space
958780	962420	and they're implemented a bunch of machine learning models
962420	964580	within this computational environment
964580	966740	that this blockchain had, which is called Cairo.
966740	968220	The computational environment is called Cairo,
968220	969940	the blockchain is called Tarkin
969940	971900	and they're building products, right?
971900	973300	So they're building, for example,
973300	974980	tooling so that financial products
974980	976540	that are deployed on StarkNet
976540	979220	can leverage machine learning within their,
979220	981780	for example, prediction models for financial services
981780	985020	so they can predict where the highest yield is
985020	986740	to routes my money into
986740	988940	so I can get the highest yield on my collateral
988940	991540	or my assets so I can use machine learning off-chain
991540	992580	or machine learning on-chain
992580	994060	within this computational environment,
994060	995780	I can prove it, et cetera, et cetera.
995780	996820	So essentially Giza is building
996820	998180	a lot of the product side of things,
998180	1000540	like different abstraction, different SDKs,
1000540	1003620	different representations of models, on-chain, et cetera.
1003620	1008260	Modulus is mostly working on the foundational side of things
1008260	1011420	and by foundation, I mean the primordial science
1011420	1015140	of doing cryptography and doing engineering.
1015140	1017580	So they're essentially building their own,
1017580	1019660	the thing so-called like approving system
1019660	1022740	which is how do you implement a zero-knowledge scheme?
1022740	1025020	Like zero-knowledge, it boiled down to mathematics
1025020	1027940	and working with polynomials and finite fields
1027940	1032260	and mostly like linear algebra and abstract algebra
1032260	1034940	and modular arithmetic and bunch of things of this sort.
1034940	1037020	So essentially they're trying to build
1037020	1039980	better cryptographic models and better cryptographic systems
1039980	1042020	in order for zero-knowledge machine learning
1042020	1045460	to be more efficient within the actual representation of it
1045460	1047460	in the computing sense, right?
1047460	1049580	So this is what they're working on.
1049580	1051580	I'm happy to then after this presentation
1051580	1054140	or in the FAQ, I'm happy to share more if anyone wants.
1054140	1057020	Like there's plenty of resources to learn more
1057020	1058940	and Ithiko is mostly building, tooling
1058940	1061180	and also some of the products type of things
1061180	1063580	and infrastructure, a lot of infrastructure as well.
1063580	1066140	So Ithiko, for example, is building an abstraction layer
1066140	1067980	that allows developers that are,
1067980	1069740	they come from the machine learning world.
1069740	1071620	So people who usually know Python
1071620	1074180	or right now in this context is just Python.
1074180	1077020	So people who know Python and know the standard tools
1077020	1078300	for building machine learning models,
1078300	1080220	whether it's TensorFlow or Pycorg
1080220	1083580	or I could learn or any other library for machine learning,
1083580	1085700	they even have a standardized representation
1085700	1088260	of a model which can be exported
1088260	1091340	to this model representation called ONIX, ONIX,
1091340	1093540	open your network exchange format.
1093540	1095780	And this format essentially is something
1095780	1098460	that represents what the model looks like
1098460	1099700	in the computational sense.
1099700	1101660	It's a computational graph of different operations
1101660	1103060	that you need to perform.
1103060	1106980	And Ithiko allows you to convert whatever you are building
1106980	1110300	within Python to something that can be ZK proven,
1110300	1113300	that you can make a proof of in a very easy way.
1113300	1115740	So you just import a Python library,
1115740	1116780	you will create your model
1116780	1119500	and then you just do model that ZK proved
1119500	1121300	and you are able to ZK prove that
1121300	1123060	without you as a Python engineer,
1123060	1124380	as a machine learning engineer,
1124380	1126500	you don't need to know how ZK works.
1126500	1127620	You just create a proof of it
1127620	1130980	because Ithiko has built a tool that helped convert
1130980	1132660	the way that you work with ML
1132660	1134620	to something that cryptography can,
1134620	1137060	the cryptography tooling can create proofs of.
1137060	1138820	And of course, academia, academia
1138820	1141100	has been a crucial element of all of this.
1141140	1142700	There's lots of cryptography,
1142700	1145860	new cryptography coming out every week almost.
1145860	1147580	There's new proving systems,
1147580	1149860	there's new types of final field arithmetic,
1149860	1152540	there's new discoveries in different field.
1152540	1153740	There's different optimizations
1153740	1156500	like computing optimization from representation,
1156500	1158500	better models on the machine learning side.
1158500	1159740	There's also improvements
1159740	1162020	and since usually ZK ML,
1162020	1164540	you need both things to become more performant.
1164540	1166460	If academia comes up with better models
1166460	1168820	and better quantization schemes and whatnot,
1168820	1171540	all of these improvements, compounds, right?
1171540	1173180	It's usually the worst of both,
1173180	1176300	the thing that becomes the worst for the aggregate.
1176300	1178300	So the worst of KML, or sorry,
1178300	1179940	the worst of ZK and the worst of ML
1179940	1183060	become the worst of ZK ML, like the bottlenecks.
1183060	1185860	So academia is working a lot of the foundational bottlenecks
1185860	1187740	when it comes to cryptography
1187740	1189860	and all these other things that I mentioned.
1191060	1192940	So some of the use cases,
1192940	1194500	I do wanna talk about use cases
1194500	1195940	because I've seen a lot of people
1195940	1199020	who are really deeply interested in the technology,
1199020	1201060	but the only way that I've seen technology
1201060	1204580	actually progress forward is if there's funding
1204580	1207180	and people actually are interested and excited about it.
1207180	1209180	For example, in the case of zero knowledge cryptography,
1209180	1210660	because I mostly spend most of my time
1210660	1212100	in the blockchain space,
1212100	1215020	zero knowledge cryptography has become really popular
1215020	1216380	in the last two to three years,
1216380	1219220	mostly because there's been products that actually use it,
1219220	1220740	whether it's scalability solutions,
1220740	1222660	whether it's privacy solutions,
1222660	1224700	whether it's digital identity solutions,
1224740	1226900	there's been product market fit for this technology
1226900	1228820	and so new companies have been created
1228820	1231100	and a lot of capital has been poured in
1231100	1233780	and this capital was reinvested
1233780	1235940	into the actual development of better cryptography,
1235940	1238300	better tooling, better hardware,
1238300	1241100	and also there's a lot of network effects, right?
1241100	1243300	So if there's lots of people using something,
1243300	1245100	other vendors like hardware vendors
1245100	1247300	might want to create hardware for these people,
1247300	1249620	so it will even speed it up even further.
1249620	1251900	So I do believe that, for example,
1251900	1254060	ZKML needs a lot of product market fit
1254060	1256860	or products or catalysts and use cases,
1256860	1259340	which would improve the state of the art
1259340	1261460	just by the fact that there's many people looking at it,
1261460	1264380	there's a lot of mind share, there's high excitement.
1264380	1266420	So of course there's negative parts to this as well,
1266420	1268620	but mostly I think it's good.
1268620	1271500	So some of the use cases that I've seen around,
1271500	1272860	so I personally work at WorldQuake
1272860	1276220	and that's like the way that I got exposed to it.
1276220	1279660	I'll explain our specific youth case towards the end.
1279660	1281420	So provable inference is one,
1281420	1285580	so I mentioned earlier on that I do not know
1285580	1286780	if I'm using chat GPT,
1286780	1289060	that someone is actually serving me
1289060	1290900	the model that they claim they are.
1290900	1292780	So provable inference is just this concept
1292780	1296340	where I can know that whomever who used a model
1296340	1298900	to infer some output, I know where it came from.
1298900	1300100	I know which model it came from.
1300100	1301300	If it's public, of course,
1301300	1304300	the APIs can choose to keep the model private,
1304300	1306460	but at least they can, for example, commit to it.
1306460	1307940	Something that I can do if, let's say,
1307940	1309260	GPT-Force Quilt Force, right?
1309260	1310700	Open source is not open source.
1310740	1313860	OpenAI did not open source GPT-Force, as of now.
1313860	1316380	And so if, for example,
1316380	1319380	I cannot know that someone used GPT-Force
1319380	1321060	because I don't have the weights, right?
1321060	1323380	It's not a public thing.
1323380	1326020	But something that OpenAI could do or anyone else
1326020	1327700	with a private model could do
1327700	1331140	is that they can commit to a specific model,
1331140	1332380	let's say a hash.
1332380	1335100	And for example, I know that for the entire user base,
1335100	1336660	they're using the same model.
1336660	1338220	So they cannot fool any single user
1338220	1340300	that they're using specific different models
1340300	1341460	for anyone else.
1341460	1343180	At least they can commit to it
1343180	1345620	with a cryptographic hash, so I can just hash.
1345620	1347420	I know that there is one deterministic output
1347420	1348260	for this model.
1348260	1350460	And I know every single user knows
1350460	1351580	that they're using the same model
1351580	1353340	because within the zero-knowledge group,
1353340	1356700	they have a commitment to some specific set of weights,
1356700	1358020	but they do not reveal the weight.
1358020	1359060	They're just committed.
1359060	1361820	And maybe later, they open source the model,
1361820	1363020	they can reveal the weights,
1363020	1364300	and you can see that the commitment
1364300	1365460	does indeed match the weight.
1365460	1369500	So you actually learn that you did indeed learn about that.
1369500	1372460	That they did actually use the model they claimed they were.
1372460	1373540	In this case, GPT-4,
1373540	1375900	they managed to open source the weights.
1375900	1377300	So that's provable inference.
1377300	1378580	It can be used for APIs.
1378580	1379780	So I mentioned like chat GPT,
1379780	1381700	but there's many others, like video games.
1381700	1383340	If I'm playing an on-chain game
1383340	1384820	and there's some form of ML,
1384820	1386540	how do I know that the game is not cheating?
1386540	1389300	How do I know that I have fair rules on there?
1389300	1391220	The second one is bringing AI on chain.
1391220	1393180	So there's lots of smart contracts,
1393180	1395380	lots of applications that people are building
1395380	1396940	within the blockchain domain.
1396940	1398820	And within the blockchain domain right now,
1398820	1401020	it's very limited in terms of things it can do.
1401020	1403860	And machine learning can provide lots of cool solutions
1403860	1405860	to a lot of different problems, right?
1405860	1407980	At the end of the day, machine learning is able to provide
1407980	1410980	good enough approximations to problems that people have.
1410980	1412860	And so if we're able to bring that on chain,
1412860	1415220	we might be able to bring some interesting
1415220	1416620	opportunity to the table.
1416620	1418340	I mentioned the financial ones,
1418340	1421540	where for example, I have a yield protocol on chain
1421540	1424340	where I deposit assets to this protocol
1424340	1426660	and it tries to optimize the yield
1426660	1428140	that I get on those assets.
1428140	1430300	But it can, it has to use a strategy.
1430300	1433220	Usually these strategies are called source and hidden,
1433220	1435700	but at least I can commit to a strategy.
1435700	1437700	And this strategy can now leverage machine learning
1437700	1439500	and I can take proof to the protocol
1439500	1442420	that we're using a machine learning strategy fairly
1442420	1444300	and we're not updating the weights.
1444300	1445500	And we can also, for example,
1445500	1447380	prove that it was trained on some historical data
1447380	1448780	with some accuracy, right?
1448780	1451380	I can make a proof that my model is accurate
1451380	1453860	on some historical data in terms of yield routing
1453860	1455660	with some accuracy and it's routed
1455660	1459180	the most performant way, for example.
1459180	1462020	There's also another one which is agents or intents
1462020	1463060	in the context of blockchain.
1463060	1466580	So agents is a word that comes from the ML lingo,
1466580	1469380	which is like a program that has the ability
1469380	1470860	to do actions on their own, right?
1470860	1472700	They're a player, some system,
1472700	1474740	like game theoretical system in this case.
1474740	1477580	So if we have some system, let's say that, I don't know,
1477580	1479660	like we have a program and we allow
1479660	1481980	this machine learning algorithm, let's say robotics.
1481980	1485300	Robotic agents are, is a good analogy, right?
1485340	1487820	So I have a robot and the robot is able
1487820	1489100	to interact with the real world
1489100	1491860	because it has limbs, it has different tools
1491860	1493180	like cameras, et cetera.
1493180	1494700	And it's able to interact with the real world.
1494700	1496420	The robot in this case is an agent, right?
1496420	1498700	It's a program which is able to perform actions
1498700	1499540	in the real world.
1499540	1501140	It doesn't have to be a real world agent.
1501140	1502220	It can be a digital agent.
1502220	1503500	It can interact with a website.
1503500	1504660	It can browse the web.
1504660	1508180	It can watch a video and give me some information about it.
1508180	1510700	But essentially on-chain agent could, for example,
1510700	1512340	interact with a blockchain
1512340	1513980	if they have maybe some knowledge, right?
1513980	1516020	So if it's a smart agent, it sees that,
1516020	1517460	okay, something happened here.
1517460	1519860	So maybe I see that there's a liquidation happening.
1519860	1522380	So let me do this, let me buy this, let me sell that.
1522380	1524860	There's different agents that can learn based on information
1524860	1527540	and if they have a set of steps that they can do,
1527540	1529860	they can maybe try and optimize for some goal
1529860	1531860	and then they become agents in the system.
1531860	1534740	Blockchain people like to call this intense, yeah?
1534740	1537820	And another one is attestations, for example.
1537820	1540660	So I can make attestations about things, right?
1540660	1543340	I can prove to a smart contract that I'm over 18.
1543380	1547140	I can prove that all sorts of different things, right?
1547140	1549540	Essentially, I'm just able to use machine learning off-chain
1549540	1551900	and I can prove that and bring it on-chain.
1551900	1553420	Private and machine learning cruise.
1553420	1555460	So this one is a cool one.
1555460	1557300	So for example, in the context of medicine,
1557300	1561060	let's say that there's a cancer diagnosis model
1561060	1564620	and I as the patient, I do not wanna reveal to anyone
1564620	1567580	like my personal health records.
1567580	1568980	But for example, there's a doctor
1568980	1570660	or some health institution
1570660	1574100	which signs some form of report
1574100	1575500	or some form of certificates
1575500	1578420	to some personal health records or data.
1578420	1580580	And then there's a machine learning model
1580580	1583420	which uses this data to essentially evaluate it
1583420	1585940	and tells you whether you're likely to have cancer
1585940	1589100	or whether you have cancer and with what's uncertainty.
1589100	1590940	So something that you can hear
1590940	1592660	is that if you want to prove to, for example,
1592660	1596100	let's say that an insurance or a payout
1596100	1598300	had a condition that you've been insured against
1598300	1599980	or something like that,
1599980	1601420	you'd be able to prove to them
1601420	1604060	that there's some health institution
1604060	1606900	or a specific health institution if you want
1606900	1610340	that has concluded that I am indeed this
1610340	1612620	or have been diagnosed with a specific thing
1612620	1616180	without revealing the model, without revealing the weight,
1616180	1618300	but you at least know that there's some specific thing
1618300	1620180	that you can make a proof about.
1620180	1622500	The possibilities here are early big
1622500	1625620	in the sense that there is generally programmable.
1625620	1627900	So this is like just one concrete example,
1627900	1631500	but people can essentially make proofs about anything,
1631500	1634060	any data that they have and computations that they did
1634060	1636580	when they're machine learning base or not
1636580	1638540	without revealing the data itself, right?
1638540	1640260	The only person who learns about the data
1640260	1643340	in the context of ZK is the person making the proof.
1643340	1646420	I mentioned earlier that this property called completeness.
1646420	1648260	So in order to make a valid proof,
1648260	1649660	I do need to have the data.
1649660	1652580	So the problem is that there's always a prover,
1652580	1653660	always learns the data,
1653660	1655940	but if the prover is controlled by myself,
1655940	1658660	then only it's the same thing as me learning data.
1658660	1660620	So it's something that is a worthy trade-off.
1660620	1662700	So if I'm making a proof on my own computer,
1662700	1663540	that makes sense.
1663540	1665220	And I can prove to anyone else anything
1665220	1666420	without revealing my data.
1666420	1668780	But if I am, for example, delegating it to a server,
1668780	1670620	the server doesn't need to learn my information,
1670620	1672500	so I need to be careful.
1672500	1674180	And digital identity.
1674180	1676140	So I do want to explain very briefly,
1676140	1678860	like how did we come to this at WorldCoding?
1678860	1681540	So we have this hardware device, it's called an org.
1681540	1682940	I do have it with me.
1682940	1686060	Maybe if you guys want, I can just go grab it.
1686060	1688620	And one second in the FAQs, I can show it.
1688620	1692140	But essentially the WorldCoding org is a piece of hardware
1692140	1693820	that verifies two things.
1693820	1696460	It proves that there's a real person
1696460	1698260	in front of this hardware device.
1698260	1702980	It does this bunch of phenomenally detection like methods,
1702980	1706900	and some other like statistical-based methods,
1706900	1709580	some sensors that it has like infrared sensors,
1709580	1711300	and it has like field of depth sensors,
1711340	1713820	it has high-resolution cameras, et cetera.
1713820	1715660	And it's able to determine that there's a real person
1715660	1717500	in front of the hardware device, the org,
1717500	1719940	with like a shiny ball, I'll show it in a bit.
1719940	1723380	And it can also prove the person in front of it is unique.
1723380	1725740	And the way that it does that is that it takes
1725740	1728580	a high-resolution image of the person's IRC's,
1728580	1731620	and it's able to compute a unique representation of them
1731620	1733020	called an IRS code.
1733020	1735060	And this IRS code, the good thing about it,
1735060	1736500	is that it's not deemed personally
1736500	1739380	that if I put information, it's just the representation
1739380	1743500	of the uniqueness or of the randomness of a person's IRS,
1743500	1746180	and I can use that to measure how unique they are.
1746180	1748580	And if the distance between two different IRS codes
1748580	1751780	is big enough, I can prove that this user is unique.
1751780	1753860	And then once I prove that the person is unique,
1753860	1757100	I'm able to essentially put them in a set of verified users.
1757100	1760180	And then what we do is we have a protocol called WorldID,
1760180	1762660	which allows you to prove that you're a member
1762660	1764700	of this set of verified users
1764700	1766660	without revealing which member you are,
1766660	1768180	using zoological photography as well,
1768180	1771700	but not ZKML, just traditional zero-knowledge cryptography.
1771700	1774340	You're able to prove that I am a unique verified human being
1774340	1775980	without revealing who you are,
1775980	1778460	and the data that we collect, which is just this IRS code,
1778460	1780220	is not personally data-fibre information.
1780220	1783780	We don't collect the raw biometric images, which is cool,
1783780	1786460	because you're able to essentially leverage modern cryptography,
1786460	1789900	modern biometric literature, and the modern tools,
1789900	1792220	like modern hardware like GPUs and everything.
1792220	1793540	Everything happens client-side,
1793540	1796060	like within this actual hardware device, right?
1796060	1798660	So the hardware device does this computation,
1798660	1800580	nothing leaves the actual orb,
1800580	1804380	and then the orb deletes everything within its secure enclave
1804380	1806540	and computational environment.
1806540	1809060	Within this model of how work can work,
1809060	1812260	very simplistic model, there's one specific problem,
1812260	1814460	which in our biometrics pipeline,
1814460	1818620	if you change the pipeline in any significant way,
1818620	1821540	you change the outputs of this uniqueness representation,
1821540	1824500	you change the output space of the IRS codes,
1824500	1826100	you can think of them as vectors, right?
1826100	1828700	So you essentially take this vector space,
1828700	1830380	and you convert it to a different one.
1830380	1833660	So the same user will have a different representation
1833660	1834940	in this new space,
1834940	1838260	therefore you will not be able to measure uniqueness anymore.
1838260	1839980	So if you ever update the model,
1839980	1842020	you have to re-sign up all users.
1842020	1844100	And since you have this physical hardware device
1844100	1845380	that people have to go to,
1845380	1847660	it means that all the users that have signed up to date
1847660	1851660	to WorldID have to go in person again to this hardware device
1851660	1852940	and get re-signed up.
1852940	1854140	And this is terrible,
1854140	1856660	because it's already been really hard enough
1856660	1859060	for us to get 5 million plus users
1859060	1861980	and to have to force our users to re-sign up
1861980	1864380	every single time that we update the biometrics pipeline,
1864380	1866220	it would be really bad.
1866220	1868420	And it has like really terrible user experience.
1868420	1870660	And so this is where one of my coworkers,
1870660	1873300	his name is Remco, at the WorldCon Foundation,
1873300	1877740	he came across with a solution or an idea,
1877740	1882140	which was what if users self-custody their own biometrics,
1882140	1883660	meaning that the orb,
1883660	1886020	which has essentially a secure enclave
1886020	1888020	and a trusted execution environment.
1888020	1890740	So essentially these two pieces of chips
1890740	1893540	or these two chips allow you to sign things.
1893540	1895300	So I'm able to cryptograph a big sign,
1895300	1897380	something that the orb sees.
1897380	1899020	So whenever the user gets verified
1899020	1901340	that they're a real and unique person,
1901340	1903260	the orb can sign their raw biometric,
1903260	1906420	which it has in its memory for a given lifetime,
1906420	1908460	and it can give it to the user,
1908460	1911660	and the user can store their own biometrics
1911660	1914780	in their phone and they can encrypt them, of course,
1914780	1916460	store them safely in an encrypted fashion
1916460	1919580	on their own phone or cloud or whatever they prefer.
1919580	1922540	And they would be able to then have a signature
1922540	1924900	from the orb on the actual biometric, right?
1924900	1928620	You know that this image was seen at one point by the orb
1928620	1930860	and it said that this is a unique human being
1930860	1932180	and this is a real human being,
1932180	1934100	most importantly is the real part.
1934100	1937220	Whenever we want to update the model,
1937220	1939380	what the user could do is they would be able
1939420	1942140	to download the new model, the weights of the model,
1942140	1945100	and they seek the approving library for that specific model
1945100	1946500	and they would be able to create a group
1946500	1951060	that created this iris code within a zero-knowledge environment.
1951060	1953140	So they would be able to create a zero-knowledge proof
1953140	1955100	that they've created a valid iris code
1955100	1958220	from an input image, which was attested to by the orb.
1958220	1961820	So essentially the pipeline of trust here is not broke, right?
1961820	1964020	I know that I've created an iris code
1964020	1965100	from an original biometric,
1965100	1967180	which was verified by the orb to be unique.
1967220	1970740	And with this, I'm able to permissionlessly
1970740	1972420	or out of my own accord,
1972420	1975020	I'm able to permissionlessly insert myself
1975020	1976900	into the set of verified users
1976900	1978660	without having to go to the orb again
1978660	1981100	because I have the entire set of steps that I need
1981100	1983740	in order to prove to the world ID protocol
1983740	1986260	that I'm a unique user without revealing who I am again.
1986260	1989300	I just proved to you that, hey, the orb saw me at one point in time,
1989300	1992820	the orb did indeed sign this my image, I store my images,
1992820	1993660	and then I make a proof
1993660	1996180	that I've created this derived representation of uniqueness
1996180	1998020	for my biometrics,
1998020	1999660	and I can prove to them I'm a unique human
1999660	2001940	in this new representation and this new model
2001940	2004020	without revealing who I am again, right?
2004020	2006060	So this is like perfect things,
2006060	2007500	like a perfect solution for us.
2007500	2012500	It's actually quite crazy that this problem didn't exist,
2013500	2017260	at least that's what I felt when I first covered it through Remco.
2017260	2018900	And so right now, for example,
2018900	2020300	we're working with one of the companies
2020300	2021380	which I mentioned earlier,
2021380	2024940	Modulus to essentially do this client-side
2024940	2026860	zero-knowledge machine learning proving
2026860	2028580	inside of a user's phone, right?
2028580	2030580	So that people can self-cassellate the biometrics
2030580	2032460	and permissionlessly insert themselves.
2032460	2033660	It's like very early stages,
2033660	2035140	R&D is not yet in production,
2035140	2037260	but there's been a lot of good progress here.
2037260	2039500	And two years ago, it seemed like sci-fi,
2039500	2042020	now there's already like concrete proof concepts
2042020	2044860	and implementation and there's benchmarks
2044860	2046620	and things that are improving.
2046620	2049500	But yeah, this is like one of the things that I saw.
2050980	2052580	One last thing that I do want to mention
2052580	2055380	before I leave you to ask me questions
2055380	2057940	and for me to go grab my orb as well
2057940	2059580	is technical bottlenecks.
2059580	2060980	Zero-knowledge machine learning, right?
2060980	2062180	We've seen some use cases,
2062180	2064220	we've seen what people are working on,
2064220	2065580	what people are doing, what it is,
2065580	2067380	some of the things that we are doing,
2067380	2068860	but where do we go now, right?
2068860	2070860	If you're someone who is interested in this topic,
2070860	2072660	where could you contribute if you want to,
2072660	2074020	if you end up learning more?
2074020	2075660	How do you contribute to these?
2076540	2078380	Or what are the problems that are hard
2078380	2080940	and that would help us improve in the front?
2080940	2084060	So one is better cryptography, right?
2084060	2085980	Because it's better ZK.
2085980	2088620	As I mentioned, the worst in ZK and the worst of ML
2088620	2089980	create a joint bottleneck.
2089980	2092420	So if you improve ZK or if you improve ML,
2092420	2093900	you improve the KML.
2093900	2095980	But there is also an intersection
2095980	2098380	where if you just focus on the ZK parts
2098380	2099420	that would make ML better
2099420	2101700	and on the ML parts that would make ZK better
2101700	2105100	or simpler, that's the most focused effort
2105100	2108260	that you can make to essentially improve everything.
2108260	2109860	The one is better cryptography.
2109860	2111340	So remainder, for example,
2111340	2112700	the thing that I mentioned here,
2112700	2114220	remainder is a proving library
2114220	2117620	that is built by modulus labs or modulus,
2117620	2120740	which essentially uses a type of cryptography
2120740	2122980	which better models the structured nature
2122980	2124300	of machine learning computing.
2124300	2125660	Where like machine learning usually have
2125660	2127340	matrix multiplication.
2127340	2129220	They have some non-linearities.
2129220	2132060	So like functions that are non-linear, in this case,
2132060	2133620	there's like activation functions
2133620	2135060	or like a good example of this.
2135060	2137300	So there's things like ReLU,
2137300	2138860	which is one of the most popular ones,
2139420	2141820	defined linear unit, something like that.
2141820	2143740	Like tanh, there's a bunch of activation functions,
2143740	2144980	non-linear function.
2144980	2147980	So they built a cryptographic system
2147980	2152060	which is able to represent the structured computation
2152060	2154420	in a much more efficient way.
2154420	2158220	So when it comes to proving these structured computations,
2158220	2160420	it takes a lot less computational power to do so
2160420	2163260	because the representation is much more succinct
2163260	2164620	and much more efficient.
2164620	2166300	And so it makes it a lot faster
2166300	2167660	and a lot more performant
2167660	2169700	and less computationally intensive.
2169700	2172260	So this would potentially make it feasible
2172260	2174620	to run a DK machine learning prover
2174620	2177380	on a personal phone, for example.
2177380	2178860	Another one is better hardware.
2178860	2181420	So hardware and specialized hardware
2181420	2184260	is one of the things that modern science
2184260	2185860	has benefited from most.
2185860	2188100	We've seen the transistor consistently shrinking
2188100	2188940	and shrinking.
2188940	2190620	We fit more transistors on a chip,
2190620	2192580	almost like 2X every 18 months, right?
2192580	2195180	There's Moore's Law, which goes exponentially.
2195180	2197300	And now we're at like the two nanometer scale
2197300	2199660	where we have transistors that are two nanometers wide
2199660	2202460	and we're able to pack trillions of them on modern GPUs.
2202460	2204860	And for example, in the context of machine learning,
2204860	2206780	machine learning was really terrible
2206780	2209580	on traditional computers like CPUs back in the day,
2209580	2211580	like in the 60s and 70s and 90s.
2211580	2213820	So no one actually did machine learning back then.
2213820	2217180	But when these people were playing video game for some reason,
2217180	2218940	people started building chips
2218940	2222820	that represent graphical interfaces a lot better.
2222820	2226780	And it happens that there's an overlap of the mathematics
2226780	2230300	that are used to represent graphics and graphics card
2230300	2232540	and the machine learning, right?
2232540	2234660	Machine learning is the matrix multiplication
2234660	2237500	and the way that you represent pictures is matrices, right?
2237500	2240580	It's just zeros and one that represent the RGB values
2240580	2242660	of every single pixel on the screen
2242660	2244220	and transformations between them.
2244220	2245660	And so you have to do these operations
2245660	2246860	between pixels really fast.
2246860	2248300	And it just happened that it's the same thing
2248300	2250820	as doing machine learning like neural network
2250820	2253900	fast multiplication across multiple connected layers.
2253900	2255420	It's like very similar structure.
2255420	2258980	And so people start using GPUs to speed up machine learning
2258980	2261780	and machine learning became feasible all of a sudden in 2012
2261780	2263180	with convolutional neural networks
2263180	2266260	and all these new like booms that we've been writing
2266260	2267900	until now with modern LLMs.
2267900	2270380	Like LLMs and all these new generative AI models
2270380	2272580	are only possible because of this specialized hardware
2272580	2276540	that come from NVIDIA, DCMC, AMD, ASML,
2276540	2278780	like all these like transistor manufacturers,
2278780	2281860	graphic car manufacturers, specialized hardware manufacturers.
2281860	2282980	These ones are for machine learning,
2282980	2284940	GPUs and tensor processing unit,
2284940	2287540	GPUs, cryptography on the other hand,
2287540	2289340	they work with a different type of math.
2289340	2291780	Instead of working with floating points or arithmetic,
2291780	2295340	they work with finite fields and sixth point arithmetic.
2295340	2297820	And so you need to design fundamentally different hardware.
2297820	2299380	And so we need to build better hardware
2299380	2302340	to improve the computational capabilities
2302340	2304100	of zero knowledge machine learning
2304100	2306700	or just zero knowledge cryptography in this then.
2306700	2308300	So there's lots of things to be done here.
2308300	2310580	So I'm, for example, I'm also an investor in Fabric,
2310580	2311420	I'm sorry for that,
2311420	2314700	but Fabric is one of the ZK hardware company.
2314700	2317020	In GoNyama, not size thick, it's size thick,
2317020	2319460	sorry about that, some misspelling without the T.
2319460	2320380	And irreducible,
2320380	2323260	there's some of the biggest ZK hardware companies.
2323260	2325740	And yeah, so these are trying to essentially model
2325740	2328140	the software in hardware so that it's faster
2328140	2329740	and there's less overhead.
2329740	2331100	Another one is better tooling.
2331100	2332540	So I mentioned Ezekiel and Giza.
2332540	2334180	They're building tooling that makes it easier
2334180	2336820	for developers to use ZK.
2336820	2338260	And if I'm a machine learning engineer,
2338260	2340940	there's no way in hell I'm gonna spend six years
2340940	2343860	learning cryptography and learning the state of the art
2343860	2345020	and trying to contribute there
2345020	2346980	so that I can prove my machine learning model.
2346980	2348100	As a machine learning engineer,
2348100	2351340	I just care about something that ZK can bring to me.
2351340	2352980	And vice versa, if I'm a ZK guy,
2352980	2355220	I just care about something that ML can bring to me
2355220	2358440	to get better or like somehow make it on chain.
2358440	2361180	So whenever we've brought down the cost of barriers,
2361180	2364140	like barriers that the cost barriers
2364140	2366380	that prevent us from doing something,
2366380	2367660	people start experimenting, right?
2367660	2370500	Like same thing happened with the web.
2370500	2372260	Like anyone can build a website nowadays
2372260	2373180	and you can build a business,
2373180	2374260	you can just be Shopify.
2374260	2375420	And if I'm a business guy,
2375420	2376780	I don't need to know web development.
2376780	2378060	Shopify and I have my store
2378060	2380020	and I can process millions of dollars of payment.
2380020	2382860	I can have a truly user and everything.
2382860	2384900	And otherwise I would have to learn web development,
2384900	2385980	servers, everything.
2385980	2386940	I don't have to care about that.
2386940	2387780	I just do my business
2387780	2390340	and I use web technology without having to know how it works.
2390340	2392980	So the same thing applies to KML of course.
2392980	2395020	More robust than secure implementation.
2395020	2397060	That one is a bit like self-explanatory
2397060	2398980	but essentially like the more security
2398980	2402380	the less prevent like if we can prevent hacks and exploit
2402380	2403980	then if it's more robust,
2403980	2406460	it can sustain more users, et cetera.
2406460	2408340	And the other one is like what I mentioned before
2408340	2409420	pretty much at the same point,
2409420	2411620	like better tooling and easier interfaces
2411620	2412660	is pretty much the same thing
2412660	2414260	because the easier it is to use,
2414260	2416460	the more experimentation there for the more products,
2416460	2417660	the more product market fits,
2417660	2418940	the more businesses can build
2418940	2421220	and the more technology can accelerate
2421220	2424180	towards the direction of growth.
2424180	2427020	So yeah, that's everything about my presentation
2427020	2431540	and I would love to answer any of your questions.
2431540	2433060	I don't know how long we have.
2433060	2435260	I think it's 14 minutes for FAQ.
2435260	2438260	I can also go run, get the orb if you guys wanna see it.
2438260	2440540	And thank you for having me.
2440540	2441380	This was fantastic.
2441380	2442380	Thank you so much.
2442380	2443220	What a world.
2443220	2445100	And we have a few questions already here
2445100	2446260	from people in the chat
2446260	2448500	and then maybe after a few we give you some time to breathe
2448500	2450620	and get the orb, that would be great.
2450620	2452100	Okay, so first one, Shadi,
2452100	2454420	if you wanna unmute your first.
2454420	2456300	Hi, yeah, thanks for the great presentation.
2456380	2457580	Very informative.
2457580	2460740	I had a question about the personally identifying information
2460740	2464420	from the hash from the iris biometrics.
2464420	2467820	Isn't a hash or iris still uniquely identifying
2467820	2471500	if you know the hashing function to produce that digest?
2471500	2474700	Or did you mean that make the function is kept secret
2474700	2478860	and nobody can easily take like a photograph of someone
2478860	2482100	and then produce the same hash and that look on chain,
2482100	2483460	for example, I don't think you posted on chain
2483460	2486220	but look on chain, for example, to try to match that.
2486940	2490380	Yeah, there's one unfortunate naming collision here.
2490380	2491740	So in biometric literature,
2491740	2494780	people use a hash in a non-urgorous way.
2494780	2496180	And so what we mean here,
2496180	2497260	or what we used to mean,
2497260	2499900	we've changed the way that we explain these things.
2499900	2501700	We no longer use the terminology of hash
2501700	2505500	because we work in the intersection of AI and cryptography
2505500	2507900	and if you use a term that means a different thing in both,
2507900	2511420	it's like ambiguous and it can cause problems
2511420	2513100	like this one right now.
2513100	2516780	Actually, the way the biometrics pipeline works
2516780	2520020	is that there's this essentially convolution-like algorithm.
2520020	2523020	It's called the GABER wavelet or GABER filter,
2523020	2524660	which essentially applies convolutions
2524660	2526660	into original biometrics many times over
2526660	2530500	and it's able to compute like a randomness representation.
2530500	2534460	And this one essentially compresses the image so much,
2534460	2536300	like after performing all these operations,
2536300	2540820	you end up with a pretty much a small representation
2540820	2542660	of a few bits, like I think it's 200,
2542660	2544860	something that, so the vector in the end,
2544860	2546660	like the embedding in the end is like a few bit.
2546660	2549980	And this one is not able to be reconstructed
2550980	2553660	to its original, at least like a lossy function, right?
2553660	2556140	If I go from a compressed representation
2556140	2558580	to a fully, try to expand it back,
2558580	2559900	I lose information in the process
2559900	2561940	of converting it to this compressed representation.
2561940	2563980	Therefore, I'm not able to reconstruct the same one.
2563980	2567060	And the good part of this is that I'm able to,
2567060	2568860	I'm able to reconstruct something similar,
2568860	2571300	but it's not personalized identifiable,
2571300	2574340	at least not considered so in modern literature, right?
2574340	2576860	This may change and this is why we've been working
2576860	2578580	on a lot of other things within world economy,
2578580	2581380	like more of the party computation solutions and whatnot.
2581380	2583180	We're gonna be publishing a lot about this
2583180	2585180	in the coming month, but if you're interested
2585180	2588060	in like follow this, the biometrics pipeline works
2588060	2590140	and have the definition of it and how it works
2590140	2591500	and what is actually going on,
2591500	2593740	I recommend going to the link I just said in the chat,
2593740	2595300	my paper that work in the org.
2595300	2597260	Also, one of my teammates is in the,
2597260	2599460	actually one of the former teammates,
2599460	2600820	he's at Tool for Humanity,
2600820	2602220	which is the labs entity.
2602220	2604620	I'm at the foundation of different legal entities,
2604620	2606740	but they're both contributing to the world team project.
2606740	2608380	His name is Daniel Gershiewicz.
2608380	2609660	He is in the cause law.
2609660	2611700	So he's also able to explain a bit more.
2611700	2612860	He's on the org software team.
2612860	2616140	So he works a lot more with the biometrics pipeline than I do.
2616140	2617740	I'm more still in the cryptography
2617740	2618980	protocol side of thing.
2618980	2620460	But yeah, within the white paper,
2620460	2621900	white paper.worldcoin.org,
2621900	2623300	you have a biometric action
2623300	2624380	and you have the third definition
2624380	2625660	of what it is that we're doing
2625660	2627260	and how we preserve privacy.
2627260	2629900	So to answer your question in a specific way,
2629900	2631620	it's not a hash, it's not a cryptographic hash.
2631620	2633060	There's no digest, there's no plaintext.
2633060	2635660	It's essentially a convolutional like operation
2635660	2636700	which happens many times
2636700	2638300	and it leaves the input unrecognizable
2638300	2639380	and you compress certain information
2639380	2641100	of the randomness that you get.
2641100	2643340	You cannot use that to reconstruct the original thing
2643340	2644380	that you put into this function
2644380	2645980	because it's a very lofty function.
2645980	2648780	And this is good enough to prevent
2648780	2652060	like getting the raw biometric out again.
2653180	2654020	Got it.
2654020	2656420	So the idea is even if I had access
2656420	2659060	to the kernels that you used to train,
2659220	2664220	then I wouldn't be able to deconvolute the output.
2665740	2666580	I see.
2666580	2670500	Ideally to try and break our own assumptions
2670500	2671700	and try to reverse engineer
2671700	2673220	and actually get the original image.
2673220	2675420	And now we've gone ahead a step further
2675420	2677100	because if it was possible,
2677100	2678100	we've gone a step further
2678100	2679940	and we're now storing everything in ciphertext
2679940	2681300	and the uniqueness check,
2681300	2684060	it's happening on ciphertext with multi-party computation.
2684060	2686860	So yeah, that's like cool, cool new research stuff.
2687820	2688660	Love it.
2688660	2690100	Dan also shared, I think the white paper
2690100	2691700	that you referenced directly here in the chat
2691700	2693340	already a little further up.
2693340	2694420	Thanks for that, Dan.
2694420	2697380	Next one up we have Richard and then we have Micah.
2698540	2700620	Yeah, I think the previous discussion
2700620	2701780	answered my question there.
2701780	2703020	Thank you.
2703020	2704060	Awesome.
2704060	2704900	Wonderful.
2704900	2706340	Micah, you go.
2706340	2707540	Micah, we can't hear you.
2707540	2709740	Feel free if you can't unmute to put your chat,
2709740	2711700	your question in the chat.
2711700	2713420	Okay, he's going to rejoin.
2713420	2715860	This could be a great opportunity for you to get the orb.
2715860	2719780	I also have a few questions, but God, you go.
2720620	2721460	Can you hear me?
2722580	2724020	Yes, Dan, we can hear you.
2725300	2727180	I want to go back, there was this question about
2727180	2729740	and things being personally identifiable
2729740	2731820	or uniquely identifying information.
2731820	2734540	And then the question turned into ashes
2734540	2737420	versus wavelet encodings.
2738500	2739340	I don't know.
2739340	2741820	I think the question actually got lots of discussions.
2741820	2744740	And the interesting thing is that even if you've had
2744740	2746220	either a hash or an encoding,
2746220	2749580	and you somehow broke this and could reverse that image,
2750580	2752940	actually the privacy comes from
2752940	2754340	what was briefly mentioned in the talk,
2754340	2758900	which is that when you prove your ownership of such,
2758900	2761140	you're proving ownership of a key
2761140	2765780	that was linked to this biometric encoding.
2765780	2767260	So when you prove ownership of that key,
2767260	2769500	you're not pointing to which encoding is yours.
2769500	2770420	So you're a member of the set
2770420	2772220	without revealing which member.
2772220	2773420	And that means that these encodings
2773420	2775540	are cryptographically delinked from anything else,
2775540	2777580	your transactions, your accounts.
2777580	2778540	Nothing can be linked back.
2778540	2783180	So if you did reverse those codes, you wouldn't know.
2783180	2785260	Also, one additional thing is that
2785260	2787500	these encodings are not public.
2787500	2790340	They're hidden in a database that we have.
2790340	2793020	The thing that is public is the public key
2793020	2796220	associated with the user that has undergone a unique question.
2796220	2798380	So if I have this unique coding,
2798380	2801460	and I prove that I'm a unique within the coding set,
2801500	2804500	which is kept not on any public sphere,
2804500	2806980	it's now kept in this multi-party computation
2806980	2808500	encrypted environment in a database
2808500	2811380	that is run by three different parties
2811380	2814340	in an MPC setting, again, multi-party.
2814340	2816660	And when the user is verified to be unique,
2816660	2818060	we take the user's public key,
2818060	2819660	which was generated by the World app,
2819660	2821620	which is the way that you interface with World ID
2821620	2823980	and the wallet and a bunch of other things that we're building.
2823980	2825460	Essentially, the public key that was generated
2825460	2827300	by the World app, by the user,
2827300	2829100	which is a unique person that's been just verified
2829100	2832780	by the World, gets inserted into the set of verified users,
2832780	2834860	and then I'm able to make a knowledge proof
2834860	2837820	that I own a private key to a public key
2837820	2839420	and the set of verified users.
2839420	2841220	So even then, there's one more step
2841220	2844100	that removed from your biometric completely,
2844100	2846300	because a public key is just random,
2846300	2849420	cryptographical, gibberish that I can make proofs about,
2849420	2851420	and I'm able to prove to you that I'm a unique member
2851420	2852500	of the set because I own a private key
2852500	2855020	to a public key in the set, but I don't know which one.
2855020	2856540	And there's another cool part,
2856540	2858660	which is the nullifier scheme that we have,
2858660	2861100	which allows you to represent unique actions.
2861100	2863700	For example, one is like unique governance,
2863700	2866220	like one person, one vote, digital governance,
2866220	2867820	or voting protocol.
2867820	2868980	Currently, there's no way to prove
2868980	2870580	that you're not a bot online.
2870580	2872620	So if you, for example, let's say, I don't know,
2872620	2875780	Elon Musk puts a poll on Twitter or on X
2875780	2878380	that, hey, is this doc cute or no?
2878380	2883100	I can create a bajillion X accounts and vote for no, right?
2883100	2884300	There's no way for me to prove
2884300	2885940	that this is a one person, one vote.
2885940	2888420	So whomever who posts a poll on whatever thing
2888460	2890220	doesn't matter, like the opinion doesn't matter,
2890220	2891820	the result of the poll doesn't matter.
2891820	2893580	There's million bots that have incentives
2893580	2895740	and both, like presidential elections,
2895740	2898140	if Elon says, is this candidate a good guy?
2899100	2901100	People can vote yes, but it can be like a third,
2901100	2904380	like external nation state actor trying to just
2904380	2906740	civil attack, which means like attack a protocol
2906740	2909100	where you need unique members in a way
2909100	2911340	that just make the protocol broken
2911340	2912940	completely beyond repair.
2912940	2914140	So this is where we step in,
2914140	2916780	where we create a unit of account of uniqueness
2916780	2919140	for humans in a digital environment,
2919140	2921620	whether it's on chain or it's off chain doesn't matter.
2921620	2923300	As long as you're able to make these cryptographic
2923300	2924980	at the station that I am a unique person
2924980	2926820	and I've not done an action before,
2926820	2929260	so I'm able to prove that I'm a unique actor.
2929260	2931420	I'm a unique member of this protocol
2931420	2933020	and I only voted once.
2933020	2936420	So I can say that this dog was cute only once.
2936420	2937940	And if I want to weigh the outcome,
2937940	2939660	the only way to weigh the outcome is that I need
2939660	2941340	to convince a thousand other members
2941340	2942460	to vote for the same thing,
2942460	2944660	but I'm not able to just create a million accounts
2944660	2947620	and vote for a same thing to weigh the outcome.
2947620	2948460	Yeah.
2948460	2949300	Awesome.
2949300	2950340	Yeah, I think Bramco talked a little bit about that,
2950340	2952340	especially like with possible future applications
2952340	2954380	also that you also listed very briefly,
2954380	2956580	including in medicine and so forth
2956580	2959020	that I think these groups are just like really incredible for
2959020	2961260	because it can't for medicine, for financial risk,
2961260	2962100	for insurance and stuff,
2962100	2965220	you just can't really access the data any other way.
2965220	2968020	Or you just can't do too much anyways with the information.
2968020	2969140	Okay, we have another question,
2969140	2971540	but you also have the orb now or?
2971540	2972780	It's right here, my lap.
2974740	2976140	Wonderful.
2976140	2978100	By the call, the battery,
2979500	2980340	at least.
2981500	2982540	All right, sorry.
2982540	2984700	I'm gonna unbler my background.
2984700	2986660	I do have a, I just moved into a new apartment,
2986660	2988300	so forgive me for,
2990060	2995060	but yeah, the battery right here.
2995300	2997940	Yeah, that is able to say a lot more about the orb than I can.
2997940	2999900	If you work on the orb software team,
2999900	3003300	you've been working on this for three years plus.
3003300	3004140	Yeah.
3004940	3008300	Yeah, I've come in multiple close contacts with the orb already
3008300	3011020	and I think in 50 years you even have it like taken apart,
3011020	3012180	you know, and it's different components,
3012180	3013620	which is really fun to see.
3013620	3015340	So yeah, thanks.
3015340	3017740	The orb hardware specs are publicly available
3017740	3018940	on GitHub as well.
3018940	3020700	So people can see the PCB design,
3020700	3022700	they can see like what components it's made out of.
3022700	3023900	There's also an hour paper,
3023900	3026980	there's like an annotated set of every single component
3026980	3029940	and like what it does, how it works, et cetera.
3029940	3031140	Yeah, I've been following like
3031140	3032380	just how many people are signing up
3032380	3034580	and like the very, very long lines.
3035340	3038060	Sign up stations, which has been really interesting.
3038060	3040700	Okay, Micah, you rejoined and you raised your hand.
3040700	3042940	Do you want to ask you a final question?
3042940	3044140	Testing, can you hear me?
3045540	3046580	Yep.
3046580	3048780	Quick comment and then a question after that.
3048780	3051700	The, I believe that while the transactions,
3051700	3054660	your transactions made using your unique ID
3054660	3055820	can't be linked back to you.
3055820	3057580	You can be linked back to your transaction.
3057580	3060060	Someone has an orb or the algorithm in the orb
3060060	3061940	and they can get a picture of you, so to speak.
3061940	3064500	They can then regenerate your unique ID
3064500	3067180	and this is an unnecessary piece.
3067180	3069420	You can't get rid of it because in order to have
3069420	3071820	a unique human, you need to be able to verify.
3071820	3073620	There needs to be only one outcome.
3073620	3075380	You can't introduce randomness here, right?
3075380	3079740	And so if your goal is to not have someone be able
3079740	3081700	to tell which transactions you did,
3081700	3082660	that is not possible here.
3082660	3085980	But someone can't tell just by looking at the transactions
3085980	3086820	that they were yours.
3086820	3088220	It's a one-way thing.
3088220	3089980	This one held you down and put an orb in front of your eye,
3089980	3091600	they can then figure out all your transactions,
3091600	3093260	but they couldn't look at your transaction
3093260	3096620	and figure out which eye they belong to, so to speak.
3096620	3100260	Yes and no, there's one step that helps us mitigate this,
3100260	3102180	which is the separation of the public key, right?
3102180	3103900	Like your transactions are not being done
3103900	3105860	by your iris code or whatever.
3105860	3107940	The transactions are being done by a public key
3107940	3109220	which was owned by a user,
3109220	3111900	which just happened to verify at the orb, right?
3111900	3114580	You would have to get the user's private key
3114580	3116500	to learn what they did on the game.
3116500	3118460	And then you would also have to get their bandwidth forked
3118460	3120820	to try and interlink too, right?
3120820	3122060	You'd have to get their bandwidth forked,
3122060	3123220	you'd generate the iris code,
3123220	3125780	and then get somehow steal from them their private key.
3125780	3126980	And then with a private key,
3126980	3130100	you're able to de-anonymize the on-chain state
3130100	3131700	that they performed in a DK way
3131700	3134340	because you're not able to just get the live iris.
3134340	3135960	You know you can generate them.
3135960	3139780	And so there's two things that you need to compromise there.
3139780	3140980	I'll just, my actual question,
3140980	3141820	I'll try to keep it,
3141820	3143460	I know I've got one minute left.
3143460	3146180	Last I checked, DK proof of an execution takes
3146180	3148740	on the order of a thousand times or so,
3148740	3151580	executing the same thing without a DK proof.
3151580	3153740	Even though inference is significantly cheaper
3153740	3156700	than training, execution costs is still very non-trivial.
3156700	3159340	That's why you need giant GPUs and whatnot,
3159340	3161220	just to do inference.
3161220	3162620	The use cases you're thinking of,
3162620	3165580	are they all things that are like become useful
3165580	3169180	once we can get the DK proofing costs down by a hundred times?
3169180	3171460	Or do you think there's some things that are usable
3171460	3174460	even with that thousand increase in execution costs?
3174460	3177300	Right now there's already like DK use cases on-chain, right?
3177300	3180460	And there's equally expensive in the ML lens,
3180460	3184020	like already proved a hundred million like parameter models
3184020	3185700	in an inexpensive way, right?
3185700	3187100	In a usable way, right?
3187100	3189220	Let's say you have a small convolutional neural network
3189220	3192500	classifier with 200 million like weights,
3192500	3193580	like flowing point,
3193580	3196340	or in this case for a ZKML with field elements,
3196340	3198820	but you're able to make proofs in reasonable time,
3198820	3201020	one to two minutes for inference, right?
3201020	3204540	Where the evaluation of it in the normal world
3204540	3205700	is a thousand nights less,
3205700	3208740	two millisecond, 20 millisecond, like point two second,
3208740	3210340	or 200 million, my bad.
3210340	3212780	So yeah, like this is like the costly incur,
3212780	3214740	but it makes sense for some things.
3214740	3216820	And right now, as I mentioned,
3216820	3218300	like the things that we're doing could prove the thing,
3218300	3220140	like cryptography, better implementation,
3220140	3221900	better hardware, specialized hardware, et cetera,
3221900	3223620	they're gonna bring down this cost significantly,
3223620	3225460	it's gonna make more things feasible.
3225460	3227220	Like now we can maybe prove an LLM,
3227220	3229140	it may take 10 minutes,
3229140	3231100	but maybe proving that LLM once
3231100	3234700	for something that's really important enables a new thing.
3234700	3235900	And it's always happened like this,
3235900	3237620	that like the use case and the demand for it
3237660	3239060	and bring the proving time down,
3239060	3240580	the overhead down, the performance up,
3240580	3243060	and we're still like have a bunch of things to do.
3243060	3245260	So it should work out eventually.
3246340	3247180	Love it.
3247180	3248020	Any final words?
3248020	3248980	How can people find out more?
3248980	3250300	I know that you said, for example,
3250300	3251700	there's an announcement coming on soon,
3251700	3253100	but if people are really excited about this
3253100	3254300	or they just wanna learn more,
3254300	3257060	what are any possible action items that people can take?
3257060	3258220	So action items.
3258220	3260500	So if you have a specific question about this presentation,
3260500	3262980	you wanna ask me, I think my Twitter or my Telegram,
3262980	3264940	or like my ex and my Telegram are the best.
3264940	3267500	So my handle is DC build 3R.
3267540	3271820	So DC builder, but with the 3 instead of an E at the end.
3271820	3273500	So that is for asking me questions.
3273500	3276660	If you were interested in ZKML itself,
3276660	3280940	I do have a resource aggregator for ZKML things.
3280940	3283540	It's on my, or one of my GitHub's,
3283540	3288540	which is github.com slash ZKML dash community slash
3288860	3290860	awesome dash ZKML.
3290860	3293940	I'm able to leave the link in the chat real quick,
3293940	3296380	but I think like the spelling makes sense.
3296420	3299380	Besides that, there's a bunch of startups working in ZKML,
3299380	3303820	mostly like what I mentioned, like Modulus, Giza, and Ezekio.
3303820	3306940	And these three keep coming out with new developments,
3306940	3308900	new things, new announcements, et cetera.
3308900	3311300	There's cryptographic papers coming out on ZKML,
3311300	3313820	which I also try and keep up to date on my resource.
3313820	3316900	So yeah, those are like the best ones, I think.
3317740	3318580	Love it.
3318580	3319580	And I just saw from Dan that he's bringing
3319580	3321580	up to our upcoming May workshop though.
3321580	3324020	If you're going to that one, you may be able to try it.
3324020	3324980	Hey, thank you so much.
3324980	3326220	This was really fantastic.
3326260	3327820	Thanks for staying on three minutes longer.
3327820	3328740	I really appreciated it.
3328740	3330380	Thanks for all of your great questions, everyone,
3330380	3331860	and I hope to see you guys soon.
3331860	3332700	Bye-bye.
