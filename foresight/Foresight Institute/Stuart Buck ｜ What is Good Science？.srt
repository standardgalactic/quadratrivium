1
00:00:00,000 --> 00:00:04,800
Hi, everyone. Welcome to Foresight's existential podcast. We're really delighted to have Stuart

2
00:00:04,800 --> 00:00:11,440
Buck here today. We actually met in person at a recent EAG before. I've had heard of you research

3
00:00:11,440 --> 00:00:16,560
before and just booked a call with you or an in-person meeting there. We stumbled over a bunch

4
00:00:16,560 --> 00:00:21,600
of really interesting metascience projects and problems, challenges, and possible solutions,

5
00:00:21,600 --> 00:00:26,320
and then dove into a little bit of perhaps the new emerging landscapes of interesting orgs.

6
00:00:26,960 --> 00:00:30,560
I'll talk to you a little bit about that in the introduction. I'd love to hear from you on

7
00:00:30,560 --> 00:00:34,800
that. I guess we were just ships passing the night itself by Southwest too bad,

8
00:00:34,800 --> 00:00:39,920
but I decided to hopefully next time in person connect you. All right, so maybe just to get

9
00:00:39,920 --> 00:00:44,720
us started, would you want to share a little bit more about good sciences really up to and if you

10
00:00:44,720 --> 00:00:49,920
can also get your journey into the organization? I think that usually helps people like map the

11
00:00:49,920 --> 00:00:55,200
genealogy of your work a bit. Sure. I should start by reminding about 12 years or so.

12
00:00:56,080 --> 00:01:00,080
Around 2012, I went to go work for a place called the Laura and John Arnold Foundation.

13
00:01:00,080 --> 00:01:05,520
It's this major billion-dollar plus philanthropy that focuses a lot on evidence-based policy,

14
00:01:05,520 --> 00:01:08,960
and it's grown a lot since I joined. When I joined, it was pretty new. There were like

15
00:01:08,960 --> 00:01:13,680
eight or 10 people. There are now it's over 100. The scope and the scale of it is really

16
00:01:13,680 --> 00:01:18,000
growing, but right from the start, the Arnold's, who themselves are around 50 years old,

17
00:01:18,000 --> 00:01:23,040
they had retired at 38 and were devoting their wealth to philanthropy. They were mostly interested

18
00:01:23,040 --> 00:01:27,840
in just evidence-based policy across lots of areas like education and criminal justice and health.

19
00:01:29,200 --> 00:01:34,080
One thing that we initially started noticing was the what you call the reproducibility crisis,

20
00:01:34,080 --> 00:01:38,800
the problem of trying to replicate research. We first noticed it in psychology, but I started

21
00:01:38,800 --> 00:01:43,120
digging into it as director of research there. It's a problem in a lot of fields,

22
00:01:43,120 --> 00:01:48,160
including medicine and cancer biology and economics. Pretty much any field you dig into

23
00:01:48,160 --> 00:01:52,000
that turns out there are some issues with replication, sometimes outright fraud.

24
00:01:52,960 --> 00:01:58,320
Just the publication process is often biased towards exciting positive results, which is

25
00:01:58,320 --> 00:02:03,520
natural. We all want to have exciting positive results come out of the scientific world, but

26
00:02:03,520 --> 00:02:08,080
when there's a bias towards that, then people feel compelled to sometimes stretch the truth,

27
00:02:08,080 --> 00:02:12,400
push the boundaries of acceptable practices. The Arnold's decided that if you want to pursue

28
00:02:12,400 --> 00:02:16,320
evidence-based policy, it's really difficult to do that if you're not sure how much you can trust

29
00:02:16,320 --> 00:02:20,800
the evidence, or if you think the evidence has been biased in a positive direction.

30
00:02:21,760 --> 00:02:27,200
Their initial vision of philanthropy had been that they would look to education, for example,

31
00:02:27,200 --> 00:02:32,320
and it would be fairly simple just to find what are the best ideas supported by the best evidence,

32
00:02:32,320 --> 00:02:36,240
and then just write a big check to the best idea, and then it's very simple.

33
00:02:36,240 --> 00:02:39,760
But it turns out it's much more complicated than that if you really start digging into the evidence

34
00:02:39,760 --> 00:02:45,920
as to what works. I started a grant-making program there with the Arnold's money, of

35
00:02:45,920 --> 00:02:52,960
course, focused on open science and reproducibility and trying to improve science. I handed out

36
00:02:52,960 --> 00:02:59,280
probably $60-plus million over several years, and then in the process of that endeavor,

37
00:02:59,280 --> 00:03:03,920
I ran across a guy named Patrick Collison who runs a company called Stripe. I ran into him

38
00:03:03,920 --> 00:03:08,240
several years ago at a conference, and he was very interested in trying to improve science,

39
00:03:08,240 --> 00:03:12,640
but not just reproducibility, but improving innovation, improving the pace of innovation,

40
00:03:12,640 --> 00:03:17,200
the freedom that the best scientists have to explore the universe and explore their best

41
00:03:17,200 --> 00:03:23,440
ideas without having to cater to what funders most desire. So anyway, I introduced him to John

42
00:03:23,440 --> 00:03:29,600
Earl, and we continued conversations, and then a couple of years ago, actually, time has flown.

43
00:03:29,600 --> 00:03:33,920
Actually, it's going on two and a half, almost three years ago. I had some further conversations

44
00:03:33,920 --> 00:03:38,480
with Patrick Collison that led to him being the initial funder for what I'm doing on my own now,

45
00:03:38,480 --> 00:03:42,960
which is a good science project. It's a small, I guess you could say, think tank focused on

46
00:03:42,960 --> 00:03:48,080
trying to improve federal science funding and policy so that we have faster innovation and,

47
00:03:48,080 --> 00:03:53,600
hopefully, more breakthroughs and clean up reproducibility as well. So that's the journey

48
00:03:53,600 --> 00:03:58,320
as to how I got to where I am now. Robert, I think it's always really interesting to hear,

49
00:03:58,320 --> 00:04:02,480
and the individuals involved and so forth, and somewhat of the serendipity in it. Okay,

50
00:04:02,480 --> 00:04:07,120
that's wonderful. Maybe let's dive into a few of the topics that you actually focus on to give

51
00:04:07,120 --> 00:04:11,520
people a little bit of a taste of what you guys are working on. I know that you've published a

52
00:04:11,520 --> 00:04:15,920
lot on Substack, but in other outlets, too, and you sent me a few really interesting docs,

53
00:04:15,920 --> 00:04:20,560
so I just want to jump around here a bit, if you don't mind. And one that I thought was really

54
00:04:20,560 --> 00:04:24,480
interesting and is, of course, has become, I think, a pretty prominent field recently,

55
00:04:24,480 --> 00:04:28,880
is the field of meta science progress, and it's not really necessarily involved with individual

56
00:04:28,880 --> 00:04:33,200
scientific fields. For example, Forsyte was a part specific researchers working on one technology,

57
00:04:33,200 --> 00:04:36,480
but it's really also looking at a broader, with a broader lens of what could be improved

58
00:04:36,480 --> 00:04:40,080
in the ecosystem. And you've written some really interesting stuff there. And so I'd just love

59
00:04:40,080 --> 00:04:44,080
to know, including, for example, how much progress we've made in meta science, but where we could

60
00:04:44,080 --> 00:04:49,600
still speed up progress. And I'd love to get your thoughts on the broader, if you think about

61
00:04:49,600 --> 00:04:53,520
science from this meta lens, like you mentioned reproducibility as one, but what are a few of

62
00:04:53,520 --> 00:04:57,200
the different areas that you think are really holding scientists back right now at producing

63
00:04:57,200 --> 00:05:00,720
the research that we would all want from them? And then perhaps a few recommendations that you

64
00:05:00,720 --> 00:05:05,920
have here. Sure, that's really broad. So I'll just pick one issue that I care about a lot. And

65
00:05:06,000 --> 00:05:10,720
a lot of folks have focused on, and it's really tough to crack down. It's the issue of bureaucracy.

66
00:05:10,720 --> 00:05:15,440
Everyone hates bureaucracy in the abstract. But the problem is that everyone loves bureaucracy

67
00:05:15,440 --> 00:05:21,120
when you point to any particular feature of it. So to take us back, there have been multiple

68
00:05:21,120 --> 00:05:26,560
surveys of federally funded scientists over the past couple of decades. And the most recent survey

69
00:05:26,560 --> 00:05:31,600
surveyed thousands of federally funded scientists. And they said, on average, that they were spending

70
00:05:31,600 --> 00:05:38,480
44% of their time on bureaucracy, basically filing reports and budgets and proposals and

71
00:05:38,480 --> 00:05:44,080
just all the machinery that comes with getting a federal grant. And so everyone points to that

72
00:05:44,080 --> 00:05:48,480
and says, that's a huge problem. We're scientists spending nearly half their time on bureaucracy.

73
00:05:48,480 --> 00:05:54,480
That seems like we're paying people to dig a hole and fill it back in. And as you say,

74
00:05:54,480 --> 00:05:59,200
that's an average. There are some scientists who are fortunate enough to have great administrative

75
00:05:59,200 --> 00:06:04,240
help. And so they don't have personally have to spend as much on the other extreme. I talked to

76
00:06:04,240 --> 00:06:09,920
one scientist at the University of North Carolina, who said that he's probably spent 70% of his time

77
00:06:09,920 --> 00:06:14,960
on bureaucracy, because he said he does animal experiments. And he said that quite frankly,

78
00:06:14,960 --> 00:06:20,080
his administrative help in the department wasn't very good. And so he had to do all the ethics

79
00:06:20,080 --> 00:06:25,360
paperwork himself. And so he felt like his direct quote from him was, I just don't feel like there's

80
00:06:25,360 --> 00:06:29,200
time to do science anymore. And so that seems quite paradoxical. What are we paying people to do

81
00:06:29,200 --> 00:06:33,920
just to fill out reports about the money that we handed them? It just makes no sense. And it's

82
00:06:33,920 --> 00:06:38,400
depressing. No one goes into science thinking they're going to spend 70% of their time filling out

83
00:06:38,400 --> 00:06:43,680
reports and filling out paperwork and et cetera, right? They go into science because they love

84
00:06:43,680 --> 00:06:48,160
a particular field and they want to learn more and they want to make discoveries and so forth.

85
00:06:48,160 --> 00:06:53,120
And it just drains all the excitement out of science. But here's the problem. Every bureaucratic

86
00:06:53,200 --> 00:06:58,480
requirement has some justification for it. There are ethics requirements as to animal

87
00:06:58,480 --> 00:07:01,920
experiments. And those are there for good reasons, because animals can be abused and

88
00:07:01,920 --> 00:07:07,600
can suffer horrifically in experimentation. We've developed a whole set of procedures to protect

89
00:07:07,600 --> 00:07:14,160
animal safety and to protect against unnecessary deaths of animals and so forth.

90
00:07:14,160 --> 00:07:19,920
The same goes for experiments involving human beings. And that's again, thanks to a kind of

91
00:07:19,920 --> 00:07:26,640
horrific history of experimentation done in the 20th century on unsuspecting human subjects that

92
00:07:26,640 --> 00:07:30,880
were mistreated. And so there's a whole set of ethical requirements there so that nobody wants

93
00:07:30,880 --> 00:07:35,920
to get rid of that. It is federal money that's being spent. And so there's going to be some oversight

94
00:07:35,920 --> 00:07:40,560
like the budget and how the money is spent and so forth. Right now, there's a lot of focus on

95
00:07:40,560 --> 00:07:47,920
international security and focus on our researchers unwittingly passing the top technological secrets

96
00:07:47,920 --> 00:07:52,320
to researchers in China. And there's probably that new focus on China and maybe some discrimination

97
00:07:52,320 --> 00:07:57,680
involved. But yeah, it's still a fair consideration. But how much should we fund research that might

98
00:07:57,680 --> 00:08:02,800
unwittingly be used to support a foreign adversary, let's say. So anyway, like any bureaucratic

99
00:08:02,800 --> 00:08:06,480
requirement you point to, someone somewhere is going to say, there's a good reason for that,

100
00:08:06,480 --> 00:08:10,720
right? We need to keep that one. So it's really hard. And the whole is like, that's

101
00:08:10,720 --> 00:08:15,360
by a thousand cuts. But if you point to any one specific bureaucratic requirement, again,

102
00:08:16,000 --> 00:08:18,960
there was a good reason for it. There was some scandal. There was some

103
00:08:18,960 --> 00:08:22,480
problem that someone was trying to solve with this rule, with this procedure.

104
00:08:23,200 --> 00:08:28,320
And so that's why there have been many efforts to get rid of or to try to limit bureaucracy,

105
00:08:28,320 --> 00:08:32,480
but they haven't really gone anywhere. Because what you really need is to have some person,

106
00:08:33,040 --> 00:08:38,880
like with almost, I hate to use this word, but almost dictatorial authority over an agency like

107
00:08:38,960 --> 00:08:45,280
NIH or over an agency like NSF, to just go through the entire bureaucracy and take a red

108
00:08:45,280 --> 00:08:49,840
pen and slash through the stuff that isn't necessary or that isn't the top priority.

109
00:08:49,840 --> 00:08:54,560
And with the objective of, let's say, reducing the burden on scientists time to 20%, let's say,

110
00:08:54,560 --> 00:09:00,000
rather than 44%. And you'd have to have someone who is willing to make some really difficult

111
00:09:00,000 --> 00:09:05,120
tradeoffs and difficult choices and prioritize. We have a thousand things that everyone wants

112
00:09:05,120 --> 00:09:08,720
researchers to do. They'd take up too much of their time. So you're going to have to slash

113
00:09:08,720 --> 00:09:12,400
through some of them that even though they individually, they might sound like a good idea

114
00:09:12,400 --> 00:09:18,880
because you just have to prioritize. So you need someone or some committee that has the power and

115
00:09:18,880 --> 00:09:26,160
the will to actually get rid of some rules and regulations that maybe seem like a good idea.

116
00:09:26,160 --> 00:09:29,520
And that's politically difficult. But I think it's still worth trying because otherwise we're

117
00:09:29,520 --> 00:09:34,880
in a trajectory where ultimately we'll just be paying for 50%, 60% of science at this time.

118
00:09:36,160 --> 00:09:40,240
And it's insanity. Scientists need to have more time to focus on their science.

119
00:09:40,800 --> 00:09:44,320
So that's one of the issues that I've written. But I could dive into many more.

120
00:09:45,040 --> 00:09:48,960
Just to attach on this for a little bit longer, who would be an org that could do this?

121
00:09:48,960 --> 00:09:52,320
Or do you think it would be an individual at each scientific organization? Or could that be

122
00:09:52,320 --> 00:09:56,960
something for the GOA? Or would that be something like scientists writing an open letter about

123
00:09:56,960 --> 00:10:01,280
specific things that they get particularly up about in their research? In terms of thinking about

124
00:10:01,280 --> 00:10:07,680
solutions, if it's not as easy as a science gone coming in and doing it, are there any

125
00:10:07,680 --> 00:10:14,000
pathways that you think are worthwhile exploring? Another challenge is that these rules and regulations

126
00:10:14,000 --> 00:10:19,200
arise from different places. They arise from to dig into the weeds of the American government,

127
00:10:19,200 --> 00:10:26,000
like the Office of Management and Budget, or OMB, that has federal-wide authority to regulate how

128
00:10:26,000 --> 00:10:30,160
federal monies are spent and accounted for. And so they have a lot of budgetary requirements

129
00:10:30,160 --> 00:10:34,400
amongst others. So that's one source. But that's under the control of the White House.

130
00:10:34,400 --> 00:10:38,320
And so the White House could do something about that. Some of the requirements come from agencies

131
00:10:38,320 --> 00:10:44,240
like NIH or NSF themselves. Some requirements come directly from Congress. So Congress mandates

132
00:10:44,240 --> 00:10:49,520
particular practices and says that agencies need to look into X, Y, or Z. Some requirements

133
00:10:49,840 --> 00:10:55,680
honestly come from universities that want to behave conservatively, so to speak. They're

134
00:10:55,680 --> 00:11:00,640
risk averse and they want to make sure that they cross every T and dot every I. And some universities

135
00:11:00,640 --> 00:11:05,920
perhaps over-regulate their own researchers or over-stats their own departments that are in

136
00:11:05,920 --> 00:11:11,920
charge of monitoring and evaluating and submitting budgets and all of that. So yeah, it comes from

137
00:11:11,920 --> 00:11:16,640
many different places. So that's one challenge. I do think the White House could, in theory,

138
00:11:16,640 --> 00:11:21,920
issue an executive order that asked the Office of Management and Budget to review all of its

139
00:11:21,920 --> 00:11:27,520
practices and its rules with the guide towards slashing stuff that hasn't a time requirement

140
00:11:27,520 --> 00:11:32,640
on researchers or the impact on researchers directly. The White House could also review

141
00:11:32,640 --> 00:11:36,960
its past executive orders because some executive... So here's an example. Some of these requirements

142
00:11:36,960 --> 00:11:40,480
come from the White House itself, from prior executive orders. So there was an executive order

143
00:11:40,480 --> 00:11:45,680
in the 1990s signed by President Clinton that says that if you get federal money, you need to

144
00:11:45,680 --> 00:11:51,280
certify that you make people wear seatbelts. And again, it's well-motivated. There was nobody's

145
00:11:51,280 --> 00:11:56,480
really against seatbelts and it's a good idea to probably save some lives perhaps. But some 20,

146
00:11:57,120 --> 00:12:03,120
30, almost 30 years later, with their seatbelt laws in various states and probably anyone who

147
00:12:03,120 --> 00:12:07,200
wears it wants to wear a seatbelt or he does wear a seatbelt. And it's done clear that making

148
00:12:07,200 --> 00:12:13,040
people check that box and on federal applications really does need good. And so you could go back

149
00:12:13,040 --> 00:12:16,800
and look at it through prior executive orders like that and say, look, here's some executive orders

150
00:12:16,800 --> 00:12:22,560
that may have been a nice idea at the time, but we don't need to make everybody at every university

151
00:12:22,560 --> 00:12:26,240
certify that they do everything with a good idea in the world. We can prioritize and say, look,

152
00:12:26,240 --> 00:12:30,880
the time that passed when we needed to investigate whether people use seatbelts. And so there are

153
00:12:30,880 --> 00:12:35,280
probably any number of requirements like that that again, individually, sure, that's fine to make

154
00:12:35,280 --> 00:12:40,400
people wear seatbelts and it's not that much time to check a box. But just in terms of priorities,

155
00:12:40,400 --> 00:12:43,600
we should be able to streamline that. So the White House could, as I say,

156
00:12:43,600 --> 00:12:48,080
go back and look at the requirements that it has come up with over decades and see where it does

157
00:12:48,080 --> 00:12:52,000
streamline. So yeah, I think there are some opportunities that the White House could take

158
00:12:52,000 --> 00:12:56,400
advantage of if they want to. Okay, if anyone relevant is hearing, that feels a little bit

159
00:12:56,400 --> 00:13:00,880
about the action. I also guess like the problem here is almost just getting worse over time just

160
00:13:00,880 --> 00:13:05,040
because like people rarely ever take things out, but they just add to the pile. Like you never

161
00:13:05,040 --> 00:13:10,000
really know how large the pile in total becomes once you start adding stuff to it.

162
00:13:10,000 --> 00:13:13,040
But your individual thing that you want to add to it is really important right now

163
00:13:13,040 --> 00:13:17,360
without considering the entirety of it. So it's definitely, hopefully it's not getting much worse

164
00:13:17,360 --> 00:13:22,880
on the long run. But yeah, okay, that's definitely really important one that I couldn't agree more.

165
00:13:22,880 --> 00:13:27,120
Like riding grants is already complicated enough. And if that is a big one, that's that that could

166
00:13:27,120 --> 00:13:31,040
be an easy one to perhaps cut down on. The other thing that you've also written about really

167
00:13:31,040 --> 00:13:35,840
interestingly is I think on like patents in general. And I think the sub seg post was actually

168
00:13:35,840 --> 00:13:39,680
titled like how we screwing over researchers or something like that. And it had a lead at least

169
00:13:39,680 --> 00:13:43,840
a bit like patent component in there. And that's I guess university targeted to some extent also.

170
00:13:43,840 --> 00:13:47,280
So perhaps you could share a little bit more about what you were addressing there and perhaps even

171
00:13:47,280 --> 00:13:52,640
tell the quick story of Catalina Carrico, if you feel like it. Sure. Yeah. Yeah. I admittedly

172
00:13:52,640 --> 00:13:57,360
chose a kind of provocative title for that article set to hopefully get people to read at least a

173
00:13:57,360 --> 00:14:03,360
little bit to rewind a little bit. There was this famous act in 1980, the Bidol Act that was passed

174
00:14:03,360 --> 00:14:09,440
in the United States. Prior to that, it's complicated. But basically, when NIH NSF, when the federal

175
00:14:09,440 --> 00:14:14,800
government funded research, it would often end up taking control the government itself would end

176
00:14:14,800 --> 00:14:19,440
up taking control of patents arising from that research. And there was this perception that

177
00:14:19,440 --> 00:14:24,320
the government is not the most efficient user of patents. It doesn't know what to do with them.

178
00:14:24,400 --> 00:14:28,720
They weren't being actually used very well or commercialized or turned into

179
00:14:28,720 --> 00:14:33,440
something that was useful for the market, useful for medical patients and so forth.

180
00:14:34,080 --> 00:14:37,680
There's this idea that instead of having the government take control of patents,

181
00:14:37,680 --> 00:14:41,440
let's shift that and have universities take control of patents because universities

182
00:14:41,440 --> 00:14:46,880
are technically the recipients of all the federal grants that come from NIH and NSF, etc.

183
00:14:46,880 --> 00:14:50,640
So the money doesn't go straight to it. We talk about a researcher getting grant, but it doesn't.

184
00:14:51,280 --> 00:14:55,040
The money goes straight to the researcher's pocket, right? It goes to the university. They're

185
00:14:55,040 --> 00:14:59,040
the ones who handle the money, and they pay the researcher, right? So universities are the...

186
00:14:59,040 --> 00:15:06,320
They have a lot of indirect costs. That's a whole separate issue. But yeah, the indirect costs at

187
00:15:06,320 --> 00:15:12,080
top universities are often between 60 and 70%. So what that... And just to clarify what that means,

188
00:15:12,080 --> 00:15:16,320
so if you get a... If you as a researcher get a grant and let's say it's $100 to be simple,

189
00:15:17,280 --> 00:15:21,920
like NIH would give $100 designated for you, the researcher, and they would add

190
00:15:21,920 --> 00:15:26,800
60% on top of that, $60, let's say, as indirect costs to the university. So the total grant would

191
00:15:26,800 --> 00:15:33,360
have to be 160. So the 60% is on top of rather than taken out of. So it's not like... So anyways,

192
00:15:33,360 --> 00:15:37,360
that's just how that works. And by the way, indirect costs are also supporting a lot of

193
00:15:37,360 --> 00:15:42,880
bureaucracy that universities administer. So the bureaucracy issue is tied in to indirect costs.

194
00:15:42,880 --> 00:15:48,560
So anyways, university started patenting. And the story since then has been like, this is a great

195
00:15:48,560 --> 00:15:53,680
success. So until 1980, you had all these patents that were either didn't exist or went unused.

196
00:15:53,680 --> 00:15:58,000
And then afterwards, you have this huge flourishing university-based patents.

197
00:15:58,560 --> 00:16:02,640
And so around 20 years ago, there were a bunch of European countries that also started moving

198
00:16:02,640 --> 00:16:08,000
in that direction and trying to give universities more control over patents. But here's the thing,

199
00:16:08,160 --> 00:16:14,080
in some of those universities, in some of those European countries, the prior rule had not been

200
00:16:14,080 --> 00:16:17,520
that the government controlled the patents. The prior rule had been that the professor or the

201
00:16:17,520 --> 00:16:21,520
researcher controlled the patent. And in fact, they call it professor's privilege in some of

202
00:16:21,520 --> 00:16:26,960
those countries. And so they were switching from in a different direction, right? They were switching

203
00:16:26,960 --> 00:16:32,880
to the US regime that was perceived as successful. But they were switching from completely different

204
00:16:32,960 --> 00:16:38,080
place where the professor or the researcher had more control. And so there's been some empirical

205
00:16:38,080 --> 00:16:43,440
research on that that has shown that when European countries moved in that direction,

206
00:16:43,440 --> 00:16:48,720
the rate of patenting actually went down, which actually doesn't seem too surprising because

207
00:16:48,720 --> 00:16:55,280
universities often are very diffused. They encompass many departments. They may not have

208
00:16:55,280 --> 00:17:00,160
anyone who's a specialist in what one professor is doing and the commercialization of that

209
00:17:00,160 --> 00:17:06,480
research. And so maybe they put less priority overall on trying to commercialize anyone given

210
00:17:06,480 --> 00:17:11,600
discovery or possible patent than the researcher themselves who has more skin in the game, so

211
00:17:11,600 --> 00:17:15,760
to speak. So that's where the empirical evidence seems to lie so far is that it would be better

212
00:17:15,760 --> 00:17:19,600
to give better for innovation, better for patenting, better for commercialization,

213
00:17:19,600 --> 00:17:23,920
to get professors more of a say and perhaps to control over the patents rather than the

214
00:17:23,920 --> 00:17:27,920
universities themselves. Now, this came to a head with Katalin Kiriko, which is the story that I

215
00:17:27,920 --> 00:17:33,200
talked about quite a bit. So she was a Hungarian researcher who came over to the U.S. and worked

216
00:17:33,200 --> 00:17:39,840
at the University of Pennsylvania and got demoted repeatedly at Pennsylvania because she couldn't

217
00:17:39,840 --> 00:17:46,320
get NIH grants to support the work that she was doing on early mRNA research, which at the time

218
00:17:46,320 --> 00:17:50,720
it was not very popular. They weren't further of it now because it turned out to be the basis

219
00:17:50,720 --> 00:17:55,760
of some COVID vaccines. But in the 90s, it wasn't very popular at all. And it was seen as the dead

220
00:17:55,760 --> 00:17:59,840
end for whatever or something that's extremely difficult. It would never work. So she couldn't

221
00:17:59,840 --> 00:18:04,800
get grants to support the work. And the NAS, by the way, opens up a whole other huge topic,

222
00:18:04,800 --> 00:18:10,720
which is the role of NIH money and so-called soft money in universities, soft money, meaning

223
00:18:10,720 --> 00:18:14,560
researchers like Kiriko who were expected to pay for their own salary. It's like they're

224
00:18:15,680 --> 00:18:20,000
they're not given a salary directly by the university or not 100% of their salary. They're

225
00:18:20,000 --> 00:18:24,640
expected to raise their own salary for themselves to their grants. And so that makes them very

226
00:18:24,640 --> 00:18:29,680
dependent on appealing to whatever NIH wants to fund at any given point in time.

227
00:18:29,680 --> 00:18:34,080
So Karen Kiriko was repeatedly demoted and eventually basically pushed out of academia

228
00:18:34,080 --> 00:18:40,880
even after what became a paper that later won her and her co-author the Nobel Prize.

229
00:18:40,880 --> 00:18:44,880
Now, of course, no one knew that at the time. Like Ben, the University of Pennsylvania couldn't

230
00:18:44,880 --> 00:18:49,520
foresee the future. But the University of Pennsylvania did, as I found from reading some

231
00:18:49,520 --> 00:18:54,640
student newspapers from Pennsylvania, they kept the patents on for work, even after pushing her

232
00:18:54,640 --> 00:19:00,880
out of academia. And the student newspaper produced a chart like the universities across

233
00:19:00,880 --> 00:19:04,960
the country and how much money they're making royalties from patents. And the University of

234
00:19:04,960 --> 00:19:10,960
Pennsylvania was far and away, making many times more money than Stanford or other universities

235
00:19:10,960 --> 00:19:16,160
that you might think of as hotbeds of discovery and technological advancement.

236
00:19:16,160 --> 00:19:21,440
And so Pennsylvania made something like over a billion dollars in one recent year from the

237
00:19:21,440 --> 00:19:25,920
Fed with vaccines and from the patents on apparently from the patents on Kiriko's research,

238
00:19:25,920 --> 00:19:29,840
even though she's long gone from Penn and they not only didn't help with her research,

239
00:19:29,840 --> 00:19:33,120
they drove her out of academia. So things like this kind of glaring

240
00:19:33,760 --> 00:19:39,200
unfairness and on top of the inefficiency process. So yeah, I think that's an area that yeah, I

241
00:19:39,200 --> 00:19:43,760
think definitely deserves some reform or at least some really detailed, I think Congress

242
00:19:43,760 --> 00:19:48,720
definitely should fund or require some really detailed investigations of what is even happening

243
00:19:48,720 --> 00:19:52,800
at these university offices that are supposed to be patenting, researching, commercializing it.

244
00:19:52,800 --> 00:19:58,640
How many patents go unrealized or uncommercialized? How many take too long? There are lots of

245
00:19:58,640 --> 00:20:03,120
anecdotal complaints at certain universities that the process takes too long and the university

246
00:20:03,120 --> 00:20:09,680
demands too much of a cut. I think it'd be better to have a more systematic kind of investigation

247
00:20:09,680 --> 00:20:14,400
to add to the anecdotal stories. But in any event, I think I do think that the story from

248
00:20:14,400 --> 00:20:18,720
Europe or the empirical evidence from Europe shows that it might be better to move back in the

249
00:20:18,720 --> 00:20:23,360
direction of giving the professor or the researcher more of a say in what happens to their own

250
00:20:23,360 --> 00:20:26,720
discoveries. Yeah, it's crazy when you think about it, it's almost like all the incentives

251
00:20:26,720 --> 00:20:32,640
that are wrong. It's a few, right? But like, why even, how to come up with that type of system

252
00:20:32,640 --> 00:20:37,920
in the first place, I think. But yeah, I think I love that there is this almost not really an

253
00:20:37,920 --> 00:20:42,640
A-B testing, but at least some precedent of how it used to possibly work better and that that was

254
00:20:42,640 --> 00:20:47,760
useful to go back into that. You already mentioned one of the bits on funding and with the kind of

255
00:20:47,760 --> 00:20:53,120
like soft support from the NIH. And so I think another really interesting, super detailed analysis

256
00:20:53,120 --> 00:20:58,560
that you've done is specifically on NIH reform. You list a full laundry list there of things

257
00:20:58,560 --> 00:21:02,880
that could be improved with the NIH. And I don't think we get through all of them. I think accounting

258
00:21:03,840 --> 00:21:08,640
is almost 10. And we already touched on them individually. But when you think about the

259
00:21:08,640 --> 00:21:13,920
the NIH, what are like a few kind of crucial areas that you yourself are perhaps really excited about

260
00:21:13,920 --> 00:21:18,480
and promoting that there could be possibly a good reform applied to the NIH?

261
00:21:19,680 --> 00:21:24,480
Sure. I think there are any number of ideas, as you say. I think one thing the NIH should

262
00:21:24,480 --> 00:21:30,160
consider using more is an approach called basically fund the person, not the project.

263
00:21:30,800 --> 00:21:35,840
Now, it's interesting. There is a program at one of the NIH institutes called the National

264
00:21:35,840 --> 00:21:40,560
Institute for General Medical Sciences, NIGMS. And if you're wondering what that is,

265
00:21:40,560 --> 00:21:44,400
because many folks might, I did when I first started it. It's basically the NIH Institute

266
00:21:44,400 --> 00:21:50,880
that funds basic research as opposed to National Cancer Institute, focuses on cancer, the National

267
00:21:50,880 --> 00:21:56,400
Heart, Lung and Blood Institute that focuses on the cardiovascular disease, etc. NIGMS is focused

268
00:21:56,400 --> 00:22:03,280
on truly basic science that's not necessarily connected to any one specific disease like some

269
00:22:03,280 --> 00:22:09,680
of the other NIH institutes are. So NIGMS has this program called Maximizing Investigators

270
00:22:09,680 --> 00:22:17,280
Research Awards, M-I-R-A, and they pronounce it Mira. And this program is really intended to give

271
00:22:17,280 --> 00:22:21,920
researchers more flexibility and freedom to give them funding for several years where they don't

272
00:22:21,920 --> 00:22:27,360
have to spend as much time trying to pre-specify everything that they're going to do and then

273
00:22:27,360 --> 00:22:31,360
report back on what they said they were going to do three years ago or four years ago,

274
00:22:31,360 --> 00:22:37,040
instead it's intended to give them more freedom to follow the science and to follow their nose,

275
00:22:37,040 --> 00:22:43,280
so to speak, as to what the best ideas are at any given point. And I think that there's some

276
00:22:43,280 --> 00:22:49,840
evidence that the papers produced by that are performing equally as well or better in terms

277
00:22:49,840 --> 00:22:55,040
of citations. That's only one very limited metric, I think longer term you would want to see a rate of

278
00:22:55,680 --> 00:22:59,920
breakthrough discoveries. And that's hard to see because you can't expect a breakthrough every

279
00:22:59,920 --> 00:23:06,000
day from everyone. That's impossible. And as the director of NIGMS, John Lorsch has said,

280
00:23:06,000 --> 00:23:09,600
if I knew where the next breakthrough was going to come from, I would have already made that

281
00:23:09,600 --> 00:23:15,040
discovery myself. So part of his idea of funding is that you want to spread the money widely

282
00:23:15,040 --> 00:23:19,280
amongst talented scientists and give them the freedom and who knows where the next breakthrough

283
00:23:19,360 --> 00:23:24,000
will come from. Especially with basic science, often there's any number of stories from basic

284
00:23:24,000 --> 00:23:28,560
science where a discovery will be made in one decade and then three decades later it turns out

285
00:23:28,560 --> 00:23:34,240
that it's amazingly useful or influential. But yeah, I think that's a good example of a program

286
00:23:34,240 --> 00:23:38,720
that's experimental at NIH in the sense that it's a new thing. It's still fairly new. It's been

287
00:23:38,720 --> 00:23:44,800
around for a few years. I think that program could be expanded to other institutes at NIH like

288
00:23:44,800 --> 00:23:48,960
National Cancer Institute. And there are other institutes that have a version of this, but it's

289
00:23:48,960 --> 00:23:55,840
much smaller. NIGMS funds this type of grant four times as much when I last reviewed the numbers.

290
00:23:55,840 --> 00:24:00,720
Four times as much as the rest of NIH put together. Huge imbalance. NIGMS is very much focused on

291
00:24:00,720 --> 00:24:06,320
this for basic science, but I think you could try that approach elsewhere at NIH. And again,

292
00:24:06,320 --> 00:24:11,040
with the idea of giving scientists more flexibility and freedom. And here's another key idea that

293
00:24:11,040 --> 00:24:16,080
was in the document I sent you, which is NIH should take a more deliberate approach to

294
00:24:16,080 --> 00:24:21,040
experimenting and learning from what it does. Take a very meta science approach instead of just

295
00:24:21,040 --> 00:24:24,800
starting up new programs and saying, all right, everybody's going to do this. Then you don't

296
00:24:24,800 --> 00:24:30,160
have as much of a chance to learn and to iterate and to introduce deliberate experimentation.

297
00:24:30,160 --> 00:24:35,680
NIH is big enough that you could do like the literal randomized experiments and how you hand

298
00:24:35,680 --> 00:24:42,560
out money. They'd fund something like 90,000 grants at any given point in time. There's 90,000

299
00:24:42,560 --> 00:24:48,000
grants and each grant often supports multiple researchers. So I'm not sure the exact total,

300
00:24:48,640 --> 00:24:53,680
but it's hundreds of thousands of researchers that are supported by NIH. That's plenty of

301
00:24:53,680 --> 00:24:58,160
opportunity. So you could do a randomized trial that involved a thousand researchers,

302
00:24:58,160 --> 00:25:04,000
and that would be a drop in the bucket from what NIH supports. And you could randomize 500 to be

303
00:25:04,080 --> 00:25:10,400
funded in one way, 500 to be funded in a different way, and they just fold over time and see what

304
00:25:10,400 --> 00:25:15,840
happens. See what are the results from funding that offers more flexibility and freedom versus the

305
00:25:15,840 --> 00:25:20,240
more usual way that NIH does things. And I think that kind of deliberate experimentation

306
00:25:20,240 --> 00:25:25,280
is something that NIH should do at all. They've really ripped on that.

307
00:25:25,280 --> 00:25:29,440
Yeah, I think on your last point, it's really interesting just to hop back on the kind of

308
00:25:29,440 --> 00:25:33,920
funding people, not projects that is somewhat also pretty present right now, I guess in the

309
00:25:33,920 --> 00:25:37,520
private space where I think, for example, the Lieberman Brothers, they're now launching a new

310
00:25:37,520 --> 00:25:41,840
effort to actually fund individuals early on before basically making bets early on and almost

311
00:25:41,840 --> 00:25:46,080
invest into individuals like you invest in companies. And I think that kind of at least that

312
00:25:46,080 --> 00:25:52,400
meme or that new approach should also hopefully extrapolate outward through different and perhaps

313
00:25:52,400 --> 00:25:55,680
even scientific funding organizations. I think that would be wonderful. I think.

314
00:25:56,240 --> 00:25:59,840
I'd like the parallel to venture capital because I mean, I think there are a lot of,

315
00:25:59,840 --> 00:26:05,360
any number of, again, statements or examples of venture capitalists who say, yeah, you're making

316
00:26:05,360 --> 00:26:11,200
a bet on the person. There are many examples where a founder of a company ends up pivoting and doing

317
00:26:11,200 --> 00:26:16,400
something slightly different or a lot different. And venture capitalists often are happy with that

318
00:26:16,400 --> 00:26:19,920
because you're trying something and you realize it didn't work and now you found something that

319
00:26:19,920 --> 00:26:26,480
did work. And if you would be unthinkable to go to, I don't know, to go to Mark Zuckerberg and say,

320
00:26:26,480 --> 00:26:30,240
wait a minute, like your original proposal said that you were going to do X, Y, and Z and said

321
00:26:30,240 --> 00:26:35,920
that you were going to spend $5,000 in year three on this line item and where it shows that you

322
00:26:35,920 --> 00:26:40,560
spent the money that way. You would never want that level of micro management of a talented

323
00:26:40,560 --> 00:26:45,200
entrepreneur. Okay, students want to give them a little more freedom and flexibility to adapt to

324
00:26:45,200 --> 00:26:50,320
the market in which we should treat a lot more scientists the same way that you should give

325
00:26:50,320 --> 00:26:56,800
them the same respect and autonomy that entrepreneurs routinely have and give them the freedom of

326
00:26:56,800 --> 00:27:02,400
flexibility to say, wait a minute, I said I was going to do X, Y, and Z, but I tried it and it

327
00:27:02,400 --> 00:27:07,440
didn't work and I came up with a better idea the next year and now I'm going to want to do the better

328
00:27:07,440 --> 00:27:12,080
idea. Of course, we should all follow the better idea when that comes up. Yeah, I guess. Otherwise,

329
00:27:12,160 --> 00:27:15,280
you just assume that people aren't learning as they're actually in the field and then

330
00:27:15,280 --> 00:27:19,200
fermenting and dropping things. It's a crazy assumption to make in the first place, I think.

331
00:27:20,560 --> 00:27:25,520
The process should be about learning and changing and adapting to new ideas of the MLA. That's

332
00:27:25,520 --> 00:27:29,200
a bold point. If you already knew what you were going to do five years from now,

333
00:27:29,680 --> 00:27:35,280
I think it's, I'm paraphrasing, but well, one scientist that I know at Pennsylvania as well,

334
00:27:35,280 --> 00:27:39,040
he said, if I'm doing what I said I was going to do five years ago, I should be fired because I

335
00:27:39,040 --> 00:27:44,000
should have warranted and adapted in that book. Yeah, I love it. We only got to a small part of

336
00:27:44,000 --> 00:27:47,600
the laundry list that you have for the NIH, including other misopryptonies and stuff, but I

337
00:27:47,600 --> 00:27:51,760
think I want to close at least my part of the podcast with a question perhaps a little bit on

338
00:27:51,760 --> 00:27:57,040
a hopeful note to already lean into the extension whole part of the podcast after, but we have

339
00:27:57,040 --> 00:28:01,520
briefly touched on when we met and you've also pointed out again afterwards that there's actually

340
00:28:01,520 --> 00:28:08,160
now a really cool new landscape emerging of these alternative possible homes for researchers and

341
00:28:08,160 --> 00:28:13,360
scientists. I think there's FROs, there's Future House, Park Institute, Astera, Spectac, there's

342
00:28:13,360 --> 00:28:18,160
a few really interesting new orgs that have been popping up and we certainly had a few founders

343
00:28:18,160 --> 00:28:22,240
and executive directors of these orgs already on here for podcasts and for individual topics,

344
00:28:22,240 --> 00:28:26,240
but could you share a little bit of how you see that landscape changing and what these new organizations

345
00:28:26,240 --> 00:28:30,160
are setting out to do and perhaps what we can hope for in the minutes? We don't have to cover all

346
00:28:30,160 --> 00:28:35,040
of them, but just the general spirit, a great idea. And you could add to that some, there's

347
00:28:35,040 --> 00:28:40,640
some government agency, surprisingly enough. There's a health version of DARPA, so it's called

348
00:28:40,640 --> 00:28:45,280
ARPA-H, which I'm sure any listeners are familiar with, but yeah, it's intended to be like a

349
00:28:46,160 --> 00:28:51,120
government agency that has the innovative approach that DARPA and it's taken for decades.

350
00:28:51,120 --> 00:28:55,360
And there's a version of that in the UK as well called ARIA, also trying to be like the

351
00:28:56,320 --> 00:28:59,920
imitator of DARPA in a way. I think that's all tremendously hopeful. I do think it's

352
00:28:59,920 --> 00:29:04,640
hardly borne out of discontent with the current system, but as I say, it tends to be

353
00:29:05,520 --> 00:29:10,720
very bureaucratic and very top-heavy and a system in which it's hard to get a

354
00:29:10,720 --> 00:29:16,480
foothold, the average age for NIH, the first major NIH grant is so we're making people like

355
00:29:16,480 --> 00:29:22,960
slave away in other people's labs for 15 years before they finally get a foothold as a researcher.

356
00:29:22,960 --> 00:29:26,880
And so they're long past the age, which a lot of people in previous generations did some of

357
00:29:26,880 --> 00:29:30,720
their best scientific work. Einstein made some of his greatest discoveries at age 25, and so

358
00:29:30,720 --> 00:29:34,480
I think Newton, James Watson, there's any number of examples of that kind of thing.

359
00:29:35,600 --> 00:29:40,000
So there's a lot of discontent with that system. And so there are people within government and

360
00:29:40,000 --> 00:29:44,960
private philanthropists who say this is an opportunity to diversify the landscape,

361
00:29:44,960 --> 00:29:50,080
to come up with new ways, hopefully better ways of doing science and of funding science and of

362
00:29:50,080 --> 00:29:56,640
organizing science, all very important meta science issues. And so one thing that I really

363
00:29:56,640 --> 00:30:02,480
hope we're able to do, we being just collective meta science community and policymakers over the

364
00:30:02,480 --> 00:30:09,120
next five to 10 years, is just really deliberately learn from what all these new organizations

365
00:30:09,120 --> 00:30:15,680
have produced and do it in a systematic way. I do think possibly one risk, like any new

366
00:30:15,680 --> 00:30:21,120
organizations, like just the same as universities, their temptation is going to be to send out

367
00:30:21,120 --> 00:30:26,240
press releases about all the amazing things they've done and brag about that. And that's fine,

368
00:30:26,240 --> 00:30:30,880
that's to be expected, and often that's even true. But there will be cases in which they fail,

369
00:30:30,880 --> 00:30:35,360
and I think it's important to learn from failure, and it's important to be public about that.

370
00:30:35,360 --> 00:30:40,880
And so I think hopefully over time, we can have a meta science discussion that's able to have

371
00:30:40,880 --> 00:30:46,800
just a truly honest appraisal of what's working, what has failed, and failure isn't a bad thing.

372
00:30:46,800 --> 00:30:50,160
We should learn from not everything that's going to work for the first try. Let's learn

373
00:30:50,160 --> 00:30:55,760
about how to design organizations more efficiently or with more perfectly going forwards.

374
00:30:55,840 --> 00:30:59,680
And so I think that's where I hope the conversation goes for the next five to 10 years.

375
00:30:59,680 --> 00:31:05,440
Yeah, thank you for that. So yeah, so this part is more like the philosophical part. We've been

376
00:31:05,440 --> 00:31:10,640
talking about the meta science and everything, but it's diving into the sort of existential hope

377
00:31:10,640 --> 00:31:15,200
aspects of where science and the progress of it can take us. You touched on it now,

378
00:31:15,200 --> 00:31:18,800
but I would be really curious to hear, do you feel like things are changing? Are you gaining

379
00:31:18,800 --> 00:31:24,400
traction with this? Yeah, I do think things are changing. Again, we just touched on that with all

380
00:31:24,400 --> 00:31:31,200
the proliferation of new organizations, both inside government and outside government. I think

381
00:31:31,200 --> 00:31:40,000
that's a really helpful sign. And again, my hope is that over time, there should really be a diversity

382
00:31:40,000 --> 00:31:45,520
of approaches and organizations that probably something that will be bad for science is to

383
00:31:45,520 --> 00:31:53,360
say everybody has to fit in the exact same box. That probably isn't good, even if the box makes

384
00:31:53,360 --> 00:31:58,960
sense, so to speak. So we talked about fund the person, not the project. That seems like a good

385
00:31:58,960 --> 00:32:04,960
idea. And we should use it sometimes. But I think if literally 100% of science funding was fund the

386
00:32:04,960 --> 00:32:08,880
person, not the project, there probably be some failure points there as well. There probably be

387
00:32:08,880 --> 00:32:14,080
some things that got missed. There are probably some examples where you should fund an organization,

388
00:32:14,080 --> 00:32:19,120
like fund a team, not the person. So there are lots of different approaches one could use.

389
00:32:19,200 --> 00:32:23,920
And in fact, in some areas of science, the Large Shader and Collider or big astrophysics

390
00:32:23,920 --> 00:32:28,640
efforts, you're not funding one person, you're funding like a team of 1000 people. You know what

391
00:32:28,640 --> 00:32:33,200
I mean? Yeah, if you want to approach this to science and to science funding, I think would

392
00:32:33,200 --> 00:32:39,760
probably be suboptimal. But I do think that hopefully going forward, I think one thing that

393
00:32:39,760 --> 00:32:46,560
could help improve the pace of innovation is having a more deliberately diverse approach

394
00:32:46,560 --> 00:32:51,440
in like how we fund science and the source of people to get funded, and so forth. And

395
00:32:51,440 --> 00:32:55,680
when I say the source of people to get funded, that's also an interesting point to emphasize.

396
00:32:56,320 --> 00:33:00,720
There are lots and lots of examples from history where great scientific advances come from places

397
00:33:00,720 --> 00:33:05,040
that sometimes you wouldn't necessarily have expected or they come from people that were

398
00:33:05,600 --> 00:33:10,960
heretics at their time, Semmelweis and the germ theory of disease, like people despise them at

399
00:33:10,960 --> 00:33:15,680
the time. There are tons of examples like that. It doesn't mean we should, again, we don't want

400
00:33:15,680 --> 00:33:19,840
a science funding system that only funds people that are outcasts and heretics. That probably would

401
00:33:19,840 --> 00:33:24,080
be in the wrong direction too. But I do think there should be some space within science,

402
00:33:24,080 --> 00:33:29,440
like a National Institute for Oddball. We should have a deliberate approach to fund some things

403
00:33:29,440 --> 00:33:34,800
that are outside the box. And now some of them will be crazy and won't work. We might end up

404
00:33:34,800 --> 00:33:38,720
funding some of the greatest breakthroughs ever if we've made more space within the scientific

405
00:33:38,720 --> 00:33:43,040
funding system or people with truly outside the box ideas and approaches that don't get funded

406
00:33:43,280 --> 00:33:50,720
correctly. Yeah, I remember I was talking to an economist, I think, Johan Nurebe,

407
00:33:51,280 --> 00:33:57,360
about he was talking about how technology often progresses. It comes from quite dirty areas or

408
00:33:57,360 --> 00:34:02,880
not necessarily the most appreciated ones. I think his example was the technology of online

409
00:34:02,880 --> 00:34:08,000
payment solutions coming from porn, like people wanting to watch porn online and that was like

410
00:34:08,000 --> 00:34:13,360
what are online payment solutions? And now that's a very useful and established thing.

411
00:34:13,360 --> 00:34:17,920
And I'm sure there are more exciting scientific innovations that came from the not the most

412
00:34:17,920 --> 00:34:25,040
appreciated areas maybe. Yeah, but it sounds like you're positive and you're optimistic.

413
00:34:25,040 --> 00:34:27,360
Would you say that you're optimistic about the future?

414
00:34:28,480 --> 00:34:33,920
That is a big question. In general, yes, I think that there's lots of possibilities. There's

415
00:34:33,920 --> 00:34:38,240
dangers as well. We've both kind of, I think I've met you in an effective algorithm conference.

416
00:34:38,240 --> 00:34:45,360
There are certainly areas like nuclear proliferation or biosecurity or artificial

417
00:34:45,360 --> 00:34:52,480
general intelligence. There are some areas of potential R&D advancement that do possibly have

418
00:34:52,480 --> 00:34:57,600
some risk and some would say existential risk. But I think the theme of your podcast existential

419
00:34:57,920 --> 00:35:04,240
right is that hopefully with the broad sweep of innovation and improvement that we can address

420
00:35:04,240 --> 00:35:09,280
those existential risks and that we can hopefully still keep progressing and making life better

421
00:35:09,280 --> 00:35:14,400
for everyone. And so I think that's where I guess my hopes and efforts would lie is like trying to

422
00:35:14,400 --> 00:35:20,320
figure out what we're doing wrong, all the ways in which we're holding back scientists and science

423
00:35:20,320 --> 00:35:27,040
itself and figuring out ways to hopefully speed up and accelerate the pace of innovation. I think

424
00:35:27,040 --> 00:35:34,000
that does offer more hope for the future. Yeah. I'd be interested to hear in relation to the

425
00:35:34,000 --> 00:35:39,280
like increasing pace of progress on like in relation to maybe AI in specific,

426
00:35:39,280 --> 00:35:42,640
do you see the science landscape shifting due to that?

427
00:35:44,720 --> 00:35:53,600
Due to AI in general, I haven't looked at it that much in depth. There are some really amazing

428
00:35:53,600 --> 00:35:57,440
advances that have been made, for example, AlphaFold and protein folding. There are other

429
00:35:57,440 --> 00:36:03,520
similar tools that are like that that offer the potential to speed up at least some components

430
00:36:03,520 --> 00:36:10,640
of science. I'm less certain myself with what I've seen out of large language models so far. It

431
00:36:10,640 --> 00:36:15,840
seems like they have, they sometimes show science a great sophistication, but then sometimes they

432
00:36:16,480 --> 00:36:21,680
like just completely hallucinate, at least the ones to date. And so I'm a little nervous about

433
00:36:21,760 --> 00:36:25,840
that. You could end up with kind of pollution of the scientific literature with people using

434
00:36:25,840 --> 00:36:30,720
AI models to write papers thinking this will help them publish more and then it'll end up on

435
00:36:30,720 --> 00:36:36,960
live somewhere and might be largely fake or largely just made up or hallucinated. Another

436
00:36:36,960 --> 00:36:41,840
thing that I worry about, and this is born out of the work that I did while I was at the Arnold

437
00:36:41,840 --> 00:36:48,080
Foundation, a lot of the published literature just isn't that good. Some of it's outright fraudulent

438
00:36:48,880 --> 00:36:53,120
and there are more discoveries about academic broad that come out, it seems, every week.

439
00:36:53,120 --> 00:36:57,760
A lot of it isn't that reproducible. And even the stuff that is reproducible, it often isn't

440
00:36:57,760 --> 00:37:02,960
described that well in print. So one thing that I funded, for example, was called the

441
00:37:02,960 --> 00:37:08,320
Reproducibility Project in Cancer Biology. And their original idea was to replicate

442
00:37:08,320 --> 00:37:14,240
the experiments from 50 top-sided cancer biology papers. And ultimately, they could only complete

443
00:37:14,320 --> 00:37:20,080
fewer than half of their intended experiments. And the reason was that in every single case,

444
00:37:20,080 --> 00:37:24,320
you couldn't just go off the publish paper. You had to go back to the original lab and ask them

445
00:37:24,320 --> 00:37:28,560
to fill in all the gaps and fill in all the details about what they had actually done.

446
00:37:28,560 --> 00:37:33,040
And some of the original labs were not cooperative at all. In some cases, just were

447
00:37:33,040 --> 00:37:39,440
hostile. Some cases just said, we're not sure. And when they worked for operative,

448
00:37:39,440 --> 00:37:44,800
the answer was always that you need twice as much materials as you expected. And so it's

449
00:37:44,800 --> 00:37:49,040
going to cost more and take longer. And they're like, worry about AI models that might be

450
00:37:49,680 --> 00:37:56,960
trained on scientific literature as if what's written down on paper is the complete entry.

451
00:37:56,960 --> 00:38:02,240
And that's often not the case. And you just worry that might spiral into even more

452
00:38:02,240 --> 00:38:07,920
irreproducible work. So again, cautiously optimistic about some of the specialized tools

453
00:38:07,920 --> 00:38:14,720
that are out there that are based on like really rigorous systematic databases. But then

454
00:38:14,720 --> 00:38:18,560
there's a whole wealth or a whole broad millions and millions of scientific articles

455
00:38:19,200 --> 00:38:25,200
that I would say probably aren't worth trying to use in an AI model in the first place or at

456
00:38:25,200 --> 00:38:29,120
least with great skepticism and a lot less of corrections needed before you just

457
00:38:29,920 --> 00:38:33,760
train an AI model and expect to get useful information out of it. So it's all over the

458
00:38:33,760 --> 00:38:38,400
place. But yes, that's my answer for now. Yeah, that's very interesting. I hadn't really thought

459
00:38:38,400 --> 00:38:44,720
about that in relation to that it would get access to like maybe incorrect data technically.

460
00:38:44,720 --> 00:38:48,880
But yeah, I guess that I heard about the Center for Open Science and that's like

461
00:38:49,440 --> 00:38:54,640
where they try to pre-register the studies that they're doing so that you can actually check if

462
00:38:54,640 --> 00:39:00,800
they are correct in that they're reproducible or that that oftentimes you don't really publish

463
00:39:01,760 --> 00:39:05,520
what's maybe not like an exciting result or something like that. But here you can see what

464
00:39:05,520 --> 00:39:10,880
people have published or have been doing research on even if the results aren't like that exciting.

465
00:39:10,880 --> 00:39:16,960
Any other like projects like that that you think are promising or seem like they could

466
00:39:16,960 --> 00:39:21,920
actually make a difference in this field? Yeah, there's a lot of directions I could take this. So

467
00:39:22,960 --> 00:39:27,440
the idea of pre-registration really came out of medicine from some recommendations that were

468
00:39:27,440 --> 00:39:32,080
made. And I think it was early the late 1980s or definitely by the early 90s, there were some

469
00:39:32,080 --> 00:39:36,640
folks writing about it, the idea in medicine. These folks in medicine, they were trying to do

470
00:39:36,640 --> 00:39:41,760
meta research in medicine to try to summarize the whole body of literature. And as of the 90s,

471
00:39:42,800 --> 00:39:46,880
probably still true somewhat today. They were very frustrated. And I remember an article

472
00:39:46,880 --> 00:39:51,440
by a couple of folks at the time where they said that it's just really strange to them that you

473
00:39:51,440 --> 00:39:57,040
can find much more information on baseball statistics and up to the minute information

474
00:39:57,040 --> 00:40:01,120
about every baseball team, every baseball player, everything they've ever done. But yes,

475
00:40:01,120 --> 00:40:05,200
it's impossible to find that information about clinical trials, that much wealth of information

476
00:40:05,200 --> 00:40:10,960
about clinical trials in medicine, which is to them more important than baseball. So pre-registration

477
00:40:10,960 --> 00:40:16,080
was in part a way to get access to information about the trials that are being conducted

478
00:40:16,960 --> 00:40:23,280
on human beings and involving drugs or other types of treatments that might be used in medicine.

479
00:40:23,280 --> 00:40:28,640
That was one motivation for me to still be able to see the kind of see the whole denominator,

480
00:40:28,640 --> 00:40:32,560
so to speak. It's easy to see the published successes and the press releases of announcing

481
00:40:32,560 --> 00:40:38,400
that a drug will cure in the US, the commercial that say try this drug. But it's harder to find

482
00:40:38,400 --> 00:40:43,280
the failure since the idea of pre-registration in part was intended to address that. And then

483
00:40:43,280 --> 00:40:48,720
like subsequently, I guess you're able to like what there was one famous article by kind of

484
00:40:48,720 --> 00:40:54,960
Eric Turner and some colleagues where they reviewed something like 70 or 80 clinical trials

485
00:40:54,960 --> 00:41:00,320
that have been done on antidepressants. And I'm going off a memory here, so I might get a number

486
00:41:00,320 --> 00:41:04,880
slightly wrong, but roughly half of the trials showed that the antidepressants work and roughly

487
00:41:04,880 --> 00:41:09,920
half of the trials showed no results. The antidepressant wasn't any better than placebo.

488
00:41:09,920 --> 00:41:15,120
And they had access to all this because of both pre-registration and because they went to the FDA

489
00:41:15,120 --> 00:41:19,440
and these were FDA-free drugs and the FDA demands to see all the evidence and not just

490
00:41:19,440 --> 00:41:23,360
what got in the top journal. So you have the whole denominator there to look at.

491
00:41:23,360 --> 00:41:28,000
And so what they found was that basically all of the positive trials, I think except one,

492
00:41:28,000 --> 00:41:34,240
got published in a top journal. And the negative trials or the null trials mostly went unpublished,

493
00:41:34,240 --> 00:41:40,000
but I think a few of them were published and then with a kind of spin or misrepresentation

494
00:41:40,000 --> 00:41:44,480
as if they've been positive studies. And so yeah, if you look at the medical literature,

495
00:41:44,480 --> 00:41:49,040
and this is also back to what I was saying about AI, if you looked at just the medical literature

496
00:41:49,040 --> 00:41:52,880
on those antidepressants, you would say, wow, that'll work. Everything's amazing. But if you

497
00:41:52,880 --> 00:41:56,480
look at the whole body of evidence that the FDA could look at, you would say sometimes they work,

498
00:41:56,480 --> 00:42:00,320
sometimes they don't own or sometimes the one medicine is better than others. It's a lot more

499
00:42:00,320 --> 00:42:04,480
complicated and messy when you have all the evidence sitting before you. Yeah, there have been

500
00:42:04,480 --> 00:42:09,360
efforts like that in medicine. Pre-registration has been growing in other disciplines, psychology and

501
00:42:09,360 --> 00:42:15,120
economics in particular over the past 10 years or so. The American Economics Association, like

502
00:42:15,120 --> 00:42:20,560
the Center for International Science, started demanding pre-registration about 10 years ago,

503
00:42:21,360 --> 00:42:26,960
and the rate at which that happened on a yearly basis that's gone up within economics alone.

504
00:42:26,960 --> 00:42:31,680
Yeah, I think that's all kind of hopeful signs of progress in different fields, just being better

505
00:42:31,680 --> 00:42:37,120
practices about being public about what kinds of studies you're doing. And then ultimately,

506
00:42:37,680 --> 00:42:43,440
it's kind of all for not unless you publicize the failures and the nor results as well. Because

507
00:42:43,440 --> 00:42:47,600
again, if you only publish the positive results and don't say anything publicly about the negative

508
00:42:47,600 --> 00:42:52,000
results, it's very skewed. It'd be like if you flip a coin and then you only announce when you flip

509
00:42:52,000 --> 00:42:57,520
the heads. Yeah, you would look like you'd flip 100% heads, but that wouldn't be true. So be very

510
00:42:57,520 --> 00:43:06,240
misleading. Yeah, I'm going to make a bit of a turn with the next question now, which is it's a

511
00:43:06,240 --> 00:43:11,440
question that we always ask in this podcast, which is in relation to the like AU catastrophe. So

512
00:43:11,440 --> 00:43:16,640
it's a term that means basically the opposite of a catastrophe. So once it's happened, the value of

513
00:43:16,640 --> 00:43:23,200
the world would be much higher. And feel free to relate this question to like your area of expertise

514
00:43:23,200 --> 00:43:29,600
within science. And I think also think ambitiously when I ask you this question, which is if you

515
00:43:29,600 --> 00:43:36,160
could envision like a specific you could have for the progress of science, what would that be?

516
00:43:36,160 --> 00:43:41,680
What would be an existential hope scenario of science? If you like, if all the work that you're

517
00:43:41,680 --> 00:43:48,560
doing now came true, what would happen? That's great. Yeah, I first came across that word,

518
00:43:48,560 --> 00:43:53,360
you catastrophe and the writings of JR Volkian. Did he invent the word? I can't remember. Yeah,

519
00:43:53,360 --> 00:43:59,760
I believe he did. Yeah, it's a long time. Yeah, that's that is a super interesting question.

520
00:43:59,760 --> 00:44:05,120
The kind of revolutionary side of me says that in terms of meta science, what would be really cool

521
00:44:05,120 --> 00:44:11,920
is if you could duplicate all federal funding of science, but with an entirely new set of

522
00:44:11,920 --> 00:44:18,080
institutions. Imagine we spend 50 billion or so on NIH, what if we add in an extra 50 billion on

523
00:44:18,080 --> 00:44:22,560
biomedical research? Or with the condition that it has to be run completely differently

524
00:44:22,640 --> 00:44:27,200
from NIH, and it has to be in the hands of different people. And again, touching on these

525
00:44:27,200 --> 00:44:34,160
ideas of exploiting the and proliferating the diversity of the approaches and the scientists

526
00:44:34,160 --> 00:44:38,400
that get funded and the ways in which funding is handed out, because then you have two different

527
00:44:38,400 --> 00:44:43,440
systems operating. And so thinking very meta here, you have two different systems operating

528
00:44:43,440 --> 00:44:48,000
with equal amounts of funding. And now you can really see at a grand scale, hopefully,

529
00:44:48,000 --> 00:44:52,800
what happens? What are the results? It would be a chance to test out lots of different meta

530
00:44:52,800 --> 00:44:58,560
science ideas that people have discussed for years or for decades, but you just really need

531
00:44:58,560 --> 00:45:04,880
a kind of the grand landscape in which to experiment with that. So again, that's very meta,

532
00:45:04,880 --> 00:45:11,360
but my hope would be that we learn a lot about how to fund science and how to organize scientists

533
00:45:11,360 --> 00:45:16,720
into organizations, into labs and universities. We need entirely different institutions that

534
00:45:16,720 --> 00:45:21,040
are not don't look anything like universities, for example, maybe you should maybe you should

535
00:45:21,040 --> 00:45:24,960
set up a whole national institute where the rule is you can only fund scientists who are

536
00:45:24,960 --> 00:45:29,200
working out of their garage. I don't know, it just you need some kind of like wild ideas out

537
00:45:29,200 --> 00:45:34,240
there to try out different approaches, different scientists, different ideas. And my hope would

538
00:45:34,240 --> 00:45:39,360
be that at a minimum, we'd learn something from that. And but in terms of existential hope, I think

539
00:45:39,360 --> 00:45:45,120
we we might end up creating a number of great breakthroughs that wouldn't have happened otherwise.

540
00:45:45,120 --> 00:45:52,400
And today is hot, heavy, bureaucratic system that is so conformist and so focused on doing

541
00:45:52,400 --> 00:45:55,920
whatever gets you approval from the kind of the existing bureaucracy.

542
00:45:57,920 --> 00:46:07,520
Yeah, I really, really like that. It's if you like were to introduce someone who is entirely

543
00:46:07,520 --> 00:46:12,640
new to this field, is there anything you'd recommend that they like read or watch? Maybe it's

544
00:46:12,640 --> 00:46:16,640
just your own sub stack? Or is there anything else that you would recommend for an intro to the

545
00:46:16,640 --> 00:46:22,640
field? Sure, the writings that Good Science Project has produced are good points to look at.

546
00:46:22,640 --> 00:46:26,880
You mentioned the Center for Open Science, they've had a number of publications and projects that

547
00:46:26,880 --> 00:46:31,520
are related to meta science. I would say that there's more stuff coming out from the Institute

548
00:46:31,520 --> 00:46:35,600
for Progress and the Federation of American Scientists, they've had a series of papers on

549
00:46:35,600 --> 00:46:40,800
open science, for example. Yeah, that might be that gives you plenty of reading to start with.

550
00:46:41,360 --> 00:46:48,000
That sounds like a great place to start. And how can listeners best stay updated with your work

551
00:46:48,000 --> 00:46:52,400
and the work of the Good Science Project? Sure, you mentioned sub stack, it's just

552
00:46:52,400 --> 00:46:59,520
goodscience.substack.com. Also, the website is just goodscienceproject.org. And yeah,

553
00:47:00,560 --> 00:47:05,440
we'll have to hear from folks who have ideas or enjoy the writings and want to learn more.

554
00:47:05,600 --> 00:47:12,080
Great. Thank you so much, Stuart, for coming on this podcast. We really appreciate it and

555
00:47:12,080 --> 00:47:15,840
we'll definitely keep an eye on all the work that you're doing with the Good Science Project

556
00:47:15,840 --> 00:47:18,080
in the future. So thank you so much.

