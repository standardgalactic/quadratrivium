{"text": " Hi, Vern. Welcome to FOSAT's Intelligent Cooperation Group. We're really excited about this seminar. We have DC here from Welkheim. Thank you so much for joining us to give an overview of ZKML, which is a topic that has come up so much, especially, I guess, data coming up already a few years ago in this group, and then much, much more to have really ramping up in the last year. And so I'm really excited for you to share a little bit more about it because we have not had a dedicated seminar to this topic yet. So thanks a lot for joining. We're really excited about you guys' work. Without further ado, please take it away. I'll be in the chat monitoring questions, and we're off to the races. Awesome. Thank you for having me. So today, I'll be talking about the zero-knowledge machine learning. I'll be giving a very brief introduction. But first, maybe let me start by saying a little bit about myself. I'm a research engineer at the World Coin Foundation, and World Coin is this project that is trying to build the largest identity and financial network. And there is an interplay of various technologies in the products and things that we're building at World Coin, and some of which are AI, and some of which are cryptography. So we've had expertise in both realms from different teams internally. And there have been some essentially experimentation that naturally occurred. We have some AI parts of the stack, and there are some reasons why cryptography might be useful in the AI sector of the stack. And so this prompted us to think about this topic about two years ago in August. Almost two years ago, August of 2022. One of my teammates was just playing around with cryptography and trying to prove machine learning models. But so this is a little bit of a background. I mostly run our grants program, where we give grants to people to help decentralize World Coin and solve some more problems, and also help with a bunch of different R&D efforts as an individual contribute. So without further ado, let me start with the presentation. Usually, the way that I like to start with this presentation is that I like to decompose the statement into its constituents so that people have an understanding of which elements or what is zero-knowledge machine learning? What is the zero-knowledge part? What is the machine learning part? I like to decompose it into its fundamental compositions, so competition. And the first one that I want to talk about is zero-knowledge cryptography, or called zero-knowledge. I don't know if many of you understand the reference that or get the reference that I put here, which is Waldo, finding Waldo. Waldo essentially is a character or one specific analogy, which is very simple, very easy, very friendly to explain what zero-knowledge cryptography is to people who have never heard about it. Because there is this essentially like poster that many people know where there's just lots of characters which are in the city, and there's one Waldo. And it takes some amount of time to essentially find the Waldo, and that's like the challenge of these specific games. And so there is one specific analogy which allows people to explain what zero-knowledge is, which is that if I put a bigger like white paper on top of the poster, defining Waldo game, and if I create a little tiny cutout for the head of Waldo, and I place the cutout just on top of Waldo's head, but covering the entire game itself, or the poster itself, then I'm able to prove to anyone that I know where Waldo is without revealing his location within the poster. So this is like the good analogy for explaining what zero-knowledge cryptography is, because I'm able to prove things that I know to someone else, to an outside observer, without them learning everything about the things I'm making a proof of. I can selectively prove specific statements because I have information, but I don't have to reveal everything in order to be able to prove that statement. So this is like a good analogy to explain DK. So some of the properties that zero-knowledge cryptography has. So the first one, which I think is the most important one, especially in the context of blockchain, I saw that many of you were like in previous presentations of this specific group. I saw that there's a few crypto people that were talking about different things. I'm sure that came along a few times. So succinct list essentially just means that in order to verify a proof of a statement, it's a lot less computationally expensive or a lot less expensive than to actually prove the computation or to just perform the computation yourself, right? So essentially verifying that I know where Waldo is, as an outside observer, it's a lot easier than me finding Waldo myself. So this is really important in the context of blockchains because for example, for scalability solutions, instead of everyone having to re-execute the same transactions in a block, I can just verify a proof and I can just update my state without having to secure it myself, for example. So this property is important for the kid because it allows us to very easily, computationally easily verify things without having to do computation ourselves. This is a really important property. The second one, arguably the one that is most known for, is correctness. So correctness essentially means that I can have almost 100% certainty that this statement that I'm proving is correct, right? That I cannot lie. There is no way that I as a prover can lie to a verifier unless if the cryptography is sound in this case. There's two specific properties that constitute correctness. One is soundness. So soundness means that I, if I'm a prover, someone making this claim, someone making a statement, I'm not able to fool a verifier with invalid proof and completeness is another property where I'm not essentially able to create a valid proof unless I know the truth. If I don't know the truth, I cannot make a valid proof as a prover. And the third one, which is name after, zero knowledge, is this property where I can hide parts of the statement. For example, let's say that I have, I don't know, this is a good example. Let's say I have my passport. So I have my name, my nationality, my date of birth, where I'm from, which country I was born in, for example, the place of birth. So something that would be useful or like something that would constitute a zero knowledge proof is that I can make the statement that my age is over 18 years old without revealing to anyone my age, but anyone can just verify that I'm actually over 18. The way that this is actually implemented is that within a zero knowledge proof, I can verify a signature from some issuing body like the government. And then I can make a statement that like A, this age, which was attested to or essentially committed to by a government is over 18 and you don't learn my age. So this is the zero knowledge part where I'm able to hide parts of the state that I'm making a statement about or proof about according to some constraint or some statement, right? Like I can say greater than, less than, equal to a bunch of other properties that I think can put. So the second part of the statement of zero knowledge machine learning, this machine learning, right? I think that one is much more familiar to most of you since it's been generating such a buzz everywhere, like machine learning through a generative AI, like in things like ChagYPT or Dali or a lot of generative AI models or natural language processing, categorizing models like robots, the machine learning essentially, the way that I think about it is that it's a tool that allows us to give us not the non-deterministic solutions or just estimates for short for problems that don't really have a concrete solution, right? There's usually there's some problems which we can solve algorithmically and we can just have a set of steps that we can just execute in order to solve it and we will have a perfect solution every time. In the case of machine learning, however, most of the problems that are being solved by machine learning are not such problems. Therefore, we need to, because maybe like the space of solutions is too big or the space of the steps that we can take is too big, so it's really hard to navigate deterministically, then we just have this sort of juristic. A juristic essentially is a good enough approximation to the real solution which we can work with and which has some form of accuracy, right? So in the context of machine learning, we have some sort of juristic for some problem. So let's say I want to categorize whether an image that I see is the image of a dog or a cat. This I can train a machine learning model to essentially solve this task, but the machine learning model will never be 100% correct. It will have some accuracy, right? It will have some that will fail on, it will have, most of them it will get correct. But essentially the way that the machine learning model works is that it trains on some data. So I feed some data to some model or to some machine learning algorithm and it gets better. And this juristic keep getting better and better for things that it hasn't seen before. It generalizes over the data and it's able to make essentially like predictions or classifications or all sorts of things. So in the case, for example, of foundational models for large language models, they get better at creating like cohesive explanations or at reasoning or at mathematics or at all sorts of different things. And we can have these benchmarks and with more data, they get better at these benchmarks, which essentially provide better juristic problems that we're grading them on. So another concept that I want to explain here is that there's two specific parts within machine learning or two specific things you can do usually, is that when you have a model, you can train a model. The act of training a model is the act of creating a function which gets actually better and better at giving you the juristics, the more data you feed into it, right? So I'm able to update the parameters of this function by learning, this is what learning is, right? I'm updating parameters of a function in order to get a better jurist. And this process is really expensive, right? It's really hard, you have to co-locate a data center, it's running lots of graphics cards in a big place and consuming lots of electricity for months on end in order to be able to create something useful or meaningful, this is really expensive. But the end product is very easy to run. So once I've trained this function, once I have my set of parameters, evaluating this function at some input, is usually inexpensive or very inexpensive compared to actually training it, several orders of magnitude less. So these are like the, how I usually explain machine learning just very briefly. And so now I want to get, what is zero knowledge machine learning, right? We have some intuitions from the ZK side, some intuitions from the machine learning side. Now we can discuss what ZK machine learning can be or is within the modern understanding of it. So essentially what ZKML is, is the creation of zero knowledge proofs of machine learning algorithms, right? So zero knowledge cryptography allows you to create proofs of arbitrary computations. It can be a proof that I've computed some specific thing. It can be a proof that some variable is bigger than another. Essentially it's making proofs about computation and you know the person who's verifying that proof knows that someone has executed that computation on some inputs and has produced some output. So in the context of machine learning, usually you have some input, a function or a model, and then some output, right? So if it's let's say like charge EPT, I have a prompt which I feed into the model. The model just takes that prompt and evaluates its model and it gives you an output, which is the thing that you then read in the end, which is the result, right? So zero knowledge machine learning would be the art or act of creating a proof that I have fed an input to a model and I've produced some output. And I can verify that this output came from a model without essentially having to evaluate this myself personally. I just know that this comes from a model because I have a cryptographic proof that this indeed happened. So something that many people in the space actually use as a good analogy that accountable AI, right? Usually AI or machine learning models, you don't know that they're actually correct unless you run them yourself, right? If you run it yourself on your own local machine or your own data center which you have privileged access to, then you know that you've run the right thing. But let's say that you're using some form of server or API, right? If I go to chat GPT, like the website, how do I know that OpenAI is actually serving me the right model? They claim, like in the UI, in the front end, they claim that I am using GPT-4, but how do I know that? There's no way of me of actually verifying that this is GPT-4. They might be serving me a worse model, which is cheaper to run and just pocketing the difference, for example, right? I don't know. So one good thing that ZKML provides is this form of accountability where I, as the consumer of some API or some model, I know that this actually came from something because I can verify a cryptographic proof that this indeed happened. So we can make AIs accountable. We can make anyone using AI accountable because we can make proofs of computation. Many of the framing for ZKML in the modern way is that they want to essentially bring machine learning on-chain or onto the blockchain in this case, right? Like we have the blockchains where we have very interconnected networks of low-end hardware, mostly. It's like consumer hardware, which is available everywhere, to run these decentralized networks. And the problem with this is that every single computer on this network needs to re-execute everything that the network sees in order to validate that the network is progressing correctly. And this is a big problem because now everything is really expensive. If it's already expensive running it on your own machine, if you have to run it on 1,000 machines or 10,000 or 100,000 machines, it's as many times more expensive. So it's unfeasible to essentially do machine learning on-chain right now because it's just too expensive. And the computational environment that these usually like blockchains have, like virtual machines, let's say if they're about the EVM, Solana has SVM, the Solana virtual machine, that every single blockchain or most blockchains do have some form of execution capabilities or computing capabilities. And these are very constrained. So some of the things that blockchains are good at is cryptography because they're usually subsidized within the cost of execution in blockchain. So I can verify a zero-knowledge proof on a blockchain and I can bring something that I run off-chain on-chain by providing a proof, right? So for example, if I'm evaluating a model locally, I'm able to create a zero-knowledge proof that I've evaluated a model on some input and I can just send the output to the chain and verify a proof. And then the chain or the smart contract can know that I've actually run a model on some inputs and I don't have to run that within the environment, the computing environment, the blockchain, so I can save a lot of cost. And if I make ML more accessible on-chain, I can actually bring it and I can build application that leverage machine learning for lots of different things which I'll get into a little bit later. And the last one is the zero-knowledge part of things where I'm able to hide specific parts of the computation or specific parts of the data that I'm making proofs about and therefore I can make machine learning private. In order to make machine learning private, there's other techniques as well which is like fully homomorphic encryption and multi-party computation. Each of these other types of cryptography or types of distributed systems engineering and compute it usually have different trade-offs. So for example, fully homomorphic encryption does not give you this correctness assumption or like probability. I cannot verify that something happened correctly, but I can, for example, make computations on Cyphertex. So if I encrypt some data, I'm able to perform computations on encrypted data. And when I decrypt, I have the computation performed on the original input, on the original plaintext which is something that is quite fascinating. However, I do lose this property of ZK where I cannot verify that something happened correctly. Multi-party computation is like the better of both worlds but for example, these protocols require multiple parties to work in unison and this is really hard to manage, for example. So yeah, so I want to talk a bit about who is actually building Zeronautic Machine Learning nowadays. Like which startups, which teams, what are they're focusing on? So full disclosure, I am an investor in Giza and Modulus so I don't want to put that out there, just so you know. So essentially there's different avenues on what is there to build on within the ZKML domain in order to make these systems better or to make this more interesting or faster and all sorts of different things. So the three main companies that usually go around are Giza, Modulus and Ezekio. On Giza, for example, they're building on top of the StarkNet ecosystem which is like one specific scalability solution in the Ethereum space and they're implemented a bunch of machine learning models within this computational environment that this blockchain had, which is called Cairo. The computational environment is called Cairo, the blockchain is called Tarkin and they're building products, right? So they're building, for example, tooling so that financial products that are deployed on StarkNet can leverage machine learning within their, for example, prediction models for financial services so they can predict where the highest yield is to routes my money into so I can get the highest yield on my collateral or my assets so I can use machine learning off-chain or machine learning on-chain within this computational environment, I can prove it, et cetera, et cetera. So essentially Giza is building a lot of the product side of things, like different abstraction, different SDKs, different representations of models, on-chain, et cetera. Modulus is mostly working on the foundational side of things and by foundation, I mean the primordial science of doing cryptography and doing engineering. So they're essentially building their own, the thing so-called like approving system which is how do you implement a zero-knowledge scheme? Like zero-knowledge, it boiled down to mathematics and working with polynomials and finite fields and mostly like linear algebra and abstract algebra and modular arithmetic and bunch of things of this sort. So essentially they're trying to build better cryptographic models and better cryptographic systems in order for zero-knowledge machine learning to be more efficient within the actual representation of it in the computing sense, right? So this is what they're working on. I'm happy to then after this presentation or in the FAQ, I'm happy to share more if anyone wants. Like there's plenty of resources to learn more and Ithiko is mostly building, tooling and also some of the products type of things and infrastructure, a lot of infrastructure as well. So Ithiko, for example, is building an abstraction layer that allows developers that are, they come from the machine learning world. So people who usually know Python or right now in this context is just Python. So people who know Python and know the standard tools for building machine learning models, whether it's TensorFlow or Pycorg or I could learn or any other library for machine learning, they even have a standardized representation of a model which can be exported to this model representation called ONIX, ONIX, open your network exchange format. And this format essentially is something that represents what the model looks like in the computational sense. It's a computational graph of different operations that you need to perform. And Ithiko allows you to convert whatever you are building within Python to something that can be ZK proven, that you can make a proof of in a very easy way. So you just import a Python library, you will create your model and then you just do model that ZK proved and you are able to ZK prove that without you as a Python engineer, as a machine learning engineer, you don't need to know how ZK works. You just create a proof of it because Ithiko has built a tool that helped convert the way that you work with ML to something that cryptography can, the cryptography tooling can create proofs of. And of course, academia, academia has been a crucial element of all of this. There's lots of cryptography, new cryptography coming out every week almost. There's new proving systems, there's new types of final field arithmetic, there's new discoveries in different field. There's different optimizations like computing optimization from representation, better models on the machine learning side. There's also improvements and since usually ZK ML, you need both things to become more performant. If academia comes up with better models and better quantization schemes and whatnot, all of these improvements, compounds, right? It's usually the worst of both, the thing that becomes the worst for the aggregate. So the worst of KML, or sorry, the worst of ZK and the worst of ML become the worst of ZK ML, like the bottlenecks. So academia is working a lot of the foundational bottlenecks when it comes to cryptography and all these other things that I mentioned. So some of the use cases, I do wanna talk about use cases because I've seen a lot of people who are really deeply interested in the technology, but the only way that I've seen technology actually progress forward is if there's funding and people actually are interested and excited about it. For example, in the case of zero knowledge cryptography, because I mostly spend most of my time in the blockchain space, zero knowledge cryptography has become really popular in the last two to three years, mostly because there's been products that actually use it, whether it's scalability solutions, whether it's privacy solutions, whether it's digital identity solutions, there's been product market fit for this technology and so new companies have been created and a lot of capital has been poured in and this capital was reinvested into the actual development of better cryptography, better tooling, better hardware, and also there's a lot of network effects, right? So if there's lots of people using something, other vendors like hardware vendors might want to create hardware for these people, so it will even speed it up even further. So I do believe that, for example, ZKML needs a lot of product market fit or products or catalysts and use cases, which would improve the state of the art just by the fact that there's many people looking at it, there's a lot of mind share, there's high excitement. So of course there's negative parts to this as well, but mostly I think it's good. So some of the use cases that I've seen around, so I personally work at WorldQuake and that's like the way that I got exposed to it. I'll explain our specific youth case towards the end. So provable inference is one, so I mentioned earlier on that I do not know if I'm using chat GPT, that someone is actually serving me the model that they claim they are. So provable inference is just this concept where I can know that whomever who used a model to infer some output, I know where it came from. I know which model it came from. If it's public, of course, the APIs can choose to keep the model private, but at least they can, for example, commit to it. Something that I can do if, let's say, GPT-Force Quilt Force, right? Open source is not open source. OpenAI did not open source GPT-Force, as of now. And so if, for example, I cannot know that someone used GPT-Force because I don't have the weights, right? It's not a public thing. But something that OpenAI could do or anyone else with a private model could do is that they can commit to a specific model, let's say a hash. And for example, I know that for the entire user base, they're using the same model. So they cannot fool any single user that they're using specific different models for anyone else. At least they can commit to it with a cryptographic hash, so I can just hash. I know that there is one deterministic output for this model. And I know every single user knows that they're using the same model because within the zero-knowledge group, they have a commitment to some specific set of weights, but they do not reveal the weight. They're just committed. And maybe later, they open source the model, they can reveal the weights, and you can see that the commitment does indeed match the weight. So you actually learn that you did indeed learn about that. That they did actually use the model they claimed they were. In this case, GPT-4, they managed to open source the weights. So that's provable inference. It can be used for APIs. So I mentioned like chat GPT, but there's many others, like video games. If I'm playing an on-chain game and there's some form of ML, how do I know that the game is not cheating? How do I know that I have fair rules on there? The second one is bringing AI on chain. So there's lots of smart contracts, lots of applications that people are building within the blockchain domain. And within the blockchain domain right now, it's very limited in terms of things it can do. And machine learning can provide lots of cool solutions to a lot of different problems, right? At the end of the day, machine learning is able to provide good enough approximations to problems that people have. And so if we're able to bring that on chain, we might be able to bring some interesting opportunity to the table. I mentioned the financial ones, where for example, I have a yield protocol on chain where I deposit assets to this protocol and it tries to optimize the yield that I get on those assets. But it can, it has to use a strategy. Usually these strategies are called source and hidden, but at least I can commit to a strategy. And this strategy can now leverage machine learning and I can take proof to the protocol that we're using a machine learning strategy fairly and we're not updating the weights. And we can also, for example, prove that it was trained on some historical data with some accuracy, right? I can make a proof that my model is accurate on some historical data in terms of yield routing with some accuracy and it's routed the most performant way, for example. There's also another one which is agents or intents in the context of blockchain. So agents is a word that comes from the ML lingo, which is like a program that has the ability to do actions on their own, right? They're a player, some system, like game theoretical system in this case. So if we have some system, let's say that, I don't know, like we have a program and we allow this machine learning algorithm, let's say robotics. Robotic agents are, is a good analogy, right? So I have a robot and the robot is able to interact with the real world because it has limbs, it has different tools like cameras, et cetera. And it's able to interact with the real world. The robot in this case is an agent, right? It's a program which is able to perform actions in the real world. It doesn't have to be a real world agent. It can be a digital agent. It can interact with a website. It can browse the web. It can watch a video and give me some information about it. But essentially on-chain agent could, for example, interact with a blockchain if they have maybe some knowledge, right? So if it's a smart agent, it sees that, okay, something happened here. So maybe I see that there's a liquidation happening. So let me do this, let me buy this, let me sell that. There's different agents that can learn based on information and if they have a set of steps that they can do, they can maybe try and optimize for some goal and then they become agents in the system. Blockchain people like to call this intense, yeah? And another one is attestations, for example. So I can make attestations about things, right? I can prove to a smart contract that I'm over 18. I can prove that all sorts of different things, right? Essentially, I'm just able to use machine learning off-chain and I can prove that and bring it on-chain. Private and machine learning cruise. So this one is a cool one. So for example, in the context of medicine, let's say that there's a cancer diagnosis model and I as the patient, I do not wanna reveal to anyone like my personal health records. But for example, there's a doctor or some health institution which signs some form of report or some form of certificates to some personal health records or data. And then there's a machine learning model which uses this data to essentially evaluate it and tells you whether you're likely to have cancer or whether you have cancer and with what's uncertainty. So something that you can hear is that if you want to prove to, for example, let's say that an insurance or a payout had a condition that you've been insured against or something like that, you'd be able to prove to them that there's some health institution or a specific health institution if you want that has concluded that I am indeed this or have been diagnosed with a specific thing without revealing the model, without revealing the weight, but you at least know that there's some specific thing that you can make a proof about. The possibilities here are early big in the sense that there is generally programmable. So this is like just one concrete example, but people can essentially make proofs about anything, any data that they have and computations that they did when they're machine learning base or not without revealing the data itself, right? The only person who learns about the data in the context of ZK is the person making the proof. I mentioned earlier that this property called completeness. So in order to make a valid proof, I do need to have the data. So the problem is that there's always a prover, always learns the data, but if the prover is controlled by myself, then only it's the same thing as me learning data. So it's something that is a worthy trade-off. So if I'm making a proof on my own computer, that makes sense. And I can prove to anyone else anything without revealing my data. But if I am, for example, delegating it to a server, the server doesn't need to learn my information, so I need to be careful. And digital identity. So I do want to explain very briefly, like how did we come to this at WorldCoding? So we have this hardware device, it's called an org. I do have it with me. Maybe if you guys want, I can just go grab it. And one second in the FAQs, I can show it. But essentially the WorldCoding org is a piece of hardware that verifies two things. It proves that there's a real person in front of this hardware device. It does this bunch of phenomenally detection like methods, and some other like statistical-based methods, some sensors that it has like infrared sensors, and it has like field of depth sensors, it has high-resolution cameras, et cetera. And it's able to determine that there's a real person in front of the hardware device, the org, with like a shiny ball, I'll show it in a bit. And it can also prove the person in front of it is unique. And the way that it does that is that it takes a high-resolution image of the person's IRC's, and it's able to compute a unique representation of them called an IRS code. And this IRS code, the good thing about it, is that it's not deemed personally that if I put information, it's just the representation of the uniqueness or of the randomness of a person's IRS, and I can use that to measure how unique they are. And if the distance between two different IRS codes is big enough, I can prove that this user is unique. And then once I prove that the person is unique, I'm able to essentially put them in a set of verified users. And then what we do is we have a protocol called WorldID, which allows you to prove that you're a member of this set of verified users without revealing which member you are, using zoological photography as well, but not ZKML, just traditional zero-knowledge cryptography. You're able to prove that I am a unique verified human being without revealing who you are, and the data that we collect, which is just this IRS code, is not personally data-fibre information. We don't collect the raw biometric images, which is cool, because you're able to essentially leverage modern cryptography, modern biometric literature, and the modern tools, like modern hardware like GPUs and everything. Everything happens client-side, like within this actual hardware device, right? So the hardware device does this computation, nothing leaves the actual orb, and then the orb deletes everything within its secure enclave and computational environment. Within this model of how work can work, very simplistic model, there's one specific problem, which in our biometrics pipeline, if you change the pipeline in any significant way, you change the outputs of this uniqueness representation, you change the output space of the IRS codes, you can think of them as vectors, right? So you essentially take this vector space, and you convert it to a different one. So the same user will have a different representation in this new space, therefore you will not be able to measure uniqueness anymore. So if you ever update the model, you have to re-sign up all users. And since you have this physical hardware device that people have to go to, it means that all the users that have signed up to date to WorldID have to go in person again to this hardware device and get re-signed up. And this is terrible, because it's already been really hard enough for us to get 5 million plus users and to have to force our users to re-sign up every single time that we update the biometrics pipeline, it would be really bad. And it has like really terrible user experience. And so this is where one of my coworkers, his name is Remco, at the WorldCon Foundation, he came across with a solution or an idea, which was what if users self-custody their own biometrics, meaning that the orb, which has essentially a secure enclave and a trusted execution environment. So essentially these two pieces of chips or these two chips allow you to sign things. So I'm able to cryptograph a big sign, something that the orb sees. So whenever the user gets verified that they're a real and unique person, the orb can sign their raw biometric, which it has in its memory for a given lifetime, and it can give it to the user, and the user can store their own biometrics in their phone and they can encrypt them, of course, store them safely in an encrypted fashion on their own phone or cloud or whatever they prefer. And they would be able to then have a signature from the orb on the actual biometric, right? You know that this image was seen at one point by the orb and it said that this is a unique human being and this is a real human being, most importantly is the real part. Whenever we want to update the model, what the user could do is they would be able to download the new model, the weights of the model, and they seek the approving library for that specific model and they would be able to create a group that created this iris code within a zero-knowledge environment. So they would be able to create a zero-knowledge proof that they've created a valid iris code from an input image, which was attested to by the orb. So essentially the pipeline of trust here is not broke, right? I know that I've created an iris code from an original biometric, which was verified by the orb to be unique. And with this, I'm able to permissionlessly or out of my own accord, I'm able to permissionlessly insert myself into the set of verified users without having to go to the orb again because I have the entire set of steps that I need in order to prove to the world ID protocol that I'm a unique user without revealing who I am again. I just proved to you that, hey, the orb saw me at one point in time, the orb did indeed sign this my image, I store my images, and then I make a proof that I've created this derived representation of uniqueness for my biometrics, and I can prove to them I'm a unique human in this new representation and this new model without revealing who I am again, right? So this is like perfect things, like a perfect solution for us. It's actually quite crazy that this problem didn't exist, at least that's what I felt when I first covered it through Remco. And so right now, for example, we're working with one of the companies which I mentioned earlier, Modulus to essentially do this client-side zero-knowledge machine learning proving inside of a user's phone, right? So that people can self-cassellate the biometrics and permissionlessly insert themselves. It's like very early stages, R&D is not yet in production, but there's been a lot of good progress here. And two years ago, it seemed like sci-fi, now there's already like concrete proof concepts and implementation and there's benchmarks and things that are improving. But yeah, this is like one of the things that I saw. One last thing that I do want to mention before I leave you to ask me questions and for me to go grab my orb as well is technical bottlenecks. Zero-knowledge machine learning, right? We've seen some use cases, we've seen what people are working on, what people are doing, what it is, some of the things that we are doing, but where do we go now, right? If you're someone who is interested in this topic, where could you contribute if you want to, if you end up learning more? How do you contribute to these? Or what are the problems that are hard and that would help us improve in the front? So one is better cryptography, right? Because it's better ZK. As I mentioned, the worst in ZK and the worst of ML create a joint bottleneck. So if you improve ZK or if you improve ML, you improve the KML. But there is also an intersection where if you just focus on the ZK parts that would make ML better and on the ML parts that would make ZK better or simpler, that's the most focused effort that you can make to essentially improve everything. The one is better cryptography. So remainder, for example, the thing that I mentioned here, remainder is a proving library that is built by modulus labs or modulus, which essentially uses a type of cryptography which better models the structured nature of machine learning computing. Where like machine learning usually have matrix multiplication. They have some non-linearities. So like functions that are non-linear, in this case, there's like activation functions or like a good example of this. So there's things like ReLU, which is one of the most popular ones, defined linear unit, something like that. Like tanh, there's a bunch of activation functions, non-linear function. So they built a cryptographic system which is able to represent the structured computation in a much more efficient way. So when it comes to proving these structured computations, it takes a lot less computational power to do so because the representation is much more succinct and much more efficient. And so it makes it a lot faster and a lot more performant and less computationally intensive. So this would potentially make it feasible to run a DK machine learning prover on a personal phone, for example. Another one is better hardware. So hardware and specialized hardware is one of the things that modern science has benefited from most. We've seen the transistor consistently shrinking and shrinking. We fit more transistors on a chip, almost like 2X every 18 months, right? There's Moore's Law, which goes exponentially. And now we're at like the two nanometer scale where we have transistors that are two nanometers wide and we're able to pack trillions of them on modern GPUs. And for example, in the context of machine learning, machine learning was really terrible on traditional computers like CPUs back in the day, like in the 60s and 70s and 90s. So no one actually did machine learning back then. But when these people were playing video game for some reason, people started building chips that represent graphical interfaces a lot better. And it happens that there's an overlap of the mathematics that are used to represent graphics and graphics card and the machine learning, right? Machine learning is the matrix multiplication and the way that you represent pictures is matrices, right? It's just zeros and one that represent the RGB values of every single pixel on the screen and transformations between them. And so you have to do these operations between pixels really fast. And it just happened that it's the same thing as doing machine learning like neural network fast multiplication across multiple connected layers. It's like very similar structure. And so people start using GPUs to speed up machine learning and machine learning became feasible all of a sudden in 2012 with convolutional neural networks and all these new like booms that we've been writing until now with modern LLMs. Like LLMs and all these new generative AI models are only possible because of this specialized hardware that come from NVIDIA, DCMC, AMD, ASML, like all these like transistor manufacturers, graphic car manufacturers, specialized hardware manufacturers. These ones are for machine learning, GPUs and tensor processing unit, GPUs, cryptography on the other hand, they work with a different type of math. Instead of working with floating points or arithmetic, they work with finite fields and sixth point arithmetic. And so you need to design fundamentally different hardware. And so we need to build better hardware to improve the computational capabilities of zero knowledge machine learning or just zero knowledge cryptography in this then. So there's lots of things to be done here. So I'm, for example, I'm also an investor in Fabric, I'm sorry for that, but Fabric is one of the ZK hardware company. In GoNyama, not size thick, it's size thick, sorry about that, some misspelling without the T. And irreducible, there's some of the biggest ZK hardware companies. And yeah, so these are trying to essentially model the software in hardware so that it's faster and there's less overhead. Another one is better tooling. So I mentioned Ezekiel and Giza. They're building tooling that makes it easier for developers to use ZK. And if I'm a machine learning engineer, there's no way in hell I'm gonna spend six years learning cryptography and learning the state of the art and trying to contribute there so that I can prove my machine learning model. As a machine learning engineer, I just care about something that ZK can bring to me. And vice versa, if I'm a ZK guy, I just care about something that ML can bring to me to get better or like somehow make it on chain. So whenever we've brought down the cost of barriers, like barriers that the cost barriers that prevent us from doing something, people start experimenting, right? Like same thing happened with the web. Like anyone can build a website nowadays and you can build a business, you can just be Shopify. And if I'm a business guy, I don't need to know web development. Shopify and I have my store and I can process millions of dollars of payment. I can have a truly user and everything. And otherwise I would have to learn web development, servers, everything. I don't have to care about that. I just do my business and I use web technology without having to know how it works. So the same thing applies to KML of course. More robust than secure implementation. That one is a bit like self-explanatory but essentially like the more security the less prevent like if we can prevent hacks and exploit then if it's more robust, it can sustain more users, et cetera. And the other one is like what I mentioned before pretty much at the same point, like better tooling and easier interfaces is pretty much the same thing because the easier it is to use, the more experimentation there for the more products, the more product market fits, the more businesses can build and the more technology can accelerate towards the direction of growth. So yeah, that's everything about my presentation and I would love to answer any of your questions. I don't know how long we have. I think it's 14 minutes for FAQ. I can also go run, get the orb if you guys wanna see it. And thank you for having me. This was fantastic. Thank you so much. What a world. And we have a few questions already here from people in the chat and then maybe after a few we give you some time to breathe and get the orb, that would be great. Okay, so first one, Shadi, if you wanna unmute your first. Hi, yeah, thanks for the great presentation. Very informative. I had a question about the personally identifying information from the hash from the iris biometrics. Isn't a hash or iris still uniquely identifying if you know the hashing function to produce that digest? Or did you mean that make the function is kept secret and nobody can easily take like a photograph of someone and then produce the same hash and that look on chain, for example, I don't think you posted on chain but look on chain, for example, to try to match that. Yeah, there's one unfortunate naming collision here. So in biometric literature, people use a hash in a non-urgorous way. And so what we mean here, or what we used to mean, we've changed the way that we explain these things. We no longer use the terminology of hash because we work in the intersection of AI and cryptography and if you use a term that means a different thing in both, it's like ambiguous and it can cause problems like this one right now. Actually, the way the biometrics pipeline works is that there's this essentially convolution-like algorithm. It's called the GABER wavelet or GABER filter, which essentially applies convolutions into original biometrics many times over and it's able to compute like a randomness representation. And this one essentially compresses the image so much, like after performing all these operations, you end up with a pretty much a small representation of a few bits, like I think it's 200, something that, so the vector in the end, like the embedding in the end is like a few bit. And this one is not able to be reconstructed to its original, at least like a lossy function, right? If I go from a compressed representation to a fully, try to expand it back, I lose information in the process of converting it to this compressed representation. Therefore, I'm not able to reconstruct the same one. And the good part of this is that I'm able to, I'm able to reconstruct something similar, but it's not personalized identifiable, at least not considered so in modern literature, right? This may change and this is why we've been working on a lot of other things within world economy, like more of the party computation solutions and whatnot. We're gonna be publishing a lot about this in the coming month, but if you're interested in like follow this, the biometrics pipeline works and have the definition of it and how it works and what is actually going on, I recommend going to the link I just said in the chat, my paper that work in the org. Also, one of my teammates is in the, actually one of the former teammates, he's at Tool for Humanity, which is the labs entity. I'm at the foundation of different legal entities, but they're both contributing to the world team project. His name is Daniel Gershiewicz. He is in the cause law. So he's also able to explain a bit more. He's on the org software team. So he works a lot more with the biometrics pipeline than I do. I'm more still in the cryptography protocol side of thing. But yeah, within the white paper, white paper.worldcoin.org, you have a biometric action and you have the third definition of what it is that we're doing and how we preserve privacy. So to answer your question in a specific way, it's not a hash, it's not a cryptographic hash. There's no digest, there's no plaintext. It's essentially a convolutional like operation which happens many times and it leaves the input unrecognizable and you compress certain information of the randomness that you get. You cannot use that to reconstruct the original thing that you put into this function because it's a very lofty function. And this is good enough to prevent like getting the raw biometric out again. Got it. So the idea is even if I had access to the kernels that you used to train, then I wouldn't be able to deconvolute the output. I see. Ideally to try and break our own assumptions and try to reverse engineer and actually get the original image. And now we've gone ahead a step further because if it was possible, we've gone a step further and we're now storing everything in ciphertext and the uniqueness check, it's happening on ciphertext with multi-party computation. So yeah, that's like cool, cool new research stuff. Love it. Dan also shared, I think the white paper that you referenced directly here in the chat already a little further up. Thanks for that, Dan. Next one up we have Richard and then we have Micah. Yeah, I think the previous discussion answered my question there. Thank you. Awesome. Wonderful. Micah, you go. Micah, we can't hear you. Feel free if you can't unmute to put your chat, your question in the chat. Okay, he's going to rejoin. This could be a great opportunity for you to get the orb. I also have a few questions, but God, you go. Can you hear me? Yes, Dan, we can hear you. I want to go back, there was this question about and things being personally identifiable or uniquely identifying information. And then the question turned into ashes versus wavelet encodings. I don't know. I think the question actually got lots of discussions. And the interesting thing is that even if you've had either a hash or an encoding, and you somehow broke this and could reverse that image, actually the privacy comes from what was briefly mentioned in the talk, which is that when you prove your ownership of such, you're proving ownership of a key that was linked to this biometric encoding. So when you prove ownership of that key, you're not pointing to which encoding is yours. So you're a member of the set without revealing which member. And that means that these encodings are cryptographically delinked from anything else, your transactions, your accounts. Nothing can be linked back. So if you did reverse those codes, you wouldn't know. Also, one additional thing is that these encodings are not public. They're hidden in a database that we have. The thing that is public is the public key associated with the user that has undergone a unique question. So if I have this unique coding, and I prove that I'm a unique within the coding set, which is kept not on any public sphere, it's now kept in this multi-party computation encrypted environment in a database that is run by three different parties in an MPC setting, again, multi-party. And when the user is verified to be unique, we take the user's public key, which was generated by the World app, which is the way that you interface with World ID and the wallet and a bunch of other things that we're building. Essentially, the public key that was generated by the World app, by the user, which is a unique person that's been just verified by the World, gets inserted into the set of verified users, and then I'm able to make a knowledge proof that I own a private key to a public key and the set of verified users. So even then, there's one more step that removed from your biometric completely, because a public key is just random, cryptographical, gibberish that I can make proofs about, and I'm able to prove to you that I'm a unique member of the set because I own a private key to a public key in the set, but I don't know which one. And there's another cool part, which is the nullifier scheme that we have, which allows you to represent unique actions. For example, one is like unique governance, like one person, one vote, digital governance, or voting protocol. Currently, there's no way to prove that you're not a bot online. So if you, for example, let's say, I don't know, Elon Musk puts a poll on Twitter or on X that, hey, is this doc cute or no? I can create a bajillion X accounts and vote for no, right? There's no way for me to prove that this is a one person, one vote. So whomever who posts a poll on whatever thing doesn't matter, like the opinion doesn't matter, the result of the poll doesn't matter. There's million bots that have incentives and both, like presidential elections, if Elon says, is this candidate a good guy? People can vote yes, but it can be like a third, like external nation state actor trying to just civil attack, which means like attack a protocol where you need unique members in a way that just make the protocol broken completely beyond repair. So this is where we step in, where we create a unit of account of uniqueness for humans in a digital environment, whether it's on chain or it's off chain doesn't matter. As long as you're able to make these cryptographic at the station that I am a unique person and I've not done an action before, so I'm able to prove that I'm a unique actor. I'm a unique member of this protocol and I only voted once. So I can say that this dog was cute only once. And if I want to weigh the outcome, the only way to weigh the outcome is that I need to convince a thousand other members to vote for the same thing, but I'm not able to just create a million accounts and vote for a same thing to weigh the outcome. Yeah. Awesome. Yeah, I think Bramco talked a little bit about that, especially like with possible future applications also that you also listed very briefly, including in medicine and so forth that I think these groups are just like really incredible for because it can't for medicine, for financial risk, for insurance and stuff, you just can't really access the data any other way. Or you just can't do too much anyways with the information. Okay, we have another question, but you also have the orb now or? It's right here, my lap. Wonderful. By the call, the battery, at least. All right, sorry. I'm gonna unbler my background. I do have a, I just moved into a new apartment, so forgive me for, but yeah, the battery right here. Yeah, that is able to say a lot more about the orb than I can. If you work on the orb software team, you've been working on this for three years plus. Yeah. Yeah, I've come in multiple close contacts with the orb already and I think in 50 years you even have it like taken apart, you know, and it's different components, which is really fun to see. So yeah, thanks. The orb hardware specs are publicly available on GitHub as well. So people can see the PCB design, they can see like what components it's made out of. There's also an hour paper, there's like an annotated set of every single component and like what it does, how it works, et cetera. Yeah, I've been following like just how many people are signing up and like the very, very long lines. Sign up stations, which has been really interesting. Okay, Micah, you rejoined and you raised your hand. Do you want to ask you a final question? Testing, can you hear me? Yep. Quick comment and then a question after that. The, I believe that while the transactions, your transactions made using your unique ID can't be linked back to you. You can be linked back to your transaction. Someone has an orb or the algorithm in the orb and they can get a picture of you, so to speak. They can then regenerate your unique ID and this is an unnecessary piece. You can't get rid of it because in order to have a unique human, you need to be able to verify. There needs to be only one outcome. You can't introduce randomness here, right? And so if your goal is to not have someone be able to tell which transactions you did, that is not possible here. But someone can't tell just by looking at the transactions that they were yours. It's a one-way thing. This one held you down and put an orb in front of your eye, they can then figure out all your transactions, but they couldn't look at your transaction and figure out which eye they belong to, so to speak. Yes and no, there's one step that helps us mitigate this, which is the separation of the public key, right? Like your transactions are not being done by your iris code or whatever. The transactions are being done by a public key which was owned by a user, which just happened to verify at the orb, right? You would have to get the user's private key to learn what they did on the game. And then you would also have to get their bandwidth forked to try and interlink too, right? You'd have to get their bandwidth forked, you'd generate the iris code, and then get somehow steal from them their private key. And then with a private key, you're able to de-anonymize the on-chain state that they performed in a DK way because you're not able to just get the live iris. You know you can generate them. And so there's two things that you need to compromise there. I'll just, my actual question, I'll try to keep it, I know I've got one minute left. Last I checked, DK proof of an execution takes on the order of a thousand times or so, executing the same thing without a DK proof. Even though inference is significantly cheaper than training, execution costs is still very non-trivial. That's why you need giant GPUs and whatnot, just to do inference. The use cases you're thinking of, are they all things that are like become useful once we can get the DK proofing costs down by a hundred times? Or do you think there's some things that are usable even with that thousand increase in execution costs? Right now there's already like DK use cases on-chain, right? And there's equally expensive in the ML lens, like already proved a hundred million like parameter models in an inexpensive way, right? In a usable way, right? Let's say you have a small convolutional neural network classifier with 200 million like weights, like flowing point, or in this case for a ZKML with field elements, but you're able to make proofs in reasonable time, one to two minutes for inference, right? Where the evaluation of it in the normal world is a thousand nights less, two millisecond, 20 millisecond, like point two second, or 200 million, my bad. So yeah, like this is like the costly incur, but it makes sense for some things. And right now, as I mentioned, like the things that we're doing could prove the thing, like cryptography, better implementation, better hardware, specialized hardware, et cetera, they're gonna bring down this cost significantly, it's gonna make more things feasible. Like now we can maybe prove an LLM, it may take 10 minutes, but maybe proving that LLM once for something that's really important enables a new thing. And it's always happened like this, that like the use case and the demand for it and bring the proving time down, the overhead down, the performance up, and we're still like have a bunch of things to do. So it should work out eventually. Love it. Any final words? How can people find out more? I know that you said, for example, there's an announcement coming on soon, but if people are really excited about this or they just wanna learn more, what are any possible action items that people can take? So action items. So if you have a specific question about this presentation, you wanna ask me, I think my Twitter or my Telegram, or like my ex and my Telegram are the best. So my handle is DC build 3R. So DC builder, but with the 3 instead of an E at the end. So that is for asking me questions. If you were interested in ZKML itself, I do have a resource aggregator for ZKML things. It's on my, or one of my GitHub's, which is github.com slash ZKML dash community slash awesome dash ZKML. I'm able to leave the link in the chat real quick, but I think like the spelling makes sense. Besides that, there's a bunch of startups working in ZKML, mostly like what I mentioned, like Modulus, Giza, and Ezekio. And these three keep coming out with new developments, new things, new announcements, et cetera. There's cryptographic papers coming out on ZKML, which I also try and keep up to date on my resource. So yeah, those are like the best ones, I think. Love it. And I just saw from Dan that he's bringing up to our upcoming May workshop though. If you're going to that one, you may be able to try it. Hey, thank you so much. This was really fantastic. Thanks for staying on three minutes longer. I really appreciated it. Thanks for all of your great questions, everyone, and I hope to see you guys soon. Bye-bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.2, "text": " Hi, Vern. Welcome to FOSAT's Intelligent Cooperation Group.", "tokens": [50364, 2421, 11, 33220, 13, 4027, 281, 479, 4367, 2218, 311, 18762, 25002, 3066, 43493, 10500, 13, 50524], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 1, "seek": 0, "start": 3.2, "end": 5.08, "text": " We're really excited about this seminar.", "tokens": [50524, 492, 434, 534, 2919, 466, 341, 29235, 13, 50618], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 2, "seek": 0, "start": 5.08, "end": 7.5600000000000005, "text": " We have DC here from Welkheim.", "tokens": [50618, 492, 362, 9114, 510, 490, 3778, 74, 18673, 13, 50742], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 3, "seek": 0, "start": 7.5600000000000005, "end": 11.16, "text": " Thank you so much for joining us to give an overview of ZKML,", "tokens": [50742, 1044, 291, 370, 709, 337, 5549, 505, 281, 976, 364, 12492, 295, 1176, 42, 12683, 11, 50922], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 4, "seek": 0, "start": 11.16, "end": 13.56, "text": " which is a topic that has come up so much,", "tokens": [50922, 597, 307, 257, 4829, 300, 575, 808, 493, 370, 709, 11, 51042], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 5, "seek": 0, "start": 13.56, "end": 16.92, "text": " especially, I guess, data coming up already a few years ago", "tokens": [51042, 2318, 11, 286, 2041, 11, 1412, 1348, 493, 1217, 257, 1326, 924, 2057, 51210], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 6, "seek": 0, "start": 16.92, "end": 18.56, "text": " in this group, and then much, much more", "tokens": [51210, 294, 341, 1594, 11, 293, 550, 709, 11, 709, 544, 51292], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 7, "seek": 0, "start": 18.56, "end": 20.64, "text": " to have really ramping up in the last year.", "tokens": [51292, 281, 362, 534, 12428, 278, 493, 294, 264, 1036, 1064, 13, 51396], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 8, "seek": 0, "start": 20.64, "end": 22.72, "text": " And so I'm really excited for you to share a little bit more", "tokens": [51396, 400, 370, 286, 478, 534, 2919, 337, 291, 281, 2073, 257, 707, 857, 544, 51500], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 9, "seek": 0, "start": 22.72, "end": 24.84, "text": " about it because we have not had a dedicated seminar", "tokens": [51500, 466, 309, 570, 321, 362, 406, 632, 257, 8374, 29235, 51606], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 10, "seek": 0, "start": 24.84, "end": 25.6, "text": " to this topic yet.", "tokens": [51606, 281, 341, 4829, 1939, 13, 51644], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 11, "seek": 0, "start": 25.6, "end": 26.96, "text": " So thanks a lot for joining.", "tokens": [51644, 407, 3231, 257, 688, 337, 5549, 13, 51712], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 12, "seek": 0, "start": 26.96, "end": 28.64, "text": " We're really excited about you guys' work.", "tokens": [51712, 492, 434, 534, 2919, 466, 291, 1074, 6, 589, 13, 51796], "temperature": 0.0, "avg_logprob": -0.22211288270496188, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.11343052983283997}, {"id": 13, "seek": 2864, "start": 28.72, "end": 30.12, "text": " Without further ado, please take it away.", "tokens": [50368, 9129, 3052, 22450, 11, 1767, 747, 309, 1314, 13, 50438], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 14, "seek": 2864, "start": 30.12, "end": 31.76, "text": " I'll be in the chat monitoring questions,", "tokens": [50438, 286, 603, 312, 294, 264, 5081, 11028, 1651, 11, 50520], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 15, "seek": 2864, "start": 31.76, "end": 33.76, "text": " and we're off to the races.", "tokens": [50520, 293, 321, 434, 766, 281, 264, 15484, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 16, "seek": 2864, "start": 33.76, "end": 35.6, "text": " Awesome. Thank you for having me.", "tokens": [50620, 10391, 13, 1044, 291, 337, 1419, 385, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 17, "seek": 2864, "start": 35.6, "end": 38.64, "text": " So today, I'll be talking about the zero-knowledge machine", "tokens": [50712, 407, 965, 11, 286, 603, 312, 1417, 466, 264, 4018, 12, 15869, 3042, 3479, 50864], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 18, "seek": 2864, "start": 38.64, "end": 39.120000000000005, "text": " learning.", "tokens": [50864, 2539, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 19, "seek": 2864, "start": 39.120000000000005, "end": 41.6, "text": " I'll be giving a very brief introduction.", "tokens": [50888, 286, 603, 312, 2902, 257, 588, 5353, 9339, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 20, "seek": 2864, "start": 41.6, "end": 45.120000000000005, "text": " But first, maybe let me start by saying a little bit about myself.", "tokens": [51012, 583, 700, 11, 1310, 718, 385, 722, 538, 1566, 257, 707, 857, 466, 2059, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 21, "seek": 2864, "start": 45.120000000000005, "end": 48.2, "text": " I'm a research engineer at the World Coin Foundation,", "tokens": [51188, 286, 478, 257, 2132, 11403, 412, 264, 3937, 39054, 10335, 11, 51342], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 22, "seek": 2864, "start": 48.2, "end": 49.64, "text": " and World Coin is this project that", "tokens": [51342, 293, 3937, 39054, 307, 341, 1716, 300, 51414], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 23, "seek": 2864, "start": 49.64, "end": 53.64, "text": " is trying to build the largest identity and financial network.", "tokens": [51414, 307, 1382, 281, 1322, 264, 6443, 6575, 293, 4669, 3209, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 24, "seek": 2864, "start": 53.64, "end": 56.72, "text": " And there is an interplay of various technologies", "tokens": [51614, 400, 456, 307, 364, 728, 2858, 295, 3683, 7943, 51768], "temperature": 0.0, "avg_logprob": -0.1670522413391998, "compression_ratio": 1.6184615384615384, "no_speech_prob": 0.004326678812503815}, {"id": 25, "seek": 5672, "start": 56.72, "end": 59.6, "text": " in the products and things that we're building at World Coin,", "tokens": [50364, 294, 264, 3383, 293, 721, 300, 321, 434, 2390, 412, 3937, 39054, 11, 50508], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 26, "seek": 5672, "start": 59.6, "end": 63.04, "text": " and some of which are AI, and some of which are cryptography.", "tokens": [50508, 293, 512, 295, 597, 366, 7318, 11, 293, 512, 295, 597, 366, 9844, 5820, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 27, "seek": 5672, "start": 63.04, "end": 65.16, "text": " So we've had expertise in both realms", "tokens": [50680, 407, 321, 600, 632, 11769, 294, 1293, 42824, 50786], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 28, "seek": 5672, "start": 65.16, "end": 66.84, "text": " from different teams internally.", "tokens": [50786, 490, 819, 5491, 19501, 13, 50870], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 29, "seek": 5672, "start": 66.84, "end": 69.84, "text": " And there have been some essentially experimentation", "tokens": [50870, 400, 456, 362, 668, 512, 4476, 37142, 51020], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 30, "seek": 5672, "start": 69.84, "end": 70.96, "text": " that naturally occurred.", "tokens": [51020, 300, 8195, 11068, 13, 51076], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 31, "seek": 5672, "start": 70.96, "end": 72.84, "text": " We have some AI parts of the stack,", "tokens": [51076, 492, 362, 512, 7318, 3166, 295, 264, 8630, 11, 51170], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 32, "seek": 5672, "start": 72.84, "end": 75.2, "text": " and there are some reasons why cryptography might", "tokens": [51170, 293, 456, 366, 512, 4112, 983, 9844, 5820, 1062, 51288], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 33, "seek": 5672, "start": 75.2, "end": 77.56, "text": " be useful in the AI sector of the stack.", "tokens": [51288, 312, 4420, 294, 264, 7318, 6977, 295, 264, 8630, 13, 51406], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 34, "seek": 5672, "start": 77.56, "end": 81.2, "text": " And so this prompted us to think about this topic", "tokens": [51406, 400, 370, 341, 31042, 505, 281, 519, 466, 341, 4829, 51588], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 35, "seek": 5672, "start": 81.2, "end": 83.0, "text": " about two years ago in August.", "tokens": [51588, 466, 732, 924, 2057, 294, 6897, 13, 51678], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 36, "seek": 5672, "start": 83.0, "end": 85.28, "text": " Almost two years ago, August of 2022.", "tokens": [51678, 12627, 732, 924, 2057, 11, 6897, 295, 20229, 13, 51792], "temperature": 0.0, "avg_logprob": -0.16576716394135446, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.0426902137696743}, {"id": 37, "seek": 8528, "start": 85.76, "end": 88.24, "text": " One of my teammates was just playing around with cryptography", "tokens": [50388, 1485, 295, 452, 20461, 390, 445, 2433, 926, 365, 9844, 5820, 50512], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 38, "seek": 8528, "start": 88.24, "end": 90.16, "text": " and trying to prove machine learning models.", "tokens": [50512, 293, 1382, 281, 7081, 3479, 2539, 5245, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 39, "seek": 8528, "start": 90.16, "end": 92.64, "text": " But so this is a little bit of a background.", "tokens": [50608, 583, 370, 341, 307, 257, 707, 857, 295, 257, 3678, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 40, "seek": 8528, "start": 92.64, "end": 94.92, "text": " I mostly run our grants program, where", "tokens": [50732, 286, 5240, 1190, 527, 16101, 1461, 11, 689, 50846], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 41, "seek": 8528, "start": 94.92, "end": 96.92, "text": " we give grants to people to help decentralize World", "tokens": [50846, 321, 976, 16101, 281, 561, 281, 854, 26515, 1125, 3937, 50946], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 42, "seek": 8528, "start": 96.92, "end": 98.4, "text": " Coin and solve some more problems,", "tokens": [50946, 39054, 293, 5039, 512, 544, 2740, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 43, "seek": 8528, "start": 98.4, "end": 101.04, "text": " and also help with a bunch of different R&D efforts", "tokens": [51020, 293, 611, 854, 365, 257, 3840, 295, 819, 497, 5, 35, 6484, 51152], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 44, "seek": 8528, "start": 101.04, "end": 103.08, "text": " as an individual contribute.", "tokens": [51152, 382, 364, 2609, 10586, 13, 51254], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 45, "seek": 8528, "start": 103.08, "end": 106.24000000000001, "text": " So without further ado, let me start with the presentation.", "tokens": [51254, 407, 1553, 3052, 22450, 11, 718, 385, 722, 365, 264, 5860, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 46, "seek": 8528, "start": 106.24000000000001, "end": 108.72, "text": " Usually, the way that I like to start with this presentation", "tokens": [51412, 11419, 11, 264, 636, 300, 286, 411, 281, 722, 365, 341, 5860, 51536], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 47, "seek": 8528, "start": 108.72, "end": 112.4, "text": " is that I like to decompose the statement into its constituents", "tokens": [51536, 307, 300, 286, 411, 281, 22867, 541, 264, 5629, 666, 1080, 30847, 51720], "temperature": 0.0, "avg_logprob": -0.1699596896316066, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.004608582239598036}, {"id": 48, "seek": 11240, "start": 112.4, "end": 115.56, "text": " so that people have an understanding of which elements", "tokens": [50364, 370, 300, 561, 362, 364, 3701, 295, 597, 4959, 50522], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 49, "seek": 11240, "start": 115.56, "end": 117.04, "text": " or what is zero-knowledge machine learning?", "tokens": [50522, 420, 437, 307, 4018, 12, 15869, 3042, 3479, 2539, 30, 50596], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 50, "seek": 11240, "start": 117.04, "end": 118.28, "text": " What is the zero-knowledge part?", "tokens": [50596, 708, 307, 264, 4018, 12, 15869, 3042, 644, 30, 50658], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 51, "seek": 11240, "start": 118.28, "end": 120.04, "text": " What is the machine learning part?", "tokens": [50658, 708, 307, 264, 3479, 2539, 644, 30, 50746], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 52, "seek": 11240, "start": 120.04, "end": 123.0, "text": " I like to decompose it into its fundamental compositions,", "tokens": [50746, 286, 411, 281, 22867, 541, 309, 666, 1080, 8088, 43401, 11, 50894], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 53, "seek": 11240, "start": 123.0, "end": 124.24000000000001, "text": " so competition.", "tokens": [50894, 370, 6211, 13, 50956], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 54, "seek": 11240, "start": 124.24000000000001, "end": 126.28, "text": " And the first one that I want to talk about", "tokens": [50956, 400, 264, 700, 472, 300, 286, 528, 281, 751, 466, 51058], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 55, "seek": 11240, "start": 126.28, "end": 130.28, "text": " is zero-knowledge cryptography, or called zero-knowledge.", "tokens": [51058, 307, 4018, 12, 15869, 3042, 9844, 5820, 11, 420, 1219, 4018, 12, 15869, 3042, 13, 51258], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 56, "seek": 11240, "start": 130.28, "end": 132.72, "text": " I don't know if many of you understand the reference that", "tokens": [51258, 286, 500, 380, 458, 498, 867, 295, 291, 1223, 264, 6408, 300, 51380], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 57, "seek": 11240, "start": 132.72, "end": 135.32, "text": " or get the reference that I put here, which is Waldo,", "tokens": [51380, 420, 483, 264, 6408, 300, 286, 829, 510, 11, 597, 307, 9707, 2595, 11, 51510], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 58, "seek": 11240, "start": 135.32, "end": 136.32, "text": " finding Waldo.", "tokens": [51510, 5006, 9707, 2595, 13, 51560], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 59, "seek": 11240, "start": 136.32, "end": 140.16, "text": " Waldo essentially is a character or one specific analogy,", "tokens": [51560, 9707, 2595, 4476, 307, 257, 2517, 420, 472, 2685, 21663, 11, 51752], "temperature": 0.0, "avg_logprob": -0.21113673000470967, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.006096528843045235}, {"id": 60, "seek": 14016, "start": 140.2, "end": 142.32, "text": " which is very simple, very easy, very friendly", "tokens": [50366, 597, 307, 588, 2199, 11, 588, 1858, 11, 588, 9208, 50472], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 61, "seek": 14016, "start": 142.32, "end": 144.28, "text": " to explain what zero-knowledge cryptography is", "tokens": [50472, 281, 2903, 437, 4018, 12, 15869, 3042, 9844, 5820, 307, 50570], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 62, "seek": 14016, "start": 144.28, "end": 146.32, "text": " to people who have never heard about it.", "tokens": [50570, 281, 561, 567, 362, 1128, 2198, 466, 309, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 63, "seek": 14016, "start": 146.32, "end": 148.76, "text": " Because there is this essentially like poster", "tokens": [50672, 1436, 456, 307, 341, 4476, 411, 17171, 50794], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 64, "seek": 14016, "start": 148.76, "end": 151.07999999999998, "text": " that many people know where there's just lots of characters", "tokens": [50794, 300, 867, 561, 458, 689, 456, 311, 445, 3195, 295, 4342, 50910], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 65, "seek": 14016, "start": 151.07999999999998, "end": 153.96, "text": " which are in the city, and there's one Waldo.", "tokens": [50910, 597, 366, 294, 264, 2307, 11, 293, 456, 311, 472, 9707, 2595, 13, 51054], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 66, "seek": 14016, "start": 153.96, "end": 156.96, "text": " And it takes some amount of time to essentially find the Waldo,", "tokens": [51054, 400, 309, 2516, 512, 2372, 295, 565, 281, 4476, 915, 264, 9707, 2595, 11, 51204], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 67, "seek": 14016, "start": 156.96, "end": 159.72, "text": " and that's like the challenge of these specific games.", "tokens": [51204, 293, 300, 311, 411, 264, 3430, 295, 613, 2685, 2813, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 68, "seek": 14016, "start": 159.72, "end": 161.84, "text": " And so there is one specific analogy", "tokens": [51342, 400, 370, 456, 307, 472, 2685, 21663, 51448], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 69, "seek": 14016, "start": 161.84, "end": 164.35999999999999, "text": " which allows people to explain what zero-knowledge is,", "tokens": [51448, 597, 4045, 561, 281, 2903, 437, 4018, 12, 15869, 3042, 307, 11, 51574], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 70, "seek": 14016, "start": 164.35999999999999, "end": 170.12, "text": " which is that if I put a bigger like white paper", "tokens": [51574, 597, 307, 300, 498, 286, 829, 257, 3801, 411, 2418, 3035, 51862], "temperature": 0.0, "avg_logprob": -0.15539552153443262, "compression_ratio": 1.8383838383838385, "no_speech_prob": 0.020016619935631752}, {"id": 71, "seek": 17012, "start": 170.24, "end": 173.4, "text": " on top of the poster, defining Waldo game,", "tokens": [50370, 322, 1192, 295, 264, 17171, 11, 17827, 9707, 2595, 1216, 11, 50528], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 72, "seek": 17012, "start": 173.4, "end": 176.12, "text": " and if I create a little tiny cutout for the head of Waldo,", "tokens": [50528, 293, 498, 286, 1884, 257, 707, 5870, 1723, 346, 337, 264, 1378, 295, 9707, 2595, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 73, "seek": 17012, "start": 176.12, "end": 179.52, "text": " and I place the cutout just on top of Waldo's head,", "tokens": [50664, 293, 286, 1081, 264, 1723, 346, 445, 322, 1192, 295, 9707, 2595, 311, 1378, 11, 50834], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 74, "seek": 17012, "start": 179.52, "end": 183.96, "text": " but covering the entire game itself, or the poster itself,", "tokens": [50834, 457, 10322, 264, 2302, 1216, 2564, 11, 420, 264, 17171, 2564, 11, 51056], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 75, "seek": 17012, "start": 183.96, "end": 186.96, "text": " then I'm able to prove to anyone that I know where Waldo is", "tokens": [51056, 550, 286, 478, 1075, 281, 7081, 281, 2878, 300, 286, 458, 689, 9707, 2595, 307, 51206], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 76, "seek": 17012, "start": 186.96, "end": 189.4, "text": " without revealing his location within the poster.", "tokens": [51206, 1553, 23983, 702, 4914, 1951, 264, 17171, 13, 51328], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 77, "seek": 17012, "start": 189.4, "end": 190.68, "text": " So this is like the good analogy", "tokens": [51328, 407, 341, 307, 411, 264, 665, 21663, 51392], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 78, "seek": 17012, "start": 190.68, "end": 192.64000000000001, "text": " for explaining what zero-knowledge cryptography is,", "tokens": [51392, 337, 13468, 437, 4018, 12, 15869, 3042, 9844, 5820, 307, 11, 51490], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 79, "seek": 17012, "start": 192.64000000000001, "end": 195.6, "text": " because I'm able to prove things that I know", "tokens": [51490, 570, 286, 478, 1075, 281, 7081, 721, 300, 286, 458, 51638], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 80, "seek": 17012, "start": 195.6, "end": 197.88, "text": " to someone else, to an outside observer,", "tokens": [51638, 281, 1580, 1646, 11, 281, 364, 2380, 27878, 11, 51752], "temperature": 0.0, "avg_logprob": -0.12405768562765683, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0029347084928303957}, {"id": 81, "seek": 19788, "start": 197.88, "end": 200.35999999999999, "text": " without them learning everything", "tokens": [50364, 1553, 552, 2539, 1203, 50488], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 82, "seek": 19788, "start": 200.35999999999999, "end": 201.79999999999998, "text": " about the things I'm making a proof of.", "tokens": [50488, 466, 264, 721, 286, 478, 1455, 257, 8177, 295, 13, 50560], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 83, "seek": 19788, "start": 201.79999999999998, "end": 204.44, "text": " I can selectively prove specific statements", "tokens": [50560, 286, 393, 3048, 3413, 7081, 2685, 12363, 50692], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 84, "seek": 19788, "start": 204.44, "end": 205.96, "text": " because I have information,", "tokens": [50692, 570, 286, 362, 1589, 11, 50768], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 85, "seek": 19788, "start": 205.96, "end": 207.88, "text": " but I don't have to reveal everything", "tokens": [50768, 457, 286, 500, 380, 362, 281, 10658, 1203, 50864], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 86, "seek": 19788, "start": 207.88, "end": 209.35999999999999, "text": " in order to be able to prove that statement.", "tokens": [50864, 294, 1668, 281, 312, 1075, 281, 7081, 300, 5629, 13, 50938], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 87, "seek": 19788, "start": 209.35999999999999, "end": 212.24, "text": " So this is like a good analogy to explain DK.", "tokens": [50938, 407, 341, 307, 411, 257, 665, 21663, 281, 2903, 31934, 13, 51082], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 88, "seek": 19788, "start": 212.24, "end": 215.51999999999998, "text": " So some of the properties that zero-knowledge cryptography has.", "tokens": [51082, 407, 512, 295, 264, 7221, 300, 4018, 12, 15869, 3042, 9844, 5820, 575, 13, 51246], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 89, "seek": 19788, "start": 215.51999999999998, "end": 217.6, "text": " So the first one, which I think is the most important one,", "tokens": [51246, 407, 264, 700, 472, 11, 597, 286, 519, 307, 264, 881, 1021, 472, 11, 51350], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 90, "seek": 19788, "start": 217.6, "end": 219.2, "text": " especially in the context of blockchain,", "tokens": [51350, 2318, 294, 264, 4319, 295, 17176, 11, 51430], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 91, "seek": 19788, "start": 219.2, "end": 222.8, "text": " I saw that many of you were like in previous presentations", "tokens": [51430, 286, 1866, 300, 867, 295, 291, 645, 411, 294, 3894, 18964, 51610], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 92, "seek": 19788, "start": 222.8, "end": 224.72, "text": " of this specific group.", "tokens": [51610, 295, 341, 2685, 1594, 13, 51706], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 93, "seek": 19788, "start": 224.72, "end": 226.12, "text": " I saw that there's a few crypto people", "tokens": [51706, 286, 1866, 300, 456, 311, 257, 1326, 17240, 561, 51776], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 94, "seek": 19788, "start": 226.12, "end": 227.24, "text": " that were talking about different things.", "tokens": [51776, 300, 645, 1417, 466, 819, 721, 13, 51832], "temperature": 0.0, "avg_logprob": -0.138720524938483, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0007321535958908498}, {"id": 95, "seek": 22724, "start": 227.24, "end": 229.92000000000002, "text": " I'm sure that came along a few times.", "tokens": [50364, 286, 478, 988, 300, 1361, 2051, 257, 1326, 1413, 13, 50498], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 96, "seek": 22724, "start": 229.92000000000002, "end": 231.96, "text": " So succinct list essentially just means", "tokens": [50498, 407, 21578, 5460, 1329, 4476, 445, 1355, 50600], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 97, "seek": 22724, "start": 231.96, "end": 235.32000000000002, "text": " that in order to verify a proof of a statement,", "tokens": [50600, 300, 294, 1668, 281, 16888, 257, 8177, 295, 257, 5629, 11, 50768], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 98, "seek": 22724, "start": 235.32000000000002, "end": 237.68, "text": " it's a lot less computationally expensive", "tokens": [50768, 309, 311, 257, 688, 1570, 24903, 379, 5124, 50886], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 99, "seek": 22724, "start": 237.68, "end": 239.16, "text": " or a lot less expensive", "tokens": [50886, 420, 257, 688, 1570, 5124, 50960], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 100, "seek": 22724, "start": 239.16, "end": 242.16, "text": " than to actually prove the computation", "tokens": [50960, 813, 281, 767, 7081, 264, 24903, 51110], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 101, "seek": 22724, "start": 242.16, "end": 244.52, "text": " or to just perform the computation yourself, right?", "tokens": [51110, 420, 281, 445, 2042, 264, 24903, 1803, 11, 558, 30, 51228], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 102, "seek": 22724, "start": 244.52, "end": 247.60000000000002, "text": " So essentially verifying that I know where Waldo is,", "tokens": [51228, 407, 4476, 1306, 5489, 300, 286, 458, 689, 9707, 2595, 307, 11, 51382], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 103, "seek": 22724, "start": 247.60000000000002, "end": 248.68, "text": " as an outside observer,", "tokens": [51382, 382, 364, 2380, 27878, 11, 51436], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 104, "seek": 22724, "start": 248.68, "end": 251.32000000000002, "text": " it's a lot easier than me finding Waldo myself.", "tokens": [51436, 309, 311, 257, 688, 3571, 813, 385, 5006, 9707, 2595, 2059, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 105, "seek": 22724, "start": 251.32000000000002, "end": 253.28, "text": " So this is really important in the context of blockchains", "tokens": [51568, 407, 341, 307, 534, 1021, 294, 264, 4319, 295, 3461, 339, 2315, 51666], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 106, "seek": 22724, "start": 253.28, "end": 255.16000000000003, "text": " because for example, for scalability solutions,", "tokens": [51666, 570, 337, 1365, 11, 337, 15664, 2310, 6547, 11, 51760], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 107, "seek": 22724, "start": 255.16000000000003, "end": 257.08, "text": " instead of everyone having to re-execute", "tokens": [51760, 2602, 295, 1518, 1419, 281, 319, 12, 3121, 3045, 1169, 51856], "temperature": 0.0, "avg_logprob": -0.10890713395743534, "compression_ratio": 1.7928802588996764, "no_speech_prob": 0.0010321656009182334}, {"id": 108, "seek": 25708, "start": 257.08, "end": 258.71999999999997, "text": " the same transactions in a block,", "tokens": [50364, 264, 912, 16856, 294, 257, 3461, 11, 50446], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 109, "seek": 25708, "start": 258.71999999999997, "end": 260.0, "text": " I can just verify a proof", "tokens": [50446, 286, 393, 445, 16888, 257, 8177, 50510], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 110, "seek": 25708, "start": 260.0, "end": 261.28, "text": " and I can just update my state", "tokens": [50510, 293, 286, 393, 445, 5623, 452, 1785, 50574], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 111, "seek": 25708, "start": 261.28, "end": 263.76, "text": " without having to secure it myself, for example.", "tokens": [50574, 1553, 1419, 281, 7144, 309, 2059, 11, 337, 1365, 13, 50698], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 112, "seek": 25708, "start": 263.76, "end": 265.64, "text": " So this property is important for the kid", "tokens": [50698, 407, 341, 4707, 307, 1021, 337, 264, 1636, 50792], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 113, "seek": 25708, "start": 265.64, "end": 267.71999999999997, "text": " because it allows us to very easily,", "tokens": [50792, 570, 309, 4045, 505, 281, 588, 3612, 11, 50896], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 114, "seek": 25708, "start": 267.71999999999997, "end": 270.44, "text": " computationally easily verify things", "tokens": [50896, 24903, 379, 3612, 16888, 721, 51032], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 115, "seek": 25708, "start": 270.44, "end": 272.0, "text": " without having to do computation ourselves.", "tokens": [51032, 1553, 1419, 281, 360, 24903, 4175, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 116, "seek": 25708, "start": 272.0, "end": 273.59999999999997, "text": " This is a really important property.", "tokens": [51110, 639, 307, 257, 534, 1021, 4707, 13, 51190], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 117, "seek": 25708, "start": 273.59999999999997, "end": 277.24, "text": " The second one, arguably the one that is most known for,", "tokens": [51190, 440, 1150, 472, 11, 26771, 264, 472, 300, 307, 881, 2570, 337, 11, 51372], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 118, "seek": 25708, "start": 277.24, "end": 278.2, "text": " is correctness.", "tokens": [51372, 307, 3006, 1287, 13, 51420], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 119, "seek": 25708, "start": 278.2, "end": 279.64, "text": " So correctness essentially means", "tokens": [51420, 407, 3006, 1287, 4476, 1355, 51492], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 120, "seek": 25708, "start": 279.64, "end": 282.96, "text": " that I can have almost 100% certainty", "tokens": [51492, 300, 286, 393, 362, 1920, 2319, 4, 27022, 51658], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 121, "seek": 25708, "start": 282.96, "end": 286.4, "text": " that this statement that I'm proving is correct, right?", "tokens": [51658, 300, 341, 5629, 300, 286, 478, 27221, 307, 3006, 11, 558, 30, 51830], "temperature": 0.0, "avg_logprob": -0.12540135070355268, "compression_ratio": 1.7926421404682273, "no_speech_prob": 0.0029347732197493315}, {"id": 122, "seek": 28640, "start": 286.44, "end": 287.67999999999995, "text": " That I cannot lie.", "tokens": [50366, 663, 286, 2644, 4544, 13, 50428], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 123, "seek": 28640, "start": 287.67999999999995, "end": 289.84, "text": " There is no way that I as a prover", "tokens": [50428, 821, 307, 572, 636, 300, 286, 382, 257, 447, 331, 50536], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 124, "seek": 28640, "start": 289.84, "end": 292.0, "text": " can lie to a verifier", "tokens": [50536, 393, 4544, 281, 257, 1306, 9902, 50644], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 125, "seek": 28640, "start": 292.0, "end": 295.0, "text": " unless if the cryptography is sound in this case.", "tokens": [50644, 5969, 498, 264, 9844, 5820, 307, 1626, 294, 341, 1389, 13, 50794], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 126, "seek": 28640, "start": 295.0, "end": 296.52, "text": " There's two specific properties", "tokens": [50794, 821, 311, 732, 2685, 7221, 50870], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 127, "seek": 28640, "start": 296.52, "end": 298.03999999999996, "text": " that constitute correctness.", "tokens": [50870, 300, 41658, 3006, 1287, 13, 50946], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 128, "seek": 28640, "start": 298.03999999999996, "end": 299.44, "text": " One is soundness.", "tokens": [50946, 1485, 307, 1626, 1287, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 129, "seek": 28640, "start": 299.44, "end": 301.76, "text": " So soundness means that I, if I'm a prover,", "tokens": [51016, 407, 1626, 1287, 1355, 300, 286, 11, 498, 286, 478, 257, 447, 331, 11, 51132], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 130, "seek": 28640, "start": 301.76, "end": 304.56, "text": " someone making this claim, someone making a statement,", "tokens": [51132, 1580, 1455, 341, 3932, 11, 1580, 1455, 257, 5629, 11, 51272], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 131, "seek": 28640, "start": 304.56, "end": 308.71999999999997, "text": " I'm not able to fool a verifier with invalid proof", "tokens": [51272, 286, 478, 406, 1075, 281, 7979, 257, 1306, 9902, 365, 34702, 8177, 51480], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 132, "seek": 28640, "start": 308.71999999999997, "end": 311.03999999999996, "text": " and completeness is another property", "tokens": [51480, 293, 1557, 15264, 307, 1071, 4707, 51596], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 133, "seek": 28640, "start": 311.03999999999996, "end": 314.28, "text": " where I'm not essentially able to create a valid proof", "tokens": [51596, 689, 286, 478, 406, 4476, 1075, 281, 1884, 257, 7363, 8177, 51758], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 134, "seek": 28640, "start": 314.28, "end": 315.21999999999997, "text": " unless I know the truth.", "tokens": [51758, 5969, 286, 458, 264, 3494, 13, 51805], "temperature": 0.0, "avg_logprob": -0.10403884346805402, "compression_ratio": 1.83984375, "no_speech_prob": 0.004264320246875286}, {"id": 135, "seek": 31522, "start": 315.22, "end": 316.18, "text": " If I don't know the truth,", "tokens": [50364, 759, 286, 500, 380, 458, 264, 3494, 11, 50412], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 136, "seek": 31522, "start": 316.18, "end": 320.38000000000005, "text": " I cannot make a valid proof as a prover.", "tokens": [50412, 286, 2644, 652, 257, 7363, 8177, 382, 257, 447, 331, 13, 50622], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 137, "seek": 31522, "start": 320.38000000000005, "end": 323.46000000000004, "text": " And the third one, which is name after, zero knowledge,", "tokens": [50622, 400, 264, 2636, 472, 11, 597, 307, 1315, 934, 11, 4018, 3601, 11, 50776], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 138, "seek": 31522, "start": 323.46000000000004, "end": 326.86, "text": " is this property where I can hide parts of the statement.", "tokens": [50776, 307, 341, 4707, 689, 286, 393, 6479, 3166, 295, 264, 5629, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 139, "seek": 31522, "start": 326.86, "end": 328.66, "text": " For example, let's say that I have,", "tokens": [50946, 1171, 1365, 11, 718, 311, 584, 300, 286, 362, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 140, "seek": 31522, "start": 328.66, "end": 329.82000000000005, "text": " I don't know, this is a good example.", "tokens": [51036, 286, 500, 380, 458, 11, 341, 307, 257, 665, 1365, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 141, "seek": 31522, "start": 329.82000000000005, "end": 331.26000000000005, "text": " Let's say I have my passport.", "tokens": [51094, 961, 311, 584, 286, 362, 452, 24694, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 142, "seek": 31522, "start": 331.26000000000005, "end": 334.38000000000005, "text": " So I have my name, my nationality, my date of birth,", "tokens": [51166, 407, 286, 362, 452, 1315, 11, 452, 4048, 507, 11, 452, 4002, 295, 3965, 11, 51322], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 143, "seek": 31522, "start": 334.38000000000005, "end": 336.98, "text": " where I'm from, which country I was born in,", "tokens": [51322, 689, 286, 478, 490, 11, 597, 1941, 286, 390, 4232, 294, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 144, "seek": 31522, "start": 336.98, "end": 339.1, "text": " for example, the place of birth.", "tokens": [51452, 337, 1365, 11, 264, 1081, 295, 3965, 13, 51558], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 145, "seek": 31522, "start": 339.1, "end": 340.46000000000004, "text": " So something that would be useful", "tokens": [51558, 407, 746, 300, 576, 312, 4420, 51626], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 146, "seek": 31522, "start": 340.46000000000004, "end": 343.22, "text": " or like something that would constitute a zero knowledge proof", "tokens": [51626, 420, 411, 746, 300, 576, 41658, 257, 4018, 3601, 8177, 51764], "temperature": 0.0, "avg_logprob": -0.1252009963989258, "compression_ratio": 1.8256227758007118, "no_speech_prob": 0.0003625638782978058}, {"id": 147, "seek": 34322, "start": 343.22, "end": 344.5, "text": " is that I can make the statement", "tokens": [50364, 307, 300, 286, 393, 652, 264, 5629, 50428], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 148, "seek": 34322, "start": 344.5, "end": 347.5, "text": " that my age is over 18 years old", "tokens": [50428, 300, 452, 3205, 307, 670, 2443, 924, 1331, 50578], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 149, "seek": 34322, "start": 347.5, "end": 349.46000000000004, "text": " without revealing to anyone my age,", "tokens": [50578, 1553, 23983, 281, 2878, 452, 3205, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 150, "seek": 34322, "start": 349.46000000000004, "end": 353.06, "text": " but anyone can just verify that I'm actually over 18.", "tokens": [50676, 457, 2878, 393, 445, 16888, 300, 286, 478, 767, 670, 2443, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 151, "seek": 34322, "start": 353.06, "end": 354.54, "text": " The way that this is actually implemented", "tokens": [50856, 440, 636, 300, 341, 307, 767, 12270, 50930], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 152, "seek": 34322, "start": 354.54, "end": 355.82000000000005, "text": " is that within a zero knowledge proof,", "tokens": [50930, 307, 300, 1951, 257, 4018, 3601, 8177, 11, 50994], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 153, "seek": 34322, "start": 355.82000000000005, "end": 359.1, "text": " I can verify a signature from some issuing body", "tokens": [50994, 286, 393, 16888, 257, 13397, 490, 512, 43214, 1772, 51158], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 154, "seek": 34322, "start": 359.1, "end": 360.16, "text": " like the government.", "tokens": [51158, 411, 264, 2463, 13, 51211], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 155, "seek": 34322, "start": 360.16, "end": 362.02000000000004, "text": " And then I can make a statement that like A,", "tokens": [51211, 400, 550, 286, 393, 652, 257, 5629, 300, 411, 316, 11, 51304], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 156, "seek": 34322, "start": 362.02000000000004, "end": 363.82000000000005, "text": " this age, which was attested to", "tokens": [51304, 341, 3205, 11, 597, 390, 951, 21885, 281, 51394], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 157, "seek": 34322, "start": 363.82000000000005, "end": 367.42, "text": " or essentially committed to by a government is over 18", "tokens": [51394, 420, 4476, 7784, 281, 538, 257, 2463, 307, 670, 2443, 51574], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 158, "seek": 34322, "start": 367.42, "end": 368.78000000000003, "text": " and you don't learn my age.", "tokens": [51574, 293, 291, 500, 380, 1466, 452, 3205, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 159, "seek": 34322, "start": 368.78000000000003, "end": 370.1, "text": " So this is the zero knowledge part", "tokens": [51642, 407, 341, 307, 264, 4018, 3601, 644, 51708], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 160, "seek": 34322, "start": 370.1, "end": 371.94000000000005, "text": " where I'm able to hide parts of the state", "tokens": [51708, 689, 286, 478, 1075, 281, 6479, 3166, 295, 264, 1785, 51800], "temperature": 0.0, "avg_logprob": -0.1140329413218041, "compression_ratio": 1.8951048951048952, "no_speech_prob": 0.0019877091981470585}, {"id": 161, "seek": 37194, "start": 371.94, "end": 374.74, "text": " that I'm making a statement about or proof about", "tokens": [50364, 300, 286, 478, 1455, 257, 5629, 466, 420, 8177, 466, 50504], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 162, "seek": 37194, "start": 374.74, "end": 377.78, "text": " according to some constraint or some statement, right?", "tokens": [50504, 4650, 281, 512, 25534, 420, 512, 5629, 11, 558, 30, 50656], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 163, "seek": 37194, "start": 377.78, "end": 379.58, "text": " Like I can say greater than, less than,", "tokens": [50656, 1743, 286, 393, 584, 5044, 813, 11, 1570, 813, 11, 50746], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 164, "seek": 37194, "start": 379.58, "end": 383.02, "text": " equal to a bunch of other properties", "tokens": [50746, 2681, 281, 257, 3840, 295, 661, 7221, 50918], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 165, "seek": 37194, "start": 383.02, "end": 384.54, "text": " that I think can put.", "tokens": [50918, 300, 286, 519, 393, 829, 13, 50994], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 166, "seek": 37194, "start": 384.54, "end": 387.02, "text": " So the second part of the statement", "tokens": [50994, 407, 264, 1150, 644, 295, 264, 5629, 51118], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 167, "seek": 37194, "start": 387.02, "end": 388.34, "text": " of zero knowledge machine learning,", "tokens": [51118, 295, 4018, 3601, 3479, 2539, 11, 51184], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 168, "seek": 37194, "start": 388.34, "end": 389.42, "text": " this machine learning, right?", "tokens": [51184, 341, 3479, 2539, 11, 558, 30, 51238], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 169, "seek": 37194, "start": 389.42, "end": 392.02, "text": " I think that one is much more familiar to most of you", "tokens": [51238, 286, 519, 300, 472, 307, 709, 544, 4963, 281, 881, 295, 291, 51368], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 170, "seek": 37194, "start": 392.02, "end": 395.1, "text": " since it's been generating such a buzz everywhere,", "tokens": [51368, 1670, 309, 311, 668, 17746, 1270, 257, 13036, 5315, 11, 51522], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 171, "seek": 37194, "start": 395.1, "end": 396.86, "text": " like machine learning through a generative AI,", "tokens": [51522, 411, 3479, 2539, 807, 257, 1337, 1166, 7318, 11, 51610], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 172, "seek": 37194, "start": 396.86, "end": 399.06, "text": " like in things like ChagYPT or Dali", "tokens": [51610, 411, 294, 721, 411, 761, 559, 56, 47, 51, 420, 413, 5103, 51720], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 173, "seek": 37194, "start": 399.06, "end": 401.3, "text": " or a lot of generative AI models", "tokens": [51720, 420, 257, 688, 295, 1337, 1166, 7318, 5245, 51832], "temperature": 0.0, "avg_logprob": -0.20958198385035737, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.0008968775509856641}, {"id": 174, "seek": 40130, "start": 401.34000000000003, "end": 403.94, "text": " or natural language processing, categorizing models", "tokens": [50366, 420, 3303, 2856, 9007, 11, 19250, 3319, 5245, 50496], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 175, "seek": 40130, "start": 403.94, "end": 406.18, "text": " like robots, the machine learning essentially,", "tokens": [50496, 411, 14733, 11, 264, 3479, 2539, 4476, 11, 50608], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 176, "seek": 40130, "start": 406.18, "end": 409.54, "text": " the way that I think about it is that it's a tool", "tokens": [50608, 264, 636, 300, 286, 519, 466, 309, 307, 300, 309, 311, 257, 2290, 50776], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 177, "seek": 40130, "start": 409.54, "end": 413.34000000000003, "text": " that allows us to give us not the non-deterministic solutions", "tokens": [50776, 300, 4045, 505, 281, 976, 505, 406, 264, 2107, 12, 49136, 259, 3142, 6547, 50966], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 178, "seek": 40130, "start": 413.34000000000003, "end": 416.22, "text": " or just estimates for short for problems", "tokens": [50966, 420, 445, 20561, 337, 2099, 337, 2740, 51110], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 179, "seek": 40130, "start": 416.22, "end": 418.94, "text": " that don't really have a concrete solution, right?", "tokens": [51110, 300, 500, 380, 534, 362, 257, 9859, 3827, 11, 558, 30, 51246], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 180, "seek": 40130, "start": 418.94, "end": 420.86, "text": " There's usually there's some problems", "tokens": [51246, 821, 311, 2673, 456, 311, 512, 2740, 51342], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 181, "seek": 40130, "start": 420.86, "end": 422.82, "text": " which we can solve algorithmically", "tokens": [51342, 597, 321, 393, 5039, 9284, 984, 51440], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 182, "seek": 40130, "start": 422.82, "end": 424.18, "text": " and we can just have a set of steps", "tokens": [51440, 293, 321, 393, 445, 362, 257, 992, 295, 4439, 51508], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 183, "seek": 40130, "start": 424.18, "end": 427.14, "text": " that we can just execute in order to solve it", "tokens": [51508, 300, 321, 393, 445, 14483, 294, 1668, 281, 5039, 309, 51656], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 184, "seek": 40130, "start": 427.14, "end": 429.62, "text": " and we will have a perfect solution every time.", "tokens": [51656, 293, 321, 486, 362, 257, 2176, 3827, 633, 565, 13, 51780], "temperature": 0.0, "avg_logprob": -0.13678209800419844, "compression_ratio": 1.7657342657342658, "no_speech_prob": 0.0013044343795627356}, {"id": 185, "seek": 42962, "start": 429.62, "end": 431.9, "text": " In the case of machine learning, however,", "tokens": [50364, 682, 264, 1389, 295, 3479, 2539, 11, 4461, 11, 50478], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 186, "seek": 42962, "start": 431.9, "end": 433.42, "text": " most of the problems that are being solved", "tokens": [50478, 881, 295, 264, 2740, 300, 366, 885, 13041, 50554], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 187, "seek": 42962, "start": 433.42, "end": 435.34000000000003, "text": " by machine learning are not such problems.", "tokens": [50554, 538, 3479, 2539, 366, 406, 1270, 2740, 13, 50650], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 188, "seek": 42962, "start": 435.34000000000003, "end": 436.82, "text": " Therefore, we need to,", "tokens": [50650, 7504, 11, 321, 643, 281, 11, 50724], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 189, "seek": 42962, "start": 436.82, "end": 439.5, "text": " because maybe like the space of solutions is too big", "tokens": [50724, 570, 1310, 411, 264, 1901, 295, 6547, 307, 886, 955, 50858], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 190, "seek": 42962, "start": 439.5, "end": 442.42, "text": " or the space of the steps that we can take is too big,", "tokens": [50858, 420, 264, 1901, 295, 264, 4439, 300, 321, 393, 747, 307, 886, 955, 11, 51004], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 191, "seek": 42962, "start": 442.42, "end": 444.94, "text": " so it's really hard to navigate deterministically,", "tokens": [51004, 370, 309, 311, 534, 1152, 281, 12350, 15957, 20458, 11, 51130], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 192, "seek": 42962, "start": 444.94, "end": 447.06, "text": " then we just have this sort of juristic.", "tokens": [51130, 550, 321, 445, 362, 341, 1333, 295, 12721, 3142, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 193, "seek": 42962, "start": 447.06, "end": 450.62, "text": " A juristic essentially is a good enough approximation", "tokens": [51236, 316, 12721, 3142, 4476, 307, 257, 665, 1547, 28023, 51414], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 194, "seek": 42962, "start": 450.62, "end": 453.46, "text": " to the real solution which we can work with", "tokens": [51414, 281, 264, 957, 3827, 597, 321, 393, 589, 365, 51556], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 195, "seek": 42962, "start": 453.46, "end": 455.82, "text": " and which has some form of accuracy, right?", "tokens": [51556, 293, 597, 575, 512, 1254, 295, 14170, 11, 558, 30, 51674], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 196, "seek": 42962, "start": 455.82, "end": 457.54, "text": " So in the context of machine learning,", "tokens": [51674, 407, 294, 264, 4319, 295, 3479, 2539, 11, 51760], "temperature": 0.0, "avg_logprob": -0.12321985899096857, "compression_ratio": 1.8122866894197953, "no_speech_prob": 0.0009109389502555132}, {"id": 197, "seek": 45754, "start": 457.54, "end": 460.3, "text": " we have some sort of juristic for some problem.", "tokens": [50364, 321, 362, 512, 1333, 295, 12721, 3142, 337, 512, 1154, 13, 50502], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 198, "seek": 45754, "start": 460.3, "end": 462.54, "text": " So let's say I want to categorize", "tokens": [50502, 407, 718, 311, 584, 286, 528, 281, 19250, 1125, 50614], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 199, "seek": 45754, "start": 462.54, "end": 466.06, "text": " whether an image that I see is the image of a dog or a cat.", "tokens": [50614, 1968, 364, 3256, 300, 286, 536, 307, 264, 3256, 295, 257, 3000, 420, 257, 3857, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 200, "seek": 45754, "start": 466.06, "end": 468.34000000000003, "text": " This I can train a machine learning model", "tokens": [50790, 639, 286, 393, 3847, 257, 3479, 2539, 2316, 50904], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 201, "seek": 45754, "start": 468.34000000000003, "end": 469.98, "text": " to essentially solve this task,", "tokens": [50904, 281, 4476, 5039, 341, 5633, 11, 50986], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 202, "seek": 45754, "start": 469.98, "end": 472.3, "text": " but the machine learning model will never be 100% correct.", "tokens": [50986, 457, 264, 3479, 2539, 2316, 486, 1128, 312, 2319, 4, 3006, 13, 51102], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 203, "seek": 45754, "start": 472.3, "end": 474.46000000000004, "text": " It will have some accuracy, right?", "tokens": [51102, 467, 486, 362, 512, 14170, 11, 558, 30, 51210], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 204, "seek": 45754, "start": 474.46000000000004, "end": 475.78000000000003, "text": " It will have some that will fail on,", "tokens": [51210, 467, 486, 362, 512, 300, 486, 3061, 322, 11, 51276], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 205, "seek": 45754, "start": 475.78000000000003, "end": 478.94, "text": " it will have, most of them it will get correct.", "tokens": [51276, 309, 486, 362, 11, 881, 295, 552, 309, 486, 483, 3006, 13, 51434], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 206, "seek": 45754, "start": 478.94, "end": 481.86, "text": " But essentially the way that the machine learning model works", "tokens": [51434, 583, 4476, 264, 636, 300, 264, 3479, 2539, 2316, 1985, 51580], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 207, "seek": 45754, "start": 481.86, "end": 484.38, "text": " is that it trains on some data.", "tokens": [51580, 307, 300, 309, 16329, 322, 512, 1412, 13, 51706], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 208, "seek": 45754, "start": 484.38, "end": 486.62, "text": " So I feed some data to some model", "tokens": [51706, 407, 286, 3154, 512, 1412, 281, 512, 2316, 51818], "temperature": 0.0, "avg_logprob": -0.12725159171577935, "compression_ratio": 1.905109489051095, "no_speech_prob": 0.002434208756312728}, {"id": 209, "seek": 48662, "start": 486.74, "end": 489.34000000000003, "text": " or to some machine learning algorithm and it gets better.", "tokens": [50370, 420, 281, 512, 3479, 2539, 9284, 293, 309, 2170, 1101, 13, 50500], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 210, "seek": 48662, "start": 489.34000000000003, "end": 491.94, "text": " And this juristic keep getting better and better", "tokens": [50500, 400, 341, 12721, 3142, 1066, 1242, 1101, 293, 1101, 50630], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 211, "seek": 48662, "start": 491.94, "end": 494.06, "text": " for things that it hasn't seen before.", "tokens": [50630, 337, 721, 300, 309, 6132, 380, 1612, 949, 13, 50736], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 212, "seek": 48662, "start": 494.06, "end": 495.82, "text": " It generalizes over the data", "tokens": [50736, 467, 2674, 5660, 670, 264, 1412, 50824], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 213, "seek": 48662, "start": 495.82, "end": 498.54, "text": " and it's able to make essentially like predictions", "tokens": [50824, 293, 309, 311, 1075, 281, 652, 4476, 411, 21264, 50960], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 214, "seek": 48662, "start": 498.54, "end": 501.06, "text": " or classifications or all sorts of things.", "tokens": [50960, 420, 1508, 7833, 420, 439, 7527, 295, 721, 13, 51086], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 215, "seek": 48662, "start": 501.06, "end": 502.3, "text": " So in the case, for example,", "tokens": [51086, 407, 294, 264, 1389, 11, 337, 1365, 11, 51148], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 216, "seek": 48662, "start": 502.3, "end": 505.22, "text": " of foundational models for large language models,", "tokens": [51148, 295, 32195, 5245, 337, 2416, 2856, 5245, 11, 51294], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 217, "seek": 48662, "start": 505.22, "end": 508.62, "text": " they get better at creating like cohesive explanations", "tokens": [51294, 436, 483, 1101, 412, 4084, 411, 43025, 28708, 51464], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 218, "seek": 48662, "start": 508.62, "end": 510.62, "text": " or at reasoning or at mathematics", "tokens": [51464, 420, 412, 21577, 420, 412, 18666, 51564], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 219, "seek": 48662, "start": 510.62, "end": 512.62, "text": " or at all sorts of different things.", "tokens": [51564, 420, 412, 439, 7527, 295, 819, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 220, "seek": 48662, "start": 512.62, "end": 514.38, "text": " And we can have these benchmarks", "tokens": [51664, 400, 321, 393, 362, 613, 43751, 51752], "temperature": 0.0, "avg_logprob": -0.12084221258396055, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.008709525689482689}, {"id": 221, "seek": 51438, "start": 514.38, "end": 516.86, "text": " and with more data, they get better at these benchmarks,", "tokens": [50364, 293, 365, 544, 1412, 11, 436, 483, 1101, 412, 613, 43751, 11, 50488], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 222, "seek": 51438, "start": 516.86, "end": 519.54, "text": " which essentially provide better juristic problems", "tokens": [50488, 597, 4476, 2893, 1101, 12721, 3142, 2740, 50622], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 223, "seek": 51438, "start": 519.54, "end": 521.06, "text": " that we're grading them on.", "tokens": [50622, 300, 321, 434, 35540, 552, 322, 13, 50698], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 224, "seek": 51438, "start": 521.06, "end": 524.18, "text": " So another concept that I want to explain here", "tokens": [50698, 407, 1071, 3410, 300, 286, 528, 281, 2903, 510, 50854], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 225, "seek": 51438, "start": 524.18, "end": 527.14, "text": " is that there's two specific parts within machine learning", "tokens": [50854, 307, 300, 456, 311, 732, 2685, 3166, 1951, 3479, 2539, 51002], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 226, "seek": 51438, "start": 527.14, "end": 530.58, "text": " or two specific things you can do usually,", "tokens": [51002, 420, 732, 2685, 721, 291, 393, 360, 2673, 11, 51174], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 227, "seek": 51438, "start": 530.58, "end": 533.38, "text": " is that when you have a model, you can train a model.", "tokens": [51174, 307, 300, 562, 291, 362, 257, 2316, 11, 291, 393, 3847, 257, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 228, "seek": 51438, "start": 533.38, "end": 536.54, "text": " The act of training a model is the act of creating a function", "tokens": [51314, 440, 605, 295, 3097, 257, 2316, 307, 264, 605, 295, 4084, 257, 2445, 51472], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 229, "seek": 51438, "start": 536.54, "end": 538.86, "text": " which gets actually better and better", "tokens": [51472, 597, 2170, 767, 1101, 293, 1101, 51588], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 230, "seek": 51438, "start": 538.86, "end": 540.82, "text": " at giving you the juristics,", "tokens": [51588, 412, 2902, 291, 264, 12721, 6006, 11, 51686], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 231, "seek": 51438, "start": 540.82, "end": 542.7, "text": " the more data you feed into it, right?", "tokens": [51686, 264, 544, 1412, 291, 3154, 666, 309, 11, 558, 30, 51780], "temperature": 0.0, "avg_logprob": -0.09564536064863205, "compression_ratio": 1.84, "no_speech_prob": 0.00030533186509273946}, {"id": 232, "seek": 54270, "start": 542.7, "end": 544.7800000000001, "text": " So I'm able to update the parameters of this function", "tokens": [50364, 407, 286, 478, 1075, 281, 5623, 264, 9834, 295, 341, 2445, 50468], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 233, "seek": 54270, "start": 544.7800000000001, "end": 547.82, "text": " by learning, this is what learning is, right?", "tokens": [50468, 538, 2539, 11, 341, 307, 437, 2539, 307, 11, 558, 30, 50620], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 234, "seek": 54270, "start": 547.82, "end": 549.34, "text": " I'm updating parameters of a function", "tokens": [50620, 286, 478, 25113, 9834, 295, 257, 2445, 50696], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 235, "seek": 54270, "start": 549.34, "end": 550.98, "text": " in order to get a better jurist.", "tokens": [50696, 294, 1668, 281, 483, 257, 1101, 12721, 468, 13, 50778], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 236, "seek": 54270, "start": 550.98, "end": 552.9000000000001, "text": " And this process is really expensive, right?", "tokens": [50778, 400, 341, 1399, 307, 534, 5124, 11, 558, 30, 50874], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 237, "seek": 54270, "start": 552.9000000000001, "end": 555.82, "text": " It's really hard, you have to co-locate a data center,", "tokens": [50874, 467, 311, 534, 1152, 11, 291, 362, 281, 598, 12, 5842, 473, 257, 1412, 3056, 11, 51020], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 238, "seek": 54270, "start": 555.82, "end": 559.34, "text": " it's running lots of graphics cards in a big place", "tokens": [51020, 309, 311, 2614, 3195, 295, 11837, 5632, 294, 257, 955, 1081, 51196], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 239, "seek": 54270, "start": 559.34, "end": 561.46, "text": " and consuming lots of electricity for months on end", "tokens": [51196, 293, 19867, 3195, 295, 10356, 337, 2493, 322, 917, 51302], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 240, "seek": 54270, "start": 561.46, "end": 563.5, "text": " in order to be able to create something useful", "tokens": [51302, 294, 1668, 281, 312, 1075, 281, 1884, 746, 4420, 51404], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 241, "seek": 54270, "start": 563.5, "end": 565.5400000000001, "text": " or meaningful, this is really expensive.", "tokens": [51404, 420, 10995, 11, 341, 307, 534, 5124, 13, 51506], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 242, "seek": 54270, "start": 565.5400000000001, "end": 567.58, "text": " But the end product is very easy to run.", "tokens": [51506, 583, 264, 917, 1674, 307, 588, 1858, 281, 1190, 13, 51608], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 243, "seek": 54270, "start": 567.58, "end": 569.0200000000001, "text": " So once I've trained this function,", "tokens": [51608, 407, 1564, 286, 600, 8895, 341, 2445, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 244, "seek": 54270, "start": 569.0200000000001, "end": 570.7, "text": " once I have my set of parameters,", "tokens": [51680, 1564, 286, 362, 452, 992, 295, 9834, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 245, "seek": 54270, "start": 570.7, "end": 572.62, "text": " evaluating this function at some input,", "tokens": [51764, 27479, 341, 2445, 412, 512, 4846, 11, 51860], "temperature": 0.0, "avg_logprob": -0.12291766675703365, "compression_ratio": 1.9125, "no_speech_prob": 0.0013884532963857055}, {"id": 246, "seek": 57262, "start": 572.62, "end": 575.34, "text": " is usually inexpensive or very inexpensive", "tokens": [50364, 307, 2673, 28382, 420, 588, 28382, 50500], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 247, "seek": 57262, "start": 575.34, "end": 577.3, "text": " compared to actually training it,", "tokens": [50500, 5347, 281, 767, 3097, 309, 11, 50598], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 248, "seek": 57262, "start": 577.3, "end": 579.5, "text": " several orders of magnitude less.", "tokens": [50598, 2940, 9470, 295, 15668, 1570, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 249, "seek": 57262, "start": 579.5, "end": 581.22, "text": " So these are like the,", "tokens": [50708, 407, 613, 366, 411, 264, 11, 50794], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 250, "seek": 57262, "start": 581.22, "end": 584.3, "text": " how I usually explain machine learning just very briefly.", "tokens": [50794, 577, 286, 2673, 2903, 3479, 2539, 445, 588, 10515, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 251, "seek": 57262, "start": 584.3, "end": 585.7, "text": " And so now I want to get,", "tokens": [50948, 400, 370, 586, 286, 528, 281, 483, 11, 51018], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 252, "seek": 57262, "start": 585.7, "end": 587.66, "text": " what is zero knowledge machine learning, right?", "tokens": [51018, 437, 307, 4018, 3601, 3479, 2539, 11, 558, 30, 51116], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 253, "seek": 57262, "start": 587.66, "end": 589.62, "text": " We have some intuitions from the ZK side,", "tokens": [51116, 492, 362, 512, 16224, 626, 490, 264, 1176, 42, 1252, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 254, "seek": 57262, "start": 589.62, "end": 591.74, "text": " some intuitions from the machine learning side.", "tokens": [51214, 512, 16224, 626, 490, 264, 3479, 2539, 1252, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 255, "seek": 57262, "start": 591.74, "end": 595.7, "text": " Now we can discuss what ZK machine learning can be", "tokens": [51320, 823, 321, 393, 2248, 437, 1176, 42, 3479, 2539, 393, 312, 51518], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 256, "seek": 57262, "start": 595.7, "end": 598.78, "text": " or is within the modern understanding of it.", "tokens": [51518, 420, 307, 1951, 264, 4363, 3701, 295, 309, 13, 51672], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 257, "seek": 57262, "start": 598.78, "end": 600.78, "text": " So essentially what ZKML is,", "tokens": [51672, 407, 4476, 437, 1176, 42, 12683, 307, 11, 51772], "temperature": 0.0, "avg_logprob": -0.1207480239868164, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.000925303203985095}, {"id": 258, "seek": 60078, "start": 600.78, "end": 604.06, "text": " is the creation of zero knowledge proofs", "tokens": [50364, 307, 264, 8016, 295, 4018, 3601, 8177, 82, 50528], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 259, "seek": 60078, "start": 604.06, "end": 605.98, "text": " of machine learning algorithms, right?", "tokens": [50528, 295, 3479, 2539, 14642, 11, 558, 30, 50624], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 260, "seek": 60078, "start": 605.98, "end": 608.4599999999999, "text": " So zero knowledge cryptography allows you", "tokens": [50624, 407, 4018, 3601, 9844, 5820, 4045, 291, 50748], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 261, "seek": 60078, "start": 608.4599999999999, "end": 611.42, "text": " to create proofs of arbitrary computations.", "tokens": [50748, 281, 1884, 8177, 82, 295, 23211, 2807, 763, 13, 50896], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 262, "seek": 60078, "start": 611.42, "end": 614.62, "text": " It can be a proof that I've computed some specific thing.", "tokens": [50896, 467, 393, 312, 257, 8177, 300, 286, 600, 40610, 512, 2685, 551, 13, 51056], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 263, "seek": 60078, "start": 614.62, "end": 617.5, "text": " It can be a proof that some variable is bigger than another.", "tokens": [51056, 467, 393, 312, 257, 8177, 300, 512, 7006, 307, 3801, 813, 1071, 13, 51200], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 264, "seek": 60078, "start": 617.5, "end": 619.3, "text": " Essentially it's making proofs about computation", "tokens": [51200, 23596, 309, 311, 1455, 8177, 82, 466, 24903, 51290], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 265, "seek": 60078, "start": 619.3, "end": 622.14, "text": " and you know the person who's verifying that proof", "tokens": [51290, 293, 291, 458, 264, 954, 567, 311, 1306, 5489, 300, 8177, 51432], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 266, "seek": 60078, "start": 622.14, "end": 624.54, "text": " knows that someone has executed that computation", "tokens": [51432, 3255, 300, 1580, 575, 17577, 300, 24903, 51552], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 267, "seek": 60078, "start": 624.54, "end": 627.86, "text": " on some inputs and has produced some output.", "tokens": [51552, 322, 512, 15743, 293, 575, 7126, 512, 5598, 13, 51718], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 268, "seek": 60078, "start": 627.86, "end": 629.1, "text": " So in the context of machine learning,", "tokens": [51718, 407, 294, 264, 4319, 295, 3479, 2539, 11, 51780], "temperature": 0.0, "avg_logprob": -0.11878617348209504, "compression_ratio": 1.8868613138686132, "no_speech_prob": 0.0011159639107063413}, {"id": 269, "seek": 62910, "start": 629.1, "end": 632.26, "text": " usually you have some input, a function or a model,", "tokens": [50364, 2673, 291, 362, 512, 4846, 11, 257, 2445, 420, 257, 2316, 11, 50522], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 270, "seek": 62910, "start": 632.26, "end": 633.5, "text": " and then some output, right?", "tokens": [50522, 293, 550, 512, 5598, 11, 558, 30, 50584], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 271, "seek": 62910, "start": 633.5, "end": 635.46, "text": " So if it's let's say like charge EPT,", "tokens": [50584, 407, 498, 309, 311, 718, 311, 584, 411, 4602, 25330, 51, 11, 50682], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 272, "seek": 62910, "start": 635.46, "end": 637.7, "text": " I have a prompt which I feed into the model.", "tokens": [50682, 286, 362, 257, 12391, 597, 286, 3154, 666, 264, 2316, 13, 50794], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 273, "seek": 62910, "start": 637.7, "end": 641.38, "text": " The model just takes that prompt and evaluates its model", "tokens": [50794, 440, 2316, 445, 2516, 300, 12391, 293, 6133, 1024, 1080, 2316, 50978], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 274, "seek": 62910, "start": 641.38, "end": 642.7, "text": " and it gives you an output,", "tokens": [50978, 293, 309, 2709, 291, 364, 5598, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 275, "seek": 62910, "start": 642.7, "end": 644.66, "text": " which is the thing that you then read in the end,", "tokens": [51044, 597, 307, 264, 551, 300, 291, 550, 1401, 294, 264, 917, 11, 51142], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 276, "seek": 62910, "start": 644.66, "end": 646.0600000000001, "text": " which is the result, right?", "tokens": [51142, 597, 307, 264, 1874, 11, 558, 30, 51212], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 277, "seek": 62910, "start": 646.0600000000001, "end": 648.78, "text": " So zero knowledge machine learning would be the art", "tokens": [51212, 407, 4018, 3601, 3479, 2539, 576, 312, 264, 1523, 51348], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 278, "seek": 62910, "start": 648.78, "end": 653.0600000000001, "text": " or act of creating a proof that I have fed an input", "tokens": [51348, 420, 605, 295, 4084, 257, 8177, 300, 286, 362, 4636, 364, 4846, 51562], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 279, "seek": 62910, "start": 653.0600000000001, "end": 654.94, "text": " to a model and I've produced some output.", "tokens": [51562, 281, 257, 2316, 293, 286, 600, 7126, 512, 5598, 13, 51656], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 280, "seek": 62910, "start": 654.94, "end": 658.0600000000001, "text": " And I can verify that this output came from a model", "tokens": [51656, 400, 286, 393, 16888, 300, 341, 5598, 1361, 490, 257, 2316, 51812], "temperature": 0.0, "avg_logprob": -0.12927691141764322, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.001324947108514607}, {"id": 281, "seek": 65806, "start": 658.0999999999999, "end": 661.6999999999999, "text": " without essentially having to evaluate this myself personally.", "tokens": [50366, 1553, 4476, 1419, 281, 13059, 341, 2059, 5665, 13, 50546], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 282, "seek": 65806, "start": 661.6999999999999, "end": 663.9, "text": " I just know that this comes from a model", "tokens": [50546, 286, 445, 458, 300, 341, 1487, 490, 257, 2316, 50656], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 283, "seek": 65806, "start": 663.9, "end": 667.0999999999999, "text": " because I have a cryptographic proof that this indeed happened.", "tokens": [50656, 570, 286, 362, 257, 9844, 12295, 8177, 300, 341, 6451, 2011, 13, 50816], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 284, "seek": 65806, "start": 667.0999999999999, "end": 669.3, "text": " So something that many people in the space", "tokens": [50816, 407, 746, 300, 867, 561, 294, 264, 1901, 50926], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 285, "seek": 65806, "start": 669.3, "end": 673.14, "text": " actually use as a good analogy that accountable AI, right?", "tokens": [50926, 767, 764, 382, 257, 665, 21663, 300, 18024, 7318, 11, 558, 30, 51118], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 286, "seek": 65806, "start": 673.14, "end": 675.38, "text": " Usually AI or machine learning models,", "tokens": [51118, 11419, 7318, 420, 3479, 2539, 5245, 11, 51230], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 287, "seek": 65806, "start": 675.38, "end": 677.54, "text": " you don't know that they're actually correct", "tokens": [51230, 291, 500, 380, 458, 300, 436, 434, 767, 3006, 51338], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 288, "seek": 65806, "start": 677.54, "end": 680.18, "text": " unless you run them yourself, right?", "tokens": [51338, 5969, 291, 1190, 552, 1803, 11, 558, 30, 51470], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 289, "seek": 65806, "start": 680.18, "end": 682.26, "text": " If you run it yourself on your own local machine", "tokens": [51470, 759, 291, 1190, 309, 1803, 322, 428, 1065, 2654, 3479, 51574], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 290, "seek": 65806, "start": 682.26, "end": 684.9399999999999, "text": " or your own data center which you have privileged access to,", "tokens": [51574, 420, 428, 1065, 1412, 3056, 597, 291, 362, 25293, 2105, 281, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 291, "seek": 65806, "start": 684.9399999999999, "end": 687.5799999999999, "text": " then you know that you've run the right thing.", "tokens": [51708, 550, 291, 458, 300, 291, 600, 1190, 264, 558, 551, 13, 51840], "temperature": 0.0, "avg_logprob": -0.09172002049802824, "compression_ratio": 1.799342105263158, "no_speech_prob": 0.0006461780285462737}, {"id": 292, "seek": 68758, "start": 687.58, "end": 690.6600000000001, "text": " But let's say that you're using some form of server or API, right?", "tokens": [50364, 583, 718, 311, 584, 300, 291, 434, 1228, 512, 1254, 295, 7154, 420, 9362, 11, 558, 30, 50518], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 293, "seek": 68758, "start": 690.6600000000001, "end": 693.86, "text": " If I go to chat GPT, like the website,", "tokens": [50518, 759, 286, 352, 281, 5081, 26039, 51, 11, 411, 264, 3144, 11, 50678], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 294, "seek": 68758, "start": 693.86, "end": 696.94, "text": " how do I know that OpenAI is actually serving me", "tokens": [50678, 577, 360, 286, 458, 300, 7238, 48698, 307, 767, 8148, 385, 50832], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 295, "seek": 68758, "start": 696.94, "end": 697.86, "text": " the right model?", "tokens": [50832, 264, 558, 2316, 30, 50878], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 296, "seek": 68758, "start": 697.86, "end": 700.94, "text": " They claim, like in the UI, in the front end,", "tokens": [50878, 814, 3932, 11, 411, 294, 264, 15682, 11, 294, 264, 1868, 917, 11, 51032], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 297, "seek": 68758, "start": 700.94, "end": 703.74, "text": " they claim that I am using GPT-4, but how do I know that?", "tokens": [51032, 436, 3932, 300, 286, 669, 1228, 26039, 51, 12, 19, 11, 457, 577, 360, 286, 458, 300, 30, 51172], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 298, "seek": 68758, "start": 703.74, "end": 706.5400000000001, "text": " There's no way of me of actually verifying that this is GPT-4.", "tokens": [51172, 821, 311, 572, 636, 295, 385, 295, 767, 1306, 5489, 300, 341, 307, 26039, 51, 12, 19, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 299, "seek": 68758, "start": 706.5400000000001, "end": 708.1800000000001, "text": " They might be serving me a worse model,", "tokens": [51312, 814, 1062, 312, 8148, 385, 257, 5324, 2316, 11, 51394], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 300, "seek": 68758, "start": 708.1800000000001, "end": 709.5400000000001, "text": " which is cheaper to run", "tokens": [51394, 597, 307, 12284, 281, 1190, 51462], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 301, "seek": 68758, "start": 709.5400000000001, "end": 712.0200000000001, "text": " and just pocketing the difference, for example, right?", "tokens": [51462, 293, 445, 8963, 278, 264, 2649, 11, 337, 1365, 11, 558, 30, 51586], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 302, "seek": 68758, "start": 712.0200000000001, "end": 712.9000000000001, "text": " I don't know.", "tokens": [51586, 286, 500, 380, 458, 13, 51630], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 303, "seek": 68758, "start": 712.9000000000001, "end": 715.46, "text": " So one good thing that ZKML provides", "tokens": [51630, 407, 472, 665, 551, 300, 1176, 42, 12683, 6417, 51758], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 304, "seek": 68758, "start": 715.46, "end": 716.7800000000001, "text": " is this form of accountability", "tokens": [51758, 307, 341, 1254, 295, 19380, 51824], "temperature": 0.0, "avg_logprob": -0.10426767188382435, "compression_ratio": 1.6949685534591195, "no_speech_prob": 0.0008829949656501412}, {"id": 305, "seek": 71678, "start": 716.78, "end": 720.66, "text": " where I, as the consumer of some API or some model,", "tokens": [50364, 689, 286, 11, 382, 264, 9711, 295, 512, 9362, 420, 512, 2316, 11, 50558], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 306, "seek": 71678, "start": 720.66, "end": 722.9, "text": " I know that this actually came from something", "tokens": [50558, 286, 458, 300, 341, 767, 1361, 490, 746, 50670], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 307, "seek": 71678, "start": 722.9, "end": 725.1, "text": " because I can verify a cryptographic proof", "tokens": [50670, 570, 286, 393, 16888, 257, 9844, 12295, 8177, 50780], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 308, "seek": 71678, "start": 725.1, "end": 726.02, "text": " that this indeed happened.", "tokens": [50780, 300, 341, 6451, 2011, 13, 50826], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 309, "seek": 71678, "start": 726.02, "end": 727.74, "text": " So we can make AIs accountable.", "tokens": [50826, 407, 321, 393, 652, 316, 6802, 18024, 13, 50912], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 310, "seek": 71678, "start": 727.74, "end": 729.78, "text": " We can make anyone using AI accountable", "tokens": [50912, 492, 393, 652, 2878, 1228, 7318, 18024, 51014], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 311, "seek": 71678, "start": 729.78, "end": 732.62, "text": " because we can make proofs of computation.", "tokens": [51014, 570, 321, 393, 652, 8177, 82, 295, 24903, 13, 51156], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 312, "seek": 71678, "start": 732.62, "end": 737.02, "text": " Many of the framing for ZKML in the modern way", "tokens": [51156, 5126, 295, 264, 28971, 337, 1176, 42, 12683, 294, 264, 4363, 636, 51376], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 313, "seek": 71678, "start": 737.02, "end": 740.3399999999999, "text": " is that they want to essentially bring machine learning on-chain", "tokens": [51376, 307, 300, 436, 528, 281, 4476, 1565, 3479, 2539, 322, 12, 11509, 51542], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 314, "seek": 71678, "start": 740.3399999999999, "end": 742.22, "text": " or onto the blockchain in this case, right?", "tokens": [51542, 420, 3911, 264, 17176, 294, 341, 1389, 11, 558, 30, 51636], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 315, "seek": 71678, "start": 742.22, "end": 744.86, "text": " Like we have the blockchains where we have", "tokens": [51636, 1743, 321, 362, 264, 3461, 339, 2315, 689, 321, 362, 51768], "temperature": 0.0, "avg_logprob": -0.12797882443382627, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.0019266247982159257}, {"id": 316, "seek": 74486, "start": 744.86, "end": 749.1, "text": " very interconnected networks of low-end hardware, mostly.", "tokens": [50364, 588, 36611, 9590, 295, 2295, 12, 521, 8837, 11, 5240, 13, 50576], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 317, "seek": 74486, "start": 749.1, "end": 752.22, "text": " It's like consumer hardware, which is available everywhere,", "tokens": [50576, 467, 311, 411, 9711, 8837, 11, 597, 307, 2435, 5315, 11, 50732], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 318, "seek": 74486, "start": 752.22, "end": 754.46, "text": " to run these decentralized networks.", "tokens": [50732, 281, 1190, 613, 32870, 9590, 13, 50844], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 319, "seek": 74486, "start": 754.46, "end": 756.7, "text": " And the problem with this is that every single computer", "tokens": [50844, 400, 264, 1154, 365, 341, 307, 300, 633, 2167, 3820, 50956], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 320, "seek": 74486, "start": 756.7, "end": 759.42, "text": " on this network needs to re-execute everything", "tokens": [50956, 322, 341, 3209, 2203, 281, 319, 12, 3121, 3045, 1169, 1203, 51092], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 321, "seek": 74486, "start": 759.42, "end": 761.62, "text": " that the network sees in order to validate", "tokens": [51092, 300, 264, 3209, 8194, 294, 1668, 281, 29562, 51202], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 322, "seek": 74486, "start": 761.62, "end": 763.86, "text": " that the network is progressing correctly.", "tokens": [51202, 300, 264, 3209, 307, 36305, 8944, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 323, "seek": 74486, "start": 763.86, "end": 765.1, "text": " And this is a big problem", "tokens": [51314, 400, 341, 307, 257, 955, 1154, 51376], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 324, "seek": 74486, "start": 765.1, "end": 767.0600000000001, "text": " because now everything is really expensive.", "tokens": [51376, 570, 586, 1203, 307, 534, 5124, 13, 51474], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 325, "seek": 74486, "start": 767.0600000000001, "end": 769.94, "text": " If it's already expensive running it on your own machine,", "tokens": [51474, 759, 309, 311, 1217, 5124, 2614, 309, 322, 428, 1065, 3479, 11, 51618], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 326, "seek": 74486, "start": 769.94, "end": 772.1, "text": " if you have to run it on 1,000 machines", "tokens": [51618, 498, 291, 362, 281, 1190, 309, 322, 502, 11, 1360, 8379, 51726], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 327, "seek": 74486, "start": 772.1, "end": 773.9, "text": " or 10,000 or 100,000 machines,", "tokens": [51726, 420, 1266, 11, 1360, 420, 2319, 11, 1360, 8379, 11, 51816], "temperature": 0.0, "avg_logprob": -0.0886384879841524, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.020017143338918686}, {"id": 328, "seek": 77390, "start": 773.9399999999999, "end": 776.62, "text": " it's as many times more expensive.", "tokens": [50366, 309, 311, 382, 867, 1413, 544, 5124, 13, 50500], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 329, "seek": 77390, "start": 776.62, "end": 780.3, "text": " So it's unfeasible to essentially do machine learning", "tokens": [50500, 407, 309, 311, 517, 2106, 296, 964, 281, 4476, 360, 3479, 2539, 50684], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 330, "seek": 77390, "start": 780.3, "end": 783.5, "text": " on-chain right now because it's just too expensive.", "tokens": [50684, 322, 12, 11509, 558, 586, 570, 309, 311, 445, 886, 5124, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 331, "seek": 77390, "start": 783.5, "end": 785.14, "text": " And the computational environment", "tokens": [50844, 400, 264, 28270, 2823, 50926], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 332, "seek": 77390, "start": 785.14, "end": 787.3, "text": " that these usually like blockchains have,", "tokens": [50926, 300, 613, 2673, 411, 3461, 339, 2315, 362, 11, 51034], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 333, "seek": 77390, "start": 787.3, "end": 789.9, "text": " like virtual machines, let's say if they're about the EVM,", "tokens": [51034, 411, 6374, 8379, 11, 718, 311, 584, 498, 436, 434, 466, 264, 15733, 44, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 334, "seek": 77390, "start": 789.9, "end": 792.9, "text": " Solana has SVM, the Solana virtual machine,", "tokens": [51164, 7026, 2095, 575, 31910, 44, 11, 264, 7026, 2095, 6374, 3479, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 335, "seek": 77390, "start": 792.9, "end": 795.06, "text": " that every single blockchain or most blockchains", "tokens": [51314, 300, 633, 2167, 17176, 420, 881, 3461, 339, 2315, 51422], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 336, "seek": 77390, "start": 795.06, "end": 797.4599999999999, "text": " do have some form of execution capabilities", "tokens": [51422, 360, 362, 512, 1254, 295, 15058, 10862, 51542], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 337, "seek": 77390, "start": 797.4599999999999, "end": 798.74, "text": " or computing capabilities.", "tokens": [51542, 420, 15866, 10862, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 338, "seek": 77390, "start": 798.74, "end": 800.42, "text": " And these are very constrained.", "tokens": [51606, 400, 613, 366, 588, 38901, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1516812590302014, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0006771583575755358}, {"id": 339, "seek": 80042, "start": 800.4599999999999, "end": 803.9399999999999, "text": " So some of the things that blockchains are good at", "tokens": [50366, 407, 512, 295, 264, 721, 300, 3461, 339, 2315, 366, 665, 412, 50540], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 340, "seek": 80042, "start": 803.9399999999999, "end": 806.5799999999999, "text": " is cryptography because they're usually subsidized", "tokens": [50540, 307, 9844, 5820, 570, 436, 434, 2673, 20051, 1602, 50672], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 341, "seek": 80042, "start": 806.5799999999999, "end": 810.0999999999999, "text": " within the cost of execution in blockchain.", "tokens": [50672, 1951, 264, 2063, 295, 15058, 294, 17176, 13, 50848], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 342, "seek": 80042, "start": 810.0999999999999, "end": 813.6999999999999, "text": " So I can verify a zero-knowledge proof on a blockchain", "tokens": [50848, 407, 286, 393, 16888, 257, 4018, 12, 15869, 3042, 8177, 322, 257, 17176, 51028], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 343, "seek": 80042, "start": 813.6999999999999, "end": 816.54, "text": " and I can bring something that I run off-chain", "tokens": [51028, 293, 286, 393, 1565, 746, 300, 286, 1190, 766, 12, 11509, 51170], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 344, "seek": 80042, "start": 816.54, "end": 818.06, "text": " on-chain by providing a proof, right?", "tokens": [51170, 322, 12, 11509, 538, 6530, 257, 8177, 11, 558, 30, 51246], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 345, "seek": 80042, "start": 818.06, "end": 821.3, "text": " So for example, if I'm evaluating a model locally,", "tokens": [51246, 407, 337, 1365, 11, 498, 286, 478, 27479, 257, 2316, 16143, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 346, "seek": 80042, "start": 821.3, "end": 822.8199999999999, "text": " I'm able to create a zero-knowledge proof", "tokens": [51408, 286, 478, 1075, 281, 1884, 257, 4018, 12, 15869, 3042, 8177, 51484], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 347, "seek": 80042, "start": 822.8199999999999, "end": 824.9, "text": " that I've evaluated a model on some input", "tokens": [51484, 300, 286, 600, 25509, 257, 2316, 322, 512, 4846, 51588], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 348, "seek": 80042, "start": 824.9, "end": 827.4599999999999, "text": " and I can just send the output to the chain", "tokens": [51588, 293, 286, 393, 445, 2845, 264, 5598, 281, 264, 5021, 51716], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 349, "seek": 80042, "start": 827.4599999999999, "end": 828.66, "text": " and verify a proof.", "tokens": [51716, 293, 16888, 257, 8177, 13, 51776], "temperature": 0.0, "avg_logprob": -0.09368275884372085, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.00022340334544423968}, {"id": 350, "seek": 82866, "start": 828.66, "end": 830.8199999999999, "text": " And then the chain or the smart contract", "tokens": [50364, 400, 550, 264, 5021, 420, 264, 4069, 4364, 50472], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 351, "seek": 82866, "start": 830.8199999999999, "end": 833.86, "text": " can know that I've actually run a model on some inputs", "tokens": [50472, 393, 458, 300, 286, 600, 767, 1190, 257, 2316, 322, 512, 15743, 50624], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 352, "seek": 82866, "start": 833.86, "end": 837.06, "text": " and I don't have to run that within the environment,", "tokens": [50624, 293, 286, 500, 380, 362, 281, 1190, 300, 1951, 264, 2823, 11, 50784], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 353, "seek": 82866, "start": 837.06, "end": 838.18, "text": " the computing environment, the blockchain,", "tokens": [50784, 264, 15866, 2823, 11, 264, 17176, 11, 50840], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 354, "seek": 82866, "start": 838.18, "end": 839.66, "text": " so I can save a lot of cost.", "tokens": [50840, 370, 286, 393, 3155, 257, 688, 295, 2063, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 355, "seek": 82866, "start": 839.66, "end": 842.3399999999999, "text": " And if I make ML more accessible on-chain,", "tokens": [50914, 400, 498, 286, 652, 21601, 544, 9515, 322, 12, 11509, 11, 51048], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 356, "seek": 82866, "start": 842.3399999999999, "end": 844.8199999999999, "text": " I can actually bring it and I can build application", "tokens": [51048, 286, 393, 767, 1565, 309, 293, 286, 393, 1322, 3861, 51172], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 357, "seek": 82866, "start": 844.8199999999999, "end": 847.42, "text": " that leverage machine learning for lots of different things", "tokens": [51172, 300, 13982, 3479, 2539, 337, 3195, 295, 819, 721, 51302], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 358, "seek": 82866, "start": 847.42, "end": 849.8199999999999, "text": " which I'll get into a little bit later.", "tokens": [51302, 597, 286, 603, 483, 666, 257, 707, 857, 1780, 13, 51422], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 359, "seek": 82866, "start": 849.8199999999999, "end": 852.66, "text": " And the last one is the zero-knowledge part of things", "tokens": [51422, 400, 264, 1036, 472, 307, 264, 4018, 12, 15869, 3042, 644, 295, 721, 51564], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 360, "seek": 82866, "start": 852.66, "end": 855.66, "text": " where I'm able to hide specific parts of the computation", "tokens": [51564, 689, 286, 478, 1075, 281, 6479, 2685, 3166, 295, 264, 24903, 51714], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 361, "seek": 82866, "start": 855.66, "end": 858.42, "text": " or specific parts of the data that I'm making proofs about", "tokens": [51714, 420, 2685, 3166, 295, 264, 1412, 300, 286, 478, 1455, 8177, 82, 466, 51852], "temperature": 0.0, "avg_logprob": -0.09864053031466655, "compression_ratio": 1.8111455108359134, "no_speech_prob": 0.0015730316517874599}, {"id": 362, "seek": 85842, "start": 858.42, "end": 861.3399999999999, "text": " and therefore I can make machine learning private.", "tokens": [50364, 293, 4412, 286, 393, 652, 3479, 2539, 4551, 13, 50510], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 363, "seek": 85842, "start": 861.3399999999999, "end": 862.66, "text": " In order to make machine learning private,", "tokens": [50510, 682, 1668, 281, 652, 3479, 2539, 4551, 11, 50576], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 364, "seek": 85842, "start": 862.66, "end": 864.54, "text": " there's other techniques as well", "tokens": [50576, 456, 311, 661, 7512, 382, 731, 50670], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 365, "seek": 85842, "start": 864.54, "end": 866.26, "text": " which is like fully homomorphic encryption", "tokens": [50670, 597, 307, 411, 4498, 3655, 32702, 299, 29575, 50756], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 366, "seek": 85842, "start": 866.26, "end": 867.86, "text": " and multi-party computation.", "tokens": [50756, 293, 4825, 12, 23409, 24903, 13, 50836], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 367, "seek": 85842, "start": 867.86, "end": 870.3399999999999, "text": " Each of these other types of cryptography", "tokens": [50836, 6947, 295, 613, 661, 3467, 295, 9844, 5820, 50960], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 368, "seek": 85842, "start": 870.3399999999999, "end": 873.6999999999999, "text": " or types of distributed systems engineering and compute it", "tokens": [50960, 420, 3467, 295, 12631, 3652, 7043, 293, 14722, 309, 51128], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 369, "seek": 85842, "start": 873.6999999999999, "end": 875.4599999999999, "text": " usually have different trade-offs.", "tokens": [51128, 2673, 362, 819, 4923, 12, 19231, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 370, "seek": 85842, "start": 875.4599999999999, "end": 877.5799999999999, "text": " So for example, fully homomorphic encryption", "tokens": [51216, 407, 337, 1365, 11, 4498, 3655, 32702, 299, 29575, 51322], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 371, "seek": 85842, "start": 877.5799999999999, "end": 880.14, "text": " does not give you this correctness assumption", "tokens": [51322, 775, 406, 976, 291, 341, 3006, 1287, 15302, 51450], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 372, "seek": 85842, "start": 880.14, "end": 881.5, "text": " or like probability.", "tokens": [51450, 420, 411, 8482, 13, 51518], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 373, "seek": 85842, "start": 881.5, "end": 883.62, "text": " I cannot verify that something happened correctly,", "tokens": [51518, 286, 2644, 16888, 300, 746, 2011, 8944, 11, 51624], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 374, "seek": 85842, "start": 883.62, "end": 887.02, "text": " but I can, for example, make computations on Cyphertex.", "tokens": [51624, 457, 286, 393, 11, 337, 1365, 11, 652, 2807, 763, 322, 10295, 79, 511, 975, 87, 13, 51794], "temperature": 0.0, "avg_logprob": -0.13232108222113714, "compression_ratio": 1.819078947368421, "no_speech_prob": 0.003883807919919491}, {"id": 375, "seek": 88702, "start": 887.02, "end": 888.5, "text": " So if I encrypt some data,", "tokens": [50364, 407, 498, 286, 17972, 662, 512, 1412, 11, 50438], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 376, "seek": 88702, "start": 888.5, "end": 891.22, "text": " I'm able to perform computations on encrypted data.", "tokens": [50438, 286, 478, 1075, 281, 2042, 2807, 763, 322, 36663, 1412, 13, 50574], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 377, "seek": 88702, "start": 891.22, "end": 894.38, "text": " And when I decrypt, I have the computation performed", "tokens": [50574, 400, 562, 286, 979, 627, 662, 11, 286, 362, 264, 24903, 10332, 50732], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 378, "seek": 88702, "start": 894.38, "end": 897.02, "text": " on the original input, on the original plaintext", "tokens": [50732, 322, 264, 3380, 4846, 11, 322, 264, 3380, 11121, 25111, 50864], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 379, "seek": 88702, "start": 897.02, "end": 899.18, "text": " which is something that is quite fascinating.", "tokens": [50864, 597, 307, 746, 300, 307, 1596, 10343, 13, 50972], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 380, "seek": 88702, "start": 899.18, "end": 901.14, "text": " However, I do lose this property of ZK", "tokens": [50972, 2908, 11, 286, 360, 3624, 341, 4707, 295, 1176, 42, 51070], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 381, "seek": 88702, "start": 901.14, "end": 904.22, "text": " where I cannot verify that something happened correctly.", "tokens": [51070, 689, 286, 2644, 16888, 300, 746, 2011, 8944, 13, 51224], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 382, "seek": 88702, "start": 904.22, "end": 907.06, "text": " Multi-party computation is like the better of both worlds", "tokens": [51224, 29238, 12, 23409, 24903, 307, 411, 264, 1101, 295, 1293, 13401, 51366], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 383, "seek": 88702, "start": 907.06, "end": 910.38, "text": " but for example, these protocols require multiple parties", "tokens": [51366, 457, 337, 1365, 11, 613, 20618, 3651, 3866, 8265, 51532], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 384, "seek": 88702, "start": 910.38, "end": 912.14, "text": " to work in unison", "tokens": [51532, 281, 589, 294, 517, 2770, 51620], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 385, "seek": 88702, "start": 912.14, "end": 914.74, "text": " and this is really hard to manage, for example.", "tokens": [51620, 293, 341, 307, 534, 1152, 281, 3067, 11, 337, 1365, 13, 51750], "temperature": 0.0, "avg_logprob": -0.10655523836612701, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0005527526955120265}, {"id": 386, "seek": 91474, "start": 914.74, "end": 917.0600000000001, "text": " So yeah, so I want to talk a bit", "tokens": [50364, 407, 1338, 11, 370, 286, 528, 281, 751, 257, 857, 50480], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 387, "seek": 91474, "start": 917.0600000000001, "end": 918.62, "text": " about who is actually building", "tokens": [50480, 466, 567, 307, 767, 2390, 50558], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 388, "seek": 91474, "start": 918.62, "end": 921.38, "text": " Zeronautic Machine Learning nowadays.", "tokens": [50558, 1176, 260, 266, 1375, 299, 22155, 15205, 13434, 13, 50696], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 389, "seek": 91474, "start": 921.38, "end": 923.26, "text": " Like which startups, which teams,", "tokens": [50696, 1743, 597, 28041, 11, 597, 5491, 11, 50790], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 390, "seek": 91474, "start": 923.26, "end": 924.7, "text": " what are they're focusing on?", "tokens": [50790, 437, 366, 436, 434, 8416, 322, 30, 50862], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 391, "seek": 91474, "start": 925.7, "end": 928.5, "text": " So full disclosure, I am an investor in Giza and Modulus", "tokens": [50912, 407, 1577, 30392, 11, 286, 669, 364, 18479, 294, 460, 13427, 293, 6583, 26107, 51052], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 392, "seek": 91474, "start": 928.5, "end": 931.98, "text": " so I don't want to put that out there, just so you know.", "tokens": [51052, 370, 286, 500, 380, 528, 281, 829, 300, 484, 456, 11, 445, 370, 291, 458, 13, 51226], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 393, "seek": 91474, "start": 931.98, "end": 935.1800000000001, "text": " So essentially there's different avenues", "tokens": [51226, 407, 4476, 456, 311, 819, 43039, 51386], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 394, "seek": 91474, "start": 935.1800000000001, "end": 938.98, "text": " on what is there to build on within the ZKML domain", "tokens": [51386, 322, 437, 307, 456, 281, 1322, 322, 1951, 264, 1176, 42, 12683, 9274, 51576], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 395, "seek": 91474, "start": 938.98, "end": 940.82, "text": " in order to make these systems better", "tokens": [51576, 294, 1668, 281, 652, 613, 3652, 1101, 51668], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 396, "seek": 91474, "start": 940.82, "end": 942.54, "text": " or to make this more interesting", "tokens": [51668, 420, 281, 652, 341, 544, 1880, 51754], "temperature": 0.0, "avg_logprob": -0.24615000140282414, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0019875734578818083}, {"id": 397, "seek": 94254, "start": 942.54, "end": 945.14, "text": " or faster and all sorts of different things.", "tokens": [50364, 420, 4663, 293, 439, 7527, 295, 819, 721, 13, 50494], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 398, "seek": 94254, "start": 945.14, "end": 948.2199999999999, "text": " So the three main companies that usually go around", "tokens": [50494, 407, 264, 1045, 2135, 3431, 300, 2673, 352, 926, 50648], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 399, "seek": 94254, "start": 948.2199999999999, "end": 950.74, "text": " are Giza, Modulus and Ezekio.", "tokens": [50648, 366, 460, 13427, 11, 6583, 26107, 293, 462, 19878, 1004, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 400, "seek": 94254, "start": 950.74, "end": 953.3, "text": " On Giza, for example, they're building", "tokens": [50774, 1282, 460, 13427, 11, 337, 1365, 11, 436, 434, 2390, 50902], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 401, "seek": 94254, "start": 953.3, "end": 954.86, "text": " on top of the StarkNet ecosystem", "tokens": [50902, 322, 1192, 295, 264, 28967, 31890, 11311, 50980], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 402, "seek": 94254, "start": 954.86, "end": 957.06, "text": " which is like one specific scalability solution", "tokens": [50980, 597, 307, 411, 472, 2685, 15664, 2310, 3827, 51090], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 403, "seek": 94254, "start": 957.06, "end": 958.78, "text": " in the Ethereum space", "tokens": [51090, 294, 264, 26894, 1901, 51176], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 404, "seek": 94254, "start": 958.78, "end": 962.42, "text": " and they're implemented a bunch of machine learning models", "tokens": [51176, 293, 436, 434, 12270, 257, 3840, 295, 3479, 2539, 5245, 51358], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 405, "seek": 94254, "start": 962.42, "end": 964.5799999999999, "text": " within this computational environment", "tokens": [51358, 1951, 341, 28270, 2823, 51466], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 406, "seek": 94254, "start": 964.5799999999999, "end": 966.74, "text": " that this blockchain had, which is called Cairo.", "tokens": [51466, 300, 341, 17176, 632, 11, 597, 307, 1219, 30983, 340, 13, 51574], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 407, "seek": 94254, "start": 966.74, "end": 968.2199999999999, "text": " The computational environment is called Cairo,", "tokens": [51574, 440, 28270, 2823, 307, 1219, 30983, 340, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 408, "seek": 94254, "start": 968.2199999999999, "end": 969.9399999999999, "text": " the blockchain is called Tarkin", "tokens": [51648, 264, 17176, 307, 1219, 314, 809, 259, 51734], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 409, "seek": 94254, "start": 969.9399999999999, "end": 971.9, "text": " and they're building products, right?", "tokens": [51734, 293, 436, 434, 2390, 3383, 11, 558, 30, 51832], "temperature": 0.0, "avg_logprob": -0.1445309795550446, "compression_ratio": 1.743421052631579, "no_speech_prob": 0.0013883435167372227}, {"id": 410, "seek": 97190, "start": 971.9, "end": 973.3, "text": " So they're building, for example,", "tokens": [50364, 407, 436, 434, 2390, 11, 337, 1365, 11, 50434], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 411, "seek": 97190, "start": 973.3, "end": 974.98, "text": " tooling so that financial products", "tokens": [50434, 46593, 370, 300, 4669, 3383, 50518], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 412, "seek": 97190, "start": 974.98, "end": 976.54, "text": " that are deployed on StarkNet", "tokens": [50518, 300, 366, 17826, 322, 28967, 31890, 50596], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 413, "seek": 97190, "start": 976.54, "end": 979.22, "text": " can leverage machine learning within their,", "tokens": [50596, 393, 13982, 3479, 2539, 1951, 641, 11, 50730], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 414, "seek": 97190, "start": 979.22, "end": 981.78, "text": " for example, prediction models for financial services", "tokens": [50730, 337, 1365, 11, 17630, 5245, 337, 4669, 3328, 50858], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 415, "seek": 97190, "start": 981.78, "end": 985.02, "text": " so they can predict where the highest yield is", "tokens": [50858, 370, 436, 393, 6069, 689, 264, 6343, 11257, 307, 51020], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 416, "seek": 97190, "start": 985.02, "end": 986.74, "text": " to routes my money into", "tokens": [51020, 281, 18242, 452, 1460, 666, 51106], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 417, "seek": 97190, "start": 986.74, "end": 988.9399999999999, "text": " so I can get the highest yield on my collateral", "tokens": [51106, 370, 286, 393, 483, 264, 6343, 11257, 322, 452, 41875, 51216], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 418, "seek": 97190, "start": 988.9399999999999, "end": 991.54, "text": " or my assets so I can use machine learning off-chain", "tokens": [51216, 420, 452, 9769, 370, 286, 393, 764, 3479, 2539, 766, 12, 11509, 51346], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 419, "seek": 97190, "start": 991.54, "end": 992.5799999999999, "text": " or machine learning on-chain", "tokens": [51346, 420, 3479, 2539, 322, 12, 11509, 51398], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 420, "seek": 97190, "start": 992.5799999999999, "end": 994.06, "text": " within this computational environment,", "tokens": [51398, 1951, 341, 28270, 2823, 11, 51472], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 421, "seek": 97190, "start": 994.06, "end": 995.78, "text": " I can prove it, et cetera, et cetera.", "tokens": [51472, 286, 393, 7081, 309, 11, 1030, 11458, 11, 1030, 11458, 13, 51558], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 422, "seek": 97190, "start": 995.78, "end": 996.8199999999999, "text": " So essentially Giza is building", "tokens": [51558, 407, 4476, 460, 13427, 307, 2390, 51610], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 423, "seek": 97190, "start": 996.8199999999999, "end": 998.18, "text": " a lot of the product side of things,", "tokens": [51610, 257, 688, 295, 264, 1674, 1252, 295, 721, 11, 51678], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 424, "seek": 97190, "start": 998.18, "end": 1000.54, "text": " like different abstraction, different SDKs,", "tokens": [51678, 411, 819, 37765, 11, 819, 37135, 82, 11, 51796], "temperature": 0.0, "avg_logprob": -0.12954730079287574, "compression_ratio": 1.8935483870967742, "no_speech_prob": 0.00035695842234417796}, {"id": 425, "seek": 100054, "start": 1000.54, "end": 1003.62, "text": " different representations of models, on-chain, et cetera.", "tokens": [50364, 819, 33358, 295, 5245, 11, 322, 12, 11509, 11, 1030, 11458, 13, 50518], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 426, "seek": 100054, "start": 1003.62, "end": 1008.26, "text": " Modulus is mostly working on the foundational side of things", "tokens": [50518, 6583, 26107, 307, 5240, 1364, 322, 264, 32195, 1252, 295, 721, 50750], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 427, "seek": 100054, "start": 1008.26, "end": 1011.42, "text": " and by foundation, I mean the primordial science", "tokens": [50750, 293, 538, 7030, 11, 286, 914, 264, 2886, 765, 831, 3497, 50908], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 428, "seek": 100054, "start": 1011.42, "end": 1015.14, "text": " of doing cryptography and doing engineering.", "tokens": [50908, 295, 884, 9844, 5820, 293, 884, 7043, 13, 51094], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 429, "seek": 100054, "start": 1015.14, "end": 1017.5799999999999, "text": " So they're essentially building their own,", "tokens": [51094, 407, 436, 434, 4476, 2390, 641, 1065, 11, 51216], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 430, "seek": 100054, "start": 1017.5799999999999, "end": 1019.66, "text": " the thing so-called like approving system", "tokens": [51216, 264, 551, 370, 12, 11880, 411, 2075, 798, 1185, 51320], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 431, "seek": 100054, "start": 1019.66, "end": 1022.74, "text": " which is how do you implement a zero-knowledge scheme?", "tokens": [51320, 597, 307, 577, 360, 291, 4445, 257, 4018, 12, 15869, 3042, 12232, 30, 51474], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 432, "seek": 100054, "start": 1022.74, "end": 1025.02, "text": " Like zero-knowledge, it boiled down to mathematics", "tokens": [51474, 1743, 4018, 12, 15869, 3042, 11, 309, 21058, 760, 281, 18666, 51588], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 433, "seek": 100054, "start": 1025.02, "end": 1027.94, "text": " and working with polynomials and finite fields", "tokens": [51588, 293, 1364, 365, 22560, 12356, 293, 19362, 7909, 51734], "temperature": 0.0, "avg_logprob": -0.14472116006387248, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.0018378404201939702}, {"id": 434, "seek": 102794, "start": 1027.94, "end": 1032.26, "text": " and mostly like linear algebra and abstract algebra", "tokens": [50364, 293, 5240, 411, 8213, 21989, 293, 12649, 21989, 50580], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 435, "seek": 102794, "start": 1032.26, "end": 1034.94, "text": " and modular arithmetic and bunch of things of this sort.", "tokens": [50580, 293, 31111, 42973, 293, 3840, 295, 721, 295, 341, 1333, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 436, "seek": 102794, "start": 1034.94, "end": 1037.02, "text": " So essentially they're trying to build", "tokens": [50714, 407, 4476, 436, 434, 1382, 281, 1322, 50818], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 437, "seek": 102794, "start": 1037.02, "end": 1039.98, "text": " better cryptographic models and better cryptographic systems", "tokens": [50818, 1101, 9844, 12295, 5245, 293, 1101, 9844, 12295, 3652, 50966], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 438, "seek": 102794, "start": 1039.98, "end": 1042.02, "text": " in order for zero-knowledge machine learning", "tokens": [50966, 294, 1668, 337, 4018, 12, 15869, 3042, 3479, 2539, 51068], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 439, "seek": 102794, "start": 1042.02, "end": 1045.46, "text": " to be more efficient within the actual representation of it", "tokens": [51068, 281, 312, 544, 7148, 1951, 264, 3539, 10290, 295, 309, 51240], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 440, "seek": 102794, "start": 1045.46, "end": 1047.46, "text": " in the computing sense, right?", "tokens": [51240, 294, 264, 15866, 2020, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 441, "seek": 102794, "start": 1047.46, "end": 1049.5800000000002, "text": " So this is what they're working on.", "tokens": [51340, 407, 341, 307, 437, 436, 434, 1364, 322, 13, 51446], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 442, "seek": 102794, "start": 1049.5800000000002, "end": 1051.5800000000002, "text": " I'm happy to then after this presentation", "tokens": [51446, 286, 478, 2055, 281, 550, 934, 341, 5860, 51546], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 443, "seek": 102794, "start": 1051.5800000000002, "end": 1054.14, "text": " or in the FAQ, I'm happy to share more if anyone wants.", "tokens": [51546, 420, 294, 264, 19894, 48, 11, 286, 478, 2055, 281, 2073, 544, 498, 2878, 2738, 13, 51674], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 444, "seek": 102794, "start": 1054.14, "end": 1057.02, "text": " Like there's plenty of resources to learn more", "tokens": [51674, 1743, 456, 311, 7140, 295, 3593, 281, 1466, 544, 51818], "temperature": 0.0, "avg_logprob": -0.12661624517966444, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013883855426684022}, {"id": 445, "seek": 105702, "start": 1057.02, "end": 1058.94, "text": " and Ithiko is mostly building, tooling", "tokens": [50364, 293, 286, 392, 10770, 307, 5240, 2390, 11, 46593, 50460], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 446, "seek": 105702, "start": 1058.94, "end": 1061.18, "text": " and also some of the products type of things", "tokens": [50460, 293, 611, 512, 295, 264, 3383, 2010, 295, 721, 50572], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 447, "seek": 105702, "start": 1061.18, "end": 1063.58, "text": " and infrastructure, a lot of infrastructure as well.", "tokens": [50572, 293, 6896, 11, 257, 688, 295, 6896, 382, 731, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 448, "seek": 105702, "start": 1063.58, "end": 1066.1399999999999, "text": " So Ithiko, for example, is building an abstraction layer", "tokens": [50692, 407, 286, 392, 10770, 11, 337, 1365, 11, 307, 2390, 364, 37765, 4583, 50820], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 449, "seek": 105702, "start": 1066.1399999999999, "end": 1067.98, "text": " that allows developers that are,", "tokens": [50820, 300, 4045, 8849, 300, 366, 11, 50912], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 450, "seek": 105702, "start": 1067.98, "end": 1069.74, "text": " they come from the machine learning world.", "tokens": [50912, 436, 808, 490, 264, 3479, 2539, 1002, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 451, "seek": 105702, "start": 1069.74, "end": 1071.62, "text": " So people who usually know Python", "tokens": [51000, 407, 561, 567, 2673, 458, 15329, 51094], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 452, "seek": 105702, "start": 1071.62, "end": 1074.18, "text": " or right now in this context is just Python.", "tokens": [51094, 420, 558, 586, 294, 341, 4319, 307, 445, 15329, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 453, "seek": 105702, "start": 1074.18, "end": 1077.02, "text": " So people who know Python and know the standard tools", "tokens": [51222, 407, 561, 567, 458, 15329, 293, 458, 264, 3832, 3873, 51364], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 454, "seek": 105702, "start": 1077.02, "end": 1078.3, "text": " for building machine learning models,", "tokens": [51364, 337, 2390, 3479, 2539, 5245, 11, 51428], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 455, "seek": 105702, "start": 1078.3, "end": 1080.22, "text": " whether it's TensorFlow or Pycorg", "tokens": [51428, 1968, 309, 311, 37624, 420, 9953, 66, 4646, 51524], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 456, "seek": 105702, "start": 1080.22, "end": 1083.58, "text": " or I could learn or any other library for machine learning,", "tokens": [51524, 420, 286, 727, 1466, 420, 604, 661, 6405, 337, 3479, 2539, 11, 51692], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 457, "seek": 105702, "start": 1083.58, "end": 1085.7, "text": " they even have a standardized representation", "tokens": [51692, 436, 754, 362, 257, 31677, 10290, 51798], "temperature": 0.0, "avg_logprob": -0.2034775348419839, "compression_ratio": 1.8617363344051447, "no_speech_prob": 0.0017272231634706259}, {"id": 458, "seek": 108570, "start": 1085.7, "end": 1088.26, "text": " of a model which can be exported", "tokens": [50364, 295, 257, 2316, 597, 393, 312, 42055, 50492], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 459, "seek": 108570, "start": 1088.26, "end": 1091.3400000000001, "text": " to this model representation called ONIX, ONIX,", "tokens": [50492, 281, 341, 2316, 10290, 1219, 9299, 21124, 11, 9299, 21124, 11, 50646], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 460, "seek": 108570, "start": 1091.3400000000001, "end": 1093.54, "text": " open your network exchange format.", "tokens": [50646, 1269, 428, 3209, 7742, 7877, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 461, "seek": 108570, "start": 1093.54, "end": 1095.78, "text": " And this format essentially is something", "tokens": [50756, 400, 341, 7877, 4476, 307, 746, 50868], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 462, "seek": 108570, "start": 1095.78, "end": 1098.46, "text": " that represents what the model looks like", "tokens": [50868, 300, 8855, 437, 264, 2316, 1542, 411, 51002], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 463, "seek": 108570, "start": 1098.46, "end": 1099.7, "text": " in the computational sense.", "tokens": [51002, 294, 264, 28270, 2020, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 464, "seek": 108570, "start": 1099.7, "end": 1101.66, "text": " It's a computational graph of different operations", "tokens": [51064, 467, 311, 257, 28270, 4295, 295, 819, 7705, 51162], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 465, "seek": 108570, "start": 1101.66, "end": 1103.06, "text": " that you need to perform.", "tokens": [51162, 300, 291, 643, 281, 2042, 13, 51232], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 466, "seek": 108570, "start": 1103.06, "end": 1106.98, "text": " And Ithiko allows you to convert whatever you are building", "tokens": [51232, 400, 286, 392, 10770, 4045, 291, 281, 7620, 2035, 291, 366, 2390, 51428], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 467, "seek": 108570, "start": 1106.98, "end": 1110.3, "text": " within Python to something that can be ZK proven,", "tokens": [51428, 1951, 15329, 281, 746, 300, 393, 312, 1176, 42, 12785, 11, 51594], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 468, "seek": 108570, "start": 1110.3, "end": 1113.3, "text": " that you can make a proof of in a very easy way.", "tokens": [51594, 300, 291, 393, 652, 257, 8177, 295, 294, 257, 588, 1858, 636, 13, 51744], "temperature": 0.0, "avg_logprob": -0.14479346110902983, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.020013734698295593}, {"id": 469, "seek": 111330, "start": 1113.3, "end": 1115.74, "text": " So you just import a Python library,", "tokens": [50364, 407, 291, 445, 974, 257, 15329, 6405, 11, 50486], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 470, "seek": 111330, "start": 1115.74, "end": 1116.78, "text": " you will create your model", "tokens": [50486, 291, 486, 1884, 428, 2316, 50538], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 471, "seek": 111330, "start": 1116.78, "end": 1119.5, "text": " and then you just do model that ZK proved", "tokens": [50538, 293, 550, 291, 445, 360, 2316, 300, 1176, 42, 14617, 50674], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 472, "seek": 111330, "start": 1119.5, "end": 1121.3, "text": " and you are able to ZK prove that", "tokens": [50674, 293, 291, 366, 1075, 281, 1176, 42, 7081, 300, 50764], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 473, "seek": 111330, "start": 1121.3, "end": 1123.06, "text": " without you as a Python engineer,", "tokens": [50764, 1553, 291, 382, 257, 15329, 11403, 11, 50852], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 474, "seek": 111330, "start": 1123.06, "end": 1124.3799999999999, "text": " as a machine learning engineer,", "tokens": [50852, 382, 257, 3479, 2539, 11403, 11, 50918], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 475, "seek": 111330, "start": 1124.3799999999999, "end": 1126.5, "text": " you don't need to know how ZK works.", "tokens": [50918, 291, 500, 380, 643, 281, 458, 577, 1176, 42, 1985, 13, 51024], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 476, "seek": 111330, "start": 1126.5, "end": 1127.62, "text": " You just create a proof of it", "tokens": [51024, 509, 445, 1884, 257, 8177, 295, 309, 51080], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 477, "seek": 111330, "start": 1127.62, "end": 1130.98, "text": " because Ithiko has built a tool that helped convert", "tokens": [51080, 570, 286, 392, 10770, 575, 3094, 257, 2290, 300, 4254, 7620, 51248], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 478, "seek": 111330, "start": 1130.98, "end": 1132.6599999999999, "text": " the way that you work with ML", "tokens": [51248, 264, 636, 300, 291, 589, 365, 21601, 51332], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 479, "seek": 111330, "start": 1132.6599999999999, "end": 1134.62, "text": " to something that cryptography can,", "tokens": [51332, 281, 746, 300, 9844, 5820, 393, 11, 51430], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 480, "seek": 111330, "start": 1134.62, "end": 1137.06, "text": " the cryptography tooling can create proofs of.", "tokens": [51430, 264, 9844, 5820, 46593, 393, 1884, 8177, 82, 295, 13, 51552], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 481, "seek": 111330, "start": 1137.06, "end": 1138.82, "text": " And of course, academia, academia", "tokens": [51552, 400, 295, 1164, 11, 28937, 11, 28937, 51640], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 482, "seek": 111330, "start": 1138.82, "end": 1141.1, "text": " has been a crucial element of all of this.", "tokens": [51640, 575, 668, 257, 11462, 4478, 295, 439, 295, 341, 13, 51754], "temperature": 0.0, "avg_logprob": -0.12154989571406924, "compression_ratio": 1.778546712802768, "no_speech_prob": 0.0023594233207404613}, {"id": 483, "seek": 114110, "start": 1141.1399999999999, "end": 1142.6999999999998, "text": " There's lots of cryptography,", "tokens": [50366, 821, 311, 3195, 295, 9844, 5820, 11, 50444], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 484, "seek": 114110, "start": 1142.6999999999998, "end": 1145.86, "text": " new cryptography coming out every week almost.", "tokens": [50444, 777, 9844, 5820, 1348, 484, 633, 1243, 1920, 13, 50602], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 485, "seek": 114110, "start": 1145.86, "end": 1147.58, "text": " There's new proving systems,", "tokens": [50602, 821, 311, 777, 27221, 3652, 11, 50688], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 486, "seek": 114110, "start": 1147.58, "end": 1149.86, "text": " there's new types of final field arithmetic,", "tokens": [50688, 456, 311, 777, 3467, 295, 2572, 2519, 42973, 11, 50802], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 487, "seek": 114110, "start": 1149.86, "end": 1152.54, "text": " there's new discoveries in different field.", "tokens": [50802, 456, 311, 777, 28400, 294, 819, 2519, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 488, "seek": 114110, "start": 1152.54, "end": 1153.74, "text": " There's different optimizations", "tokens": [50936, 821, 311, 819, 5028, 14455, 50996], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 489, "seek": 114110, "start": 1153.74, "end": 1156.5, "text": " like computing optimization from representation,", "tokens": [50996, 411, 15866, 19618, 490, 10290, 11, 51134], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 490, "seek": 114110, "start": 1156.5, "end": 1158.5, "text": " better models on the machine learning side.", "tokens": [51134, 1101, 5245, 322, 264, 3479, 2539, 1252, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 491, "seek": 114110, "start": 1158.5, "end": 1159.74, "text": " There's also improvements", "tokens": [51234, 821, 311, 611, 13797, 51296], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 492, "seek": 114110, "start": 1159.74, "end": 1162.02, "text": " and since usually ZK ML,", "tokens": [51296, 293, 1670, 2673, 1176, 42, 21601, 11, 51410], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 493, "seek": 114110, "start": 1162.02, "end": 1164.54, "text": " you need both things to become more performant.", "tokens": [51410, 291, 643, 1293, 721, 281, 1813, 544, 2042, 394, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 494, "seek": 114110, "start": 1164.54, "end": 1166.4599999999998, "text": " If academia comes up with better models", "tokens": [51536, 759, 28937, 1487, 493, 365, 1101, 5245, 51632], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 495, "seek": 114110, "start": 1166.4599999999998, "end": 1168.82, "text": " and better quantization schemes and whatnot,", "tokens": [51632, 293, 1101, 4426, 2144, 26954, 293, 25882, 11, 51750], "temperature": 0.0, "avg_logprob": -0.1347549313404521, "compression_ratio": 1.8028673835125448, "no_speech_prob": 0.00239641428925097}, {"id": 496, "seek": 116882, "start": 1168.82, "end": 1171.54, "text": " all of these improvements, compounds, right?", "tokens": [50364, 439, 295, 613, 13797, 11, 21810, 11, 558, 30, 50500], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 497, "seek": 116882, "start": 1171.54, "end": 1173.1799999999998, "text": " It's usually the worst of both,", "tokens": [50500, 467, 311, 2673, 264, 5855, 295, 1293, 11, 50582], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 498, "seek": 116882, "start": 1173.1799999999998, "end": 1176.3, "text": " the thing that becomes the worst for the aggregate.", "tokens": [50582, 264, 551, 300, 3643, 264, 5855, 337, 264, 26118, 13, 50738], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 499, "seek": 116882, "start": 1176.3, "end": 1178.3, "text": " So the worst of KML, or sorry,", "tokens": [50738, 407, 264, 5855, 295, 591, 12683, 11, 420, 2597, 11, 50838], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 500, "seek": 116882, "start": 1178.3, "end": 1179.9399999999998, "text": " the worst of ZK and the worst of ML", "tokens": [50838, 264, 5855, 295, 1176, 42, 293, 264, 5855, 295, 21601, 50920], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 501, "seek": 116882, "start": 1179.9399999999998, "end": 1183.06, "text": " become the worst of ZK ML, like the bottlenecks.", "tokens": [50920, 1813, 264, 5855, 295, 1176, 42, 21601, 11, 411, 264, 44641, 2761, 13, 51076], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 502, "seek": 116882, "start": 1183.06, "end": 1185.86, "text": " So academia is working a lot of the foundational bottlenecks", "tokens": [51076, 407, 28937, 307, 1364, 257, 688, 295, 264, 32195, 44641, 2761, 51216], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 503, "seek": 116882, "start": 1185.86, "end": 1187.74, "text": " when it comes to cryptography", "tokens": [51216, 562, 309, 1487, 281, 9844, 5820, 51310], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 504, "seek": 116882, "start": 1187.74, "end": 1189.86, "text": " and all these other things that I mentioned.", "tokens": [51310, 293, 439, 613, 661, 721, 300, 286, 2835, 13, 51416], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 505, "seek": 116882, "start": 1191.06, "end": 1192.9399999999998, "text": " So some of the use cases,", "tokens": [51476, 407, 512, 295, 264, 764, 3331, 11, 51570], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 506, "seek": 116882, "start": 1192.9399999999998, "end": 1194.5, "text": " I do wanna talk about use cases", "tokens": [51570, 286, 360, 1948, 751, 466, 764, 3331, 51648], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 507, "seek": 116882, "start": 1194.5, "end": 1195.9399999999998, "text": " because I've seen a lot of people", "tokens": [51648, 570, 286, 600, 1612, 257, 688, 295, 561, 51720], "temperature": 0.0, "avg_logprob": -0.10978110156842132, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.0006165581289678812}, {"id": 508, "seek": 119594, "start": 1195.94, "end": 1199.02, "text": " who are really deeply interested in the technology,", "tokens": [50364, 567, 366, 534, 8760, 3102, 294, 264, 2899, 11, 50518], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 509, "seek": 119594, "start": 1199.02, "end": 1201.06, "text": " but the only way that I've seen technology", "tokens": [50518, 457, 264, 787, 636, 300, 286, 600, 1612, 2899, 50620], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 510, "seek": 119594, "start": 1201.06, "end": 1204.5800000000002, "text": " actually progress forward is if there's funding", "tokens": [50620, 767, 4205, 2128, 307, 498, 456, 311, 6137, 50796], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 511, "seek": 119594, "start": 1204.5800000000002, "end": 1207.18, "text": " and people actually are interested and excited about it.", "tokens": [50796, 293, 561, 767, 366, 3102, 293, 2919, 466, 309, 13, 50926], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 512, "seek": 119594, "start": 1207.18, "end": 1209.18, "text": " For example, in the case of zero knowledge cryptography,", "tokens": [50926, 1171, 1365, 11, 294, 264, 1389, 295, 4018, 3601, 9844, 5820, 11, 51026], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 513, "seek": 119594, "start": 1209.18, "end": 1210.66, "text": " because I mostly spend most of my time", "tokens": [51026, 570, 286, 5240, 3496, 881, 295, 452, 565, 51100], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 514, "seek": 119594, "start": 1210.66, "end": 1212.1000000000001, "text": " in the blockchain space,", "tokens": [51100, 294, 264, 17176, 1901, 11, 51172], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 515, "seek": 119594, "start": 1212.1000000000001, "end": 1215.02, "text": " zero knowledge cryptography has become really popular", "tokens": [51172, 4018, 3601, 9844, 5820, 575, 1813, 534, 3743, 51318], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 516, "seek": 119594, "start": 1215.02, "end": 1216.38, "text": " in the last two to three years,", "tokens": [51318, 294, 264, 1036, 732, 281, 1045, 924, 11, 51386], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 517, "seek": 119594, "start": 1216.38, "end": 1219.22, "text": " mostly because there's been products that actually use it,", "tokens": [51386, 5240, 570, 456, 311, 668, 3383, 300, 767, 764, 309, 11, 51528], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 518, "seek": 119594, "start": 1219.22, "end": 1220.74, "text": " whether it's scalability solutions,", "tokens": [51528, 1968, 309, 311, 15664, 2310, 6547, 11, 51604], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 519, "seek": 119594, "start": 1220.74, "end": 1222.66, "text": " whether it's privacy solutions,", "tokens": [51604, 1968, 309, 311, 11427, 6547, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 520, "seek": 119594, "start": 1222.66, "end": 1224.7, "text": " whether it's digital identity solutions,", "tokens": [51700, 1968, 309, 311, 4562, 6575, 6547, 11, 51802], "temperature": 0.0, "avg_logprob": -0.11933399649227366, "compression_ratio": 1.9793103448275862, "no_speech_prob": 0.00898291077464819}, {"id": 521, "seek": 122470, "start": 1224.74, "end": 1226.9, "text": " there's been product market fit for this technology", "tokens": [50366, 456, 311, 668, 1674, 2142, 3318, 337, 341, 2899, 50474], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 522, "seek": 122470, "start": 1226.9, "end": 1228.82, "text": " and so new companies have been created", "tokens": [50474, 293, 370, 777, 3431, 362, 668, 2942, 50570], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 523, "seek": 122470, "start": 1228.82, "end": 1231.1000000000001, "text": " and a lot of capital has been poured in", "tokens": [50570, 293, 257, 688, 295, 4238, 575, 668, 23270, 294, 50684], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 524, "seek": 122470, "start": 1231.1000000000001, "end": 1233.78, "text": " and this capital was reinvested", "tokens": [50684, 293, 341, 4238, 390, 6561, 5571, 292, 50818], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 525, "seek": 122470, "start": 1233.78, "end": 1235.94, "text": " into the actual development of better cryptography,", "tokens": [50818, 666, 264, 3539, 3250, 295, 1101, 9844, 5820, 11, 50926], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 526, "seek": 122470, "start": 1235.94, "end": 1238.3, "text": " better tooling, better hardware,", "tokens": [50926, 1101, 46593, 11, 1101, 8837, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 527, "seek": 122470, "start": 1238.3, "end": 1241.1000000000001, "text": " and also there's a lot of network effects, right?", "tokens": [51044, 293, 611, 456, 311, 257, 688, 295, 3209, 5065, 11, 558, 30, 51184], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 528, "seek": 122470, "start": 1241.1000000000001, "end": 1243.3, "text": " So if there's lots of people using something,", "tokens": [51184, 407, 498, 456, 311, 3195, 295, 561, 1228, 746, 11, 51294], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 529, "seek": 122470, "start": 1243.3, "end": 1245.1000000000001, "text": " other vendors like hardware vendors", "tokens": [51294, 661, 22056, 411, 8837, 22056, 51384], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 530, "seek": 122470, "start": 1245.1000000000001, "end": 1247.3, "text": " might want to create hardware for these people,", "tokens": [51384, 1062, 528, 281, 1884, 8837, 337, 613, 561, 11, 51494], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 531, "seek": 122470, "start": 1247.3, "end": 1249.6200000000001, "text": " so it will even speed it up even further.", "tokens": [51494, 370, 309, 486, 754, 3073, 309, 493, 754, 3052, 13, 51610], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 532, "seek": 122470, "start": 1249.6200000000001, "end": 1251.9, "text": " So I do believe that, for example,", "tokens": [51610, 407, 286, 360, 1697, 300, 11, 337, 1365, 11, 51724], "temperature": 0.0, "avg_logprob": -0.12380097061395645, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0066901338286697865}, {"id": 533, "seek": 125190, "start": 1251.9, "end": 1254.0600000000002, "text": " ZKML needs a lot of product market fit", "tokens": [50364, 1176, 42, 12683, 2203, 257, 688, 295, 1674, 2142, 3318, 50472], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 534, "seek": 125190, "start": 1254.0600000000002, "end": 1256.8600000000001, "text": " or products or catalysts and use cases,", "tokens": [50472, 420, 3383, 420, 23868, 82, 293, 764, 3331, 11, 50612], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 535, "seek": 125190, "start": 1256.8600000000001, "end": 1259.3400000000001, "text": " which would improve the state of the art", "tokens": [50612, 597, 576, 3470, 264, 1785, 295, 264, 1523, 50736], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 536, "seek": 125190, "start": 1259.3400000000001, "end": 1261.46, "text": " just by the fact that there's many people looking at it,", "tokens": [50736, 445, 538, 264, 1186, 300, 456, 311, 867, 561, 1237, 412, 309, 11, 50842], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 537, "seek": 125190, "start": 1261.46, "end": 1264.38, "text": " there's a lot of mind share, there's high excitement.", "tokens": [50842, 456, 311, 257, 688, 295, 1575, 2073, 11, 456, 311, 1090, 14755, 13, 50988], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 538, "seek": 125190, "start": 1264.38, "end": 1266.42, "text": " So of course there's negative parts to this as well,", "tokens": [50988, 407, 295, 1164, 456, 311, 3671, 3166, 281, 341, 382, 731, 11, 51090], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 539, "seek": 125190, "start": 1266.42, "end": 1268.6200000000001, "text": " but mostly I think it's good.", "tokens": [51090, 457, 5240, 286, 519, 309, 311, 665, 13, 51200], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 540, "seek": 125190, "start": 1268.6200000000001, "end": 1271.5, "text": " So some of the use cases that I've seen around,", "tokens": [51200, 407, 512, 295, 264, 764, 3331, 300, 286, 600, 1612, 926, 11, 51344], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 541, "seek": 125190, "start": 1271.5, "end": 1272.8600000000001, "text": " so I personally work at WorldQuake", "tokens": [51344, 370, 286, 5665, 589, 412, 3937, 8547, 619, 51412], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 542, "seek": 125190, "start": 1272.8600000000001, "end": 1276.22, "text": " and that's like the way that I got exposed to it.", "tokens": [51412, 293, 300, 311, 411, 264, 636, 300, 286, 658, 9495, 281, 309, 13, 51580], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 543, "seek": 125190, "start": 1276.22, "end": 1279.66, "text": " I'll explain our specific youth case towards the end.", "tokens": [51580, 286, 603, 2903, 527, 2685, 7503, 1389, 3030, 264, 917, 13, 51752], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 544, "seek": 125190, "start": 1279.66, "end": 1281.42, "text": " So provable inference is one,", "tokens": [51752, 407, 1439, 712, 38253, 307, 472, 11, 51840], "temperature": 0.0, "avg_logprob": -0.13718929290771484, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.10658425092697144}, {"id": 545, "seek": 128142, "start": 1281.42, "end": 1285.5800000000002, "text": " so I mentioned earlier on that I do not know", "tokens": [50364, 370, 286, 2835, 3071, 322, 300, 286, 360, 406, 458, 50572], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 546, "seek": 128142, "start": 1285.5800000000002, "end": 1286.78, "text": " if I'm using chat GPT,", "tokens": [50572, 498, 286, 478, 1228, 5081, 26039, 51, 11, 50632], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 547, "seek": 128142, "start": 1286.78, "end": 1289.0600000000002, "text": " that someone is actually serving me", "tokens": [50632, 300, 1580, 307, 767, 8148, 385, 50746], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 548, "seek": 128142, "start": 1289.0600000000002, "end": 1290.9, "text": " the model that they claim they are.", "tokens": [50746, 264, 2316, 300, 436, 3932, 436, 366, 13, 50838], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 549, "seek": 128142, "start": 1290.9, "end": 1292.78, "text": " So provable inference is just this concept", "tokens": [50838, 407, 1439, 712, 38253, 307, 445, 341, 3410, 50932], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 550, "seek": 128142, "start": 1292.78, "end": 1296.3400000000001, "text": " where I can know that whomever who used a model", "tokens": [50932, 689, 286, 393, 458, 300, 315, 423, 331, 567, 1143, 257, 2316, 51110], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 551, "seek": 128142, "start": 1296.3400000000001, "end": 1298.9, "text": " to infer some output, I know where it came from.", "tokens": [51110, 281, 13596, 512, 5598, 11, 286, 458, 689, 309, 1361, 490, 13, 51238], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 552, "seek": 128142, "start": 1298.9, "end": 1300.1000000000001, "text": " I know which model it came from.", "tokens": [51238, 286, 458, 597, 2316, 309, 1361, 490, 13, 51298], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 553, "seek": 128142, "start": 1300.1000000000001, "end": 1301.3000000000002, "text": " If it's public, of course,", "tokens": [51298, 759, 309, 311, 1908, 11, 295, 1164, 11, 51358], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 554, "seek": 128142, "start": 1301.3000000000002, "end": 1304.3000000000002, "text": " the APIs can choose to keep the model private,", "tokens": [51358, 264, 21445, 393, 2826, 281, 1066, 264, 2316, 4551, 11, 51508], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 555, "seek": 128142, "start": 1304.3000000000002, "end": 1306.46, "text": " but at least they can, for example, commit to it.", "tokens": [51508, 457, 412, 1935, 436, 393, 11, 337, 1365, 11, 5599, 281, 309, 13, 51616], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 556, "seek": 128142, "start": 1306.46, "end": 1307.94, "text": " Something that I can do if, let's say,", "tokens": [51616, 6595, 300, 286, 393, 360, 498, 11, 718, 311, 584, 11, 51690], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 557, "seek": 128142, "start": 1307.94, "end": 1309.26, "text": " GPT-Force Quilt Force, right?", "tokens": [51690, 26039, 51, 12, 12587, 384, 2326, 2352, 10580, 11, 558, 30, 51756], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 558, "seek": 128142, "start": 1309.26, "end": 1310.7, "text": " Open source is not open source.", "tokens": [51756, 7238, 4009, 307, 406, 1269, 4009, 13, 51828], "temperature": 0.0, "avg_logprob": -0.18067536530671297, "compression_ratio": 1.732258064516129, "no_speech_prob": 0.0013457455206662416}, {"id": 559, "seek": 131070, "start": 1310.74, "end": 1313.8600000000001, "text": " OpenAI did not open source GPT-Force, as of now.", "tokens": [50366, 7238, 48698, 630, 406, 1269, 4009, 26039, 51, 12, 12587, 384, 11, 382, 295, 586, 13, 50522], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 560, "seek": 131070, "start": 1313.8600000000001, "end": 1316.38, "text": " And so if, for example,", "tokens": [50522, 400, 370, 498, 11, 337, 1365, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 561, "seek": 131070, "start": 1316.38, "end": 1319.38, "text": " I cannot know that someone used GPT-Force", "tokens": [50648, 286, 2644, 458, 300, 1580, 1143, 26039, 51, 12, 12587, 384, 50798], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 562, "seek": 131070, "start": 1319.38, "end": 1321.06, "text": " because I don't have the weights, right?", "tokens": [50798, 570, 286, 500, 380, 362, 264, 17443, 11, 558, 30, 50882], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 563, "seek": 131070, "start": 1321.06, "end": 1323.38, "text": " It's not a public thing.", "tokens": [50882, 467, 311, 406, 257, 1908, 551, 13, 50998], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 564, "seek": 131070, "start": 1323.38, "end": 1326.02, "text": " But something that OpenAI could do or anyone else", "tokens": [50998, 583, 746, 300, 7238, 48698, 727, 360, 420, 2878, 1646, 51130], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 565, "seek": 131070, "start": 1326.02, "end": 1327.7, "text": " with a private model could do", "tokens": [51130, 365, 257, 4551, 2316, 727, 360, 51214], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 566, "seek": 131070, "start": 1327.7, "end": 1331.14, "text": " is that they can commit to a specific model,", "tokens": [51214, 307, 300, 436, 393, 5599, 281, 257, 2685, 2316, 11, 51386], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 567, "seek": 131070, "start": 1331.14, "end": 1332.38, "text": " let's say a hash.", "tokens": [51386, 718, 311, 584, 257, 22019, 13, 51448], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 568, "seek": 131070, "start": 1332.38, "end": 1335.1000000000001, "text": " And for example, I know that for the entire user base,", "tokens": [51448, 400, 337, 1365, 11, 286, 458, 300, 337, 264, 2302, 4195, 3096, 11, 51584], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 569, "seek": 131070, "start": 1335.1000000000001, "end": 1336.66, "text": " they're using the same model.", "tokens": [51584, 436, 434, 1228, 264, 912, 2316, 13, 51662], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 570, "seek": 131070, "start": 1336.66, "end": 1338.22, "text": " So they cannot fool any single user", "tokens": [51662, 407, 436, 2644, 7979, 604, 2167, 4195, 51740], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 571, "seek": 131070, "start": 1338.22, "end": 1340.3, "text": " that they're using specific different models", "tokens": [51740, 300, 436, 434, 1228, 2685, 819, 5245, 51844], "temperature": 0.0, "avg_logprob": -0.09203631302406048, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0008693448617123067}, {"id": 572, "seek": 134030, "start": 1340.3, "end": 1341.46, "text": " for anyone else.", "tokens": [50364, 337, 2878, 1646, 13, 50422], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 573, "seek": 134030, "start": 1341.46, "end": 1343.18, "text": " At least they can commit to it", "tokens": [50422, 1711, 1935, 436, 393, 5599, 281, 309, 50508], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 574, "seek": 134030, "start": 1343.18, "end": 1345.62, "text": " with a cryptographic hash, so I can just hash.", "tokens": [50508, 365, 257, 9844, 12295, 22019, 11, 370, 286, 393, 445, 22019, 13, 50630], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 575, "seek": 134030, "start": 1345.62, "end": 1347.4199999999998, "text": " I know that there is one deterministic output", "tokens": [50630, 286, 458, 300, 456, 307, 472, 15957, 3142, 5598, 50720], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 576, "seek": 134030, "start": 1347.4199999999998, "end": 1348.26, "text": " for this model.", "tokens": [50720, 337, 341, 2316, 13, 50762], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 577, "seek": 134030, "start": 1348.26, "end": 1350.46, "text": " And I know every single user knows", "tokens": [50762, 400, 286, 458, 633, 2167, 4195, 3255, 50872], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 578, "seek": 134030, "start": 1350.46, "end": 1351.58, "text": " that they're using the same model", "tokens": [50872, 300, 436, 434, 1228, 264, 912, 2316, 50928], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 579, "seek": 134030, "start": 1351.58, "end": 1353.34, "text": " because within the zero-knowledge group,", "tokens": [50928, 570, 1951, 264, 4018, 12, 15869, 3042, 1594, 11, 51016], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 580, "seek": 134030, "start": 1353.34, "end": 1356.7, "text": " they have a commitment to some specific set of weights,", "tokens": [51016, 436, 362, 257, 8371, 281, 512, 2685, 992, 295, 17443, 11, 51184], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 581, "seek": 134030, "start": 1356.7, "end": 1358.02, "text": " but they do not reveal the weight.", "tokens": [51184, 457, 436, 360, 406, 10658, 264, 3364, 13, 51250], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 582, "seek": 134030, "start": 1358.02, "end": 1359.06, "text": " They're just committed.", "tokens": [51250, 814, 434, 445, 7784, 13, 51302], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 583, "seek": 134030, "start": 1359.06, "end": 1361.82, "text": " And maybe later, they open source the model,", "tokens": [51302, 400, 1310, 1780, 11, 436, 1269, 4009, 264, 2316, 11, 51440], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 584, "seek": 134030, "start": 1361.82, "end": 1363.02, "text": " they can reveal the weights,", "tokens": [51440, 436, 393, 10658, 264, 17443, 11, 51500], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 585, "seek": 134030, "start": 1363.02, "end": 1364.3, "text": " and you can see that the commitment", "tokens": [51500, 293, 291, 393, 536, 300, 264, 8371, 51564], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 586, "seek": 134030, "start": 1364.3, "end": 1365.46, "text": " does indeed match the weight.", "tokens": [51564, 775, 6451, 2995, 264, 3364, 13, 51622], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 587, "seek": 134030, "start": 1365.46, "end": 1369.5, "text": " So you actually learn that you did indeed learn about that.", "tokens": [51622, 407, 291, 767, 1466, 300, 291, 630, 6451, 1466, 466, 300, 13, 51824], "temperature": 0.0, "avg_logprob": -0.12810171103175683, "compression_ratio": 1.8681672025723473, "no_speech_prob": 0.00806038174778223}, {"id": 588, "seek": 136950, "start": 1369.5, "end": 1372.46, "text": " That they did actually use the model they claimed they were.", "tokens": [50364, 663, 436, 630, 767, 764, 264, 2316, 436, 12941, 436, 645, 13, 50512], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 589, "seek": 136950, "start": 1372.46, "end": 1373.54, "text": " In this case, GPT-4,", "tokens": [50512, 682, 341, 1389, 11, 26039, 51, 12, 19, 11, 50566], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 590, "seek": 136950, "start": 1373.54, "end": 1375.9, "text": " they managed to open source the weights.", "tokens": [50566, 436, 6453, 281, 1269, 4009, 264, 17443, 13, 50684], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 591, "seek": 136950, "start": 1375.9, "end": 1377.3, "text": " So that's provable inference.", "tokens": [50684, 407, 300, 311, 1439, 712, 38253, 13, 50754], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 592, "seek": 136950, "start": 1377.3, "end": 1378.58, "text": " It can be used for APIs.", "tokens": [50754, 467, 393, 312, 1143, 337, 21445, 13, 50818], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 593, "seek": 136950, "start": 1378.58, "end": 1379.78, "text": " So I mentioned like chat GPT,", "tokens": [50818, 407, 286, 2835, 411, 5081, 26039, 51, 11, 50878], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 594, "seek": 136950, "start": 1379.78, "end": 1381.7, "text": " but there's many others, like video games.", "tokens": [50878, 457, 456, 311, 867, 2357, 11, 411, 960, 2813, 13, 50974], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 595, "seek": 136950, "start": 1381.7, "end": 1383.34, "text": " If I'm playing an on-chain game", "tokens": [50974, 759, 286, 478, 2433, 364, 322, 12, 11509, 1216, 51056], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 596, "seek": 136950, "start": 1383.34, "end": 1384.82, "text": " and there's some form of ML,", "tokens": [51056, 293, 456, 311, 512, 1254, 295, 21601, 11, 51130], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 597, "seek": 136950, "start": 1384.82, "end": 1386.54, "text": " how do I know that the game is not cheating?", "tokens": [51130, 577, 360, 286, 458, 300, 264, 1216, 307, 406, 18309, 30, 51216], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 598, "seek": 136950, "start": 1386.54, "end": 1389.3, "text": " How do I know that I have fair rules on there?", "tokens": [51216, 1012, 360, 286, 458, 300, 286, 362, 3143, 4474, 322, 456, 30, 51354], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 599, "seek": 136950, "start": 1389.3, "end": 1391.22, "text": " The second one is bringing AI on chain.", "tokens": [51354, 440, 1150, 472, 307, 5062, 7318, 322, 5021, 13, 51450], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 600, "seek": 136950, "start": 1391.22, "end": 1393.18, "text": " So there's lots of smart contracts,", "tokens": [51450, 407, 456, 311, 3195, 295, 4069, 13952, 11, 51548], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 601, "seek": 136950, "start": 1393.18, "end": 1395.38, "text": " lots of applications that people are building", "tokens": [51548, 3195, 295, 5821, 300, 561, 366, 2390, 51658], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 602, "seek": 136950, "start": 1395.38, "end": 1396.94, "text": " within the blockchain domain.", "tokens": [51658, 1951, 264, 17176, 9274, 13, 51736], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 603, "seek": 136950, "start": 1396.94, "end": 1398.82, "text": " And within the blockchain domain right now,", "tokens": [51736, 400, 1951, 264, 17176, 9274, 558, 586, 11, 51830], "temperature": 0.0, "avg_logprob": -0.14543003259703172, "compression_ratio": 1.766961651917404, "no_speech_prob": 0.0013668903848156333}, {"id": 604, "seek": 139882, "start": 1398.82, "end": 1401.02, "text": " it's very limited in terms of things it can do.", "tokens": [50364, 309, 311, 588, 5567, 294, 2115, 295, 721, 309, 393, 360, 13, 50474], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 605, "seek": 139882, "start": 1401.02, "end": 1403.86, "text": " And machine learning can provide lots of cool solutions", "tokens": [50474, 400, 3479, 2539, 393, 2893, 3195, 295, 1627, 6547, 50616], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 606, "seek": 139882, "start": 1403.86, "end": 1405.86, "text": " to a lot of different problems, right?", "tokens": [50616, 281, 257, 688, 295, 819, 2740, 11, 558, 30, 50716], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 607, "seek": 139882, "start": 1405.86, "end": 1407.98, "text": " At the end of the day, machine learning is able to provide", "tokens": [50716, 1711, 264, 917, 295, 264, 786, 11, 3479, 2539, 307, 1075, 281, 2893, 50822], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 608, "seek": 139882, "start": 1407.98, "end": 1410.98, "text": " good enough approximations to problems that people have.", "tokens": [50822, 665, 1547, 8542, 763, 281, 2740, 300, 561, 362, 13, 50972], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 609, "seek": 139882, "start": 1410.98, "end": 1412.86, "text": " And so if we're able to bring that on chain,", "tokens": [50972, 400, 370, 498, 321, 434, 1075, 281, 1565, 300, 322, 5021, 11, 51066], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 610, "seek": 139882, "start": 1412.86, "end": 1415.22, "text": " we might be able to bring some interesting", "tokens": [51066, 321, 1062, 312, 1075, 281, 1565, 512, 1880, 51184], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 611, "seek": 139882, "start": 1415.22, "end": 1416.62, "text": " opportunity to the table.", "tokens": [51184, 2650, 281, 264, 3199, 13, 51254], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 612, "seek": 139882, "start": 1416.62, "end": 1418.34, "text": " I mentioned the financial ones,", "tokens": [51254, 286, 2835, 264, 4669, 2306, 11, 51340], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 613, "seek": 139882, "start": 1418.34, "end": 1421.54, "text": " where for example, I have a yield protocol on chain", "tokens": [51340, 689, 337, 1365, 11, 286, 362, 257, 11257, 10336, 322, 5021, 51500], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 614, "seek": 139882, "start": 1421.54, "end": 1424.34, "text": " where I deposit assets to this protocol", "tokens": [51500, 689, 286, 19107, 9769, 281, 341, 10336, 51640], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 615, "seek": 139882, "start": 1424.34, "end": 1426.6599999999999, "text": " and it tries to optimize the yield", "tokens": [51640, 293, 309, 9898, 281, 19719, 264, 11257, 51756], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 616, "seek": 139882, "start": 1426.6599999999999, "end": 1428.1399999999999, "text": " that I get on those assets.", "tokens": [51756, 300, 286, 483, 322, 729, 9769, 13, 51830], "temperature": 0.0, "avg_logprob": -0.14580506748623318, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.004133353475481272}, {"id": 617, "seek": 142814, "start": 1428.14, "end": 1430.3000000000002, "text": " But it can, it has to use a strategy.", "tokens": [50364, 583, 309, 393, 11, 309, 575, 281, 764, 257, 5206, 13, 50472], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 618, "seek": 142814, "start": 1430.3000000000002, "end": 1433.22, "text": " Usually these strategies are called source and hidden,", "tokens": [50472, 11419, 613, 9029, 366, 1219, 4009, 293, 7633, 11, 50618], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 619, "seek": 142814, "start": 1433.22, "end": 1435.7, "text": " but at least I can commit to a strategy.", "tokens": [50618, 457, 412, 1935, 286, 393, 5599, 281, 257, 5206, 13, 50742], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 620, "seek": 142814, "start": 1435.7, "end": 1437.7, "text": " And this strategy can now leverage machine learning", "tokens": [50742, 400, 341, 5206, 393, 586, 13982, 3479, 2539, 50842], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 621, "seek": 142814, "start": 1437.7, "end": 1439.5, "text": " and I can take proof to the protocol", "tokens": [50842, 293, 286, 393, 747, 8177, 281, 264, 10336, 50932], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 622, "seek": 142814, "start": 1439.5, "end": 1442.42, "text": " that we're using a machine learning strategy fairly", "tokens": [50932, 300, 321, 434, 1228, 257, 3479, 2539, 5206, 6457, 51078], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 623, "seek": 142814, "start": 1442.42, "end": 1444.3000000000002, "text": " and we're not updating the weights.", "tokens": [51078, 293, 321, 434, 406, 25113, 264, 17443, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 624, "seek": 142814, "start": 1444.3000000000002, "end": 1445.5, "text": " And we can also, for example,", "tokens": [51172, 400, 321, 393, 611, 11, 337, 1365, 11, 51232], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 625, "seek": 142814, "start": 1445.5, "end": 1447.38, "text": " prove that it was trained on some historical data", "tokens": [51232, 7081, 300, 309, 390, 8895, 322, 512, 8584, 1412, 51326], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 626, "seek": 142814, "start": 1447.38, "end": 1448.7800000000002, "text": " with some accuracy, right?", "tokens": [51326, 365, 512, 14170, 11, 558, 30, 51396], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 627, "seek": 142814, "start": 1448.7800000000002, "end": 1451.38, "text": " I can make a proof that my model is accurate", "tokens": [51396, 286, 393, 652, 257, 8177, 300, 452, 2316, 307, 8559, 51526], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 628, "seek": 142814, "start": 1451.38, "end": 1453.8600000000001, "text": " on some historical data in terms of yield routing", "tokens": [51526, 322, 512, 8584, 1412, 294, 2115, 295, 11257, 32722, 51650], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 629, "seek": 142814, "start": 1453.8600000000001, "end": 1455.66, "text": " with some accuracy and it's routed", "tokens": [51650, 365, 512, 14170, 293, 309, 311, 4020, 292, 51740], "temperature": 0.0, "avg_logprob": -0.13332243844972436, "compression_ratio": 1.8993055555555556, "no_speech_prob": 0.0004802518233191222}, {"id": 630, "seek": 145566, "start": 1455.66, "end": 1459.18, "text": " the most performant way, for example.", "tokens": [50364, 264, 881, 2042, 394, 636, 11, 337, 1365, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 631, "seek": 145566, "start": 1459.18, "end": 1462.02, "text": " There's also another one which is agents or intents", "tokens": [50540, 821, 311, 611, 1071, 472, 597, 307, 12554, 420, 560, 791, 50682], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 632, "seek": 145566, "start": 1462.02, "end": 1463.0600000000002, "text": " in the context of blockchain.", "tokens": [50682, 294, 264, 4319, 295, 17176, 13, 50734], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 633, "seek": 145566, "start": 1463.0600000000002, "end": 1466.5800000000002, "text": " So agents is a word that comes from the ML lingo,", "tokens": [50734, 407, 12554, 307, 257, 1349, 300, 1487, 490, 264, 21601, 287, 18459, 11, 50910], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 634, "seek": 145566, "start": 1466.5800000000002, "end": 1469.38, "text": " which is like a program that has the ability", "tokens": [50910, 597, 307, 411, 257, 1461, 300, 575, 264, 3485, 51050], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 635, "seek": 145566, "start": 1469.38, "end": 1470.8600000000001, "text": " to do actions on their own, right?", "tokens": [51050, 281, 360, 5909, 322, 641, 1065, 11, 558, 30, 51124], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 636, "seek": 145566, "start": 1470.8600000000001, "end": 1472.7, "text": " They're a player, some system,", "tokens": [51124, 814, 434, 257, 4256, 11, 512, 1185, 11, 51216], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 637, "seek": 145566, "start": 1472.7, "end": 1474.74, "text": " like game theoretical system in this case.", "tokens": [51216, 411, 1216, 20864, 1185, 294, 341, 1389, 13, 51318], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 638, "seek": 145566, "start": 1474.74, "end": 1477.5800000000002, "text": " So if we have some system, let's say that, I don't know,", "tokens": [51318, 407, 498, 321, 362, 512, 1185, 11, 718, 311, 584, 300, 11, 286, 500, 380, 458, 11, 51460], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 639, "seek": 145566, "start": 1477.5800000000002, "end": 1479.66, "text": " like we have a program and we allow", "tokens": [51460, 411, 321, 362, 257, 1461, 293, 321, 2089, 51564], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 640, "seek": 145566, "start": 1479.66, "end": 1481.98, "text": " this machine learning algorithm, let's say robotics.", "tokens": [51564, 341, 3479, 2539, 9284, 11, 718, 311, 584, 34145, 13, 51680], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 641, "seek": 145566, "start": 1481.98, "end": 1485.3000000000002, "text": " Robotic agents are, is a good analogy, right?", "tokens": [51680, 5424, 9411, 12554, 366, 11, 307, 257, 665, 21663, 11, 558, 30, 51846], "temperature": 0.0, "avg_logprob": -0.13367081994879737, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.005059928633272648}, {"id": 642, "seek": 148530, "start": 1485.34, "end": 1487.82, "text": " So I have a robot and the robot is able", "tokens": [50366, 407, 286, 362, 257, 7881, 293, 264, 7881, 307, 1075, 50490], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 643, "seek": 148530, "start": 1487.82, "end": 1489.1, "text": " to interact with the real world", "tokens": [50490, 281, 4648, 365, 264, 957, 1002, 50554], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 644, "seek": 148530, "start": 1489.1, "end": 1491.86, "text": " because it has limbs, it has different tools", "tokens": [50554, 570, 309, 575, 29315, 11, 309, 575, 819, 3873, 50692], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 645, "seek": 148530, "start": 1491.86, "end": 1493.18, "text": " like cameras, et cetera.", "tokens": [50692, 411, 8622, 11, 1030, 11458, 13, 50758], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 646, "seek": 148530, "start": 1493.18, "end": 1494.7, "text": " And it's able to interact with the real world.", "tokens": [50758, 400, 309, 311, 1075, 281, 4648, 365, 264, 957, 1002, 13, 50834], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 647, "seek": 148530, "start": 1494.7, "end": 1496.4199999999998, "text": " The robot in this case is an agent, right?", "tokens": [50834, 440, 7881, 294, 341, 1389, 307, 364, 9461, 11, 558, 30, 50920], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 648, "seek": 148530, "start": 1496.4199999999998, "end": 1498.7, "text": " It's a program which is able to perform actions", "tokens": [50920, 467, 311, 257, 1461, 597, 307, 1075, 281, 2042, 5909, 51034], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 649, "seek": 148530, "start": 1498.7, "end": 1499.54, "text": " in the real world.", "tokens": [51034, 294, 264, 957, 1002, 13, 51076], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 650, "seek": 148530, "start": 1499.54, "end": 1501.1399999999999, "text": " It doesn't have to be a real world agent.", "tokens": [51076, 467, 1177, 380, 362, 281, 312, 257, 957, 1002, 9461, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 651, "seek": 148530, "start": 1501.1399999999999, "end": 1502.22, "text": " It can be a digital agent.", "tokens": [51156, 467, 393, 312, 257, 4562, 9461, 13, 51210], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 652, "seek": 148530, "start": 1502.22, "end": 1503.5, "text": " It can interact with a website.", "tokens": [51210, 467, 393, 4648, 365, 257, 3144, 13, 51274], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 653, "seek": 148530, "start": 1503.5, "end": 1504.6599999999999, "text": " It can browse the web.", "tokens": [51274, 467, 393, 31442, 264, 3670, 13, 51332], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 654, "seek": 148530, "start": 1504.6599999999999, "end": 1508.18, "text": " It can watch a video and give me some information about it.", "tokens": [51332, 467, 393, 1159, 257, 960, 293, 976, 385, 512, 1589, 466, 309, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 655, "seek": 148530, "start": 1508.18, "end": 1510.7, "text": " But essentially on-chain agent could, for example,", "tokens": [51508, 583, 4476, 322, 12, 11509, 9461, 727, 11, 337, 1365, 11, 51634], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 656, "seek": 148530, "start": 1510.7, "end": 1512.34, "text": " interact with a blockchain", "tokens": [51634, 4648, 365, 257, 17176, 51716], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 657, "seek": 148530, "start": 1512.34, "end": 1513.98, "text": " if they have maybe some knowledge, right?", "tokens": [51716, 498, 436, 362, 1310, 512, 3601, 11, 558, 30, 51798], "temperature": 0.0, "avg_logprob": -0.11030586467069738, "compression_ratio": 1.9933774834437086, "no_speech_prob": 0.003324028104543686}, {"id": 658, "seek": 151398, "start": 1513.98, "end": 1516.02, "text": " So if it's a smart agent, it sees that,", "tokens": [50364, 407, 498, 309, 311, 257, 4069, 9461, 11, 309, 8194, 300, 11, 50466], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 659, "seek": 151398, "start": 1516.02, "end": 1517.46, "text": " okay, something happened here.", "tokens": [50466, 1392, 11, 746, 2011, 510, 13, 50538], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 660, "seek": 151398, "start": 1517.46, "end": 1519.8600000000001, "text": " So maybe I see that there's a liquidation happening.", "tokens": [50538, 407, 1310, 286, 536, 300, 456, 311, 257, 6553, 399, 2737, 13, 50658], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 661, "seek": 151398, "start": 1519.8600000000001, "end": 1522.38, "text": " So let me do this, let me buy this, let me sell that.", "tokens": [50658, 407, 718, 385, 360, 341, 11, 718, 385, 2256, 341, 11, 718, 385, 3607, 300, 13, 50784], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 662, "seek": 151398, "start": 1522.38, "end": 1524.8600000000001, "text": " There's different agents that can learn based on information", "tokens": [50784, 821, 311, 819, 12554, 300, 393, 1466, 2361, 322, 1589, 50908], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 663, "seek": 151398, "start": 1524.8600000000001, "end": 1527.54, "text": " and if they have a set of steps that they can do,", "tokens": [50908, 293, 498, 436, 362, 257, 992, 295, 4439, 300, 436, 393, 360, 11, 51042], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 664, "seek": 151398, "start": 1527.54, "end": 1529.8600000000001, "text": " they can maybe try and optimize for some goal", "tokens": [51042, 436, 393, 1310, 853, 293, 19719, 337, 512, 3387, 51158], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 665, "seek": 151398, "start": 1529.8600000000001, "end": 1531.8600000000001, "text": " and then they become agents in the system.", "tokens": [51158, 293, 550, 436, 1813, 12554, 294, 264, 1185, 13, 51258], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 666, "seek": 151398, "start": 1531.8600000000001, "end": 1534.74, "text": " Blockchain people like to call this intense, yeah?", "tokens": [51258, 48916, 561, 411, 281, 818, 341, 9447, 11, 1338, 30, 51402], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 667, "seek": 151398, "start": 1534.74, "end": 1537.82, "text": " And another one is attestations, for example.", "tokens": [51402, 400, 1071, 472, 307, 951, 377, 763, 11, 337, 1365, 13, 51556], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 668, "seek": 151398, "start": 1537.82, "end": 1540.66, "text": " So I can make attestations about things, right?", "tokens": [51556, 407, 286, 393, 652, 951, 377, 763, 466, 721, 11, 558, 30, 51698], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 669, "seek": 151398, "start": 1540.66, "end": 1543.34, "text": " I can prove to a smart contract that I'm over 18.", "tokens": [51698, 286, 393, 7081, 281, 257, 4069, 4364, 300, 286, 478, 670, 2443, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11451337322499017, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.0024724004324525595}, {"id": 670, "seek": 154334, "start": 1543.3799999999999, "end": 1547.1399999999999, "text": " I can prove that all sorts of different things, right?", "tokens": [50366, 286, 393, 7081, 300, 439, 7527, 295, 819, 721, 11, 558, 30, 50554], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 671, "seek": 154334, "start": 1547.1399999999999, "end": 1549.54, "text": " Essentially, I'm just able to use machine learning off-chain", "tokens": [50554, 23596, 11, 286, 478, 445, 1075, 281, 764, 3479, 2539, 766, 12, 11509, 50674], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 672, "seek": 154334, "start": 1549.54, "end": 1551.8999999999999, "text": " and I can prove that and bring it on-chain.", "tokens": [50674, 293, 286, 393, 7081, 300, 293, 1565, 309, 322, 12, 11509, 13, 50792], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 673, "seek": 154334, "start": 1551.8999999999999, "end": 1553.4199999999998, "text": " Private and machine learning cruise.", "tokens": [50792, 30386, 293, 3479, 2539, 17754, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 674, "seek": 154334, "start": 1553.4199999999998, "end": 1555.4599999999998, "text": " So this one is a cool one.", "tokens": [50868, 407, 341, 472, 307, 257, 1627, 472, 13, 50970], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 675, "seek": 154334, "start": 1555.4599999999998, "end": 1557.3, "text": " So for example, in the context of medicine,", "tokens": [50970, 407, 337, 1365, 11, 294, 264, 4319, 295, 7195, 11, 51062], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 676, "seek": 154334, "start": 1557.3, "end": 1561.06, "text": " let's say that there's a cancer diagnosis model", "tokens": [51062, 718, 311, 584, 300, 456, 311, 257, 5592, 15217, 2316, 51250], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 677, "seek": 154334, "start": 1561.06, "end": 1564.62, "text": " and I as the patient, I do not wanna reveal to anyone", "tokens": [51250, 293, 286, 382, 264, 4537, 11, 286, 360, 406, 1948, 10658, 281, 2878, 51428], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 678, "seek": 154334, "start": 1564.62, "end": 1567.58, "text": " like my personal health records.", "tokens": [51428, 411, 452, 2973, 1585, 7724, 13, 51576], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 679, "seek": 154334, "start": 1567.58, "end": 1568.98, "text": " But for example, there's a doctor", "tokens": [51576, 583, 337, 1365, 11, 456, 311, 257, 4631, 51646], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 680, "seek": 154334, "start": 1568.98, "end": 1570.6599999999999, "text": " or some health institution", "tokens": [51646, 420, 512, 1585, 7818, 51730], "temperature": 0.0, "avg_logprob": -0.13464505331856863, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00035695297992788255}, {"id": 681, "seek": 157066, "start": 1570.66, "end": 1574.1000000000001, "text": " which signs some form of report", "tokens": [50364, 597, 7880, 512, 1254, 295, 2275, 50536], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 682, "seek": 157066, "start": 1574.1000000000001, "end": 1575.5, "text": " or some form of certificates", "tokens": [50536, 420, 512, 1254, 295, 32941, 50606], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 683, "seek": 157066, "start": 1575.5, "end": 1578.42, "text": " to some personal health records or data.", "tokens": [50606, 281, 512, 2973, 1585, 7724, 420, 1412, 13, 50752], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 684, "seek": 157066, "start": 1578.42, "end": 1580.5800000000002, "text": " And then there's a machine learning model", "tokens": [50752, 400, 550, 456, 311, 257, 3479, 2539, 2316, 50860], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 685, "seek": 157066, "start": 1580.5800000000002, "end": 1583.42, "text": " which uses this data to essentially evaluate it", "tokens": [50860, 597, 4960, 341, 1412, 281, 4476, 13059, 309, 51002], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 686, "seek": 157066, "start": 1583.42, "end": 1585.94, "text": " and tells you whether you're likely to have cancer", "tokens": [51002, 293, 5112, 291, 1968, 291, 434, 3700, 281, 362, 5592, 51128], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 687, "seek": 157066, "start": 1585.94, "end": 1589.1000000000001, "text": " or whether you have cancer and with what's uncertainty.", "tokens": [51128, 420, 1968, 291, 362, 5592, 293, 365, 437, 311, 15697, 13, 51286], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 688, "seek": 157066, "start": 1589.1000000000001, "end": 1590.94, "text": " So something that you can hear", "tokens": [51286, 407, 746, 300, 291, 393, 1568, 51378], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 689, "seek": 157066, "start": 1590.94, "end": 1592.66, "text": " is that if you want to prove to, for example,", "tokens": [51378, 307, 300, 498, 291, 528, 281, 7081, 281, 11, 337, 1365, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 690, "seek": 157066, "start": 1592.66, "end": 1596.1000000000001, "text": " let's say that an insurance or a payout", "tokens": [51464, 718, 311, 584, 300, 364, 7214, 420, 257, 1689, 346, 51636], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 691, "seek": 157066, "start": 1596.1000000000001, "end": 1598.3000000000002, "text": " had a condition that you've been insured against", "tokens": [51636, 632, 257, 4188, 300, 291, 600, 668, 1028, 3831, 1970, 51746], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 692, "seek": 157066, "start": 1598.3000000000002, "end": 1599.98, "text": " or something like that,", "tokens": [51746, 420, 746, 411, 300, 11, 51830], "temperature": 0.0, "avg_logprob": -0.08464338684082032, "compression_ratio": 1.8074074074074074, "no_speech_prob": 0.0012643549125641584}, {"id": 693, "seek": 159998, "start": 1599.98, "end": 1601.42, "text": " you'd be able to prove to them", "tokens": [50364, 291, 1116, 312, 1075, 281, 7081, 281, 552, 50436], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 694, "seek": 159998, "start": 1601.42, "end": 1604.06, "text": " that there's some health institution", "tokens": [50436, 300, 456, 311, 512, 1585, 7818, 50568], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 695, "seek": 159998, "start": 1604.06, "end": 1606.9, "text": " or a specific health institution if you want", "tokens": [50568, 420, 257, 2685, 1585, 7818, 498, 291, 528, 50710], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 696, "seek": 159998, "start": 1606.9, "end": 1610.34, "text": " that has concluded that I am indeed this", "tokens": [50710, 300, 575, 22960, 300, 286, 669, 6451, 341, 50882], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 697, "seek": 159998, "start": 1610.34, "end": 1612.6200000000001, "text": " or have been diagnosed with a specific thing", "tokens": [50882, 420, 362, 668, 16899, 365, 257, 2685, 551, 50996], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 698, "seek": 159998, "start": 1612.6200000000001, "end": 1616.18, "text": " without revealing the model, without revealing the weight,", "tokens": [50996, 1553, 23983, 264, 2316, 11, 1553, 23983, 264, 3364, 11, 51174], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 699, "seek": 159998, "start": 1616.18, "end": 1618.3, "text": " but you at least know that there's some specific thing", "tokens": [51174, 457, 291, 412, 1935, 458, 300, 456, 311, 512, 2685, 551, 51280], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 700, "seek": 159998, "start": 1618.3, "end": 1620.18, "text": " that you can make a proof about.", "tokens": [51280, 300, 291, 393, 652, 257, 8177, 466, 13, 51374], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 701, "seek": 159998, "start": 1620.18, "end": 1622.5, "text": " The possibilities here are early big", "tokens": [51374, 440, 12178, 510, 366, 2440, 955, 51490], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 702, "seek": 159998, "start": 1622.5, "end": 1625.6200000000001, "text": " in the sense that there is generally programmable.", "tokens": [51490, 294, 264, 2020, 300, 456, 307, 5101, 37648, 712, 13, 51646], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 703, "seek": 159998, "start": 1625.6200000000001, "end": 1627.9, "text": " So this is like just one concrete example,", "tokens": [51646, 407, 341, 307, 411, 445, 472, 9859, 1365, 11, 51760], "temperature": 0.0, "avg_logprob": -0.13178507212934823, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0003682771639432758}, {"id": 704, "seek": 162790, "start": 1627.9, "end": 1631.5, "text": " but people can essentially make proofs about anything,", "tokens": [50364, 457, 561, 393, 4476, 652, 8177, 82, 466, 1340, 11, 50544], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 705, "seek": 162790, "start": 1631.5, "end": 1634.0600000000002, "text": " any data that they have and computations that they did", "tokens": [50544, 604, 1412, 300, 436, 362, 293, 2807, 763, 300, 436, 630, 50672], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 706, "seek": 162790, "start": 1634.0600000000002, "end": 1636.5800000000002, "text": " when they're machine learning base or not", "tokens": [50672, 562, 436, 434, 3479, 2539, 3096, 420, 406, 50798], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 707, "seek": 162790, "start": 1636.5800000000002, "end": 1638.5400000000002, "text": " without revealing the data itself, right?", "tokens": [50798, 1553, 23983, 264, 1412, 2564, 11, 558, 30, 50896], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 708, "seek": 162790, "start": 1638.5400000000002, "end": 1640.26, "text": " The only person who learns about the data", "tokens": [50896, 440, 787, 954, 567, 27152, 466, 264, 1412, 50982], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 709, "seek": 162790, "start": 1640.26, "end": 1643.3400000000001, "text": " in the context of ZK is the person making the proof.", "tokens": [50982, 294, 264, 4319, 295, 1176, 42, 307, 264, 954, 1455, 264, 8177, 13, 51136], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 710, "seek": 162790, "start": 1643.3400000000001, "end": 1646.42, "text": " I mentioned earlier that this property called completeness.", "tokens": [51136, 286, 2835, 3071, 300, 341, 4707, 1219, 1557, 15264, 13, 51290], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 711, "seek": 162790, "start": 1646.42, "end": 1648.26, "text": " So in order to make a valid proof,", "tokens": [51290, 407, 294, 1668, 281, 652, 257, 7363, 8177, 11, 51382], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 712, "seek": 162790, "start": 1648.26, "end": 1649.66, "text": " I do need to have the data.", "tokens": [51382, 286, 360, 643, 281, 362, 264, 1412, 13, 51452], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 713, "seek": 162790, "start": 1649.66, "end": 1652.5800000000002, "text": " So the problem is that there's always a prover,", "tokens": [51452, 407, 264, 1154, 307, 300, 456, 311, 1009, 257, 447, 331, 11, 51598], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 714, "seek": 162790, "start": 1652.5800000000002, "end": 1653.66, "text": " always learns the data,", "tokens": [51598, 1009, 27152, 264, 1412, 11, 51652], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 715, "seek": 162790, "start": 1653.66, "end": 1655.94, "text": " but if the prover is controlled by myself,", "tokens": [51652, 457, 498, 264, 447, 331, 307, 10164, 538, 2059, 11, 51766], "temperature": 0.0, "avg_logprob": -0.10090702858524046, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.00031999460770748556}, {"id": 716, "seek": 165594, "start": 1655.94, "end": 1658.66, "text": " then only it's the same thing as me learning data.", "tokens": [50364, 550, 787, 309, 311, 264, 912, 551, 382, 385, 2539, 1412, 13, 50500], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 717, "seek": 165594, "start": 1658.66, "end": 1660.6200000000001, "text": " So it's something that is a worthy trade-off.", "tokens": [50500, 407, 309, 311, 746, 300, 307, 257, 14829, 4923, 12, 4506, 13, 50598], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 718, "seek": 165594, "start": 1660.6200000000001, "end": 1662.7, "text": " So if I'm making a proof on my own computer,", "tokens": [50598, 407, 498, 286, 478, 1455, 257, 8177, 322, 452, 1065, 3820, 11, 50702], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 719, "seek": 165594, "start": 1662.7, "end": 1663.54, "text": " that makes sense.", "tokens": [50702, 300, 1669, 2020, 13, 50744], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 720, "seek": 165594, "start": 1663.54, "end": 1665.22, "text": " And I can prove to anyone else anything", "tokens": [50744, 400, 286, 393, 7081, 281, 2878, 1646, 1340, 50828], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 721, "seek": 165594, "start": 1665.22, "end": 1666.42, "text": " without revealing my data.", "tokens": [50828, 1553, 23983, 452, 1412, 13, 50888], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 722, "seek": 165594, "start": 1666.42, "end": 1668.78, "text": " But if I am, for example, delegating it to a server,", "tokens": [50888, 583, 498, 286, 669, 11, 337, 1365, 11, 15824, 990, 309, 281, 257, 7154, 11, 51006], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 723, "seek": 165594, "start": 1668.78, "end": 1670.6200000000001, "text": " the server doesn't need to learn my information,", "tokens": [51006, 264, 7154, 1177, 380, 643, 281, 1466, 452, 1589, 11, 51098], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 724, "seek": 165594, "start": 1670.6200000000001, "end": 1672.5, "text": " so I need to be careful.", "tokens": [51098, 370, 286, 643, 281, 312, 5026, 13, 51192], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 725, "seek": 165594, "start": 1672.5, "end": 1674.18, "text": " And digital identity.", "tokens": [51192, 400, 4562, 6575, 13, 51276], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 726, "seek": 165594, "start": 1674.18, "end": 1676.14, "text": " So I do want to explain very briefly,", "tokens": [51276, 407, 286, 360, 528, 281, 2903, 588, 10515, 11, 51374], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 727, "seek": 165594, "start": 1676.14, "end": 1678.8600000000001, "text": " like how did we come to this at WorldCoding?", "tokens": [51374, 411, 577, 630, 321, 808, 281, 341, 412, 3937, 34, 8616, 30, 51510], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 728, "seek": 165594, "start": 1678.8600000000001, "end": 1681.54, "text": " So we have this hardware device, it's called an org.", "tokens": [51510, 407, 321, 362, 341, 8837, 4302, 11, 309, 311, 1219, 364, 14045, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 729, "seek": 165594, "start": 1681.54, "end": 1682.94, "text": " I do have it with me.", "tokens": [51644, 286, 360, 362, 309, 365, 385, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14864599704742432, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.0006263139075599611}, {"id": 730, "seek": 168294, "start": 1682.94, "end": 1686.06, "text": " Maybe if you guys want, I can just go grab it.", "tokens": [50364, 2704, 498, 291, 1074, 528, 11, 286, 393, 445, 352, 4444, 309, 13, 50520], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 731, "seek": 168294, "start": 1686.06, "end": 1688.6200000000001, "text": " And one second in the FAQs, I can show it.", "tokens": [50520, 400, 472, 1150, 294, 264, 19894, 48, 82, 11, 286, 393, 855, 309, 13, 50648], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 732, "seek": 168294, "start": 1688.6200000000001, "end": 1692.14, "text": " But essentially the WorldCoding org is a piece of hardware", "tokens": [50648, 583, 4476, 264, 3937, 34, 8616, 14045, 307, 257, 2522, 295, 8837, 50824], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 733, "seek": 168294, "start": 1692.14, "end": 1693.8200000000002, "text": " that verifies two things.", "tokens": [50824, 300, 1306, 11221, 732, 721, 13, 50908], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 734, "seek": 168294, "start": 1693.8200000000002, "end": 1696.46, "text": " It proves that there's a real person", "tokens": [50908, 467, 25019, 300, 456, 311, 257, 957, 954, 51040], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 735, "seek": 168294, "start": 1696.46, "end": 1698.26, "text": " in front of this hardware device.", "tokens": [51040, 294, 1868, 295, 341, 8837, 4302, 13, 51130], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 736, "seek": 168294, "start": 1698.26, "end": 1702.98, "text": " It does this bunch of phenomenally detection like methods,", "tokens": [51130, 467, 775, 341, 3840, 295, 9388, 379, 17784, 411, 7150, 11, 51366], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 737, "seek": 168294, "start": 1702.98, "end": 1706.9, "text": " and some other like statistical-based methods,", "tokens": [51366, 293, 512, 661, 411, 22820, 12, 6032, 7150, 11, 51562], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 738, "seek": 168294, "start": 1706.9, "end": 1709.5800000000002, "text": " some sensors that it has like infrared sensors,", "tokens": [51562, 512, 14840, 300, 309, 575, 411, 30361, 14840, 11, 51696], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 739, "seek": 168294, "start": 1709.5800000000002, "end": 1711.3, "text": " and it has like field of depth sensors,", "tokens": [51696, 293, 309, 575, 411, 2519, 295, 7161, 14840, 11, 51782], "temperature": 0.0, "avg_logprob": -0.179902982711792, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0018673599697649479}, {"id": 740, "seek": 171130, "start": 1711.34, "end": 1713.82, "text": " it has high-resolution cameras, et cetera.", "tokens": [50366, 309, 575, 1090, 12, 495, 3386, 8622, 11, 1030, 11458, 13, 50490], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 741, "seek": 171130, "start": 1713.82, "end": 1715.6599999999999, "text": " And it's able to determine that there's a real person", "tokens": [50490, 400, 309, 311, 1075, 281, 6997, 300, 456, 311, 257, 957, 954, 50582], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 742, "seek": 171130, "start": 1715.6599999999999, "end": 1717.5, "text": " in front of the hardware device, the org,", "tokens": [50582, 294, 1868, 295, 264, 8837, 4302, 11, 264, 14045, 11, 50674], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 743, "seek": 171130, "start": 1717.5, "end": 1719.94, "text": " with like a shiny ball, I'll show it in a bit.", "tokens": [50674, 365, 411, 257, 16997, 2594, 11, 286, 603, 855, 309, 294, 257, 857, 13, 50796], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 744, "seek": 171130, "start": 1719.94, "end": 1723.3799999999999, "text": " And it can also prove the person in front of it is unique.", "tokens": [50796, 400, 309, 393, 611, 7081, 264, 954, 294, 1868, 295, 309, 307, 3845, 13, 50968], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 745, "seek": 171130, "start": 1723.3799999999999, "end": 1725.74, "text": " And the way that it does that is that it takes", "tokens": [50968, 400, 264, 636, 300, 309, 775, 300, 307, 300, 309, 2516, 51086], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 746, "seek": 171130, "start": 1725.74, "end": 1728.58, "text": " a high-resolution image of the person's IRC's,", "tokens": [51086, 257, 1090, 12, 495, 3386, 3256, 295, 264, 954, 311, 16486, 34, 311, 11, 51228], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 747, "seek": 171130, "start": 1728.58, "end": 1731.62, "text": " and it's able to compute a unique representation of them", "tokens": [51228, 293, 309, 311, 1075, 281, 14722, 257, 3845, 10290, 295, 552, 51380], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 748, "seek": 171130, "start": 1731.62, "end": 1733.02, "text": " called an IRS code.", "tokens": [51380, 1219, 364, 16486, 50, 3089, 13, 51450], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 749, "seek": 171130, "start": 1733.02, "end": 1735.06, "text": " And this IRS code, the good thing about it,", "tokens": [51450, 400, 341, 16486, 50, 3089, 11, 264, 665, 551, 466, 309, 11, 51552], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 750, "seek": 171130, "start": 1735.06, "end": 1736.5, "text": " is that it's not deemed personally", "tokens": [51552, 307, 300, 309, 311, 406, 27637, 5665, 51624], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 751, "seek": 171130, "start": 1736.5, "end": 1739.3799999999999, "text": " that if I put information, it's just the representation", "tokens": [51624, 300, 498, 286, 829, 1589, 11, 309, 311, 445, 264, 10290, 51768], "temperature": 0.0, "avg_logprob": -0.17490864399843994, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.00037407828494906425}, {"id": 752, "seek": 173938, "start": 1739.38, "end": 1743.5, "text": " of the uniqueness or of the randomness of a person's IRS,", "tokens": [50364, 295, 264, 48294, 420, 295, 264, 4974, 1287, 295, 257, 954, 311, 16486, 50, 11, 50570], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 753, "seek": 173938, "start": 1743.5, "end": 1746.18, "text": " and I can use that to measure how unique they are.", "tokens": [50570, 293, 286, 393, 764, 300, 281, 3481, 577, 3845, 436, 366, 13, 50704], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 754, "seek": 173938, "start": 1746.18, "end": 1748.5800000000002, "text": " And if the distance between two different IRS codes", "tokens": [50704, 400, 498, 264, 4560, 1296, 732, 819, 16486, 50, 14211, 50824], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 755, "seek": 173938, "start": 1748.5800000000002, "end": 1751.7800000000002, "text": " is big enough, I can prove that this user is unique.", "tokens": [50824, 307, 955, 1547, 11, 286, 393, 7081, 300, 341, 4195, 307, 3845, 13, 50984], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 756, "seek": 173938, "start": 1751.7800000000002, "end": 1753.8600000000001, "text": " And then once I prove that the person is unique,", "tokens": [50984, 400, 550, 1564, 286, 7081, 300, 264, 954, 307, 3845, 11, 51088], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 757, "seek": 173938, "start": 1753.8600000000001, "end": 1757.1000000000001, "text": " I'm able to essentially put them in a set of verified users.", "tokens": [51088, 286, 478, 1075, 281, 4476, 829, 552, 294, 257, 992, 295, 31197, 5022, 13, 51250], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 758, "seek": 173938, "start": 1757.1000000000001, "end": 1760.18, "text": " And then what we do is we have a protocol called WorldID,", "tokens": [51250, 400, 550, 437, 321, 360, 307, 321, 362, 257, 10336, 1219, 3937, 2777, 11, 51404], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 759, "seek": 173938, "start": 1760.18, "end": 1762.66, "text": " which allows you to prove that you're a member", "tokens": [51404, 597, 4045, 291, 281, 7081, 300, 291, 434, 257, 4006, 51528], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 760, "seek": 173938, "start": 1762.66, "end": 1764.7, "text": " of this set of verified users", "tokens": [51528, 295, 341, 992, 295, 31197, 5022, 51630], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 761, "seek": 173938, "start": 1764.7, "end": 1766.66, "text": " without revealing which member you are,", "tokens": [51630, 1553, 23983, 597, 4006, 291, 366, 11, 51728], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 762, "seek": 173938, "start": 1766.66, "end": 1768.18, "text": " using zoological photography as well,", "tokens": [51728, 1228, 710, 1092, 664, 804, 13865, 382, 731, 11, 51804], "temperature": 0.0, "avg_logprob": -0.10032492012813174, "compression_ratio": 1.8231292517006803, "no_speech_prob": 0.003222247352823615}, {"id": 763, "seek": 176818, "start": 1768.18, "end": 1771.7, "text": " but not ZKML, just traditional zero-knowledge cryptography.", "tokens": [50364, 457, 406, 1176, 42, 12683, 11, 445, 5164, 4018, 12, 15869, 3042, 9844, 5820, 13, 50540], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 764, "seek": 176818, "start": 1771.7, "end": 1774.3400000000001, "text": " You're able to prove that I am a unique verified human being", "tokens": [50540, 509, 434, 1075, 281, 7081, 300, 286, 669, 257, 3845, 31197, 1952, 885, 50672], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 765, "seek": 176818, "start": 1774.3400000000001, "end": 1775.98, "text": " without revealing who you are,", "tokens": [50672, 1553, 23983, 567, 291, 366, 11, 50754], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 766, "seek": 176818, "start": 1775.98, "end": 1778.46, "text": " and the data that we collect, which is just this IRS code,", "tokens": [50754, 293, 264, 1412, 300, 321, 2500, 11, 597, 307, 445, 341, 16486, 50, 3089, 11, 50878], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 767, "seek": 176818, "start": 1778.46, "end": 1780.22, "text": " is not personally data-fibre information.", "tokens": [50878, 307, 406, 5665, 1412, 12, 69, 897, 265, 1589, 13, 50966], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 768, "seek": 176818, "start": 1780.22, "end": 1783.78, "text": " We don't collect the raw biometric images, which is cool,", "tokens": [50966, 492, 500, 380, 2500, 264, 8936, 3228, 29470, 5267, 11, 597, 307, 1627, 11, 51144], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 769, "seek": 176818, "start": 1783.78, "end": 1786.46, "text": " because you're able to essentially leverage modern cryptography,", "tokens": [51144, 570, 291, 434, 1075, 281, 4476, 13982, 4363, 9844, 5820, 11, 51278], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 770, "seek": 176818, "start": 1786.46, "end": 1789.9, "text": " modern biometric literature, and the modern tools,", "tokens": [51278, 4363, 3228, 29470, 10394, 11, 293, 264, 4363, 3873, 11, 51450], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 771, "seek": 176818, "start": 1789.9, "end": 1792.22, "text": " like modern hardware like GPUs and everything.", "tokens": [51450, 411, 4363, 8837, 411, 18407, 82, 293, 1203, 13, 51566], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 772, "seek": 176818, "start": 1792.22, "end": 1793.54, "text": " Everything happens client-side,", "tokens": [51566, 5471, 2314, 6423, 12, 1812, 11, 51632], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 773, "seek": 176818, "start": 1793.54, "end": 1796.0600000000002, "text": " like within this actual hardware device, right?", "tokens": [51632, 411, 1951, 341, 3539, 8837, 4302, 11, 558, 30, 51758], "temperature": 0.0, "avg_logprob": -0.14950120952767385, "compression_ratio": 1.728125, "no_speech_prob": 0.0003740816318895668}, {"id": 774, "seek": 179606, "start": 1796.06, "end": 1798.6599999999999, "text": " So the hardware device does this computation,", "tokens": [50364, 407, 264, 8837, 4302, 775, 341, 24903, 11, 50494], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 775, "seek": 179606, "start": 1798.6599999999999, "end": 1800.58, "text": " nothing leaves the actual orb,", "tokens": [50494, 1825, 5510, 264, 3539, 14715, 11, 50590], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 776, "seek": 179606, "start": 1800.58, "end": 1804.3799999999999, "text": " and then the orb deletes everything within its secure enclave", "tokens": [50590, 293, 550, 264, 14715, 1103, 37996, 1203, 1951, 1080, 7144, 2058, 27995, 50780], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 777, "seek": 179606, "start": 1804.3799999999999, "end": 1806.54, "text": " and computational environment.", "tokens": [50780, 293, 28270, 2823, 13, 50888], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 778, "seek": 179606, "start": 1806.54, "end": 1809.06, "text": " Within this model of how work can work,", "tokens": [50888, 15996, 341, 2316, 295, 577, 589, 393, 589, 11, 51014], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 779, "seek": 179606, "start": 1809.06, "end": 1812.26, "text": " very simplistic model, there's one specific problem,", "tokens": [51014, 588, 44199, 2316, 11, 456, 311, 472, 2685, 1154, 11, 51174], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 780, "seek": 179606, "start": 1812.26, "end": 1814.46, "text": " which in our biometrics pipeline,", "tokens": [51174, 597, 294, 527, 3228, 649, 10716, 15517, 11, 51284], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 781, "seek": 179606, "start": 1814.46, "end": 1818.62, "text": " if you change the pipeline in any significant way,", "tokens": [51284, 498, 291, 1319, 264, 15517, 294, 604, 4776, 636, 11, 51492], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 782, "seek": 179606, "start": 1818.62, "end": 1821.54, "text": " you change the outputs of this uniqueness representation,", "tokens": [51492, 291, 1319, 264, 23930, 295, 341, 48294, 10290, 11, 51638], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 783, "seek": 179606, "start": 1821.54, "end": 1824.5, "text": " you change the output space of the IRS codes,", "tokens": [51638, 291, 1319, 264, 5598, 1901, 295, 264, 16486, 50, 14211, 11, 51786], "temperature": 0.0, "avg_logprob": -0.18529492999435565, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.001524638500995934}, {"id": 784, "seek": 182450, "start": 1824.5, "end": 1826.1, "text": " you can think of them as vectors, right?", "tokens": [50364, 291, 393, 519, 295, 552, 382, 18875, 11, 558, 30, 50444], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 785, "seek": 182450, "start": 1826.1, "end": 1828.7, "text": " So you essentially take this vector space,", "tokens": [50444, 407, 291, 4476, 747, 341, 8062, 1901, 11, 50574], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 786, "seek": 182450, "start": 1828.7, "end": 1830.38, "text": " and you convert it to a different one.", "tokens": [50574, 293, 291, 7620, 309, 281, 257, 819, 472, 13, 50658], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 787, "seek": 182450, "start": 1830.38, "end": 1833.66, "text": " So the same user will have a different representation", "tokens": [50658, 407, 264, 912, 4195, 486, 362, 257, 819, 10290, 50822], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 788, "seek": 182450, "start": 1833.66, "end": 1834.94, "text": " in this new space,", "tokens": [50822, 294, 341, 777, 1901, 11, 50886], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 789, "seek": 182450, "start": 1834.94, "end": 1838.26, "text": " therefore you will not be able to measure uniqueness anymore.", "tokens": [50886, 4412, 291, 486, 406, 312, 1075, 281, 3481, 48294, 3602, 13, 51052], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 790, "seek": 182450, "start": 1838.26, "end": 1839.98, "text": " So if you ever update the model,", "tokens": [51052, 407, 498, 291, 1562, 5623, 264, 2316, 11, 51138], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 791, "seek": 182450, "start": 1839.98, "end": 1842.02, "text": " you have to re-sign up all users.", "tokens": [51138, 291, 362, 281, 319, 12, 82, 788, 493, 439, 5022, 13, 51240], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 792, "seek": 182450, "start": 1842.02, "end": 1844.1, "text": " And since you have this physical hardware device", "tokens": [51240, 400, 1670, 291, 362, 341, 4001, 8837, 4302, 51344], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 793, "seek": 182450, "start": 1844.1, "end": 1845.38, "text": " that people have to go to,", "tokens": [51344, 300, 561, 362, 281, 352, 281, 11, 51408], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 794, "seek": 182450, "start": 1845.38, "end": 1847.66, "text": " it means that all the users that have signed up to date", "tokens": [51408, 309, 1355, 300, 439, 264, 5022, 300, 362, 8175, 493, 281, 4002, 51522], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 795, "seek": 182450, "start": 1847.66, "end": 1851.66, "text": " to WorldID have to go in person again to this hardware device", "tokens": [51522, 281, 3937, 2777, 362, 281, 352, 294, 954, 797, 281, 341, 8837, 4302, 51722], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 796, "seek": 182450, "start": 1851.66, "end": 1852.94, "text": " and get re-signed up.", "tokens": [51722, 293, 483, 319, 12, 82, 16690, 493, 13, 51786], "temperature": 0.0, "avg_logprob": -0.11961334903223984, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.018259909003973007}, {"id": 797, "seek": 185294, "start": 1852.94, "end": 1854.14, "text": " And this is terrible,", "tokens": [50364, 400, 341, 307, 6237, 11, 50424], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 798, "seek": 185294, "start": 1854.14, "end": 1856.66, "text": " because it's already been really hard enough", "tokens": [50424, 570, 309, 311, 1217, 668, 534, 1152, 1547, 50550], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 799, "seek": 185294, "start": 1856.66, "end": 1859.06, "text": " for us to get 5 million plus users", "tokens": [50550, 337, 505, 281, 483, 1025, 2459, 1804, 5022, 50670], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 800, "seek": 185294, "start": 1859.06, "end": 1861.98, "text": " and to have to force our users to re-sign up", "tokens": [50670, 293, 281, 362, 281, 3464, 527, 5022, 281, 319, 12, 82, 788, 493, 50816], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 801, "seek": 185294, "start": 1861.98, "end": 1864.38, "text": " every single time that we update the biometrics pipeline,", "tokens": [50816, 633, 2167, 565, 300, 321, 5623, 264, 3228, 649, 10716, 15517, 11, 50936], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 802, "seek": 185294, "start": 1864.38, "end": 1866.22, "text": " it would be really bad.", "tokens": [50936, 309, 576, 312, 534, 1578, 13, 51028], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 803, "seek": 185294, "start": 1866.22, "end": 1868.42, "text": " And it has like really terrible user experience.", "tokens": [51028, 400, 309, 575, 411, 534, 6237, 4195, 1752, 13, 51138], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 804, "seek": 185294, "start": 1868.42, "end": 1870.66, "text": " And so this is where one of my coworkers,", "tokens": [51138, 400, 370, 341, 307, 689, 472, 295, 452, 43465, 11, 51250], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 805, "seek": 185294, "start": 1870.66, "end": 1873.3, "text": " his name is Remco, at the WorldCon Foundation,", "tokens": [51250, 702, 1315, 307, 4080, 1291, 11, 412, 264, 3937, 9838, 10335, 11, 51382], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 806, "seek": 185294, "start": 1873.3, "end": 1877.74, "text": " he came across with a solution or an idea,", "tokens": [51382, 415, 1361, 2108, 365, 257, 3827, 420, 364, 1558, 11, 51604], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 807, "seek": 185294, "start": 1877.74, "end": 1882.14, "text": " which was what if users self-custody their own biometrics,", "tokens": [51604, 597, 390, 437, 498, 5022, 2698, 12, 66, 381, 843, 641, 1065, 3228, 649, 10716, 11, 51824], "temperature": 0.0, "avg_logprob": -0.17795829486129874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.001837951480410993}, {"id": 808, "seek": 188214, "start": 1882.14, "end": 1883.66, "text": " meaning that the orb,", "tokens": [50364, 3620, 300, 264, 14715, 11, 50440], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 809, "seek": 188214, "start": 1883.66, "end": 1886.0200000000002, "text": " which has essentially a secure enclave", "tokens": [50440, 597, 575, 4476, 257, 7144, 2058, 27995, 50558], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 810, "seek": 188214, "start": 1886.0200000000002, "end": 1888.0200000000002, "text": " and a trusted execution environment.", "tokens": [50558, 293, 257, 16034, 15058, 2823, 13, 50658], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 811, "seek": 188214, "start": 1888.0200000000002, "end": 1890.74, "text": " So essentially these two pieces of chips", "tokens": [50658, 407, 4476, 613, 732, 3755, 295, 11583, 50794], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 812, "seek": 188214, "start": 1890.74, "end": 1893.5400000000002, "text": " or these two chips allow you to sign things.", "tokens": [50794, 420, 613, 732, 11583, 2089, 291, 281, 1465, 721, 13, 50934], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 813, "seek": 188214, "start": 1893.5400000000002, "end": 1895.3000000000002, "text": " So I'm able to cryptograph a big sign,", "tokens": [50934, 407, 286, 478, 1075, 281, 9844, 3108, 257, 955, 1465, 11, 51022], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 814, "seek": 188214, "start": 1895.3000000000002, "end": 1897.38, "text": " something that the orb sees.", "tokens": [51022, 746, 300, 264, 14715, 8194, 13, 51126], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 815, "seek": 188214, "start": 1897.38, "end": 1899.0200000000002, "text": " So whenever the user gets verified", "tokens": [51126, 407, 5699, 264, 4195, 2170, 31197, 51208], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 816, "seek": 188214, "start": 1899.0200000000002, "end": 1901.3400000000001, "text": " that they're a real and unique person,", "tokens": [51208, 300, 436, 434, 257, 957, 293, 3845, 954, 11, 51324], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 817, "seek": 188214, "start": 1901.3400000000001, "end": 1903.26, "text": " the orb can sign their raw biometric,", "tokens": [51324, 264, 14715, 393, 1465, 641, 8936, 3228, 29470, 11, 51420], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 818, "seek": 188214, "start": 1903.26, "end": 1906.42, "text": " which it has in its memory for a given lifetime,", "tokens": [51420, 597, 309, 575, 294, 1080, 4675, 337, 257, 2212, 11364, 11, 51578], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 819, "seek": 188214, "start": 1906.42, "end": 1908.46, "text": " and it can give it to the user,", "tokens": [51578, 293, 309, 393, 976, 309, 281, 264, 4195, 11, 51680], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 820, "seek": 188214, "start": 1908.46, "end": 1911.66, "text": " and the user can store their own biometrics", "tokens": [51680, 293, 264, 4195, 393, 3531, 641, 1065, 3228, 649, 10716, 51840], "temperature": 0.0, "avg_logprob": -0.16628411990493092, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.05258123204112053}, {"id": 821, "seek": 191166, "start": 1911.66, "end": 1914.78, "text": " in their phone and they can encrypt them, of course,", "tokens": [50364, 294, 641, 2593, 293, 436, 393, 17972, 662, 552, 11, 295, 1164, 11, 50520], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 822, "seek": 191166, "start": 1914.78, "end": 1916.46, "text": " store them safely in an encrypted fashion", "tokens": [50520, 3531, 552, 11750, 294, 364, 36663, 6700, 50604], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 823, "seek": 191166, "start": 1916.46, "end": 1919.5800000000002, "text": " on their own phone or cloud or whatever they prefer.", "tokens": [50604, 322, 641, 1065, 2593, 420, 4588, 420, 2035, 436, 4382, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 824, "seek": 191166, "start": 1919.5800000000002, "end": 1922.5400000000002, "text": " And they would be able to then have a signature", "tokens": [50760, 400, 436, 576, 312, 1075, 281, 550, 362, 257, 13397, 50908], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 825, "seek": 191166, "start": 1922.5400000000002, "end": 1924.9, "text": " from the orb on the actual biometric, right?", "tokens": [50908, 490, 264, 14715, 322, 264, 3539, 3228, 29470, 11, 558, 30, 51026], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 826, "seek": 191166, "start": 1924.9, "end": 1928.6200000000001, "text": " You know that this image was seen at one point by the orb", "tokens": [51026, 509, 458, 300, 341, 3256, 390, 1612, 412, 472, 935, 538, 264, 14715, 51212], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 827, "seek": 191166, "start": 1928.6200000000001, "end": 1930.8600000000001, "text": " and it said that this is a unique human being", "tokens": [51212, 293, 309, 848, 300, 341, 307, 257, 3845, 1952, 885, 51324], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 828, "seek": 191166, "start": 1930.8600000000001, "end": 1932.18, "text": " and this is a real human being,", "tokens": [51324, 293, 341, 307, 257, 957, 1952, 885, 11, 51390], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 829, "seek": 191166, "start": 1932.18, "end": 1934.1000000000001, "text": " most importantly is the real part.", "tokens": [51390, 881, 8906, 307, 264, 957, 644, 13, 51486], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 830, "seek": 191166, "start": 1934.1000000000001, "end": 1937.22, "text": " Whenever we want to update the model,", "tokens": [51486, 14159, 321, 528, 281, 5623, 264, 2316, 11, 51642], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 831, "seek": 191166, "start": 1937.22, "end": 1939.38, "text": " what the user could do is they would be able", "tokens": [51642, 437, 264, 4195, 727, 360, 307, 436, 576, 312, 1075, 51750], "temperature": 0.0, "avg_logprob": -0.11607291481711647, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.0028445511125028133}, {"id": 832, "seek": 193938, "start": 1939.42, "end": 1942.14, "text": " to download the new model, the weights of the model,", "tokens": [50366, 281, 5484, 264, 777, 2316, 11, 264, 17443, 295, 264, 2316, 11, 50502], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 833, "seek": 193938, "start": 1942.14, "end": 1945.1000000000001, "text": " and they seek the approving library for that specific model", "tokens": [50502, 293, 436, 8075, 264, 2075, 798, 6405, 337, 300, 2685, 2316, 50650], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 834, "seek": 193938, "start": 1945.1000000000001, "end": 1946.5, "text": " and they would be able to create a group", "tokens": [50650, 293, 436, 576, 312, 1075, 281, 1884, 257, 1594, 50720], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 835, "seek": 193938, "start": 1946.5, "end": 1951.0600000000002, "text": " that created this iris code within a zero-knowledge environment.", "tokens": [50720, 300, 2942, 341, 3418, 271, 3089, 1951, 257, 4018, 12, 15869, 3042, 2823, 13, 50948], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 836, "seek": 193938, "start": 1951.0600000000002, "end": 1953.14, "text": " So they would be able to create a zero-knowledge proof", "tokens": [50948, 407, 436, 576, 312, 1075, 281, 1884, 257, 4018, 12, 15869, 3042, 8177, 51052], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 837, "seek": 193938, "start": 1953.14, "end": 1955.1000000000001, "text": " that they've created a valid iris code", "tokens": [51052, 300, 436, 600, 2942, 257, 7363, 3418, 271, 3089, 51150], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 838, "seek": 193938, "start": 1955.1000000000001, "end": 1958.22, "text": " from an input image, which was attested to by the orb.", "tokens": [51150, 490, 364, 4846, 3256, 11, 597, 390, 951, 21885, 281, 538, 264, 14715, 13, 51306], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 839, "seek": 193938, "start": 1958.22, "end": 1961.8200000000002, "text": " So essentially the pipeline of trust here is not broke, right?", "tokens": [51306, 407, 4476, 264, 15517, 295, 3361, 510, 307, 406, 6902, 11, 558, 30, 51486], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 840, "seek": 193938, "start": 1961.8200000000002, "end": 1964.0200000000002, "text": " I know that I've created an iris code", "tokens": [51486, 286, 458, 300, 286, 600, 2942, 364, 3418, 271, 3089, 51596], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 841, "seek": 193938, "start": 1964.0200000000002, "end": 1965.1000000000001, "text": " from an original biometric,", "tokens": [51596, 490, 364, 3380, 3228, 29470, 11, 51650], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 842, "seek": 193938, "start": 1965.1000000000001, "end": 1967.18, "text": " which was verified by the orb to be unique.", "tokens": [51650, 597, 390, 31197, 538, 264, 14715, 281, 312, 3845, 13, 51754], "temperature": 0.0, "avg_logprob": -0.12707085445009428, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.003376094624400139}, {"id": 843, "seek": 196718, "start": 1967.22, "end": 1970.74, "text": " And with this, I'm able to permissionlessly", "tokens": [50366, 400, 365, 341, 11, 286, 478, 1075, 281, 11226, 12048, 50542], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 844, "seek": 196718, "start": 1970.74, "end": 1972.42, "text": " or out of my own accord,", "tokens": [50542, 420, 484, 295, 452, 1065, 18640, 11, 50626], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 845, "seek": 196718, "start": 1972.42, "end": 1975.02, "text": " I'm able to permissionlessly insert myself", "tokens": [50626, 286, 478, 1075, 281, 11226, 12048, 8969, 2059, 50756], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 846, "seek": 196718, "start": 1975.02, "end": 1976.9, "text": " into the set of verified users", "tokens": [50756, 666, 264, 992, 295, 31197, 5022, 50850], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 847, "seek": 196718, "start": 1976.9, "end": 1978.66, "text": " without having to go to the orb again", "tokens": [50850, 1553, 1419, 281, 352, 281, 264, 14715, 797, 50938], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 848, "seek": 196718, "start": 1978.66, "end": 1981.1000000000001, "text": " because I have the entire set of steps that I need", "tokens": [50938, 570, 286, 362, 264, 2302, 992, 295, 4439, 300, 286, 643, 51060], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 849, "seek": 196718, "start": 1981.1000000000001, "end": 1983.74, "text": " in order to prove to the world ID protocol", "tokens": [51060, 294, 1668, 281, 7081, 281, 264, 1002, 7348, 10336, 51192], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 850, "seek": 196718, "start": 1983.74, "end": 1986.26, "text": " that I'm a unique user without revealing who I am again.", "tokens": [51192, 300, 286, 478, 257, 3845, 4195, 1553, 23983, 567, 286, 669, 797, 13, 51318], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 851, "seek": 196718, "start": 1986.26, "end": 1989.3, "text": " I just proved to you that, hey, the orb saw me at one point in time,", "tokens": [51318, 286, 445, 14617, 281, 291, 300, 11, 4177, 11, 264, 14715, 1866, 385, 412, 472, 935, 294, 565, 11, 51470], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 852, "seek": 196718, "start": 1989.3, "end": 1992.8200000000002, "text": " the orb did indeed sign this my image, I store my images,", "tokens": [51470, 264, 14715, 630, 6451, 1465, 341, 452, 3256, 11, 286, 3531, 452, 5267, 11, 51646], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 853, "seek": 196718, "start": 1992.8200000000002, "end": 1993.66, "text": " and then I make a proof", "tokens": [51646, 293, 550, 286, 652, 257, 8177, 51688], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 854, "seek": 196718, "start": 1993.66, "end": 1996.18, "text": " that I've created this derived representation of uniqueness", "tokens": [51688, 300, 286, 600, 2942, 341, 18949, 10290, 295, 48294, 51814], "temperature": 0.0, "avg_logprob": -0.12493125706502836, "compression_ratio": 1.8435374149659864, "no_speech_prob": 0.002889288356527686}, {"id": 855, "seek": 199618, "start": 1996.18, "end": 1998.02, "text": " for my biometrics,", "tokens": [50364, 337, 452, 3228, 649, 10716, 11, 50456], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 856, "seek": 199618, "start": 1998.02, "end": 1999.66, "text": " and I can prove to them I'm a unique human", "tokens": [50456, 293, 286, 393, 7081, 281, 552, 286, 478, 257, 3845, 1952, 50538], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 857, "seek": 199618, "start": 1999.66, "end": 2001.94, "text": " in this new representation and this new model", "tokens": [50538, 294, 341, 777, 10290, 293, 341, 777, 2316, 50652], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 858, "seek": 199618, "start": 2001.94, "end": 2004.02, "text": " without revealing who I am again, right?", "tokens": [50652, 1553, 23983, 567, 286, 669, 797, 11, 558, 30, 50756], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 859, "seek": 199618, "start": 2004.02, "end": 2006.0600000000002, "text": " So this is like perfect things,", "tokens": [50756, 407, 341, 307, 411, 2176, 721, 11, 50858], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 860, "seek": 199618, "start": 2006.0600000000002, "end": 2007.5, "text": " like a perfect solution for us.", "tokens": [50858, 411, 257, 2176, 3827, 337, 505, 13, 50930], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 861, "seek": 199618, "start": 2007.5, "end": 2012.5, "text": " It's actually quite crazy that this problem didn't exist,", "tokens": [50930, 467, 311, 767, 1596, 3219, 300, 341, 1154, 994, 380, 2514, 11, 51180], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 862, "seek": 199618, "start": 2013.5, "end": 2017.26, "text": " at least that's what I felt when I first covered it through Remco.", "tokens": [51230, 412, 1935, 300, 311, 437, 286, 2762, 562, 286, 700, 5343, 309, 807, 4080, 1291, 13, 51418], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 863, "seek": 199618, "start": 2017.26, "end": 2018.9, "text": " And so right now, for example,", "tokens": [51418, 400, 370, 558, 586, 11, 337, 1365, 11, 51500], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 864, "seek": 199618, "start": 2018.9, "end": 2020.3, "text": " we're working with one of the companies", "tokens": [51500, 321, 434, 1364, 365, 472, 295, 264, 3431, 51570], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 865, "seek": 199618, "start": 2020.3, "end": 2021.38, "text": " which I mentioned earlier,", "tokens": [51570, 597, 286, 2835, 3071, 11, 51624], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 866, "seek": 199618, "start": 2021.38, "end": 2024.94, "text": " Modulus to essentially do this client-side", "tokens": [51624, 6583, 26107, 281, 4476, 360, 341, 6423, 12, 1812, 51802], "temperature": 0.0, "avg_logprob": -0.19776987306999438, "compression_ratio": 1.5933333333333333, "no_speech_prob": 0.004398246295750141}, {"id": 867, "seek": 202494, "start": 2024.94, "end": 2026.8600000000001, "text": " zero-knowledge machine learning proving", "tokens": [50364, 4018, 12, 15869, 3042, 3479, 2539, 27221, 50460], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 868, "seek": 202494, "start": 2026.8600000000001, "end": 2028.5800000000002, "text": " inside of a user's phone, right?", "tokens": [50460, 1854, 295, 257, 4195, 311, 2593, 11, 558, 30, 50546], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 869, "seek": 202494, "start": 2028.5800000000002, "end": 2030.5800000000002, "text": " So that people can self-cassellate the biometrics", "tokens": [50546, 407, 300, 561, 393, 2698, 12, 66, 8604, 285, 473, 264, 3228, 649, 10716, 50646], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 870, "seek": 202494, "start": 2030.5800000000002, "end": 2032.46, "text": " and permissionlessly insert themselves.", "tokens": [50646, 293, 11226, 12048, 8969, 2969, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 871, "seek": 202494, "start": 2032.46, "end": 2033.66, "text": " It's like very early stages,", "tokens": [50740, 467, 311, 411, 588, 2440, 10232, 11, 50800], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 872, "seek": 202494, "start": 2033.66, "end": 2035.14, "text": " R&D is not yet in production,", "tokens": [50800, 497, 5, 35, 307, 406, 1939, 294, 4265, 11, 50874], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 873, "seek": 202494, "start": 2035.14, "end": 2037.26, "text": " but there's been a lot of good progress here.", "tokens": [50874, 457, 456, 311, 668, 257, 688, 295, 665, 4205, 510, 13, 50980], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 874, "seek": 202494, "start": 2037.26, "end": 2039.5, "text": " And two years ago, it seemed like sci-fi,", "tokens": [50980, 400, 732, 924, 2057, 11, 309, 6576, 411, 2180, 12, 13325, 11, 51092], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 875, "seek": 202494, "start": 2039.5, "end": 2042.02, "text": " now there's already like concrete proof concepts", "tokens": [51092, 586, 456, 311, 1217, 411, 9859, 8177, 10392, 51218], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 876, "seek": 202494, "start": 2042.02, "end": 2044.8600000000001, "text": " and implementation and there's benchmarks", "tokens": [51218, 293, 11420, 293, 456, 311, 43751, 51360], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 877, "seek": 202494, "start": 2044.8600000000001, "end": 2046.6200000000001, "text": " and things that are improving.", "tokens": [51360, 293, 721, 300, 366, 11470, 13, 51448], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 878, "seek": 202494, "start": 2046.6200000000001, "end": 2049.5, "text": " But yeah, this is like one of the things that I saw.", "tokens": [51448, 583, 1338, 11, 341, 307, 411, 472, 295, 264, 721, 300, 286, 1866, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 879, "seek": 202494, "start": 2050.98, "end": 2052.58, "text": " One last thing that I do want to mention", "tokens": [51666, 1485, 1036, 551, 300, 286, 360, 528, 281, 2152, 51746], "temperature": 0.0, "avg_logprob": -0.16900988800884925, "compression_ratio": 1.6719745222929936, "no_speech_prob": 0.003428287338465452}, {"id": 880, "seek": 205258, "start": 2052.58, "end": 2055.38, "text": " before I leave you to ask me questions", "tokens": [50364, 949, 286, 1856, 291, 281, 1029, 385, 1651, 50504], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 881, "seek": 205258, "start": 2055.38, "end": 2057.94, "text": " and for me to go grab my orb as well", "tokens": [50504, 293, 337, 385, 281, 352, 4444, 452, 14715, 382, 731, 50632], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 882, "seek": 205258, "start": 2057.94, "end": 2059.58, "text": " is technical bottlenecks.", "tokens": [50632, 307, 6191, 44641, 2761, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 883, "seek": 205258, "start": 2059.58, "end": 2060.98, "text": " Zero-knowledge machine learning, right?", "tokens": [50714, 17182, 12, 15869, 3042, 3479, 2539, 11, 558, 30, 50784], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 884, "seek": 205258, "start": 2060.98, "end": 2062.18, "text": " We've seen some use cases,", "tokens": [50784, 492, 600, 1612, 512, 764, 3331, 11, 50844], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 885, "seek": 205258, "start": 2062.18, "end": 2064.22, "text": " we've seen what people are working on,", "tokens": [50844, 321, 600, 1612, 437, 561, 366, 1364, 322, 11, 50946], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 886, "seek": 205258, "start": 2064.22, "end": 2065.58, "text": " what people are doing, what it is,", "tokens": [50946, 437, 561, 366, 884, 11, 437, 309, 307, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 887, "seek": 205258, "start": 2065.58, "end": 2067.38, "text": " some of the things that we are doing,", "tokens": [51014, 512, 295, 264, 721, 300, 321, 366, 884, 11, 51104], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 888, "seek": 205258, "start": 2067.38, "end": 2068.86, "text": " but where do we go now, right?", "tokens": [51104, 457, 689, 360, 321, 352, 586, 11, 558, 30, 51178], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 889, "seek": 205258, "start": 2068.86, "end": 2070.86, "text": " If you're someone who is interested in this topic,", "tokens": [51178, 759, 291, 434, 1580, 567, 307, 3102, 294, 341, 4829, 11, 51278], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 890, "seek": 205258, "start": 2070.86, "end": 2072.66, "text": " where could you contribute if you want to,", "tokens": [51278, 689, 727, 291, 10586, 498, 291, 528, 281, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 891, "seek": 205258, "start": 2072.66, "end": 2074.02, "text": " if you end up learning more?", "tokens": [51368, 498, 291, 917, 493, 2539, 544, 30, 51436], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 892, "seek": 205258, "start": 2074.02, "end": 2075.66, "text": " How do you contribute to these?", "tokens": [51436, 1012, 360, 291, 10586, 281, 613, 30, 51518], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 893, "seek": 205258, "start": 2076.54, "end": 2078.38, "text": " Or what are the problems that are hard", "tokens": [51562, 1610, 437, 366, 264, 2740, 300, 366, 1152, 51654], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 894, "seek": 205258, "start": 2078.38, "end": 2080.94, "text": " and that would help us improve in the front?", "tokens": [51654, 293, 300, 576, 854, 505, 3470, 294, 264, 1868, 30, 51782], "temperature": 0.0, "avg_logprob": -0.11795228682224106, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0027148055378347635}, {"id": 895, "seek": 208094, "start": 2080.94, "end": 2084.06, "text": " So one is better cryptography, right?", "tokens": [50364, 407, 472, 307, 1101, 9844, 5820, 11, 558, 30, 50520], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 896, "seek": 208094, "start": 2084.06, "end": 2085.98, "text": " Because it's better ZK.", "tokens": [50520, 1436, 309, 311, 1101, 1176, 42, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 897, "seek": 208094, "start": 2085.98, "end": 2088.62, "text": " As I mentioned, the worst in ZK and the worst of ML", "tokens": [50616, 1018, 286, 2835, 11, 264, 5855, 294, 1176, 42, 293, 264, 5855, 295, 21601, 50748], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 898, "seek": 208094, "start": 2088.62, "end": 2089.98, "text": " create a joint bottleneck.", "tokens": [50748, 1884, 257, 7225, 44641, 547, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 899, "seek": 208094, "start": 2089.98, "end": 2092.42, "text": " So if you improve ZK or if you improve ML,", "tokens": [50816, 407, 498, 291, 3470, 1176, 42, 420, 498, 291, 3470, 21601, 11, 50938], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 900, "seek": 208094, "start": 2092.42, "end": 2093.9, "text": " you improve the KML.", "tokens": [50938, 291, 3470, 264, 591, 12683, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 901, "seek": 208094, "start": 2093.9, "end": 2095.98, "text": " But there is also an intersection", "tokens": [51012, 583, 456, 307, 611, 364, 15236, 51116], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 902, "seek": 208094, "start": 2095.98, "end": 2098.38, "text": " where if you just focus on the ZK parts", "tokens": [51116, 689, 498, 291, 445, 1879, 322, 264, 1176, 42, 3166, 51236], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 903, "seek": 208094, "start": 2098.38, "end": 2099.42, "text": " that would make ML better", "tokens": [51236, 300, 576, 652, 21601, 1101, 51288], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 904, "seek": 208094, "start": 2099.42, "end": 2101.7000000000003, "text": " and on the ML parts that would make ZK better", "tokens": [51288, 293, 322, 264, 21601, 3166, 300, 576, 652, 1176, 42, 1101, 51402], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 905, "seek": 208094, "start": 2101.7000000000003, "end": 2105.1, "text": " or simpler, that's the most focused effort", "tokens": [51402, 420, 18587, 11, 300, 311, 264, 881, 5178, 4630, 51572], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 906, "seek": 208094, "start": 2105.1, "end": 2108.26, "text": " that you can make to essentially improve everything.", "tokens": [51572, 300, 291, 393, 652, 281, 4476, 3470, 1203, 13, 51730], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 907, "seek": 208094, "start": 2108.26, "end": 2109.86, "text": " The one is better cryptography.", "tokens": [51730, 440, 472, 307, 1101, 9844, 5820, 13, 51810], "temperature": 0.0, "avg_logprob": -0.11667130833907093, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.004679580684751272}, {"id": 908, "seek": 210986, "start": 2109.86, "end": 2111.34, "text": " So remainder, for example,", "tokens": [50364, 407, 29837, 11, 337, 1365, 11, 50438], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 909, "seek": 210986, "start": 2111.34, "end": 2112.7000000000003, "text": " the thing that I mentioned here,", "tokens": [50438, 264, 551, 300, 286, 2835, 510, 11, 50506], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 910, "seek": 210986, "start": 2112.7000000000003, "end": 2114.2200000000003, "text": " remainder is a proving library", "tokens": [50506, 29837, 307, 257, 27221, 6405, 50582], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 911, "seek": 210986, "start": 2114.2200000000003, "end": 2117.6200000000003, "text": " that is built by modulus labs or modulus,", "tokens": [50582, 300, 307, 3094, 538, 42287, 20339, 420, 42287, 11, 50752], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 912, "seek": 210986, "start": 2117.6200000000003, "end": 2120.7400000000002, "text": " which essentially uses a type of cryptography", "tokens": [50752, 597, 4476, 4960, 257, 2010, 295, 9844, 5820, 50908], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 913, "seek": 210986, "start": 2120.7400000000002, "end": 2122.98, "text": " which better models the structured nature", "tokens": [50908, 597, 1101, 5245, 264, 18519, 3687, 51020], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 914, "seek": 210986, "start": 2122.98, "end": 2124.3, "text": " of machine learning computing.", "tokens": [51020, 295, 3479, 2539, 15866, 13, 51086], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 915, "seek": 210986, "start": 2124.3, "end": 2125.6600000000003, "text": " Where like machine learning usually have", "tokens": [51086, 2305, 411, 3479, 2539, 2673, 362, 51154], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 916, "seek": 210986, "start": 2125.6600000000003, "end": 2127.34, "text": " matrix multiplication.", "tokens": [51154, 8141, 27290, 13, 51238], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 917, "seek": 210986, "start": 2127.34, "end": 2129.2200000000003, "text": " They have some non-linearities.", "tokens": [51238, 814, 362, 512, 2107, 12, 28263, 1088, 13, 51332], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 918, "seek": 210986, "start": 2129.2200000000003, "end": 2132.06, "text": " So like functions that are non-linear, in this case,", "tokens": [51332, 407, 411, 6828, 300, 366, 2107, 12, 28263, 11, 294, 341, 1389, 11, 51474], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 919, "seek": 210986, "start": 2132.06, "end": 2133.6200000000003, "text": " there's like activation functions", "tokens": [51474, 456, 311, 411, 24433, 6828, 51552], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 920, "seek": 210986, "start": 2133.6200000000003, "end": 2135.06, "text": " or like a good example of this.", "tokens": [51552, 420, 411, 257, 665, 1365, 295, 341, 13, 51624], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 921, "seek": 210986, "start": 2135.06, "end": 2137.3, "text": " So there's things like ReLU,", "tokens": [51624, 407, 456, 311, 721, 411, 1300, 43, 52, 11, 51736], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 922, "seek": 210986, "start": 2137.3, "end": 2138.86, "text": " which is one of the most popular ones,", "tokens": [51736, 597, 307, 472, 295, 264, 881, 3743, 2306, 11, 51814], "temperature": 0.0, "avg_logprob": -0.19198967913071885, "compression_ratio": 1.8101694915254238, "no_speech_prob": 0.0005614635883830488}, {"id": 923, "seek": 213886, "start": 2139.42, "end": 2141.82, "text": " defined linear unit, something like that.", "tokens": [50392, 7642, 8213, 4985, 11, 746, 411, 300, 13, 50512], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 924, "seek": 213886, "start": 2141.82, "end": 2143.7400000000002, "text": " Like tanh, there's a bunch of activation functions,", "tokens": [50512, 1743, 7603, 71, 11, 456, 311, 257, 3840, 295, 24433, 6828, 11, 50608], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 925, "seek": 213886, "start": 2143.7400000000002, "end": 2144.98, "text": " non-linear function.", "tokens": [50608, 2107, 12, 28263, 2445, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 926, "seek": 213886, "start": 2144.98, "end": 2147.98, "text": " So they built a cryptographic system", "tokens": [50670, 407, 436, 3094, 257, 9844, 12295, 1185, 50820], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 927, "seek": 213886, "start": 2147.98, "end": 2152.06, "text": " which is able to represent the structured computation", "tokens": [50820, 597, 307, 1075, 281, 2906, 264, 18519, 24903, 51024], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 928, "seek": 213886, "start": 2152.06, "end": 2154.42, "text": " in a much more efficient way.", "tokens": [51024, 294, 257, 709, 544, 7148, 636, 13, 51142], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 929, "seek": 213886, "start": 2154.42, "end": 2158.2200000000003, "text": " So when it comes to proving these structured computations,", "tokens": [51142, 407, 562, 309, 1487, 281, 27221, 613, 18519, 2807, 763, 11, 51332], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 930, "seek": 213886, "start": 2158.2200000000003, "end": 2160.42, "text": " it takes a lot less computational power to do so", "tokens": [51332, 309, 2516, 257, 688, 1570, 28270, 1347, 281, 360, 370, 51442], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 931, "seek": 213886, "start": 2160.42, "end": 2163.26, "text": " because the representation is much more succinct", "tokens": [51442, 570, 264, 10290, 307, 709, 544, 21578, 5460, 51584], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 932, "seek": 213886, "start": 2163.26, "end": 2164.6200000000003, "text": " and much more efficient.", "tokens": [51584, 293, 709, 544, 7148, 13, 51652], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 933, "seek": 213886, "start": 2164.6200000000003, "end": 2166.3, "text": " And so it makes it a lot faster", "tokens": [51652, 400, 370, 309, 1669, 309, 257, 688, 4663, 51736], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 934, "seek": 213886, "start": 2166.3, "end": 2167.6600000000003, "text": " and a lot more performant", "tokens": [51736, 293, 257, 688, 544, 2042, 394, 51804], "temperature": 0.0, "avg_logprob": -0.14807824063892208, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0016739858547225595}, {"id": 935, "seek": 216766, "start": 2167.66, "end": 2169.7, "text": " and less computationally intensive.", "tokens": [50364, 293, 1570, 24903, 379, 18957, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 936, "seek": 216766, "start": 2169.7, "end": 2172.2599999999998, "text": " So this would potentially make it feasible", "tokens": [50466, 407, 341, 576, 7263, 652, 309, 26648, 50594], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 937, "seek": 216766, "start": 2172.2599999999998, "end": 2174.62, "text": " to run a DK machine learning prover", "tokens": [50594, 281, 1190, 257, 31934, 3479, 2539, 447, 331, 50712], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 938, "seek": 216766, "start": 2174.62, "end": 2177.3799999999997, "text": " on a personal phone, for example.", "tokens": [50712, 322, 257, 2973, 2593, 11, 337, 1365, 13, 50850], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 939, "seek": 216766, "start": 2177.3799999999997, "end": 2178.8599999999997, "text": " Another one is better hardware.", "tokens": [50850, 3996, 472, 307, 1101, 8837, 13, 50924], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 940, "seek": 216766, "start": 2178.8599999999997, "end": 2181.42, "text": " So hardware and specialized hardware", "tokens": [50924, 407, 8837, 293, 19813, 8837, 51052], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 941, "seek": 216766, "start": 2181.42, "end": 2184.2599999999998, "text": " is one of the things that modern science", "tokens": [51052, 307, 472, 295, 264, 721, 300, 4363, 3497, 51194], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 942, "seek": 216766, "start": 2184.2599999999998, "end": 2185.8599999999997, "text": " has benefited from most.", "tokens": [51194, 575, 33605, 490, 881, 13, 51274], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 943, "seek": 216766, "start": 2185.8599999999997, "end": 2188.1, "text": " We've seen the transistor consistently shrinking", "tokens": [51274, 492, 600, 1612, 264, 34750, 14961, 41684, 51386], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 944, "seek": 216766, "start": 2188.1, "end": 2188.94, "text": " and shrinking.", "tokens": [51386, 293, 41684, 13, 51428], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 945, "seek": 216766, "start": 2188.94, "end": 2190.62, "text": " We fit more transistors on a chip,", "tokens": [51428, 492, 3318, 544, 1145, 46976, 322, 257, 11409, 11, 51512], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 946, "seek": 216766, "start": 2190.62, "end": 2192.58, "text": " almost like 2X every 18 months, right?", "tokens": [51512, 1920, 411, 568, 55, 633, 2443, 2493, 11, 558, 30, 51610], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 947, "seek": 216766, "start": 2192.58, "end": 2195.18, "text": " There's Moore's Law, which goes exponentially.", "tokens": [51610, 821, 311, 21644, 311, 7744, 11, 597, 1709, 37330, 13, 51740], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 948, "seek": 216766, "start": 2195.18, "end": 2197.2999999999997, "text": " And now we're at like the two nanometer scale", "tokens": [51740, 400, 586, 321, 434, 412, 411, 264, 732, 14067, 13606, 4373, 51846], "temperature": 0.0, "avg_logprob": -0.14875254236665883, "compression_ratio": 1.6421725239616614, "no_speech_prob": 0.000273703335551545}, {"id": 949, "seek": 219730, "start": 2197.3, "end": 2199.6600000000003, "text": " where we have transistors that are two nanometers wide", "tokens": [50364, 689, 321, 362, 1145, 46976, 300, 366, 732, 14067, 34675, 4874, 50482], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 950, "seek": 219730, "start": 2199.6600000000003, "end": 2202.46, "text": " and we're able to pack trillions of them on modern GPUs.", "tokens": [50482, 293, 321, 434, 1075, 281, 2844, 504, 46279, 295, 552, 322, 4363, 18407, 82, 13, 50622], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 951, "seek": 219730, "start": 2202.46, "end": 2204.86, "text": " And for example, in the context of machine learning,", "tokens": [50622, 400, 337, 1365, 11, 294, 264, 4319, 295, 3479, 2539, 11, 50742], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 952, "seek": 219730, "start": 2204.86, "end": 2206.78, "text": " machine learning was really terrible", "tokens": [50742, 3479, 2539, 390, 534, 6237, 50838], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 953, "seek": 219730, "start": 2206.78, "end": 2209.5800000000004, "text": " on traditional computers like CPUs back in the day,", "tokens": [50838, 322, 5164, 10807, 411, 13199, 82, 646, 294, 264, 786, 11, 50978], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 954, "seek": 219730, "start": 2209.5800000000004, "end": 2211.5800000000004, "text": " like in the 60s and 70s and 90s.", "tokens": [50978, 411, 294, 264, 4060, 82, 293, 5285, 82, 293, 4289, 82, 13, 51078], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 955, "seek": 219730, "start": 2211.5800000000004, "end": 2213.82, "text": " So no one actually did machine learning back then.", "tokens": [51078, 407, 572, 472, 767, 630, 3479, 2539, 646, 550, 13, 51190], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 956, "seek": 219730, "start": 2213.82, "end": 2217.1800000000003, "text": " But when these people were playing video game for some reason,", "tokens": [51190, 583, 562, 613, 561, 645, 2433, 960, 1216, 337, 512, 1778, 11, 51358], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 957, "seek": 219730, "start": 2217.1800000000003, "end": 2218.94, "text": " people started building chips", "tokens": [51358, 561, 1409, 2390, 11583, 51446], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 958, "seek": 219730, "start": 2218.94, "end": 2222.82, "text": " that represent graphical interfaces a lot better.", "tokens": [51446, 300, 2906, 35942, 28416, 257, 688, 1101, 13, 51640], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 959, "seek": 219730, "start": 2222.82, "end": 2226.78, "text": " And it happens that there's an overlap of the mathematics", "tokens": [51640, 400, 309, 2314, 300, 456, 311, 364, 19959, 295, 264, 18666, 51838], "temperature": 0.0, "avg_logprob": -0.09987150021453402, "compression_ratio": 1.713375796178344, "no_speech_prob": 0.0006361374398693442}, {"id": 960, "seek": 222678, "start": 2226.78, "end": 2230.3, "text": " that are used to represent graphics and graphics card", "tokens": [50364, 300, 366, 1143, 281, 2906, 11837, 293, 11837, 2920, 50540], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 961, "seek": 222678, "start": 2230.3, "end": 2232.5400000000004, "text": " and the machine learning, right?", "tokens": [50540, 293, 264, 3479, 2539, 11, 558, 30, 50652], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 962, "seek": 222678, "start": 2232.5400000000004, "end": 2234.6600000000003, "text": " Machine learning is the matrix multiplication", "tokens": [50652, 22155, 2539, 307, 264, 8141, 27290, 50758], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 963, "seek": 222678, "start": 2234.6600000000003, "end": 2237.5, "text": " and the way that you represent pictures is matrices, right?", "tokens": [50758, 293, 264, 636, 300, 291, 2906, 5242, 307, 32284, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 964, "seek": 222678, "start": 2237.5, "end": 2240.5800000000004, "text": " It's just zeros and one that represent the RGB values", "tokens": [50900, 467, 311, 445, 35193, 293, 472, 300, 2906, 264, 31231, 4190, 51054], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 965, "seek": 222678, "start": 2240.5800000000004, "end": 2242.6600000000003, "text": " of every single pixel on the screen", "tokens": [51054, 295, 633, 2167, 19261, 322, 264, 2568, 51158], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 966, "seek": 222678, "start": 2242.6600000000003, "end": 2244.2200000000003, "text": " and transformations between them.", "tokens": [51158, 293, 34852, 1296, 552, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 967, "seek": 222678, "start": 2244.2200000000003, "end": 2245.6600000000003, "text": " And so you have to do these operations", "tokens": [51236, 400, 370, 291, 362, 281, 360, 613, 7705, 51308], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 968, "seek": 222678, "start": 2245.6600000000003, "end": 2246.86, "text": " between pixels really fast.", "tokens": [51308, 1296, 18668, 534, 2370, 13, 51368], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 969, "seek": 222678, "start": 2246.86, "end": 2248.3, "text": " And it just happened that it's the same thing", "tokens": [51368, 400, 309, 445, 2011, 300, 309, 311, 264, 912, 551, 51440], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 970, "seek": 222678, "start": 2248.3, "end": 2250.82, "text": " as doing machine learning like neural network", "tokens": [51440, 382, 884, 3479, 2539, 411, 18161, 3209, 51566], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 971, "seek": 222678, "start": 2250.82, "end": 2253.9, "text": " fast multiplication across multiple connected layers.", "tokens": [51566, 2370, 27290, 2108, 3866, 4582, 7914, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 972, "seek": 222678, "start": 2253.9, "end": 2255.42, "text": " It's like very similar structure.", "tokens": [51720, 467, 311, 411, 588, 2531, 3877, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1366171725960665, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.0014550016494467854}, {"id": 973, "seek": 225542, "start": 2255.42, "end": 2258.98, "text": " And so people start using GPUs to speed up machine learning", "tokens": [50364, 400, 370, 561, 722, 1228, 18407, 82, 281, 3073, 493, 3479, 2539, 50542], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 974, "seek": 225542, "start": 2258.98, "end": 2261.78, "text": " and machine learning became feasible all of a sudden in 2012", "tokens": [50542, 293, 3479, 2539, 3062, 26648, 439, 295, 257, 3990, 294, 9125, 50682], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 975, "seek": 225542, "start": 2261.78, "end": 2263.1800000000003, "text": " with convolutional neural networks", "tokens": [50682, 365, 45216, 304, 18161, 9590, 50752], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 976, "seek": 225542, "start": 2263.1800000000003, "end": 2266.26, "text": " and all these new like booms that we've been writing", "tokens": [50752, 293, 439, 613, 777, 411, 748, 4785, 300, 321, 600, 668, 3579, 50906], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 977, "seek": 225542, "start": 2266.26, "end": 2267.9, "text": " until now with modern LLMs.", "tokens": [50906, 1826, 586, 365, 4363, 441, 43, 26386, 13, 50988], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 978, "seek": 225542, "start": 2267.9, "end": 2270.38, "text": " Like LLMs and all these new generative AI models", "tokens": [50988, 1743, 441, 43, 26386, 293, 439, 613, 777, 1337, 1166, 7318, 5245, 51112], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 979, "seek": 225542, "start": 2270.38, "end": 2272.58, "text": " are only possible because of this specialized hardware", "tokens": [51112, 366, 787, 1944, 570, 295, 341, 19813, 8837, 51222], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 980, "seek": 225542, "start": 2272.58, "end": 2276.54, "text": " that come from NVIDIA, DCMC, AMD, ASML,", "tokens": [51222, 300, 808, 490, 426, 3958, 6914, 11, 9114, 39261, 11, 34808, 11, 7469, 12683, 11, 51420], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 981, "seek": 225542, "start": 2276.54, "end": 2278.78, "text": " like all these like transistor manufacturers,", "tokens": [51420, 411, 439, 613, 411, 34750, 18455, 11, 51532], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 982, "seek": 225542, "start": 2278.78, "end": 2281.86, "text": " graphic car manufacturers, specialized hardware manufacturers.", "tokens": [51532, 14089, 1032, 18455, 11, 19813, 8837, 18455, 13, 51686], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 983, "seek": 225542, "start": 2281.86, "end": 2282.98, "text": " These ones are for machine learning,", "tokens": [51686, 1981, 2306, 366, 337, 3479, 2539, 11, 51742], "temperature": 0.0, "avg_logprob": -0.12882241537404615, "compression_ratio": 1.7651006711409396, "no_speech_prob": 0.0008558596600778401}, {"id": 984, "seek": 228298, "start": 2282.98, "end": 2284.94, "text": " GPUs and tensor processing unit,", "tokens": [50364, 18407, 82, 293, 40863, 9007, 4985, 11, 50462], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 985, "seek": 228298, "start": 2284.94, "end": 2287.54, "text": " GPUs, cryptography on the other hand,", "tokens": [50462, 18407, 82, 11, 9844, 5820, 322, 264, 661, 1011, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 986, "seek": 228298, "start": 2287.54, "end": 2289.34, "text": " they work with a different type of math.", "tokens": [50592, 436, 589, 365, 257, 819, 2010, 295, 5221, 13, 50682], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 987, "seek": 228298, "start": 2289.34, "end": 2291.78, "text": " Instead of working with floating points or arithmetic,", "tokens": [50682, 7156, 295, 1364, 365, 12607, 2793, 420, 42973, 11, 50804], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 988, "seek": 228298, "start": 2291.78, "end": 2295.34, "text": " they work with finite fields and sixth point arithmetic.", "tokens": [50804, 436, 589, 365, 19362, 7909, 293, 15102, 935, 42973, 13, 50982], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 989, "seek": 228298, "start": 2295.34, "end": 2297.82, "text": " And so you need to design fundamentally different hardware.", "tokens": [50982, 400, 370, 291, 643, 281, 1715, 17879, 819, 8837, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 990, "seek": 228298, "start": 2297.82, "end": 2299.38, "text": " And so we need to build better hardware", "tokens": [51106, 400, 370, 321, 643, 281, 1322, 1101, 8837, 51184], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 991, "seek": 228298, "start": 2299.38, "end": 2302.34, "text": " to improve the computational capabilities", "tokens": [51184, 281, 3470, 264, 28270, 10862, 51332], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 992, "seek": 228298, "start": 2302.34, "end": 2304.1, "text": " of zero knowledge machine learning", "tokens": [51332, 295, 4018, 3601, 3479, 2539, 51420], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 993, "seek": 228298, "start": 2304.1, "end": 2306.7, "text": " or just zero knowledge cryptography in this then.", "tokens": [51420, 420, 445, 4018, 3601, 9844, 5820, 294, 341, 550, 13, 51550], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 994, "seek": 228298, "start": 2306.7, "end": 2308.3, "text": " So there's lots of things to be done here.", "tokens": [51550, 407, 456, 311, 3195, 295, 721, 281, 312, 1096, 510, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 995, "seek": 228298, "start": 2308.3, "end": 2310.58, "text": " So I'm, for example, I'm also an investor in Fabric,", "tokens": [51630, 407, 286, 478, 11, 337, 1365, 11, 286, 478, 611, 364, 18479, 294, 17440, 1341, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 996, "seek": 228298, "start": 2310.58, "end": 2311.42, "text": " I'm sorry for that,", "tokens": [51744, 286, 478, 2597, 337, 300, 11, 51786], "temperature": 0.0, "avg_logprob": -0.1644888851377699, "compression_ratio": 1.7523219814241486, "no_speech_prob": 0.007694190368056297}, {"id": 997, "seek": 231142, "start": 2311.42, "end": 2314.7000000000003, "text": " but Fabric is one of the ZK hardware company.", "tokens": [50364, 457, 17440, 1341, 307, 472, 295, 264, 1176, 42, 8837, 2237, 13, 50528], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 998, "seek": 231142, "start": 2314.7000000000003, "end": 2317.02, "text": " In GoNyama, not size thick, it's size thick,", "tokens": [50528, 682, 1037, 45, 88, 2404, 11, 406, 2744, 5060, 11, 309, 311, 2744, 5060, 11, 50644], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 999, "seek": 231142, "start": 2317.02, "end": 2319.46, "text": " sorry about that, some misspelling without the T.", "tokens": [50644, 2597, 466, 300, 11, 512, 1713, 494, 2669, 1553, 264, 314, 13, 50766], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1000, "seek": 231142, "start": 2319.46, "end": 2320.38, "text": " And irreducible,", "tokens": [50766, 400, 16014, 769, 32128, 11, 50812], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1001, "seek": 231142, "start": 2320.38, "end": 2323.26, "text": " there's some of the biggest ZK hardware companies.", "tokens": [50812, 456, 311, 512, 295, 264, 3880, 1176, 42, 8837, 3431, 13, 50956], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1002, "seek": 231142, "start": 2323.26, "end": 2325.7400000000002, "text": " And yeah, so these are trying to essentially model", "tokens": [50956, 400, 1338, 11, 370, 613, 366, 1382, 281, 4476, 2316, 51080], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1003, "seek": 231142, "start": 2325.7400000000002, "end": 2328.14, "text": " the software in hardware so that it's faster", "tokens": [51080, 264, 4722, 294, 8837, 370, 300, 309, 311, 4663, 51200], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1004, "seek": 231142, "start": 2328.14, "end": 2329.7400000000002, "text": " and there's less overhead.", "tokens": [51200, 293, 456, 311, 1570, 19922, 13, 51280], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1005, "seek": 231142, "start": 2329.7400000000002, "end": 2331.1, "text": " Another one is better tooling.", "tokens": [51280, 3996, 472, 307, 1101, 46593, 13, 51348], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1006, "seek": 231142, "start": 2331.1, "end": 2332.54, "text": " So I mentioned Ezekiel and Giza.", "tokens": [51348, 407, 286, 2835, 462, 19878, 1187, 293, 460, 13427, 13, 51420], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1007, "seek": 231142, "start": 2332.54, "end": 2334.1800000000003, "text": " They're building tooling that makes it easier", "tokens": [51420, 814, 434, 2390, 46593, 300, 1669, 309, 3571, 51502], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1008, "seek": 231142, "start": 2334.1800000000003, "end": 2336.82, "text": " for developers to use ZK.", "tokens": [51502, 337, 8849, 281, 764, 1176, 42, 13, 51634], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1009, "seek": 231142, "start": 2336.82, "end": 2338.26, "text": " And if I'm a machine learning engineer,", "tokens": [51634, 400, 498, 286, 478, 257, 3479, 2539, 11403, 11, 51706], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1010, "seek": 231142, "start": 2338.26, "end": 2340.94, "text": " there's no way in hell I'm gonna spend six years", "tokens": [51706, 456, 311, 572, 636, 294, 4921, 286, 478, 799, 3496, 2309, 924, 51840], "temperature": 0.0, "avg_logprob": -0.18227760880081742, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.0013039277400821447}, {"id": 1011, "seek": 234094, "start": 2340.94, "end": 2343.86, "text": " learning cryptography and learning the state of the art", "tokens": [50364, 2539, 9844, 5820, 293, 2539, 264, 1785, 295, 264, 1523, 50510], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1012, "seek": 234094, "start": 2343.86, "end": 2345.02, "text": " and trying to contribute there", "tokens": [50510, 293, 1382, 281, 10586, 456, 50568], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1013, "seek": 234094, "start": 2345.02, "end": 2346.98, "text": " so that I can prove my machine learning model.", "tokens": [50568, 370, 300, 286, 393, 7081, 452, 3479, 2539, 2316, 13, 50666], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1014, "seek": 234094, "start": 2346.98, "end": 2348.1, "text": " As a machine learning engineer,", "tokens": [50666, 1018, 257, 3479, 2539, 11403, 11, 50722], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1015, "seek": 234094, "start": 2348.1, "end": 2351.34, "text": " I just care about something that ZK can bring to me.", "tokens": [50722, 286, 445, 1127, 466, 746, 300, 1176, 42, 393, 1565, 281, 385, 13, 50884], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1016, "seek": 234094, "start": 2351.34, "end": 2352.98, "text": " And vice versa, if I'm a ZK guy,", "tokens": [50884, 400, 11964, 25650, 11, 498, 286, 478, 257, 1176, 42, 2146, 11, 50966], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1017, "seek": 234094, "start": 2352.98, "end": 2355.2200000000003, "text": " I just care about something that ML can bring to me", "tokens": [50966, 286, 445, 1127, 466, 746, 300, 21601, 393, 1565, 281, 385, 51078], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1018, "seek": 234094, "start": 2355.2200000000003, "end": 2358.44, "text": " to get better or like somehow make it on chain.", "tokens": [51078, 281, 483, 1101, 420, 411, 6063, 652, 309, 322, 5021, 13, 51239], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1019, "seek": 234094, "start": 2358.44, "end": 2361.18, "text": " So whenever we've brought down the cost of barriers,", "tokens": [51239, 407, 5699, 321, 600, 3038, 760, 264, 2063, 295, 13565, 11, 51376], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1020, "seek": 234094, "start": 2361.18, "end": 2364.14, "text": " like barriers that the cost barriers", "tokens": [51376, 411, 13565, 300, 264, 2063, 13565, 51524], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1021, "seek": 234094, "start": 2364.14, "end": 2366.38, "text": " that prevent us from doing something,", "tokens": [51524, 300, 4871, 505, 490, 884, 746, 11, 51636], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1022, "seek": 234094, "start": 2366.38, "end": 2367.66, "text": " people start experimenting, right?", "tokens": [51636, 561, 722, 29070, 11, 558, 30, 51700], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1023, "seek": 234094, "start": 2367.66, "end": 2370.5, "text": " Like same thing happened with the web.", "tokens": [51700, 1743, 912, 551, 2011, 365, 264, 3670, 13, 51842], "temperature": 0.0, "avg_logprob": -0.13783452245924208, "compression_ratio": 1.8745762711864407, "no_speech_prob": 0.0018383483402431011}, {"id": 1024, "seek": 237050, "start": 2370.5, "end": 2372.26, "text": " Like anyone can build a website nowadays", "tokens": [50364, 1743, 2878, 393, 1322, 257, 3144, 13434, 50452], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1025, "seek": 237050, "start": 2372.26, "end": 2373.18, "text": " and you can build a business,", "tokens": [50452, 293, 291, 393, 1322, 257, 1606, 11, 50498], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1026, "seek": 237050, "start": 2373.18, "end": 2374.26, "text": " you can just be Shopify.", "tokens": [50498, 291, 393, 445, 312, 43991, 13, 50552], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1027, "seek": 237050, "start": 2374.26, "end": 2375.42, "text": " And if I'm a business guy,", "tokens": [50552, 400, 498, 286, 478, 257, 1606, 2146, 11, 50610], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1028, "seek": 237050, "start": 2375.42, "end": 2376.78, "text": " I don't need to know web development.", "tokens": [50610, 286, 500, 380, 643, 281, 458, 3670, 3250, 13, 50678], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1029, "seek": 237050, "start": 2376.78, "end": 2378.06, "text": " Shopify and I have my store", "tokens": [50678, 43991, 293, 286, 362, 452, 3531, 50742], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1030, "seek": 237050, "start": 2378.06, "end": 2380.02, "text": " and I can process millions of dollars of payment.", "tokens": [50742, 293, 286, 393, 1399, 6803, 295, 3808, 295, 10224, 13, 50840], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1031, "seek": 237050, "start": 2380.02, "end": 2382.86, "text": " I can have a truly user and everything.", "tokens": [50840, 286, 393, 362, 257, 4908, 4195, 293, 1203, 13, 50982], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1032, "seek": 237050, "start": 2382.86, "end": 2384.9, "text": " And otherwise I would have to learn web development,", "tokens": [50982, 400, 5911, 286, 576, 362, 281, 1466, 3670, 3250, 11, 51084], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1033, "seek": 237050, "start": 2384.9, "end": 2385.98, "text": " servers, everything.", "tokens": [51084, 15909, 11, 1203, 13, 51138], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1034, "seek": 237050, "start": 2385.98, "end": 2386.94, "text": " I don't have to care about that.", "tokens": [51138, 286, 500, 380, 362, 281, 1127, 466, 300, 13, 51186], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1035, "seek": 237050, "start": 2386.94, "end": 2387.78, "text": " I just do my business", "tokens": [51186, 286, 445, 360, 452, 1606, 51228], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1036, "seek": 237050, "start": 2387.78, "end": 2390.34, "text": " and I use web technology without having to know how it works.", "tokens": [51228, 293, 286, 764, 3670, 2899, 1553, 1419, 281, 458, 577, 309, 1985, 13, 51356], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1037, "seek": 237050, "start": 2390.34, "end": 2392.98, "text": " So the same thing applies to KML of course.", "tokens": [51356, 407, 264, 912, 551, 13165, 281, 591, 12683, 295, 1164, 13, 51488], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1038, "seek": 237050, "start": 2392.98, "end": 2395.02, "text": " More robust than secure implementation.", "tokens": [51488, 5048, 13956, 813, 7144, 11420, 13, 51590], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1039, "seek": 237050, "start": 2395.02, "end": 2397.06, "text": " That one is a bit like self-explanatory", "tokens": [51590, 663, 472, 307, 257, 857, 411, 2698, 12, 3121, 16554, 4745, 51692], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1040, "seek": 237050, "start": 2397.06, "end": 2398.98, "text": " but essentially like the more security", "tokens": [51692, 457, 4476, 411, 264, 544, 3825, 51788], "temperature": 0.0, "avg_logprob": -0.15820110740000112, "compression_ratio": 1.8057142857142856, "no_speech_prob": 0.0007096431218087673}, {"id": 1041, "seek": 239898, "start": 2398.98, "end": 2402.38, "text": " the less prevent like if we can prevent hacks and exploit", "tokens": [50364, 264, 1570, 4871, 411, 498, 321, 393, 4871, 33617, 293, 25924, 50534], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1042, "seek": 239898, "start": 2402.38, "end": 2403.98, "text": " then if it's more robust,", "tokens": [50534, 550, 498, 309, 311, 544, 13956, 11, 50614], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1043, "seek": 239898, "start": 2403.98, "end": 2406.46, "text": " it can sustain more users, et cetera.", "tokens": [50614, 309, 393, 6769, 544, 5022, 11, 1030, 11458, 13, 50738], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1044, "seek": 239898, "start": 2406.46, "end": 2408.34, "text": " And the other one is like what I mentioned before", "tokens": [50738, 400, 264, 661, 472, 307, 411, 437, 286, 2835, 949, 50832], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1045, "seek": 239898, "start": 2408.34, "end": 2409.42, "text": " pretty much at the same point,", "tokens": [50832, 1238, 709, 412, 264, 912, 935, 11, 50886], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1046, "seek": 239898, "start": 2409.42, "end": 2411.62, "text": " like better tooling and easier interfaces", "tokens": [50886, 411, 1101, 46593, 293, 3571, 28416, 50996], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1047, "seek": 239898, "start": 2411.62, "end": 2412.66, "text": " is pretty much the same thing", "tokens": [50996, 307, 1238, 709, 264, 912, 551, 51048], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1048, "seek": 239898, "start": 2412.66, "end": 2414.26, "text": " because the easier it is to use,", "tokens": [51048, 570, 264, 3571, 309, 307, 281, 764, 11, 51128], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1049, "seek": 239898, "start": 2414.26, "end": 2416.46, "text": " the more experimentation there for the more products,", "tokens": [51128, 264, 544, 37142, 456, 337, 264, 544, 3383, 11, 51238], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1050, "seek": 239898, "start": 2416.46, "end": 2417.66, "text": " the more product market fits,", "tokens": [51238, 264, 544, 1674, 2142, 9001, 11, 51298], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1051, "seek": 239898, "start": 2417.66, "end": 2418.94, "text": " the more businesses can build", "tokens": [51298, 264, 544, 6011, 393, 1322, 51362], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1052, "seek": 239898, "start": 2418.94, "end": 2421.22, "text": " and the more technology can accelerate", "tokens": [51362, 293, 264, 544, 2899, 393, 21341, 51476], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1053, "seek": 239898, "start": 2421.22, "end": 2424.18, "text": " towards the direction of growth.", "tokens": [51476, 3030, 264, 3513, 295, 4599, 13, 51624], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1054, "seek": 239898, "start": 2424.18, "end": 2427.02, "text": " So yeah, that's everything about my presentation", "tokens": [51624, 407, 1338, 11, 300, 311, 1203, 466, 452, 5860, 51766], "temperature": 0.0, "avg_logprob": -0.16353255377875434, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0036494198720902205}, {"id": 1055, "seek": 242702, "start": 2427.02, "end": 2431.54, "text": " and I would love to answer any of your questions.", "tokens": [50364, 293, 286, 576, 959, 281, 1867, 604, 295, 428, 1651, 13, 50590], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1056, "seek": 242702, "start": 2431.54, "end": 2433.06, "text": " I don't know how long we have.", "tokens": [50590, 286, 500, 380, 458, 577, 938, 321, 362, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1057, "seek": 242702, "start": 2433.06, "end": 2435.2599999999998, "text": " I think it's 14 minutes for FAQ.", "tokens": [50666, 286, 519, 309, 311, 3499, 2077, 337, 19894, 48, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1058, "seek": 242702, "start": 2435.2599999999998, "end": 2438.2599999999998, "text": " I can also go run, get the orb if you guys wanna see it.", "tokens": [50776, 286, 393, 611, 352, 1190, 11, 483, 264, 14715, 498, 291, 1074, 1948, 536, 309, 13, 50926], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1059, "seek": 242702, "start": 2438.2599999999998, "end": 2440.54, "text": " And thank you for having me.", "tokens": [50926, 400, 1309, 291, 337, 1419, 385, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1060, "seek": 242702, "start": 2440.54, "end": 2441.38, "text": " This was fantastic.", "tokens": [51040, 639, 390, 5456, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1061, "seek": 242702, "start": 2441.38, "end": 2442.38, "text": " Thank you so much.", "tokens": [51082, 1044, 291, 370, 709, 13, 51132], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1062, "seek": 242702, "start": 2442.38, "end": 2443.22, "text": " What a world.", "tokens": [51132, 708, 257, 1002, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1063, "seek": 242702, "start": 2443.22, "end": 2445.1, "text": " And we have a few questions already here", "tokens": [51174, 400, 321, 362, 257, 1326, 1651, 1217, 510, 51268], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1064, "seek": 242702, "start": 2445.1, "end": 2446.2599999999998, "text": " from people in the chat", "tokens": [51268, 490, 561, 294, 264, 5081, 51326], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1065, "seek": 242702, "start": 2446.2599999999998, "end": 2448.5, "text": " and then maybe after a few we give you some time to breathe", "tokens": [51326, 293, 550, 1310, 934, 257, 1326, 321, 976, 291, 512, 565, 281, 10192, 51438], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1066, "seek": 242702, "start": 2448.5, "end": 2450.62, "text": " and get the orb, that would be great.", "tokens": [51438, 293, 483, 264, 14715, 11, 300, 576, 312, 869, 13, 51544], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1067, "seek": 242702, "start": 2450.62, "end": 2452.1, "text": " Okay, so first one, Shadi,", "tokens": [51544, 1033, 11, 370, 700, 472, 11, 1160, 5688, 11, 51618], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1068, "seek": 242702, "start": 2452.1, "end": 2454.42, "text": " if you wanna unmute your first.", "tokens": [51618, 498, 291, 1948, 41445, 428, 700, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1069, "seek": 242702, "start": 2454.42, "end": 2456.3, "text": " Hi, yeah, thanks for the great presentation.", "tokens": [51734, 2421, 11, 1338, 11, 3231, 337, 264, 869, 5860, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2195573091506958, "compression_ratio": 1.6741935483870967, "no_speech_prob": 0.000495406799018383}, {"id": 1070, "seek": 245630, "start": 2456.38, "end": 2457.5800000000004, "text": " Very informative.", "tokens": [50368, 4372, 27759, 13, 50428], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1071, "seek": 245630, "start": 2457.5800000000004, "end": 2460.7400000000002, "text": " I had a question about the personally identifying information", "tokens": [50428, 286, 632, 257, 1168, 466, 264, 5665, 16696, 1589, 50586], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1072, "seek": 245630, "start": 2460.7400000000002, "end": 2464.42, "text": " from the hash from the iris biometrics.", "tokens": [50586, 490, 264, 22019, 490, 264, 3418, 271, 3228, 649, 10716, 13, 50770], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1073, "seek": 245630, "start": 2464.42, "end": 2467.82, "text": " Isn't a hash or iris still uniquely identifying", "tokens": [50770, 6998, 380, 257, 22019, 420, 3418, 271, 920, 31474, 16696, 50940], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1074, "seek": 245630, "start": 2467.82, "end": 2471.5, "text": " if you know the hashing function to produce that digest?", "tokens": [50940, 498, 291, 458, 264, 575, 571, 2445, 281, 5258, 300, 13884, 30, 51124], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1075, "seek": 245630, "start": 2471.5, "end": 2474.7000000000003, "text": " Or did you mean that make the function is kept secret", "tokens": [51124, 1610, 630, 291, 914, 300, 652, 264, 2445, 307, 4305, 4054, 51284], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1076, "seek": 245630, "start": 2474.7000000000003, "end": 2478.86, "text": " and nobody can easily take like a photograph of someone", "tokens": [51284, 293, 5079, 393, 3612, 747, 411, 257, 8348, 295, 1580, 51492], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1077, "seek": 245630, "start": 2478.86, "end": 2482.1000000000004, "text": " and then produce the same hash and that look on chain,", "tokens": [51492, 293, 550, 5258, 264, 912, 22019, 293, 300, 574, 322, 5021, 11, 51654], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1078, "seek": 245630, "start": 2482.1000000000004, "end": 2483.46, "text": " for example, I don't think you posted on chain", "tokens": [51654, 337, 1365, 11, 286, 500, 380, 519, 291, 9437, 322, 5021, 51722], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1079, "seek": 245630, "start": 2483.46, "end": 2486.2200000000003, "text": " but look on chain, for example, to try to match that.", "tokens": [51722, 457, 574, 322, 5021, 11, 337, 1365, 11, 281, 853, 281, 2995, 300, 13, 51860], "temperature": 0.0, "avg_logprob": -0.20653400113505702, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.027547746896743774}, {"id": 1080, "seek": 248630, "start": 2486.94, "end": 2490.38, "text": " Yeah, there's one unfortunate naming collision here.", "tokens": [50396, 865, 11, 456, 311, 472, 17843, 25290, 24644, 510, 13, 50568], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1081, "seek": 248630, "start": 2490.38, "end": 2491.7400000000002, "text": " So in biometric literature,", "tokens": [50568, 407, 294, 3228, 29470, 10394, 11, 50636], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1082, "seek": 248630, "start": 2491.7400000000002, "end": 2494.78, "text": " people use a hash in a non-urgorous way.", "tokens": [50636, 561, 764, 257, 22019, 294, 257, 2107, 12, 5476, 16099, 636, 13, 50788], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1083, "seek": 248630, "start": 2494.78, "end": 2496.1800000000003, "text": " And so what we mean here,", "tokens": [50788, 400, 370, 437, 321, 914, 510, 11, 50858], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1084, "seek": 248630, "start": 2496.1800000000003, "end": 2497.26, "text": " or what we used to mean,", "tokens": [50858, 420, 437, 321, 1143, 281, 914, 11, 50912], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1085, "seek": 248630, "start": 2497.26, "end": 2499.9, "text": " we've changed the way that we explain these things.", "tokens": [50912, 321, 600, 3105, 264, 636, 300, 321, 2903, 613, 721, 13, 51044], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1086, "seek": 248630, "start": 2499.9, "end": 2501.7000000000003, "text": " We no longer use the terminology of hash", "tokens": [51044, 492, 572, 2854, 764, 264, 27575, 295, 22019, 51134], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1087, "seek": 248630, "start": 2501.7000000000003, "end": 2505.5, "text": " because we work in the intersection of AI and cryptography", "tokens": [51134, 570, 321, 589, 294, 264, 15236, 295, 7318, 293, 9844, 5820, 51324], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1088, "seek": 248630, "start": 2505.5, "end": 2507.9, "text": " and if you use a term that means a different thing in both,", "tokens": [51324, 293, 498, 291, 764, 257, 1433, 300, 1355, 257, 819, 551, 294, 1293, 11, 51444], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1089, "seek": 248630, "start": 2507.9, "end": 2511.42, "text": " it's like ambiguous and it can cause problems", "tokens": [51444, 309, 311, 411, 39465, 293, 309, 393, 3082, 2740, 51620], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1090, "seek": 248630, "start": 2511.42, "end": 2513.1000000000004, "text": " like this one right now.", "tokens": [51620, 411, 341, 472, 558, 586, 13, 51704], "temperature": 0.0, "avg_logprob": -0.12533515930175781, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0006070370436646044}, {"id": 1091, "seek": 251310, "start": 2513.1, "end": 2516.7799999999997, "text": " Actually, the way the biometrics pipeline works", "tokens": [50364, 5135, 11, 264, 636, 264, 3228, 649, 10716, 15517, 1985, 50548], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1092, "seek": 251310, "start": 2516.7799999999997, "end": 2520.02, "text": " is that there's this essentially convolution-like algorithm.", "tokens": [50548, 307, 300, 456, 311, 341, 4476, 45216, 12, 4092, 9284, 13, 50710], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1093, "seek": 251310, "start": 2520.02, "end": 2523.02, "text": " It's called the GABER wavelet or GABER filter,", "tokens": [50710, 467, 311, 1219, 264, 22841, 11074, 22144, 302, 420, 22841, 11074, 6608, 11, 50860], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1094, "seek": 251310, "start": 2523.02, "end": 2524.66, "text": " which essentially applies convolutions", "tokens": [50860, 597, 4476, 13165, 3754, 15892, 50942], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1095, "seek": 251310, "start": 2524.66, "end": 2526.66, "text": " into original biometrics many times over", "tokens": [50942, 666, 3380, 3228, 649, 10716, 867, 1413, 670, 51042], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1096, "seek": 251310, "start": 2526.66, "end": 2530.5, "text": " and it's able to compute like a randomness representation.", "tokens": [51042, 293, 309, 311, 1075, 281, 14722, 411, 257, 4974, 1287, 10290, 13, 51234], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1097, "seek": 251310, "start": 2530.5, "end": 2534.46, "text": " And this one essentially compresses the image so much,", "tokens": [51234, 400, 341, 472, 4476, 14778, 279, 264, 3256, 370, 709, 11, 51432], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1098, "seek": 251310, "start": 2534.46, "end": 2536.2999999999997, "text": " like after performing all these operations,", "tokens": [51432, 411, 934, 10205, 439, 613, 7705, 11, 51524], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1099, "seek": 251310, "start": 2536.2999999999997, "end": 2540.8199999999997, "text": " you end up with a pretty much a small representation", "tokens": [51524, 291, 917, 493, 365, 257, 1238, 709, 257, 1359, 10290, 51750], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1100, "seek": 251310, "start": 2540.8199999999997, "end": 2542.66, "text": " of a few bits, like I think it's 200,", "tokens": [51750, 295, 257, 1326, 9239, 11, 411, 286, 519, 309, 311, 2331, 11, 51842], "temperature": 0.0, "avg_logprob": -0.22964753395269724, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.03731418401002884}, {"id": 1101, "seek": 254266, "start": 2542.66, "end": 2544.8599999999997, "text": " something that, so the vector in the end,", "tokens": [50364, 746, 300, 11, 370, 264, 8062, 294, 264, 917, 11, 50474], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1102, "seek": 254266, "start": 2544.8599999999997, "end": 2546.66, "text": " like the embedding in the end is like a few bit.", "tokens": [50474, 411, 264, 12240, 3584, 294, 264, 917, 307, 411, 257, 1326, 857, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1103, "seek": 254266, "start": 2546.66, "end": 2549.98, "text": " And this one is not able to be reconstructed", "tokens": [50564, 400, 341, 472, 307, 406, 1075, 281, 312, 31499, 292, 50730], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1104, "seek": 254266, "start": 2550.98, "end": 2553.66, "text": " to its original, at least like a lossy function, right?", "tokens": [50780, 281, 1080, 3380, 11, 412, 1935, 411, 257, 4470, 88, 2445, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1105, "seek": 254266, "start": 2553.66, "end": 2556.14, "text": " If I go from a compressed representation", "tokens": [50914, 759, 286, 352, 490, 257, 30353, 10290, 51038], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1106, "seek": 254266, "start": 2556.14, "end": 2558.58, "text": " to a fully, try to expand it back,", "tokens": [51038, 281, 257, 4498, 11, 853, 281, 5268, 309, 646, 11, 51160], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1107, "seek": 254266, "start": 2558.58, "end": 2559.8999999999996, "text": " I lose information in the process", "tokens": [51160, 286, 3624, 1589, 294, 264, 1399, 51226], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1108, "seek": 254266, "start": 2559.8999999999996, "end": 2561.94, "text": " of converting it to this compressed representation.", "tokens": [51226, 295, 29942, 309, 281, 341, 30353, 10290, 13, 51328], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1109, "seek": 254266, "start": 2561.94, "end": 2563.98, "text": " Therefore, I'm not able to reconstruct the same one.", "tokens": [51328, 7504, 11, 286, 478, 406, 1075, 281, 31499, 264, 912, 472, 13, 51430], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1110, "seek": 254266, "start": 2563.98, "end": 2567.06, "text": " And the good part of this is that I'm able to,", "tokens": [51430, 400, 264, 665, 644, 295, 341, 307, 300, 286, 478, 1075, 281, 11, 51584], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1111, "seek": 254266, "start": 2567.06, "end": 2568.8599999999997, "text": " I'm able to reconstruct something similar,", "tokens": [51584, 286, 478, 1075, 281, 31499, 746, 2531, 11, 51674], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1112, "seek": 254266, "start": 2568.8599999999997, "end": 2571.2999999999997, "text": " but it's not personalized identifiable,", "tokens": [51674, 457, 309, 311, 406, 28415, 2473, 30876, 11, 51796], "temperature": 0.0, "avg_logprob": -0.14505719151990168, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.0008425981504842639}, {"id": 1113, "seek": 257130, "start": 2571.3, "end": 2574.34, "text": " at least not considered so in modern literature, right?", "tokens": [50364, 412, 1935, 406, 4888, 370, 294, 4363, 10394, 11, 558, 30, 50516], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1114, "seek": 257130, "start": 2574.34, "end": 2576.86, "text": " This may change and this is why we've been working", "tokens": [50516, 639, 815, 1319, 293, 341, 307, 983, 321, 600, 668, 1364, 50642], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1115, "seek": 257130, "start": 2576.86, "end": 2578.5800000000004, "text": " on a lot of other things within world economy,", "tokens": [50642, 322, 257, 688, 295, 661, 721, 1951, 1002, 5010, 11, 50728], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1116, "seek": 257130, "start": 2578.5800000000004, "end": 2581.38, "text": " like more of the party computation solutions and whatnot.", "tokens": [50728, 411, 544, 295, 264, 3595, 24903, 6547, 293, 25882, 13, 50868], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1117, "seek": 257130, "start": 2581.38, "end": 2583.1800000000003, "text": " We're gonna be publishing a lot about this", "tokens": [50868, 492, 434, 799, 312, 17832, 257, 688, 466, 341, 50958], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1118, "seek": 257130, "start": 2583.1800000000003, "end": 2585.1800000000003, "text": " in the coming month, but if you're interested", "tokens": [50958, 294, 264, 1348, 1618, 11, 457, 498, 291, 434, 3102, 51058], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1119, "seek": 257130, "start": 2585.1800000000003, "end": 2588.0600000000004, "text": " in like follow this, the biometrics pipeline works", "tokens": [51058, 294, 411, 1524, 341, 11, 264, 3228, 649, 10716, 15517, 1985, 51202], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1120, "seek": 257130, "start": 2588.0600000000004, "end": 2590.1400000000003, "text": " and have the definition of it and how it works", "tokens": [51202, 293, 362, 264, 7123, 295, 309, 293, 577, 309, 1985, 51306], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1121, "seek": 257130, "start": 2590.1400000000003, "end": 2591.5, "text": " and what is actually going on,", "tokens": [51306, 293, 437, 307, 767, 516, 322, 11, 51374], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1122, "seek": 257130, "start": 2591.5, "end": 2593.7400000000002, "text": " I recommend going to the link I just said in the chat,", "tokens": [51374, 286, 2748, 516, 281, 264, 2113, 286, 445, 848, 294, 264, 5081, 11, 51486], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1123, "seek": 257130, "start": 2593.7400000000002, "end": 2595.3, "text": " my paper that work in the org.", "tokens": [51486, 452, 3035, 300, 589, 294, 264, 14045, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1124, "seek": 257130, "start": 2595.3, "end": 2597.26, "text": " Also, one of my teammates is in the,", "tokens": [51564, 2743, 11, 472, 295, 452, 20461, 307, 294, 264, 11, 51662], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1125, "seek": 257130, "start": 2597.26, "end": 2599.46, "text": " actually one of the former teammates,", "tokens": [51662, 767, 472, 295, 264, 5819, 20461, 11, 51772], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1126, "seek": 257130, "start": 2599.46, "end": 2600.82, "text": " he's at Tool for Humanity,", "tokens": [51772, 415, 311, 412, 15934, 337, 10294, 507, 11, 51840], "temperature": 0.0, "avg_logprob": -0.23370229547674007, "compression_ratio": 1.7884057971014493, "no_speech_prob": 0.0028446593787521124}, {"id": 1127, "seek": 260082, "start": 2600.82, "end": 2602.2200000000003, "text": " which is the labs entity.", "tokens": [50364, 597, 307, 264, 20339, 13977, 13, 50434], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1128, "seek": 260082, "start": 2602.2200000000003, "end": 2604.6200000000003, "text": " I'm at the foundation of different legal entities,", "tokens": [50434, 286, 478, 412, 264, 7030, 295, 819, 5089, 16667, 11, 50554], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1129, "seek": 260082, "start": 2604.6200000000003, "end": 2606.7400000000002, "text": " but they're both contributing to the world team project.", "tokens": [50554, 457, 436, 434, 1293, 19270, 281, 264, 1002, 1469, 1716, 13, 50660], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1130, "seek": 260082, "start": 2606.7400000000002, "end": 2608.38, "text": " His name is Daniel Gershiewicz.", "tokens": [50660, 2812, 1315, 307, 8033, 460, 433, 71, 1093, 17946, 13, 50742], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1131, "seek": 260082, "start": 2608.38, "end": 2609.6600000000003, "text": " He is in the cause law.", "tokens": [50742, 634, 307, 294, 264, 3082, 2101, 13, 50806], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1132, "seek": 260082, "start": 2609.6600000000003, "end": 2611.7000000000003, "text": " So he's also able to explain a bit more.", "tokens": [50806, 407, 415, 311, 611, 1075, 281, 2903, 257, 857, 544, 13, 50908], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1133, "seek": 260082, "start": 2611.7000000000003, "end": 2612.86, "text": " He's on the org software team.", "tokens": [50908, 634, 311, 322, 264, 14045, 4722, 1469, 13, 50966], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1134, "seek": 260082, "start": 2612.86, "end": 2616.1400000000003, "text": " So he works a lot more with the biometrics pipeline than I do.", "tokens": [50966, 407, 415, 1985, 257, 688, 544, 365, 264, 3228, 649, 10716, 15517, 813, 286, 360, 13, 51130], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1135, "seek": 260082, "start": 2616.1400000000003, "end": 2617.7400000000002, "text": " I'm more still in the cryptography", "tokens": [51130, 286, 478, 544, 920, 294, 264, 9844, 5820, 51210], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1136, "seek": 260082, "start": 2617.7400000000002, "end": 2618.98, "text": " protocol side of thing.", "tokens": [51210, 10336, 1252, 295, 551, 13, 51272], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1137, "seek": 260082, "start": 2618.98, "end": 2620.46, "text": " But yeah, within the white paper,", "tokens": [51272, 583, 1338, 11, 1951, 264, 2418, 3035, 11, 51346], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1138, "seek": 260082, "start": 2620.46, "end": 2621.9, "text": " white paper.worldcoin.org,", "tokens": [51346, 2418, 3035, 13, 13217, 8562, 13, 4646, 11, 51418], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1139, "seek": 260082, "start": 2621.9, "end": 2623.3, "text": " you have a biometric action", "tokens": [51418, 291, 362, 257, 3228, 29470, 3069, 51488], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1140, "seek": 260082, "start": 2623.3, "end": 2624.38, "text": " and you have the third definition", "tokens": [51488, 293, 291, 362, 264, 2636, 7123, 51542], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1141, "seek": 260082, "start": 2624.38, "end": 2625.6600000000003, "text": " of what it is that we're doing", "tokens": [51542, 295, 437, 309, 307, 300, 321, 434, 884, 51606], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1142, "seek": 260082, "start": 2625.6600000000003, "end": 2627.26, "text": " and how we preserve privacy.", "tokens": [51606, 293, 577, 321, 15665, 11427, 13, 51686], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1143, "seek": 260082, "start": 2627.26, "end": 2629.9, "text": " So to answer your question in a specific way,", "tokens": [51686, 407, 281, 1867, 428, 1168, 294, 257, 2685, 636, 11, 51818], "temperature": 0.0, "avg_logprob": -0.23455755445692275, "compression_ratio": 1.7337110481586402, "no_speech_prob": 0.00026945758145302534}, {"id": 1144, "seek": 262990, "start": 2629.9, "end": 2631.62, "text": " it's not a hash, it's not a cryptographic hash.", "tokens": [50364, 309, 311, 406, 257, 22019, 11, 309, 311, 406, 257, 9844, 12295, 22019, 13, 50450], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1145, "seek": 262990, "start": 2631.62, "end": 2633.06, "text": " There's no digest, there's no plaintext.", "tokens": [50450, 821, 311, 572, 13884, 11, 456, 311, 572, 11121, 25111, 13, 50522], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1146, "seek": 262990, "start": 2633.06, "end": 2635.6600000000003, "text": " It's essentially a convolutional like operation", "tokens": [50522, 467, 311, 4476, 257, 45216, 304, 411, 6916, 50652], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1147, "seek": 262990, "start": 2635.6600000000003, "end": 2636.7000000000003, "text": " which happens many times", "tokens": [50652, 597, 2314, 867, 1413, 50704], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1148, "seek": 262990, "start": 2636.7000000000003, "end": 2638.3, "text": " and it leaves the input unrecognizable", "tokens": [50704, 293, 309, 5510, 264, 4846, 517, 13867, 2912, 22395, 50784], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1149, "seek": 262990, "start": 2638.3, "end": 2639.38, "text": " and you compress certain information", "tokens": [50784, 293, 291, 14778, 1629, 1589, 50838], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1150, "seek": 262990, "start": 2639.38, "end": 2641.1, "text": " of the randomness that you get.", "tokens": [50838, 295, 264, 4974, 1287, 300, 291, 483, 13, 50924], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1151, "seek": 262990, "start": 2641.1, "end": 2643.34, "text": " You cannot use that to reconstruct the original thing", "tokens": [50924, 509, 2644, 764, 300, 281, 31499, 264, 3380, 551, 51036], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1152, "seek": 262990, "start": 2643.34, "end": 2644.38, "text": " that you put into this function", "tokens": [51036, 300, 291, 829, 666, 341, 2445, 51088], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1153, "seek": 262990, "start": 2644.38, "end": 2645.98, "text": " because it's a very lofty function.", "tokens": [51088, 570, 309, 311, 257, 588, 34419, 88, 2445, 13, 51168], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1154, "seek": 262990, "start": 2645.98, "end": 2648.78, "text": " And this is good enough to prevent", "tokens": [51168, 400, 341, 307, 665, 1547, 281, 4871, 51308], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1155, "seek": 262990, "start": 2648.78, "end": 2652.06, "text": " like getting the raw biometric out again.", "tokens": [51308, 411, 1242, 264, 8936, 3228, 29470, 484, 797, 13, 51472], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1156, "seek": 262990, "start": 2653.1800000000003, "end": 2654.02, "text": " Got it.", "tokens": [51528, 5803, 309, 13, 51570], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1157, "seek": 262990, "start": 2654.02, "end": 2656.42, "text": " So the idea is even if I had access", "tokens": [51570, 407, 264, 1558, 307, 754, 498, 286, 632, 2105, 51690], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1158, "seek": 262990, "start": 2656.42, "end": 2659.06, "text": " to the kernels that you used to train,", "tokens": [51690, 281, 264, 23434, 1625, 300, 291, 1143, 281, 3847, 11, 51822], "temperature": 0.0, "avg_logprob": -0.18344899096520118, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0014323085779324174}, {"id": 1159, "seek": 265906, "start": 2659.22, "end": 2664.22, "text": " then I wouldn't be able to deconvolute the output.", "tokens": [50372, 550, 286, 2759, 380, 312, 1075, 281, 979, 266, 9646, 1169, 264, 5598, 13, 50622], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1160, "seek": 265906, "start": 2665.74, "end": 2666.58, "text": " I see.", "tokens": [50698, 286, 536, 13, 50740], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1161, "seek": 265906, "start": 2666.58, "end": 2670.5, "text": " Ideally to try and break our own assumptions", "tokens": [50740, 40817, 281, 853, 293, 1821, 527, 1065, 17695, 50936], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1162, "seek": 265906, "start": 2670.5, "end": 2671.7, "text": " and try to reverse engineer", "tokens": [50936, 293, 853, 281, 9943, 11403, 50996], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1163, "seek": 265906, "start": 2671.7, "end": 2673.22, "text": " and actually get the original image.", "tokens": [50996, 293, 767, 483, 264, 3380, 3256, 13, 51072], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1164, "seek": 265906, "start": 2673.22, "end": 2675.42, "text": " And now we've gone ahead a step further", "tokens": [51072, 400, 586, 321, 600, 2780, 2286, 257, 1823, 3052, 51182], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1165, "seek": 265906, "start": 2675.42, "end": 2677.1, "text": " because if it was possible,", "tokens": [51182, 570, 498, 309, 390, 1944, 11, 51266], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1166, "seek": 265906, "start": 2677.1, "end": 2678.1, "text": " we've gone a step further", "tokens": [51266, 321, 600, 2780, 257, 1823, 3052, 51316], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1167, "seek": 265906, "start": 2678.1, "end": 2679.94, "text": " and we're now storing everything in ciphertext", "tokens": [51316, 293, 321, 434, 586, 26085, 1203, 294, 269, 21240, 25111, 51408], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1168, "seek": 265906, "start": 2679.94, "end": 2681.2999999999997, "text": " and the uniqueness check,", "tokens": [51408, 293, 264, 48294, 1520, 11, 51476], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1169, "seek": 265906, "start": 2681.2999999999997, "end": 2684.06, "text": " it's happening on ciphertext with multi-party computation.", "tokens": [51476, 309, 311, 2737, 322, 269, 21240, 25111, 365, 4825, 12, 23409, 24903, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1170, "seek": 265906, "start": 2684.06, "end": 2686.86, "text": " So yeah, that's like cool, cool new research stuff.", "tokens": [51614, 407, 1338, 11, 300, 311, 411, 1627, 11, 1627, 777, 2132, 1507, 13, 51754], "temperature": 0.0, "avg_logprob": -0.22263973999023437, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.0010814827401190996}, {"id": 1171, "seek": 268686, "start": 2687.82, "end": 2688.6600000000003, "text": " Love it.", "tokens": [50412, 5956, 309, 13, 50454], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1172, "seek": 268686, "start": 2688.6600000000003, "end": 2690.1, "text": " Dan also shared, I think the white paper", "tokens": [50454, 3394, 611, 5507, 11, 286, 519, 264, 2418, 3035, 50526], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1173, "seek": 268686, "start": 2690.1, "end": 2691.7000000000003, "text": " that you referenced directly here in the chat", "tokens": [50526, 300, 291, 32734, 3838, 510, 294, 264, 5081, 50606], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1174, "seek": 268686, "start": 2691.7000000000003, "end": 2693.34, "text": " already a little further up.", "tokens": [50606, 1217, 257, 707, 3052, 493, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1175, "seek": 268686, "start": 2693.34, "end": 2694.42, "text": " Thanks for that, Dan.", "tokens": [50688, 2561, 337, 300, 11, 3394, 13, 50742], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1176, "seek": 268686, "start": 2694.42, "end": 2697.38, "text": " Next one up we have Richard and then we have Micah.", "tokens": [50742, 3087, 472, 493, 321, 362, 9809, 293, 550, 321, 362, 5818, 545, 13, 50890], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1177, "seek": 268686, "start": 2698.54, "end": 2700.6200000000003, "text": " Yeah, I think the previous discussion", "tokens": [50948, 865, 11, 286, 519, 264, 3894, 5017, 51052], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1178, "seek": 268686, "start": 2700.6200000000003, "end": 2701.78, "text": " answered my question there.", "tokens": [51052, 10103, 452, 1168, 456, 13, 51110], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1179, "seek": 268686, "start": 2701.78, "end": 2703.02, "text": " Thank you.", "tokens": [51110, 1044, 291, 13, 51172], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1180, "seek": 268686, "start": 2703.02, "end": 2704.06, "text": " Awesome.", "tokens": [51172, 10391, 13, 51224], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1181, "seek": 268686, "start": 2704.06, "end": 2704.9, "text": " Wonderful.", "tokens": [51224, 22768, 13, 51266], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1182, "seek": 268686, "start": 2704.9, "end": 2706.34, "text": " Micah, you go.", "tokens": [51266, 5818, 545, 11, 291, 352, 13, 51338], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1183, "seek": 268686, "start": 2706.34, "end": 2707.54, "text": " Micah, we can't hear you.", "tokens": [51338, 5818, 545, 11, 321, 393, 380, 1568, 291, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1184, "seek": 268686, "start": 2707.54, "end": 2709.7400000000002, "text": " Feel free if you can't unmute to put your chat,", "tokens": [51398, 14113, 1737, 498, 291, 393, 380, 41445, 281, 829, 428, 5081, 11, 51508], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1185, "seek": 268686, "start": 2709.7400000000002, "end": 2711.7000000000003, "text": " your question in the chat.", "tokens": [51508, 428, 1168, 294, 264, 5081, 13, 51606], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1186, "seek": 268686, "start": 2711.7000000000003, "end": 2713.42, "text": " Okay, he's going to rejoin.", "tokens": [51606, 1033, 11, 415, 311, 516, 281, 22087, 259, 13, 51692], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1187, "seek": 268686, "start": 2713.42, "end": 2715.86, "text": " This could be a great opportunity for you to get the orb.", "tokens": [51692, 639, 727, 312, 257, 869, 2650, 337, 291, 281, 483, 264, 14715, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14745900554041708, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0017002788372337818}, {"id": 1188, "seek": 271586, "start": 2715.86, "end": 2719.78, "text": " I also have a few questions, but God, you go.", "tokens": [50364, 286, 611, 362, 257, 1326, 1651, 11, 457, 1265, 11, 291, 352, 13, 50560], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1189, "seek": 271586, "start": 2720.6200000000003, "end": 2721.46, "text": " Can you hear me?", "tokens": [50602, 1664, 291, 1568, 385, 30, 50644], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1190, "seek": 271586, "start": 2722.58, "end": 2724.02, "text": " Yes, Dan, we can hear you.", "tokens": [50700, 1079, 11, 3394, 11, 321, 393, 1568, 291, 13, 50772], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1191, "seek": 271586, "start": 2725.3, "end": 2727.1800000000003, "text": " I want to go back, there was this question about", "tokens": [50836, 286, 528, 281, 352, 646, 11, 456, 390, 341, 1168, 466, 50930], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1192, "seek": 271586, "start": 2727.1800000000003, "end": 2729.7400000000002, "text": " and things being personally identifiable", "tokens": [50930, 293, 721, 885, 5665, 2473, 30876, 51058], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1193, "seek": 271586, "start": 2729.7400000000002, "end": 2731.82, "text": " or uniquely identifying information.", "tokens": [51058, 420, 31474, 16696, 1589, 13, 51162], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1194, "seek": 271586, "start": 2731.82, "end": 2734.54, "text": " And then the question turned into ashes", "tokens": [51162, 400, 550, 264, 1168, 3574, 666, 32942, 51298], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1195, "seek": 271586, "start": 2734.54, "end": 2737.42, "text": " versus wavelet encodings.", "tokens": [51298, 5717, 22144, 302, 2058, 378, 1109, 13, 51442], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1196, "seek": 271586, "start": 2738.5, "end": 2739.34, "text": " I don't know.", "tokens": [51496, 286, 500, 380, 458, 13, 51538], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1197, "seek": 271586, "start": 2739.34, "end": 2741.82, "text": " I think the question actually got lots of discussions.", "tokens": [51538, 286, 519, 264, 1168, 767, 658, 3195, 295, 11088, 13, 51662], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1198, "seek": 271586, "start": 2741.82, "end": 2744.7400000000002, "text": " And the interesting thing is that even if you've had", "tokens": [51662, 400, 264, 1880, 551, 307, 300, 754, 498, 291, 600, 632, 51808], "temperature": 0.0, "avg_logprob": -0.27913756919118154, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0004801393370144069}, {"id": 1199, "seek": 274474, "start": 2744.74, "end": 2746.22, "text": " either a hash or an encoding,", "tokens": [50364, 2139, 257, 22019, 420, 364, 43430, 11, 50438], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1200, "seek": 274474, "start": 2746.22, "end": 2749.58, "text": " and you somehow broke this and could reverse that image,", "tokens": [50438, 293, 291, 6063, 6902, 341, 293, 727, 9943, 300, 3256, 11, 50606], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1201, "seek": 274474, "start": 2750.58, "end": 2752.9399999999996, "text": " actually the privacy comes from", "tokens": [50656, 767, 264, 11427, 1487, 490, 50774], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1202, "seek": 274474, "start": 2752.9399999999996, "end": 2754.3399999999997, "text": " what was briefly mentioned in the talk,", "tokens": [50774, 437, 390, 10515, 2835, 294, 264, 751, 11, 50844], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1203, "seek": 274474, "start": 2754.3399999999997, "end": 2758.8999999999996, "text": " which is that when you prove your ownership of such,", "tokens": [50844, 597, 307, 300, 562, 291, 7081, 428, 15279, 295, 1270, 11, 51072], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1204, "seek": 274474, "start": 2758.8999999999996, "end": 2761.14, "text": " you're proving ownership of a key", "tokens": [51072, 291, 434, 27221, 15279, 295, 257, 2141, 51184], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1205, "seek": 274474, "start": 2761.14, "end": 2765.7799999999997, "text": " that was linked to this biometric encoding.", "tokens": [51184, 300, 390, 9408, 281, 341, 3228, 29470, 43430, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1206, "seek": 274474, "start": 2765.7799999999997, "end": 2767.2599999999998, "text": " So when you prove ownership of that key,", "tokens": [51416, 407, 562, 291, 7081, 15279, 295, 300, 2141, 11, 51490], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1207, "seek": 274474, "start": 2767.2599999999998, "end": 2769.5, "text": " you're not pointing to which encoding is yours.", "tokens": [51490, 291, 434, 406, 12166, 281, 597, 43430, 307, 6342, 13, 51602], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1208, "seek": 274474, "start": 2769.5, "end": 2770.4199999999996, "text": " So you're a member of the set", "tokens": [51602, 407, 291, 434, 257, 4006, 295, 264, 992, 51648], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1209, "seek": 274474, "start": 2770.4199999999996, "end": 2772.22, "text": " without revealing which member.", "tokens": [51648, 1553, 23983, 597, 4006, 13, 51738], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1210, "seek": 274474, "start": 2772.22, "end": 2773.4199999999996, "text": " And that means that these encodings", "tokens": [51738, 400, 300, 1355, 300, 613, 2058, 378, 1109, 51798], "temperature": 0.0, "avg_logprob": -0.15891739629930066, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.0036464608274400234}, {"id": 1211, "seek": 277342, "start": 2773.42, "end": 2775.54, "text": " are cryptographically delinked from anything else,", "tokens": [50364, 366, 9844, 3108, 984, 1103, 475, 292, 490, 1340, 1646, 11, 50470], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1212, "seek": 277342, "start": 2775.54, "end": 2777.58, "text": " your transactions, your accounts.", "tokens": [50470, 428, 16856, 11, 428, 9402, 13, 50572], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1213, "seek": 277342, "start": 2777.58, "end": 2778.54, "text": " Nothing can be linked back.", "tokens": [50572, 6693, 393, 312, 9408, 646, 13, 50620], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1214, "seek": 277342, "start": 2778.54, "end": 2783.1800000000003, "text": " So if you did reverse those codes, you wouldn't know.", "tokens": [50620, 407, 498, 291, 630, 9943, 729, 14211, 11, 291, 2759, 380, 458, 13, 50852], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1215, "seek": 277342, "start": 2783.1800000000003, "end": 2785.26, "text": " Also, one additional thing is that", "tokens": [50852, 2743, 11, 472, 4497, 551, 307, 300, 50956], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1216, "seek": 277342, "start": 2785.26, "end": 2787.5, "text": " these encodings are not public.", "tokens": [50956, 613, 2058, 378, 1109, 366, 406, 1908, 13, 51068], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1217, "seek": 277342, "start": 2787.5, "end": 2790.34, "text": " They're hidden in a database that we have.", "tokens": [51068, 814, 434, 7633, 294, 257, 8149, 300, 321, 362, 13, 51210], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1218, "seek": 277342, "start": 2790.34, "end": 2793.02, "text": " The thing that is public is the public key", "tokens": [51210, 440, 551, 300, 307, 1908, 307, 264, 1908, 2141, 51344], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1219, "seek": 277342, "start": 2793.02, "end": 2796.2200000000003, "text": " associated with the user that has undergone a unique question.", "tokens": [51344, 6615, 365, 264, 4195, 300, 575, 833, 39743, 257, 3845, 1168, 13, 51504], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1220, "seek": 277342, "start": 2796.2200000000003, "end": 2798.38, "text": " So if I have this unique coding,", "tokens": [51504, 407, 498, 286, 362, 341, 3845, 17720, 11, 51612], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1221, "seek": 277342, "start": 2798.38, "end": 2801.46, "text": " and I prove that I'm a unique within the coding set,", "tokens": [51612, 293, 286, 7081, 300, 286, 478, 257, 3845, 1951, 264, 17720, 992, 11, 51766], "temperature": 0.0, "avg_logprob": -0.20258293752595197, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.002115333918482065}, {"id": 1222, "seek": 280146, "start": 2801.5, "end": 2804.5, "text": " which is kept not on any public sphere,", "tokens": [50366, 597, 307, 4305, 406, 322, 604, 1908, 16687, 11, 50516], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1223, "seek": 280146, "start": 2804.5, "end": 2806.98, "text": " it's now kept in this multi-party computation", "tokens": [50516, 309, 311, 586, 4305, 294, 341, 4825, 12, 23409, 24903, 50640], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1224, "seek": 280146, "start": 2806.98, "end": 2808.5, "text": " encrypted environment in a database", "tokens": [50640, 36663, 2823, 294, 257, 8149, 50716], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1225, "seek": 280146, "start": 2808.5, "end": 2811.38, "text": " that is run by three different parties", "tokens": [50716, 300, 307, 1190, 538, 1045, 819, 8265, 50860], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1226, "seek": 280146, "start": 2811.38, "end": 2814.34, "text": " in an MPC setting, again, multi-party.", "tokens": [50860, 294, 364, 376, 12986, 3287, 11, 797, 11, 4825, 12, 23409, 13, 51008], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1227, "seek": 280146, "start": 2814.34, "end": 2816.66, "text": " And when the user is verified to be unique,", "tokens": [51008, 400, 562, 264, 4195, 307, 31197, 281, 312, 3845, 11, 51124], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1228, "seek": 280146, "start": 2816.66, "end": 2818.06, "text": " we take the user's public key,", "tokens": [51124, 321, 747, 264, 4195, 311, 1908, 2141, 11, 51194], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1229, "seek": 280146, "start": 2818.06, "end": 2819.66, "text": " which was generated by the World app,", "tokens": [51194, 597, 390, 10833, 538, 264, 3937, 724, 11, 51274], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1230, "seek": 280146, "start": 2819.66, "end": 2821.62, "text": " which is the way that you interface with World ID", "tokens": [51274, 597, 307, 264, 636, 300, 291, 9226, 365, 3937, 7348, 51372], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1231, "seek": 280146, "start": 2821.62, "end": 2823.98, "text": " and the wallet and a bunch of other things that we're building.", "tokens": [51372, 293, 264, 16599, 293, 257, 3840, 295, 661, 721, 300, 321, 434, 2390, 13, 51490], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1232, "seek": 280146, "start": 2823.98, "end": 2825.46, "text": " Essentially, the public key that was generated", "tokens": [51490, 23596, 11, 264, 1908, 2141, 300, 390, 10833, 51564], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1233, "seek": 280146, "start": 2825.46, "end": 2827.3, "text": " by the World app, by the user,", "tokens": [51564, 538, 264, 3937, 724, 11, 538, 264, 4195, 11, 51656], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1234, "seek": 280146, "start": 2827.3, "end": 2829.1, "text": " which is a unique person that's been just verified", "tokens": [51656, 597, 307, 257, 3845, 954, 300, 311, 668, 445, 31197, 51746], "temperature": 0.0, "avg_logprob": -0.17267089276700406, "compression_ratio": 1.8377483443708609, "no_speech_prob": 0.001622755080461502}, {"id": 1235, "seek": 282910, "start": 2829.1, "end": 2832.7799999999997, "text": " by the World, gets inserted into the set of verified users,", "tokens": [50364, 538, 264, 3937, 11, 2170, 27992, 666, 264, 992, 295, 31197, 5022, 11, 50548], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1236, "seek": 282910, "start": 2832.7799999999997, "end": 2834.86, "text": " and then I'm able to make a knowledge proof", "tokens": [50548, 293, 550, 286, 478, 1075, 281, 652, 257, 3601, 8177, 50652], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1237, "seek": 282910, "start": 2834.86, "end": 2837.8199999999997, "text": " that I own a private key to a public key", "tokens": [50652, 300, 286, 1065, 257, 4551, 2141, 281, 257, 1908, 2141, 50800], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1238, "seek": 282910, "start": 2837.8199999999997, "end": 2839.42, "text": " and the set of verified users.", "tokens": [50800, 293, 264, 992, 295, 31197, 5022, 13, 50880], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1239, "seek": 282910, "start": 2839.42, "end": 2841.22, "text": " So even then, there's one more step", "tokens": [50880, 407, 754, 550, 11, 456, 311, 472, 544, 1823, 50970], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1240, "seek": 282910, "start": 2841.22, "end": 2844.1, "text": " that removed from your biometric completely,", "tokens": [50970, 300, 7261, 490, 428, 3228, 29470, 2584, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1241, "seek": 282910, "start": 2844.1, "end": 2846.2999999999997, "text": " because a public key is just random,", "tokens": [51114, 570, 257, 1908, 2141, 307, 445, 4974, 11, 51224], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1242, "seek": 282910, "start": 2846.2999999999997, "end": 2849.42, "text": " cryptographical, gibberish that I can make proofs about,", "tokens": [51224, 9844, 48434, 11, 4553, 43189, 300, 286, 393, 652, 8177, 82, 466, 11, 51380], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1243, "seek": 282910, "start": 2849.42, "end": 2851.42, "text": " and I'm able to prove to you that I'm a unique member", "tokens": [51380, 293, 286, 478, 1075, 281, 7081, 281, 291, 300, 286, 478, 257, 3845, 4006, 51480], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1244, "seek": 282910, "start": 2851.42, "end": 2852.5, "text": " of the set because I own a private key", "tokens": [51480, 295, 264, 992, 570, 286, 1065, 257, 4551, 2141, 51534], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1245, "seek": 282910, "start": 2852.5, "end": 2855.02, "text": " to a public key in the set, but I don't know which one.", "tokens": [51534, 281, 257, 1908, 2141, 294, 264, 992, 11, 457, 286, 500, 380, 458, 597, 472, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1246, "seek": 282910, "start": 2855.02, "end": 2856.54, "text": " And there's another cool part,", "tokens": [51660, 400, 456, 311, 1071, 1627, 644, 11, 51736], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1247, "seek": 282910, "start": 2856.54, "end": 2858.66, "text": " which is the nullifier scheme that we have,", "tokens": [51736, 597, 307, 264, 18184, 9902, 12232, 300, 321, 362, 11, 51842], "temperature": 0.0, "avg_logprob": -0.1426561496875904, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.014952358789741993}, {"id": 1248, "seek": 285866, "start": 2858.66, "end": 2861.1, "text": " which allows you to represent unique actions.", "tokens": [50364, 597, 4045, 291, 281, 2906, 3845, 5909, 13, 50486], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1249, "seek": 285866, "start": 2861.1, "end": 2863.7, "text": " For example, one is like unique governance,", "tokens": [50486, 1171, 1365, 11, 472, 307, 411, 3845, 17449, 11, 50616], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1250, "seek": 285866, "start": 2863.7, "end": 2866.22, "text": " like one person, one vote, digital governance,", "tokens": [50616, 411, 472, 954, 11, 472, 4740, 11, 4562, 17449, 11, 50742], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1251, "seek": 285866, "start": 2866.22, "end": 2867.8199999999997, "text": " or voting protocol.", "tokens": [50742, 420, 10419, 10336, 13, 50822], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1252, "seek": 285866, "start": 2867.8199999999997, "end": 2868.98, "text": " Currently, there's no way to prove", "tokens": [50822, 19964, 11, 456, 311, 572, 636, 281, 7081, 50880], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1253, "seek": 285866, "start": 2868.98, "end": 2870.58, "text": " that you're not a bot online.", "tokens": [50880, 300, 291, 434, 406, 257, 10592, 2950, 13, 50960], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1254, "seek": 285866, "start": 2870.58, "end": 2872.62, "text": " So if you, for example, let's say, I don't know,", "tokens": [50960, 407, 498, 291, 11, 337, 1365, 11, 718, 311, 584, 11, 286, 500, 380, 458, 11, 51062], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1255, "seek": 285866, "start": 2872.62, "end": 2875.7799999999997, "text": " Elon Musk puts a poll on Twitter or on X", "tokens": [51062, 28498, 26019, 8137, 257, 6418, 322, 5794, 420, 322, 1783, 51220], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1256, "seek": 285866, "start": 2875.7799999999997, "end": 2878.3799999999997, "text": " that, hey, is this doc cute or no?", "tokens": [51220, 300, 11, 4177, 11, 307, 341, 3211, 4052, 420, 572, 30, 51350], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1257, "seek": 285866, "start": 2878.3799999999997, "end": 2883.1, "text": " I can create a bajillion X accounts and vote for no, right?", "tokens": [51350, 286, 393, 1884, 257, 23589, 11836, 1783, 9402, 293, 4740, 337, 572, 11, 558, 30, 51586], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1258, "seek": 285866, "start": 2883.1, "end": 2884.2999999999997, "text": " There's no way for me to prove", "tokens": [51586, 821, 311, 572, 636, 337, 385, 281, 7081, 51646], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1259, "seek": 285866, "start": 2884.2999999999997, "end": 2885.94, "text": " that this is a one person, one vote.", "tokens": [51646, 300, 341, 307, 257, 472, 954, 11, 472, 4740, 13, 51728], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1260, "seek": 285866, "start": 2885.94, "end": 2888.42, "text": " So whomever who posts a poll on whatever thing", "tokens": [51728, 407, 315, 423, 331, 567, 12300, 257, 6418, 322, 2035, 551, 51852], "temperature": 0.0, "avg_logprob": -0.14093441840929863, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.0038836367893964052}, {"id": 1261, "seek": 288842, "start": 2888.46, "end": 2890.2200000000003, "text": " doesn't matter, like the opinion doesn't matter,", "tokens": [50366, 1177, 380, 1871, 11, 411, 264, 4800, 1177, 380, 1871, 11, 50454], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1262, "seek": 288842, "start": 2890.2200000000003, "end": 2891.82, "text": " the result of the poll doesn't matter.", "tokens": [50454, 264, 1874, 295, 264, 6418, 1177, 380, 1871, 13, 50534], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1263, "seek": 288842, "start": 2891.82, "end": 2893.58, "text": " There's million bots that have incentives", "tokens": [50534, 821, 311, 2459, 35410, 300, 362, 23374, 50622], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1264, "seek": 288842, "start": 2893.58, "end": 2895.7400000000002, "text": " and both, like presidential elections,", "tokens": [50622, 293, 1293, 11, 411, 16902, 12870, 11, 50730], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1265, "seek": 288842, "start": 2895.7400000000002, "end": 2898.14, "text": " if Elon says, is this candidate a good guy?", "tokens": [50730, 498, 28498, 1619, 11, 307, 341, 11532, 257, 665, 2146, 30, 50850], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1266, "seek": 288842, "start": 2899.1, "end": 2901.1, "text": " People can vote yes, but it can be like a third,", "tokens": [50898, 3432, 393, 4740, 2086, 11, 457, 309, 393, 312, 411, 257, 2636, 11, 50998], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1267, "seek": 288842, "start": 2901.1, "end": 2904.38, "text": " like external nation state actor trying to just", "tokens": [50998, 411, 8320, 4790, 1785, 8747, 1382, 281, 445, 51162], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1268, "seek": 288842, "start": 2904.38, "end": 2906.7400000000002, "text": " civil attack, which means like attack a protocol", "tokens": [51162, 5605, 2690, 11, 597, 1355, 411, 2690, 257, 10336, 51280], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1269, "seek": 288842, "start": 2906.7400000000002, "end": 2909.1, "text": " where you need unique members in a way", "tokens": [51280, 689, 291, 643, 3845, 2679, 294, 257, 636, 51398], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1270, "seek": 288842, "start": 2909.1, "end": 2911.34, "text": " that just make the protocol broken", "tokens": [51398, 300, 445, 652, 264, 10336, 5463, 51510], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1271, "seek": 288842, "start": 2911.34, "end": 2912.94, "text": " completely beyond repair.", "tokens": [51510, 2584, 4399, 10535, 13, 51590], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1272, "seek": 288842, "start": 2912.94, "end": 2914.14, "text": " So this is where we step in,", "tokens": [51590, 407, 341, 307, 689, 321, 1823, 294, 11, 51650], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1273, "seek": 288842, "start": 2914.14, "end": 2916.78, "text": " where we create a unit of account of uniqueness", "tokens": [51650, 689, 321, 1884, 257, 4985, 295, 2696, 295, 48294, 51782], "temperature": 0.0, "avg_logprob": -0.21068906092989273, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0028894052375108004}, {"id": 1274, "seek": 291678, "start": 2916.78, "end": 2919.1400000000003, "text": " for humans in a digital environment,", "tokens": [50364, 337, 6255, 294, 257, 4562, 2823, 11, 50482], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1275, "seek": 291678, "start": 2919.1400000000003, "end": 2921.6200000000003, "text": " whether it's on chain or it's off chain doesn't matter.", "tokens": [50482, 1968, 309, 311, 322, 5021, 420, 309, 311, 766, 5021, 1177, 380, 1871, 13, 50606], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1276, "seek": 291678, "start": 2921.6200000000003, "end": 2923.3, "text": " As long as you're able to make these cryptographic", "tokens": [50606, 1018, 938, 382, 291, 434, 1075, 281, 652, 613, 9844, 12295, 50690], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1277, "seek": 291678, "start": 2923.3, "end": 2924.98, "text": " at the station that I am a unique person", "tokens": [50690, 412, 264, 5214, 300, 286, 669, 257, 3845, 954, 50774], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1278, "seek": 291678, "start": 2924.98, "end": 2926.82, "text": " and I've not done an action before,", "tokens": [50774, 293, 286, 600, 406, 1096, 364, 3069, 949, 11, 50866], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1279, "seek": 291678, "start": 2926.82, "end": 2929.26, "text": " so I'm able to prove that I'm a unique actor.", "tokens": [50866, 370, 286, 478, 1075, 281, 7081, 300, 286, 478, 257, 3845, 8747, 13, 50988], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1280, "seek": 291678, "start": 2929.26, "end": 2931.42, "text": " I'm a unique member of this protocol", "tokens": [50988, 286, 478, 257, 3845, 4006, 295, 341, 10336, 51096], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1281, "seek": 291678, "start": 2931.42, "end": 2933.02, "text": " and I only voted once.", "tokens": [51096, 293, 286, 787, 13415, 1564, 13, 51176], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1282, "seek": 291678, "start": 2933.02, "end": 2936.42, "text": " So I can say that this dog was cute only once.", "tokens": [51176, 407, 286, 393, 584, 300, 341, 3000, 390, 4052, 787, 1564, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1283, "seek": 291678, "start": 2936.42, "end": 2937.94, "text": " And if I want to weigh the outcome,", "tokens": [51346, 400, 498, 286, 528, 281, 13843, 264, 9700, 11, 51422], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1284, "seek": 291678, "start": 2937.94, "end": 2939.6600000000003, "text": " the only way to weigh the outcome is that I need", "tokens": [51422, 264, 787, 636, 281, 13843, 264, 9700, 307, 300, 286, 643, 51508], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1285, "seek": 291678, "start": 2939.6600000000003, "end": 2941.34, "text": " to convince a thousand other members", "tokens": [51508, 281, 13447, 257, 4714, 661, 2679, 51592], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1286, "seek": 291678, "start": 2941.34, "end": 2942.46, "text": " to vote for the same thing,", "tokens": [51592, 281, 4740, 337, 264, 912, 551, 11, 51648], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1287, "seek": 291678, "start": 2942.46, "end": 2944.6600000000003, "text": " but I'm not able to just create a million accounts", "tokens": [51648, 457, 286, 478, 406, 1075, 281, 445, 1884, 257, 2459, 9402, 51758], "temperature": 0.0, "avg_logprob": -0.13039019063937884, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.01883046329021454}, {"id": 1288, "seek": 294466, "start": 2944.66, "end": 2947.62, "text": " and vote for a same thing to weigh the outcome.", "tokens": [50364, 293, 4740, 337, 257, 912, 551, 281, 13843, 264, 9700, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1289, "seek": 294466, "start": 2947.62, "end": 2948.46, "text": " Yeah.", "tokens": [50512, 865, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1290, "seek": 294466, "start": 2948.46, "end": 2949.2999999999997, "text": " Awesome.", "tokens": [50554, 10391, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1291, "seek": 294466, "start": 2949.2999999999997, "end": 2950.3399999999997, "text": " Yeah, I think Bramco talked a little bit about that,", "tokens": [50596, 865, 11, 286, 519, 1603, 335, 1291, 2825, 257, 707, 857, 466, 300, 11, 50648], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1292, "seek": 294466, "start": 2950.3399999999997, "end": 2952.3399999999997, "text": " especially like with possible future applications", "tokens": [50648, 2318, 411, 365, 1944, 2027, 5821, 50748], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1293, "seek": 294466, "start": 2952.3399999999997, "end": 2954.3799999999997, "text": " also that you also listed very briefly,", "tokens": [50748, 611, 300, 291, 611, 10052, 588, 10515, 11, 50850], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1294, "seek": 294466, "start": 2954.3799999999997, "end": 2956.58, "text": " including in medicine and so forth", "tokens": [50850, 3009, 294, 7195, 293, 370, 5220, 50960], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1295, "seek": 294466, "start": 2956.58, "end": 2959.02, "text": " that I think these groups are just like really incredible for", "tokens": [50960, 300, 286, 519, 613, 3935, 366, 445, 411, 534, 4651, 337, 51082], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1296, "seek": 294466, "start": 2959.02, "end": 2961.2599999999998, "text": " because it can't for medicine, for financial risk,", "tokens": [51082, 570, 309, 393, 380, 337, 7195, 11, 337, 4669, 3148, 11, 51194], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1297, "seek": 294466, "start": 2961.2599999999998, "end": 2962.1, "text": " for insurance and stuff,", "tokens": [51194, 337, 7214, 293, 1507, 11, 51236], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1298, "seek": 294466, "start": 2962.1, "end": 2965.22, "text": " you just can't really access the data any other way.", "tokens": [51236, 291, 445, 393, 380, 534, 2105, 264, 1412, 604, 661, 636, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1299, "seek": 294466, "start": 2965.22, "end": 2968.02, "text": " Or you just can't do too much anyways with the information.", "tokens": [51392, 1610, 291, 445, 393, 380, 360, 886, 709, 13448, 365, 264, 1589, 13, 51532], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1300, "seek": 294466, "start": 2968.02, "end": 2969.14, "text": " Okay, we have another question,", "tokens": [51532, 1033, 11, 321, 362, 1071, 1168, 11, 51588], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1301, "seek": 294466, "start": 2969.14, "end": 2971.54, "text": " but you also have the orb now or?", "tokens": [51588, 457, 291, 611, 362, 264, 14715, 586, 420, 30, 51708], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1302, "seek": 294466, "start": 2971.54, "end": 2972.7799999999997, "text": " It's right here, my lap.", "tokens": [51708, 467, 311, 558, 510, 11, 452, 13214, 13, 51770], "temperature": 0.0, "avg_logprob": -0.2233008604783278, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012061623856425285}, {"id": 1303, "seek": 297466, "start": 2974.74, "end": 2976.14, "text": " Wonderful.", "tokens": [50368, 22768, 13, 50438], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1304, "seek": 297466, "start": 2976.14, "end": 2978.1, "text": " By the call, the battery,", "tokens": [50438, 3146, 264, 818, 11, 264, 5809, 11, 50536], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1305, "seek": 297466, "start": 2979.5, "end": 2980.3399999999997, "text": " at least.", "tokens": [50606, 412, 1935, 13, 50648], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1306, "seek": 297466, "start": 2981.5, "end": 2982.54, "text": " All right, sorry.", "tokens": [50706, 1057, 558, 11, 2597, 13, 50758], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1307, "seek": 297466, "start": 2982.54, "end": 2984.7, "text": " I'm gonna unbler my background.", "tokens": [50758, 286, 478, 799, 517, 65, 1918, 452, 3678, 13, 50866], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1308, "seek": 297466, "start": 2984.7, "end": 2986.66, "text": " I do have a, I just moved into a new apartment,", "tokens": [50866, 286, 360, 362, 257, 11, 286, 445, 4259, 666, 257, 777, 9587, 11, 50964], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1309, "seek": 297466, "start": 2986.66, "end": 2988.2999999999997, "text": " so forgive me for,", "tokens": [50964, 370, 10718, 385, 337, 11, 51046], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1310, "seek": 297466, "start": 2990.06, "end": 2995.06, "text": " but yeah, the battery right here.", "tokens": [51134, 457, 1338, 11, 264, 5809, 558, 510, 13, 51384], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1311, "seek": 297466, "start": 2995.2999999999997, "end": 2997.94, "text": " Yeah, that is able to say a lot more about the orb than I can.", "tokens": [51396, 865, 11, 300, 307, 1075, 281, 584, 257, 688, 544, 466, 264, 14715, 813, 286, 393, 13, 51528], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1312, "seek": 297466, "start": 2997.94, "end": 2999.8999999999996, "text": " If you work on the orb software team,", "tokens": [51528, 759, 291, 589, 322, 264, 14715, 4722, 1469, 11, 51626], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1313, "seek": 297466, "start": 2999.8999999999996, "end": 3003.2999999999997, "text": " you've been working on this for three years plus.", "tokens": [51626, 291, 600, 668, 1364, 322, 341, 337, 1045, 924, 1804, 13, 51796], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1314, "seek": 297466, "start": 3003.2999999999997, "end": 3004.14, "text": " Yeah.", "tokens": [51796, 865, 13, 51838], "temperature": 0.0, "avg_logprob": -0.4140369152200633, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.002148835454136133}, {"id": 1315, "seek": 300466, "start": 3004.94, "end": 3008.2999999999997, "text": " Yeah, I've come in multiple close contacts with the orb already", "tokens": [50378, 865, 11, 286, 600, 808, 294, 3866, 1998, 15836, 365, 264, 14715, 1217, 50546], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1316, "seek": 300466, "start": 3008.2999999999997, "end": 3011.02, "text": " and I think in 50 years you even have it like taken apart,", "tokens": [50546, 293, 286, 519, 294, 2625, 924, 291, 754, 362, 309, 411, 2726, 4936, 11, 50682], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1317, "seek": 300466, "start": 3011.02, "end": 3012.18, "text": " you know, and it's different components,", "tokens": [50682, 291, 458, 11, 293, 309, 311, 819, 6677, 11, 50740], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1318, "seek": 300466, "start": 3012.18, "end": 3013.62, "text": " which is really fun to see.", "tokens": [50740, 597, 307, 534, 1019, 281, 536, 13, 50812], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1319, "seek": 300466, "start": 3013.62, "end": 3015.3399999999997, "text": " So yeah, thanks.", "tokens": [50812, 407, 1338, 11, 3231, 13, 50898], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1320, "seek": 300466, "start": 3015.3399999999997, "end": 3017.74, "text": " The orb hardware specs are publicly available", "tokens": [50898, 440, 14715, 8837, 27911, 366, 14843, 2435, 51018], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1321, "seek": 300466, "start": 3017.74, "end": 3018.94, "text": " on GitHub as well.", "tokens": [51018, 322, 23331, 382, 731, 13, 51078], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1322, "seek": 300466, "start": 3018.94, "end": 3020.7, "text": " So people can see the PCB design,", "tokens": [51078, 407, 561, 393, 536, 264, 42065, 1715, 11, 51166], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1323, "seek": 300466, "start": 3020.7, "end": 3022.7, "text": " they can see like what components it's made out of.", "tokens": [51166, 436, 393, 536, 411, 437, 6677, 309, 311, 1027, 484, 295, 13, 51266], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1324, "seek": 300466, "start": 3022.7, "end": 3023.8999999999996, "text": " There's also an hour paper,", "tokens": [51266, 821, 311, 611, 364, 1773, 3035, 11, 51326], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1325, "seek": 300466, "start": 3023.8999999999996, "end": 3026.98, "text": " there's like an annotated set of every single component", "tokens": [51326, 456, 311, 411, 364, 25339, 770, 992, 295, 633, 2167, 6542, 51480], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1326, "seek": 300466, "start": 3026.98, "end": 3029.94, "text": " and like what it does, how it works, et cetera.", "tokens": [51480, 293, 411, 437, 309, 775, 11, 577, 309, 1985, 11, 1030, 11458, 13, 51628], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1327, "seek": 300466, "start": 3029.94, "end": 3031.14, "text": " Yeah, I've been following like", "tokens": [51628, 865, 11, 286, 600, 668, 3480, 411, 51688], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1328, "seek": 300466, "start": 3031.14, "end": 3032.3799999999997, "text": " just how many people are signing up", "tokens": [51688, 445, 577, 867, 561, 366, 13393, 493, 51750], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1329, "seek": 300466, "start": 3032.3799999999997, "end": 3034.58, "text": " and like the very, very long lines.", "tokens": [51750, 293, 411, 264, 588, 11, 588, 938, 3876, 13, 51860], "temperature": 0.0, "avg_logprob": -0.21666044212249388, "compression_ratio": 1.7317784256559767, "no_speech_prob": 0.0006665128748863935}, {"id": 1330, "seek": 303458, "start": 3035.34, "end": 3038.06, "text": " Sign up stations, which has been really interesting.", "tokens": [50402, 13515, 493, 13390, 11, 597, 575, 668, 534, 1880, 13, 50538], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1331, "seek": 303458, "start": 3038.06, "end": 3040.7, "text": " Okay, Micah, you rejoined and you raised your hand.", "tokens": [50538, 1033, 11, 5818, 545, 11, 291, 22087, 2001, 293, 291, 6005, 428, 1011, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1332, "seek": 303458, "start": 3040.7, "end": 3042.94, "text": " Do you want to ask you a final question?", "tokens": [50670, 1144, 291, 528, 281, 1029, 291, 257, 2572, 1168, 30, 50782], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1333, "seek": 303458, "start": 3042.94, "end": 3044.14, "text": " Testing, can you hear me?", "tokens": [50782, 45517, 11, 393, 291, 1568, 385, 30, 50842], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1334, "seek": 303458, "start": 3045.54, "end": 3046.58, "text": " Yep.", "tokens": [50912, 7010, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1335, "seek": 303458, "start": 3046.58, "end": 3048.7799999999997, "text": " Quick comment and then a question after that.", "tokens": [50964, 12101, 2871, 293, 550, 257, 1168, 934, 300, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1336, "seek": 303458, "start": 3048.7799999999997, "end": 3051.7, "text": " The, I believe that while the transactions,", "tokens": [51074, 440, 11, 286, 1697, 300, 1339, 264, 16856, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1337, "seek": 303458, "start": 3051.7, "end": 3054.66, "text": " your transactions made using your unique ID", "tokens": [51220, 428, 16856, 1027, 1228, 428, 3845, 7348, 51368], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1338, "seek": 303458, "start": 3054.66, "end": 3055.8199999999997, "text": " can't be linked back to you.", "tokens": [51368, 393, 380, 312, 9408, 646, 281, 291, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1339, "seek": 303458, "start": 3055.8199999999997, "end": 3057.58, "text": " You can be linked back to your transaction.", "tokens": [51426, 509, 393, 312, 9408, 646, 281, 428, 14425, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1340, "seek": 303458, "start": 3057.58, "end": 3060.06, "text": " Someone has an orb or the algorithm in the orb", "tokens": [51514, 8734, 575, 364, 14715, 420, 264, 9284, 294, 264, 14715, 51638], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1341, "seek": 303458, "start": 3060.06, "end": 3061.94, "text": " and they can get a picture of you, so to speak.", "tokens": [51638, 293, 436, 393, 483, 257, 3036, 295, 291, 11, 370, 281, 1710, 13, 51732], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1342, "seek": 303458, "start": 3061.94, "end": 3064.5, "text": " They can then regenerate your unique ID", "tokens": [51732, 814, 393, 550, 26358, 473, 428, 3845, 7348, 51860], "temperature": 0.0, "avg_logprob": -0.1654692225986057, "compression_ratio": 1.7800687285223367, "no_speech_prob": 0.00034058987512253225}, {"id": 1343, "seek": 306450, "start": 3064.5, "end": 3067.18, "text": " and this is an unnecessary piece.", "tokens": [50364, 293, 341, 307, 364, 19350, 2522, 13, 50498], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1344, "seek": 306450, "start": 3067.18, "end": 3069.42, "text": " You can't get rid of it because in order to have", "tokens": [50498, 509, 393, 380, 483, 3973, 295, 309, 570, 294, 1668, 281, 362, 50610], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1345, "seek": 306450, "start": 3069.42, "end": 3071.82, "text": " a unique human, you need to be able to verify.", "tokens": [50610, 257, 3845, 1952, 11, 291, 643, 281, 312, 1075, 281, 16888, 13, 50730], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1346, "seek": 306450, "start": 3071.82, "end": 3073.62, "text": " There needs to be only one outcome.", "tokens": [50730, 821, 2203, 281, 312, 787, 472, 9700, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1347, "seek": 306450, "start": 3073.62, "end": 3075.38, "text": " You can't introduce randomness here, right?", "tokens": [50820, 509, 393, 380, 5366, 4974, 1287, 510, 11, 558, 30, 50908], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1348, "seek": 306450, "start": 3075.38, "end": 3079.74, "text": " And so if your goal is to not have someone be able", "tokens": [50908, 400, 370, 498, 428, 3387, 307, 281, 406, 362, 1580, 312, 1075, 51126], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1349, "seek": 306450, "start": 3079.74, "end": 3081.7, "text": " to tell which transactions you did,", "tokens": [51126, 281, 980, 597, 16856, 291, 630, 11, 51224], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1350, "seek": 306450, "start": 3081.7, "end": 3082.66, "text": " that is not possible here.", "tokens": [51224, 300, 307, 406, 1944, 510, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1351, "seek": 306450, "start": 3082.66, "end": 3085.98, "text": " But someone can't tell just by looking at the transactions", "tokens": [51272, 583, 1580, 393, 380, 980, 445, 538, 1237, 412, 264, 16856, 51438], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1352, "seek": 306450, "start": 3085.98, "end": 3086.82, "text": " that they were yours.", "tokens": [51438, 300, 436, 645, 6342, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1353, "seek": 306450, "start": 3086.82, "end": 3088.22, "text": " It's a one-way thing.", "tokens": [51480, 467, 311, 257, 472, 12, 676, 551, 13, 51550], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1354, "seek": 306450, "start": 3088.22, "end": 3089.98, "text": " This one held you down and put an orb in front of your eye,", "tokens": [51550, 639, 472, 5167, 291, 760, 293, 829, 364, 14715, 294, 1868, 295, 428, 3313, 11, 51638], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1355, "seek": 306450, "start": 3089.98, "end": 3091.6, "text": " they can then figure out all your transactions,", "tokens": [51638, 436, 393, 550, 2573, 484, 439, 428, 16856, 11, 51719], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1356, "seek": 306450, "start": 3091.6, "end": 3093.26, "text": " but they couldn't look at your transaction", "tokens": [51719, 457, 436, 2809, 380, 574, 412, 428, 14425, 51802], "temperature": 0.0, "avg_logprob": -0.1572359561920166, "compression_ratio": 1.803125, "no_speech_prob": 0.00014882994582876563}, {"id": 1357, "seek": 309326, "start": 3093.26, "end": 3096.6200000000003, "text": " and figure out which eye they belong to, so to speak.", "tokens": [50364, 293, 2573, 484, 597, 3313, 436, 5784, 281, 11, 370, 281, 1710, 13, 50532], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1358, "seek": 309326, "start": 3096.6200000000003, "end": 3100.26, "text": " Yes and no, there's one step that helps us mitigate this,", "tokens": [50532, 1079, 293, 572, 11, 456, 311, 472, 1823, 300, 3665, 505, 27336, 341, 11, 50714], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1359, "seek": 309326, "start": 3100.26, "end": 3102.1800000000003, "text": " which is the separation of the public key, right?", "tokens": [50714, 597, 307, 264, 14634, 295, 264, 1908, 2141, 11, 558, 30, 50810], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1360, "seek": 309326, "start": 3102.1800000000003, "end": 3103.9, "text": " Like your transactions are not being done", "tokens": [50810, 1743, 428, 16856, 366, 406, 885, 1096, 50896], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1361, "seek": 309326, "start": 3103.9, "end": 3105.86, "text": " by your iris code or whatever.", "tokens": [50896, 538, 428, 3418, 271, 3089, 420, 2035, 13, 50994], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1362, "seek": 309326, "start": 3105.86, "end": 3107.94, "text": " The transactions are being done by a public key", "tokens": [50994, 440, 16856, 366, 885, 1096, 538, 257, 1908, 2141, 51098], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1363, "seek": 309326, "start": 3107.94, "end": 3109.2200000000003, "text": " which was owned by a user,", "tokens": [51098, 597, 390, 11684, 538, 257, 4195, 11, 51162], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1364, "seek": 309326, "start": 3109.2200000000003, "end": 3111.9, "text": " which just happened to verify at the orb, right?", "tokens": [51162, 597, 445, 2011, 281, 16888, 412, 264, 14715, 11, 558, 30, 51296], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1365, "seek": 309326, "start": 3111.9, "end": 3114.5800000000004, "text": " You would have to get the user's private key", "tokens": [51296, 509, 576, 362, 281, 483, 264, 4195, 311, 4551, 2141, 51430], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1366, "seek": 309326, "start": 3114.5800000000004, "end": 3116.5, "text": " to learn what they did on the game.", "tokens": [51430, 281, 1466, 437, 436, 630, 322, 264, 1216, 13, 51526], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1367, "seek": 309326, "start": 3116.5, "end": 3118.46, "text": " And then you would also have to get their bandwidth forked", "tokens": [51526, 400, 550, 291, 576, 611, 362, 281, 483, 641, 23647, 17716, 292, 51624], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1368, "seek": 309326, "start": 3118.46, "end": 3120.82, "text": " to try and interlink too, right?", "tokens": [51624, 281, 853, 293, 728, 22473, 886, 11, 558, 30, 51742], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1369, "seek": 309326, "start": 3120.82, "end": 3122.0600000000004, "text": " You'd have to get their bandwidth forked,", "tokens": [51742, 509, 1116, 362, 281, 483, 641, 23647, 17716, 292, 11, 51804], "temperature": 0.0, "avg_logprob": -0.19974975344500964, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0022864544298499823}, {"id": 1370, "seek": 312206, "start": 3122.06, "end": 3123.22, "text": " you'd generate the iris code,", "tokens": [50364, 291, 1116, 8460, 264, 3418, 271, 3089, 11, 50422], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1371, "seek": 312206, "start": 3123.22, "end": 3125.7799999999997, "text": " and then get somehow steal from them their private key.", "tokens": [50422, 293, 550, 483, 6063, 11009, 490, 552, 641, 4551, 2141, 13, 50550], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1372, "seek": 312206, "start": 3125.7799999999997, "end": 3126.98, "text": " And then with a private key,", "tokens": [50550, 400, 550, 365, 257, 4551, 2141, 11, 50610], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1373, "seek": 312206, "start": 3126.98, "end": 3130.1, "text": " you're able to de-anonymize the on-chain state", "tokens": [50610, 291, 434, 1075, 281, 368, 12, 282, 12732, 1125, 264, 322, 12, 11509, 1785, 50766], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1374, "seek": 312206, "start": 3130.1, "end": 3131.7, "text": " that they performed in a DK way", "tokens": [50766, 300, 436, 10332, 294, 257, 31934, 636, 50846], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1375, "seek": 312206, "start": 3131.7, "end": 3134.34, "text": " because you're not able to just get the live iris.", "tokens": [50846, 570, 291, 434, 406, 1075, 281, 445, 483, 264, 1621, 3418, 271, 13, 50978], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1376, "seek": 312206, "start": 3134.34, "end": 3135.96, "text": " You know you can generate them.", "tokens": [50978, 509, 458, 291, 393, 8460, 552, 13, 51059], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1377, "seek": 312206, "start": 3135.96, "end": 3139.7799999999997, "text": " And so there's two things that you need to compromise there.", "tokens": [51059, 400, 370, 456, 311, 732, 721, 300, 291, 643, 281, 18577, 456, 13, 51250], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1378, "seek": 312206, "start": 3139.7799999999997, "end": 3140.98, "text": " I'll just, my actual question,", "tokens": [51250, 286, 603, 445, 11, 452, 3539, 1168, 11, 51310], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1379, "seek": 312206, "start": 3140.98, "end": 3141.82, "text": " I'll try to keep it,", "tokens": [51310, 286, 603, 853, 281, 1066, 309, 11, 51352], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1380, "seek": 312206, "start": 3141.82, "end": 3143.46, "text": " I know I've got one minute left.", "tokens": [51352, 286, 458, 286, 600, 658, 472, 3456, 1411, 13, 51434], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1381, "seek": 312206, "start": 3143.46, "end": 3146.18, "text": " Last I checked, DK proof of an execution takes", "tokens": [51434, 5264, 286, 10033, 11, 31934, 8177, 295, 364, 15058, 2516, 51570], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1382, "seek": 312206, "start": 3146.18, "end": 3148.74, "text": " on the order of a thousand times or so,", "tokens": [51570, 322, 264, 1668, 295, 257, 4714, 1413, 420, 370, 11, 51698], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1383, "seek": 312206, "start": 3148.74, "end": 3151.58, "text": " executing the same thing without a DK proof.", "tokens": [51698, 32368, 264, 912, 551, 1553, 257, 31934, 8177, 13, 51840], "temperature": 0.0, "avg_logprob": -0.27058944234087423, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.0017005546251311898}, {"id": 1384, "seek": 315158, "start": 3151.58, "end": 3153.74, "text": " Even though inference is significantly cheaper", "tokens": [50364, 2754, 1673, 38253, 307, 10591, 12284, 50472], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1385, "seek": 315158, "start": 3153.74, "end": 3156.7, "text": " than training, execution costs is still very non-trivial.", "tokens": [50472, 813, 3097, 11, 15058, 5497, 307, 920, 588, 2107, 12, 83, 470, 22640, 13, 50620], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1386, "seek": 315158, "start": 3156.7, "end": 3159.34, "text": " That's why you need giant GPUs and whatnot,", "tokens": [50620, 663, 311, 983, 291, 643, 7410, 18407, 82, 293, 25882, 11, 50752], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1387, "seek": 315158, "start": 3159.34, "end": 3161.22, "text": " just to do inference.", "tokens": [50752, 445, 281, 360, 38253, 13, 50846], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1388, "seek": 315158, "start": 3161.22, "end": 3162.62, "text": " The use cases you're thinking of,", "tokens": [50846, 440, 764, 3331, 291, 434, 1953, 295, 11, 50916], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1389, "seek": 315158, "start": 3162.62, "end": 3165.58, "text": " are they all things that are like become useful", "tokens": [50916, 366, 436, 439, 721, 300, 366, 411, 1813, 4420, 51064], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1390, "seek": 315158, "start": 3165.58, "end": 3169.18, "text": " once we can get the DK proofing costs down by a hundred times?", "tokens": [51064, 1564, 321, 393, 483, 264, 31934, 8177, 278, 5497, 760, 538, 257, 3262, 1413, 30, 51244], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1391, "seek": 315158, "start": 3169.18, "end": 3171.46, "text": " Or do you think there's some things that are usable", "tokens": [51244, 1610, 360, 291, 519, 456, 311, 512, 721, 300, 366, 29975, 51358], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1392, "seek": 315158, "start": 3171.46, "end": 3174.46, "text": " even with that thousand increase in execution costs?", "tokens": [51358, 754, 365, 300, 4714, 3488, 294, 15058, 5497, 30, 51508], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1393, "seek": 315158, "start": 3174.46, "end": 3177.2999999999997, "text": " Right now there's already like DK use cases on-chain, right?", "tokens": [51508, 1779, 586, 456, 311, 1217, 411, 31934, 764, 3331, 322, 12, 11509, 11, 558, 30, 51650], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1394, "seek": 315158, "start": 3177.2999999999997, "end": 3180.46, "text": " And there's equally expensive in the ML lens,", "tokens": [51650, 400, 456, 311, 12309, 5124, 294, 264, 21601, 6765, 11, 51808], "temperature": 0.0, "avg_logprob": -0.17547839923496664, "compression_ratio": 1.7110389610389611, "no_speech_prob": 0.0014549107290804386}, {"id": 1395, "seek": 318046, "start": 3180.46, "end": 3184.02, "text": " like already proved a hundred million like parameter models", "tokens": [50364, 411, 1217, 14617, 257, 3262, 2459, 411, 13075, 5245, 50542], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1396, "seek": 318046, "start": 3184.02, "end": 3185.7, "text": " in an inexpensive way, right?", "tokens": [50542, 294, 364, 28382, 636, 11, 558, 30, 50626], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1397, "seek": 318046, "start": 3185.7, "end": 3187.1, "text": " In a usable way, right?", "tokens": [50626, 682, 257, 29975, 636, 11, 558, 30, 50696], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1398, "seek": 318046, "start": 3187.1, "end": 3189.2200000000003, "text": " Let's say you have a small convolutional neural network", "tokens": [50696, 961, 311, 584, 291, 362, 257, 1359, 45216, 304, 18161, 3209, 50802], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1399, "seek": 318046, "start": 3189.2200000000003, "end": 3192.5, "text": " classifier with 200 million like weights,", "tokens": [50802, 1508, 9902, 365, 2331, 2459, 411, 17443, 11, 50966], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1400, "seek": 318046, "start": 3192.5, "end": 3193.58, "text": " like flowing point,", "tokens": [50966, 411, 13974, 935, 11, 51020], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1401, "seek": 318046, "start": 3193.58, "end": 3196.34, "text": " or in this case for a ZKML with field elements,", "tokens": [51020, 420, 294, 341, 1389, 337, 257, 1176, 42, 12683, 365, 2519, 4959, 11, 51158], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1402, "seek": 318046, "start": 3196.34, "end": 3198.82, "text": " but you're able to make proofs in reasonable time,", "tokens": [51158, 457, 291, 434, 1075, 281, 652, 8177, 82, 294, 10585, 565, 11, 51282], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1403, "seek": 318046, "start": 3198.82, "end": 3201.02, "text": " one to two minutes for inference, right?", "tokens": [51282, 472, 281, 732, 2077, 337, 38253, 11, 558, 30, 51392], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1404, "seek": 318046, "start": 3201.02, "end": 3204.54, "text": " Where the evaluation of it in the normal world", "tokens": [51392, 2305, 264, 13344, 295, 309, 294, 264, 2710, 1002, 51568], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1405, "seek": 318046, "start": 3204.54, "end": 3205.7, "text": " is a thousand nights less,", "tokens": [51568, 307, 257, 4714, 13249, 1570, 11, 51626], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1406, "seek": 318046, "start": 3205.7, "end": 3208.7400000000002, "text": " two millisecond, 20 millisecond, like point two second,", "tokens": [51626, 732, 27940, 18882, 11, 945, 27940, 18882, 11, 411, 935, 732, 1150, 11, 51778], "temperature": 0.0, "avg_logprob": -0.23013561875072877, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.001926663564518094}, {"id": 1407, "seek": 320874, "start": 3208.74, "end": 3210.3399999999997, "text": " or 200 million, my bad.", "tokens": [50364, 420, 2331, 2459, 11, 452, 1578, 13, 50444], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1408, "seek": 320874, "start": 3210.3399999999997, "end": 3212.7799999999997, "text": " So yeah, like this is like the costly incur,", "tokens": [50444, 407, 1338, 11, 411, 341, 307, 411, 264, 28328, 35774, 11, 50566], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1409, "seek": 320874, "start": 3212.7799999999997, "end": 3214.74, "text": " but it makes sense for some things.", "tokens": [50566, 457, 309, 1669, 2020, 337, 512, 721, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1410, "seek": 320874, "start": 3214.74, "end": 3216.8199999999997, "text": " And right now, as I mentioned,", "tokens": [50664, 400, 558, 586, 11, 382, 286, 2835, 11, 50768], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1411, "seek": 320874, "start": 3216.8199999999997, "end": 3218.2999999999997, "text": " like the things that we're doing could prove the thing,", "tokens": [50768, 411, 264, 721, 300, 321, 434, 884, 727, 7081, 264, 551, 11, 50842], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1412, "seek": 320874, "start": 3218.2999999999997, "end": 3220.14, "text": " like cryptography, better implementation,", "tokens": [50842, 411, 9844, 5820, 11, 1101, 11420, 11, 50934], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1413, "seek": 320874, "start": 3220.14, "end": 3221.8999999999996, "text": " better hardware, specialized hardware, et cetera,", "tokens": [50934, 1101, 8837, 11, 19813, 8837, 11, 1030, 11458, 11, 51022], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1414, "seek": 320874, "start": 3221.8999999999996, "end": 3223.62, "text": " they're gonna bring down this cost significantly,", "tokens": [51022, 436, 434, 799, 1565, 760, 341, 2063, 10591, 11, 51108], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1415, "seek": 320874, "start": 3223.62, "end": 3225.4599999999996, "text": " it's gonna make more things feasible.", "tokens": [51108, 309, 311, 799, 652, 544, 721, 26648, 13, 51200], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1416, "seek": 320874, "start": 3225.4599999999996, "end": 3227.22, "text": " Like now we can maybe prove an LLM,", "tokens": [51200, 1743, 586, 321, 393, 1310, 7081, 364, 441, 43, 44, 11, 51288], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1417, "seek": 320874, "start": 3227.22, "end": 3229.14, "text": " it may take 10 minutes,", "tokens": [51288, 309, 815, 747, 1266, 2077, 11, 51384], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1418, "seek": 320874, "start": 3229.14, "end": 3231.1, "text": " but maybe proving that LLM once", "tokens": [51384, 457, 1310, 27221, 300, 441, 43, 44, 1564, 51482], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1419, "seek": 320874, "start": 3231.1, "end": 3234.7, "text": " for something that's really important enables a new thing.", "tokens": [51482, 337, 746, 300, 311, 534, 1021, 17077, 257, 777, 551, 13, 51662], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1420, "seek": 320874, "start": 3234.7, "end": 3235.8999999999996, "text": " And it's always happened like this,", "tokens": [51662, 400, 309, 311, 1009, 2011, 411, 341, 11, 51722], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1421, "seek": 320874, "start": 3235.8999999999996, "end": 3237.62, "text": " that like the use case and the demand for it", "tokens": [51722, 300, 411, 264, 764, 1389, 293, 264, 4733, 337, 309, 51808], "temperature": 0.0, "avg_logprob": -0.18349308823094224, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.003027469851076603}, {"id": 1422, "seek": 323762, "start": 3237.66, "end": 3239.06, "text": " and bring the proving time down,", "tokens": [50366, 293, 1565, 264, 27221, 565, 760, 11, 50436], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1423, "seek": 323762, "start": 3239.06, "end": 3240.58, "text": " the overhead down, the performance up,", "tokens": [50436, 264, 19922, 760, 11, 264, 3389, 493, 11, 50512], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1424, "seek": 323762, "start": 3240.58, "end": 3243.06, "text": " and we're still like have a bunch of things to do.", "tokens": [50512, 293, 321, 434, 920, 411, 362, 257, 3840, 295, 721, 281, 360, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1425, "seek": 323762, "start": 3243.06, "end": 3245.2599999999998, "text": " So it should work out eventually.", "tokens": [50636, 407, 309, 820, 589, 484, 4728, 13, 50746], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1426, "seek": 323762, "start": 3246.3399999999997, "end": 3247.18, "text": " Love it.", "tokens": [50800, 5956, 309, 13, 50842], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1427, "seek": 323762, "start": 3247.18, "end": 3248.02, "text": " Any final words?", "tokens": [50842, 2639, 2572, 2283, 30, 50884], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1428, "seek": 323762, "start": 3248.02, "end": 3248.98, "text": " How can people find out more?", "tokens": [50884, 1012, 393, 561, 915, 484, 544, 30, 50932], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1429, "seek": 323762, "start": 3248.98, "end": 3250.2999999999997, "text": " I know that you said, for example,", "tokens": [50932, 286, 458, 300, 291, 848, 11, 337, 1365, 11, 50998], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1430, "seek": 323762, "start": 3250.2999999999997, "end": 3251.7, "text": " there's an announcement coming on soon,", "tokens": [50998, 456, 311, 364, 12847, 1348, 322, 2321, 11, 51068], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1431, "seek": 323762, "start": 3251.7, "end": 3253.1, "text": " but if people are really excited about this", "tokens": [51068, 457, 498, 561, 366, 534, 2919, 466, 341, 51138], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1432, "seek": 323762, "start": 3253.1, "end": 3254.2999999999997, "text": " or they just wanna learn more,", "tokens": [51138, 420, 436, 445, 1948, 1466, 544, 11, 51198], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1433, "seek": 323762, "start": 3254.2999999999997, "end": 3257.06, "text": " what are any possible action items that people can take?", "tokens": [51198, 437, 366, 604, 1944, 3069, 4754, 300, 561, 393, 747, 30, 51336], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1434, "seek": 323762, "start": 3257.06, "end": 3258.22, "text": " So action items.", "tokens": [51336, 407, 3069, 4754, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1435, "seek": 323762, "start": 3258.22, "end": 3260.5, "text": " So if you have a specific question about this presentation,", "tokens": [51394, 407, 498, 291, 362, 257, 2685, 1168, 466, 341, 5860, 11, 51508], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1436, "seek": 323762, "start": 3260.5, "end": 3262.98, "text": " you wanna ask me, I think my Twitter or my Telegram,", "tokens": [51508, 291, 1948, 1029, 385, 11, 286, 519, 452, 5794, 420, 452, 14889, 1342, 11, 51632], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1437, "seek": 323762, "start": 3262.98, "end": 3264.94, "text": " or like my ex and my Telegram are the best.", "tokens": [51632, 420, 411, 452, 454, 293, 452, 14889, 1342, 366, 264, 1151, 13, 51730], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1438, "seek": 323762, "start": 3264.94, "end": 3267.5, "text": " So my handle is DC build 3R.", "tokens": [51730, 407, 452, 4813, 307, 9114, 1322, 805, 49, 13, 51858], "temperature": 0.0, "avg_logprob": -0.1687055652061205, "compression_ratio": 1.7277777777777779, "no_speech_prob": 0.0004878347390331328}, {"id": 1439, "seek": 326750, "start": 3267.54, "end": 3271.82, "text": " So DC builder, but with the 3 instead of an E at the end.", "tokens": [50366, 407, 9114, 27377, 11, 457, 365, 264, 805, 2602, 295, 364, 462, 412, 264, 917, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1440, "seek": 326750, "start": 3271.82, "end": 3273.5, "text": " So that is for asking me questions.", "tokens": [50580, 407, 300, 307, 337, 3365, 385, 1651, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1441, "seek": 326750, "start": 3273.5, "end": 3276.66, "text": " If you were interested in ZKML itself,", "tokens": [50664, 759, 291, 645, 3102, 294, 1176, 42, 12683, 2564, 11, 50822], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1442, "seek": 326750, "start": 3276.66, "end": 3280.94, "text": " I do have a resource aggregator for ZKML things.", "tokens": [50822, 286, 360, 362, 257, 7684, 16743, 1639, 337, 1176, 42, 12683, 721, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1443, "seek": 326750, "start": 3280.94, "end": 3283.54, "text": " It's on my, or one of my GitHub's,", "tokens": [51036, 467, 311, 322, 452, 11, 420, 472, 295, 452, 23331, 311, 11, 51166], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1444, "seek": 326750, "start": 3283.54, "end": 3288.54, "text": " which is github.com slash ZKML dash community slash", "tokens": [51166, 597, 307, 290, 355, 836, 13, 1112, 17330, 1176, 42, 12683, 8240, 1768, 17330, 51416], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1445, "seek": 326750, "start": 3288.86, "end": 3290.86, "text": " awesome dash ZKML.", "tokens": [51432, 3476, 8240, 1176, 42, 12683, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1446, "seek": 326750, "start": 3290.86, "end": 3293.94, "text": " I'm able to leave the link in the chat real quick,", "tokens": [51532, 286, 478, 1075, 281, 1856, 264, 2113, 294, 264, 5081, 957, 1702, 11, 51686], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1447, "seek": 326750, "start": 3293.94, "end": 3296.38, "text": " but I think like the spelling makes sense.", "tokens": [51686, 457, 286, 519, 411, 264, 22254, 1669, 2020, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1069731278852983, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.0015728730941191316}, {"id": 1448, "seek": 329638, "start": 3296.42, "end": 3299.38, "text": " Besides that, there's a bunch of startups working in ZKML,", "tokens": [50366, 13212, 300, 11, 456, 311, 257, 3840, 295, 28041, 1364, 294, 1176, 42, 12683, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1449, "seek": 329638, "start": 3299.38, "end": 3303.82, "text": " mostly like what I mentioned, like Modulus, Giza, and Ezekio.", "tokens": [50514, 5240, 411, 437, 286, 2835, 11, 411, 6583, 26107, 11, 460, 13427, 11, 293, 462, 19878, 1004, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1450, "seek": 329638, "start": 3303.82, "end": 3306.94, "text": " And these three keep coming out with new developments,", "tokens": [50736, 400, 613, 1045, 1066, 1348, 484, 365, 777, 20862, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1451, "seek": 329638, "start": 3306.94, "end": 3308.9, "text": " new things, new announcements, et cetera.", "tokens": [50892, 777, 721, 11, 777, 23785, 11, 1030, 11458, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1452, "seek": 329638, "start": 3308.9, "end": 3311.3, "text": " There's cryptographic papers coming out on ZKML,", "tokens": [50990, 821, 311, 9844, 12295, 10577, 1348, 484, 322, 1176, 42, 12683, 11, 51110], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1453, "seek": 329638, "start": 3311.3, "end": 3313.82, "text": " which I also try and keep up to date on my resource.", "tokens": [51110, 597, 286, 611, 853, 293, 1066, 493, 281, 4002, 322, 452, 7684, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1454, "seek": 329638, "start": 3313.82, "end": 3316.9, "text": " So yeah, those are like the best ones, I think.", "tokens": [51236, 407, 1338, 11, 729, 366, 411, 264, 1151, 2306, 11, 286, 519, 13, 51390], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1455, "seek": 329638, "start": 3317.7400000000002, "end": 3318.58, "text": " Love it.", "tokens": [51432, 5956, 309, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1456, "seek": 329638, "start": 3318.58, "end": 3319.58, "text": " And I just saw from Dan that he's bringing", "tokens": [51474, 400, 286, 445, 1866, 490, 3394, 300, 415, 311, 5062, 51524], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1457, "seek": 329638, "start": 3319.58, "end": 3321.58, "text": " up to our upcoming May workshop though.", "tokens": [51524, 493, 281, 527, 11500, 1891, 13541, 1673, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1458, "seek": 329638, "start": 3321.58, "end": 3324.02, "text": " If you're going to that one, you may be able to try it.", "tokens": [51624, 759, 291, 434, 516, 281, 300, 472, 11, 291, 815, 312, 1075, 281, 853, 309, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1459, "seek": 329638, "start": 3324.02, "end": 3324.98, "text": " Hey, thank you so much.", "tokens": [51746, 1911, 11, 1309, 291, 370, 709, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1460, "seek": 329638, "start": 3324.98, "end": 3326.2200000000003, "text": " This was really fantastic.", "tokens": [51794, 639, 390, 534, 5456, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1677146728881105, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005112348007969558}, {"id": 1461, "seek": 332622, "start": 3326.2599999999998, "end": 3327.8199999999997, "text": " Thanks for staying on three minutes longer.", "tokens": [50366, 2561, 337, 7939, 322, 1045, 2077, 2854, 13, 50444], "temperature": 0.0, "avg_logprob": -0.18077590617727726, "compression_ratio": 1.3008130081300813, "no_speech_prob": 0.0010469695553183556}, {"id": 1462, "seek": 332622, "start": 3327.8199999999997, "end": 3328.74, "text": " I really appreciated it.", "tokens": [50444, 286, 534, 17169, 309, 13, 50490], "temperature": 0.0, "avg_logprob": -0.18077590617727726, "compression_ratio": 1.3008130081300813, "no_speech_prob": 0.0010469695553183556}, {"id": 1463, "seek": 332622, "start": 3328.74, "end": 3330.3799999999997, "text": " Thanks for all of your great questions, everyone,", "tokens": [50490, 2561, 337, 439, 295, 428, 869, 1651, 11, 1518, 11, 50572], "temperature": 0.0, "avg_logprob": -0.18077590617727726, "compression_ratio": 1.3008130081300813, "no_speech_prob": 0.0010469695553183556}, {"id": 1464, "seek": 332622, "start": 3330.3799999999997, "end": 3331.8599999999997, "text": " and I hope to see you guys soon.", "tokens": [50572, 293, 286, 1454, 281, 536, 291, 1074, 2321, 13, 50646], "temperature": 0.0, "avg_logprob": -0.18077590617727726, "compression_ratio": 1.3008130081300813, "no_speech_prob": 0.0010469695553183556}, {"id": 1465, "seek": 332622, "start": 3331.8599999999997, "end": 3332.7, "text": " Bye-bye.", "tokens": [50646, 4621, 12, 6650, 13, 50688], "temperature": 0.0, "avg_logprob": -0.18077590617727726, "compression_ratio": 1.3008130081300813, "no_speech_prob": 0.0010469695553183556}], "language": "en"}