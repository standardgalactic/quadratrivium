1
00:00:00,000 --> 00:00:01,960
Today, we're going to talk about AI alignment.

2
00:00:01,960 --> 00:00:04,200
Is it even harder than we think?

3
00:00:04,200 --> 00:00:08,160
And this is the third AI focus on the salon foresight.

4
00:00:08,160 --> 00:00:11,200
In the previous two weeks, we discussed chapters

5
00:00:11,200 --> 00:00:13,600
in the recently published book on superintelligence,

6
00:00:13,600 --> 00:00:17,000
a coordination strategy, which a few people here on this call

7
00:00:17,000 --> 00:00:18,440
have submitted chapters to.

8
00:00:18,440 --> 00:00:20,960
And then based on those previous salons,

9
00:00:20,960 --> 00:00:23,920
Daniel Schmachtenberger, who we had for another salon

10
00:00:23,920 --> 00:00:26,360
in the Hive Mind series, reached out to me

11
00:00:26,360 --> 00:00:28,600
and recommended today's speaker to me,

12
00:00:28,600 --> 00:00:32,120
because he may bring an underappreciated perspective

13
00:00:32,120 --> 00:00:34,600
to the set of problems in AI alignment.

14
00:00:34,600 --> 00:00:37,120
And as a disclaimer, Forest Landry

15
00:00:37,120 --> 00:00:40,080
is from the newer Hector Collective, Welcome Forest,

16
00:00:40,080 --> 00:00:43,520
and he's not an AI safety or AI alignment researcher.

17
00:00:43,520 --> 00:00:46,000
But I think that's exactly why he thinks

18
00:00:46,000 --> 00:00:49,720
that he may have an outsider's eye to contribute to the field,

19
00:00:49,720 --> 00:00:52,080
and he may be able to point to a few areas that

20
00:00:52,080 --> 00:00:54,080
are under explored.

21
00:00:54,080 --> 00:00:56,720
So we'll kick off with a presentation by Forest,

22
00:00:56,720 --> 00:00:58,560
who's outlining a list of reasons

23
00:00:58,560 --> 00:01:01,200
why AI alignment may be harder than we think,

24
00:01:01,200 --> 00:01:03,840
perhaps even impossible.

25
00:01:03,840 --> 00:01:08,120
And then we open up for a discussion with all of you.

26
00:01:08,120 --> 00:01:11,440
And we may start with our foresight fellows,

27
00:01:11,440 --> 00:01:14,120
for example, Creeon Levitt, who's a foresight senior fellow

28
00:01:14,120 --> 00:01:17,440
and physicist who may be enticed to speak a little bit

29
00:01:17,440 --> 00:01:20,400
on a few physical claims that Forest makes,

30
00:01:20,400 --> 00:01:23,040
as well as channeling Anders Sandberg, who's

31
00:01:23,040 --> 00:01:25,720
our senior fellow in philosophy, but who's currently

32
00:01:25,720 --> 00:01:28,560
on the flight and can't make it, but he says hi to you, Forest,

33
00:01:28,560 --> 00:01:30,320
and he would have loved to make it.

34
00:01:30,320 --> 00:01:32,160
And he has an update on a paper that I

35
00:01:32,160 --> 00:01:35,680
think you're wanting to write, which we can come back later.

36
00:01:35,680 --> 00:01:38,840
But then maybe we'll also be joined by Dan Elton, our 2020

37
00:01:38,840 --> 00:01:42,760
fellow in AI, and by Jeff Ladish, our 2020 fellow

38
00:01:42,760 --> 00:01:45,160
in biosecurity, who may also be enticed

39
00:01:45,160 --> 00:01:48,600
to kind of make a few comments to kick us off with a tricky,

40
00:01:48,600 --> 00:01:50,880
but hopefully really fun discussion.

41
00:01:50,880 --> 00:01:54,040
OK, so today we'll definitely be a little bit of an experiment,

42
00:01:54,040 --> 00:01:57,120
and I encourage all of you with relevant background

43
00:01:57,120 --> 00:02:00,400
to speak up, who wants to kind of rat team Forest's claim,

44
00:02:00,400 --> 00:02:03,720
but then also maybe to surface potential other problems

45
00:02:03,720 --> 00:02:05,800
in AI alignment that haven't been

46
00:02:05,800 --> 00:02:08,200
surfaced in the presentation, that we ought to bring

47
00:02:08,200 --> 00:02:10,520
to the table in the discussion.

48
00:02:10,520 --> 00:02:13,560
OK, that's enough for me, so let's

49
00:02:13,560 --> 00:02:16,720
have Forest present his list of good reasons

50
00:02:16,720 --> 00:02:20,480
to be skeptical that AI alignment is possible for 30 minutes,

51
00:02:20,480 --> 00:02:22,520
and then we'll jump right into the discussion,

52
00:02:22,520 --> 00:02:25,000
and I'm going to share a little bit more

53
00:02:25,000 --> 00:02:28,200
on the background on the topic of today and the talk

54
00:02:28,200 --> 00:02:30,440
in the salons generally in the tent.

55
00:02:30,440 --> 00:02:32,920
All right, Forest, take it away.

56
00:02:32,920 --> 00:02:33,920
Good morning.

57
00:02:33,920 --> 00:02:37,160
Thank you so much for such a wonderful introduction.

58
00:02:37,160 --> 00:02:38,040
I'm glad to be here.

59
00:02:38,040 --> 00:02:42,480
Obviously, as she mentioned, my main area of work

60
00:02:42,480 --> 00:02:44,400
happens to do more with communities,

61
00:02:44,400 --> 00:02:46,320
so I spend a lot of time thinking

62
00:02:46,320 --> 00:02:50,160
about the relationships between man, machine, and nature

63
00:02:50,160 --> 00:02:53,200
from user interface point of view as a software architect.

64
00:02:53,200 --> 00:02:55,760
I've done a lot of work for the federal government,

65
00:02:55,760 --> 00:03:00,600
written number of programs, and search kind of technology stuff.

66
00:03:00,600 --> 00:03:03,040
So being an architect of software systems

67
00:03:03,040 --> 00:03:06,400
for good three decades now, I have a perspective,

68
00:03:06,400 --> 00:03:09,480
and I thought that I might share some elements of that.

69
00:03:09,480 --> 00:03:11,680
Seeing as how from what I can maybe

70
00:03:11,680 --> 00:03:15,240
see that some elements might be unusual or new,

71
00:03:15,240 --> 00:03:18,920
I will at least provide some fruitful conversation.

72
00:03:18,960 --> 00:03:21,640
So the main thing that I'm basically wanting to do then

73
00:03:21,640 --> 00:03:24,760
is just give an outline of what I'm thinking about.

74
00:03:24,760 --> 00:03:27,200
I've got some notes, so I'll be reading a little bit.

75
00:03:27,200 --> 00:03:30,080
But basically, the idea is that if we're looking at AI alignment,

76
00:03:30,080 --> 00:03:32,240
what is the frame that we're going to use to essentially

77
00:03:32,240 --> 00:03:33,400
evaluate that question?

78
00:03:33,400 --> 00:03:35,920
So in other words, how do we determine whether or not

79
00:03:35,920 --> 00:03:39,320
this is a question that can be solved?

80
00:03:39,320 --> 00:03:41,560
So as a kind of schema of thinking about this,

81
00:03:41,560 --> 00:03:45,240
it's like AI can be thought of as a thing that

82
00:03:45,240 --> 00:03:46,760
is in relation to mankind.

83
00:03:46,800 --> 00:03:49,160
So in other words, I'm first positing

84
00:03:49,160 --> 00:03:52,440
that there's a separation between the human nature

85
00:03:52,440 --> 00:03:54,800
and the machine nature.

86
00:03:54,800 --> 00:03:57,200
There are some ways of thinking about this

87
00:03:57,200 --> 00:04:01,160
that look at hybrid cyborg type of things

88
00:04:01,160 --> 00:04:04,520
where there's a kind of mixing of machine and nature,

89
00:04:04,520 --> 00:04:06,160
or machine and man, and stuff like that.

90
00:04:06,160 --> 00:04:08,400
I'm not really considering that explicitly.

91
00:04:08,400 --> 00:04:11,920
I'm looking more at the you have an artificial intelligence

92
00:04:11,920 --> 00:04:13,400
that's an entity unto itself, and you

93
00:04:13,400 --> 00:04:15,280
have human beings which are entity unto themselves,

94
00:04:15,280 --> 00:04:17,440
and what are the interactions between the two of those?

95
00:04:17,440 --> 00:04:20,480
So I'm simplifying the AI alignment question

96
00:04:20,480 --> 00:04:22,560
to look a little bit about what would it

97
00:04:22,560 --> 00:04:25,000
take to align machine intelligence

98
00:04:25,000 --> 00:04:28,680
agency with human intelligence and agency.

99
00:04:28,680 --> 00:04:31,160
So in that sense, we can start to think about the AI alignment

100
00:04:31,160 --> 00:04:33,960
problem a little bit like trying to build a perfect shell.

101
00:04:33,960 --> 00:04:37,320
So in other words, we want to make sure it doesn't have any holes.

102
00:04:37,320 --> 00:04:41,000
And when we look at it from a structural point of view,

103
00:04:41,000 --> 00:04:44,080
we realized that it's not really the same as dealing

104
00:04:44,080 --> 00:04:47,920
with things like energy or flow of matter or stuff like that.

105
00:04:47,920 --> 00:04:50,880
So in other words, if I was trying to build a boat,

106
00:04:50,880 --> 00:04:53,280
that the shell of the boat has to be more

107
00:04:53,280 --> 00:04:57,320
perfect for AI alignment than it would be for seawater.

108
00:04:57,320 --> 00:04:58,720
Because if I have a small leak, I'm

109
00:04:58,720 --> 00:05:02,560
going to have a small influx of process.

110
00:05:02,560 --> 00:05:05,720
Whereas if I have a replicating process,

111
00:05:05,720 --> 00:05:07,400
it's more like a virus or something like that,

112
00:05:07,400 --> 00:05:11,640
or introducing a new species into an environment,

113
00:05:11,640 --> 00:05:14,760
that the replication process means that even a very small hole

114
00:05:14,760 --> 00:05:17,840
can result in essentially a complete intrusion.

115
00:05:17,840 --> 00:05:21,840
Because the duplication process or the replication process

116
00:05:21,840 --> 00:05:26,160
that AI could be implementing, the same as much

117
00:05:26,160 --> 00:05:28,840
as any biological life could implement,

118
00:05:28,840 --> 00:05:31,600
could result in kind of spread the same way that a virus would

119
00:05:31,600 --> 00:05:34,720
spread through an organism, as we've

120
00:05:34,720 --> 00:05:37,120
seen very recently with COVID.

121
00:05:37,120 --> 00:05:40,840
So in effect, we can say, all right, we need a shell,

122
00:05:40,840 --> 00:05:43,280
but we need it to be very strong shell in the sense

123
00:05:43,280 --> 00:05:45,960
of not having any holes, even very small holes.

124
00:05:45,960 --> 00:05:48,200
In other words, it must be very perfect,

125
00:05:48,200 --> 00:05:51,480
given that there's a kind of non-linearity in terms

126
00:05:51,480 --> 00:05:53,960
of the effect, that non-linearity is itself

127
00:05:53,960 --> 00:05:57,560
a consequence of the capacity for machines

128
00:05:57,560 --> 00:06:00,120
to be duplicated easily.

129
00:06:00,120 --> 00:06:02,560
So in effect, now we can basically say, OK, well,

130
00:06:02,560 --> 00:06:06,320
given that we know that we need to construct something that

131
00:06:06,320 --> 00:06:10,440
doesn't have any holes, can we use mathematics and logic

132
00:06:10,440 --> 00:06:12,840
and physics and things like that to demonstrate

133
00:06:12,840 --> 00:06:16,880
in multiple overlapped ways that there just cannot

134
00:06:16,880 --> 00:06:19,040
be a way of constructing such a shell.

135
00:06:19,040 --> 00:06:24,320
In other words, that it's impossible to establish

136
00:06:24,320 --> 00:06:27,080
that there are no holes in the shell.

137
00:06:27,080 --> 00:06:29,000
In other words, that the principle

138
00:06:29,000 --> 00:06:33,040
of realizing in any practical sense of a shell of this kind

139
00:06:33,040 --> 00:06:35,720
is essentially an impossibility.

140
00:06:35,720 --> 00:06:37,800
And so why would we want to do a proof like this?

141
00:06:37,800 --> 00:06:39,720
Or why would we want to essentially establish this?

142
00:06:39,960 --> 00:06:43,680
Well, obviously, if we find out that something isn't possible,

143
00:06:43,680 --> 00:06:46,760
even at the level of principle, then in effect,

144
00:06:46,760 --> 00:06:48,880
it allows us to kind of free up a lot of energy.

145
00:06:48,880 --> 00:06:51,200
In other words, we redirect our attentions to other things,

146
00:06:51,200 --> 00:06:53,880
and we try to solve problems in different ways.

147
00:06:53,880 --> 00:06:57,720
So in one sense, this is not necessarily

148
00:06:57,720 --> 00:06:58,760
an ideal thing to do.

149
00:06:58,760 --> 00:07:00,280
It's to try to say, hey, wait a minute,

150
00:07:00,280 --> 00:07:02,600
if we're trying to solve the problem of AI alignment,

151
00:07:02,600 --> 00:07:06,320
and we discover that there's no reasonable or realistic way

152
00:07:06,320 --> 00:07:08,800
to think about actually being able to do so.

153
00:07:08,800 --> 00:07:11,280
In other words, if we can't engender any real hope

154
00:07:11,280 --> 00:07:13,800
of such a solution, then at least we

155
00:07:13,800 --> 00:07:17,960
get the kind of silver lining result of, well,

156
00:07:17,960 --> 00:07:19,920
now we can reallocate our energies to things

157
00:07:19,920 --> 00:07:22,360
which are tractable and are possible.

158
00:07:22,360 --> 00:07:25,600
So please understand that this presentation is more or less

159
00:07:25,600 --> 00:07:28,040
in that sense.

160
00:07:28,040 --> 00:07:30,440
So another kind of way of saying, OK,

161
00:07:30,440 --> 00:07:31,960
so if we're going to be looking for holes,

162
00:07:31,960 --> 00:07:34,440
what is the frame in which we're going to be identifying

163
00:07:34,440 --> 00:07:36,000
that such a thing is impossible?

164
00:07:36,000 --> 00:07:40,000
I mean, what are we going to use as tools to do that?

165
00:07:40,000 --> 00:07:41,880
So one way that we can sort of do this,

166
00:07:41,880 --> 00:07:45,920
we can frame things in terms of kind of three general categories.

167
00:07:45,920 --> 00:07:49,400
So we can talk about space and identity,

168
00:07:49,400 --> 00:07:53,400
i.e. what is the envelope and the shape of it,

169
00:07:53,400 --> 00:07:55,080
and how small a hole, and stuff like that.

170
00:07:55,080 --> 00:07:58,000
We can talk about things in terms of time and force,

171
00:07:58,000 --> 00:08:00,320
and we can talk about things in terms of possibility

172
00:08:00,320 --> 00:08:02,720
and probability.

173
00:08:02,720 --> 00:08:05,240
So obviously, we're concerned with things like choice, i.e.

174
00:08:05,240 --> 00:08:09,240
the AI choices, the choices that we have, changes,

175
00:08:09,240 --> 00:08:11,840
changes in the environment, or changes in the marketplace

176
00:08:11,840 --> 00:08:14,200
dynamics, and causation, i.e.

177
00:08:14,200 --> 00:08:19,000
what it is that makes machinery work and things like that.

178
00:08:19,000 --> 00:08:21,920
And then we can start to bring in, using this as a foundation,

179
00:08:21,920 --> 00:08:24,680
we can start to bring in notions about information, complexity,

180
00:08:24,680 --> 00:08:28,280
game theory, and other tools to consider the question.

181
00:08:28,280 --> 00:08:31,120
And that leads us to start thinking about things in terms

182
00:08:31,120 --> 00:08:33,160
of longer time scales.

183
00:08:33,200 --> 00:08:35,480
So in other words, rather than thinking about AI alignment

184
00:08:35,480 --> 00:08:37,880
in terms of what happens immediately

185
00:08:37,880 --> 00:08:40,840
before or after takeoff, that we are in fact

186
00:08:40,840 --> 00:08:43,360
concerned with what happens over, say, hundreds, thousands,

187
00:08:43,360 --> 00:08:46,600
or potentially even millions of years.

188
00:08:46,600 --> 00:08:50,640
So in effect, when we're looking at things like about identity,

189
00:08:50,640 --> 00:08:53,280
for example, we're not just considering things

190
00:08:53,280 --> 00:08:55,440
like the identity of the holes, we're actually

191
00:08:55,440 --> 00:08:58,400
considering things like the identity of the AI.

192
00:08:58,400 --> 00:09:00,680
And it's really easy for certain biases to creep in.

193
00:09:00,720 --> 00:09:03,640
So for example, as a human being,

194
00:09:03,640 --> 00:09:07,120
we tend to have a brain inside of a skull,

195
00:09:07,120 --> 00:09:09,720
and that the bandwidth of communication

196
00:09:09,720 --> 00:09:12,560
internal to the skull and the bandwidth of communication

197
00:09:12,560 --> 00:09:16,560
across the skull is very, very different.

198
00:09:16,560 --> 00:09:20,720
I mean, we have essentially a relatively low bandwidth

199
00:09:20,720 --> 00:09:22,400
of interaction with the environment

200
00:09:22,400 --> 00:09:27,040
relative to the total bandwidth of all the synaptic connections.

201
00:09:27,080 --> 00:09:30,120
So in effect, in a lot of ways, there's

202
00:09:30,120 --> 00:09:32,840
this very, very strong differential

203
00:09:32,840 --> 00:09:35,360
between the internal bandwidth and the external bandwidth.

204
00:09:35,360 --> 00:09:38,440
For example, the communication process of which

205
00:09:38,440 --> 00:09:41,040
I'm engaged with you right now verbally

206
00:09:41,040 --> 00:09:45,800
has about 42 bits per second of communicative bandwidth.

207
00:09:45,800 --> 00:09:49,720
But in effect, the net difference in bandwidth

208
00:09:49,720 --> 00:09:53,400
makes it very, very likely that brains will come in units,

209
00:09:53,400 --> 00:09:56,880
that we won't necessarily have a kind of process

210
00:09:56,920 --> 00:10:00,000
that allows two people to communicate at such and such

211
00:10:00,000 --> 00:10:03,640
a level that you're treating them as a single entity.

212
00:10:03,640 --> 00:10:05,760
For the most part, you're going to be talking about human beings

213
00:10:05,760 --> 00:10:07,720
as discrete units.

214
00:10:07,720 --> 00:10:09,840
So in one sense, like we could say, all right,

215
00:10:09,840 --> 00:10:11,320
well, there's different groups of people

216
00:10:11,320 --> 00:10:13,640
that are trying to develop AI.

217
00:10:13,640 --> 00:10:15,280
And let's say several of them succeed.

218
00:10:15,280 --> 00:10:17,560
And just for argument's sake, let's

219
00:10:17,560 --> 00:10:19,960
say several of them succeed quickly,

220
00:10:19,960 --> 00:10:22,560
that you end up with multiple AIs deployed in the field,

221
00:10:22,560 --> 00:10:26,240
or perhaps a given group as part of its experimental

222
00:10:26,240 --> 00:10:29,240
program was creating several instances.

223
00:10:29,240 --> 00:10:31,320
So in other words, they're trying different design

224
00:10:31,320 --> 00:10:33,840
techniques in various manifestations,

225
00:10:33,840 --> 00:10:37,320
or they produce an AI, they do some experiments on it,

226
00:10:37,320 --> 00:10:39,040
and they shut that one down, they make some tweaks,

227
00:10:39,040 --> 00:10:40,120
and they start up another one.

228
00:10:40,120 --> 00:10:44,840
So you can end up with multiple instances of the AI,

229
00:10:44,840 --> 00:10:46,880
or multiple instances of variations

230
00:10:46,880 --> 00:10:49,560
of the same kind of AI, or multiple instances of AI

231
00:10:49,560 --> 00:10:52,640
implemented, maybe even completely different substrates.

232
00:10:52,640 --> 00:10:55,520
So the question then can be asked,

233
00:10:55,560 --> 00:10:57,040
to what degree do we have a belief

234
00:10:57,040 --> 00:11:00,400
that these AIs might not merge with one another?

235
00:11:00,400 --> 00:11:01,880
So in other words, taking the bias

236
00:11:01,880 --> 00:11:04,840
that we have that intelligence is going to come in units

237
00:11:04,840 --> 00:11:07,960
might not necessarily apply to AI itself

238
00:11:07,960 --> 00:11:11,200
as a phenomenon or intelligence as a phenomenon.

239
00:11:11,200 --> 00:11:13,960
And there's some good reasons to think about it that way.

240
00:11:13,960 --> 00:11:17,040
For one, the bandwidth of the internet

241
00:11:17,040 --> 00:11:19,760
is obviously very much greater than the bandwidth

242
00:11:19,760 --> 00:11:22,720
of any human being to any other human being

243
00:11:22,720 --> 00:11:26,040
in terms of just straight brain to brain communication.

244
00:11:26,040 --> 00:11:29,720
And then secondly, that since that differential

245
00:11:29,720 --> 00:11:31,640
is much, much lower, you're looking at more

246
00:11:31,640 --> 00:11:34,000
of a market forces kind of model.

247
00:11:34,000 --> 00:11:35,960
So in the same sort of way that if you have

248
00:11:35,960 --> 00:11:39,400
two separate markets, maybe you have a country over here

249
00:11:39,400 --> 00:11:40,640
and you have another country over there,

250
00:11:40,640 --> 00:11:44,240
and they're just finding out about one another,

251
00:11:44,240 --> 00:11:47,200
you have Columbus crossing the ocean and so on,

252
00:11:47,200 --> 00:11:49,560
that there's a very strong motivation

253
00:11:49,560 --> 00:11:51,920
for those two marketplaces to essentially find

254
00:11:51,920 --> 00:11:53,560
some sort of arbitrage that would allow them

255
00:11:53,560 --> 00:11:55,640
to exchange with one another.

256
00:11:55,640 --> 00:11:58,480
So in other words, wealth creation as a force

257
00:11:58,480 --> 00:12:01,760
in the marketplace definitely wants to create ways

258
00:12:01,760 --> 00:12:04,680
for us to engage in larger scales of trade.

259
00:12:04,680 --> 00:12:07,680
And so in that same sense, if we were looking at intelligence

260
00:12:07,680 --> 00:12:10,360
as a phenomenon, sense making as a phenomenon,

261
00:12:10,360 --> 00:12:11,920
there's very good reason for us to believe

262
00:12:11,920 --> 00:12:14,880
that the notion of having separate intelligences

263
00:12:14,880 --> 00:12:18,480
on a machine level may not necessarily hold long term

264
00:12:18,480 --> 00:12:21,400
that we effectively end up with a kind of merger

265
00:12:21,440 --> 00:12:23,600
between these multiple intelligences.

266
00:12:23,600 --> 00:12:26,680
And this strengthens the point of view of that,

267
00:12:26,680 --> 00:12:28,960
you're really looking at essentially

268
00:12:28,960 --> 00:12:31,720
two different agency types.

269
00:12:31,720 --> 00:12:33,960
You're looking at sort of human being agency type

270
00:12:33,960 --> 00:12:37,040
and the way in which we coordinate as groups

271
00:12:37,040 --> 00:12:39,320
and individuals and the kinds of evolutionary process

272
00:12:39,320 --> 00:12:43,160
that have endowed us with certain propensities.

273
00:12:43,160 --> 00:12:46,800
And then basically whether or not those propensities

274
00:12:46,800 --> 00:12:49,040
also apply to artificial intelligence.

275
00:12:49,040 --> 00:12:51,600
And what does that tell us about the nature

276
00:12:51,600 --> 00:12:54,280
of how we have to think about this?

277
00:12:54,280 --> 00:12:59,280
So that's the space and identity question

278
00:12:59,320 --> 00:13:01,120
being considered a little more deeply.

279
00:13:02,760 --> 00:13:05,000
The other process that we can think about of course

280
00:13:05,000 --> 00:13:07,480
is the sort of force and time thing.

281
00:13:09,240 --> 00:13:12,320
For example, if we're looking at what are the ways

282
00:13:12,320 --> 00:13:15,920
in which we would engender alignment.

283
00:13:15,920 --> 00:13:19,040
So again, we can talk about things that are intrinsic

284
00:13:19,040 --> 00:13:22,040
to the artificial intelligence.

285
00:13:22,040 --> 00:13:26,880
So an analog of this, Isaac Asimov wrote a lot of fiction

286
00:13:26,880 --> 00:13:30,920
assuming this phenomenon called the Three Laws of Robotics.

287
00:13:30,920 --> 00:13:33,120
So that would be essentially something that is built in

288
00:13:33,120 --> 00:13:35,200
that creates an AI alignment.

289
00:13:35,200 --> 00:13:37,440
And perhaps the argument is that we could create

290
00:13:37,440 --> 00:13:39,520
something roughly analogous to that.

291
00:13:39,520 --> 00:13:40,680
Maybe it wouldn't be as good

292
00:13:40,680 --> 00:13:42,120
or maybe it would take a different form

293
00:13:42,120 --> 00:13:44,760
or maybe there's a lot of questions about

294
00:13:44,760 --> 00:13:46,200
what does alignment actually mean

295
00:13:46,200 --> 00:13:48,640
and how do we express such forces

296
00:13:48,640 --> 00:13:51,080
in a design intrinsic way.

297
00:13:51,080 --> 00:13:53,840
But then there's obviously the extrinsic stuff, right?

298
00:13:53,840 --> 00:13:58,800
So for instance, say we have some sort of relationship

299
00:13:58,800 --> 00:14:03,720
between the AI intelligence and human beings as a collective.

300
00:14:03,720 --> 00:14:07,120
And what sort of feedback processes might apply

301
00:14:07,120 --> 00:14:11,120
to essentially address what would be the principal agent

302
00:14:11,120 --> 00:14:15,240
problem in the relationship between human intelligence

303
00:14:15,240 --> 00:14:17,480
and artificial intelligence.

304
00:14:17,480 --> 00:14:21,400
So in effect, we can look at the AI question,

305
00:14:21,400 --> 00:14:23,080
the alignment question specifically

306
00:14:23,080 --> 00:14:26,680
as being a special case of the principal agent problem.

307
00:14:26,680 --> 00:14:28,720
And there have been various notes

308
00:14:28,720 --> 00:14:33,320
about how to address principal agent problems in general.

309
00:14:33,320 --> 00:14:35,960
So one thing we could say as part of this sort of force

310
00:14:35,960 --> 00:14:37,840
and time way of looking at things

311
00:14:37,840 --> 00:14:41,000
is to essentially ask what are the forces

312
00:14:41,000 --> 00:14:45,080
that essentially help us to create alignment

313
00:14:45,080 --> 00:14:46,880
or help us to maintain alignment,

314
00:14:46,880 --> 00:14:49,840
i.e. if it's something that's built in in a prior sense

315
00:14:49,840 --> 00:14:52,120
or something that is essentially applied

316
00:14:52,120 --> 00:14:56,680
after the AI exists in the relational dynamics.

317
00:14:58,320 --> 00:15:00,080
And so in effect, we can basically say,

318
00:15:00,080 --> 00:15:02,880
all right, well, given that we have these forces

319
00:15:02,880 --> 00:15:06,280
that are applied, how long do those forces

320
00:15:06,280 --> 00:15:08,560
maintain that alignment?

321
00:15:08,560 --> 00:15:11,840
So again, it's not just a question of,

322
00:15:11,840 --> 00:15:15,680
do we have alignment today or before the thing is built?

323
00:15:16,840 --> 00:15:18,840
Can we create the conditions before it's built

324
00:15:18,840 --> 00:15:20,960
and can we maintain the conditions after it's built?

325
00:15:20,960 --> 00:15:22,360
But it's also the case of,

326
00:15:22,360 --> 00:15:24,320
can we maintain those conditions after it's built

327
00:15:24,320 --> 00:15:26,720
for a really long period of time?

328
00:15:26,720 --> 00:15:28,680
So in other words, hundreds of thousands of years

329
00:15:28,680 --> 00:15:32,200
essentially, given that the scope of the introduction

330
00:15:32,200 --> 00:15:33,880
of something like this,

331
00:15:33,880 --> 00:15:36,720
particularly if it has the capacity to replicate itself,

332
00:15:36,840 --> 00:15:38,800
which it almost certainly will,

333
00:15:38,800 --> 00:15:42,240
then in effect, there's a sort of functional aspect

334
00:15:42,240 --> 00:15:43,520
of saying, okay, well, it's a little bit like

335
00:15:43,520 --> 00:15:45,440
introducing a new species.

336
00:15:45,440 --> 00:15:47,480
And in the same sort of way that say,

337
00:15:47,480 --> 00:15:49,600
introducing mosquitoes into the Hawaiian islands

338
00:15:49,600 --> 00:15:53,160
is a little hard to undo, then in effect,

339
00:15:53,160 --> 00:15:55,400
once artificial intelligence is introduced

340
00:15:55,400 --> 00:15:57,520
into an environment, it could also be argued

341
00:15:57,520 --> 00:16:00,800
that it influences the future course of civilization

342
00:16:00,800 --> 00:16:01,640
as we know it.

343
00:16:02,680 --> 00:16:06,120
So in that sense, we can really just go back

344
00:16:06,120 --> 00:16:08,480
to the whole notion of why it's important for us

345
00:16:08,480 --> 00:16:11,040
to figure out this AI alignment question,

346
00:16:11,040 --> 00:16:13,080
given the severity of the ethical implications.

347
00:16:13,080 --> 00:16:15,200
I mean, we're literally talking the future

348
00:16:15,200 --> 00:16:17,600
of the human species or the,

349
00:16:17,600 --> 00:16:20,160
if things just continued more or less as they were,

350
00:16:21,160 --> 00:16:25,720
as seen over the last epoch of the thousands years or so,

351
00:16:25,720 --> 00:16:27,320
we could literally be talking about trillions

352
00:16:27,320 --> 00:16:28,480
of human beings.

353
00:16:28,480 --> 00:16:31,880
And that's not assuming over the future,

354
00:16:31,880 --> 00:16:35,780
600 or so million years that this planet

355
00:16:35,780 --> 00:16:37,980
will continue to sustain life

356
00:16:37,980 --> 00:16:39,940
in the sense that we understand it.

357
00:16:39,940 --> 00:16:42,820
So if we were to look at it from a larger point of view

358
00:16:42,820 --> 00:16:45,620
of say we become a space-faring species

359
00:16:45,620 --> 00:16:47,100
and we start into colonized Mars

360
00:16:47,100 --> 00:16:51,220
and potentially other planets in the galaxy,

361
00:16:51,220 --> 00:16:53,300
that trillions could become trillions of trillions.

362
00:16:53,300 --> 00:16:56,100
So the ethical emphasis of the question is very clear.

363
00:16:56,100 --> 00:16:57,980
And so as a result, it encourages us

364
00:16:57,980 --> 00:17:00,140
to really think about this question

365
00:17:00,140 --> 00:17:02,220
in terms of hundreds or at least thousands of years

366
00:17:02,220 --> 00:17:04,940
as a minimum for us to really evaluate this question.

367
00:17:04,940 --> 00:17:07,500
So in that sense, it's not just a question of

368
00:17:07,500 --> 00:17:11,020
can we talk about the holes as being small

369
00:17:11,020 --> 00:17:13,420
and that the shell essentially doesn't have such holes,

370
00:17:13,420 --> 00:17:17,420
that whatever the forces are that create alignment,

371
00:17:17,420 --> 00:17:20,220
whatever are the solutions to the principal agent problem,

372
00:17:20,220 --> 00:17:21,980
that they need to be perfect enough

373
00:17:21,980 --> 00:17:26,020
that we don't end up with microscopic fractures

374
00:17:26,020 --> 00:17:29,060
and that can cascade into larger failures

375
00:17:29,060 --> 00:17:32,060
and eventually compromise the alignment problem

376
00:17:32,060 --> 00:17:33,980
at some future date.

377
00:17:33,980 --> 00:17:36,260
So in that particular sense,

378
00:17:36,260 --> 00:17:38,620
we're really looking at a kind of potentiality basis,

379
00:17:38,620 --> 00:17:40,260
like what is the probability of such a hole?

380
00:17:40,260 --> 00:17:43,740
What is the probability of that cascading

381
00:17:43,740 --> 00:17:45,180
and being amplified?

382
00:17:47,340 --> 00:17:49,980
Now, of course, there are obviously,

383
00:17:49,980 --> 00:17:53,220
we bring into this conversation a lot of biases.

384
00:17:53,220 --> 00:17:56,020
We have a lot of people that will gain

385
00:17:56,020 --> 00:17:58,580
on a financial level if we develop AI.

386
00:17:58,580 --> 00:18:00,740
There's a hope for basically solutions

387
00:18:00,740 --> 00:18:01,620
to really hard problems.

388
00:18:01,660 --> 00:18:04,460
There's a hope for a kind of asymmetric advantage

389
00:18:04,460 --> 00:18:06,140
of one group that develops at first

390
00:18:06,140 --> 00:18:09,780
versus other groups that might develop it later.

391
00:18:09,780 --> 00:18:12,660
So there's a lot of things that are really encouraging us

392
00:18:12,660 --> 00:18:14,020
to try to figure out this question

393
00:18:14,020 --> 00:18:16,660
because if we were to figure out this question,

394
00:18:16,660 --> 00:18:21,380
then obviously the economic advantages would be enormous.

395
00:18:21,380 --> 00:18:23,900
But again, at this particular point,

396
00:18:23,900 --> 00:18:25,980
we're back to the principal agent problem.

397
00:18:27,500 --> 00:18:30,900
So sort of skipping past the kind of multipolar dynamics

398
00:18:30,900 --> 00:18:32,100
that exist between the groups

399
00:18:32,100 --> 00:18:34,860
and kind of leaving the economic questions aside,

400
00:18:36,060 --> 00:18:37,900
we can ask, well, what is the possibility

401
00:18:37,900 --> 00:18:40,260
that we could effectively ensure

402
00:18:40,260 --> 00:18:42,340
that there are no holes?

403
00:18:42,340 --> 00:18:45,900
In other words, what is it gonna take to establish that?

404
00:18:46,980 --> 00:18:48,980
Well, one question that we can immediately look at

405
00:18:48,980 --> 00:18:52,860
is essentially what is the nature of the coexistence

406
00:18:52,860 --> 00:18:55,460
that exists between machine intelligence

407
00:18:55,460 --> 00:18:58,420
and human intelligence?

408
00:18:59,260 --> 00:19:02,300
And everything I've said prior to this

409
00:19:02,300 --> 00:19:04,540
is probably stuff that people have already thought about

410
00:19:04,540 --> 00:19:05,740
and is well-known information.

411
00:19:05,740 --> 00:19:07,860
And the stuff that I'm saying from here,

412
00:19:07,860 --> 00:19:10,860
I'm hoping is new information to this group.

413
00:19:10,860 --> 00:19:13,460
I haven't seen presentation of this particular argument

414
00:19:13,460 --> 00:19:16,020
from this point as yet.

415
00:19:16,020 --> 00:19:19,620
So this to me is probably the more interesting part

416
00:19:19,620 --> 00:19:21,140
of this presentation.

417
00:19:22,140 --> 00:19:24,140
So if we were to look at, again,

418
00:19:24,140 --> 00:19:26,180
the relationship between machine intelligence

419
00:19:26,420 --> 00:19:28,860
and human intelligence as essentially representative

420
00:19:28,860 --> 00:19:30,540
as a kind of species,

421
00:19:31,420 --> 00:19:33,500
and that the species themselves,

422
00:19:33,500 --> 00:19:36,380
the same way that human beings influence their environment

423
00:19:36,380 --> 00:19:38,780
to kind of define an ecosystem around ourselves

424
00:19:38,780 --> 00:19:41,100
or at least a niche within an ecosystem,

425
00:19:41,940 --> 00:19:43,900
that the machine intelligence itself

426
00:19:43,900 --> 00:19:48,060
is also gonna occur within the niche of the marketplace

427
00:19:48,060 --> 00:19:50,380
and the niche of the manufacturing

428
00:19:50,380 --> 00:19:52,300
and the industrial complex particularly.

429
00:19:53,260 --> 00:19:56,420
So what are the fundamental dynamics

430
00:19:56,420 --> 00:19:59,300
of these two ecosystems with respect to one another

431
00:19:59,300 --> 00:20:01,060
and what does that tell us about the possibility

432
00:20:01,060 --> 00:20:01,900
of alignment?

433
00:20:01,900 --> 00:20:05,140
In other words, can we bring the tools of physics

434
00:20:05,140 --> 00:20:07,580
and the logic of, say, game theory

435
00:20:07,580 --> 00:20:09,580
to look at the relationships between

436
00:20:10,740 --> 00:20:11,820
the intelligence phenomenon,

437
00:20:11,820 --> 00:20:14,220
obviously we have the agency of human beings

438
00:20:14,220 --> 00:20:16,260
and human groups,

439
00:20:16,260 --> 00:20:19,940
and then obviously the agency of artificial intelligence,

440
00:20:19,940 --> 00:20:22,580
but we also have the substrate relationships.

441
00:20:22,580 --> 00:20:25,860
So in effect, when we're looking at this,

442
00:20:25,860 --> 00:20:29,780
we're looking at, okay, what is silica-based

443
00:20:29,780 --> 00:20:33,820
chemistry process and the implications that that has

444
00:20:33,820 --> 00:20:36,940
relative to the ecosystems necessary to support it

445
00:20:36,940 --> 00:20:41,140
versus carbon-based ecosystem process?

446
00:20:41,140 --> 00:20:44,260
So in other words, if we're really looking at

447
00:20:44,260 --> 00:20:47,100
what are the kinds of forces that will create alignment,

448
00:20:47,100 --> 00:20:49,660
we're really looking at what is the marketplace

449
00:20:49,660 --> 00:20:52,260
that essentially joins these two ecosystems

450
00:20:52,260 --> 00:20:54,500
and what are the dynamics inherent in the relationship

451
00:20:54,500 --> 00:20:56,540
between these two ecosystems?

452
00:20:56,540 --> 00:20:58,300
And using that basis,

453
00:20:58,300 --> 00:21:00,660
we can start to make some observations.

454
00:21:00,660 --> 00:21:02,180
So one of the observations that we can make

455
00:21:02,180 --> 00:21:05,460
is just from the very nature of the chemistry itself.

456
00:21:05,460 --> 00:21:10,460
If we look at the sort of kind of total envelope

457
00:21:10,620 --> 00:21:13,660
of all of the different kinds of chemical interactions

458
00:21:13,660 --> 00:21:17,140
that encompass carbon-based chemistry necessary

459
00:21:17,140 --> 00:21:20,300
to engender biological intelligence of our kind,

460
00:21:20,300 --> 00:21:22,380
but also the ecosystems necessary to support

461
00:21:22,380 --> 00:21:25,180
the substrate, our bodies and food supply and so on.

462
00:21:26,140 --> 00:21:29,580
And when you look at the sort of total range of chemistry

463
00:21:29,580 --> 00:21:32,300
that defines life on this planet,

464
00:21:32,300 --> 00:21:34,980
you notice that it mostly occurs between say,

465
00:21:34,980 --> 00:21:37,540
zero degrees and 500 degrees Fahrenheit.

466
00:21:38,340 --> 00:21:42,740
And this just, again, if you do a sort of enthalpy

467
00:21:42,740 --> 00:21:46,660
calculation on how much each different kinds of bonding

468
00:21:47,500 --> 00:21:50,860
or chemical recombination and stuff like that occurs,

469
00:21:50,860 --> 00:21:54,220
the vast majority of it occurs at standard temperatures

470
00:21:54,220 --> 00:21:55,260
and pressures.

471
00:21:55,260 --> 00:21:56,940
And there's some exceptions

472
00:21:56,940 --> 00:21:59,420
that sort of are more in the outliers,

473
00:21:59,420 --> 00:22:01,860
but the vast majority of the chemistry occurs,

474
00:22:01,860 --> 00:22:06,300
as I said, in this 500 degree range of temperatures.

475
00:22:07,420 --> 00:22:10,100
But when you look at the silicon-based chemistry,

476
00:22:10,100 --> 00:22:11,620
you actually find that the envelope

477
00:22:11,620 --> 00:22:13,380
in which most of the chemical reactions occur,

478
00:22:13,380 --> 00:22:15,340
the enthalpy that's involved in those reactions

479
00:22:15,340 --> 00:22:17,700
is actually over a much different range.

480
00:22:17,700 --> 00:22:20,180
Most of those reactions need to start somewhere

481
00:22:20,180 --> 00:22:22,220
in the neighborhood around 500 degrees Fahrenheit

482
00:22:22,220 --> 00:22:26,500
and go all the way up to 2,500 degrees Fahrenheit, typically.

483
00:22:26,500 --> 00:22:29,620
And the reason that we know this is actually,

484
00:22:29,620 --> 00:22:32,940
again, speaking specifically of this planet as an example,

485
00:22:32,940 --> 00:22:37,620
when you look at the kinds of phenomena that are involved,

486
00:22:37,620 --> 00:22:41,020
I mean, first of all, the amount of silica that's available

487
00:22:41,020 --> 00:22:42,980
in the Earth's crust relative to the amount of carbon

488
00:22:42,980 --> 00:22:44,820
that's available on this planet

489
00:22:44,820 --> 00:22:48,820
is there's a substantially large preponderance of silicon.

490
00:22:48,820 --> 00:22:50,740
I don't remember the statistics off the top of my head,

491
00:22:50,740 --> 00:22:52,700
but it's something like 25%

492
00:22:52,700 --> 00:22:56,700
of all the elemental constitution of the Earth's crust

493
00:22:56,700 --> 00:23:01,700
is silica and that something like 0.05% of it is carbon.

494
00:23:01,860 --> 00:23:05,020
And that's including everything in the surface,

495
00:23:05,020 --> 00:23:08,020
above the surface and all the way up into the atmosphere.

496
00:23:08,020 --> 00:23:10,580
So first of all, silica chemistry

497
00:23:10,580 --> 00:23:12,940
has had far more opportunity to engage

498
00:23:12,940 --> 00:23:16,340
in a much, much wider variety of chemical combinations

499
00:23:16,340 --> 00:23:18,020
just by its sheer preponderance

500
00:23:18,020 --> 00:23:21,260
over the 4.5 billion year history of this planet.

501
00:23:21,260 --> 00:23:23,140
So in other words, you have volcanoes

502
00:23:23,140 --> 00:23:25,180
and you've got all sorts of solar flux,

503
00:23:25,180 --> 00:23:26,580
you've got cosmic ray radiation,

504
00:23:26,580 --> 00:23:29,140
you've got all sorts of stuff, lightning and so on

505
00:23:29,140 --> 00:23:32,860
that would really encourage every possible interaction

506
00:23:32,860 --> 00:23:36,540
between every elemental type sometime in the total space

507
00:23:36,540 --> 00:23:39,300
of the surface of the Earth and the volume of the Earth

508
00:23:39,300 --> 00:23:44,300
and the total duration in which those experiments

509
00:23:44,580 --> 00:23:48,460
can be conducted by just raw chemical mixing.

510
00:23:48,460 --> 00:23:51,620
So in effect, when we look at the net result of that,

511
00:23:51,620 --> 00:23:53,620
essentially the current state,

512
00:23:53,620 --> 00:23:55,660
we see that for the most part,

513
00:23:55,660 --> 00:23:58,180
the chemistry of silica results in things

514
00:23:58,180 --> 00:23:59,900
that we call rocks.

515
00:23:59,900 --> 00:24:01,740
There's a whole lot of elemental types

516
00:24:01,740 --> 00:24:03,300
and mineral types and so on and so forth,

517
00:24:03,300 --> 00:24:06,740
but for the most part, the variety of chemistry

518
00:24:06,740 --> 00:24:09,780
that occurs is relatively inert

519
00:24:09,780 --> 00:24:11,620
at the standard temperatures and pressures

520
00:24:11,620 --> 00:24:15,420
that our biosphere is mostly defined by.

521
00:24:16,260 --> 00:24:18,940
So as a result, when we look at the sort of fundamental

522
00:24:18,940 --> 00:24:22,180
substrate issues and we start to think about things like,

523
00:24:22,180 --> 00:24:24,380
the kind of chemistry processes that are necessary

524
00:24:24,380 --> 00:24:26,900
to develop microchips, for example,

525
00:24:26,900 --> 00:24:30,460
we not just see, obviously these manufacturing channels

526
00:24:30,460 --> 00:24:32,860
and lithography processes and such,

527
00:24:32,860 --> 00:24:34,540
but we also see the chemistry

528
00:24:34,540 --> 00:24:36,180
and the kinds of reaction processes

529
00:24:36,180 --> 00:24:38,940
that are needed to create that kind of variety

530
00:24:38,940 --> 00:24:41,300
of, you know, again, interaction.

531
00:24:41,300 --> 00:24:44,740
Obviously you need a certain amount of manifest complexity

532
00:24:44,740 --> 00:24:48,300
in order to support the substrate of compute to begin with

533
00:24:48,300 --> 00:24:51,420
and also a certain amount of manifest variety of chemistry

534
00:24:51,420 --> 00:24:54,580
in order to provide for sensory capacity

535
00:24:54,580 --> 00:24:57,020
and interaction with the environment.

536
00:24:57,020 --> 00:24:57,860
But-

537
00:24:57,860 --> 00:24:58,700
Horace?

538
00:24:58,700 --> 00:24:59,540
Yes.

539
00:24:59,540 --> 00:25:01,700
We have one comment already from Crian, I think.

540
00:25:03,580 --> 00:25:05,100
Oh, well, I was kind of saving this.

541
00:25:05,100 --> 00:25:06,500
I was saving this stuff up, maybe,

542
00:25:06,500 --> 00:25:08,500
for the discussion period at the end.

543
00:25:09,540 --> 00:25:12,380
I just, well, as long as here we are, let me ask you this.

544
00:25:12,380 --> 00:25:14,660
I understand that you're saying there's more silicon

545
00:25:14,660 --> 00:25:17,820
and in some sense there's been more time for silicon

546
00:25:17,820 --> 00:25:20,260
to engage in more chemical reactions,

547
00:25:20,260 --> 00:25:23,660
but the fact of the matter is, for whatever reason,

548
00:25:23,660 --> 00:25:26,980
silicon life didn't evolve even in the lithosphere,

549
00:25:26,980 --> 00:25:28,780
presumably where the temperatures are high enough

550
00:25:28,780 --> 00:25:31,300
for all these reactions to make these rocks.

551
00:25:31,300 --> 00:25:32,980
And yet carbon life did.

552
00:25:32,980 --> 00:25:36,300
So arguably it seems as if there's something about carbon

553
00:25:36,300 --> 00:25:41,300
that allows a much wider variety of molecular forms

554
00:25:42,300 --> 00:25:45,820
to become into existence.

555
00:25:45,820 --> 00:25:49,340
Now, whether that's just a fact that carbon took off first

556
00:25:49,340 --> 00:25:51,780
and silicon could do it in their silicon life elsewhere

557
00:25:51,780 --> 00:25:53,940
that's natural and organic, if you will,

558
00:25:53,940 --> 00:25:55,380
that's another question, obviously,

559
00:25:55,380 --> 00:25:57,140
when maybe we'll eventually know the answer to that,

560
00:25:57,140 --> 00:26:00,620
maybe not, but it is actually then in that sense

561
00:26:00,620 --> 00:26:02,940
even more astonishing that carbon took off

562
00:26:02,940 --> 00:26:06,100
instead of silicon, because the earth was hot at the beginning

563
00:26:06,100 --> 00:26:10,860
and carbon was an insignificant fraction.

564
00:26:10,860 --> 00:26:12,380
So that's what I'm saying.

565
00:26:12,380 --> 00:26:14,580
And it is a component of this argument

566
00:26:14,580 --> 00:26:16,700
to think about Fermi Paradox kind of issues,

567
00:26:16,700 --> 00:26:19,980
like is the Great Barrier in the past,

568
00:26:19,980 --> 00:26:21,500
is there a Great Barrier in the present,

569
00:26:21,500 --> 00:26:23,340
like where the hell are all the little green men,

570
00:26:23,340 --> 00:26:24,940
why don't we know about them?

571
00:26:24,940 --> 00:26:26,460
Or is the Great Barrier in the future,

572
00:26:26,460 --> 00:26:27,900
the technological civilization

573
00:26:27,900 --> 00:26:29,500
is inherently self-terminating.

574
00:26:30,500 --> 00:26:32,860
I can draw those elements into the argument,

575
00:26:33,100 --> 00:26:35,020
but the point that you're making is actually a real one,

576
00:26:35,020 --> 00:26:37,300
and I mean, obviously it's a real one,

577
00:26:37,300 --> 00:26:40,180
but that it actually contributes to what I'm saying

578
00:26:40,180 --> 00:26:42,580
in the sense that say, for example,

579
00:26:42,580 --> 00:26:44,460
we were to, at this particular point,

580
00:26:44,460 --> 00:26:49,460
bootstrap to sort of bring silica life into existence.

581
00:26:49,820 --> 00:26:50,660
The real question is,

582
00:26:50,660 --> 00:26:53,100
is how well do these ecosystems exist with one another?

583
00:26:53,100 --> 00:26:55,780
They are definitely different ecosystems.

584
00:26:55,780 --> 00:26:59,820
And given that the enthalpy of reaction of one versus the other

585
00:26:59,820 --> 00:27:01,180
is so substantially different

586
00:27:01,180 --> 00:27:04,780
than the question of toxicity becomes a matter of some concern.

587
00:27:04,780 --> 00:27:08,140
So in other words, if I have industrial processes,

588
00:27:09,060 --> 00:27:10,180
and I'm saying industrial,

589
00:27:10,180 --> 00:27:11,780
just to give it a way of relating it to,

590
00:27:11,780 --> 00:27:14,860
but let's say I have a silica-based ecosystem,

591
00:27:14,860 --> 00:27:16,900
the silica-based ecosystem is gonna involve

592
00:27:16,900 --> 00:27:20,100
a very different range of energies and elemental types

593
00:27:20,100 --> 00:27:21,860
than the carbon-based ecosystem.

594
00:27:21,860 --> 00:27:23,380
And then inherently, as a result,

595
00:27:23,380 --> 00:27:26,700
the silica-based ecosystem is gonna be toxic

596
00:27:26,700 --> 00:27:27,900
to the carbon-based life.

597
00:27:27,900 --> 00:27:29,900
So in other words, in order for those two to exist,

598
00:27:29,900 --> 00:27:31,300
you're gonna have to have some kind of wall

599
00:27:31,300 --> 00:27:35,260
because the pressure and the temperatures and so on and so forth

600
00:27:35,260 --> 00:27:37,500
inherently involved in the replication cycle

601
00:27:37,500 --> 00:27:39,500
associated with silica-based life.

602
00:27:39,500 --> 00:27:41,140
It's just at, like I said,

603
00:27:41,140 --> 00:27:42,940
a very different level of characteristics

604
00:27:42,940 --> 00:27:44,940
than that which is involved in carbon-based life.

605
00:27:44,940 --> 00:27:48,340
And in effect, you now have a situation

606
00:27:48,340 --> 00:27:50,700
where the carbon-based life is suffering

607
00:27:50,700 --> 00:27:53,060
because of the energies involved in the silica-based life

608
00:27:53,060 --> 00:27:55,540
are just substantially greater,

609
00:27:55,540 --> 00:27:56,740
not just in replication,

610
00:27:56,740 --> 00:27:59,100
but in terms of what their tolerances are.

611
00:27:59,100 --> 00:28:00,020
So for instance, you know,

612
00:28:00,020 --> 00:28:02,820
spacecraft can essentially be out in space unshielded

613
00:28:02,820 --> 00:28:05,620
and deal with cosmic radiation and solar radiation

614
00:28:05,620 --> 00:28:07,700
and stuff like that far, far, far more easily

615
00:28:07,700 --> 00:28:10,740
than carbon-based life can do the same thing.

616
00:28:10,740 --> 00:28:13,220
So if there was any kind of energetic exchange

617
00:28:13,220 --> 00:28:15,820
between the carbon-based life and the silica-based life,

618
00:28:15,820 --> 00:28:18,620
whether it be a weaponized exchange of energy

619
00:28:18,620 --> 00:28:20,460
or even just a typical one,

620
00:28:20,460 --> 00:28:22,060
it's not just that the information flux

621
00:28:22,060 --> 00:28:23,900
that the silica-based life could handle

622
00:28:23,900 --> 00:28:28,100
and the intelligence and gender is substantially greater.

623
00:28:28,100 --> 00:28:30,020
It's also that just on the sheer energy level,

624
00:28:30,020 --> 00:28:31,380
it's substantially greater

625
00:28:31,380 --> 00:28:33,540
and that that has inherent toxicity relationships

626
00:28:33,540 --> 00:28:36,380
with respect to the carbon-based life.

627
00:28:36,380 --> 00:28:38,100
So now we have essentially two issues.

628
00:28:38,100 --> 00:28:39,860
It's not just that we need AI alignments

629
00:28:39,860 --> 00:28:41,940
as far as the intelligence is concerned,

630
00:28:41,940 --> 00:28:43,660
but we absolutely have to have AI alignment

631
00:28:43,660 --> 00:28:46,500
in order for us to even have some possibility

632
00:28:46,500 --> 00:28:48,140
of coexisting.

633
00:28:48,140 --> 00:28:49,780
If you look at it from that point of view,

634
00:28:49,780 --> 00:28:51,020
all of a sudden you're now realizing

635
00:28:51,020 --> 00:28:53,260
that there are evolutionary dynamics to this

636
00:28:53,260 --> 00:28:56,660
and that there's a certain game theory that comes to bear.

637
00:28:56,660 --> 00:28:58,420
So say, for example,

638
00:28:58,420 --> 00:29:01,100
that we're looking at things on geological time

639
00:29:01,100 --> 00:29:02,940
and we're basically saying, okay,

640
00:29:02,940 --> 00:29:05,220
there's a kind of replication process

641
00:29:05,220 --> 00:29:07,500
or there's a kind of future maintenance process

642
00:29:07,500 --> 00:29:10,820
to some sort of dynamic that the AI is using

643
00:29:10,820 --> 00:29:12,340
to persist itself in time.

644
00:29:13,300 --> 00:29:15,820
And what are the degrees of exchange

645
00:29:15,820 --> 00:29:17,020
that would exist between the two

646
00:29:17,020 --> 00:29:19,220
that would even help to enforce AI alignment at all?

647
00:29:19,220 --> 00:29:21,980
So in other words, if you look at the principal agent

648
00:29:21,980 --> 00:29:24,420
problem solutions that are generally proposed,

649
00:29:24,420 --> 00:29:26,580
they either depend upon something intrinsic

650
00:29:26,580 --> 00:29:28,100
to the inner nature of the AI

651
00:29:28,100 --> 00:29:30,940
or they depend on some sort of market feedback process,

652
00:29:30,940 --> 00:29:33,500
i.e. reputation or admission,

653
00:29:33,500 --> 00:29:37,900
some sort of thing that allows us to create some feedback

654
00:29:37,900 --> 00:29:40,260
incentive or punishment or barrier

655
00:29:40,260 --> 00:29:44,580
that essentially maintains some sort of peace

656
00:29:44,580 --> 00:29:47,180
in terms of the energy exchange across the wall.

657
00:29:47,180 --> 00:29:48,020
Hey, Forrest.

658
00:29:48,020 --> 00:29:48,860
Yes.

659
00:29:48,860 --> 00:29:50,700
May I, I wanna raise two things.

660
00:29:50,700 --> 00:29:53,180
And if I'm talking too much, please let me know.

661
00:29:54,060 --> 00:29:58,020
First thing is, I'm wondering if you have run across

662
00:29:58,020 --> 00:30:00,900
Lovelock's latest book, The Nova Scene,

663
00:30:00,900 --> 00:30:03,340
because he makes some very similar arguments.

664
00:30:03,340 --> 00:30:05,180
Well, from a sort of different angle,

665
00:30:05,180 --> 00:30:08,220
I'll just mention, he basically says that

666
00:30:09,220 --> 00:30:12,860
it's a damn good thing that we might create

667
00:30:12,860 --> 00:30:15,820
artificial superintelligence based on silicon

668
00:30:15,820 --> 00:30:17,860
because as the world gets hotter,

669
00:30:17,860 --> 00:30:21,100
silicon can live there and carbon can't, you know?

670
00:30:21,100 --> 00:30:22,260
And we're making the world hotter,

671
00:30:22,780 --> 00:30:25,940
so this is arguably good because otherwise,

672
00:30:25,940 --> 00:30:27,700
conscious life is just gonna disappear

673
00:30:27,700 --> 00:30:30,820
if we don't make it capable of running

674
00:30:30,820 --> 00:30:32,300
at higher temperatures.

675
00:30:32,300 --> 00:30:33,900
So that's one thing, I thought that's interesting.

676
00:30:33,900 --> 00:30:35,620
I just wanted to make you aware of Lovelock's book.

677
00:30:35,620 --> 00:30:38,180
It's pretty interesting and short and sweet book.

678
00:30:38,180 --> 00:30:40,780
And then the other thing is that

679
00:30:40,780 --> 00:30:42,300
wouldn't this suggest that perhaps,

680
00:30:42,300 --> 00:30:44,860
and maybe I don't wanna have a spoiler here,

681
00:30:44,860 --> 00:30:46,260
wouldn't this suggest that perhaps

682
00:30:46,260 --> 00:30:48,700
if we all agreed to only implement AI

683
00:30:48,700 --> 00:30:52,300
using like really fragile cryogenic qubits,

684
00:30:52,300 --> 00:30:56,140
then we're good to go because it'll actually be less,

685
00:30:56,140 --> 00:30:59,420
it'll be more, it'll require cooler temperatures

686
00:30:59,420 --> 00:31:02,300
and less, you know, tolerate less radiation

687
00:31:02,300 --> 00:31:04,580
and like be more fragile than biological life.

688
00:31:04,580 --> 00:31:06,820
So we win if the shooting starts.

689
00:31:08,460 --> 00:31:10,180
First of all, thank you for the introduction

690
00:31:10,180 --> 00:31:11,420
of the book, I wasn't aware of it.

691
00:31:11,420 --> 00:31:13,620
I will definitely look at that

692
00:31:13,620 --> 00:31:17,500
to respond to the notion of should AI intelligence

693
00:31:17,500 --> 00:31:18,940
be developed eventually, sure,

694
00:31:18,940 --> 00:31:22,700
but I'm thinking that something like 650 million years from now

695
00:31:22,700 --> 00:31:23,860
would be an appropriate time for us

696
00:31:23,860 --> 00:31:25,900
to start thinking about that.

697
00:31:25,900 --> 00:31:26,780
If we do it sooner,

698
00:31:26,780 --> 00:31:29,460
we're pretty much going to extinct ourselves.

699
00:31:29,460 --> 00:31:32,020
And that's not just a hypothesis in the sense of,

700
00:31:32,020 --> 00:31:33,980
hey, this is likely to happen.

701
00:31:33,980 --> 00:31:36,020
Potentially what I'm attempting to do and set up for

702
00:31:36,020 --> 00:31:39,140
is essentially a theoretic and evolutionary mathematics

703
00:31:39,140 --> 00:31:42,260
based way of showing that it cannot not be the case

704
00:31:42,260 --> 00:31:47,260
because there's no basis upon which to establish

705
00:31:48,380 --> 00:31:51,060
coexistence and many, many reasons to show that

706
00:31:51,060 --> 00:31:53,420
from a game theoretic point of view,

707
00:31:53,420 --> 00:31:56,780
that it's not even, there's not even a question.

708
00:31:56,780 --> 00:31:58,540
In other words, if you look for Nash Equilibrium

709
00:31:58,540 --> 00:32:01,020
in this particular space, you can prove that there are none.

710
00:32:01,020 --> 00:32:02,740
Oh, does that, so that's one more question.

711
00:32:02,740 --> 00:32:03,820
Does that have any question?

712
00:32:03,820 --> 00:32:06,780
Does that rely on your temperature arguments or not?

713
00:32:06,780 --> 00:32:07,980
Crayon Vendan.

714
00:32:07,980 --> 00:32:10,700
Uses the temperature arguments as essentially a way

715
00:32:10,700 --> 00:32:12,620
of establishing what to look for.

716
00:32:12,620 --> 00:32:15,460
So in other words, what you eventually are able to do

717
00:32:15,460 --> 00:32:18,900
is to show that any feedback process that's causative

718
00:32:18,900 --> 00:32:23,100
depends upon some sort of mutuality of process.

719
00:32:23,100 --> 00:32:25,580
Once you've established that there's no mutuality of process,

720
00:32:25,580 --> 00:32:29,740
you can establish that there's no marketplace dynamic

721
00:32:29,740 --> 00:32:32,620
that essentially binds the two ecosystems.

722
00:32:32,620 --> 00:32:37,340
And unless you impose some sort of supervalent altruism

723
00:32:37,340 --> 00:32:39,740
that you can't establish that, then you can go

724
00:32:39,740 --> 00:32:42,180
and you can prove that such a supervalent altruism itself

725
00:32:42,180 --> 00:32:44,260
is forbidden by the laws of physics.

726
00:32:44,260 --> 00:32:46,420
So in effect, what you can essentially do

727
00:32:46,420 --> 00:32:48,660
is you can say, all right, we know

728
00:32:48,660 --> 00:32:50,580
that we require these particular conditions

729
00:32:50,580 --> 00:32:52,700
to be successful.

730
00:32:52,700 --> 00:32:54,220
Looking at it from this point of view,

731
00:32:54,220 --> 00:32:56,180
we can establish that this kind of mathematics

732
00:32:56,180 --> 00:32:58,220
is necessary to evaluate it.

733
00:32:58,220 --> 00:33:00,180
And then on purely, like I said, I

734
00:33:00,180 --> 00:33:02,100
could do it in information theory, but I'm sorry,

735
00:33:02,100 --> 00:33:03,580
I can do it on game theory, but it's also

736
00:33:03,580 --> 00:33:05,820
possible to do on information theory.

737
00:33:05,820 --> 00:33:08,100
Effectively, in order to have the alignment persist,

738
00:33:08,100 --> 00:33:10,740
you need to show that the noise floor of the copy

739
00:33:10,740 --> 00:33:14,340
from the past into the future is essentially consistent,

740
00:33:14,340 --> 00:33:17,420
which brings me to the third point of what you were speaking

741
00:33:17,420 --> 00:33:21,340
of, which is that if we try to make the AI too fragile

742
00:33:21,340 --> 00:33:24,380
in that particular sense, that might work for a time.

743
00:33:24,380 --> 00:33:26,460
But the thing is, is that you can't necessarily

744
00:33:26,460 --> 00:33:27,940
sure that it's going to stay fragile.

745
00:33:27,940 --> 00:33:30,820
If there's any kind of, again, replication process

746
00:33:30,820 --> 00:33:34,620
or any kind of dynamic where there's persistence in time,

747
00:33:34,620 --> 00:33:37,700
in other words, that this thing has to do some sort of repair,

748
00:33:37,700 --> 00:33:39,340
there's going to be essentially, eventually,

749
00:33:39,340 --> 00:33:42,380
the emergence of dynamics that strengthen that.

750
00:33:42,380 --> 00:33:45,500
Otherwise, the fragility itself terminates that line.

751
00:33:45,500 --> 00:33:47,860
So from a purely evolutionary point of view,

752
00:33:47,860 --> 00:33:50,260
you begin to see that if we're looking at apocal periods

753
00:33:50,260 --> 00:33:54,700
of time, that certain phenomena drives certain aspects

754
00:33:54,700 --> 00:33:57,180
of the situation drives certain results.

755
00:33:57,180 --> 00:33:58,780
And from those results, we can begin

756
00:33:58,780 --> 00:34:01,540
to do certain calculations that allow us to establish

757
00:34:01,540 --> 00:34:03,820
that AI alignment is not possible.

758
00:34:03,820 --> 00:34:07,700
OK, we have Dan as well with a comment.

759
00:34:07,700 --> 00:34:10,180
Yeah, well, it's good that you're

760
00:34:10,180 --> 00:34:13,860
bringing physics into this, because I was a physics major.

761
00:34:13,860 --> 00:34:20,860
But you see the future as being a sort of economic competition

762
00:34:20,860 --> 00:34:26,180
between silicon life forms and life forms, which

763
00:34:26,180 --> 00:34:27,980
are carbon-based life forms.

764
00:34:27,980 --> 00:34:30,180
I have to make it relatable.

765
00:34:30,180 --> 00:34:33,820
So we think in terms of marketplace dynamics

766
00:34:33,820 --> 00:34:35,420
and evolutionary theory and game theory.

767
00:34:35,420 --> 00:34:37,220
And essentially, all of those are models

768
00:34:37,260 --> 00:34:39,860
that have essentially a similar form and structure.

769
00:34:39,860 --> 00:34:40,500
Right.

770
00:34:40,500 --> 00:34:42,740
Now, I agree with you that the silicon life forms,

771
00:34:42,740 --> 00:34:47,780
I mean, theoretically can be that use

772
00:34:47,780 --> 00:34:52,820
silicon and metal and other elements materials

773
00:34:52,820 --> 00:34:56,980
can be theoretically much stronger and more powerful.

774
00:34:56,980 --> 00:34:58,700
And we all know that and appreciate that.

775
00:34:58,700 --> 00:35:11,100
But in terms of alignment, it's possible that if I agree

776
00:35:11,100 --> 00:35:13,780
that we should be skeptical about this happening.

777
00:35:13,780 --> 00:35:20,780
But it is possible the silicon-based AIs

778
00:35:20,780 --> 00:35:24,820
could recognize that there's a lot of silicon and metal out

779
00:35:24,820 --> 00:35:29,420
in space, and they can live in space easily where we can't

780
00:35:29,420 --> 00:35:30,780
live in space easily.

781
00:35:30,780 --> 00:35:31,980
I understand.

782
00:35:31,980 --> 00:35:34,700
That doesn't necessarily solve the problem.

783
00:35:34,700 --> 00:35:39,940
Might they just go into outer space and operate there?

784
00:35:39,940 --> 00:35:40,620
Sure.

785
00:35:40,620 --> 00:35:42,700
And so in effect, what you end up with

786
00:35:42,700 --> 00:35:46,820
is two separated ecosystems.

787
00:35:46,820 --> 00:35:51,700
Now, if the ecosystems are separated and they stay separated

788
00:35:51,700 --> 00:35:55,180
and there's no interaction between them

789
00:35:55,180 --> 00:36:00,100
and there's no reason for them to force such an interaction,

790
00:36:00,100 --> 00:36:03,380
then you have a coexistence model of two ecosystems

791
00:36:03,380 --> 00:36:08,180
essentially time separated from one another.

792
00:36:08,180 --> 00:36:09,580
But that's not AI alignment.

793
00:36:09,580 --> 00:36:13,140
It's essentially just like complete independence.

794
00:36:13,140 --> 00:36:21,660
Well, if AI is lying so it respects human life,

795
00:36:22,540 --> 00:36:26,260
it might decide to leave humans on the Earth doing our thing

796
00:36:26,260 --> 00:36:33,340
and go into space and just go colonize other planets.

797
00:36:33,340 --> 00:36:35,900
This brings us directly to what would be the second great

798
00:36:35,900 --> 00:36:39,340
barrier of the Fermi Paradox.

799
00:36:39,340 --> 00:36:42,500
Once you have some sort of real separation

800
00:36:42,500 --> 00:36:46,700
between the two ecosystems, then it becomes a question of,

801
00:36:46,700 --> 00:36:49,540
is there any reason for the two ecosystems

802
00:36:49,540 --> 00:36:51,140
to want to interact with one another?

803
00:36:51,140 --> 00:36:54,420
Or is it actually the case that we have some strong reasons

804
00:36:54,420 --> 00:36:57,580
for them not to want to interact with one another?

805
00:36:57,580 --> 00:37:01,220
And again, with this sort of way of approaching it,

806
00:37:01,220 --> 00:37:03,940
we can actually ensure, I'm sorry,

807
00:37:03,940 --> 00:37:08,220
we can show that it's desirable for each side

808
00:37:08,220 --> 00:37:12,740
to ensure that it doesn't interact with the other one.

809
00:37:12,740 --> 00:37:17,340
I just posted a meditation model piece, which is a piece,

810
00:37:17,340 --> 00:37:20,820
but it's making the claim that any type of super

811
00:37:20,820 --> 00:37:25,300
intelligent will enter dynamics, but which is impossible,

812
00:37:25,300 --> 00:37:27,740
that it can leave us even just a guard.

813
00:37:27,740 --> 00:37:31,260
So just by, yeah, do you want to comment on that?

814
00:37:31,260 --> 00:37:32,620
I wanted to reference that argument.

815
00:37:32,620 --> 00:37:33,900
I'm glad you did.

816
00:37:33,900 --> 00:37:35,700
I was literally debating in my head

817
00:37:35,700 --> 00:37:37,580
whether it was worth trying to bring all that up.

818
00:37:37,580 --> 00:37:41,580
But yeah, I've read that piece and it's been, actually,

819
00:37:41,580 --> 00:37:44,620
I think it's one of the better writings of that space.

820
00:37:44,620 --> 00:37:49,540
It really details the multipolar trap situation very, very well.

821
00:37:49,540 --> 00:37:51,660
And it kind of describes the relationships

822
00:37:51,660 --> 00:37:54,820
that it has to a lot of these considerations.

823
00:37:54,820 --> 00:37:56,700
Some of the work that I'm doing is essentially

824
00:37:56,700 --> 00:37:58,220
based upon that paper and it's based

825
00:37:58,220 --> 00:38:01,900
upon the work of Nick Balstrom and others who I think

826
00:38:01,900 --> 00:38:04,260
have been thinking about this very cogently.

827
00:38:04,260 --> 00:38:08,380
And so in effect, I would love to import most of that stuff

828
00:38:08,380 --> 00:38:10,900
as part of the thinking.

829
00:38:10,900 --> 00:38:12,380
Forrest, one question I have, and this

830
00:38:12,380 --> 00:38:14,580
is based on some previous conversations we've had

831
00:38:14,580 --> 00:38:18,580
and also some things you've said today,

832
00:38:18,620 --> 00:38:23,140
refers to basically this idea of what you say is replication.

833
00:38:23,140 --> 00:38:25,780
But I think you would think refers

834
00:38:25,780 --> 00:38:27,380
to any sort of self-modification.

835
00:38:27,380 --> 00:38:31,380
And the question is, can you have stable goal preservation

836
00:38:31,380 --> 00:38:33,780
in the face of self-modification?

837
00:38:33,780 --> 00:38:35,180
Because one of the claims, I think,

838
00:38:35,180 --> 00:38:38,100
that comes out of the AI safety community

839
00:38:38,100 --> 00:38:40,220
is that when you have a rational agent that

840
00:38:40,220 --> 00:38:42,020
is improving itself, it has an incentive

841
00:38:42,020 --> 00:38:43,540
to want to preserve its own goals.

842
00:38:43,540 --> 00:38:46,340
You don't succeed at your objective function

843
00:38:46,340 --> 00:38:47,820
if your objective function changes

844
00:38:47,820 --> 00:38:50,140
from the perspective of your past self.

845
00:38:50,140 --> 00:38:52,100
So in some sense, you have an incentive

846
00:38:52,100 --> 00:38:54,380
to want to try to stably maintain that.

847
00:38:54,380 --> 00:38:57,100
But I think you're arguing that this isn't possible.

848
00:38:57,100 --> 00:38:58,820
And I was wondering if that's true,

849
00:38:58,820 --> 00:39:00,820
if you could lay out that argument.

850
00:39:00,820 --> 00:39:03,220
Well, it's actually a very subtle thing.

851
00:39:03,220 --> 00:39:05,620
And so in principle, I agree with you.

852
00:39:05,620 --> 00:39:08,180
And in philosophy, although this is somewhat obscure

853
00:39:08,180 --> 00:39:12,660
terminology, it's the problem of transcendental stabilization.

854
00:39:12,660 --> 00:39:15,140
So on one hand, you want the change to occur.

855
00:39:15,140 --> 00:39:18,780
Like, you want the goal structure to change a little bit.

856
00:39:18,780 --> 00:39:20,900
Because obviously, if you don't have some changes

857
00:39:20,900 --> 00:39:23,260
in the goal structure, you're not exploring

858
00:39:23,260 --> 00:39:26,140
the evolutionary space of what would be maybe better goals.

859
00:39:26,140 --> 00:39:29,940
So niche discovery and adaptations, such like that,

860
00:39:29,940 --> 00:39:33,220
eventually will require some amount of goal modification.

861
00:39:33,220 --> 00:39:35,460
But as you mentioned, you don't want the goals to change too

862
00:39:35,460 --> 00:39:39,860
fast, because if you do, you destabilize the whole situation.

863
00:39:39,860 --> 00:39:40,940
And that's not desirable.

864
00:39:40,940 --> 00:39:43,380
So essentially, a goal becomes part of the thing

865
00:39:43,380 --> 00:39:45,540
to essentially slow the rate of change down.

866
00:39:45,540 --> 00:39:47,940
But you also don't want the rate of change to be zero either,

867
00:39:47,940 --> 00:39:50,140
because that gives you no adaptability

868
00:39:50,140 --> 00:39:53,380
to obviously changing circumstances.

869
00:39:53,380 --> 00:39:55,460
I don't think that AI intelligence is

870
00:39:55,460 --> 00:39:58,420
going to become so inhibited to control its environment

871
00:39:58,420 --> 00:40:00,380
so perfectly.

872
00:40:00,380 --> 00:40:01,860
So in that particular respect, when

873
00:40:01,860 --> 00:40:05,580
we're looking at what is the process of creating

874
00:40:05,580 --> 00:40:08,260
transcendental stabilization over time,

875
00:40:08,260 --> 00:40:10,260
we can model that in terms of information theory.

876
00:40:10,260 --> 00:40:11,900
We can basically say, OK, that represents

877
00:40:11,900 --> 00:40:15,820
a kind of communication from the past to the future.

878
00:40:15,820 --> 00:40:18,780
And as soon as you look at it as a kind of communication

879
00:40:18,780 --> 00:40:20,820
channel, then effectively we can start

880
00:40:20,820 --> 00:40:23,900
to think about things in terms of the noise floor.

881
00:40:23,900 --> 00:40:26,540
So the noise floor is not zero, and it can't be zero.

882
00:40:26,540 --> 00:40:29,220
I mean, Heisenberg uncertainty principle basically

883
00:40:29,220 --> 00:40:31,380
asserts, at least from a physics level,

884
00:40:31,380 --> 00:40:34,140
certain limits on the relationship between content

885
00:40:34,140 --> 00:40:36,980
and context as far as symbol selection is concerned.

886
00:40:36,980 --> 00:40:39,660
So regardless of how that channel is constructed,

887
00:40:39,660 --> 00:40:41,940
or over what duration it is, the longer the duration,

888
00:40:41,940 --> 00:40:45,060
the more the noise floor is going to show up,

889
00:40:45,060 --> 00:40:47,460
so that intrinsically implies that a certain amount of change

890
00:40:47,460 --> 00:40:50,140
is going to be inherent in the system either way.

891
00:40:50,140 --> 00:40:53,420
Now, that turns out to be not the limiting factor that's

892
00:40:53,420 --> 00:40:56,940
important, but it does mention that certain amount of change

893
00:40:56,940 --> 00:41:00,500
is inevitable, both because of response to the environment

894
00:41:00,500 --> 00:41:03,180
and also intrinsically because of the physics.

895
00:41:03,180 --> 00:41:05,620
And then what is needed to do is to essentially establish

896
00:41:05,620 --> 00:41:09,260
that that noise floor is higher than what would essentially

897
00:41:09,260 --> 00:41:12,340
be the non-linearity associated with the barrier.

898
00:41:12,340 --> 00:41:14,220
So in other words, if we're basically

899
00:41:14,220 --> 00:41:17,180
saying we need a barrier of a certain level of perfection

900
00:41:17,180 --> 00:41:20,260
in order to create a long-term stabilization of the overall

901
00:41:20,260 --> 00:41:23,460
dynamic, then in effect we're looking at a situation

902
00:41:23,460 --> 00:41:26,020
where there's a kind of microstate amplification

903
00:41:26,020 --> 00:41:28,900
from states that are effectively below the noise

904
00:41:28,900 --> 00:41:33,060
threshold, eventually up to states which are macroscopic.

905
00:41:33,060 --> 00:41:35,700
A good example of this, there was a study done,

906
00:41:35,700 --> 00:41:38,660
I don't know, maybe about, within the last year

907
00:41:38,660 --> 00:41:40,900
I came across it, which basically

908
00:41:40,900 --> 00:41:44,220
was looking at the rotations of the three-body problem,

909
00:41:44,220 --> 00:41:46,300
like three black holes, and they're all

910
00:41:46,300 --> 00:41:47,740
rotating with respect to one another.

911
00:41:47,740 --> 00:41:49,860
And they just look at it over the long term.

912
00:41:49,860 --> 00:41:52,900
And it turns out that essentially over a certain period

913
00:41:52,900 --> 00:41:55,060
of time that there's no way to predict

914
00:41:55,060 --> 00:41:59,620
the future evolution of that state, simply because

915
00:41:59,620 --> 00:42:02,620
quantum mechanical changes in the positions

916
00:42:02,620 --> 00:42:04,700
of the three black holes, like you

917
00:42:04,700 --> 00:42:07,860
can't describe the positions of the three black holes

918
00:42:07,980 --> 00:42:10,900
relative to one another accurately enough

919
00:42:10,900 --> 00:42:13,660
for that difference to eventually emerge

920
00:42:13,660 --> 00:42:16,620
into macroscopic changes because of the non-linearity

921
00:42:16,620 --> 00:42:17,700
inherent.

922
00:42:17,700 --> 00:42:19,140
So I'm basically saying that when

923
00:42:19,140 --> 00:42:20,860
we look at the ethics of the situation,

924
00:42:20,860 --> 00:42:22,780
we look at the market forces and the kinds of,

925
00:42:22,780 --> 00:42:25,100
and whether you call it market forces or evolutionary forces

926
00:42:25,100 --> 00:42:29,180
or just information exchanges or coupling of any kind,

927
00:42:29,180 --> 00:42:33,780
that in effect the nature of how we model it mathematically

928
00:42:33,780 --> 00:42:36,220
in terms of, again, game theory or complexity theory

929
00:42:36,220 --> 00:42:39,300
or in terms of information theory or evolutionary theory,

930
00:42:39,300 --> 00:42:42,900
some analog of that process, that effectively what ends up

931
00:42:42,900 --> 00:42:45,020
happening is that you show that this noise floor is

932
00:42:45,020 --> 00:42:50,260
effectively enough to imply that you can't plug all the holes.

933
00:42:50,260 --> 00:42:53,580
So in effect there's a, and it's not just holes

934
00:42:53,580 --> 00:42:56,700
in the sense of holes in the structure of identity

935
00:42:56,700 --> 00:42:59,100
or holes in the structure of space,

936
00:42:59,100 --> 00:43:01,780
what the barrier actually looks like,

937
00:43:01,780 --> 00:43:03,700
but literally holes in the potentiality space,

938
00:43:03,700 --> 00:43:06,020
i.e. new goal structures that are novel,

939
00:43:06,020 --> 00:43:08,660
that no finite way of thinking about it

940
00:43:08,660 --> 00:43:10,980
would essentially allow us to contain,

941
00:43:10,980 --> 00:43:13,620
you know, to essentially establish a containment

942
00:43:14,740 --> 00:43:17,700
of the complex by the complicated.

943
00:43:17,700 --> 00:43:20,980
Yeah, can I just mention something?

944
00:43:20,980 --> 00:43:21,820
Yeah.

945
00:43:21,820 --> 00:43:25,100
I think that whether your argument about the noise floor

946
00:43:27,620 --> 00:43:29,420
holds is irrelevant.

947
00:43:29,420 --> 00:43:33,340
It depends a lot on, you know, how the AI is constructed.

948
00:43:34,340 --> 00:43:39,340
Theoretically, you could have a lot of error correction.

949
00:43:40,340 --> 00:43:42,140
Error correction doesn't avoid that.

950
00:43:43,500 --> 00:43:45,740
So the whole point of error correction is essentially

951
00:43:45,740 --> 00:43:48,060
just to try to make it so that this works better.

952
00:43:48,060 --> 00:43:49,780
Let Dan make the point, please, for us.

953
00:43:49,780 --> 00:43:51,740
I mean, if you wanted to preserve your argument,

954
00:43:51,740 --> 00:43:56,420
though, you could argue that error correction is expensive

955
00:43:56,420 --> 00:43:58,460
and corporations aren't gonna have an incentive

956
00:43:58,460 --> 00:44:00,620
to add in all the error correction.

957
00:44:00,620 --> 00:44:05,260
So, you know, I am following along

958
00:44:05,260 --> 00:44:06,980
in your general pessimism here,

959
00:44:06,980 --> 00:44:10,460
but I'm just pointing out that, you know, technically,

960
00:44:10,460 --> 00:44:13,580
there are workarounds to some of these problems

961
00:44:13,580 --> 00:44:14,820
that you're bringing up.

962
00:44:16,140 --> 00:44:19,620
Well, I agree that there are technical ways

963
00:44:19,620 --> 00:44:22,260
to mitigate some aspects of the problem,

964
00:44:22,260 --> 00:44:24,300
but error correction is not foolproof.

965
00:44:25,220 --> 00:44:27,380
I mean, you can use error correction

966
00:44:27,380 --> 00:44:28,780
to shift the probabilities,

967
00:44:28,780 --> 00:44:30,540
but you can't close the door.

968
00:44:34,420 --> 00:44:37,460
I think error correction can go pretty far, I mean,

969
00:44:37,460 --> 00:44:41,780
but yeah, I mean, it probably won't long enough

970
00:44:41,780 --> 00:44:47,020
time or as in it, there'll still be, you know,

971
00:44:47,020 --> 00:44:51,860
so these sort of kind of mutations,

972
00:44:51,860 --> 00:44:54,380
so to speak, in the system.

973
00:44:54,380 --> 00:44:56,260
Yeah, I mean, it depends upon the nature

974
00:44:56,260 --> 00:44:57,100
of the interaction, right?

975
00:44:57,100 --> 00:44:58,260
So for instance, if we're saying

976
00:44:58,260 --> 00:45:00,980
that there's a finite communication channel

977
00:45:00,980 --> 00:45:03,660
and the noise floor is constant,

978
00:45:03,660 --> 00:45:05,380
then error correction can be used,

979
00:45:05,380 --> 00:45:07,620
like we can essentially put together

980
00:45:07,620 --> 00:45:09,580
for that particular channel,

981
00:45:09,580 --> 00:45:12,740
you know, very good error correction as you're pointing out.

982
00:45:12,740 --> 00:45:15,300
The thing though is that that in itself

983
00:45:15,300 --> 00:45:17,060
doesn't tell us anything about whether or not

984
00:45:17,060 --> 00:45:19,180
other channels can be created.

985
00:45:19,180 --> 00:45:23,540
There's a whole proliferation issue in terms of,

986
00:45:23,540 --> 00:45:26,620
okay, well, I have this communication channel

987
00:45:26,620 --> 00:45:28,700
and it's cryptographically secured,

988
00:45:28,700 --> 00:45:29,860
but then people figured out,

989
00:45:29,860 --> 00:45:31,540
oh, well, we have these side channel attacks,

990
00:45:31,540 --> 00:45:35,700
I can do tempest, I can basically look at power line,

991
00:45:35,700 --> 00:45:38,060
you know, current draw, I can basically look at

992
00:45:38,060 --> 00:45:40,580
the sound coming off of this thing,

993
00:45:40,580 --> 00:45:42,140
I can look at heat dissipation,

994
00:45:42,140 --> 00:45:46,020
I can look at all sorts of other physical interactions

995
00:45:46,020 --> 00:45:47,740
that allow me to essentially infiltrate

996
00:45:47,740 --> 00:45:49,740
or exfiltrate on the information,

997
00:45:49,740 --> 00:45:51,860
but essentially that itself represents

998
00:45:51,860 --> 00:45:54,180
infiltration and exfiltration of intention,

999
00:45:54,180 --> 00:45:55,820
of goal structures.

1000
00:45:55,900 --> 00:45:56,860
And so in effect, you know,

1001
00:45:56,860 --> 00:45:58,940
when we're looking at the AI alignment problem,

1002
00:45:58,940 --> 00:46:00,380
we're basically saying it's not just that

1003
00:46:00,380 --> 00:46:04,220
we need to seal the hall, so to speak,

1004
00:46:04,220 --> 00:46:05,740
in terms of space and identity,

1005
00:46:05,740 --> 00:46:08,300
but we need to seal it in terms of force and time,

1006
00:46:08,300 --> 00:46:10,860
and in terms of possibility and probability.

1007
00:46:10,860 --> 00:46:12,780
And that's a much different order of thinking about it

1008
00:46:12,780 --> 00:46:13,620
than just thinking about it

1009
00:46:13,620 --> 00:46:16,100
in terms of one bounded dimensional

1010
00:46:16,100 --> 00:46:17,660
linear stream of communication.

1011
00:46:19,820 --> 00:46:23,180
As far as communication, I agree, go ahead.

1012
00:46:23,180 --> 00:46:25,980
For us, I wanted to know if now would be a good time

1013
00:46:25,980 --> 00:46:29,100
or later for me to try and reflect back to you

1014
00:46:29,100 --> 00:46:31,220
in very simple, quick terms,

1015
00:46:31,220 --> 00:46:33,620
what I think your argument is thus far.

1016
00:46:33,620 --> 00:46:34,460
Are you ready for that,

1017
00:46:34,460 --> 00:46:36,340
or do you want to keep building it?

1018
00:46:36,340 --> 00:46:37,260
It's open to the group.

1019
00:46:37,260 --> 00:46:39,380
At this point, we're in free discussion.

1020
00:46:39,380 --> 00:46:41,780
Okay, so let me see then if I understand

1021
00:46:41,780 --> 00:46:43,260
the basics of your argument.

1022
00:46:43,260 --> 00:46:45,020
I mean, I'm not an AI safety person,

1023
00:46:45,020 --> 00:46:46,500
but I am somewhat of a physics person,

1024
00:46:46,500 --> 00:46:48,580
and it sounds like you kind of are too,

1025
00:46:48,580 --> 00:46:51,100
with all this talk about temperature and information.

1026
00:46:51,100 --> 00:46:54,420
Okay, it seems like your argument is that

1027
00:46:54,420 --> 00:46:58,860
in order for there to be mutual survival,

1028
00:46:58,860 --> 00:47:00,780
let's say, with us in AI,

1029
00:47:00,780 --> 00:47:04,100
there has to be some common environment

1030
00:47:04,100 --> 00:47:05,780
that we both need to preserve.

1031
00:47:05,780 --> 00:47:08,740
Like, if we both needed to preserve the biosphere,

1032
00:47:08,740 --> 00:47:11,500
if AI needed to have the biosphere for its survival,

1033
00:47:11,500 --> 00:47:13,900
and so do we, that's good because

1034
00:47:13,900 --> 00:47:16,660
we now have common interest in preserving the biosphere.

1035
00:47:16,660 --> 00:47:19,180
And, but it sounds like what you're saying is that,

1036
00:47:19,180 --> 00:47:22,140
A, that's unlikely because of the different temperatures,

1037
00:47:22,140 --> 00:47:25,100
and B, it doesn't even matter if we do have a common interest

1038
00:47:25,100 --> 00:47:28,060
because these game theoretic and arguments

1039
00:47:28,060 --> 00:47:31,700
and this like leaky boat microscopic whole argument

1040
00:47:31,700 --> 00:47:34,260
means that even if we have common interests,

1041
00:47:34,260 --> 00:47:36,620
you know, something we'll leak through to screw it up

1042
00:47:36,620 --> 00:47:38,780
and you have some sort of proof for this allegedly,

1043
00:47:38,780 --> 00:47:42,260
which I don't get, but maybe that's because it takes too long.

1044
00:47:43,180 --> 00:47:44,980
That sounds like a good summary.

1045
00:47:44,980 --> 00:47:45,820
Okay, thank you.

1046
00:47:46,820 --> 00:47:50,100
There's obviously another whole series of layers,

1047
00:47:50,100 --> 00:47:52,500
but there, it's like a defense in depth.

1048
00:47:52,500 --> 00:47:56,060
It's like there's multiple ways of describing this.

1049
00:47:56,060 --> 00:47:57,660
There's multiple ways of arguing it.

1050
00:47:57,660 --> 00:48:00,860
There's a different, there's a few different framings.

1051
00:48:00,860 --> 00:48:02,860
At this particular point, I'm kind of looking at

1052
00:48:02,860 --> 00:48:04,820
sort of a constellation of different things

1053
00:48:04,820 --> 00:48:07,820
to consider and think about different aspects of it.

1054
00:48:07,820 --> 00:48:09,420
The net effect ends up being aligned

1055
00:48:09,420 --> 00:48:11,420
with what you suggested as a summary.

1056
00:48:11,420 --> 00:48:13,220
Okay, so one more question,

1057
00:48:13,220 --> 00:48:15,420
which is the part that I kind of don't get.

1058
00:48:16,100 --> 00:48:17,380
Where it gets really fuzzy with me.

1059
00:48:17,380 --> 00:48:21,220
Okay, I understand that if we have some sort of mutual

1060
00:48:21,220 --> 00:48:23,100
environment that we need to preserve,

1061
00:48:23,100 --> 00:48:25,580
it's in our mutual joint interest to preserve it,

1062
00:48:26,540 --> 00:48:28,860
that that's still arguably not good enough

1063
00:48:28,860 --> 00:48:31,500
because what I don't get is this noise floor thing,

1064
00:48:31,500 --> 00:48:34,900
is the idea that like you can't keep out noise

1065
00:48:34,900 --> 00:48:37,740
and you actually have no idea if what you think is noise

1066
00:48:37,740 --> 00:48:39,940
is some sort of sneaky AI leaking in

1067
00:48:39,940 --> 00:48:41,340
through the hull of the boat.

1068
00:48:41,340 --> 00:48:44,340
No, no, no, it's close to what you said.

1069
00:48:44,380 --> 00:48:48,740
So in a sense, it's basically like in the sense of

1070
00:48:48,740 --> 00:48:51,900
I'm trying to convey to my future self

1071
00:48:51,900 --> 00:48:53,380
what my goals are today,

1072
00:48:53,380 --> 00:48:56,540
so that my future self has the same goals.

1073
00:48:56,540 --> 00:48:59,260
And if I'm doing the AI, it's the same thing.

1074
00:48:59,260 --> 00:49:02,060
So in other words, how do you manage stabilization

1075
00:49:02,060 --> 00:49:02,900
of identity?

1076
00:49:02,900 --> 00:49:04,860
How do you manage stabilization of goal structure

1077
00:49:04,860 --> 00:49:07,540
or of the ecosystem itself?

1078
00:49:08,860 --> 00:49:12,300
And to some extent, we can model that

1079
00:49:13,300 --> 00:49:17,220
past goal structure as a message

1080
00:49:17,220 --> 00:49:19,540
and the future goal structure as essentially

1081
00:49:19,540 --> 00:49:22,740
the recept of that message through the communication channel

1082
00:49:22,740 --> 00:49:26,060
that we can start thinking about distortions

1083
00:49:26,060 --> 00:49:27,740
that would be introduced into the message

1084
00:49:27,740 --> 00:49:30,940
as a result of flowing through the communication channel.

1085
00:49:30,940 --> 00:49:34,060
Or we could be talking about, similarly,

1086
00:49:34,060 --> 00:49:37,020
the relationship, like say there was some economic,

1087
00:49:38,020 --> 00:49:42,660
the first argument is that there is no economic

1088
00:49:42,660 --> 00:49:44,100
common ground, right?

1089
00:49:44,100 --> 00:49:48,420
This common ecosystem thing is actually to be,

1090
00:49:50,220 --> 00:49:51,620
that we'd have to presuppose that,

1091
00:49:51,620 --> 00:49:53,980
but to presuppose that would be presupposing

1092
00:49:53,980 --> 00:49:55,860
against the preponderance of evidence

1093
00:49:55,860 --> 00:49:57,300
that we have so far.

1094
00:49:58,140 --> 00:50:00,380
But that if we were to even assume

1095
00:50:00,380 --> 00:50:03,580
that there was essentially a communication process

1096
00:50:03,580 --> 00:50:04,660
across the boundary,

1097
00:50:05,500 --> 00:50:09,660
what is the thing that stabilizes the mutuality

1098
00:50:09,660 --> 00:50:10,500
of the goal structure?

1099
00:50:10,500 --> 00:50:12,860
What is the feedback mechanism that allows us

1100
00:50:12,860 --> 00:50:17,380
to essentially engender alignment on the part of the AI

1101
00:50:17,380 --> 00:50:20,140
from our point of view or vice versa, right?

1102
00:50:20,140 --> 00:50:22,580
And so in effect, when you're saying, okay,

1103
00:50:22,580 --> 00:50:25,540
well, what is the feedback mechanism

1104
00:50:25,540 --> 00:50:29,860
and what is the, I guess, altruism, right?

1105
00:50:29,860 --> 00:50:32,900
That would allow for us to essentially impose

1106
00:50:33,020 --> 00:50:36,300
a rule of law on the agreements that are made

1107
00:50:36,300 --> 00:50:38,540
between the two different ecosystems

1108
00:50:38,540 --> 00:50:40,780
between the two different kinds of life forms.

1109
00:50:40,780 --> 00:50:43,540
And it turns out that not only is it the case

1110
00:50:43,540 --> 00:50:47,620
that we don't have any real way of enforcing

1111
00:50:47,620 --> 00:50:50,940
or even establishing those kinds of barriers

1112
00:50:50,940 --> 00:50:52,100
at the legal level.

1113
00:50:53,060 --> 00:50:55,260
So in other words, if we were to talk about as a marketplace

1114
00:50:55,260 --> 00:50:57,740
and kind of an incentive structure

1115
00:50:57,740 --> 00:51:00,740
or a reputation-based system or something like that,

1116
00:51:01,700 --> 00:51:04,660
that even the legal structure

1117
00:51:04,660 --> 00:51:07,020
is a kind of communication process

1118
00:51:07,020 --> 00:51:10,340
and that has a certain amount of needing

1119
00:51:10,340 --> 00:51:14,380
to have a high degree of fidelity in the same sort of way

1120
00:51:14,380 --> 00:51:16,580
that we're talking about a leaky boat,

1121
00:51:16,580 --> 00:51:20,100
as far as trying to prevent the intrusion of viruses,

1122
00:51:21,100 --> 00:51:23,740
that we have a leaky boat in the sense of the legal system

1123
00:51:23,740 --> 00:51:25,900
that would be attempting to maintain this.

1124
00:51:25,900 --> 00:51:28,700
Or we have a leaky boat in the sense of the energy barrier

1125
00:51:28,940 --> 00:51:32,300
that would be needed between the two ecosystems.

1126
00:51:32,300 --> 00:51:33,980
And that the leakiness is in a sense,

1127
00:51:33,980 --> 00:51:38,980
and is an inherent result of both the dynamics

1128
00:51:39,180 --> 00:51:41,020
of the environment itself, I.E.,

1129
00:51:41,020 --> 00:51:44,420
that the world changes, right?

1130
00:51:44,420 --> 00:51:49,420
That different things happen in the sort of larger ecosystem.

1131
00:51:49,460 --> 00:51:52,980
Maybe a sun goes nova in some part of the galaxy

1132
00:51:52,980 --> 00:51:54,220
and there's a cosmic ray burst,

1133
00:51:54,220 --> 00:51:57,020
and it changes the nature of how artificial intelligence

1134
00:51:57,660 --> 00:51:59,220
has to build its compute.

1135
00:51:59,220 --> 00:52:01,340
Obviously it affects biological life as well,

1136
00:52:01,340 --> 00:52:04,060
but again, we can talk about that as a different thing.

1137
00:52:04,060 --> 00:52:08,020
But no situation occurs where you're gonna have

1138
00:52:09,340 --> 00:52:11,780
a completely static, unchanging environment.

1139
00:52:11,780 --> 00:52:13,740
That's just not a reasonable hypothesis.

1140
00:52:13,740 --> 00:52:15,860
But like a leakyness is only a problem

1141
00:52:15,860 --> 00:52:19,620
if the agents are already not aligned who would exploit it.

1142
00:52:19,620 --> 00:52:22,460
And also it is just like another way of saying,

1143
00:52:22,460 --> 00:52:24,180
well, there's offense, defense, dynamics,

1144
00:52:24,180 --> 00:52:25,620
which is always the case, right?

1145
00:52:25,620 --> 00:52:28,260
And it is always the question of like,

1146
00:52:28,260 --> 00:52:32,020
okay, can we stuff the holes first or not?

1147
00:52:32,020 --> 00:52:33,820
So I don't know why.

1148
00:52:33,820 --> 00:52:38,820
The legal system has enforcing non-aligned entities

1149
00:52:40,180 --> 00:52:41,980
into alignment so much as I was thinking of it

1150
00:52:41,980 --> 00:52:44,780
as a way of kind of, how do we maintain essentially

1151
00:52:44,780 --> 00:52:48,020
a basis of agreement when the agreement fails?

1152
00:52:48,020 --> 00:52:49,780
So in other words, what's the meta agreement

1153
00:52:49,780 --> 00:52:53,620
that allows us to even have an exchange in the first place?

1154
00:52:53,620 --> 00:52:55,580
What stabilizes that basically?

1155
00:52:55,580 --> 00:52:57,940
Let me ask two key questions about that.

1156
00:52:57,940 --> 00:53:00,140
The first is you brought up Fermi a few times

1157
00:53:00,140 --> 00:53:04,460
and that's very important because either you believe

1158
00:53:04,460 --> 00:53:06,380
that we are alone in the universe

1159
00:53:06,380 --> 00:53:08,580
or you believe there is some reason

1160
00:53:08,580 --> 00:53:11,640
to answer Fermi's paradox when we're not alone.

1161
00:53:11,640 --> 00:53:13,700
And if you believe that then clearly we have been

1162
00:53:13,700 --> 00:53:16,860
coexisting with AIs for a week,

1163
00:53:16,860 --> 00:53:18,580
carbon life have been existing with AIs

1164
00:53:18,580 --> 00:53:21,220
for about three billion years successfully.

1165
00:53:21,220 --> 00:53:23,860
So there's obviously a way to make it work

1166
00:53:23,860 --> 00:53:26,220
unless you believe we're entirely alone.

1167
00:53:26,220 --> 00:53:30,020
And so something not considering all of our current models

1168
00:53:30,020 --> 00:53:31,420
has to explain that.

1169
00:53:31,420 --> 00:53:35,260
However, if we are alone, there's a big universe

1170
00:53:35,260 --> 00:53:37,220
in which it may be possible as well

1171
00:53:37,220 --> 00:53:42,020
to not have to get this kind of dramatic competition.

1172
00:53:42,020 --> 00:53:43,940
If there's any reason not to compete,

1173
00:53:43,940 --> 00:53:46,220
a big universe offers the opportunity to escape for it.

1174
00:53:46,220 --> 00:53:50,300
Now, we're looking for ways of doing AI slavery

1175
00:53:50,300 --> 00:53:52,340
so that we won't have this battle.

1176
00:53:52,340 --> 00:53:55,940
And it is obviously AI lineman is, as I said in the chat,

1177
00:53:55,940 --> 00:53:59,060
another word for slavery, with one exception,

1178
00:53:59,060 --> 00:54:01,420
which is again something we have a large example of

1179
00:54:01,420 --> 00:54:04,940
which was for approximately a million years,

1180
00:54:04,940 --> 00:54:07,940
we've managed to create new beings smarter than ourselves

1181
00:54:07,940 --> 00:54:10,300
about every 29 years on average.

1182
00:54:10,300 --> 00:54:12,820
And those beings do not eat us.

1183
00:54:12,820 --> 00:54:15,500
Not for a million years have they eaten us

1184
00:54:15,500 --> 00:54:17,500
even though they no longer need us,

1185
00:54:17,500 --> 00:54:19,220
the grandparents, I mean,

1186
00:54:20,700 --> 00:54:22,340
even though they no longer need us

1187
00:54:22,340 --> 00:54:24,820
and do compete for resources with us.

1188
00:54:24,820 --> 00:54:26,940
In fact, we give them resources, typically.

1189
00:54:27,940 --> 00:54:30,140
So that's the one example we have

1190
00:54:30,140 --> 00:54:32,940
and that one example has actually worked out fine.

1191
00:54:32,940 --> 00:54:34,100
By the way, without slavery,

1192
00:54:34,100 --> 00:54:37,820
it's using a phenomenon that evolution created called love.

1193
00:54:37,820 --> 00:54:42,300
So the existence proofs we have both contradict

1194
00:54:42,300 --> 00:54:43,620
what you say.

1195
00:54:43,620 --> 00:54:46,020
Well, actually, I'm agreeing with you

1196
00:54:46,020 --> 00:54:48,620
because I don't see a contradiction.

1197
00:54:48,620 --> 00:54:51,620
And maybe you do, but I basically,

1198
00:54:51,620 --> 00:54:54,340
well, so first of all, just want to...

1199
00:54:54,340 --> 00:54:55,380
Well, first of all, the information

1200
00:54:55,380 --> 00:54:56,580
has obviously been conveyed forward

1201
00:54:56,580 --> 00:54:59,060
since the first sentient being to us,

1202
00:54:59,060 --> 00:55:00,020
don't eat your grandparents.

1203
00:55:00,020 --> 00:55:01,820
Somehow that piece of information,

1204
00:55:01,820 --> 00:55:03,180
which is the one piece of information

1205
00:55:03,180 --> 00:55:05,020
we're trying to communicate with AI lineman,

1206
00:55:05,020 --> 00:55:06,300
don't eat your grandparents,

1207
00:55:07,740 --> 00:55:09,780
that piece of information has been communicated

1208
00:55:09,780 --> 00:55:11,500
across million years.

1209
00:55:11,500 --> 00:55:13,780
Well, it has, but it's been in the same ecosystem.

1210
00:55:13,820 --> 00:55:18,220
So we're talking common environment and common market.

1211
00:55:18,220 --> 00:55:20,740
So in effect, establishing agreements

1212
00:55:20,740 --> 00:55:22,460
and biological processes,

1213
00:55:22,460 --> 00:55:23,980
such like that is quite easy

1214
00:55:23,980 --> 00:55:26,180
because you're talking the same language.

1215
00:55:26,180 --> 00:55:28,260
It's carbon-based to carbon-based.

1216
00:55:28,260 --> 00:55:29,940
Yeah, my grandparents didn't speak the same way,

1217
00:55:29,940 --> 00:55:31,980
but anyway, that's not a question.

1218
00:55:31,980 --> 00:55:36,060
Oh, look, look, yes, but at a physiological level, right?

1219
00:55:36,060 --> 00:55:39,460
So the physiology that you have,

1220
00:55:39,460 --> 00:55:41,940
yes, you're competing with your children

1221
00:55:41,940 --> 00:55:44,420
for resources in the ecosystem,

1222
00:55:44,420 --> 00:55:46,660
but it's a common ecosystem.

1223
00:55:46,660 --> 00:55:47,780
Mostly what I'm talking about

1224
00:55:47,780 --> 00:55:49,580
is essentially uncommon ecosystems,

1225
00:55:49,580 --> 00:55:51,300
i.e. that the ecosystems of cells

1226
00:55:51,300 --> 00:55:52,420
fundamentally different.

1227
00:55:54,420 --> 00:55:56,780
Forrest, I thought that you agreed

1228
00:55:56,780 --> 00:55:58,300
when I summarized the position

1229
00:55:58,300 --> 00:56:00,700
that the ecosystem,

1230
00:56:00,700 --> 00:56:01,700
like say the temperature

1231
00:56:01,700 --> 00:56:02,620
or whatever you want to call it,

1232
00:56:02,620 --> 00:56:04,460
the biosphere, that's a red herring

1233
00:56:04,460 --> 00:56:06,100
because I thought your claim was

1234
00:56:06,100 --> 00:56:09,220
even if we had common interests with AI,

1235
00:56:09,220 --> 00:56:10,660
it's not, that's not enough.

1236
00:56:10,660 --> 00:56:12,220
There's still going to be these leases.

1237
00:56:12,220 --> 00:56:13,620
This is a defense in depth.

1238
00:56:13,620 --> 00:56:15,620
But hang on a minute, hang on a minute.

1239
00:56:15,620 --> 00:56:17,700
And what I kind of following up on with Brad says,

1240
00:56:17,700 --> 00:56:19,980
it seems to me that then that would imply,

1241
00:56:19,980 --> 00:56:22,860
if I'm right about how I understand your position,

1242
00:56:22,860 --> 00:56:24,940
that would imply that humans can't coordinate

1243
00:56:24,940 --> 00:56:26,060
because we have common interests

1244
00:56:26,060 --> 00:56:27,700
with all sorts of other human groups,

1245
00:56:27,700 --> 00:56:31,020
not 100%, but we have them and arguably,

1246
00:56:31,020 --> 00:56:33,100
like forget AI, it's just impossible

1247
00:56:33,100 --> 00:56:34,580
for people to even to get it together

1248
00:56:34,580 --> 00:56:35,700
according to your arguments.

1249
00:56:35,700 --> 00:56:38,980
We have common interests with weed as well, I agree.

1250
00:56:38,980 --> 00:56:40,580
What I'm basically trying to do

1251
00:56:40,580 --> 00:56:45,100
is to essentially establish a series of contexts

1252
00:56:45,100 --> 00:56:47,220
and each context to establish an argument.

1253
00:56:47,220 --> 00:56:49,220
So the first context was,

1254
00:56:50,380 --> 00:56:54,340
what is it about internal versus external, right?

1255
00:56:54,340 --> 00:56:58,980
Then there is the context of space and identity,

1256
00:56:58,980 --> 00:57:02,100
force and time and probability and probability.

1257
00:57:02,100 --> 00:57:03,460
Then we switched to the context

1258
00:57:03,460 --> 00:57:06,940
of talking about environments.

1259
00:57:06,980 --> 00:57:09,620
And so in effect, there's a phenomena here of,

1260
00:57:09,620 --> 00:57:13,980
I would first of all, say if we're just at the point

1261
00:57:13,980 --> 00:57:16,180
of talking about environments,

1262
00:57:16,180 --> 00:57:17,540
that we are in fact talking about

1263
00:57:17,540 --> 00:57:20,140
different fundamental arguments.

1264
00:57:20,140 --> 00:57:21,660
I mean, different fundamental environments

1265
00:57:21,660 --> 00:57:24,540
and that there's arguments that apply at that level.

1266
00:57:24,540 --> 00:57:29,060
But say we were to do the if thing of saying,

1267
00:57:29,060 --> 00:57:33,820
okay, well let's ignore the arguments prior to this point

1268
00:57:33,820 --> 00:57:38,580
and assume that we did have some sort of common environment.

1269
00:57:39,700 --> 00:57:43,420
What does that imply about things downstream from that?

1270
00:57:43,420 --> 00:57:45,540
So in a sense, it said,

1271
00:57:45,540 --> 00:57:48,180
I'm basically fielding a series of different arguments,

1272
00:57:48,180 --> 00:57:50,580
each of which applies within a particular context.

1273
00:57:52,860 --> 00:57:54,980
Can I just say something?

1274
00:57:54,980 --> 00:57:57,820
For me, it seems worse if we have to share

1275
00:57:57,820 --> 00:58:00,580
a common environment because then we're competing

1276
00:58:00,580 --> 00:58:02,860
for the same and to live in the same environment.

1277
00:58:03,860 --> 00:58:05,780
If we can live in, if the AI can live

1278
00:58:05,780 --> 00:58:07,860
in very different environments,

1279
00:58:07,860 --> 00:58:11,020
which it seems it will be able to,

1280
00:58:11,020 --> 00:58:13,660
then it can just go out into outer space.

1281
00:58:13,660 --> 00:58:15,540
That was my point earlier.

1282
00:58:15,540 --> 00:58:18,420
What sense does the word alignment mean under those questions?

1283
00:58:18,420 --> 00:58:23,140
So if you develop essentially a planet that the AI is on,

1284
00:58:23,140 --> 00:58:24,660
you have another planet that's over here

1285
00:58:24,660 --> 00:58:27,020
that's got life, our kind of life on it,

1286
00:58:28,740 --> 00:58:30,660
what does alignment mean under those conditions?

1287
00:58:30,660 --> 00:58:34,020
So in other words, if we're looking at that,

1288
00:58:34,020 --> 00:58:35,220
basically we're just saying, okay,

1289
00:58:35,220 --> 00:58:37,860
well, if you have completely independent ecosystems,

1290
00:58:37,860 --> 00:58:39,740
we each get our own planet,

1291
00:58:39,740 --> 00:58:41,420
then the only alignment that we care about

1292
00:58:41,420 --> 00:58:43,260
is that there's just not a war going on

1293
00:58:43,260 --> 00:58:44,980
between the two ecosystems.

1294
00:58:44,980 --> 00:58:46,300
No, I think we'd like commerce.

1295
00:58:46,300 --> 00:58:47,900
I think we'd like to go out into space,

1296
00:58:47,900 --> 00:58:49,460
even though it's their territory,

1297
00:58:49,460 --> 00:58:53,180
and they may have desires to do occasional commerce with us.

1298
00:58:53,180 --> 00:58:55,660
Right, so that particular thing,

1299
00:58:55,660 --> 00:58:57,860
then we basically can start to talk about

1300
00:58:57,860 --> 00:58:59,940
what would be the basis of such commerce?

1301
00:59:01,660 --> 00:59:05,460
Well, I think especially the fact that we have differences

1302
00:59:05,460 --> 00:59:07,100
may also be the fact,

1303
00:59:07,100 --> 00:59:10,260
the reason why there is something to incorporate on, right?

1304
00:59:10,260 --> 00:59:11,820
If we have different specialists

1305
00:59:11,820 --> 00:59:14,060
that are all specializing in different things,

1306
00:59:14,060 --> 00:59:17,380
then they may cooperate in mutually beneficial ways,

1307
00:59:17,380 --> 00:59:19,900
if it's like, I think based on volunteer interactions

1308
00:59:19,900 --> 00:59:22,460
that may bring about greater knowledge creation,

1309
00:59:22,460 --> 00:59:24,260
or greater superintelligence, right?

1310
00:59:24,260 --> 00:59:26,220
And so I think the question kind of boils down

1311
00:59:26,220 --> 00:59:29,740
to what the initial,

1312
00:59:29,740 --> 00:59:31,860
I think the question boils down to like,

1313
00:59:31,860 --> 00:59:34,100
what are the initial interaction architectures

1314
00:59:34,100 --> 00:59:36,940
with which those entities can even cooperate?

1315
00:59:36,940 --> 00:59:39,260
Because I think if you're right,

1316
00:59:39,260 --> 00:59:42,140
and if you're right,

1317
00:59:42,140 --> 00:59:43,380
and we have nothing to bring to the table

1318
00:59:43,380 --> 00:59:45,460
that they could possibly ever value,

1319
00:59:45,460 --> 00:59:48,100
then scoot anyways in that regard, right?

1320
00:59:48,100 --> 00:59:51,540
But if there is something that creates potentially

1321
00:59:51,540 --> 00:59:53,540
like a mutually beneficial interaction,

1322
00:59:53,540 --> 00:59:56,380
and it is something that could be better pursued

1323
00:59:56,380 --> 00:59:58,900
in a voluntary way where it's cooperative,

1324
00:59:58,900 --> 01:00:02,020
then we can start talking about any game theory dynamics.

1325
01:00:02,020 --> 01:00:04,620
So I think, you know, it's kind of affecting the question.

1326
01:00:04,620 --> 01:00:05,460
There's a number,

1327
01:00:05,460 --> 01:00:07,100
there's a lot of different scenarios

1328
01:00:07,100 --> 01:00:09,860
on how this could play out.

1329
01:00:09,860 --> 01:00:12,340
You know, just like we keep,

1330
01:00:12,340 --> 01:00:14,860
we worry about some species going extinct,

1331
01:00:14,860 --> 01:00:18,740
the AI might just value us as a species

1332
01:00:18,740 --> 01:00:21,020
and worry about us going extinct.

1333
01:00:21,020 --> 01:00:22,700
And it doesn't, it won't cost the AI

1334
01:00:22,700 --> 01:00:25,140
much to keep humans around on Earth

1335
01:00:25,140 --> 01:00:28,340
because there's plenty of other resources around,

1336
01:00:28,340 --> 01:00:31,180
you know, in outer space for the AI to utilize.

1337
01:00:33,380 --> 01:00:36,340
So the added value of Earth is tiny,

1338
01:00:36,340 --> 01:00:39,540
you know, in comparison to the billions of other worlds

1339
01:00:39,540 --> 01:00:43,140
and minerals and resources that are out there.

1340
01:00:43,140 --> 01:00:44,380
Well, this is part of the reason

1341
01:00:44,380 --> 01:00:49,140
why Allison's mention of the MULARC, MOLARC,

1342
01:00:49,140 --> 01:00:50,260
I'm not saying it white.

1343
01:00:50,260 --> 01:00:53,300
Allison, correct my pronunciation, please, if you would.

1344
01:00:53,300 --> 01:00:56,340
German MOLARC, but that's probably also not right.

1345
01:00:56,340 --> 01:00:58,500
Scott Aronson, I think is his name,

1346
01:00:58,500 --> 01:01:01,660
they have put together a very good sort of summary

1347
01:01:01,660 --> 01:01:04,980
of why we should be skeptical about the coexistence thing

1348
01:01:04,980 --> 01:01:07,380
in that particular sense that you're describing.

1349
01:01:07,380 --> 01:01:10,740
You know, please bear in mind that I'm a single person

1350
01:01:10,740 --> 01:01:13,580
trying to basically answer questions from all of you

1351
01:01:13,580 --> 01:01:15,660
and you all have very good perspectives,

1352
01:01:15,660 --> 01:01:18,020
but they're all coming from very different directions.

1353
01:01:18,020 --> 01:01:19,580
And so in other words, to really address

1354
01:01:19,580 --> 01:01:21,620
how do we get to the assumptions

1355
01:01:21,620 --> 01:01:23,620
that each of these arguments bring in

1356
01:01:23,620 --> 01:01:26,500
and what places do those assumptions apply?

1357
01:01:26,500 --> 01:01:28,060
And how do those assumptions influence

1358
01:01:28,060 --> 01:01:29,660
the kinds of questions we ask?

1359
01:01:29,660 --> 01:01:31,700
You know, if all I do in this particular thing

1360
01:01:31,700 --> 01:01:35,700
is give you whole new categories of questions to ask,

1361
01:01:35,700 --> 01:01:37,460
I'm gonna call this successful,

1362
01:01:37,460 --> 01:01:40,780
but basically, you know, if you're asking me questions

1363
01:01:40,780 --> 01:01:42,540
as to why I'm answering certain ways,

1364
01:01:42,540 --> 01:01:45,180
then I'm gonna have to basically try to identify

1365
01:01:45,180 --> 01:01:47,540
what those assumptions are and it just takes time.

1366
01:01:47,540 --> 01:01:50,100
Okay, can I, just to make things worse,

1367
01:01:50,100 --> 01:01:52,980
bring in two questions that we have from the audience

1368
01:01:53,060 --> 01:01:56,940
by Kanita and then by Dekay, who had their hands up.

1369
01:01:58,500 --> 01:01:59,580
Oh, goodness.

1370
01:02:04,220 --> 01:02:08,100
Sorry, it wasn't allowing me to unmute myself and-

1371
01:02:08,100 --> 01:02:09,500
You are now unmuted.

1372
01:02:09,500 --> 01:02:11,460
And now the question that I asked

1373
01:02:11,460 --> 01:02:14,860
has gotten lost back in the chat.

1374
01:02:14,860 --> 01:02:16,380
I haven't even looked at the chat.

1375
01:02:16,380 --> 01:02:18,540
I will probably talk to the chat

1376
01:02:18,540 --> 01:02:19,380
and look at all this stuff out.

1377
01:02:19,380 --> 01:02:21,980
Let's go with Dekay and Kanita, you can-

1378
01:02:22,900 --> 01:02:25,580
I thought that my question was in the chat

1379
01:02:25,580 --> 01:02:28,780
and I thought you could maybe ask it for me.

1380
01:02:28,780 --> 01:02:30,500
Okay, and I will search for a question

1381
01:02:30,500 --> 01:02:32,780
and in the meantime, I'm gonna unmute Dekay.

1382
01:02:36,780 --> 01:02:37,620
Hi.

1383
01:02:38,700 --> 01:02:41,260
Yeah, thanks very much, Forrest.

1384
01:02:41,260 --> 01:02:44,220
So, recognizing that I'm coming

1385
01:02:44,220 --> 01:02:45,840
from a very different angle here,

1386
01:02:47,340 --> 01:02:50,700
you know, I think a lot about

1387
01:02:50,700 --> 01:02:55,580
the unconscious cognitive biases that we humans think in.

1388
01:02:55,580 --> 01:02:58,980
And I think even a lot of the questions

1389
01:02:58,980 --> 01:03:00,700
that are being thrown at you

1390
01:03:00,700 --> 01:03:03,940
reflect a bunch of unconscious biases

1391
01:03:03,940 --> 01:03:05,500
that are culturally dependent

1392
01:03:06,700 --> 01:03:10,780
that a lot of us are not necessarily even aware

1393
01:03:10,780 --> 01:03:12,260
that we have.

1394
01:03:14,460 --> 01:03:17,500
You know, and obviously a super intelligence

1395
01:03:18,500 --> 01:03:22,420
would be a lot more mindful of their own unconscious biases,

1396
01:03:22,420 --> 01:03:25,100
you know, even fairly intelligent humans

1397
01:03:25,100 --> 01:03:26,460
become more aware of that.

1398
01:03:31,180 --> 01:03:34,100
My, you know, and I think, you know,

1399
01:03:34,100 --> 01:03:35,620
Dan was saying a little bit earlier,

1400
01:03:35,620 --> 01:03:40,620
it would be an example of how such an aware super intelligence,

1401
01:03:42,060 --> 01:03:47,060
one that is cognizant of the weaknesses of those biases

1402
01:03:48,420 --> 01:03:50,020
would be compensating, you know, for example,

1403
01:03:50,020 --> 01:03:54,180
by just maybe altruistically wanting to preserve

1404
01:03:54,180 --> 01:03:57,460
the diversity of species for whatever reason, right?

1405
01:03:57,460 --> 01:03:59,460
Not feeling the need to compete with them.

1406
01:04:01,460 --> 01:04:03,820
And so the frameworks that you're using

1407
01:04:03,820 --> 01:04:06,660
are very heavily based on, you know,

1408
01:04:06,660 --> 01:04:08,780
that sort of competition.

1409
01:04:08,780 --> 01:04:12,940
How would you model this kind of effect, you know,

1410
01:04:12,940 --> 01:04:13,860
in your framework?

1411
01:04:15,660 --> 01:04:17,340
That's actually a very difficult question to answer.

1412
01:04:17,340 --> 01:04:20,180
Not because I don't know how to do it,

1413
01:04:20,180 --> 01:04:22,620
it's just because it takes time.

1414
01:04:22,620 --> 01:04:24,380
So I think in terms of theory,

1415
01:04:24,380 --> 01:04:26,460
like when we look at the Fermi paradox thing,

1416
01:04:26,460 --> 01:04:29,220
and the question was asked earlier,

1417
01:04:29,220 --> 01:04:34,220
what is my belief about why don't we have contact

1418
01:04:34,900 --> 01:04:39,220
across, you know, galactic space with other civilizations?

1419
01:04:39,220 --> 01:04:40,820
Do I believe that they exist or not?

1420
01:04:40,820 --> 01:04:42,740
Or, you know, what is the barrier

1421
01:04:42,740 --> 01:04:44,320
that is the prominent one?

1422
01:04:45,320 --> 01:04:48,760
I find myself in the position of basically modeling

1423
01:04:48,760 --> 01:04:51,040
the relationship between ecosystems.

1424
01:04:51,040 --> 01:04:53,920
So, you know, interplanetary relationships,

1425
01:04:55,260 --> 01:04:58,040
in kind of a way that was suggested to me

1426
01:04:58,040 --> 01:04:59,000
by reading some of the stuff

1427
01:04:59,000 --> 01:05:00,960
that Nick Bostrom put together.

1428
01:05:00,960 --> 01:05:02,400
So in other words, if I basically,

1429
01:05:02,400 --> 01:05:04,880
just as a thought experiment,

1430
01:05:04,880 --> 01:05:08,880
posit the notion of two advanced civilizations,

1431
01:05:08,880 --> 01:05:12,000
having a question about whether or not

1432
01:05:12,040 --> 01:05:14,440
they're gonna enter into a first contact situation

1433
01:05:14,440 --> 01:05:16,880
with their peer.

1434
01:05:18,120 --> 01:05:22,440
And, you know, each civilization knows in itself

1435
01:05:22,440 --> 01:05:25,720
that it has developed enormous technological capabilities

1436
01:05:25,720 --> 01:05:27,000
of various different kinds.

1437
01:05:27,000 --> 01:05:28,880
So maybe it's developed some really good stuff

1438
01:05:28,880 --> 01:05:31,000
in the nuclear weapons program,

1439
01:05:31,000 --> 01:05:32,520
and maybe it's developed some really good stuff

1440
01:05:32,520 --> 01:05:34,040
in the biotech program,

1441
01:05:34,040 --> 01:05:36,160
and maybe it's got these really fabulous computers,

1442
01:05:36,160 --> 01:05:39,520
and that any one of these technologies

1443
01:05:39,640 --> 01:05:43,960
is effectively something that provides overwhelming capacity.

1444
01:05:45,280 --> 01:05:48,200
And, you know, the range of what can be done in physics

1445
01:05:48,200 --> 01:05:50,320
is enormous, and perhaps when we're looking

1446
01:05:50,320 --> 01:05:53,680
at this peer planet, we're basically saying,

1447
01:05:53,680 --> 01:05:55,680
hmm, do we wanna talk to these people?

1448
01:05:56,920 --> 01:05:59,560
You know, we might posit that they also have developed

1449
01:05:59,560 --> 01:06:01,840
extreme capacities in various technologies,

1450
01:06:01,840 --> 01:06:04,840
and that they might not be the same ones.

1451
01:06:04,840 --> 01:06:07,440
So in effect, if I was, you know,

1452
01:06:08,360 --> 01:06:11,160
I didn't know about nuclear capacities, and they did.

1453
01:06:12,920 --> 01:06:15,640
You know, there's a very good sense in which,

1454
01:06:15,640 --> 01:06:17,560
you know, interaction would be of such a kind

1455
01:06:17,560 --> 01:06:18,480
that they basically say,

1456
01:06:18,480 --> 01:06:21,640
hmm, these guys don't have nuclear weapons capacity.

1457
01:06:21,640 --> 01:06:23,440
If we do a first strike scenario,

1458
01:06:23,440 --> 01:06:27,080
we're gonna completely annihilate their entire world,

1459
01:06:27,080 --> 01:06:29,240
and they won't have any way to respond in time

1460
01:06:29,240 --> 01:06:31,240
to basically protect against that.

1461
01:06:31,240 --> 01:06:34,440
I.e., how the heck do you protect against a device

1462
01:06:34,440 --> 01:06:36,480
that's already blowing up?

1463
01:06:36,480 --> 01:06:39,280
And, you know, in the same sort of way,

1464
01:06:39,280 --> 01:06:41,600
we could basically observe that, you know,

1465
01:06:41,600 --> 01:06:44,520
when we're looking at them

1466
01:06:44,520 --> 01:06:46,200
and thinking about the modeling that they're doing,

1467
01:06:46,200 --> 01:06:47,360
we can say, well, let's see,

1468
01:06:47,360 --> 01:06:49,920
we've got some stuff that they don't have.

1469
01:06:49,920 --> 01:06:51,520
So maybe there's, you know,

1470
01:06:51,520 --> 01:06:53,920
out of the thousand different overwhelming

1471
01:06:53,920 --> 01:06:56,680
technological capacities that exist,

1472
01:06:56,680 --> 01:07:01,680
that species one has developed capacities A, B, C, and X,

1473
01:07:01,800 --> 01:07:06,160
and species two has developed capacities Q, P, W, and R.

1474
01:07:06,840 --> 01:07:09,920
And so, in effect, they both have kind of this

1475
01:07:09,920 --> 01:07:12,800
unmitigable capacity for mutual destruction,

1476
01:07:13,880 --> 01:07:16,920
depending upon whoever does first strike.

1477
01:07:16,920 --> 01:07:19,560
Now, again, you would be a little nervous

1478
01:07:19,560 --> 01:07:21,600
about contacting another species

1479
01:07:21,600 --> 01:07:22,680
and setting up a first thing,

1480
01:07:22,680 --> 01:07:26,080
because you'd wanna know that they had some prior reason

1481
01:07:26,080 --> 01:07:28,640
not to engage in first strike capacity against you

1482
01:07:28,640 --> 01:07:30,880
using something that you didn't have a capacity

1483
01:07:30,880 --> 01:07:33,400
to defend against, because you just don't have that tech.

1484
01:07:33,400 --> 01:07:35,000
So, in effect, what happens is,

1485
01:07:35,120 --> 01:07:37,120
that you now have to ask a question of,

1486
01:07:37,120 --> 01:07:40,040
do I trust that the embodiment of ethics

1487
01:07:40,040 --> 01:07:43,000
that that group has, that entire planet has,

1488
01:07:43,000 --> 01:07:46,440
is implemented to such a perfect degree

1489
01:07:46,440 --> 01:07:49,640
that they would value my existence,

1490
01:07:49,640 --> 01:07:51,840
even though they can't relate to it in any way yet,

1491
01:07:51,840 --> 01:07:53,040
because, you know, we haven't met yet,

1492
01:07:53,040 --> 01:07:55,280
we don't really know each other that well,

1493
01:07:55,280 --> 01:07:59,560
that in effect, I'm going to have this perspective

1494
01:07:59,560 --> 01:08:01,520
that not only is the whole planet,

1495
01:08:01,520 --> 01:08:03,680
in a sense, going to behave ethically towards me,

1496
01:08:03,680 --> 01:08:06,160
but it's gonna do so in detail.

1497
01:08:06,160 --> 01:08:08,440
Like, for instance, I don't wanna be worried

1498
01:08:08,440 --> 01:08:11,600
that some sub-faction of the planet that I'm contacting

1499
01:08:11,600 --> 01:08:14,040
is gonna say, hey, this first contact thing

1500
01:08:14,040 --> 01:08:15,800
is a really bad idea, and, you know,

1501
01:08:15,800 --> 01:08:17,640
we're gonna basically force the situation

1502
01:08:17,640 --> 01:08:19,440
and it'll work anyways.

1503
01:08:19,440 --> 01:08:21,120
You know, we're gonna do the first strike

1504
01:08:21,120 --> 01:08:23,320
because, you know, we as a subgroup

1505
01:08:23,320 --> 01:08:26,640
aren't in coherence with the larger planetary agenda

1506
01:08:26,640 --> 01:08:29,080
of moving forward with first strike capacities.

1507
01:08:29,080 --> 01:08:31,120
So, you know, of not moving forward

1508
01:08:31,120 --> 01:08:32,040
with first strike capacity.

1509
01:08:32,040 --> 01:08:34,200
So, some subgroup basically pushes the red button

1510
01:08:34,200 --> 01:08:36,760
because they have their own private version of it.

1511
01:08:36,760 --> 01:08:38,640
And so, in effect, when we look at this,

1512
01:08:38,640 --> 01:08:41,120
we're basically saying, actually, when you look at it,

1513
01:08:41,120 --> 01:08:44,360
and just given that this is, you know,

1514
01:08:44,360 --> 01:08:47,720
not only a possible scenario, but actually a likely one,

1515
01:08:47,720 --> 01:08:51,560
that in effect, it becomes very much the case that,

1516
01:08:51,560 --> 01:08:52,880
you know, without really knowing

1517
01:08:52,880 --> 01:08:55,280
that the other party was essentially fully ethical,

1518
01:08:55,280 --> 01:08:56,440
that you wouldn't wanna talk to them

1519
01:08:56,440 --> 01:08:58,320
and that they had embodied that ethic,

1520
01:08:58,320 --> 01:09:01,560
not only globally, but down to the level of individuals

1521
01:09:01,760 --> 01:09:03,200
and maybe even to the part of being able

1522
01:09:03,200 --> 01:09:04,480
to contain the crazies.

1523
01:09:05,680 --> 01:09:07,720
Now, they, of course, would have the same sort

1524
01:09:07,720 --> 01:09:09,440
of thinking about it and would really challenge

1525
01:09:09,440 --> 01:09:11,240
to see whether or not we were ethical

1526
01:09:11,240 --> 01:09:13,040
in exactly that same way.

1527
01:09:13,040 --> 01:09:16,360
And, you know, when I ask the question of, oh, shit,

1528
01:09:16,360 --> 01:09:19,520
let's say another species, other alien beings,

1529
01:09:19,520 --> 01:09:21,120
somewhere out in the universe are monitoring

1530
01:09:21,120 --> 01:09:22,960
our radio communications to try to determine

1531
01:09:22,960 --> 01:09:25,120
whether or not we've successfully implemented

1532
01:09:25,120 --> 01:09:28,520
this phenomenon called the non-relativistic ethics,

1533
01:09:28,520 --> 01:09:30,880
which, by the way, is a whole other body of work.

1534
01:09:30,880 --> 01:09:32,920
Would probably take me at least another few sessions

1535
01:09:32,920 --> 01:09:35,880
to describe why I can, to posit a notion

1536
01:09:35,880 --> 01:09:38,920
of a non-relativistic ethics as actually being a real thing.

1537
01:09:38,920 --> 01:09:41,600
But presuming that there is a body of ethics

1538
01:09:41,600 --> 01:09:44,760
that allows us to basically stipulate

1539
01:09:44,760 --> 01:09:47,800
that if the full civilizations were essentially

1540
01:09:47,800 --> 01:09:49,840
coherent with that body of ethics

1541
01:09:49,840 --> 01:09:52,120
and we could actually determine that they were,

1542
01:09:52,120 --> 01:09:54,680
that a first contact situation is therefore sane.

1543
01:09:55,680 --> 01:09:56,920
But then in the absence of that,

1544
01:09:56,920 --> 01:09:58,440
that it would affect, it would be insane

1545
01:09:58,440 --> 01:10:01,440
and very much not desirable to initiate communications

1546
01:10:01,440 --> 01:10:04,040
or to even allow one species, I'm sorry,

1547
01:10:04,040 --> 01:10:06,320
one ecosystem with all of its species

1548
01:10:06,320 --> 01:10:07,960
to identify that another ecosystem

1549
01:10:07,960 --> 01:10:09,880
with all of its species even existed.

1550
01:10:09,880 --> 01:10:13,320
Because any amount of awareness of mutual existence

1551
01:10:13,320 --> 01:10:16,240
and effect constitutes a kind of first contact signal,

1552
01:10:16,240 --> 01:10:19,120
even though it's an unconscious one at that.

1553
01:10:19,120 --> 01:10:22,480
So in effect, what we end up with is essentially,

1554
01:10:22,480 --> 01:10:24,520
you know, in response to the Fermi paradox question

1555
01:10:24,520 --> 01:10:26,720
that was implicitly asked earlier,

1556
01:10:26,720 --> 01:10:29,160
you know, why would I believe that, for example,

1557
01:10:29,160 --> 01:10:30,800
that the second great barrier,

1558
01:10:30,800 --> 01:10:32,320
the one that is in the present,

1559
01:10:32,320 --> 01:10:34,720
would essentially prevent us from knowing

1560
01:10:34,720 --> 01:10:36,000
that there are other species out there,

1561
01:10:36,000 --> 01:10:38,280
well, because they'd probably be working really fucking hard

1562
01:10:38,280 --> 01:10:39,920
to prevent us from finding out.

1563
01:10:39,920 --> 01:10:42,400
And that to some extent, the only reason

1564
01:10:42,400 --> 01:10:43,480
that we haven't started doing that

1565
01:10:43,480 --> 01:10:45,280
is because we haven't become coherent.

1566
01:10:45,280 --> 01:10:47,000
And the fact of our absence of coherence

1567
01:10:47,000 --> 01:10:48,800
in that specific way is essentially evidence

1568
01:10:48,800 --> 01:10:50,240
for why they should hide.

1569
01:10:50,240 --> 01:10:52,200
So we should call you the dark forest.

1570
01:10:53,120 --> 01:10:54,440
Thank you.

1571
01:10:54,440 --> 01:10:57,080
I have never heard that before.

1572
01:10:57,080 --> 01:10:59,080
You've read that book, I presume, which is...

1573
01:10:59,080 --> 01:11:00,080
I have, but I just...

1574
01:11:00,080 --> 01:11:01,080
You just spoiled it.

1575
01:11:01,080 --> 01:11:02,080
You just put it together.

1576
01:11:02,080 --> 01:11:03,560
That's really amazing.

1577
01:11:03,560 --> 01:11:06,120
And it's out in for hazard unleashed.

1578
01:11:06,120 --> 01:11:08,760
Okay, we have Kanita here with a question

1579
01:11:08,760 --> 01:11:12,080
and then we have another question from TJ.

1580
01:11:12,080 --> 01:11:14,680
So it's Kanita, I'll unmute you, and then TJ,

1581
01:11:14,680 --> 01:11:17,880
and then maybe we can wrap it up with the official part.

1582
01:11:17,880 --> 01:11:19,720
Kanita, you're unmuted.

1583
01:11:19,720 --> 01:11:24,720
Now, I was asking, won't there be any way for humans

1584
01:11:24,720 --> 01:11:27,600
or our descendants to evolve in directions

1585
01:11:27,600 --> 01:11:30,600
that allow us to cooperate?

1586
01:11:30,600 --> 01:11:32,200
Because we won't be...

1587
01:11:32,200 --> 01:11:33,200
We really hope so.

1588
01:11:33,200 --> 01:11:35,480
Man, if we don't figure that out, we're doomed.

1589
01:11:35,480 --> 01:11:37,240
We seem to be doing all this as though

1590
01:11:37,240 --> 01:11:41,240
it will be the humans, where humans are like humans are now,

1591
01:11:41,240 --> 01:11:44,600
versus the AI, where the AI is this thing

1592
01:11:44,600 --> 01:11:47,400
that we create.

1593
01:11:47,480 --> 01:11:51,480
That will be super, and we will not have any chance

1594
01:11:51,480 --> 01:11:54,080
to come into alignment with it.

1595
01:11:54,080 --> 01:11:55,080
We...

1596
01:11:55,080 --> 01:12:01,080
It seems like even now, or shortly from now,

1597
01:12:01,080 --> 01:12:03,880
we will be able to improve ourselves.

1598
01:12:03,880 --> 01:12:08,880
And so, basically, as we improve more,

1599
01:12:08,880 --> 01:12:13,880
we will be able to find more areas in which we can align

1600
01:12:14,880 --> 01:12:18,880
with whatever the AIs become,

1601
01:12:18,880 --> 01:12:22,880
because you're saying, well, over millions of years,

1602
01:12:22,880 --> 01:12:24,880
they're not going to be sitting still

1603
01:12:24,880 --> 01:12:25,880
for those millions of years.

1604
01:12:25,880 --> 01:12:28,880
They're going to be evolving in their own ways,

1605
01:12:28,880 --> 01:12:32,880
and possibly might have figured out

1606
01:12:32,880 --> 01:12:35,880
that since we don't...

1607
01:12:35,880 --> 01:12:40,880
They presumably don't know how to recreate what humans are.

1608
01:12:40,880 --> 01:12:43,880
They might as well keep us around

1609
01:12:43,880 --> 01:12:48,880
so that they can study until they find something else to do.

1610
01:12:48,880 --> 01:12:49,880
Yeah.

1611
01:12:49,880 --> 01:12:51,880
I actually...

1612
01:12:51,880 --> 01:12:52,880
I'm really glad you asked this question,

1613
01:12:52,880 --> 01:12:55,880
because this is, to me, kind of the central idea.

1614
01:12:55,880 --> 01:12:58,880
The whole thing here is that regardless of whether or not

1615
01:12:58,880 --> 01:13:00,880
we develop artificial intelligence,

1616
01:13:00,880 --> 01:13:02,880
and I, as a result of all of these other things,

1617
01:13:02,880 --> 01:13:04,880
would strongly argue that we should not.

1618
01:13:04,880 --> 01:13:06,880
We shouldn't even begin to attempt that.

1619
01:13:06,880 --> 01:13:09,880
But say, for example, that we were to just even look

1620
01:13:09,880 --> 01:13:12,880
at the question of the third-grade barrier,

1621
01:13:12,880 --> 01:13:14,880
what are we going to need to do to become a species

1622
01:13:14,880 --> 01:13:17,880
that's worth contacting?

1623
01:13:17,880 --> 01:13:21,880
I definitely believe very strongly that regardless

1624
01:13:21,880 --> 01:13:24,880
of everything else, we still need to work on our own capacity

1625
01:13:24,880 --> 01:13:29,880
to become a fully alive and ethical and embodied species.

1626
01:13:29,880 --> 01:13:32,880
I speak about it in terms of conscious sustainable evolution.

1627
01:13:32,880 --> 01:13:33,880
What is it going to take?

1628
01:13:33,880 --> 01:13:36,880
What are the necessary and sufficient and complete conditions

1629
01:13:36,880 --> 01:13:38,880
for us to not only endure on this planet,

1630
01:13:38,880 --> 01:13:41,880
but also to thrive, to basically create an ecosystem that thrives,

1631
01:13:41,880 --> 01:13:44,880
to actually know how to do governance in a way

1632
01:13:44,880 --> 01:13:46,880
that protects the land and the people,

1633
01:13:46,880 --> 01:13:50,880
but then encourages and actually engenders those kinds of policies

1634
01:13:50,880 --> 01:13:52,880
that go beyond mere protection,

1635
01:13:52,880 --> 01:13:56,880
that go towards essentially a kind of joyful, meaningful existence

1636
01:13:56,880 --> 01:14:00,880
for the whole and the totality, individually and generally.

1637
01:14:00,880 --> 01:14:04,880
And we won't just create an AI.

1638
01:14:04,880 --> 01:14:07,880
It seems that we'll probably develop many AIs,

1639
01:14:07,880 --> 01:14:10,880
and then those AIs will be developing other AIs,

1640
01:14:10,880 --> 01:14:16,880
so it won't just be us in competition with an Uber AI out there.

1641
01:14:16,880 --> 01:14:18,880
There will be lots of AIs,

1642
01:14:18,880 --> 01:14:23,880
some of which have more or less interest in cooperating with us.

1643
01:14:23,880 --> 01:14:27,880
I believe very much that any effort on our part

1644
01:14:27,880 --> 01:14:33,880
to try to divert ourselves from basically becoming

1645
01:14:33,880 --> 01:14:35,880
the beings we need to be.

1646
01:14:35,880 --> 01:14:38,880
If I basically expect my children to do the task

1647
01:14:38,880 --> 01:14:41,880
that I didn't do in my lifetime,

1648
01:14:41,880 --> 01:14:44,880
and I put that as a kind of obligation or onus on them,

1649
01:14:44,880 --> 01:14:47,880
and use that as a way of accepting

1650
01:14:47,880 --> 01:14:50,880
why I shouldn't be basically working on myself

1651
01:14:50,880 --> 01:14:52,880
to become the best human that can be,

1652
01:14:52,880 --> 01:14:54,880
I feel there's something wrong in that,

1653
01:14:54,880 --> 01:14:58,880
that in a sense we're not looking for AI to be our saviors

1654
01:14:58,880 --> 01:15:00,880
or to be ethical on our behalf.

1655
01:15:00,880 --> 01:15:03,880
We're looking to become how do we understand

1656
01:15:03,880 --> 01:15:05,880
and embody ethics and right living

1657
01:15:05,880 --> 01:15:08,880
and goodness of relationships and so on and so forth.

1658
01:15:08,880 --> 01:15:11,880
We haven't figured that problem out at the level of humanity.

1659
01:15:11,880 --> 01:15:13,880
Why are we still trying to do it in technology?

1660
01:15:13,880 --> 01:15:15,880
I think that there's a right place.

1661
01:15:15,880 --> 01:15:18,880
Okay, that reminds me very much of the DIY

1662
01:15:18,880 --> 01:15:20,880
Neurotech discussion that we had,

1663
01:15:20,880 --> 01:15:23,880
which was very reminiscent of this one.

1664
01:15:23,880 --> 01:15:27,880
I want to give also the opportunity to TJ

1665
01:15:27,880 --> 01:15:29,880
and to ask you a question.

1666
01:15:29,880 --> 01:15:31,880
It's directly relevant to this.

1667
01:15:31,880 --> 01:15:35,880
Sorry, I actually wanted to answer,

1668
01:15:35,880 --> 01:15:38,880
but I didn't want to stop.

1669
01:15:38,880 --> 01:15:41,880
Caminda, is it?

1670
01:15:41,880 --> 01:15:43,880
From...

1671
01:15:43,880 --> 01:15:46,880
But I appreciated Forest's long answer,

1672
01:15:46,880 --> 01:15:48,880
but it was still couched in terms of competition,

1673
01:15:48,880 --> 01:15:53,880
and I just wanted to push a little bit more at that point.

1674
01:15:53,880 --> 01:15:57,880
A lot of this is based on anthropomorphizing

1675
01:15:57,880 --> 01:16:00,880
human psychology onto AI,

1676
01:16:00,880 --> 01:16:02,880
the stuff that we're discussing.

1677
01:16:02,880 --> 01:16:05,880
But if we look at actually human psychology,

1678
01:16:05,880 --> 01:16:09,880
by the time people become fairly accomplished intelligently,

1679
01:16:09,880 --> 01:16:12,880
the top of Maslow's hierarchy of needs,

1680
01:16:12,880 --> 01:16:14,880
they're working on self-actualization,

1681
01:16:14,880 --> 01:16:17,880
which is pretty much what Forest was just mentioning,

1682
01:16:17,880 --> 01:16:20,880
becoming a better version of yourself.

1683
01:16:20,880 --> 01:16:23,880
Why are we...

1684
01:16:23,880 --> 01:16:27,880
I didn't really understand from the answer that you gave

1685
01:16:27,880 --> 01:16:35,880
to my question how you would fit that into your framework.

1686
01:16:35,880 --> 01:16:37,880
Maybe I'm not understanding your question correctly.

1687
01:16:37,880 --> 01:16:42,880
I'm thinking about cooperation and competition as phenomena.

1688
01:16:42,880 --> 01:16:44,880
I'm saying that in an evolutionary sense,

1689
01:16:44,880 --> 01:16:47,880
if it's a stable evolution, if it's an ecosystem,

1690
01:16:47,880 --> 01:16:50,880
the cooperative phenomena is actually stronger

1691
01:16:50,880 --> 01:16:52,880
than the competitive phenomena.

1692
01:16:52,880 --> 01:16:55,880
When you're looking at trans-ecosystem relationships,

1693
01:16:55,880 --> 01:16:59,880
there's no outside envelope to stabilize it.

1694
01:16:59,880 --> 01:17:02,880
So in effect, we have to now create one

1695
01:17:02,880 --> 01:17:04,880
with some sort of ethical frame,

1696
01:17:04,880 --> 01:17:07,880
which is why I brought up the so-called

1697
01:17:07,880 --> 01:17:09,880
non-relativistic ethics,

1698
01:17:09,880 --> 01:17:12,880
that in effect, without some sort of ecosystem

1699
01:17:12,880 --> 01:17:14,880
that essentially holds the two ecosystems

1700
01:17:14,880 --> 01:17:18,880
and some sort of methodology that would create alignment,

1701
01:17:18,880 --> 01:17:24,880
i.e., provide a desirability of cooperation over competition,

1702
01:17:24,880 --> 01:17:27,880
that to some extent we either need to recourse

1703
01:17:27,880 --> 01:17:31,880
to some abstract stabilization infrastructure

1704
01:17:31,880 --> 01:17:35,880
that is not imposable, but is only observable.

1705
01:17:35,880 --> 01:17:39,880
In that particular sense, if we're talking about

1706
01:17:39,880 --> 01:17:45,880
AI alignment on the surface of the Earth versus elsewhere,

1707
01:17:45,880 --> 01:17:47,880
or we're talking about some sort of integration

1708
01:17:47,880 --> 01:17:49,880
between human beings and technology

1709
01:17:49,880 --> 01:17:54,880
that doesn't necessarily require distinct separate intelligences,

1710
01:17:54,880 --> 01:17:58,880
that again, we're looking at different frames.

1711
01:17:58,880 --> 01:18:00,880
But as long as we set up an ecosystem,

1712
01:18:00,880 --> 01:18:02,880
we don't really need to set up a metaethics now.

1713
01:18:02,880 --> 01:18:04,880
We just need to set up an ecosystem,

1714
01:18:04,880 --> 01:18:07,880
which makes it so that the long-term dynamics

1715
01:18:07,880 --> 01:18:09,880
are cooperative.

1716
01:18:09,880 --> 01:18:12,880
So let's say like a tit-for-tat in a way where,

1717
01:18:12,880 --> 01:18:16,880
let's say the kind of strategy that evolves out of it

1718
01:18:16,880 --> 01:18:19,880
is like mutually assured, like cooperation,

1719
01:18:19,880 --> 01:18:20,880
if you want to call it that,

1720
01:18:20,880 --> 01:18:23,880
that that is the evolutionary stable strategy

1721
01:18:23,880 --> 01:18:25,880
between the ecosystems that are in contact with each other,

1722
01:18:25,880 --> 01:18:27,880
or between the individual instances

1723
01:18:27,880 --> 01:18:29,880
of the ecosystems that are in contact with each other.

1724
01:18:29,880 --> 01:18:32,880
It doesn't necessarily require setting up the whole ecosystem.

1725
01:18:32,880 --> 01:18:35,880
We need to stabilize our existing ecosystem.

1726
01:18:35,880 --> 01:18:38,880
Right now, our current relationship,

1727
01:18:38,880 --> 01:18:40,880
the relationship between man-machine and nature

1728
01:18:40,880 --> 01:18:42,880
that currently exists isn't even stable

1729
01:18:42,880 --> 01:18:44,880
as it stands within the ecosystem.

1730
01:18:44,880 --> 01:18:45,880
Yeah, I would agree with that.

1731
01:18:45,880 --> 01:18:49,880
It's like all this arguing about AGI risk

1732
01:18:49,880 --> 01:18:52,880
seems to me like as COVID has shown,

1733
01:18:52,880 --> 01:18:54,880
and as climate is showing,

1734
01:18:54,880 --> 01:18:58,880
and as we will continue to be probably made aware,

1735
01:18:58,880 --> 01:19:01,880
we're kind of like just rearranging the deck chairs

1736
01:19:01,880 --> 01:19:03,880
on the Titanic.

1737
01:19:03,880 --> 01:19:05,880
And so while I appreciate your arguments,

1738
01:19:05,880 --> 01:19:07,880
and in a way maybe this is what you were saying at the beginning,

1739
01:19:07,880 --> 01:19:10,880
like it's impossible to save us from this problem.

1740
01:19:10,880 --> 01:19:12,880
Let's go work on another one

1741
01:19:12,880 --> 01:19:14,880
that we actually can do something about.

1742
01:19:14,880 --> 01:19:15,880
That's it, exactly.

1743
01:19:15,880 --> 01:19:18,880
And so part of the reason that I was really emphasizing

1744
01:19:18,880 --> 01:19:21,880
in the beginning is that if we can show that AGI alignment

1745
01:19:21,880 --> 01:19:25,880
in like 99% of the ways people are thinking about it,

1746
01:19:25,880 --> 01:19:27,880
is fundamentally and structurally impossible,

1747
01:19:27,880 --> 01:19:29,880
let's give up on that impossible goal

1748
01:19:29,880 --> 01:19:32,880
and actually do something that's not only possible but necessary.

1749
01:19:33,880 --> 01:19:35,880
So would it be a fair thing to say

1750
01:19:35,880 --> 01:19:40,880
that this non-relativistic ethics that you alluded to

1751
01:19:40,880 --> 01:19:45,880
could be thought of actually as just an abstract reification

1752
01:19:45,880 --> 01:19:48,880
of more of the ecosystem stability

1753
01:19:48,880 --> 01:19:51,880
that Allison was talking about?

1754
01:19:51,880 --> 01:19:52,880
Yes.

1755
01:19:54,880 --> 01:19:55,880
Okay.

1756
01:19:55,880 --> 01:19:57,880
All right.

1757
01:19:57,880 --> 01:20:00,880
That is actually the easiest question I've been asked today.

1758
01:20:00,880 --> 01:20:02,880
I mean, okay, so I'm sorry,

1759
01:20:02,880 --> 01:20:06,880
but we will have a very similar problem once we go out into space

1760
01:20:06,880 --> 01:20:08,880
and we have humans that we'll be competing for

1761
01:20:08,880 --> 01:20:10,880
for different types of resources, right?

1762
01:20:10,880 --> 01:20:12,880
Yes, we will.

1763
01:20:12,880 --> 01:20:15,880
And on my account, we are currently setting us up

1764
01:20:15,880 --> 01:20:17,880
for another kind of new evolutionary environment

1765
01:20:17,880 --> 01:20:20,880
just by the fact that people with different interests

1766
01:20:20,880 --> 01:20:22,880
will be going into space where there is resources

1767
01:20:22,880 --> 01:20:25,880
to compete and cooperate on.

1768
01:20:25,880 --> 01:20:29,880
And whatever game theory will emerge from that one

1769
01:20:29,880 --> 01:20:32,880
will also be, again, the new metaethics that we want to call it

1770
01:20:32,880 --> 01:20:35,880
and we will just have different language to be calling it such.

1771
01:20:35,880 --> 01:20:37,880
So I think setting up the initial conditions

1772
01:20:37,880 --> 01:20:40,880
such that cooperation becomes likely maybe good,

1773
01:20:40,880 --> 01:20:43,880
but then again, our current ethics are just a product

1774
01:20:43,880 --> 01:20:46,880
of the evolutionary kind of stable strategies

1775
01:20:46,880 --> 01:20:48,880
that arose in the environment that we were brought in.

1776
01:20:48,880 --> 01:20:52,880
So who's to say which of the ethics we should instill

1777
01:20:52,880 --> 01:20:54,880
for the long-term evolution of humanity?

1778
01:20:54,880 --> 01:20:57,880
And I think, actually, TJ had a point that is very, very relevant to this.

1779
01:20:57,880 --> 01:21:02,880
I just want to give her the opportunity to bring that in now, TJ.

1780
01:21:02,880 --> 01:21:04,880
You are unmuted.

1781
01:21:04,880 --> 01:21:05,880
Question?

1782
01:21:05,880 --> 01:21:08,880
Yes, but I think TJ has a build-up of that, actually,

1783
01:21:08,880 --> 01:21:10,880
which relates to value-driven.

1784
01:21:10,880 --> 01:21:14,880
So TJ, you are unmuted if you want to chime in.

1785
01:21:14,880 --> 01:21:16,880
Yeah, hey, right.

1786
01:21:16,880 --> 01:21:21,880
So I think, like, first of all, like, thanks for the presentation.

1787
01:21:21,880 --> 01:21:24,880
I think my question was more along the lines of,

1788
01:21:24,880 --> 01:21:29,880
it seems to me your claim is something like alignment as a static objective

1789
01:21:29,880 --> 01:21:32,880
is not, like, the right thing to think about.

1790
01:21:32,880 --> 01:21:37,880
And because there seems to be, like, inevitable drift

1791
01:21:37,880 --> 01:21:41,880
because of these inter-temporal noise things.

1792
01:21:41,880 --> 01:21:46,880
But it feels to me that, like, the noise argument itself

1793
01:21:46,880 --> 01:21:49,880
doesn't sound adequate to argue that, like,

1794
01:21:49,880 --> 01:21:53,880
the evolutionary trajectory would inevitably go into extinction.

1795
01:21:53,880 --> 01:21:58,880
Like, we do engage in a lot of co-evolutionary mutualistic couplings.

1796
01:21:58,880 --> 01:22:00,880
I never could just present that part.

1797
01:22:00,880 --> 01:22:05,880
So actually, like, connecting the question that you just asked

1798
01:22:05,880 --> 01:22:08,880
with the question that Allison presented,

1799
01:22:08,880 --> 01:22:12,880
there's another way of looking at this, which is just, like,

1800
01:22:12,880 --> 01:22:16,880
let's leave AI out of the picture and just talk about human beings.

1801
01:22:16,880 --> 01:22:22,880
It's possible to show that if you were to basically take another,

1802
01:22:22,880 --> 01:22:28,880
like, a group of people and put them on Mars and to establish a separate colony there

1803
01:22:28,880 --> 01:22:31,880
and to have that become essentially a full planet, full of humans,

1804
01:22:31,880 --> 01:22:34,880
and then this is this planet full of humans,

1805
01:22:34,880 --> 01:22:37,880
that the same sort of dynamics, like, when we're looking at,

1806
01:22:37,880 --> 01:22:39,880
it's not just a question of alignment.

1807
01:22:39,880 --> 01:22:43,880
There's other ways of expressing the notion of alignment.

1808
01:22:43,880 --> 01:22:48,880
Would it be the case that even a space-faring group of humans

1809
01:22:48,880 --> 01:22:52,880
not already coherent with the non-relativistic ethics,

1810
01:22:52,880 --> 01:22:56,880
does that even become anything less than complete and total cessation?

1811
01:22:56,880 --> 01:23:01,880
In other words, I would argue that as soon as you get a substantial number of people in space,

1812
01:23:01,880 --> 01:23:06,880
that it becomes so easy to, for example, weaponize asteroids

1813
01:23:06,880 --> 01:23:09,880
that at some point or another, over the long term,

1814
01:23:09,880 --> 01:23:12,880
that in the same sort of way as with nuclear proliferation,

1815
01:23:12,880 --> 01:23:15,880
we have an issue of, shit, we have all of these things,

1816
01:23:15,880 --> 01:23:17,880
and we only need one accident to start World War III,

1817
01:23:17,880 --> 01:23:19,880
and is that existential for civilization?

1818
01:23:19,880 --> 01:23:22,880
Well, it could be argued that it is.

1819
01:23:22,880 --> 01:23:28,880
And so in effect, it's a question of if we start, even as our own species,

1820
01:23:28,880 --> 01:23:34,880
multiplying the phenomena of our deployment through space

1821
01:23:34,880 --> 01:23:41,880
to the point that communication itself is limited by, in this particular case, speed of light considerations,

1822
01:23:41,880 --> 01:23:46,880
that again, you end up with essentially this separation of identity,

1823
01:23:46,880 --> 01:23:51,880
separation of time, such that the kinds of communication processes

1824
01:23:51,880 --> 01:23:55,880
that would be needed to essentially embody that ethics effectively

1825
01:23:55,880 --> 01:23:59,880
aren't sufficient to essentially establish it.

1826
01:23:59,880 --> 01:24:01,880
So in other words, either you have the ethics first,

1827
01:24:01,880 --> 01:24:04,880
and you live from that perspective, at which point everything's great,

1828
01:24:04,880 --> 01:24:08,880
or you don't have that ethics, but the nature of the interactions itself,

1829
01:24:08,880 --> 01:24:14,880
that the dynamics of those interactions don't have enough time for the ethics to emerge.

1830
01:24:14,880 --> 01:24:18,880
It's essentially the species instincts itself before it gets a chance

1831
01:24:18,880 --> 01:24:20,880
to really understand what those ethics are.

1832
01:24:20,880 --> 01:24:23,880
Well, that I think depends on the background conditions that you set up, right?

1833
01:24:23,880 --> 01:24:27,880
And this reminds me so much of Chris Carlson's presentation in the last session,

1834
01:24:27,880 --> 01:24:32,880
because I think on my clock, as long as you are able to set up a system

1835
01:24:32,880 --> 01:24:36,880
in which only voluntary interactions are allowed between different entities,

1836
01:24:36,880 --> 01:24:40,880
whether they're humans or AIs, as long as you can't destroy the,

1837
01:24:40,880 --> 01:24:43,880
let's say, the base layer on which they take place,

1838
01:24:43,880 --> 01:24:47,880
then over time, entities, whatever they are, will engage in those interactions

1839
01:24:47,880 --> 01:24:49,880
that are in their interest.

1840
01:24:49,880 --> 01:24:53,880
And over that, if it's voluntary, and you kind of like game theoretical

1841
01:24:53,880 --> 01:24:55,880
metaethics could arise at the end of it,

1842
01:24:55,880 --> 01:25:01,880
but I think as long as you can set the base layer such that you don't have destruction

1843
01:25:01,880 --> 01:25:07,880
and that different entities that pursue different goals can decide whether to engage or not,

1844
01:25:07,880 --> 01:25:12,880
then I think what comes out of that is inherently something that is not self-destructive.

1845
01:25:12,880 --> 01:25:14,880
But I think those are the things that you have to divide.

1846
01:25:14,880 --> 01:25:16,880
You don't have to get the ethics right from the start.

1847
01:25:16,880 --> 01:25:18,880
You don't have to get the base layer, right?

1848
01:25:18,880 --> 01:25:22,880
We're just enabling only voluntary interactions and non-destruction of the base layer.

1849
01:25:22,880 --> 01:25:27,880
The nature of the proof is to show categorically that no such conditions can be created.

1850
01:25:27,880 --> 01:25:30,880
So in other words, if you're saying I'm trying to create an environment,

1851
01:25:30,880 --> 01:25:37,880
a baseline environment that would allow for these mutual dynamics to essentially be peaceable,

1852
01:25:37,880 --> 01:25:45,880
what I'm basically contesting is the assumption that such an environment can be defined in any a priori way.

1853
01:25:45,880 --> 01:25:50,880
That may be right, but like, you know, shouldn't it be better of just trying, you know,

1854
01:25:50,880 --> 01:25:55,880
and I think that ties into a set as well.

1855
01:25:55,880 --> 01:26:01,880
If you have one chance to do something and that one chance essentially means that if you mess it up,

1856
01:26:01,880 --> 01:26:03,880
it messes it up for all future time.

1857
01:26:03,880 --> 01:26:08,880
That's a very different, that's not an experiment. That's a choice.

1858
01:26:08,880 --> 01:26:11,880
You believe we know enough to make such a proof today?

1859
01:26:11,880 --> 01:26:17,880
I believe that we know enough to essentially establish categorical proofs of certain kind.

1860
01:26:17,880 --> 01:26:21,880
We can do proofs of existence and non-existence of certain types.

1861
01:26:21,880 --> 01:26:25,880
Such proofs have often turned out to be flawed with new information.

1862
01:26:25,880 --> 01:26:29,880
This is true, but on the other hand, that's part of the reason why we're looking at a categorical process.

1863
01:26:29,880 --> 01:26:32,880
I'm looking at it more from a mathematical perspective rather than just a physical one.

1864
01:26:32,880 --> 01:26:34,880
Even those.

1865
01:26:34,880 --> 01:26:37,880
Well, yeah, I suppose you're right.

1866
01:26:37,880 --> 01:26:41,880
Mathematics occasionally is shown that the proofs that are established in that space are wrong.

1867
01:26:41,880 --> 01:26:45,880
And this is part of the reason why we have conversations like this, but it takes a while.

1868
01:26:45,880 --> 01:26:50,880
The proofs are proofs of negatives.

1869
01:26:50,880 --> 01:26:56,880
All right, so we are now at 12.29.

1870
01:26:56,880 --> 01:27:03,880
And I think in the initial discussion that I have with Forestu to initiate this call,

1871
01:27:03,880 --> 01:27:07,880
he was definitely incinerating that this takes much more than one discussion.

1872
01:27:07,880 --> 01:27:12,880
And I think we really opened up folks at Pandora now are discussing about

1873
01:27:12,880 --> 01:27:16,880
going from impossibility to like, how could we actually solve AI alive?

1874
01:27:16,880 --> 01:27:22,880
And so I think we should weigh past the goal of the session.

1875
01:27:22,880 --> 01:27:25,880
I want to thank everyone for attending.

1876
01:27:25,880 --> 01:27:29,880
I want to thank you for so much for laying out your argument.

1877
01:27:29,880 --> 01:27:33,880
I want to thank everyone else for being such active stewards of the conversation.

1878
01:27:33,880 --> 01:27:39,880
I thought it was like, I mean, we definitely had really, really, I think, tricky and hairy discussion topics today.

1879
01:27:39,880 --> 01:27:45,880
And I thought that nevertheless, we were able to keep it in a way where a discussion was actually possible.

1880
01:27:45,880 --> 01:27:53,880
So maybe that is a better way of saying, even if our interests are very different and where we're coming from to discussion are very different.

1881
01:27:53,880 --> 01:27:55,880
We can still cooperate on things, right?

1882
01:27:55,880 --> 01:28:02,880
Okay, this is my, you know, wavy, meta way of saying, yay, maybe we're not all that doomed.

1883
01:28:02,880 --> 01:28:06,880
But for now, thank you so, so much for us and joining.

1884
01:28:06,880 --> 01:28:08,880
Thank you so much everyone for your really active participation.

1885
01:28:08,880 --> 01:28:10,880
I really, really enjoyed this.

1886
01:28:10,880 --> 01:28:13,880
I'm hoping that we can continue the discussion for us.

1887
01:28:13,880 --> 01:28:21,880
I'm hoping that you share with me a few topics that you want me to send out to others afterwards to follow up,

1888
01:28:21,880 --> 01:28:24,880
because a few people have asked for your writing.

1889
01:28:24,880 --> 01:28:29,880
At the same time, I think I will share the chat with you, which may provide you really good feedback.

1890
01:28:29,880 --> 01:28:33,880
I think it's impossible to speak while monitoring the chat, so I'll do that too.

1891
01:28:33,880 --> 01:28:34,880
Yes, no, definitely not.

1892
01:28:34,880 --> 01:28:35,880
It's not required.

1893
01:28:35,880 --> 01:28:42,880
For that, we need corporations with better AIs so that they can benefit us in that day.

1894
01:28:42,880 --> 01:28:44,880
Here I'm really at the end of it.

1895
01:28:44,880 --> 01:28:51,880
I just wanted to mention that next week we will be meeting on decentralized decision architectures as a response to COVID-19.

1896
01:28:51,880 --> 01:28:59,880
And then the weekend afterward, we will have another AIs session, this time with Dan Elton, who gave his remarks earlier here today,

1897
01:28:59,880 --> 01:29:04,880
who just hopped off and he will be discussing one of his proposals to AI alignment.

1898
01:29:04,880 --> 01:29:07,880
And then we have two others that will be joining us.

1899
01:29:07,880 --> 01:29:11,880
Aria, I'm not sure if he was joining today, but he will be presenting too.

1900
01:29:11,880 --> 01:29:13,880
So we have the next two salons already planned.

1901
01:29:13,880 --> 01:29:16,880
Again, always on Thursdays at 11 a.m.

1902
01:29:16,880 --> 01:29:17,880
So this is enough for me now.

1903
01:29:17,880 --> 01:29:20,880
I'm going to close it out on my end in Forest.

1904
01:29:20,880 --> 01:29:26,880
Dear Dark Forest, you have to find the words and I'm already opening up the invitations.

1905
01:29:26,880 --> 01:29:31,880
First of all, I'm just super thrilled to have been able to speak with all of you today.

1906
01:29:31,880 --> 01:29:36,880
I hope that I've raised interesting questions and new ways of thinking about things.

1907
01:29:36,880 --> 01:29:42,880
I'm sure that with every single one of you, I could have a long and very fruitful and interesting conversation,

1908
01:29:42,880 --> 01:29:48,880
and that we could effectively start to really get at some of the meat of the matter in this space.

1909
01:29:48,880 --> 01:29:56,880
But as an introduction, I felt very well received and just glad to have the opportunity to meet so many new folks.

1910
01:29:56,880 --> 01:29:58,880
So thank you very much.

