WEBVTT

00:00.000 --> 00:03.200
So I wouldn't be surprised if we had AGI-like systems

00:03.200 --> 00:04.360
within the next decade.

00:04.360 --> 00:06.400
It was pretty surprising to almost everyone,

00:06.400 --> 00:08.880
including the people who first worked on the scaling

00:08.880 --> 00:10.880
hypotheses, that how far it's gone.

00:10.880 --> 00:13.400
In a way, I look at the large models today,

00:13.400 --> 00:14.920
and I think they're almost unreasonably

00:14.920 --> 00:16.240
effective for what they are.

00:16.240 --> 00:18.760
It's an empirical question whether that will hit an asymptote

00:18.760 --> 00:19.760
or a brick wall.

00:19.760 --> 00:21.080
I think no one knows.

00:21.080 --> 00:23.000
When you think about superhuman intelligence,

00:23.000 --> 00:25.600
is it still controlled by a private company?

00:25.600 --> 00:28.040
As Gemini becoming more multimodal,

00:28.040 --> 00:30.240
and we start ingesting audio-visual data,

00:30.240 --> 00:32.840
as well as text data, I do think our systems

00:32.840 --> 00:36.280
are going to start to understand the physics of the real world

00:36.280 --> 00:36.920
better.

00:36.920 --> 00:38.560
The world's about to become very exciting,

00:38.560 --> 00:40.160
I think, in the next few years as we start

00:40.160 --> 00:44.240
getting used to the idea of what true multimodality means.

00:44.240 --> 00:48.240
OK, today it is a true honor to speak with Demis Isavis, who

00:48.240 --> 00:50.240
is the CEO of DeepMind.

00:50.240 --> 00:51.640
Demis, welcome to the podcast.

00:51.640 --> 00:52.720
Thanks for having me.

00:52.720 --> 00:55.480
First question, given your neuroscience background,

00:55.480 --> 00:56.880
how do you think about intelligence?

00:56.920 --> 01:00.000
Specifically, do you think it's like one higher-level general

01:00.000 --> 01:01.480
reasoning circuit, or do you think

01:01.480 --> 01:05.560
it's thousands of independent sub-scales and heuristics?

01:05.560 --> 01:09.880
Well, it's interesting because intelligence is so broad,

01:09.880 --> 01:14.760
and what we use it for is so generally applicable.

01:14.760 --> 01:17.120
I think that suggests that there must

01:17.120 --> 01:21.840
be some sort of high-level common things,

01:21.840 --> 01:24.160
common algorithmic themes, I think,

01:24.160 --> 01:27.320
around how the brain processes the world around us.

01:27.320 --> 01:31.680
So of course, then there are specialized parts of the brain

01:31.680 --> 01:34.080
that do specific things.

01:34.080 --> 01:36.440
But I think there are probably some underlying principles

01:36.440 --> 01:37.800
that underpin all of that.

01:37.800 --> 01:40.680
Yeah, how do you make sense of the fact that in these LLMs,

01:40.680 --> 01:42.720
though, when you give them a lot of data

01:42.720 --> 01:44.160
in any specific domain, they tend

01:44.160 --> 01:47.680
to get asymmetrically better in that domain?

01:47.680 --> 01:50.040
Wouldn't we expect a general improvement

01:50.040 --> 01:51.520
across all the different areas?

01:51.520 --> 01:53.600
Well, I think you first of all, I think you do actually

01:53.640 --> 01:56.480
sometimes get surprising improvement in other domains

01:56.480 --> 01:58.200
when you improve in a specific domain.

01:58.200 --> 02:01.680
So for example, when these large models sort of improve

02:01.680 --> 02:05.280
at coding, that can actually improve their general reasoning.

02:05.280 --> 02:07.520
So there is some evidence of some transfer,

02:07.520 --> 02:11.600
though I think we would like a lot more evidence of that.

02:11.600 --> 02:14.360
But also, that's how the human brain learns, too,

02:14.360 --> 02:17.480
is if we experience and practice a lot of things like chess

02:17.480 --> 02:20.120
or writing, creative writing, or whatever that is,

02:20.120 --> 02:22.200
we also tend to specialize and get better

02:22.200 --> 02:23.440
at that specific thing.

02:23.440 --> 02:26.600
Even though we're using sort of general learning techniques

02:26.600 --> 02:28.520
and general learning systems in order

02:28.520 --> 02:31.080
to get good at that domain.

02:31.080 --> 02:33.160
Yeah, well, what's been the most surprising example

02:33.160 --> 02:34.960
of this kind of transfer for you?

02:34.960 --> 02:37.840
Like, you see language and code or images and text?

02:37.840 --> 02:40.080
Yeah, I think probably, I mean, I'm

02:40.080 --> 02:42.760
hoping we're going to see a lot more of this China transfer.

02:42.760 --> 02:46.600
But I think things like getting better at coding and math

02:46.600 --> 02:48.920
and generally improving your reasoning,

02:48.920 --> 02:51.520
that is how it works with us as human learners.

02:51.520 --> 02:53.000
But I think it's interesting seeing

02:53.000 --> 02:55.880
that in these artificial systems.

02:55.880 --> 02:59.120
And can you see the sort of mechanistic way in which,

02:59.120 --> 03:00.720
let's say in the language and code example,

03:00.720 --> 03:03.120
there's like, I found a place in a neural network that's

03:03.120 --> 03:04.920
getting better with both the language and the code.

03:04.920 --> 03:07.000
Is that too far down the weeds?

03:07.000 --> 03:09.840
Yeah, well, I don't think our analysis techniques

03:09.840 --> 03:13.480
are quite sophisticated enough to be able to hone in on that.

03:13.480 --> 03:15.080
I think that's actually one of the areas

03:15.080 --> 03:17.600
that a lot more research needs to be done

03:17.600 --> 03:20.640
on kind of mechanistic analysis of the representations

03:20.680 --> 03:21.920
that these systems build up.

03:21.920 --> 03:25.360
And I sometimes like to call it virtual brain analytics.

03:25.360 --> 03:29.360
In a way, it's a bit like doing fMRI or a single cell

03:29.360 --> 03:31.880
recording from a real brain.

03:31.880 --> 03:34.640
What's the analogous sort of analysis techniques

03:34.640 --> 03:36.320
for these artificial minds?

03:36.320 --> 03:38.560
And there's a lot of great work going on

03:38.560 --> 03:39.400
on this sort of stuff.

03:39.400 --> 03:41.840
People like Chris Ola, I really like his work.

03:41.840 --> 03:44.040
And a lot of computational neuroscience techniques,

03:44.040 --> 03:46.040
I think, could be brought to bear

03:46.040 --> 03:48.560
on analyzing these current systems we're building.

03:48.560 --> 03:50.840
In fact, I try to encourage a lot of my computational

03:50.840 --> 03:54.000
neuroscience friends to start thinking in that direction

03:54.000 --> 03:58.440
and applying their know-how to the large models.

03:58.440 --> 04:01.680
Yeah, what do other researchers not understand

04:01.680 --> 04:05.200
about human intelligence that you have some sort of insight

04:05.200 --> 04:06.680
on, given your neuroscience background?

04:06.680 --> 04:10.600
I think neuroscience has added a lot.

04:10.600 --> 04:12.760
If you look at the last sort of 10, 20 years

04:12.760 --> 04:14.480
that we've been at it, at least.

04:14.480 --> 04:18.080
And I've been thinking about this for 30 plus years.

04:18.080 --> 04:21.760
I think in the earlier days of this sort of new wave of AI,

04:21.760 --> 04:24.760
I think neuroscience was providing a lot of interesting

04:24.760 --> 04:26.160
directional clues.

04:26.160 --> 04:28.000
So things like reinforcement learning,

04:28.000 --> 04:29.640
combining that with deep learning,

04:29.640 --> 04:31.640
some of our pioneering work we did there,

04:31.640 --> 04:35.400
things like experience replay, even the notion of attention,

04:35.400 --> 04:37.840
which has become super important.

04:37.840 --> 04:41.240
A lot of those original sort of inspirations

04:41.240 --> 04:44.040
come from some understanding about how the brain works.

04:44.040 --> 04:45.200
Not the exact specifics.

04:45.200 --> 04:47.280
Of course, one's an engineered system.

04:47.280 --> 04:48.440
The other one's a natural system.

04:48.440 --> 04:50.840
So it's not so much about a one-to-one mapping

04:50.840 --> 04:52.240
of a specific algorithm.

04:52.240 --> 04:54.480
It's more kind of inspirational direction,

04:54.480 --> 04:57.240
maybe some ideas for architecture or algorithmic ideas

04:57.240 --> 04:59.160
or representational ideas.

04:59.160 --> 05:01.640
And because you know the brains in existence

05:01.640 --> 05:04.200
prove that general intelligence is possible at all,

05:04.200 --> 05:07.160
I think the history of human endeavors

05:07.160 --> 05:09.680
has been that once you know something's possible,

05:09.680 --> 05:11.760
it's easier to push hard in that direction.

05:11.760 --> 05:14.080
Because you know it's a question of effort then

05:14.080 --> 05:17.080
and sort of a question of when, not if.

05:17.080 --> 05:19.440
And that allows you to I think make progress

05:19.440 --> 05:20.520
a lot more quickly.

05:20.520 --> 05:23.240
So I think neurosciences has had a lot of,

05:24.680 --> 05:27.000
has inspired a lot of the thinking,

05:27.000 --> 05:30.080
at least in a soft way behind where we are today.

05:31.000 --> 05:33.400
But as for going forwards,

05:33.400 --> 05:36.960
I think that there's still a lot of interesting things

05:36.960 --> 05:38.760
to be resolved around planning

05:38.760 --> 05:42.560
and how does the brain construct the right world models.

05:43.680 --> 05:47.060
I studied for example, how the brain does imagination,

05:47.060 --> 05:50.020
or you can think of it as a mental simulation.

05:50.020 --> 05:54.260
So how do we create very rich visual spatial simulations

05:54.260 --> 05:56.100
of the world in order for us to plan better?

05:56.100 --> 05:57.540
Yeah, actually I'm curious how you think

05:57.540 --> 05:59.340
that will sort of interface with LLM.

05:59.340 --> 06:01.500
So obviously DeepMinders are the frontier

06:01.500 --> 06:03.340
and has been for many years,

06:03.340 --> 06:05.140
systems like AlphaZero and so forth,

06:05.140 --> 06:06.980
of having these agents who can like think through

06:06.980 --> 06:09.460
different steps to get to an end outcome.

06:09.460 --> 06:10.820
All right, will this just be,

06:10.820 --> 06:13.020
is a path for LLMs to have this sort of

06:13.020 --> 06:14.740
tree search kind of thing on top of them?

06:14.740 --> 06:15.780
How do you think about this?

06:15.780 --> 06:18.860
I think that's a super promising direction in my opinion.

06:18.860 --> 06:22.740
So we've got to carry on improving the large models

06:22.740 --> 06:25.820
and we've got to carry on basically making

06:25.820 --> 06:28.020
the more and more accurate predictors of the world.

06:28.020 --> 06:30.300
So in effect, making them more and more reliable

06:30.300 --> 06:32.580
world models, that's clearly unnecessary,

06:32.580 --> 06:34.740
but I would say probably not sufficient component

06:34.740 --> 06:36.500
of an AGI system.

06:36.500 --> 06:37.820
And then on top of that,

06:37.820 --> 06:41.140
I would, we're working on things like AlphaZero,

06:41.140 --> 06:44.700
like planning mechanisms on top that make use of that model

06:44.700 --> 06:46.580
in order to make concrete plans

06:46.580 --> 06:48.900
to achieve certain goals in the world

06:48.900 --> 06:52.780
and perhaps sort of chain thought together

06:52.780 --> 06:54.380
or lines of reasoning together

06:54.380 --> 06:56.740
and maybe use search to kind of explore

06:56.740 --> 06:58.380
massive spaces of possibility.

06:58.380 --> 06:59.860
I think that's kind of missing

06:59.860 --> 07:01.980
from our current large models.

07:01.980 --> 07:05.860
How do you get past the sort of immense amount of compute

07:05.860 --> 07:07.220
that these approaches tend to require?

07:07.220 --> 07:11.500
So even the AlphaGo system was a pretty expensive system

07:11.540 --> 07:14.740
because you had to do the sort of running an LLM on each node

07:14.740 --> 07:16.100
of the tree.

07:16.100 --> 07:17.660
How do you anticipate that will get more,

07:17.660 --> 07:18.500
made more efficient?

07:18.500 --> 07:22.820
Well, I mean, one thing is Moore's law tends to help

07:22.820 --> 07:27.620
if every year, of course, more computation comes in,

07:27.620 --> 07:30.060
but we focus a lot on efficient,

07:30.060 --> 07:34.460
sample efficient methods and reusing existing data,

07:34.460 --> 07:36.220
things like experience replay,

07:36.220 --> 07:39.140
and also just looking at more efficient ways.

07:39.140 --> 07:40.980
I mean, the better your world model is,

07:40.980 --> 07:42.660
the more efficient your search can be.

07:42.660 --> 07:44.620
So one example I always get with AlphaZero,

07:44.620 --> 07:47.620
our system to play Go and chess and any game,

07:47.620 --> 07:51.340
is that it's stronger than world champion level,

07:51.340 --> 07:53.940
human world champion level at all these games.

07:53.940 --> 07:58.020
And it uses a lot less search than a brute force method

07:58.020 --> 07:59.940
like Deep Blue, say to play chess.

07:59.940 --> 08:02.460
Deep Blue, one of these traditional stockfish

08:02.460 --> 08:05.380
or Deep Blue systems would maybe look at millions

08:05.380 --> 08:08.740
of possible moves for every decision it's gonna make.

08:08.740 --> 08:11.660
AlphaZero and AlphaGo made, you know,

08:11.660 --> 08:15.860
looked at around tens of thousands of possible positions

08:15.860 --> 08:18.260
in order to make a decision about what to move next.

08:18.260 --> 08:21.500
But a human grandmaster, a human world champion,

08:21.500 --> 08:23.980
probably only looks at a few hundreds of moves,

08:23.980 --> 08:25.020
even the top ones,

08:25.020 --> 08:27.980
in order to make their very good decision

08:27.980 --> 08:29.260
about what to play next.

08:29.260 --> 08:32.500
So that suggests that obviously the brute force systems

08:32.500 --> 08:35.020
don't have any real model other than heuristics

08:35.020 --> 08:36.100
about the game.

08:36.140 --> 08:39.660
AlphaZero has quite a decent model,

08:39.660 --> 08:41.860
but the human, you know,

08:41.860 --> 08:44.020
top human players have a much richer,

08:44.020 --> 08:46.860
much more accurate model than of Go or chess.

08:46.860 --> 08:48.500
So that allows them to make, you know,

08:48.500 --> 08:51.780
world class decisions on a very small amount of search.

08:51.780 --> 08:52.860
So I think there's still,

08:52.860 --> 08:54.180
there's a sort of trade off there,

08:54.180 --> 08:56.020
like, you know, if you improve the models,

08:56.020 --> 08:58.420
then I think your search can be more efficient

08:58.420 --> 09:00.180
and therefore you can get further with your search.

09:00.180 --> 09:02.500
Yeah, I have two questions based on that.

09:02.500 --> 09:04.220
The first being with AlphaZero,

09:04.220 --> 09:06.900
you had a very concrete win condition of, you know,

09:06.900 --> 09:07.740
at the end of the day,

09:07.740 --> 09:08.580
do I win this game or not?

09:08.580 --> 09:10.420
And you can reinforce on that.

09:10.420 --> 09:12.420
When you're just thinking of like an LLM,

09:12.420 --> 09:13.500
putting out thought,

09:13.500 --> 09:15.780
what will, do you think there'll be this kind of ability

09:15.780 --> 09:17.420
to discriminate in the end,

09:17.420 --> 09:19.980
whether that was like a good thing to reward or not?

09:19.980 --> 09:21.500
Well, of course that's why we, you know,

09:21.500 --> 09:23.740
we pioneered and DeepMind sort of famous

09:23.740 --> 09:26.940
for using games as a proving ground,

09:26.940 --> 09:28.620
partly because obviously it's efficient

09:28.620 --> 09:30.100
to research in that domain.

09:30.100 --> 09:31.980
But the other reason is obviously it's, you know,

09:31.980 --> 09:34.060
extremely easy to specify a reward function,

09:34.060 --> 09:35.740
winning the game or improving the score,

09:35.740 --> 09:38.100
something like that sort of built into most games.

09:38.100 --> 09:41.940
So that is one of the challenges of real world systems

09:41.940 --> 09:44.860
is how does one define the right objective function,

09:44.860 --> 09:47.540
the right reward function and the right goals

09:47.540 --> 09:51.500
and specify them in a, you know, in a general way,

09:51.500 --> 09:52.660
but that's specific enough

09:52.660 --> 09:55.660
and actually points the system in the right direction.

09:55.660 --> 09:58.940
And for real world problems, that can be a lot harder.

09:58.940 --> 10:01.260
But actually, if you think about it

10:01.260 --> 10:03.420
in even scientific problems,

10:03.420 --> 10:05.580
there are usually ways that you can specify

10:05.580 --> 10:07.220
the goal that you're after.

10:07.220 --> 10:08.660
And then when you think about human intelligence,

10:08.660 --> 10:09.860
you're just saying, well, you know,

10:09.860 --> 10:10.940
the humans thinking about these thoughts

10:10.940 --> 10:12.860
are just super sample efficient.

10:12.860 --> 10:15.140
How do you, I understand coming up with relativity, right?

10:15.140 --> 10:16.900
There's just like thousands of possible permutations

10:16.900 --> 10:17.980
of the equations.

10:17.980 --> 10:19.660
Do you think it's also this sort of sense

10:19.660 --> 10:20.980
of like different heuristics of like,

10:20.980 --> 10:22.340
I'm going to try out this approach instead of this

10:22.340 --> 10:25.300
or is it a totally different way of approaching

10:25.300 --> 10:27.740
coming up with a solution than, you know,

10:27.740 --> 10:29.340
what AlphaGo does to plan the next move?

10:29.340 --> 10:30.740
Yeah, well, look, I think it's different

10:30.740 --> 10:32.500
because our brains are not built

10:32.540 --> 10:35.220
for doing Monte Carlo research, right?

10:35.220 --> 10:39.980
It's just not the way our organic brains would work.

10:39.980 --> 10:42.740
So I think that in order to compensate for that,

10:42.740 --> 10:44.860
you know, people like Einstein have come up, you know,

10:44.860 --> 10:47.140
their brains have using their intuition

10:47.140 --> 10:49.260
and, you know, we maybe come to what intuition is,

10:49.260 --> 10:51.420
but they use their sort of knowledge

10:51.420 --> 10:54.540
and their experience to build extremely, you know,

10:54.540 --> 10:57.500
in Einstein's case, extremely accurate models of physics,

10:57.500 --> 10:59.860
including these sort of mental simulations.

10:59.860 --> 11:01.180
I think if you read about Einstein

11:01.180 --> 11:03.620
and how he came up with things, he used to visualize

11:03.620 --> 11:07.780
and sort of really kind of feel

11:07.780 --> 11:09.740
what these physical systems should be like,

11:09.740 --> 11:10.980
not just the mathematics of it,

11:10.980 --> 11:12.780
but have a really intuitive feel

11:12.780 --> 11:14.500
for what they would be like in reality.

11:14.500 --> 11:17.580
And that allowed him to think these sort of very outlandish

11:17.580 --> 11:19.140
thoughts at the time.

11:19.140 --> 11:21.740
So I think that it's the sophistication

11:21.740 --> 11:23.420
of the world models that we're building,

11:23.420 --> 11:25.940
which then, you know, if you imagine your world model

11:25.940 --> 11:29.260
can get you to a certain node in a tree that you're searching,

11:29.260 --> 11:31.300
and then you just do a little bit of search

11:31.300 --> 11:33.580
around that node, that leaf node,

11:33.580 --> 11:35.940
and that gets you to these original places.

11:35.940 --> 11:37.940
But obviously, if your model is,

11:37.940 --> 11:41.020
and your judgment on that model is very, very good,

11:41.020 --> 11:43.980
then you can pick which leaf nodes you should sort of expand

11:43.980 --> 11:45.900
with search much more accurately.

11:45.900 --> 11:48.140
So therefore, overall, you do a lot less search.

11:48.140 --> 11:49.900
I mean, there's no way that, you know,

11:49.900 --> 11:52.460
any human could do a kind of brute force search

11:52.460 --> 11:54.580
over any kind of significant space.

11:54.580 --> 11:55.940
Yeah, yeah, yeah.

11:55.940 --> 11:57.540
A big sort of open question right now

11:57.540 --> 12:00.620
is whether RL will allow these models to do the self-placed

12:00.620 --> 12:02.540
synthetic data to get over the data bottleneck.

12:02.540 --> 12:04.060
It sounds like you're optimistic about this.

12:04.060 --> 12:05.580
Yeah, I'm very optimistic about that.

12:05.580 --> 12:07.620
I mean, I think, well, first of all,

12:07.620 --> 12:09.780
there's still a lot more data, I think, that can be used,

12:09.780 --> 12:12.300
especially if one views like multimodal and video

12:12.300 --> 12:13.300
and these kind of things.

12:13.300 --> 12:16.700
And obviously, you know, society's adding more data

12:16.700 --> 12:18.580
all the time.

12:18.580 --> 12:21.580
But I think to the internet and things like that.

12:21.580 --> 12:23.980
But I think that there's a lot of scope

12:23.980 --> 12:26.140
for creating synthetic data.

12:26.140 --> 12:29.620
We're looking at different ways partly through simulation

12:29.620 --> 12:32.100
and using very realistic games environments,

12:32.100 --> 12:35.260
for example, to generate realistic data,

12:35.260 --> 12:37.100
but also self-play.

12:37.100 --> 12:41.300
So that's where systems interact with each other

12:41.300 --> 12:43.460
or converse with each other.

12:43.460 --> 12:44.780
And in the sense of, you know,

12:44.780 --> 12:46.620
work very well for us with AlphaGo and AlphaZero,

12:46.620 --> 12:49.100
where we got the systems to play against each other

12:49.100 --> 12:50.940
and actually learn from each other's mistakes

12:50.940 --> 12:52.900
and build up a knowledge base that way.

12:52.900 --> 12:54.820
And I think there are some good analogies for that.

12:54.820 --> 12:55.980
It's a little bit more complicated,

12:55.980 --> 12:59.820
but to build a general kind of world data.

12:59.820 --> 13:01.820
How do you get to the point where these models,

13:01.820 --> 13:03.900
the sort of synthetic data they're outputting

13:03.900 --> 13:05.460
and their self-play they're doing,

13:05.460 --> 13:07.500
is not just more of what they've already got

13:07.500 --> 13:08.340
in their data set,

13:08.340 --> 13:10.500
but is something they haven't seen before?

13:10.500 --> 13:12.260
You know what I mean, to actually improve the abilities.

13:12.260 --> 13:15.940
Yeah, so there, I think there's a whole science needed.

13:15.940 --> 13:17.940
And I think we're still in the nascent stage of this

13:17.940 --> 13:20.020
of data curation and data analysis.

13:20.020 --> 13:22.340
So actually analyzing the holes

13:22.340 --> 13:24.300
that you have in your data distribution.

13:24.300 --> 13:26.580
And this is important for things like fairness and bias

13:26.580 --> 13:28.980
and other stuff to remove that from the system is to,

13:28.980 --> 13:31.500
is to try and really make sure that your data set

13:31.500 --> 13:33.020
is representative of the distribution

13:33.020 --> 13:34.220
you're trying to learn.

13:34.220 --> 13:36.540
And, you know, there are many tricks there.

13:36.540 --> 13:38.020
One can use like over-weighting

13:38.020 --> 13:40.020
or replaying certain parts of the data.

13:40.020 --> 13:42.540
Or you could imagine if you identify some,

13:42.540 --> 13:44.020
some gap in your data set,

13:44.020 --> 13:46.820
that's where you put your synthetic generation capabilities

13:46.820 --> 13:47.660
to work on.

13:47.660 --> 13:50.580
Yeah, so, you know, nowadays people are paying attention

13:50.580 --> 13:55.420
to the RL stuff that Alfa deep-minded many years before.

13:55.420 --> 13:58.060
What are the sort of either early research directions

13:58.060 --> 13:59.860
or something that was done way back in the past,

13:59.860 --> 14:01.820
but people just haven't been paying attention to,

14:01.820 --> 14:03.100
that you think will be a big deal, right?

14:03.100 --> 14:04.700
Like there's a time where people weren't paying attention

14:04.700 --> 14:05.540
to scaling.

14:05.540 --> 14:07.060
What's the thing now where it's like totally underrated?

14:07.060 --> 14:08.500
Well, actually, I think that, you know,

14:08.500 --> 14:11.100
there's the history of the sort of last couple of decades

14:11.100 --> 14:13.300
has been things coming in and out of fashion, right?

14:13.300 --> 14:15.980
And I do feel like a while ago,

14:15.980 --> 14:17.500
when, you know, maybe five plus years ago,

14:17.500 --> 14:19.220
when we were pioneering with AlphaGo

14:19.220 --> 14:22.420
and before that, DQN, where it was the first system,

14:22.420 --> 14:23.780
you know, that worked on Atari,

14:23.780 --> 14:26.780
about how first big system really more than 10 years ago now

14:26.780 --> 14:29.660
that scaled up Q learning and reinforcement learning techniques

14:29.660 --> 14:32.140
to deal, you know, combine that with deep learning

14:32.140 --> 14:34.020
to create deep reinforcement learning

14:34.020 --> 14:37.260
and then use that to scale up to complete some,

14:37.260 --> 14:39.180
you know, master some pretty complex tasks

14:39.180 --> 14:41.380
like playing Atari games just from the pixels.

14:41.380 --> 14:45.500
And I do actually think a lot of those ideas

14:45.500 --> 14:46.940
need to come back in again.

14:46.940 --> 14:48.100
And as we talked about earlier,

14:48.100 --> 14:51.540
combine it with the new advances in large models

14:51.540 --> 14:52.740
and large multimodal models,

14:52.740 --> 14:54.060
which is obviously very exciting as well.

14:54.060 --> 14:56.460
So I do think there's a lot of potential

14:56.460 --> 14:59.460
for combining some of those older ideas together

14:59.460 --> 15:00.460
with the newer ones.

15:00.460 --> 15:03.460
Is there any potential for something to come,

15:03.460 --> 15:06.380
the AGI to eventually come from just a pure RRL approach?

15:06.380 --> 15:07.940
Like the way we're talking about it,

15:07.940 --> 15:09.700
it sounds like there'll be,

15:09.700 --> 15:12.140
the LLM will form the gripe fryer

15:12.140 --> 15:13.820
and then this sort of research will go on top of that.

15:13.820 --> 15:15.460
Or is it a possibility to just like completely

15:15.460 --> 15:16.300
out of the dark?

15:16.300 --> 15:17.220
I think I certainly, you know,

15:17.220 --> 15:18.940
theoretically I think there's no reason

15:18.940 --> 15:21.500
why you couldn't go full alpha zero like on it.

15:21.500 --> 15:25.340
And there are some people here at Google DeepMind

15:25.340 --> 15:28.140
and in the RL community who work on that, right?

15:28.140 --> 15:32.540
And fully assuming no priors, no data

15:32.540 --> 15:35.780
and just build all knowledge from scratch.

15:35.780 --> 15:38.620
And I think that's valuable because of course, you know,

15:38.620 --> 15:40.500
those ideas and those algorithms

15:40.500 --> 15:43.420
should also work when you have some knowledge too.

15:43.420 --> 15:44.260
But having said that,

15:44.260 --> 15:46.380
I think by far probably my betting

15:46.380 --> 15:48.220
would be the quickest way to get to AGI

15:48.220 --> 15:49.820
in the most likely plausible way

15:49.820 --> 15:51.660
is to use all the knowledge

15:51.660 --> 15:53.180
that's existing in the world right now

15:53.180 --> 15:55.140
on things like the web and that we've collected

15:55.140 --> 15:57.660
and we have these scalable algorithms

15:57.660 --> 16:00.740
like transformers that are capable

16:00.740 --> 16:03.020
of ingesting all of that information.

16:03.020 --> 16:06.660
And I don't see why you wouldn't start with a model

16:06.660 --> 16:09.820
as a kind of prior or to build on and to make predictions

16:09.820 --> 16:11.860
that helps bootstrap your learning.

16:11.860 --> 16:13.980
I just think it doesn't make sense

16:13.980 --> 16:15.260
not to make use of that.

16:15.300 --> 16:18.820
So my betting would be is that, you know,

16:18.820 --> 16:23.460
the final AGI system will have these large multimodals

16:23.460 --> 16:26.180
as part of the overall solution

16:26.180 --> 16:28.500
but probably won't be enough on their own.

16:28.500 --> 16:31.260
You will need this additional planning search on top.

16:31.260 --> 16:32.780
Okay, this sounds like the answer

16:32.780 --> 16:34.980
to the question we're about to ask which is

16:34.980 --> 16:37.420
what is somebody who's been in this field

16:37.420 --> 16:39.580
for a long time and seen different trends come and go,

16:39.580 --> 16:41.460
what do you think that the strong version

16:41.460 --> 16:42.820
of the scaling hypothesis gets right

16:42.820 --> 16:43.660
and what does it get wrong?

16:43.660 --> 16:44.580
It's just the idea that you just throw

16:44.580 --> 16:46.540
and have computed a wide enough distribution of data

16:46.540 --> 16:47.380
and you get intelligence.

16:47.380 --> 16:49.180
Yeah, look, my view is this is kind

16:49.180 --> 16:50.620
of an empirical question right now.

16:50.620 --> 16:52.140
So I think it was pretty surprising

16:52.140 --> 16:55.660
to almost everyone, including the people who first worked

16:55.660 --> 16:58.140
on the scaling hypotheses that how far it's gone.

16:58.140 --> 17:01.900
In a way, I mean, I sort of look at the large models today

17:01.900 --> 17:03.900
and I think they're almost unreasonably effective

17:03.900 --> 17:04.900
for what they are.

17:04.900 --> 17:07.100
You know, I think it's pretty surprising some

17:07.100 --> 17:09.540
of the properties that emerge, things like, you know,

17:09.540 --> 17:12.860
it's clearly in my opinion got some form of concepts

17:12.860 --> 17:15.060
and abstractions and some things like that.

17:15.060 --> 17:17.340
And I think if we were talking five plus years ago,

17:17.340 --> 17:19.540
I would have said to you, maybe we need an additional

17:19.540 --> 17:22.540
algorithmic breakthrough in order to do that.

17:22.540 --> 17:25.020
Like, you know, maybe more like the brain works.

17:25.020 --> 17:26.540
And I think that's still true

17:26.540 --> 17:29.580
if we want explicit abstract concepts, need concepts,

17:29.580 --> 17:32.460
but it seems that these systems can implicitly learn that.

17:32.460 --> 17:35.100
Another really interesting, I think an unexpected thing

17:35.100 --> 17:38.620
was that these systems have some sort of grounding.

17:38.620 --> 17:40.340
You know, even though they don't experience the world

17:40.340 --> 17:42.140
multimodally or at least until more recently

17:42.140 --> 17:43.820
we have the multimodal models.

17:43.820 --> 17:46.740
And that's surprising that the amount of information

17:46.740 --> 17:49.580
that can be, and models that can be built up

17:49.580 --> 17:50.620
just from language.

17:50.620 --> 17:53.700
And I think that I have some hypotheses about why that is.

17:53.700 --> 17:57.060
I think we get some grounding through the RLHF feedback systems

17:57.060 --> 17:59.900
because obviously the human raters are by definition

17:59.900 --> 18:04.380
grounded people, we're grounded, right, in reality.

18:04.380 --> 18:06.420
So our feedback's also grounded.

18:06.420 --> 18:08.620
So perhaps there's some grounding coming in through there.

18:08.620 --> 18:11.060
And also maybe language contains more grounding,

18:11.060 --> 18:13.620
you know, if you're able to ingest all of it,

18:13.620 --> 18:17.140
then we perhaps thought, linguists perhaps thought before.

18:17.140 --> 18:19.300
So that's just some very interesting philosophical questions.

18:19.300 --> 18:21.820
I think we haven't, people haven't even really

18:21.820 --> 18:23.540
scratched the surface off yet,

18:23.540 --> 18:26.020
looking at the advances that have been made.

18:26.900 --> 18:28.220
You know, it's quite interesting to think about

18:28.220 --> 18:29.700
where it's going to go next.

18:29.700 --> 18:32.660
But in terms of your question of like the large models,

18:32.660 --> 18:36.180
I think we've got to push scaling as hard as we can.

18:36.180 --> 18:37.500
And that's what we're doing here.

18:37.500 --> 18:39.140
And you know, it's an empirical question

18:39.140 --> 18:41.860
whether that will hit an asymptote or brick wall.

18:41.860 --> 18:44.260
And there are, you know, different people argue about that.

18:44.260 --> 18:45.780
But actually, I think we should just test it.

18:45.780 --> 18:47.420
I think no one knows.

18:47.420 --> 18:48.940
And but in the meantime,

18:48.940 --> 18:52.940
we should also double down on innovation and invention.

18:52.940 --> 18:55.620
And this is something that the Google research

18:55.620 --> 18:58.700
and DeepMind and Google Brain have, you know,

18:58.700 --> 19:00.900
we've pioneered many, many things over the last decade.

19:00.900 --> 19:02.700
That's something that's our bread and butter.

19:02.700 --> 19:05.340
And, you know, you can think of half our effort

19:05.340 --> 19:07.100
as to do with scaling and half our efforts

19:07.380 --> 19:09.900
to do with inventing the next architectures,

19:09.900 --> 19:12.100
the next algorithms that will be needed,

19:12.100 --> 19:15.060
knowing that you've got this scaled larger and larger model

19:15.060 --> 19:16.380
coming along the lines.

19:16.380 --> 19:19.020
So my betting right now,

19:19.020 --> 19:21.860
but it's a loose betting is that you would need both.

19:21.860 --> 19:23.300
But I think, you know,

19:23.300 --> 19:25.540
it's you've got to push both of them as hard as possible.

19:25.540 --> 19:27.300
And we're in a lucky position that we can do that.

19:27.300 --> 19:28.660
Yeah. I want to ask more about the grounding.

19:28.660 --> 19:30.820
So you can imagine two things that might change,

19:30.820 --> 19:32.580
which would make the grounding more difficult.

19:32.580 --> 19:34.780
One is that if these models gets from Arder,

19:34.780 --> 19:37.460
they're going to be able to operate in domains

19:37.460 --> 19:39.660
where we just can generate enough human labels,

19:39.660 --> 19:40.940
just because we're not smart enough, right?

19:40.940 --> 19:42.860
So if it does like a million line pull request,

19:42.860 --> 19:44.260
you know, how do we tell it?

19:44.260 --> 19:46.580
Like this is within the constraints of our morality

19:46.580 --> 19:48.700
and the end goal we wanted and this isn't.

19:48.700 --> 19:50.820
And the other is it sounds like you're saying

19:50.820 --> 19:53.100
more of the compute of so far we've been doing,

19:53.100 --> 19:54.100
you know, next token prediction

19:54.100 --> 19:55.500
and in some sense it's a guardrail

19:55.500 --> 19:57.500
because you have to talk as a human would talk

19:57.500 --> 19:58.940
and think as a human would think.

19:58.940 --> 20:01.660
Now, if additional compute is going to come

20:01.660 --> 20:04.140
in the form of reinforcement learning,

20:04.140 --> 20:06.100
where just to get to the end objective,

20:06.100 --> 20:08.780
we can't really trace how you got there.

20:08.780 --> 20:09.940
When you combine those two,

20:09.940 --> 20:13.180
how worried are you that the sort of grounding goes away?

20:13.180 --> 20:16.780
Well, look, I think if the grounding,

20:16.780 --> 20:18.260
you know, if it's not properly grounded,

20:18.260 --> 20:21.020
the system won't be able to achieve those goals properly,

20:21.020 --> 20:21.860
right? I think so.

20:21.860 --> 20:24.620
I think in a sense you sort of have to have the grounding

20:24.620 --> 20:26.820
or at least some of it in order for a system

20:26.820 --> 20:29.420
to actually achieve goals in the real world.

20:29.420 --> 20:31.900
I do actually think that as these systems

20:32.020 --> 20:34.580
and things like Gemini are becoming more multimodal

20:34.580 --> 20:36.900
and we start ingesting things like video

20:36.900 --> 20:41.500
and, you know, audio visual data as well as text data.

20:41.500 --> 20:43.180
And then, you know, the system starts

20:43.180 --> 20:44.900
correlating those things together.

20:45.780 --> 20:49.900
I think that is a form of proper grounding actually.

20:49.900 --> 20:54.380
So I do think our systems are going to start to understand,

20:54.380 --> 20:56.540
you know, the physics of the real world better.

20:56.540 --> 20:58.820
And then one could imagine the active version of that

20:58.820 --> 21:00.780
is being in a very realistic simulation

21:00.780 --> 21:03.660
or game environment where you're starting to learn

21:03.660 --> 21:06.060
about what your actions do in the world

21:06.060 --> 21:10.340
and how that affects the world itself,

21:10.340 --> 21:11.340
the world stay itself,

21:11.340 --> 21:14.060
but also what next learning episode you're getting.

21:14.060 --> 21:16.660
So, you know, these RL agents we've always been working on

21:16.660 --> 21:19.180
and pioneered like AlphaZero and AlphaGo,

21:19.180 --> 21:21.220
they actually affect their active learners.

21:21.220 --> 21:23.260
What they decide to do next affects

21:23.260 --> 21:25.980
what the next learning piece of data

21:25.980 --> 21:27.620
or experience they're going to get.

21:27.620 --> 21:29.380
So there's this very interesting sort of feedback loop.

21:29.380 --> 21:31.940
And of course, if we ever want to be good at things like robotics,

21:31.940 --> 21:33.300
we're going to have to understand

21:33.300 --> 21:35.540
how to act in the real world.

21:35.540 --> 21:37.300
Yeah, so there's a grounding in terms of

21:37.300 --> 21:39.260
will the capabilities be able to proceed?

21:39.260 --> 21:41.340
Will they be like enough in touch with the reality

21:41.340 --> 21:42.820
to be able to like do the things we want?

21:42.820 --> 21:45.300
And there's another sense of grounding of

21:45.300 --> 21:46.580
we've gotten lucky in the sense that

21:46.580 --> 21:47.940
since they're trained on human thought,

21:47.940 --> 21:49.620
they like maybe think like a human.

21:49.620 --> 21:51.380
To what extent does that stay true

21:51.380 --> 21:53.780
when more of the compute for trading comes from

21:53.780 --> 21:56.380
just did you get the right outcome and not guard real?

21:56.380 --> 21:58.540
Like, are you like proceeding on the next token

21:58.540 --> 21:59.380
as a human would?

21:59.380 --> 22:01.780
Maybe the broader question I'll like post to you is

22:01.780 --> 22:03.020
and this is what I asked Shane as well,

22:03.020 --> 22:04.580
what would it take to align a system

22:04.580 --> 22:05.620
that's smarter than a human?

22:05.620 --> 22:07.860
Maybe things in alien concepts

22:07.860 --> 22:09.260
and you can't like really monitor

22:09.260 --> 22:10.260
the million line pull request

22:10.260 --> 22:12.180
because you can't really understand the whole thing.

22:12.180 --> 22:14.500
Yeah, I mean, look, this is something Shane and I

22:14.500 --> 22:15.660
and many others here,

22:15.660 --> 22:17.100
we've had that forefront of our minds

22:17.100 --> 22:19.300
for since before we started DeMind

22:19.300 --> 22:21.860
and because we planned for success crazy,

22:21.860 --> 22:23.660
you know, 2010, no one was thinking about AI,

22:23.660 --> 22:24.980
let alone AGI,

22:24.980 --> 22:27.460
but we already knew that if we could make progress

22:27.460 --> 22:29.460
with these systems and these ideas,

22:29.460 --> 22:31.100
it, you know, the technology

22:31.100 --> 22:31.940
where there would be creator

22:31.940 --> 22:33.700
being unbelievably transformative.

22:33.700 --> 22:35.460
So we already were thinking, you know,

22:35.460 --> 22:37.540
20 years ago about, well, how, you know,

22:37.540 --> 22:39.060
what would the consequences of that be?

22:39.060 --> 22:40.540
Both positive and negative.

22:40.540 --> 22:43.100
Of course, the positive direction is amazing science,

22:43.100 --> 22:44.140
things like alpha fold,

22:44.140 --> 22:46.500
incredible breakthroughs in health and science

22:46.500 --> 22:50.220
and maths and discovery, scientific discovery.

22:50.220 --> 22:52.060
But then also we got to make sure these systems

22:52.060 --> 22:54.100
are sort of understandable and controllable.

22:54.100 --> 22:56.180
And I think there's sort of several, you know,

22:56.180 --> 22:58.220
this would be a whole sort of discussion in itself,

22:58.220 --> 23:00.980
but there are many, many ideas that people have

23:00.980 --> 23:03.500
from much more stringent eval systems.

23:03.500 --> 23:04.580
I think we don't have good enough

23:04.580 --> 23:07.300
at evaluations and benchmarks for things like

23:07.300 --> 23:09.180
can the system deceive you?

23:09.180 --> 23:10.540
Can it exotrate its own code?

23:10.540 --> 23:13.100
It was sort of undesirable behaviors.

23:13.100 --> 23:15.620
And then there's, you know,

23:15.620 --> 23:19.500
ideas of actually using AI, maybe narrow AIs.

23:19.500 --> 23:20.820
So not general learning ones,

23:20.820 --> 23:23.460
but systems that are specialized for a domain

23:23.460 --> 23:27.580
to help us as the human scientists analyze

23:27.580 --> 23:30.860
and summarize what the more general system is doing, right?

23:30.860 --> 23:33.540
So kind of narrow AI tools.

23:33.540 --> 23:35.340
I think that there's a lot of promise

23:35.340 --> 23:38.540
in creating hardened sandboxes or simulations

23:38.540 --> 23:42.540
so that are hardened with cybersecurity arrangements

23:44.020 --> 23:47.460
around the simulation, both to keep the AI in,

23:47.460 --> 23:50.940
but also as cybersecurity to keep hackers out.

23:50.940 --> 23:53.420
And then you could experiment a lot more

23:53.420 --> 23:55.500
freely within that sandbox domain.

23:55.500 --> 23:58.340
And I think a lot of these ideas are,

23:58.340 --> 23:59.820
and there's many, many others,

23:59.820 --> 24:01.940
including the analysis stuff we talked about earlier

24:01.940 --> 24:04.380
where can we analyze and understand

24:04.380 --> 24:06.420
what the concepts are that this system is building,

24:06.420 --> 24:07.780
what the representations are.

24:07.780 --> 24:09.700
So maybe they're not so alien to us

24:09.700 --> 24:11.540
and we can actually keep track

24:11.540 --> 24:13.820
of the kind of knowledge that it's building.

24:13.820 --> 24:14.820
Yeah, yeah.

24:14.820 --> 24:15.660
So big backup fit.

24:15.660 --> 24:16.820
I'm curious what your timelines are.

24:16.820 --> 24:19.540
So Shane said he's like, I think modal outcome is 2028.

24:19.540 --> 24:20.580
I think that's maybe he's median.

24:20.580 --> 24:21.420
Yeah.

24:21.420 --> 24:22.260
What is yours?

24:22.380 --> 24:26.060
I don't have prescribed kind of specific numbers to it

24:26.060 --> 24:28.860
because I think there's so many unknowns and uncertainties

24:28.860 --> 24:32.540
and human ingenuity and endeavor

24:32.540 --> 24:34.380
comes up with surprises all the time.

24:34.380 --> 24:37.940
So that could meaningfully move the timelines.

24:37.940 --> 24:41.540
But I will say that when we started DeepMind back in 2010,

24:41.540 --> 24:43.540
we thought of it as a 20 year project.

24:43.540 --> 24:45.780
And actually, I think we're on track,

24:45.780 --> 24:47.820
which is kind of amazing for 20 year projects.

24:47.820 --> 24:49.820
Because usually they're always 20 years away.

24:49.820 --> 24:51.700
So that's the joke about whatever it is

24:51.700 --> 24:54.540
that you use in quantum AI, take your pick.

24:54.540 --> 24:56.980
And but I think we're on track.

24:56.980 --> 25:00.780
So I wouldn't be surprised if we had AGI like systems

25:00.780 --> 25:02.220
within the next decade.

25:02.220 --> 25:04.860
And do you buy the model that once you have an AGI,

25:04.860 --> 25:07.580
you have a system that basically speeds up further AI research?

25:07.580 --> 25:09.100
Maybe not like an overnight sense,

25:09.100 --> 25:10.940
but over the course of months and years,

25:10.940 --> 25:11.860
you have much faster progress

25:11.860 --> 25:12.700
than you would have on the right side.

25:12.700 --> 25:15.260
I think that's potentially possible.

25:15.260 --> 25:18.220
I think it partly depends what we decide,

25:18.220 --> 25:20.740
we as society decide to use the first

25:20.740 --> 25:24.660
nascent AGI systems or even proto AGI systems for.

25:24.660 --> 25:29.580
So, even the current LLMs seem to be pretty good at coding.

25:29.580 --> 25:32.300
So, and we have systems like AlphaCode,

25:32.300 --> 25:33.980
we also got theorem proving systems.

25:33.980 --> 25:37.940
So one could imagine combining these ideas together

25:37.940 --> 25:39.900
and making them a lot better.

25:39.900 --> 25:43.460
And then I could imagine these systems being quite good

25:43.460 --> 25:47.060
at designing and helping us build future versions

25:47.060 --> 25:48.380
of themselves.

25:48.380 --> 25:50.180
But we also have to think about the safety implications

25:50.220 --> 25:51.100
of that, of course.

25:51.100 --> 25:52.060
Yeah, I'm curious what you think about that.

25:52.060 --> 25:54.380
So, I mean, I'm not saying this is happening this year

25:54.380 --> 25:56.780
or anything, but eventually you'll be developing a model

25:56.780 --> 25:58.260
where during the process of development,

25:58.260 --> 25:59.980
you think, you know, there's some chance

25:59.980 --> 26:01.340
that once this is fully developed,

26:01.340 --> 26:03.300
it'll be capable of like an intelligence explosion

26:03.300 --> 26:04.980
like dynamic.

26:04.980 --> 26:07.780
What would have to be true of that model at that point

26:07.780 --> 26:09.500
where you're like, you know,

26:09.500 --> 26:11.020
I've seen these specific evals,

26:11.020 --> 26:13.780
I've like, I've like understand it's internal thinking enough

26:13.780 --> 26:14.860
and like it's future thinking

26:14.860 --> 26:17.220
that I'm comfortable continuing development of the system.

26:17.220 --> 26:20.300
Well, look, we need a lot more understanding

26:20.300 --> 26:21.420
of the systems than we do today

26:21.420 --> 26:23.060
before I would be even confident

26:23.060 --> 26:26.900
of even explaining to you what we would need to tick box there.

26:26.900 --> 26:28.620
So I think actually what we've got to do

26:28.620 --> 26:30.300
in the next few years and the time we have

26:30.300 --> 26:33.540
before those systems start arriving is come up

26:33.540 --> 26:36.220
with the right evaluations and metrics

26:36.220 --> 26:38.580
and maybe ideally formal proofs,

26:38.580 --> 26:40.020
but you know, it's going to be hard

26:40.020 --> 26:40.940
for these types of systems,

26:40.940 --> 26:43.860
but at least empirical bounds

26:43.860 --> 26:46.100
around what these systems can do.

26:46.100 --> 26:49.380
And that's why I think about things like deception

26:49.380 --> 26:52.340
and has been quite root node traits that you don't want

26:52.340 --> 26:54.540
because if you're confident that your system

26:54.540 --> 26:58.780
is sort of exposing what it actually thinks,

26:58.780 --> 27:00.180
then you could potentially,

27:00.180 --> 27:03.020
that opens up possibilities of using the system itself

27:03.020 --> 27:06.020
to explain aspects of itself to you.

27:06.020 --> 27:08.620
The way I think about that actually is like,

27:08.620 --> 27:11.020
if I was to play a game of chess against Gary Kasparov,

27:11.020 --> 27:13.260
right, which I played in the past or Magnus Carlson,

27:13.260 --> 27:15.660
you know, the amazing chess players with graceful time,

27:16.100 --> 27:18.820
I wouldn't be able to come up with a move that they could,

27:18.820 --> 27:22.860
but they could explain to me why they came up

27:22.860 --> 27:26.740
with that move and I could understand it post hoc, right?

27:26.740 --> 27:29.220
And that's the sort of thing one could imagine

27:30.580 --> 27:34.700
one of the capabilities that we could make use

27:34.700 --> 27:37.540
of these systems is for them to explain it to us

27:37.540 --> 27:39.780
and even maybe the proofs behind why they're thinking

27:39.780 --> 27:41.860
something, certainly in a mathematical,

27:41.860 --> 27:42.980
any mathematical problem.

27:42.980 --> 27:43.820
Got it.

27:43.820 --> 27:46.220
Do you have a sense of what the converse answer would be?

27:46.220 --> 27:48.220
So what would have to be true where tomorrow morning,

27:48.220 --> 27:50.660
you're like, oh man, I didn't anticipate this.

27:50.660 --> 27:52.220
You see some specific observation tomorrow morning

27:52.220 --> 27:54.100
where like, we got to stop Gemini II training.

27:54.100 --> 27:55.860
Like, what would specifically...

27:55.860 --> 27:57.500
Yeah, I could imagine that.

27:57.500 --> 28:00.700
And this is where things like the sandbox simulations,

28:00.700 --> 28:03.020
I would hope we're experimenting

28:03.020 --> 28:06.060
in a safe, secure environment.

28:06.060 --> 28:08.700
And then something happens in it

28:08.700 --> 28:11.220
where very unexpected happens

28:11.220 --> 28:13.060
and you unexpected capability

28:13.060 --> 28:15.660
or something that we didn't want explicitly told the system

28:15.660 --> 28:18.460
we didn't want and that it did, but then lied about.

28:18.460 --> 28:21.540
These are the kinds of things where one would want to

28:21.540 --> 28:26.540
then dig in carefully with the systems that are around today

28:26.540 --> 28:29.140
which are not dangerous in my opinion today,

28:29.140 --> 28:32.460
but in a few years they might be, have potential.

28:33.420 --> 28:37.020
And then you would sort of ideally kind of pause

28:37.020 --> 28:40.580
and then really get to the bottom of why it was doing

28:40.580 --> 28:42.500
those things before one continued.

28:42.500 --> 28:44.020
Yeah, going back to Gemini,

28:44.020 --> 28:47.340
I'm curious what the bottlenecks were in the development.

28:47.340 --> 28:49.540
Like, why not make it immediately one order of magnitude

28:49.540 --> 28:52.460
bigger if scaling works?

28:52.460 --> 28:54.620
Well, look, first of all, there are practical limits.

28:54.620 --> 28:57.940
How much compute can you actually fit in one data center?

28:57.940 --> 29:01.500
And actually, you're bumping up against very interesting

29:04.380 --> 29:06.700
distributed computing kind of challenges, right?

29:06.700 --> 29:08.540
Unfortunately, we have some of the best people in the world

29:08.660 --> 29:11.900
on those challenges and cross data center training,

29:11.900 --> 29:14.260
all these kinds of things, very interesting challenges,

29:14.260 --> 29:16.860
hardware challenges, and we have our TPUs and so on

29:16.860 --> 29:18.780
that we're building and designing all the time

29:18.780 --> 29:20.500
as well as using GPUs.

29:20.500 --> 29:22.820
And so there's all of that.

29:22.820 --> 29:25.740
And then you also have to, the scaling laws,

29:25.740 --> 29:27.220
they didn't just work by magic.

29:27.220 --> 29:30.060
You sort of, you still need to scale up the hyperparameters

29:30.060 --> 29:32.380
and various innovations are going in all the time

29:32.380 --> 29:33.300
with each new scale.

29:33.300 --> 29:35.980
It's not just about repeating the same recipe.

29:35.980 --> 29:38.460
At each new scale, you have to adjust the recipe

29:39.180 --> 29:40.940
and that's a bit of an art form in a way.

29:40.940 --> 29:43.300
And you have to sort of almost get new data points.

29:43.300 --> 29:45.340
If you try and extend your predictions

29:45.340 --> 29:48.500
and extrapolate them, say several orders of magnitude out,

29:48.500 --> 29:50.460
sometimes they don't hold anymore, right?

29:50.460 --> 29:52.580
Because new capabilities,

29:52.580 --> 29:55.540
there can be step functions in terms of new capabilities

29:55.540 --> 29:58.300
and some things just, some things hold

29:58.300 --> 29:59.300
and other things don't.

29:59.300 --> 30:02.380
So often you do need those intermediate data points

30:02.380 --> 30:06.380
actually to correct some of your hyperparameter optimization

30:06.380 --> 30:07.220
and other things.

30:07.580 --> 30:09.820
That the scaling law continues to be true.

30:09.820 --> 30:13.700
So there's sort of various practical limitations

30:13.700 --> 30:15.380
on to that.

30:15.380 --> 30:18.700
So kind of one order of magnitude is about probably

30:18.700 --> 30:21.420
the maximum that you want to carry on.

30:21.420 --> 30:24.140
You want to sort of do between each era.

30:24.140 --> 30:25.700
Oh, that's so fascinating.

30:25.700 --> 30:26.980
In the GPT for technical report,

30:26.980 --> 30:29.660
they say that they were able to predict the training loss

30:31.340 --> 30:33.860
tens of thousands of times less compute than GPT-4

30:33.860 --> 30:34.860
that they could see the curve.

30:34.860 --> 30:36.900
But at the point you're making is that the actual capabilities

30:36.900 --> 30:39.100
that loss implies may not be so clear.

30:39.100 --> 30:41.140
Yeah, the downstream capabilities sometimes don't follow

30:41.140 --> 30:43.580
from the, you can often predict the core metrics

30:43.580 --> 30:45.660
like training loss or something like that.

30:45.660 --> 30:48.820
But then it doesn't actually translate into MMLU

30:48.820 --> 30:52.500
or math or some other actual capability

30:52.500 --> 30:53.340
that you care about.

30:53.340 --> 30:56.020
They're not necessarily linear all the time.

30:56.020 --> 30:57.580
So there's sort of non-linear effects there.

30:57.580 --> 30:58.580
What was the biggest surprise to you

30:58.580 --> 31:00.020
during the development of Gemini?

31:00.020 --> 31:02.620
So something like this happening?

31:02.620 --> 31:05.460
Well, I mean, I wouldn't say there was one big surprise,

31:05.460 --> 31:08.060
but it was very interesting trying to train things

31:08.060 --> 31:13.060
at that size and learning about all sorts of things

31:13.060 --> 31:15.460
from organizational, how to babysit such a system

31:15.460 --> 31:16.660
and to track it.

31:16.660 --> 31:20.900
And I think things like getting a better understanding

31:20.900 --> 31:23.180
of the metrics you're optimizing

31:23.180 --> 31:26.740
versus the final capabilities that you want.

31:26.740 --> 31:30.460
I would say that's still not a perfectly understood mapping.

31:30.460 --> 31:31.580
But it's an interesting one

31:31.580 --> 31:32.860
that we're getting better and better at.

31:32.860 --> 31:33.700
Yeah, yeah.

31:33.700 --> 31:34.900
There's a perception that maybe other labs

31:34.900 --> 31:38.660
are more compute efficient than DeepMind has been

31:38.660 --> 31:39.500
with Gemini.

31:39.500 --> 31:40.620
I don't know what you make of that for something.

31:40.620 --> 31:41.820
I don't think that's the case.

31:41.820 --> 31:46.660
I mean, I think that actually Gemini one

31:46.660 --> 31:48.220
use roughly the same amount of compute,

31:48.220 --> 31:50.500
maybe slightly more than what was rumored for GPT-4.

31:50.500 --> 31:51.980
I don't know exactly what was used.

31:51.980 --> 31:55.620
So I think it was in the same ballpark.

31:55.620 --> 31:57.220
I think we're very efficient with our compute

31:57.220 --> 31:59.260
and we use our compute for many things.

31:59.260 --> 32:00.340
One is not just the scaling,

32:00.340 --> 32:02.820
but going back to earlier to these more innovation

32:03.460 --> 32:05.540
and ideas, you've got to,

32:05.540 --> 32:08.300
it's only useful a new innovation, a new invention

32:08.300 --> 32:10.340
if it's also can scale.

32:10.340 --> 32:13.580
So in a way, you also need quite a lot of compute

32:13.580 --> 32:17.140
to do new invention because you've got to test many things

32:17.140 --> 32:18.980
at least some reasonable scale

32:18.980 --> 32:20.860
and make sure that they work at that scale.

32:20.860 --> 32:24.020
And also some new ideas may not work at a toy scale,

32:24.020 --> 32:26.060
but do work at a larger scale.

32:26.060 --> 32:27.740
And in fact, those are the more valuable ones.

32:27.740 --> 32:30.300
So you actually, if you think about that exploration process,

32:30.300 --> 32:33.500
you need quite a lot of compute to be able to do that.

32:33.500 --> 32:36.420
I mean, the good news is, is I think we,

32:36.420 --> 32:38.300
we're pretty lucky at Google that we,

32:38.300 --> 32:40.060
I think this year certainly we're going to have

32:40.060 --> 32:42.860
the most compute by far of any sort of research lab.

32:42.860 --> 32:45.420
And we hope to make very efficient and good use of that

32:45.420 --> 32:49.380
in terms of both scaling and the capability of our systems

32:49.380 --> 32:50.980
and also new inventions.

32:50.980 --> 32:51.820
Yeah.

32:51.820 --> 32:53.100
What's been the biggest surprise to you

32:53.100 --> 32:55.580
if you go back to yourself in 2010

32:55.580 --> 32:56.580
when you were starting DeepMind

32:56.580 --> 32:58.660
in terms of what AI progress has looked like?

32:58.660 --> 33:01.660
Did you anticipate back then that it would in some large sense

33:01.660 --> 33:03.980
amount to spend as, you know, dumping billions of dollars

33:03.980 --> 33:04.820
into these models?

33:04.820 --> 33:05.940
Or did you have a different sense of what it would look like?

33:05.940 --> 33:07.460
We thought that, and actually, you know,

33:07.460 --> 33:09.780
if you, I know you've interviewed my colleague Shane

33:09.780 --> 33:14.660
and he always thought that in terms of like compute curves

33:14.660 --> 33:17.420
and then maybe comparing roughly to like the brain

33:17.420 --> 33:19.860
and how many neurons and synapses there are very loosely,

33:19.860 --> 33:22.420
but we're actually interestingly in that kind of regime

33:22.420 --> 33:24.660
that roughly in the right order of magnitude of,

33:24.660 --> 33:26.220
you know, number of synapses in the brain

33:26.380 --> 33:28.780
and the sort of compute that we have.

33:28.780 --> 33:30.980
But I think more fundamentally, you know,

33:30.980 --> 33:36.340
we always thought that we bet on generality and learning, right?

33:36.340 --> 33:39.820
So those were always at the core of the any technique we would use.

33:39.820 --> 33:41.980
That's why we triangulated on reinforcement learning

33:41.980 --> 33:44.820
and search and deep learning, right?

33:44.820 --> 33:48.780
As three types of algorithms that would scale

33:48.780 --> 33:51.620
and would be very general

33:51.620 --> 33:55.020
and not require a lot of handcrafted human priors,

33:55.020 --> 33:57.580
which we thought was the sort of failure mode, really,

33:57.580 --> 34:00.940
of the efforts to build AI in the 90s, right?

34:00.940 --> 34:04.580
Places like MIT where there were very logic-based systems,

34:04.580 --> 34:07.740
expert systems, you know, masses of hand-coded,

34:07.740 --> 34:10.060
hand-crafted human information going into them

34:10.060 --> 34:12.420
that turned out to be wrong or too rigid.

34:12.420 --> 34:13.860
So we wanted to move away from that.

34:13.860 --> 34:17.300
And I think we spotted that trend early and became, you know,

34:17.300 --> 34:19.780
and obviously we use games as our proving ground

34:19.780 --> 34:21.220
and we did very well with that.

34:21.220 --> 34:23.220
And I think all of that was very successful

34:23.220 --> 34:26.900
and I think maybe inspired others to, you know, things like AlphaGo.

34:26.900 --> 34:29.740
I think it was a big moment for inspiring many others to think,

34:29.740 --> 34:32.420
oh, actually, these systems are ready to scale.

34:32.420 --> 34:34.540
And then, of course, with the advent of Transformers

34:34.540 --> 34:37.540
invented by our colleagues at Google, you know, research and brain,

34:37.540 --> 34:40.740
that was then, you know, the type of deep learning

34:40.740 --> 34:44.500
that allowed us to ingest masses of amounts of information.

34:44.500 --> 34:47.780
And that, of course, is really turbocharged where we are today.

34:47.780 --> 34:49.700
So I think that's all part of the same lineage.

34:49.700 --> 34:52.740
You know, we couldn't have predicted every twist and turn there,

34:52.740 --> 34:56.940
but I think the general direction we were going in was the right one.

34:56.940 --> 34:59.660
Yeah. And in fact, it's like fascinating because actually,

34:59.660 --> 35:02.060
if you like read your old papers or Shane's old papers,

35:02.060 --> 35:04.500
Shane's thesis, I think in 2009, he said, like, well, you know,

35:04.500 --> 35:07.020
the way we would test for AI is if it can you come press Wikipedia.

35:07.020 --> 35:08.780
And that's like literally the last function of our labs

35:08.780 --> 35:11.340
or like your own paper in like 2016 before Transformers

35:11.340 --> 35:14.340
where we said, like, you were comparing your science and AI.

35:14.340 --> 35:16.260
And he said, attention is what is needed.

35:16.260 --> 35:17.540
Exactly. Exactly.

35:17.540 --> 35:20.900
So we had these things called out and actually we had some early attention

35:20.980 --> 35:24.580
papers, but they weren't as elegant as Transformers in the end,

35:24.580 --> 35:26.380
like, Neuroturing Machines and things like this.

35:26.380 --> 35:29.260
Yeah. And then Transformers was the was the nicer

35:29.260 --> 35:30.820
and more general architecture of that.

35:30.820 --> 35:31.980
Yeah, yeah, yeah.

35:31.980 --> 35:34.140
When you extrapolate all this out forward,

35:34.140 --> 35:38.460
anything about superhuman intelligence or is like,

35:38.460 --> 35:39.740
what does that landscape look like to you?

35:39.740 --> 35:42.420
Is it like still controlled by a private company?

35:42.420 --> 35:45.340
Like, what should the governance of that look like concretely?

35:45.340 --> 35:49.020
Yeah, look, I would love, you know, I think that this has to be.

35:49.980 --> 35:52.140
This is so consequential, this technology.

35:52.140 --> 35:57.340
I think it's much bigger than any one company or or or even industry in general.

35:57.340 --> 36:01.340
I think it has to be a big collaboration with many stakeholders

36:01.500 --> 36:04.380
from civil society, academia, government.

36:04.540 --> 36:08.060
And the good news is I think with the popularity of the recent chatbot systems

36:08.060 --> 36:12.540
and so on, I think that has woken up many of these other parts of society

36:12.540 --> 36:15.900
that this is coming and what it will be like to interact with these systems.

36:16.060 --> 36:16.740
And that's great.

36:16.780 --> 36:20.100
So it's opened up lots of doors for very good conversations.

36:20.220 --> 36:24.620
I mean, an example of that was the safety summit in the UK hosted a few months ago,

36:24.620 --> 36:28.020
which I thought was a big success to start getting this international dialogue going.

36:28.260 --> 36:32.420
And and and, you know, I think the whole society needs to be involved in deciding

36:32.420 --> 36:34.620
what do we want to deploy these models for?

36:34.620 --> 36:35.660
How do we want to use them?

36:35.660 --> 36:37.140
What do we not want to use them for?

36:37.140 --> 36:40.380
You know, I think we've got to try and get some international consensus around that.

36:40.820 --> 36:44.260
And then also making sure that the benefits of these systems

36:45.140 --> 36:48.340
benefit everyone, you know, for the good of everyone and society in general.

36:48.500 --> 36:51.580
And that's why I push so hard things like AI for science.

36:51.580 --> 36:55.140
And and I hope that, you know, with things like our spin out isomorphic,

36:55.140 --> 36:58.300
we're going to start curing diseases, you know, terrible diseases with AI

36:58.300 --> 37:01.980
and accelerate drug discovery, amazing things, climate change and other things.

37:01.980 --> 37:05.180
I think big challenges that face us and face humanity.

37:05.900 --> 37:09.100
Massive challenges, actually, which I'm optimistic we can solve

37:09.540 --> 37:13.420
because we've got this incredibly powerful tool coming along down the line of AI

37:13.940 --> 37:17.740
that we can apply and I think help us and solve many of these problems.

37:17.740 --> 37:20.380
So, you know, ideally, we would have a big

37:21.420 --> 37:25.260
consensus around that and a big discussion, you know, sort of almost like

37:25.260 --> 37:27.140
the UN level, if possible.

37:27.140 --> 37:29.500
You know, one interesting thing is if you look at these systems,

37:29.500 --> 37:32.940
they you chat with them and they're immensely powerful and intelligent.

37:33.540 --> 37:37.020
But it's interesting to the extent of which they haven't like automated

37:37.020 --> 37:38.700
large sections of the economy yet.

37:38.700 --> 37:41.260
Whereas a five years ago, I showed you a Gemini, you'd be like, wow,

37:41.260 --> 37:43.500
this is like, you know, totally coming for a lot of things.

37:43.660 --> 37:45.100
So how do you account for that?

37:45.100 --> 37:48.180
Like what's going on where it hasn't had a broader impact yet?

37:48.180 --> 37:50.980
I think it's we're still I think that just shows we're still at the beginning

37:50.980 --> 37:53.380
of this new era. Yeah.

37:53.380 --> 37:56.180
And I think that for these systems, I think there are some interesting

37:56.180 --> 38:00.420
use cases, you know, you know, where you can use things to some,

38:00.420 --> 38:04.020
you know, these these these chatbot systems to summarize stuff for you

38:04.020 --> 38:09.780
and and maybe do some simple writing and maybe more kind of boilerplate type writing.

38:09.940 --> 38:13.700
But that's only a small part of what, you know, we all do every day.

38:13.700 --> 38:18.140
So I think for more general use cases, I think we need still need new

38:18.140 --> 38:22.460
capabilities, things like planning and search, but also maybe things like

38:22.460 --> 38:26.020
personalization and memory, episodic memory.

38:26.020 --> 38:29.340
So not just long context windows, but actually remembering what I what

38:29.340 --> 38:31.420
we spoke about a hundred conversations ago.

38:32.220 --> 38:35.820
And I think once they start coming in, I mean, I'm really looking forward

38:35.820 --> 38:39.540
to things like recommendation systems that that help me find better,

38:39.540 --> 38:43.300
more enriching material, whether that's books or films or music and so on.

38:43.420 --> 38:45.260
You know, I would use that type of system every day.

38:45.260 --> 38:49.820
So I think we're just scratching the surface of what these AI,

38:50.020 --> 38:54.420
say, assistants could actually do for us in our general everyday lives.

38:54.580 --> 38:58.420
And also in our work context as well, I think they're not reliable yet enough

38:58.420 --> 39:00.420
to do things like science with them.

39:00.420 --> 39:03.900
But I think one day, you know, once we fix factuality and grounding and other things,

39:04.460 --> 39:07.140
I think they could end up becoming like, you know, the world's best

39:07.140 --> 39:12.580
research assistant for you as a scientist or as a clinician.

39:13.540 --> 39:16.460
I want to ask about memory, by the way, you had this fascinating paper

39:16.460 --> 39:20.220
in 2007 where you talk about the links between memory and imagination

39:20.220 --> 39:22.060
and how they, in some sense, are very similar.

39:23.860 --> 39:26.340
People often claim that these models are just memorizing.

39:26.580 --> 39:28.860
How do you think about that claim that people make?

39:29.300 --> 39:30.900
Is memorization all you need?

39:30.900 --> 39:32.900
Because in some some deep sense, that's compression.

39:32.900 --> 39:34.300
Or, you know, what's your intuition?

39:34.300 --> 39:37.740
Yeah, I mean, sort of at the limit, one maybe could try and memorize everything,

39:37.740 --> 39:40.180
but it wouldn't generalize out of out of your distribution.

39:40.180 --> 39:43.340
And I think these systems are clearly I think the early the early

39:44.940 --> 39:49.020
criticisms of these early systems were that they were just regurgitating

39:49.020 --> 39:53.300
and memorizing, but I think clearly the new era, the Gemini GPT-4 type era,

39:53.300 --> 39:56.300
they are definitely generalizing to new constructs.

39:57.380 --> 40:00.580
So but actually, you know, in my thesis and that paper,

40:00.580 --> 40:04.860
particularly, that started that era of imagination in neuroscience was showing

40:04.860 --> 40:07.900
that, you know, first of all, memory, certainly at least human memory

40:07.900 --> 40:09.220
is a reconstructive process.

40:09.220 --> 40:10.300
It's not a videotape, right?

40:10.300 --> 40:13.820
We sort of put it together back from components that seems familiar to us,

40:13.820 --> 40:16.860
that the ensemble, and that's what made me think that imagination

40:16.860 --> 40:20.980
might be the same thing, except in this case, you're using the same semantic components.

40:21.140 --> 40:24.140
But now you're putting it together into a way that your brain thinks is novel,

40:24.260 --> 40:26.060
right, for a particular purpose like planning.

40:26.300 --> 40:31.620
And and so I do think that that kind of idea is still probably missing

40:31.620 --> 40:36.180
from our current systems, this sort of pulling together different parts

40:36.180 --> 40:40.620
of your world model to simulate something new that then helps with your planning,

40:40.940 --> 40:43.020
which is what I would call imagination.

40:43.020 --> 40:43.860
Yeah, for sure.

40:43.860 --> 40:46.580
So again, now you guys have the best models in the world,

40:47.300 --> 40:49.780
you know, with the Gemini models.

40:49.780 --> 40:53.060
Do you have do you plan on putting out some sort of framework like the other

40:53.060 --> 40:56.500
two major labs have of, you know, once we see these specific capabilities,

40:56.820 --> 41:00.420
unless we have these specific safeguards, we're not going to continue development

41:00.420 --> 41:02.580
or we're not going to ship the product out.

41:02.580 --> 41:06.060
Yes, we have actually we I mean, we have already lots of internal checks

41:06.060 --> 41:09.020
and balances, but we're going to start publishing actually, you know,

41:09.020 --> 41:10.100
sort of watch the spaces.

41:10.100 --> 41:13.580
We're working on a whole bunch of blog posts and technical papers

41:13.860 --> 41:17.420
that we'll be putting out in the next few months that, you know,

41:17.420 --> 41:20.300
along the similar lines of things like responsible scaling laws and so on.

41:20.420 --> 41:25.380
We have those implicitly internally in various safety councils and so on,

41:25.380 --> 41:27.780
people like Shane, Chair and so on.

41:27.780 --> 41:30.980
But but it's time for us to talk about that more publicly, I think.

41:30.980 --> 41:33.100
So we'll be doing that throughout the course of the year.

41:33.100 --> 41:34.060
That's great to hear.

41:34.060 --> 41:37.500
And another thing I'm curious about is so it's not only the risk of,

41:37.500 --> 41:41.700
like, you know, the deployed model being something that people can use to do bad things,

41:41.700 --> 41:46.700
but also rogue actors, bad foreign agents, so forth, being able to steal the weights

41:46.700 --> 41:48.260
and then fine tune them to do crazy things.

41:48.660 --> 41:52.620
How do you think about securing the weights to make sure something like this

41:52.620 --> 41:56.700
doesn't happen, making sure a very like key group of people have access to them

41:56.700 --> 41:57.140
and so forth?

41:57.140 --> 41:57.900
Yeah, it's interesting.

41:57.900 --> 41:59.500
So first of all, there's sort of two parts of this.

41:59.500 --> 42:02.180
One is security, one is open source, maybe we can discuss.

42:02.180 --> 42:04.380
But the security, I think, is super key.

42:04.380 --> 42:08.100
Like just a sort of normal cyber security type things.

42:08.100 --> 42:10.140
And I think we're lucky at Google DeepMind.

42:10.140 --> 42:14.260
We're kind of behind Google's firewall and cloud protection, which is, you know,

42:14.260 --> 42:17.620
I think best, you know, best in class in the world, corporately.

42:17.740 --> 42:19.260
So we already have that protection.

42:19.260 --> 42:25.260
And then behind that, we have specific DeepMind protections within our code base.

42:25.260 --> 42:27.300
So it's sort of a double layer of protection.

42:27.300 --> 42:28.700
So I feel pretty good about that.

42:28.700 --> 42:31.700
That that's, I mean, we, you know, you can never be complacent on that.

42:31.700 --> 42:37.580
But I feel it's already sort of best in the world in terms of cyber defences.

42:37.580 --> 42:39.500
But we've got to carry on improving that.

42:39.500 --> 42:43.740
And again, things like the hard and sandboxes could be a way of doing that as well.

42:43.780 --> 42:48.540
And maybe even there are, you know, specifically secure data centers

42:48.540 --> 42:51.060
or hardware solutions to this, too, that we're thinking about.

42:51.060 --> 42:55.820
I think that maybe in the next three, four, five years, we would also want air gaps

42:55.820 --> 42:58.980
and various other things that are known in the security community.

42:58.980 --> 42:59.780
So I think that's key.

42:59.780 --> 43:03.020
And I think all frontier labs should be doing that because otherwise, you know,

43:03.020 --> 43:06.780
nation states and other things, rogue nation, you know, states and other other

43:06.780 --> 43:10.180
dangerous actors, that there would be obviously a lot of incentive for them

43:10.180 --> 43:11.780
to to steal things like the weights.

43:12.700 --> 43:15.580
And then, you know, of course, open source is another interesting question,

43:15.580 --> 43:18.420
which is we're huge proponents of open source and open science.

43:18.420 --> 43:21.420
I mean, almost every, you know, we've published thousands of papers

43:21.420 --> 43:24.420
and things like Alpha Fold and Transformers, of course.

43:24.420 --> 43:28.620
And Alpha Gold, all of these things we put out there into the world, published

43:28.620 --> 43:33.220
and open source, many of them, GraphCast, most recently, our weather prediction system.

43:33.220 --> 43:37.860
But when it comes to, you know, the core technology, the foundational technology

43:37.860 --> 43:41.460
and very general purpose, I think the question I would have is,

43:42.380 --> 43:47.020
if you, you know, first of all, open source proponents is that how does one

43:47.020 --> 43:54.020
stop bad actors, individuals or, you know, up to rogue states, taking those

43:54.020 --> 43:58.020
same open source systems and repurposing them because their general purpose

43:58.020 --> 43:59.780
for harmful ends, right?

43:59.780 --> 44:01.860
So we have to answer that question.

44:01.860 --> 44:05.780
And I haven't heard a compelling, I mean, I don't know what the answer is to that,

44:05.780 --> 44:10.700
but I haven't heard a compelling, clear answer to that from proponents

44:10.900 --> 44:12.900
of just sort of open sourcing everything.

44:12.900 --> 44:15.940
So I think there has to be some balance there, but, you know,

44:15.940 --> 44:18.180
obviously it's a complex question of what that is.

44:18.180 --> 44:21.100
Yeah, yeah, I feel like tech doesn't get the credit it deserves for, like,

44:21.100 --> 44:23.420
funding, you know, hundreds of billions of dollars worth of R&D.

44:24.220 --> 44:26.900
And, you know, obviously I have deep bind with systems like Alpha Fold and so on.

44:27.620 --> 44:30.940
Well, but when we talk about securing the weights, you know, as we said,

44:30.940 --> 44:33.900
like maybe right now, it's not something that, like, is going to cause the end

44:33.900 --> 44:35.980
of the world or anything, but as these systems get better and better,

44:35.980 --> 44:39.300
the worry that, yes, a foreign agent or something gets access to them.

44:39.580 --> 44:42.020
Presumably right now, there's like dozens to hundreds of researchers

44:42.020 --> 44:43.220
who have access to the weights.

44:43.220 --> 44:46.020
How do you, well, what's a plan for, like, getting into, like,

44:46.020 --> 44:47.780
the situation or getting the weights in the situation rooms?

44:47.780 --> 44:50.580
If you're like, if you need to access to them, it's like, you know,

44:50.580 --> 44:52.060
some extremely strenuous process.

44:52.060 --> 44:54.340
You know, nobody, nobody individual can really take them out.

44:54.340 --> 44:55.020
Yeah, yeah.

44:55.020 --> 44:58.540
I mean, one has to balance that with, with, with allowing for collaboration

44:58.540 --> 44:59.420
and speed of progress.

44:59.420 --> 45:03.140
Actually, another interesting thing is, of course, you want, you know,

45:03.140 --> 45:07.500
brilliant independent researchers from academia or, or things like the UK

45:07.580 --> 45:13.860
AI Safety Institute and US1 to be able to kind of red team these systems.

45:13.860 --> 45:17.420
So, so one has to expose them to a certain extent, although that's not

45:17.420 --> 45:18.340
necessarily the weights.

45:18.980 --> 45:22.900
And then, you know, we have a lot of processes in place about making sure

45:22.900 --> 45:27.020
that, you know, only if you need them that, that you have access to, you

45:27.020 --> 45:29.260
know, those people who need access, have access.

45:29.900 --> 45:33.740
And right now, I think we're still in the early days of those kinds of

45:33.740 --> 45:35.220
systems being at risk.

45:35.420 --> 45:38.340
And as that, as these systems become more powerful and more general and

45:38.340 --> 45:41.940
more capable, I think one has to look at the, the access question.

45:42.820 --> 45:45.860
So some of these other labs have specialized in different things relative

45:45.860 --> 45:48.460
to safety, like Anthropoc, for example, with interoperability.

45:48.460 --> 45:52.900
And do you have some sense of where you guys might have an edge where as so

45:52.900 --> 45:54.980
that, you know, now that you have the frontier model, you're going to

45:54.980 --> 45:58.100
scale up safety, where you guys are going to be able to put out the best

45:58.100 --> 45:59.180
frontier research on safety.

45:59.180 --> 46:02.380
I think, you know, well, we helped pioneer RLHF and other things like that,

46:02.380 --> 46:05.340
which can also be obviously used for performance, but also for safety.

46:06.220 --> 46:11.100
I think that, you know, a lot of the self-play ideas and these kinds of

46:11.100 --> 46:17.620
things could also be used potentially to, to auto-test a lot of the boundary

46:17.620 --> 46:19.540
conditions that you have with the new systems.

46:19.700 --> 46:23.420
I mean, part of the issue is that with these sort of very general systems,

46:24.020 --> 46:28.140
there's so much surface area to cover, like about how these systems behave.

46:28.340 --> 46:31.780
So I think we are going to need some automated testing.

46:31.940 --> 46:36.540
And again, with things like simulations and games environment, very realistic

46:36.540 --> 46:40.740
environments, virtual environments, I think we have a long history in that

46:40.740 --> 46:46.260
and using those kinds of systems and making use of them for building AI algorithms.

46:46.260 --> 46:49.060
So I think we can leverage all of that history.

46:49.700 --> 46:52.380
And then, you know, around at Google, we're very lucky we have some of the

46:52.380 --> 46:55.940
world's best cybersecurity experts, hardware designers.

46:56.140 --> 47:00.380
So I think we can bring that to bear in, you know, for security and safety as well.

47:00.580 --> 47:02.260
Great, great. Let's talk about Gemini.

47:02.820 --> 47:05.060
So, you know, now you guys have the best model in the world.

47:06.260 --> 47:09.900
So I'm curious, you know, the default way to interact with these systems has

47:09.900 --> 47:12.060
been through chat so far.

47:12.220 --> 47:14.900
Now that we have multimodal and all these new capabilities, how do you

47:15.020 --> 47:15.780
anticipate that changing?

47:15.780 --> 47:17.060
Or do you think that will still be the case?

47:17.580 --> 47:20.420
Yeah, I think we're just at the beginning of actually understanding what a

47:20.420 --> 47:25.820
full multimodal model system, how exciting that might be to interact with

47:25.820 --> 47:29.540
them, and it will be quite different to, I think, what we're used to today with

47:29.540 --> 47:30.260
the chat bots.

47:30.260 --> 47:34.980
I think the next versions of this over the next year, 18 months, you know,

47:35.180 --> 47:38.500
maybe we'll have some contextual understanding around the environment around

47:38.500 --> 47:40.740
you through a camera or whatever it is, a phone.

47:41.620 --> 47:44.580
You know, I could imagine that as the next awesome glasses at the next step.

47:45.340 --> 47:50.060
And then I think that we'll start becoming more fluid in understanding, oh,

47:50.260 --> 47:52.260
let's sample from a video.

47:52.260 --> 47:53.620
Let's use voice.

47:54.620 --> 47:59.460
Maybe even eventually things like touch and, you know, if you think about robotics

47:59.460 --> 48:02.420
and other things, you know, sensors, other types of sensors.

48:02.620 --> 48:05.580
So I think the world's about to become very exciting.

48:05.580 --> 48:08.540
I think in the next few years, as we start getting used to the idea of what

48:08.700 --> 48:10.300
true multimodality means.

48:11.500 --> 48:15.620
On the robotic subject, Ilya said when he was on the podcast that the reason

48:15.620 --> 48:18.740
opening I gave up on robotics was because they didn't have enough data in that

48:18.740 --> 48:20.300
domain, at least at the time they were pursuing it.

48:21.300 --> 48:24.220
I mean, you guys have put out different things like Robo Transformer and other things.

48:24.420 --> 48:27.780
How do you think that's still a bottleneck for robotics progress or will we

48:27.780 --> 48:30.700
see progress in the world of atoms as well as the world of bits?

48:30.700 --> 48:35.060
We're very excited about our progress with things like GATO and RT2, you know,

48:35.060 --> 48:40.180
Robotic Transformer, and we actually think so we've always liked robotics

48:40.180 --> 48:44.420
and we've had, you know, amazing research and now we still have that going now

48:44.620 --> 48:48.940
because we like the fact that it's a data poor regime because that pushes us

48:49.140 --> 48:51.980
on some very interesting research directions that we think are going to be

48:52.180 --> 48:56.100
useful anyway, like sampling efficiency and data efficiency in general and transfer

48:56.300 --> 48:59.700
learning, learning from simulation, transferring that to reality.

48:59.900 --> 49:03.860
All of these very, you know, similar to real, all of these very interesting

49:04.060 --> 49:07.620
actually general challenges that we would like to solve.

49:07.820 --> 49:09.220
So the control problem.

49:09.420 --> 49:11.420
So we've always pushed hard on that.

49:11.620 --> 49:15.580
And actually, I think so Ilya is right that that is more challenging

49:15.580 --> 49:19.820
because of the data problem, but it's also I think we're starting to see the

49:20.020 --> 49:25.500
beginnings of these large models being transferable to the robotics regime,

49:25.700 --> 49:28.340
learning in the general domain, language domain and other things.

49:28.540 --> 49:32.580
And then just treating tokens like GATO as any type of token, you know,

49:32.580 --> 49:35.900
the token could be an action, it could be a word, it could be part of an image,

49:35.900 --> 49:37.100
a pixel or whatever it is.

49:37.300 --> 49:39.660
And that's what I think true multimodality is.

49:39.860 --> 49:44.140
And to begin with, it's harder to train a system like that than a straightforward

49:44.260 --> 49:46.340
text language system.

49:46.540 --> 49:48.860
But actually, you know, going back to our

49:49.060 --> 49:53.860
early conversation of transfer learning, you start seeing that a true multimodal

49:54.060 --> 49:58.260
system, the other modalities benefit some different modality.

49:58.260 --> 50:02.020
So you get better at language because you now understand a little bit about video.

50:02.220 --> 50:07.340
So I do think it's harder to get going, but actually ultimately

50:07.540 --> 50:10.220
we'll have a more general, more capable system like that.

50:10.420 --> 50:11.460
Whatever happened to GATO?

50:11.700 --> 50:14.820
That was super fascinating that you could have like play games and also do like

50:15.020 --> 50:16.180
video and also do text.

50:16.380 --> 50:19.780
We're still working on those kinds of systems, but you can imagine we're just

50:19.980 --> 50:25.060
trying to, those ideas we're trying to build into our future generations of Gemini.

50:25.260 --> 50:30.100
You know, to be able to do all of those things and robotics transformers and things like

50:30.300 --> 50:33.620
that, you can think of them as sort of follow-ups to that.

50:33.820 --> 50:37.180
Well, we see asymmetric progress towards the domains in which the self-play

50:37.180 --> 50:39.660
kinds of things we're talking about will be especially powerful.

50:39.660 --> 50:42.740
So math and code, you know, obviously recently you have these papers out about

50:42.940 --> 50:47.780
this or yeah, you can use these things to do really cool novel things.

50:47.980 --> 50:49.860
Will they just be like superhuman coders?

50:49.860 --> 50:52.140
But like in other ways, they might be still worse than humans?

50:52.140 --> 50:52.820
Or how do you think about that?

50:53.020 --> 50:58.940
So look, I think that we're making great progress with math and things like

50:59.140 --> 51:03.660
theorem proving and coding, but it's still interesting.

51:03.860 --> 51:08.540
If one looks at, I mean, creativity in general and scientific endeavor in general,

51:08.660 --> 51:12.260
I think we're getting to the stage where our systems could help the best human

51:12.460 --> 51:16.260
scientists make their breakthroughs quicker, like almost triage the search space

51:16.460 --> 51:21.380
in some ways or perhaps find a solution like AlphaFold does with a protein structure.

51:21.580 --> 51:25.660
But it can't, they're not at the level where they can create the hypothesis

51:25.860 --> 51:28.060
themselves or ask the right question.

51:28.260 --> 51:32.380
And as any top scientists will tell you, that that's the hardest part of science

51:32.580 --> 51:36.380
is actually asking the right question, boiling down that space to like, what's

51:36.420 --> 51:39.620
the critical question we should go after the critical problem and then

51:39.820 --> 51:42.100
formulating that problem in the right way to attack it.

51:42.300 --> 51:46.740
And that's not something our systems will we have really have any idea how our

51:46.940 --> 51:49.620
systems could do, but they can.

51:49.820 --> 51:54.900
They are suitable for searching large combinatorial spaces if one can specify

51:55.100 --> 51:57.580
the problem in that way with a clear objective function.

51:57.780 --> 52:01.500
So that's very useful for already many of the problems we deal with today,

52:01.700 --> 52:05.140
but not the most high level creative problems.

52:05.300 --> 52:09.260
Right, so deep mind obviously has published all kinds of interesting stuff

52:09.460 --> 52:12.020
and, you know, speeding up science in different areas.

52:12.220 --> 52:15.460
How do you think about that in the context of if you think AGI is going to happen

52:15.660 --> 52:19.300
in the next 10, 20 years, why not just wait for the AGI to do it for you?

52:19.500 --> 52:21.500
Why build these domain specific solutions?

52:21.700 --> 52:23.580
Well, I think

52:23.780 --> 52:25.740
we don't know how long AGI is going to be.

52:25.940 --> 52:31.660
And we always used to say, you know, back even when we started DeepMind that

52:31.900 --> 52:37.300
we don't have to wait for AGI in order to bring incredible benefits to the world.

52:37.500 --> 52:44.140
And especially, you know, my personal passion has been AI for science and health.

52:44.340 --> 52:47.940
And you can see that with things like AlphaFold and all of our various

52:47.940 --> 52:50.940
nature papers of different domains and material science work and so on.

52:50.940 --> 52:54.460
I think there's lots of exciting directions and also impact in the world

52:54.460 --> 52:55.300
through products, too.

52:55.500 --> 52:59.100
I think it's very exciting and a huge opportunity, a unique opportunity we have

52:59.140 --> 53:05.980
as part of Google, of, you know, they got dozens of billion user products, right?

53:06.180 --> 53:10.220
That we can immediately ship our advances into and then

53:10.420 --> 53:14.740
billions of people can, you know, improve their daily lives, right?

53:14.740 --> 53:17.340
And enriches their daily lives and enhances their daily lives.

53:17.540 --> 53:21.500
So I think it's a fantastic opportunity for impact on all those fronts.

53:21.700 --> 53:26.020
And I think the other reason from a point of view of AGI specifically is

53:26.220 --> 53:29.260
that it battle tests your ideas, right?

53:29.460 --> 53:33.180
So you don't want to be in a sort of research bunker where you just,

53:33.380 --> 53:35.900
you know, theoretically are pushing things, some things forward.

53:36.100 --> 53:40.620
But then actually your internal metrics start deviating from

53:40.820 --> 53:43.420
real world things that people would care about, right?

53:43.620 --> 53:44.820
Or real world impact.

53:45.020 --> 53:48.300
So you get a lot of feedback, direct feedback from these real world

53:48.500 --> 53:52.860
applications that then tells you whether your systems really are scaling or or

53:52.900 --> 53:56.900
actually is, you know, do we need to be more data efficient or sample efficient

53:57.100 --> 54:00.780
because most real world challenges require that, right?

54:00.980 --> 54:05.460
And so it kind of keeps you honest and pushes you, you know, keep sort of

54:05.660 --> 54:09.660
nudging and steering your research directions to make sure they're on the right path.

54:09.660 --> 54:11.060
So I think it's fantastic.

54:11.060 --> 54:15.060
And of course, the world benefits from that society benefits from that on the way.

54:15.260 --> 54:18.140
Many, many, maybe many, many years before AGI arrives.

54:18.340 --> 54:22.100
Yeah. Well, the development of Gemini is super interesting because it comes right

54:22.140 --> 54:26.380
at the heels of merging these different organizations, Brain and DeepMind.

54:26.580 --> 54:28.540
Yeah, I'm curious, what have been the challenges there?

54:28.540 --> 54:30.180
What have been the synergies?

54:30.380 --> 54:32.740
And it's been successful in the sense that you have the best model in the world now.

54:32.940 --> 54:36.180
Well, look, it's been fantastic actually over the last year.

54:36.180 --> 54:40.340
Of course, it's been challenging to do that, like any big integration coming together.

54:40.540 --> 54:44.500
But you're talking about two, you know, world-class organizations,

54:44.700 --> 54:48.740
long storied histories of inventing many, many important things, you know,

54:48.740 --> 54:50.620
from deep reinforcement learning to transformers.

54:50.740 --> 54:54.180
And so it's very exciting, actually, pooling all of that together and

54:54.380 --> 54:55.980
and collaborating much more closely.

54:56.180 --> 55:00.060
We always used to be collaborating, but more on a on a on a, you know,

55:00.260 --> 55:05.180
sort of project by project basis versus a much deeper, broader collaboration

55:05.380 --> 55:10.100
like we have now in Gemini is the first fruit of of that collaboration,

55:10.300 --> 55:13.060
including the name Gemini actually, you know, implying twins.

55:13.260 --> 55:16.220
And and of course, a lot of other things are made more efficient,

55:16.420 --> 55:19.780
like pooling compute resources together and ideas and engineering,

55:20.020 --> 55:24.500
which I think at the stage we're at now, where there's huge amounts of world-class

55:24.500 --> 55:27.620
engineering that has to go on to build the frontier systems.

55:27.820 --> 55:30.780
I think it makes sense to to coordinate that more closely.

55:30.780 --> 55:34.740
Yeah. So I mean, you and Shane started DeepMind partly because you were concerned

55:34.940 --> 55:38.540
about safety and you saw AGI coming as like a live possibility.

55:38.740 --> 55:42.820
Do you do you think the people who were formerly part of brain, the half of Google

55:42.820 --> 55:45.220
DeepMind now, do they do you think they approach it in the same way?

55:45.220 --> 55:47.260
Have there been cultural differences there in terms of that question?

55:47.300 --> 55:48.620
Yeah, no, I think overrun.

55:48.660 --> 55:52.100
And this is why, you know, I think one of the reasons we joined forces with Google

55:52.300 --> 55:56.860
back in 2014 was I think the entirety of Google and Alphabet, not just brain

55:56.860 --> 56:00.340
and DeepMind, take these questions very seriously of responsibility.

56:00.340 --> 56:04.500
And, you know, I kind of mantra is to try and be bold and responsible with these systems.

56:04.700 --> 56:08.500
So, you know, I would I would classify as I'm obviously a huge techno optimist,

56:08.700 --> 56:12.860
but I want us to be cautious with that, given the transformative power of what

56:13.060 --> 56:15.860
we're bringing, bringing into the world, you know, collectively.

56:15.980 --> 56:19.940
And I think it's important, you know, I think it's going to be one of the most

56:20.140 --> 56:22.380
important technologies humanity will ever invent.

56:22.580 --> 56:26.300
So we've got to put, you know, all our efforts into getting this right and be

56:26.500 --> 56:31.460
thoughtful and sort of also humble about what we know and don't know about

56:31.660 --> 56:33.980
the systems that are coming and the uncertainties around that.

56:34.180 --> 56:37.780
And in my view, the only the only sensible approach when you have huge

56:37.980 --> 56:42.220
uncertainty is to be sort of cautiously optimistic and use the scientific method

56:42.260 --> 56:45.780
to try and have as much foresight and understanding about what's coming down

56:45.780 --> 56:48.300
the line and the consequences of that before it happens.

56:48.500 --> 56:51.780
You know, you don't want to be live A, B testing out in the world with these

56:51.980 --> 56:56.380
very consequential systems, because unintended consequences may be maybe quite severe.

56:56.580 --> 57:01.820
So, you know, I want us to move away as a as a field from a sort of move fast

57:01.820 --> 57:05.060
and break things attitude, which is, you know, maybe serve the valley very well

57:05.260 --> 57:09.460
in the past and obviously created important innovations.

57:09.620 --> 57:14.340
But but I think in this case, you know, we want to be bold with the with the

57:14.340 --> 57:18.020
positive things that it can do and make sure we realize things like medicine

57:18.220 --> 57:22.500
and science and advancing all of those things whilst being, you know,

57:22.700 --> 57:27.820
responsible and thoughtful with with as far as possible with with mitigating the risks.

57:28.020 --> 57:30.660
Yeah, yeah. And that's why it seems like the responsible

57:30.660 --> 57:33.740
scaling process or something like that is a very good empirical way to

57:33.740 --> 57:34.980
pre-commit to these kinds of things.

57:34.980 --> 57:35.860
Yes, exactly.

57:35.860 --> 57:38.340
Yeah. And I'm curious if you have a sense of like, for example, when you're

57:38.420 --> 57:42.300
doing these evaluations, if it turns out your next model could help a layperson

57:42.300 --> 57:46.180
build a pandemic class or bio-weapon or something, how you would think, first of

57:46.380 --> 57:50.140
all, of making sure those weights are secure so that that doesn't get out?

57:50.140 --> 57:53.540
And second, what would have to be true for you to be comfortable deploying

57:53.540 --> 57:54.660
that system? How comfortable?

57:54.660 --> 57:58.340
Like, how would you make sure that that that lane capability isn't exposed?

57:58.340 --> 58:01.940
Yeah. Well, first, I mean, you know, the secure model part, I think we've covered

58:01.940 --> 58:05.340
with the cybersecurity and make sure that's well class and you're monitoring

58:05.340 --> 58:10.300
all those things. I think if the capability was discovered like that

58:10.300 --> 58:15.580
through red teaming or external testing by, you know, government institutes

58:15.780 --> 58:20.060
or academia or whatever, independent testers, then we would have to fix

58:20.060 --> 58:22.740
that loophole depending what it was, right?

58:23.220 --> 58:29.740
If that required more a different kind of perhaps constitution or different

58:29.740 --> 58:33.860
guardrails or more RLHF to avoid that or removing some training data,

58:34.420 --> 58:36.700
they could, I mean, depending on what the problem is, I think there could be a

58:36.700 --> 58:38.660
number of mitigations.

58:38.820 --> 58:42.780
And so the first part is making sure you detect it ahead of time.

58:42.860 --> 58:46.500
So that's about the right evaluations and right benchmarking and right and

58:46.500 --> 58:50.900
right testing. And then the question is how one would fix that before, you know,

58:50.900 --> 58:54.180
you deployed it. But I think it would need to be fixed before it was deployed

58:54.180 --> 58:57.180
generally, for sure, if that was an exposure surface.

58:57.180 --> 58:59.180
Right. Right. Final question.

59:00.380 --> 59:03.660
You know, you've been thinking in terms of like the end goal of Asia at a time

59:03.660 --> 59:06.460
when other people thought it was ridiculous in 2010, now that we're

59:06.460 --> 59:10.300
seeing this like slow takeoff where we're actually seeing these like generalization

59:10.300 --> 59:14.300
and intelligence, what is like the psychologically seeing this?

59:14.300 --> 59:15.300
What has that been like?

59:15.300 --> 59:17.260
Has it just like sort of priced into your role model?

59:17.260 --> 59:20.300
So you like it's not new news for you or is it like actually just seeing it live?

59:20.300 --> 59:23.820
You're like, wow, like this is something's like really changed or what does it feel

59:23.820 --> 59:28.220
like? Yeah, well, for me, yes, it's already priced into my world, one of how

59:28.220 --> 59:30.420
things were going to go, at least from the technology side.

59:30.420 --> 59:35.100
But obviously, I didn't we didn't necessarily anticipate the general

59:35.100 --> 59:39.580
public would be that interested this early in the sequence, right, of things

59:39.580 --> 59:44.460
like maybe one could think of if we were to produce more, if say like a chat

59:44.460 --> 59:48.740
GPT and chatbots hadn't got the kind of got the interest they'd ended up getting.

59:48.860 --> 59:51.500
So I think it was quite surprising to everyone that people were ready to use

59:51.500 --> 59:55.220
these things, even though they were lacking in certain directions, right?

59:55.220 --> 59:59.220
Impressive, though they are, then we would have produced more specialized

59:59.220 --> 01:00:03.460
systems, I think, built off of the main track, like Alpha Folds and Alpha goes

01:00:03.460 --> 01:00:05.940
and and so on and our scientific work.

01:00:06.140 --> 01:00:12.540
And then I think the general public maybe would have only paid attention

01:00:12.540 --> 01:00:15.620
later down the road, where in a few years time, we have more generally

01:00:15.620 --> 01:00:17.500
useful assistant type systems.

01:00:17.780 --> 01:00:19.060
So that's been interesting.

01:00:19.060 --> 01:00:22.380
So that's created a different type of environment that we're now all

01:00:22.380 --> 01:00:25.260
operating in as a field.

01:00:25.420 --> 01:00:29.100
So I mean, it's a little bit more chaotic because there's so many more things

01:00:29.100 --> 01:00:33.300
going on and there's so much VC money going into it and everyone sort of

01:00:33.460 --> 01:00:37.740
almost losing their minds over it, I think, and what I just the thing I

01:00:37.740 --> 01:00:41.860
worry about is I want to make sure that as a field, we act responsibly

01:00:41.860 --> 01:00:45.660
and thoughtfully and scientifically about this and use the scientific

01:00:45.660 --> 01:00:50.540
method to approach this in a in a, as I said, an optimistic, but careful way.

01:00:50.660 --> 01:00:54.340
And I think that's the I've always believed that's the right approach for

01:00:54.380 --> 01:00:59.300
something like AI, and I just hope that doesn't get lost in this huge rush.

01:00:59.380 --> 01:01:00.060
Sure, sure.

01:01:00.220 --> 01:01:01.580
Well, I think that's a great place to close.

01:01:01.620 --> 01:01:02.540
Dennis, so much thanks to you.

01:01:02.540 --> 01:01:04.220
Thank you so much for your time and for coming on the podcast.

01:01:04.220 --> 01:01:04.500
Thanks.

01:01:04.500 --> 01:01:05.260
It's been a real pleasure.

01:01:07.420 --> 01:01:09.820
Hey, everybody, I hope we enjoyed that episode.

01:01:10.380 --> 01:01:14.060
As always, the most helpful thing you can do is to share the podcast,

01:01:14.380 --> 01:01:17.740
send it to people you think might enjoy it, put it in Twitter, your group chats,

01:01:17.740 --> 01:01:21.300
et cetera, just splits the world, appreciate your listening.

01:01:21.340 --> 01:01:22.300
I'll see you next time.

01:01:22.460 --> 01:01:22.900
Cheers.

