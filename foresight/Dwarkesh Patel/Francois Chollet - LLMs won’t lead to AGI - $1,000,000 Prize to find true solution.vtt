WEBVTT

00:00.000 --> 00:03.000
LLMs are very good at memorizing static programs.

00:03.000 --> 00:05.880
If you scale up the size of your database,

00:05.880 --> 00:09.160
you are not increasing the intelligence of the system

00:09.160 --> 00:09.640
one bit.

00:09.640 --> 00:11.520
I feel like you're using words like memorization, which

00:11.520 --> 00:12.960
we would never use for human children.

00:12.960 --> 00:16.120
If they can just solve any arbitrary algebraic problem,

00:16.120 --> 00:17.920
you wouldn't say they've memorized algebra.

00:17.920 --> 00:19.360
They'd say they've learned algebra.

00:19.360 --> 00:20.880
So you've got a million dollar price pool,

00:20.880 --> 00:23.800
and there's a $500,000 price for the first team that

00:23.800 --> 00:25.920
can get to the 85% benchmark.

00:25.920 --> 00:29.000
If ARC survives three months from here, we'll pull up the price.

00:29.000 --> 00:32.120
Open AI basically set back progress towards HGI

00:32.120 --> 00:33.800
by probably like five to 10 years.

00:33.800 --> 00:35.880
They caused this complete closing down

00:35.880 --> 00:37.320
of frontier research publishing.

00:37.320 --> 00:41.400
And now LLMs have sucked the oxygen out of the room,

00:41.400 --> 00:44.000
like everyone is just doing LLMs.

00:44.000 --> 00:46.880
OK, today I have the pleasure to speak

00:46.880 --> 00:51.600
with Francois Chollet, who is a AI researcher at Google

00:51.600 --> 00:53.280
and creator of Keras.

00:53.280 --> 00:56.360
And he's launching a prize in collaboration

00:56.360 --> 00:58.560
with Mike Canouf, the co-founder of Xavier,

00:58.560 --> 01:00.280
who we'll also be talking to in a second,

01:00.280 --> 01:03.840
a million dollar prize to solve the ARC benchmark

01:03.840 --> 01:04.720
that he created.

01:04.720 --> 01:06.960
So first question, what is the ARC benchmark,

01:06.960 --> 01:08.440
and why do we even need this prize?

01:08.440 --> 01:10.120
Why won't the biggest LLM we have in a year

01:10.120 --> 01:11.800
be able to just saturate it?

01:11.800 --> 01:12.360
Sure.

01:12.360 --> 01:15.120
So ARC is intended as a kind of IQ test

01:15.120 --> 01:16.640
for machine intelligence.

01:16.640 --> 01:20.080
And what makes it different from most LLM benchmarks out there

01:20.080 --> 01:23.800
is that it's designed to be resistant to memorization.

01:23.800 --> 01:25.880
So if you look at the way LLMs work,

01:25.880 --> 01:29.280
they're basically this big interpolative memory.

01:29.280 --> 01:31.480
And the way you scale up their capabilities

01:31.480 --> 01:35.080
is by trying to cram as much knowledge and patterns

01:35.080 --> 01:36.880
as possible into them.

01:36.880 --> 01:41.440
And by contrast, ARC does not require a lot of knowledge

01:41.440 --> 01:42.240
at all.

01:42.240 --> 01:43.960
It's designed to only require what's

01:43.960 --> 01:48.360
known as core knowledge, which is basic knowledge about things

01:48.360 --> 01:52.360
like elementary physics, objectness, counting,

01:52.360 --> 01:54.640
that sort of thing, the sort of knowledge

01:54.640 --> 01:59.200
that any four-year-old or five-year-old possesses.

01:59.200 --> 02:02.600
But what's interesting is that each puzzle in ARC

02:02.600 --> 02:06.080
is novel, is something that you've probably not encountered

02:06.080 --> 02:09.680
before, even if you've memorized the entire internet.

02:09.680 --> 02:15.800
And that's what makes ARC challenging for LLMs.

02:15.800 --> 02:19.400
And so far, LLMs have not been doing very well on it.

02:19.400 --> 02:21.720
In fact, the approaches that are working well

02:21.720 --> 02:24.400
are more towards discrete program search, program

02:24.400 --> 02:25.760
synthesis.

02:25.760 --> 02:28.320
So first of all, I'll make a comment

02:28.320 --> 02:30.720
that I'm glad that as a skeptic of LLM,

02:30.720 --> 02:35.640
you have put out yourself a benchmark that is it accurate

02:35.640 --> 02:38.880
to say that, suppose that the biggest model we have in a year

02:38.880 --> 02:41.800
is able to get 80% on this, then your view would be

02:41.800 --> 02:43.880
we are on track to AGI with LLMs.

02:43.880 --> 02:45.240
How would you think about that?

02:45.240 --> 02:47.400
Right.

02:47.400 --> 02:48.840
I'm pretty skeptical that we're going

02:48.840 --> 02:51.600
to see LLM do 80% in a year.

02:51.600 --> 02:53.720
That said, if we do see it, you would also

02:53.720 --> 02:56.120
have to look at how this was achieved.

02:56.120 --> 03:00.320
If you just train the model and millions or billions

03:00.320 --> 03:02.440
of puzzles similar to ARC so that you're

03:02.440 --> 03:07.440
relying on the ability to have some overlap between the tasks

03:07.440 --> 03:08.800
that you train on and the tasks that you're

03:08.800 --> 03:10.520
going to see at this time, then you're still

03:10.520 --> 03:12.480
using memorization.

03:12.480 --> 03:14.440
And maybe it can work.

03:14.440 --> 03:17.520
Hopefully, ARC is going to be good enough

03:17.560 --> 03:20.520
that it's going to be resistant to this sort of attempt

03:20.520 --> 03:22.280
at brute forcing.

03:22.280 --> 03:23.280
But you never know.

03:23.280 --> 03:24.560
Maybe it could happen.

03:24.560 --> 03:26.320
I'm not saying it's not going to happen.

03:26.320 --> 03:28.200
ARC is not a perfect benchmark.

03:28.200 --> 03:30.200
Maybe it has flaws.

03:30.200 --> 03:33.120
Maybe it could be hacked in that way.

03:33.120 --> 03:37.120
So I guess I'm curious about what would GPTI

03:37.120 --> 03:40.760
have to do that you're very confident that it's

03:40.760 --> 03:42.160
on the path to AGI?

03:42.160 --> 03:44.720
What would make me change my mind about LLMs

03:44.720 --> 03:49.920
is basically, if I start seeing a critical mass of cases

03:49.920 --> 03:52.120
where you show the model with something

03:52.120 --> 03:55.080
it has not seen before, a task that's actually

03:55.080 --> 03:57.920
novel from the perspective of its training data, something

03:57.920 --> 03:59.960
that's not in the training data, and if it can actually

03:59.960 --> 04:02.880
adapt on the fly.

04:02.880 --> 04:03.960
And this is true for LLMs.

04:03.960 --> 04:06.080
But really, this would catch my attention

04:06.080 --> 04:08.840
with any for any AI technique out there.

04:08.840 --> 04:13.600
If I can see the ability to adapt to novelty on the fly

04:13.600 --> 04:15.600
to pick up new skills efficiently,

04:15.600 --> 04:18.040
then I would be extremely interested.

04:18.040 --> 04:21.440
I would think this is on the path to AGI.

04:21.440 --> 04:24.680
So the advantage they have is that they do get to see everything.

04:24.680 --> 04:27.640
Maybe I'll take issue with how much they are relying on that.

04:27.640 --> 04:29.040
But let's suppose that they are relying.

04:29.040 --> 04:32.200
Obviously, they're relying on that more than humans do.

04:32.200 --> 04:35.560
To the extent that they do have so much indistribution,

04:35.560 --> 04:37.840
to the extent that we have trouble distinguishing

04:37.840 --> 04:41.520
whether an example is indistribution or not,

04:41.520 --> 04:43.240
well, if they have everything in distribution,

04:43.240 --> 04:45.280
then they can do everything that we can do.

04:45.280 --> 04:47.720
Maybe it's not indistribution for us.

04:47.720 --> 04:50.160
Why is it so crucial that it has to be out of distribution

04:50.160 --> 04:51.520
for them?

04:51.520 --> 04:52.920
Why can't we just leverage the fact

04:52.920 --> 04:54.440
that they do get to see everything?

04:54.440 --> 04:55.680
Right.

04:55.680 --> 04:57.560
You're asking basically what's the difference

04:57.560 --> 04:59.280
between actual intelligence, which

04:59.280 --> 05:01.920
is the ability to adapt to things you've not been prepared

05:01.920 --> 05:06.040
for, and pure memorization, like reciting what you've seen

05:06.040 --> 05:06.920
before.

05:06.920 --> 05:10.120
And it's not just some semantic difference.

05:10.120 --> 05:13.840
The big difference is that you can never

05:13.840 --> 05:18.520
pre-train on everything that you might see at test time,

05:18.520 --> 05:20.840
because the world changes all the time.

05:20.840 --> 05:24.400
So it's not just the fact that the space of possible tasks

05:24.400 --> 05:25.280
is infinite.

05:25.280 --> 05:28.080
And even if you're trained on millions of them,

05:28.080 --> 05:30.120
you've only seen zero person out of the total space.

05:30.120 --> 05:34.720
It's also the fact that the world is changing every day.

05:34.720 --> 05:37.800
This is why we, the human species,

05:37.840 --> 05:40.480
developed intelligence in the first place.

05:40.480 --> 05:44.720
If there was a shifting as a distribution for the world,

05:44.720 --> 05:47.080
for the universe, for our lives, then we would not

05:47.080 --> 05:48.400
need intelligence at all.

05:48.400 --> 05:51.840
In fact, many creatures, many insects, for instance,

05:51.840 --> 05:53.440
do not have intelligence.

05:53.440 --> 05:58.440
Instead, what they have is they have in their connectome,

05:58.440 --> 06:01.120
in their genes, hard-coded programs,

06:01.120 --> 06:03.800
behavioral programs that map some stimuli

06:03.800 --> 06:05.320
to appropriate response.

06:05.320 --> 06:07.720
And they can actually navigate their lives

06:07.720 --> 06:11.160
to environments in a way that's very evolutionary fit.

06:11.160 --> 06:14.400
That way, without needing to learn anything.

06:14.400 --> 06:17.560
And while if our environment was static enough,

06:17.560 --> 06:19.920
predictable enough, what would have happened

06:19.920 --> 06:21.960
is that evolution would have found

06:21.960 --> 06:24.400
the perfect behavioral program, a hard-coded,

06:24.400 --> 06:25.720
static behavioral program.

06:25.720 --> 06:28.480
We'd have written it into our genes.

06:28.480 --> 06:30.880
We would have a hard-coded brain connectome.

06:30.880 --> 06:32.240
And that's what we were running on.

06:32.240 --> 06:33.440
But no, that's not what happened.

06:33.440 --> 06:36.240
Instead, we have general intelligence.

06:36.280 --> 06:40.360
We are born with extremely little knowledge about the world.

06:40.360 --> 06:43.760
But we are born with the ability to learn very efficiently

06:43.760 --> 06:45.960
and to adapt in the face of things

06:45.960 --> 06:47.720
that we've never seen before.

06:47.720 --> 06:48.920
And that's what makes us unique.

06:48.920 --> 06:51.960
And that's what is really, really challenging

06:51.960 --> 06:53.640
to recreate in machines.

06:53.640 --> 06:55.240
I want to wrap it all in that a little bit.

06:55.240 --> 06:57.760
But before I do that, maybe I'm going

06:57.760 --> 06:59.920
to overlay some examples of what an arc-like challenge looks

06:59.920 --> 07:01.760
like for the YouTube audience.

07:01.760 --> 07:03.800
But maybe for people listening on audio,

07:03.800 --> 07:06.680
can you just describe what an example arc challenge

07:06.680 --> 07:07.160
will look like?

07:07.160 --> 07:07.640
Sure.

07:07.640 --> 07:11.800
So one arc puzzle, it looks kind of like an IQ test puzzle.

07:11.800 --> 07:15.480
You've got a number of demonstration input-adput pairs.

07:15.480 --> 07:19.200
So one pair is made of two grids.

07:19.200 --> 07:21.800
So one grid shows you an input.

07:21.800 --> 07:24.040
And the second grid shows you what

07:24.040 --> 07:27.840
you should produce as a response to that input.

07:27.840 --> 07:30.600
And you get a couple pairs like this

07:30.600 --> 07:32.480
to demonstrate the nature of the task,

07:32.480 --> 07:35.080
to demonstrate what you're supposed to do with your inputs.

07:35.080 --> 07:39.160
And then you get a new test input.

07:39.160 --> 07:43.000
And your job is to produce the corresponding test outputs.

07:43.000 --> 07:44.920
You look at the demonstration pairs.

07:44.920 --> 07:48.640
And from that, you figure out what you're supposed to do.

07:48.640 --> 07:51.240
And you show that you've understood it on this new test

07:51.240 --> 07:52.800
pair.

07:52.800 --> 07:57.960
And importantly, in order to the knowledge basis

07:57.960 --> 08:01.640
that you need, in order to approach these challenges,

08:01.640 --> 08:03.400
is you just need core knowledge.

08:03.400 --> 08:06.200
And core knowledge is basically the knowledge

08:06.200 --> 08:10.760
of what makes an object, basic counting, basic geometry,

08:10.760 --> 08:13.400
topology, symmetries, that sort of thing.

08:13.400 --> 08:15.360
So extremely basic knowledge.

08:15.360 --> 08:17.640
LLMs for sure possess such knowledge.

08:17.640 --> 08:21.480
Any child possesses such knowledge.

08:21.480 --> 08:24.800
And what's really interesting is that each puzzle is new.

08:24.800 --> 08:26.040
So it's not something that you're

08:26.040 --> 08:31.000
going to find elsewhere on the internet, for instance.

08:31.040 --> 08:34.360
And that means that whether it's as a human or as a machine,

08:34.360 --> 08:37.520
every puzzle you have to approach it from scratch.

08:37.520 --> 08:39.520
You have to actually reason your way through it.

08:39.520 --> 08:43.000
You cannot just fetch the response from your memory.

08:43.000 --> 08:47.640
So the core knowledge, one contention here

08:47.640 --> 08:52.600
is we are only now getting multimodal models who,

08:52.600 --> 08:54.520
because of the data that are trained on,

08:54.520 --> 08:57.280
are trained to do spatial reasoning.

08:57.280 --> 08:59.280
Whereas, obviously, not only humans,

08:59.280 --> 09:01.760
but for billions of years of revolution,

09:01.760 --> 09:04.200
we've had our ancestors have had to learn

09:04.200 --> 09:08.600
how to understand abstract, physical, and spatial properties

09:08.600 --> 09:10.600
and recognize the patterns there.

09:10.600 --> 09:14.920
And so one view would be, in the next year,

09:14.920 --> 09:17.400
as we gain models that are multimodal native,

09:17.400 --> 09:20.840
that isn't just a second class that is an add-on,

09:20.840 --> 09:23.600
but the multimodal capability is a priority.

09:23.600 --> 09:26.600
That it will understand these kinds of patterns

09:26.600 --> 09:28.480
because that's something we see natively.

09:28.480 --> 09:31.800
Whereas, right now, what Arc sees is some JSON string

09:31.800 --> 09:35.600
of 100100, and it's supposed to recognize a pattern there.

09:35.600 --> 09:39.720
And even if you showed a sequence of these kinds of numbers,

09:39.720 --> 09:41.600
it would have a challenge making sense

09:41.600 --> 09:44.240
of what kind of question you're asking it.

09:44.240 --> 09:46.080
So why want it to be the case that,

09:46.080 --> 09:47.480
as soon as we get multimodal models,

09:47.480 --> 09:49.440
which we're on the path to unlock right now,

09:49.440 --> 09:50.360
they're going to be so much better

09:50.360 --> 09:52.000
at Arc-type spatial reasoning?

09:52.000 --> 09:53.400
That's an incredibly cool question,

09:53.400 --> 09:54.800
so I guess we're going to see the answer

09:54.800 --> 09:55.920
within a few months.

09:55.920 --> 09:59.160
But my answer to that is Arc grids,

09:59.160 --> 10:02.240
they're just discrete 2D grids of symbols.

10:02.240 --> 10:05.280
They're pretty small, like it's not like...

10:05.280 --> 10:08.280
If you flatten an image as a sequence of pixels,

10:08.280 --> 10:10.200
for instance, then you get something

10:10.200 --> 10:12.120
that's actually very, very difficult to parse.

10:12.120 --> 10:15.200
But that's not true for Arc because the grids are very small.

10:15.200 --> 10:17.120
You only have 10 possible symbols.

10:17.120 --> 10:18.840
So there's these 2D grids that are actually

10:18.840 --> 10:21.360
very easy to flatten as sequences.

10:21.360 --> 10:23.480
And transformers, LLMs, they're very good

10:23.480 --> 10:24.560
at processing the sequences.

10:24.560 --> 10:29.240
In fact, you can show that LLMs do fine

10:29.240 --> 10:32.240
with processing Arc-like data

10:32.240 --> 10:37.240
by simply fine-tuning LLMs on some subsets of the tasks

10:38.440 --> 10:42.160
and then trying to test it on small variations

10:42.160 --> 10:43.240
of these tasks.

10:43.240 --> 10:46.240
And you see that, yeah, the LLMs can encode

10:46.240 --> 10:49.440
just fine solution programs for tasks

10:49.440 --> 10:50.800
that they've seen before.

10:50.800 --> 10:52.560
So it does not really have a problem

10:52.560 --> 10:57.560
parsing the input or figuring out the program.

10:57.640 --> 11:01.400
The reason why LLMs don't do well on Arc

11:01.400 --> 11:04.640
is really just the unfamiliarity aspect.

11:04.640 --> 11:07.560
The fact that each new task is different

11:07.560 --> 11:09.240
from every other task.

11:09.240 --> 11:11.880
You cannot, basically, you cannot memorize

11:11.880 --> 11:13.840
the solution programs in advance.

11:13.840 --> 11:16.640
You have to synthesize a new solution program

11:16.640 --> 11:18.480
on the fly for each new task.

11:18.480 --> 11:20.800
And that's really what LLMs are struggling with.

11:20.840 --> 11:22.480
So before I do more devil's advocate,

11:22.480 --> 11:24.200
I just want to step back and explain

11:24.200 --> 11:27.240
why I'm especially interested in having this conversation.

11:27.240 --> 11:29.480
And obviously the million dollar Arc prize,

11:29.480 --> 11:31.920
I'm excited to actually play with it myself.

11:31.920 --> 11:35.400
And hopefully the Vesuvius challenge,

11:35.400 --> 11:39.760
which was Nat Friedman's prize for solving decoding scrolls,

11:39.760 --> 11:42.000
the winner of that, decoding the scrolls from

11:42.000 --> 11:43.600
that were buried in the volcanoes

11:43.600 --> 11:46.760
in the Herculaneum library that was solved

11:46.760 --> 11:48.680
by a 22 year old who was listening

11:48.680 --> 11:49.840
to the podcast, Luke Farator.

11:49.840 --> 11:51.800
So hopefully somebody listening will find

11:51.800 --> 11:54.240
this challenge intriguing and find a solution.

11:54.240 --> 11:58.200
So I'm, and the reason I've had on recently

11:58.200 --> 12:01.680
a lot of people who are bullish on LLMs

12:01.680 --> 12:03.560
and I've had discussions with them

12:03.560 --> 12:05.640
before interviewing you about how do we explain the fact

12:05.640 --> 12:07.440
that LLMs don't seem to be natively performing

12:07.440 --> 12:08.800
that well on Arc.

12:08.800 --> 12:12.440
And I found their explanations somewhat contrived

12:12.440 --> 12:15.120
and I'll try out some of the reasons on you.

12:15.120 --> 12:17.640
But it is actually an intriguing fact

12:17.840 --> 12:18.960
that they actually, these are,

12:18.960 --> 12:21.120
some of these problems are relatively straightforward

12:21.120 --> 12:22.400
for humans to understand.

12:22.400 --> 12:24.360
And they do struggle with them

12:24.360 --> 12:25.920
if you just input them natively.

12:25.920 --> 12:27.640
All of them are very easy for humans.

12:27.640 --> 12:29.760
Like any smart human should be able

12:29.760 --> 12:32.560
to do 90%, 95% on Arc.

12:32.560 --> 12:33.400
Smart human.

12:33.400 --> 12:35.360
A smart human, but even a five year old.

12:35.360 --> 12:37.320
So with very, very little knowledge,

12:37.320 --> 12:40.280
they could definitely do over 50%.

12:40.280 --> 12:44.560
So let's talk about that because you,

12:45.480 --> 12:48.480
I agree that smart humans will do very well on this test,

12:48.480 --> 12:53.480
but the average human will probably do mediocre.

12:53.680 --> 12:54.640
Not really.

12:54.640 --> 12:56.920
So we actually tried with average humans,

12:56.920 --> 12:58.280
the score about 85.

12:58.280 --> 13:00.640
That was with Amazon Mechanical Turk workers, right?

13:00.640 --> 13:02.800
I honestly don't know the demographic profile

13:02.800 --> 13:04.040
of Amazon Mechanical Turk workers,

13:04.040 --> 13:07.320
but imagine just interacting with the platform

13:07.320 --> 13:09.080
that Amazon has set up to do remote work.

13:09.080 --> 13:11.920
That's not the median human across the planet, I'm guessing.

13:12.120 --> 13:14.680
I mean, the broader point here being that,

13:14.680 --> 13:17.200
so we see the spectrum in humans

13:17.200 --> 13:20.160
where humans obviously have AGI,

13:20.160 --> 13:22.000
but even within humans, you see a spectrum

13:22.000 --> 13:24.200
where some people are relatively dumber

13:24.200 --> 13:27.920
and they'll do perform work on IQ like tests.

13:27.920 --> 13:29.480
For example, Raven's progressive matrices.

13:29.480 --> 13:31.680
If you look at how the average person performs on that

13:31.680 --> 13:32.920
and you look at the quick kind of questions

13:32.920 --> 13:34.400
that is this sort of midtermists,

13:34.400 --> 13:35.360
half of people will get it right,

13:35.360 --> 13:36.360
half of people will get it wrong.

13:36.360 --> 13:37.800
Some of them are like pretty trivial.

13:37.800 --> 13:40.560
For us, we might think like this was kind of trivial.

13:40.560 --> 13:42.200
And so humans have AGI,

13:42.200 --> 13:45.200
but from relatively small tweaks,

13:45.200 --> 13:47.440
you can go from somebody who misses

13:47.440 --> 13:48.920
these kinds of basic IQ test questions

13:48.920 --> 13:50.240
to somebody who gets them all right,

13:50.240 --> 13:51.560
which suggests that actually,

13:51.560 --> 13:54.480
if these models are doing natively,

13:54.480 --> 13:56.240
we'll talk about some of the previous performances

13:56.240 --> 13:57.120
that people have tried with these models,

13:57.120 --> 13:59.000
but somebody with a Jack Cole

13:59.000 --> 14:02.800
with a 240 million parameter model got 35%.

14:02.800 --> 14:05.080
Doesn't that suggest that they're on this spectrum

14:05.080 --> 14:06.480
that clearly exists within humans

14:06.480 --> 14:08.560
and they're gonna be saturated at pretty soon?

14:08.560 --> 14:11.360
Yeah, so that's a bunch of interesting points here.

14:11.360 --> 14:16.360
So there is indeed a branch of LLM approaches

14:16.800 --> 14:19.960
suspended by Jack Cole that are doing quite well,

14:19.960 --> 14:23.000
that are in fact a state of the art.

14:23.000 --> 14:25.520
But you have to look at what's going on there.

14:25.520 --> 14:26.360
So there are two things.

14:26.360 --> 14:29.480
The first thing is that to guess these numbers,

14:29.480 --> 14:31.840
you need to pre-train your LLM

14:31.840 --> 14:34.520
on millions of generated art tasks.

14:34.520 --> 14:37.840
And of course, if you compare that to a five-year-old child

14:37.880 --> 14:39.680
looking at art for the first time,

14:39.680 --> 14:41.400
the child has never done like you did before,

14:41.400 --> 14:44.040
has never seen something like an art task before.

14:44.040 --> 14:46.160
The only overlap between what they know

14:46.160 --> 14:49.320
and what they have to do in the test is core knowledge,

14:49.320 --> 14:51.560
is knowing about like counting and objects

14:51.560 --> 14:53.040
and symmetries and things like that.

14:53.040 --> 14:56.160
And still, they're gonna do really well

14:56.160 --> 14:57.880
and they're gonna do much better than the LLM

14:57.880 --> 15:00.560
trained on millions of similar tasks.

15:00.560 --> 15:03.400
And the second thing that's something to note

15:03.400 --> 15:07.360
about the Jack Cole approach is one thing

15:07.360 --> 15:10.520
that's really critical to making the model work at all

15:10.520 --> 15:12.560
is test time fine tuning.

15:12.560 --> 15:14.320
And that's something that's really missing, by the way,

15:14.320 --> 15:19.320
from LLM approaches right now is that, you know,

15:19.360 --> 15:21.680
most of the time when you're using an LLM,

15:21.680 --> 15:23.760
it's just doing static inference.

15:23.760 --> 15:26.920
The model is frozen and you're just prompting it

15:26.920 --> 15:28.280
and then you're getting an answer.

15:28.280 --> 15:31.400
So the model is not actually learning anything on the fly.

15:31.400 --> 15:35.880
Its state is not adapting to the task at hand.

15:36.120 --> 15:38.280
What Jack Cole is actually doing is that

15:38.280 --> 15:41.960
for every test problem is on the fly,

15:41.960 --> 15:46.960
is fine tuning a version of the LLM for that task.

15:47.080 --> 15:48.920
And that's really what's unlocking performance.

15:48.920 --> 15:51.720
If you don't do that, you get like 1%, 2%.

15:51.720 --> 15:55.000
So basically something completely negligible.

15:55.000 --> 15:57.080
And if you do test time fine tuning

15:57.080 --> 15:58.880
and you add a bunch of tricks on top,

15:58.880 --> 16:01.080
then you end up with interesting performance numbers.

16:01.080 --> 16:04.240
So I think what he's doing is trying to address

16:04.240 --> 16:07.080
one of the key limitations of LLMs today,

16:07.080 --> 16:08.680
which is the lack of active inference,

16:08.680 --> 16:11.600
is actually adding active inference to LLMs.

16:11.600 --> 16:13.360
And that's working extremely well actually.

16:13.360 --> 16:14.800
So that's fascinating to me.

16:14.800 --> 16:17.200
That there's so many interesting rabbit holes there.

16:18.360 --> 16:20.120
Should I take them in sequence or deal with them all at once?

16:20.120 --> 16:21.200
Let me just start.

16:21.200 --> 16:24.480
So the point you made about the fact

16:24.480 --> 16:26.880
that you need to unlock the adapter compute

16:26.880 --> 16:30.640
slash test time compute, a lot of the scale maximalist,

16:30.640 --> 16:31.800
I think this will be interesting rabbit hole

16:31.800 --> 16:32.720
to explore with you,

16:32.720 --> 16:35.040
because a lot of the scaling maximalist

16:35.040 --> 16:37.600
have your broader perspective in the sense

16:37.600 --> 16:40.400
that they think that in addition to scaling,

16:40.400 --> 16:41.960
you need these kinds of things,

16:41.960 --> 16:44.080
like unlocking adaptive compute

16:44.080 --> 16:47.400
or doing some sort of RL to get the system to working.

16:47.400 --> 16:49.920
And their perspective is that this is a relatively

16:49.920 --> 16:52.320
straightforward thing that will be added atop

16:52.320 --> 16:55.440
the representations that a scaled up model

16:55.440 --> 16:57.880
has greater access to.

16:57.880 --> 17:00.400
No, it's not just a technical detail.

17:00.400 --> 17:01.840
It's not a straightforward thing.

17:01.840 --> 17:03.240
It is everything.

17:03.240 --> 17:05.120
It is the important part.

17:05.120 --> 17:08.800
And the scale maximalist argument,

17:08.800 --> 17:13.080
you know, it boils down to, you know,

17:13.080 --> 17:15.320
these people, they refer to scaling loss,

17:15.320 --> 17:17.440
which is this empirical relationship

17:17.440 --> 17:19.720
that you can draw between how much compute

17:19.720 --> 17:21.000
you spend on training a model

17:21.000 --> 17:23.600
and the performance you're getting on benchmarks, right?

17:23.600 --> 17:26.320
And the key question here, of course, is,

17:26.320 --> 17:28.160
well, how do you measure performance?

17:28.160 --> 17:31.000
What it is that you're actually improving

17:31.000 --> 17:33.080
by adding more compute and more data?

17:33.080 --> 17:35.480
And, well, it's benchmark performance, right?

17:35.480 --> 17:38.400
And the thing is, the way you measure performance

17:38.400 --> 17:41.360
is not a technical detail.

17:41.360 --> 17:46.160
It's not an afterthought because it's gonna narrow down

17:46.160 --> 17:47.840
the set of questions that you're asking.

17:47.840 --> 17:50.400
And so, accordingly, it's gonna narrow down

17:50.400 --> 17:53.280
the set of answers that you're looking for.

17:53.280 --> 17:56.840
If you look at the benchmarks we're using for LMS,

17:56.840 --> 17:59.640
they're all memorization-based benchmarks.

17:59.640 --> 18:01.880
Like, sometimes they are literally just knowledge-based,

18:01.880 --> 18:03.440
like a school test.

18:03.440 --> 18:04.760
And even if you look at the ones

18:04.760 --> 18:08.920
that are, you know, explicitly about reasoning,

18:08.920 --> 18:10.640
you realize, if you look closely,

18:10.640 --> 18:13.000
that it's, in order to solve them,

18:13.000 --> 18:18.000
it's enough to memorize a finite set of reasoning patterns.

18:19.080 --> 18:20.520
And then you just reapply them.

18:20.520 --> 18:22.840
They're like static programs.

18:22.840 --> 18:25.600
LMS are very good at memorizing static programs,

18:25.600 --> 18:26.560
small static programs.

18:26.560 --> 18:31.560
And they've got this sort of like bank of solution programs.

18:31.760 --> 18:33.720
And when you give them a new puzzle,

18:33.720 --> 18:37.240
they can just fetch the appropriate program, apply it.

18:37.240 --> 18:39.160
And it's looking like it's reasoning,

18:39.160 --> 18:41.000
but really it's not doing any sort of

18:41.000 --> 18:42.640
on-the-flight program synthesis.

18:42.640 --> 18:45.280
All it's doing is program fetching.

18:45.280 --> 18:47.400
So you can actually solve all these benchmarks

18:47.400 --> 18:48.680
with memorization.

18:48.680 --> 18:51.800
And so, what you're scaling up here,

18:51.800 --> 18:53.360
like if you look at the models,

18:53.360 --> 18:56.680
they are big parametric curves

18:56.680 --> 18:58.320
fitted to a data distribution,

18:58.320 --> 18:59.480
which I call in descent.

18:59.480 --> 19:03.600
So they're basically these big interpolative databases,

19:03.600 --> 19:04.960
interpolative memories.

19:04.960 --> 19:08.560
And of course, if you scale up the size of your database

19:08.560 --> 19:11.400
and you cram into it more knowledge,

19:11.400 --> 19:13.640
more patterns and so on,

19:13.640 --> 19:16.760
you are gonna be increasing its performance

19:16.760 --> 19:19.400
as measured by a memorization benchmark.

19:19.400 --> 19:20.960
That's kind of obvious.

19:20.960 --> 19:22.440
But as you're doing it,

19:22.440 --> 19:25.080
you are not increasing the intelligence

19:25.080 --> 19:26.760
of the system one bit.

19:26.760 --> 19:28.640
You are increasing the skill of the system.

19:28.640 --> 19:30.600
You are increasing its usefulness,

19:30.600 --> 19:34.360
its scope of applicability, but not its intelligence

19:34.360 --> 19:36.640
because skill is not intelligence.

19:36.640 --> 19:38.480
And that's the fundamental confusion

19:39.480 --> 19:42.120
that people run into is that

19:42.120 --> 19:44.160
they're confusing skill and intelligence.

19:44.160 --> 19:45.640
Yeah, there's a lot of fascinating things

19:45.640 --> 19:46.480
to talk about here.

19:46.480 --> 19:50.640
So skill, intelligence, interpolation.

19:50.640 --> 19:52.240
I mean, okay, so the thing about

19:52.240 --> 19:56.720
they're fitting some manifold into that maps the input data,

19:56.720 --> 19:58.440
there's a reductionist way to talk about what happens

19:58.440 --> 19:59.960
in the human brain that says

19:59.960 --> 20:03.200
that it's just axons firing at each other.

20:03.200 --> 20:05.880
But we don't care about the reductionist explanation

20:05.880 --> 20:06.720
of what's happening.

20:06.720 --> 20:11.200
We care about what the sort of meta at the

20:11.200 --> 20:13.840
macroscopic level, what happens when these things combine.

20:13.840 --> 20:15.960
As far as the interpolation goes,

20:15.960 --> 20:19.120
so okay, let's look at one of the benchmarks here.

20:19.120 --> 20:22.680
There's one benchmark that does great school math

20:22.680 --> 20:26.720
and these are problems that like a smart high schooler

20:26.720 --> 20:28.480
would be able to solve.

20:28.480 --> 20:31.760
It's called GSM 8K and these models get 95% on these.

20:31.760 --> 20:33.200
Like basically they always nail it.

20:33.200 --> 20:34.320
That's memorization benchmark.

20:34.320 --> 20:35.520
Okay, let's talk about what that means.

20:35.520 --> 20:38.480
So here's one question about from that benchmark.

20:38.480 --> 20:40.120
So 30 students are in a class,

20:40.120 --> 20:41.880
one fifth of them are 12 year olds,

20:41.880 --> 20:45.040
one third are 13 year old, one 10th are 11 year olds.

20:45.040 --> 20:48.320
How many of them are not 11, 12 or 13 years old?

20:48.320 --> 20:50.440
So I agree, like this is not rocket science, right?

20:50.440 --> 20:53.200
You can write down on paper how you go through this problem

20:53.200 --> 20:54.600
and a high school kid,

20:54.600 --> 20:56.640
at least a smart high school kid should be able to solve it.

20:56.640 --> 20:58.920
Now, when you say memorization,

20:58.920 --> 21:03.000
it still has to reason through how to think about fractions

21:03.000 --> 21:05.000
and what is the context of the whole problem

21:05.000 --> 21:07.840
and then combining the different calculations it's doing.

21:07.840 --> 21:10.200
It depends how you want to define reasoning,

21:10.200 --> 21:12.200
but there are two definitions you can use.

21:12.200 --> 21:17.200
So one is I have available a set of program templates.

21:17.680 --> 21:21.200
It's like the structure of the puzzle,

21:21.200 --> 21:22.960
which can also generate its solution.

21:22.960 --> 21:25.600
And I'm just gonna identify the right template,

21:25.600 --> 21:26.960
which is in my memory.

21:27.800 --> 21:29.840
I'm gonna input the new values into the template,

21:29.840 --> 21:31.840
run the program, get the solution.

21:31.840 --> 21:33.480
And you could say this is reasoning.

21:33.480 --> 21:35.400
And I say, yeah, sure, okay.

21:35.400 --> 21:37.560
But another definition you can use is reasoning

21:37.560 --> 21:41.280
is the ability to, when you're faced with a puzzle,

21:41.280 --> 21:44.480
given that you don't have already a program in memory

21:44.480 --> 21:49.480
to solve it, you must synthesize on-the-fly a new program

21:49.680 --> 21:52.960
based on bits of pieces of existing programs that you have.

21:52.960 --> 21:55.480
You have to do on-the-fly program synthesis.

21:55.480 --> 21:57.280
And it's actually dramatically harder

21:57.280 --> 22:00.040
than just fetching the right memorized program

22:00.040 --> 22:01.320
and replying it.

22:01.320 --> 22:04.560
So I think maybe we are overestimating

22:04.560 --> 22:06.840
the extent to which humans are so sample efficient.

22:06.840 --> 22:10.200
They also don't need training in this way

22:10.200 --> 22:12.760
where they have to drill in these kinds

22:12.800 --> 22:16.760
of pathways of reasoning through certain kinds of problems.

22:16.760 --> 22:19.200
So let's take math, for example.

22:19.200 --> 22:21.240
It's not like you can just show a baby

22:21.240 --> 22:22.840
the axioms of set theory.

22:22.840 --> 22:24.080
And now they know math, right?

22:24.080 --> 22:25.680
So when they're growing up,

22:25.680 --> 22:28.120
you had to do years of teaching them pre-algebra.

22:28.120 --> 22:30.360
Then you got to do a year of teaching them doing drills

22:30.360 --> 22:32.240
and going through the same kind of problem in algebra,

22:32.240 --> 22:35.040
then geometry, pre-calculus, calculus.

22:35.040 --> 22:36.520
Absolutely, so training?

22:36.520 --> 22:37.880
Yeah, but isn't that like the same kind of thing

22:37.880 --> 22:40.080
where you can't just see one example

22:40.080 --> 22:41.960
and now you have the program or whatever.

22:41.960 --> 22:42.800
You actually had to drill it.

22:42.800 --> 22:43.800
These models also had to drill it

22:43.800 --> 22:45.080
with a bunch of returning data.

22:45.080 --> 22:48.320
Sure, I mean, in order to do on-the-fly program synthesis,

22:48.320 --> 22:51.920
you actually need building blocks to work from.

22:51.920 --> 22:55.440
So knowledge and memory are actually tremendously important

22:55.440 --> 22:56.280
in the process.

22:56.280 --> 23:00.560
I'm not saying it's memory versus reasoning.

23:00.560 --> 23:04.640
In order to do effective reasoning, you need memory.

23:04.640 --> 23:07.680
But it sounds like it's compatible with your story

23:07.680 --> 23:10.440
that through seeing a lot of different kinds of examples,

23:10.440 --> 23:11.960
these things can learn to reason

23:11.960 --> 23:13.800
within the context of those examples.

23:13.800 --> 23:16.160
And we can also see within bigger and bigger models.

23:16.160 --> 23:18.800
So that was an example of a high school level math problem.

23:19.880 --> 23:22.120
Let's say a model that's smaller than GPT-3

23:22.120 --> 23:23.680
couldn't do that at all.

23:23.680 --> 23:24.760
As these models get bigger,

23:24.760 --> 23:26.400
they seem to be able to pick up bigger and bigger.

23:26.400 --> 23:28.400
It's not really a size issue.

23:28.400 --> 23:30.600
It's more like a training data issue in this case.

23:30.600 --> 23:33.680
Well, bigger models can pick up these kinds of circuits

23:33.680 --> 23:36.040
which smaller models apparently don't do a good job

23:36.040 --> 23:37.280
of doing this even if you were to train them

23:37.280 --> 23:38.240
on this kind of data.

23:38.240 --> 23:39.080
Doesn't that just suggest

23:39.080 --> 23:40.400
that you have bigger and bigger models?

23:40.400 --> 23:42.240
They can pick up bigger and bigger pathways

23:42.240 --> 23:44.040
or more general ways of reasoning.

23:44.040 --> 23:44.960
Absolutely.

23:44.960 --> 23:46.400
But then isn't that intelligence?

23:46.400 --> 23:47.600
No, no, it's not.

23:47.600 --> 23:49.520
If you scale up your database

23:49.520 --> 23:52.280
and you keep adding to it more knowledge,

23:52.280 --> 23:53.640
more program templates,

23:53.640 --> 23:55.560
then sure it becomes more and more skillful.

23:55.560 --> 23:57.400
You can apply it to more and more tasks.

23:57.400 --> 24:01.120
But general intelligence is not tasks with six skills

24:01.120 --> 24:03.400
scaled up to many skills.

24:03.400 --> 24:06.640
Because there is an infinite space of possible skills.

24:06.680 --> 24:09.200
General intelligence is the ability to approach

24:09.200 --> 24:11.040
any problem, any skill,

24:11.040 --> 24:13.960
and very quickly master it using valid or data.

24:13.960 --> 24:16.920
Because this is what makes you able to face

24:16.920 --> 24:18.240
anything you might ever encounter.

24:18.240 --> 24:19.360
This is what makes,

24:20.800 --> 24:22.600
this is the definition of generality.

24:22.600 --> 24:25.640
Like generality is now specifically scaled up.

24:25.640 --> 24:29.120
It is the ability to apply your mind

24:29.120 --> 24:31.480
to anything at all, to arbitrary things.

24:31.480 --> 24:33.040
And this requires, fundamentally,

24:33.040 --> 24:35.040
it requires the ability to adapt,

24:35.080 --> 24:37.080
to learn on the fly efficiently.

24:37.080 --> 24:41.280
So, my claim is that by doing this free training

24:41.280 --> 24:42.680
on bigger and bigger models,

24:42.680 --> 24:44.440
you are gaining that capacity

24:44.440 --> 24:46.760
to then generalize very efficiently.

24:46.760 --> 24:48.120
Let me give you an example.

24:48.120 --> 24:49.280
Let me give you an example.

24:49.280 --> 24:51.240
So, your own company, Google,

24:51.240 --> 24:54.240
in their paper on Gemini 1.5,

24:54.240 --> 24:55.920
they had this very interesting example

24:55.920 --> 25:00.520
where they would give, in context,

25:00.520 --> 25:01.840
they would give the model,

25:01.840 --> 25:04.360
the grammar book and the dictionary

25:04.360 --> 25:07.520
of a language that has less than 200 living speakers.

25:07.520 --> 25:09.480
So, it's not in the free training data.

25:09.480 --> 25:11.520
And you just give them the dictionary

25:11.520 --> 25:14.120
and it basically is able to speak this language

25:14.120 --> 25:15.080
and translate to it,

25:15.080 --> 25:17.640
including the complex and organic ways

25:17.640 --> 25:20.200
in which languages are structured.

25:20.200 --> 25:21.760
So, a human, if you showed me a dictionary

25:21.760 --> 25:22.720
from English to Spanish,

25:22.720 --> 25:24.560
I'm not gonna be able to pick up the

25:24.560 --> 25:25.840
how to structure sentences

25:25.840 --> 25:28.040
and how to say things in Spanish.

25:28.040 --> 25:30.280
The fact that because of the representations

25:30.280 --> 25:33.120
that it has gained through this free training,

25:33.120 --> 25:35.240
it is able to now extremely efficiently

25:35.240 --> 25:36.360
learn a new language.

25:36.360 --> 25:38.600
Doesn't that show that this kind of free training

25:38.600 --> 25:41.240
actually does increase your ability to learn new tasks?

25:41.240 --> 25:43.200
If you're right, if you were right,

25:43.200 --> 25:45.440
LLMs would do really well on arch puzzles

25:45.440 --> 25:47.680
because arch puzzles are not complex.

25:47.680 --> 25:49.880
Each one of them requires very little knowledge.

25:49.880 --> 25:52.280
Each one of them is very low on complexity.

25:52.280 --> 25:54.600
You don't need to think very hard about it.

25:54.600 --> 25:56.520
They're actually extremely obvious for humans,

25:56.520 --> 25:58.240
like even children can do them.

25:58.240 --> 26:02.920
But LLMs cannot, even LLMs that have, you know,

26:02.920 --> 26:05.360
100,000 times more knowledge than you do.

26:05.360 --> 26:06.400
They still cannot.

26:06.400 --> 26:09.560
And the only thing that makes arch special

26:09.560 --> 26:11.680
is that it was designed with this intent

26:11.680 --> 26:13.080
to resist memorization.

26:13.080 --> 26:14.320
This is the only thing.

26:14.320 --> 26:18.960
And this is the huge blocker for LLM performance, right?

26:18.960 --> 26:23.960
And so, you know, I think if you look at LLMs closely,

26:25.240 --> 26:28.280
it's pretty obvious that they're not really like

26:28.280 --> 26:30.600
synthesizing new programs on the fly

26:30.600 --> 26:33.160
to solve the tasks that they're faced with.

26:33.160 --> 26:34.640
They're very much replying things

26:34.640 --> 26:36.640
that they've stored in memory.

26:36.640 --> 26:39.280
For instance, one thing that's very striking

26:39.280 --> 26:42.440
is LLMs can solve a CISA cipher,

26:42.440 --> 26:43.760
you know, like a CISA cipher,

26:43.760 --> 26:48.760
like transposing letters to code a message.

26:49.200 --> 26:52.600
And well, that's a very complex algorithm, right?

26:52.600 --> 26:54.920
But it comes up quite a bit on the internet.

26:54.920 --> 26:56.440
So they've basically memorized it.

26:56.440 --> 26:59.680
And what's really interesting is that they can do it

26:59.800 --> 27:02.760
for a transposition length of like three or five

27:02.760 --> 27:04.360
because there are very, very common numbers

27:04.360 --> 27:05.880
in examples provided on the internet.

27:05.880 --> 27:09.000
But if you try to do it with an arbitrary number,

27:09.000 --> 27:11.120
like nine, it's gonna fail.

27:11.120 --> 27:14.520
Because it does not encode the generalized form

27:14.520 --> 27:16.520
of the algorithm, but only specific cases.

27:16.520 --> 27:20.040
It does memorize specific cases of the algorithm, right?

27:20.040 --> 27:23.160
And if it could actually synthesize on the fly

27:23.160 --> 27:26.840
the solver algorithm, then the value of N

27:26.840 --> 27:28.520
would not matter at all

27:28.520 --> 27:30.880
because it does not increase the problem complexity.

27:30.880 --> 27:32.440
I think this is true of humans as well,

27:32.440 --> 27:34.360
where what was the study that-

27:34.360 --> 27:37.080
Humans use memorization pattern matching all the time,

27:37.080 --> 27:39.880
of course, but humans are not limited

27:39.880 --> 27:41.440
to memorization pattern matching.

27:41.440 --> 27:43.160
They have this very unique ability

27:43.160 --> 27:45.560
to adapt to new situations on the fly.

27:45.560 --> 27:48.160
This is exactly what enables you to navigate

27:49.160 --> 27:50.760
every new day in your life.

27:50.760 --> 27:51.600
I'm forgetting the details,

27:51.600 --> 27:54.120
but there was some study that chess grandmasters

27:54.120 --> 27:56.840
will perform very well within the context of the moves that-

27:56.840 --> 27:59.560
Excellent example, because chess at the highest level

27:59.560 --> 28:02.000
is all about memorization, chess memorization.

28:02.000 --> 28:02.960
Okay, sure, we can leave that aside.

28:02.960 --> 28:05.520
What is your explanation for the original question of

28:05.520 --> 28:10.520
why in context the GPT- sorry, Gemini 1.5

28:11.800 --> 28:13.520
was able to learn a language,

28:13.520 --> 28:15.600
including the complex grammar structure?

28:15.600 --> 28:17.400
Doesn't that show that they can pick up new knowledge?

28:17.400 --> 28:20.120
I would assume that it has simply mined

28:20.120 --> 28:23.880
from its extremely extensive and imaginably vast

28:23.880 --> 28:27.400
training data, it has mined the required template

28:27.400 --> 28:28.880
and then it's just reusing it.

28:28.880 --> 28:31.360
We know that they have a very poor ability

28:31.360 --> 28:34.840
to synthesize new program templates like this on the fly

28:34.840 --> 28:36.640
or even adapt existing ones.

28:36.640 --> 28:38.760
They're very much limited to fetching.

28:38.760 --> 28:40.880
Suppose there's a programmer at Google,

28:40.880 --> 28:42.880
they go into the office in the morning.

28:42.880 --> 28:44.280
At what point are they doing something

28:44.280 --> 28:47.760
that 100% cannot be due to fetching some template

28:47.760 --> 28:50.520
that even if they, suppose they were an LLM,

28:50.520 --> 28:52.120
they could not do if they had fetched some template

28:52.120 --> 28:52.960
from their program.

28:53.000 --> 28:53.920
At what point do they have to use

28:53.920 --> 28:55.680
this so-called extreme generalization capability?

28:55.680 --> 28:57.560
Forget about Google software developers.

28:57.560 --> 29:00.400
Every human, every day of their lives

29:00.400 --> 29:03.920
is full of novel things that they've not been prepared for.

29:03.920 --> 29:07.880
You cannot navigate your life based on memorization alone.

29:07.880 --> 29:08.720
It's impossible.

29:08.720 --> 29:11.440
I'm sort of denying the premise that they're,

29:11.440 --> 29:13.040
you also agree they're not doing like,

29:13.040 --> 29:14.600
quote-unquote memorization.

29:14.600 --> 29:17.400
It seems like you're saying they're less capable

29:17.400 --> 29:19.440
of generalization, but I'm just curious of like,

29:19.440 --> 29:21.640
the kind of generalization they do,

29:21.960 --> 29:24.240
if you get into the office

29:24.240 --> 29:25.680
and you try to do this kind of generalization,

29:25.680 --> 29:26.640
you're gonna fail at your job.

29:26.640 --> 29:28.640
But what is the first point, you're a programmer.

29:28.640 --> 29:30.640
What is the first point when you try to do that generalization,

29:30.640 --> 29:32.120
you would lose your job

29:32.120 --> 29:34.640
because you can't do the extreme generalization?

29:34.640 --> 29:36.120
I don't have any specific examples,

29:36.120 --> 29:41.120
but literally like, take this situation for instance,

29:41.120 --> 29:43.680
you've never been here in this room.

29:43.680 --> 29:46.400
Maybe you've been in this city a few times, I don't know,

29:46.400 --> 29:49.280
but there's a fair amount of novelty.

29:49.280 --> 29:51.880
You've never been interviewing me.

29:51.880 --> 29:53.800
There's a fair amount of novelty

29:53.800 --> 29:55.880
every hour of every day in your life.

29:55.880 --> 29:58.920
And it's in fact, by and large,

29:58.920 --> 30:02.280
more novelty than any LLM could handle.

30:02.280 --> 30:04.840
Like if you just put a LLM in a robot,

30:04.840 --> 30:06.200
it could not be doing all the things

30:06.200 --> 30:08.800
that you've been doing today, right?

30:08.800 --> 30:11.600
Or take on like cell driving cars, for instance.

30:11.600 --> 30:15.320
You take a cell driving car operating in the barrier.

30:15.320 --> 30:18.000
Do you think you could just drop it in New York City

30:18.000 --> 30:21.240
or drop it in London where people drive on the left?

30:21.240 --> 30:22.360
No, it's gonna fail.

30:22.360 --> 30:24.880
So not only can you drop, not like,

30:24.880 --> 30:29.880
make it generalize to a change of rules of driving rules,

30:31.600 --> 30:34.480
but you can not even make it generalize to a new city.

30:34.480 --> 30:38.200
It needs to be trained on each specific environment.

30:38.200 --> 30:41.680
I mean, I agree that self-driving cars aren't AGI.

30:41.680 --> 30:42.840
But it's the same type of model,

30:42.840 --> 30:44.120
they're transformers as well.

30:44.120 --> 30:45.640
I mean, I don't know,

30:46.240 --> 30:48.040
they also have brains with neurons in them,

30:48.040 --> 30:49.880
but they're less intelligent because they're small.

30:49.880 --> 30:50.720
It's not the same architecture.

30:50.720 --> 30:51.560
We can get into that.

30:51.560 --> 30:56.560
But so I still don't understand like a concrete thing of,

30:57.560 --> 30:58.920
we also need training.

30:58.920 --> 31:00.000
That's why education exists.

31:00.000 --> 31:02.200
That's why we had to spend the first 18 years of our life

31:02.200 --> 31:03.280
doing drills.

31:03.280 --> 31:06.160
We have a memory, but we are not a memory.

31:06.160 --> 31:08.120
We are not limited to just a memory.

31:08.120 --> 31:10.000
I'm denying the premise that that's necessarily

31:10.000 --> 31:11.120
the only thing these models are doing.

31:11.120 --> 31:13.720
And I'm still not sure what is the task

31:13.720 --> 31:17.000
that a remote worker would have to,

31:17.000 --> 31:19.200
suppose you do some remote work with an LLM

31:19.200 --> 31:20.480
and they're programmer,

31:20.480 --> 31:22.040
what is the first point that you realize

31:22.040 --> 31:23.760
this is not a human, this is an LLM?

31:23.760 --> 31:26.080
What about they just send them a knock puzzle

31:26.080 --> 31:27.280
and see how they do?

31:27.280 --> 31:29.240
No, like part of their job, you know?

31:29.240 --> 31:32.440
But you have to deal with novelty all the time.

31:32.440 --> 31:34.720
Okay, so if you, is there a world in which

31:34.720 --> 31:36.560
all the programmers are replaced?

31:36.560 --> 31:38.360
And then we're still saying,

31:38.360 --> 31:40.120
but they're only doing memorization

31:40.120 --> 31:41.440
late in programming tasks,

31:41.440 --> 31:43.160
but they're still producing a trillion dollars

31:44.160 --> 31:46.600
worth of output in the form of code.

31:46.600 --> 31:48.400
Software development is actually a pretty good example

31:48.400 --> 31:51.640
of a job where you're dealing with novelty all the time.

31:51.640 --> 31:53.720
Or if you're not, well, I'm not sure what you're doing.

31:53.720 --> 31:57.600
So I personally use Genetic VI very little

31:57.600 --> 31:59.720
in my software development job.

31:59.720 --> 32:03.080
And before LLMs, I think I was also using

32:03.080 --> 32:05.000
Stack Overflow very little.

32:05.000 --> 32:07.440
You know, some people maybe are just copy-pasting stuff

32:07.440 --> 32:08.280
from Stack Overflow,

32:08.280 --> 32:10.680
or nowadays copy-pasting stuff from an LLM.

32:11.680 --> 32:14.960
Personally, I try to focus on problem-solving.

32:14.960 --> 32:16.920
The syntax is just a technical detail.

32:16.920 --> 32:19.360
What's really important is the problem-solving.

32:19.360 --> 32:23.960
Like the essence of programming is engineering

32:23.960 --> 32:27.360
mental models, like mental representations

32:27.360 --> 32:29.480
of the problem you're trying to solve.

32:29.480 --> 32:32.520
But you can, you know, we have many,

32:32.520 --> 32:34.200
people can interact with these systems themselves

32:34.200 --> 32:36.200
and you can go to chat GPT and say,

32:36.200 --> 32:38.640
here's a specification of the kind of program I want.

32:38.640 --> 32:39.560
They'll build it for you.

32:39.640 --> 32:41.800
As long as there are many examples of this program

32:41.800 --> 32:43.600
on like GitHub and Stack Overflow and so on,

32:43.600 --> 32:47.320
sure, they will fetch the program for you from their memory.

32:47.320 --> 32:49.160
But you can change arbitrary details.

32:49.160 --> 32:52.320
You can say I need it to work on this different kind of server.

32:52.320 --> 32:55.920
If that were true, there would be no software engineers today.

32:55.920 --> 32:58.280
I agree. We're not at a full AGI yet,

32:58.280 --> 33:00.360
in the sense that these models have,

33:00.360 --> 33:02.560
let's say, less than a trillion parameters.

33:02.560 --> 33:04.200
A human brain has somewhere on the order

33:04.200 --> 33:05.960
of 10 to 30 trillion synapses.

33:05.960 --> 33:08.480
I mean, if you were just doing some naive math,

33:08.520 --> 33:11.120
you're like at least 10x under parameterized.

33:11.120 --> 33:12.640
So I agree we're not there yet,

33:12.640 --> 33:17.280
but I'm sort of confused on why we're not on the spectrum,

33:17.280 --> 33:19.400
where yes, I agree that there's many kinds

33:19.400 --> 33:20.880
of generalization they can do,

33:20.880 --> 33:22.760
but it seems like they're on this kind of smooth spectrum

33:22.760 --> 33:24.160
that we see even within humans,

33:24.160 --> 33:27.360
where some humans would have a hard time doing an ARC type test.

33:27.360 --> 33:28.800
We see that based on the performance

33:28.800 --> 33:31.040
on progressive Ravens matrices type IQ tests.

33:31.040 --> 33:34.040
I'm not a fan of IQ tests because for the most part,

33:34.040 --> 33:37.720
you can train on IQ tests and get better at them.

33:37.760 --> 33:39.880
So they have very much memorization based.

33:39.880 --> 33:42.360
And this is actually the main pitfall

33:42.360 --> 33:45.880
that ARC tries not to fall far.

33:45.880 --> 33:46.760
I'm still not confused.

33:46.760 --> 33:49.880
So if all remote jobs are automated

33:49.880 --> 33:52.240
in the next five years, let's say,

33:52.240 --> 33:54.800
at least that don't require you to be like sort of a service.

33:54.800 --> 33:56.000
It's not like a salesperson

33:56.000 --> 33:57.480
where you want the human to be talking,

33:57.480 --> 33:58.840
but like programming or whatever.

33:58.840 --> 34:02.840
In that world, would you say that that's not possible

34:02.840 --> 34:05.600
because a lot of what a programmer needs to do,

34:05.600 --> 34:07.640
definitely requires things that would not be

34:07.640 --> 34:08.720
in any free training corpus?

34:08.720 --> 34:10.040
Sure. I mean, in five years,

34:10.040 --> 34:11.560
there will be more software engineers

34:11.560 --> 34:13.200
than there are today and not too well.

34:13.200 --> 34:14.280
But I just want to understand.

34:14.280 --> 34:16.040
So I'm still not sure.

34:16.040 --> 34:18.160
I mean, I know how to, I studied computer science.

34:18.160 --> 34:20.120
I think if I had become a code monkey out of college,

34:20.120 --> 34:22.040
like what would I be doing?

34:22.040 --> 34:23.320
I go to my job.

34:23.320 --> 34:26.360
What is the first thing my boss tells me something to do?

34:26.360 --> 34:30.280
When does he realize I'm an LLM if I was an LLM?

34:30.280 --> 34:32.560
Probably on the first day, you know?

34:32.560 --> 34:33.400
Again,

34:33.680 --> 34:38.680
if it were true that LLMs could generalize

34:40.000 --> 34:41.560
to novel problems like this

34:41.560 --> 34:45.040
and you can actually develop software

34:45.040 --> 34:46.680
to solve a problem they've never seen before,

34:46.680 --> 34:48.720
you would not need software engineers anymore.

34:48.720 --> 34:51.360
In practice, if I look at how people are using LLMs

34:51.360 --> 34:53.160
in their software engineering job today,

34:53.160 --> 34:56.640
they're using it as a stack of a flow replacement.

34:56.640 --> 35:00.840
So they're using it as a way to copy paste code snippets

35:00.840 --> 35:03.080
to perform very common actions.

35:03.080 --> 35:06.600
And what they actually need is a database of code snippets.

35:06.600 --> 35:09.560
They don't actually need any of the abilities

35:09.560 --> 35:10.880
that actually make them software engineers.

35:10.880 --> 35:13.560
I mean, when we talk about interpolating

35:13.560 --> 35:15.240
between stack overflow databases,

35:15.240 --> 35:16.600
if you look at the kinds of math problems

35:16.600 --> 35:20.040
or coding problems, maybe to say that they're,

35:21.520 --> 35:22.960
maybe let's step back on interpolation

35:22.960 --> 35:24.640
and let me ask the question this way.

35:24.640 --> 35:26.000
Why can't creativity,

35:26.000 --> 35:28.280
why isn't creativity just interpolation

35:28.280 --> 35:31.400
in a higher dimension where if a bigger model

35:31.400 --> 35:33.320
can learn a more complex manifold,

35:33.320 --> 35:34.800
we're gonna use the ML language.

35:34.800 --> 35:38.240
And if you look at read a biography of a scientist,

35:38.240 --> 35:40.360
it doesn't feel like they're not zero shot

35:40.360 --> 35:41.280
in new scientific theories.

35:41.280 --> 35:43.160
They're playing with existing ideas.

35:43.160 --> 35:45.200
They're trying to juxtapose them in their head.

35:45.200 --> 35:49.720
They try out some like slightly ever in the tree

35:49.720 --> 35:51.600
of intellectual descendants,

35:51.600 --> 35:53.680
they try out a different evolutionary path.

35:53.680 --> 35:55.960
You sort of run the experiment there

35:55.960 --> 35:57.920
in terms of publishing the paper, whatever.

35:57.920 --> 35:59.480
It seems like a similar kind of thing humans are doing.

35:59.480 --> 36:01.800
There's like at a higher level of generalization.

36:01.800 --> 36:04.040
And what you see across bigger and bigger models

36:04.040 --> 36:05.360
is they seem to be approaching

36:05.360 --> 36:06.960
higher and higher level of generalization

36:06.960 --> 36:10.360
where GPT-2 couldn't do a great school level math problem

36:10.360 --> 36:11.440
that requires more generalization

36:11.440 --> 36:13.640
that it has capability for, even that skill.

36:13.640 --> 36:15.760
Then GPT-3 and 4 can.

36:15.760 --> 36:16.600
So not quite.

36:16.600 --> 36:19.960
So GPT-4 has a higher degree of skill

36:19.960 --> 36:21.520
and higher range of skills.

36:21.520 --> 36:22.360
Because it's-

36:22.360 --> 36:23.200
I don't want to get into semantics here,

36:23.200 --> 36:24.040
but I think-

36:24.040 --> 36:24.880
The same degree of generalization.

36:24.880 --> 36:25.720
I don't want to get into semantics here,

36:25.720 --> 36:28.640
but the question of why can't creativity

36:28.640 --> 36:32.800
be just interpolation on a higher dimension?

36:32.800 --> 36:35.320
I think interpolation can be creative, absolutely.

36:35.320 --> 36:36.920
And you know, to your point,

36:36.920 --> 36:39.160
I do think that on some level,

36:39.160 --> 36:41.640
humans also do a lot of memorization,

36:41.640 --> 36:43.440
a lot of reciting, a lot of pattern matching,

36:43.440 --> 36:45.120
a lot of interpolation as well.

36:45.120 --> 36:47.640
So it's very much a spectrum

36:48.920 --> 36:51.760
between pattern matching and true reasoning.

36:51.760 --> 36:52.600
It's a spectrum.

36:52.600 --> 36:57.480
And humans are never really at one end of the spectrum.

36:57.480 --> 36:59.720
They're never really doing pure pattern matching

36:59.720 --> 37:00.560
or pure reasoning.

37:00.560 --> 37:02.920
They're usually doing some mixture of both.

37:02.920 --> 37:06.400
Even if you're doing something that seems very reasoning heavy,

37:06.400 --> 37:08.880
like proving a mathematical theorem,

37:08.880 --> 37:09.960
as you're doing it, sure,

37:09.960 --> 37:12.480
you're doing quite a bit of discrete search in your mind,

37:12.480 --> 37:14.360
quite a bit of actual reasoning.

37:14.360 --> 37:18.120
But you're also very much guided by intuition,

37:18.120 --> 37:19.280
guided by pattern matching,

37:19.280 --> 37:22.960
guided by the shape of proofs that you've seen before,

37:22.960 --> 37:25.000
by your knowledge of mathematics.

37:25.000 --> 37:26.840
So it's never really,

37:26.880 --> 37:28.200
you know, all of our thoughts,

37:28.200 --> 37:31.760
everything we do is a mixture of this sort of like

37:31.760 --> 37:34.280
interpolative memorization based thinking,

37:34.280 --> 37:38.600
this sort of like type one thinking and type two thinking.

37:40.280 --> 37:43.200
Why are bigger models more sample efficient?

37:43.200 --> 37:47.600
Because they have more reusable building blocks

37:47.600 --> 37:52.120
that they can lean on to pick up new patterns

37:52.120 --> 37:52.960
in their train data.

37:52.960 --> 37:54.680
And does that pattern keep continuing

37:54.680 --> 37:56.040
as you keep getting bigger and bigger?

37:56.040 --> 37:58.280
To the extent that the new patterns

37:58.280 --> 38:00.520
you're giving the model to learn

38:00.520 --> 38:03.000
are good match for what it has learned before.

38:03.000 --> 38:05.240
If you present something that is actually novel,

38:05.240 --> 38:07.600
that is not in a state of distribution like an arc puzzle,

38:07.600 --> 38:09.240
for instance, it will fail.

38:09.240 --> 38:10.240
Let me make this claim.

38:10.240 --> 38:12.360
The program synthesis I think is a very,

38:12.360 --> 38:14.120
very useful intuition pump.

38:14.120 --> 38:15.600
Why can't it be the case that what's happening

38:15.600 --> 38:19.440
in the transformer is the early layers are doing the,

38:19.440 --> 38:22.240
figuring out how to represent the inputting tokens.

38:22.240 --> 38:24.600
And what the middle layers do is this kind of program search,

38:24.600 --> 38:27.440
program synthesis, where they combine the inputs

38:27.440 --> 38:31.080
to all the circuits in the model

38:31.080 --> 38:33.920
where they go from the low level representation

38:33.920 --> 38:35.240
to a higher level representation

38:35.240 --> 38:36.320
near the middle of the model.

38:36.320 --> 38:39.680
They use these programs, they combine these concepts,

38:39.680 --> 38:42.600
then what comes out at the other end is the reasoning

38:42.600 --> 38:45.160
based on that high level intelligence.

38:45.160 --> 38:46.640
Possibly, why not?

38:47.720 --> 38:50.840
But if these models were actually capable

38:50.840 --> 38:54.440
of synthesizing novel programs,

38:54.480 --> 38:57.360
however simple they should be able to do arc

38:57.360 --> 38:59.400
because for any arc task,

38:59.400 --> 39:02.320
if you write down the solution program in Python,

39:02.320 --> 39:05.600
it's not a complex program, it's extremely simple

39:05.600 --> 39:07.880
and humans can figure it out.

39:07.880 --> 39:10.240
So why can LLMs not do it?

39:10.240 --> 39:12.880
Okay, I think that's a fair point.

39:12.880 --> 39:15.480
And if I turn the question around to you,

39:15.480 --> 39:18.960
so suppose that it's the case that in a year,

39:18.960 --> 39:22.200
a multimodal model can solve arc,

39:22.240 --> 39:25.520
let's say get 80%, whatever the average human would get,

39:25.520 --> 39:27.200
then AGI?

39:27.200 --> 39:28.360
Quite possibly, yes.

39:28.360 --> 39:30.960
I think if you start, so honestly,

39:30.960 --> 39:34.720
what I would like to see is an LLM type model

39:34.720 --> 39:36.760
solving arc at like 80%,

39:36.760 --> 39:39.880
but after having only been trained

39:39.880 --> 39:42.840
on core knowledge related stuff.

39:42.840 --> 39:45.320
But human kids, I don't think we're necessarily

39:45.320 --> 39:47.360
just trading on, it's not just that we have

39:47.360 --> 39:48.800
in our show is object permanence.

39:48.800 --> 39:50.600
Okay, let me rephrase that.

39:50.600 --> 39:55.600
Only trained on information that is not explicitly

39:55.720 --> 39:58.960
trying to anticipate what's gonna be in the arc test set.

39:58.960 --> 40:01.280
But isn't the whole point of arc that you can't,

40:01.280 --> 40:03.640
sort of, it's a new type of intelligence

40:03.640 --> 40:04.480
every single time?

40:04.480 --> 40:05.320
Yes, that is the point.

40:05.320 --> 40:07.880
So if arc were perfect, flawless benchmark,

40:07.880 --> 40:10.680
it would be impossible to anticipate within the test set.

40:10.680 --> 40:14.160
And arc was released more than four years ago

40:14.160 --> 40:16.640
and so far it's been resistant to memorization.

40:16.640 --> 40:20.840
So I think it has, to some extent, passed a test of time.

40:20.840 --> 40:22.960
But I don't think it's perfect.

40:22.960 --> 40:26.200
I think if you try to make by hand

40:27.120 --> 40:29.240
hundreds of thousands of arc tasks

40:29.240 --> 40:31.800
and then you try to multiply them

40:32.760 --> 40:35.000
by programmatically generating variations

40:35.000 --> 40:38.520
and then you end up with maybe hundreds of millions of tasks.

40:38.520 --> 40:41.040
Just by brute forcing the task space,

40:41.040 --> 40:43.680
there will be enough overlap between what you're trained on

40:43.680 --> 40:45.440
and what's in the test set that you can actually score

40:45.440 --> 40:46.280
very highly.

40:46.280 --> 40:49.600
So, you know, with enough scale, you can always cheat.

40:49.600 --> 40:51.240
If you can do this for every single thing

40:51.240 --> 40:52.880
that supposedly requires intelligence,

40:52.880 --> 40:53.920
then what good is intelligence?

40:53.920 --> 40:55.560
Apparently you can just brute force intelligence.

40:55.560 --> 40:59.920
If the world, if your life, were a static distribution,

40:59.920 --> 41:01.520
then sure, you could just brute force

41:01.520 --> 41:03.720
the space of possible behaviors.

41:03.720 --> 41:07.960
You could like, you know, the way we think about intelligence,

41:07.960 --> 41:09.960
there are several metaphors, I like to use,

41:09.960 --> 41:12.640
but one of them is you can think of intelligence

41:12.640 --> 41:17.000
as a past finding algorithm in future situation space.

41:17.000 --> 41:19.160
Like, I don't know if you're familiar with game development,

41:19.160 --> 41:23.400
like RTS game development, but you have a map, right?

41:23.400 --> 41:26.000
And you have, it's like a 2D map.

41:26.000 --> 41:28.800
And you have partial information about it.

41:28.800 --> 41:31.880
Like there is some fog of war on your map.

41:31.880 --> 41:34.040
There are areas that you haven't explored yet.

41:34.040 --> 41:35.080
You know nothing about them.

41:35.080 --> 41:36.640
And then there are areas that you've explored,

41:36.640 --> 41:39.920
but you only know how they were like in the past.

41:39.920 --> 41:41.520
You don't know how they are like today.

41:41.520 --> 41:46.760
And now, instead of thinking about a 2D map,

41:46.760 --> 41:50.040
think about the space of possible future situations

41:50.040 --> 41:52.640
that you might encounter and how they're connected to each other.

41:52.640 --> 41:54.440
Intelligence is a past finding algorithm.

41:54.440 --> 41:57.400
So once you set a goal, it will tell you

41:57.400 --> 42:00.680
how to get there optimally.

42:00.680 --> 42:05.240
But of course, it's constrained by the information you have.

42:05.240 --> 42:09.240
It cannot pass find in an area that you know nothing about.

42:09.240 --> 42:12.640
It cannot also anticipate changes.

42:12.640 --> 42:20.040
And the thing is, if you had complete information about the map,

42:20.040 --> 42:22.360
then you could solve the past finding problem

42:22.360 --> 42:26.040
by simply memorizing every possible path, every mapping

42:26.040 --> 42:31.320
from point A to point B. You could solve the problem

42:31.320 --> 42:32.400
with pure memory.

42:32.400 --> 42:35.040
But the reason you cannot do that in real life

42:35.040 --> 42:37.120
is because you don't actually know what's

42:37.120 --> 42:38.920
going to happen in the future.

42:39.800 --> 42:41.600
I feel like you're using words like memorization, which

42:41.600 --> 42:43.000
we would never use for human children.

42:43.000 --> 42:46.680
If your kid learns to do algebra and then now learns

42:46.680 --> 42:48.760
to do calculus, you wouldn't say they memorized calculus.

42:48.760 --> 42:52.160
If they can just solve any arbitrary algebraic problem,

42:52.160 --> 42:54.000
you wouldn't say they memorized algebra.

42:54.000 --> 42:55.080
They say they've learned algebra.

42:55.080 --> 42:58.160
Humans are never redoing pure memorization or pure reasoning.

42:58.160 --> 42:59.520
But that's only because you're semantically

42:59.520 --> 43:01.160
labeling when the human does the skill.

43:01.160 --> 43:03.560
It's a memorization when the exact same skill is done by the LLM

43:03.560 --> 43:04.880
as you can measure by these benchmarks.

43:04.880 --> 43:06.640
And you can just plug in any sort of math problem.

43:06.680 --> 43:08.760
Sometimes humans are doing the exact same as the LLM

43:08.760 --> 43:10.840
is doing, which is just, for instance,

43:10.840 --> 43:12.640
I know if you learn to add numbers,

43:12.640 --> 43:14.800
you're memorizing an algorithm.

43:14.800 --> 43:15.960
You're memorizing a program.

43:15.960 --> 43:17.640
And then you can reapply it.

43:17.640 --> 43:21.720
You are not synthesizing on the fly the addition program.

43:21.720 --> 43:24.000
So obviously at some point, some human had to figure out

43:24.000 --> 43:24.600
how to do addition.

43:24.600 --> 43:27.000
But the way a kid learns it is not

43:27.000 --> 43:30.160
that they figure out from the actions of set theory

43:30.160 --> 43:30.800
how to do addition.

43:30.800 --> 43:32.120
I think what you're learning in school

43:32.120 --> 43:33.960
is mostly memorization.

43:34.000 --> 43:37.080
So my claim is that, listen, these models

43:37.080 --> 43:40.880
are vastly underparameterized relative to how many flops

43:40.880 --> 43:43.240
or how many parameters you have in the human brain.

43:43.240 --> 43:45.800
And so yeah, they're not going to be coming up

43:45.800 --> 43:48.800
with new theorems like the smartest humans can.

43:48.800 --> 43:51.120
But most humans can't do that either.

43:51.120 --> 43:52.880
What most humans do, it sounds like it's

43:52.880 --> 43:55.040
similar to what you were calling memorization, which

43:55.040 --> 44:00.120
is memorizing skills or memorizing techniques

44:00.120 --> 44:01.080
that you've learned.

44:01.080 --> 44:03.560
And so it sounds like it's compatible.

44:03.680 --> 44:04.720
Tell me if this is wrong.

44:04.720 --> 44:07.600
Is it compatible in your world if all the remote workers

44:07.600 --> 44:09.960
are gone, but they're doing skills

44:09.960 --> 44:12.120
which we can potentially make synthetic data of?

44:12.120 --> 44:15.440
So we record everybody's screen and every single remote worker

44:15.440 --> 44:16.160
screen.

44:16.160 --> 44:18.680
We sort of understand the skills they're performing there.

44:18.680 --> 44:20.680
And now we've trained a model that can do all this.

44:20.680 --> 44:22.360
All the remote workers are unemployed.

44:22.360 --> 44:23.760
We're generating trillions of dollars

44:23.760 --> 44:26.600
to economic activity for AI remote workers.

44:26.600 --> 44:28.920
In that world, are we still in the memorization regime?

44:28.920 --> 44:30.320
So sure.

44:30.360 --> 44:33.840
With memorization, you can automate almost anything

44:33.840 --> 44:35.880
as long as it's a static distribution,

44:35.880 --> 44:38.000
as long as you don't have to deal with change.

44:38.000 --> 44:41.000
Are most jobs part of such a static distribution?

44:41.000 --> 44:44.400
Potentially, there are lots of things that you can automate.

44:44.400 --> 44:46.880
And LLMs are an excellent tool for automation.

44:46.880 --> 44:48.240
And I think that's true.

44:48.240 --> 44:50.520
But you have to understand that automation is not

44:50.520 --> 44:51.440
the same as intelligence.

44:51.440 --> 44:53.800
I'm not saying that LLMs are useless.

44:53.800 --> 44:57.000
I've been a huge proponent of deep learning for many years.

44:57.000 --> 44:58.920
And for many years, I've been saying two things.

44:58.920 --> 45:01.680
I've been saying that if you keep scaling up deep learning,

45:01.680 --> 45:03.200
it will keep paying off.

45:03.200 --> 45:04.720
And at the same time, I've been saying,

45:04.720 --> 45:06.200
if you keep scaling up deep learning,

45:06.200 --> 45:08.640
this will not lead to a GI.

45:08.640 --> 45:10.680
So we can automate more and more things.

45:10.680 --> 45:12.520
And yes, this is economically valuable.

45:12.520 --> 45:14.480
And yes, potentially, there are many jobs.

45:14.480 --> 45:15.840
You could automate a way like this.

45:15.840 --> 45:17.880
And that would be economically valuable.

45:17.880 --> 45:20.080
But you're still not going to have intelligence.

45:20.080 --> 45:22.400
So you can ask, OK, so what does it

45:22.400 --> 45:24.680
matter if we can generate all this economic value?

45:24.680 --> 45:26.280
Maybe we don't need intelligence after all.

45:26.280 --> 45:28.440
Well, you need intelligence the moment

45:28.480 --> 45:32.120
you have to deal with change, with novelty, with uncertainty.

45:32.120 --> 45:34.000
As long as you are in a space that

45:34.000 --> 45:37.240
can be exactly described in advance,

45:37.240 --> 45:41.600
you can just automate your pure memorization.

45:41.600 --> 45:44.120
In fact, you can always solve any problem.

45:44.120 --> 45:48.760
You can always display arbitrary levels of skills

45:48.760 --> 45:54.640
on any task without leveraging any intelligence whatsoever,

45:54.640 --> 45:58.840
as long as it is possible to describe the problem

45:58.840 --> 46:01.320
and its solution very, very precisely.

46:01.320 --> 46:03.200
But when they do deal with novelty,

46:03.200 --> 46:05.200
then you just call it interpolation, right?

46:05.200 --> 46:08.240
And so interpolation is not enough

46:08.240 --> 46:09.920
to deal with all kinds of novelty

46:09.920 --> 46:13.360
if it were, then LLMs would be a GI.

46:13.360 --> 46:14.360
Well, I agree they're not a GI.

46:14.360 --> 46:16.520
I'm just trying to figure out how do we figure out

46:16.520 --> 46:17.320
we're on the path to a GI.

46:17.320 --> 46:20.680
And I think a sort of crux here is maybe

46:20.680 --> 46:23.840
that it seems to me that these things are on a spectrum

46:23.880 --> 46:26.640
and we're clearly covering the earliest part of the spectrum

46:26.640 --> 46:27.480
with LLMs.

46:27.480 --> 46:28.320
I think so.

46:28.320 --> 46:29.440
And oh, OK, interesting.

46:29.440 --> 46:31.520
But here's another sort of thing

46:31.520 --> 46:34.240
that I think is evidence for this, grokking, right?

46:34.240 --> 46:36.760
So clearly, even within deep learning,

46:36.760 --> 46:39.560
there's a difference between the memorization regime

46:39.560 --> 46:42.640
and the generalization regime, where at first they'll just

46:42.640 --> 46:46.800
memorize the data set of if you're doing modular addition,

46:46.800 --> 46:47.920
how to add digits.

46:47.920 --> 46:50.120
And then at some point, if you keep training on that,

46:50.120 --> 46:51.280
they'll learn the skill.

46:51.280 --> 46:53.360
So the fact that there is that distinction

46:53.360 --> 46:56.400
suggests that the generalized circuit, the deep learning

46:56.400 --> 46:59.560
can learn, there is a regime in enters where it generalizes.

46:59.560 --> 47:01.120
If you have an over-parameterized model,

47:01.120 --> 47:02.960
which you don't have in comparison to all the tasks

47:02.960 --> 47:04.720
we want these models to do right now.

47:04.720 --> 47:06.320
Grokking is a very, very old phenomenon.

47:06.320 --> 47:09.160
We've been observing it for decades.

47:09.160 --> 47:13.480
It's basically an instance of the minimum description length

47:13.480 --> 47:16.680
principle, where, sure, given a problem,

47:16.680 --> 47:22.760
you can just memorize a point-wise input-to-output mapping,

47:22.800 --> 47:24.160
which is completely overfit.

47:24.160 --> 47:26.000
So it does not generalize at all,

47:26.000 --> 47:29.560
but it solves the problem on the train data.

47:29.560 --> 47:33.320
And from there, you can actually keep proving it,

47:33.320 --> 47:36.240
keep making your mapping simpler and simpler and more

47:36.240 --> 47:37.320
compressed.

47:37.320 --> 47:40.520
And at some point, it will start generalizing.

47:40.520 --> 47:44.280
And so that's something called the minimum description

47:44.280 --> 47:45.040
length principle.

47:45.040 --> 47:48.520
It's this idea that the program that will generalize best

47:48.520 --> 47:51.280
is the shortest, right?

47:51.280 --> 47:54.520
And it doesn't mean that you're doing anything

47:54.520 --> 47:55.840
other than memorization, but you're

47:55.840 --> 47:58.480
doing memorization plus regularization.

47:58.480 --> 48:00.680
Right, AKA generalization.

48:00.680 --> 48:03.640
Yeah, and that is absolutely, at least to generalization.

48:03.640 --> 48:05.560
Right, and then so you do that within one skill,

48:05.560 --> 48:07.720
but then the pattern you see here of meta-learning

48:07.720 --> 48:10.760
is that it's more efficient to store a program that can perform

48:10.760 --> 48:13.240
many skills rather than one skill, which is what we might

48:13.240 --> 48:14.520
call fluid intelligence.

48:14.520 --> 48:16.040
And so as you get bigger and bigger models,

48:16.040 --> 48:18.720
you would expect it to go up this hierarchy of generalization

48:18.720 --> 48:20.880
where it generalizes to a skill, then it generalizes

48:21.080 --> 48:22.040
multiple skills.

48:22.040 --> 48:23.400
That's correct, that's correct.

48:23.400 --> 48:27.200
And you know, LLMs, they're not infinitely large.

48:27.200 --> 48:29.560
They have only a fixed number of parameters.

48:29.560 --> 48:32.440
And so they have to compress their knowledge

48:32.440 --> 48:33.720
as much as possible.

48:33.720 --> 48:35.560
And in practice, so LLMs are mostly

48:35.560 --> 48:40.400
storing reusable bits of programs, like vector programs.

48:40.400 --> 48:42.800
And because they have this need for compression,

48:42.800 --> 48:44.960
it means that every time they're learning a new program,

48:44.960 --> 48:47.120
they're going to try to express it

48:47.120 --> 48:50.160
in terms of existing bits and pieces of programs

48:50.160 --> 48:52.440
that they've already learned before, right?

48:52.440 --> 48:54.560
Isn't this the generalization?

48:54.560 --> 48:55.760
Absolutely.

48:55.760 --> 48:57.200
Oh, wait, so.

48:57.200 --> 48:59.360
This is why, you know, clearly LLMs

48:59.360 --> 49:01.400
have some degree of generalization.

49:01.400 --> 49:03.880
And this is precisely why, it's because they have to compress.

49:03.880 --> 49:05.680
And why is that intrinsically limited?

49:05.680 --> 49:07.760
Why can't you just go, at some point,

49:07.760 --> 49:09.600
it has to learn a higher level of generalization,

49:09.600 --> 49:11.360
a higher level, and then the highest level

49:11.360 --> 49:12.440
is the fluid intelligence.

49:12.440 --> 49:15.080
It's intrinsically limited because the substrate

49:15.080 --> 49:18.360
of your model is a big parametric curve.

49:18.360 --> 49:21.960
And all you can do with this is local generalization.

49:21.960 --> 49:25.480
If you want to go beyond this towards broader

49:25.480 --> 49:27.520
or even extreme generalization, you

49:27.520 --> 49:29.840
have to move to a different type of model.

49:29.840 --> 49:34.000
And my paradigm of choice is discrete program search,

49:34.000 --> 49:35.040
program synthesis.

49:35.040 --> 49:37.280
So and if you want to understand that,

49:37.280 --> 49:41.520
you can sort of like compare and contrast it with deep learning.

49:41.520 --> 49:45.400
So in deep learning, your model is a parametric curve,

49:45.400 --> 49:47.080
a differentiable parametric curve.

49:47.080 --> 49:49.640
In program synthesis, your model

49:49.640 --> 49:52.560
is a discrete graph of operators.

49:52.560 --> 49:55.080
So you've got like a set of logical operators,

49:55.080 --> 49:57.400
like a domain-specific language.

49:57.400 --> 49:59.800
You're picking instances of it.

49:59.800 --> 50:01.960
You're structuring that into a graph.

50:01.960 --> 50:03.240
That's a program.

50:03.240 --> 50:05.480
And that's actually very similar to like a program

50:05.480 --> 50:09.400
you might write in Python or C++ and so on.

50:09.400 --> 50:11.880
And in deep learning, your learning engine,

50:11.880 --> 50:13.600
because we are doing much learning here,

50:13.600 --> 50:16.800
like we're trying to automatically learn these models.

50:16.800 --> 50:21.320
In deep learning, your learning engine is quite in the sense.

50:21.320 --> 50:24.440
And quite in the sense is very compute efficient,

50:24.440 --> 50:27.080
because you have this very strong, informative feedback

50:27.080 --> 50:29.760
signal about where the solution is.

50:29.760 --> 50:31.760
So you can get to the solution very quickly.

50:31.760 --> 50:35.040
But it is very data inefficient, meaning

50:35.040 --> 50:36.840
that in order to make it work, you

50:36.840 --> 50:39.640
need a dense sampling of the operating space.

50:39.640 --> 50:41.800
You need a dense sampling of the data distribution.

50:41.800 --> 50:44.400
And then you're limited to only generalizing

50:44.400 --> 50:46.080
within that data distribution.

50:46.080 --> 50:48.160
And the reason why you have this limitation

50:48.160 --> 50:50.000
is because your model is a curve.

50:50.000 --> 50:53.680
And meanwhile, if you look at discrete program search,

50:53.680 --> 50:56.840
the learning engine is combinatorial search.

50:56.840 --> 50:58.920
You're just trying a bunch of programs

50:58.920 --> 51:01.800
until you find one that actually miss your spec.

51:01.800 --> 51:04.240
This process is extremely data efficient.

51:04.240 --> 51:06.120
You can learn a generalizable program

51:06.120 --> 51:08.480
from just one example, two examples, which

51:08.480 --> 51:10.720
is why it works so well on Arc, by the way.

51:10.720 --> 51:14.400
But the big limitation is that it's extremely compute

51:14.400 --> 51:17.320
inefficient, because you're running into combinatorial

51:17.320 --> 51:18.800
explosion, of course.

51:18.800 --> 51:22.320
And so you can sort of see here how

51:22.320 --> 51:24.880
the planning and discrete program search,

51:24.880 --> 51:29.680
they have very complementary strengths and limitations

51:29.680 --> 51:30.240
as well.

51:30.240 --> 51:33.680
Every limitation of deep learning has a strength,

51:33.680 --> 51:37.440
a corresponding strength in program synthesis and inversely.

51:37.440 --> 51:40.640
And I think the path forward is going to be to merge the two,

51:40.640 --> 51:42.040
to basically start doing.

51:42.080 --> 51:44.240
So another way you can think about it

51:44.240 --> 51:48.760
is, so these parametric curves, train with ground descent,

51:48.760 --> 51:51.080
there are great fits for everything

51:51.080 --> 51:55.600
that's system one type thinking, like pattern cognition,

51:55.600 --> 51:58.520
intuition, memorization, and so on.

51:58.520 --> 52:02.320
And discrete program search is a great fit

52:02.320 --> 52:06.000
for type two thinking, system two thinking.

52:06.000 --> 52:08.960
For instance, planning, reasoning,

52:08.960 --> 52:11.360
quickly figuring out a generalizable model,

52:11.360 --> 52:14.000
that matches just one or two examples,

52:14.000 --> 52:16.000
like for an archbishop, for instance.

52:16.000 --> 52:20.640
And I think humans are never doing pure system one

52:20.640 --> 52:21.600
or pure system two.

52:21.600 --> 52:24.600
They're always mixing and matching both.

52:24.600 --> 52:27.040
And right now, we have all the tools for system one.

52:27.040 --> 52:29.320
We have almost nothing for system two.

52:29.320 --> 52:32.080
The way forward is to create a hybrid system.

52:32.080 --> 52:34.160
And I think the form it's going to take

52:34.160 --> 52:37.240
is it's going to be mostly system two.

52:37.240 --> 52:40.560
So the outer structure is going to be a discrete program

52:40.560 --> 52:42.040
search system.

52:42.040 --> 52:44.680
But you're going to fix the fundamental limitation

52:44.680 --> 52:46.840
of discrete program search, which is combinator explosion.

52:46.840 --> 52:49.440
You're going to fix it with deep learning.

52:49.440 --> 52:52.400
You're going to leverage deep learning to guide,

52:52.400 --> 52:55.480
to provide intuition in program space,

52:55.480 --> 52:57.640
to guide the program search.

52:57.640 --> 53:00.960
And I think that's very similar to what you see,

53:00.960 --> 53:03.800
for instance, when you're playing chess

53:03.800 --> 53:06.480
or when you're trying to prove a theorem,

53:06.520 --> 53:11.400
is that it's mostly a reasoning thing,

53:11.400 --> 53:13.600
but you start out with some intuition

53:13.600 --> 53:15.440
about the shape of the solution.

53:15.440 --> 53:18.040
And that's very much something you can get

53:18.040 --> 53:19.640
via a deep learning model.

53:19.640 --> 53:23.360
Deep learning models, they're very much like intuition machines.

53:23.360 --> 53:25.360
They're pattern matching machines.

53:25.360 --> 53:30.160
So you start from this shape of the solution,

53:30.160 --> 53:33.760
and then you're going to do actual explicit discrete

53:33.760 --> 53:35.160
program search.

53:35.160 --> 53:38.120
But you're not going to do it via brute force.

53:38.120 --> 53:42.560
You're not going to try things kind of like randomly.

53:42.560 --> 53:45.920
You're actually going to ask another deep learning model

53:45.920 --> 53:46.960
for suggestions.

53:46.960 --> 53:49.840
Like, here's the best likely next step.

53:49.840 --> 53:52.040
Here's where in the graph you should be going.

53:52.040 --> 53:54.560
And you can also use yet another deep learning model

53:54.560 --> 53:57.720
for feedback about, well, here's what I had so far.

53:57.720 --> 53:58.760
Is it looking good?

53:58.760 --> 54:01.240
Should I just backtrack and try something new?

54:01.240 --> 54:06.040
So I think discrete program search is going to be the key,

54:06.040 --> 54:08.200
but you want to make it dramatically better,

54:08.200 --> 54:09.840
all those of magnitude more efficient,

54:09.840 --> 54:11.120
by leveraging deep learning.

54:11.120 --> 54:13.520
And by the way, another thing that you can use deep learning

54:13.520 --> 54:16.640
is, of course, things like common sense knowledge,

54:16.640 --> 54:18.920
and knowledge in general.

54:18.920 --> 54:20.560
And I think you're going to end up

54:20.560 --> 54:22.560
with this sort of system where you

54:22.560 --> 54:27.360
have this on-the-fly synthesis engine that

54:27.360 --> 54:29.400
can adapt to new situations.

54:29.440 --> 54:31.160
But the way it adapts is that it's

54:31.160 --> 54:35.960
going to fetch from a bank of patterns,

54:35.960 --> 54:39.160
modules that could be themselves,

54:39.160 --> 54:42.240
curves that could be differentiable modules,

54:42.240 --> 54:44.560
and some others that could be algorithmic in nature.

54:44.560 --> 54:48.760
It's going to assemble them via this process that's

54:48.760 --> 54:50.360
intuition-guided.

54:50.360 --> 54:52.640
And it's going to give you, for every new situation you

54:52.640 --> 54:54.240
might be faced with, it's going to give you

54:54.240 --> 54:57.400
with a generalizable model that was synthesized

54:57.400 --> 55:00.600
using very, very little data.

55:00.600 --> 55:02.520
Something like this would sort of arc.

55:02.520 --> 55:05.640
That's actually a really interesting prompt,

55:05.640 --> 55:08.720
because I think an interesting crux here

55:08.720 --> 55:11.440
is when I talk to my friends who are extremely

55:11.440 --> 55:17.480
optimistic about LLMs and expect AGI within the next couple

55:17.480 --> 55:20.520
of years, they also, in some sense,

55:20.520 --> 55:23.760
agree that scaling is not all you need,

55:23.760 --> 55:26.600
but that the rest of the progress is undergirded

55:26.640 --> 55:28.640
and enabled by scaling.

55:28.640 --> 55:31.520
But still, you need to add the system

55:31.520 --> 55:34.920
to the test time compute atop these models.

55:34.920 --> 55:36.880
And their perspective is that it's relatively

55:36.880 --> 55:38.960
straightforward to do that, because you

55:38.960 --> 55:41.600
have this library of representations

55:41.600 --> 55:43.560
that you built up from free training,

55:43.560 --> 55:46.800
but it's almost talking like, it's just

55:46.800 --> 55:48.640
like skimming through textbooks.

55:48.640 --> 55:52.120
You need some more deliberate way in which it engages

55:52.120 --> 55:53.480
with the material it learns.

55:53.480 --> 55:56.520
In-context learning is extremely sample-efficient.

55:56.520 --> 55:59.000
But to actually distill that into the weights,

55:59.000 --> 56:01.480
you need the model to talk through the things that sees

56:01.480 --> 56:03.000
and then add it back to the weights.

56:03.000 --> 56:05.640
As far as the system 2 goes, they talk about adding some kind

56:05.640 --> 56:08.280
of RL setup so that it is encouraged

56:08.280 --> 56:12.640
to proceed on the reasoning traces that end up being correct.

56:12.640 --> 56:14.720
And they think this is relatively straightforward stuff

56:14.720 --> 56:16.560
that will be added within the next couple of years.

56:16.560 --> 56:17.840
That's an empirical question.

56:17.840 --> 56:18.880
So I think we'll see.

56:18.880 --> 56:20.440
Your intuition, I assume, is not that.

56:20.440 --> 56:21.120
I'm curious.

56:21.120 --> 56:24.840
My intuition is, in fact, this whole system

56:24.840 --> 56:27.160
2 architecture is the hard part.

56:27.160 --> 56:29.040
It's the very hard and non-obvious part.

56:29.040 --> 56:32.800
Scaling up the interpolative memory is the easy part.

56:32.800 --> 56:37.080
All you need is, like, it's literally just a big curve.

56:37.080 --> 56:38.120
All you need is more data.

56:38.120 --> 56:39.560
It's representation of a data set,

56:39.560 --> 56:41.840
interpolative representation of data set.

56:41.840 --> 56:42.840
That's the easy part.

56:42.840 --> 56:45.960
The hard part is the architecture of intelligence.

56:45.960 --> 56:48.600
Memory and intelligence are separate components.

56:48.600 --> 56:49.480
We have the memory.

56:49.480 --> 56:51.040
We don't have the intelligence yet.

56:51.040 --> 56:53.400
And I agree with you that, well, having the memory

56:53.400 --> 56:54.920
is actually very useful.

56:54.920 --> 56:57.080
And if you just had the intelligence,

56:57.080 --> 56:59.080
but it was not hooked up to an extensive memory,

56:59.080 --> 57:01.320
it would not be that useful, because it would not

57:01.320 --> 57:04.240
have enough material to work from.

57:04.240 --> 57:04.960
Yeah.

57:04.960 --> 57:07.720
The alternative hypothesis here that former guest Trenton

57:07.720 --> 57:11.360
Brickin advanced is that intelligence

57:11.360 --> 57:14.840
is just hierarchically associated memory

57:14.840 --> 57:18.000
where higher-level patterns, when Sherlock Holmes goes

57:18.000 --> 57:20.600
into a crime scene, and he's extremely sample-efficient,

57:20.600 --> 57:22.360
he can just look at a few clues and figure out

57:22.400 --> 57:23.920
who was a murderer, and the way he's

57:23.920 --> 57:26.800
able to do that is he has learned higher-level

57:26.800 --> 57:28.080
sort of associations.

57:28.080 --> 57:30.280
It's memory in some fundamental sense.

57:30.280 --> 57:33.960
But so here's one way to ask the question.

57:33.960 --> 57:37.400
In the brain, supposedly we do program synthesis,

57:37.400 --> 57:40.400
but it is just synapses connected to one another,

57:40.400 --> 57:43.080
each other, and so physically it's

57:43.080 --> 57:45.560
got to be that you just query the right circuit, right?

57:45.560 --> 57:46.720
You are, yeah, yeah, yeah.

57:46.720 --> 57:48.320
It's a matter of degree.

57:48.320 --> 57:51.800
But if you can learn it, if training in the environment

57:52.360 --> 57:53.960
human ancestors are trained in means

57:53.960 --> 57:55.680
you learn those circuits, training

57:55.680 --> 57:57.520
on the same kinds of outputs that humans produce,

57:57.520 --> 58:00.080
which to replicate require these kinds of circuits,

58:00.080 --> 58:03.480
wouldn't that train the same kind of whatever humans have?

58:03.480 --> 58:05.040
You know, it's a matter of degree.

58:07.720 --> 58:09.560
If you have a system that has a memory

58:09.560 --> 58:13.680
and is only capable of doing local generalization from that,

58:13.680 --> 58:16.760
it's not going to be very adaptable.

58:16.760 --> 58:19.160
To be really general, you need the memory

58:19.160 --> 58:23.200
plus the ability to search to quite some depth,

58:23.200 --> 58:26.880
to achieve broader even extramuralization.

58:26.880 --> 58:31.600
You know, like one of my favorite psychologists,

58:31.600 --> 58:35.320
so Jean Piaget was the founder of the Elemental Psychology.

58:35.320 --> 58:37.720
He had a very good quote about intelligence.

58:37.720 --> 58:40.960
He said, intelligence is what you use when you don't know what

58:40.960 --> 58:42.000
to do.

58:42.000 --> 58:46.480
And it's like, as a human living your life,

58:46.480 --> 58:48.760
in most situations you already know what to do,

58:48.760 --> 58:50.680
because you've been in this situation before.

58:50.680 --> 58:53.440
You already have the answer, right?

58:53.440 --> 58:55.560
And you're only going to need to use intelligence

58:55.560 --> 58:59.280
when you're faced with novelty, with something you didn't expect,

58:59.280 --> 59:01.240
with something that you weren't prepared for,

59:01.240 --> 59:04.960
either by your own experience, your own life experience,

59:04.960 --> 59:07.480
or by your evolutionary history.

59:07.480 --> 59:11.560
Like, this day that you're living right now is different

59:11.560 --> 59:14.840
in some important ways from every day you've lived before,

59:14.840 --> 59:17.440
but it's also different from any day ever lived

59:17.440 --> 59:18.880
by any of your ancestors.

59:18.880 --> 59:22.600
And still, you're capable of being functional, right?

59:22.600 --> 59:23.480
How is it possible?

59:23.480 --> 59:25.760
I'm not denying that generalization is extremely important,

59:25.760 --> 59:28.960
and is the basis for intelligence.

59:28.960 --> 59:30.600
That's not the correct, the correct is like,

59:30.600 --> 59:32.120
how much of that is happening in the models?

59:32.120 --> 59:35.000
But, okay, let me ask a separate question.

59:35.000 --> 59:38.520
We might keep going in the circle here.

59:38.520 --> 59:41.000
The differences in intelligence between humans,

59:41.000 --> 59:43.480
maybe the intelligence tests because of reasons

59:43.480 --> 59:44.600
you mentioned are not measuring it well,

59:44.600 --> 59:46.080
but clearly there's differences in intelligence

59:46.080 --> 59:47.080
between different humans.

59:47.680 --> 59:49.640
What is your explanation for what's going on there?

59:49.640 --> 59:52.160
Because I think that's sort of compatible with my story

59:52.160 --> 59:53.720
that there's a spectrum of generality

59:53.720 --> 59:56.520
and that these models are climbing up to a human level,

59:56.520 --> 59:58.280
and even some humans haven't even climbed up

59:58.280 --> 01:00:02.400
to the Einstein level or the Francois level, but.

01:00:02.400 --> 01:00:04.440
That's a great question, you know.

01:00:04.440 --> 01:00:07.960
There is extensive evidence that intelligence,

01:00:07.960 --> 01:00:11.000
difference in intelligence are mostly genetic in nature,

01:00:11.000 --> 01:00:11.840
right?

01:00:11.840 --> 01:00:14.400
Meaning that if you take someone who is not very intelligent,

01:00:14.400 --> 01:00:17.880
there is no amount of training, of like training data,

01:00:17.880 --> 01:00:19.720
you can expose that person to that would

01:00:21.120 --> 01:00:22.880
make them become Einstein.

01:00:22.880 --> 01:00:25.360
And this kind of points to the fact

01:00:25.360 --> 01:00:28.560
that you really need a better architecture,

01:00:28.560 --> 01:00:30.240
you need a better algorithm,

01:00:30.240 --> 01:00:34.040
and more training data is not in fact all you need.

01:00:34.040 --> 01:00:35.920
I think I agree with that.

01:00:35.920 --> 01:00:39.000
I think what, maybe the way I might phrase it is that

01:00:39.000 --> 01:00:42.240
the people who are smarter have in ML language

01:00:42.280 --> 01:00:45.680
better initializations, the neural wiring,

01:00:45.680 --> 01:00:48.040
if you just look at, it's more efficient,

01:00:48.040 --> 01:00:51.400
they have maybe greater density of firing.

01:00:51.400 --> 01:00:53.200
And so as some part of the story is scaling,

01:00:53.200 --> 01:00:55.320
there is some correlation between brain size

01:00:55.320 --> 01:00:56.600
and intelligence.

01:00:56.600 --> 01:01:00.160
And we also see within the context of quote unquote,

01:01:00.160 --> 01:01:01.360
scaling that people talk about

01:01:01.360 --> 01:01:04.400
within the context of LLMs, architectural improvements,

01:01:04.400 --> 01:01:07.920
where a model like Gemini 1.5 flash

01:01:07.920 --> 01:01:10.720
is performs as well as GPT-4 did

01:01:10.720 --> 01:01:12.560
when GPT-4 was released a year ago,

01:01:12.560 --> 01:01:15.320
but is 57 times cheaper on output.

01:01:15.320 --> 01:01:17.480
So the part of the scaling story

01:01:17.480 --> 01:01:19.240
is that the architectural improvements are,

01:01:19.240 --> 01:01:21.640
we're in like extremely low hanging fruit territory

01:01:21.640 --> 01:01:23.160
when it comes to those.

01:01:23.160 --> 01:01:27.560
Okay, we're back now with the co-founder of Zapier,

01:01:27.560 --> 01:01:31.040
Mike Canouf, we had to restart a few times there.

01:01:31.040 --> 01:01:32.880
And you're funding this prize

01:01:32.880 --> 01:01:35.800
and you're running this prize with Francois.

01:01:35.800 --> 01:01:38.760
And so tell me about how this came together,

01:01:39.720 --> 01:01:41.760
what more prompted you guys to launch this prize?

01:01:41.760 --> 01:01:44.240
Yeah, I guess I've been sort of like AI curious

01:01:44.240 --> 01:01:46.680
for 13 years, I co-founded Zapier,

01:01:46.680 --> 01:01:48.520
been running it for the last 13 years.

01:01:48.520 --> 01:01:51.240
And I think I first got introduced to your work

01:01:51.240 --> 01:01:54.560
and during COVID, I kind of went down the rabbit hole,

01:01:54.560 --> 01:01:56.320
we had a lot of free time.

01:01:56.320 --> 01:01:58.960
And it was right after you published your

01:01:58.960 --> 01:01:59.960
on measure of intelligence paper,

01:01:59.960 --> 01:02:01.840
you sort of introduced the concept of AGI,

01:02:01.840 --> 01:02:03.400
this like efficiency of skill acquisition

01:02:03.400 --> 01:02:06.280
is like the right definition and the arc puzzles.

01:02:06.320 --> 01:02:09.040
But I don't think the first Kaggle contest was done yet.

01:02:09.040 --> 01:02:10.480
I think it was still running.

01:02:10.480 --> 01:02:12.600
And so I kind of, it was interesting,

01:02:12.600 --> 01:02:14.880
but I just parked the idea.

01:02:14.880 --> 01:02:16.920
And my bigger fish to fry at Zapier

01:02:16.920 --> 01:02:18.480
were in this middle of this big turnaround

01:02:18.480 --> 01:02:21.240
of trying to get to our second product.

01:02:21.240 --> 01:02:24.040
And then it was January, 2022,

01:02:24.040 --> 01:02:25.680
when the chain of thought paper came out

01:02:25.680 --> 01:02:28.560
that really like awoken me to sort of the progress.

01:02:28.560 --> 01:02:30.560
I gave a whole presentation to the Zapier

01:02:30.560 --> 01:02:31.880
on like the GPT-3 paper events.

01:02:31.880 --> 01:02:33.720
I'd sort of felt like I had priced in everything

01:02:33.720 --> 01:02:35.720
that Elms could do and that paper was

01:02:35.760 --> 01:02:37.360
really shocking to me in terms of

01:02:37.360 --> 01:02:39.240
these latent capabilities that Elms have

01:02:39.240 --> 01:02:41.520
that I didn't expect that they had.

01:02:41.520 --> 01:02:45.600
And so I actually gave up my exact team role at Zapier.

01:02:45.600 --> 01:02:46.720
I was running half the company at that point

01:02:46.720 --> 01:02:48.800
and I went back to be an individual contributor

01:02:48.800 --> 01:02:50.880
and just to go do AI research

01:02:50.880 --> 01:02:52.480
alongside Brian, my co-founder.

01:02:53.920 --> 01:02:56.560
And all of that led me to back towards arc.

01:02:56.560 --> 01:02:57.800
I was looking into it again

01:02:57.800 --> 01:03:02.120
and I had sort of expected to see this saturation effect

01:03:02.120 --> 01:03:05.680
that MMLU has, that GMSK 8K has.

01:03:05.680 --> 01:03:07.840
And when I looked at the scores and the progress

01:03:07.840 --> 01:03:11.160
since the last four years, I was really again, shocked to see

01:03:11.160 --> 01:03:13.440
actually we've made very little objective progress

01:03:13.440 --> 01:03:16.640
towards it and it felt very,

01:03:16.640 --> 01:03:18.280
it felt like a really, really important Eval.

01:03:18.280 --> 01:03:19.640
And as I sort of spent the last year

01:03:19.640 --> 01:03:21.120
asking people, quizzing people about it

01:03:21.120 --> 01:03:22.880
and sort of my network and community,

01:03:23.760 --> 01:03:26.040
very few people even knew it existed.

01:03:26.040 --> 01:03:29.000
And that felt like, okay, if it's right

01:03:29.000 --> 01:03:31.440
that this is a really, really like globally

01:03:31.440 --> 01:03:34.840
singularly unique EGI Eval.

01:03:34.840 --> 01:03:36.440
And it's different from every other Eval that exists

01:03:36.440 --> 01:03:40.200
that are more narrowly measures AI skill.

01:03:40.200 --> 01:03:42.120
Like more people should know about this thing.

01:03:42.120 --> 01:03:44.400
I had my own ideas on how to beat the arc as well.

01:03:44.400 --> 01:03:46.320
So like I was working on nights and weekends on that

01:03:46.320 --> 01:03:49.600
and I flew up to meet Francois earlier this year

01:03:49.600 --> 01:03:51.480
to sort of quiz him, show him my ideas.

01:03:51.480 --> 01:03:54.160
And ultimately I was like, well,

01:03:54.160 --> 01:03:56.480
why don't you think more people know about arc?

01:03:56.480 --> 01:03:57.440
I think you should actually answer that.

01:03:57.440 --> 01:03:59.280
I think it's a really interesting question.

01:03:59.280 --> 01:04:01.320
Like why don't you think more people know about arc?

01:04:01.320 --> 01:04:05.000
Sure, you know, I think benchmarks that gain traction

01:04:05.000 --> 01:04:06.520
in the research community are benchmarks

01:04:06.520 --> 01:04:08.680
that are already fairly tractable

01:04:08.680 --> 01:04:11.600
because the dynamic that you see is that some research group

01:04:11.600 --> 01:04:13.720
is gonna make some initial breakthrough

01:04:13.720 --> 01:04:16.760
and then this is gonna catch the attention of everyone else.

01:04:16.760 --> 01:04:18.480
And so you're gonna get follow-up papers

01:04:18.480 --> 01:04:22.200
with people trying to beat the first team and so on.

01:04:22.200 --> 01:04:24.520
And for arc, this has not really happened

01:04:24.520 --> 01:04:26.360
because arc is actually very hard

01:04:26.360 --> 01:04:27.800
for existing AI techniques.

01:04:27.800 --> 01:04:30.920
Kind of arc requires you to try new ideas.

01:04:31.000 --> 01:04:33.280
And that's very much the point, by the way.

01:04:33.280 --> 01:04:35.280
Like the point is not that, yeah,

01:04:35.280 --> 01:04:37.640
you should just be able to apply existing technology

01:04:37.640 --> 01:04:38.480
and solve arc.

01:04:38.480 --> 01:04:42.920
The point is that existing technology has reached a plateau

01:04:42.920 --> 01:04:44.800
and if you want to go beyond that,

01:04:44.800 --> 01:04:47.760
if you want to start being able to tackle problems

01:04:47.760 --> 01:04:50.800
that you haven't memorized, that you haven't seen before,

01:04:50.800 --> 01:04:52.440
you need to try new ideas.

01:04:52.440 --> 01:04:57.440
And arc is not just meant to be this sort of like measure

01:04:58.440 --> 01:05:01.000
of how close we are to a GI.

01:05:01.000 --> 01:05:04.360
It's also meant to be a source of inspiration.

01:05:04.360 --> 01:05:06.680
Like I want researchers to look at these puzzles

01:05:06.680 --> 01:05:09.080
and be like, hey, it's really strange

01:05:09.080 --> 01:05:11.560
that these puzzles are so simple

01:05:11.560 --> 01:05:15.320
and most humans can just do them very quickly.

01:05:15.320 --> 01:05:18.400
Why is it so hard for existing AI systems?

01:05:18.400 --> 01:05:20.800
Why is it so hard for LLMs and so on?

01:05:20.800 --> 01:05:23.480
And it's true for LLMs, but arc was actually released

01:05:23.480 --> 01:05:25.480
before LLMs were really a thing.

01:05:25.520 --> 01:05:28.880
And the only thing that made it special at the time

01:05:28.880 --> 01:05:32.240
was that it was designed to be a resistance to memorization.

01:05:32.240 --> 01:05:34.800
And the fact that it has survived LLMs

01:05:34.800 --> 01:05:37.480
and Genia in general so well,

01:05:37.480 --> 01:05:38.680
kind of shows that yes,

01:05:38.680 --> 01:05:40.800
it is actually resistant to memorization.

01:05:40.800 --> 01:05:42.280
This is what nerds night me

01:05:42.280 --> 01:05:44.200
because I went and took a bunch of the puzzles myself.

01:05:44.200 --> 01:05:45.720
I've showed it to all my friends and family too

01:05:45.720 --> 01:05:48.560
and they're all like, oh yeah, this is like super easy.

01:05:49.480 --> 01:05:51.240
Are you sure AI can't solve this?

01:05:51.240 --> 01:05:54.160
Like that's the reaction in the same one for me as well.

01:05:54.160 --> 01:05:55.440
And the more you dig in, you're like, okay,

01:05:55.440 --> 01:05:57.320
yep, there's not just empirical evidence

01:05:57.320 --> 01:05:58.720
over the last four years that it's unbeaten,

01:05:58.720 --> 01:06:01.800
but there's theoretical like concepts behind why.

01:06:02.640 --> 01:06:04.200
And I completely agree at this point

01:06:04.200 --> 01:06:06.240
that like new ideas basically are needed to be dark.

01:06:06.240 --> 01:06:08.000
And there's a lot of current trends in the world

01:06:08.000 --> 01:06:09.240
that are actually, I think,

01:06:09.240 --> 01:06:12.280
working against that happening basically.

01:06:12.280 --> 01:06:13.320
I think we're actually less likely

01:06:13.320 --> 01:06:14.880
to generate new ideas right now.

01:06:15.840 --> 01:06:17.680
You know, I think one of the kind of trends

01:06:17.680 --> 01:06:19.480
is the closing up frontier research, right?

01:06:19.480 --> 01:06:22.720
The GP4 paper from Open AI had no technical details shared.

01:06:22.720 --> 01:06:24.480
The Gemini paper had no technical details shared

01:06:24.520 --> 01:06:27.160
and like the longer context part of that work.

01:06:27.160 --> 01:06:30.200
And yet that open innovation and open progress and sharing

01:06:30.200 --> 01:06:32.120
is what got us to transformers in the first place.

01:06:32.120 --> 01:06:35.120
That's what got us to LMS in the first place.

01:06:35.120 --> 01:06:37.920
So it's kind of disappointing a little bit actually

01:06:37.920 --> 01:06:40.000
that like so much frontier work has gone closed.

01:06:40.000 --> 01:06:42.560
It's really making a bet that like these individual labs

01:06:42.560 --> 01:06:43.760
are going to have the breakthrough

01:06:43.760 --> 01:06:46.240
and not the ecosystem is going to have the breakthrough.

01:06:46.240 --> 01:06:48.360
And I think sort of the internet open source has shown

01:06:48.360 --> 01:06:50.360
that that's like the most powerful innovation ecosystem

01:06:50.360 --> 01:06:52.480
that's ever existed probably in the entire world.

01:06:52.480 --> 01:06:54.080
I think that's actually really sad

01:06:54.080 --> 01:06:57.720
that frontier research is no longer being published.

01:06:57.720 --> 01:06:59.960
If you look back, you know, four years ago,

01:07:01.280 --> 01:07:03.080
well, everything was just openly shared

01:07:03.080 --> 01:07:05.880
like all the state of the art results were published

01:07:05.880 --> 01:07:07.160
and this is no longer the case.

01:07:07.160 --> 01:07:08.400
And it's very much, you know,

01:07:08.400 --> 01:07:11.400
Open AI single-handedly changed the game.

01:07:11.400 --> 01:07:16.400
And I think Open AI basically set back progress towards HGI

01:07:17.560 --> 01:07:20.160
by quite a few years, probably like five to 10 years

01:07:20.160 --> 01:07:21.000
for two reasons.

01:07:21.000 --> 01:07:25.640
And one is that, well, they cause this complete closing down

01:07:25.640 --> 01:07:28.200
of research, frontier research publishing,

01:07:28.200 --> 01:07:33.200
but also they trigger this initial burst of hype

01:07:34.480 --> 01:07:35.520
around LLMS.

01:07:35.520 --> 01:07:39.640
And now LLMS have sucked the oxygen out of the room

01:07:39.640 --> 01:07:43.320
like everything, everyone is just doing LLMS.

01:07:43.320 --> 01:07:47.080
And I see LLMS as a more often off-ramp

01:07:47.080 --> 01:07:49.840
on the path to HGI actually.

01:07:49.880 --> 01:07:51.960
And all these new resources,

01:07:51.960 --> 01:07:54.160
they're actually going to LLMS instead

01:07:54.160 --> 01:07:56.920
of everything else they could be going to.

01:07:56.920 --> 01:07:59.720
And, you know, if you look further into the past

01:07:59.720 --> 01:08:02.760
to like 2015, 2016,

01:08:02.760 --> 01:08:05.520
there were like a thousand times fewer people

01:08:05.520 --> 01:08:07.280
doing AI back then.

01:08:07.280 --> 01:08:10.740
And yet I feel like the rate of progress was higher

01:08:10.740 --> 01:08:14.480
because people were exploring more directions.

01:08:14.480 --> 01:08:16.400
The world felt more open-ended.

01:08:16.400 --> 01:08:18.400
Like you could just go and try,

01:08:18.400 --> 01:08:20.400
like have a cool idea of a launch

01:08:20.400 --> 01:08:22.360
and try it and get some interesting results.

01:08:22.360 --> 01:08:24.440
So there was this energy.

01:08:24.440 --> 01:08:27.440
And now everyone is very much doing some variation

01:08:27.440 --> 01:08:28.800
of the same thing.

01:08:28.800 --> 01:08:32.720
And the big labs also tried their hand on arc,

01:08:32.720 --> 01:08:34.560
but because they got bad results,

01:08:34.560 --> 01:08:35.840
they didn't publish anything.

01:08:35.840 --> 01:08:39.520
Like, you know, people only publish positive results.

01:08:39.520 --> 01:08:43.840
I wonder how much effort people have put into

01:08:43.840 --> 01:08:46.400
trying to prompt or scaffold,

01:08:46.400 --> 01:08:48.920
do some sort of maybe Devon type approach

01:08:48.920 --> 01:08:52.280
into getting the frontier models

01:08:52.280 --> 01:08:54.280
and the frontier models of today, not just a year ago,

01:08:54.280 --> 01:08:55.440
because a lot of post-training

01:08:55.440 --> 01:08:57.120
has gone into making them better.

01:08:57.120 --> 01:09:00.000
So Claude Friropas or GPT-40

01:09:00.000 --> 01:09:02.880
into getting good solutions on arc.

01:09:04.760 --> 01:09:06.760
I hope that one of the things this episode does

01:09:06.760 --> 01:09:09.200
is get people to try out this open competition

01:09:09.200 --> 01:09:12.920
where they have to put in an open source model to compete.

01:09:12.920 --> 01:09:14.880
But also to like figure out if they're,

01:09:14.880 --> 01:09:17.880
maybe the like capability is latent in Claude Opus

01:09:17.880 --> 01:09:20.240
and just see if you can show that.

01:09:20.240 --> 01:09:21.920
I think that would be super interesting.

01:09:21.920 --> 01:09:23.240
So let's talk about the prize.

01:09:23.240 --> 01:09:25.760
How much do you win if you solve it?

01:09:25.760 --> 01:09:27.920
You know, get whatever percent on arc.

01:09:27.920 --> 01:09:29.680
How much do you get if you get the best of vision,

01:09:29.680 --> 01:09:30.600
but don't crack it?

01:09:30.600 --> 01:09:31.600
So we got a million dollar,

01:09:31.600 --> 01:09:32.600
actually a little over a million dollars

01:09:32.600 --> 01:09:33.640
is the price pool.

01:09:33.640 --> 01:09:35.880
We're running the contest on an annual basis.

01:09:35.880 --> 01:09:37.520
We're gonna, we're starting it today

01:09:37.520 --> 01:09:39.800
through the middle of November.

01:09:39.800 --> 01:09:41.840
And the goal is to get 85%.

01:09:41.840 --> 01:09:43.320
That's the lower bound and human average

01:09:43.320 --> 01:09:44.920
that you guys talked about earlier.

01:09:44.920 --> 01:09:48.080
And there's a $500,000 prize for the first team

01:09:48.080 --> 01:09:50.560
that can get to the 85% benchmark.

01:09:50.560 --> 01:09:51.720
We're also gonna run,

01:09:51.720 --> 01:09:54.000
we don't expect that to happen this year actually.

01:09:54.000 --> 01:09:57.040
One of the early statisticians that's up here

01:09:57.040 --> 01:09:59.360
giving this line that has always stuck with me

01:09:59.360 --> 01:10:01.280
that the longer it takes, the longer it takes.

01:10:01.280 --> 01:10:04.600
So my prior is that like arc is gonna take years to solve.

01:10:05.360 --> 01:10:06.240
And so we're gonna keep to,

01:10:06.240 --> 01:10:08.840
we're also gonna break down and do a progress price this year.

01:10:08.840 --> 01:10:10.960
So there's a $100,000 progress price,

01:10:10.960 --> 01:10:13.760
which we will pay out to the top scores.

01:10:13.760 --> 01:10:18.640
So $50,000 is gonna go to the top objective scores this year

01:10:18.640 --> 01:10:19.600
on the Kaggle leaderboard,

01:10:19.600 --> 01:10:21.320
which we're hosting it on Kaggle.

01:10:21.320 --> 01:10:23.160
And then we're gonna have a $50,000 pot set

01:10:23.160 --> 01:10:26.200
for a paper award for the best paper

01:10:26.200 --> 01:10:28.720
that explains conceptually the scores

01:10:28.720 --> 01:10:30.160
that they were able to achieve.

01:10:30.160 --> 01:10:31.600
And one of the I think interesting things

01:10:31.600 --> 01:10:33.680
we're also gonna be doing is,

01:10:33.680 --> 01:10:35.880
we're gonna be requiring that in order to win the prize money

01:10:35.880 --> 01:10:38.040
that you put the solution or your paper

01:10:38.040 --> 01:10:39.360
out into public domain.

01:10:40.320 --> 01:10:41.960
The reason for this is,

01:10:41.960 --> 01:10:43.360
typically with contests,

01:10:43.360 --> 01:10:45.320
you see a lot of like closed up sharing.

01:10:45.320 --> 01:10:46.600
People are kind of private secret.

01:10:46.600 --> 01:10:47.720
They wanna hold their alpha to themselves

01:10:47.720 --> 01:10:49.120
during the contest period.

01:10:49.120 --> 01:10:52.080
And because we expect it's gonna be multiple years,

01:10:52.080 --> 01:10:53.160
we wanna enter a game here.

01:10:53.160 --> 01:10:56.520
So the plan is at the end of November,

01:10:56.520 --> 01:10:58.520
we will award the $100,000 prize money

01:10:58.520 --> 01:10:59.920
to the top progress prize

01:10:59.920 --> 01:11:03.440
and then use the downtime between December, January, February

01:11:03.440 --> 01:11:06.840
to share out all the knowledge from the top scores

01:11:06.840 --> 01:11:08.200
and the approaches folks were taking

01:11:08.200 --> 01:11:10.080
in order to re-baseline the community

01:11:10.080 --> 01:11:11.720
up to whatever the state of the art is

01:11:11.720 --> 01:11:13.360
and then run the contest again next year.

01:11:13.360 --> 01:11:16.400
And keep doing that on a yearly basis until we get 85%.

01:11:16.400 --> 01:11:17.760
I'll give some people some context

01:11:17.760 --> 01:11:20.440
on why I think this prize is very interesting.

01:11:20.440 --> 01:11:22.720
I was having conversations with my friends

01:11:22.720 --> 01:11:26.360
who are very much believers in models as they exist today.

01:11:26.360 --> 01:11:28.200
And first of all, it was intriguing to me

01:11:28.200 --> 01:11:29.960
that they didn't know about ARC.

01:11:29.960 --> 01:11:32.240
These are experienced ML researchers.

01:11:32.240 --> 01:11:35.560
And so you show them this happened a couple of nights ago.

01:11:35.560 --> 01:11:38.240
We went to dinner and I showed them an example problem.

01:11:38.240 --> 01:11:39.160
And they said, of course,

01:11:39.160 --> 01:11:41.040
an LLM would be able to solve something like this.

01:11:41.040 --> 01:11:42.280
And then we take a screenshot of it.

01:11:42.280 --> 01:11:44.240
We just put it into our chat GPT app

01:11:44.240 --> 01:11:45.520
and it doesn't get the pattern.

01:11:45.520 --> 01:11:48.600
And so I think it's very interesting.

01:11:48.600 --> 01:11:49.720
Like it is a notable fact.

01:11:49.720 --> 01:11:51.600
I was sort of playing devil's advocate against you

01:11:51.600 --> 01:11:52.440
on these kinds of questions.

01:11:52.440 --> 01:11:53.920
But this is a very intriguing fact.

01:11:53.920 --> 01:11:56.800
And I'm extreme, I think this prize is extremely interesting

01:11:56.800 --> 01:11:58.240
because we're gonna learn,

01:11:58.240 --> 01:12:01.240
we're gonna learn something fascinating one way or another.

01:12:01.240 --> 01:12:03.920
So with regards to the 85%,

01:12:03.960 --> 01:12:04.920
separate from this prize,

01:12:04.920 --> 01:12:07.640
I'd be very curious if somebody could replicate that result

01:12:07.640 --> 01:12:11.640
because obviously in psychology and other kinds of fields,

01:12:11.640 --> 01:12:15.000
which this result seems to be analogous to

01:12:15.000 --> 01:12:18.920
when you run test on some small sample of people,

01:12:18.920 --> 01:12:20.200
often they're hard to replicate.

01:12:20.200 --> 01:12:22.000
So I'd be very curious if you try to replicate this,

01:12:22.000 --> 01:12:25.440
how, what does an average human perform on ARC?

01:12:25.440 --> 01:12:27.360
Ask for the difficulty on how long it will take

01:12:27.360 --> 01:12:28.920
to crack this benchmark.

01:12:28.920 --> 01:12:31.160
It's very interesting because the other benchmarks

01:12:31.200 --> 01:12:34.040
that are now fully saturated like MMLU math,

01:12:34.040 --> 01:12:36.400
actually the people who made them,

01:12:36.400 --> 01:12:39.680
Dan Hendricks and Colin Burns who did MMLU and math,

01:12:39.680 --> 01:12:41.840
I think they were grad students or college students

01:12:41.840 --> 01:12:43.160
when they made it.

01:12:43.160 --> 01:12:45.400
And the goal when they made it just a couple of years ago

01:12:45.400 --> 01:12:47.960
was that this will be a test of AGI.

01:12:47.960 --> 01:12:49.240
And of course it got totally saturated.

01:12:49.240 --> 01:12:52.480
And I know you all argue that these are test memorization,

01:12:52.480 --> 01:12:54.120
but I think the pattern we've seen,

01:12:54.120 --> 01:12:57.120
in fact, Epoch AI has a very interesting graph

01:12:57.120 --> 01:12:59.440
that I'll sort of overlay for the YouTube version here

01:12:59.480 --> 01:13:02.200
where you see this almost exponential

01:13:02.200 --> 01:13:06.120
where it gets 5%, 10%, 30%, 40%

01:13:06.120 --> 01:13:07.880
as you increase the compute across models

01:13:07.880 --> 01:13:09.920
and then it just shoots up.

01:13:09.920 --> 01:13:12.640
And in the GPT-4 technical report,

01:13:12.640 --> 01:13:14.160
they had this interesting graph

01:13:14.160 --> 01:13:16.240
of the human eval problem set,

01:13:16.240 --> 01:13:18.400
which was 22 coding problems.

01:13:18.400 --> 01:13:22.200
And they had to graph it on the mean log pass curve,

01:13:22.200 --> 01:13:24.960
basically because early on in training

01:13:24.960 --> 01:13:28.200
or even smaller models can have the right idea

01:13:28.200 --> 01:13:29.760
of how to solve this problem,

01:13:29.760 --> 01:13:31.720
but it takes a lot of reliability

01:13:31.720 --> 01:13:34.000
to make sure they stay on track to solve the whole problem.

01:13:34.000 --> 01:13:36.120
And so you really wanna up wait the signal

01:13:36.120 --> 01:13:38.000
where they get it right at least some of the time,

01:13:38.000 --> 01:13:39.720
maybe one in a hundred times or one in a thousand.

01:13:39.720 --> 01:13:41.560
And then so they go from like one in a thousand,

01:13:41.560 --> 01:13:42.480
one in a hundred, one in 10,

01:13:42.480 --> 01:13:44.620
and then they just like totally saturated.

01:13:44.620 --> 01:13:46.280
I guess the question I have,

01:13:46.280 --> 01:13:47.160
this is all leading up to,

01:13:47.160 --> 01:13:49.900
is why won't the same thing happen with ARC

01:13:49.900 --> 01:13:53.480
where people had to try really hard, bigger models.

01:13:54.400 --> 01:13:56.240
And now they figured out these techniques

01:13:56.280 --> 01:13:57.120
that Jack Cole has figured out

01:13:57.120 --> 01:14:00.120
with only a 240 million parameter language model

01:14:00.120 --> 01:14:02.440
that can get 35%.

01:14:02.440 --> 01:14:03.560
Shouldn't we see the same pattern we saw

01:14:03.560 --> 01:14:04.480
across all these other benchmarks

01:14:04.480 --> 01:14:05.880
where you just like sort of eke out.

01:14:05.880 --> 01:14:07.560
And then once you get the general idea,

01:14:07.560 --> 01:14:09.800
then you just go all the way to a hundred.

01:14:09.800 --> 01:14:10.920
That's an empirical question.

01:14:10.920 --> 01:14:12.840
So we'll see in practice what happens.

01:14:13.840 --> 01:14:16.560
But what Jack Cole is doing is actually very unique.

01:14:16.560 --> 01:14:19.680
It's not just pre-training an LLM and then prompting it,

01:14:19.680 --> 01:14:21.960
he's actually trying to do active inference.

01:14:21.960 --> 01:14:23.160
He's doing a test time, right?

01:14:23.160 --> 01:14:24.000
He's doing like test time functioning.

01:14:24.000 --> 01:14:25.680
Exactly, test time functioning.

01:14:25.680 --> 01:14:27.680
And this is actually trying to lift

01:14:27.680 --> 01:14:29.680
one of the key limitations of LLMs,

01:14:29.680 --> 01:14:31.520
which is that at inference time,

01:14:31.520 --> 01:14:32.680
they cannot learn anything new.

01:14:32.680 --> 01:14:35.520
They cannot adapt on the flight what they're seeing.

01:14:35.520 --> 01:14:38.720
And he's actually trying to learn.

01:14:38.720 --> 01:14:40.800
So what he's doing is effectively

01:14:40.800 --> 01:14:42.960
a form of program synthesis.

01:14:44.080 --> 01:14:46.920
Because the LLM contains a lot of useful building blocks,

01:14:46.920 --> 01:14:48.680
like programming building blocks,

01:14:48.680 --> 01:14:52.080
and by finding it on the task at test time,

01:14:52.080 --> 01:14:54.600
you are trying to assemble these building blocks

01:14:54.600 --> 01:14:57.680
into the right pattern that matches the task.

01:14:57.680 --> 01:15:00.560
This is exactly what program synthesis is about.

01:15:00.560 --> 01:15:03.640
And the way we contrast this approach

01:15:03.640 --> 01:15:05.920
with discrete program search is that

01:15:05.920 --> 01:15:07.760
in discrete program search,

01:15:07.760 --> 01:15:10.240
so you're trying to assemble a program

01:15:10.240 --> 01:15:12.160
from a set of primitives.

01:15:12.160 --> 01:15:13.520
You have very few primitives.

01:15:13.520 --> 01:15:15.800
So people working on discrete program search on Arc,

01:15:15.800 --> 01:15:18.800
for instance, they tend to work with DSLs that have like

01:15:18.800 --> 01:15:21.920
100 to 200 primitive programs.

01:15:21.920 --> 01:15:23.640
So very small DSL,

01:15:23.640 --> 01:15:26.720
but then they're trying to combine these primitives

01:15:26.720 --> 01:15:29.160
into very complex programs.

01:15:29.160 --> 01:15:32.280
So there's very deep depths of search.

01:15:32.280 --> 01:15:34.600
And on the other hand,

01:15:34.600 --> 01:15:37.120
if you look at what Jack is doing with LLMs,

01:15:37.120 --> 01:15:42.120
is that he's got this sort of like vector program database,

01:15:43.200 --> 01:15:47.440
DSL of millions of building blocks in the LLM

01:15:47.440 --> 01:15:50.240
that are mined by pre-training the LLM,

01:15:50.240 --> 01:15:52.720
not just on a ton of programming problems,

01:15:52.720 --> 01:15:56.400
but also on millions of generated Arc-like tasks.

01:15:56.400 --> 01:15:59.680
So you have an extraordinarily large DSL.

01:15:59.680 --> 01:16:03.000
And then the fun tuning is very, very shallow

01:16:03.000 --> 01:16:04.880
recombination of these primitives.

01:16:04.880 --> 01:16:08.560
So discrete program search, very deep recombination,

01:16:08.560 --> 01:16:12.000
very small set of primitive programs.

01:16:12.000 --> 01:16:14.160
And the LLM approach is the same,

01:16:14.160 --> 01:16:17.260
but on the complete opposite end of that spectrum,

01:16:17.260 --> 01:16:19.400
where you scale up the memorization

01:16:19.400 --> 01:16:23.560
by a massive factor and you're doing very, very shallow search,

01:16:23.560 --> 01:16:25.400
but they are the same thing,

01:16:25.400 --> 01:16:27.400
just different ends of the spectrum.

01:16:27.400 --> 01:16:31.160
And I think where you're gonna get the most value

01:16:32.200 --> 01:16:36.200
for your compute cycles is gonna be somewhere in between.

01:16:36.200 --> 01:16:40.680
You want to leverage memorization to build up a richer,

01:16:40.680 --> 01:16:43.800
more useful bank of primitive programs.

01:16:43.800 --> 01:16:46.080
And you don't want them to be hard-coded

01:16:46.080 --> 01:16:48.400
like what we saw for the typical artist.

01:16:48.400 --> 01:16:51.200
You want them to be learned from examples.

01:16:51.200 --> 01:16:55.840
But then you also want to do some degree of deep search.

01:16:55.840 --> 01:16:58.160
As long as you're only doing very shallow search,

01:16:58.160 --> 01:17:00.120
you are limited to local generalization.

01:17:00.120 --> 01:17:01.960
If you want to generalize further,

01:17:01.960 --> 01:17:06.960
more broadly, this depth of search is gonna be critical.

01:17:07.240 --> 01:17:11.840
I might argue that the reason that he had to rely so heavily

01:17:11.840 --> 01:17:16.840
on the synthetic data was because he used a 240 million

01:17:16.840 --> 01:17:19.600
parameter model because the Kaggle competition at the time

01:17:19.600 --> 01:17:22.520
required him to use a P100 GPU,

01:17:22.520 --> 01:17:26.400
which has like a 10th or something of the flops of an H100.

01:17:26.400 --> 01:17:28.880
And so obviously he can't use,

01:17:28.880 --> 01:17:32.400
if you believe that sort of scaling will solve

01:17:32.400 --> 01:17:33.760
this kind of reasoning,

01:17:33.760 --> 01:17:36.560
then there you can just rely on the generalization,

01:17:36.560 --> 01:17:38.400
whereas if you're using a much smaller,

01:17:38.400 --> 01:17:39.920
for context for the listeners, by the way,

01:17:39.920 --> 01:17:41.520
the frontier models today are literally

01:17:41.520 --> 01:17:43.080
a thousand X bigger than that.

01:17:43.080 --> 01:17:46.280
And so for your competition,

01:17:46.280 --> 01:17:47.960
from what I remember,

01:17:47.960 --> 01:17:50.520
the submission you'll have to submit

01:17:50.520 --> 01:17:53.360
can't make any API calls, can't go online,

01:17:53.360 --> 01:17:57.640
and has to run on NVIDIA Tesla T4.

01:17:57.640 --> 01:17:58.480
P100.

01:17:58.480 --> 01:17:59.320
P100.

01:17:59.320 --> 01:18:00.160
Oh, is it P100?

01:18:00.160 --> 01:18:01.000
Yeah.

01:18:01.000 --> 01:18:02.320
Okay, so again, it's like significantly less powerful.

01:18:02.320 --> 01:18:03.760
There's a 12-hour runtime limit, basically.

01:18:03.760 --> 01:18:06.280
There's a forcing function of efficiency in the eval.

01:18:06.280 --> 01:18:09.600
But here's the thing, you only have 100 test tasks.

01:18:09.600 --> 01:18:12.040
So the amount of computing available for each task

01:18:12.040 --> 01:18:13.040
is actually quite a bit,

01:18:13.080 --> 01:18:14.720
especially if you contrast that

01:18:14.720 --> 01:18:16.640
with the simplicity of each task.

01:18:16.640 --> 01:18:19.600
So it would be seven minutes per task, basically,

01:18:19.600 --> 01:18:22.520
which for, people have tried to do these estimates

01:18:22.520 --> 01:18:24.600
of how many flops does a human brain have.

01:18:24.600 --> 01:18:26.080
And you can take them with a grain of salt,

01:18:26.080 --> 01:18:28.240
but as a sort of anchor,

01:18:28.240 --> 01:18:31.360
it's basically the amount of flops an H100 has.

01:18:31.360 --> 01:18:32.960
And I guess maybe you would argue with that,

01:18:32.960 --> 01:18:35.400
well, a human brain can solve this question

01:18:35.400 --> 01:18:36.560
in faster than 7.2 minutes.

01:18:36.560 --> 01:18:38.040
So even with a tenth of the compute,

01:18:38.040 --> 01:18:40.560
you should be able to do it in seven minutes.

01:18:40.560 --> 01:18:43.040
Obviously, we have less memory than, you know,

01:18:43.040 --> 01:18:45.880
like petabytes of fast access memory in the brain.

01:18:45.880 --> 01:18:48.840
And with these, you know, 29 or whatever gigabytes

01:18:48.840 --> 01:18:50.400
in this H100.

01:18:50.400 --> 01:18:52.480
Anyway, I guess the broader question I'm asking is,

01:18:55.360 --> 01:18:58.560
I wish there was a way to also test this prize

01:18:58.560 --> 01:19:01.120
with some sort of scaffolding on the biggest models

01:19:01.120 --> 01:19:04.000
as a way to test whether scaling is the path

01:19:04.000 --> 01:19:07.880
to get to solving ARC.

01:19:07.880 --> 01:19:08.720
Absolutely.

01:19:08.720 --> 01:19:09.920
So in the context of the computation,

01:19:09.920 --> 01:19:12.320
we want to see how much progress we can do

01:19:12.320 --> 01:19:13.800
with limited resources.

01:19:13.800 --> 01:19:16.320
But you're entirely right that it's a super interesting

01:19:16.320 --> 01:19:17.360
open question.

01:19:17.360 --> 01:19:20.600
What could the biggest model out there actually do on ARC?

01:19:20.600 --> 01:19:24.760
So we want to actually also make available a private

01:19:24.760 --> 01:19:29.480
sort of like one-off track where you can submit to us a VM

01:19:29.480 --> 01:19:32.000
and so you can put on it any model you want.

01:19:32.000 --> 01:19:34.560
Like you can take one of the largest open source models

01:19:34.560 --> 01:19:37.200
out there and find you need to do whatever you want

01:19:37.200 --> 01:19:39.880
and just give us an image.

01:19:39.880 --> 01:19:42.960
And then we run it on the H100 for like 24 hours

01:19:42.960 --> 01:19:44.680
or something and you see what you get.

01:19:44.680 --> 01:19:47.760
I think it's worth pointing out that there's two different

01:19:47.760 --> 01:19:48.600
test sets.

01:19:48.600 --> 01:19:50.760
There is a public test set that's in the public

01:19:50.760 --> 01:19:53.720
GitHub repository that anyone can use to train, you know,

01:19:53.720 --> 01:19:56.400
put it in an open API call, whatever you'd like to do.

01:19:56.400 --> 01:19:57.720
And then there's the private test set,

01:19:57.720 --> 01:19:59.320
which is the 100 that is actually measuring

01:19:59.320 --> 01:20:00.720
the state of the art.

01:20:00.720 --> 01:20:03.400
So I think it is pretty open and interesting to have folks

01:20:03.400 --> 01:20:05.920
attempt to at least use the public test set and go try it.

01:20:05.920 --> 01:20:09.160
Now, there is an asterisk on any score that's reported on

01:20:09.160 --> 01:20:11.320
against the public test set because it is public.

01:20:11.320 --> 01:20:13.320
It could have leaked into the training data.

01:20:13.320 --> 01:20:15.240
And this is actually what people are already doing.

01:20:15.240 --> 01:20:18.760
Like you can already try to prompt one of the best models

01:20:18.760 --> 01:20:22.880
like the latest Jaminar, the latest GPT-4 with tasks

01:20:22.880 --> 01:20:24.360
from the public evaluation set.

01:20:24.360 --> 01:20:27.400
And you know, again, the primary set, these tasks

01:20:27.400 --> 01:20:30.400
are available as JSON files on GitHub.

01:20:30.400 --> 01:20:32.440
These models are also trained on GitHub.

01:20:32.440 --> 01:20:34.800
So they're actually trained on these tasks.

01:20:35.720 --> 01:20:38.960
And yeah, that kind of creates uncertainty about

01:20:38.960 --> 01:20:40.960
if they can actually solve some of the tasks,

01:20:40.960 --> 01:20:43.960
is that because they memorized the answer or not.

01:20:43.960 --> 01:20:47.080
You know, maybe you would be better off trying to create

01:20:47.080 --> 01:20:53.080
your own private, arc-like, very novel test set.

01:20:53.120 --> 01:20:54.720
Don't make the task difficult.

01:20:54.720 --> 01:20:55.760
Don't make them complex.

01:20:55.760 --> 01:20:57.160
Make them very obvious for humans.

01:20:57.160 --> 01:21:00.840
But make sure to make them original as much as possible.

01:21:00.840 --> 01:21:02.480
Make them unique, different.

01:21:02.480 --> 01:21:06.720
And see how much your GPT-4 and so on GPT-5 does on them.

01:21:06.720 --> 01:21:08.720
Well, they're having tests on whether these models

01:21:08.720 --> 01:21:11.040
are being overtrained on these benchmarks.

01:21:11.040 --> 01:21:14.120
Scale recently did this where on the GSM-

01:21:14.120 --> 01:21:14.960
That's really interesting.

01:21:14.960 --> 01:21:17.560
AK, they basically replicated the benchmark

01:21:17.560 --> 01:21:18.840
with different questions.

01:21:18.840 --> 01:21:20.480
And so some of the models actually were extremely

01:21:20.480 --> 01:21:24.000
overfit on the benchmark like Mistral and so forth.

01:21:24.000 --> 01:21:28.600
And but the frontier models, Claude and GPT actually did

01:21:28.600 --> 01:21:30.760
as well on their novel benchmark that they did

01:21:30.760 --> 01:21:32.160
on the specific questions that were

01:21:32.160 --> 01:21:35.160
in the existing public benchmark.

01:21:35.160 --> 01:21:37.880
So I would be relatively optimistic about them

01:21:37.880 --> 01:21:40.320
just sort of training on the JSON.

01:21:40.320 --> 01:21:44.080
I was joking with Mike that you should allow API access

01:21:44.080 --> 01:21:49.080
but sort of keep an even more private validation set

01:21:49.160 --> 01:21:51.440
of these arc questions.

01:21:51.440 --> 01:21:53.760
And so allow API access, people can sort of play

01:21:53.760 --> 01:21:56.400
with GPT-4 scaffolding to enter into this contest.

01:21:56.400 --> 01:21:58.960
And if it turns out maybe later on you run the validation

01:21:58.960 --> 01:22:01.440
set on the API and if it performs worse

01:22:01.440 --> 01:22:03.880
than the test set that you allowed the API access

01:22:03.880 --> 01:22:07.160
to originally, that means that open AI is training

01:22:07.160 --> 01:22:09.760
on your API calls and you like go public with this

01:22:09.760 --> 01:22:10.760
and show them like, oh my God,

01:22:10.760 --> 01:22:13.080
they've like leaked your data.

01:22:13.080 --> 01:22:15.680
We do want to make, we want to evolve the arc data set.

01:22:15.680 --> 01:22:17.560
Like that is a goal that we want to do.

01:22:17.560 --> 01:22:19.520
I think Francois you mentioned, you know, it's not perfect.

01:22:19.520 --> 01:22:22.080
Yeah, no, arc is not perfect for perfect benchmark.

01:22:22.080 --> 01:22:24.080
I mean, I made it like four years ago

01:22:24.080 --> 01:22:26.720
over four years ago, almost five now.

01:22:26.720 --> 01:22:28.840
This was in a time before LMS.

01:22:28.840 --> 01:22:31.800
And I think we learned a lot actually since

01:22:31.800 --> 01:22:34.200
about what potential flaws there might be.

01:22:34.200 --> 01:22:37.560
I think there is some redundancy in the set of tasks

01:22:37.560 --> 01:22:40.200
which is of course against the goals of the benchmark.

01:22:40.200 --> 01:22:42.680
Every task is supposed to be unique in practice.

01:22:42.680 --> 01:22:44.000
That's not quite true.

01:22:44.000 --> 01:22:47.080
I think there's also, every task is supposed

01:22:47.080 --> 01:22:49.760
to be very novel, but in practice, they might not be.

01:22:49.760 --> 01:22:52.440
They might be structurally similar to something

01:22:52.440 --> 01:22:54.680
that you might find online somewhere.

01:22:54.680 --> 01:22:56.480
So we want to keep iterating

01:22:56.480 --> 01:23:00.240
and release an arc two version later this year.

01:23:00.240 --> 01:23:01.720
And I think when we do that,

01:23:01.720 --> 01:23:06.720
we're gonna want to make the old private test set available.

01:23:06.800 --> 01:23:08.720
So maybe we won't be releasing it publicly,

01:23:08.720 --> 01:23:13.400
but what we could do is just create a test server

01:23:13.400 --> 01:23:16.520
where you can query, get a task, you submit a solution,

01:23:16.520 --> 01:23:18.360
and of course you can use whatever frontier model

01:23:18.360 --> 01:23:19.640
you want there.

01:23:19.640 --> 01:23:22.720
So that way, because you actually have to query this API,

01:23:22.760 --> 01:23:26.160
you're making sure that no one is gonna buy accident train

01:23:26.160 --> 01:23:27.000
on this data.

01:23:27.000 --> 01:23:29.800
It's unlike like the current public article

01:23:29.800 --> 01:23:31.240
which is literally on GitHub.

01:23:31.240 --> 01:23:33.160
So there's no question about whether the models

01:23:33.160 --> 01:23:34.000
are actually trained on it.

01:23:34.000 --> 01:23:36.440
Yes, they are because they're trained on GitHub.

01:23:36.440 --> 01:23:39.520
So by sort of like gating access

01:23:39.520 --> 01:23:42.520
to querying this API with a various issue.

01:23:42.520 --> 01:23:44.120
And then we would see, you know,

01:23:44.120 --> 01:23:46.920
for people who actually wanna try

01:23:46.920 --> 01:23:48.120
whatever technique they have in mind

01:23:48.120 --> 01:23:50.760
using whatever resources they want,

01:23:50.760 --> 01:23:52.600
that would be a way for them to get an answer.

01:23:52.640 --> 01:23:54.160
I wonder what might happen.

01:23:54.160 --> 01:23:55.320
I'm not sure.

01:23:55.320 --> 01:23:58.520
One answer is that they come up with a whole new algorithm

01:23:58.520 --> 01:24:02.320
for AI with some explicit program synthesis

01:24:02.320 --> 01:24:03.840
that now we're on a new track.

01:24:03.840 --> 01:24:06.680
And another is they did something hacky

01:24:06.680 --> 01:24:10.320
with the existing models in a way that actually is valid,

01:24:10.320 --> 01:24:12.800
which reveals that movie intelligence is more

01:24:12.800 --> 01:24:15.320
of getting things to the right part of the distribution,

01:24:15.320 --> 01:24:16.640
but then it can reason.

01:24:16.640 --> 01:24:19.760
And in that world, I guess that will be interesting.

01:24:19.760 --> 01:24:21.440
And maybe that'll indicate that, you know,

01:24:21.480 --> 01:24:23.240
you had to do something hacky with current models

01:24:23.240 --> 01:24:24.080
as they get better,

01:24:24.080 --> 01:24:25.840
you won't have to do something hacky.

01:24:27.120 --> 01:24:29.200
I'm also gonna be very curious to see

01:24:29.200 --> 01:24:30.720
how these multimodal models,

01:24:30.720 --> 01:24:33.840
if they will perform natively much better at arc like tests.

01:24:33.840 --> 01:24:35.360
If arc survives three months from here,

01:24:35.360 --> 01:24:37.000
we'll blow up the price.

01:24:37.000 --> 01:24:39.000
I think we're about to make a really important moment

01:24:39.000 --> 01:24:41.640
of like contact with reality by blowing up the price,

01:24:41.640 --> 01:24:43.360
putting a much big price pool against it.

01:24:43.360 --> 01:24:44.400
We're gonna learn really quickly

01:24:44.400 --> 01:24:46.600
if there's like low hanging fruit of ideas.

01:24:46.600 --> 01:24:47.920
Again, I think new ideas are needed.

01:24:47.920 --> 01:24:49.000
I think anyone listening this

01:24:49.000 --> 01:24:51.160
might have the idea in their head.

01:24:51.160 --> 01:24:53.520
And I'd encourage everyone to like give it a try.

01:24:53.520 --> 01:24:55.520
And I think as time goes on,

01:24:55.520 --> 01:24:56.920
that adds strength to the argument

01:24:56.920 --> 01:24:59.280
that like we sort of stall that in progress

01:24:59.280 --> 01:25:00.920
and that new ideas are necessary to be dark.

01:25:00.920 --> 01:25:03.600
Yeah, that's the point of having a money price

01:25:03.600 --> 01:25:06.120
is that you attract more people,

01:25:06.120 --> 01:25:07.720
you get them to try to solve it.

01:25:07.720 --> 01:25:09.960
And if there's a easy way to hack the benchmark

01:25:09.960 --> 01:25:11.480
that reveals that the benchmark is valid,

01:25:11.480 --> 01:25:12.640
then you're gonna know about it.

01:25:12.640 --> 01:25:13.520
In fact, that was the point

01:25:13.520 --> 01:25:18.200
of the original Carol competition back in 2020 for arc.

01:25:19.040 --> 01:25:20.280
I was running this competition

01:25:20.320 --> 01:25:22.440
because I had released this dataset

01:25:22.440 --> 01:25:26.640
and I wanted to know if it was hackable, if you could cheat.

01:25:26.640 --> 01:25:28.800
So there was a small money price at the time,

01:25:28.800 --> 01:25:30.520
there was like 20K.

01:25:30.520 --> 01:25:32.360
And this was right around the same time

01:25:32.360 --> 01:25:34.720
as GPT-3 was released.

01:25:34.720 --> 01:25:37.560
So people of course tried GPT-3 on the public data,

01:25:37.560 --> 01:25:38.480
it scored zero.

01:25:39.560 --> 01:25:42.560
But I think what the first context

01:25:42.560 --> 01:25:45.240
the first context taught us is that

01:25:45.240 --> 01:25:48.160
there is no obvious shortcuts, right?

01:25:49.160 --> 01:25:50.680
And well, now there's more money,

01:25:50.680 --> 01:25:53.960
there's gonna be more people looking into it.

01:25:53.960 --> 01:25:56.120
Well, we're gonna find out,

01:25:56.120 --> 01:25:58.720
we're gonna see if the benchmark is gonna survive.

01:25:58.720 --> 01:26:02.400
And you know, if we end up with a solution

01:26:02.400 --> 01:26:05.840
that is not like trying to brute force

01:26:05.840 --> 01:26:07.400
the space of possible arc tasks

01:26:07.400 --> 01:26:09.920
that's just trained on core knowledge,

01:26:09.920 --> 01:26:13.800
I don't think it's necessarily gonna be in and by itself, AGI,

01:26:13.800 --> 01:26:16.440
but it's probably gonna be a huge milestone

01:26:16.440 --> 01:26:18.040
on the way to AGI.

01:26:18.120 --> 01:26:23.120
Because what it represents is the ability to synthesize,

01:26:25.800 --> 01:26:28.560
task a problem solving program

01:26:28.560 --> 01:26:31.920
from just two or three examples.

01:26:31.920 --> 01:26:35.080
And that alone is a new way to program.

01:26:35.080 --> 01:26:38.000
It's an entirely new paradigm for software development

01:26:38.000 --> 01:26:39.760
where you can start programming

01:26:39.760 --> 01:26:41.720
potentially quite complex programs

01:26:41.720 --> 01:26:44.000
that will generalize very well.

01:26:44.000 --> 01:26:46.680
And instead of programming them by coming up

01:26:46.720 --> 01:26:49.920
with the shape of the program in your mind

01:26:49.920 --> 01:26:52.080
and then tapping it up,

01:26:52.080 --> 01:26:54.800
you're actually just showing the computer

01:26:54.800 --> 01:26:55.960
what add what you want

01:26:55.960 --> 01:26:58.520
and you let the computer figure it out.

01:26:58.520 --> 01:27:00.400
I think that's what is extremely powerful.

01:27:00.400 --> 01:27:03.360
I wanna riff a little bit on what kinds of solutions

01:27:03.360 --> 01:27:04.200
might be possible here

01:27:04.200 --> 01:27:06.400
and which you would consider sort of defeating

01:27:06.400 --> 01:27:09.320
the purpose of arc and which are sort of valid.

01:27:10.600 --> 01:27:14.000
Here's one I'll mention which is my friends

01:27:14.000 --> 01:27:16.480
that Ryan and Buck stayed up last night

01:27:16.480 --> 01:27:17.840
because I told them about this

01:27:17.840 --> 01:27:19.240
and they were like, oh, of course,

01:27:19.240 --> 01:27:20.080
I was gonna solve this.

01:27:20.080 --> 01:27:20.920
Thank you for spreading the word.

01:27:20.920 --> 01:27:21.760
Of course, I was gonna solve this.

01:27:21.760 --> 01:27:23.560
And then so they were trying to prompt,

01:27:23.560 --> 01:27:25.400
I think Claude, Opus on this

01:27:25.400 --> 01:27:29.640
and they say they got 25% on the public arc test.

01:27:30.520 --> 01:27:33.320
And what they did was have other examples

01:27:33.320 --> 01:27:34.760
of some of the arc tests

01:27:34.760 --> 01:27:36.760
and in context explain the reasoning

01:27:36.760 --> 01:27:39.520
of why you went from one output to another output

01:27:39.520 --> 01:27:41.400
and then now you have the current problem.

01:27:41.400 --> 01:27:44.960
And I think also maybe expressing the JSON in a way

01:27:44.960 --> 01:27:48.240
that is more amenable to the tokenizer.

01:27:48.240 --> 01:27:51.880
And another thing was using the code interpreter.

01:27:51.880 --> 01:27:54.080
So I'm curious actually,

01:27:54.080 --> 01:27:55.800
if you think the code interpreter,

01:27:55.800 --> 01:27:58.320
which keeps getting better as these models get smarter

01:27:58.320 --> 01:28:00.640
is just the program synthesis right there

01:28:00.640 --> 01:28:02.200
because what they were able to do

01:28:02.200 --> 01:28:06.280
was the actual output of the cells, the JSON output,

01:28:06.280 --> 01:28:08.640
they got through the code interpreter,

01:28:08.640 --> 01:28:10.880
like write the Python program that gets right up here.

01:28:10.880 --> 01:28:13.240
Do you think that the program synthesis

01:28:13.240 --> 01:28:14.440
kind of researcher talking about

01:28:14.440 --> 01:28:16.800
will look like just using the code interpreter

01:28:16.800 --> 01:28:17.760
in large language models?

01:28:17.760 --> 01:28:20.320
I think whatever solution we see that will score well

01:28:20.320 --> 01:28:24.560
is gonna probably need to leverage some aspects

01:28:24.560 --> 01:28:27.040
from deep learning models and LLMs in particular.

01:28:27.040 --> 01:28:30.040
We've shown already that LLMs can do quite well,

01:28:30.040 --> 01:28:32.400
that's basically the jack code approach.

01:28:32.400 --> 01:28:35.080
We've also shown that pure discrete program search

01:28:35.080 --> 01:28:37.480
from a small DSL does very, very well

01:28:37.480 --> 01:28:39.120
before jack code, this was the state of the art.

01:28:39.120 --> 01:28:41.320
In fact, it's still extremely close to the state of the art.

01:28:41.440 --> 01:28:44.160
And there's no deep learning involved at all in these models.

01:28:44.160 --> 01:28:48.240
So we have two approaches that have basically no overlap

01:28:48.240 --> 01:28:49.280
that are doing quite well.

01:28:49.280 --> 01:28:53.640
And they're very much at two opposite ends of one spectrum,

01:28:53.640 --> 01:28:56.840
where on one end you have these extremely large banks

01:28:56.840 --> 01:28:58.800
of millions of vector programs,

01:28:58.800 --> 01:29:01.120
but very, very shallow recombination,

01:29:01.120 --> 01:29:02.720
like simplistic recombination.

01:29:02.720 --> 01:29:05.840
And on the other end, you have very simplistic DSLs,

01:29:05.840 --> 01:29:08.840
very simple, like 100 or 200 primitives,

01:29:08.840 --> 01:29:11.920
but very deep, very sophisticated program search.

01:29:12.840 --> 01:29:15.160
The solution is gonna be somewhere in between, right?

01:29:15.160 --> 01:29:19.720
So the people are gonna be winning the art competition

01:29:19.720 --> 01:29:21.840
and we're gonna be making the most progress

01:29:21.840 --> 01:29:23.960
towards near-term NGR are gonna be users

01:29:23.960 --> 01:29:27.000
that manage to merge the deep learning paradigm

01:29:27.000 --> 01:29:28.800
and the discrete program search paradigm

01:29:28.800 --> 01:29:31.560
into one elegant way.

01:29:31.560 --> 01:29:33.280
And you know, you ask like,

01:29:33.280 --> 01:29:36.560
what would be legitimate and what would be cheating,

01:29:36.560 --> 01:29:37.400
for instance?

01:29:37.840 --> 01:29:41.080
You wanna add a code interpreter to the system.

01:29:41.080 --> 01:29:42.760
I think that's great, that's sort of legitimate.

01:29:42.760 --> 01:29:45.080
The part that would be cheating is try to

01:29:47.000 --> 01:29:49.240
anticipate what might be in the test set,

01:29:49.240 --> 01:29:52.320
like brute force the space of possible tasks

01:29:52.320 --> 01:29:55.160
and then train a memorization system on it.

01:29:55.160 --> 01:29:57.720
And then rely on the fact that you're generating

01:29:57.720 --> 01:29:59.880
so many tasks, like millions and millions and millions,

01:29:59.880 --> 01:30:02.160
that inevitably there's gonna be some overlap

01:30:02.160 --> 01:30:04.720
between what you're generating and what's in the test set.

01:30:04.760 --> 01:30:07.960
I think that's defeating the purpose of benchmark

01:30:07.960 --> 01:30:09.840
because then you can just solve it with that

01:30:09.840 --> 01:30:13.280
and you need to adapt just by fetching a memorized solution.

01:30:13.280 --> 01:30:15.640
So hopefully arc will resist to that,

01:30:15.640 --> 01:30:18.160
but you know, nothing, no benchmark is necessarily perfect.

01:30:18.160 --> 01:30:20.160
So maybe there's a way to hack it

01:30:20.160 --> 01:30:22.200
and I guess we are gonna get an answer very soon.

01:30:22.200 --> 01:30:24.120
Although I think some amount of fine tuning is valid

01:30:24.120 --> 01:30:27.520
because these models don't natively think in terms of,

01:30:27.520 --> 01:30:28.840
especially the language models alone,

01:30:28.840 --> 01:30:31.080
which the open source models that they would have to use

01:30:31.080 --> 01:30:33.200
to be competitive here compete here.

01:30:33.800 --> 01:30:34.920
They're like natively language,

01:30:34.920 --> 01:30:38.080
so they need to be able to think in this kind of...

01:30:38.080 --> 01:30:38.920
Yes.

01:30:38.920 --> 01:30:39.760
The arc type way.

01:30:39.760 --> 01:30:41.480
You want to input corner ledge,

01:30:41.480 --> 01:30:44.440
like arc like corner ledge into the model,

01:30:44.440 --> 01:30:47.280
but surely you don't need tens of millions of tasks

01:30:47.280 --> 01:30:50.080
to do this, like corner ledge is extremely basic.

01:30:50.080 --> 01:30:52.960
If you look at some of these arc type questions,

01:30:54.360 --> 01:30:56.480
I actually do think they rely a little bit

01:30:56.480 --> 01:30:59.640
on things I have seen throughout my life.

01:30:59.640 --> 01:31:02.800
And for the same, like for example,

01:31:02.800 --> 01:31:04.640
like something bounces off a wall

01:31:04.640 --> 01:31:06.160
and comes back and you see that pattern.

01:31:06.160 --> 01:31:07.480
It's like I played arcade games

01:31:07.480 --> 01:31:09.600
and I've seen like pong or something.

01:31:09.600 --> 01:31:11.440
And I think for example, when you see the Flynn effect

01:31:11.440 --> 01:31:14.120
and people's intelligence has measured on

01:31:14.120 --> 01:31:15.560
very advanced progressive matrices

01:31:15.560 --> 01:31:17.360
increasing on these kinds of questions,

01:31:17.360 --> 01:31:19.360
it's probably a simpler story where since now,

01:31:19.360 --> 01:31:21.160
since childhood, we actually see these sorts of patterns

01:31:21.160 --> 01:31:23.400
in TV and whatever, spatial patterns.

01:31:23.400 --> 01:31:26.200
And so I don't think this is sort of core knowledge.

01:31:26.200 --> 01:31:29.240
I think actually this is also part of the quote unquote

01:31:29.240 --> 01:31:31.720
trying tuning that humans have as they grow up

01:31:31.720 --> 01:31:34.080
of seeing different kinds of spatial patterns

01:31:34.080 --> 01:31:35.480
and trying to pattern match to them.

01:31:35.480 --> 01:31:37.800
I would definitely file that under core knowledge.

01:31:37.800 --> 01:31:40.600
Like core knowledge includes basic physics,

01:31:40.600 --> 01:31:43.160
for instance, bouncing or trajectories,

01:31:43.160 --> 01:31:44.400
that would be included.

01:31:44.400 --> 01:31:45.800
But yeah, I think you're entirely right.

01:31:45.800 --> 01:31:47.160
The reason why as a human,

01:31:47.160 --> 01:31:49.040
you're able to quickly figure out the solution

01:31:49.040 --> 01:31:51.840
is because you have this set of building blocks,

01:31:51.840 --> 01:31:54.160
this set of patterns in your mind that you can recombine.

01:31:54.160 --> 01:31:57.240
Is core knowledge required to attain intelligence?

01:31:57.240 --> 01:31:58.680
Any algorithm you have,

01:31:58.760 --> 01:32:00.760
does the core knowledge have to be in some sense hard coded

01:32:00.760 --> 01:32:03.760
or can even the core knowledge be learned through intelligence?

01:32:03.760 --> 01:32:05.040
Core knowledge can be learned.

01:32:05.040 --> 01:32:07.200
And I think in the case of humans,

01:32:07.200 --> 01:32:09.600
some amount of core knowledge is something

01:32:09.600 --> 01:32:10.640
that you're born with.

01:32:10.640 --> 01:32:13.640
Like we're actually born with a small amount of knowledge

01:32:13.640 --> 01:32:15.560
about the world we're gonna live in.

01:32:15.560 --> 01:32:17.080
We're not blank slates.

01:32:17.080 --> 01:32:20.640
But most core knowledge is acquired through experience.

01:32:20.640 --> 01:32:22.000
But the thing with core knowledge

01:32:22.000 --> 01:32:25.320
that it's not gonna be acquired like for instance in school,

01:32:25.320 --> 01:32:27.520
it's actually acquired very, very early

01:32:27.520 --> 01:32:30.400
in the first like three to four years of your life.

01:32:30.400 --> 01:32:31.440
And by age four,

01:32:31.440 --> 01:32:34.560
you have all the core knowledge you're gonna need as an adult.

01:32:34.560 --> 01:32:36.040
Okay, interesting.

01:32:36.040 --> 01:32:37.800
So I mean, on the price itself,

01:32:37.800 --> 01:32:40.960
I'm super excited to see both the open source versions

01:32:40.960 --> 01:32:44.120
of maybe with a Lama 70B or something

01:32:44.120 --> 01:32:46.520
what people can score in the competition itself.

01:32:46.520 --> 01:32:50.520
Then if to sort of test specifically

01:32:50.520 --> 01:32:51.600
the scaling hypothesis,

01:32:51.600 --> 01:32:53.560
I'm very curious to see if you can prompt

01:32:53.560 --> 01:32:55.120
on the public version of ARC,

01:32:55.120 --> 01:32:56.080
which I guess when we compare,

01:32:56.080 --> 01:32:58.640
you will be able to submit to this competition itself.

01:32:58.640 --> 01:33:00.080
But I'd be very curious to see how,

01:33:00.080 --> 01:33:02.800
if people can sort of crack that and get ARC working there

01:33:02.800 --> 01:33:04.640
and if that would update your reviews on AGI.

01:33:04.640 --> 01:33:05.640
It's gonna be motivating.

01:33:05.640 --> 01:33:06.760
We're gonna keep running the contest

01:33:06.760 --> 01:33:09.040
until somebody puts a reproducible open source version

01:33:09.040 --> 01:33:09.880
into public domain.

01:33:09.880 --> 01:33:13.760
So even if somebody privately beats the ARC eval,

01:33:13.760 --> 01:33:14.880
we're gonna still keep the price money

01:33:14.880 --> 01:33:16.080
until someone can reproduce it

01:33:16.080 --> 01:33:18.560
and put the public reproducible version out there.

01:33:18.560 --> 01:33:19.400
Yeah, exactly.

01:33:19.400 --> 01:33:22.280
Like the goal is to accelerate progress towards AGI.

01:33:22.280 --> 01:33:24.280
And a key part of that is that

01:33:24.280 --> 01:33:26.160
any sort of meaningful bits of progress

01:33:26.160 --> 01:33:28.800
needs to be shared, needs to be public.

01:33:28.800 --> 01:33:30.440
So everyone can know about it

01:33:30.440 --> 01:33:32.160
and can try to iterate on it.

01:33:32.160 --> 01:33:33.720
If there's no sharing, there's no progress.

01:33:33.720 --> 01:33:35.040
What I'm especially curious about

01:33:35.040 --> 01:33:36.640
is sort of disaggregating the bets

01:33:36.640 --> 01:33:39.840
of like, can we make an open version of this

01:33:39.840 --> 01:33:43.320
versus is this a thing that's just possible with scaling?

01:33:43.320 --> 01:33:45.680
And we can, I guess test both of them

01:33:45.680 --> 01:33:47.840
based on the public and the private version.

01:33:47.840 --> 01:33:50.680
We're making contact with reality as well with this, right?

01:33:50.680 --> 01:33:51.520
We're gonna learn a lot, I think,

01:33:51.520 --> 01:33:52.920
about what the actual limits of the compute

01:33:52.920 --> 01:33:53.920
where if someone showed up and said,

01:33:53.920 --> 01:33:55.120
hey, here's a closed source model

01:33:55.120 --> 01:33:57.240
that like I'm getting 50 plus percent on,

01:33:57.240 --> 01:33:58.720
I think that would probably update us on like,

01:33:58.720 --> 01:34:00.400
okay, perhaps we should increase the amount of compute

01:34:00.400 --> 01:34:01.600
that we give on the private test set

01:34:01.600 --> 01:34:03.760
in order to balance some of the decisions

01:34:03.760 --> 01:34:04.960
that initially are somewhat arbitrary

01:34:04.960 --> 01:34:07.240
in order to learn about, okay, what do people want?

01:34:07.240 --> 01:34:08.240
What does progress look like?

01:34:08.240 --> 01:34:10.480
And I think both of us are sort of committed

01:34:10.480 --> 01:34:12.640
to evolving it over time in order to be the best,

01:34:12.640 --> 01:34:14.360
or the closest to perfect as we can get it.

01:34:14.360 --> 01:34:15.840
Awesome, and where can people go to learn more

01:34:15.840 --> 01:34:18.280
about the prize and maybe give their hand at it?

01:34:18.280 --> 01:34:19.320
ARCPrize.org.

01:34:19.320 --> 01:34:21.000
Which goes live today, so.

01:34:21.000 --> 01:34:21.840
It's live now.

01:34:21.880 --> 01:34:23.640
$70 million is on this line, people.

01:34:23.640 --> 01:34:24.480
Good luck.

01:34:24.480 --> 01:34:25.320
Thank you guys for coming on the podcast.

01:34:25.320 --> 01:34:26.840
It was super fun to go through all the cruxes

01:34:26.840 --> 01:34:28.920
on intelligence and get a different perspective,

01:34:28.920 --> 01:34:30.880
and also to announce a prize here.

01:34:30.880 --> 01:34:31.800
So this is awesome.

01:34:31.800 --> 01:34:32.960
Thank you for helping break news.

01:34:32.960 --> 01:34:33.960
Thank you, Finest.

