1
00:00:00,000 --> 00:00:01,440
That's not even a question for me,

2
00:00:01,440 --> 00:00:04,080
whether we're going to go take a swing at building the next thing.

3
00:00:04,080 --> 00:00:06,640
I'm just incapable of not doing that.

4
00:00:06,640 --> 00:00:09,840
There's a bunch of times when we wanted to launch features

5
00:00:09,840 --> 00:00:12,480
and then Apple's just like, nope, you're not launching that.

6
00:00:12,480 --> 00:00:13,360
I was like, that sucks.

7
00:00:14,160 --> 00:00:18,960
Are we set up for that with AI, where you're going to get a handful of companies

8
00:00:18,960 --> 00:00:22,240
that run these closed models that are going to be in control of the APIs

9
00:00:22,240 --> 00:00:24,480
and therefore are going to be able to tell you what you can build?

10
00:00:24,480 --> 00:00:29,360
Then when you start getting into building a data center that's like 300 megawatts

11
00:00:29,360 --> 00:00:31,920
or 500 megawatts or a gigawatt,

12
00:00:31,920 --> 00:00:34,320
just no one has built a single gigawatt data center yet.

13
00:00:34,320 --> 00:00:37,200
But from wherever you sit, there's going to be some actor who you don't trust.

14
00:00:37,200 --> 00:00:39,600
If they're the ones who have like the super strong AI,

15
00:00:39,600 --> 00:00:43,040
I think that that's potentially a much bigger risk.

16
00:00:43,920 --> 00:00:45,280
Mark, welcome to the podcast.

17
00:00:45,280 --> 00:00:47,200
Hey, thanks for having me, big fan of your podcast.

18
00:00:47,200 --> 00:00:48,560
Oh, thank you. That's very nice of you to say.

19
00:00:49,920 --> 00:00:54,000
Okay, so let's start by talking about the releases that will go out

20
00:00:54,000 --> 00:00:55,040
when this interview goes out.

21
00:00:55,920 --> 00:00:57,920
Tell me about the models, tell me about meta AI,

22
00:00:57,920 --> 00:00:59,440
what's new, what's exciting about them?

23
00:00:59,440 --> 00:01:02,640
Yeah, sure. I think the main thing that most people in the world

24
00:01:02,640 --> 00:01:04,240
are going to see is the new version of meta AI.

25
00:01:06,640 --> 00:01:10,400
The most important thing about what we're doing is the upgrade to the model.

26
00:01:10,400 --> 00:01:11,600
We're rolling out Lama 3.

27
00:01:11,600 --> 00:01:14,960
We're doing it both as open source for the dev community,

28
00:01:14,960 --> 00:01:17,120
and it is now going to be powering meta AI.

29
00:01:18,880 --> 00:01:20,880
There's a lot that I'm sure we'll go into around Lama 3,

30
00:01:20,880 --> 00:01:24,160
but I think the bottom line on this is that with Lama 3,

31
00:01:24,160 --> 00:01:28,080
we now think that meta AI is the most intelligent AI assistant

32
00:01:28,080 --> 00:01:30,000
that people can use that's freely available.

33
00:01:30,720 --> 00:01:33,760
We're also integrating Google and Bing for real-time knowledge.

34
00:01:34,480 --> 00:01:37,200
We're going to make it a lot more prominent across our apps.

35
00:01:37,200 --> 00:01:40,960
So basically, at the top of WhatsApp and Instagram

36
00:01:40,960 --> 00:01:45,680
and Facebook and Messenger, you'll just be able to use the search box

37
00:01:45,680 --> 00:01:47,280
right there to ask it any question.

38
00:01:48,080 --> 00:01:50,720
And there's a bunch of new creation features that we added

39
00:01:50,720 --> 00:01:52,720
that I think are pretty cool that I think people enjoy.

40
00:01:54,400 --> 00:01:56,480
And I think animations is a good one.

41
00:01:57,200 --> 00:01:59,360
You can basically just take any image and animate it.

42
00:01:59,360 --> 00:02:03,520
But I think one that people are going to find pretty wild is

43
00:02:04,400 --> 00:02:07,680
it now generates high-quality images so quickly.

44
00:02:07,680 --> 00:02:09,360
I don't know if you've gotten a chance to play with this,

45
00:02:09,360 --> 00:02:12,880
that it actually generates it as you're typing and updates it in real-time.

46
00:02:12,880 --> 00:02:16,880
So you're typing your query and it's honing in on...

47
00:02:16,880 --> 00:02:22,480
And it's like, okay, here, show me a picture of a cow in a field

48
00:02:22,560 --> 00:02:25,920
with mountains in the background and just like eating macadamia nuts,

49
00:02:25,920 --> 00:02:30,080
drinking beer and it's updating the image in real-time.

50
00:02:30,800 --> 00:02:31,520
It's pretty wild.

51
00:02:31,520 --> 00:02:32,720
I think people are going to enjoy that.

52
00:02:33,920 --> 00:02:37,200
So yeah, that's what most people are going to see in the world.

53
00:02:37,200 --> 00:02:39,360
We're rolling that out, not everywhere,

54
00:02:39,360 --> 00:02:41,760
but we're starting in a handful of countries

55
00:02:41,760 --> 00:02:44,080
and we'll do more over the coming weeks and months.

56
00:02:45,520 --> 00:02:47,760
So that I think is going to be a pretty big deal.

57
00:02:48,880 --> 00:02:50,560
And I'm really excited to get that in people's hands.

58
00:02:51,040 --> 00:02:53,680
It's a big step forward for Met AI.

59
00:02:55,440 --> 00:02:57,680
But I think if you want to get under the hood a bit,

60
00:02:58,480 --> 00:03:01,520
the Llama 3 stuff is obviously the most technically interesting.

61
00:03:01,520 --> 00:03:04,240
So we're basically, for the first version,

62
00:03:04,240 --> 00:03:08,720
we're training three versions, an 8 billion and a 70 billion,

63
00:03:08,720 --> 00:03:13,360
which we're releasing today and a 405 billion dense model,

64
00:03:13,360 --> 00:03:16,080
which is still training, so we're not releasing that today.

65
00:03:16,480 --> 00:03:23,040
But the 8 and 70, I mean, I'm pretty excited about how they turned out.

66
00:03:23,040 --> 00:03:26,800
I mean, they're leading for their scale.

67
00:03:28,800 --> 00:03:32,880
You know, it's, I mean, we'll release a blog post with all the benchmarks

68
00:03:32,880 --> 00:03:35,200
so people can check it out themselves and obviously it's open source

69
00:03:35,200 --> 00:03:36,720
so people get a chance to play with it.

70
00:03:37,840 --> 00:03:40,080
We have a roadmap of new releases coming

71
00:03:40,960 --> 00:03:44,640
that are going to bring multi-modality, more multi-linguality,

72
00:03:45,600 --> 00:03:47,520
bigger context windows to those as well.

73
00:03:48,720 --> 00:03:51,520
And then, you know, hopefully sometime later in the year,

74
00:03:51,520 --> 00:03:56,320
we'll get to roll out the 405, which I think is, in training,

75
00:03:56,320 --> 00:04:01,040
it's still training, but for where it is right now in training,

76
00:04:01,040 --> 00:04:08,720
it is already at around 85 mmlu and just,

77
00:04:08,720 --> 00:04:10,640
we expect that it's going to have leading benchmarks

78
00:04:10,640 --> 00:04:12,800
on a bunch of the benchmarks.

79
00:04:12,800 --> 00:04:14,640
So, I'm pretty excited about all of that.

80
00:04:14,640 --> 00:04:18,480
I mean, the 70 billion is great too.

81
00:04:18,480 --> 00:04:19,440
I mean, we're releasing that today.

82
00:04:19,440 --> 00:04:23,200
It's around 82 mmlu and has leading scores on math and reasoning.

83
00:04:23,200 --> 00:04:25,280
So, I mean, it's, I think just getting this in people's hands

84
00:04:25,280 --> 00:04:26,480
is going to be pretty wild.

85
00:04:26,480 --> 00:04:27,040
Oh, interesting.

86
00:04:27,040 --> 00:04:28,240
Yeah, that's the first time hearing this benchmark.

87
00:04:28,240 --> 00:04:28,960
That's super impressive.

88
00:04:28,960 --> 00:04:35,840
Yeah, and the 8 billion is nearly as powerful

89
00:04:35,840 --> 00:04:38,240
as the biggest version of Llama 2 that we released.

90
00:04:38,240 --> 00:04:41,120
So, it's like the smallest Llama 3 is basically as powerful

91
00:04:41,200 --> 00:04:43,360
as the biggest Llama 2.

92
00:04:43,360 --> 00:04:45,680
Okay, so before we dig into these models,

93
00:04:45,680 --> 00:04:47,280
I actually want to go back in time.

94
00:04:47,840 --> 00:04:51,520
2022 is, I'm assuming, when you started acquiring these H100s,

95
00:04:52,560 --> 00:04:54,960
or you can tell me when, where you're like,

96
00:04:54,960 --> 00:04:56,400
stock price is getting hammered.

97
00:04:56,400 --> 00:04:58,400
People are like, what's happening with all this capex?

98
00:04:58,400 --> 00:04:59,920
People aren't buying the metaverse.

99
00:04:59,920 --> 00:05:01,360
And presumably, you're spending that capex

100
00:05:01,360 --> 00:05:02,720
to get these H100s.

101
00:05:02,720 --> 00:05:05,040
How, back then, how did you know to get the H100s?

102
00:05:05,040 --> 00:05:06,560
How did you know we'll need the GPUs?

103
00:05:07,760 --> 00:05:10,160
I think it was because we were working on Reels.

104
00:05:10,160 --> 00:05:15,440
So, we got into this situation where we always

105
00:05:16,160 --> 00:05:19,520
want to have enough capacity to build something

106
00:05:19,520 --> 00:05:23,280
that we can't quite see that were on the horizon yet.

107
00:05:24,160 --> 00:05:26,640
And we got into this position with Reels,

108
00:05:26,640 --> 00:05:30,800
where we needed more GPUs to train the models.

109
00:05:30,800 --> 00:05:33,920
It was this big evolution for our services,

110
00:05:33,920 --> 00:05:35,760
where instead of just ranking content from people

111
00:05:35,760 --> 00:05:39,200
who you follow, or your friends, and whatever pages you follow,

112
00:05:41,040 --> 00:05:45,040
we made this big push to basically start recommending

113
00:05:45,600 --> 00:05:47,280
what we call unconnected content,

114
00:05:47,280 --> 00:05:49,520
to basically connect content from people

115
00:05:49,520 --> 00:05:50,800
or pages that you're not following.

116
00:05:50,800 --> 00:05:55,760
So, now, kind of the corpus of kind of content candidates

117
00:05:55,760 --> 00:05:57,840
that we could potentially show you expanded from,

118
00:05:57,840 --> 00:05:59,680
you know, on the order of thousands

119
00:05:59,680 --> 00:06:02,480
to on the order of hundreds of millions.

120
00:06:02,480 --> 00:06:04,240
So, completely different infrastructure.

121
00:06:04,960 --> 00:06:08,720
And we started working on doing that,

122
00:06:08,720 --> 00:06:12,960
and we were constrained on basically the infrastructure

123
00:06:12,960 --> 00:06:15,600
that we had to catch up to what TikTok was doing

124
00:06:15,600 --> 00:06:16,880
as quickly as we would have wanted to.

125
00:06:17,760 --> 00:06:19,040
So, I basically looked at that, and I was like,

126
00:06:19,040 --> 00:06:22,560
hey, we have to make sure that we're never in this situation again.

127
00:06:22,560 --> 00:06:26,320
So, let's order enough GPUs to do what we need to do

128
00:06:26,320 --> 00:06:28,640
on Reels and ranking content and feed,

129
00:06:28,640 --> 00:06:30,480
but let's also double that, right?

130
00:06:30,480 --> 00:06:32,800
Because, again, like our normal principle is,

131
00:06:32,800 --> 00:06:34,240
there's going to be something on the horizon

132
00:06:34,240 --> 00:06:35,040
that we can't see yet.

133
00:06:35,040 --> 00:06:35,840
Did you know it would be AI?

134
00:06:36,400 --> 00:06:38,480
Well, we thought it would be,

135
00:06:39,440 --> 00:06:40,320
we thought it was going to be something

136
00:06:40,320 --> 00:06:42,400
that I had to do with training large models, right?

137
00:06:42,400 --> 00:06:44,000
I mean, but at the time, I thought it was probably

138
00:06:44,000 --> 00:06:46,080
going to be more something that I had to do with content.

139
00:06:46,080 --> 00:06:46,800
But I don't know.

140
00:06:46,800 --> 00:06:49,520
I mean, it's almost just the pattern matching

141
00:06:49,520 --> 00:06:53,680
and running the company is there's always another thing, right?

142
00:06:53,680 --> 00:06:55,920
So, I'm not even sure I had, at that time,

143
00:06:55,920 --> 00:06:58,480
I was so deep in just, you know, trying to get,

144
00:06:58,480 --> 00:07:00,800
you know, the recommendations working for Reels

145
00:07:00,800 --> 00:07:02,560
and other content, because I mean,

146
00:07:02,560 --> 00:07:04,960
that's just such a big unlock for Instagram and Facebook

147
00:07:04,960 --> 00:07:06,720
to now being able to show people content

148
00:07:06,720 --> 00:07:08,240
that's interesting to them that they're from people

149
00:07:08,240 --> 00:07:09,840
that they're not even following.

150
00:07:09,840 --> 00:07:15,280
But, yeah, that ended up being a very good decision

151
00:07:15,280 --> 00:07:16,000
in retrospect.

152
00:07:16,000 --> 00:07:16,960
Yeah, yeah.

153
00:07:16,960 --> 00:07:18,320
Okay, and it came from being behind.

154
00:07:18,320 --> 00:07:20,640
So, then it wasn't like I was, you know,

155
00:07:20,640 --> 00:07:22,160
it wasn't like, oh, I was so far ahead.

156
00:07:22,160 --> 00:07:23,440
Actually, most of the times, I think,

157
00:07:23,440 --> 00:07:25,680
where we kind of make some decision

158
00:07:25,680 --> 00:07:28,800
that ends up seeming good is because we messed something up

159
00:07:28,800 --> 00:07:30,640
before and just didn't want to repeat the mistake.

160
00:07:31,280 --> 00:07:32,160
This is a total detour,

161
00:07:32,160 --> 00:07:33,840
but actually, I want to ask about this while we're on this.

162
00:07:33,840 --> 00:07:35,680
We'll get back to AI in a second.

163
00:07:36,480 --> 00:07:38,240
So, you didn't suffer one billion,

164
00:07:38,240 --> 00:07:39,520
but presumably there's some amount

165
00:07:39,520 --> 00:07:40,720
you would have sold for, right?

166
00:07:40,720 --> 00:07:42,480
Did you write down in your head, like,

167
00:07:42,480 --> 00:07:45,120
I think the actual valuation of Facebook at the time is this

168
00:07:45,120 --> 00:07:46,960
and they're not actually getting the valuation right?

169
00:07:46,960 --> 00:07:49,040
Like, the average $5 trillion, of course, you would have sold.

170
00:07:49,040 --> 00:07:52,000
So, like, how did you think about that choice?

171
00:07:52,720 --> 00:07:53,520
Yeah, I don't know.

172
00:07:53,520 --> 00:07:56,080
I mean, look, I think some of these things are just personal.

173
00:07:58,160 --> 00:08:01,040
I don't know at the time that I was sophisticated enough

174
00:08:01,040 --> 00:08:02,080
to do that analysis,

175
00:08:02,080 --> 00:08:05,280
but I had all these people around me who were making

176
00:08:05,280 --> 00:08:09,120
all these arguments for how, like, a billion dollars was,

177
00:08:09,120 --> 00:08:11,200
you know, it's like, here's the revenue that we need to make

178
00:08:11,200 --> 00:08:12,640
and here's how big we need to be

179
00:08:12,640 --> 00:08:14,640
and, like, it's clearly so many years in the future.

180
00:08:14,640 --> 00:08:16,560
Like, and it was, it was very far ahead

181
00:08:16,560 --> 00:08:18,080
of where we were at the time.

182
00:08:18,080 --> 00:08:23,040
And I don't know, I didn't really have the financial sophistication

183
00:08:23,040 --> 00:08:26,240
to really even engage with that kind of debate.

184
00:08:26,240 --> 00:08:29,520
I just, I think I sort of deep down believed

185
00:08:29,520 --> 00:08:30,640
in what we were doing.

186
00:08:30,640 --> 00:08:32,000
And I did some analysis.

187
00:08:33,840 --> 00:08:38,640
I was like, okay, well, what would I go do if I wasn't doing this?

188
00:08:38,640 --> 00:08:41,360
It's like, well, I really like building things

189
00:08:41,360 --> 00:08:43,120
and I like helping people communicate

190
00:08:43,120 --> 00:08:46,560
and I like understanding what's going on with people

191
00:08:46,560 --> 00:08:48,000
and the dynamics between people.

192
00:08:48,000 --> 00:08:50,080
So, I think if I sold this company,

193
00:08:50,080 --> 00:08:51,920
I'd just go build another company like this.

194
00:08:51,920 --> 00:08:54,480
And I kind of like the one I have.

195
00:08:54,480 --> 00:08:58,560
So, so, I mean, you know, what's, why, why, right?

196
00:08:58,560 --> 00:09:00,960
But I don't know.

197
00:09:00,960 --> 00:09:04,080
I think a lot of the biggest bets that people make

198
00:09:05,520 --> 00:09:08,240
are often just based on conviction and values.

199
00:09:09,200 --> 00:09:13,200
Not, it's actually usually very hard to do the analyses

200
00:09:13,200 --> 00:09:14,560
trying to connect the dots forward.

201
00:09:14,560 --> 00:09:15,120
Yeah.

202
00:09:15,120 --> 00:09:18,160
So, you've had Facebook AI research for a long time.

203
00:09:19,280 --> 00:09:21,760
Now it's become seemingly central to your company.

204
00:09:23,120 --> 00:09:27,040
At what point did making AGI or whatever,

205
00:09:27,040 --> 00:09:28,640
however you consider that mission,

206
00:09:28,640 --> 00:09:29,440
at what point is that like,

207
00:09:29,440 --> 00:09:31,520
this is a Cree priority of what Meta is doing?

208
00:09:32,800 --> 00:09:32,960
Yeah.

209
00:09:32,960 --> 00:09:34,960
I mean, it's been a big deal for a while.

210
00:09:34,960 --> 00:09:38,720
So, we started fair about 10 years ago.

211
00:09:38,720 --> 00:09:44,320
And the idea was that along the way to general intelligence

212
00:09:44,320 --> 00:09:46,640
or AI, like full AI, whatever you want to call it,

213
00:09:47,360 --> 00:09:49,280
there can be all these different innovations

214
00:09:49,280 --> 00:09:51,680
and that's going to just improve everything that we do.

215
00:09:51,680 --> 00:09:55,440
So, we didn't kind of conceive it as a product.

216
00:09:55,440 --> 00:09:57,520
It was more kind of a research group.

217
00:09:58,080 --> 00:10:01,280
And over the last 10 years,

218
00:10:01,280 --> 00:10:03,360
it has created a lot of different things

219
00:10:03,360 --> 00:10:06,320
that have basically improved all of our products

220
00:10:07,040 --> 00:10:09,840
and advanced the field and allowed other people in the field

221
00:10:09,840 --> 00:10:11,760
to create things that have improved our products too.

222
00:10:11,760 --> 00:10:13,040
So, I think that that's been great.

223
00:10:13,600 --> 00:10:18,400
But there's obviously a big change in the last few years

224
00:10:18,400 --> 00:10:20,560
when ChatGPT comes out,

225
00:10:21,200 --> 00:10:23,520
the diffusion models or an image creation come out.

226
00:10:24,000 --> 00:10:25,760
I mean, this is some pretty wild stuff

227
00:10:25,760 --> 00:10:27,920
that I think is pretty clearly going to affect

228
00:10:27,920 --> 00:10:32,320
how people interact with every app that's out there.

229
00:10:34,400 --> 00:10:39,600
At that point, we started a second group, the GenAI group,

230
00:10:40,480 --> 00:10:44,400
with the goal of basically bringing that stuff into our products,

231
00:10:44,400 --> 00:10:46,480
so building leading foundation models

232
00:10:46,480 --> 00:10:48,880
that would sort of power all these different products.

233
00:10:49,440 --> 00:10:51,920
And initially, when we started doing that,

234
00:10:53,760 --> 00:10:55,120
the theory at first was, hey,

235
00:10:55,680 --> 00:10:58,560
a lot of the stuff that we're doing is pretty social, right?

236
00:10:58,560 --> 00:11:01,920
So, it's helping people interact with creators,

237
00:11:01,920 --> 00:11:04,880
helping people interact with businesses.

238
00:11:04,880 --> 00:11:08,160
So, the businesses can sell things or do customer support

239
00:11:08,160 --> 00:11:11,200
or basic assistant functionality for,

240
00:11:12,160 --> 00:11:14,880
you know, whether it's for our apps or the smart glasses

241
00:11:14,880 --> 00:11:16,960
or VR or like all these different things.

242
00:11:17,600 --> 00:11:21,600
So, initially, it wasn't completely clear

243
00:11:21,600 --> 00:11:25,280
that you were going to need kind of full AGI

244
00:11:26,080 --> 00:11:27,680
to be able to support those use cases.

245
00:11:27,680 --> 00:11:29,520
But then through working on them,

246
00:11:29,520 --> 00:11:31,520
I think it's actually become clear that you do, right?

247
00:11:31,520 --> 00:11:32,480
In all these subtle ways.

248
00:11:32,480 --> 00:11:35,680
So, for example, for Llama 2, when we were working on it,

249
00:11:35,680 --> 00:11:37,200
we didn't prioritize coding.

250
00:11:37,200 --> 00:11:39,280
And the reason why we didn't prioritize coding

251
00:11:39,280 --> 00:11:41,520
is because people aren't going to ask MetaAI

252
00:11:41,520 --> 00:11:43,360
a lot of coding questions in WhatsApp.

253
00:11:43,360 --> 00:11:44,400
Now they will, right?

254
00:11:44,400 --> 00:11:44,880
Well, I don't know.

255
00:11:44,880 --> 00:11:46,960
I'm not sure that WhatsApp is like the UI

256
00:11:46,960 --> 00:11:48,960
that people are going to be doing a lot of coding questions.

257
00:11:48,960 --> 00:11:51,120
So, we're like, all right, look, in terms of the things that,

258
00:11:51,200 --> 00:11:52,880
you know, or Facebook or Instagram

259
00:11:52,880 --> 00:11:54,400
or, you know, those different services,

260
00:11:54,400 --> 00:11:58,160
maybe the website, right, meta.ai that we're launching, I think.

261
00:11:58,160 --> 00:12:01,440
But the thing that was sort of, I think,

262
00:12:01,440 --> 00:12:06,160
has been a somewhat surprising result over the last 18 months

263
00:12:06,160 --> 00:12:10,720
is that it turns out that coding is important

264
00:12:10,720 --> 00:12:12,480
for a lot of domains, not just coding, right?

265
00:12:12,480 --> 00:12:15,440
So, even if people aren't asking coding questions to the models,

266
00:12:16,160 --> 00:12:20,080
training the models on coding helps them just be more rigorous

267
00:12:20,080 --> 00:12:23,360
and answer the question and kind of help reason

268
00:12:23,360 --> 00:12:25,280
across a lot of different types of domains.

269
00:12:25,280 --> 00:12:26,400
Okay, so that's one example where it's like,

270
00:12:26,400 --> 00:12:27,360
all right, so for Llama 3,

271
00:12:27,360 --> 00:12:29,680
we're like really focused on training it with a lot of coding

272
00:12:29,680 --> 00:12:30,320
because it's like, all right,

273
00:12:30,320 --> 00:12:32,160
that's going to make it better on all these things,

274
00:12:32,160 --> 00:12:33,520
even if people aren't answering,

275
00:12:33,520 --> 00:12:35,280
aren't asking primarily coding questions.

276
00:12:36,080 --> 00:12:37,680
Reasoning, I think, is another example.

277
00:12:38,320 --> 00:12:41,600
It's like, okay, yeah, maybe you want to chat with a creator

278
00:12:41,600 --> 00:12:42,960
or, you know, you're a business

279
00:12:42,960 --> 00:12:45,200
and you're trying to interact with a customer.

280
00:12:45,200 --> 00:12:46,720
You know, that interaction is not just like,

281
00:12:46,720 --> 00:12:49,200
okay, the person sends you a message

282
00:12:49,200 --> 00:12:50,640
and you just reply, right?

283
00:12:50,640 --> 00:12:53,040
It's like a multi-step interaction

284
00:12:53,040 --> 00:12:54,560
where you're trying to think through

285
00:12:54,560 --> 00:12:56,400
how do I accomplish the person's goals

286
00:12:56,400 --> 00:12:58,720
and, you know, a lot of times when a customer comes,

287
00:12:58,720 --> 00:13:00,640
they don't necessarily know exactly

288
00:13:00,640 --> 00:13:02,640
what they're looking for or how to ask their questions.

289
00:13:02,640 --> 00:13:04,960
So, it's not really the job of the AI

290
00:13:04,960 --> 00:13:06,400
to just respond to the question.

291
00:13:06,400 --> 00:13:08,640
It's like, you need to kind of think about it more holistically.

292
00:13:08,640 --> 00:13:10,400
It really becomes a reasoning problem, right?

293
00:13:10,400 --> 00:13:11,520
So, if someone else, you know,

294
00:13:11,520 --> 00:13:14,160
solves reasoning or makes good advances on reasoning,

295
00:13:14,160 --> 00:13:17,040
and we're sitting here with a basic chat bot,

296
00:13:17,040 --> 00:13:18,480
then, like, our product is lame

297
00:13:18,480 --> 00:13:20,240
compared to what other people are building.

298
00:13:20,240 --> 00:13:21,520
So, it's like, it's okay.

299
00:13:21,520 --> 00:13:23,360
So, at the end of the day, we've got,

300
00:13:23,360 --> 00:13:25,520
we, you know, we basically realized

301
00:13:25,520 --> 00:13:27,200
we've got to solve general intelligence

302
00:13:28,320 --> 00:13:31,040
and we just kind of upped the ante and the investment

303
00:13:31,040 --> 00:13:32,480
to make sure that we could do that.

304
00:13:32,480 --> 00:13:38,480
So, the version of Lama that's going to solve

305
00:13:38,480 --> 00:13:40,960
all these use cases for users,

306
00:13:40,960 --> 00:13:43,040
is that the version that will be powerful enough

307
00:13:43,040 --> 00:13:45,840
to, like, replace a programmer you might have in this building?

308
00:13:46,720 --> 00:13:47,920
I mean, I just think that all this stuff

309
00:13:47,920 --> 00:13:49,360
is going to be progressive over time.

310
00:13:49,360 --> 00:13:50,960
But, in case, Lama 10.

311
00:13:53,920 --> 00:13:56,400
I mean, I think that there's a lot baked into that question.

312
00:13:56,400 --> 00:13:58,800
I'm not sure that we're replacing people

313
00:13:58,800 --> 00:14:02,080
as much as giving people tools to do more stuff.

314
00:14:02,080 --> 00:14:03,920
Is a programmer in this building 10x more productive

315
00:14:03,920 --> 00:14:04,320
after Lama 10?

316
00:14:04,320 --> 00:14:05,120
I would have more.

317
00:14:05,120 --> 00:14:08,320
But no, I mean, look, I'm not,

318
00:14:08,320 --> 00:14:09,520
I don't believe that there's, like,

319
00:14:09,520 --> 00:14:12,560
a single threshold of intelligence for humanity,

320
00:14:12,560 --> 00:14:14,560
because, I mean, people have different skills.

321
00:14:14,560 --> 00:14:16,400
And at some point, I think that AI is going to be,

322
00:14:17,520 --> 00:14:21,280
is probably going to surpass people at most of those things,

323
00:14:21,280 --> 00:14:23,040
depending on how powerful the models are.

324
00:14:23,040 --> 00:14:26,960
But I think it's progressive.

325
00:14:26,960 --> 00:14:28,400
And I don't think AGI is one thing.

326
00:14:28,400 --> 00:14:30,960
I think it's, you're basically adding different capabilities.

327
00:14:30,960 --> 00:14:35,360
So, multimodality is kind of a key one that we're focused on now,

328
00:14:35,360 --> 00:14:38,000
initially with photos and images and text,

329
00:14:38,000 --> 00:14:39,360
but eventually with videos.

330
00:14:39,360 --> 00:14:41,280
And then, because we're so focused on the metaverse,

331
00:14:41,360 --> 00:14:43,440
kind of 3D type stuff is important.

332
00:14:45,040 --> 00:14:47,680
One modality that I'm pretty focused on that I haven't seen

333
00:14:47,680 --> 00:14:50,560
as many other people in the industry focus on this

334
00:14:50,560 --> 00:14:53,680
is sort of like emotional understanding.

335
00:14:53,680 --> 00:14:56,320
Like, I mean, so much of the human brain

336
00:14:56,320 --> 00:14:59,200
is just dedicated to understanding people

337
00:14:59,200 --> 00:15:02,240
and kind of like understanding your expressions and emotions.

338
00:15:02,240 --> 00:15:04,800
And I think that that's like its own whole modality, right?

339
00:15:04,800 --> 00:15:06,400
That, I mean, you could say, okay,

340
00:15:06,400 --> 00:15:07,840
maybe it's just video or image,

341
00:15:07,840 --> 00:15:10,880
but it's like clearly a very specialized version of those too.

342
00:15:10,880 --> 00:15:12,880
So, there's all these different capabilities

343
00:15:12,880 --> 00:15:17,680
that I think you wanna basically train the models to focus on,

344
00:15:17,680 --> 00:15:19,840
as well as getting a lot better at reasoning,

345
00:15:19,840 --> 00:15:21,040
getting a lot better at memory,

346
00:15:21,040 --> 00:15:23,040
which I think is kind of its own whole thing.

347
00:15:23,040 --> 00:15:24,480
It's, I mean, I don't think we're gonna be,

348
00:15:24,480 --> 00:15:26,720
you know, primarily shoving context

349
00:15:26,720 --> 00:15:31,120
or kind of things into a query context window in the future

350
00:15:31,120 --> 00:15:32,880
to ask more complicated questions.

351
00:15:32,880 --> 00:15:35,360
I think that there'll be kind of different stores of memory

352
00:15:35,360 --> 00:15:36,480
or different custom models

353
00:15:36,480 --> 00:15:39,760
that are maybe more personalized to people.

354
00:15:39,760 --> 00:15:41,440
But I don't know, I think that these are all

355
00:15:41,440 --> 00:15:42,640
just different capabilities.

356
00:15:42,640 --> 00:15:44,400
And then obviously making them big and small,

357
00:15:44,400 --> 00:15:46,800
we care about both because, you know, we wanna,

358
00:15:46,800 --> 00:15:49,200
you know, if you're running something like meta AI,

359
00:15:49,200 --> 00:15:52,480
then we have the ability to, that's pretty server-based,

360
00:15:52,480 --> 00:15:54,320
but we also want it running on smart glasses.

361
00:15:54,320 --> 00:15:56,720
And, you know, there's not a lot of space in smart glasses.

362
00:15:56,720 --> 00:16:00,080
So, you wanna have something that's very efficient for that.

363
00:16:00,080 --> 00:16:03,120
What is the use case that if you're doing tens of billions

364
00:16:03,120 --> 00:16:04,000
of dollars worth of inference,

365
00:16:04,000 --> 00:16:05,600
or even eventually hundreds of billions of dollars

366
00:16:05,600 --> 00:16:09,120
worth of inference, using intelligence in an industrial scale,

367
00:16:09,120 --> 00:16:10,240
what is the use case?

368
00:16:10,240 --> 00:16:11,360
Is it simulations?

369
00:16:11,360 --> 00:16:13,120
Is it the AIs that will be in the metaverse?

370
00:16:13,120 --> 00:16:15,520
What will we be using the data centers for?

371
00:16:19,120 --> 00:16:21,200
I mean, our bet is that it's gonna,

372
00:16:21,200 --> 00:16:23,440
this is basically gonna change all of the products, right?

373
00:16:23,440 --> 00:16:27,760
So, I think that there's gonna be a kind of meta AI

374
00:16:27,760 --> 00:16:29,440
general assistant product.

375
00:16:29,440 --> 00:16:32,480
And I think that that will shift from something

376
00:16:32,480 --> 00:16:34,080
that feels more like a chat bot

377
00:16:34,080 --> 00:16:35,600
where it's like you just ask a question

378
00:16:35,600 --> 00:16:37,440
and it kind of formulates an answer

379
00:16:37,440 --> 00:16:38,720
to things where you're increasingly

380
00:16:38,720 --> 00:16:40,320
giving it more complicated tasks

381
00:16:40,320 --> 00:16:41,600
and that goes away and does them.

382
00:16:42,400 --> 00:16:44,480
So, that's gonna take a lot of inference.

383
00:16:44,480 --> 00:16:46,240
It's gonna take a lot of compute in other ways too.

384
00:16:47,920 --> 00:16:50,880
Then I think that there's a big part of what we're gonna do

385
00:16:50,880 --> 00:16:56,560
that is like interacting with other agents for other people.

386
00:16:56,560 --> 00:16:58,400
So, whether it's businesses or creators,

387
00:17:00,160 --> 00:17:01,760
I guess a big part of my theory on this

388
00:17:01,760 --> 00:17:04,080
is that there's not just gonna be like one singular AI

389
00:17:04,080 --> 00:17:05,280
that you interact with,

390
00:17:05,280 --> 00:17:08,960
because I think every business is gonna like want an AI

391
00:17:08,960 --> 00:17:10,240
that represents their interests.

392
00:17:10,240 --> 00:17:12,800
They're not gonna like wanna primarily interact with you

393
00:17:12,800 --> 00:17:16,240
through an AI that is gonna sell their competitors' customers.

394
00:17:16,240 --> 00:17:18,160
So, sorry, their competitors' products.

395
00:17:19,040 --> 00:17:25,280
So, yeah, so I think creators is gonna be a big one.

396
00:17:25,280 --> 00:17:28,640
I mean, there are about 200 million creators on our platforms.

397
00:17:28,640 --> 00:17:30,480
They all basically have the pattern where

398
00:17:31,600 --> 00:17:33,040
they want to engage their community,

399
00:17:33,040 --> 00:17:34,560
but they're limited by hours in the day

400
00:17:34,560 --> 00:17:36,800
and their community generally wants to engage them,

401
00:17:36,800 --> 00:17:38,800
but they don't have, they're limited by hours in the day.

402
00:17:39,520 --> 00:17:43,840
So, if you could create something where an AI could basically,

403
00:17:44,400 --> 00:17:46,640
that creator can basically own the AI

404
00:17:46,640 --> 00:17:48,080
and train it in the way that they want

405
00:17:49,920 --> 00:17:51,600
and can engage their community,

406
00:17:51,600 --> 00:17:53,760
I think that that's gonna be super powerful too.

407
00:17:53,760 --> 00:17:56,240
So, I think that there's gonna be a ton of engagement

408
00:17:56,240 --> 00:17:57,200
across all these things.

409
00:17:59,440 --> 00:18:01,120
But these are just the consumer use cases.

410
00:18:01,120 --> 00:18:02,800
I mean, I think when you think about stuff like,

411
00:18:03,680 --> 00:18:06,640
I mean, I run our foundation,

412
00:18:07,440 --> 00:18:09,120
Chan Zuckerberg Initiative with my wife,

413
00:18:09,120 --> 00:18:11,200
and we're doing a bunch of stuff on science,

414
00:18:11,200 --> 00:18:13,760
and there's obviously a lot of AI work

415
00:18:13,760 --> 00:18:16,880
that I think is gonna advance science and healthcare

416
00:18:16,880 --> 00:18:17,680
and all these things too.

417
00:18:17,680 --> 00:18:19,200
So, I think that it's like,

418
00:18:19,200 --> 00:18:21,040
this is, I think, an end up affecting

419
00:18:21,040 --> 00:18:25,360
basically every area of the products and the economy.

420
00:18:25,360 --> 00:18:27,200
The thing you mentioned about an AI

421
00:18:27,200 --> 00:18:29,040
that can just go out and do something for you

422
00:18:29,040 --> 00:18:31,520
that's multi-step, is that a bigger model?

423
00:18:31,520 --> 00:18:34,240
Is that you'll make, like, Lama 4 will still,

424
00:18:34,240 --> 00:18:35,840
there'll be a version that's still 70B,

425
00:18:35,840 --> 00:18:38,000
but will just be, you'll just train it on the right data,

426
00:18:38,000 --> 00:18:39,840
and that will be super powerful.

427
00:18:39,840 --> 00:18:41,280
Like, what does the progression look like?

428
00:18:41,280 --> 00:18:43,600
Is it scaling? Is it just same size,

429
00:18:43,600 --> 00:18:45,680
but different banks like you were talking about?

430
00:18:49,200 --> 00:18:51,600
I don't know that we know the answer to that.

431
00:18:51,600 --> 00:18:55,840
So, I think one thing that seems to be a pattern

432
00:18:55,840 --> 00:18:58,000
is that you have the Lama,

433
00:18:58,000 --> 00:18:59,680
sorry, the Lama model,

434
00:18:59,680 --> 00:19:04,000
and then you build some kind of other

435
00:19:04,000 --> 00:19:06,320
application-specific code around it, right?

436
00:19:06,320 --> 00:19:08,640
So, some of it is the fine-tuning for the use case,

437
00:19:08,640 --> 00:19:12,560
but some of it is just like logic for, okay, how,

438
00:19:14,320 --> 00:19:16,240
like, how Met AI should integrate,

439
00:19:17,040 --> 00:19:19,280
that should work with tools like Google or Bing

440
00:19:19,280 --> 00:19:20,560
to bring in real-time knowledge.

441
00:19:20,560 --> 00:19:22,080
I mean, that's not part of the base Lama model.

442
00:19:22,080 --> 00:19:22,960
That's like part of it.

443
00:19:22,960 --> 00:19:25,840
Okay, so, for Lama 2, we had some of that,

444
00:19:26,640 --> 00:19:29,440
and it was a little more kind of hand-engineered.

445
00:19:29,440 --> 00:19:31,440
And then part of our goal for Lama 3

446
00:19:32,080 --> 00:19:34,480
was to bring more of that into the model itself.

447
00:19:35,120 --> 00:19:36,320
And, but for Lama 3,

448
00:19:36,320 --> 00:19:39,440
as we start getting into more of these agent-like behaviors,

449
00:19:40,000 --> 00:19:42,880
I think some of that is going to be more hand-engineered.

450
00:19:42,880 --> 00:19:44,960
And then I think our goal for Lama 4

451
00:19:44,960 --> 00:19:46,800
will be to bring more of that into the model.

452
00:19:46,800 --> 00:19:50,080
So, I think at each point, like at each step along the way,

453
00:19:50,080 --> 00:19:53,280
you kind of have a sense of what's going to be possible

454
00:19:53,280 --> 00:19:54,000
on the horizon.

455
00:19:54,000 --> 00:19:56,080
You start messing with it and hacking around it.

456
00:19:56,880 --> 00:19:59,360
And then I think that that helps you hone your intuition

457
00:19:59,680 --> 00:20:01,520
for what you want to try to train

458
00:20:01,520 --> 00:20:03,440
into the next version of the model itself.

459
00:20:03,440 --> 00:20:03,920
Interesting.

460
00:20:03,920 --> 00:20:04,960
Which makes it more general,

461
00:20:04,960 --> 00:20:07,200
because obviously anything that you're hand-coding

462
00:20:07,200 --> 00:20:10,240
is, you know, you can unlock some use cases,

463
00:20:10,240 --> 00:20:12,320
but it's just inherently brittle and non-general.

464
00:20:13,520 --> 00:20:14,400
Hey, everybody.

465
00:20:14,400 --> 00:20:16,720
Real quick, I want to tell you about a tool

466
00:20:16,720 --> 00:20:19,120
that I wish more applications used.

467
00:20:19,120 --> 00:20:21,920
So, obviously, you've noticed every single company

468
00:20:21,920 --> 00:20:25,120
is trying to add an AI chatbot to their website.

469
00:20:25,120 --> 00:20:28,480
But as a user, I usually find them really annoying

470
00:20:28,480 --> 00:20:31,600
because they give these long, generic, often useless answers.

471
00:20:32,320 --> 00:20:35,360
Command Bar is a user assistant that you can just embed

472
00:20:35,360 --> 00:20:36,880
into your website or application.

473
00:20:37,440 --> 00:20:40,640
And it feels like you're talking to a friendly human support

474
00:20:40,640 --> 00:20:43,440
agent who is browsing with you and for you.

475
00:20:44,000 --> 00:20:47,520
And it's much more personalized than a regular chatbot.

476
00:20:47,520 --> 00:20:49,440
It can actually look up users' history

477
00:20:49,440 --> 00:20:51,600
and respond differently based on that.

478
00:20:51,600 --> 00:20:54,560
It can use APIs to perform actions.

479
00:20:54,560 --> 00:20:58,080
It can even practically nudge users to explore new features.

480
00:20:58,560 --> 00:21:00,160
One thing that I think is really cool

481
00:21:00,160 --> 00:21:02,640
is that instead of just outputting text,

482
00:21:02,640 --> 00:21:05,680
Command Bar can kind of just say, here, let me show you

483
00:21:05,680 --> 00:21:07,920
and start browsing alongside the user.

484
00:21:08,560 --> 00:21:11,120
Anyways, they're in a bunch of great products already.

485
00:21:11,120 --> 00:21:14,960
You can learn more about them at commandbar.com.

486
00:21:15,520 --> 00:21:17,520
Thanks to them for sponsoring this episode.

487
00:21:17,520 --> 00:21:18,560
And now back to Mark.

488
00:21:19,120 --> 00:21:20,560
What do you say into the model itself?

489
00:21:20,560 --> 00:21:24,080
You train it on the thing that you want in the model itself?

490
00:21:24,080 --> 00:21:25,920
What do you mean by into the model itself?

491
00:21:25,920 --> 00:21:29,520
Well, I think the example that I gave for Llama 2,

492
00:21:29,520 --> 00:21:37,040
where for Llama 2, the tool use was very specific.

493
00:21:37,920 --> 00:21:40,800
Whereas Llama 3 has the ability to have much better tool use.

494
00:21:40,800 --> 00:21:44,080
So we don't have to hand code all the stuff

495
00:21:44,080 --> 00:21:47,280
to have it use Google to go do a search.

496
00:21:48,160 --> 00:21:49,680
It just kind of can do that.

497
00:21:51,840 --> 00:21:54,800
And similarly for coding and kind of running code

498
00:21:54,800 --> 00:21:56,640
and a bunch of stuff like that.

499
00:21:59,040 --> 00:22:01,040
But I think once you kind of get that capability,

500
00:22:01,920 --> 00:22:04,960
then you get a peek of, okay, well, what can we start doing next?

501
00:22:04,960 --> 00:22:07,760
Okay, well, I don't necessarily want to wait until Llama 4 is around

502
00:22:07,760 --> 00:22:09,280
to start building those capabilities.

503
00:22:09,280 --> 00:22:10,640
So let's start hacking around it.

504
00:22:10,640 --> 00:22:13,040
And so you do a bunch of hand coding

505
00:22:13,040 --> 00:22:16,320
and that makes the products better for the interim.

506
00:22:16,320 --> 00:22:18,240
But then that also helps show the way

507
00:22:18,240 --> 00:22:21,280
of what we want to try to build into the next version of the model.

508
00:22:21,280 --> 00:22:23,600
What is the community fine tune of Llama 3

509
00:22:23,600 --> 00:22:24,640
you're most excited by?

510
00:22:24,640 --> 00:22:26,400
Maybe not the one that will be most useful to you,

511
00:22:26,400 --> 00:22:28,320
but Jess, you'll just enjoy playing it with the most.

512
00:22:29,760 --> 00:22:31,200
They like fine tune it on antiquity

513
00:22:31,200 --> 00:22:32,960
and you'll just be like talking to Virgil or something.

514
00:22:32,960 --> 00:22:34,320
What are you excited about?

515
00:22:34,320 --> 00:22:34,880
I don't know.

516
00:22:36,320 --> 00:22:38,400
I mean, I think the nature of the stuff is it's like,

517
00:22:39,600 --> 00:22:41,200
you get surprised, right?

518
00:22:41,200 --> 00:22:44,960
So I think like any specific thing that I sort of

519
00:22:46,560 --> 00:22:49,520
thought would be valuable, we'd probably be building, right?

520
00:22:49,520 --> 00:22:54,960
So, but I think you'll get distilled versions.

521
00:22:54,960 --> 00:22:57,120
I think you'll get kind of smaller versions.

522
00:22:57,120 --> 00:23:01,760
I mean, one thing that I think is 8 billion,

523
00:23:01,760 --> 00:23:05,440
I don't think is quite small enough for a bunch of use cases, right?

524
00:23:05,440 --> 00:23:09,680
I think like over time, I'd love to get a billion parameter model

525
00:23:09,680 --> 00:23:11,680
or a 2 billion parameter model

526
00:23:11,680 --> 00:23:14,720
or even like a, I don't know, maybe like a 500 million parameter model

527
00:23:14,720 --> 00:23:15,680
and see what you can do with that.

528
00:23:15,680 --> 00:23:19,520
Because I mean, as they start getting, if with 8 billion parameters

529
00:23:19,520 --> 00:23:23,440
we're basically nearly as powerful as the largest llama 2 model,

530
00:23:23,440 --> 00:23:25,360
then with a billion parameters,

531
00:23:25,360 --> 00:23:27,200
you should be able to do something that's interesting, right?

532
00:23:27,200 --> 00:23:30,720
And faster, good for classification

533
00:23:30,720 --> 00:23:33,200
or a lot of kind of like basic things that people do

534
00:23:33,200 --> 00:23:37,920
before kind of understanding the intent of a user query

535
00:23:37,920 --> 00:23:39,600
and feeding it to the most powerful model

536
00:23:39,600 --> 00:23:42,320
to kind of hone what the prompt should be.

537
00:23:44,000 --> 00:23:44,480
So I don't know.

538
00:23:44,480 --> 00:23:46,640
I think that's one thing that maybe the community can help fill in.

539
00:23:46,640 --> 00:23:49,200
But I mean, we'll also, we're also thinking about getting around

540
00:23:49,200 --> 00:23:51,520
to distilling some of these ourselves,

541
00:23:51,520 --> 00:23:55,840
but right now the GPUs are pegged training the 405.

542
00:23:55,840 --> 00:23:57,840
So what, okay, so you have all these GPUs,

543
00:24:00,080 --> 00:24:02,080
I think 350,000 by the end of the year.

544
00:24:02,080 --> 00:24:02,960
That's the whole fleet.

545
00:24:02,960 --> 00:24:10,000
I mean, we built two, I think it's like 22, 24,000 clusters

546
00:24:10,000 --> 00:24:12,320
that are kind of the single clusters that we have

547
00:24:12,320 --> 00:24:13,440
for training the big models.

548
00:24:13,840 --> 00:24:16,000
I mean, obviously across a lot of the stuff that we do,

549
00:24:16,000 --> 00:24:19,120
a lot of our stuff goes towards training like reels models

550
00:24:19,120 --> 00:24:22,160
and like Facebook news feed and Instagram feed.

551
00:24:22,160 --> 00:24:23,840
And then inference is a huge thing for us

552
00:24:23,840 --> 00:24:25,360
because we serve a ton of people, right?

553
00:24:25,360 --> 00:24:32,320
So our ratio of inference compute required to training

554
00:24:32,320 --> 00:24:34,640
is probably much higher than most other companies

555
00:24:34,640 --> 00:24:36,800
that are doing this stuff just because of the sheer volume

556
00:24:36,800 --> 00:24:38,800
of the community that we're serving.

557
00:24:38,800 --> 00:24:39,840
Yeah, yeah.

558
00:24:39,840 --> 00:24:42,240
That was really interesting in the material they shared with me before

559
00:24:42,240 --> 00:24:45,200
that you trained it on more data than is computer optimal

560
00:24:45,200 --> 00:24:47,680
just for training because the inference is such a big deal

561
00:24:47,680 --> 00:24:49,360
for you guys and also for the community

562
00:24:49,360 --> 00:24:50,960
that it makes sense to just have this thing

563
00:24:50,960 --> 00:24:52,800
and have a trillion to tokens in there.

564
00:24:52,800 --> 00:24:53,520
Yeah, yeah.

565
00:24:53,520 --> 00:24:55,920
Although, and one of the interesting things about it

566
00:24:55,920 --> 00:24:58,320
that we saw even with the 70 billion is we thought

567
00:24:58,320 --> 00:25:02,240
it would get more saturated at, you know,

568
00:25:02,240 --> 00:25:04,720
it's like we trained on around 15 trillion tokens.

569
00:25:04,720 --> 00:25:05,520
Yeah.

570
00:25:05,520 --> 00:25:07,920
We, I guess our prediction going in was that

571
00:25:08,560 --> 00:25:10,720
it was going to ask some to it more,

572
00:25:10,720 --> 00:25:13,920
but even by the end it was still learning, right?

573
00:25:13,920 --> 00:25:17,200
It's like we probably could have fed it more tokens

574
00:25:17,200 --> 00:25:19,040
and it would have gotten somewhat better.

575
00:25:19,040 --> 00:25:21,200
But I mean, at some point, you know, you're running a company

576
00:25:21,200 --> 00:25:24,240
you need to do these meta reasoning questions of like,

577
00:25:24,240 --> 00:25:26,400
all right, how do I want to spend our GPUs

578
00:25:26,400 --> 00:25:29,040
on like training this 70 billion model further?

579
00:25:29,040 --> 00:25:31,280
Do we want to kind of get on with it

580
00:25:31,280 --> 00:25:33,840
so we can start testing hypotheses for Llama 4?

581
00:25:33,840 --> 00:25:36,800
So we kind of needed to make that call.

582
00:25:36,800 --> 00:25:38,000
And I think we got it,

583
00:25:38,000 --> 00:25:39,280
I think we got to a reasonable balance

584
00:25:39,760 --> 00:25:41,200
for this version of the 70 billion.

585
00:25:42,880 --> 00:25:44,080
There will be others in the future

586
00:25:44,080 --> 00:25:45,760
where, you know, 70 billion multimodal one

587
00:25:45,760 --> 00:25:47,760
that'll come over the next period.

588
00:25:47,760 --> 00:25:51,280
But yeah, I mean, that was fascinating

589
00:25:51,280 --> 00:25:53,280
that you could just, that it's the architectures

590
00:25:53,280 --> 00:25:55,600
at this point can just take so much data.

591
00:25:55,600 --> 00:25:56,400
Yeah, that's really interesting.

592
00:25:56,400 --> 00:25:58,240
So what is this imply by future models?

593
00:25:58,960 --> 00:26:02,320
You mentioned that the Llama 3 8B is better

594
00:26:02,320 --> 00:26:03,520
than the Llama 270B?

595
00:26:03,520 --> 00:26:04,720
No, no, no, it's nearly as good.

596
00:26:04,720 --> 00:26:05,440
Okay.

597
00:26:05,440 --> 00:26:06,320
I don't overstep.

598
00:26:06,320 --> 00:26:07,440
But does that mean like the Llama 4?

599
00:26:07,440 --> 00:26:08,320
The same order of magnitude.

600
00:26:08,400 --> 00:26:09,280
Does that mean like the Llama 4?

601
00:26:09,280 --> 00:26:10,800
70B will be as good as the Llama 3?

602
00:26:10,800 --> 00:26:11,760
4 or 5B?

603
00:26:11,760 --> 00:26:15,120
I mean, this is one of the great questions, right?

604
00:26:15,120 --> 00:26:17,920
That I think no one knows is basically,

605
00:26:19,680 --> 00:26:22,800
you know, it's one of the trickiest things in the world

606
00:26:22,800 --> 00:26:25,280
to plan around is when you have an exponential curve,

607
00:26:25,280 --> 00:26:26,880
how long does it keep going for?

608
00:26:27,520 --> 00:26:32,080
And I think it's likely enough that it will keep going,

609
00:26:32,080 --> 00:26:36,240
that it is worth investing the tens or, you know,

610
00:26:36,320 --> 00:26:39,360
100 billion plus in building the infrastructure

611
00:26:39,360 --> 00:26:42,480
to assume that if that kind of keeps going,

612
00:26:42,480 --> 00:26:44,720
you're going to get some really amazing things

613
00:26:44,720 --> 00:26:46,480
that are just going to make amazing products.

614
00:26:47,040 --> 00:26:50,640
But I don't think anyone in the industry can really tell you

615
00:26:51,600 --> 00:26:55,200
that it will continue scaling at that rate for sure, right?

616
00:26:55,200 --> 00:26:58,640
In general, in history, you hit bottlenecks at certain points.

617
00:26:58,640 --> 00:27:00,800
And now there's so much energy on this

618
00:27:00,800 --> 00:27:03,920
that maybe those bottlenecks get knocked over pretty quickly.

619
00:27:03,920 --> 00:27:08,080
But I don't know. I think that's an interesting question.

620
00:27:08,080 --> 00:27:11,120
What does the world look like where there aren't these bottlenecks?

621
00:27:11,120 --> 00:27:14,240
Suppose like progress just continues at this pace,

622
00:27:14,240 --> 00:27:17,520
which seems like plausible, like zooming out.

623
00:27:17,520 --> 00:27:19,840
Well, they're going to be different bottlenecks.

624
00:27:20,560 --> 00:27:22,800
Right. So if not training, then like, oh, yeah, go ahead.

625
00:27:23,680 --> 00:27:27,600
Well, I think at some point, over the last few years,

626
00:27:27,600 --> 00:27:31,200
I think there was this issue of GPU production.

627
00:27:31,200 --> 00:27:33,760
Yeah. Right. So even companies that had the models,

628
00:27:35,200 --> 00:27:37,120
sorry, that had the money to pay for the GPUs,

629
00:27:38,800 --> 00:27:40,640
couldn't necessarily get as many as they wanted

630
00:27:40,640 --> 00:27:43,120
because there were all these supply constraints.

631
00:27:43,120 --> 00:27:45,680
Now I think that's sort of getting less.

632
00:27:46,240 --> 00:27:50,320
So now I think you're seeing a bunch of companies think about,

633
00:27:50,320 --> 00:27:52,640
wow, we should just like really invest a lot of money

634
00:27:52,640 --> 00:27:53,840
in building out these things.

635
00:27:53,840 --> 00:27:56,960
And I think that that will go for some period of time.

636
00:27:57,840 --> 00:28:02,400
I think there's a, there is a capital question of like, okay,

637
00:28:03,280 --> 00:28:06,640
at what point does it stop being worth it to put the capital in?

638
00:28:06,640 --> 00:28:09,040
But I actually think before we hit that,

639
00:28:09,040 --> 00:28:11,120
you're going to run into energy constraints.

640
00:28:11,120 --> 00:28:14,080
Right. Because I just, I mean,

641
00:28:14,080 --> 00:28:19,120
I don't think anyone's built a gigawatt single training cluster yet.

642
00:28:19,120 --> 00:28:21,600
Right. And then you run into these things

643
00:28:21,600 --> 00:28:23,200
that just end up being slower in the world.

644
00:28:23,200 --> 00:28:26,400
Like getting energy permitted

645
00:28:26,400 --> 00:28:31,040
is like a very heavily regulated government function.

646
00:28:31,040 --> 00:28:34,320
Right. So you're going from on the one hand software,

647
00:28:34,320 --> 00:28:36,240
which is somewhat regulated.

648
00:28:36,240 --> 00:28:38,320
I'd argue that it is more regulated

649
00:28:38,320 --> 00:28:41,360
than I think a lot of people in the tech community feel,

650
00:28:41,360 --> 00:28:42,480
although it's obviously different.

651
00:28:42,480 --> 00:28:43,680
If you're starting a small company,

652
00:28:43,680 --> 00:28:45,680
maybe you feel that less if you're a big company,

653
00:28:45,680 --> 00:28:47,360
you know, we just interact with people,

654
00:28:47,360 --> 00:28:49,440
but different governments and regulators are,

655
00:28:49,440 --> 00:28:52,560
you know, we have kind of lots of rules

656
00:28:52,560 --> 00:28:53,600
that we need to kind of follow

657
00:28:53,600 --> 00:28:55,280
and make sure we do a good job with around the world.

658
00:28:56,720 --> 00:28:58,800
But I think that there's no doubt that like energy,

659
00:28:58,800 --> 00:29:02,720
and if you're talking about building large new power plants

660
00:29:02,720 --> 00:29:05,520
or large buildouts and then building transmission lines

661
00:29:05,520 --> 00:29:09,920
that cross other private or public land,

662
00:29:09,920 --> 00:29:11,760
that is just a heavily regulated thing.

663
00:29:11,760 --> 00:29:14,480
So you're talking about many years of lead time.

664
00:29:14,480 --> 00:29:18,800
So if we wanted to stand up to some like massive facility

665
00:29:18,800 --> 00:29:23,040
to power that, I think that that is,

666
00:29:24,080 --> 00:29:26,800
that's a very long-term project, right?

667
00:29:26,800 --> 00:29:29,600
And so I don't know, I think that that's,

668
00:29:29,600 --> 00:29:30,960
I think people will do it,

669
00:29:30,960 --> 00:29:33,280
but I don't think that this is like something

670
00:29:33,280 --> 00:29:35,440
that can be quite as magical as just like,

671
00:29:35,440 --> 00:29:37,760
okay, you get a level of AI and you get a bunch of capital

672
00:29:37,760 --> 00:29:39,200
and you put it in and then like all of a sudden

673
00:29:39,200 --> 00:29:41,440
the models are just going to kind of like interest,

674
00:29:41,440 --> 00:29:43,840
like I think you do hit different bottlenecks along the way.

675
00:29:43,840 --> 00:29:46,320
Yeah. Is there something, a project,

676
00:29:46,320 --> 00:29:48,160
maybe I realized maybe not,

677
00:29:48,160 --> 00:29:51,360
that even a company like Meta doesn't have the resources for,

678
00:29:51,360 --> 00:29:54,640
like if your R&D budget or CapEx budget was 10x what it is now,

679
00:29:54,640 --> 00:29:56,800
then you could pursue it, like it's in the back of your mind,

680
00:29:56,800 --> 00:30:00,000
but Meta today, maybe you could like,

681
00:30:00,000 --> 00:30:01,680
even you can't even issue a stock or bond for it,

682
00:30:01,680 --> 00:30:03,520
it's like just 10x bigger than your budget.

683
00:30:03,520 --> 00:30:05,280
Well, I think energy is one piece, right?

684
00:30:06,400 --> 00:30:10,160
I think we would probably build out bigger clusters

685
00:30:10,160 --> 00:30:15,840
than we currently can if we could get the energy to do it.

686
00:30:15,840 --> 00:30:21,920
So I think that's fundamentally money bottlenecked in the limit,

687
00:30:21,920 --> 00:30:23,040
like if you had a trillion dollars.

688
00:30:23,040 --> 00:30:24,640
I think it's time, right?

689
00:30:26,320 --> 00:30:28,720
Well, if you look at it in terms of,

690
00:30:28,720 --> 00:30:31,520
but it depends on how far the exponential curves go, right?

691
00:30:31,520 --> 00:30:34,160
Like I think a number of companies are working on,

692
00:30:34,160 --> 00:30:36,800
you know, right now I think a lot of data centers

693
00:30:36,800 --> 00:30:39,120
are on the order of 50 megawatts or 100 megawatts,

694
00:30:39,120 --> 00:30:41,360
or like a big one might be 150 megawatts.

695
00:30:41,360 --> 00:30:44,160
Okay, so you take a whole data center and you fill it up with

696
00:30:44,240 --> 00:30:46,160
just all the stuff that you need to do for training

697
00:30:46,160 --> 00:30:47,520
and you build the biggest cluster you can.

698
00:30:47,520 --> 00:30:49,680
I think that's kind of,

699
00:30:49,680 --> 00:30:51,760
I think a bunch of companies are running at stuff like that.

700
00:30:53,200 --> 00:30:57,920
But then when you start getting into building a data center

701
00:30:57,920 --> 00:31:02,640
that's like 300 megawatts or 500 megawatts or a gigawatt,

702
00:31:02,640 --> 00:31:06,320
I mean, just no one has built a single gigawatt data center yet.

703
00:31:06,320 --> 00:31:07,600
So I think it will happen, right?

704
00:31:07,600 --> 00:31:08,800
I mean, this is only a matter of time,

705
00:31:08,800 --> 00:31:11,840
but it's not going to be like next year, right?

706
00:31:12,400 --> 00:31:16,240
I think that some of these things will take, I don't know,

707
00:31:17,040 --> 00:31:19,120
some number of years to build out.

708
00:31:19,120 --> 00:31:20,880
And then the question is, okay, well, if you,

709
00:31:22,400 --> 00:31:24,240
I mean, just to, I guess, put this in perspective,

710
00:31:25,440 --> 00:31:29,520
I think a gigawatt, it's like around the size of like

711
00:31:30,160 --> 00:31:32,400
a meaningful nuclear power plant

712
00:31:32,400 --> 00:31:34,800
only going towards training a model.

713
00:31:34,800 --> 00:31:36,400
Didn't Amazon do this?

714
00:31:36,400 --> 00:31:38,960
There's like, they have a 950 megawatt thing.

715
00:31:38,960 --> 00:31:40,560
Yeah, I'm not exactly sure what you did.

716
00:31:41,120 --> 00:31:42,240
What they did, you'd have to ask them.

717
00:31:43,840 --> 00:31:45,280
But it doesn't have to be in the same place, right?

718
00:31:45,280 --> 00:31:47,200
If distributed training works, it can be distributed.

719
00:31:47,200 --> 00:31:48,080
That I think is a big question.

720
00:31:48,080 --> 00:31:48,400
Yeah.

721
00:31:48,400 --> 00:31:49,920
Right, is basically how that's going to work.

722
00:31:49,920 --> 00:31:51,120
And I do think in the future,

723
00:31:52,160 --> 00:31:56,560
it seems quite possible that more of what we call training

724
00:31:56,560 --> 00:32:01,680
for these big models is actually more along the lines

725
00:32:02,240 --> 00:32:04,880
of inference generating synthetic data

726
00:32:04,880 --> 00:32:06,560
to then go feed into the model.

727
00:32:06,560 --> 00:32:08,560
So I don't know what that ratio is going to be,

728
00:32:08,560 --> 00:32:12,400
but I consider the generation of synthetic data

729
00:32:12,400 --> 00:32:14,480
to be more inference than training today.

730
00:32:14,480 --> 00:32:16,800
But obviously, if you're doing it in order to train a model,

731
00:32:16,800 --> 00:32:18,720
it's part of the broader training process.

732
00:32:19,280 --> 00:32:23,440
So I don't know, that's an open question,

733
00:32:23,440 --> 00:32:25,440
is to kind of where, what the balance of that

734
00:32:25,440 --> 00:32:26,480
and how that plays out.

735
00:32:26,480 --> 00:32:29,760
If that's the case, would that potentially also

736
00:32:29,760 --> 00:32:31,200
be the case with Lama 3?

737
00:32:31,200 --> 00:32:34,000
And maybe like Lama 4 onwards, where you put this out

738
00:32:34,000 --> 00:32:35,840
and if somebody has a ton of compute,

739
00:32:35,840 --> 00:32:37,680
then using the models that you've put out,

740
00:32:37,760 --> 00:32:39,920
you can just keep making these things arbitrarily smarter.

741
00:32:41,120 --> 00:32:44,720
Some Kuwait or UAE or some random country has a ton of compute,

742
00:32:45,600 --> 00:32:48,240
and they can just actually just use Lama 4

743
00:32:48,240 --> 00:32:49,440
to just make something much smarter.

744
00:32:52,240 --> 00:32:55,600
I do think that there are going to be dynamics like that,

745
00:32:55,600 --> 00:33:00,960
but I also think that there is a fundamental limitation

746
00:33:01,520 --> 00:33:05,760
on kind of the network architecture,

747
00:33:06,320 --> 00:33:08,080
the kind of model architecture.

748
00:33:08,080 --> 00:33:11,360
So I think like a 70 billion model

749
00:33:12,320 --> 00:33:14,560
that kind of we trained with the Lama 3 architecture

750
00:33:14,560 --> 00:33:16,800
can get better, it can keep going.

751
00:33:16,800 --> 00:33:20,160
Like I was saying, we felt like if we kept on feeding it

752
00:33:20,160 --> 00:33:24,080
more data or rotated the high value tokens through again,

753
00:33:24,080 --> 00:33:26,160
then it would continue getting better.

754
00:33:26,880 --> 00:33:31,520
But, and we've seen a bunch of other people around the world,

755
00:33:32,480 --> 00:33:35,280
you know, different companies basically take the Lama 2

756
00:33:35,920 --> 00:33:38,240
70 billion base, like take that model architecture

757
00:33:38,240 --> 00:33:39,200
and then build a new model.

758
00:33:41,360 --> 00:33:44,160
It's still the case that when you make a generational improvement

759
00:33:44,160 --> 00:33:47,200
to the kind of Lama 3 70 billion or the Lama 3 405,

760
00:33:47,200 --> 00:33:49,920
there's nothing open source anything like that today, right?

761
00:33:49,920 --> 00:33:53,120
Like it's not, I think that that's like,

762
00:33:53,120 --> 00:33:54,960
it's a big step function

763
00:33:54,960 --> 00:33:57,120
and what people are going to be able to build on top of

764
00:33:57,120 --> 00:33:59,760
that I don't think can go infinitely from there.

765
00:33:59,840 --> 00:34:02,480
I think it can, there can be some optimization in that

766
00:34:02,480 --> 00:34:03,920
until you get to the next step function.

767
00:34:04,800 --> 00:34:05,680
Yeah. Okay.

768
00:34:05,680 --> 00:34:08,720
So let's zoom out a little bit from specific models

769
00:34:08,720 --> 00:34:12,000
and even the many years lead times you would need

770
00:34:12,000 --> 00:34:13,920
to get energy approvals and so on.

771
00:34:13,920 --> 00:34:16,240
Like big picture, these next couple of decades,

772
00:34:16,240 --> 00:34:17,280
what's happening with AI?

773
00:34:18,080 --> 00:34:20,000
Does it feel like another technology,

774
00:34:20,000 --> 00:34:21,440
like metaverse or social,

775
00:34:21,440 --> 00:34:23,520
or does it feel like a fundamentally different thing

776
00:34:23,520 --> 00:34:24,720
in the course of human history?

777
00:34:25,600 --> 00:34:30,000
I think it's going to be pretty fundamental.

778
00:34:30,000 --> 00:34:33,280
I think it's going to be more like the creation

779
00:34:33,280 --> 00:34:36,160
of computing in the first place, right?

780
00:34:36,160 --> 00:34:41,840
So you'll get all these new apps in the same way

781
00:34:42,640 --> 00:34:45,440
that when you got the web or you got mobile phones,

782
00:34:45,440 --> 00:34:48,800
you got like people basically rethought all these experiences

783
00:34:48,800 --> 00:34:50,400
and a lot of things that weren't possible

784
00:34:50,400 --> 00:34:51,760
before now became possible.

785
00:34:52,080 --> 00:34:53,440
Something that will happen,

786
00:34:53,440 --> 00:34:56,240
but I think it's a much lower level innovation.

787
00:34:56,240 --> 00:34:59,040
It's going to be more like going from

788
00:34:59,040 --> 00:35:02,240
people didn't have computers to people have computers,

789
00:35:02,240 --> 00:35:03,200
is my sense.

790
00:35:05,360 --> 00:35:09,440
But it's also, it's, I don't know,

791
00:35:09,440 --> 00:35:14,400
it's very hard to reason about exactly how this goes.

792
00:35:14,400 --> 00:35:17,440
I tend to think that, you know,

793
00:35:17,440 --> 00:35:19,120
in like the cosmic scale, obviously,

794
00:35:19,360 --> 00:35:22,480
it'll happen quickly over a couple of decades or something.

795
00:35:22,480 --> 00:35:25,680
But I do think that there is some set of people

796
00:35:25,680 --> 00:35:27,680
who are afraid of like, you know,

797
00:35:27,680 --> 00:35:30,080
it really just kind of spins and goes from being

798
00:35:30,080 --> 00:35:32,960
like somewhat intelligent to extremely intelligent overnight.

799
00:35:32,960 --> 00:35:34,880
And I just think that there's all these physical constraints

800
00:35:34,880 --> 00:35:37,360
that make that, so that that's unlikely to happen.

801
00:35:37,360 --> 00:35:41,280
I just don't, I don't really see that playing out.

802
00:35:41,280 --> 00:35:42,480
So I think you'll have,

803
00:35:42,480 --> 00:35:44,720
I think we'll have time to kind of acclimate a bit,

804
00:35:44,720 --> 00:35:46,320
but it will really change.

805
00:35:46,320 --> 00:35:49,760
The way that we work and give people all these creative tools

806
00:35:49,760 --> 00:35:52,880
to do different things that they, yeah.

807
00:35:52,880 --> 00:35:54,560
I think it's going to be,

808
00:35:54,560 --> 00:35:56,720
it's going to really enable people to do

809
00:35:56,720 --> 00:35:59,120
the things that they want a lot more, as is my view.

810
00:36:00,400 --> 00:36:01,840
Okay, so maybe not overnight,

811
00:36:01,840 --> 00:36:04,560
but is it your view that like on a cosmic scale,

812
00:36:04,560 --> 00:36:08,000
if you think like humans evolved and then like AI happened

813
00:36:08,000 --> 00:36:10,000
and then they like went out through the galaxy

814
00:36:10,000 --> 00:36:13,200
or maybe it takes many decades, maybe it takes a century,

815
00:36:13,200 --> 00:36:14,720
but like, you know,

816
00:36:14,960 --> 00:36:16,320
is that like the grand scheme

817
00:36:16,320 --> 00:36:18,000
of what's happening right now in history?

818
00:36:19,600 --> 00:36:20,640
Sorry, in what sense?

819
00:36:20,640 --> 00:36:22,800
I mean, in the sense that there were other technologies

820
00:36:22,800 --> 00:36:24,160
like computers and even like fire,

821
00:36:24,160 --> 00:36:26,800
but like the AI happening is as significant

822
00:36:26,800 --> 00:36:28,640
as like humans evolving in the first place.

823
00:36:29,600 --> 00:36:31,200
I think that's tricky.

824
00:36:31,200 --> 00:36:35,040
I think people like to, you know,

825
00:36:35,040 --> 00:36:37,360
the history of humanity, I think has been

826
00:36:38,240 --> 00:36:40,640
people basically, you know,

827
00:36:40,640 --> 00:36:42,400
thinking that certain things

828
00:36:43,360 --> 00:36:51,200
of humanity are like really unique in different ways.

829
00:36:51,840 --> 00:36:55,600
And then coming to grips with the fact

830
00:36:55,600 --> 00:36:56,480
that that's not true,

831
00:36:56,480 --> 00:36:59,280
but humanity is actually still super special, right?

832
00:36:59,280 --> 00:37:03,920
So it's like we thought that the earth

833
00:37:03,920 --> 00:37:05,200
was the center of the universe.

834
00:37:05,200 --> 00:37:08,400
And it's like, it's not, but like humans

835
00:37:08,400 --> 00:37:09,840
are still pretty awesome, right?

836
00:37:09,840 --> 00:37:11,280
And pretty unique.

837
00:37:12,480 --> 00:37:14,720
I think that another bias that people tend to have

838
00:37:15,520 --> 00:37:18,000
is thinking that intelligence is somehow

839
00:37:20,160 --> 00:37:23,360
kind of fundamentally connected to life.

840
00:37:24,000 --> 00:37:26,960
And it's not actually clear that it is, right?

841
00:37:26,960 --> 00:37:28,960
I think like people think that,

842
00:37:31,520 --> 00:37:32,880
I mean, I don't know that we have

843
00:37:32,880 --> 00:37:34,800
a clear enough definition of consciousness

844
00:37:34,800 --> 00:37:39,920
or life to kind of fully interrogate this,

845
00:37:39,920 --> 00:37:43,280
but there's all this science fiction about,

846
00:37:43,280 --> 00:37:44,880
okay, you create intelligence

847
00:37:44,880 --> 00:37:47,520
and now it like starts taking on all these human

848
00:37:47,520 --> 00:37:50,320
like behaviors and things like that.

849
00:37:50,320 --> 00:37:52,560
But I actually think that the current incarnation

850
00:37:52,560 --> 00:37:54,400
of all this stuff at least kind of feels

851
00:37:54,400 --> 00:37:55,440
like it's going in a direction

852
00:37:55,440 --> 00:37:57,760
where intelligence can be pretty separated

853
00:37:57,760 --> 00:38:01,200
from consciousness and agency and things like that,

854
00:38:01,200 --> 00:38:05,040
that I think just makes it a super valuable tool.

855
00:38:05,040 --> 00:38:06,000
So I don't know.

856
00:38:06,000 --> 00:38:08,160
I mean, obviously it's very difficult to predict

857
00:38:08,160 --> 00:38:10,320
what direction the stuff goes in over time,

858
00:38:10,320 --> 00:38:14,160
which is why I don't think anyone should be dogmatic

859
00:38:14,160 --> 00:38:17,360
about how they plan to develop it or what they plan to do.

860
00:38:17,360 --> 00:38:19,680
I think you want to kind of look at like each release.

861
00:38:19,680 --> 00:38:21,920
You know, it's like, we're obviously very pro open source,

862
00:38:21,920 --> 00:38:23,520
but I haven't committed that we're going to like release

863
00:38:23,520 --> 00:38:24,880
every single thing that we do.

864
00:38:24,880 --> 00:38:29,120
But it's basically, I'm just generally very inclined

865
00:38:29,120 --> 00:38:30,320
to thinking that open sourcing it

866
00:38:30,320 --> 00:38:32,640
is going to be good for the community

867
00:38:32,640 --> 00:38:33,840
and also good for us, right?

868
00:38:33,840 --> 00:38:36,320
Because we'll benefit from the innovations.

869
00:38:36,560 --> 00:38:40,080
But if at some point like there's some qualitative change

870
00:38:40,080 --> 00:38:42,000
in what the thing is capable of,

871
00:38:42,000 --> 00:38:44,720
and we feel like it's just not responsible to open source it,

872
00:38:44,720 --> 00:38:48,640
then we won't, but so I don't know.

873
00:38:48,640 --> 00:38:50,640
It's all very difficult to predict.

874
00:38:50,640 --> 00:38:51,600
Yeah.

875
00:38:51,600 --> 00:38:53,520
What is a kind of qualitative change,

876
00:38:53,520 --> 00:38:54,800
like a specific thing?

877
00:38:54,800 --> 00:38:57,920
You're training lamify, lamaphore, and you've seen this,

878
00:38:57,920 --> 00:38:59,360
and like, you know what?

879
00:38:59,360 --> 00:39:00,560
I'm not sure about open sourcing it.

880
00:39:00,560 --> 00:39:06,800
I think that that, it's a little hard to answer that

881
00:39:06,800 --> 00:39:10,160
in the abstract because there are negative behaviors

882
00:39:10,800 --> 00:39:14,320
that any product can exhibit that as long as you can mitigate it,

883
00:39:14,960 --> 00:39:17,440
it's like, it's okay, right?

884
00:39:17,440 --> 00:39:20,640
So, I mean, there's bad things about social media

885
00:39:20,640 --> 00:39:22,080
that we work to mitigate, right?

886
00:39:22,080 --> 00:39:24,000
There's bad things about llama two

887
00:39:24,000 --> 00:39:26,000
that we spend a lot of time trying to make sure

888
00:39:26,000 --> 00:39:29,440
that it's not like, you know, helping people commit violent acts

889
00:39:29,440 --> 00:39:30,400
or things like that, right?

890
00:39:30,400 --> 00:39:34,480
I mean, that doesn't mean that it's like a kind of autonomous

891
00:39:34,480 --> 00:39:35,760
or intelligent agent.

892
00:39:35,760 --> 00:39:37,840
It just means that it's learned a lot about the world,

893
00:39:37,840 --> 00:39:39,280
and it can answer a set of questions

894
00:39:39,280 --> 00:39:41,840
that we think it would be unhelpful for it to answer.

895
00:39:43,440 --> 00:39:47,200
So, I don't know.

896
00:39:47,200 --> 00:39:51,680
I think the question isn't really what behaviors would it show.

897
00:39:51,680 --> 00:39:54,000
It's what things would we not be able to mitigate

898
00:39:54,000 --> 00:39:58,480
after it shows that, and I don't know.

899
00:39:59,120 --> 00:40:01,520
I think that there's so many ways

900
00:40:01,520 --> 00:40:03,440
in which something can be good or bad

901
00:40:03,440 --> 00:40:05,360
that it's hard to actually enumerate them all up front.

902
00:40:05,360 --> 00:40:10,320
If you even look at what we've had to deal with in social media

903
00:40:10,320 --> 00:40:12,880
and the different types of harms, we've basically gotten to.

904
00:40:12,880 --> 00:40:16,240
It's like, there's like 18 or 19 categories of harmful things

905
00:40:16,240 --> 00:40:19,840
that people do, and we've basically built AI systems

906
00:40:19,840 --> 00:40:22,080
to try to go identify what those things are

907
00:40:22,080 --> 00:40:23,520
that people are doing and try to make sure

908
00:40:23,520 --> 00:40:26,400
that that doesn't happen on our network as much as possible.

909
00:40:26,400 --> 00:40:28,400
So, yeah, I think you can...

910
00:40:28,400 --> 00:40:30,000
Over time, I think you'll be able to break down

911
00:40:31,600 --> 00:40:33,200
this into more of a taxonomy, too,

912
00:40:33,200 --> 00:40:36,000
and I think this is a thing that we spend time researching, too,

913
00:40:36,000 --> 00:40:37,360
because we want to make sure that we understand that.

914
00:40:38,240 --> 00:40:40,080
So, one of the things I asked Mark

915
00:40:40,080 --> 00:40:44,000
is what industrial-scale use of LLMs would look like.

916
00:40:44,000 --> 00:40:46,000
You see this in previous technological revolutions

917
00:40:46,000 --> 00:40:48,400
where, at first, they're thinking in a very small-scale way

918
00:40:48,400 --> 00:40:49,440
about what's enabled,

919
00:40:49,440 --> 00:40:52,080
and I think that's what chatbots might be for LLMs.

920
00:40:52,080 --> 00:40:53,920
And I think the large-scale use case

921
00:40:53,920 --> 00:40:56,480
might look something like what V7 Go is.

922
00:40:56,480 --> 00:40:58,400
And, by the way, it's made by V7 Labs

923
00:40:58,400 --> 00:40:59,680
who's sponsoring this episode.

924
00:41:00,240 --> 00:41:02,160
So, it's like a spreadsheet.

925
00:41:02,160 --> 00:41:06,160
You put in raw information, like documents, images, whatever,

926
00:41:06,160 --> 00:41:07,600
and they become rows,

927
00:41:07,600 --> 00:41:11,120
and the columns are populated by an LLM of your choice.

928
00:41:11,120 --> 00:41:13,680
And, in fact, I used it to prepare for Mark,

929
00:41:13,680 --> 00:41:16,160
so I fed in a bunch of blog posts and papers

930
00:41:16,160 --> 00:41:17,920
from Metas AI Research,

931
00:41:17,920 --> 00:41:20,080
and, as you can see, if you're on YouTube,

932
00:41:20,080 --> 00:41:23,280
it summarizes and extracts exactly the information I want

933
00:41:23,360 --> 00:41:24,320
as columns.

934
00:41:24,320 --> 00:41:26,480
And, obviously, mine is a small use case,

935
00:41:26,480 --> 00:41:29,440
but you can imagine, for example, a company like FedEx

936
00:41:29,440 --> 00:41:32,080
has to process half a million documents a day.

937
00:41:32,080 --> 00:41:34,080
Obviously, a chatbot can't do that.

938
00:41:34,080 --> 00:41:35,280
A spreadsheet can,

939
00:41:35,280 --> 00:41:37,440
because this is just like a fire hose of intelligence

940
00:41:37,440 --> 00:41:38,480
in there, right?

941
00:41:38,480 --> 00:41:40,160
Anyways, you can learn more about them

942
00:41:40,160 --> 00:41:42,560
at v7labs.com slash go,

943
00:41:42,560 --> 00:41:44,160
or the link in the description.

944
00:41:44,160 --> 00:41:45,280
Back to Mark.

945
00:41:45,280 --> 00:41:45,840
Yeah.

946
00:41:45,840 --> 00:41:48,480
Like, it seems to me it would be a good idea.

947
00:41:48,480 --> 00:41:50,000
I would be disappointed in a future

948
00:41:50,000 --> 00:41:51,680
where AI systems aren't broadly deployed

949
00:41:51,680 --> 00:41:53,040
and everybody doesn't have access to them.

950
00:41:54,000 --> 00:41:54,960
At the same time,

951
00:41:54,960 --> 00:41:57,120
I want to better understand the mitigations,

952
00:41:58,000 --> 00:42:00,480
because if the mitigation is the fine-tuning,

953
00:42:00,480 --> 00:42:02,320
well, the whole thing about open weights

954
00:42:02,320 --> 00:42:05,680
is that you can then remove the fine-tuning,

955
00:42:05,680 --> 00:42:07,920
which is often superficial on top of these capabilities.

956
00:42:07,920 --> 00:42:10,240
Like, if it's like talking on Slack

957
00:42:10,240 --> 00:42:12,000
with a biology researcher,

958
00:42:12,000 --> 00:42:14,000
and again, I think models are very far from this.

959
00:42:14,000 --> 00:42:15,120
Right now, they're like Google search,

960
00:42:16,160 --> 00:42:17,920
but I can show them my Petri disk

961
00:42:17,920 --> 00:42:18,560
and they can next lane.

962
00:42:18,560 --> 00:42:21,680
Like, here's why your smallpox sample didn't grow.

963
00:42:21,680 --> 00:42:22,400
Here's what to change.

964
00:42:23,600 --> 00:42:24,720
How do you mitigate that?

965
00:42:24,720 --> 00:42:27,040
Because somebody can just fine-tune that in there, right?

966
00:42:27,840 --> 00:42:30,640
Yeah. I mean, that's true.

967
00:42:30,640 --> 00:42:32,800
I think a lot of people will basically use

968
00:42:32,800 --> 00:42:34,880
the off-the-shelf model,

969
00:42:34,880 --> 00:42:38,320
and some people who have basically bad faith

970
00:42:38,320 --> 00:42:40,560
are going to try to strip out all the bad stuff,

971
00:42:40,560 --> 00:42:41,600
so I do think that that's an issue.

972
00:42:44,720 --> 00:42:46,720
The flip side of this is that,

973
00:42:46,720 --> 00:42:47,920
and this is one of the reasons

974
00:42:47,920 --> 00:42:51,280
why I'm kind of philosophically so pro-open source,

975
00:42:52,160 --> 00:42:56,720
is I do think that a concentration of AI in the future

976
00:42:57,840 --> 00:43:00,240
has the potential to be as dangerous

977
00:43:00,880 --> 00:43:03,760
as kind of it being widespread.

978
00:43:03,760 --> 00:43:04,560
So, I think a lot of people,

979
00:43:05,360 --> 00:43:07,120
they think about the questions of,

980
00:43:07,120 --> 00:43:08,400
okay, well, if we can do this stuff,

981
00:43:08,400 --> 00:43:10,080
is it bad for it to be out wild?

982
00:43:10,080 --> 00:43:12,640
Like, just kind of widely available.

983
00:43:14,800 --> 00:43:16,720
I think another version of this is like,

984
00:43:16,720 --> 00:43:19,360
okay, well, it's probably also pretty bad

985
00:43:20,000 --> 00:43:24,800
for one institution to have an AI

986
00:43:24,800 --> 00:43:27,920
that is way more powerful than everyone else's AI, right?

987
00:43:27,920 --> 00:43:29,680
So, if you look at, like, I guess,

988
00:43:29,680 --> 00:43:32,240
one security analogy that I think of is,

989
00:43:34,240 --> 00:43:38,160
you know, it doesn't take AI to basically,

990
00:43:38,160 --> 00:43:40,560
okay, there's security holes in so many different things,

991
00:43:41,200 --> 00:43:44,720
and if you could travel back in time a year or two years,

992
00:43:44,720 --> 00:43:46,880
right, it's like, that's not AI,

993
00:43:46,880 --> 00:43:48,240
it's like you just, let's say you just have,

994
00:43:48,320 --> 00:43:50,240
like, one year or two years more knowledge

995
00:43:50,240 --> 00:43:51,520
of the security holes,

996
00:43:51,520 --> 00:43:53,760
it's pretty much hack into, like, any system, right?

997
00:43:53,760 --> 00:43:56,000
So, it's not that far-fetched to believe

998
00:43:56,800 --> 00:44:00,240
that a very intelligent AI would probably be able

999
00:44:00,240 --> 00:44:03,520
to identify some holes and basically be,

1000
00:44:03,520 --> 00:44:04,800
like, a human who could potentially

1001
00:44:04,800 --> 00:44:06,000
go back in time a year or two

1002
00:44:06,000 --> 00:44:07,200
and compromise all these systems.

1003
00:44:07,200 --> 00:44:09,840
Okay, so how have we dealt with that as a society?

1004
00:44:09,840 --> 00:44:13,200
Well, one big part is open-source software

1005
00:44:13,200 --> 00:44:14,720
that makes it so that when improvements

1006
00:44:14,720 --> 00:44:16,080
are made to the software,

1007
00:44:16,080 --> 00:44:17,680
it doesn't just kind of get stuck

1008
00:44:17,680 --> 00:44:19,440
in one company's products,

1009
00:44:19,440 --> 00:44:22,080
but it can kind of be broadly deployed

1010
00:44:22,080 --> 00:44:23,440
to a lot of different systems,

1011
00:44:23,440 --> 00:44:26,080
whether it's banks or hospitals or government stuff,

1012
00:44:26,080 --> 00:44:28,000
and, like, just everyone can kind of,

1013
00:44:28,000 --> 00:44:29,520
like, as the software gets hardened,

1014
00:44:30,160 --> 00:44:31,840
which happens because more people can see it

1015
00:44:31,840 --> 00:44:33,040
and more people can bang on it,

1016
00:44:33,680 --> 00:44:35,680
and there are standards on how this stuff works,

1017
00:44:37,040 --> 00:44:39,920
the world can kind of get upgraded together pretty quickly.

1018
00:44:39,920 --> 00:44:43,680
And I kind of think that a world where AI

1019
00:44:43,680 --> 00:44:45,440
is very widely deployed

1020
00:44:45,760 --> 00:44:49,680
in a way where it's gotten hardened progressively over time

1021
00:44:50,880 --> 00:44:54,240
is one where all the different systems will be in check

1022
00:44:54,240 --> 00:44:56,880
in a way that seems like it is fundamentally

1023
00:44:56,880 --> 00:44:58,000
more healthy to me

1024
00:44:58,000 --> 00:45:00,160
than one where this is more concentrated.

1025
00:45:00,160 --> 00:45:02,960
So there are risks on all sides,

1026
00:45:02,960 --> 00:45:07,440
but I think that that's one risk that I think people,

1027
00:45:07,440 --> 00:45:09,120
I don't hear them talking about quite as much.

1028
00:45:09,120 --> 00:45:11,520
I think, like, there's sort of the risk of, like,

1029
00:45:11,520 --> 00:45:13,520
okay, well, what if the AI system does something bad?

1030
00:45:14,000 --> 00:45:18,400
I am more, like, you know, I stay up at night more worrying,

1031
00:45:18,400 --> 00:45:22,240
well, what if, like, some actor that, whatever.

1032
00:45:22,240 --> 00:45:23,360
It's like, from wherever you sit,

1033
00:45:23,360 --> 00:45:25,360
there's going to be some actor who you don't trust

1034
00:45:25,360 --> 00:45:27,520
if they're the ones who have, like, the super strong AI,

1035
00:45:27,520 --> 00:45:29,360
whether it's some, like, other government

1036
00:45:29,360 --> 00:45:32,880
that is sort of, like, an opponent of our country

1037
00:45:32,880 --> 00:45:35,680
or some company that you don't trust or whatever it is.

1038
00:45:37,680 --> 00:45:43,120
Like, I think that that's potentially a much bigger risk

1039
00:45:43,200 --> 00:45:47,360
as in they could, like, overthrow our government

1040
00:45:47,360 --> 00:45:50,080
because they have a weapon that, like, nobody else has.

1041
00:45:50,080 --> 00:45:51,600
Cause a lot of mayhem.

1042
00:45:51,600 --> 00:45:54,880
Right, it's, I think it's, like, I mean,

1043
00:45:54,880 --> 00:45:57,360
I think the intuition is that this stuff ends up being

1044
00:45:57,360 --> 00:46:00,960
pretty kind of important and valuable

1045
00:46:00,960 --> 00:46:03,920
for both kind of economic and kind of security

1046
00:46:03,920 --> 00:46:04,640
and other things.

1047
00:46:04,640 --> 00:46:08,000
And I don't know, I just think, yeah, if, like,

1048
00:46:08,000 --> 00:46:10,880
if someone who you don't trust or is an adversary of you

1049
00:46:10,960 --> 00:46:12,960
gets something that is more powerful,

1050
00:46:12,960 --> 00:46:15,120
then I think that that could be an issue.

1051
00:46:15,120 --> 00:46:17,360
And I think probably the best way to mitigate that

1052
00:46:17,360 --> 00:46:20,400
is to have good open source AI

1053
00:46:20,400 --> 00:46:22,640
that basically becomes the standard

1054
00:46:23,440 --> 00:46:26,160
and in a lot of ways kind of can become the leader.

1055
00:46:26,160 --> 00:46:30,080
And in that way, it just ensures that it's a much more

1056
00:46:30,080 --> 00:46:32,960
kind of even and balanced playing field.

1057
00:46:32,960 --> 00:46:34,400
Yeah, that seems plausible to me.

1058
00:46:34,400 --> 00:46:36,800
And if that works out, that would be the future I prefer.

1059
00:46:37,920 --> 00:46:40,560
I guess I want to understand, like, mechanistically

1060
00:46:40,640 --> 00:46:44,080
how if somebody was going to cause mayhem with AI systems,

1061
00:46:44,080 --> 00:46:46,480
how the fact that there are other open source systems

1062
00:46:46,480 --> 00:46:48,080
in the world prevents that?

1063
00:46:48,080 --> 00:46:49,920
Like the specific example of, like,

1064
00:46:49,920 --> 00:46:51,200
somebody coming with a bio weapon,

1065
00:46:51,840 --> 00:46:53,280
is it just that we'll do a bunch of, like,

1066
00:46:53,280 --> 00:46:55,040
R&D in the rest of the world to, like,

1067
00:46:55,040 --> 00:46:56,400
figure out vaccines really fast?

1068
00:46:56,400 --> 00:46:57,280
Like, what's happening?

1069
00:46:57,280 --> 00:46:58,480
Would you take, like, the computer,

1070
00:46:58,480 --> 00:47:00,640
the security one that I was talking about?

1071
00:47:00,640 --> 00:47:02,400
I think someone with a weaker AI

1072
00:47:02,400 --> 00:47:04,640
trying to hack into a system that is, like,

1073
00:47:04,640 --> 00:47:07,040
protected by a stronger AI will succeed less.

1074
00:47:07,760 --> 00:47:10,560
Right, so I think that that's, I mean,

1075
00:47:10,560 --> 00:47:11,760
that's, like, in terms of software security.

1076
00:47:11,760 --> 00:47:13,360
How do you know everything in the world is like that?

1077
00:47:13,360 --> 00:47:15,040
Like, what if bio weapons aren't like that?

1078
00:47:16,080 --> 00:47:17,920
No, I mean, I don't know that everything

1079
00:47:17,920 --> 00:47:18,800
in the world is like that.

1080
00:47:21,520 --> 00:47:24,160
I think that that's, I guess,

1081
00:47:24,160 --> 00:47:26,080
one of the, bio weapons are one of the areas

1082
00:47:26,080 --> 00:47:28,320
where I think the people who are most worried

1083
00:47:28,320 --> 00:47:29,920
about this stuff are focused.

1084
00:47:29,920 --> 00:47:33,200
And I think that that's, I think that makes

1085
00:47:33,200 --> 00:47:34,320
a lot of sense to think about that.

1086
00:47:35,200 --> 00:47:38,800
The, and I think that there are certain mitigations.

1087
00:47:38,800 --> 00:47:41,600
You can try to not train certain knowledge

1088
00:47:41,600 --> 00:47:42,880
into the model, right?

1089
00:47:42,880 --> 00:47:45,760
There's different things, but, yeah,

1090
00:47:45,760 --> 00:47:48,400
I mean, it's some level, I mean,

1091
00:47:48,400 --> 00:47:50,880
if you get a sufficiently bad actor

1092
00:47:50,880 --> 00:47:54,560
and you don't have other AI that can sort of balance them

1093
00:47:54,560 --> 00:47:57,840
and understand what's going on and what the threats are,

1094
00:47:57,840 --> 00:48:00,320
then that could be a risk.

1095
00:48:00,320 --> 00:48:01,440
So I think that that's one of the things

1096
00:48:01,440 --> 00:48:02,560
that we need to watch out for.

1097
00:48:02,960 --> 00:48:06,640
Is there something you could see

1098
00:48:06,640 --> 00:48:08,000
in the deployment of these systems

1099
00:48:08,000 --> 00:48:12,160
where you observe like you're training Lama 4

1100
00:48:12,160 --> 00:48:13,680
and it's like, it lied to you

1101
00:48:13,680 --> 00:48:15,520
because you thought you were noticing or something.

1102
00:48:15,520 --> 00:48:18,160
And you're like, whoa, what's going on here?

1103
00:48:18,160 --> 00:48:20,480
Not that this is probably not likely

1104
00:48:20,480 --> 00:48:21,440
with the Lama 4.0 system,

1105
00:48:21,440 --> 00:48:23,360
but is there something you can imagine like that

1106
00:48:23,360 --> 00:48:26,480
where you'd be really concerned about deceptiveness

1107
00:48:26,480 --> 00:48:28,960
and if billions of copies of things are out in the wild?

1108
00:48:29,840 --> 00:48:34,400
Yeah, I mean, I think that that's not necessarily,

1109
00:48:34,400 --> 00:48:37,680
I mean, right now, we see a lot of hallucinations, right?

1110
00:48:37,680 --> 00:48:39,360
So I think it's more that.

1111
00:48:41,200 --> 00:48:42,640
I think it's an interesting question

1112
00:48:42,640 --> 00:48:43,680
how you would tell the difference

1113
00:48:43,680 --> 00:48:46,320
between a hallucination and deception.

1114
00:48:46,320 --> 00:48:47,520
But yeah, I mean, look, I mean,

1115
00:48:47,520 --> 00:48:49,920
I think there's a lot of risks and things to think about.

1116
00:48:49,920 --> 00:48:55,120
The flip side of all this is that there are also a lot of,

1117
00:48:56,080 --> 00:48:59,040
I try to, in running our company at least,

1118
00:49:00,000 --> 00:49:05,120
balance what I think of as these longer term theoretical risks

1119
00:49:07,440 --> 00:49:10,640
with what I actually think are quite real risks that exist today.

1120
00:49:10,640 --> 00:49:14,400
So like when you talk about deception,

1121
00:49:14,400 --> 00:49:16,080
the form of that that I worry about most

1122
00:49:16,080 --> 00:49:18,560
is people using this to generate misinformation

1123
00:49:18,560 --> 00:49:19,920
and then like pump that through

1124
00:49:19,920 --> 00:49:21,840
whether it's our networks or others.

1125
00:49:21,920 --> 00:49:25,680
So the way that we've basically combated

1126
00:49:25,680 --> 00:49:27,840
a lot of this type of harmful content

1127
00:49:28,400 --> 00:49:30,000
is by building AI systems

1128
00:49:30,000 --> 00:49:32,240
that are smarter than the adversarial ones.

1129
00:49:32,240 --> 00:49:33,920
And I guess this is part of,

1130
00:49:33,920 --> 00:49:35,920
this kind of informs part of my theory on this, right?

1131
00:49:35,920 --> 00:49:38,000
Is if you look at like the different types of harm

1132
00:49:38,000 --> 00:49:41,680
that people do or try to do through social networks,

1133
00:49:44,320 --> 00:49:46,800
there are ones that are not very adversarial.

1134
00:49:46,800 --> 00:49:51,520
So for example, like hate speech,

1135
00:49:51,520 --> 00:49:53,920
I would say is not super adversarial

1136
00:49:53,920 --> 00:49:56,240
in the sense that like people aren't getting

1137
00:49:57,280 --> 00:49:59,440
better at being racist, right?

1138
00:49:59,440 --> 00:50:01,920
They're just like, it's, you just like, okay,

1139
00:50:01,920 --> 00:50:04,560
if you kind of, that's one where I think the AIs

1140
00:50:04,560 --> 00:50:07,680
are generally just getting way more sophisticated,

1141
00:50:07,680 --> 00:50:09,600
faster than people are at those issues.

1142
00:50:09,600 --> 00:50:11,360
So we have, and we have issues both ways.

1143
00:50:11,360 --> 00:50:14,240
It's like people do bad things

1144
00:50:14,240 --> 00:50:16,480
that whether they're trying to incite violence or something.

1145
00:50:18,240 --> 00:50:19,840
But we also have a lot of false positives, right?

1146
00:50:19,920 --> 00:50:22,240
So where we basically censor stuff that we shouldn't,

1147
00:50:22,240 --> 00:50:25,280
and I think understandably make a lot of people annoyed.

1148
00:50:25,280 --> 00:50:27,920
So I think having an AI that just gets increasingly

1149
00:50:27,920 --> 00:50:30,560
precise on that, that's gonna be good over time.

1150
00:50:30,560 --> 00:50:31,680
But let me give you another example,

1151
00:50:31,680 --> 00:50:34,320
which is like nation states trying to interfere in elections.

1152
00:50:34,880 --> 00:50:37,200
That's an example where they're absolutely,

1153
00:50:37,200 --> 00:50:38,800
they have cutting edge technology

1154
00:50:38,800 --> 00:50:41,440
and absolutely get better each year.

1155
00:50:41,440 --> 00:50:44,640
So we block some technique, they learn what we did,

1156
00:50:44,640 --> 00:50:46,640
they come at us with a different technique, right?

1157
00:50:46,640 --> 00:50:52,240
It's not like a person trying to say mean things, right?

1158
00:50:52,240 --> 00:50:55,120
It's like, they're basically, they have a goal,

1159
00:50:55,120 --> 00:50:56,960
they're sophisticated, they have a lot of technology.

1160
00:50:58,320 --> 00:51:00,320
In those cases, I still think the ability

1161
00:51:00,320 --> 00:51:04,800
to kind of have RAI systems grow in sophistication

1162
00:51:04,800 --> 00:51:07,680
at a faster rate than theirs have, it's an arms race,

1163
00:51:07,680 --> 00:51:10,640
but I think we're at least currently winning that arms race.

1164
00:51:12,240 --> 00:51:13,920
So I don't know, I think that that's,

1165
00:51:13,920 --> 00:51:15,040
but this is like a lot of the stuff

1166
00:51:15,040 --> 00:51:17,200
that I spend time thinking about is like, okay,

1167
00:51:18,240 --> 00:51:22,240
yes, it is possible that whether it's llama four

1168
00:51:22,240 --> 00:51:24,240
or llama five or llama six, yeah,

1169
00:51:24,240 --> 00:51:26,480
we need to think about what behaviors we're observing

1170
00:51:26,480 --> 00:51:27,200
and it's not just us.

1171
00:51:27,200 --> 00:51:28,800
And part of the reason why you make this open source

1172
00:51:28,800 --> 00:51:30,960
is that there are a lot of other people who study this too.

1173
00:51:30,960 --> 00:51:34,240
So yeah, we wanna see what other people are observing,

1174
00:51:34,240 --> 00:51:36,720
what we're observing, what we can mitigate,

1175
00:51:36,720 --> 00:51:38,480
and then we'll make our assessment

1176
00:51:38,480 --> 00:51:41,680
on whether we can make it open source.

1177
00:51:41,680 --> 00:51:44,960
But I think for the foreseeable future,

1178
00:51:44,960 --> 00:51:47,200
I'm optimistic we will be able to.

1179
00:51:47,200 --> 00:51:51,040
And in the near term, I don't wanna take our eye off the ball

1180
00:51:51,040 --> 00:51:52,960
of what our actual bad things

1181
00:51:52,960 --> 00:51:54,960
that people are trying to use the models for today,

1182
00:51:54,960 --> 00:51:56,160
even if they're not existential,

1183
00:51:56,160 --> 00:52:00,560
but they're like pretty bad kind of day-to-day harms

1184
00:52:00,560 --> 00:52:03,120
that we are familiar with in running our services.

1185
00:52:04,880 --> 00:52:06,400
That's actually a lot of what we have to,

1186
00:52:06,400 --> 00:52:07,680
I think, spend our time on as well.

1187
00:52:07,680 --> 00:52:08,720
Yeah, yeah.

1188
00:52:08,720 --> 00:52:11,440
Actually, I found the synthetic data thing really curious.

1189
00:52:12,640 --> 00:52:14,640
I'm actually interested in why you don't think,

1190
00:52:15,440 --> 00:52:16,720
like current models, it makes sense

1191
00:52:16,720 --> 00:52:18,080
why there might be an asymptote

1192
00:52:18,080 --> 00:52:20,480
with just doing the synthetic data again and again.

1193
00:52:20,480 --> 00:52:22,320
If it gets smarter and uses the kind of techniques

1194
00:52:22,320 --> 00:52:24,320
you talk about in the paper or the blog post

1195
00:52:24,320 --> 00:52:26,880
that's coming out on the day this will be released

1196
00:52:26,880 --> 00:52:30,080
where it goes to the thought chain

1197
00:52:30,080 --> 00:52:32,480
that is the most correct.

1198
00:52:33,040 --> 00:52:35,680
Why this wouldn't like lead to a loop

1199
00:52:35,680 --> 00:52:37,200
that, of course, it wouldn't be overnight,

1200
00:52:37,200 --> 00:52:38,800
but over many months or years of training,

1201
00:52:38,800 --> 00:52:40,320
potentially, with a smarter model,

1202
00:52:40,320 --> 00:52:41,920
it gets smarter, makes better output,

1203
00:52:41,920 --> 00:52:43,040
gets smarter, and so forth.

1204
00:52:45,280 --> 00:52:47,200
Well, I think it could within the parameter

1205
00:52:47,200 --> 00:52:49,520
of whatever the model architecture is.

1206
00:52:49,520 --> 00:52:53,920
It's just that at some level, I don't know,

1207
00:52:54,560 --> 00:52:58,640
I think today is eight billion parameter models.

1208
00:52:59,280 --> 00:53:02,560
I just don't think you're going to be able to get to be as good

1209
00:53:02,560 --> 00:53:06,160
as the state-of-the-art multi-hundred billion

1210
00:53:06,240 --> 00:53:08,720
parameter models that are incorporating new research

1211
00:53:08,720 --> 00:53:10,080
into the architecture itself.

1212
00:53:12,640 --> 00:53:14,160
But those will be open source as well, right?

1213
00:53:14,880 --> 00:53:16,160
Well, yeah, but I think that that's,

1214
00:53:17,200 --> 00:53:20,880
I mean, subject to all the questions

1215
00:53:20,880 --> 00:53:21,760
that we just talked about.

1216
00:53:21,760 --> 00:53:23,920
Yes, I mean, we would hope that that'll be the case,

1217
00:53:23,920 --> 00:53:26,880
but I think that at each point, I don't know,

1218
00:53:26,880 --> 00:53:29,360
it's like when you're building software,

1219
00:53:29,360 --> 00:53:31,920
there's like a ton of stuff that you can do with software,

1220
00:53:31,920 --> 00:53:33,840
but then at some level, you're constrained

1221
00:53:33,840 --> 00:53:35,920
by the chips that it's running on.

1222
00:53:36,240 --> 00:53:39,600
Right? So there are always going to be different

1223
00:53:40,400 --> 00:53:41,680
physical constraints.

1224
00:53:41,680 --> 00:53:44,960
And it's like how big are the models is going to be constrained

1225
00:53:44,960 --> 00:53:49,840
by how much energy you can get and use for inference.

1226
00:53:50,800 --> 00:53:56,160
So I guess I'm simultaneously very optimistic

1227
00:53:56,160 --> 00:53:58,160
that this stuff will continue to improve quickly.

1228
00:53:58,960 --> 00:54:05,600
And also a little more measured than I think

1229
00:54:05,680 --> 00:54:09,520
some people are about kind of it's,

1230
00:54:10,480 --> 00:54:12,800
I just don't think the runaway case

1231
00:54:12,800 --> 00:54:15,040
is like a particularly likely one.

1232
00:54:16,080 --> 00:54:18,320
I think it makes sense to keep your options open.

1233
00:54:18,320 --> 00:54:19,440
Like there's so much we don't know.

1234
00:54:20,480 --> 00:54:22,080
There's a case in which like it's really important

1235
00:54:22,080 --> 00:54:23,040
to keep the balance of power.

1236
00:54:23,040 --> 00:54:25,200
So when nobody becomes like a technology or a dictator,

1237
00:54:25,200 --> 00:54:26,480
there's a case in which like,

1238
00:54:26,480 --> 00:54:28,640
you don't want to open source the architecture

1239
00:54:28,640 --> 00:54:32,240
because like China can use it to catch up to America's AIs

1240
00:54:32,240 --> 00:54:33,840
and like there is an intelligence explosion

1241
00:54:33,920 --> 00:54:34,800
and they like win that.

1242
00:54:35,840 --> 00:54:37,120
Yeah, a lot of things are impossible.

1243
00:54:37,120 --> 00:54:38,640
Just like keeping your options open,

1244
00:54:38,640 --> 00:54:40,800
considering all of them seems reasonable.

1245
00:54:40,800 --> 00:54:40,960
Yeah.

1246
00:54:42,480 --> 00:54:43,520
Let's talk about some other things.

1247
00:54:43,520 --> 00:54:44,080
Go for it.

1248
00:54:44,080 --> 00:54:48,560
Okay. Metaverse, what time period in human history

1249
00:54:48,560 --> 00:54:50,640
would you be most interested in going into?

1250
00:54:50,640 --> 00:54:52,640
A 100,000 BCE to now.

1251
00:54:52,640 --> 00:54:53,680
You just want to see what it was like.

1252
00:54:53,680 --> 00:54:54,720
Well, that's through the past.

1253
00:54:54,720 --> 00:54:55,120
Huh?

1254
00:54:55,120 --> 00:54:55,840
It has to be the past.

1255
00:54:55,840 --> 00:54:56,800
Oh yeah, it has to be the past.

1256
00:55:01,760 --> 00:55:02,400
I don't know.

1257
00:55:02,400 --> 00:55:04,400
I mean, I have the periods of time that I'm interested.

1258
00:55:04,400 --> 00:55:06,240
I mean, I'm really interested in American history

1259
00:55:06,240 --> 00:55:10,320
and classical history and I'm really interested

1260
00:55:10,320 --> 00:55:11,520
in the history of science too.

1261
00:55:11,520 --> 00:55:15,920
So I actually think seeing and trying to understand more

1262
00:55:18,720 --> 00:55:20,960
about how some of the big advances came about.

1263
00:55:20,960 --> 00:55:24,400
I mean, all we have are like somewhat limited writings

1264
00:55:24,400 --> 00:55:25,920
about some of that stuff.

1265
00:55:25,920 --> 00:55:27,520
I'm not sure the metaverse is going to let you do that

1266
00:55:27,520 --> 00:55:29,440
because I mean, it's, you know, we can't,

1267
00:55:29,520 --> 00:55:32,640
it's going to be hard to kind of go back in time

1268
00:55:32,640 --> 00:55:34,560
for things that we don't have records of.

1269
00:55:34,560 --> 00:55:39,040
But I'm actually not sure that going back in time

1270
00:55:39,040 --> 00:55:41,520
is going to be that important thing for them.

1271
00:55:41,520 --> 00:55:43,440
I mean, I think it's going to be cool for history classes

1272
00:55:43,440 --> 00:55:46,960
and stuff, but that's probably not the use case

1273
00:55:46,960 --> 00:55:49,600
that I'm most excited about for the metaverse overall.

1274
00:55:49,600 --> 00:55:52,960
I mean, the main thing is just the ability

1275
00:55:52,960 --> 00:55:55,280
to feel present with people no matter where you are.

1276
00:55:55,280 --> 00:55:56,640
I think that's going to be killer.

1277
00:55:56,640 --> 00:56:01,360
I mean, there's, I mean, in the AI conversation

1278
00:56:01,360 --> 00:56:04,560
that we were having, I mean, it's, you know,

1279
00:56:04,560 --> 00:56:06,640
so much of it is about physical constraints

1280
00:56:06,640 --> 00:56:09,120
that kind of underlie all of this, right?

1281
00:56:09,120 --> 00:56:12,320
And you want to move, I mean, one lesson of technology

1282
00:56:12,320 --> 00:56:15,600
is you want to move things from the physical constraint realm

1283
00:56:15,600 --> 00:56:17,200
into software as much as possible

1284
00:56:17,200 --> 00:56:21,120
because software is so much easier to build and evolve.

1285
00:56:21,120 --> 00:56:22,800
And like you can democratize it more

1286
00:56:22,800 --> 00:56:25,520
because like not everyone is going to have a data center,

1287
00:56:25,520 --> 00:56:28,800
but like a lot of people can kind of write code

1288
00:56:28,800 --> 00:56:30,880
and take open source code and modify it.

1289
00:56:33,280 --> 00:56:35,840
The metaverse version of this is I think

1290
00:56:35,840 --> 00:56:38,480
enabling realistic digital presence

1291
00:56:39,600 --> 00:56:43,760
is going to be just an absolutely huge difference

1292
00:56:43,760 --> 00:56:48,000
for making it so that people don't feel

1293
00:56:48,000 --> 00:56:49,840
like they have to physically be together

1294
00:56:49,840 --> 00:56:50,880
for as many things.

1295
00:56:51,520 --> 00:56:52,720
Now, I mean, I think that there are going to be things

1296
00:56:52,720 --> 00:56:54,240
that are better about being physically together.

1297
00:56:56,240 --> 00:56:58,000
So it's not, I mean, these things aren't binary,

1298
00:56:58,000 --> 00:56:58,960
it's not going to be like, okay,

1299
00:56:58,960 --> 00:57:00,960
now it's, you don't need to do that anymore.

1300
00:57:00,960 --> 00:57:07,200
But overall, I mean, I think that this,

1301
00:57:07,200 --> 00:57:09,760
it's just going to be really powerful for socializing,

1302
00:57:09,760 --> 00:57:12,640
for feeling connected with people, for working,

1303
00:57:13,600 --> 00:57:18,000
for, I don't know, parts of industry, for medicine,

1304
00:57:18,000 --> 00:57:19,920
for like so many things.

1305
00:57:19,920 --> 00:57:21,040
I want to go back to something you said

1306
00:57:21,040 --> 00:57:22,240
at the beginning of the conversation

1307
00:57:22,240 --> 00:57:24,960
where you didn't sell the company for a billion dollars

1308
00:57:24,960 --> 00:57:26,640
and like the metaverse, you knew we were going to do this

1309
00:57:26,640 --> 00:57:29,520
even though the market was hammering you for it.

1310
00:57:29,520 --> 00:57:30,960
And then I'm actually curious,

1311
00:57:30,960 --> 00:57:32,720
like what is the source of that edge?

1312
00:57:32,720 --> 00:57:35,280
And you said like, oh, values, I have this intuition,

1313
00:57:35,280 --> 00:57:36,800
but like everybody says that, right?

1314
00:57:37,360 --> 00:57:39,200
If you had to say something that's specific to you,

1315
00:57:39,200 --> 00:57:41,200
what is, how would you express what that is?

1316
00:57:41,200 --> 00:57:43,280
Like why were you so convinced about the metaverse?

1317
00:57:49,920 --> 00:57:51,440
Well, I think that those are different questions.

1318
00:57:51,440 --> 00:57:56,560
So what, I mean, what are the things that kind of power me?

1319
00:57:58,640 --> 00:57:59,920
I think we've talked about a bunch of things.

1320
00:57:59,920 --> 00:58:03,200
So it's, I mean, I just really like building things.

1321
00:58:05,200 --> 00:58:09,280
I specifically like building things around how people communicate

1322
00:58:09,280 --> 00:58:11,680
and sort of understanding how people express themselves

1323
00:58:11,680 --> 00:58:12,800
and how people work, right?

1324
00:58:12,800 --> 00:58:14,000
And when I was in college, I was,

1325
00:58:14,000 --> 00:58:16,560
I was studying computer science and psychology.

1326
00:58:16,560 --> 00:58:17,760
I think a lot of other people in the industry

1327
00:58:17,760 --> 00:58:19,440
started studying computer science, right?

1328
00:58:19,440 --> 00:58:23,600
So it's always been sort of the intersection

1329
00:58:23,600 --> 00:58:25,120
of those two things for me.

1330
00:58:25,120 --> 00:58:31,920
But I think it's also sort of this like really deep drive.

1331
00:58:31,920 --> 00:58:35,760
I don't know how to explain it, but I just feel like in,

1332
00:58:35,760 --> 00:58:39,280
like constitutionally, like I'm doing something wrong

1333
00:58:39,280 --> 00:58:41,920
if I'm not building something new, right?

1334
00:58:41,920 --> 00:58:46,560
And so I think that there's like

1335
00:58:49,760 --> 00:58:52,480
even when we're putting together the business case for,

1336
00:58:53,440 --> 00:58:57,360
you know, investing like $100 billion in AI

1337
00:58:57,360 --> 00:58:59,360
or some huge amount in the metaverse.

1338
00:58:59,360 --> 00:59:01,520
So it's like, yeah, I mean, we have plans

1339
00:59:01,520 --> 00:59:03,840
that I think make it pretty clear that if our stuff works,

1340
00:59:03,840 --> 00:59:05,360
it'll be a good investment.

1341
00:59:05,360 --> 00:59:08,320
But like, you can't know for certain from the outset.

1342
00:59:08,960 --> 00:59:12,000
And there's all these arguments that people have,

1343
00:59:12,000 --> 00:59:14,880
you know, whether it's like, you know, with advisors

1344
00:59:14,880 --> 00:59:17,680
or different folks, it's like, well, how, how could you,

1345
00:59:17,760 --> 00:59:20,320
like it's how are you confident enough to do this?

1346
00:59:20,320 --> 00:59:24,880
And it's like, well, the day I stop trying to build new things,

1347
00:59:25,520 --> 00:59:26,240
I'm just done.

1348
00:59:26,240 --> 00:59:28,240
I'm going to go build new things somewhere else, right?

1349
00:59:28,240 --> 00:59:32,880
It's like, it's like, it is, I'm fundamentally incapable

1350
00:59:33,520 --> 00:59:38,560
of running something or in my own life

1351
00:59:38,560 --> 00:59:41,520
and like not trying to build new things

1352
00:59:41,520 --> 00:59:42,880
that I think are interesting.

1353
00:59:42,880 --> 00:59:45,040
It's like, that's not even a question for me, right?

1354
00:59:45,040 --> 00:59:47,680
It's like whether, like whether we're going to go take a swing

1355
00:59:47,680 --> 00:59:49,040
at like building the next thing.

1356
00:59:49,040 --> 00:59:52,960
It's like, it's like, I'm just incapable of not doing that.

1357
00:59:54,800 --> 00:59:59,760
And I don't know, and I'm kind of like this

1358
00:59:59,760 --> 01:00:01,760
in like all the different aspects of my life, right?

1359
01:00:01,760 --> 01:00:04,320
It's like we built this like, you know,

1360
01:00:04,320 --> 01:00:08,720
family built this ranch in Kauai and like, I just like

1361
01:00:10,240 --> 01:00:11,840
worked to like design all these buildings.

1362
01:00:11,840 --> 01:00:15,200
I'm like kind of trying to like, we started raising cattle

1363
01:00:15,200 --> 01:00:16,560
and I'm like, all right, well, I want to make

1364
01:00:16,560 --> 01:00:18,160
like the best cattle in the world, right?

1365
01:00:18,160 --> 01:00:20,240
So it's like, how do we like, how do we architect this

1366
01:00:20,240 --> 01:00:22,560
so that way we can figure this out and like and build

1367
01:00:23,200 --> 01:00:25,200
and call the stuff up that we need to try to do that?

1368
01:00:26,640 --> 01:00:27,840
So I don't know, that's me.

1369
01:00:29,600 --> 01:00:30,800
What was the other part of the question?

1370
01:00:32,080 --> 01:00:34,880
Look, Metta is just a really amazing tech company, right?

1371
01:00:34,880 --> 01:00:36,960
They have all these great software engineers

1372
01:00:36,960 --> 01:00:39,840
and even they work with Stripe to handle payments.

1373
01:00:39,840 --> 01:00:41,360
And I think that's just a really notable fact.

1374
01:00:41,840 --> 01:00:45,200
That Stripe's ability to engineer these checkout experiences

1375
01:00:45,200 --> 01:00:48,720
is so good that big companies like Ford, Zoom, Metta,

1376
01:00:48,720 --> 01:00:52,000
even OpenAI, they work with Stripe to handle payments

1377
01:00:52,000 --> 01:00:53,840
because just think about how many different possibilities

1378
01:00:53,840 --> 01:00:54,880
you have to handle.

1379
01:00:54,880 --> 01:00:56,880
If you're in a different country, you'll pay a different way

1380
01:00:56,880 --> 01:00:58,480
and if you're buying a certain kind of item

1381
01:00:58,480 --> 01:01:00,400
that might affect how you decide to pay.

1382
01:01:00,400 --> 01:01:03,840
And Stripe is able to test these fine-grained optimizations

1383
01:01:03,840 --> 01:01:06,400
across tens of billions of transactions a day

1384
01:01:06,400 --> 01:01:08,560
to figure out what will convert people

1385
01:01:08,560 --> 01:01:11,280
and obviously conversion means more revenue for you.

1386
01:01:11,280 --> 01:01:13,680
And look, I'm not a big company like Metta or anything,

1387
01:01:13,680 --> 01:01:15,280
but I've been using Stripe since long

1388
01:01:15,280 --> 01:01:16,800
before they were advertisers.

1389
01:01:16,800 --> 01:01:20,080
Stripe Atlas was just the easiest way for me to set up an LLC

1390
01:01:20,080 --> 01:01:22,160
and they have these payments and invoicing features

1391
01:01:22,160 --> 01:01:26,000
that make it super convenient for me to get money from advertisers.

1392
01:01:26,000 --> 01:01:28,080
And obviously without that, it would have been much harder

1393
01:01:28,080 --> 01:01:30,000
for me to earn money from the podcast.

1394
01:01:30,000 --> 01:01:31,600
And so it's been great for me.

1395
01:01:31,600 --> 01:01:33,680
Go to stripe.com to learn more.

1396
01:01:33,680 --> 01:01:35,440
Thanks to them for sponsoring the episode.

1397
01:01:35,440 --> 01:01:36,240
Now back to Mark.

1398
01:01:37,120 --> 01:01:39,280
I'm not sure, but I'm actually curious about something else

1399
01:01:39,280 --> 01:01:44,400
which is, so the 19-year-old Mark reads a bunch

1400
01:01:44,400 --> 01:01:47,280
of like antiquity in classics, high school, college.

1401
01:01:47,920 --> 01:01:49,440
What important lesson did you learn from it?

1402
01:01:49,440 --> 01:01:50,560
Not just interesting things you found,

1403
01:01:50,560 --> 01:01:53,200
but like there aren't that many tokens you consumed

1404
01:01:53,200 --> 01:01:54,160
by the time you're 19.

1405
01:01:54,160 --> 01:01:55,680
A bunch of them were about the classics.

1406
01:01:55,680 --> 01:01:56,880
Clearly that was important in some way.

1407
01:01:56,880 --> 01:01:58,240
And that many tokens you consumed.

1408
01:02:05,680 --> 01:02:06,080
I don't know.

1409
01:02:06,080 --> 01:02:07,120
That's a good question.

1410
01:02:07,120 --> 01:02:08,320
I mean, one of the things that I thought

1411
01:02:08,320 --> 01:02:17,440
was really fascinating is, so when Augustus was first,

1412
01:02:18,000 --> 01:02:24,480
so he became emperor, and he was trying to establish peace.

1413
01:02:24,480 --> 01:02:30,400
And there was no real conception of peace at the time.

1414
01:02:30,400 --> 01:02:33,040
Like the people's understanding of peace was,

1415
01:02:34,000 --> 01:02:36,400
it is the temporary time between when you're

1416
01:02:36,400 --> 01:02:38,960
and amuse will inevitably attack you again,

1417
01:02:38,960 --> 01:02:40,240
so you get like a short rest.

1418
01:02:40,800 --> 01:02:43,200
And he had this view, which is like, look,

1419
01:02:43,200 --> 01:02:46,720
like we want to change the economy from instead of being

1420
01:02:46,720 --> 01:02:50,080
so mercenary and like in kind of militaristic

1421
01:02:50,960 --> 01:02:53,920
to like actually this positive something.

1422
01:02:54,640 --> 01:02:57,760
It's like a very novel idea at the time.

1423
01:03:02,400 --> 01:03:02,960
I don't know.

1424
01:03:02,960 --> 01:03:04,560
I think that there's like something that's

1425
01:03:04,560 --> 01:03:06,080
just really fundamental about that.

1426
01:03:06,080 --> 01:03:09,280
It's like in terms of the bounds on like what people

1427
01:03:09,280 --> 01:03:12,960
can conceive at the time of like what are rational ways to work.

1428
01:03:13,680 --> 01:03:17,280
And I don't know, I mean, going back to like,

1429
01:03:17,280 --> 01:03:19,600
I mean, this applies to both the metaverse and the AI stuff,

1430
01:03:19,600 --> 01:03:22,400
but like a lot of investors and just different people

1431
01:03:22,400 --> 01:03:24,880
just can't wrap their head around why we would open source this.

1432
01:03:25,440 --> 01:03:29,440
And it's like, I don't understand.

1433
01:03:29,440 --> 01:03:30,320
It's like open source.

1434
01:03:30,320 --> 01:03:32,160
That must just be like the temporary time

1435
01:03:32,160 --> 01:03:34,080
between which you're making things proprietary.

1436
01:03:34,080 --> 01:03:35,200
Right.

1437
01:03:35,200 --> 01:03:37,840
And it's, but I actually think it's like

1438
01:03:38,480 --> 01:03:44,160
this very profound thing in tech that has actually,

1439
01:03:44,160 --> 01:03:46,240
it creates a lot of winners.

1440
01:03:46,240 --> 01:03:46,560
Right.

1441
01:03:46,560 --> 01:03:50,720
And it's and so I don't want to strain the analogy too much,

1442
01:03:50,720 --> 01:03:54,960
but I do think that there's a lot of times,

1443
01:03:54,960 --> 01:03:56,480
I think ways where you can

1444
01:03:59,360 --> 01:04:01,760
that are just like models for building things

1445
01:04:02,000 --> 01:04:05,520
that people can't even like they just like often can't wrap

1446
01:04:05,520 --> 01:04:08,240
their head around how that would be a valuable thing

1447
01:04:08,240 --> 01:04:11,760
for people to go do, or like a reasonable state of the world

1448
01:04:11,760 --> 01:04:17,680
that it's, I mean, it's, I think that there's more reasonable

1449
01:04:17,680 --> 01:04:18,880
things than people think.

1450
01:04:18,880 --> 01:04:20,720
That's super fascinating.

1451
01:04:20,720 --> 01:04:22,800
Can I give you my answer when I was thinking

1452
01:04:22,800 --> 01:04:24,480
what you might have gotten from it?

1453
01:04:24,480 --> 01:04:28,160
This is probably totally off, but just how young

1454
01:04:28,160 --> 01:04:30,960
some of these people are who have very important roles

1455
01:04:31,840 --> 01:04:32,480
in the empire.

1456
01:04:32,480 --> 01:04:34,720
Like Caesar Augustus, like by the time he's 19,

1457
01:04:34,720 --> 01:04:37,120
he's actually incredibly one of the most prominent people

1458
01:04:37,120 --> 01:04:38,320
in Roman politics.

1459
01:04:38,320 --> 01:04:39,760
And he's like leading battles and forming

1460
01:04:39,760 --> 01:04:41,040
the second prime emirate.

1461
01:04:41,040 --> 01:04:42,800
I wonder if you're like the 19 year old is like,

1462
01:04:42,800 --> 01:04:44,000
I can actually do this because like

1463
01:04:44,000 --> 01:04:46,800
I think that's an interesting example,

1464
01:04:46,800 --> 01:04:50,240
both from a lot of history and American history.

1465
01:04:50,240 --> 01:04:50,720
Yeah.

1466
01:04:50,720 --> 01:04:55,040
I mean, it's, I mean, one of my favorite quotes is,

1467
01:04:55,040 --> 01:04:57,680
it's this Picasso quote that all children are artists

1468
01:04:57,680 --> 01:04:59,520
and the challenge is how do you remain an artist

1469
01:04:59,520 --> 01:05:00,560
when you grow up?

1470
01:05:00,560 --> 01:05:03,280
And it's like basically I think because when you're younger,

1471
01:05:04,080 --> 01:05:09,680
I think it's just easier to have kind of wild ideas

1472
01:05:09,680 --> 01:05:11,360
and you're not, you know, you have no,

1473
01:05:12,560 --> 01:05:15,200
there are all these analogies to the innovators dilemma

1474
01:05:15,200 --> 01:05:18,640
that exist in your life as well as your company

1475
01:05:18,640 --> 01:05:19,840
or whatever you've built, right?

1476
01:05:19,840 --> 01:05:22,080
So, you know, you're kind of earlier on your trajectory.

1477
01:05:22,080 --> 01:05:24,960
It's easier to pivot and take in new ideas

1478
01:05:24,960 --> 01:05:26,800
without it disrupting other commitments

1479
01:05:26,800 --> 01:05:29,040
that you've made to different things.

1480
01:05:29,040 --> 01:05:31,920
And so, I don't know.

1481
01:05:31,920 --> 01:05:34,560
I think that's an interesting part of running a company

1482
01:05:34,560 --> 01:05:36,960
is like how do you kind of stay dynamic?

1483
01:05:38,640 --> 01:05:40,400
Going back to the investors in open source,

1484
01:05:41,440 --> 01:05:42,880
the $10 billion model,

1485
01:05:42,880 --> 01:05:45,840
suppose it's totally safe, you've done these evaluations

1486
01:05:45,840 --> 01:05:47,280
and unlike in this case,

1487
01:05:47,280 --> 01:05:48,960
the evaluators can also fine tune the model,

1488
01:05:49,680 --> 01:05:51,360
which hopefully will be the case in future models.

1489
01:05:52,560 --> 01:05:54,560
Would you open source that, the $10 billion model?

1490
01:05:55,360 --> 01:05:57,280
Well, I mean, as long as it's helping us, then yeah.

1491
01:05:57,280 --> 01:05:59,360
But would it like to $10 billion of R&D

1492
01:05:59,360 --> 01:06:00,960
and then now it's like open source or anything?

1493
01:06:00,960 --> 01:06:02,880
Well, I think here's, I think a question,

1494
01:06:02,880 --> 01:06:06,640
which we'll have to evaluate this as time goes on too, but

1495
01:06:10,320 --> 01:06:12,960
we have a long history of open sourcing software, right?

1496
01:06:12,960 --> 01:06:15,520
We don't tend to open source our product, right?

1497
01:06:15,520 --> 01:06:18,880
So, it's not like we don't take like the code for Instagram

1498
01:06:18,880 --> 01:06:19,760
and make it open source,

1499
01:06:19,760 --> 01:06:22,880
but we take like a lot of the low level infrastructure

1500
01:06:23,440 --> 01:06:25,600
and we make that open source, right?

1501
01:06:26,080 --> 01:06:29,520
Probably the biggest one in our history was open compute project

1502
01:06:29,520 --> 01:06:34,640
where we took the designs for kind of all of our servers

1503
01:06:34,640 --> 01:06:36,240
and network switches and data centers

1504
01:06:36,240 --> 01:06:38,880
and made it open source and ended up being super helpful

1505
01:06:38,880 --> 01:06:41,520
because, you know, I mean, a lot of people can design servers,

1506
01:06:41,520 --> 01:06:43,520
but now like the industry standardized on our design,

1507
01:06:43,520 --> 01:06:45,280
which meant that the supply chains

1508
01:06:46,160 --> 01:06:47,920
basically all got built out around our design,

1509
01:06:47,920 --> 01:06:48,720
the volumes went up,

1510
01:06:48,720 --> 01:06:51,600
so it got cheaper for everyone and saved us billions of dollars.

1511
01:06:51,600 --> 01:06:53,360
So, awesome, right?

1512
01:06:53,440 --> 01:06:56,240
Okay, so there's multiple ways where open source,

1513
01:06:56,240 --> 01:06:57,920
I think, could be helpful for us.

1514
01:06:57,920 --> 01:07:01,440
One is if people figure out how to run the models more cheaply.

1515
01:07:01,440 --> 01:07:04,640
Well, we're going to be spending tens or like 100 billion dollars

1516
01:07:04,640 --> 01:07:07,040
or more over time on all this stuff.

1517
01:07:07,680 --> 01:07:10,160
So, if we can do that 10% more effectively,

1518
01:07:10,160 --> 01:07:12,720
we're saving billions or tens of billions of dollars.

1519
01:07:12,720 --> 01:07:14,320
Okay, that's probably worth a lot by itself,

1520
01:07:15,920 --> 01:07:17,840
especially if there's other competitive models out there.

1521
01:07:17,840 --> 01:07:19,760
It's not like our thing is like

1522
01:07:19,760 --> 01:07:21,760
be giving away some kind of crazy advantage.

1523
01:07:22,720 --> 01:07:25,600
So, is there a view that the trading will be commodified?

1524
01:07:29,040 --> 01:07:30,880
I think there's a bunch of ways that this could play out.

1525
01:07:30,880 --> 01:07:31,840
That's one.

1526
01:07:31,840 --> 01:07:38,560
The other is that, so commodity kind of implies

1527
01:07:39,120 --> 01:07:40,480
that it's going to get very cheap

1528
01:07:40,480 --> 01:07:43,040
because there's lots of options.

1529
01:07:43,040 --> 01:07:44,560
The other direction that this could go in

1530
01:07:45,360 --> 01:07:47,040
is qualitative improvements.

1531
01:07:47,040 --> 01:07:49,360
So, you mentioned fine-tuning, right?

1532
01:07:49,360 --> 01:07:51,760
It's like right now, it's pretty limited,

1533
01:07:51,760 --> 01:07:55,120
what you can do with fine-tuning major other models out there.

1534
01:07:55,120 --> 01:07:56,400
And there are some options,

1535
01:07:56,400 --> 01:07:58,400
but generally not for the biggest models.

1536
01:07:59,840 --> 01:08:01,280
So, I think being able to do that

1537
01:08:01,280 --> 01:08:06,160
and be able to kind of do different app-specific things

1538
01:08:06,160 --> 01:08:07,440
or use case-specific things

1539
01:08:07,440 --> 01:08:09,120
or build them into specific tool chains,

1540
01:08:10,880 --> 01:08:15,600
I think will not only enable kind of more efficient development,

1541
01:08:15,600 --> 01:08:17,520
it could enable qualitatively different things.

1542
01:08:18,480 --> 01:08:19,680
Here's one analogy on this.

1543
01:08:22,160 --> 01:08:24,240
So, one thing that I think generally sucks

1544
01:08:24,240 --> 01:08:25,520
about the mobile ecosystem

1545
01:08:26,080 --> 01:08:30,160
is that you have these two gatekeeper companies,

1546
01:08:30,160 --> 01:08:31,200
Apple and Google,

1547
01:08:31,200 --> 01:08:33,200
that can tell you what you're allowed to build.

1548
01:08:33,200 --> 01:08:35,440
And there are lots of times in our history,

1549
01:08:35,440 --> 01:08:36,800
so there's the economic version of that,

1550
01:08:36,800 --> 01:08:38,080
which is like, all right, we build something in there,

1551
01:08:38,080 --> 01:08:39,600
just like, I'm going to take a bunch of your money.

1552
01:08:39,600 --> 01:08:43,520
But then there's the qualitative version,

1553
01:08:43,520 --> 01:08:46,080
which is actually what kind of upsets me more,

1554
01:08:46,160 --> 01:08:47,520
which is there's a bunch of times

1555
01:08:47,520 --> 01:08:50,160
when we've launched or wanted to launch features,

1556
01:08:50,720 --> 01:08:53,280
and then Apple's just like, nope, you're not launching that.

1557
01:08:53,280 --> 01:08:55,520
So, it's like, that sucks, right?

1558
01:08:55,520 --> 01:08:59,440
And so, the question is, what is,

1559
01:08:59,440 --> 01:09:04,560
like, are we kind of set up for a world like that with AI,

1560
01:09:04,560 --> 01:09:07,440
where like, you're going to get a handful of companies

1561
01:09:07,440 --> 01:09:08,720
that run these closed models

1562
01:09:08,720 --> 01:09:10,640
that are going to be in control of the APIs,

1563
01:09:10,640 --> 01:09:11,840
and therefore are going to be able to tell you

1564
01:09:11,840 --> 01:09:12,640
what you can build.

1565
01:09:13,520 --> 01:09:15,520
Well, for one, I can say, for us,

1566
01:09:16,480 --> 01:09:18,800
it is worth it to go build a model ourselves

1567
01:09:18,800 --> 01:09:20,960
to make sure that we're not in that position, right?

1568
01:09:20,960 --> 01:09:23,680
Like, I don't want any of those other companies

1569
01:09:23,680 --> 01:09:24,800
telling us what we can build.

1570
01:09:26,000 --> 01:09:28,000
But from an open source perspective,

1571
01:09:28,000 --> 01:09:29,920
I think a lot of developers don't want those companies

1572
01:09:29,920 --> 01:09:31,200
telling them what they can build either.

1573
01:09:32,560 --> 01:09:35,680
So, the question is, what is the ecosystem

1574
01:09:35,680 --> 01:09:36,880
that gets built out around that?

1575
01:09:36,880 --> 01:09:38,400
What are interesting new things?

1576
01:09:38,400 --> 01:09:40,080
How much does that improve our products?

1577
01:09:42,560 --> 01:09:43,920
I think that there's a lot of cases

1578
01:09:43,920 --> 01:09:46,000
where if this ends up being like, you know,

1579
01:09:46,000 --> 01:09:50,160
like our databases or caching systems or architecture,

1580
01:09:50,720 --> 01:09:52,800
we'll get valuable contributions from the community

1581
01:09:52,800 --> 01:09:54,000
that will make our stuff better,

1582
01:09:54,000 --> 01:09:56,320
and then our app-specific work that we do

1583
01:09:56,320 --> 01:09:57,920
will still be so differentiated

1584
01:09:57,920 --> 01:09:59,280
that it won't really matter, right?

1585
01:09:59,280 --> 01:10:01,360
It's like, we'll be able to do what we do,

1586
01:10:01,360 --> 01:10:02,800
we'll benefit in all the systems,

1587
01:10:02,800 --> 01:10:04,320
ours and the communities will be better

1588
01:10:04,320 --> 01:10:05,280
because it's open source.

1589
01:10:06,080 --> 01:10:10,320
There is one world where maybe it's not that.

1590
01:10:10,320 --> 01:10:11,520
I mean, maybe the model just ends up

1591
01:10:11,520 --> 01:10:13,200
being more of the product itself.

1592
01:10:13,280 --> 01:10:17,760
In that case, then I think it's a trickier economic

1593
01:10:17,760 --> 01:10:20,000
calculation about whether you open source that

1594
01:10:20,000 --> 01:10:23,200
because then you are kind of commoditizing yourself a lot.

1595
01:10:23,840 --> 01:10:24,800
From what I can see so far,

1596
01:10:24,800 --> 01:10:25,920
it doesn't seem like we're in that zone.

1597
01:10:26,560 --> 01:10:28,480
Do you expect to earn significant revenue

1598
01:10:28,480 --> 01:10:30,640
from licensing your model to the cloud providers?

1599
01:10:30,640 --> 01:10:32,720
So, they have to pay you a fee to actually serve the model?

1600
01:10:36,160 --> 01:10:38,640
We want to have an arrangement like that,

1601
01:10:38,640 --> 01:10:40,800
but I don't know how significant it'll be.

1602
01:10:40,800 --> 01:10:44,640
And we have this, this is basically our license for Lama.

1603
01:10:46,800 --> 01:10:49,040
In a lot of ways, it's like a very permissive

1604
01:10:49,040 --> 01:10:51,680
open source license, except that we have a limit

1605
01:10:51,680 --> 01:10:53,440
for the largest companies using it.

1606
01:10:53,440 --> 01:10:56,240
And this is why we put that limit in,

1607
01:10:56,240 --> 01:10:58,640
is we're not trying to prevent them from using it.

1608
01:10:58,640 --> 01:10:59,920
We just want them to come talk to us

1609
01:10:59,920 --> 01:11:01,120
because if they're going to just basically

1610
01:11:01,120 --> 01:11:03,680
take what we built and resell it and make money off of it,

1611
01:11:03,680 --> 01:11:06,320
then it's like, okay, well, if you're like,

1612
01:11:07,200 --> 01:11:09,200
Microsoft Azure or Amazon,

1613
01:11:09,680 --> 01:11:11,200
yeah, if you're going to reselling the model,

1614
01:11:11,200 --> 01:11:12,720
then we should have some revenue share on that.

1615
01:11:12,720 --> 01:11:14,800
So, just come talk to us before you go do that.

1616
01:11:14,800 --> 01:11:15,760
And that's how that's played out.

1617
01:11:15,760 --> 01:11:19,520
So, for Lama 2, it's, I mean, we basically just have deals

1618
01:11:19,520 --> 01:11:22,320
with all these major cloud companies,

1619
01:11:22,320 --> 01:11:26,480
and Lama 2 is available as a hosted service on all those clouds.

1620
01:11:28,480 --> 01:11:31,200
I assume that as we release bigger and bigger models,

1621
01:11:31,200 --> 01:11:32,240
that'll become a bigger thing.

1622
01:11:32,240 --> 01:11:33,440
It's not the main thing that we're doing,

1623
01:11:33,440 --> 01:11:35,040
but I just think if others are,

1624
01:11:35,040 --> 01:11:36,640
if those companies are going to be selling our models,

1625
01:11:36,640 --> 01:11:38,240
it makes sense that we should, you know,

1626
01:11:38,240 --> 01:11:39,600
share the upside of that somehow.

1627
01:11:39,600 --> 01:11:42,480
Yeah. With regards to the other open source dangers,

1628
01:11:42,480 --> 01:11:44,720
I think I have a genuine legion of points

1629
01:11:44,720 --> 01:11:45,920
about the balance of power stuff,

1630
01:11:46,960 --> 01:11:49,280
and potentially like the harms you can get rid of

1631
01:11:49,280 --> 01:11:51,280
because we have better alignment techniques or something.

1632
01:11:52,240 --> 01:11:54,320
I wish there was some sort of framework that Meta had,

1633
01:11:54,320 --> 01:11:56,320
like other labs have this where they say like,

1634
01:11:56,320 --> 01:11:58,400
if we see this is a concrete thing,

1635
01:11:58,400 --> 01:12:00,720
then that's a no-go on the open source,

1636
01:12:00,720 --> 01:12:03,200
or like even potentially on deployment,

1637
01:12:03,200 --> 01:12:06,080
just like writing it down so like the company is ready for it,

1638
01:12:06,720 --> 01:12:08,800
people have expectations around it and so forth.

1639
01:12:08,800 --> 01:12:10,640
Yeah. No, I think that that's a fair point

1640
01:12:10,640 --> 01:12:12,000
on the existential risk side.

1641
01:12:12,000 --> 01:12:14,800
Right now, we focus more on the types of risks

1642
01:12:14,800 --> 01:12:17,760
that we see today, which are more of these content risks.

1643
01:12:17,760 --> 01:12:23,120
So, you know, we have lines on, we don't want the model to be

1644
01:12:24,080 --> 01:12:27,040
basically doing things that are helping people commit violence

1645
01:12:27,040 --> 01:12:30,320
or fraud or, you know, just harming people in different ways.

1646
01:12:30,320 --> 01:12:35,200
So in practice for today's models,

1647
01:12:35,200 --> 01:12:37,920
and I would guess the next generation

1648
01:12:37,920 --> 01:12:39,600
and maybe even the generation after that,

1649
01:12:40,320 --> 01:12:44,800
I think while it is somewhat more maybe intellectually interesting

1650
01:12:44,800 --> 01:12:47,040
to talk about the existential risks,

1651
01:12:47,600 --> 01:12:53,840
I actually think the real harms that need more energy being mitigated

1652
01:12:53,840 --> 01:12:57,840
are things that are going to like have someone take a model

1653
01:12:57,840 --> 01:13:00,960
and do something to hurt a person with today's parameters

1654
01:13:00,960 --> 01:13:04,240
of and kind of the types of kind of more mundane harms

1655
01:13:04,320 --> 01:13:07,280
that we see today, like people kind of committing fraud

1656
01:13:07,280 --> 01:13:08,720
against each other or things like that.

1657
01:13:08,720 --> 01:13:13,360
So that, I just don't want to short change that.

1658
01:13:13,360 --> 01:13:15,280
I think we have a responsibility

1659
01:13:15,280 --> 01:13:16,960
to make sure we do a good job on that.

1660
01:13:16,960 --> 01:13:18,880
Yeah, Meta's a big company, you can handle both.

1661
01:13:18,880 --> 01:13:22,960
Yeah. Okay, so as far as the open source goes,

1662
01:13:22,960 --> 01:13:25,520
I'm actually curious if you think the impact of the open source

1663
01:13:25,520 --> 01:13:28,320
from PyTorch, React, open compute, these things

1664
01:13:28,320 --> 01:13:29,840
has been bigger for the world

1665
01:13:29,840 --> 01:13:32,000
than even the social media aspects of Meta.

1666
01:13:32,000 --> 01:13:33,600
Because I like talk to people who use these services

1667
01:13:33,600 --> 01:13:34,800
and think like it's plausible

1668
01:13:34,800 --> 01:13:36,560
because a big part of the internet runs on these things.

1669
01:13:38,960 --> 01:13:40,160
It's an interesting question.

1670
01:13:40,160 --> 01:13:43,360
I mean, I think almost half the world uses our...

1671
01:13:43,360 --> 01:13:44,800
Yeah, that's an interesting point.

1672
01:13:47,360 --> 01:13:49,760
So I think it's hard to beat that.

1673
01:13:49,760 --> 01:13:52,880
But no, I think open sources,

1674
01:13:54,560 --> 01:13:57,360
it's really powerful as a new way of building things.

1675
01:13:57,360 --> 01:14:01,600
And yeah, I mean, it's possible.

1676
01:14:01,600 --> 01:14:03,920
I mean, it's maybe one of these things where...

1677
01:14:06,320 --> 01:14:09,520
I don't know, like Bell Labs, where they...

1678
01:14:09,520 --> 01:14:12,880
It's like they were working on the transistor

1679
01:14:12,880 --> 01:14:15,360
because they wanted to enable long distance calling.

1680
01:14:15,920 --> 01:14:17,280
And they did.

1681
01:14:17,280 --> 01:14:19,120
And it ended up being really profitable for them

1682
01:14:19,120 --> 01:14:21,120
that they were able to enable long distance calling.

1683
01:14:21,120 --> 01:14:25,440
And if you ask them five to 10 years out from that,

1684
01:14:27,280 --> 01:14:29,600
what was the most useful thing that they invented?

1685
01:14:29,600 --> 01:14:31,440
It's like, okay, well, we enable long distance calling

1686
01:14:31,440 --> 01:14:33,040
and now all these people are long distance calling.

1687
01:14:33,040 --> 01:14:34,400
But if you ask 100 years later,

1688
01:14:34,400 --> 01:14:35,760
maybe it's a different question.

1689
01:14:35,760 --> 01:14:40,320
So I think that that's true of a lot of the things

1690
01:14:40,320 --> 01:14:41,280
that we're building, right?

1691
01:14:41,280 --> 01:14:45,280
Reality Labs, some of the AI stuff, some of the open source stuff.

1692
01:14:45,280 --> 01:14:48,480
I think it's like the specific products evolve

1693
01:14:48,480 --> 01:14:50,400
and to some degree come and go.

1694
01:14:50,400 --> 01:14:54,080
But I think like the advances for humanity persist.

1695
01:14:54,080 --> 01:14:57,360
And that's like a cool part of what we all get to do.

1696
01:14:58,080 --> 01:14:59,920
By when will the Lama models be trained

1697
01:14:59,920 --> 01:15:01,040
on your own custom silicon?

1698
01:15:06,480 --> 01:15:08,320
Soon, not Lama 4.

1699
01:15:09,760 --> 01:15:14,640
The approach that we took is first we basically built

1700
01:15:14,640 --> 01:15:16,560
custom silicon that could handle inference

1701
01:15:17,200 --> 01:15:20,000
for our ranking and recommendation type stuff.

1702
01:15:20,000 --> 01:15:22,560
So reels, newsfeed, ads.

1703
01:15:23,200 --> 01:15:26,800
And that was consuming a lot of GPUs.

1704
01:15:28,000 --> 01:15:30,800
But when we were able to move that to our own silicon,

1705
01:15:30,800 --> 01:15:34,720
we now were able to use the more expensive Nvidia GPUs

1706
01:15:35,440 --> 01:15:36,480
only for training.

1707
01:15:37,120 --> 01:15:44,480
So at some point, we will hopefully have silicon ourselves

1708
01:15:44,480 --> 01:15:47,600
that we can be using for probably first training

1709
01:15:47,600 --> 01:15:49,680
some of the simpler things that eventually training

1710
01:15:50,560 --> 01:15:53,200
these like really large models.

1711
01:15:53,840 --> 01:15:58,160
But in the meantime, I'd say the program is going quite well

1712
01:15:58,160 --> 01:15:59,920
and we're just rolling it out methodically

1713
01:15:59,920 --> 01:16:01,600
and have a long-term roadmap for it.

1714
01:16:02,800 --> 01:16:03,760
Final question.

1715
01:16:03,760 --> 01:16:05,120
This is sort of the out of left field,

1716
01:16:05,120 --> 01:16:08,640
but if you were made CEO of Google+, could you have made it work?

1717
01:16:08,640 --> 01:16:10,240
Google+, oof.

1718
01:16:11,440 --> 01:16:12,800
Well, I don't know.

1719
01:16:14,160 --> 01:16:14,720
I don't know.

1720
01:16:14,720 --> 01:16:18,800
That's a very difficult, very difficult counterfactual.

1721
01:16:19,440 --> 01:16:21,040
Okay, then the real final question will be

1722
01:16:21,040 --> 01:16:22,480
when Gemini was launched,

1723
01:16:22,480 --> 01:16:24,720
was there any chance that somebody in the office

1724
01:16:24,720 --> 01:16:26,080
uttered Karthica Dalinda Est?

1725
01:16:27,600 --> 01:16:29,520
No, I think we're tamer now.

1726
01:16:30,720 --> 01:16:31,520
Cool, cool.

1727
01:16:31,520 --> 01:16:32,160
Awesome, Mark.

1728
01:16:35,680 --> 01:16:36,800
Yeah, I don't know.

1729
01:16:36,800 --> 01:16:37,360
It's a good question.

1730
01:16:38,720 --> 01:16:41,120
The problem is there was no CEO of Google+,

1731
01:16:41,120 --> 01:16:43,120
it was just like a division within a company.

1732
01:16:43,840 --> 01:16:46,160
I think it's like, and you asked before about

1733
01:16:46,160 --> 01:16:48,560
what are the kind of scarcest commodities,

1734
01:16:48,560 --> 01:16:50,240
but you asked about it in terms of dollars.

1735
01:16:50,800 --> 01:16:52,800
And I actually think for most companies,

1736
01:16:53,680 --> 01:16:57,280
it's, of this scale at least, it's focus, right?

1737
01:16:57,280 --> 01:16:58,400
It's like when you're a startup,

1738
01:16:58,400 --> 01:17:00,000
maybe you're more constrained on capital.

1739
01:17:01,120 --> 01:17:03,200
You know, you just are working on one idea

1740
01:17:03,200 --> 01:17:05,280
and you might not have all the resources.

1741
01:17:05,280 --> 01:17:07,360
I think you cross some threshold at some point

1742
01:17:07,360 --> 01:17:09,600
where the nature of what you're doing,

1743
01:17:09,600 --> 01:17:11,120
you're building multiple things

1744
01:17:11,120 --> 01:17:13,120
and you're creating more value across them,

1745
01:17:13,120 --> 01:17:17,360
but you become more constrained on what can you direct

1746
01:17:17,360 --> 01:17:19,200
and to go well.

1747
01:17:19,200 --> 01:17:21,920
And like, there's always the cases

1748
01:17:21,920 --> 01:17:24,000
where something just random awesome happens

1749
01:17:24,000 --> 01:17:25,840
in the organization, I don't even know about it.

1750
01:17:25,840 --> 01:17:28,160
And those are, that's great.

1751
01:17:28,160 --> 01:17:29,520
But like, but I think in general,

1752
01:17:30,640 --> 01:17:33,680
the organization's capacity is largely limited

1753
01:17:33,680 --> 01:17:38,480
by what like the CEO and the management team

1754
01:17:38,480 --> 01:17:41,440
are able to kind of oversee and kind of manage.

1755
01:17:42,480 --> 01:17:45,360
I think that that's just been a big focus for us.

1756
01:17:45,360 --> 01:17:49,600
It's like, all right, keep the, as I guess Ben Horowitz says,

1757
01:17:49,600 --> 01:17:51,920
keep the main thing, the main thing, right?

1758
01:17:51,920 --> 01:17:57,440
And try to kind of stay focused on your key priorities.

1759
01:17:58,000 --> 01:17:59,600
Yeah, all right, awesome.

1760
01:17:59,600 --> 01:18:00,400
That was excellent, Mark.

1761
01:18:00,400 --> 01:18:01,520
Thanks so much. That was a lot of fun.

1762
01:18:01,520 --> 01:18:02,720
Yeah, really fun.

1763
01:18:02,720 --> 01:18:03,440
Thanks for having me.

1764
01:18:03,440 --> 01:18:04,160
Yeah, absolutely.

1765
01:18:04,960 --> 01:18:05,840
Hey, everybody.

1766
01:18:05,840 --> 01:18:07,920
I hope you enjoyed that episode with Mark.

1767
01:18:07,920 --> 01:18:09,760
As you can see, I'm now doing ads.

1768
01:18:09,760 --> 01:18:12,800
So if you're interested in advertising on the podcast,

1769
01:18:12,800 --> 01:18:14,160
go to the link in the description.

1770
01:18:14,800 --> 01:18:17,520
Otherwise, as you know, the most helpful thing you can do

1771
01:18:17,520 --> 01:18:20,240
is just share the podcast with people who you think

1772
01:18:20,240 --> 01:18:21,040
might enjoy it.

1773
01:18:21,040 --> 01:18:23,040
You know, your friends, group chats, Twitter,

1774
01:18:23,680 --> 01:18:24,560
I guess threads.

1775
01:18:25,120 --> 01:18:27,680
Yeah, I hope you enjoyed and I'll see you on the next one.

