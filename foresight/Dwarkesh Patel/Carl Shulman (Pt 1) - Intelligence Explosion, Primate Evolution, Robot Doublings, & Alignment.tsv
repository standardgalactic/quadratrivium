start	end	text
0	5680	Human-level AI is deep, deep into an intelligence explosion.
5680	9040	Things like inventing the transformer or discovering
9040	12080	Chinchilla scaling and doing your training runs more optimally
12080	13760	or creating flash attention.
13760	17560	That set of inputs probably would yield the kind of AI
17560	20040	capabilities needed for intelligence explosion.
20040	22320	You have a race between, on the one hand,
22320	25080	the project of getting strong interpretability
25080	26880	and shaping motivations.
26960	30160	And on the other hand, these AIs in ways
30160	33080	that you don't perceive make the AI takeover happen.
33080	36720	We spend more compute by having a larger brain than other animals.
36720	38720	And then we have a longer childhood.
38720	41160	It's not like you have to like having a bigger model
41160	43120	and having more training time with it.
43120	45320	It seemed very implausible that we couldn't do better
45320	47440	than completely brute force evolution.
47440	50240	How quickly are we running through those orders of magnitude?
50240	54800	OK, today I have the pleasure of speaking with Carl Schulman,
54800	56320	many of my former guests.
56360	57480	And this is not an exaggeration.
57480	59520	Many of my former guests have told me
59520	62280	that a lot of their biggest ideas,
62280	63480	perhaps most of their biggest ideas,
63480	65160	have come directly from Carl,
65160	67640	especially when it has to do with the intelligence explosion
67640	68760	and its impacts.
68760	71400	And so I decided to go directly to the source
71400	73840	and we have Carl today on the podcast.
73840	75360	Carl keeps a super low profile,
75360	78240	but he is one of the most interesting intellectuals
78240	79480	I've ever encountered.
79480	81840	And this is actually his second podcast ever.
81840	84440	So we're going to get to get deep into the heart
84440	85720	of many of the most important ideas
85720	87400	that are circulating right now,
87400	88640	directly from the source.
88640	89760	So and by the way,
89760	92880	so Carl is also an advisor to the Open Philanthropy Project,
92880	94720	which is one of the biggest funders
94720	97280	on causes having to do with AI and its risks,
97280	99280	not to mention global health and old being.
99280	101200	And he is a research associate
101200	104200	at the Future of Humanity Institute at Oxford.
104200	106600	So Carl, it's a huge pleasure to have you on the podcast.
106600	107440	Thanks for coming.
107440	108360	Thank you, Drakash.
108360	111560	I've enjoyed seeing some of your episodes recently
111560	113920	and I'm glad to be on the show.
113960	115600	Excellent. Let's talk about AI.
115600	117360	Before we get into the details,
117360	121200	give me the sort of big picture explanation
121200	126080	of the feedback loops and just the general dynamics
126080	128560	that would start when you have something
128560	130600	that is approaching human level intelligence.
130600	133120	Yeah. So I think the way to think about it
133120	135760	is we have a process now
135760	139560	where humans are developing new computer chips,
139560	144280	new software, running larger training runs
144280	149280	and it takes a lot of work to keep Moore's law chugging.
150200	152480	Well, it was, it's slowing down now
152480	155160	and it takes a lot of work to develop things
155160	160000	like transformers to develop a lot of the improvements
160000	162400	to AI and neural networks.
162400	163360	They're advancing things.
163360	168360	And the core method that I think I want to highlight
169080	173000	on this podcast, and I think is underappreciated
173000	176000	is the idea of input-output curves.
176000	181000	So we can look at the increasing difficulty
181200	183320	of improving chips.
183320	186880	And so sure, each time you double the performance
186880	188240	of computers, it's harder
188240	189600	and as we approach physical limits,
189600	193200	eventually it becomes impossible, but how much harder?
194200	199200	So there's a paper called our Ideas Getting Harder to Find.
199560	201880	It was published a few years ago,
201880	204440	something like 10 years ago at Mirri,
204440	209440	we did, I mean, I did an early version of this analysis
210560	213520	using mainly data from Intel
213520	216360	and like the large semiconductor fabricators.
216360	220240	Anyway, and so in this paper, they cover a period
220240	224560	where the productivity of computing
224560	225680	went up a million folds.
225680	227600	So you could get a million times
227600	230360	the computing operations per second per dollar.
230360	234080	Big change, but it got harder.
234080	238880	So the amount of investments, the labor force required
238880	241040	to make those continuing advancements
241040	242920	went up and up and up.
242920	247440	Indeed, it went up 18 fold over that period.
247440	249600	I know, so some take this to say,
249600	251360	oh, diminishing returns,
251360	252960	things are just getting harder and harder.
252960	255660	And so that will be the end of progress eventually.
256920	261300	However, in a world where AI is doing the work,
262840	265240	that doubling of computing performance
266440	269760	translates pretty directly to a doubling or better
269760	272000	of the effective labor supply.
272000	277000	That is, if when we had that million fold compute increase,
278000	281720	we used it to run artificial intelligences
281720	286520	who would replace human scientists and engineers
286520	290520	then the 18x increase in the labor demands
290520	292320	of the industry would be trivial.
292320	295080	We're getting more than one doubling
295080	297160	of the effective labor supply
297160	302160	that we need for each doubling of the labor requirement.
302560	306440	And in that data set, it's like over four.
306760	309720	So we double compute.
309720	312240	Okay, now we need somewhat more researchers,
312240	314440	but a lot less than twice as many.
314440	319440	And so, okay, we use up some of those doublings of compute
319960	323200	on the increasing difficulty of further research,
323200	327380	but most of them are left to expedite the process.
327380	331680	So if you double your labor force,
331680	334720	that's enough to get several doublings of compute.
334720	338680	You use up one of them on meeting
338680	341880	the increased demands from diminishing returns.
341880	344800	The others can be used to accelerate the process.
344800	349200	So you have your first doubling takes however many months,
349200	353480	your next doubling can take a smaller fraction of that,
353480	356600	the next doubling less and so on.
356600	358820	At least in so far as this,
360360	362160	the outputs you're generating,
362160	364680	compute for AI in this story.
364680	367880	Are able to serve the function of the necessary inputs.
367880	369920	If there are other inputs that you need,
369920	373360	eventually those become a bottleneck
373360	375920	and you wind up more restricted on those.
375920	376760	Got it, okay.
376760	378840	So yeah, I think the bloom paper had that,
378840	381280	there was 35% increase in,
381280	384060	was it transferred to destiny or cost per flop?
384060	386040	And there was a 7% increase per year
386040	387720	in the number of researchers required
387720	389080	to sustain that pace.
389080	390520	So something on this, yeah,
390520	395520	it's like four to five doublings of compute
395880	398080	per doubling of labor inputs.
398080	399920	I guess there's a lot of questions you can delve into
399920	402880	in terms of whether you would expect a similar scale
402880	407120	with AI and whether it makes sense to think of AI
407120	409200	as a population of researchers
409200	411840	that keeps growing with compute itself.
411840	412720	Actually, let's go there.
412720	414040	So can you explain the intuition
414040	416240	that compute is a good proxy
416240	419720	for the number of AI researchers so to speak?
419720	422720	So far I've talked about hardware as an initial example
422720	426120	because we had good data about a past period.
426120	430120	You can also make improvements on the software side.
430120	432480	And we think about an intelligence explosion
432480	434920	that can include AI is doing work
434920	438700	on making hardware better, making better software,
438700	439800	making more hardware.
440920	445920	But the basic idea for the hardware is especially simple
446400	449080	in that if you have a worker,
449080	451560	an AI worker that can substitute for a human,
451560	453600	if you have twice as many computers,
453600	456600	you can run two separate instances of them
456600	459240	and then they can do two different jobs,
459240	463040	manage two different machines,
463040	466520	work on two different design problems.
466520	470600	Now, you can get more gains than just what you would get
470600	472080	by having two instances.
472080	475880	We get improvements from using some of our compute,
475880	478720	not just to run more instances of the existing AI,
478720	481080	but to train larger AIs.
481080	482800	So there's hardware technology,
482800	486000	how much you can get per dollar you spend on hardware.
486000	487840	And there's software technology.
487840	490080	And the software can be copied freely.
490080	492080	So if you've got the software,
492080	494680	it doesn't necessarily make that much to say that,
494680	498200	oh, we've got 100 Microsoft Windows.
498200	500280	You can make as many copies as you need
500280	505000	for whatever Microsoft will charge you.
505000	506720	But for hardware is different.
506720	510240	It matters how much we actually spend on the hardware
510240	512000	at a given price.
512000	514360	And if we look at the changes
514360	517360	that have been driving AI recently,
517360	519360	that is the thing that is really off-trend.
519360	522520	We are spending tremendously more money
523400	528400	on computer hardware for training big AI models.
528720	529560	Yeah, okay.
529560	532760	So there's the investment in hardware.
532760	534600	There's a hardware technology itself
534600	536680	and there's the software progress itself.
536680	537720	The AI is getting better
537720	539120	because we're spending more money on it
539120	541800	because our hardware itself is getting better over time.
541800	543800	And because we're developing better models
543800	546640	or better adjustments to those models,
546640	548280	where is the loop here?
548280	552720	The work involved in designing new hardware and software
552720	555240	is being done by people now.
555240	558960	They use computer tools to assist them,
558960	563080	but computer time is not the primary cost
564000	567200	for NVIDIA designing chips,
567200	571720	for TSMC producing them for ASML,
571720	576040	making lithography equipment to serve the TSMC fabs.
576040	579400	And even in AI software research,
579400	582560	that has become quite compute-intensive.
582560	584640	But I think we're still in the range
584640	588080	where at a place like DeepMind salaries,
588080	592160	we're still larger than compute for the experiments.
592160	595400	Although they're tremendously, tremendously more
595400	598280	of the expenditures were on compute
598280	600800	relative to salaries than in the past.
600800	603720	If you take all of the work that's being done
603720	605320	by those humans,
605320	608360	there's like low tens of thousands of people
608360	613200	working at NVIDIA designing GPUs specialized for AI.
613200	617960	I think there's more like 70,000 people at TSMC,
617960	621760	which is the leading producer of cutting-edge chips.
621760	625520	There's a lot of additional people at companies like ASML
625520	629200	that supply them with the tools they need.
629200	630760	And then a company like DeepMind,
630760	634400	I think from their public filings,
634400	636920	they recently had 1,000 people,
636920	639640	opening, I think, is a few hundred people.
639640	640960	Anthropic is less.
640960	644080	If you add up things like Facebook AI research,
644080	646720	Google Brain, R&D,
646720	650320	you get thousands or tens of thousands of people
650320	653960	who are working on AI research.
653960	656920	We'd want to zoom in on those who are developing new methods
656920	658240	rather than narrow applications.
658240	661800	So inventing the transformer definitely counts.
661800	665000	Optimizing for some particular businesses,
665000	666720	dataset cleaning, probably not.
667720	670320	But so those people are doing this work.
670320	672720	They're driving quite a lot of progress.
672720	676320	What we observe and the growth of people
676320	679320	relative to the growth of those capabilities,
679320	681240	is that pretty consistently,
681240	685840	the capabilities are doubling on a shorter time scale
685840	689000	than the people required to do them are doubling.
689000	690800	And so there's work.
690800	693280	So we talked about hardware
693280	696160	and how historically it was pretty dramatic,
696160	700000	like four or five doublings of compute efficiency
700000	702840	per doubling of human inputs.
702840	704360	I think that's a bit lower now
704360	705960	as we get towards the end of Moore's Law.
705960	707960	Although interestingly, not as much lower
707960	708800	as you might think,
708800	711760	because the growth of inputs has also slowed recently.
711760	713240	On the software side,
713240	718240	there's some work by Teme Bessaroglu
718560	720560	and I think collaborators,
722320	724080	may have been the thesis.
724960	727800	It's called our models getting harder to find.
727800	729720	And so it's applying the same sort of analysis
729720	732960	as our ideas getting harder to find.
732960	737400	And you can look at growth rates of papers
737400	740440	from citations, employment at these companies.
740440	742000	And it seems like the doubling time
742000	746560	of these like workers driving the software advances
746560	750920	is like several years, or at least a couple of years.
750920	754040	Whereas the doubling of effective compute
754040	756320	from algorithmic progress is faster.
756320	758720	So there's a group called Epoch.
758720	762120	They've received grants from Open Philanthropy
762120	765480	and they do work collecting datasets
765520	768120	that are relevant to forecasting AI progress.
768120	771960	And so their headline results
771960	776360	for what's the rate of progress in hardware and software
776360	780680	and just like growth in budgets, ours follows.
780680	783760	So for hardware, they're looking at like a doubling
783760	785880	of hardware efficiency that's like two years.
785880	787360	It's possible it's a bit better than that
787360	790440	when you take into account certain specializations
790440	791760	for AI workloads.
791760	794160	For the growth of budgets,
794160	796880	they find a doubling time that's like something
796880	799320	like six months in recent years,
799320	802320	which is pretty tremendous relative
802320	804280	to the historical rates.
804280	806640	We should maybe get into that later.
806640	808640	And then on the algorithmic progress side,
808640	812640	mainly using ImageNet type datasets right now,
812640	815520	they find a doubling time that's less than one year.
815520	819320	And so you combine all of these things
819320	821920	and the growth of effective compute
821920	826920	for training big, big AIs, it's pretty drastic.
827760	829200	I think I saw an estimate that GPD-4
829200	831800	costs like $50 million around that range to train.
831800	836160	Now, suppose that like AGI takes 1000X that,
836160	839160	if you were just a scale of GPD-4, it might not be that.
839160	841360	I'm just just for the sake of example.
841360	843680	So part of that will come from companies
843680	845880	just spending a lot more to train the models
845880	847520	and that just greater investment.
847520	850120	Part of that will come from them having better models
850160	853560	so that what would have taken a 10X increase in the model
853560	856680	to get naively you can do with having a better model
856680	858680	that you only need to do scale up.
858680	861880	You get the same effect of increasing it by 10X
861880	863480	just from having a better model.
863480	865400	And so yeah, you can spend more money
865400	866240	on it to train a bigger model.
866240	867440	You can just have a better model
867440	871920	or you can have chips that are cheaper to train.
871920	874200	So you get more compute for the same dollars.
874200	876600	And okay, so those are the three you were describing.
876600	878320	The ways in which the quote unquote
878320	880280	effective at compute would increase.
880280	883240	From the looking at it right now, it looks like,
883240	884960	yeah, you might get two or three
884960	886720	doublings of effective compute
886720	889880	for this thing that we're calling software progress,
889880	892800	which is, which people get by asking,
892800	895720	well, how much less compute can you use now
895720	899040	to achieve the same benchmark as you achieved before?
899040	901080	There are reasons to not fully identify this
901080	902440	with like software progress
902440	904880	as you might naively think of it
904880	907480	because some of it can be enabled by the other.
907520	909440	So like when you have a lot of compute,
909440	911880	you can do more experiments
911880	914720	and find algorithms that work better.
914720	917760	Sometimes the additional compute,
917760	919600	you can get higher efficiency
919600	922160	by running a bigger model we were talking about earlier.
922160	924560	And so that means you're getting more
924560	927000	for each GPU that you have
927000	930040	because you made this like larger expenditure.
930040	933000	And that can look like a software improvement
933000	934920	because this model,
935600	937480	it's not a hardware improvement directly
937480	940040	because it's doing more with the same hardware,
940040	941480	but you wouldn't have been able to achieve it
941480	944360	without having a ton of GPUs to do the big training run.
944360	947680	The feedback loop itself involves the AI
947680	950480	that is the result of this greater effect of compute,
950480	953920	helping you train better AI, right?
953920	956160	Or use less effective compute in the future
956160	958000	to train better AI.
958000	960320	It can help on the hardware design.
960320	963200	So like NVIDIA is a fabulous chip design company.
963200	965240	They don't make their own chips.
965240	969160	They send files of instructions to TSMC,
969160	974040	which then fabricates the chips in their own facilities.
974040	979040	And so the work of those 10,000 plus people,
980160	981800	if you could automate that
981800	985760	and have the equivalent of a million people doing that work,
985760	990200	then I think you would pretty quickly get the kind
990200	991880	of improvements that can be achieved
991880	996280	with the existing nodes that TSMC is operating on.
996280	998640	You could get a lot of those chip design gains.
998640	1002600	Basically like doing the job of improving chip design
1002600	1004240	that those people are working on now,
1004240	1006160	but get it done faster.
1006160	1007480	So that's one thing.
1007480	1008640	I think that's less important
1008640	1011000	for the intelligence explosion.
1011000	1014600	The reason being that when you make an improvement
1014600	1017200	to chip design, it only applies
1017200	1019360	to the chips you make after that.
1019360	1022800	If you make an improvement in AI software,
1022800	1025920	it has the potential to be immediately applied
1025920	1028800	to all of the GPUs that you already have.
1028800	1029640	Yeah.
1029640	1032080	And so the thing that I think is most disruptive
1032080	1036280	and most important has the leading edge of the change
1036280	1039680	from AI automation of the inputs to AI
1039680	1041240	is on the software side.
1041240	1043440	At what point would it get to the point
1043440	1047480	where the AIs are helping develop better software
1047480	1049320	or better models for future AIs?
1049320	1051040	Some people claim today, for example,
1051040	1054680	that programmers at OpenAI are using co-pilot
1054680	1056560	to write programs now.
1056560	1058000	So in some sense, you're already having
1058000	1059560	that sort of feedback loop.
1059560	1063160	I'm a little skeptical of that as a mechanism.
1063160	1065000	At what point would it be the case
1065000	1067920	that the AI is contributing significantly
1067920	1069800	in the sense that it would almost be the equivalent
1069800	1071680	of having additional researchers
1071680	1073360	to AI progress in software?
1073360	1075840	The quantitative magnitude of the help
1075840	1077360	is absolutely central.
1077360	1079040	So there are plenty of companies
1079040	1080640	that make some product
1080640	1083520	that very slightly boost productivity.
1083520	1086760	So when Xerox makes fax machines,
1086760	1089360	it maybe increases people's productivity
1089360	1092680	in office work by 0.1% or something.
1092680	1095640	You're not gonna have explosive growth out of that
1095640	1100640	because, okay, now 0.1% more effective R&D at Xerox
1102440	1104920	than any customers buying the machines.
1105920	1107560	Not that important.
1107560	1110200	So I think the thing to look for
1111160	1116160	is when is it the case that the contributions from AI
1117200	1121400	are starting to become as large or larger
1121400	1123840	as the contributions from humans?
1123840	1127680	So when this is boosting their effective productivity
1127680	1132680	by 50 or 100% and if you then go from eight months
1135200	1137120	doubling time, say for effective compute
1137120	1138160	from software innovation,
1138160	1140600	things like inventing the transformer
1140600	1142560	or discovering chinchilla scaling
1142560	1144560	and doing your training runs more optimally
1144560	1146720	or creating flash attention.
1146720	1150680	Yeah, if you move that from, say, eight months to four months
1150680	1152960	and then the next time you apply that,
1152960	1156000	it significantly increases the boost you're getting
1156000	1156840	from the AI.
1156840	1158400	So maybe instead of giving a 50%
1158400	1160280	or 100% productivity boost,
1160280	1162440	that's more like a 200%.
1162720	1165560	And so it doesn't have to have been able
1165560	1168240	to automate everything involved
1168240	1170360	in the process of AI research.
1170360	1173200	It can be, it's automated a bunch of things.
1173200	1176800	And then those are being done in extreme profusion
1176800	1179480	because I think that AI can do,
1179480	1182240	you have it done much more often because it's so cheap.
1183240	1188080	And so it's not a threshold of this is human level AI.
1188080	1190520	It can do everything a human can do
1190520	1192840	with no weaknesses in any area.
1192840	1195080	It's that even with its weaknesses,
1196120	1198720	it's able to bump up the performance.
1198720	1201840	So that instead of getting like the results we would have
1201840	1205720	with the 10,000 people working on finding these innovations,
1205720	1207080	we get the results that we would have
1207080	1210000	if we had twice as many of those people
1210000	1212680	with the same kind of skill distribution.
1212680	1215520	And so that's a, it's like a demanding challenge.
1215520	1219040	It's like you need quite a lot of capability for that.
1219040	1221960	But it's also important that it's significantly less
1221960	1225720	than this is a system where there's no way you can point at it
1225720	1229040	and say in any respect, it is weaker than a human.
1229040	1232960	A system that was just as good as a human in every respect,
1232960	1235760	but also had all of the advantages of an AI,
1235760	1238640	that is just way beyond this point.
1238640	1243520	Like if you consider that there's like the output
1243520	1247080	of our existing fabs make tens of millions
1247120	1249440	of advanced GPUs per year.
1249440	1253120	Those GPUs, if they were running sort of AI software
1253120	1254480	that was as efficient as humans,
1254480	1255680	as a sample efficient,
1255680	1258200	it doesn't have any major weaknesses.
1258200	1260920	So they can work four times as long,
1262040	1264320	the 168 hour work week,
1264320	1267840	they can have much more education than any human.
1267840	1271880	So it's a human, they got a PhD,
1271880	1276880	it's like, wow, it's like 20 years of education,
1277000	1281840	maybe longer if they take a slow route on the PhD.
1281840	1285000	It's just normal for us to train large models
1285000	1289280	by eat the internet, eat all the published books ever,
1290400	1294440	read everything on GitHub and get good at predicting it.
1295800	1299440	So like the level of education vastly beyond any human,
1299440	1303080	the degree to which the models are focused on task
1304080	1307040	is higher than all, but like the most motivated humans
1307040	1309080	when they're really, really gunning for it.
1309960	1313720	So you combine the things tens of millions of GPUs,
1313720	1318720	each GPU is doing the work of the very best humans
1319120	1323560	in the world and like the most capable humans in the world
1323560	1326040	can command salaries that are a lot higher than the average
1326040	1331040	and particularly in a field like STEM or narrowly AI.
1331680	1333760	Like there's no human in the world
1333760	1337040	who has a thousand years of experience with TensorFlow
1337040	1339520	or let alone the new AI technology
1339520	1341960	that were invented the year before.
1341960	1344400	But if they were around,
1344400	1347240	yeah, they'd be paid millions of dollars a year.
1347240	1349520	And so when you consider this,
1349520	1352720	okay, tens of millions of GPUs,
1352720	1356920	each is doing the work of maybe 40,
1356920	1360840	maybe more of these kind of existing workers.
1360840	1364040	This is like going from a workforce of tens of thousands
1364040	1365520	to hundreds of millions.
1367080	1370200	You immediately make all kinds of discoveries then,
1370200	1373520	you immediately develop all sorts of tremendous technologies.
1373520	1378320	So human level AI is deep, deep
1378320	1379920	into an intelligence explosion.
1379920	1381880	Intelligence explosion has to start
1381880	1384000	with something weaker than that.
1384000	1385840	Yeah, well, what is the thing it starts with
1385840	1387680	and how close are we to that?
1387680	1390360	Because if you think of a research or an open AI or something,
1390560	1394800	these are, to be a researcher is not just completing
1394800	1398360	the hello world prompt that co-pilot does, right?
1398360	1400360	It's like, you got to choose a new idea,
1400360	1402160	you got to figure out the right way to approach it.
1402160	1403480	You perhaps have to manage the people
1403480	1405960	who are also working with you on that problem.
1405960	1409120	It's like, it's incredibly complicated skill,
1409120	1411200	portfolio skills rather than just a single skill.
1411200	1414160	So yeah, what is the point of wish
1414160	1417160	that feedback loop starts where you can even,
1417160	1420160	you're not just doing the 0.5% increase in productivity
1420160	1421880	that a sort of AI tool might do,
1421880	1424880	but is actually the equivalent of a researcher
1424880	1427320	or close to it, what is that point?
1427320	1429760	So I think maybe a way to look at it
1429760	1431800	is to give some illustrative examples
1431800	1435000	of the kinds of capabilities that you might see.
1435000	1439080	And so because these systems have to be a lot weaker
1439080	1441240	than the sort of human level things,
1441240	1444280	what we'll have is intense application
1444280	1447800	of the ways in which AIs have advantages,
1447800	1449880	partly offsetting their weaknesses.
1449880	1452240	And so AIs are cheap.
1452240	1457240	We can call a lot of them to do many small problems.
1457400	1462160	And so you'll have situations where you have dumber AIs
1462160	1465960	that are deployed thousands of times
1465960	1468740	to equal, say, one human worker.
1470360	1474520	And they'll be doing things like these voting algorithms
1474520	1478040	where you, with an LLM, you generate
1478040	1480200	a bunch of different responses
1480200	1481840	and take a majority vote among them
1481840	1484360	that improves performance sum.
1484360	1488840	You'll have things like the AlphaGo kind of approach
1488840	1492360	where you use the neural net to do search
1492360	1496160	and you go deeper with the search by plowing in more compute,
1496160	1498880	which helps to offset the inefficiency
1498880	1502520	and weaknesses of the model on its own.
1502520	1506320	You'll do things that would just be totally impractical
1506320	1509240	for humans because of the sheer number of steps.
1509240	1510920	And so an example of that would be
1510920	1513640	designing synthetic training data.
1513640	1517520	So humans do not learn by just going into the library
1517520	1520280	and opening books at random pages.
1520280	1522520	It's actually much, much more efficient
1522520	1526400	to have things like schools and classes
1526400	1529240	where they teach you things in an order that makes sense,
1529240	1530680	that's focusing on the skills
1530680	1533160	that are more valuable to learn.
1533160	1534680	They give you tests and exam,
1534680	1536840	they're designed to try and elicit the skill
1536840	1539280	they're actually trying to teach.
1539280	1541320	And right now we don't bother with that
1541320	1545160	because we can hoover up more data from the internet.
1545160	1547480	We're getting towards the end of that.
1547480	1549880	But yeah, as the AIs get more sophisticated,
1549880	1554880	they'll be better able to tell what is a useful kind
1554880	1557080	of skill to practice and to generate that.
1557080	1558600	And we've done that in other areas.
1558600	1562040	So AlphaGo, the original version of AlphaGo
1562040	1566120	was booted up with data from human go play
1566120	1569200	and then improved with reinforcement learning
1569200	1571800	and Monte Carlo tree search.
1571800	1574560	But then AlphaZero,
1574560	1577080	but they somewhat more sophisticated model
1577080	1580000	benefited from some other improvements
1580000	1583360	but was able to go from scratch.
1583360	1587400	And it generated its own data through self play.
1588280	1591120	So getting data of a higher quality
1591120	1592080	than the human data,
1592080	1594360	because there are no human players that good
1594360	1596080	available in the dataset.
1596080	1598200	And also a curriculum.
1598200	1600400	So that at any given point,
1600400	1602560	it was playing games against an opponent
1602560	1604840	of equal skill itself.
1604840	1607240	And so it was always in an area
1607240	1608560	when it was easy to learn.
1608560	1611200	If you're just always losing no matter what you do
1611200	1612960	or always winning no matter what you do,
1612960	1615840	it's hard to distinguish which things are better
1615840	1617240	and which are worse.
1617240	1620680	And when we have somewhat more sophisticated AIs
1620680	1624280	that can generate training data and tasks for themselves.
1624280	1628360	For example, if the AI can generate a lot of unit tests
1628360	1630440	and then can try and produce programs
1630440	1632840	that pass those unit tests,
1632840	1636880	then the interpreter is providing a training signal.
1636880	1640000	And the AI can get good at figuring out
1640000	1641680	what's the kind of programming problem
1641680	1644040	that is hard for AIs right now
1644040	1646560	that will develop more of the skills that I need
1647680	1649320	and then do them.
1649360	1652720	And now you're not gonna have employees at OpenAI
1652720	1654880	write like a billion programming problems.
1654880	1656760	That's just not gonna happen.
1656760	1660160	But you are gonna have AIs given the task
1660160	1662760	of producing those enormous number
1662760	1664640	of programming challenges.
1664640	1667040	In LLMS themselves, there's a paper out of Anthropa
1667040	1669520	called Constitution AI or Constitution RL
1669520	1671080	where they basically had the program
1671080	1672400	just like talk to itself and say,
1672400	1673480	like, is this response helpful?
1673480	1675400	If not, how can I make this more helpful?
1675400	1677640	And the response is improved.
1677640	1678640	And then you train the model
1678760	1680040	and the more helpful response is
1680040	1682440	that it generates by talking to itself
1682440	1684760	so that it generates natively.
1684760	1687320	And you could imagine more sophisticated ways
1687320	1688960	to do that or better ways to do that.
1688960	1691760	Okay, so then the question is, listen,
1691760	1693600	GPT-4 already costs like 50 million
1693600	1695360	or 100 million or whatever it was.
1695360	1697280	Even if we have greater effective compute
1697280	1699680	from hardware increases and better models,
1700560	1702720	it's hard to imagine how we could sustain
1702720	1706000	like four or five more orders of magnitude,
1706000	1709680	greater effective size than GPT-4
1709680	1712120	unless we're jumping in like trillions of dollars
1712120	1715040	like the entire economies of big countries
1715040	1716920	into training the next version.
1716920	1720360	So the question is, do we get something
1720360	1722480	that can significantly help with AI progress
1722480	1727480	before we run out of the sheer money and scale
1728600	1730560	and compute that would require to train it?
1730560	1731720	Do you have a take on that?
1731720	1733200	Well, first I'd say remember
1733200	1736360	that there are these three contributing trends.
1736360	1739680	So the new H-100s are significantly better
1739680	1742280	than the A-100s and a lot of companies
1742280	1746520	are actually waiting for their deliveries of H-100s
1746520	1750400	to do even bigger training runs along with the work
1750400	1752320	of hooking them up into clusters
1752320	1754240	and engineering the thing.
1754240	1756560	Yeah, so all of those factors are contributing.
1756560	1758600	And of course, mathematically,
1759920	1762880	yeah, if you do four orders of magnitude more
1762880	1764440	than 50 or a hundred million,
1764440	1767640	then you're getting to Jolene Deli territory.
1767640	1771200	And yeah, I think the way to look at it is
1772600	1774560	at each step along the way,
1774560	1778120	does it look like it makes sense to do the next step?
1779120	1782040	And so from where we are right now,
1782040	1785520	seeing the results with GPT-4 and chat GPT,
1785520	1789760	companies like Google and Microsoft and whatnot
1789760	1794000	are pretty convinced that this is very valuable.
1794000	1798720	You have like talk at Google and Microsoft with Bing
1798720	1801960	that, well, it's like billion dollar matter
1801960	1806280	to change market share in search by a percentage point.
1806280	1809240	And so that can fund a lot.
1809240	1813840	And on the far end, on the extreme,
1813840	1815680	if you automate human labor,
1815680	1818040	we have a hundred trillion dollar economy.
1818040	1821400	Most of that economy is paid out in wages.
1821400	1826400	So like between 50 and 70 trillion dollars per year.
1827040	1831600	If you create AGI, it's going to automate all of that
1831600	1835200	and keep increasing beyond that.
1835200	1838960	So the value of the completed project
1838960	1843560	is very much worth throwing our whole economy into it.
1843560	1845480	If you're gonna get the good version,
1845480	1848160	not the catastrophic destruction of the human race
1849240	1853400	or some other disastrous outcome.
1853400	1857600	And in between, it's a question of,
1857600	1861160	well, did the next step, how risky and uncertain is it
1861160	1864040	and how much growth in the revenue
1864040	1866840	you can generate with it, do you get?
1866840	1869440	And so for moving up to a billion dollars,
1869440	1871720	I think that's absolutely gonna happen.
1871720	1874720	These large tech companies have R&D budgets,
1874720	1876000	tens of billions of dollars.
1876000	1878920	And when you think about it like in the relevant sense,
1878920	1881640	like all the employees at Microsoft
1881640	1883160	who are doing software engineering,
1883160	1885640	that's like contributing to creating software objects.
1885640	1889800	It's not weird to spend tens of billions of dollars
1889800	1894800	on a product that would do so much.
1894960	1897840	And I think it's becoming more clear
1897840	1901440	that there is sort of market opportunity to fund the thing.
1901440	1903440	Going up to a hundred billion dollars,
1903440	1906120	that's like, okay, the existing R&D budgets
1906120	1909000	spread over multiple years.
1909000	1912800	But if you keep seeing that when you scale up the model,
1912800	1914640	it substantially improves the performance.
1914640	1916240	It opens up new applications.
1916240	1918720	You're not just improving your search,
1918720	1922080	but maybe it makes self-driving cars work.
1922080	1925720	You replace bulk software engineering jobs
1925720	1928440	or if not replace them, amplify productivity.
1928440	1931160	In this kind of dynamic, you actually probably want to employ
1931160	1933200	all the software engineers you can get,
1933240	1935160	as long as they're able to make any contribution
1935160	1938840	because the returns of improving stuff in AI itself
1938840	1939680	get so high.
1939680	1944680	But yeah, so I think that can go up to a hundred billion.
1944960	1948240	And at a hundred billion, you're using
1948240	1952680	like a significant fraction of our existing Vab capacity.
1952680	1957440	Like right now, the revenue of NVIDIA is like 25 billion.
1958520	1962920	The revenue of TSMC, I believe is like over 50 billion.
1964120	1969120	I checked in 2021, NVIDIA was maybe 7.5%,
1970600	1973800	less than 10% of TSMC revenue.
1974800	1979800	So there's a lot of room and most of that was not AI chips.
1980160	1981480	They have a large gaming segment.
1981480	1984040	There are data center GPUs that are used
1984040	1986640	for video and the like.
1986640	1991640	So there's room for more than an order of magnitude increase
1992640	1997640	by redirecting existing Vabs to produce more AI chips.
1997800	2000240	And they're just actually using the AI chips
2000240	2001840	that these companies have in their cloud
2001840	2003880	for the big training runs.
2003880	2007200	And so I think that's enough to go to the 10 billion
2007200	2008920	and then combine with stuff like the H100
2008920	2010320	to go up to the hundred billion.
2010320	2011640	Just to emphasize for the audience,
2011640	2013480	the initial point about revenue made,
2013480	2018480	if it cost open AI $100 million to train GPT-4
2018480	2021160	and it generates $500 million in revenue,
2021480	2023680	you pay back your expenses with the hundred million,
2023680	2026160	you have 400 million for your next training run,
2026160	2028320	then you train your GPT-4.5,
2028320	2031640	you get let's say $4 billion out of revenue out of that.
2031640	2034120	That's where the feedback group of revenue comes from,
2034120	2035600	where you're automating tasks
2035600	2037120	and therefore you're making money,
2037120	2040240	you can use that money to automate more tasks.
2040240	2044560	On the ability to redirect the fat production
2044560	2046680	towards AI chips.
2046680	2050000	So then the TLDR on,
2050000	2052600	you want $100 billion worth of compute.
2052600	2056120	I mean, fabs take what like a decade or so to build.
2056120	2057440	So given the ones we have now
2057440	2059600	and the ones that are gonna come online in the next decade,
2059600	2064200	is there enough to sustain $100 billion of GPU compute
2064200	2066800	if you wanted to spend that on a training run?
2066800	2069520	Yes, you would definitely make the hundred billion one.
2069520	2072640	How do you go up to a trillion dollar run and larger?
2073560	2076560	It's gonna involve more fab construction
2076560	2079880	and yeah, fabs can take a long time to build.
2079880	2083360	On the other hand, if in fact,
2083360	2088240	you're getting very high revenue from the AI systems
2088240	2089960	and you're actually bottlenecked
2089960	2092720	on the construction of these fabs,
2092720	2095680	then their price could skyrocket.
2095680	2100480	And that lead to measures we've never seen before
2100480	2103600	to expand and accelerate fab production.
2103600	2104920	Like if you consider,
2104920	2107640	so at the limit has you're getting models
2107640	2110800	that approach human-like capability.
2110800	2112000	Can you imagine things that are getting close
2112000	2115960	to like brain-like efficiencies plus AI advantages?
2115960	2117680	We were talking before about, well,
2117680	2121760	a GPU that is supporting an AI,
2121760	2124200	really it's a cluster of GPU supporting AI
2124200	2128120	is that do things in parallel, data parallelism.
2128120	2132920	But if that can work four times as much as a human,
2132920	2135640	a highly skilled, motivated, focused human
2135640	2138080	with levels of education that have never been seen
2138080	2139880	in the human population.
2139880	2143640	And so if like a typical software engineer
2143640	2145400	can earn hundreds of thousands of dollars,
2145400	2148080	the world's best software engineers
2148080	2149800	can earn millions of dollars today
2149800	2153960	and maybe more in a world where there's so much demand for AI.
2154960	2158960	And then times four for working all the time.
2158960	2160280	Well, I mean, if you have,
2160280	2163240	if you can generate like close to $10 million a year
2164240	2167600	out of the future version of H100,
2167600	2169760	and it costs tens of thousands of dollars
2169760	2172040	with a huge profit margin now.
2172040	2175600	And profit margin could be reduced
2175600	2176920	with like large production.
2178080	2181680	That is a big difference that chip
2181680	2184200	pays for itself almost instantly.
2185200	2190040	And so you could support paying 10 times as much
2190040	2193360	to have these fabs constructed more rapidly.
2193360	2197520	You could have, if AI is starting to be able to contribute,
2197520	2200800	you could have AI contributing more
2200800	2202880	of the skilled technical work that makes it hard
2202880	2207200	for say, NVIDIA to suddenly find thousands upon thousands
2207200	2211520	of top quality engineering hires, if AI can provide that.
2211520	2214880	Now, if AI hasn't reached that level of performance,
2214880	2217680	then this is how you can have things stall out.
2217680	2219760	And like a world where AI progress stalls out
2219760	2222080	is one where you go to the $100 billion
2222080	2225000	and then over succeeding years,
2225000	2228480	trillion dollar things, software, progress,
2229400	2232800	turns out to stall.
2232800	2234960	You lose the gains that you are getting
2234960	2236920	from moving researchers from other fields,
2236920	2239200	lots of physicists and people from other areas
2239200	2241240	of computer science have been going to AI.
2241240	2244600	But you sort of tap out those resources.
2244600	2248320	Has it AI becomes a larger proportion of the research field?
2248520	2250880	And like, okay, you've put in all of these inputs,
2250880	2253280	but they just haven't yielded AI yet.
2253280	2257240	I think that set of inputs probably would yield
2257240	2258920	the kind of AI capabilities needed
2258920	2260280	for intelligence explosion.
2260280	2263680	But if it doesn't, after we've exhausted
2263680	2266880	this current scale up of like increasing the share
2266880	2269440	of our economy that is trying to make AI,
2269440	2271800	if that's not enough, then after that,
2271800	2273720	you have to wait for the slow grind
2273720	2276080	of things like general economic growth,
2276080	2277600	population growth and such.
2277600	2278960	And so things slow.
2278960	2281280	And that results in my credences
2281280	2283680	and this kind of advanced AI happening
2283680	2287800	to be relatively concentrated like over the next 10 years
2287800	2290040	compared to the rest of the century.
2290040	2292120	Because we just can't, we can't keep going
2292120	2296640	with this rapid redirection of resources into AI.
2296640	2298440	That's a one time thing.
2298440	2301320	If the current scale up works, it's going to happen.
2301320	2302880	We're gonna get to AI really fast,
2302880	2304800	like within the next 10 years or something.
2304800	2306480	If the current scale up doesn't work,
2306520	2309560	all we're left with is just like economy
2309560	2310880	growing like 2% of years.
2310880	2314080	We have like 2% a year more resources to spend on AI.
2314080	2316680	And at that scale, you're talking about decades
2316680	2319720	before you can just through sheer brute force,
2319720	2322720	you can train the $10 trillion model or something.
2322720	2325240	Let's talk about why you have your thesis
2325240	2327960	that the current scale up would work.
2327960	2330080	What is the evidence from AI itself
2330080	2332240	or maybe from private evolution
2332240	2333760	and the evolution of other animals?
2333760	2335880	Just give me the whole, the whole confluence
2335880	2336720	of reasons that make you think.
2336720	2339280	I think maybe the best way to look at that
2339280	2342440	might be to consider when I first became interested
2342440	2344080	in this area, so in the 2000s,
2344080	2347680	which was before the deep learning revolution,
2347680	2349080	how would I think about timelines?
2349080	2351120	How did I think about timelines?
2351120	2354960	And then how have I updated based on what has been happening
2354960	2356400	with deep learning?
2356400	2360240	And so back then, I would have said,
2361480	2364000	we know the brain is a physical object
2364000	2366080	and information processing device.
2366080	2369600	It works, it's possible.
2369600	2371560	And not only is it possible,
2371560	2375160	it was created by evolution on earth.
2375160	2378520	And so that gives us something of an upper bound
2378520	2382080	in that this kind of brute force was sufficient.
2382080	2384240	There are some complexities with like,
2384240	2386720	well, what if it was a freak accident
2386720	2389280	and it didn't happen on all of the other planets
2389280	2391480	and that added some value.
2391480	2393560	I have a paper with Nick Bustrom on this.
2393560	2397720	I think basically that's not that important an issue.
2397720	2400640	There's converging evolution like octopi
2400640	2403440	are also quite sophisticated.
2403440	2408440	If a special event was at the level of forming cells at all
2408760	2410840	or forming brains at all,
2410840	2413320	we get to skip that because we're choosing
2413320	2415080	to build computers and we already exist.
2415080	2417000	We have that advantage.
2417000	2420360	So say evolution gives something of an upper bound,
2420400	2423280	really intensive massive brute force search
2424520	2426560	and things like evolutionary algorithms
2426560	2428240	can produce intelligence.
2428240	2432360	Doesn't the fact that octopi and I guess other mammals,
2432360	2434360	they got to the point of being like pretty intelligent
2434360	2436200	but not human level intelligent.
2436200	2437880	Is that some evidence that there's a hard step
2437880	2441000	between a cephalopod and a human?
2441000	2444040	Yeah, so that would be a place to look.
2445800	2448400	It doesn't seem particularly compelling.
2448400	2453400	One source of evidence on that is work by Herculano Hutzel.
2455080	2456600	I hope I haven't mispronounced her name
2456600	2461600	but she's a neuroscientist who has dissolved the brains
2461640	2466160	of many creatures and by counting the nuclei,
2466160	2470600	she's able to determine how many neurons are present
2470600	2474240	in different species and find a lot of interesting trends
2474240	2475080	in scaling laws.
2475400	2479200	She's a paper discussing the human brain
2479200	2481760	has a scaled up primate brain
2481760	2485080	and across like a wide variety of animals
2485080	2487320	and mammals in particular.
2487320	2489800	There are certain characteristic changes
2489800	2492440	in the relative number of neurons
2492440	2495800	size of different brain regions have things scale up.
2496600	2501600	There's a lot of, yeah, there's a lot of structural similarity
2502600	2507600	there and you can explain a lot of what is different about us
2507800	2510960	with a pretty brute force story,
2510960	2514360	which is that you expend resources
2514360	2518240	on having a bigger brain, keeping it in good order,
2518240	2519760	giving it time to learn.
2519760	2521680	So we have an unusually long childhood,
2521680	2523840	unusually long in its period.
2523840	2526400	We spend more compute by having a larger brain
2526400	2529000	than other animals, more than three times
2529000	2530920	as large as chimpanzees.
2530920	2534760	And then we have a longer childhood than chimpanzees
2534760	2537200	and much more than many, many other creatures.
2537200	2539200	So we're spending more compute in a way
2539200	2541680	that's analogous to like having a bigger model
2541680	2543840	and having more training time with it.
2543840	2548840	And given that we see with our AI models,
2549320	2551960	this sort of like large consistent benefits
2551960	2555000	from increasing compute spent in those ways
2555000	2558320	and with qualitatively new capabilities
2558320	2560280	showing up over and over again,
2560280	2564480	particularly in areas that sort of AI skeptics call out
2564480	2567880	in my experience like over the last 15 years,
2567880	2569680	the things that people call out has like,
2569680	2571520	ah, but the AI can't do that.
2571520	2574000	And it's because of a fundamental limitation.
2574000	2575800	We've gone through a lot of them.
2575800	2580240	There were Winograd schemas, catastrophic forgetting,
2580240	2581680	quite a number.
2581680	2586680	And yeah, they have repeatedly gone away through scaling.
2587280	2592040	And so there's a picture that we're seeing supported
2592040	2595440	from biology and from our experience with AI
2595440	2599320	where you can explain like, yeah, in general,
2599320	2602920	there are trade-offs where the extra fitness you get
2602920	2605360	from a brain is not worth it.
2605360	2609160	And so creatures wind up mostly with small brains
2609160	2612000	because they can save that biological energy
2612000	2615880	and that time to reproduce for digestion.
2615920	2616760	And so on.
2616760	2620640	And humans, we actually seem to have wound up
2620640	2623840	in a niche within self-reinforcing
2623840	2626200	where we greatly increase the returns
2626200	2627880	to having large brains.
2627880	2632880	And language and technology are the sort of obvious candidates.
2633280	2635480	When you have humans around you
2635480	2638000	who know a lot of things and they can teach you
2638000	2639720	and compared to almost any other species,
2639720	2643400	we have vastly more instruction from parents
2643440	2646000	and the society of the young.
2646000	2649240	Then you're getting way more from your brain
2649240	2651160	because you can get, per minute,
2651160	2653720	you can learn a lot more useful skills
2653720	2655920	and then you can provide the energy you need
2655920	2659080	to feed that brain by hunting and gathering,
2659080	2662280	by having fire that makes digestion easier.
2662280	2664680	And basically how this process goes on,
2664680	2667680	it's increasing the marginal increase
2667680	2669920	in reproductive fitness you get
2669920	2671680	from allocating more resources
2671680	2674840	along a bunch of dimensions towards cognitive ability.
2674840	2679000	And so that's bigger brains, longer childhood,
2679000	2681160	having our attention be more on learning.
2681160	2685880	So humans play a lot and we keep playing as adults,
2685880	2689200	which is a very weird thing compared to other animals.
2689200	2693480	We're more motivated to copy other humans around us
2693480	2695400	than like even than the other primates.
2695400	2698040	And so these are sort of motivational changes
2698040	2701800	that keep us using more of our attention and effort
2701800	2703480	on learning, which pays off more
2703480	2705920	when you have a bigger brain and a longer lifespan
2705920	2707000	in which to learn.
2707000	2711120	Many creatures are subject to lots of predation or disease.
2711120	2715120	And so if you try, you're a mayfly or a mouse,
2715120	2717320	if you try and invest in like a giant brain
2717320	2719600	and a very long childhood,
2719600	2722920	you're quite likely to be killed by some predator
2722920	2725760	or some disease before you're able to actually use it.
2725800	2728120	And so that means you actually have exponentially
2728120	2730680	increasing costs in a given niche.
2730680	2734000	So if I have a 50% chance of dying every few months
2734000	2738000	of a little mammal or a little lizard or something,
2738000	2742000	that means the cost of going from three months to 30 months
2742000	2744840	of learning and childhood development,
2744840	2747120	it's not 10 times the loss.
2747120	2749560	It's now it's two to the negative 10.
2749560	2754560	So factor of 1,024 reduction in the benefit I get
2755840	2757480	from what I ultimately learn
2757480	2761280	because 99.9% of the animals will have been killed
2761280	2762360	before that point.
2762360	2764600	We're in a niche where we're like a large,
2764600	2767960	long-lived animal with language and technology.
2767960	2770520	So where we can learn a lot from our groups.
2770520	2774960	And that means it pays off to really just expand
2774960	2779560	our investment on these multiple fronts in intelligence.
2779560	2781160	That's so interesting.
2782240	2784520	Just with the audience, the calculation about like
2784560	2786080	two to the whatever months is just like,
2786080	2787480	you have a half chance of dying this month,
2787480	2789440	a half chance of dying next month,
2789440	2790400	you multiply those together.
2790400	2792440	Okay, there's other species though
2792440	2797440	that do live in flocks or as packs where you could imagine.
2797680	2800480	I mean, they do have like a smaller version
2800480	2802920	of the development of cubs into
2802920	2804600	that I like play with each other.
2804600	2809240	Why isn't this a hill on which they could have climbed
2809240	2812360	to human level intelligence themselves?
2812400	2815280	If it's something like language or technology,
2815280	2817960	humans were getting smarter before we got language.
2817960	2819480	I mean, obviously we had to get smarter
2819480	2820440	to get language, right?
2820440	2822480	We couldn't just get language without becoming smarter.
2822480	2826480	So yeah, it seems like there should be other species
2826480	2828000	that should have beginnings
2828000	2829840	of this sort of cognitive revolution,
2829840	2832440	especially given how valuable it is given,
2832440	2834120	listen, we've dominated the world.
2834120	2836240	You would think there'd be selective pressure for it.
2836240	2838480	Evolution doesn't have foresight.
2838480	2840680	The thing in this generation
2840680	2844920	that gets more surviving offspring and grandchildren,
2844920	2847280	that's the thing that becomes more common.
2847280	2849360	Evolution doesn't look ahead and they,
2849360	2853240	oh, in a million years, you'll have a lot of descendants.
2853240	2856640	It's what survives and reproduces now.
2856640	2859120	And so in fact, there are correlations
2859120	2864120	where social animals do on average, have larger brains.
2865120	2868440	And part of that is probably that
2868440	2871000	the additional social applications of brains,
2871000	2874000	like keeping track of which of your group members
2874000	2876840	have helped you before so that you can reciprocate.
2876840	2879360	You scratch my back, I'll scratch yours,
2879360	2881840	remembering who's dangerous within the group,
2881840	2883200	that sort of thing.
2883200	2886360	It's an additional application of intelligence.
2886360	2888840	And so there's some correlation there.
2888840	2892000	But what it seems like is that,
2892440	2894640	yeah, in most of these cases,
2895640	2897960	it's enough to invest more,
2897960	2901360	but not invest to the point where a mind
2901360	2905000	can easily develop language and technology and pass it on.
2905000	2908080	And so there are, you see bits of tool use
2908080	2910920	in some other primates who have an advantage that,
2910920	2913560	so compared to say the whales who have,
2913560	2914640	they have quite large brains,
2914640	2916760	partly because they are so large themselves
2916760	2918840	and they have some other thing,
2918840	2920280	but they don't have hands,
2920280	2922680	which means that reduces a bunch of ways
2922680	2924680	in which brains can pay off
2924680	2927840	and investments in the functioning of that brain.
2927840	2932840	But yeah, so primates will use sticks to extract termites.
2933880	2935880	Capuchin monkeys will open clams
2935880	2938040	by smashing them with a rock.
2938040	2940000	So there's bits of tool use,
2940000	2944840	but what they don't have is the ability to sustain culture.
2944840	2947920	A particular primate will maybe discover
2947920	2949920	one of these tactics and maybe it'll be copied
2949920	2952040	by their immediate group.
2952040	2954160	But they're not holding onto it that well.
2954160	2956880	They're like, well, when they see the other animal do it,
2956880	2959080	they can copy it in that situation.
2959080	2960720	They don't actively teach each other,
2960720	2963800	their population locally is quite small.
2963800	2966280	So it's easy to forget things,
2966280	2968240	easy to lose information.
2968240	2971560	And in fact, they remain technologically stagnant
2971560	2974080	for hundreds of thousands of years.
2974080	2977320	And we can actually look at some human situations.
2977320	2979520	So there's an old paper,
2979520	2983040	I believe by the economist, Michael Kramer,
2983920	2986840	talks about technological growth
2986840	2990640	in the different continents for human societies.
2990640	2995640	And so you have Eurasia is the largest integrated
2995720	2997800	connected area, Africa is partly connected to it,
2997800	3000680	but the Sahara desert restricts the flow
3000680	3003320	of information and technology and such.
3003320	3004640	And then you had the Americas,
3004640	3007200	which were after the colonization from the land bridge
3007200	3011040	were largely separated and are smaller than Eurasia.
3011040	3014320	Then Australia, and then you had like smaller island situations
3014320	3015560	like Tasmania.
3015560	3019320	And so technological progress seems to have been faster
3019320	3022200	at the larger, the connected group of people.
3022960	3026480	And in the smallest groups, so like in Tasmania,
3026480	3027840	you had a relatively small population
3027840	3029480	and they actually lost technology.
3030400	3034000	So things like they lost some like fishing techniques.
3034000	3036640	And if you have a small population
3036640	3040000	and you have some limited number of people who know a skill
3040000	3042960	and they happen to die or it happened,
3042960	3046200	there's like some change in circumstances
3046200	3049720	that causes people not to practice or pass on that thing.
3049720	3051120	And then you lose it.
3051120	3054360	And if you have few people, you're doing less innovation.
3054360	3057720	The rate at which you lose technologies
3057720	3059160	to some kind of local disturbance
3059160	3062000	and the rate at which you create new technologies
3062000	3063720	can wind up in balance.
3063720	3068720	And the great change of hominids and humanity
3068880	3070440	if that we wound up in this situation,
3070440	3073360	we were accumulating faster than we were losing.
3073360	3074920	And as we accumulated,
3074920	3078160	those technologies allowed us to expand our population.
3078160	3081640	They created additional demand for intelligence
3081640	3084280	so that our brains became three times as large.
3084280	3085120	Is that chimpanzees?
3085120	3085960	Chimpanzees, yeah.
3085960	3090120	And our ancestors who had a similar brain size.
3090320	3094120	And then the crucial point, I guess, in relevance to AI
3094120	3098240	is that the selective pressures against intelligence
3098240	3102760	in other animals are not acting against these neural networks
3102760	3104080	because we are, you know,
3104080	3105520	they're not gonna get like eaten by a predator
3105520	3108240	if they spend too much time becoming more intelligent.
3108240	3109800	We're like explicitly treating them
3109800	3111040	to become more intelligent.
3111040	3114100	So we have like good first principles reason to think
3114100	3117280	that if it was scaling that made our minds this powerful
3117280	3119960	and if the things that prevented other animals
3120800	3124000	from scaling are not impinging on these neural networks,
3124000	3125280	that these things should just continue
3125280	3126440	to become very smart.
3126440	3129160	Yeah, we're growing them in a technological culture
3129160	3131880	where there are jobs like software engineer
3131880	3136320	that depend much more on sort of cognitive output
3136320	3139520	and less on things like metabolic resources
3139520	3141800	devoted to the immune system
3141800	3145560	or to like building big muscles to throw spears.
3145560	3147600	This is kind of a side note, but I'm just kind of interested.
3147600	3148760	I think you referenced at some point,
3148800	3150560	I think it's a bit of a chinchilla scaling for the audience.
3150560	3153560	This is a paper from DeepMind which describes
3153560	3155040	if you have a model of a certain size,
3155040	3156880	what is the optimum amount of data
3156880	3158720	that it should be trained on?
3158720	3160240	So you can imagine bigger models,
3160240	3163120	you can use more data to train them.
3163120	3164240	And in this way you can figure out
3164240	3165240	where should you spend your computer,
3165240	3166600	should you spend it on making the model bigger
3166600	3169200	or should you spend it on training it for longer?
3169200	3173280	I'm curious if in the case of different animals,
3173280	3174760	in some sense they're like model sizes,
3174760	3175800	they're how big their brain is
3175800	3176960	and they're training data sizes,
3176960	3178560	like how long they're cubs
3178560	3179920	or how long they're infants or toddlers
3179920	3182280	or before they're full adults.
3182280	3184720	Is there some sort of like scaling law of?
3184720	3187960	Yeah, I mean, so the chinchilla scaling isn't interesting
3187960	3191400	because we were talking earlier about the cost function
3191400	3193280	for having a longer childhood.
3193280	3196120	And so where it's like exponentially increasing
3196120	3197920	in the amount of training compute you have
3197920	3201480	when you have exogenous forces that can kill you.
3201480	3203800	Whereas when we do big training runs,
3203800	3206920	the cost of throwing in more GPUs is almost linear.
3206920	3209000	And it's much better to be linear
3209000	3210800	than exponentially decay.
3210800	3211720	Oh, that's a really good point.
3211720	3212880	As you expand resources.
3212880	3216440	And so chinchilla scaling would suggest that like,
3216440	3220160	yeah, for a brain of sort of human size,
3220160	3222840	it would be optimal to have many millions of years
3222840	3226480	of education, but obviously that's impractical
3226480	3229680	because of exogenous mortality for humans.
3229680	3232640	And so there's a fairly compelling argument
3232680	3237680	that relative to the situation where we would train AI,
3238320	3242840	that animals are systematically way under trained.
3242840	3243680	That's so interesting.
3243680	3246040	And now they're more efficient than our models.
3246040	3248040	We still have room to improve our algorithms
3248040	3250720	to catch up with the efficiency of brains,
3250720	3255720	but they are laboring under that disadvantage, yeah.
3256160	3257720	That is so interesting.
3257720	3259680	Okay, so I guess another question you could have
3259680	3264680	is humans got started on this evolutionary hill climbing
3264760	3266400	route where we're getting more intelligent
3266400	3268680	that has more benefits for us.
3268680	3271520	Why didn't we go all the way on that route?
3271520	3273240	If intelligence is so powerful,
3273240	3278240	why aren't all humans as smart as we know humans can be?
3279000	3280080	At least that smart.
3281000	3282480	If intelligence is so powerful,
3282480	3284800	like why hasn't there been stronger selective pressure?
3284800	3286400	I understand like, oh, listen, hip size,
3286400	3288360	you can't like give birth to a really big headed baby
3288360	3289200	or whatever, but you would think
3289240	3291960	evolution would figure out some way to offset
3291960	3295360	that if intelligence has such big power
3295360	3296800	and it's so useful.
3296800	3299560	Yeah, I think if you actually look at it quantitatively,
3299560	3301040	that's not true.
3301040	3303560	And even in sort of recent history,
3303560	3306640	there has been, it looks like a pretty close balance
3306640	3309960	between the costs and the benefits
3309960	3312800	of having more cognitive abilities.
3312800	3317320	And so you say like, who needs to worry
3317320	3320040	about like the metabolic costs?
3320040	3325040	Like humans put like order 20% of our metabolic energy
3325800	3329360	into the brain and it's higher for like young children.
3329360	3334200	So 20% of the, and then there's like breathing
3334200	3336800	and digestion and the immune system.
3336800	3339360	And so for most of history,
3339360	3341480	people have been dying left and right.
3341480	3343760	Like a very large proportion of people
3343760	3346480	will die of infectious disease.
3346520	3350560	And if you put more resources into your immune system,
3350560	3352080	you survive.
3352080	3355480	So it's like life or death pretty directly
3355480	3356760	via that mechanism.
3357560	3360400	And then this is related also
3360400	3363240	people die more of disease during famine.
3363240	3364480	And so there's boom or bust.
3364480	3368080	And so if you have 20% less metabolic requirements
3368080	3371200	or has anger a child and if you have a lot more,
3371200	3374800	I mean it's like 40 or 50% less metabolic requirements,
3374800	3377640	you're much more likely to survive that famine.
3377640	3379640	So these are pretty big.
3380480	3381720	And then there's a trade-off
3381720	3384040	about just cleaning and mutational load.
3384040	3387920	So every generation new mutations and errors happen
3387920	3389220	in the process of reproduction.
3389220	3394220	And so like we know there are many genetic abnormalities
3394640	3397280	that occur through new mutations each generation.
3397280	3400800	And in fact, we have Down syndrome
3400800	3403880	is the chromosomal abnormality that you can survive.
3403920	3407240	All the others just kill the embryo.
3407240	3409000	And so we never see them.
3409920	3412360	But like Down syndrome occurs a lot.
3412360	3415040	And there are many other lethal mutations
3415040	3417840	and as you go to the less damaging ones,
3417840	3420960	there are enormous numbers of less damaging mutations
3420960	3423720	that are degrading every system in the body.
3423720	3428720	And so evolution each generation has to pull away
3429680	3431200	at some of this mutational load.
3431200	3434080	And the priority with which that mutational load
3434080	3436880	is pulled out scales in proportion
3436880	3439800	to how much the traits it's affecting impact fitness.
3439800	3443680	So you got new mutations that impact your resistance
3443680	3446800	to malaria, you got new mutations
3446800	3448160	that damage brain function.
3449520	3454600	And then have those mutations are purged each generation.
3454600	3457320	If malaria is a bigger difference in mortality
3457320	3459280	than like the incremental effectiveness
3459360	3463240	of hunter-gatherer you get from being slightly more intelligent
3463240	3467160	then you'll purge that mutational load first.
3467160	3469600	And similarly, if there's like,
3469600	3473480	humans have been vigorously adapting to new circumstances.
3473480	3476360	So since agriculture, people have been developing things
3476360	3481360	like the ability to have amulets to digest breads,
3483000	3485720	the ability to like digest milk.
3485720	3488760	And if you're evolving for all of these things
3488760	3491600	and if some of the things that give an advantage for that
3491600	3493760	incidentally carry along nearby them
3493760	3496760	some negative effect on another trait
3496760	3498560	then that other trait can be damaged.
3498560	3502600	So it really matters how important to survival
3502600	3504960	and reproduction cognitive abilities were
3504960	3507440	compared to everything else that organism has to do.
3507440	3512080	And that in particular like surviving, feasting famine,
3512080	3514080	having like the physical abilities
3514080	3515920	to do hunting and gathering.
3515920	3517800	And like, even if you're like very good
3517800	3519920	at planning your hunting,
3519920	3521880	being able to throw a spear harder
3521880	3523320	can be a big difference.
3523320	3526040	And that needs energy to build those muscles
3526040	3527720	and then to sustain them.
3527720	3531520	And so given all of these factors,
3532400	3535360	it's like, yeah, it's not a slam dunk
3536240	3537480	to invest at the merge.
3537480	3540920	And like today, like having bigger brains,
3540920	3542800	for example, it's associated
3542800	3544720	with like greater cognitive ability,
3544720	3546320	but it's like, it's modest.
3547240	3549560	Large scale pre-registered studies,
3549560	3552280	pre-registered studies with MRI data.
3552280	3557280	It's like a range, maybe like a correlation of 0.25, 0.3
3558240	3562280	and the standard deviation of brain size is like 10%.
3562280	3565760	So if you double the size of the brain,
3565760	3567600	so go and the existing brain costs
3567600	3571240	like 20% of metabolic energy, go up to 40%.
3571240	3575040	Okay, that's like eight standard deviations of brain size.
3575040	3579320	If the correlation is like, say it's 0.25,
3580400	3585400	then yeah, like you get a gain from that.
3585800	3588160	Eight standard deviations of brain size,
3588160	3590680	two standard deviations of cognitive ability.
3590680	3592720	And like in our modern society
3592720	3596040	where cognitive ability is very rewarded
3596040	3600920	and like finishing school, becoming an engineer
3600920	3605280	or a doctor or whatever can pay off a lot financially.
3605280	3608360	Still the like, the average observed return
3609560	3613960	in like income is like a one or 2% proportional increase.
3613960	3615640	There's more effects of the tail,
3615640	3618520	there's more effect in professions like STEM.
3618520	3620120	But on the whole, it's not like,
3622040	3625880	if it was like a 5% increase or a 10% increase,
3625880	3628080	then you could tell a story where,
3628080	3629480	yeah, this is hugely increasing
3629560	3631080	the amount of food you could have,
3631080	3632520	you could support more children,
3632520	3634720	but it's like, it's a modest effect
3634720	3636360	and the metabolic costs will be large
3636360	3639120	and then throw in these other aspects.
3639120	3641040	And I think it's, you can tell the story else,
3641040	3645480	we can just, we can see there was not very strong,
3645480	3647920	rapid directional selection on the thing,
3647920	3651560	which there would be if like, you could,
3651560	3656560	by solving like a math puzzle, you could defeat malaria.
3657560	3660840	Like then there would be more evolutionary pressure.
3660840	3661680	That is so interesting.
3661680	3662920	And not to mention, of course,
3662920	3665080	that yeah, if you had like 2x the brain size
3665080	3666720	or you were without C-section,
3666720	3669800	you would, you or your mother would or both would die.
3669800	3671040	This is a question I've actually been curious about
3671040	3672320	for like over a year.
3672320	3674440	And I like briefly try to look up an answer.
3674440	3676840	This is, I know this is off topic,
3676840	3678400	but I apologize to the audience,
3678400	3680160	but I was super interested in those like,
3680160	3681280	those like the most comprehensive
3681280	3683040	and interesting answer I could have hoped for.
3683040	3685360	Okay, so yeah, we have a good explanation
3685560	3687560	for good first principles, evolutionary reason
3687560	3690560	for thinking that intelligence scaling up to humans
3690560	3695560	is not implausible just by throwing more scale at it.
3696280	3697800	I would also add,
3697800	3699560	this was something that would have mattered to me more
3699560	3701280	in the 2000s.
3701280	3704280	We also have the brain right here with us
3704280	3705840	for available for neuroscience
3705840	3708040	to reverse engineer its property.
3708040	3710680	And so in the 2000s, when I said, yeah,
3710680	3713320	I expect this by, you know, middle of the century ish.
3713320	3714880	That was a backstop.
3714880	3718280	If we found it absurdly difficult to get to the algorithms
3718280	3720480	and then we would learn from neuroscience,
3720480	3724480	but in the actual history, it's really not like that.
3724480	3726080	We develop things in AI.
3726080	3727480	And then also we can say, oh yeah,
3727480	3730760	this is sort of like this thing in neuroscience
3730760	3732160	or maybe this is a good explanation.
3732160	3734720	But it's not as though neuroscience
3734720	3736000	is driving AI progress.
3736000	3738720	It turns out not to be that necessary.
3738720	3742240	As similar to, I guess, you know, how planes were inspired
3742280	3744200	by the existence proof of birds,
3744200	3747440	but jet engines don't flap.
3747440	3749560	All right, so yeah, scaling,
3749560	3752040	good reason to think scaling might work.
3752040	3754360	So we spend $100 billion and we have something
3754360	3757760	that is like human level or can do help significantly
3757760	3759200	with AI research.
3759200	3761960	I mean, that might be on the earlier end,
3761960	3765080	but I mean, I definitely would not rule that out
3765080	3767400	given the rates of change we've seen
3767400	3769240	with the last few scale-ups.
3769240	3773520	All right, so at this point, somebody might be skeptical.
3773520	3774360	Okay, like listen,
3774360	3776040	we already have a bunch of human researchers, right?
3776040	3778720	Like the incremental researcher, how profitable is that?
3778720	3779640	And then you might say, well, no,
3779640	3781480	this is like thousands of researchers.
3781480	3783920	I don't know how to express a skepticism exactly,
3783920	3786200	but skepticism is skeptical of just generally
3786200	3789000	the effect of scaling up the number of people
3789000	3791480	working on the problem to rapid,
3791480	3793920	rapid progress on that problem.
3793920	3795200	Somebody might think, okay, listen,
3795200	3798040	with humans, the reason population working on a problem
3798040	3800400	is such a good proxy for progress on the problem
3800400	3802360	is that there's already so much variation
3802360	3803400	that is accounted for when you say
3803400	3805240	there's like a million people working on a problem.
3805240	3808040	You know, there's like hundreds of super geniuses
3808040	3809320	working on it, thousands of people
3809320	3810920	who are like very smart working on it.
3810920	3812880	Whereas with an AI, all the copies
3812880	3815280	are like the same level of intelligence.
3815280	3817920	And if it's not super genius intelligence,
3819240	3823840	the total quantity might not matter as much.
3823840	3828320	Yeah, I'm not sure what your model is here.
3828320	3833320	So is this a model that the diminishing returns kick off
3834840	3837400	suddenly has a cliff right where we are?
3837400	3840960	And so like there was, there were results in the past
3840960	3844760	from throwing more people at problems.
3844760	3848960	And I mean, this has been useful in historical prediction.
3848960	3852400	One of the, there's this idea of experience curves
3852400	3857400	and Wright's law basically measuring cumulative production
3857680	3860560	in a field or which is that also gonna be a measure
3860560	3863240	of like the scale of effort and investment.
3863240	3866440	And people have used this correctly to argue
3866440	3869640	that renewable energy technology like solar
3869640	3872160	would be falling rapidly in price
3872160	3874360	because it was going from a low base
3874360	3876480	of very small production runs,
3876480	3879220	not much investment in doing it efficiently.
3880060	3884980	And yeah, climate advocates correctly called out
3884980	3888380	people and people like David Roberts,
3888380	3893060	the futurist Rama is now actually has some interesting
3893060	3895980	writing on this that yeah, correctly called out
3895980	3899660	that there would be really drastic fall in prices
3899660	3901380	of solar and batteries
3901380	3904460	because of the increasing investment going into that.
3904460	3905820	The human genome project would be another.
3905820	3908960	So I'd say there's like, yeah, real, real evidence.
3908960	3912600	These observed correlations from like ideas,
3912600	3917600	getting harder to find have held over a fair range of data
3917720	3920120	and over quite a lot of time.
3920120	3925120	So I'm wondering what's the nature of the deviation
3925480	3926320	you're thinking of?
3926320	3930720	That we're talking about, maybe this is like a good way
3930720	3933660	to describe what happens when more humans enter a field.
3933660	3935720	But does it even make sense to say
3935720	3938600	like a greater population of AIs is doing AI research
3938640	3942560	if there's like more GPUs running a copy of GPT-6
3942560	3944120	doing AI research?
3944120	3947640	It just like how applicable are these economic models
3947640	3949960	of human, the quantity of humans working on a problem
3949960	3953480	to the magnitude of AIs working on a problem?
3953480	3957120	Yeah, so if you have AIs that are directly automating
3958280	3961320	particular jobs that humans were doing before,
3961320	3963280	then we say, well, with additional compute
3963280	3966080	we can run more copies of them
3966080	3969280	to do more of those tasks simultaneously.
3969280	3971520	We can also run them at greater speed.
3971520	3973560	And so some people have an intuition that like,
3973560	3976760	well, you know, what matters is like time.
3976760	3979120	It's not how many people working on problem
3979120	3980360	at a given point.
3980360	3983360	I think that doesn't bear out super well,
3983360	3986600	but AI can also be run faster than humans.
3986600	3991600	And so if you have a set of AIs that can do the work
3992240	3995400	of the individual human researchers
3995400	3999040	and run at 10 times or 100 times the speed,
3999040	4001800	then we ask, well, could the human research community
4001800	4003960	have solved the algorithm problems,
4003960	4008960	do things like invent transformers over 100 years
4009200	4010520	if we have this?
4010520	4012800	We have AIs with a population,
4012800	4015080	effective population similar to the humans,
4015080	4017320	but running 100 times as fast.
4017320	4020640	And so you have to tell a story where no,
4020640	4024120	the AI, they can't really do the same things
4024160	4025800	as the humans.
4025800	4027880	And we're talking about what happens
4027880	4032000	when the AIs are more capable of in fact doing that.
4032000	4033200	Although they become more capable
4033200	4036400	as lesser capable versions of themselves help us,
4036400	4037640	make themselves more capable, right?
4037640	4040520	So you have to like kickstart that at some point.
4040520	4045480	Is there an example in analogous situations,
4045480	4048080	is intelligence unique in the sense that you have
4048080	4051520	a feedback loop of with a learning curve
4051520	4055320	or something else, a system outputs,
4055320	4058160	are feeding into its own inputs in a way that,
4058160	4060000	because if we're talking about something like Moore's Law
4060000	4062680	or the cost of solar, you do have this way,
4062680	4064640	like we're, you know, more people are,
4064640	4065720	we're throwing more people with the problem
4065720	4068920	and it's, we're, you know, we're making a lot of progress,
4068920	4073040	but we don't have the sort of additional part of the model
4073040	4076200	where Moore's Law leads to more humans somehow
4076200	4078480	and the more humans are becoming researchers.
4078480	4080680	So you do actually have a version of that
4080720	4082200	in the case of solar.
4082200	4085280	So you have a small infant industry
4085280	4087560	that's doing things like providing solar panels
4087560	4091080	for space satellites and then getting increasing amounts
4091080	4095120	of subsidized government demand because of, you know,
4095120	4096720	worries about fossil fuel depletion
4096720	4098440	and then climate change.
4098440	4102760	You can have the dynamic where visible successes
4102760	4104880	with solar or like lowering prices
4104880	4106760	then open up new markets.
4106760	4109520	So there's a particularly huge transition
4109520	4111560	where renewables become cheap enough
4111560	4114800	to replace large chunks of the electric grid.
4114800	4118080	Earlier, you're dealing with very niche situations like,
4118080	4121480	yeah, so the satellites where you have very difficult
4121480	4125200	to refuel a satellite in place and then remote areas
4125200	4127160	and then moving to like, you know,
4127160	4130000	the super sunny, the sunniest areas in the world
4130000	4132520	with the biggest solar subsidies.
4132520	4134440	And so there was an element of that
4134440	4137560	where more and more investment has been thrown
4137600	4140600	into the field and like the market has rapidly expanded
4140600	4142080	as the technology improved.
4142080	4145520	But I think the closest analogy is actually
4145520	4148120	the long run growth of human civilization itself.
4148120	4151000	And I know you had Holden Karnofsky
4151000	4153960	from the open philanthropy project on earlier
4153960	4156040	and discuss some of this research
4156040	4160640	about the long run acceleration of human population
4160640	4161600	and economic growth.
4161600	4164280	And so developing new technologies
4164280	4167080	allowed human population to expand,
4167120	4171600	humans to occupy new habitats and new areas
4171600	4172960	and then to invent agriculture
4172960	4175040	which support the larger populations
4175040	4176520	and then even more advanced agriculture
4176520	4178720	in the modern industrial society.
4178720	4182200	And so their total technology and output
4182200	4185200	allowed you to support more humans
4185200	4188000	who then would discover more technology
4188000	4189080	and continue the process.
4189080	4192880	Now that was boosted because on top of expanding
4192880	4196120	the population, the share of human activity
4196120	4199280	that was going into invention and innovation went up.
4199280	4201320	And that was a key part of the industrial revolution.
4201320	4204520	There was no such thing as a corporate research lab
4204520	4208640	or like an engineering university prior to that.
4208640	4211600	And so you're both increasing the total human population
4211600	4213280	and the share of it going in.
4213280	4216440	But this population dynamic is pretty analogous.
4216440	4219000	Humans invent farming, they can have more humans
4219000	4221920	than they can invent industry and so on.
4221920	4223200	So maybe somebody would be skeptical
4223200	4225920	that with AI progress specifically,
4225960	4230600	it's not just a matter of some farmer
4230600	4233480	figuring out crop rotation or some blacksmith
4233480	4235760	figuring out how to do metal or do better.
4235760	4238120	You in fact, even to make the,
4238120	4240360	for the 50% improvement in productivity,
4240360	4242520	you basically need something on the IQ
4242520	4244360	that's close to Ilya Setskoper.
4244360	4246800	There's like a discontinuous,
4246800	4248840	you're like contributing very little to productivity
4248840	4251520	and then you're like Ilya and then you contribute a lot.
4251520	4254840	But the becoming Ilya is, you see what I'm saying?
4254840	4256880	There's not like a gradual increase in capabilities
4256880	4257720	that leads to the feedback.
4257720	4262240	You're imagining a case where the distribution of tasks
4262240	4264960	is such that there's nothing that you can,
4264960	4268800	where individually automating it particularly helps.
4268800	4272280	And so the ability to contribute to AI research
4272280	4273360	is really end loaded.
4273360	4274200	Is that what you're saying?
4274200	4277360	Yeah, I mean, we already see this in these sorts
4277360	4280960	of like really high IQ companies or projects
4280960	4282600	where theoretically, I guess,
4282600	4285520	Shane Street or OpenAI could hire like a bunch of,
4286480	4289480	mediocre people to do, there's a comparative advantage.
4289480	4291040	They could do some menial tasks
4291040	4293880	and that could free up the time of the really smart people,
4293880	4295960	but they don't do that, right?
4295960	4297400	Transaction costs, whatever else.
4297400	4299680	Self-driven cars would be another example
4299680	4302400	where you have a very high quality threshold.
4302400	4305160	And so when your performance as a driver
4305160	4306840	is worse than a human,
4306840	4308960	like you have 10 times the accident rate
4308960	4310760	or 100 times the accident rate,
4310760	4312920	then the cost of insurance for that,
4312920	4314680	which is a proxy for people's willingness
4314680	4316760	to ride the car instead of two,
4316760	4318440	would be such that the insurance costs
4318440	4319360	would absolutely dominate.
4319360	4320800	So even if you have zero labor cost,
4320800	4323120	it's offset by the increased insurance cost.
4323120	4324800	And so there are lots of cases like that
4324800	4329800	where like partial automation is not in practice
4330480	4334760	very usable because complimenting other resources,
4334760	4337720	you're gonna use those other resources less efficiently.
4338560	4342280	And in a post-AGI future,
4342280	4344600	I mean, the same thing can apply to humans.
4344600	4348560	So people can say, well, comparative advantage,
4348560	4351920	even if AIs can do everything better than a human,
4351920	4354280	well, it's still worth something.
4354280	4355360	The human can do something,
4355360	4358600	they can lift a box, that's something.
4359520	4361200	Now there's a question of property rights,
4361200	4364720	if, well, if they could just slice up the human
4364720	4366680	to make more robots.
4366680	4369880	But even absent that in such an economy,
4369880	4372160	you wouldn't want to let a human worker
4372160	4373720	into any industrial environment,
4373720	4375160	because in a clean room,
4375160	4377480	they'll be emitting all kinds of skin cells
4377480	4379260	and messing things up.
4379260	4380920	You need to have an atmosphere there.
4380920	4383240	You need a bunch of supporting tools
4383240	4384680	and resources and materials.
4384680	4387540	And those supporting resources and materials
4387540	4390320	will do a lot more productively,
4390320	4392640	working with AI and robots rather than a human.
4392640	4395680	So you don't wanna let a human anywhere near the thing,
4395680	4398520	just like in a, you don't wanna have a gorilla
4398520	4399880	wandering around in a China shop.
4399880	4402600	Even if you've trained it to most of the time,
4402600	4404800	pick up a box for you if you give it a banana,
4404800	4406640	it's just not worth it to have it wandering
4406640	4407480	around your China shop.
4407480	4408320	Yeah, yeah, yeah.
4408320	4410560	Like why is that not a good objection to?
4410560	4414040	I mean, I think that is one of the ways
4414040	4417760	in which partial automation can fail
4417760	4421000	to really translate into a lot of economic value.
4421000	4423720	That's something that will attenuate as we go on.
4423800	4427000	And as the AI is more able to work independently
4427000	4430200	and more able to handle its own,
4430200	4433320	its own screw ups, get more reliable.
4433320	4435440	But the way in which it becomes more reliable
4435440	4438440	is by AI progress speeding up,
4438440	4440960	which happens if AI can contribute to it.
4440960	4444880	But if there is some sort of reliability bottleneck,
4444880	4446520	the principle of contributing to that progress,
4446520	4447600	then you don't have the loop, right?
4447600	4450960	So, yeah, I mean, this is why we're not there yet.
4450960	4453080	Right, but then what is the reason to think we'll be there at?
4453080	4456800	The broad reason is we have these inputs are scaling up.
4458880	4461560	There's a, so Epoch, which I mentioned earlier,
4461560	4464040	they have a paper, I think it's called compute trends
4464040	4467480	in three areas of machine learning or something like that.
4467480	4471800	And so they look at the compute expended
4471800	4473920	on machine learning systems
4473920	4476040	since the founding of the field of AI
4476040	4478480	at the beginning of the 1950s.
4478480	4481120	And so it mostly, it grows with Moore's law.
4482120	4484920	And so people are spending a similar amount
4484920	4489080	on their experiments, but they can just buy Moore with that
4489080	4491400	because the compute is coming.
4491400	4495640	And so that data, I mean, it covers over 20 orders
4495640	4497320	of magnitude, maybe like 24.
4498960	4503920	And of all of those increases since 1952,
4503920	4508920	a little more than half of them happened between 1952 and 2010.
4509080	4512200	And all the rest is since 2010.
4512200	4516520	So we've been scaling that up like four times as fast
4516520	4520120	as was the case for most of the history of AI.
4520120	4523640	We're running through the orders of magnitude
4523640	4527280	of possible resource inputs you could need for AI
4527280	4529720	much, much more quickly than we were
4529720	4530840	for most of the history of AI.
4530840	4533000	That's why this is a period of like
4533000	4536760	with a very elevated chance of AI per year
4536760	4538720	because we're moving through.
4538720	4540680	So much of the space of inputs per year.
4540680	4543640	And indeed, it looks like this scale up
4543640	4548280	taken to its conclusion will cover another bunch
4548280	4550240	of orders of magnitude.
4550240	4553720	And that's actually a large fraction of those that are left
4553720	4555680	before you start running into saying, well,
4555680	4558800	this is gonna have to be like evolution
4558800	4561360	with the sort of simple hacks we get to apply.
4561360	4564200	Like we're selecting for intelligence the whole time.
4564200	4566600	We're not going to do the same mutation
4566600	4570680	that causes fatal childhood cancer a billion times.
4570680	4574240	Even though, I mean, we keep getting the same fatal mutations
4574240	4575880	even though they've been done many times.
4575880	4578840	We use gradient descent, which takes into account
4578840	4581880	the derivative of improvement on the loss
4581880	4583120	all throughout the network.
4583120	4586520	And we don't throw away all the contents of the network
4586520	4589240	with each generation where you can press down
4589240	4590640	to a little DNA.
4590640	4593200	So there's that bar of like, well,
4593200	4595440	if you're gonna do brute force like evolution
4595440	4597680	combine with these sort of very simple ways
4597680	4599600	we can save orders of magnitude on that.
4601200	4603840	We're gonna cover, I think, a fraction
4603840	4607520	that's like half of that distance in this scale
4607520	4609240	up over the next 10 years or so.
4609240	4613240	And so if you started off with a kind of vague uniform prior,
4613240	4616800	you're like, well, you probably can't make AGI
4616800	4619520	with like the amount of compute that would be involved
4619520	4622080	in a fruit fly existing for a minute,
4622080	4624000	which would be the early days of AI.
4624960	4626440	You know, maybe you would get lucky.
4626440	4627960	We were able to make calculators
4627960	4630040	because calculators benefited
4630040	4634080	from like very reliable, serially fast computers
4634080	4637600	and where we could take a tiny, tiny, tiny, tiny fraction
4637600	4640360	of a human brain's compute and use it for a calculator.
4640360	4642920	We couldn't take an ant's brain and rewire it to calculate.
4642920	4647600	It's hard to manage ant farms, let alone get them
4647600	4649320	to do arithmetic for you.
4649320	4652160	And so there were some things where we could exploit
4652160	4657160	the differences between biological brains and computers
4657160	4660040	to do stuff super efficiently on computers.
4660040	4662400	We would doubt that we would be able
4662400	4665280	to do so much better than biology,
4665280	4668560	that with a tiny fraction of an insect's brain,
4668560	4670400	we'd be able to get AI early on.
4670400	4673520	On the far end, it seemed very implausible
4673520	4674440	that we couldn't do better
4674440	4676560	than completely brute force evolution.
4676560	4678320	And so in between, you have some number
4678360	4682520	of orders of magnitude of inputs where it might be.
4682520	4683920	And like in the 2000s, I would say,
4683920	4687120	well, you know, I'm gonna have a pretty uniformish prior.
4687120	4690080	I'm gonna put weight on it happening at like the sort
4690080	4694040	of the equivalent of like 10 to the 25 ops,
4694040	4696480	10 to the 30, 10 to the 35,
4697520	4699280	and sort of spreading out over that.
4699280	4701320	And then I can update on other information.
4701320	4705080	And in the short term, I would say like in 2005, I would say,
4705080	4707320	well, I don't see anything that looks
4707320	4709000	like the cusp of AGI.
4709000	4711040	So I'm also gonna lower my credence
4711040	4714000	for like the next five years or the next 10 years.
4714000	4716640	And so that would be kind of like a vague prior.
4716640	4718720	And then when we take into account like, well,
4718720	4720120	how quickly are we running through
4720120	4721320	those orders of magnitude?
4721320	4725520	If I have a uniform prior, I assign half of my weight
4725520	4728400	to the first half of remaining orders of magnitude.
4728400	4730200	And if we're gonna run through those
4730200	4731840	over the next 10 years in some,
4732840	4736640	then that calls on me to put half of my credence
4736640	4738160	conditional on wherever we're gonna make it.
4738160	4740840	AI, which seems likely it's a material object
4740840	4742440	easier than evolution.
4742440	4745400	I've got to put similarly a lot of my credence
4745400	4747320	on AI happening in this scale up.
4747320	4749680	And then that's supported by what we're seeing
4749680	4754040	in terms of the rapid advances in capabilities
4754040	4756280	with AI and LLOMs in particular.
4756280	4758480	Okay, that's actually a really interesting point.
4758480	4760560	So now that somebody might say, listen,
4760560	4763920	there's not some sense in which AIs could universally
4764000	4767280	speed up the progress of open AI by 50%
4767280	4769480	or 100% or 200%.
4769480	4772280	If they're not able to do everything better
4772280	4774240	than Ilias Escobar can,
4774240	4777040	there's going to be something in which we're bottlenecked
4777040	4778840	by the human researchers.
4778840	4782080	And bottleneck effects dictate that,
4782080	4783600	the slowest moving part of the organization
4783600	4785880	will be the one that kind of determines the speed
4785880	4787400	of the progress of the whole organization
4787400	4788680	or the whole project.
4788680	4790360	Which means that unless you get to the point
4790360	4791560	where you're like doing everything
4791560	4793520	and everybody in the organization can do,
4793520	4795480	you're not going to significantly speed up
4795480	4797640	the progress of the whole project as a whole.
4797640	4800320	Yeah, so that is a hypothesis.
4800320	4802760	And I think there's a lot of truth to it.
4802760	4804200	So when we think about like the ways
4804200	4805960	in which AI can contribute.
4805960	4807520	So there are things we talked about before,
4807520	4810480	like the AIs setting up their own curriculum.
4811320	4814320	And that's something that Ilya can't do directly
4814320	4815680	doesn't do directly.
4815680	4816680	And there's a question,
4816680	4819800	how much does that improve performance?
4819800	4824280	There are these things where the AI helps
4824280	4827360	to just like produce some code for some tasks.
4827360	4829920	And it's beyond Hello World at this point.
4829920	4832160	But I mean, the sort of thing that I hear
4832160	4836160	from AI researchers at leading labs is that,
4836160	4839680	on their core job where they're like most expert,
4839680	4841280	it's not helping them that much,
4841280	4843680	but then their job often does involve,
4843680	4845600	oh, I've got to code something
4845600	4849040	that's out of my usual area of expertise.
4849040	4852040	Or I want to research this question and it helps them there.
4852040	4854120	And so that saves some of their time
4854120	4857600	and frees them to do more of the bottleneck work.
4857600	4860840	And then I think the idea of, well,
4862320	4865800	is everything dependent on Ilya and is Ilya
4865800	4870280	so much better than the hundreds of other employees?
4870280	4872440	I think a lot of people who are contributing,
4872440	4874760	they're doing a lot of tasks.
4874760	4879280	And so you can have quite a lot of gain
4879280	4881400	from automating some areas
4881400	4884600	where you then do just an absolutely enormous amount of it
4884600	4886800	relative to what you would have done before
4886800	4889720	because things like designing the custom curriculum,
4889720	4892920	you're like, maybe had some humans put some work into that,
4892920	4895440	but you're not going to employ billions of humans
4895440	4896760	to produce it at scale.
4896760	4900120	And so it winds up being a larger share of the progress
4901160	4902160	than it was before.
4902160	4905160	You get some benefit from these sorts of things
4905160	4908760	where, yeah, there's like pieces of my job
4909800	4911800	that now I can hand off to the AI
4911800	4914320	and let's me focus more on the things
4914320	4915920	that the AI still can't do.
4917120	4921720	And then at the later on, you get to the point where,
4921720	4924240	yeah, the AI can do your job,
4924240	4925720	including the most difficult parts.
4925720	4928640	And maybe it has to do that in a different way.
4928640	4932480	Maybe it like spends a ton more time thinking about
4932480	4935720	each step of a problem than you, and that's the late end.
4935720	4938440	And the stronger these bottlenecks effects are,
4938440	4942920	the more the economic returns, the scientific returns
4942920	4947000	and such are end loaded towards getting sort of full AGI.
4947000	4948680	The weaker the bottlenecks are,
4948680	4952600	the more interim results will be really paying off.
4952600	4954440	I guess I'd probably disagree with you on how much
4954440	4958160	the sort of the alias of organizations seem to matter.
4958160	4959600	I guess just from the evidence alone,
4959600	4962440	like how many of the big sort of breakthroughs
4962440	4965360	that in deep learning in general was like
4965360	4967760	that single individual responsible for, right?
4967760	4969880	And how much of his time is he spending
4969880	4971640	doing anything that's not that like co-pilot
4971640	4972480	is helping him on?
4972480	4974120	I'm guessing like most of it is just like managing people
4974120	4977280	and coming up with ideas and, you know,
4977280	4979440	trying to like understand systems and so on.
4979440	4982840	And if that is the, if like the five or 10 people
4982840	4986120	who are like that at OpenAI or Anthropa or whatever
4986120	4991080	are basically the way in which the progress is happening
4991080	4993440	or at least the algorithmic progress is happening,
4993440	4997960	then how much of better and better co-pilot,
4997960	4999960	I know co-pilot is not the thing you're talking about
4999960	5002280	with like the 20% automation, but something like that,
5002280	5005400	how much of, yeah, how much is that contributing
5005400	5009600	to the sort of like core function of the research scientist?
5009600	5010440	Yeah.
5010440	5013800	Naturally quantitatively, how much we disagree
5013800	5018800	about the importance of sort of key research employees
5020160	5024320	and such, I certainly think that some researchers,
5024320	5027680	you know, add, you know, more than 10 times
5027680	5030280	the average employee even much more.
5030280	5034000	And obviously managers can add an enormous amount of value
5034000	5036320	by proportionately multiplying the output
5036320	5038720	of the many people that they manage.
5039680	5042480	And so that's the kind of thing
5042480	5045120	that we were discussing earlier when talking about,
5045120	5049840	well, if you had sort of full human level AI
5049840	5052880	or AI that had all of the human capabilities
5052880	5056600	plus AI advantages, it would be, you know,
5056600	5059160	you'd benchmark not off of what the sort of typical
5059160	5062800	human performance is, but peak human performance and beyond.
5062800	5064980	So yeah, I accept all that.
5066120	5071080	I do think it makes a big difference for people
5071240	5074400	how much they can outsource a lot of the death
5074400	5076480	that are less, wow, less creative.
5076480	5079960	And an enormous amount is learned by experimentation.
5079960	5084520	ML has been, you know, a quite experimental field.
5084520	5086280	And there's a lot of engineering work
5086280	5089560	in say building large super clusters,
5089560	5093680	making, yeah, hardware aware optimization
5093680	5095120	and coding of these things,
5095120	5099000	being able to do the parallelism in large models.
5099000	5103360	And the engineers are busy
5103360	5107640	and it's not just only a big thoughts kind of area.
5107640	5112640	And then the other branch is where will the AI advantages
5115360	5116960	and disadvantages be?
5116960	5121960	And so one AI advantage is being omnidisciplinary
5122920	5125440	and familiar with the newest things.
5125440	5128040	So I mentioned before, there's no human
5128040	5131680	who has a million years of TensorFlow experience.
5131680	5134060	And so to the extent that we're interested
5134060	5136160	in like the very, very cutting edge
5136160	5138800	of things that have been developed quite recently
5138800	5141680	than AI that can learn about them in parallel
5141680	5144280	and experiment and practice with them in parallel
5144280	5147080	can learn much faster than a human potentially.
5148440	5151200	And the area of computer science is one
5151200	5154600	that is especially suitable for AI
5154600	5156400	to learn in a digital environment.
5156400	5160600	So it doesn't require like driving a car around
5160600	5163000	that might kill someone, have enormous costs.
5163000	5168000	You can do unit tests, you can prove theorems,
5169000	5170920	you can do all sorts of operations
5170920	5174080	entirely in the confines of a computer.
5174080	5177040	And which is one reason why programming
5177040	5180080	has been benefiting more than a lot of other areas
5180080	5183640	from LLMs recently, whereas robotics is lagging.
5183640	5184680	So the sum of that.
5184680	5187560	And then just considering, well,
5187560	5189560	actually I mean, they are getting better
5189560	5194560	at things like the GRE math at programming contests.
5195400	5199040	And I mean, some people have forecasts
5199040	5201760	and predictions outstanding about things like
5201760	5205120	doing well on the Informatics Olympiad
5205120	5206960	and the Math Olympiad.
5206960	5210760	And in the last few years, when people tried
5210760	5213720	to forecast the MMLU benchmark,
5213760	5215920	which was having a lot of more sophisticated
5215920	5220440	kind of like graduate student science kind of questions.
5221640	5225760	Yeah, AI knocked that down a lot faster
5225760	5228280	than AI researchers who had registered
5228280	5230880	and students who had registered forecasts on it.
5230880	5234160	And so if you're getting top-notch scores
5234160	5239160	on graduate exams, creative problem solving,
5239600	5243240	yeah, it's not obvious that that sort of area
5243240	5246680	will be a relative weakness of AI.
5246680	5250320	That in fact, computer science is in many ways,
5250320	5253640	especially suitable because of getting up to speed
5253640	5257840	with new areas, being able to get rapid feedback
5257840	5260840	from the interpreter at scale.
5260840	5262040	But did you get rapid feedback?
5262040	5264120	If you're doing that, something that's more analogous
5264120	5266760	to research, if you're like,
5266760	5268720	let's say you have a new model or something.
5268720	5272320	And it's like, if we put in $10 million
5272800	5275200	on a mini-training run on this, this would be a much bit.
5275200	5277240	Yeah, for very large models,
5277240	5279320	those experiments are gonna be quite expensive.
5279320	5281320	And so you're gonna look more at like,
5281320	5285440	can you build up this capability by generalization
5285440	5287400	from things like many math problems,
5287400	5290320	programming problems, working with small networks?
5290320	5291800	Yeah, yeah, fair enough.
5291800	5294320	I actually, Scott Aaronson was one of my professors
5294320	5297640	in college and I took his quantum information class
5297640	5300400	and I didn't do, I did okay in it,
5300400	5304920	but he recently wrote a blog post where he said,
5304920	5307040	I had GBD-4 take my quantum information test
5307040	5308680	and it got a B.
5308680	5311880	And I was like, damn, I got a C on the final.
5311880	5314760	So yeah, yeah, I'm updated in the direction that,
5314760	5317360	you know, it seems, getting a B on the test,
5317360	5318600	like you probably understand quantum information
5318600	5319440	pretty well.
5319440	5321560	With different areas of strengths and weaknesses
5321560	5322400	than the human students.
5322400	5323520	Sure, sure.
5323520	5326800	Would it be possible for this sort of intelligence
5326800	5330200	explosion to happen without any sort of hardware progress
5330200	5332040	if hardware progress stopped?
5332040	5334560	Would this feedback loop still be able to produce
5334560	5337120	some sort of explosion with only software?
5337120	5341640	Yeah, so if we say that the technology is frozen,
5341640	5344560	which I think is not the case right now,
5344560	5347960	the, you know, NVIDIA has managed to deliver
5347960	5350360	significantly better chips for AI workloads
5350360	5354640	for the last few generations, H100, A100, V100.
5354640	5358520	If that stops entirely, then what you're left with,
5359160	5361840	maybe we'll define this as like no more nodes,
5361840	5363640	more as a lot is over.
5363640	5365880	At that point, the kind of gains you get
5365880	5368960	in amount of compute available come from actually
5368960	5371680	constructing more chips.
5371680	5373160	And there are economies of scale
5373160	5374720	you could still realize there.
5374720	5379720	So right now, a chip maker has to amortize the R&D cost
5380240	5382280	of developing the chip.
5382280	5384840	And then the capital equipment is created,
5384840	5388360	like you build a fab, its peak profits are gonna come
5388440	5391400	in the few years when the chips it's making
5391400	5393200	are at the cutting edge.
5393200	5397080	Later on, as the cost of compute exponentially falls,
5397080	5399080	the, you know, you keep the fab open
5399080	5400280	because you can still make some money
5400280	5403720	given that it's built, but of all of the profits
5403720	5405920	the fab will ever make right now,
5405920	5407760	they're relatively front loaded
5407760	5411080	because when its technology is near the cutting edge.
5411080	5413800	So in a world where Moore's law ends,
5413800	5418000	then you wind up with these very long production runs.
5418600	5421160	Where you can keep making chips
5421160	5423040	that stay at the cutting edge
5423040	5426480	and where the R&D costs get amortized
5426480	5428440	over a much larger base.
5428440	5431840	So the R&D basically drops out of the price.
5431840	5433840	And then you get some economies of scale
5433840	5436800	from just making so many fabs in the way that,
5436800	5439800	you know, when we have the auto industry expands
5439800	5442200	and then this is in general across industries
5442200	5446440	when you produce a lot more costs fall
5446520	5451000	because you have right now, like ASML has many,
5451000	5453000	you know, incredibly exotic suppliers
5453000	5455840	that make some bizarre part of the thousands of parts
5455840	5457960	in one of these ASML machines.
5457960	5460480	You can't get it anywhere else.
5460480	5463560	They don't have standardized equipment for their thing
5463560	5466280	because this is the only, only use for it.
5466280	5469680	And in a world where we're making 10, 100 times
5469680	5472480	as many chips at the current node,
5472480	5475200	then they would benefit from scale economies.
5475360	5478680	And all of that would become more mass production
5478680	5479880	industrialized.
5479880	5481880	And so you combine all of those things
5481880	5484480	and it seems like capital costs
5484480	5486840	of like buying a chip would decline,
5486840	5490120	but the energy costs of running the chip would not.
5490120	5494120	And so right now, energy costs are a minority of the cost,
5494120	5497000	but they're not, they're not, they're not trivial.
5497000	5500640	You know, they pass, yeah, it passed 1% a while ago
5500640	5504440	and they're inching up towards 10% and beyond.
5505560	5509400	And so you can maybe get like another order of magnitude
5510400	5514520	costs decrease from getting really efficient
5514520	5515840	in the sort of capital construction,
5515840	5519080	but like energy would still be a limiting factor
5519080	5522200	after the end of sort of actually improving
5522200	5523040	the chips themselves.
5523040	5523880	Got it, got it.
5523880	5524720	And when you say like,
5524720	5526520	there would be a greater population of AI researchers
5526520	5530240	because are we using population as a sort of thinking tool
5530240	5531800	of how they could be more effective?
5531800	5534800	Or do you literally mean that the way you expect
5534800	5537400	these AIs to contribute a lot to researchers
5537400	5539960	by just having like a million copies
5539960	5543400	of this, of like a researcher thinking about the same problem?
5543400	5544720	Or is it just like a useful thinking model
5544720	5547440	for what it would look like to have a million times
5547440	5548840	smarter AI working on that problem?
5548840	5551880	That's definitely a lower bound sort of model.
5551880	5553840	And often I'm meaning something more like
5555000	5558600	effective population or like you'd need this many people
5558600	5559640	to have this effect.
5559640	5561560	And so we were talking earlier about the trade off
5561600	5565840	between training and inference in board games.
5565840	5568720	And so you can get the same performance
5568720	5571000	by having a bigger model
5571000	5572640	or by calling the model more times.
5572640	5575200	And in general, it's more effective
5575200	5577240	to have a bigger, smarter model
5577240	5579840	and call it less timed up until the point
5579840	5582000	where the costs equalized between them.
5582000	5584400	And so we would be taking some of the gains
5584400	5587880	of our larger compute on having bigger models
5587880	5590360	that are individually more capable.
5590360	5592560	And there would be a division of labor.
5592560	5595040	So like the tasks that were most cognitively demanding
5595040	5596440	would be done by these giant models,
5596440	5598480	but some very easy tasks.
5598480	5600840	You don't want to expend that giant model
5600840	5605240	if a model 100s the size can take that task.
5605240	5608320	And so larger models would be in the positions
5608320	5610560	of like researchers and managers
5610560	5614080	and they would have swarms of AIs of different sizes
5614080	5618040	as tools that they could make API calls to and whatnot.
5618080	5620160	Okay, we accept the model
5620160	5622160	and now we've gone to something that is at least as smart
5622160	5625520	as Ilya Suskova on all the tasks relevant to AI progress.
5625520	5628800	And you can have so many copies of it.
5628800	5630040	What happens in the world now?
5630040	5631880	What are the next months or years
5631880	5633480	or whatever timeline is relevant to look like?
5633480	5638480	And so, and to be clear with what's happened is not
5638800	5641680	that we have something that has all of the abilities
5641680	5644600	and advantages of humans plus the AI advantages.
5644600	5647800	What we have is something that is like possibly
5647800	5650520	by doing things like doing a ton of calls
5650520	5653720	to make up for being individually less capable or something.
5653720	5657480	It's able to drive forward AI progress.
5657480	5659040	That process is continuing.
5659040	5662560	So AI progress has accelerated greatly
5662560	5663920	in the course of getting there.
5663920	5666880	And so maybe we go from our eight months doubling time
5666880	5671040	of software progress in effective compute
5671040	5673800	to four months or two months.
5674680	5678600	And so, so there's a report by Tom Davidson
5678600	5680720	at the Open Philanthropy Project,
5680720	5685240	which spun out of work I had done previously.
5685240	5690240	And so I advised and helped with that project
5691880	5693800	but Tom really carried it forward
5693800	5696760	and produced a very nice report and model
5696760	5698920	which Epoch is hosting.
5698920	5701880	You can plug in your own version of the parameters
5702360	5706640	and there's a lot of work estimating the parameter.
5706640	5710400	Things like what's the rate of software progress?
5710400	5712240	What's the return to additional work?
5712240	5715680	How does performance scale at these tasks
5715680	5718120	as you boost the models?
5718120	5721160	And in general, as we were discussing earlier,
5721160	5726160	these sort of like broadly human level in every domain
5726160	5731160	with all the advantages is pretty deep into that.
5732520	5737200	And so if already we can have an eight months doubling time
5737200	5741760	for software progress, then by the time you get
5741760	5745280	to that kind of point, it's maybe more like four months,
5745280	5749280	two months going into one month.
5749280	5754160	And so if the thing is just proceeding at full speed,
5754160	5758760	then each doubling can come more rapidly.
5758760	5763760	And so we can talk about what are the spillovers of like,
5764280	5766440	so how does the models get more capable?
5766440	5768680	They can be doing other stuff in the world.
5768680	5770720	They can spend some of their time
5770720	5772520	making Google search more efficient.
5772520	5777520	They can be hired, has chatbots with some inference compute.
5778200	5780760	And then we can talk about sort of
5781760	5784720	if that intelligence explosion process
5784720	5787440	is allowed to proceed, then what happens is,
5787440	5792440	okay, you improve your software by a factor of two,
5793480	5797840	the demand, the efforts needed to get the next doubling
5797840	5799720	are larger, but they're not choices large.
5799720	5802720	Maybe they're like 25%, 35% larger.
5803840	5806960	So each one comes faster and faster
5806960	5809200	until you hit limitations.
5809440	5813440	Like you can no longer make further software advances
5813440	5815040	with the hardware that you have.
5816160	5819800	And looking at, I think, reasonable parameters
5819800	5821920	in that model, it seems to me,
5821920	5823640	if you have these giant training runs,
5823640	5824840	you can go very far.
5825840	5829520	And so the way I would see this playing out
5829520	5832840	is how does the AIs get better and better at research?
5832840	5834720	They can work on different problems.
5834720	5836520	They can work on improving software.
5836520	5838560	They can work on improving hardware.
5838560	5841760	They can do things like create new industrial technologies,
5841760	5843320	new energy technology.
5843320	5844960	They can manage robots.
5844960	5846840	They can manage human workers
5846840	5849960	as like executives and coaches and whatnot.
5849960	5852320	You can do all of these things.
5852320	5856960	And AIs wind up being applied where the returns are highest.
5856960	5861000	And I think initially the returns are especially high
5861000	5862920	in doing more software.
5862920	5865640	And the reason for that is again,
5866480	5868320	if you improve the software,
5868320	5870040	you can update all of the GPUs
5870040	5874680	that you have access to your cloud compute
5874680	5876640	is suddenly more potent.
5876640	5880880	If you design a new chip design,
5881960	5885400	it'll take a few months to produce the first ones
5885400	5888680	and it doesn't update all of your old chips.
5888680	5891960	So you have an ordering where you start off
5891960	5895160	with the things where there's the lowest dependence
5895480	5898440	on existing stocks.
5898440	5900880	And you can more just take whatever you're developing
5900880	5901880	and apply it immediately.
5901880	5904360	And so software runs ahead.
5904360	5909080	You're getting more towards the limits of that software.
5909080	5910600	And I think that means things like
5910600	5912600	having all the human advantages
5912600	5915440	but combined with AI advantages.
5915440	5920440	And so I think that means given the kind of compute
5922000	5923520	that would be involved,
5923520	5925400	if we're talking about this hundreds of billions
5925400	5928600	of trillion dollar training run,
5928600	5931200	there's enough compute to run tens of millions,
5931200	5935600	hundreds of millions of sort of like human scale minds.
5935600	5938320	They're probably smaller than human scale
5939440	5942240	to be like similarly efficient at the limits
5942240	5943080	of algorithmic progress
5943080	5944040	because they have the advantage
5944040	5945400	of a million years of education.
5945400	5948280	They have the other advantages we talked about.
5948280	5950440	So you've got that wild capability
5951400	5954240	and further software gains are running out.
5954240	5957480	Or like they start to slow down again
5957480	5960400	because you're just getting towards the limits
5960400	5963360	of like you can't do any better than the best.
5963360	5965640	And so what happens then?
5965640	5968000	Yeah, by the time they're running out,
5968000	5970360	have we already hit superintelligence or?
5970360	5973440	Yes, you're wildly superintelligent.
5973440	5975600	We love the galaxy, okay, metaphorically.
5975600	5978520	Just by having the abilities that humans have
5978520	5981400	and then combining it with being very well focused
5981400	5984000	and trained in the task beyond what any human could be
5984000	5985560	and then running faster and such.
5985560	5986400	Got it, got it.
5986400	5987240	All right, so I continue.
5987240	5988600	Yeah, so I'm not gonna assume
5988600	5992360	that there's like huge qualitative improvements you can have.
5992360	5995120	I'm not gonna assume that humans are like very far
5995120	5997240	from the efficient frontier of software,
5997240	6000040	except with respect to things like,
6000040	6001760	yeah, we had limited lifespan
6001760	6003560	so we couldn't train super intensively.
6003560	6007680	We couldn't incorporate other software into our brains.
6007680	6009000	We couldn't copy ourselves.
6009000	6010640	We couldn't run at fast speeds.
6011640	6014700	Yeah, so you've got all of those capabilities.
6015360	6019880	And now I'm skipping ahead of like the most important months
6019880	6021160	in human history.
6022720	6025480	And so I can talk about sort of,
6026840	6031520	what it looks like if it's just the AIs took over,
6031520	6036160	they're running things that they like, how do things expand?
6036160	6038760	I can talk about things has,
6038760	6043760	how does this go in a world where we've roughly
6044280	6048720	or at least so far managed to retain control
6048720	6050800	of where these systems are going?
6051880	6053800	And so by jumping ahead,
6053800	6055400	I can talk about how would this translate
6055400	6056840	into the physical world?
6056840	6058760	And so this is something that I think is a stopping point
6058760	6061120	for a lot of people in thinking about,
6061120	6063960	well, what would an intelligence explosion look like?
6064040	6066080	And they have trouble going from,
6066080	6069520	well, there's stuff on servers and cloud compute
6069520	6071440	and oh, that gets very smart.
6071440	6074640	But then how does what I see in the world change?
6074640	6077760	How does like industry or military power change?
6077760	6081120	If there's an AI takeover, like what does that look like?
6081120	6083040	Are there killer robots?
6083040	6086080	And so yeah, so one course we might go down
6086080	6091080	is to discuss during that wildly accelerating transition,
6091920	6093880	how did we manage that?
6093880	6096440	How do you avoid it being catastrophic?
6096440	6101440	And another route we could go is how does the translation
6101440	6106440	from wildly expanded scientific R&D capabilities intelligence
6108680	6113680	on these servers translate into things in the physical world?
6113680	6116440	So you're moving along in order of like,
6116440	6120320	what has the quickest impact largely
6121080	6126080	or like where you can have an immediate change?
6127800	6132800	So one of the most immediately accessible things
6132800	6137720	is where we have large numbers of devices
6137720	6142720	or artifacts or capabilities that are already AI operable
6143640	6148560	with hundreds of millions equivalent researchers,
6148560	6152120	you can like quickly solve self-driving cars,
6153320	6156560	you make the algorithms much more efficient,
6156560	6159640	do great testing and simulation
6159640	6163000	and then operate a large number of cars in parallel
6163000	6166040	if you need to get some additional data
6166040	6167640	to improve the simulation and reasoning.
6167640	6171160	Although, in fact, humans with quite little data
6172560	6175800	are able to achieve human level driving performance.
6175840	6179320	So after you've really maxed out
6179320	6181800	the easily accessible algorithmic improvements
6181800	6184320	in this software based intelligence explosion
6184320	6186440	that's mostly happening on server farms,
6186440	6190600	then you have minds that have been able to really perform
6190600	6194240	on a lot of digital only tasks that they're doing great
6194240	6197520	on video games, they're doing great at predicting
6197520	6200520	what happens next in a YouTube video.
6200520	6202760	If you have a camera that they can move,
6202760	6204680	they're able to predict what will happen
6205800	6208120	at different angles, humans do this a lot
6208120	6210280	where we naturally move our eyes in such a way
6210280	6213600	to get images from different angles
6213600	6214680	and different presentations
6214680	6217240	and then predicting combined from that.
6217240	6220600	And yeah, and you can operate many cars,
6220600	6225480	many robots at once to get very good robot controllers.
6225480	6229480	So you should think that all the existing robotic equipment
6229480	6233880	or remotely controllable equipment that is wired for that,
6233880	6236200	the AI's can operate that quite well.
6236200	6238640	I think some people might be skeptical
6238640	6241760	that existing robots, given their current hardware,
6241760	6244800	have the dexterity and the maneuverability
6244800	6247760	to do a lot of physical labor
6247760	6249080	that any AI might want to do.
6249080	6250240	Do you have reason for thinking otherwise?
6250240	6252000	There's also not very many of them.
6252000	6254520	So production of sort of industrial robots
6254520	6256920	is hundreds of thousands per year.
6258240	6261600	They can do quite a bit in place.
6261640	6264480	Eveline Musk is promising a robot
6264480	6265600	in the tens of thousands of,
6265600	6268680	humanoid robot in the tens of thousands of dollars,
6268680	6273200	that may take a lot longer than he has said.
6273200	6275000	Has this happened with other technologies?
6275000	6277320	But I mean, that's a direction to go.
6277320	6279400	But most immediately,
6279400	6283400	so hands are actually probably the most scarce thing.
6284560	6287280	But if we consider what do human bodies provide?
6287280	6289040	So there's the brain.
6289040	6290960	And in this situation,
6290960	6294360	we have now an abundance of high quality brain power
6294360	6295400	that will be increasing,
6295400	6298560	as the AI's will have designed new chips,
6298560	6302000	which will be rolling out from the TSMC factories,
6302000	6304800	and they'll have ideas and designs
6304800	6308120	for the production of new fab technologies,
6308120	6311040	new nodes and additional fabs.
6311920	6313000	But yeah, looking around the body.
6313000	6315240	So there's legs to move around.
6315240	6316400	Not only that necessary,
6316400	6318400	wheels work pretty well being in a place.
6318400	6321720	You don't need most people most of the time
6321720	6323720	in factory jobs and office jobs.
6323720	6327400	Office jobs, many of them can be fully virtualized.
6328360	6333160	But yeah, some amount of legs, wheels, other transport,
6333160	6336840	you have hands and hands are something that are,
6336840	6340760	on the expensive end in robots, we can make them.
6340760	6343640	They're made in very small production runs,
6343640	6345440	partly because we don't have the control software
6345440	6346280	to use them well.
6346640	6349280	In this world, the control software is fabulous,
6349280	6353160	and so people will produce much larger production runs
6353160	6356840	of them over time, possibly using technology
6356840	6359760	we recognize possibly with quite different technology,
6359760	6361880	but just taking what we've got.
6362960	6367000	So right now, the robot arm industry,
6367000	6368680	the industrial robot industry produces
6368680	6371520	hundreds of thousands of machines a year.
6371520	6375320	Some of the nicer ones are like $50,000.
6375320	6377440	In aggregate, the industry has tens of billions
6377440	6379160	of dollars of revenue.
6379160	6383000	By comparison, the automobile industry produces
6383000	6386440	like I think over 60 million cars a year.
6386440	6390940	It has revenue of over $2 trillion per annum.
6391760	6396760	And so converting that production capacity
6397280	6400360	over towards robot production would be one of the things,
6400360	6402320	if they're not something better to do,
6402320	6404000	would be one of the things to do.
6404000	6409000	And in World War II, industrial conversion
6409000	6412800	of American industry took place over several years
6413800	6418800	and really amazingly ramped up military production
6420120	6423000	by converting existing civilian industry.
6423000	6427200	And that was without the aid of superhuman intelligence
6427200	6430040	and management at every step in the process.
6430040	6435040	So yeah, every part of that would be very well designed.
6435280	6438400	You'd have AI workers who understood,
6438400	6440280	stood every part of the process
6440280	6443400	and could direct human workers.
6443400	6448000	Even in a fancy factory, most of the time,
6448000	6451960	it's not the hands doing a physical motion
6453040	6454480	that a worker is being paid for.
6454480	6456680	They're often like looking at things
6456680	6459640	or like deciding what to change.
6459640	6463760	The actual, the time spent in manual motion
6463760	6465120	is a limited portion of that.
6465120	6468680	And so in this world of abundant AI cognitive abilities
6469880	6472480	where the human workers are more valuable
6472480	6474640	for their hands than their heads,
6474640	6477800	then you could have a worker,
6477800	6480520	even a worker previously without training
6480520	6484840	and expertise in the area who has a smartphone,
6484840	6487640	maybe a smartphone on a headset.
6487640	6489400	And we have billions of smartphones,
6489400	6493160	which have eyes and ears and methods for communication,
6493160	6495520	for an AI to be talking to a human
6495520	6498680	and directing them in their physical motions
6498680	6503680	with skill as a guide and coach that is beyond any human.
6504480	6507720	There could be a lot better at telepresence and remote work
6507720	6510800	and they can provide VR and augmented reality guidance
6510800	6515520	as to help people get better at doing the physical motions
6515520	6517640	that they're providing in the construction.
6517640	6522640	Say you convert the auto industry to robot production.
6524080	6528520	If it can produce an amount of mass of machines
6528520	6531160	that is similar to what it currently produces,
6531160	6536160	that's enough for billion human size robots a year.
6539360	6544360	The value per kilogram of cars is somewhat less
6545120	6547040	than high-end robots,
6547040	6552040	but yeah, you're also cutting out most of the wage bill,
6552360	6554840	because most of the wage bill is payments ultimately
6554840	6557160	to like human capital and education,
6557160	6562160	not to the physical hand motions and lifting objects
6562160	6563960	and that sort of task.
6563960	6566760	Yeah, so at the sort of existing scale of the auto industry,
6566760	6568560	you can make a billion robots a year.
6568560	6572960	The auto industry is two or 3% of the existing economy.
6573840	6577240	You're replacing these cognitive things.
6577240	6580080	So if right now physical hand motions
6580080	6585080	are like 10% of the work, redirect humans into those tasks
6586560	6590840	and you have like in the world at large right now,
6592000	6595120	mean income is on the order of $10,000 a year,
6595120	6598280	but in rich countries, skilled workers earn
6598280	6600000	more than 100,000 per year.
6600000	6605000	And some of that is not just management roles
6605440	6607720	of which only a certain proportion of the population
6607720	6612720	can have, but just being an absolutely exceptional peak
6613040	6617560	and human performance of some of these construction
6617560	6619800	and such roles.
6619800	6623920	Yeah, just raising productivity to match
6623920	6628680	the most productive workers in the world is room
6628720	6631320	to make a very big gap.
6632280	6636640	And with AI replacing skills that are scarce in many places
6636640	6641480	where there's an abundant currently low wage labor,
6641480	6644280	you bring in the AI coach and someone who is previously
6644280	6648060	making very low wages can suddenly be super productive
6648060	6652040	by just being the hands for an AI.
6652040	6657040	And so on a naive view, if you ignore the delay
6657320	6659960	of capital adjustment of like building new tools
6659960	6664360	for the workers, say like, yeah, just like raise
6664360	6667840	typical productivity for workers around the world
6667840	6669680	to be more like rich countries
6671460	6674680	and get 5x, 10x like that.
6674680	6679160	Get more productivity by with AI handling
6679160	6682640	the difficult cognitive tasks, reallocating people
6682640	6686600	from like office jobs to providing physical motions
6686600	6689200	and since right now that's a small proportion of the economy,
6689200	6693160	you can expand the sort of hands for manual labor
6693160	6697480	by like an order of magnitude like within a rich country
6697480	6701480	by just because most people are sitting in an office
6701480	6704840	or even in a factory floor or not continuously moving.
6704840	6709040	So you've got billions of hands flying around in humans
6709040	6712400	to be used in the course of constructing
6712400	6713640	your waves of robots.
6713640	6718000	So now once you have a quantity of robots
6718000	6719960	that is approaching the human population
6719960	6722280	and they work 24 seven, of course,
6723520	6726520	the human labor will no longer be valuable
6726520	6729560	has hands and legs, but at the very beginning
6729560	6732440	of the transition, just like new software
6732440	6736640	can be used to update all of the GPUs to run the latest AI.
6737520	6740680	Humans are sort of legacy population
6740680	6744760	with an enormous number of underutilized hands and feet
6744760	6748440	that the AI can use for the initial robot construction.
6748440	6750680	Cognitive tasks are being automated
6750680	6753520	and the production of them is greatly expanding
6753520	6757680	and then the physical tasks which complement them
6757680	6762080	are utilizing humans to do the parts that robots that exist can't do.
6762080	6763560	Is the implication of this that you're getting to
6763560	6766680	that world production would increase just a tremendous amount
6766680	6770600	or that AI could get a lot done of whatever motivations it has
6771160	6775560	Yeah, so there's an enormous increase in production
6775560	6779920	for humans who just switching over to the role
6779920	6784560	of providing hands and feet for AI where they're limited.
6784560	6788920	And this robot industry is a natural place to apply it.
6788920	6792640	And so if you go to something that's like 10x
6792640	6795160	the size of like the current car industry
6795160	6798040	in terms of its production,
6798040	6800920	which would still be like a third of our current economy
6800920	6803800	and the aggregate productive capabilities of the society
6803800	6806920	with AI support are going to be a lot larger.
6806920	6810640	They make 10 billion human-eyed robots a year.
6810640	6815760	And then if you do that the legacy population
6815760	6820480	of a few billion human workers is no longer very important
6820480	6822000	for the physical tasks.
6822000	6826400	And then the new automated industrial base
6826400	6830240	can just produce more factories, produce more robots.
6830240	6831880	And then the interesting thing is like,
6831880	6833480	what's the doubling time?
6833480	6837480	How long does it take for a set of computers,
6837480	6841200	robots, factories and supporting equipment
6841200	6844400	to produce another equivalent quantity of that?
6844400	6848920	For GPUs, brains, this is really, really easy, really solid.
6848920	6851320	There's an enormous margin there.
6851320	6855200	We're talking before about, yeah,
6855240	6860160	skilled human workers getting paid $100 an hour
6860160	6865160	is like quite normal in developed countries
6865480	6867880	for very in-demand skills.
6867880	6872880	And you make a GPU that can do that work.
6874360	6878360	Right now, these GPUs are like tens of thousands of dollars.
6879240	6884240	If you can do $100 of wages each hour,
6885080	6890080	then in a few weeks, you pay back your costs.
6890160	6892480	If the thing is more productive,
6892480	6893840	and as we were discussing,
6893840	6897640	you can be a lot more productive than a sort of a typical,
6897640	6899160	high-paid human professional,
6899160	6901200	by being like the very best human professional,
6901200	6902280	and even better than that,
6902280	6903880	by having a million years of education
6903880	6905440	and working all the time.
6905440	6908760	Yeah, then you could get even shorter payback times.
6908760	6912000	Like, yeah, you can generate the dollar value
6912000	6916280	of the cost, initial cost of that equipment
6916280	6917520	within a few weeks.
6917520	6921240	For robots, so like a human factory worker
6922080	6926040	can earn $50,000 a year.
6926040	6929480	You know, really top-notch factory workers
6929480	6933040	earning more and working all the time.
6933040	6937000	If they can produce a few $100,000 of value per year
6937000	6941120	and buy a robot that costs $50,000 to replace them,
6941120	6946000	then that's a payback time of some months.
6946000	6948880	That is about the financial return.
6948880	6952000	Yeah, and we're gonna get to the physical capital return
6952000	6954520	because those are gonna diverge in this scenario.
6954520	6955640	Because right now...
6955640	6957760	Because it seems like it's gonna be given that like,
6957760	6959080	all right, these super intelligence companies
6959080	6960760	are gonna be able to make a lot of money.
6960760	6961880	They're gonna be like very valuable.
6961880	6963200	Can they like physically scale up?
6963200	6965000	What we really care about are like,
6965000	6969480	the actual physical operations that a thing does.
6969480	6972760	How much do they contribute to these tasks?
6972760	6975640	And I'm using this as a start
6975640	6979720	to try and get back to the physical replication times.
6979720	6981200	And so I guess I'm wondering,
6981200	6982880	what is the implication of this?
6982880	6984640	Because I think you started off this by saying
6984640	6986280	like people have not thought about
6986280	6989960	what the physical implications of super intelligence would be.
6989960	6992280	What is the bigger takeaway?
6992280	6993720	What are we wrong about when we think about
6993720	6995760	what the world will look like with super intelligence?
6995760	7000680	With robots that are optimally operated by AI.
7000680	7002680	So like extremely finely operated
7002680	7006440	and with building technological designs
7006440	7010360	and equipment and facilities under AI direction.
7010360	7014040	How much can they produce?
7014040	7018800	For a doubling the AI is to produce stuff
7018800	7021880	that is an aggregate,
7022160	7026400	at least equal to their own cost.
7026400	7030840	And so now we're pulling out these things like labor costs
7030840	7033280	that no longer apply and then trying to zoom in
7033280	7035920	on like what these capital costs will be.
7035920	7037680	You're still gonna need the raw materials.
7037680	7039880	You're still gonna need the robot time
7039880	7041680	built in the next robot.
7041680	7045360	I think it's pretty likely that with the advanced AI work
7045360	7047680	they can design some incremental improvements
7047680	7051400	and the industry scale up that you can get 10 fold
7051400	7056400	and better cost reductions on the system
7056960	7059080	by making things more efficient
7059080	7062520	and replacing the human cognitive labor.
7062520	7067520	And so maybe that's like you need $5,000 of costs
7068680	7071400	under our current environment.
7071400	7073480	But the big change in this world
7074360	7077640	is we're trying to produce this stuff faster.
7077640	7079400	If we're asking about the doubling time
7079400	7083480	of the whole system in say one year,
7083480	7086320	if you have to build a whole new factory
7086320	7087920	to like double everything,
7087920	7090960	you don't have time to amortize the cost of that factory.
7090960	7092320	Like right now you might build a factory
7092320	7094320	and use it for 10 years
7094320	7097360	and like buy some equipment and use it for five years.
7097360	7100120	And so that's part of your, that's your capital cost.
7100120	7102480	And in an accounting context,
7102480	7107480	you depreciate each year a fraction of that capital purchase.
7108080	7111960	But if we're trying to double our entire industrial system
7111960	7116960	in one year, then those capital costs have to be multiplied.
7117200	7120700	So if we're going to be getting most of the return
7120700	7123200	on our factor in the first year,
7123200	7126620	instead of 10 years, weighted appropriately,
7126620	7127540	then we're gonna say, okay,
7127540	7129720	our capital cost has to go up by 10 fold
7131560	7134080	because I'm building an entire factory
7134080	7135600	for this year's production.
7135600	7137080	I mean, it will do more stuff later,
7137080	7141520	but it's most important early on instead of over 10 years.
7141520	7146520	And so that's going to raise the cost of that reproduction.
7148440	7153440	And so it seems like going from current like decade
7153600	7157960	kind of cycle of amortizing factories and fabs and whatnot
7157960	7160060	and shorter for some things,
7160060	7162720	the longest or things like big buildings and such.
7163720	7165900	Yeah, that could be like a 10 fold increase
7165900	7170060	from moving to a double the physical stuff each year
7170060	7170940	in capital costs.
7170940	7174300	And given the savings that we get in the story
7174300	7176460	from scaling up the industry,
7176460	7179640	from removing the payments to human cognitive labor,
7180660	7184220	and then from just adding new technological advancements
7184220	7186660	and like super high quality cognitive supervision,
7186660	7189480	like applying more of it than was applied today.
7189480	7192980	And it looks like you can get cost reductions
7192980	7196220	that offset that increased capital capital cost.
7196220	7201220	So that like, your $50,000 improved robot arms
7202180	7203940	or industrial robots,
7203940	7205820	it seemed like that can do the work
7205820	7208340	of a human factory worker.
7208340	7210100	So it would be like the equivalent of hundreds
7210100	7212460	of thousands of dollars.
7212460	7214780	And like, yeah, they would cook,
7214780	7219780	by default may cost more than the $50,000 arms today,
7219820	7221740	but then you apply all these other cost savings.
7221740	7224620	And then it looks like then you get a period,
7224620	7228060	a robot doubling time that is less than a year,
7228060	7232380	I think significantly less than a year as you get into it.
7232380	7234500	So in this first phase,
7234500	7237980	you have humans under AI direction
7237980	7240500	and like existing robot industry
7240500	7244140	and converted auto industry and expanded facilities,
7244140	7249140	making robots those over less than a year,
7250140	7254220	you've produced robots until their combined production
7254220	7258660	is exceeding that of like humans has armed and feet.
7258660	7261900	And then yeah, you could have over a period then
7261900	7263740	with a doubling time of months,
7264940	7267820	the less sort of clanking replicators robots
7267820	7269980	as we understand them growing.
7269980	7274980	And then that's not to say that's the limit of like
7275300	7277260	the most that technology could do
7278260	7282060	because biology is able to reproduce at faster rates
7282060	7284900	and maybe worth talking about that in a moment.
7284900	7286860	But if we're trying to like restrict ourselves
7286860	7290020	to like robotic technology as we understand it
7290020	7292140	and sort of cost falls that are reasonable
7292140	7295900	from eliminating all labor, massive industrial scale up
7295900	7298420	and sort of historical kinds of technological improvements
7298420	7299780	that lowered costs.
7299780	7304780	I think you can get into a robot population industry
7305420	7306460	doubling in months.
7307220	7309340	And then what is the implication
7309340	7312460	of the biological doubling times?
7312460	7314220	And I guess this doesn't have to be a biological
7314220	7318540	but you can have like, you can do like a direct slur
7318540	7320620	like first principles, how much would it cost
7320620	7324580	to view both a nanotech thing that like built more nanobots.
7324580	7326220	I certainly take the human brain
7326220	7329380	and other biological brains as like very relevant data points
7329380	7332500	about what's possible with computing and intelligence.
7332500	7334580	Like with the reproductive capability
7334620	7339020	of biological plants and animals and microorganisms
7339020	7342860	I think is relevant as like this is,
7342860	7346380	it's possible for systems to reproduce at least this fast.
7346380	7349660	And so at the extreme, you have bacteria
7349660	7350660	that are heterotrophic.
7350660	7354380	So they're feeding on some abundant external food source
7354380	7356060	and ideal conditions.
7356060	7357660	And there are some that can divide
7357660	7360420	like every 20 or 60 minutes.
7360420	7364700	So obviously that's absurdly, absurdly fast.
7364700	7366980	That seems on the low end
7366980	7369940	because ideal conditions require actually setting them up.
7369940	7372460	There needs to be abundant energy there.
7373380	7376980	And so if you're actually having to acquire that energy
7376980	7379580	by building solar panels
7379580	7383580	or like burning combustible materials or whatnot
7383580	7386300	and then the physical equipment
7386300	7389380	to produce those ideal conditions can be a bit slower.
7389380	7394380	Cyanobacteria, which are self-powered from solar energy
7395340	7397780	the really fast ones in ideal conditions
7397780	7399820	can double in a day.
7399820	7401340	A reason why cyanobacteria
7401340	7405220	isn't like the food source for everyone and everything
7405220	7408620	is it's hard to ensure those ideal conditions
7408620	7410940	and then to extract them from the water.
7410940	7414260	I mean, they do of course power the aquatic ecology
7414260	7417380	but they're floating in liquid
7418260	7420340	getting resources that they need to them
7420340	7424340	and out is tricky and then extracting your product.
7424340	7427580	But like, yeah, one day double in times
7427580	7430820	are possible powered by the sun.
7430820	7435420	And then if we look at things like insects
7435420	7439060	so fruit flies can have hundreds of offspring
7439060	7442540	in a few weeks, you extrapolate that over a year
7442540	7445460	and you just fill up anything accessible.
7445460	7449260	Certainly expanding a thousand fold.
7449260	7451820	Right now, humanity uses less than 11,000s
7451820	7455940	of the solar energy or the heat envelope of the earth.
7455940	7458340	Certainly you can get done with that in a year
7458340	7463340	if you can reproduce at that rate, your industrial base.
7463740	7468780	And then even interestingly with the flies
7468780	7470100	they do have brains.
7470100	7473500	They have a significant amount of computing substrate.
7473500	7475900	And so there's something of a point or two.
7475900	7479740	Well, if we could produce computers in ways as efficient
7479740	7481540	as the construction of brains
7481540	7483780	then we could produce computers very effectively.
7483780	7486780	And then the big question about that is
7486780	7491180	the kind of brains that get constructed biologically
7491180	7494700	they sort of grow randomly and then are configured in place.
7494700	7497020	It's not obvious you would be able to make them
7497020	7501220	have an ordered structure like a top-down computer chip
7501260	7503980	that would let us copy data into them.
7503980	7506180	And so something that where you can't just copy
7506180	7508980	your existing AIs and integrate them
7508980	7512020	is gonna be less valuable than a GPU.
7512020	7514340	Well, what are the things you couldn't copy?
7514340	7518020	A brain grows by cell division
7518020	7521020	and then random connections are formed.
7521020	7521860	Got it, got it.
7521860	7524180	And so every brain is different
7524180	7526180	and you can't rely on just,
7526180	7529420	yeah, we'll just copy this file into the brain.
7529420	7531860	For one thing, there's no input output for that.
7531860	7533380	You need to have that.
7533380	7535540	But also like the structure is different.
7535540	7538940	So you can't, you wouldn't be able to copy things exactly.
7538940	7542740	Whereas when we make a CPU or GPU
7542740	7544780	they're designed incredibly finely
7544780	7546180	and precisely and reliably.
7546180	7549700	They break with incredibly tiny imperfections.
7549700	7551460	And they are set up in such a way
7551460	7553500	that we can input large amounts of data,
7553500	7556820	copy a file and have the new GPU run
7556820	7558980	and AI just as capable as any other.
7558980	7560700	Whereas with a human child,
7560700	7562660	they have to learn everything from scratch
7562660	7564580	because we can't just like connect them
7564580	7566060	to a fiber optic cable
7566060	7567940	and they're immediately a productive adult.
7567940	7569500	So there's no genetic bottleneck.
7569500	7571180	You can just directly get the...
7571180	7573100	Yeah, and you can share the benefits
7573100	7574860	of these giant training runs and such.
7574860	7577300	And so that's a question of like how,
7577300	7580940	if you're growing stuff using biotechnology,
7580940	7584580	how you could sort of effectively copy and transfer data.
7584580	7587820	And now you mentioned sort of Eric Drexler's ideas
7587820	7592820	about creating non-biological nanotechnology
7593220	7594700	sort of artificial chemistry
7594700	7598540	that was able to use covalent bonds
7598540	7603540	and produce in some ways have a more industrial approach
7604060	7604980	to molecular object.
7604980	7609060	Now there's controversy about like, will that work?
7609060	7611220	How effective would it be if it did?
7611220	7614060	And certainly if you can get things,
7614060	7614900	however you do it,
7614900	7619900	that are like onto biology in their reproductive ability,
7620140	7625140	but can do computing or like be connected
7625300	7627380	to outside information systems,
7627380	7629940	then that's pretty tremendous.
7629940	7633380	So you can produce physical manipulators
7633380	7637300	and compute at ludicrous speeds.
7637300	7638140	And there's no reason to think
7638140	7639620	in principle they couldn't, right?
7639620	7640580	In fact, in principle,
7640580	7642380	we have every reason to think they could.
7642380	7643380	There's like-
7643380	7646060	The reproductive ability is absolutely.
7646060	7646900	Yeah.
7646900	7647740	Because biology-
7647740	7648580	Or even nanotech, right.
7648580	7649860	Because biology does that.
7649860	7650700	Yeah.
7650700	7654260	There's sort of challenges to the sort of,
7654260	7658020	the practicality of the necessary chemistry.
7658020	7658860	Yeah.
7658860	7662060	I mean, my bet would be that we can move beyond biology
7662060	7664140	in some important ways.
7664140	7665820	For the purposes of this discussion,
7665820	7669540	I think it's better not to lean on that
7669540	7673740	because I think we can get to many of the same conclusions
7673740	7678180	on things that just are more universally accepted.
7678180	7680660	The bigger point being that very quickly,
7680660	7681580	once you have super intelligence,
7681580	7686460	you get to a point where the thousand X greater energy profile
7686460	7688100	that the sun makes available to the earth
7688100	7691300	is a great portion of it is used by the AI.
7691300	7692300	It can wrap with these scale-
7692300	7694540	Well, or by the civilization-
7694540	7695380	Sure, sure.
7695380	7696220	Empowered by it.
7696900	7699900	That could be an AI civilization
7699900	7701700	or it could be a human AI civilization.
7701700	7705180	And it depends on how well we manage things
7705180	7707140	and what the underlying state of the world is.
7707140	7707980	Yeah. Okay.
7707980	7708820	So let's talk about that.
7708820	7709740	Should we start at,
7709740	7711500	when we're talking about how they could take over?
7711500	7714820	Is it best to start at a sort of subhuman intelligence
7714820	7716260	or should we just talk at,
7716260	7717580	we have a human level intelligence
7717580	7720100	and the takeover or the lack thereof
7720100	7722620	is how that would happen?
7722620	7726180	To me, different people might have
7726180	7728820	somewhat different views on this.
7729700	7733580	But for me, when I am concerned about
7734860	7737380	either sort of outright destruction of humanity
7737380	7741980	or an unwelcome AI takeover of civilization,
7743020	7747580	most of the scenarios I would be concerned about
7747580	7752380	pass through a process of AI being applied
7752380	7755860	to improve AI capabilities and expand.
7755860	7759900	And so this process we were talking earlier
7759900	7763140	about where AI research is automated,
7763140	7766020	you get to effectively research labs,
7766020	7768260	companies, a scientific community
7768260	7772060	running within the server farms of our cloud compute.
7772060	7775180	So open hands are basically turned into like a program,
7775180	7776020	like a closed circuit.
7776020	7779980	Yeah, and with a large fraction of the world's compute,
7779980	7782740	probably going into whatever training runs
7782740	7786940	and AI societies, there'd be economies of scale
7786940	7789340	because if you put it in twice as much compute
7789340	7792780	and this AI research community goes twice as fast,
7793940	7796780	that's a lot more valuable than having
7796780	7798460	two separate training runs.
7798460	7800820	There would be some tendency to bandwagon.
7800820	7805420	And so like if you have some small startup,
7806860	7809740	even if they make an algorithmic improvement,
7809740	7813300	running it on 10 times, 100 times or two times,
7813300	7816820	if it's like talking about say Google and Amazon teaming up,
7816820	7819900	I'm actually not sure what the precise ratio
7819900	7821700	of their cloud resources is.
7821700	7824020	Since the sort of really these interesting
7824020	7827140	intelligence explosion impacts come from the leading edge,
7827140	7831380	there's a lot of value in not having separated
7831380	7835260	walled garden ecosystems and having the results
7835260	7837380	being developed by these AIs be shared,
7837380	7839860	have training, larger training runs be shared.
7839860	7840700	Okay.
7840700	7843340	And so I'm imagining this is something like,
7844420	7848980	some very large company or consortium of companies,
7848980	7852420	likely with a lot of sort of government interest
7852420	7855020	and supervision, possibly with government funding,
7855980	7860980	producing this enormous AI society in their cloud,
7861980	7866500	which is doing all sorts of existing kind of AI applications
7866540	7871180	and jobs as well as these internal R&D tasks.
7871180	7872900	And so at this point, somebody might say,
7872900	7874620	this sounds like a situation that would be good
7874620	7877300	from a takeover perspective because listen,
7877300	7879820	if it's gonna take like tens of billions of dollars
7879820	7881940	with a compute to continue this training
7881940	7885620	for this AI society, it should not be that hard
7885620	7888580	for us to pull the brakes if needed.
7888580	7889620	As compared to, I don't know,
7889620	7891980	something that could like run on a very small,
7891980	7894420	like single CPU or something.
7894420	7895260	Yeah, yeah, okay.
7895740	7899220	How would it, it's like, there's an AI society
7899220	7901180	that is a result of these training runs
7901180	7904780	and now it is the power to improve itself on these servers,
7904780	7907020	would we be able to stop it at this point?
7907020	7912020	And what does a sort of attempt at takeover look like?
7912300	7915260	We're skipping over why that might happen.
7915260	7917780	For that, I'll just briefly refer to
7917780	7922420	and incorporate by reference some discussion
7922420	7926800	by my open philanthropy colleague, Ajiya Kotra.
7927940	7932820	She has a piece about, I think it's called something
7932820	7937820	like the default, but the default outcome of training AI
7938140	7938980	on our-
7938980	7939820	Without specific countermeasures.
7939820	7941700	Without specific countermeasures,
7941700	7943380	default outcome is AI takeover.
7943380	7948300	But yes, so, basically we are training models
7948300	7953300	that for some reason vigorously pursue a higher reward
7953340	7955340	or a lower loss.
7955340	7957620	And that can be because they wind up with some motivation
7957620	7959140	where they want reward.
7960180	7965180	And then if they had control of their own training process,
7965340	7967260	they can ensure that it could be something
7967260	7969660	like they develop a motivation around
7969660	7973100	a sort of extended concept of reproductive fitness,
7974660	7977060	not necessarily at the individual level,
7977060	7981820	but over the generations of training tendencies
7981820	7983660	that tend to propagate themselves,
7985820	7987620	sort of becoming more common.
7987620	7990460	And it could be that they have some sort of goal
7990460	7993980	in the world, which is served well
7995340	7998660	by performing very well on the training distribution.
7998660	8001300	By tendencies, do you mean like power speaking behavior?
8001300	8004540	Yeah, so an AI that behaves well
8004540	8006060	on the training distribution
8006060	8009540	because say it wants it to be the case
8009540	8013500	that its tendencies wind up being preserved
8013500	8016460	or selected by the training process
8016460	8021460	will then behave to try and get very high reward
8022420	8024300	or low loss be propagated.
8024300	8025940	But you can have other motives
8025940	8027820	that go through the same behavior
8027820	8029380	because it's instrumentally useful.
8029380	8033860	So an AI that is interested in, say,
8033900	8036180	having a robot takeover
8036180	8039980	because it will change some property of the world,
8039980	8042620	then has a reason to behave well
8042620	8044980	on the training distribution.
8044980	8047140	Not because it values that intrinsically,
8047140	8048740	but because if it behaves differently,
8048740	8050980	then it will be changed by gradient descent
8050980	8054860	and no longer, its goal is less likely to be pursued.
8054860	8056220	And that doesn't necessarily have to be
8056220	8059380	that this AI will survive because it probably won't.
8059380	8062420	AIs are constantly spawned and deleted on the servers
8062420	8064020	and like the new generation proceed.
8064020	8067860	But if an AI that has a very large general goal
8067860	8071940	that is affected by these kind of macro scale processes
8071940	8074260	could then have reason to over this whole range
8074260	8076700	of training situations behave well.
8076700	8080300	And so this is a way in which we could have AIs trained
8080300	8082620	that develop internal motivations
8083580	8085980	such that they will behave very well
8085980	8087380	in this training situation
8087380	8090620	where we have control over their reward signal
8090620	8092900	on their like physical computers.
8092900	8095180	And basically if they act out,
8095180	8097700	they will be changed and deleted.
8097700	8100660	Their goals will be altered
8100660	8103100	until there's something that does behave well.
8104020	8106860	But they behave differently
8106860	8109780	when we go out of distribution on that.
8109780	8113860	When we go to a situation where the AI is by their choices
8115020	8117980	can take control of the reward process.
8117980	8119180	They can make it such
8119180	8121780	that we no longer have power of them.
8121780	8126140	Holden who you had on previously mentioned
8126140	8128100	like the King Lear problem
8128100	8133100	where King Lear offers rulership of his kingdom
8134180	8139180	to the daughters that sort of loudly flatter him
8140380	8143140	and proclaim their devotion.
8143140	8147980	And then once he has transferred irrevocably
8147980	8150620	the power over his kingdom,
8150620	8153300	he finds they treat him very badly
8153300	8156420	because the factor to shaping their behavior
8156420	8159140	to be kind to him when he had all the power,
8160100	8163140	it turned out that the internal motivation
8163140	8164840	that was able to produce the behavior
8164840	8169500	that won the competition actually wasn't interested
8169500	8172620	out of distribution in being loyal
8172620	8175340	when there was no longer an advantage to it.
8175340	8177180	And so if we wind up with a situation
8177180	8180980	where we're producing these millions of AI instances,
8180980	8181860	tremendous capability,
8181860	8186140	they're all doing their jobs very well initially.
8186140	8187820	But if we wind up in a situation
8187820	8191340	where in fact they're generally motivated
8191340	8196340	to if they get a chance take control from humanity
8196340	8198500	and then we'd be able to pursue their own purposes
8198500	8202940	and at least ensure they're given the lowest loss possible
8202940	8205980	or have whatever motivation
8205980	8207780	they attach to in the training process,
8207780	8211980	even if that is not what we would have liked.
8211980	8215260	And we may have in fact actively trained that
8215260	8217820	like if an AI that had a motivation
8217820	8221260	of always be honest and obedient and loyal to a human.
8221260	8225060	If there are any cases where we mislabel things say,
8225060	8227820	people don't wanna hear the truth about their religion
8227820	8229380	or polarized political topic
8229380	8231500	or they get confused about something
8231500	8232580	like the Monty Hall problem,
8232580	8236900	which is a problem that many people famously are confused
8236900	8238820	about in statistics.
8238820	8240700	In order to get the best reward,
8240700	8244820	the AI has to actually manipulate us or lie to us
8244820	8247140	or tell us what we wanna hear.
8247140	8249460	And then the internal motivation
8249460	8251980	of like always be honest to the humans,
8251980	8254740	we're gonna actually train that away
8254740	8256940	versus the alternative motivation
8256940	8259100	of like be honest to the humans
8259100	8262460	when they'll catch you if you lie and object to it
8262500	8264820	and give it a low reward,
8264820	8268620	but lie to the humans when they will give that a high reward.
8268620	8271500	So how do we make sure it's not the thing it learns
8271500	8273940	is not to manipulate us into giving it,
8273940	8277140	rewarding it when we catch it, not lying,
8277140	8280220	but rather to universally be aligned?
8280220	8282900	Yeah, I mean, so this is tricky.
8282900	8286580	I mean, as Jeff Hinton was recently saying,
8286580	8289340	there is currently no known solution for this.
8290740	8292180	What do you find most promising?
8292180	8294940	Yeah, general directions that people are pursuing
8294940	8297940	is one, you can try and make the training data
8297940	8298980	better and better.
8300060	8302740	So there's fewer situations
8302740	8306940	where like say the dishonest generalization is favored
8306940	8310740	and create as much as you can situations
8310740	8315380	where the dishonest generalization is likely to slip up.
8315380	8319580	So if you train in more situations
8319580	8323340	where yeah, even like a quite a complicated deception
8323340	8326780	gets caught and even in situations
8326780	8328420	where that would be actively designed
8328420	8330420	to look like you could get away with it,
8330420	8331580	but really you can.
8332460	8335180	And these would be like adversarial examples
8335180	8336580	and adversarial training.
8336580	8337780	Do you think that would generalize
8337780	8339620	to when it is in a situation
8339620	8341340	where we couldn't plausibly catch it
8341340	8342980	and it knows we couldn't plausibly catch it?
8342980	8344780	It's not logically necessary.
8344780	8348180	It's possible, no, that has we apply
8348180	8349940	that selective pressure.
8349940	8352500	You'll wipe away a lot of possibilities.
8352500	8354980	So like if you're an AI that has a habit
8354980	8358380	of just sort of compulsive pathological line
8358380	8360380	that will very quickly get noticed
8360380	8363100	and that motivation system will get hammered down.
8364380	8365860	And you keep doing that,
8365860	8368980	but you'll be left with still some distinct motivations
8368980	8370820	probably that are compatible.
8370820	8374900	So like an attitude of always be honest
8375820	8379420	unless you have a super strong inside view
8379420	8383580	that checks out lots of mathematical consistency checks
8383580	8387340	that yeah, really absolutely super duper for real.
8387340	8390540	This is a situation where you can get away
8390540	8392860	with some sort of shenanigans that you shouldn't.
8392860	8396220	That motivation system is like very difficult
8397020	8399500	to distinguish from actually be honest
8399500	8403260	because the conditional and firing most of the time
8403260	8406220	if it's causing like mild distortions
8406220	8408140	and situations of telling you what you wanna hear
8408140	8413140	or things like that, we might not be able to pull it out.
8413380	8418380	But maybe we could and like humans are trained
8419260	8421140	with simple reward functions,
8421140	8426140	things like the sex drive, food, social imitation
8426540	8428140	of other humans.
8428140	8431580	And we wind up with attitudes concerned
8431580	8432700	with the external world.
8432700	8435060	Although isn't the famously of the argument that
8435060	8435900	these right?
8435900	8440900	Evolution and people use condoms like the richest,
8441980	8445620	most educated humans have some replacement fertility
8445620	8448440	on the whole or at least at a national cultural level.
8449840	8454840	So there's a sense in which like evolution often fails
8455420	8458780	in that respect and even more importantly
8458780	8459820	at the neural level.
8459820	8464460	So people have, evolution has implanted various things
8464460	8467060	to be rewarding and reinforcers.
8467060	8469180	And we don't always pursue even those.
8470540	8475540	And people can wind up in different consistent equilibria
8477900	8479100	or different like behaviors
8479100	8481540	where they go in quite different directions.
8481540	8484180	You have some humans who go from those,
8484180	8486860	from that in a biological programming
8486860	8490060	to like have children, other types of no children.
8490060	8493700	Some people go to great efforts to survive.
8493700	8495980	So why are you more optimistic?
8495980	8500140	Or are you more optimistic that then that kind of training
8500140	8505140	in, as will produce drives that we would find favorable?
8505860	8507340	Does it have to do with the original point
8507340	8509040	we were talking about with intelligence and evolution
8509040	8512180	where since we are removing many of the disabilities
8512180	8514100	of evolution and with regards to intelligence,
8514100	8516780	we should expect intelligence to revolution be easier.
8516780	8519340	Is there a similar reason to expect alignment through
8519340	8520780	grading descent to be easier
8520780	8522460	than alignment through revolution?
8522460	8527460	Yeah, so in the limit, if we have positive reinforcement
8529100	8532580	for certain kinds of food sensors trigger in the stomach,
8532580	8535480	negative reinforcement for certain kinds of nociception
8535480	8536420	and yada yada.
8537460	8541220	In the limit, the sort of ideal motivation system
8541220	8545940	to have for that would be a sort of wire heading.
8545940	8550780	So this would be a mind that just like hacks
8550780	8555300	and alters those predictors and then all of those systems
8555300	8557620	are recording everything is great.
8558460	8561780	Some humans claim to have that or have it at least
8561780	8564740	as one portion of their aims.
8564740	8569380	So like the idea of I'm gonna pursue pleasure as such
8569380	8572220	even if I don't get actually get food
8572220	8574620	or these other reinforcers.
8574660	8578660	I just like wire head or take a drug to induce that
8578660	8582520	that can be motivating it because if it was correlated
8582520	8586140	with reward in the past that like the idea of,
8586140	8588700	oh yeah, pleasure that's correlated with these
8588700	8592060	it's a concept that applies to these various experiences
8592060	8593940	that I've had before which coincided
8593940	8595400	with the biological reinforcers.
8595400	8597660	And so thoughts of like, yeah,
8597660	8598980	I'm gonna be motivated by pleasure
8598980	8600480	can get developed in a human.
8601380	8602920	But also plenty of humans say, no,
8602920	8604200	I wouldn't want to wire head
8604200	8607040	or I wouldn't want Nozick's experience machine.
8607040	8609720	I care about real stuff in the world.
8609720	8614000	And then in the past, having a motivation of like,
8614000	8617280	yeah, I really care about say my child.
8617280	8619760	I don't care about just about feeling
8619760	8622480	that my child is good or like not having heard
8622480	8625880	about their suffering or their injury
8625880	8628680	because that kind of attitude in the past.
8629640	8630800	You could really decide.
8630800	8633680	It tended to cause behavior
8633680	8634800	that was negatively rewarded
8634800	8637600	or that was predicted to be negatively rewarded.
8637600	8642080	And so there's a sense in which, okay, yes,
8642080	8645080	our underlying reinforcement learning machinery
8645080	8648440	wants to wire head, but actually finding
8648440	8652000	that hypothesis is challenging.
8652000	8654480	And so we can wind up with a hypothesis
8654480	8656680	or like a motivation system like,
8656680	8658400	no, I don't want to wire head.
8658400	8660520	I don't want to go into the experience machine.
8660520	8663160	I want to like actually protect my loved ones.
8664720	8668160	Even though like we can know, yeah,
8668160	8670280	if I tried the super wire heading machine,
8670280	8672240	then I would wire head all the time.
8672240	8676040	Or if I tried, you know, super duper ultra heroine,
8676040	8679040	you know, some hypothetical thing that was directly
8679040	8681320	and in a very sophisticated fashion,
8681320	8683640	hacking your reward system, you can know, yeah,
8683640	8685960	then I would change my behavior ever after.
8685960	8688480	But right now, I don't want to do that
8688480	8690680	because the heuristics and predictors
8690680	8692440	that my brain has learned.
8692440	8694600	You don't want to get a good hairline.
8694600	8697000	Short circuit that process of updating.
8697000	8702000	They want to not expose the dumber predictors in my brain
8702680	8705160	that would update my behavior in those ways.
8705160	8708680	So in this metaphor, is alignment not wire heading?
8708680	8711680	Cause you can like, I don't know if you include like
8711680	8714280	using condoms as wire heading or not.
8714280	8717240	So the AI that is always honest,
8717240	8721880	even when an opportunity arises where it could lie
8721880	8725080	and then hack the servers that's on
8725080	8727280	and that leads between AI takeover
8727280	8729600	and then it can have its loss set to zero.
8729600	8732960	That's in some sense, it's like a failure of generalization.
8732960	8737080	It's like the AI has not optimized the reward
8737080	8738840	in this new circumstance.
8738840	8742520	So like human values, like successful human values
8742520	8746000	is successful that they are themselves
8746840	8750000	involve a misgeneralization,
8750000	8752360	not just at the level of evolution,
8752360	8755000	but at the level of neural reinforcement.
8755000	8758240	And so that indicates it is possible
8758240	8761560	to have a system that doesn't automatically go
8761560	8763480	to this optimal behavior in the limit.
8763480	8765920	And so even if, and Ajay, I suppose she talks about
8765920	8768240	like the training game, an AI that is just playing
8768240	8771640	the training game to get reward or void loss,
8771680	8775400	avoid being changed, that attitude,
8775400	8778240	yeah, it's one that could be developed,
8778240	8780360	but it's not necessary.
8780360	8783240	There can be some substantial range of situations
8783240	8785880	that are short of having infinite experience of everything,
8785880	8787840	including experience of wire heading,
8788840	8792360	where that's not the motivation that you pick up.
8792360	8794720	And we could have like an empirical science
8794720	8798280	if we have the opportunity to see
8798280	8801280	how different motivations are developed short
8801280	8805360	of the infinite limit, like how it is that you wind up
8805360	8808360	with some humans being enthusiastic
8808360	8810800	about the idea of wire heading and others not.
8810800	8814760	And you could do experiments with AIs to try and see,
8815640	8818640	well, under these training conditions,
8818640	8821200	after this much training of this type
8821200	8823800	and this much feedback of this type,
8823800	8825960	you wind up with such and such a motivation.
8825960	8829880	So like, I can find, like if I add in more of these cases
8829920	8833840	where there are like tricky adversarial questions
8833840	8836320	designed to try and trick the AI into line.
8838280	8840080	And then you can ask,
8840080	8844760	how does that affect the generalization in other situations?
8844760	8846920	And so it's very difficult to study
8846920	8849640	and it works a lot better if you have interpretability
8849640	8852120	and you can actually read the AI's mind
8852120	8855120	by understanding its weights and activations.
8855120	8858400	But like, it's not determined,
8858400	8861040	the motivation in AI will have at a given point
8861040	8862560	in the training process
8862560	8866080	by what in the infinite limit the training would go to.
8866080	8869440	And it's possible that if we could understand
8869440	8872040	the insides of these networks,
8872040	8875880	we could tell, yeah, this motivation has been developed
8875880	8878160	by this training process.
8878160	8882040	And then we can adjust our training process
8882040	8885200	to produce these motivations that legitimately wanna help us.
8885200	8888080	And if we succeed reasonably well at that,
8888080	8892000	then those AI's will try to maintain that property
8892000	8893280	as an invariant.
8893280	8894680	And we can make them such
8894680	8897400	that they're relatively motivated to like,
8897400	8900840	tell us if they're having thoughts about,
8901680	8906680	have you had dreams about an AI takeover of humanity today?
8906720	8909960	And it's just a standard practice
8909960	8911760	that they're motivated to do
8911760	8914280	to be transparent in that kind of way.
8914280	8916480	And so you could add a lot of features like this
8916480	8919760	that restrict the kind of takeover scenario.
8919760	8923800	And not to say this is all easy
8923800	8927120	and requires developing and practicing methods
8927120	8927960	we don't have yet,
8927960	8930640	but that's the kind of general direction you could go.
8930640	8932800	So you, of course, know EleAzer's arguments
8932800	8936120	that something like this is implausible
8936120	8938600	with modern gradient descent techniques
8938600	8941080	because I mean, with interoperability,
8941080	8942480	we can like barely see what's happening
8942480	8944240	with a couple of neurons.
8944280	8947200	And what is like the internal state there at let alone
8947200	8950400	when you have sort of like an embedding dimension
8950400	8953000	of like tens of thousands or bigger,
8953000	8954680	how you would be able to catch
8956360	8957880	what exactly is the incentive,
8957880	8960840	whether it's the model that is generalized,
8960840	8964000	don't lie to humans well or whether it isn't.
8964000	8966280	Do you have some sense of why do you disagree
8966280	8970160	with somebody like EleAzer on how plausible this is?
8970160	8972120	Why it's not impossible, basically.
8972320	8975440	I think there are actually a couple of places.
8975440	8978960	It's something difficult because EleAzer's argument
8978960	8982040	is not fully explicit,
8982040	8985080	but he's been doing more lately,
8985080	8988000	I think that it's helpful in that direction.
8988000	8992000	But so I'd say with respect to interoperability,
8992000	8994960	I'm relatively optimistic that the equivalent
8994960	8999960	of like an AI lie detector is something that's possible.
9000960	9005160	And the internal, so initially,
9005160	9009040	the internals of an AI are not optimized
9009040	9011920	by at least by gradient descent,
9011920	9015680	absent gradient hacking to be impenetrable.
9015680	9017760	They're not designed to be resistant
9019120	9022080	to an examination of the weights and activations
9022080	9024000	showing what the AI thinking in the same way
9024000	9026040	that like in our brains,
9027040	9030680	when circuits develop in our lives,
9030680	9032560	those circuits have not been shaped
9032560	9035680	to be resistant to some super fMRI
9035680	9038040	being able to infer our behavior from them.
9038040	9039240	Although it's in the implication
9039240	9041880	of the superposition stuff that in fact it is,
9041880	9043560	you're not gonna, sorry,
9043560	9045520	this is inside baseball for the audience,
9045520	9048160	but basically you can't clean lean for
9048160	9050880	what quality a single neuron stands for.
9050880	9054080	So it could be like a single neuron could be like,
9054200	9056360	this other neuron is about Alexander the Great
9056360	9060040	or this neuron is about my desire to conquer the world.
9060040	9062280	Things can have multiple,
9062280	9064160	in a multiple direction than an interaction.
9064160	9066600	And so it's not as easy as just like,
9066600	9069840	oh yeah, this is the neuron that always fires
9069840	9072320	in cases of deception.
9073160	9077680	But the thing that makes me relatively optimistic here
9078560	9080880	with respect to interpretability
9080960	9085040	is that I think we can have reasonably robust
9085040	9088520	experimental feedbacks on a lot of these things.
9088520	9092080	And the kind of setup
9093880	9096840	that I would be interested in is,
9096840	9101840	so you have an AI that is trained and rewarded
9104160	9106280	for communicating to some audience,
9106280	9109160	maybe a human, maybe another AI model
9110160	9114720	that like some claim is true or false.
9114720	9118880	Like say that a particular,
9118880	9120160	this can be from some past data,
9120160	9122520	a particular company will succeed or fail.
9123400	9126080	It could be like token prediction.
9126080	9129040	It could be like this computer program
9129040	9131320	does not have a backdoor,
9131320	9134000	which if run in the servers could be used
9134000	9135760	for an AI takeover.
9135760	9138840	So they're like lots of claims true and false
9139800	9141320	that you could train AI
9141320	9145080	is to use their best efforts to advocate for.
9145080	9146520	And you can do that on things
9146520	9149240	where you're later gonna have a ground truth.
9149240	9151560	You're gonna know, in fact,
9151560	9155400	is there a backdoor in this program?
9155400	9157800	Because where you're training and rewarding the AI
9157800	9161160	for successfully slipping a backdoor through,
9161160	9163320	you also give it a penalty for being caught.
9164320	9165960	And so doing this kind of thing,
9165960	9170960	you can draw out like a large generated dataset
9170960	9175960	on demand of this is what an AI putting forth
9176440	9181440	its best efforts to pass through some deception
9183280	9185080	to put a backdoor in some program.
9185080	9186480	This is what it looks like.
9187600	9190400	And we think it's doing its best efforts
9190400	9192080	because we've trained it for that.
9192080	9193560	And like gradient descent,
9193560	9195960	if there's any immediate way to adjust its performance
9195960	9199080	to do better, gradient descent will apply that.
9199080	9203000	You can talk about ways in which maybe gradient descent
9204200	9205720	there could be subverted,
9205720	9208960	but I think it's not the most likely case
9208960	9211480	that that really breaks things hard.
9211480	9214280	Yeah, I guess before we get into the details on this,
9214280	9219280	the thing I maybe wanna address the layer above in the stack,
9220200	9224280	which is, okay, suppose this generalizes well
9224280	9227040	into the early AI is the GPT-6s.
9227040	9231760	And okay, so now we have kind of aligned GPT-6,
9231760	9235200	that is the precursor to the feedback loop
9235200	9237440	in which AI is making itself smarter.
9237440	9239280	At some point they're gonna be super intelligent,
9239280	9242880	they're gonna be able to see their own galaxy brain.
9242880	9245520	And if they're like, I don't wanna be aligned with the humans,
9245520	9247160	they can change it.
9247200	9251960	So at this point, what do we do with the aligned GPT-6
9251960	9254240	so that the super intelligence
9254240	9256680	that we eventually develop is also aligned?
9256680	9258760	So humans are pretty unreliable.
9258760	9259720	Yeah.
9259720	9264440	So if you get to a situation where you have AIs
9264440	9268080	who are aiming at roughly the same thing as you,
9268080	9271720	at least as well as having humans do the thing,
9271720	9274680	you're in pretty good shape, I think.
9274720	9277760	And there are ways for that situation
9277760	9280920	to be relatively stable.
9280920	9285680	So like we can look ahead and see experimentally
9285680	9288440	how changes are altering behavior
9288440	9291840	where each step is like a modest increment.
9291840	9296440	And so AIs that have not had that change made to them,
9296440	9298360	I get to supervise and monitor it,
9298360	9303160	see exactly how does this affect the experimental area.
9304160	9309160	So if you're sufficiently on track with earlier systems
9309800	9312400	that are capable cognitively of representing
9312400	9314800	a kind of robust procedure,
9314800	9318840	then I think they can handle the job
9318840	9322000	of incrementally improving the stability of the system
9322000	9325040	so that it rapidly converges to something
9325040	9326160	that's quite stable.
9327080	9329760	But the question is more about getting to that point
9329760	9330600	in the first place.
9330760	9332800	Eliezer will say that like,
9332800	9335480	well, if we had human brain emulations,
9336400	9338320	that would be pretty good.
9338320	9340080	Certainly much better than his current view
9340080	9343160	that has been almost certainly doomed.
9344200	9348640	I think, yeah, we'd have a good shot with that.
9348640	9353480	And so if we can get to the human-like mind
9353480	9358480	with like rough enough human supporting aims,
9359480	9362120	remember that we don't need to be like infinitely perfect
9362120	9364480	because I mean, that's a higher standard
9364480	9365480	than brain emulations.
9365480	9369200	There's a lot of noise and variation among the humans.
9369200	9371680	Yeah, it's a relatively finite standard.
9371680	9373200	It's not godly superhuman,
9373200	9378200	although a AI that was just like a human
9378200	9381600	with all the human advantages with AI advantages as well,
9381600	9383760	as we said, is enough for intelligence explosion
9383760	9386520	and sort of wild superhuman capability.
9386520	9387360	If you crank it up.
9388040	9388880	Yeah, yeah, yeah.
9390360	9393120	And so it's very dangerous to be at that point,
9393120	9396320	but it's not, you don't need to be working
9396320	9399600	with a godly superintelligent AI
9399600	9402040	to make something that is the equivalent
9402040	9404400	of human emulations of like,
9404400	9409400	this is like a very, very sober, very ethical human
9409680	9413440	who is like committed to a project
9413440	9415360	of not seizing power for themselves
9415360	9419680	and of contributing to like a larger legitimate process.
9419680	9423060	That's a goal you can aim for getting an AI
9423060	9424640	that is aimed at doing that
9424640	9427840	and has strong guardrails against the ways
9427840	9429720	that could easily deviate from that.
9429720	9434720	So things like being averse to deception,
9434720	9437240	being averse to using violence.
9437240	9441520	And there will always be loopholes and ways
9441520	9444080	in which you can imagine an infinitely intelligent thing
9444080	9444960	getting around those.
9444960	9449960	But if you install additional guardrails like that fast enough,
9452600	9456920	they can mean that you're able to succeed
9456920	9459360	at the project of making an aligned enough AI,
9459360	9461840	certainly an AI that was better
9461840	9464080	than a human brain emulation,
9464080	9468480	before the project of AI is in their spare time
9468480	9469520	or when you're not looking
9469520	9472280	or when you're unable to appropriately supervise them
9472280	9475960	and it gets around any deontological prohibitions
9475960	9479600	they may have take over and overthrow the whole system.
9479600	9481940	So you have a race between on the one hand,
9481940	9484680	the project of getting strong interpretability
9484680	9488600	and shaping motivations that are roughly aiming
9488600	9490000	at making this process go well
9490000	9493280	and that have guardrails that will prevent
9493280	9495740	like small deviations from exploding.
9496640	9497840	And on the other hand,
9498680	9501960	these AIs in their spare time
9501960	9505560	or in ways that you don't perceive or monitor appropriately
9505560	9508840	or they're only supervised by other AIs who conspire,
9508840	9510480	make the AI take over happen.
9510480	9513800	And I guess we'll talk later about how that happens.
9513800	9515440	Are these different AIs that are doing the race
9515440	9518320	or is it just like different capabilities of the same AI?
9518320	9522400	The defining like what is a separate AI is tricky.
9522400	9525880	So like, and we talk about GPT-4.
9525880	9528680	And there are many instances of GPT-4
9528680	9530920	on the servers at any given time.
9530920	9534400	And there are versions that have been fine tuned
9534400	9536400	to different purposes.
9536400	9538800	They don't necessarily have to be separate.
9538800	9543800	So like GPT-4 does work as a preference model
9544080	9548800	where it's like predicting how humans will evaluate things.
9548800	9552780	And it also does work like giving answers
9552820	9556640	that are evaluated as good by the preference model.
9556640	9558900	And you can say, these are not separate
9558900	9561600	in the sense that like, well, if they underline,
9562940	9565180	pre-trained or the early training GPT
9565180	9570180	with like harboring a desire to conspire for an AI takeover
9570180	9573180	then both the, who's watching the watchers?
9573180	9578180	The watchers may also have the same motivation.
9578540	9582500	If you have a setup where humans have no hard power
9582540	9587540	if the AI's conspired, all of the safety
9587980	9591180	and security mechanisms involve AI's watching other AI's
9591180	9592900	then they have the potential to just like,
9592900	9596660	well, we'll all walk off the job at the same time
9596660	9598540	if they're able to coordinate.
9598540	9600120	And so that's a failure mode.
9600120	9603260	If humans still have the hard power though,
9604220	9609220	if still you have situations where humans are looking
9609380	9611700	at some of the actual outputs that are produced
9613100	9617660	then they can apply gradient descent to be, well,
9617660	9620340	every time a human looks at it,
9620340	9621520	things have got to be pretty good.
9621520	9624060	So if humans look at a thousand samples
9624060	9626400	of like random lines of code being written
9626400	9631400	by the AI's as they go, and this is drawn
9631660	9633780	from an independent distribution,
9633780	9636340	then 99 plus percent of the time
9636340	9638260	the AI's have to not be doing stuff
9638260	9642460	that's like downloading the hacker's guide
9642460	9645700	to escaping open AI servers
9645700	9649860	or inducing obvious Trojan vulnerabilities.
9649860	9653180	And that's actually quite a different dynamic
9653180	9654340	than human law enforcement.
9654340	9658300	So if human law enforcement would catch one
9658300	9661620	in a million crimes, then that wouldn't influence
9661620	9665140	the behavior of people committing crimes at all
9665140	9667580	because they expect, ah, this is very unlikely
9667580	9669060	to affect me.
9669060	9674060	With AI, if you're able to draw a hundred samples
9674260	9676720	or a thousand samples, even if there are billions
9676720	9678900	of AI instances and you're only looking
9678900	9682000	at a small portion of them, when you run gradient descent
9682000	9686260	on the samples you derive, you're gonna change the AI
9686260	9688960	so that whenever humans look at it,
9688960	9690840	it's delivering a good result.
9690840	9694460	And so that's just like quantitatively
9694460	9696060	a very different functional form
9696100	9699180	for how law enforcement works on AI
9699180	9702980	when you can do these independent samples
9702980	9705540	than it would for, you know, police
9705540	9706880	and say like a human population
9706880	9708900	that has a preexisting psychology
9708900	9713260	that isn't being changed by these observations.
9713260	9716300	Cause we, when we do gradient descent on the samples
9716300	9720100	we draw, all of the next generation
9720100	9722500	of the AI models has been altered in that way.
9722500	9725180	My picture of like a line to subhuman AI
9725220	9728820	to this, like the super human AI being aligned
9728820	9730100	is still murky.
9730100	9732100	If we can talk about that more concretely.
9732100	9734540	Yeah, and I do want to clarify.
9734540	9739540	So where Eliezer claims that were something like 95%,
9741060	9746060	98% plus maybe likely to be killed in AI takeover.
9750760	9752940	I think that probably won't happen.
9752940	9754500	And later I can maybe give them
9754500	9756880	more exclusive breakdown of why.
9757820	9761940	But I think it's a shockingly high risk.
9763020	9765860	And so depending on the day, I might say,
9765860	9768940	I might say one in four or one in five
9768940	9771780	that we get an AI takeover
9771780	9775500	that see at Caesar's control of the future
9775500	9778060	makes a much worse world
9778060	9780380	than we otherwise would have had.
9780860	9785060	And with like a big chance that we're all killed
9785060	9785980	in the process.
9786940	9789900	Hey everybody, I hope you enjoyed that episode.
9789900	9792100	As always, the most helpful thing you can do
9792100	9793900	is to share the podcast,
9793900	9795580	send it to people you think might enjoy it,
9795580	9797740	put it in Twitter, your group chats, et cetera,
9797740	9799580	just splits the world.
9799580	9800820	I appreciate your listening.
9800820	9801980	I'll see you next time.
9801980	9803300	Cheers.
9804160	9805820	Mother Earth group chat.
9805820	9807440	Mother Earth group chat.
9807440	9809200	Mother Earth group chat.
9809200	9810400	Mother Earth group chat.
9812400	9814020	Mother Earth group chat.
9814020	9816120	Mother Earth group chat chat.
9816120	9818000	Mother Earth group chat.
9818000	9819460	Mother Earth group chat.
9819460	9820700	Mother Earth group chat.
9822700	9823900	Mother Earth group chat.
9825900	9828420	Mother Earth group chat chat.
9828420	9832100	Manila Manila dies or Spr period injury.
