start	end	text
0	3920	What will be at stake will not just be equal products, but whether the liberal democracy
3920	8320	survives, whether the CCP survives, what the world order for the next century will be.
8320	12640	The CCP is going to have an all-out effort to infiltrate American AI labs, billions of dollars,
12640	16560	thousands of people. The CCP is going to try to outbuild us. People don't realize how intense
16560	20000	state-level espionage can be. When we have literal superintelligence on our cluster,
20000	23840	and they can stuxnet the Chinese data centers, you really think they'll be a private company,
23840	27680	and the government would be like, oh my god, what is going on? I do think it is incredibly
27680	31200	important that these clusters are in the United States. I mean, would you do the Manhattan Project
31200	35600	in the UAE, right? 2023 was the sort of moment for me where it went from kind of AGI as a sort of
35600	39840	theoretical abstract thing, and you'd make the models to like, I see it, I feel it. I can see the
39840	43120	cluster where it's strained on, like the rough combination of algorithms, the people, like how
43120	46720	it's happening. And I think, you know, most of the world is not, you know, most of the people feel it
46720	52720	are like right here, you know, right? Okay, today I'm chatting with my friend, Leopold Aschenbrenner.
52720	57280	He grew up in Germany, graduated valedictorian of Columbia when he was 19,
58000	63680	and then he had a very interesting Gaffier, which we'll talk about, and then he was on the
63680	70880	OpenAI Superalignment team, made a recent piece, and now he, with some anchor investments from
70880	76640	Patrick and John Collison and Daniel Gross and Nat Friedman, is launching an investment firm.
76640	81520	So Leopold, I know you're off to a slow start, but life is long, and I wouldn't worry about it
81520	86720	too much. You'll make up for it in due time. But thanks for coming on the podcast.
86720	91200	Thank you. You know, I first discovered your podcast when your best episode had, you know,
91200	94960	like a couple of hundred views. And so it's just been, it's been amazing to follow your
94960	99920	trajectory. And it's a delight to be on. Yeah, yeah. Well, I think in the shelter in Trenton
99920	105200	episode, I mentioned that a lot of the things I've learned about AI, I've learned from talking with
105200	109440	them. And the third part of this triumvirate, probably the most significant in terms of the
109440	113520	things that I've learned about AI has been you will go out of the stuff on the record now.
113520	117920	Great. Okay, first thing I had to get on record, tell me about the trillion dollar cluster.
118560	122880	But by the way, I should mention, so the context of this podcast is today, there's,
122880	126960	you're releasing a series called Situational Awareness. We're going to get into it. First
126960	131520	question about that is tell me about the trillion dollar cluster. Yeah. So, you know,
131520	135520	unlike basically most things that have come out of Silicon Valley recently, you know, AI is kind of
135520	140400	this industrial process. You know, the next model doesn't just require, you know, some code,
140400	144640	it's, it's, it's building a giant new cluster. You know, now it's building giant new power plants,
144640	150000	you know, pretty soon it's going to be building giant new fabs. And, you know, since that should
150000	153680	be tea, this kind of extraordinary sort of techno capital acceleration has been set into motion.
153680	158160	I mean, basically, you know, exactly a year ago today, you know, Nvidia had their first kind of
158160	161920	blockbuster earnings call, right, where it like went out 25% after hours and everyone was like,
161920	166960	oh my god, AI, it's a thing. You know, I mean, I think within a year, you know, you know, and
166960	170640	Nvidia, Nvidia data center revenue has gone from like, you know, a few billion a quarter to like,
170640	175040	you know, 20, 25 billion a quarter now and, you know, continue to go up, like, you know,
175040	181040	big tech capex, skyrocketing. And, you know, it's funny because it's both there's this sort of,
181040	184720	this kind of crazy scramble going on, but in some sense, it's just the sort of continuation of
184720	187920	straight lines on a graph, right? There's this kind of like long run trend, basically almost a
187920	191840	decade of sort of training compute of the sort of largest AI systems growing by about, you know,
191840	197360	half an order of magnitude, you know, 0.5 booms a year. And you just kind of play that forward,
197360	203120	right? So, you know, GPT-4, you know, rumored or reported to have finished pre-training in 2022,
203120	207360	you know, the sort of cluster size there was rumored to be about, you know, 25,000 H100s,
207360	212640	you know, sorry, A100s on semi-analysis, you know, that's roughly, you know, if you do the
212640	216880	math on that, it's maybe like a $500 million cluster, you know, it's very roughly 10 megawatts.
217600	224480	And, you know, just play that forward half a new year, right? So, then 2024, that's a cluster,
224480	228800	that's, you know, 100 megawatts, that's like 100,000 H100 equivalents, you know, that's,
230720	234960	you know, costs in the billions, you know, play it forward, you know, two more years, 2026,
234960	239440	that's a cluster, that's a gigawatt, you know, that's sort of a large nuclear reactor size,
239440	242560	it's like the power of the Hoover Dam, you know, that costs tens of billions of dollars,
242560	246320	that's like a million H100 equivalents, you know, 2028, that's a cluster, that's 10 gigawatts,
246320	251600	right? That's more power than kind of like most U.S. states. That's, you know, like 10 million H100
251600	257680	equivalents, you know, costs hundreds of billions of dollars. And then 2030, trillion-dollar cluster,
258400	263680	100 gigawatts, over 20% of U.S. electricity production, you know, 100 million H100 equivalents.
264320	267840	And that's just the training cluster, right? That's like the one largest training cluster,
267840	270400	you know, and then there's more inference GPUs as well, right? Most of, you know, once there's
270400	276000	products, most of them are going to be inference GPUs. And so, you know, U.S. power
276000	280960	production has barely grown for like, you know, decades, and now we're really in for a ride.
280960	287040	So, I mean, when I had Zuck on the podcast, he was claiming, not a plateau, per se, but
287920	292400	that AI progress would be bottlenecked by specifically this constraint on energy,
292400	296880	and specifically like, oh, gigawatt data centers are going to build another three gorgeous dam or
296880	302320	something. I know that there's companies, according to public reports, who are planning
302320	307120	things on the scale of a gigawatt data center. 10 gigawatt data center, who's going to be able
307120	311200	to build that? I mean, the 100 gigawatt center, like a state, where are you getting,
311200	314640	are you going to pump that into one physical data center? How is this going to be possible?
315280	318400	Yeah, you know. What is Zuck missing? I mean, you know, I don't know. I think to 10 gigawatt,
318400	321520	you know, like six months ago, you know, 10 gigawatt was the taco town. I mean, I think,
322080	325040	I feel like now, you know, people have moved on, you know, 10 gigawatt is happening. I mean,
325040	329760	I know there's the information report on OpenAI and Microsoft planning a $100 billion
330400	334560	cluster. Is that the gigawatt or is that the 10 gigawatt? I mean, I don't know. But, you know,
334560	338320	if you try to like map out how expensive would the 10 gigawatt cluster be, you know, that's
338320	342880	maybe a couple hundred billion. So it's sort of on that scale. And they're planning it. They're
342880	349760	working on it. You know, so it's not just sort of my crazy take. I mean, AMD, AMD, I think,
349760	354640	forecasted a $400 billion AI accelerator market by 27. You know, I think it's, you know, and AI
354640	359120	accelerators are only part of the expenditures. It's sort of, you know, I think sort of a trillion
359120	363280	dollars of sort of like total AI investment by 2027 is sort of like, we're very much in track
363280	366800	on it. I think the trillion dollar cluster is going to take a bit more sort of acceleration.
366800	371360	But, you know, we saw how much sort of chat GPT unleashed, right? And so like every generation,
371360	373760	you know, the models are going to be kind of crazy and people, it's going to shift the
373760	377600	overtune window. And then, and then, you know, obviously the revenue comes in, right? So these
377600	381120	are forward looking investments. The question is, do they pay off? Right? And so if we sort of
381120	386560	estimated the, you know, the GPT four cluster at around 500 million, by the way, that's, that's
386560	389360	sort of a common mistake people make is they say, you know, people say like a hundred million
389360	393120	dollars before, but that's just the rental price, right? They're like, ah, you rent the cluster
393120	395840	for three months. But it's, you know, if you're building the biggest cluster, you got to like,
395840	398400	you got to build the whole cluster, you got to pay for the whole cluster, you can't just rent it
398400	401520	for three. But I mean, the really, you know, once, once you're trying to get into the sort of
401520	404320	hundreds of billions, eventually you got to get to like a hundred billion a year. I mean,
404320	407120	I think this is where it gets really interesting for the big tech companies, right? Because like,
407120	411200	their revenues are on order, you know, hundreds of billions, right? So it's like 10 billion fine,
411200	416160	you know, and it'll pay off the, you know, 2024 size training cluster. But, you know, really
416160	419600	when sort of big tech, it'll be gangbusters is a hundred billion a year. And so the question is
419600	424880	sort of how feasible is a hundred billion a year from AI revenue? And, you know, it's a lot more
424880	428800	than right now. But I think, you know, if you sort of believe in the trajectory of the AI systems,
428800	433600	as I do, which we'll probably talk about, it's not that crazy, right? So there's, I think there's
433600	438640	like 300 million, you know, ish Microsoft office subscribers, right? And so they have co-pilot
438640	442160	now and I don't know what they're selling it for. But, you know, suppose you sold some sort of AI
442160	446560	add-on for a hundred bucks a month, and you sold that to, you know, a third of Microsoft office
446560	449680	subscribers subscribe to that. That'd be a hundred billion right there. You know, a hundred dollars
449680	452880	a month is, you know, a lot. That's a lot. Yeah. It's a lot. It's a lot. For a third of office
452880	455760	subscribers. Yeah. But it's, but it's, you know, for the average dollars worker, it's like a few
455760	459360	hours of productivity a month. And it's, you know, kind of like, you have to be expecting pretty lame
459360	464000	AI progress to not hit like, you know, some few hours of productivity a month of, of, of, of, yeah.
464000	469040	Okay, sure. So let's assume all this. Yeah. What happens in the next few years in terms of
470000	475440	what is the one gigawatt training, the AI that's trained on the one gigawatt data center? What
475440	478960	can it do with the one on the 10 gigawatt data center? Just map out the next few years of AI
478960	483440	progress for me. Yeah. I think probably the sort of 10 gigawatt range is sort of my best guess
483440	486800	for when you get the sort of true AGI. I mean, yeah, I think it's sort of like one gigawatt
486800	489760	data center. And again, I think actually compute is overrated and we're going to talk about that.
489760	493600	But what we will talk about compute right now. So, you know, I think so 25, 26, we're going to get
493600	499520	models that are, you know, basically smarter than most college graduates. I think sort of the
499520	502400	practice, a lot of the economic usefulness, I think really depends on sort of, you know,
502400	505680	sort of on hobbling. Basically, it's, you know, the models are kind of, you know,
505680	508640	they're smart, but they're limited, right? They're, you know, there's chat bot, you know,
508640	511520	and things like being able to use a computer, things like being able to do kind of like a
511520	517440	genetic long horizon tasks. Yeah. And then I think by 27, 28, you know, if you extrapolate the trends
517440	520800	and, you know, we'll talk about that more later. And I talked about in the series, I think we hit,
520800	524240	you know, basically, you know, like as smart as the smartest experts, I think the on hobbling
524240	529600	trajectory kind of points to, you know, looks much more like an agent than a chat bot and much
529600	533200	more almost like basically a drop in remote worker, right? So it's not like, I think basically, I
533200	536320	mean, I think this is the sort of question on the economic returns. I think a lot of the,
536320	539920	a lot of the intermediate AI systems could be really useful, but, you know, it actually just
539920	543920	takes a lot of slap to integrate them, right? Like GVD4, you know, whatever, 4.5, you know,
543920	546880	probably there's a lot you could do with them in a business use case. But, you know, you really
546880	550320	got to change your workflows to make them useful. And it's just like, there's a lot of, you know,
550320	554000	it's a very Tyler Cowan-esque take. It takes a long time to diffuse. You know, it's like, you
554000	560640	know, we're an SF and so we missed that or whatever. But I think in some sense, you know,
560640	564880	the way a lot of these systems won't be integrated is, is you kind of get this sort of sonic boom
564880	569040	where it's, you know, the sort of intermediate systems could have done it, but it would have taken
569040	573040	slap. And before you do the slap to integrate them, you get much more powerful systems, much
573040	576480	more powerful systems that are sort of on hobbled. And so they're this agent and there's drop in
576560	580400	remote worker. And, you know, and then you're kind of interacting with them like a coworker,
580400	584480	right? You know, you can take do zoom calls with them and you're slacking them. And you're like,
584480	588000	ah, can you do this project? And then they go off and they, you know, go away for a week and write
588000	592960	a first draft and get feedback on them and, you know, run tests on their code. And then they come
592960	597440	back and then you see it and you tell them a little bit more things or, you know, and that'll
597440	602480	be much easier to integrate. And so, you know, it might be that actually you need a bit of overkill
602480	605280	to make the sort of transition easy and to really harvest the games.
605280	607920	What do you mean by the overkill? Overkill on the model capabilities?
607920	611120	Yeah. Yeah. So basically intermediate models could do it, but it would take a lot of slap.
611120	614400	I see. And so then, you know, the like, actually it's just the drop in remote worker kind of AGI
614400	617680	that can automate, you know, cognitive tasks that actually just ends up kind of like,
617680	620640	you know, basically it's your like, you know, the intermediate models would have made the
620640	624320	software engineer more productive, but, you know, will the software engineer adopted? And then the,
624320	628400	you know, 27 model is, well, you know, you just don't need the software engineer. You can literally
628400	631280	interact with it like a software engineer and it'll do the work of a software engineer.
632000	637920	So the last episode I did was with John Shulman. Yeah. And I was asking about basically this and
637920	642400	one of the questions I asked is, we have these models that have been coming out in the last year
642400	647200	and none of them seem to have significantly surpassed GPT-4 and certainly not in the
647200	651680	agentic way in which they are interacting with as a co-worker, you know, the brag that they
651680	656880	got a few extra points on MMLU or something. And even GPT-4.0, it's cool that they can talk
656880	662160	like Scarlett Johansson or something, but like... And honestly, I'm going to use that.
663040	667920	Oh, I guess not anymore, not anymore. Okay, but the whole co-worker thing. So
668800	671840	this is going to be a wrong question, but you can address it in any order. But
672880	677120	the, it makes sense to me why they'd be good at answering questions. They have a bunch of
677680	682240	data about how to complete Wikipedia text or whatever. Where is the equivalent training
682240	688160	data that enables it to understand what's going on in the Zoom call? How does this connect with
688160	692560	what they were talking about in the Slack? What is the cohesive project that they're going after
693200	696880	based on all this context that I have? Where is that training data coming from?
697680	702480	Yeah. So I think a really key question for sort of AI progress in the next few years is sort of
702480	706800	how hard is it to do, sort of unlock the test time compute overhang? So, you know, right now,
706800	711680	GPT-4.0 answers a question and, you know, it kind of can do a few hundred tokens of kind of chain
711680	714880	of thought. And that's already a huge improvement, right? Sort of like, this is a big on hobbling
714880	720000	before, you know, answer a math question, it's just shotgun. And, you know, if you try to kind of
720000	722640	like answer a math question by saying the first thing that came to mind, you know, you wouldn't
722640	728080	be very good. So, you know, GP-4 thinks for a few hundred tokens. And, you know, if I thought for a
728080	730800	few hundred, you know, if I think at like a hundred tokens a minute, and I thought for a few minutes-
730800	732240	You're thinking much more than a hundred tokens.
732240	736960	I don't know. If I thought for like a hundred tokens a minute, you know, it's like what GP-4
736960	740240	does, maybe it's like, you know, it's equivalent to me thinking for three minutes or whatever, right?
742480	746960	You know, suppose GP-4 could think for millions of tokens, right? That's sort of plus four rooms,
746960	750160	plus four days of magnitude on test time compute, just like on one problem.
750960	754240	It can't do it right now. It kind of gets stuck, right? Like write some code, even if, you know,
754240	756800	you can do a little bit of iterative debugging, but eventually just kind of like,
756800	760480	it can't, it kind of gets stuck in something, it can't correct its errors and so on.
760480	765040	And, you know, in a sense, there's this big overhang, right? And like other areas of ML,
765040	769040	you know, there's this great paper on AlphaGo, right? Where you can trade off train time and
769040	772320	test time compute. And if you can use, you know, four rooms, more test time compute, that's almost
772320	776000	like, you know, a three and a half room bigger model. Just because, again, like you can, you know,
776000	780560	if a hundred tokens a minute, a few million tokens, that's a few months of sort of working time.
780560	783600	There's a lot more you can do in a few months of working time than, and then right now. So the
783600	789360	question is, how hard is it to unlock that? And I think the, you know, the sort of short
789360	794720	timelines AI world is if it's not that hard. And the reason it might not be that hard is that,
795360	798960	you know, there's only really a few extra tokens you need to learn, right? You need to kind of
799040	803040	learn error correction tokens, the tokens where you're like, ah, I think I made a mistake. Let me
803040	805920	think about that again. You need to learn the kind of planning tokens. That's kind of like, I'm going
805920	810080	to start by making a plan. Here's my plan of attack. And then I'm going to write a draft. And I'm
810080	813040	going to like, now I'm going to critique my draft. I'm going to think about it. And so it's not,
813040	817440	it's not things that models can do right now. But, you know, the question is how hard is that?
818240	821040	And in some sense, also, you know, there's sort of two paths to agents, right? You know,
821680	824720	when Cholto was on your podcast, you know, he talked about kind of scaling,
824800	829280	leading to more nines of reliability. And so that's one path. I think the other path is
829280	833760	this sort of like unhobbling path where you, it needs to learn this kind of like system to
833760	838000	process. And if it can learn this sort of system to process, it can just use kind of millions of
838000	844720	tokens and think for them and be cohesive and be coherent. You know, one analogy. So when you drive,
844720	849120	here's an analogy, when you drive, right? Okay, you're driving. And, you know, most of the time,
849120	852320	you're kind of an autopilot, right? You're just kind of driving and you're doing well. And then,
853280	857040	but sometimes you hit like a weird construction zone or a weird intersection, you know, and then I
857040	860320	sometimes like, you know, my passenger seat, my girlfriend, I'm kind of like, ah, be quiet for
860320	863680	a moment. I need to like figure out what's going on, right? And that's sort of like, you know,
863680	867520	you go from autopilot to like the system to is jumping in and you're thinking about how to do
867520	871520	it. And so the scaling scaling is improving that system one autopilot. And I think it's sort of,
871520	876000	it's the brute force way to get to kind of agents who just improve that system. But if you can get
876000	881680	that system to working, then, you know, I think you could like quite quickly jump, you know,
881680	885600	to sort of this like more identified, you know, test time, compute, overhang is unlocked.
886400	893520	What's the reason to think that this is an easy win in the sense that, oh, you just get the,
893520	899040	there's like some loss function that easily enables you to train it to enable the system to
899040	902720	thinking. Yeah, there's not a lot of animals that have system to thinking, you know, it like took
902720	906640	a long time for evolution to give us system to thinking. Yeah, pre-training it like, listen,
906640	910880	I get it, you got like trillions of tokens of internet text, I get that like, yeah, you like
910880	915200	match that and you get all these, all this free training capabilities. What's the reason
915200	920000	to think that this is an easy and hobbling? Yeah, so, okay, a bunch of things. So
921840	926320	first of all, free training is magical, right? And it's, and it's, and it gave us this huge
926320	931440	advantage for, for, for models of general intelligence, because, you know, you could,
931440	934800	you just predict the next token, but predicting the next token, I mean, it's sort of a common
934800	938480	misconception. But what it does is lets this model learn these incredibly rich representations,
938480	941120	right? Like these sort of representation learning properties are the magic of deep
941120	944640	learning. You have these models, and instead of learning just kind of like, you know, whatever,
944640	947520	statistical artifacts or whatever, it learns sort of these models of the world. You know,
947520	951040	that's also why they can kind of like generalize, right? Because it learned the right representations.
952320	956400	And so, you know, you train these models and you have this sort of like raw bundle of capabilities
956400	961280	that's really useful. It's sort of this almost unformed raw mass. And sort of the unhobbling
961280	965360	we've done over sort of like GP2 to GP4 was, was you kind of took this sort of like raw mass,
965440	968880	and then you like RLHF it into a really good chat bot. And that was a huge win, right? Like,
968880	972880	you know, going, going, you know, an RL, you know, in the original, I think it's truck GPT
972880	977200	paper, you know, RLHF versus non RLHF model, it's like 100x model size win on sort of human
977200	981040	preference rating, you know, it started to be able to do like simple chain of thought and so on.
981040	985280	But you still have a disadvantage of all these kind of like raw capabilities. And I think there's
985280	988880	still like a huge amount that you're not doing with them. And by the way, I think the sort of
988880	992000	this pre training advantage is also sort of the difference to robotics, right? Where I think
992000	996480	robotics, you know, you know, I think people used to say it was a hardware problem, but I think
996480	1000320	the hardware stuff is getting solved. But the thing we have right now is you don't have this
1000320	1004160	huge advantage of being able to bootstrap yourself with pre training, you don't have all this sort
1004160	1007840	of unsupervised learning you can do, you have to start right away with the sort of RL self play
1007840	1015040	and so on. Alright, so now the question is why, you know, why might some of this unhobbling and
1015040	1021200	RL and so on work? And again, there's sort of this advantage of bootstrapping, right? So I, you
1021280	1024960	know, your Twitter bio is being pre trained, right? You're actually not being pre trained
1024960	1028640	anymore. You're not being pre trained anymore. You are pre trained in like grade school and high
1028640	1033840	school. At some point you transition to be able being able to like learn by yourself. Right?
1034720	1039040	You weren't able to do that in elementary school. I don't know middle school probably high school
1039040	1044480	maybe when sort of started some guidance. You know, college, you know, you're smart, you can kind
1044480	1048640	of teach yourself. And then sort of models are just starting to enter that regime. Right? And so
1048720	1052320	it's sort of like, it's a little bit probably a little bit more scaling. And then you got to
1052320	1057760	figure out what goes on top and it won't be trivial, right? So a lot of a lot of deep learning is
1057760	1062000	sort of like, you know, it sort of seems very obvious in retrospect. And there's sort of this
1062000	1066080	some obvious cluster of ideas, right? There's sort of some kind of like thing that seems a little
1066080	1069120	dumb, but there's kind of works, but there's a lot of details you have to get right. So I'm not
1069120	1071600	saying this, you know, we're going to get this, you know, next month or whatever, I think it's
1071600	1075200	going to take a while to like really figure out a while for you is like half a year or something.
1075920	1082000	I don't know. I think it's between six months and three years, you know. But you know, I think
1082000	1086640	it's possible. And I think there's, you know, I think, and this is, I think it's also very related
1086640	1090960	to the sort of issue of the data wall. But I mean, I think the, you know, one intuition on the sort
1090960	1095760	of like learning, learning, learning by yourself, right, is sort of pre-training is kind of the
1095760	1100560	words are flying by, right? You know, and, and, or it's like, you know, the teacher is lecturing
1100560	1103920	to you. And the models, you know, the words are flying by, you know, they're taking, they're
1103920	1108640	just getting a little bit from it. But that's sort of not what you do when you learn from yourself,
1108640	1112320	right? When you learn by yourself, you know, so you're reading a dense math textbook,
1112320	1114720	you're not just kind of like skimming through it once, you know, you wouldn't learn that much
1114720	1118560	from it. I mean, some word cells just give them through reading, reread and reread the math textbook,
1118560	1122480	and then they memorize, you know, like, you just repeated the data, then they memorize.
1122480	1125760	What you do is you kind of like, you read a page, kind of think about it, you have some internal
1125760	1130480	monologue going on, you have a conversational study buddy, you try a practice problem, you know,
1130480	1134240	you fail a bunch of times. At some point, it clicks, and you're like, this made sense,
1134240	1138400	then you read a few more pages. And so we've kind of bootstrapped our way to being, being able to
1138400	1143600	do that now with models, or like just starting to be able to do that. And then the question is,
1143600	1148640	you know, being able to like, read it, think about it, you know, try problems. And the question is,
1148640	1152000	can you, you know, all this sort of self place, synthetic data, RL is kind of like making that
1152000	1158480	thing work. So basically translate, translate translating like in context, like right now,
1158480	1162160	there's like in context learning, right, super sample efficient. There's that, you know, in the
1162160	1167120	Gemini paper, right, it just like learns a language in context. And then you're pre training, not at
1167120	1173040	all sample efficient. But you know, what humans do is they kind of like, they do in context learning,
1173040	1176400	you read a book, you think about it until eventually it clicks. But then you somehow
1176400	1180880	distill that back into the weights. And in some sense, that's sort of like what RL is trying to
1180880	1186560	do. And like when RL is super finicky, but when RL works, RL is kind of magical, because it's sort
1186640	1191040	of the best possible data for the model. It's like when you try a practice problem, and you know,
1191040	1195280	and then you fail, and at some point you kind of figure it out in a way that makes sense to you,
1195280	1198400	that's sort of like the best possible data for you, because like the way you would have solved the
1198400	1203680	problem. And that's sort of, that's what RL is. Rather than just, you know, you kind of read how
1203680	1207680	somebody else solved the problem and doesn't, you know, initially click. Yeah, by the way, if that
1207680	1212000	takes sounds familiar, because it was like part of the question I asked on showman, that goes to
1212000	1215920	illustrate the thing I said in the intro, where like a bunch of the things I've learned about AI,
1216000	1221840	just like, we do these dinners before the interviews, and I'm like, oh, what should I ask
1221840	1228880	on showman? What should I ask Dario? Okay, suppose this is the way things go, and we get these
1228880	1233600	in hobblings. Yeah. And the scaling, right? So it's like, you have this baseline, just enormous
1233600	1238320	force of scaling, right? Where it's like GP2 to GP4, you know, GP2, it could kind of like, it was
1238320	1242160	amazing, right? It could string together plausible senses. But you know, it could, it could barely
1242160	1246960	do anything. It's kind of like preschooler. And then GP4 is, you know, it's writing code, it like,
1246960	1250320	you know, can do hard math. And so it's sort of like smart high school. And so this big jump,
1250320	1253120	and you know, in sort of the essay series, I go through and kind of count the order's magnitude
1253120	1258560	of compute scale up with algorithmic progress. And so sort of scaling alone, you know, sort of by
1258560	1265040	2728 is going to do another kind of preschool to high school jump on top of GP4. And so that'll
1265040	1269200	already be just like at a per token level, just incredibly smart, they'll get you some more reliability.
1269200	1272480	And then you add these on hobblings that make it look much less like a chat bot, more like this
1272480	1278240	agent, like a drop in remote worker. And, you know, that's when things really get gone.
1278240	1283840	Okay, yeah. I want to ask you more questions about this. I think, yeah, let's zoom out. Okay,
1283840	1290720	so suppose you're right about this. Yeah. And I guess you this is because of the 2027 cluster,
1290720	1296400	we've got 10 gigawatt, 2027 10 gigawatt something 28 is the 10 gigawatt. Okay, so you'll be pulled
1296480	1303200	for it. And so I guess that's like 5.5 level by 2027, like whatever that's called, right?
1304240	1308640	What does the world look like at that point? You have these remote workers who can replace people.
1309600	1313680	What is the reaction to that in terms of the economy, politics, geopolitics?
1314960	1321280	Yeah, so, you know, I think 2023 was kind of a really interesting year to experience as somebody
1321280	1324720	who is like, you know, really following the ice stuff where, you know, before that,
1324880	1333680	what were you doing in 2023? I mean, open AI. And, and, and, you know, kind of went, you know,
1333680	1336960	I mean, I was, I was been thinking about this and, you know, like talking to a lot of people,
1336960	1339440	you know, in the years before, and it was this kind of weird thing, you know, you almost didn't
1339440	1343520	want to talk about AI or AGI, you know, it's kind of a dirty word, right? And then 2023, you know,
1343520	1347840	people saw chat, GPT for the first time in such a before, and it just like exploded, right? It
1347840	1352000	triggered this kind of like, you know, you know, a huge sort of capital expenditures from all these
1352000	1358640	firms and, and, and, and, you know, the explosion of revenue from NVIDIA and so on. And, you know,
1358640	1361680	things have been quiet since then. But, you know, the next thing has been in the oven. And I sort
1361680	1365760	of expect sort of every generation, these kind of like G forces to intensify, right? It's like,
1365760	1370400	people see the models. There's like, you know, people haven't counted them. So they're going to
1370400	1373920	be surprised. And it'll be kind of crazy. And then, you know, revenue is going to accelerate,
1373920	1377120	you know, suppose you do hit the 10 billion, you know, end of this year, suppose it like just
1377120	1380320	continues on this sort of doubling trajectory of, you know, like every six months of revenue
1380400	1383600	doubling, you know, it's like, you're not actually that far from 100 billion, you know,
1383600	1386960	maybe that's like 26. And so, you know, at some point, you know, like, you know,
1386960	1389680	sort of what happened to NVIDIA is going to happen to big tech, you know, like their stocks,
1389680	1395840	their, you know, that's going to explode. And I mean, I think a lot more people are going to feel
1395840	1401760	it, right? I mean, I think the, I think 2023 was the sort of moment for me where it went from
1401760	1405360	kind of AGI is a sort of theoretical abstract thing, and you'd make the models to like,
1405360	1409360	I see it, I feel it. And like, I see the path, I see where it's going. I like,
1410000	1413280	I think I can see the cluster where it's strained on, like the rough combination of algorithms,
1413280	1416400	the people, like how it's happening. And I think, you know, most of the world is not,
1416400	1420640	you know, most of the people feel it are like right here, right? But, but, you know, I think a
1420640	1426960	lot more of the world is going to start feeling it. And I think that's going to start being kind of
1426960	1431760	intense. Okay. So, right now, who feels that you can, you go on Twitter and there's these
1431760	1435040	GPT wrapper companies like, whoa, GPT Floreau is going to change our business.
1435040	1437760	I mean, I'm so, so bearish on the wrapper companies, right? Because like, they're the ones
1437760	1440560	that are going to be like, the wrapper companies are betting on stagnation, right? The wrapper
1440560	1443280	companies are betting like, you have these intermediate models and take so much left
1443280	1446160	to integrate them. And I'm kind of like, I'm really bearish because I'm like,
1446160	1448800	we're just going to sonic boom you, you know, and we're going to get the unhauled ones, we're
1448800	1452000	going to get the drop in remote worker. And then, you know, your stuff is not going to matter.
1452000	1459520	Okay. Sure. Sure. So that's done. Now, who, so the SF is paying attention now, or this crowd
1459520	1465200	here is paying attention. Who is going to be paying attention in 2026, 2027? And, but,
1465200	1467840	presumably this is, these are years in which the hundreds of billions of CapEx is being
1467840	1472000	spent on the eye. I mean, I think the, the national security state is going to be starting
1472000	1476400	to pay a lot of attention. And I, you know, I hope we get to talk about that.
1476400	1480080	Okay. Let's talk about it now. What happens? Yeah. Like, well, what is the sort of political
1480080	1483840	reaction immediately? Yeah. And even like internationally, like what people see like
1483840	1487040	right now, I don't know if like Xi Jinping like reads the news and sees like,
1487120	1491280	yeah, I don't know. Oh my God, like MMLU score on that. What are you doing about this comrade?
1494320	1498240	So what happens when the, like what, what are the, he's like sees a remote replacement and it
1498240	1501120	has a hundred billion dollars in revenue. There's a lot of businesses that have a hundred billion
1501120	1504720	dollars in revenue and people don't like aren't staying up all night talking about it.
1505680	1510160	The question, I think the question is when, when does the CCP and when does the sort of
1510160	1515040	American national security establishment realize that superintelligence is going to be
1515040	1518320	absolutely decisive for national power, right? And this is where, you know, the sort of intelligence
1518320	1521280	explosion stuff comes in, which, you know, we should also talk about later, you know, it's sort
1521280	1524560	of like, you know, you have AGI, you have this sort of drop in remote worker that can replace,
1524560	1527920	you know, you or me, at least that sort of remote jobs, you know, cognitive jobs.
1529600	1534960	And then, you know, I think fairly quickly, you know, I mean, by default, you know, you
1534960	1537520	turn the crank, you know, one or two more times, you know, and then you get a thing that's
1537520	1540720	smarter than humans. But I think even, even more than just turning the cramp a few more times,
1541600	1547040	you know, I think one of the first jobs to be automated is going to be that of sort of an AI
1547040	1552400	researcher engineer. And if you can automate AI research, you know, I think things can start
1552400	1556640	going very fast. You know, right now, there's already this trend of, you know, half in order of
1556640	1560240	magnitude a year of algorithmic progress, you know, suppose, you know, at this point, you're
1560240	1564880	going to have GPU fleets in the tens of millions for inference, you know, or more. And you're going
1564880	1569840	to be able to run like 100 million human human equivalents of these sort of automated AI researchers.
1569920	1574000	And if you can do that, you know, you can maybe do, you know, a decade's worth of sort of ML
1574000	1580080	research progress in a year, you know, get the some sort of 10x speed up. And if you can do that,
1580080	1584720	I think you can make the jump to kind of like AI that is vastly smarter than humans, you know,
1584720	1589040	within a year, a couple years. And then, you know, that broadens, right? So you have this,
1589040	1593280	you have this sort of initial acceleration of AI research that broadens to like you apply R&D to
1593280	1598320	a bunch of other fields of technology. And the sort of like extremes, you know, at this point,
1598320	1602560	you have like a billion, just super intelligent researchers, engineers, technicians, everything,
1602560	1606080	you're superbly competent, all the things, you know, they're going to figure out robotics.
1606080	1609280	Or we talked about it being a software problem. Well, you know, you have, you have a billion of
1610320	1613680	super smart, smarter than the smartest human researchers, AI researchers on your cluster,
1613680	1616320	you know, at some point during the intelligence explosion, they're going to be able to figure
1616320	1622320	out robotics, you know, and then again, that expands. And, you know, I think if you play this
1622320	1630880	picture forward, I think it is fairly unlike any other technology in that it will, I think,
1630880	1636160	you know, a couple years of lead could be utterly decisive in say like military competition, right?
1636160	1639760	You know, if you look at like go for one, right? Go for one, you know, like the Western coalition
1639760	1643520	forces, you know, they had, you know, like a hundred to one kill ratio, right? And that was like,
1643520	1647040	they had better sensors on their tanks, you know, and they had, they had better, you know, more
1647040	1651600	precision precision missiles, right? Like GPS, and they had, you know, stealth, and they had sort
1651680	1656400	of a few, you know, maybe 20, 30 years of technological lead, right? And they, you know,
1656400	1662800	just completely crushed them. Super intelligence applied to sort of broad fields of R&D. And
1662800	1665760	then, you know, the sort of industrial explosion as well, you have the robots, you're just making lots
1665760	1669920	of material, you know, I think that could compress, I mean, basically compress kind of like a century
1669920	1673840	worth of technological progress since the last decade. And that means that, you know, a couple
1673840	1678720	years could mean a sort of go for one style, like, you know, advantage in military affairs.
1678960	1685120	And, you know, including, like, you know, a decisive advantage that even like preempts
1685120	1688320	nukes, right? Suppose, like, you know, how do you find the stealth and nuclear submarines?
1688320	1691200	Like right now, that's a problem of like, you have sensors, you have the software,
1691200	1694720	like tech where they are, you know, you can do that, you can find them, you have kind of like
1694720	1698160	millions or billions of like mosquito-like, you know, size drones, and that, you know,
1698160	1702080	they take out the nuclear submarines, they take out the mobile launchers, they take out the other
1702080	1707440	nukes. And anyway, so I think enormously destabilizing, enormously important for national power,
1708880	1713520	and at some point, I think people are going to realize that, not yet, but they will. And when
1713520	1719760	they will, I think there will be sort of, you know, I don't think it'll just be the sort of AI
1719760	1724480	researchers in charge. And, you know, I think on the, you know, the CCP is going to, you know,
1724480	1728000	have sort of an all-out effort to like infiltrate American AI labs, right? You know, like billions
1728000	1731600	of dollars, thousands of people, you know, full force of the sort of, you know, Ministry of State
1731600	1735120	Security. CCP is going to try to, you know, like outbuild us, right? Like they, you know, their,
1735120	1739520	you know, power in China, you know, like the electric grid, you know, they added a U.S. is,
1739520	1743440	you know, a complete, like they added as much power in the last decade as like sort of entire
1743440	1746880	U.S. electric grid. So like the 100 gigawatts cluster, at least the 100 gigawatts is going
1746880	1751120	to be a lot easier for them to get. And so I think sort of, you know, by this point, I think it's
1751120	1756720	going to be like an extremely intense sort of international competition. Okay, so in this picture,
1758800	1763840	one thing I'm uncertain about is whether it's more like what you say, where it's more of an
1763840	1771680	implosion of you have developed an AGI and then you make it into an AI researcher. And for a while,
1771680	1777760	a year or something, you're only using this ability to make hundreds of millions of other AI
1777760	1783600	researchers. And then like the thing that comes out of this really frenetic process is a super
1783600	1787920	intelligence. And then that goes out in the world and is developing robotics and helping you take
1787920	1790800	over other countries and whatever. It's a little bit more, you know, it's a little bit more kind
1790800	1793600	of like, you know, it's not like, you know, on and off, it's a little bit more gradual, but it's
1793600	1797200	sort of like it's an explosion that starts narrowly. It's can do cognitive jobs, you know, the highest
1797200	1802080	RI use for cognitive jobs is make the AI better, like solve robotics, you know, and as as as you
1802080	1805680	know, you solve robotics, now you can do R&D and, you know, like biology and other technology.
1806560	1809680	You know, initially you start with the factory workers, you know, they're wearing the glasses
1809680	1812800	and the the AirPods, you know, and the AI is instructing them, right? Because, you know,
1812800	1816160	you kind of make any worker into a skilled technician, and then you have the robots come in.
1816160	1820960	And anyway, so it sort of expands, this process expands. Metas revans are a compliment to their
1820960	1824800	llama. Well, you know, whatever, like, you know, the fabs in the US, the constrained skilled workers,
1824800	1827840	right? You have, you have, even if you don't have robots that you have the cognitive super
1827840	1830640	intelligence and, you know, it can kind of make them all into skilled workers immediately. But
1830640	1834160	that's, you know, it's a very brief period, you know, robots will come soon. Sure. Okay. Okay, so
1834160	1839680	suppose this is actually how the tech progresses in the United States, maybe because these companies
1839680	1843360	are already experiencing hundreds of billions of dollars of revenue. At this point, you know,
1843360	1846640	companies are barring, you know, hundreds of billions of more in the corporate debt markets,
1846640	1851200	you know. But why is a CCP bureaucrat, some 60 year old guy, he looks at this and he's like,
1851200	1856480	oh, it's like, co-pilot has gotten better now. Why are they now? I mean, this is much more than
1856480	1863120	co-pilot has gotten better now. I mean, to them, like, yeah, because to shift the production of
1863120	1870320	an entire country, to dislocate energy that is otherwise being used for consumer goods or
1870320	1878160	something and to make it that all feed into the data centers. What part of this whole story is
1878160	1883280	you realize the super intelligence is coming soon, right? And I guess you realize it, maybe I realize
1883280	1887600	it. I'm not sure how much I realize it, but will the will the national security apparatus in the
1887600	1891920	United States and the CCP realize it? Yeah, I mean, look, I think in some sense, this is a really
1891920	1896960	key question. I think we have sort of a few more years of mid game, basically, and where you have
1896960	1903280	a few more 2023s, and that just starts updating more and more people. And I think the trend lines
1903280	1910320	will become clear. I think you will see some amount of the sort of COVID dynamic, right?
1910320	1917760	Like COVID was February of 2020. It honestly feels a lot like today, where it feels like this
1917760	1923600	utterly crazy thing is about, is impending, is coming. You kind of see the exponential and yet
1923600	1927040	most of the world just doesn't realize, right? The mayor of New York is like, go out to the shows,
1927040	1935840	and this is just Asian racism or whatever. But at some point, the exponential, at some point,
1935840	1940960	people saw it. And then just kind of crazy radical reactions came.
1940960	1944720	Right. Okay, so by the way, what were you doing during COVID? February?
1946000	1947280	Like freshman, sophomore, what?
1948080	1951760	Junior? But still, like, what were you, like 17-year-old junior or something?
1953760	1957120	And then, like, did you short the market or something?
1957120	1957680	Yeah, yeah, yeah.
1957680	1960400	Okay. Did you sell at the right time?
1960400	1960800	Yeah.
1960800	1966400	Okay. Yeah, so there will be like a March 2020 moment, the thing that was COVID, but here.
1967920	1972240	Now, then you can make the analogy that you make in the series that this will then
1973200	1978240	cause the reaction of like, we got to do the Manhattan Project for America here.
1978240	1982720	I wonder what the politics of this will be like, because the difference here is,
1982720	1988400	it's not just like, we need the bomb to beat the Nazis. It's, we're building this thing that's
1988400	1992400	making all our entry prices rise a bunch, and it's automating a bunch of our jobs.
1992400	1995360	And the climate change stuff, like people are going to be like, oh my god, it's making climate
1995360	2000400	change worse. And it's helping big tech. Like, politically, this doesn't seem like a dynamic
2000960	2005680	where the national security apparatus or the president is like, we have to step on the gas
2005680	2007120	here and like, make sure America wins.
2009440	2012800	Yeah. I mean, again, I think a lot of this really depends on sort of how much people
2012800	2014560	are feeling it, how much people are seeing it.
2018080	2021600	You know, I think there's a thing where, you know, kind of basically our generation, right?
2021600	2026400	We're kind of so used to kind of basically peace and like, you know, the world, you know,
2026400	2033040	American hegemony and nothing matters. But, you know, the sort of like extremely intense and
2033040	2037600	these extraordinary things happening in the world and like intense international competition is
2037600	2041840	like very much the historical norm. Like in some sense, it's like, you know, sort of this,
2041840	2047520	there's this sort of 20 year very unique period, but like, you know, the history of the world is
2047520	2052000	like, you know, you know, like in World War II, right, it was like 50% of GDP went to, you know,
2052080	2055600	like, you know, war prodigy and production, you know, the US borrowed over 60% of GDP, you know,
2055600	2061040	and in, you know, I think Germany and Japan over 100%, World War I, you know, UK, Japan,
2061040	2069200	sorry, UK, France, Germany all borrowed over 100% of GDP. And, you know, I think the sort of,
2070400	2073840	much more was on the line, right? Like, you know, and, you know, people talk about World War I
2073840	2078480	being so destructive and you know, like 20 million Soviet soldiers dying and like 20% of Poland.
2078480	2081040	But, you know, that was just the sort of like, that happened all the time, right? You know,
2081040	2085680	like seven years war, you know, like whatever 20, 30% of Prussia died, you know, like 30 years war,
2085680	2090160	you know, like, I think, like, you know, up to 50% of like large swath of Germany died.
2092400	2100880	And, you know, I think the question is, will these sort of like, will people see that the
2100880	2104480	stakes here are really, really high and that basically is sort of like history is actually back.
2106160	2109120	And I think, you know, I think the American national security state thinks
2109840	2113200	very seriously about stuff like this. They think very seriously about competition with China. I
2113200	2116960	think China very much thinks of itself on this historical mission and your nation, the Chinese
2116960	2121600	nation, a lot about national power, I think a lot about like the world order. And then, you know,
2123040	2127040	I think there's a real question on timing, right? Like, do they, do they start taking this seriously,
2127040	2130240	right? Like when the intelligence explosion is already happening, like quite late, or do they
2130240	2133840	start taking this seriously, like two years earlier on that matters a lot for how things play out.
2133840	2137840	But at some point, they will, and at some point, they will realize that this will be sort of
2137840	2144800	utterly decisive for, you know, not just kind of like some proxy war somewhere, but, you know,
2144800	2148800	like whether liberal democracy can continue to thrive, whether, you know, whether the CCP will
2148800	2154480	continue existing. And I think that will activate sort of forces that we haven't seen in a long time.
2156080	2159520	The great conflict, the great power conflict thing definitely seems compelling.
2160080	2164400	I think just all kinds of different things seem much more likely when you think from a historical
2164480	2168160	perspective, when you zoom out beyond the liberal democracy that we've been living in,
2168720	2174400	had the pleasure to live in America, let's say 80 years, including dictatorships, including all
2174960	2179520	obviously war, famine, whatever. I was reading the Gullig Archipelago, and one of the chapters
2179520	2184560	begins with Sojenitsyn saying, if you would have told Russian citizens under the czars that because
2184560	2189440	of all these new technologies, we wouldn't see some great Russian revival or becomes a great power,
2189440	2196240	and the citizens are made wealthy. But instead, what you would see is tens of millions of Soviet
2196240	2202080	citizens tortured by millions of beasts in the worst possible ways, and that this is what would
2202080	2207360	be the result of the 20th century, they wouldn't have believed you, they'd have called you a slanderer.
2207360	2212000	Yeah, and you know, the, you know, the possibilities for dictatorship with super
2212000	2216000	intelligence are sort of even crazier, right? I think, you know, imagine you have a perfectly
2216080	2220720	loyal military and security force, right? That's it. No more, no more rebellions, right? No more
2220720	2225120	popular uprisings, you know, perfectly loyal, you know, you have, you know, perfect lie
2225120	2228800	detection, you know, you have surveillance of everybody, you know, you can perfectly figure
2228800	2232240	out who's the dissenter, weed them out, you know, no Gorbachev would have ever risen to power,
2232240	2236320	who had some doubts about the system, you know, no military coup would have ever happened.
2236320	2239920	And I think you, I mean, you know, I think there's a real way in which,
2240000	2247840	you know, part of why things have worked out is that, you know, ideas can evolve and, you know,
2247840	2251760	there's sort of like some, some sense in which sort of time heals a lot of wounds and time, you
2251760	2255520	know, and solves, solves, you know, a lot of debates and a lot of people had really strong
2255520	2258640	convictions, but you know, a lot of those have been overturned by time because there's been this
2258640	2262000	continued pluralism and evolution. I think there's a way in which kind of like, you know,
2262000	2265840	if you take a CCP like approach to kind of like truth, truth is what the party says,
2265840	2268960	when you supercharge that with super intelligence, I think there's a way in which that could just
2268960	2273600	be like locked in and trying for, you know, a long time. And I think the possibilities are pretty
2273600	2279760	terrifying. You know, your point about, you know, history and sort of like living in America for
2279760	2284320	the past eight years, you know, I think this is one of the things I sort of took away from growing
2284320	2288160	up in Germany is a lot of the stuff feels more visceral, right? Like, you know, my mother grew
2288160	2291760	up in the former East, my father in the former West, they like met shortly after the wall fell,
2291760	2295440	right? Like the end of the Cold War was the sort of extremely pivotal moment for me because it's,
2295440	2298960	you know, it's the reason I exist, right? And then, you know, growing up in Berlin and, you know,
2299600	2305760	former wall, you know, my great grandmother, who is still alive, is very important in my life.
2305760	2310160	You know, she was born in 34, you know, grew up, you know, during the Nazi era, during, you know,
2310160	2314000	all that, you know, then World War II, you know, like South of the firebombing of Dresden from the
2314000	2318240	sort of, you know, country cottage or whatever were, you know, the day as kids were, you know,
2318240	2322160	then, and then, you know, then spends most of her life in sort of the East German Communistic
2322160	2326080	leadership. You know, she'd tell me about, you know, in like 54 when there's like the popular
2326080	2330080	uprising, you know, in Soviet tanks came in, you know, her husband was telling her to get home
2330080	2335440	really quickly, you know, get off off the streets, you know, had a, had a son who, who tried to,
2335440	2339600	you know, ride a motorcycle across, across the Iron Curtain and then was put in the Stasi prison
2339600	2345920	for a while. You know, and then finally, you know, when she's almost 60, you know, it was the first
2345920	2354080	time she lives in, you know, a free country and, and a wealthy country. And, you know, when I was
2354080	2358640	a kid, she was, she, the thing she always really didn't want me to do was like get involved in
2358640	2363360	politics because like joining a political party was just, you know, it was a very bad connotations
2363360	2370240	for her. Anyway, and she sort of raised me when I was young, you know, and so it, you know, it
2370240	2375280	doesn't feel that long ago. It feels very close. Yeah. So I wonder when we're talking today about
2375920	2381520	the CCP, listen, the people in China who will be doing the pro, their version of the project
2381520	2388880	will be AI researchers who are somewhat westernized, who interact with either got educated in the West
2388880	2398080	or have colleagues in the West. Are they going to sign up for the, the CCP project that's going to
2398080	2403600	hand over control to Xi Jinping? What's your sense on, I mean, you're just like fundamentally,
2403600	2406640	they're just people, right? Like, can't you like convince them about the dangers of super
2406640	2411200	intelligence? Will they be in charge though? I mean, since this is, I mean, this is also the case,
2411200	2416400	you know, uh, you know, in the US or whatever, this is sort of like rapidly depreciating influence
2416400	2420400	of the lab employees. Like right now, the sort of AI lab employees have so much power, right over
2420400	2424240	this, you know, like, they're going to get automated and then you saw this November event, so much
2424240	2427280	power, right? But both, I mean, both they're going to get automated and they're going to lose all
2427280	2430640	their power. And it'll just be, you know, kind of like a few people in charge with their sort of
2430720	2436080	armies of automated eyes. But also, you know, it's sort of like the politicians and the generals
2436080	2438560	and the sort of national security state, you know, a lot, you know, it's, I mean, there's sort of,
2438560	2441520	this is the sort of some of these classic scenes from the, you know, the Oppenheimer movies, you
2441520	2444960	know, the scientists built it and then it was kind of, you know, and the bomb was shipped away and
2444960	2449200	it was out of their hands. You know, I actually, yeah, I think, I actually think it's good for
2449200	2454240	like lab employees to be aware of this is like, you have a lot of power now, but you know, maybe
2454240	2459520	not for that long and, you know, use it wisely. Yeah, I do, I do think they would benefit from
2459520	2463040	some more, you know, organs of representative democracy. What do you mean by that? Oh, I mean,
2463040	2466400	I, you know, in the sort of the, in the open AI board events, you know, employee at power was
2466400	2470240	exercising a very sort of direct democracy way. And I feel like that's how some of how that went
2470240	2473520	about, you know, I think it really highlighted the benefits of representative democracy and having
2473520	2479040	some deliverative organs. Interesting. Yeah. Well, let's go back to the 100 billion revenue,
2479040	2483920	whatever, and so these companies, Nala cluster, yeah, the companies are deploying, we're trying
2483920	2488080	to build clusters that are this big. Yeah. Where are they building it? Because if you say it's
2488080	2492640	the amount of energy that would be required for a small or medium sized US state, is it then Colorado
2492640	2495840	gets no power and it's happening in the United States or is it happening somewhere else? Oh,
2495840	2498960	I mean, I think that, I mean, in some sense, this is the thing that I always find funny is, you know,
2498960	2502080	you talk about Colorado gets no power, you know, the easy way to get the power would be like, you
2502080	2506080	know, displaced, less economically useful stuff, you know, it's like, whatever, buy up the aluminum
2506080	2509440	smelting plant and, you know, that has a gigalot and, you know, we're going to replace it with the
2509440	2512880	data center because that's important. I mean, that's not actually happening because a lot of
2512880	2516640	these power contracts are really sort of long-term locked in, you know, there's obviously people
2516720	2520320	don't like things like this. And so it sort of, it seems like in practice, what it's, what it's
2520320	2524160	requiring, at least right now is building new power. The, that might change. And I think that
2524160	2527120	that's when things get really interesting when it's like, no, we're just dedicating all of the
2527120	2531840	power to the AGI. Okay, so right now it's building new power, 10 gigawatt, I think quite doable.
2532640	2535120	You know, it's like a few percent of like US natural gas production.
2536880	2539680	You know, I mean, when you have the 10 gigawatt training cluster, you have a lot more in
2539680	2543200	friends. So that starts getting more, you know, I think 100 gigawatt, that starts getting pretty
2543200	2546880	wild. You know, that's, you know, again, it's like over 20% of US electricity production.
2548320	2551920	I think it's pretty doable, especially if you're willing to go for like natural gas.
2552880	2557040	I do, I do think, I do think it is incredibly important, incredibly important that these
2557040	2559840	clusters are in the United States. And why does it matter? It's in the US?
2562640	2567840	I mean, look, I think there's some people who are, you know, trying to build clusters elsewhere. And
2567840	2571200	you know, there's like a lot of free flowing Middle Eastern money that's trying to build clusters
2571200	2576640	elsewhere. I think this comes back to the sort of like national security question we talked about
2576640	2580320	earlier. Like would you, I mean, would you do the Manhattan Project and the UAE, right? And I think,
2580320	2584000	I think basically like putting, putting the clusters, you know, I think you can put them in the US,
2584000	2588000	you can put them in sort of like ally democracies. But I think once you put them in kind of like
2588000	2591520	dictatorships, authoritarian dictatorships, you kind of create this, you know, irreversible security
2591520	2596400	risk, right? So I mean, one cluster is there, much easier for them to exfiltrate the weights.
2596400	2600080	You know, they can like literally steal the AGI, the superintelligence. It's like they got a copy of
2600080	2604240	the, you know, of the atomic bomb, you know, and they just got the direct replica of that.
2604240	2608000	And it makes it much easier to them. I mean, we're ties to China, you can ship that to China.
2608000	2611760	So that's a huge risk. Another thing is they can just seize the compute, right? Like maybe right
2611760	2614720	now they just think of this, I mean, in general, I think people, you know, I think the issue here
2614720	2617840	is people are thinking of this as they, you know, chat, GBT, big tech product clusters. But I think
2617840	2622320	the cluster is being planned now, you know, three to five years out, like it will be the like AGI
2622320	2626000	superintelligence clusters. And so anyway, so like when things get hot, you know, they might just
2626000	2630640	seize the compute. And I don't know, suppose we put like, you know, 25% of the compute capacity
2630640	2634240	in the sort of Middle Eastern decaderships, well, they seize that. And now it's sort of a ratio
2634240	2638160	of compute of three to one, and you know, still have some more, but even like, even, even only,
2638160	2642240	only 25% of compute there, like, I think it starts getting pretty hairy, you know, I think three to
2642240	2646960	one is like, not that great of a ratio, you can do a lot with that amount of compute. And then look,
2646960	2649760	even, even if they don't actually do this, right, even they don't actually seize the compute, even
2649760	2653760	they actually don't steal the weights. There's just a lot of implicit leverage you get, right?
2653760	2660880	They get, they get the seat at the AGI table. And, you know, I don't know why we're giving
2660880	2666400	authoritarian dictatorships the seat at the AGI table. Okay, so there's going to be a lot of
2666400	2670800	compute in the Middle East, if these deals go through. First of all, who's who is it? Just
2670800	2674000	like every single big tech company is just trying to figure out where they're going to be.
2674000	2680400	Okay, okay. Well, I guess there's reports, I think Microsoft or yeah, which we'll get into.
2680480	2684720	So the UAE gets a bunch of compute because we're building the clusters there.
2685760	2690960	And why, so let's say they have 25% of, why does a compute ratio matter?
2693440	2696640	If it's about them being able to kick off the intelligence explosion,
2696640	2700880	isn't it just some threshold where you have 100 million AI researchers or you don't?
2700880	2704560	I mean, you can do a lot with, you know, 33 million extremely smart scientists.
2705280	2708560	And, you know, and again, a lot of the stuff, you know, so first of all, it's like,
2708640	2711760	you know, that might be enough to build the crazy bio weapons, right? And then you're in a
2711760	2715280	situation where like now, wow, we've just like, they stole the weights, they seized the compute,
2715280	2719280	now they can make, you know, they can build these crazy new WMDs that, you know,
2719280	2722560	will be possible super intelligence. And then you just kind of like proliferated the stuff.
2722560	2726800	And, you know, it'll be really powerful. And also, I mean, I think, you know,
2727840	2731520	three acts on compute isn't actually that much. And so the, you know, the,
2733840	2736240	you know, I think a thing I worry a lot about is
2738560	2742800	I think everything, I think the riskiest situation is if we're in some sort of like
2742800	2747200	really tight neck, feverish international struggle, right? If we're like really close
2747200	2752000	with the CCP and we're like months apart. I think the situation we want to be in,
2752000	2755040	we could be in, if we played our cards, right, is a little bit more like, you know, the US,
2755040	2759200	you know, building the atomic bomb versus the German project way behind, you know, years behind.
2760560	2763520	And if we have that, I think we just have so much more wiggle room, like to get safety,
2763520	2766400	right? We're going to be building like, you know, there's going to be these crazy new WMDs,
2766400	2769520	you know, things that completely undermine, you know, nuclear deterrence, you know,
2770240	2774960	intense competition. And that's so much easier to deal with if, you know, you're like, you know,
2774960	2777440	it's not just, you know, you don't have somebody right on your tails, you got to go,
2777440	2781840	go, go, you got to go maximum speed, you have no wiggle room. You're worried that at any time
2781840	2784800	they can overtake you. I mean, they can also just try to outbuild you, right? Like they might,
2784800	2788720	they might literally win, like China might literally win if they can steal the weights,
2788720	2793440	because they can outbuild you. And they maybe have less caution, both, you know, good and bad
2793440	2798560	caution, you know, kind of like whatever unreasonable regulations we have. Or you're just
2798560	2801760	in this really tight race. And I think it is that sort of like, if you're in this really tight race,
2801760	2804880	this sort of feverish struggle, I think that's when sort of there's the greatest peril of
2804880	2810160	self-destruction. So then presumably the companies that are trying to build clusters in
2810160	2813920	the Middle East realize this, what is it? Is it just that it's impossible to do this in America?
2813920	2817520	And if you want American companies to do this at all, then you do it in Middle East or not at all.
2817520	2819920	And then you just like, I'm trying to build a three gorgeous dam cluster.
2819920	2822640	I mean, there's a few reasons. One of them is just like, people aren't thinking about this as
2822640	2826160	the AGI superintelligence cluster. They're just like, ah, you know, like cool clusters for my,
2826160	2831120	you know, for my chat. So they're building in the plans right now are clusters, which
2831120	2834160	are ones that are like, because if you're doing once we're inference, presumably you
2834160	2836880	could like spread them out across the country or something. But the ones they're building,
2836880	2841440	they realize we're going to do one training run in this thing we're building.
2841440	2844960	I just think it's harder to distinguish between inference and training compute. And so people
2844960	2847920	can claim it's training compute, but I think they might realize that actually, you know,
2847920	2851120	this is going to be useful for, yeah, sorry, they might say it's inference compute. And
2851120	2852480	actually it's useful for training compute too.
2852480	2854960	Because of the synthetic data and things like that.
2854960	2858000	Yeah. The future of training, you know, like RL looks a lot like inference, for example, right?
2858000	2861840	Or, or you just kind of like end up connecting them, you know, in time, you know, it's like
2861840	2865360	a lot raw material, you know, it's like, you know, it's, it's, it's placing your uranium refinement
2865360	2866400	facilities there. Sure.
2866400	2869520	Anyway, so a few reasons, right? One is just like, they don't think about this as the AGI cluster.
2869520	2874000	Another is just like easy money from the Middle East, right? Another one is like, you know,
2875520	2878720	people saying, some people think that, you know, you can't do it in the U.S.
2878800	2883600	And, you know, I think we actually face this sort of real system competition here, because
2883600	2886640	again, some people think it's only autocracies that can do this, that can kind of like top
2886640	2891520	down, mobilize the sort of industrial capacity, the power, you know, get the stuff done fast.
2891520	2894320	And again, this is the sort of thing, you know, we haven't faced in a while.
2895040	2898640	But, you know, during the Cold War, like we really, there was this sort of intense system
2898640	2902160	competition, right? Like East West Germany was this, right? Like West Germany kind of like
2902160	2907600	liberal democratic capitalism versus kind of, you know, communist state planned. And, you know,
2907600	2912000	now it's obvious that the sort of, you know, the free world would win. But, you know, even as
2912000	2916080	late as like 61, you know, Paul Samuelson was predicting that the Soviet Union would outgrow
2916080	2920320	the United States because they were able to sort of mobilize industry better. And so yeah,
2920320	2925120	there's some people who, you know, shippost about loving America by day, but then in private,
2925120	2929280	they're betting against America. They're betting against the liberal order. And I think, I basically
2929280	2932480	just think it's a bad bet. And the reason I think it's a bad bet is I think this stuff is just
2932480	2936080	really possible in the U.S. And so there's make it possible in the U.S. There's some amount that
2936160	2939200	we have to get our act together, right? So I think there's basically two paths to doing it in the
2939200	2943520	U.S. One is you just got to be willing to do natural gas. And there's ample natural gas, right?
2943520	2947040	You put your cluster in West Texas, you put it in, you know, Southwest Pennsylvania by the,
2947040	2952160	you know, Marcello Shale. Ten gigawatt cluster is super easy. 100 gigawatt cluster also pretty
2952160	2956160	doable. You know, I think, you know, natural gas production in the United States is, you know,
2956160	2959520	almost doubled in a decade. If you do that, you know, one more time over the next, you know,
2960080	2963760	seven years or whatever, you know, you could power multiple trillion dollar data centers.
2964000	2968080	But the issue there is, you know, a lot of people have sort of these made these climate
2968080	2971280	commitments and not just government. It's actually the private companies themselves, right? The
2971280	2975280	Microsoft, the Amazons and so on. And these climate commitments, so they won't do natural gas.
2976240	2979120	And, you know, I admire the climate commitments, but I think at some point, you know,
2979760	2982560	the national interest and national security kind of is more important.
2983840	2987120	The other path is like, you know, you can do this sort of green energy mega projects, right? You
2987120	2993760	do the solar and the batteries and the, you know, the SMRs and geothermal. But if we want to do that,
2993760	2997840	there needs to be sort of a sort of broad, deregulatory push, right? So, like, you can't
2997840	3001360	have permitting take a decade, right? So you got to reform FERC. You got to, like, have, you know,
3001360	3005760	blanket NEPA exemptions for this stuff. You know, there's like Nain state level regulations,
3005760	3009040	you know, that are like, yeah, you could build, you know, you build the solar panels and batteries
3009040	3012800	next to your data center, but it'll still take years because, you know, you actually have to
3012800	3018000	hook it up to the state electrical grid, you know, and you'll have to, like, use governmental
3018000	3021920	powers to create rights of way to kind of, like, you know, have multiple clusters and connect them,
3021920	3025920	you know, and have thick cables, basically. And so, look, I mean, ideally, we do both,
3025920	3029120	right? Ideally, we do natural gas and the broad deregulatory agenda. I think we have to do at
3029120	3033360	least one. And then I think this possible stuff is just possible in the United States.
3033360	3037920	Yeah. I think a good analogy for this, by the way, before the conversation I was reading,
3038640	3042240	there's a good book about World War II industrial mobilization in the United States called
3042240	3048400	Freedom's Forge. Yeah. And I guess when we think back on that period, especially if you're from,
3048400	3052560	if you read, like, the Patrick Hallison Fast and the Progress Study stuff, it's like,
3052560	3058080	the hat state capacity back then, and people just got you done, but now it's a cluster.
3058080	3062880	Wasn't it all the case? No, so it was really interesting. So you have people who are from
3062880	3069040	the Detroit auto industry side, like Knudsen, who are running mobilization for the United States,
3069120	3073040	and they were extremely incompetent. Yeah. But then at the same time, you had
3073040	3077840	labor organization agitation, which is actually very analogous to the climate pledges and
3077840	3084960	climate change concern we have today, where they would have these strikes while literally into 1941,
3085520	3091920	that would cost millions of man hours worth of time when we're trying to make tens of millions,
3091920	3096000	sorry, tens of thousands of planes a month or something. And they would just
3096640	3103440	debilitate factories for, you know, trivial, like pennies on the dollar kind of concessions from
3103440	3110080	capital. And it was concerns that, oh, the auto companies are trying to use the pretext of a
3110080	3117440	potential war to actually prevent paying labor that money deserves. And so the climate changes
3117440	3120800	today, like, you think, ah, fuck, America's fucked, like, we're not going to be able to build this
3120800	3125120	shit. Like, if you, if you look at Nipah or something, but I didn't realize how debilitating
3125120	3128720	labor was in like World War Two, right? It was just, you know, before at the, you know,
3128720	3132240	it's sort of like 39 or whatever, the American military was in total shambles, right? You read
3132240	3135360	about it and it reads a little bit like, you know, the German military today, right? It's like, you
3135360	3139440	know, military expenditures, I think were less than 2% of GDP, you know, all the European countries
3139440	3143920	had gone even in peacetime, you know, like above 10% of GDP, sort of this like rapid mobilization,
3143920	3147280	there's nothing, you know, like, we're making kind of like no planes, there's no military
3147280	3150800	contracts, everything had been starved during the Great Depression. But there was this latent
3150800	3155120	capacity. And, you know, at some point, the United States got their act together. I mean,
3155120	3159280	the thing I'll say is I think, you know, the supplies are the other way around too, to basically
3159280	3162640	to China, right? And I think sometimes people are, you know, they kind of count them out a little
3162640	3166560	bit and they're like the export controls and so on. And, you know, they're able to make seven
3166560	3170160	nanometer chips now. I think there's a question of like, how many could they make? But, you know,
3170160	3173040	I think there's at least a possibility that they're going to be able to mature that ability and make
3173040	3177680	a lot of seven nanometer chips. And there's a lot of latent industrial capacity in China,
3177760	3181360	and they are able to like, you know, build a lot of power fast. And maybe that isn't activated for
3181360	3186240	AI yet. But at some point, you know, the same way the United States and like, you know, a lot of
3186240	3189120	people in the US and the United States government is going to wake up, you know, at some point,
3189120	3196320	the CCP is going to wake up. Yeah. Okay. Going back to the question of presumably companies,
3196320	3200400	if are they blind to the fact that there's going to be some sort of, well, okay, so
3200400	3204080	they realize that there's going, they realize scaling is a thing, right? Obviously, their whole
3204080	3209360	plans are continued on scaling. And so they understand that we're going to be in 2020 building
3209360	3214960	the 10 gigawatt data centers. And at this point, the people who can keep up our big tech just
3214960	3220880	potentially at like the edge of their capabilities, then sovereign wealth fund, fund of things, and
3220880	3228400	also big major countries like America, China, whatever. So what's their plan? If you look at
3228400	3234000	like these AI labs, what's their plan given this landscape? Do they not want the leverage
3234000	3238240	of having being in the United States? I mean, I think, I don't know, I think, I mean, one thing
3238240	3243760	the Middle East does offer is capital, but it's like America has plenty of capital, right? It's
3243760	3246720	like, you know, we have trillion dollar companies, like what are these Middle Eastern states, they're
3246720	3249920	kind of like trillion dollar oil companies, and we have trillion dollar companies, and we have
3249920	3253200	very deep financial markets, and it's like, you know, Microsoft could issue hundreds of billions
3253200	3257440	of dollars of bonds, and they can pay for these clusters. I mean, look, I think another argument
3257440	3261760	being made, and I think it's worth taking seriously is an argument that look, if we don't work with
3261760	3267200	the UAE or with these Middle Eastern countries, they're just going to go to China, right? And so,
3267200	3270160	you know, we, you know, they're going to build data centers, they're going to pour money into AI,
3270160	3276240	regardless, and if we don't work with them, you know, they'll just support China. And look, I mean,
3276240	3281280	I think, I think there's some merit to the argument, and in the sense that I think we should
3281280	3283920	be doing basically benefit sharing with them, right? I think we should talk about this later,
3283920	3287120	but I think basically sort of on the road to AGI, there should be kind of like two tiers of
3287120	3290640	coalitions should be the sort of narrow coalition of democracies, that's sort of the
3290640	3294560	coalition that's developing AGI. And then there should be a broader coalition where we kind of
3294560	3298000	go to other countries, including, you know, dictatorships, and we're willing to offer them,
3299680	3302160	you know, we're willing to offer them some of the benefits of the AI, some of the sharing.
3302160	3306080	And so it's like, look, if, if, if the UAE wants to use AI products, if they want to run, you know,
3306080	3310160	meta recommendation engines, if they want to run, you know, like the last generation models,
3310160	3314320	that's fine. I think by default, they just like wouldn't have had this seat at the AGI table,
3314320	3317440	right? And so it's like, yeah, they have some money, but a lot of people have money.
3317680	3323200	And, you know, the only reason they're getting this sort of coursey at the AGI table, the only
3323200	3328240	reason we're giving these dictators will have this enormous amount of leverage over this extremely
3328240	3334000	national security relevant technology is because we're, you know, we're kind of getting them excited
3334000	3340080	and offering it to them. You know, I think the other who like who specifically is doing this,
3340080	3343440	like just the companies who are going there to fundraise or like this is the AGI is happening
3343520	3346880	and you can find it or you can't. It's been reported that it's been reported that, you know,
3346880	3350560	Sam is trying to raise, you know, seven trillion or whatever for a chip project. And, you know,
3350560	3354080	it's unclear how many of the clusters will be there and so on. But it's, you know, definitely,
3354080	3357600	definitely stuff is happening. I mean, look, I think another reason I'm a little bit,
3357600	3361120	at least suspicious of this argument of like, look, if the US doesn't work with them, they'll
3361120	3366640	go to China is, you know, I've heard heard from multiple people. And this wasn't, you know,
3366640	3370320	from a time at open AI and I haven't seen the memo, but I have heard from multiple people
3370400	3375120	that, you know, at some point several years ago, open AI leadership had sort of laid out a plan
3375120	3379440	to fund and sell AGI by starting a bidding war between the governments of, you know, the United
3379440	3384080	States, China and Russia. And so, you know, it's kind of surprising to me that they're willing to
3384080	3388240	sell AGI to the Chinese and Russian governments. But also there's something that sort of feels a bit
3388240	3391760	eerily familiar about kind of starting this bidding war and then kind of like playing them
3391760	3397040	off each other. And well, you know, if you don't do this, China will do it. So anyway, interesting.
3397040	3402720	Okay, so that's pretty fucked up. But given that, that's, okay, so suppose that you were right
3402720	3407920	about, we ended up in this place because we got the way one of our friends put it is that the
3407920	3414240	Middle East has like no other place in the world, billions of dollars or trillions of dollars up
3414240	3422240	for persuasion. And the Microsoft board, it's only, it's only the dictator.
3422240	3426640	Yeah. But so let's say you're right, that you shouldn't have gotten them excited about AGI in
3426640	3431920	the first place. But now we're in a place where they are excited about AGI. And they're like,
3431920	3435440	fuck, we want us to have GPT-5 where we're going to be off building super intelligence.
3435440	3439920	This Atoms for Peace thing doesn't work for us. And if you're in this place,
3441920	3444640	don't they already have the leverage? Aren't you like, and as you might as well just say-
3444640	3447920	I don't think, I think the UAE on its own is not competitive, right? It's like, I mean,
3447920	3450800	they're already export controlled. Like, you know, we're not, you know, there's like,
3450800	3454080	you're not actually supposed to ship NVIDIA chips over there, right? You know, it's not like they
3454080	3456640	have any of the leading AI labs. You know, it's like they have money, but you know,
3456640	3458480	it's actually hard to just translate money into like-
3458480	3462320	But the other things you've been saying about laying out your vision is very much there's
3462320	3467120	this almost industrial process of you put in the compute and then you put in the algorithms.
3467120	3470640	Yes. You add that up and you get AGI on the other end.
3470640	3476160	Yes. If it's something more like that, then the case for somebody being able to catch up
3476160	3478400	rapidly seems more compelling than if it's some bespoke.
3478400	3481600	Well, well, if they can steal the algorithms and if they can steal the weights.
3481600	3485040	That's really, that's really where sort of, I mean, we should talk about this. This is really
3485040	3491920	important. And I think, you know. So like right now, how easy would it be for a foreign actor to
3491920	3497840	steal the things that are like, not the things that are released about Scarlett Johansson's voice,
3497840	3500400	but the RL things are talking about the unhobblings.
3500400	3504240	I mean, I mean, all extremely easy, right? You know, I, you know, deep mind even like,
3504240	3507600	you know, they don't make a claim that it's hard, right? Deep mind put out there,
3507600	3510800	like whatever, frontier safety, something, and they like lay out security levels,
3510800	3513600	and they, you know, security level zero to four and four is this new resilient,
3513600	3516480	resistant to state actors. And they say we're at level zero, right?
3516480	3519840	And then, you know, I mean, just recently, there was like an indictment of a guy who just
3519840	3523680	like stole the code, a bunch of like really important AI code and went to China with it.
3523680	3527280	And, you know, all he had to do to steal the code was, you know, copy the code and put it
3527280	3531520	into Apple notes and then export it as PDF. And that got past their monitoring, right? And, you know,
3531520	3534800	Google is the best security of any of the iLabs probably because they have the, you know, the
3534800	3538080	Google infrastructure. I mean, I think, I don't know, roughly, I would think of this as like,
3538080	3542160	you know, security of a startup, right? And like, what does security of a startup look like,
3542160	3545840	right? You know, it's not that good. It's easy to steal.
3545840	3552000	So even if that's the case, a lot of your posts is making the argument that, you know,
3552000	3555360	why are we going to get the intelligence explosion? Because if we have somebody with
3555360	3559280	the intuition of an Alec Radford, to be able to come up with all these ideas,
3559280	3564080	that intuition is extremely valuable and you scale that up. But if it's a matter of these,
3564800	3572160	if it's just in the code, that, like, if it's just the intuition, then that's not
3572160	3575920	going to be just in the code, right? And also because of export controls, these countries
3575920	3580480	are going to have slightly different hardware. You're going to have to make different trade-offs
3580480	3585840	and probably rewrite things to be able to be compatible with that, including all these things.
3585840	3589280	Is it just a matter of getting the right pen drive and you plug it into the gigawatt data center
3589280	3591600	and exit the three gorgeous dam and then you're off to the races?
3592800	3595920	I mean, like, there's a few different things, right? So one threat model is just stealing
3595920	3600080	the weights themselves. And the weights one is sort of particularly insane, right? Because they
3600080	3604560	can just like steal the literal like end product, right? Just like make a replica of the atomic bomb
3604560	3608240	and then they're just like ready to go. And, you know, I think that one just is, you know,
3608240	3612080	extremely important around the time we have AGI and superintelligence, right? Because it's,
3612080	3616480	you know, China can build a big cluster. By default, we'd have a big lead, right? Because
3616480	3619360	we have the better scientists, but we make the superintelligence, they just steal it,
3619360	3623600	they're off to the races. Weights are a little bit less important right now. Because, you know,
3623600	3629360	who cares if they steal the GPT-4 weights, right? Like whatever. And so, you know, we still have
3629360	3632640	to get started on weight security now. Because, you know, look, if we think AGI by 27, you know,
3632640	3635600	this stuff is going to take a while. And it, you know, it doesn't, you know, it's not just going
3635600	3638800	to be like, oh, we do some access control. It's going to, you know, if you actually want to be
3638800	3643680	resistant to sort of Chinese espionage, you know, it needs to be much more intense. The thing,
3643760	3646720	though, that I think, you know, people aren't paying enough attention to is the secrets,
3646720	3652240	as you say. And, you know, I think this is, you know, the compute stuff is sexy. You know,
3652240	3655440	we talk about it. But, you know, I think that, you know, I think people underrate the secrets
3656560	3659840	because they're, you know, I think they're, you know, the half an order of magnitude a year,
3659840	3662960	just by default, sort of algorithmic progress, that's huge. You know, if we have a few-year
3662960	3667600	lead by default, you know, that's 10, 30x, 100x bigger cluster, if we protected them.
3668640	3671760	And then there's this additional layer of the data wall, right? And so, we have to get
3671760	3674720	through the data wall. That means we actually have to figure out some sort of basic new paradigm,
3674720	3679440	sort of the AlphaGo step two, right? AlphaGo step one is learns from human imitation. AlphaGo step
3679440	3683760	two is the sort of self play RL. And everyone's working on that right now. And maybe we're going
3683760	3691280	to crack it. And, you know, if China can't steal that, then they, you know, then they're stuck.
3691280	3696880	If they can't steal it, they're off to the races. But whatever that thing is, is it like literally,
3696880	3700160	I can write down on the back of a napkin, because if it's that easy, then why is it that hard for
3700160	3703520	them to figure it out? And if it's more about the intuitions, then don't you just have to hire
3703520	3706960	Alec Radford? Like, what are you copying now? Well, I think there's a few layers to this, right?
3706960	3713440	So, I think at the top is kind of like sort of the, you know, fundamental approach, right? And
3713440	3717280	sort of like, I don't know, on pre-training, it might be, you know, like, you know, unsupervised
3717280	3720640	learning, next token protection, train on the entire internet. You actually get a lot of
3720640	3724960	juice out of that already. That one's very quick to communicate. Then there's like,
3724960	3727760	there's a lot of details that matter. And you were talking about this earlier, right? It's like,
3727760	3731200	probably the way that thing people are going to figure out is going to be like somewhat
3731200	3734880	obvious, or there's going to be some kind of like clear, you know, not that complicated thing,
3734880	3737200	that'll work. But there's going to be a lot of details to getting that right.
3737200	3742960	But if that's true, then again, why are we even, why do we think that getting state-level security
3742960	3746320	in these stars will prevent China from catching up? If it's just like, oh, we know some sort of
3746320	3751040	self-play RL, we require it to get past the data wall. And if it's as easy as you say,
3751040	3752720	in some fundamental sense. Well, I don't know if it's that easy. I mean, again, like, yeah.
3752720	3756160	But it's going to be solved by 2027, you say, like, right? It's like, not that hard.
3756160	3759600	I just think, you know, the US and the sort of, I mean, all the leading antelabs in the United
3759600	3762800	States, and they have this huge lead. I mean, by default, you know, China actually has some good
3762800	3766080	LLMs. You know, why do they have good LLMs? They're just using the sort of open source code, right?
3766080	3770720	You know, llama or whatever. And so the, the, I think people really underrate the sort of,
3770720	3774800	both the sort of divergence on algorithmic progress and the lead the US would have by default,
3774800	3776960	because by the, you know, all this stuff was published until recently, right?
3776960	3780000	Like Chinchilla scaling laws were published, you know, there's a bunch of MOE papers,
3780000	3783600	there's, you know, transformers and, you know, all that stuff was published. And so that's why
3783600	3786960	open source is good. That's why China can make some good models. That stuff is now, I mean,
3786960	3791200	at least they're not publishing it anymore. And, you know, if we actually kept it secret,
3791200	3795440	it would be this huge edge. To your point about sort of like some tacit knowledge,
3795440	3798240	now like Bradford, you know, there's, there's another layer at the bottom that is something
3798240	3801600	about like, you know, large scale engineering work to make these big training ones work.
3801600	3805840	I think that is a little bit more tacit knowledge. So I think that, but I think China will be able
3805840	3808640	to figure that out. That's like sort of engineering stuff. They're going to figure out how to
3808640	3812000	figure that out, but not how to get the RL thing working.
3812240	3817680	I mean, look, I don't know, Germany during World War II, you know, they went down the wrong path,
3817680	3821200	they did heavy water, and that was wrong. And there's actually, there's an amazing anecdote in
3821920	3825360	the making of the atomic bomb on this, right? So, so secrecy is actually one of the most
3825360	3829600	contentious issues, you know, early on as well. And, you know, part of it was sort of,
3830240	3833760	you know, zillard or whatever really thought, you know, the sort of nuclear chain reaction was
3833760	3838560	possible. And so an atomic bomb was possible when you went around and it was like, this is going to
3838560	3842400	be of enormous strategic importance, military importance. And a lot of people didn't believe
3842400	3845200	it or they're kind of like, well, maybe this is possible, but you know, I'm going to act as
3845200	3850880	it's not possible. And, you know, science should be open and all these things. And anyway, and so
3850880	3855520	these early days, so there had been some sort of incorrect measurements made on graphite as a
3855520	3860080	moderator and that Germany had. And so they thought, you know, graphite was not going to work, we
3860080	3866960	have to do heavy water. But then Fermi made some new measurements on graphite. And they indicated
3866960	3871680	that graphite would work, you know, this is really important. And then, you know, zillard kind of
3871680	3876000	assaulted Fermi with the kind of another secrecy appeal. And Fermi was just kind of, he was pissed
3876000	3879600	off, you know, at a temper tantrum, you know, he was like, he thought it was absurd, you know,
3879600	3883920	like, come on, this is crazy. But, you know, you know, zillard persisted, I think they
3883920	3890320	roped in another guy, Pegram, and then Fermi didn't publish it. And, you know, that was just in time,
3890320	3894560	because Fermi not publishing it meant that the Nazis didn't figure out graphite would work,
3894560	3898160	they went down this path of heavy water. And that was the wrong path. That was one of the sort,
3898160	3901280	you know, this is a key reason why the sort of German project didn't work out. They were kind
3901280	3908160	of way behind. And, you know, I think we face a similar situation on are we are we just going to
3908160	3912240	instantly leak the sort of how do we get past the data wall? What's the next paradigm? Or are we not?
3912240	3917920	So and the reason this would matter is if there's like a being one year ahead would be a huge
3917920	3921840	advantage in the world where it's like you deploy AI over time, and then just like, God,
3921840	3925440	they're going to catch up anyway. I mean, I interviewed Richard Rhodes, the guy who wrote
3925440	3933440	the making an atomic bomb. Yeah. And one of the anecdotes he had was when, so they'd realized
3933440	3939360	America had the bomb, obviously we dropped it in Japan. And Beria goes, the guy who ran the NKBD,
3940400	3945440	just a famously ruthless guy, just evil. And he goes to, I forgot the name of the guy, the Soviet
3945440	3951200	scientist was running their version of the Mandan project. He says, comrade, you will get us
3951200	3955280	the American bomb. Yeah. And the guy says, well, listen, their implosion device actually is not
3955280	3959920	optimal. We should make it a different way. And Beria says, no, you will get us the American bomb
3959920	3967200	or your family will be camped us. But the thing that's relevant about that anecdote is actually
3967840	3971120	the Soviets would have had a better bomb if they hadn't copied the American design, at least
3971120	3975600	initially. And which suggests that often in history, this is something that's not just for the
3975600	3982320	Manhattan project, but there's this pattern of parallel invention where, because the tech tree
3982320	3986240	implies that the certain thing is next, in this case, self play, RL, whatever.
3988160	3991040	Then people are just like working on that. And like people are going to figure out around the same
3991040	3995680	time. There's not, there's not going to be that much gap in who gets it first. It wasn't like
3995680	3999200	famously the bunch of people were invented something like the light bulb around the same time and so
3999200	4003760	for it. So, but is it just that like, yeah, that might be true, but it'll be the one year or the
4003760	4006640	six months or whatever. Two years makes all the difference. I don't know if it'll be two years
4006640	4010240	though. I mean, I actually, I mean, I actually think if we locked down the labs, we have, we have
4010240	4013520	much better scientists, we're way ahead, it would be two years. But even, I think, even, I think,
4013520	4016800	I think whether you, I think, yeah, I think even six months a year would make a huge difference.
4016800	4020240	And this gets back to the sort of intelligence explosion. It's like a year might be the difference
4020240	4025440	between, you know, a system that's sort of like human level and a system that is like vastly super
4025440	4029520	human, right? It might be like five, five ooms, you know, even on the current pace, right? We went
4029600	4033760	from, you know, I think on the math benchmark recently, right? Like, you know, three years ago
4033760	4039280	on the math benchmark, we, you know, that that was, you know, this is a sort of really difficult
4039280	4043600	high school competition math problems. You know, we were at, you know, a few percent couldn't solve
4043600	4047840	anything. Now it's solved. And that was sort of at the normal pace of AI progress. You didn't have
4047840	4051520	sort of a billion super intelligent resources, researchers. So like a year is a huge difference.
4051520	4055120	And then particularly after super intelligence, right? Once this is applied to sort of lots of
4055120	4058480	elements of R&D, once you get the sort of like industrial explosion with the robots and so on,
4059200	4062880	you know, I think a year, you know, a couple years might be kind of like decades worth of
4062880	4067200	technological progress and might, you know, again, it's like go for one, right? 20, 30 years of
4067200	4071680	technological lead, totally decisive. You know, I think it really matters. The other reason it
4071680	4076560	really matters is, you know, suppose, suppose they steal the weight, suppose they steal the
4076560	4080800	algorithms and, you know, they're close on our tails. Suppose we still pull out ahead, right?
4080800	4085200	We just kind of, we were a little bit faster, you know, we're three months ahead. I think the
4085200	4089360	sort of like world in which we're really neck and neck, you know, you only have a three-month lead
4089360	4094000	are incredibly dangerous, right? And we're in this like feverish struggle where like if they get ahead,
4094000	4099840	they get to dominate, you know, sort of maybe they'd get a decisive advantage. They're about in
4099840	4104160	clusters like crazy. They're willing to throw all caution to the wind. We have to keep up. There's
4104160	4108320	some crazy new WMDs popping up. And then we're going to be in the situation where it's like,
4108320	4111920	you know, crazy new military technology, crazy new WMDs, you know, like deterrence,
4111920	4115680	mutually-disturbed interaction, like keeps changing, you know, every few weeks. And it's like,
4115680	4120240	you know, completely unstable volatile situation. That is incredibly dangerous. So it's, I think,
4120240	4122880	I think, you know, both, both from just the technologies are dangerous from the alignment
4122880	4125840	point of view. You know, I think it might be really important during the intelligence explosion to
4125840	4130240	have the sort of six-month, you know, wiggle room to be like, look, we're going to like
4130240	4133040	dedicate more compute to alignment during this period because we have to get it right. We're
4133040	4138400	feeling uneasy about how it's going. And so I think in some sense that like one of the most
4138400	4142320	important inputs, so whether we will kind of destroy ourselves or whether we will get through
4142320	4146000	this just incredibly crazy period is whether we have that buffer.
4148000	4154960	Why, so before we go further object level in this, I think it's very much worth noting that
4154960	4162400	almost nobody, at least nobody I talk to, thinks about the geopolitical implications of AI. And I
4162400	4166720	think I have some object level disagreements that we'll get into, but or at least things I want
4166800	4173120	to iron out. I may not disagree in the end. But the basic premise that obviously if you
4173120	4177920	keep scaling and obviously if people realize that this is where intelligence is headed,
4177920	4184000	it's not just going to be like the same world where like what model are we deploying tomorrow
4184000	4189200	and what is the latest like people on Twitter like, oh, there are the GPT-4O is going to
4189200	4193120	shake your expectations or whatever. You know, COVID is really interesting because
4193360	4202720	before a year or something, when March 2020 hit, it became clear to the world like president,
4202720	4207360	CEO, media, average person, there's other things happening in the world right now. But the main
4207360	4213760	thing we as a world are dealing with right now is COVID. Soon on AGI. Yeah. Okay. And then so this
4213760	4217520	is the quiet period. You know, if you want to go on vacation, you know, you want to like, you want
4218480	4224320	maybe like now is the last time you can have some kids. You know, my girlfriend sometimes
4224320	4229200	complains that, you know, when I'm like, you know, off doing work or whatever, she's like,
4229200	4233200	I'm not spending time with her. She's like, you know, she threatens to replace me with like,
4233200	4237120	you know, GPT-6 or whatever. And I'm like, you know, GPT-6 will also be too busy doing AI research.
4239760	4244400	Okay. Anyway, so what's the answer to the question of why, why aren't other people talking
4244400	4247280	about being national security? I made this mistake with COVID, right? So I, you know,
4247280	4252800	February of 2020, and I, you know, I thought just it was going to sweep the world and all the hospitals
4252800	4257040	would collapse and it would be crazy. And then, and then, you know, and then it'd be over. And you
4257040	4259680	know, a lot of people thought this kind of the beginning of COVID, they shut down their offices
4259680	4263680	a month or whatever. I think the thing I just really didn't price in was the sidal reaction,
4263680	4269760	right? And, and within weeks, you know, Congress spent over 10% of GDP on like COVID measures,
4269760	4275520	right? The entire country was shut down. It was crazy. And so I don't know, I didn't price it in
4275520	4280640	with COVID sufficiently. I don't know, why do people underrate it? I mean, I think there's,
4280640	4284800	there's a, there's a sort of way in which being kind of in the trenches actually kind of, I think
4286720	4289680	gives you a less clear picture of the trend lines. You actually have to zoom out that much
4289680	4294080	only like a few years, right? But you know, you're in the trenches, you're like trying to get the
4294080	4297360	next model to work, you know, there's always something that's hard, you know, for example,
4297360	4300880	you might underrate algorithmic progress because you're like, ah, things are hard right now or,
4300880	4304000	you know, data wall or whatever. But, you know, you zoom out just a few years and you actually
4304000	4307600	try to like count up how much algorithmic progress made in the last, you know, last few years. And
4307600	4313120	it's, it's enormous. But I also just don't think people think about this stuff. Like I think smart
4313120	4318160	people really underrate espionage, right? And, you know, I think part of the security issue is I
4318160	4322560	think people don't realize like how intense state level espionage can be, right? Like, you know,
4322640	4326800	you know, this is really company had had software that could just zero click hack any
4326800	4330160	iPhone, right? They just put in your number and then it's just like straight download of
4330160	4334560	everything, right? Like the United States infiltrated an air gap, atomic weapons program,
4334560	4339840	right? Wild, you know, like, yeah, you know, the, you know, you know, intelligence agencies have
4339840	4344720	just stockpiles of zero days, you know, when things get really hot, you know, I don't know,
4344720	4348080	maybe we'll send special forces, right? To like, you know, get go to the data center or something
4348080	4352400	that's, you know, or, you know, I mean, China does this, they threaten people's families, right?
4352400	4354880	And they're like, look, if you don't cooperate, if you don't give us the Intel,
4357360	4361120	there's a there's a good book, you know, along the lines of the gulag develop, you know, the
4361120	4367280	inside the aquarium, which is by a Soviet GRU de facto, GRU was like military intelligence,
4367280	4374720	Ilya recommended this book to me. And, you know, I think reading that is just kind of like shocked
4374800	4378480	at how intense sort of state level espionage is the whole book was about like, they go to these
4378480	4382080	European countries, and they try to like get all the technology and recruit all these people to get
4382080	4387840	the technology. I mean, yeah, maybe one anecdote, you know, so when so the spot, you know, this
4387840	4392480	eventual defector, you know, so he's being trained, he goes to the kind of GRU spy academy. And so
4392480	4396560	then to graduate from the spy academy, sort of before you're sent abroad, you kind of had to
4396560	4402480	pass a test to show that you can do this. And the test was, you know, you had to in Moscow recruit
4402560	4406400	a Soviet scientist and recruit them to give you information sort of like you would do in the
4406400	4413600	foreign country. But of course, for whomever you recruited, the penalty for giving away sort of
4413600	4420000	secret information was death. And so to graduate from the Soviet spy, the GRU spy academy, you had
4420000	4427280	to condemn a country man to death. States do this stuff. I started reading the book on
4427360	4431680	because I saw it in the series. Yeah. And I was actually wondering the fact that you use this
4431680	4437280	anecdote. Yeah. And then you're like enough of a book recommended by Ilya. Is this some sort of
4438080	4443600	is this some sort of Easter egg? We'll leave that for an exercise for the reader.
4445360	4448160	Okay, so the beatings will continue until them are all improved.
4451360	4455840	So suppose that we live in the world in which these secrets are locked down,
4456400	4461040	but China still realizes that this progress is happening in America.
4463040	4467920	In that world, especially if they realize, and I guess it's a very interesting open question,
4467920	4471440	it probably won't be locked down. Okay, but we're probably gonna live in the bad world.
4471440	4476400	Yeah, it's gonna be really bad. Why are you so confident that they won't be locked down? I mean,
4476400	4479520	I'm not confident that won't be locked down, but I think it's just it's not happening.
4480480	4487520	And so tomorrow, the lab leaders get the message. How hard like, what do they have to do?
4487520	4491600	They get the more security guards, they like air gap the, what do they do?
4491600	4496480	So again, I think basically it's, I think people, there's kind of like two two reactions there,
4496480	4503440	which is like, it's, we're already secure, not. And there's, fatalism, it's impossible.
4503440	4506160	I think the thing you need to do is you kind of got to stay ahead of the curve of basically
4506160	4510400	how EGI pillows the CCP, right? So like right now, you've got to be resistant to kind of like
4510400	4515760	normal economic espionage. They're not, right? I mean, I probably wouldn't be talking about the
4515760	4519360	stuff that the labs were, right? Because I wouldn't want to wake them up more, the CCP,
4519360	4523440	but they're not, you know, this is like, this stuff is like really trivial for them to do right
4523440	4527440	now. I mean, it's also anyway, so they're not resistant to that. I think it would be possible
4527440	4531440	for private company to be resistant to it, right? So, you know, both of us have, you know, friends
4531440	4534880	in the kind of like quantitative trading world, right? And, you know, I think actually those
4534880	4539440	secrets are shaped kind of similarly where it's like, you know, you know, they've said, you know,
4539440	4542960	yeah, if I got on a call for an hour or with somebody from a competitive firm, I could,
4542960	4547440	most of our alpha would be gone. And that's sort of like, that's the like list of details of like
4547440	4550880	really how to, how to make, you're gonna have to worry about that pretty soon. You're gonna have to
4550880	4555360	worry about that pretty soon. Yeah. Well, anyway, and so, so all alpha could be gone. But in fact,
4555360	4559360	their alpha persists, right? And, you know, often, often for many years and decades. And so this
4559360	4562640	doesn't seem to happen. And so I think there's like, you know, I think there's a lot you could go
4562720	4566000	if you went from kind of current startup security, you know, you just got to look through the window
4566000	4570800	and you can look at the slides. You know, it's kind of like, you know, you know, good private
4570800	4574160	sector security hedge funds, you know, the way Google treats, you know, customer data or whatever.
4576400	4581040	That'd be good right now. The issue is, you know, basically the CCP will also get more AI
4581040	4588000	filled. And at some point, we're going to face kind of the full force of, you know, the Ministry
4588000	4591280	of State Security. And again, you're talking about smart people underrating espionage and
4591280	4594960	sort of insane capabilities of states. I mean, this stuff is wild, right? You know, they can get
4594960	4598080	like, you know, there's papers about, you know, you can find out the location of like where you
4598080	4601280	are on a video game map, just from sounds, right? Like states can do a lot with like
4601280	4606000	electromagnetic emanations, you know, like, you know, at some point, like you got to be working
4606000	4609440	from a sketch, like your cluster needs to be air gapped and basically be a military base. It's
4609440	4612640	like, you know, you need to have, you know, intense kind of security clearance procedures
4612640	4616720	for employees, you know, they have to be like, you know, all their shit is monitored, you know,
4616720	4621280	they're, you know, they basically have security guards, you know, it's, you know, you can't use
4621280	4625040	any kind of like, you know, other dependencies, it's all got to be like intensely vetted, you know,
4625040	4632000	all your hardware has to be intensely vetted. And, you know, I think basically, if they actually
4632000	4635760	really face the full force of state level espionage, I don't really think this is the thing private
4635760	4638800	companies can do both. I mean, empirically, right, like, you know, Microsoft recently had
4638800	4642160	executives emails hacked by Russian hackers, and, you know, government emails, they've hosted
4642160	4646880	hacked by government actors. But also, you know, it's basically there's just a lot of stuff that
4646880	4649920	only kind of, you know, the people behind the security curtains know and only they deal with.
4651360	4655440	And so, you know, I think it's actually kind of resist the sort of full force of espionage,
4655440	4658480	you're going to need the government. Anyway, so I think basically we could, we could do it by
4658480	4662960	always being ahead of the curve. I think we're just going to always be behind the curve. And I
4662960	4667760	think, you know, maybe unless we get the sort of government project. Okay, so going back to the
4667760	4672960	naive perspective of we're very much coming at this from there's going to be a race in the CCP,
4672960	4678320	we must win. And listen, I understand like bad people are in charge of the Chinese government,
4678320	4684800	like the CCP and everything. But just stepping back in a sort of galactic perspective, humanity
4684800	4690560	is developing AGI. And do we want to come at this from the perspective of we need to feed China to
4690560	4696320	this, our super intelligent Jupiter brain descendants will know who China like China will
4696400	4701680	be something like distant memory that they have America to. So shouldn't it be a more the initial
4701680	4707120	approach just come to them like, listen, we this is super intelligence. This is something like
4707120	4714720	we come from a cooperative perspective. Why why immediately sort of rush into it from a hawkish
4714720	4717760	competitive perspective. I mean, look, I mean, one thing I want to say is like a lot of the
4717760	4722480	stuff I talk about in the series is, you know, is sort of primarily, you know, descriptive,
4722560	4726480	right. And so I think that on the China stuff, it's like, you know, yeah, and some ideal world,
4726480	4730720	you know, we, we, you know, it's just all, you know, merry go around and cooperation. But again,
4730720	4736320	it's sort of, I think, I think people wake up to AGI. I think the the issue particular on sort of
4736320	4739920	like, can we make a deal? Can we make an international treaty? I think it really relates to sort of
4739920	4744800	what is the stability of sort of international arms control dreamers, right. And so we did very
4744800	4750000	successful arms control on nuclear weapons in the 80s, right. And the reason it was successful
4750080	4753040	is because the sort of new equilibrium was stable, right. So you take go down from, you know,
4753040	4758320	whatever, 60,000 nukes to 10,000 nukes. You know, when you have 10,000 nukes, you know,
4758320	4762160	basically breakout, breakout doesn't matter that much, right? Suppose the other guy now try to
4762160	4765360	make 20,000 nukes. Well, it's like, who cares, right? You know, like, it's still mutually
4765360	4769120	assured destruction. Suppose a rogue state kind of went from zero nukes to one nukes. It's like,
4769120	4772160	who cares, we still have way more nukes than you. I mean, it's still not ideal for destabilization,
4772160	4776400	but it's, you know, it'd be very different if the arms control agreement had been zero nukes,
4776400	4780080	right? Because if it had been zero nukes, then it's just like one rogue state makes one nuk.
4780080	4785280	The whole thing is destabilized, breakout is very easy. You know, your adversary state
4785280	4789120	starts making nukes. And so basically, when, when you're going to sort of like very low levels of
4789120	4792560	arms, or when you're going to kind of in your sort of very dynamic technological situation,
4794080	4797520	arms control is really tough because, because breakout is easy, you know, there's, there's,
4797520	4802080	I mean, there's some other sort of stories about this in sort of like 1920s, 1930s, you know, it's
4802080	4806480	like, you know, all the European states had done disarmament and Germany was, was kind of did this
4806480	4810640	like crash program to build the Luftwaffe. And that was able to like massively destabilize things,
4810640	4813840	because not that, you know, they were the first, they were able to like pretty easily build kind
4813840	4816960	of a modern, you know, Air Force, because the others didn't really have one. And that, you know,
4816960	4821600	that really destabilized things. And so I think the issue with EGI and super intelligence is the
4821600	4825920	explosiveness of it, right? So if you have an intelligence explosion, if you're able to go from
4825920	4830240	kind of EGI to super intelligence, if that super intelligence is decisive, like either, you know,
4830240	4833760	like a year after, because you've developed some crazy WMD, or because you have some like,
4833760	4838240	you know, super hacking ability that lets you, you kind of, you know, completely deactivate the
4838240	4842800	sort of enemy arsenal. That means like, suppose, suppose you're trying to like put in a break,
4842800	4846720	you know, like we both, we're both going to like cooperate, and we're going to go slower, you know,
4846720	4850640	on the cusp of EGI or whatever, you know, it's going to be such an enormous incentive
4850640	4853840	to kind of race ahead to break out. And we're just going to do an intelligence explosion. If we
4853840	4859200	can get three months ahead, we win. I think that makes it basically, I think any sort of arms
4859280	4863760	control agreement that comes as a situation where it's close, very unstable.
4864400	4871120	That's really interesting. This is very analogous to kind of a debate I had with Rose on the podcast
4871120	4876640	where he argued for nuclear disarmament. But if some country tries to break out and starts
4876640	4882320	developing nuclear weapons, the six months or whatever that you would get is enough to get
4882320	4886560	international consensus and invade the country and prevent them from getting nukes. And I thought
4886560	4894160	that was sort of, that's not a stable clue. But on this, right? So like maybe it's a bit easier
4894160	4897920	because you have EGI and so like you can monitor the other person's cluster or something like
4897920	4901920	data centers, you can see them from space actually, you can see the energy draw they're
4901920	4905280	getting. There's a lot of things as you were saying, there's a lot of ways to get information
4905280	4911520	from an environment if you're really dedicated. And also because unlike a nukes, the data centers are
4912160	4918240	nukes, you have obviously the submarines, planes, you have bunkers, mountains, whatever, you have
4918240	4922080	them in so many different places. A data center that you're 100 gigawatt data center, we can blow
4922080	4926320	that shit up if you're like we're concerned, right? Like just some cruise missile or something.
4926320	4928080	It's like very vulnerable to sabotage.
4928080	4931760	I mean, that gets to the sort of, I mean, that gets to the sort of insane vulnerability of this
4931760	4935280	period post superintelligence, right? Because basically, I think so you have the intelligence
4935280	4938640	explosion, you have these like vastly superhuman things on your cluster, but you're like, you
4938640	4941760	haven't done the industrial explosion yet, you don't have your robots yet, you haven't kind of,
4941760	4945440	you haven't covered the desert in like robot factories yet. And that is the sort of crazy
4945440	4950080	moment where, you know, say the United States is ahead, the CCP is somewhat behind. There's
4950080	4953680	actually an enormous incentive for first strike, right? Because if they can take out your data
4953680	4958640	center, they know you're about to have just this command and decisive lead, they know if we can
4958640	4963280	just take out this data center, you know, then we can stop it and they might get desperate.
4963280	4967760	And, you know, so I think basically we're going to get into a position, it's actually, I think it's
4967760	4971520	going to be pretty hard to defend early on. I think we're basically going to be in a position
4971520	4974960	where protecting data centers with like the threat of nuclear retaliation. It's like, maybe
4974960	4978480	sounds kind of crazy though, you know, and this is the inverse of the LAZR. We got to
4979200	4984080	the data centers. Nuclear deterrence for data centers. I mean, this is a, you know, Berlin,
4984080	4988320	you know, in the like late fifties, early sixties, both Eisenhower and Kennedy multiple times kind
4988320	4992320	of made the threat of full on nuclear war against the Soviets, if they tried to encroach on West
4992320	4996560	Berlin, sort of insane. It's kind of insane that that went well. But basically, I think that's
4996560	5000000	going to be the only option for the data centers. It's a terrible option. This whole scheme is
5000000	5004480	terrible, right? Like being, being, being in this like neck and neck race, sort of at this
5004480	5008960	point is terrible. And, you know, it's also, you know, I think I have some uncertainty basically
5008960	5012080	on how easy that decisive advantage will be. I'm pretty confident that if you have super
5012080	5015200	intelligence, you have two years, you have the robots, you're able to get that 30 year lead.
5015840	5019280	Look, then you're in this like go for one situation. You have your like, you know,
5019280	5022640	millions or billions of like mosquito size drones that can just take it out. I think there's even
5022640	5025920	a possibility you can kind of get a decisive advantage earlier. So, you know, there's these
5025920	5029600	stores, you know, about these as well about, you know, like colonization and like the sort of
5029600	5034720	1500s where it was, you know, these like a few hundred kind of spanyards were able to like topple
5034720	5038240	the Aztec empire, you know, a couple, I think a couple other empires as well, you know, each of
5038240	5042000	these had a few million people. And it was not like God like technological advantage. It was some
5042000	5045520	technological advantage. It was, I mean, it was some amount of disease. And then it was kind of
5045520	5050400	like cunning strategic play. And so I think there's a, there's a possibility that even sort of early
5050400	5054080	on, you know, it's you haven't gone through the full industrial explosion yet, we have super
5054080	5057920	intelligence, but you know, you're able to kind of like manipulate the imposing generals, claim
5057920	5061760	your ally with them, then you have, you have some, you know, you have sort of like some crazy new
5061760	5065520	bioweapons, maybe, maybe there's even some way to like pretty easily get a paradigm that like
5065520	5070160	deactivates enemy nukes. Anyway, so I think this stuff could get pretty wild. Here's what I think
5070160	5076000	we should do. I really don't want this volatile period. And so a deal with China would be nice.
5076000	5078320	It's going to be really tough if you're in this unstable equilibrium.
5078720	5083840	I think basically we want to get in a position where it is clear that the United States, that a
5083840	5087600	sort of coalition of democratic allies will win. It's clear the United States would be clear to
5087600	5091360	China, you know, that will require having locked down the secrets that will require having built
5091360	5094560	the 100 gigawatt cluster in the United States and having done the natural gas and doing what's
5094560	5099280	necessary. And then when it is clear that the democratic coalition is well ahead, then you go
5099280	5104240	to China and then you offer them a deal. And you know, China will know they're going to win. This
5104480	5110080	is going to be, they're very scared of what's going to happen. We're going to know we're going to
5110080	5112800	win, but we're also very scared of what's going to happen because we really want to avoid this
5112800	5118560	kind of like breakneck race right at the end and where things could really go awry. And,
5119840	5122960	you know, and then, and so then we offer them a deal. I think there's an incentive to come to
5122960	5126320	the table. I think there's a sort of more stable arrangement you can do. It's a sort of an Adams
5126320	5129680	for peace arrangement. And we're like, look, we're going to respect you. We're not, we're not going
5129680	5132720	to like, we're not going to use super intelligence against you. You can do what you want. You're
5132720	5136400	going to get your like, you're going to get your slice of the galaxy. We're going to like,
5136400	5139520	we're going to benefit share with you. We're going to have some like computer agreement where
5139520	5142400	it's like, there's some ratio of compute that you're allowed to have. And that's like enforced
5142400	5147680	with your like composing AI's or whatever. And we're just not going to do, we're just not going
5147680	5152880	to do this kind of like volatile sort of WMD arms race to the death. We're good. And sort of it's
5152880	5157440	like a new world order that's US led, that sort of democratic led, but that respects China. Let's
5157440	5164320	then do what they want. Okay. There's so much to, there's so much there. First on the galaxies
5164320	5167920	thing, I think it's just a funny anecdote. So I want to kind of want to tell it. And this,
5167920	5171040	we were at an event and I'm respecting Chad, I'm house rules here. I'm not revealing anything
5171040	5176400	about it, but we're talking to somebody or a Leopold was talking to somebody influential.
5176400	5183440	Afterwards, that person asked the group, Leopold told me that he wants, he's not going to spend
5183440	5189200	any money on consumption until he's ready to buy galaxies. And he goes, the guy goes,
5189920	5196720	I honestly don't know if you meant galaxies, like the brand of private plane galaxy or the physical
5196720	5201360	galaxies. And there was an actual debate. Like he went away to the restroom and there was an
5201360	5208080	actual debate among people who are very influential about, oh, they can't amend galaxies. And the
5208080	5212400	other people who knew you better be like, no, he means galaxies. I mean the galaxy. I mean the
5212720	5217200	galaxies. I mean, I think it'd be interesting. I mean, I think there's, I mean, there's two
5217200	5220800	ways to buy the galaxies. One is like at some point, you know, it's like post-superintelligence,
5220800	5224960	you know, they're so crazy. But by the way, I love, okay, so what happens is he's on the ground,
5224960	5228480	I'm laughing my ass off. I'm not even saying I think people were like, having this debate.
5228480	5234400	And then Leopold comes back and the guy, somebody's like, oh, Leopold, we're having this debate
5234400	5240640	about whether you meant you want to buy the galaxy or you want to buy the other thing.
5240640	5246240	And Leopold assumes they must mean not the Friday play in the galaxy versus the actual galaxy.
5246240	5250480	But do you want to buy the property rights with the galaxy or do you just send out the probes right
5250480	5264080	now? Oh my god. All right. Back to China. There's a whole bunch of things that I could ask about
5264800	5268560	that plan about whether you're going to get credible, promised. You will get some part of
5268640	5271760	galaxies, whether they care about that. I mean, you have AIs to help you enforce stuff.
5271760	5276000	Okay, sure. We'll leave that aside. That's a different rabbit hole. The thing I want to ask is,
5276560	5280880	but it has to be the thing we need. The only way this is possible is if we lock it down.
5280880	5283760	I see. If we don't lock it down, we are in this fever struggle.
5285040	5292160	Greatest peril mankind will have ever seen. So, but given the fact that in during this period,
5292160	5296960	instead of just taking their chances and they don't really understand how this AI governance scheme
5296960	5299680	is going to work, whether they're going to check, whether we had to actually get the galaxies,
5300880	5304000	the data centers, they can't be built underground. They have to be built above ground.
5304000	5308400	Taiwan is right off the coast of us. They need the chips from there. Why aren't we just going to
5308400	5312480	invade? Listen, we don't want like worst case scenario is they win the super intelligence,
5312480	5317920	which they're on track to do anyways. Wouldn't this instigate them to either invade Taiwan or blow
5317920	5321680	up the data center in Arizona or something like that? Yeah. I mean, look, I mean, talked about
5321680	5325040	the data center one and then you probably have to like threaten nuclear retaliation to protect
5325120	5328640	that. They might also just blow it up. There's also maybe ways they can do it without sort of
5328640	5332800	attribution, right? Like you. Stuxnet. Stuxnet. Stuxnet. Yeah. I mean, this is part of, we'll
5332800	5336240	talk about this later, but you know, I think, look, I think we need to be working on the Stuxnet
5336240	5341760	for the Chinese project, but the. But by the way, for the audience. I mean, Taiwan, the Taiwan
5341760	5348160	thing, the, you know, I talk about, you know, AGI about, you know, 27 or whatever. Do you know
5348160	5353520	about the like terrible 20s? No. Okay. Well, I mean, sort of in this sort of Taiwan watcher circles,
5353520	5357360	people often talk about like the late 2020s as like maximum period of risk for Taiwan,
5357360	5360640	because it's sort of like, you know, military modernization cycles and basically extreme
5360640	5364160	fiscal tightening on the military budget in the United States over the last decade or two
5364960	5369120	has meant that sort of we're in this kind of like, you know, trough in, in, in the late 20s of like,
5369120	5372400	you know, basically overall naval capacity. And, you know, that's sort of when China is saying
5372400	5375760	they want to be ready. So it's already kind of like, it's kind of pitching, you know,
5375760	5379600	there's some sort of like, you know, parallel timeline there. Yeah. Look, it looks appealing
5379600	5383120	to invade Taiwan. I mean, maybe not because they, you know, basically remote cutoff of
5383120	5388800	the chips. And so then it doesn't mean they get the chips, but it just means they, they,
5389760	5394160	you know, it's just, it's, you know, the machines are deactivated. But look, I mean, imagine if
5394160	5398640	during the Cold War, you know, all of the world's uranium deposits had been in Berlin,
5398640	5402720	you know, and Berlin was already, I mean, almost multiple times it was caused a nuclear war.
5402720	5409600	So God help us all. Well, the Groves had a plan after the after the war, that the plan was that
5409680	5414320	America would go around the world and getting the rights to every single uranium deposit,
5414320	5416800	because they didn't realize how much uranium there was in the world. And they thought this
5416800	5420160	was the thing that was feasible. Not realizing, of course, that there's like huge deposits in
5420160	5425760	the Soviet Union itself. Right. Okay. East Germany too. There's a, there's always,
5425760	5429040	there's a lot of East German workers who kind of got screwed and got cancer.
5430880	5435680	Okay. So the framing we've been talking about that we've been assuming, and I'm not sure I buy
5435680	5441680	yet, is that the United States, this is our leverage, this is our data center,
5441680	5445680	the China is the competitor. Right now, obviously, that's not the way things are progressing.
5445680	5449280	Private companies control these AIs. They're deploying them. It's a market-based thing.
5451840	5456000	Why will it be the case that it's like the United States, it has this leverage,
5456000	5458400	or is doing this thing versus China is doing this thing?
5459920	5463520	Yeah. I mean, look, look on the, on the project, you know, I mean, there's sort of descriptive
5463600	5466960	and prescriptive claims or sort of normative positive claims. I think the main thing I'm trying
5466960	5470720	to say is, you know, you know, look, we're at, we're at these SF parties or whatever.
5470720	5474560	And I think people talk about AGI and they're always just talking about the private AI labs.
5474560	5477280	And I think I just really want to challenge that assumption. It just seems like,
5478080	5481840	seems pretty likely to me, you know, as we've talked about, for reasons we've talked about,
5481840	5485360	that look like the national security state is going to get involved.
5485360	5488800	And, you know, I think there's a lot of ways this could look like, right? Is it,
5488800	5492400	is it like nationalization? Is it a public-private partnership? Is it a kind of defense
5492400	5496080	contra-elect or like a relationship? Is it a sort of government project that soaks up all the people?
5497360	5504080	And so there's a spectrum there. But I think people are just vastly underrating the chances of
5504080	5508080	this more or less looking like a government project. And look, I mean, look, if, if,
5509520	5512400	you know, it's sort of like, you know, do you think, do you think like we all have literal,
5512400	5515520	like, you know, when we have like literal superintelligence on our cluster, right? And it's
5515520	5519040	like, you know, you have 100 billion, they're like, sorry, you have a billion like superintelligence
5519040	5522880	scientists that they can like hack everything. They can like stuxnet the Chinese data centers,
5522880	5525520	you know, they're starting to build the robo armies, you know, you like, you really think
5525520	5529520	they'll be like a private company and the government would be like, oh my God, what is going on? You
5529520	5536080	know, like, yeah. Suppose there's no China. Suppose there's people like Iran, North Korea,
5536080	5540080	who theoretically at some point will be able to do superintelligence, but they're not on our heels
5540080	5544480	and don't have the ability to be on our heels. In that world, are you advocating for the national
5544480	5550160	project or do you prefer the private path forward? Yeah. So I mean, two responses to this. One is,
5550160	5554160	I mean, you still have like Russia, you still have these other countries, you know, you've got to have
5554160	5558560	Russia proof security, right? It's like, you can't, you can't just have Russia steal all your stuff.
5558560	5561520	And like, maybe their clusters aren't going to be as big, but like, they're still going to be able
5561520	5566160	to make the crazy bio weapons and the, you know, the mosquito-sized drones, you know, and so on.
5566160	5571840	And so, I mean, I think, I think, I think the security component is just actually a pretty
5571920	5576320	large component of the project in the sense of like, I currently do not see another way
5576320	5580880	where we don't kind of like instantly proliferate this to everybody. And so, yeah. So I think it's
5580880	5583920	sort of like, you still have to deal with Russia, you know, Iran, North Korea, and you know, like,
5583920	5586960	you know, Saudi and Iran are going to be trying to get it because they want to screw each other,
5586960	5589520	and you know, Pakistan and India, because they want to screw each other. There's like this enormous
5589520	5594000	destabilization still. That said, look, I agree with you. If, if, you know, if, you know, by some,
5594000	5598000	somehow things are shaking out differently, and like, you know, AGI would have been in 2005,
5598960	5603200	you know, sort of like unparalleled, you know, American hegemony. I think there would have
5603200	5608640	been more scope for less government involvement. But again, you know, as we're talking about
5608640	5612000	earlier, I think that would have been sort of this like very unique moment in history. And I
5612000	5615280	think basically, you know, almost all other moments in history, there would have been the
5615280	5622640	sort of great power competitor. So, okay, so let's get into this debate. So I, my position here
5622640	5627360	is if you look at the people who are involved in the Manhattan Project itself, many of them
5627360	5632320	regretted their participation, as you said. Now we can infer from that that we should sort of
5633040	5640320	start off with a cautious approach to the nationalized ASI project. Then you might say, well,
5640320	5644720	listen, obviously they're super. Did they regret their participation because of the project or
5644720	5648720	because of the technology itself? I think people will regret it. But I think it's, it's, it's about
5648720	5653760	the nature of the technology, and it's not about project. I think they also probably had a sense
5653760	5658160	that different decisions would have been made if it wasn't some concerted effort that everybody
5658160	5663360	had agreed to participate in. That if it wasn't in the context of this, we need to race to beat
5663360	5668080	Germany and Japan, you might not develop, so that's a technology part, but also like you wouldn't
5668080	5672160	actually like hit them. It's like the sort of, the destructive potential, the sort of,
5672160	5675760	you know, military potential, it's not, it's not because of the project, it is because of the
5675760	5682080	technology. And that will unfold regardless. You know, I think this underrates the power of
5682960	5687200	Imagine you go through like the 20th century in like, you know, a decade. You know, it's just
5687200	5691360	the sort of, the sort of, yes, great technological progress. Let's just actually run to that example.
5691360	5694880	So suppose you actually, there was some reason that the 20th century would be run through in one
5694880	5700560	decade. Do you think the cause of that should have been, should have been like the technologies
5700560	5704560	that happened through the 20th century shouldn't have been privatized? That it should have been a
5704560	5712480	more sort of concerted government led project. You know, look, there is a history of just dual
5712480	5716640	use technologies, right? And so I think AI in some sense is going to be dual use in the same way.
5716640	5720400	And so there's going to be lots of civilian uses of it, right? Like nuclear energy, it's like
5720400	5723040	itself, right? It was like, you know, there's the government project developed the military
5723040	5726400	angle of it. And then, you know, it was like, you know, then the government worked with private
5726400	5729440	companies, there's a sort of like real like flourishing of nuclear energy until you know,
5729440	5735200	the environmentalists stopped it. You know, planes, right? Like Boeing, right? Actually,
5735200	5738720	you know, the Manhattan project wasn't the biggest defense R&D project during World War II. It was
5738720	5742320	the B-29 bomber, right? Because they needed the bomber that had long enough range to reach Japan
5743360	5747360	to destroy their cities. And then, you know, Boeing made some Boeing made that,
5747360	5751600	B Boeing made the B-47, made the B-52, you know, the plane the US military uses today.
5751600	5757280	And then they use that technology later on to, you know, build the 707 and the sort of the
5758000	5764240	later on mean this context because in the other, like I get what it means after a war to privatize,
5764240	5769520	but if you have the government has ASI, maybe just let me back up and explain my concern.
5769520	5773760	So you have the only institution in our society, which has a monopoly on violence.
5775040	5780160	And then we're going to give the, give it some, in a way that's not broadly deployed,
5780160	5785200	access to the ASI, the counterfactual, and this may be sound silly, but listen,
5785200	5790160	we're going to go through higher and higher levels of intelligence. Private companies will
5790160	5795280	be required by regulation to increase their security, but they'll still be private companies
5795280	5799040	and they're deployed this and they're going to release the AGI, now McDonald's and JP
5799040	5802320	Morgan and some random startup are now more effective organizations because they have a bunch
5802320	5807040	of AGI workers. And it'll be sort of like the industrial revolution in the sense that the
5807040	5812160	benefits were widely diffused. If you don't end up in a situation like that, then the,
5813120	5817280	I mean, even backing up, like, what is it we're trying to, why do we want to win against China?
5817280	5822560	We want to win against China because we don't want a top down authoritarian system to win.
5823120	5829200	Now, if the way to beat that is that the most important technology that humanity will have
5829200	5835840	has to be controlled by a top down government, like, what was the point? Like, maybe, so let's
5835840	5839520	like run our cards with privatization. That's the way we get to the classic liberal
5839520	5843920	market based system we want for the ESIs. Yeah. All right. So a lot of talk about here. Yeah.
5843920	5847040	I think, yeah, maybe I'll start a bit about like actually looking at what the private world would
5847040	5850640	look like. And I think this is part of where the sort of there's no alternative comes from.
5850640	5853920	And then let's look like, look at like what the government project looks like, what checks and
5853920	5858880	balances look like and so on. All right. Private world. And first of all, okay, so right, like a
5858880	5861680	lot of people right now talk about open source. And I think there's this sort of misconception
5861680	5865440	that like AGI development is going to be like, Oh, it's going to be some like beautiful decentralized
5865440	5868880	thing. And you know, like, you know, some giddy community of coders who gets to like, you know,
5868880	5872320	collaborate on it. That's not how it's going to look like, right? You know, it's, you know,
5872320	5875200	$100 billion trillion dollar cluster. It's not going to be that many people that have it. The
5875200	5878480	algorithms, you know, it's like right now, open source is kind of good, because people just use
5878480	5881760	the stuff that was published. And so they basically, you know, the algorithms were published, or, you
5881760	5885120	know, as Mistral, they just kind of like leave deep mind and take all the secrets with them. And
5885120	5890800	they just kind of replicate it. That's not going to continue being the case. And so, you know,
5890800	5894720	the sort of like open source, I mean, also people say stuff like, you know, 1026 flops,
5894720	5898080	it will be in my phone, you know, it's no, it won't, you know, it's like Moore's Law is really
5898080	5901600	slow. I mean, AI chips are getting better. But like, you know, the $100 billion computer will
5901600	5906480	not cost, you know, like $1,000, you know, within your lifetime or whatever, aside from me. So anyway,
5906480	5912720	so it's going to be, it's going to be like two or three, you know, big players on the private world.
5913840	5920400	And so look, a few things. So first of all, you know, you talk about the sort of like, you know,
5920480	5924560	enormous power that sort of superintelligence will have and the government will have.
5926080	5930400	I think it's pretty plausible that the alternative world is that like one AI company has that power,
5930400	5932880	right? And specifically, if we're talking about lead, you know, it's like, what, I don't know,
5932880	5936800	open AI has a six month lead. And then, you know, so then you're not talking, you're talking about
5936800	5941120	basically, you know, the most powerful weapon ever. And it's, you know, you're kind of making this
5941120	5945600	like radical bet on like a private company CEO is the benevolent dictator. No, no, this is not
5945600	5949760	necessarily like any other thing that's privatized, we don't account on them being benevolent. We
5949760	5955520	just look to think of, for example, somebody who manufactures industrial fertilizer, right?
5955520	5960880	This is the person with this factory, if they went back to an ancient civilization, they could
5960880	5966320	like blow up Rome, they could probably blow up Washington DC. And I think in their series,
5966320	5970800	you talk about Tyler Cowan's phrase of muddling through. And I think even with privatization,
5970800	5975120	people sort of underrate that there are actually a lot of private actors who have the ability to
5976080	5981200	there's a lot of people who control the water supply or whatever. And we can count on cooperation
5981200	5985760	and market based incentives to basically keep a balance of power. I get that things are proceeding
5985760	5989200	really fast. But we have a lot of historical evidence that this is the thing that works best.
5989200	5994160	So look, I mean, what do we do with nukes, right? The way we keep the sort of nukes in check is not
5994160	5997360	like, you know, a sort of beefed up second amendment where like each state has their own
5997360	6001520	like little nuclear arsenal and like, you know, Dario and Sam have their own little nuclear arsenal.
6001520	6009520	No, no, it's like, it's institutions, it's constitutions, it's laws, it's courts. And so I
6009520	6012720	don't actually, I'm not sure that this, you know, I'm not sure that the sort of balance of power
6012720	6017040	analogy holds. In fact, you know, sort of the government having the biggest guns was sort of
6017040	6020800	like an enormous civilizational achievement, right? Like Landfrieden in the sort of Holy Roman
6020800	6024320	Empire, right? You know, if somebody from the town over kind of committed a crime on you,
6024320	6028320	you know, you didn't kind of start a sort of a, you know, a big battle between the two towns.
6028320	6032640	No, you take it to a court of the Holy Roman Empire and they would decide. And it's a big
6032640	6036480	achievement. Now, the thing about, you know, the industrial fertilizer, I think the key difference
6036480	6040800	is kind of speed and often Stephen's balance issues, right? So it's like 20th century and,
6040800	6047280	you know, 10 years in a few years. That is an incredibly scary period. And it is incredibly
6047280	6051200	scary, you know, because it's, you know, you're going through just this sort of enormous array of
6051200	6055680	destructive technology and the sort of like enormous amount of like, you know, basically
6055680	6059360	military event. I mean, you would have gone from, you know, kind of like, you know, you know,
6059360	6064000	bayonets and horses to kind of like tank armies and fighter jets in like a couple years and then
6064000	6067760	from, you know, like, you know, and then to like, you know, nukes and, you know, ICBMs and stuff,
6067760	6074000	you know, it's just like in a matter of years. And so it is sort of that speed that creates,
6074000	6076960	I think basically the way I think about it is there's going to be this initial just incredibly
6076960	6081200	volatile and incredibly dangerous period. And somehow we have to make it through that. That's
6081200	6086240	going to be incredibly challenging. That's where you need the kind of government project.
6086240	6090000	If you can make it through that, then you kind of go to like, now we can, now, you know, the
6090000	6093360	situation has been stabilized, you know, we don't face this imminent national security threat.
6093360	6096880	You know, it's like, yes, there were kind of WMDs that came along the way, but either we've managed
6096880	6100400	to kind of like have a sort of stable offense, defense balance, right? Like I think bioweapons
6100400	6104080	initially are a huge issue, right? Like an attacker can just create like a thousand different
6104080	6107200	static, you know, viruses and spread them. And it's like going to be really hard for you to kind
6107200	6110480	of like make a defense against each, but maybe at some point you figure out the kind of like,
6110560	6113840	you know, universal defense against every possible virus. And then you're in a stable
6113840	6117280	situation and on the offense, defense balance, or you do the thing, you know, you do with planes
6117280	6120160	where it's there's like, you know, there's certain capabilities that the private sector
6120160	6123840	isn't allowed to have. And you've like figured out what's going on, restrict those. And then you
6123840	6127840	can kind of like let, let, you know, you let this sort of civilian, civilian uses.
6127840	6134640	So I'm skeptical of this because well, there's sorry, I mean, the other important thing is,
6134640	6138160	so I talked about the sort of, you know, maybe it's like, it's, it's a, you know,
6138960	6141920	it's, you know, it's one company with all this power. And I think it's like, I think it is
6141920	6145760	unprecedented because it's like the industrial fertilizer guy cannot overthrow the US government.
6145760	6149280	I think it's quite plausible that like the AI company with super intelligence can overthrow
6149280	6152240	the multiple companies, right? And I buy that one of them could be ahead.
6152240	6156080	So it's not obvious that it'll be multiple. I think it's again, if there's like a six monthly,
6156080	6159680	maybe, maybe there's two or three, but if there's two or three, then what you have is just like
6159680	6163200	the crazy race between these two or three companies, you know, it's like, you know, whatever,
6163200	6166400	Demis and Sam, they're just like, I don't want to let the other one win.
6166400	6170320	And, and they're both developing their nuclear arsenals in the road. It's just like, also like,
6170320	6172800	come on, the government is not going to let these people, you know, are they going to let,
6172800	6176080	like, you know, is Dario going to be the one developing the kind of like, you know,
6176080	6179680	you know, super hacking Stuxnet and like deploying against the Chinese data center.
6179680	6183760	The other issue though, is it won't just, if it's two or three, it won't just be two or three.
6183760	6187200	There'll be two or three and it'll be China and Russia and North Korea because the private
6187200	6191040	and the private lab world, there's no way they'll have security that is good enough.
6191040	6195120	I think we're also assuming that somehow if you nationalize it, like the security just,
6195760	6200880	especially in the world, where it did this stuff is priced in by the CCP,
6200880	6205520	that now you've like got it nailed down. And I'm not sure why we would expect that to be the case.
6205520	6207760	But on this, government's the only one who does this stuff.
6207760	6212080	So if it's not Sam or Dario, who's, we don't want to trust them to be benevolent,
6212080	6220160	dictator or whatever. So by here, we're counting on, if it's, because you can
6220160	6223120	cause a coup, the same capabilities are going to be true of the government project, right?
6223120	6229680	And so the modal president in 2020, 2025, but Donald Trump will be the person that you don't
6229680	6235280	trust Sam or Dario to have these capabilities. And why, okay, I agree that like I'm worried
6235280	6240480	if the Sam or Dario have a one year lead on ASI in that, in that world, then I'm like concerned
6240480	6245360	about this being privatized. But in that exact same world, I'm very concerned about Donald
6245360	6249120	Trump having the capability. And potentially if we're living in a world where the takeoff
6249120	6252960	is slower than you anticipate, in that world, I'm like very much I want the private companies.
6252960	6257280	So like in no part of this matrix, is it obviously true that the government
6257280	6259920	led project is better than the private project? Let's talk about the government
6259920	6263600	project a little bit and checks and balances. In some sense, I think my argument is a sort
6263600	6268720	of brookian argument, which is like American checks and balances have held for over 200 years
6268720	6273600	and through crazy technological revolutions. The US military could kill like every civilian
6273600	6276240	in the United States. But you're going to make that argument, the private public
6276240	6280640	balance of power itself for hundreds of years. But yeah, why has it held? Because the government
6280640	6285600	has the biggest guns and has never before has a single CEO or a random nonprofit board
6285600	6290400	had the ability to launch nukes. And so again, it's like, you know, what is the track record
6290400	6293120	of the government checks and balances versus the track record of the private company checks
6293120	6297120	and balances? Well, the iLab, you know, like first stress test, you know, went really badly,
6297120	6302480	you know, that didn't really work, you know? I mean, even worse in the sort of private
6302480	6307040	company world. So it's both like, it is like the two private companies and the CCP and they just
6307040	6311200	like instantly have all the shit. And then it's, you know, they probably won't have good enough
6311200	6314960	internal control. So it's like, not just like the random CEO, but it's like, you know, rogue
6314960	6318240	employees that can kind of like use these super intelligences to do whatever they want.
6318240	6322720	And this won't be true of the government? Like the rogue employees won't exist on the project?
6322720	6327360	Well, the government actually like, you know, has decades of experience and like actually
6327360	6330400	really cares about the stuff. I mean, it's like they deal, they deal with nukes, they deal with
6330400	6334160	really powerful technology. And it's, you know, this is like, this is the stuff that the national
6334160	6337680	security state cares about. You know, again, to the go, let's talk about the government checks
6337680	6341440	and balances a little bit. So, you know, what are checks and balances in the government world?
6341440	6343920	First of all, I think it's actually quite important that you have some amount of
6343920	6347040	international coalition. And I talked about these sort of two tiers before. Basically,
6347040	6350240	I think the inner tier is a sort of modeled on the cool record agreement, right? This was like
6350240	6355840	Churchill and Roosevelt, they kind of agreed secretly. We're going to like pull our efforts
6355840	6358880	on nukes, but we're not going to use them against each other. And we're not going to use them
6358880	6362640	against anyone else with their consent. And I think basically look, bring in, bring in the UK,
6362720	6366000	they have deep mind bringing in the kind of like Southeast Asian states who have the chip supply
6366000	6370560	chain, bring in some more of kind of like NATO close democratic allies for, you know, talent and
6370560	6374240	industrial resources. And you have the sort of like, you know, so you have, you have those checks
6374240	6379440	and balances in terms of like more international countries at the table. Sorry, somewhat separately,
6379440	6382640	but then you have the sort of second tier of coalitions, which is the sort of Adams for peace
6382640	6386000	thing, where you go to a bunch of countries, including like the UAE, and you're like, look,
6386000	6389600	we're going to basically like, you know, there's a deal similar to like the NPT stuff where it's
6390080	6393920	you're not allowed to like do the crazy military stuff, but we're going to share the civilian
6393920	6398240	applications. We're in fact going to help you and share the benefits and, you know, sort of kind
6398240	6402240	of like this new sort of post superintelligence world order. All right, US checks and balances,
6402240	6405360	right? So obviously, Congress is going to have to be involved, right? Appropriate in
6405360	6409600	trillions of dollars. I think probably ideally you have Congress needs to kind of like confirm
6409600	6413200	whoever's running this. So you have Congress, you have like different factions of the government,
6413200	6416800	you have the courts, I expect the First Amendment to continue being really important. And maybe that,
6416800	6419520	I think that sounds kind of crazy to people, but I actually think, again, I think these are
6420160	6424160	institutions that have held this test of time in a really sort of powerful way.
6425040	6428480	You know, eventually, you know, this is why honestly, alignment is important is like,
6428480	6432800	you know, the AIs, you program the AIs to follow the Constitution. And it's like, you know,
6432800	6438480	why does the military work? It's like generals, you know, are not allowed to follow unlawful
6438480	6442320	orders or not allowed to follow unconstitutional orders. You have the same thing for the AIs.
6442320	6446640	So what's wrong with this argument? When you say, listen, maybe you have a point in the world
6446720	6449840	where we have extremely fast takeoff. It's like one year from AGI to ASI.
6449840	6453920	Yeah. And then you have the like, years after ASI, where you have this like,
6453920	6459040	extraordinary explosion. Maybe you have a point. We don't know, you have these arguments,
6459040	6461760	we'll like get into the weeds on them about why that's a more likely world, but like maybe
6461760	6466720	that's not the world we live in. And in the other world, I'm like, very on the side of making sure
6466720	6474560	that these things are privately held. Now, when you nationalize, that's a one way function,
6474560	6480400	you can't go back. Why not wait until we have more evidence on which of those worlds we live in?
6482080	6485920	And I think like rushing on the nationalization might be a bad idea while we're not sure.
6486560	6490400	And okay, I'll just respond to that first. I mean, I don't, I don't expect us to nationalize
6490400	6493840	tomorrow. If anything, I expected to be kind of with COVID where it's like kind of too late,
6493840	6497360	like ideally you nationalize it early enough to like actually lock stuff down,
6497360	6500240	it'll probably be kind of chaotic and like, you know, you're going to be trying to like do this
6500240	6503520	crash program to lock stuff down and it'll be kind of late and it'll be kind of clear what's
6503520	6506800	happening. We're not going to nationalize when it's not clear what's happening. I think the whole
6508080	6511360	historically these institutions have held up well. First of all, they've actually
6511360	6517040	almost broken a bunch of times. It's like, this is the argument that some people who are saying
6517040	6520080	that we shouldn't be that concerned about nuclear war say, where it's like, listen,
6520080	6524400	we have the nuke for 80 years and like we've been fine so far, so the risk must be low.
6524400	6529120	And then the answer to that is no, actually, it is a really high risk. And the reason we've avoided
6529120	6533680	it is like people have gone through a lot of effort to make sure that this thing doesn't happen.
6533680	6540400	I don't think that giving government ASI without knowing what that implies is going through the
6540400	6545440	lot of effort. And I think the base rate, like you can talk about America, I think America is
6545440	6550320	very exceptional, not just in terms of dictatorship, but in terms of every other country in history
6550320	6555360	has had a complete drawdown of wealth because of war, revolution, something. America is very
6555360	6558960	unique in not having that. And the historical base rate, we're talking about Greek power
6558960	6562000	competition. I think that has a really big, that's something we haven't been thinking about
6562000	6566240	the last 80 years, but it's really big. Dictatorship is also something that is just
6566240	6573680	DD, false state of mankind. And I think relying on institutions, which in an ASI world,
6575360	6580240	it's fundamentally right now, if the government tried to overthrow, it's much harder if you
6580240	6585760	don't have the ASI, right? Like there's people who have AR-15s and there's like things that
6585760	6588960	had to make it harder. If government could crush the AR-15s. No, I think it'd actually be pretty
6588960	6591840	hard. The reason it was Vietnam and Afghanistan were pretty hard. They're just a new whole country.
6592960	6596240	Yeah, yeah, I agree, but like I'm... They could. I mean similar with ASI.
6597920	6601200	Yeah, I think it's just like easier if you have what you're talking about. But there are institutions,
6601200	6604800	there are constitutions, there are legal restraints, there are courts, there are checks and balances.
6605680	6609600	The crazy bet is the bet which are like private company CEOs. The same thing, by the way,
6609600	6612800	isn't the same thing true of nukes where we have these institutional agreements about
6612800	6616960	non-polar aspiration and whatever. And we're still very concerned about that being broken
6616960	6619360	and somebody getting nukes and like you should stay up that night worrying about that.
6619360	6624720	It's a precarious situation, but ASI is going to be a really precarious situation as well. And like
6624720	6627040	given how precarious nukes are, we've done pretty well.
6627040	6629040	And so what does privatization in this world even mean?
6629040	6630240	I mean, I think the other thing is...
6630240	6631120	Like what happened after?
6631120	6633280	I mean, the other thing, you know, because we're talking about like whether the government
6633280	6636640	project is good or not. And it's like, I have very mixed feelings about this as well.
6637200	6639200	Again, I think my primary argument is like,
6640720	6643120	you know, if you're at the point where this thing has like
6644240	6647920	vastly superhuman hacking capabilities, if you're at the point where this thing can develop,
6647920	6651200	you know, bio weapons, you know, like in crazy bio weapons, ones that are like targeted,
6651200	6654240	you know, can kill everybody but the hand Chinese or, you know, that, you know,
6655280	6658800	would wipe out, you know, entire countries, where you're talking about like building Robo
6658800	6662400	Armors, you're talking about kind of like drone swarms that are, you know, again,
6662400	6664400	the mosquito sized drones that could take it out, you know,
6667600	6670800	the United States national security state is going to be intimately involved with this.
6670800	6673600	And this will, you know, the labs, whether, you know, and I think, again, the government,
6673600	6676640	a lot of what I think is the government project looks like, it is basically a joint venture
6676640	6680400	between like, you know, the cloud providers between some of the labs and the government.
6680400	6685120	And so I think there is no world in which the government isn't intimately involved in this
6685120	6688640	like crazy period. The very least, basically, you know, like the intelligence agencies need
6688640	6691600	to be running security for these labs. So they're already kind of like, they're controlling
6691600	6694960	everything, they're controlling access to everything. Then they're going to be like,
6694960	6698400	probably again, if we're in this like really volatile international situation, like a lot
6698400	6701760	of the initial applications, it'll, it'll suck. It's not what I want to use ASI for,
6701760	6706880	will be like trying to somehow stabilize this crazy situation. Somehow we need to prevent
6706880	6711040	like proliferation of like some crazy new WMDs and like the undermining of mutually assured
6711040	6717840	destruction to kind of like, you know, North Korea and Russia and China. And so I think,
6717920	6722960	you know, I basically think your world, you know, I think there's much more spectrum than
6722960	6726080	you're acknowledging here. And I think basically the world in which it's private labs is like
6726080	6729280	extremely heavy government involvement. And really what we're debating is like, you know,
6729280	6732800	what form of government project, but it is going to look much more like, you know,
6732800	6737280	the national security state than anything it does look like, like a startup as it is right now.
6737280	6738480	And I think the, yeah.
6738480	6743120	Look, I think something like that makes sense. I would be, if it's like the Manhattan Project,
6743200	6747040	then I'm very worried where it's like, this is part of the U.S. military.
6748480	6751920	Where I've hit some more like, listen, you got to talk to Jake Sullivan before you
6751920	6753120	like run the next training one.
6753120	6756960	It's like Lockheed Martin, Skunkward's part of the U.S. military. It's like, they call the shops.
6757520	6760560	Yeah, I don't think that's great. I think that's, I think that's bad. I think it would be bad if
6760560	6762800	that happened with ASI. And like, what is it? What is the scenario?
6764400	6765520	What is the alternative?
6765520	6768640	Okay. So it's closer to my end of the spectrum where,
6768640	6772640	yeah, you do have to talk to Jake Sullivan before you can launch the next training cluster.
6772640	6775680	But there's many companies who are still going for it.
6775680	6778720	And the government will be intimately involved in the security.
6780720	6781920	But the like, three different companies are trying to-
6781920	6783360	Is Dario launching the Stuxnet attack?
6784080	6786640	Yeah. What are you, what are you, launching, launching. Okay.
6787520	6790560	So Dario's activating the Chinese data centers.
6790560	6792960	I think this is similar to the story you could tell about, there's a lot of,
6792960	6797440	like literally the big tech right now. I think Satya, if you wanted to, he probably
6797440	6800000	like could get his engineers like, what are the zero days in Windows?
6800960	6805040	And like, well, how do we get infiltrate the president's computer so that like-
6805040	6805920	Maybe shut down.
6806560	6808720	No, no, no. Like right now I'm saying Satya could do that, right?
6808720	6809120	Because he knows-
6809120	6810000	Maybe shut down.
6810000	6810720	What do you mean?
6810720	6811840	Government wouldn't let them do that.
6813280	6816000	Yeah. I think there's a story you could tell where like they could pull off a coup,
6816000	6817040	whatever. But like, I think there's like-
6817040	6818560	Maybe not pull off a coup.
6818560	6818800	Okay.
6818800	6820320	Maybe not pull off, come on.
6820320	6823120	Okay. Fine, fine, fine. I agree. I'm just saying like something closer to,
6823920	6829520	so what's wrong with the scenario where you, the government is, there's like multiple
6829520	6834880	companies going for it, but the AI is still broadly deployed and alignment works in the
6834880	6838240	sense that you can make sure that it's not, the system level prompt is like,
6838240	6842320	you can't help people make bio weapons or something, but these are still broadly deployed.
6842320	6843120	So that-
6843120	6845280	I mean, I expect the AIs to be broadly deployed. I mean, first of all-
6845280	6846240	Even if it's a government project?
6846240	6848720	Yeah. I mean, look, I think first of all, like, I think the matters of the world,
6848720	6851520	you know, open sourcing their eyes, you know, that are two years behind or whatever.
6851520	6855760	Yeah. Super valuable role. They're gonna like, you know, and so there's gonna be some question
6855760	6859600	of like, either the offense, defense balance is fine. And so like, even if they open sourced
6859600	6862880	two year old AIs, it's fine. Or it's like, there's some restrictions on the most extreme dual use
6862880	6866080	capabilities, like, you know, you don't let private companies sell kind of crazy weapons.
6866800	6870720	And that's great. And that will help with the diffusion. And, you know, after the government
6870720	6873760	project, you know, there's gonna be this initial tense period, hopefully that's stabilized.
6873760	6876320	And then look, yeah, like Boeing, they're gonna go out and they're gonna like make,
6877280	6881040	do all the flourishing civilian applications and, you know, like nuclear energy, you know,
6881040	6885360	it'll like all the civilian applications will have their day. I think part of my argument here is that-
6885360	6889200	And how does that proceed, right? Because in the other world, there's existing stocks of capital
6889200	6892720	that are worth a lot. Yeah, the clusters, they'll be still be Google clusters.
6892720	6896880	And so Google, because they got the contract from the government, they'll be the ones that control
6896880	6901040	the ASI. But like, why are they trading with anybody else? Why is there a random start up again?
6901040	6904000	It'll be the same. It'll be the same companies that would be doing it anyway. But in this,
6904000	6907600	in this world, they're just contracting with the government or like their DPA for all their compute
6907600	6913360	goes to the government. And, but in the world, it's very natural. It's like sort of how
6914320	6918320	after you get the ASI and we're building the robot armies and building fusion reactors or whatever,
6918800	6922080	that the, that's- Only the government will get to build robot armies.
6922800	6926160	Yeah, that one worried. Or like the fusion reactors and stuff.
6926160	6928800	That's what we do with this, because- It's the same situation we have today.
6928800	6931920	Because if you already have the robot armies and everything, like the existing society doesn't
6931920	6935200	have some leverage where it makes sense to the government to- But they don't have that today.
6935200	6938560	Yeah, they get in the sense that there's like a lot of capital that the government wants and
6938560	6941360	there's other things like, why was Boeing privatized after?
6941360	6944080	Government has the biggest guns. And the way we regulate is institutions,
6944080	6945440	constitutions, legal restraints.
6945440	6948560	Oh, because tell me what privatization should look like in the ASI world afterwards.
6948560	6951440	Afterwards. Like the Boeing example, right? It's like, you have this government-
6951440	6954080	Who gets it? Like Google, Microsoft, and last year-
6954080	6956160	And who are they selling it to? Like they already have the robot factory.
6956160	6958240	And then look at- Why are they selling it to us? Like they already have the,
6958240	6961680	they don't need like our, this is chum change in the ASI world.
6961680	6965920	Because we didn't get like the ASI broadly deployed throughout this takeoff.
6965920	6969520	So we don't have the robot. We don't have like the fusion reactors and whatever advanced,
6969520	6971840	decades of advanced science that you were talking about.
6971840	6973840	So like it just, what are they trading with us for?
6974800	6976000	Trading with whom for?
6976000	6977600	Everybody who was not part of the project.
6977600	6979360	They've got that technology that's decades ahead.
6979360	6981040	Yeah. I mean, look, that's a whole nother issue of like,
6981040	6984240	how does like economic distribution work or whatever? I don't know. That'll be rough.
6985280	6986560	Yeah. I think-
6986560	6988800	I'm just saying, I don't, I don't, basically I'm kind of like,
6988800	6990880	I don't see the alternative. The alternative is,
6990880	6994400	you like overturn a 500 year civilizational achievement of land fleeting.
6994400	6998000	You basically instantly leak the stuff to the CCP.
6998000	7002240	And either you like barely scrape out ahead and, but you're in this fever struggle,
7002240	7004000	you're like proliferating crazy WMDs.
7004000	7005920	It's just like enormously dangerous situation,
7005920	7007120	enormously dangerous on alignment,
7007120	7009280	because you're in this kind of like crazy race at the end.
7009280	7011760	And you don't have the ability to like take six months to get alignment.
7011760	7015760	Right. The alternative is, you know,
7015760	7018560	alternative is like you aren't actually bundling your efforts to kind of like win
7018560	7020080	the race against the authoritarian powers.
7020160	7022480	You know, yeah. And so,
7025440	7029520	you know, I don't like it. You know, I wish,
7029520	7031760	I wish the thing we use the ASI for is to like,
7031760	7034160	you know, cure the diseases and do all the good in the world.
7034160	7038560	But it is my prediction that sort of like by the, in the end game,
7041520	7044320	what will be at stake will not just be kind of cool products,
7044320	7047840	but what will be at stake is like whether liberal democracy survives,
7047840	7052320	like whether the CCP survives, like what the world order for the next century will be.
7052320	7054000	And when that is at stake,
7054000	7057680	forces will be activated that are sort of way beyond what we're talking about now.
7057680	7061840	And like, you know, in the sort of like crazy race at the end,
7062400	7065360	like the sort of national security implications will be the most important,
7065360	7068000	you know, sort of like, you know, World War II, it's like, yeah, you know,
7068640	7069840	nuclear energy, how did it stay?
7069840	7073040	But in the initial kind of period, when, you know,
7073040	7074560	when this technology was first discovered,
7074560	7076480	you had to stabilize the situation, you had to get nukes,
7076480	7077280	you had to do it right.
7078560	7080960	And then, and then the civilian applications out there.
7080960	7083760	I think of closer analogy to what this is, because nuclear,
7083760	7085920	I agree that nuclear energy is the thing that happens in Iran,
7085920	7086960	and it's like dual use in that way.
7086960	7089600	But it's, it's something that happened like literally a decade after nuclear weapons were
7089600	7094080	developed, whereas with AI, like the immediately all the applications are unlocked.
7094080	7097680	And it's closer to literally, I mean, this is analogy people are supposed to make in
7097680	7103280	the context of AGI is like, assume your society had 100 million more John Wayne Neumanns.
7103280	7105680	And I don't think like, if that was literally what happened,
7105760	7108720	if tomorrow you just have 100 million more of them, the approach should have been,
7108720	7110240	well, some of them will convert to ISIS.
7110240	7112400	And we need to like be really careful about that.
7112400	7115680	And then like, oh, you know, like what if a bunch of them are born in China?
7115680	7118880	And then we like, if we got to nationalize the John Wayne Neumanns,
7118880	7121440	I'm like, no, I think it'll be generally a good thing.
7121440	7124960	And I'd be concerned about one power had getting like all the John Wayne Neumanns.
7124960	7128240	I mean, I think the issue is the sort of like bottling up in the sort of intensely short
7128240	7130880	period of time, like this enormous sort of like, you know,
7132000	7135120	unfolding of technological progress of an industrial explosion.
7135120	7137280	And I think we do worry about the 100 million John Wayne Neumanns.
7137280	7138320	It's like rise of China.
7138320	7139920	Why are we worried about the rise of China?
7139920	7143440	Because it's like 100 billion people and they're able to do a lot of industry
7143440	7144640	and do a lot of technology.
7144640	7147680	And but it's just like, you know, the rise of China times like, you know, 100,
7147680	7149120	because it's not just 101 billion people.
7149120	7153280	It's like a billion super intelligent, crazy, you know, crazy things.
7153280	7155360	And in like, you know, a very short period.
7156160	7160800	Let's start practically, because if the goal is we need to beat China,
7160800	7161840	part of that is protecting.
7161840	7162800	I mean, that's one of the goals, right?
7162800	7163440	Yeah, I agree.
7163440	7163760	I agree.
7163760	7164880	One of the goals is to beat China.
7164960	7168080	And also manage this incredibly crazy, scary period.
7168080	7168240	Yeah.
7168240	7168560	Right.
7168560	7171760	So part of that is making sure we're not leaking algorithmic secrets to them.
7171760	7172240	Yep.
7172240	7173680	Part of that is a cluster.
7174320	7175600	I mean, building the trillion dollar cluster.
7175600	7175920	That's right.
7175920	7176000	Right.
7176000	7176240	Yeah.
7176240	7180000	But like your whole point, the Microsoft can release corporate bonds that are.
7180000	7181760	I think Microsoft can do the like hundreds of billions.
7181760	7181920	Yeah.
7181920	7185120	I think I think the trillion dollar cluster is closer to a national effort.
7185840	7189200	I thought that your earlier point was that American capital markets are deep.
7189200	7189600	They're good.
7189600	7190160	They're pretty good.
7190160	7192400	I mean, I think the trillion, I think it's possible it's private.
7192400	7192960	It's possible.
7193280	7197520	But it's going to be like, you know, by the way, at this point, we have a AGI that's
7197520	7198880	rapidly accelerating productivity.
7198880	7201840	I think the trillion dollar cluster is going to be planned before before the AGI.
7202880	7206880	I think it's sort of like you get the AGI on the like 10 gigawatt cluster, like intelligent.
7206880	7209600	Maybe you have like one more year where you're kind of doing some final on hobbling to fully
7209600	7210240	unlock it.
7210240	7211840	Then you have the intelligence explosion.
7211840	7214000	And meanwhile, the like trillion dollar cluster is almost finished.
7214000	7216880	And then you like, and then you do your super intelligence on your trillion dollar cluster,
7216880	7218480	or you run it on your trillion dollar cluster.
7218480	7221040	And by the way, you have not just your trillion dollar cluster, but like, you know,
7221120	7223600	hundreds of millions of GPUs on inference clusters everywhere.
7223600	7227200	And this isn't result, like, I think private, in this world, I think private companies have
7227200	7228800	the capital and can raise capital to do it.
7228800	7231120	I think you will need the government force to do it fast.
7231120	7235520	Well, I was just about to ask, like, wouldn't it be the, like, we know company companies are
7235520	7241440	on track to be able to do this and China, if they're unhindered by climate pledges or whatever.
7241440	7242400	Well, that's part of what I'm saying.
7242400	7248160	So if that's the case, if it really matters that we beat China, there's all kinds of
7248160	7249360	practical difficulties of like,
7249920	7253680	will the AI researchers actually join the AI effort?
7253680	7258480	If they do, there's going to be three different teams at least who are currently doing
7258480	7262480	private pre-training on different, different companies.
7262480	7266880	Now who decides, at some point, you're going to have the, you're like YOLO, the hyperparameters
7266880	7271040	of the trillion dollar cluster, who decides that?
7271040	7275920	Just like merging extremely complicated research and development processes
7275920	7277360	across very different organizations.
7278320	7281280	This is how it's supposed to speed up America against the Chinese.
7281280	7282480	Like, why don't we just let...
7282480	7283520	Brain and deep mind merge.
7283520	7284720	And it was like a little messy, but it was fine.
7284720	7285520	It was pretty messy.
7285520	7288560	And it was also the same company and also much earlier on in the process.
7288560	7289520	I mean, pretty similar, right?
7289520	7292880	Same code, different code bases and like lots of different infrastructure and different teams.
7292880	7295920	And it was like, you know, it wasn't like, it wasn't the smoothest of all processes,
7295920	7297520	but, you know, deep mind is doing, I think, very well.
7297520	7301520	I mean, look, you give the example of COVID and the COVID example is like, listen,
7301520	7305040	we woke up to it, maybe it was late, but then we deployed all this money.
7305040	7308320	And COVID response to government was a clusterfuck over.
7308320	7311520	And like the only part of it that was worked is I agree Warp Speed was like enabled by the
7311520	7311840	government.
7311840	7315920	It was literally just giving the permission that you can actually do.
7315920	7318240	Well, it was also taking, making like the big
7318240	7319120	commitments or whatever.
7319120	7319520	But I agree.
7319520	7321760	But it was like fundamentally it was like a private sector led effort.
7321760	7322080	Yeah.
7322080	7323440	That was the only part of COVID that worked.
7323440	7326080	I mean, I think, I think, again, I think the project will look closer to operation
7326080	7326640	Warp Speed.
7326640	7329600	And it's not even, I mean, I think, I think you'll have all the companies involved
7329600	7330640	in the government project.
7330640	7332800	I'm not that sold that merging is that difficult.
7332800	7336240	You know, you have one, okay, you select one code base and you know, you run free
7336240	7338480	training on like GPUs with, you know, one code base.
7338480	7341360	And then you do the sort of second RL step on the, you know, the other code base with
7341360	7343200	TPU is that I think it's fine.
7344880	7346880	I mean, to the topic of like, will people sign up for it?
7346880	7348160	It wouldn't sign up for it today.
7348160	7349680	I think this would be kind of crazy to people.
7350640	7353120	But also, you know, I mean, this is part of the like secrets thing, you know, people
7353120	7357120	gather parties or whatever, you know, you know this, you know, I don't think anyone
7357120	7360880	has really gotten up in front of these people and been like, look, you know, the thing you're
7360880	7366000	building is the most important thing for like the national security of the United States
7366000	7369280	for like weather, you know, like, you know, the free world will have another century ahead
7369280	7369600	of it.
7369600	7373840	Like this is the thing you're doing is really important, like for your country, for democracy.
7375280	7377920	And, you know, don't talk about the secrets.
7377920	7381280	And it's not just about, you know, deep mind or whatever, it's about, it's about, you know,
7381280	7382320	these really important things.
7383760	7386000	And so, you know, I don't know, like, again, we're talking about the Manhattan project,
7386000	7386080	right?
7386080	7387840	This stuff was really contentious initially.
7388800	7391600	But, you know, at some point, it was like clear that this stuff was coming.
7391600	7395440	It was clear that there was like sort of a real sort of like exigency on the military
7395440	7396880	national security front.
7396880	7401520	And, you know, I think a lot of people come around on the like weather will be competent.
7401520	7402320	I agree.
7402320	7405280	I mean, this is again, where it's like a lot of the stuff is more like predictive in the
7405280	7407280	sense, I think this is like reasonably likely.
7407280	7408720	And I think not enough people are thinking about it.
7408720	7411600	You know, like a lot of people think about like AI lab politics or whatever.
7412640	7415200	But like nobody has a plan for the project, you know, it's like, you know,
7415520	7416800	they think you're pessimistic about it.
7416800	7418160	And like, well, you don't have a plan for it.
7418160	7420800	We need to do it very soon because AGI is upon us.
7420800	7425760	Then fuck, the only capable competent technical institutions capable of making AI right now
7425760	7426560	are private companies.
7426560	7428480	And they're going to play that leading role.
7428480	7429920	It'll be a sort of a partnership basically.
7429920	7432720	But the other thing is like, you know, again, we talked about World War II and, you know,
7432720	7436240	American unpreparedness, the veneer of World War II is complete, you know, complete shambles,
7436240	7436640	right?
7436640	7438800	And so there is a sort of like very company.
7438800	7443120	I think America has a very deep bench of just like incredibly competent managerial talent.
7443120	7446880	You know, I think that, you know, there's a lot of really dedicated people.
7446880	7451440	And, you know, I think basically a sort of operational warp speed, public-private partnership,
7451440	7454560	something like that, you know, is sort of what I imagine it would look like.
7454560	7454880	Yeah.
7454880	7460000	I mean, the recruiting the talent is an interesting question because the same sort of thing where
7462320	7464640	initially for the Manhattan Project, you had to convince people,
7464640	7466960	we've got to beat the Nazis and you got to get on board.
7466960	7471200	I think a lot of them maybe regretted how much they accelerated the bomb.
7471200	7476480	And I want, I think this is generally a thing of the war where...
7476480	7478320	I mean, I think they're also wrong to regret it, but...
7480560	7481600	Yeah, I mean, why?
7482400	7483520	What's the reason for regretting it?
7484080	7488640	I think there's a world in which you don't have, the way in which nuclear weapons were
7488640	7493840	developed after the war was pretty explosive because there was a precedent that you actually
7493840	7495360	can use nuclear weapons.
7495360	7498800	Then because of the race that was set up, you immediately go to the H bomb.
7499760	7503600	I mean, I think my view is, again, this is related to the view on AI and maybe some of
7503600	7505680	our disagreement is like, that was inevitable.
7505680	7511680	Like, of course, there was this world war and then obviously there was the cold war right after.
7511680	7517760	Of course, the military and technology angle of this would be pursued with ferocious intensity.
7517760	7520000	And I don't really think there's a world in which that doesn't happen,
7520000	7521920	where it's like, ah, we're all not going to build nukes.
7521920	7524000	And also just like nukes went really well.
7524000	7525520	I think that could have gone terribly, right?
7526080	7531440	Again, I think this is not physically possible with nukes, this pocket nukes for everybody,
7531440	7535920	but I think WMDs that are proliferated and democratized and all the countries have it.
7536560	7541360	The US leading on nukes and then building this new world order that was US-led,
7541360	7544880	or at least a few great powers, and a non-proliferation regime for nukes,
7544880	7550080	a partnership and a deal that's like, look, no military application of nuclear technology,
7550080	7551920	but we're going to help you with the civilian technology.
7551920	7553840	We're going to enforce safety norms on the rest of the world.
7553920	7557040	That worked. It worked. And it could have gone so much worse.
7558000	7559440	So we're zooming on.
7559440	7560560	I don't know if you're talking about Nagasaki.
7560560	7562720	You know, I mean, this is, I mean, I say this a bit in the piece,
7562720	7565920	but it's like actually the A-bomb, you know, like the A-bomb on Hiroshima and Nagasaki was just like,
7565920	7567200	you know, the sort of firebombing.
7568560	7573760	I think the thing that really changed the game was like the super, you know, the H-bombs and ICBMs.
7573760	7576480	And then I think that's really when it took it to like a whole new level.
7576480	7582800	I think part of me thinks when you say, we'll tell the people that for the free world to survive,
7582800	7584080	we need to pursue this project.
7584720	7587760	It sounds similar to World War II is,
7588800	7591920	so World War II is a sad story, obviously in this fact that it happened,
7591920	7594400	but also like the victory is sad in the sense that
7595120	7598880	Britain goes in to protect Poland.
7598880	7604880	And at the end, the USSR, which is, you know, as your family knows,
7605920	7610800	is incredibly brutal, ends up occupying half of Europe.
7610800	7616880	And part of protecting the free world, that's why I got to rush the AI.
7616880	7619920	And like, if we end up with the American AI Leviathan,
7619920	7621920	I think there's a world where we look back on this,
7621920	7629680	where it has the same sort of twisted irony that Britain going into World War II had about trying to protect Poland.
7631280	7633840	Look, I mean, I think there's going to be a lot of unfortunate things that happen.
7633840	7635840	I'm just hoping we make it through.
7635840	7639520	I mean, to the point of it's like, I really don't think the pitch will only be the sort of like,
7639520	7643040	you know, the race, I think the race will be sort of a backdrop to it.
7643040	7646800	I think the sort of general like, look, it's important that democracy shape this technology.
7646800	7650400	We can't just like leak this stuff to, you know, North Korea is going to be important.
7650400	7653440	I think also for the just safety, including alignment,
7653440	7657520	including the sort of like creation of new WMDs, I'm not currently sold.
7657520	7658480	There's another path, right?
7658480	7661760	So it's like, if you just have the breakneck grace, both internationally,
7661760	7664400	because you're just instantly leaking all the stuff, including the weights,
7665120	7668560	and just, you know, the commercial race, you know, Demis and Dario and Sam, you know,
7668560	7669920	just kind of like, they all want to be first.
7671280	7672800	And then it's incredibly rough for safety.
7672800	7674800	And then you say, okay, safety regulation.
7674800	7677680	But, you know, it's sort of like, you know, the safety regulation that people talk about,
7677680	7681520	it's like, oh, well, NIST, and they take years and they figure out what the expert consensus is,
7681520	7683920	and then they write what's going to happen to the project as well.
7683920	7687920	But I think, I mean, I think the sort of alignment angle during the intelligence explosion,
7687920	7690640	it's going to, you know, it's not a process of like years of bureaucracy,
7690640	7692080	and then you can kind of write some standards.
7692720	7696160	I think it looks much more like basically a war and like you have a fog of war.
7696160	7698480	It's like, look, it's like, is it safe to do the next oom?
7698480	7701280	You know, and it's like, ah, you know, like, you know, we're like three ooms into the
7701280	7702080	intelligence explosion.
7702080	7703840	We don't really understand what's going on anymore.
7705040	7709600	You know, the, you know, like a bunch of our like generalization scaling curves are like,
7709600	7710960	kind of looking not great.
7710960	7713680	You know, some of our like automated AI researchers that are doing alignment are
7713680	7715680	saying it's fine, but we don't quite trust them.
7715680	7719280	In this test, you know, the like, the eyes started doing naughty things and,
7719280	7721680	ah, but then we like hammered it out and then it was fine.
7721680	7723760	And like, ah, should we, should we go ahead?
7723760	7725280	Should we take, you know, another six months?
7725360	7727680	Also, by the way, you know, like China just stole the weights.
7727680	7729760	Are we, you know, they're about to like deploy the rumor army.
7729760	7730400	Like what do we do?
7730400	7732960	I think it's this, I think it is this crazy situation.
7734480	7739840	And, um, you know, basically you, you were lying much more on kind of like a sane chain
7739840	7744160	of command than you are on sort of some like, you know, the Libertive Regulatory Scheme.
7744160	7746960	I wish you had, you were able to do the Libertive Regulatory Scheme.
7746960	7748720	And this is the thing about the private companies too.
7748720	7753280	I don't think, you know, they all claim they're going to do safety, but
7753680	7757680	I think it's really rough when you're in the commercial race and they're startups,
7757680	7760640	you know, and startups, startups or startups, you know,
7760640	7762400	I think they're not fit to handle WMDs.
7763600	7765440	Yeah, I'm coming closer to your position.
7766960	7772160	But part of me also, so with the responsible scaling policies,
7772160	7775920	I was told that people who are advancing that, that the way to think about this,
7775920	7778000	because they know I'm like a libertarian type of person.
7778000	7778560	Yeah, yeah, yeah.
7778560	7785440	And the way they approached me about it was that fundamentally this is a way to protect
7786080	7791120	market-based development of AGI in the sense that if you didn't have this at all,
7791120	7794880	then you would have the sort of misuse and then you would have to be nationalized.
7794880	7799120	And the RSPs are a way to make sure that through this deployment,
7799120	7801040	you can still have a market-based order.
7801040	7804480	But then there's these safeguards that make sure that things don't go off the rails.
7805200	7812560	And I wonder if it seems like your story seems self-consistent,
7813200	7818400	but it does feel, I know this was never your position, so I'm not looping you into this,
7818400	7822960	but sort of modern Bailey almost in the sense of...
7823760	7826000	Well, look, here's what I think about RSP-type stuff
7826000	7828160	or sort of safety regulation that's happening now.
7828160	7830720	I think they're important for helping us figure out what world we're in
7830720	7833200	and like flashing the warning signs when we're close, right?
7833200	7838800	And so the story we've been telling is sort of what I think the modal version of this decade is,
7838800	7840640	but it's like, I think there's lots of ways it could be wrong.
7840640	7842480	I really... We should talk about the data a while more.
7842480	7845360	I think there's like, again, I think there's a world where the stuff stagnates, right?
7845360	7846720	There's a world where we don't have AGI.
7847920	7851440	And so basically the RSP thing is preserving the optionality,
7851440	7854160	let's see how this stuff goes, but we need to be prepared.
7854160	7857520	Like if the red lights start flashing, if we're getting the automated eye researcher,
7857520	7860160	then it's like, and it's crunch time, and then it's time to go.
7860160	7864480	I think, okay, I can be on the same page on that, that we should have a very,
7864480	7867520	very strong prior on a proceeding in a market-based way,
7867520	7872640	unless you're right about what the explosion looks like, the intelligence explosion.
7872640	7878400	And so like, I don't move yet, but in that world where like really does seem like
7878400	7884000	Alec Radford can be automated, and that is the only bottleneck to getting TSI.
7884000	7885200	Okay, I think we can leave it at that.
7886160	7890240	I can, yeah, I am somewhat of the way there.
7890240	7892880	Okay, okay. I hope it goes well.
7894000	7896240	It's gonna be, ah, very stressful.
7896240	7897680	And again, right now is the chill time.
7899760	7901040	Enjoy your vacation a lot less.
7901600	7905680	It's funny to look out over, just like, this is San Francisco.
7905680	7906960	Yeah, yeah, yeah.
7906960	7909120	Open the eyes right there, you know, anthropics there.
7909120	7911120	I mean, again, this is kind of like, you know, it's like,
7911120	7914960	you guys have this enormous power over how it's gonna go for the next couple of years,
7914960	7916560	and that power is depreciating.
7917680	7918320	Who's you guys?
7918880	7920080	Like, you know, people at labs.
7920080	7920640	Yeah, yeah, yeah.
7921760	7922960	But it is a sort of crazy world.
7922960	7925120	And you're talking about like, you know, I feel like you talk about like,
7925120	7926560	oh, maybe they'll nationalize too soon.
7926560	7931120	It's like, you know, almost nobody like really like feels it, sees what's happening.
7931120	7934240	And it's, I think this is the thing that I find stressful about all the stuff is like,
7934240	7935440	look, maybe I'm wrong.
7935440	7938560	Like if I'm right, we're in this crazy situation where there's like, you know,
7938560	7940720	like a few hundred guys that are like paying attention.
7942800	7944480	And it's daunting.
7944960	7946880	I went to Washington a few months ago.
7946880	7950800	And I was talking to some people who are doing AI policy stuff there.
7950800	7953200	And I was asking them how likely they think nationalization is.
7954080	7957920	And they said, oh, you know, like, it's really hard to nationalize stuff.
7957920	7959200	It's been a long time since we've done it.
7959200	7963520	There's these very specific procedural constraints on what kinds of things can be nationalized.
7964400	7969760	And then I was asked, well, like ASI, so that means because there's,
7969760	7973280	there's constraints at a defense production act or whatever that won't be nationalized.
7973600	7974800	The Supreme Court would overturn that.
7975760	7978320	And they're like, yeah, I guess that would be nationalized.
7980720	7984080	That's the short summary of my post or my view on the project.
7990320	7994160	Okay. So before we go further on the ASF, let's just back off.
7996000	7996960	We began the conversation.
7996960	7997920	I think people will be confused.
7997920	8000640	You graduated valedictorian of Columbia when you were 19.
8000640	8002480	So you got to college when you were 15.
8003040	8005600	And you were in Germany, then you got to college at 15.
8007040	8008000	How the fuck did that happen?
8009520	8010960	I really wanted out of Germany.
8014160	8016240	I went to kind of a German public school.
8016240	8018160	It was not a good environment for me.
8021040	8021600	In what sense?
8021600	8022720	There's just like no peers.
8023440	8025600	Yeah, look, I mean, it was, yeah, it was, you know,
8026560	8029440	there's, I mean, there's also just a sense in which sort of like,
8029440	8031200	there's this particular sort of German cultural sense.
8031200	8033360	I think in the US, you know, there's all these like amazing high schools
8033360	8035040	and like sort of an appreciation of excellence.
8035040	8038800	And in Germany, there's really this sort of like Paul Poppy syndrome of us, right?
8038800	8041920	Where it's, you know, you're the curious kid in class and you want to learn more
8041920	8043920	instead of the teacher being like, ah, that's great.
8043920	8045440	They're like, they kind of resent you for it.
8045440	8046640	And they're like trying to crush you.
8047920	8050000	I mean, there's also like, there's no kind of like elite universities
8050000	8051520	for undergraduate, which is kind of crazy.
8053520	8055680	So, you know, the sort of, you know, there's sort of like,
8056240	8060080	basically like the meritocracy was kind of crushed in Germany at some point.
8061200	8064240	Also, I mean, there's a sort of incredible sense of complacency,
8065920	8067760	you know, across the board.
8067760	8070400	I mean, one of the things that always puzzles me is like, you know,
8071120	8074080	even just going to a US college was just kind of like radical act.
8074080	8076880	And like, you know, it doesn't seem radical to anyone here because it's like,
8076880	8079840	ah, this is obviously the thing you do and you can go to Columbia, you go to Columbia.
8079840	8081920	But it's, you know, it is very unusual.
8081920	8084720	And it's, it's, it's wild to me because it's like, you know,
8084720	8085920	this is where stuff is happening.
8085920	8087520	You can get so much of a better education.
8087520	8090960	And, you know, like America is where, you know, it's where, where, where,
8090960	8093920	where all the stuff is and people don't do it.
8093920	8099520	And, and so, um, yeah, anyway, so I, you know, I know I skipped a few grades and,
8099520	8103120	and, you know, I think, um, at the time it seemed very normal to me to kind of like
8103120	8105280	go to college and come to America.
8105280	8110080	I think, um, you know, now one of my sisters is now like turning 15, you know.
8110080	8112080	And so then I, you know, and I look at her and I'm like,
8112880	8114080	now I understand how my mother.
8114880	8119040	And as you get to college, you're like presumably the only 15 year old.
8119040	8119840	Yeah, yeah.
8119840	8122000	As it was just like normal for you to be a 15 year old.
8122000	8123520	Like, what was the initial years?
8123520	8124400	It felt so normal at the time.
8124400	8125040	You know, I didn't, yeah.
8125040	8127440	So yeah, it's like, now I understand why my mother's worried.
8127440	8130080	And, you know, I think, you know, I worked, I worked on my parents for a while.
8130080	8132160	You know, eventually I was, you know, I persuaded them.
8132160	8134240	No, but yeah, it felt, felt very normal at the time.
8134240	8134800	And it was great.
8134800	8137200	It was also great because I, you know, I actually really like college, right?
8137840	8139840	And in some sense it sort of came at the right time for me.
8140480	8145360	Where, you know, I, I mean, I, you know, for example, I really appreciate the sort of like
8145360	8148240	liberal arts education and, you know, like the core curriculum and reading sort of
8148240	8150880	core works of political philosophy and, and literature.
8150880	8152400	And you did what you can.
8152400	8156000	And I mean, my majors were math and statistics and economics.
8157200	8160800	But, you know, Columbia has a sort of pretty heavy core curriculum and liberal arts education.
8160800	8163200	And honestly, like, you know, I shouldn't have done all the majors.
8163200	8165920	I should have just, I mean, the best courses were sort of the courses where it's like,
8165920	8168320	there's some amazing professor and it's some history class.
8168400	8173200	And it's, I mean, that's, that's honestly the thing I would recommend people spend their time
8173200	8173920	on in college.
8174800	8176560	Was there one professor or class that stood out that way?
8177280	8183040	I mean, if you, there's like a class by Richard Betz on war, peace and strategy.
8183920	8185600	Adam too is obviously fantastic.
8187040	8188800	And, you know, has written very riveting books.
8189600	8190160	Yeah.
8190160	8191440	You should have them on the podcast, by the way.
8191440	8191920	I've tried.
8191920	8192240	Okay.
8192240	8192800	Try it.
8192800	8194160	I think you try it for me.
8194160	8195760	Yeah, you gotta give it on the pod.
8195760	8196160	Yeah.
8196160	8197040	Oh, it'd be so good.
8198640	8198960	Okay.
8198960	8206320	So then in a couple of years, we were talking to Tyler Cowan recently and he said that when,
8206320	8212000	the way we, he first encountered you was you wrote this paper on economic growth and existential
8212000	8217440	risk and he said, I, when I found, read it, I couldn't believe that a 17 year old had written
8217440	8217760	it.
8217760	8221040	I thought if this was a MIT dissertation, I'd be impressed.
8221040	8226160	So you were like, how did you go from your, I guess we were the junior of them,
8227040	8232000	you're writing, you're writing, you know, pretty novel economic papers.
8233600	8235680	Why did you get interested in this, this kind of thing?
8235680	8237280	And what was the process to get in that?
8238880	8239280	I don't know.
8239280	8241040	I just, you know, I get interested in things in some sense.
8241040	8243280	It's sort of like, it feels very natural to me.
8243280	8244560	It's like, I get excited about a thing.
8244560	8245200	I read about it.
8245200	8245920	I immerse myself.
8245920	8249120	I think I can, you know, I can learn information very quickly and understand it.
8250240	8255200	The, I mean, I think to the paper, I mean, I think one actual, at least for the way I work,
8255200	8259200	I feel like sort of moments of peak productivity matter much more than sort of average productivity.
8259200	8262080	I think there's some jobs, you know, like CUO or something, you know, like average
8262080	8263360	productivity really matters.
8263360	8267520	But I think there's sort of a, I often feel like I have periods of like, you know,
8267520	8270560	there's some, there's a couple months where there's sort of nephrolessence and I'm like,
8270560	8273520	you know, and the other times I'm sort of computing stuff in the background.
8273520	8276400	And at some point, you know, like writing the series, this is also kind of similar.
8276400	8280400	And it's just like you, you write it and, and it's, it's like, it's really flowing.
8280400	8282800	And that's sort of what ends up mattering.
8282880	8286480	I think even for CEOs, it might be the case that the peak productivity is very important.
8286480	8291520	There's one of our following chat-off-house rules, one of our friends in a group chat
8291520	8297040	has pointed out how many famous CEOs and founders have been bipolar manic,
8297920	8301840	which is very much the peak, like the call option on your productivity is the most
8301840	8305520	important thing and you get it by just increasing the volatility through bipolar.
8307120	8308560	Okay. So that's interesting.
8308560	8310800	And so you get interested in economics first.
8310800	8311840	First of all, why economics?
8311840	8313520	Like you could read about anything at this move.
8313520	8316560	Like you, if you wanted, you know, you kind of got a slow start on them.
8319120	8322560	You reached it all these years on the econ, there's an alternative world where you're
8322560	8325600	like on the super alignment team at 17 instead of 21 or whatever it was.
8331760	8333920	I mean, in some sense, I'm still doing economics, right?
8333920	8335760	You know, what is, what is straight lines on a graph?
8335760	8339040	I'm looking at the log-log thoughts and like figuring out what the trends are
8339040	8342240	and like thinking about the feedback loops and equilibrium arms control dynamics.
8342240	8346560	And, you know, it's, I think it is a sort of a way of thinking that I find very useful.
8347680	8352640	And, you know, like what, you know, Dario and Ilya seeing scaling early in some sense,
8352640	8353920	that is a sort of very economic way.
8353920	8356400	And also the sort of physics, kind of like empirical physics, you know,
8356400	8357200	a lot of them are physicists.
8357200	8359840	I think the economists usually can't code well enough and that's their issue.
8359840	8361920	But I think it's that sort of way of thinking.
8362880	8367600	I mean, the other thing is, you know, I thought they were sort of, you know, I thought of a lot
8367600	8370000	of the sort of like core ideas of economics.
8370000	8370960	I thought we're just beautiful.
8372160	8374640	And, you know, in some sense, I feel like I was a little duped, you know,
8374640	8377040	where it's like actually econ academia is kind of decadent now.
8377040	8380080	You know, I think that, you know, for example, the paper I wrote, you know,
8380080	8383120	it's sort of, I think the takeaway, you know, it's a long paper,
8383120	8384480	it's 100 pages of math or whatever.
8384480	8388560	I think the core takeaway I can, you know, kind of give the core intuition for in like,
8388560	8390320	you know, 30 seconds and it makes sense.
8390320	8392480	And it's, and it's like, you don't actually need the math.
8392480	8396320	I think that's the sort of the best pieces of economics are like that where you do the work,
8396400	8400960	but you do the work to kind of uncover insights that weren't obvious to you before.
8400960	8404640	Once, once you've done the work, it's like some sort of like mechanism falls out of it
8404640	8408880	that like makes a lot of crisp intuitive sense that like explains some facts about the world
8408880	8410160	that you can then use in arguments.
8410160	8413120	And I think, you know, I think, you know, like a lot of econ one-on-one like this,
8413120	8413680	and it's great.
8413680	8418240	A lot of econ in the, you know, the 50s and the 60s, you know, was like this.
8418240	8421040	And, you know, Chad Jones papers are often like this.
8421040	8422960	I really like Chad Jones papers for this.
8422960	8429600	You know, I think, you know, why did I ultimately not pursue econ academia was number of reasons.
8429600	8430640	One of them was Tyler Cowan.
8433280	8436240	You know, he kind of took me aside and he was kind of like, look, I think you're one of the
8436240	8439280	like top young economists I've ever met, but also you should probably not go to grad school.
8439280	8439840	Oh, interesting.
8439840	8441040	Yeah, I didn't realize that.
8441040	8441280	Well, yeah.
8441280	8444800	And it was, it was, it was good because he kind of introduced me to the, you know,
8444800	8448880	I don't know, like the Twitter weirdos or just like, you know, and I think the takeaway from that
8448880	8452000	was kind of, you know, got to move out last one more time.
8452080	8453840	Wait, Tyler introduced you to the Twitter weirdos?
8453840	8454320	A little bit.
8454320	8454480	Yeah.
8454480	8455840	Or just kind of like the sort of brought, you know,
8455840	8459920	Like the 60 year old, the old economist in GCT that Twitter.
8459920	8460480	Yeah.
8460480	8463840	Well, you know, I had been, I had, so I went from Germany, you know, completely, you know,
8463840	8468080	on the periphery to kind of like, you know, in the U.S. elite institution and sort of got some
8468080	8472720	vibe of like sort of, you know, meritocratic elite, you know, U.S. society.
8472720	8476240	And then sort of, yeah, basically this sort of like, there was a sort of directory then to
8476240	8478640	being like, look, I, you know, find the true American spirit.
8478640	8479440	I got to come out here.
8479920	8483200	The other reason I didn't become an economist was because, or at least econ academia,
8483200	8485760	was because I think sort of econ academia has become a bit decadent.
8485760	8489040	And maybe it's just ideas getting harder to find and maybe it's sort of things, you know,
8489040	8491360	and the sort of beautiful, simple things have been discovered.
8491360	8493200	But you know, like what are econ papers these days?
8493200	8498480	You know, it's like, you know, it's like 200 pages of like empirical analyses on what happened
8498480	8501840	when, you know, like Wisconsin bought, you know, 100,000 more textbooks on like educational
8501840	8502320	outcomes.
8502320	8504080	And I'm really happy that work happened.
8504080	8506800	I think it's important work, but I think it is not in government and covering these
8506800	8510160	sort of like fundamental insights and sort of mechanisms in society.
8511440	8514720	Or, you know, it's like, even the theory work is kind of like, here's a really complicated
8514720	8518800	model and the model spits out, you know, if the Fed does X, you know, then Y happens,
8518800	8520640	you have no idea what that hat, why that happened?
8520640	8523680	Because it's like gazillion parameters and they're all calibrated in some way.
8523680	8525280	And it's some computer simulation.
8525280	8527360	You have no idea about the validity, you know, yeah.
8527360	8530880	So I think, I think the sort of, you know, the most important insights are the ones where
8530880	8532320	you have to do a lot of work to get them.
8532320	8534240	But then there's this crisp intuition.
8534560	8534720	Yeah.
8534720	8541040	The P versus NP of, that's really interesting.
8541040	8546480	So just going back to your time in college, you say that peak productivity kind of explains
8546480	8551840	the, this paper and things, but the valedictorian, that's getting straight A's or whatever is very
8551840	8555680	much average productivity phenomenon.
8555680	8556320	Right.
8556320	8560400	So there's one award for the highest GPA, which I won, but the valedictorian is like among the
8560400	8563600	people which have the highest GPA and then like selected by faculty.
8563680	8564000	Okay.
8564000	8564480	Yeah.
8564480	8567040	So it's just not, but it's not just peak productivity.
8567040	8569920	It's just, it's just, I generally just love this stuff.
8569920	8573120	You know, I just, I was curious and I thought it was really interesting and I love learning
8573120	8577920	about it and, and I love kind of like, it made sense to me and, you know, it was very natural.
8577920	8581840	And so, you know, I think I'm, you know, I'm not, you know, I think one of my faults is
8581840	8583440	I'm not that good at eating glass or whatever.
8583440	8584880	I think there's some people who are very good at it.
8584880	8589200	I think the sort of like, the sort of moments of peak productivity come when I, you know,
8589200	8593040	I'm just really excited and engaged and, and, and, and love it.
8593040	8598080	And, you know, I, I, you know, if you take like courses, you know, that's what you got in college.
8598080	8601120	And it's, it's, it's, it's the Bruce Banner code and Avengers.
8601760	8603120	You know, I'm always angry.
8604080	8604960	I'm always excited.
8604960	8605920	I'm always curious.
8605920	8607200	That's why I'm always the equitability.
8609040	8612560	So it's interesting, by the way, when you were in college, I was also in college.
8612560	8618480	I think you were, despite being a year younger than me, I think you're ahead in college than
8618480	8620080	me or at least two years, maybe two years ahead.
8620720	8624240	And we met around this time.
8624240	8625360	Yeah, yeah, yeah.
8625360	8628800	We also met, I think through the Tyler Cowan universe.
8628800	8629520	Yeah, yeah, yeah.
8629520	8632480	And it's very insane how small the world is.
8632480	8634000	I think I, did I reach out to you?
8634000	8634800	I must have.
8634800	8634960	Yeah.
8634960	8640400	About when I had a couple of videos and they had a couple hundred views or something.
8640400	8640960	Yeah.
8640960	8642160	It's a small world.
8642160	8644080	I mean, this is the crazy thing about the eye world, right?
8644080	8648560	It's kind of like, it's the same few people at the kind of SF parties and they're the ones,
8649040	8652400	running the models at DeepMind and OpenAI and Anthropic.
8652400	8656800	And I mean, I think some other friends of ours have mentioned this,
8656800	8660480	who are now later in their career and very successful, that they actually met all the
8660480	8663040	people who are also kind of very successful in Silicon Valley now.
8663040	8666800	Like when they're in their 20s or when they're really 20.
8669120	8671680	I mean, look, I actually think, why is it a small world?
8672320	8676560	I mean, I think one of the things is some amount of some sort of agency.
8677520	8682640	And I think in a funny way, this is a thing I sort of took away from the sort of Germany
8682640	8686560	experience where it was, I mean, look, I, I, it was crushing.
8686560	8687760	I really didn't like it.
8687760	8692080	And it was like, it was such an unusual move to kind of skip grades and such an unusual move to
8692080	8692880	come to the United States.
8692880	8695840	And, you know, a lot of these things I did were kind of unusual moves.
8695840	8702640	And, you know, there's some amount where like, just like, just trying to do it.
8702640	8704000	And then it was fine.
8704000	8704560	And it worked.
8705680	8708800	That kind of reinforced, like, you know, you don't, you don't just have to kind of conform
8708800	8710080	to what the opportune window is.
8710080	8712640	You can just kind of like try to do the thing, the thing that seems right to you.
8713680	8716800	And like, you know, most people can be wrong and I know things like that.
8716800	8720240	And I think that was kind of a, you know, valuable kind of like early experience,
8720240	8721200	those sort of formative.
8721840	8722160	Okay.
8722160	8723760	So after college, what did you do?
8724400	8727360	I did econ research for a little bit, you know, Oxford and stuff.
8727360	8729520	And then, then I worked at Future Fund.
8730320	8730880	Yeah.
8730880	8731200	Okay.
8731200	8732880	So, and so tell me about it.
8735440	8739440	Future Fund was, you know, it was a foundation that was, you know, funded by
8739440	8740320	San Bank and Freed.
8740320	8742800	I mean, we were our own thing, you know, we were based in the Bay.
8744080	8746560	You know, at the time, this was in sort of early 22.
8748160	8750960	It was, it was this just like incredibly exciting opportunity, right?
8750960	8754480	It was basically like a startup, you know, foundation, which is like, you know, it doesn't
8754480	8757680	come along that, that often that, you know, we thought we'd be able to give away billions
8757680	8761120	of dollars, you know, thought we'd be able to kind of like, you know, remake how philanthropy
8761120	8765040	is done, you know, from first principles, thought we'd be able to have, you know,
8765040	8768880	this like great impact, you know, we, the causes we focused on were, you know,
8768880	8775040	biosecurity, you know, AI, you know, finding exceptional talent and putting them to work
8775040	8775840	on hard problems.
8777360	8780000	And, you know, like a lot of the stuff we did, I was, I was really excited about,
8780000	8783120	you know, like academics who would, you know, usually take six months would send us emails
8783120	8784080	like, ah, you know, this is great.
8784080	8787360	This is so quick and, you know, and straightforward, you know, in general,
8787360	8790560	I feel like I've often find that with like, you know, a little bit of encouragement, a little
8790560	8794080	bit of sort of empowerment, kind of like removing excuses, making the process easy,
8794080	8796560	you know, you can kind of like get people to do great things.
8797760	8802800	I think on the future front, the thing is context for people who might not realize,
8803680	8808320	not only were you guys planning on deploying billions of dollars, but it was a team of four
8808320	8808800	people.
8808800	8809600	Yeah, yeah, yeah.
8809600	8815600	So you at 18 are on a team of four people that is in charge of deploying billions of dollars.
8815600	8816080	Yeah.
8816080	8818240	I mean, just, I mean, yeah, I'm a future fund, you know, the,
8819360	8821840	yeah, I mean, the, you know, so that was, that was sort of the heyday, right?
8822480	8825680	And then obviously, you know, when, when in sort of, you know, November of 22,
8827520	8830880	you know, it was kind of revealed that Sam was this, you know, giant fraud.
8830880	8833280	And from one day to the next, you know, the whole thing collapsed.
8835040	8836480	That was just really tough.
8836480	8838400	I mean, you know, obviously it was devastating.
8838400	8841280	It was devastating, obviously for the people at their money on FTX,
8841840	8846400	you know, closer to home, you know, all the, you know, all these grantees, you know,
8846400	8848960	we'd wanted to help them and we thought they were doing amazing projects.
8848960	8852400	And so, but instead of helping them, we ended up saddling them with like a giant problem.
8853920	8855920	You know, personally, it was, you know, it was a startup, right?
8855920	8858480	And so I, you know, I'd worked 70 hour weeks every week for, you know,
8858480	8861200	basically a year on this to kind of build this up, you know, we're a tiny team.
8862480	8865680	And then from one day to the next, it was all gone and not just gone.
8865680	8867360	It was associated with this giant fraud.
8869200	8870960	And so, you know, that was incredibly tough.
8871920	8872640	Yeah.
8872640	8874960	And then were there any signs early on that
8876000	8876800	SPF was?
8878240	8878480	Yeah.
8878480	8880640	And like, obviously I didn't know he was a fraud and the whole, you know,
8881600	8883440	I would have never worked there, you know.
8884400	8886480	And, you know, we weren't, you know, we were a separate thing.
8886480	8887920	We weren't working with the business.
8889280	8891360	I mean, I think, I do think there were some takeaways for me.
8891360	8896480	I think one takeaway was, you know, I think there's a, I had this tendency,
8896480	8898720	I think people in general have this tendency to kind of like, you know,
8898720	8901120	give successful CEOs a pass on their behavior.
8901120	8903440	Because, you know, there's successful CEOs and that's how they are.
8903440	8905440	And that's just the successful CEO things.
8905440	8910720	And, you know, I didn't know Sandbank Manfredo was a fraud, but I knew SPF.
8910720	8913200	And I knew he was extremely risk-taking, right?
8913200	8915520	I knew he, he was narcissistic.
8917840	8920400	He didn't tolerate this agreement well, you know, sort of by the end,
8920400	8922640	he and I just like didn't get along well.
8922640	8925520	And sort of, I think the reason for that was like, there's some biosecurity grants
8925520	8928320	he really liked because they're kind of cool and flashy.
8928320	8930880	And at some point I'd kind of run the numbers and it didn't really seem
8930880	8931760	that cost effective.
8931760	8934480	And I pointed that out and he was pretty unhappy about that.
8935520	8936640	And so I knew his character.
8938640	8941520	And I think, you know, I feel like one takeaway for me was,
8943440	8947280	was, you know, like, I think it's really worth paying attention to people's character,
8947280	8949680	including like people you work for and successful CEOs.
8950880	8953760	And, you know, that can save you a lot of pain down the line.
8954720	8958160	Okay, so after that, I picked some clothes and you're out.
8958480	8968000	And then you got into, you went to OpenAI, the Super Alignment team had just started.
8968000	8970080	I think you were like part of the initial team.
8970640	8973680	And so what was the original idea?
8973680	8975440	What was compelling about that for you to join?
8976080	8976880	Yeah, totally.
8977920	8979920	So, I mean, what was the goal of the Super Alignment team?
8980960	8986560	You know, the Alignment team at OpenAI, you know, at other labs, sort of like several years ago,
8986560	8989040	kind of had done sort of basic research and they developed RLHF,
8989040	8991040	Reinforcement Learning from Human Feedback.
8991040	8994880	And that was sort of a, you know, ended up being really successful technique
8994880	8996960	for controlling sort of current generation of AI models.
8999280	9002080	What we were trying to do was basically kind of be the basic research
9002080	9004560	bet to figure out what is the successor to RLHF.
9004560	9006640	And the reason that we needed that is, you know, basically, you know,
9006640	9009280	RLHF probably won't scale to superhuman systems.
9009280	9012560	RLHF relies on sort of human raiders who kind of thumbs up, thumbs down, you know,
9012560	9015120	like the model said something, it looks fine, it looks good to me.
9015200	9017440	At some point, you know, the superhuman models, the superintelligence,
9017440	9020320	it's going to write, you know, a million lines of, you know, crazy complex code,
9020320	9022080	you don't know at all what's going on anymore.
9022080	9024560	And so how do you kind of steer and control these systems?
9024560	9025760	How do you hide side constraints?
9026560	9031360	You know, the reason I joined was I thought this was an important problem
9031360	9033680	and I thought it was just a really solvable problem, right?
9033680	9036880	I thought this was basically, you know, there's, I think there's a, I still do.
9036880	9041200	I mean, even more so do I think there's a lot of just really promising sort of ML research
9041200	9044240	on alignment on sort of aligning superhuman systems.
9045440	9050320	And maybe we should talk about that a bit more later, but so, and then it was so solvable,
9050320	9051120	you solved it in a year.
9055120	9058800	Anyway, so look, opening, I wanted to do this like really ambitious effort on alignment and,
9058800	9061840	you know, Elliot was backing it and, you know, I liked a lot of the people there.
9061840	9064640	And so I was, you know, I was really excited and I was kind of like, you know,
9064640	9067760	I think there was a lot of people sort of on alignment.
9067760	9069680	There's always a lot of people kind of making hay about it.
9069680	9073680	And, you know, I appreciate people highlighting the importance of the problem.
9073680	9075840	And I was just really into like, let's just try to solve it.
9075840	9077920	And let's do the ambitious effort, you know, let's do the, you know,
9077920	9080160	operation warp speed for solving alignment.
9080160	9082880	And it seemed like an amazing opportunity to do so.
9083760	9086880	Okay. And now basically the team doesn't exist.
9086880	9088720	I think the head of it has left.
9088720	9091520	Both heads of it have left, Jan and Ilya.
9091520	9092800	That's the news of last week.
9094080	9095600	What happened? Why did the thing break down?
9097040	9100240	I think OpenAI sort of decided to take things in a somewhat different direction.
9101200	9102320	Meaning what?
9103520	9106320	I mean, that super alignment isn't the best way to frame the...
9107440	9109920	No, I mean, look, obviously, sort of after the November board events,
9109920	9111120	you know, there were personnel changes.
9111120	9114320	I think Ilya leaving was just incredibly tragic for OpenAI.
9114320	9118000	And, you know, I think some amount of repartilization,
9118000	9120000	I think some amount of, you know, I mean,
9120000	9122240	there's been some reporting on the Superalignment Compute Commitment.
9122240	9124160	You know, there's this 20% compute commitment as part of,
9124160	9125520	you know, how a lot of people were recruited.
9125520	9127920	You know, it's like, we're going to do this ambitious effort on alignment.
9127920	9131680	And, you know, some amount of, you know,
9131680	9134240	not keeping that and deciding to go in a different direction.
9135440	9138160	Okay, so now Jan has left, Ilya has left.
9139280	9140720	So this team itself has dissolved,
9140720	9144960	but you were the sort of first person who left or was forced to leave.
9144960	9148960	You were the information reported that you were fired for leaking.
9148960	9150320	But what happened? Was this accurate?
9151360	9155520	Yeah. Look, why don't I tell you what they claim I leaked
9155520	9156640	and you can tell me what you think.
9157280	9160480	Yeah. So OpenAI did claim to employees that I was fired for leaking.
9161120	9164240	And, you know, I and others have sort of pushed them to say what the leak is.
9164240	9165760	And so here's their response in full.
9167280	9172960	You know, sometime last year, I had written a sort of brainstorming document on preparedness
9172960	9176320	on safety and security measures we need in the future on the Path to AGI.
9177120	9179920	And I shared that with three external researchers for feedback.
9179920	9181280	So that's it. That's the leak.
9182640	9183920	You know, I think for context,
9183920	9187360	it was totally normal at OpenAI at the time to share sort of safety ideas
9187360	9188880	with external researchers for feedback.
9189760	9190880	You know, it happened all the time.
9191680	9194880	You know, the doc was sort of my idea is, you know, before I shared it,
9195520	9197520	I reviewed it for anything sensitive.
9199440	9201680	The internal version had a reference to a future cluster,
9201680	9203600	but I redacted that for the external copy.
9204480	9208160	You know, there's a link in there to some slides of mine, internal slides.
9208880	9211520	You know, that was a dead link to the external people I shared it with.
9211520	9212880	You know, the slides weren't shared with them.
9213520	9217440	And so, obviously, I pressed them to sort of tell me,
9217440	9219600	what is the confidential information in this document?
9220160	9226800	And what they came back with was a line in the doc about planning for AGI by 2728,
9226800	9228480	and not setting timelines for preparedness.
9230720	9234320	You know, I wrote this doc, you know, a couple months after the super alignment
9234320	9237120	announcement, we had put out, you know, this sort of four-year planning horizon.
9237120	9239280	I didn't think that planning horizon was sensitive.
9239280	9242240	You know, it's the sort of thing Sam says publicly all the time.
9243680	9246400	Hey, I think sort of John said it on the podcast a couple weeks ago.
9247200	9248800	Anyway, so that's it.
9248800	9249280	That's it?
9249280	9254800	So that seems pretty thin for, if the cause was leaking, that seems pretty thin.
9254800	9255840	Was there anything else to it?
9256960	9259600	Yeah, I mean, so that was the leaking claim.
9259600	9262800	I mean, you can say a bit more about sort of what happened in the final.
9262800	9263040	Yeah.
9264240	9268560	So one thing was last year, I had written a memo,
9268560	9270880	internal memo, about opening iSecurity.
9270880	9272480	I thought it was, you know, egregiously insufficient.
9272480	9274560	You know, I thought it wasn't sufficient to protect, you know,
9274560	9277840	the theft of model weights or key algorithmic secrets from foreign actors.
9278640	9280080	So I wrote this memo.
9280080	9282880	I shared it with a few colleagues, a couple members of leadership,
9283520	9284960	who sort of mostly said it was helpful.
9286640	9290560	But then, you know, a couple weeks later, a sort of major security incident occurred.
9291600	9294560	And that prompted me to share the memo with a couple members of the board.
9294560	9299120	And so after I did that, you know, days later, it was made very clear to me that leadership
9299120	9301680	was very unhappy with me having shared this memo with the board.
9302800	9306480	You know, apparently the board had hassled leadership about security.
9307440	9310240	And then I got sort of an official HR warning for this memo,
9310880	9312240	you know, for sharing it with the board.
9313440	9316720	The HR person told me it was racist to worry about CCPS view.
9318480	9320240	And they said it was sort of unconstructive.
9321920	9324640	And, you know, look, I think I probably wasn't that my most diplomatic,
9324640	9326640	you know, I definitely could have been more politically savvy.
9327520	9329920	But, you know, I thought it was a really, really important issue.
9329920	9332960	And, you know, the security incident had been really worried.
9334160	9337200	Anyway, and so I guess the reason I bring this up is when I was fired,
9337200	9340880	it was sort of made very explicit that the security memo is a major reason for my being fired.
9341600	9343120	You know, I think it was something like, you know,
9343120	9346240	the reason that this is a firing and not a warning is because of the security memo.
9347760	9349280	But you were sharing it with the board?
9350160	9352080	The warning I'd gotten for the security memo.
9352080	9357280	Hmm. Anyway, and I mean, some other, you know, what might also be helpful context
9357280	9359360	is the sort of questions they asked me when they fired me.
9359360	9363920	So, you know, this was a bit over a month ago, I was pulled aside for a chat with a lawyer,
9363920	9365840	you know, that quickly turned very adversarial.
9366560	9375120	And, you know, the questions were all about my views on AI progress on AGI on the level
9375120	9381520	security appropriate for AGI on, you know, whether government should be involved in AGI on
9383040	9389520	whether I and super alignment were loyal to the company on, you know, what I was up to
9389520	9391600	during the opening of board events, you know, things like that.
9392160	9395200	And, you know, then they, you know, chatted to a couple of my colleagues,
9395200	9397040	and then they came back and told me I was fired.
9397760	9400320	And, you know, they'd gone through all of my digital artifacts from the time at my,
9400320	9402320	you know, time at opening messages docs.
9402320	9403920	And that's when they found, you know, the leak.
9405680	9408640	Yeah. And so anyway, so the main claim they made was a leaking allegation.
9408640	9410080	You know, that's what they told employees.
9411040	9412800	They, you know, the security memo.
9414160	9416160	There's a couple of other allegations they threw in.
9416720	9420160	One thing they said was that I was unforthcoming during the investigation,
9420160	9422560	because I didn't initially remember who I'd shared the doc with,
9422560	9426800	the sort of preparedness brainstorming doc, only that I had sort of spoken to some external
9426800	9428080	researchers about these ideas.
9428720	9431120	And, you know, look, the doc was over six months old.
9431120	9432320	You know, I spent the day on it.
9432960	9435280	You know, as a Google doc, I shared with my opening email.
9435280	9437760	It wasn't a, you know, screenshot or anything I was trying to hide.
9438560	9440640	It simply didn't stick because it was such a non-issue.
9442240	9446240	And then they also claim that I was engaging on policy in a way that they didn't like.
9447120	9451440	And so what they cited there was that I had spoken to a couple of external researchers,
9451440	9455920	you know, somebody got a think tank about my view that AGI would become a government project,
9455920	9456800	you know, as we discussed.
9457440	9460560	You know, in fact, I was speaking to lots of sort of people in the field about that at the time.
9460560	9462320	I thought it was a really important thing to think about.
9463520	9467280	Anyway, and so they found, you know, they found a DM that I'd written to like a friendly colleague,
9467280	9471440	you know, five or six months ago, where I relayed this and, you know, they cited that.
9472640	9476000	And, you know, I had thought it was well within open-eyed norms to kind of talk about
9476000	9479200	high-level issues on the future of AGI with the external people in the field.
9480080	9481760	So anyway, so that's what they alleged.
9481760	9482480	That's what happened.
9484240	9487840	You know, I've spoken to kind of a few dozen former colleagues about this, you know,
9487840	9491760	since I think the sort of universal reaction is kind of like, you know, that's insane.
9493520	9495760	I was sort of surprised as well.
9495760	9499200	You know, I had been promoted just a few months before.
9500640	9504480	I think, you know, I think Ilya's comment for the promotion case at the time was something like,
9504480	9505680	you know, Leopold's amazing.
9505680	9506560	We're lucky to have him.
9509600	9513040	But look, I mean, I think the thing I understand, and I think in some sense is reasonable,
9513040	9514720	is like, you know, I think I ruffled some feathers.
9514720	9517040	And, you know, I think I was probably kind of annoying at times.
9517040	9521360	You know, it's like, I security stuff, and I kind of like repeatedly raised that,
9521360	9523200	and maybe not always in the most diplomatic way.
9523360	9528880	You know, I didn't sign the employee letter during the board events, you know, despite
9528880	9530400	pressure to do so.
9531760	9534000	And you were one of like eight people or something.
9534000	9535360	Not that many people.
9535360	9539040	I guess the, I think the sort of two senior most people didn't sign were Andrey,
9539040	9547520	and yeah, I mean, on the letter, by the way, I, by the time on sort of Monday morning,
9547520	9550080	when that letter was going around, I think probably it was appropriate for the board
9550160	9550560	to resign.
9550560	9553680	I think they'd kind of like lost too much credibility and trust with the employees.
9555280	9557280	But I thought the letter had a bunch of issues.
9557280	9560240	I mean, I think one of them was it just didn't call for an independent board.
9560240	9562960	I think it's sort of like basics of corporate governance to have an independent board.
9563600	9567680	Anyway, you know, it's other things, you know, I am in sort of other discussions,
9567680	9571680	I press leadership for sort of opening eye to abide by its public commitments.
9572960	9577520	You know, I raised a bunch of tough questions about whether it was consistent with the
9577520	9581280	opening I mission and consistent with the national interests to sort of partner with
9581280	9584160	authoritarian dictatorships to build the core infrastructure for AGI.
9585920	9588560	So, you know, look, you know, it's a free country, right?
9589360	9590560	That's what I love about this country.
9590560	9591520	You know, we talked about it.
9592400	9595040	And so they have no obligation to keep me on staff.
9596960	9600800	And, you know, I think in some sense, I think it would have been perfectly reasonable
9600800	9604800	for them to come to me and say, look, you know, we're taking the company in a different direction.
9604800	9606320	You know, we disagree with your point of view.
9607280	9610800	You know, we don't trust you enough to sort of tow the company line anymore.
9610800	9615440	And, you know, thank you so much for your work at Open AI, but I think it's time to part ways.
9615440	9616400	I think that would have made sense.
9616400	9620720	I think, you know, we did start sort of materially diverging on sort of views on important issues.
9620720	9624240	I'd come in very excited and aligned with Open AI, but that sort of changed over time.
9624880	9629840	And look, I think there would have been a very amicable way to part ways.
9629840	9633360	And I think it's a bit of a shame that it sort of this is the way it went down.
9634320	9639200	You know, all that being said, I think, you know, I really want to emphasize
9640160	9642400	there's just a lot of really incredible people at Open AI.
9642400	9645360	And it was an incredible privilege to work with them.
9645360	9648640	And, you know, overall, I'm just extremely grateful for my time there.
9649600	9653120	When you left, now that there's now there's been reporting about
9654560	9661120	an NDA that former employees have to sign in order to have access to their vested equity.
9661760	9663680	Did you sign some such NDA?
9664480	9667280	No. My situation is a little different.
9667280	9669200	And that is sort of basically right before my cliff.
9670080	9671920	But then, you know, they still offered me the equity.
9673600	9676640	But I didn't want to sign a nondisparagement, you know, freedom is priceless.
9676640	9678320	And how much was how much was the equity?
9678320	9680400	It's like close to a million dollars.
9681120	9685280	So it was definitely a thing you were you and others aware of that this is like
9685920	9689040	a choice that Open AI is explicitly offering you.
9689040	9689680	Yeah.
9689680	9694560	And presumably the person on Open AI staff knew that we're offering them equity,
9694560	9697760	but they had to sign this NDA that has these conditions that you can't,
9698320	9703680	for example, give the kind of statements about your thoughts on AGI and Open AI that
9703680	9705360	you're giving on this podcast right now.
9705360	9706800	Like, I don't know what the whole situation is.
9706800	9711200	I certainly think sort of vested equity is pretty rough if you're conditioning that on NDA.
9711200	9713920	It might be a somewhat different situation if it's a sort of separate agreement.
9713920	9714400	Right.
9714400	9719360	But an Open AI employee who had signed it presumably could not give the podcast that you're giving today.
9720640	9721920	Quite plausibly not.
9721920	9722560	Yeah.
9722560	9722960	Yeah.
9722960	9723440	I don't know.
9724800	9725200	Okay.
9725200	9732400	So analyzing the situation here, I guess if you were to, yeah, the board thing is really tough
9732400	9736480	because if you were trying to defend them, you would say, well, listen,
9736480	9739040	you were just kind of going outside the regular chain of command.
9739040	9744080	And maybe there's a point there, although the way in which the person from HR thinks
9744080	9747760	that you have an adversarial relationship with or you're supposed to have an adversarial
9747760	9754560	relationship with the board where to give the board some information, which is relevant to
9756320	9760080	whether Open AI is fulfilling its mission and whether it can do that in a better way
9760080	9765760	is part of the leak as if the board is that is supposed to ensure that Open AI is following
9765760	9767680	its mission as some sort of external actor.
9768240	9769200	That seems pretty...
9769200	9773040	I mean, I think, I mean, to be clear, the leak allegation was just that sort of document
9773040	9773840	I changed the feedback.
9773840	9775840	This is just sort of a separate thing that they cited and they said,
9775840	9777920	I wouldn't have been fired if not for the security memo.
9777920	9779760	They said you wouldn't have been fired.
9779760	9783280	They said the reason this is a firing and not a warning is because of the warning you had
9783280	9784480	gotten for the security memo.
9784480	9791280	Oh, before you left, the incidents with the board happened, were Sam was fired and then
9791280	9793280	Rihard, a CEO, and now he's on the board.
9794240	9800320	Now, Ilya and Yan, who are the heads of the super lineman team and Ilya, who is a co-founder of
9800320	9805680	Open AI, obviously the most significant in terms of stash or a member of Open AI for
9805680	9807920	a research perspective, they've left.
9807920	9811680	It seems like, especially with regards to super lineman stuff and just generally the
9811680	9816160	Open AI, a lot of the sort of personnel drama has happened over the last few months.
9816800	9817680	What's going on?
9817680	9818960	Yeah, there's a lot of drama.
9821760	9823280	Yeah, so why is there so much drama?
9825360	9829120	You know, I think there would be a lot less drama all Open AI claim to be with sort of
9829120	9831440	building chat, GPT or building business software.
9831760	9835360	I think where a lot of the drama comes from is, you know, Open AI really believes they're
9835360	9836720	building AGI, right?
9836720	9842640	And it's not just, you know, a claim they make for marketing purposes, you know, whatever,
9842640	9846080	you know, there's this report that Sam is raising, you know, $7 trillion for chips and
9846080	9849200	it's like, that stuff only makes sense if you really believe in AGI.
9850400	9853680	And so I think what gets people sometimes is sort of the cognitive dissonance between
9853680	9857440	sort of really believing in AGI, but then sort of not taking some of the other implications
9857440	9860320	seriously, you know, it's, this is going to be incredible.
9860480	9863520	This is going to be incredibly powerful technology, both for good and for bad.
9863520	9867600	And that implicates really important issues like the national security issues we spoke
9867600	9870480	about, like, you know, are you protecting the secrets from the CCP?
9870480	9874640	Like, you know, does America control the core AGI infrastructure or does it, you know, a
9874640	9877120	Middle Eastern dictator control the core AGI infrastructure?
9879360	9884640	And then I mean, I think the thing that, you know, really gets people is the sort of tendency
9884640	9889440	to kind of then make commitments and sort of like, you know, they say they take these
9889440	9893120	issues really seriously, they make big commitments on them, but then sort of frequently don't
9893120	9896800	follow through, right? So, you know, again, as mentioned, there was this commitment around
9896800	9900720	super alignment compute, you know, sort of 20% of compute for this long term safety research
9900720	9904640	effort. And I think, you know, you and I could have a totally reasonable debate about what is
9904640	9909600	the appropriate level of compute for super alignment. But that's not really the issue.
9909600	9913280	The issue is that this commitment was made and it was used to recruit people and, you know,
9913280	9918560	it was, it was very public. And it was made because, you know, there's a recognition that
9918560	9921520	there would always be something more urgent than a long term safety research effort, you know,
9921520	9926160	like some new product or whatever. And then in fact, they just, you know, really didn't keep the
9926160	9929920	commitment. And so, you know, there was always something more urgent than long term safety
9929920	9934320	research. I mean, I think, I think another example of this is, you know, when I raised these issues
9934320	9939680	about security, you know, they would, they would tell me, you know, securities are number one priority.
9941120	9946720	But then, you know, invariably, when, when it came time to sort of invest serious resources,
9946720	9950800	when it came time to make trade offs, to sort of take some pretty basic measures,
9951840	9955840	security would not be prioritized. And so, yeah, I think it's the cognitive dissonance,
9955840	9960000	and I think it's the sort of unreliability that causes a bunch of the drama.
9960960	9968320	So let's zoom out, talk about the part, a big part of the story, and also a big motivation
9968320	9972480	of the way in which it must proceed with regards to geopolitics and everything,
9972480	9978160	is that once you have the AGI, pretty soon after you proceed to ASI, because superintelligence,
9978160	9985520	because you have these AGI's, which can function as researchers into further AI progress. And with
9985520	9992560	a matter of years, maybe less, you go to something that is like superintelligence. And at the high,
9992560	9996160	and then from there, then you can do up in according to your story, do all this research
9996160	10000800	and development and robotics and pocket nukes and whatever other crazy shit.
10001600	10015360	But at a high level, it's not clear to me this input-output model of research is how things
10015360	10021120	actually happen in research. We can look at economy-wide, right? Patrick Hollis and others
10021120	10025920	have made this point that from compared to 100 years ago, we have 100x more researchers in the
10025920	10031520	world. It's not like progress is happening 100x faster. So it's clearly not the case that you
10031520	10036880	can just pump in more population into research and you get higher research on the other end.
10038000	10040400	I don't know why it would be different for the AI researchers themselves.
10040400	10043920	Okay, great. So this is getting into some good stuff. I have a classic disagreement
10043920	10049840	I have with Patrick and others. So obviously, inputs matter. So it's like United States
10049840	10055840	produces a lot more scientific and technological progress than Liechtenstein or Switzerland.
10056160	10060160	And even if I made Patrick Hollis and dictator of Liechtenstein or Switzerland,
10060160	10064320	and Patrick Hollis was able to implement his Utopia of Ideal Institutions,
10064320	10068160	keeping the talent pool fixed. He's not able to do some crazy high school immigration thing or
10069200	10072320	whatever, some crazy genetic breeding scheme or whatever he wants to do.
10074160	10078080	Keeping the talent pool fixed, but amazing institutions. I claim that still,
10078080	10081280	even if you made Patrick Hollis and dictator of Switzerland, maybe you get some factor,
10081280	10084560	but Switzerland is not going to be able to outcompete the United States in scientific
10084560	10089440	and technological parts. Obviously, magnitudes matter. Okay. No, I actually, I'm not sure I
10089440	10094160	agree with this. There's been many examples in history where you have small groups of people
10094160	10098400	who are part of like Bell Labs or Skunkworks or something. There's a couple hundred researchers
10098400	10102800	open AI, right? Couple hundred researchers, they do highly selected though, right? You know,
10102800	10106800	it's like, it's like saying, you know, that's part of Patrick Hollis and his dictator is going
10106800	10110720	to do a good job of this. Well, yes, if you can highly select all the best AI researchers in
10110720	10113920	the world, you might only need a few hundred, but if you, you know, that's, that's the talent
10113920	10117280	pool. It's like you have the, you know, 300 best AI researchers in the world.
10117280	10121120	But, but there's, there has been, it's not a case that from a hundred years to now, there haven't
10121120	10125360	been, the population has increased massively. A lot of the, in fact, you would expect the density
10125360	10129600	of talent to have increased in the sense that malnutrition and other kinds of debilitation,
10129600	10134320	poverty, whatever, that have debilitated past talent at the same sort of level is no longer
10134320	10138000	debilitated in the same way. To the 100X point, right? So I don't know if it's 100X. I think it's
10138000	10142240	easy to inflate these things, probably at least 10X. And so people are sometimes like, ah, you
10142240	10145840	know, like, you know, come on, ideas haven't gotten them much harder to find, you know, why would
10145840	10150160	you have needed this 10X increase in research effort? Whereas to me, I think this is an extremely
10150160	10154000	natural story. And why is it a natural story? It's a straight line on a log log plot. This is sort
10154000	10158320	of a, you know, deep learning researchers dream, right? What is this log log plot on the X axis?
10158320	10164080	You have log cumulative research effort on the Y axis. You have some log GDP or ooms of algorithmic
10164160	10168800	progress, or, you know, log transistors per square inch, or, you know, in the sort of
10168800	10173120	experience curve for solar, kind of like, you know, whatever the log of, you know, the price for a
10173120	10177600	gigawatt of solar. And it's extremely natural for that to be a straight line. You know, this is
10177600	10181920	sort of a class, yeah, it's a classic. And, you know, it's basically the first thing is very easy,
10181920	10185120	then basically, you know, you have to have log increments of cumulative research effort to
10185120	10190480	find the next thing. And so, you know, in some sense, I think this is a natural story. Now,
10190480	10194880	one objection kind of people then make is like, oh, you know, isn't it suspicious, right? That
10194880	10199920	like ideas, you know, well, we increased research effort 10x, and ideas also just got 10x harder
10199920	10205520	defined. And so it perfectly, you know, equilibrates. And to there, I say, you know, it's just, it's an
10205520	10210080	equilibrium. It's an dodges equilibrium, right? So it's like, you know, isn't it a coincidence that
10210080	10214640	supply equals demand, you know, on the market clears, right? And that's, and the same thing here,
10214640	10218640	right? So it's, you know, ideas getting, how much ideas have gotten harder to find as a function
10218640	10223440	of how much progress you've made. And then, you know, what the overall growth rate has been is a
10223440	10227600	function of how much ideas have gotten harder to find in ratio to how much you've been able to,
10227600	10230640	like, increase research effort, what is the sort of growth, the log cumulative research effort.
10230640	10234400	So in some sense, I think the story is sort of like, fairly natural. And you see this, you see
10234400	10237760	this not just economy wide, you see it in kind of experience curve for all sorts of individual
10237760	10243040	technologies. So I think there's some process like this. And it's totally plausible that, you know,
10243040	10246560	institutions have gotten worse by some factor. Obviously, there's some sort of exponent of
10246560	10250800	diminishing returns on more people, right? So like serial time is better than just paralyzing.
10251840	10254400	But still, I think it's like, clearly inputs matter.
10255280	10262640	Yeah, I agree. But if the coefficient of how fast they diminish as you grow, the input
10262640	10267600	is high enough, then the, and the abstract, the fact that inputs matter isn't that relevant.
10267600	10271360	Okay, so I mean, we're talking at a very high level, but just like take it down to the actual
10271360	10278720	concrete thing here. Open AI has a staff of at most low hundreds who are directly involved
10278720	10283760	in the algorithmic progress in future models. If it was really the case that you could just
10283760	10288480	arbitrarily scale this number, and you can have much faster algorithmic progress, and that would
10288480	10294400	result in much higher, much better AI store open AI basically, then it's not clear why open AI
10294400	10298320	doesn't just go out and hire every single person with 150 IQ, of which there are hundreds of
10298320	10305520	thousands in the world. And my, my story there is there's transaction costs to managing all these
10305520	10311440	people that don't just go away if you have a bunch of AI's that there, these tasks aren't easy to
10311440	10317520	parallelize. And I think you, I'm not sure how you would explain the fact of like, why does an
10317520	10321280	open AI go on a recruiting binge of every single genius in the world?
10321280	10324160	All right, great. So let's talk about the open AI example, and let's talk about the automated
10324160	10327600	AI researchers. So I mean, in the open AI case, I mean, just, you know, just kind of like look
10327600	10331200	at the inflation of like AI researcher salaries over the last year. I mean, I think like, I don't
10331200	10335040	know, I don't know what it is, you know, 4x, 5x is kind of crazy. So they're clearly really trying
10335040	10338880	to recruit the best AI researchers in the world. And, you know, I don't know, it's,
10338880	10343120	they do find the best AI researchers in the world. I think my response to your thing is like, you know,
10343120	10346400	almost all of these 150 IQ people, you know, if you just hire them tomorrow, they wouldn't be
10346400	10350960	good AI researchers, they wouldn't be an Alec Radford. But they're willing to make investments
10350960	10355440	that take years to pan out of the four. The data centers they're buying right now will come
10355440	10360720	online in 2026 or something. Why wouldn't they be able to make every 150 IQ person? Some of them
10360720	10364320	won't work out. Some of them won't have the traits we like. But some of them by 2026 will be amazing
10364320	10368480	AI researchers. Why aren't they making that bet? Yeah. And so sometimes this happens, right? Like,
10368480	10371280	smart physicists have been really good at AI research, you know, it's like all the anthropocopies
10371280	10376240	co-found. But like, if you talk to, I've had Daria in the podcast, they have this very careful
10376240	10380240	policy of like, we're not going to just hire arbitrarily, we're going to be extremely selective.
10381520	10385680	Training is not as easily scalable, right? So training is very hard. You know, if you just
10385680	10389920	hired, you know, 100,000 people, it's like, you, I mean, you couldn't train them all. If you're
10389920	10393120	really hard to train them all, you know, you wouldn't be doing any AI research. Like, you know,
10393120	10396400	there's, there's huge costs to bringing on a new person training them. This is very different
10396400	10399280	with the AIs, right? And I think this is, it's really important to talk about the sort of like
10399280	10403200	advantages the AIs will have. So it's like, you know, training, right? It's like, what does it take
10403200	10406800	to be an Alec Radford? You know, we need to be in a really good engineer, right? The AIs,
10406800	10409520	they're going to be an amazing engineer. They're going to be amazing at coding. You can just train
10409520	10413840	them to do that. They need to have, you know, not just be a good engineer, but have really good
10413840	10417520	research intuitions and like really understand deep learning. And this is stuff that, you know,
10417520	10421440	Alec Radford, or, you know, somebody like him has acquired over years of research over just like
10421440	10426880	being deeply immersed in deep learning, having tried lots of things himself and failed. The AIs,
10426880	10429920	you know, they're going to be able to read every research paper I've written, every experiment
10429920	10433120	ever run at the lab, you know, like gain the intuitions from all of this, they're going to be
10433120	10436800	able to learn in parallel from all of each other's experiment, you know, experiences.
10437920	10440000	You know, I don't know what else, you know, it's like, what does it take to be an Alec
10440000	10443840	Radford? Well, there's a, there's a sort of cultural acclimation aspect of it, right? You know,
10443840	10448080	if you hire somebody new, there's like politicking, maybe they don't fit in. Well, in the AI case,
10448080	10452080	you just make replicas, right? There's a like motivation aspect for it, right? So it's like,
10452080	10455920	you know, Alec, you know, they could just like duplicate Alec Radford. And before I run every
10455920	10459760	experiment, I haven't spent like, you know, a decade's worth of human time, like double checking
10459760	10462800	the code and thinking really careful, be careful about it. I mean, first of all,
10462800	10467520	on how that many Alec Radfords, and you know, he wouldn't care. And he would not be motivated.
10467520	10470800	But you know, the AI is it can just be like, look, I have a hundred million of you guys,
10470800	10474400	I'm just going to put you on just like really making sure this code is correct. There are no
10474400	10479840	bugs. This experiment is thought through every hyperparameter is correct. Final thing I'll say
10479840	10484080	is, you know, the 100 million human equivalent AI researchers, that is just a way to visualize it.
10484080	10487200	So that doesn't mean you're going to have literally 100 million copies. You know,
10487200	10491440	so there's tradeoffs you can make between serial speed and in parallel. So you might make the
10491440	10495520	tradeoff is look, we're going to run them at, you know, 10x 100x serial speed. It's going to
10495520	10499280	result in fewer tokens overall, because it's sort of inherent tradeoffs. But you know, then we have,
10499280	10503520	I don't know what the numbers would be, but then we have, you know, 100,000 of them running at 100x
10503520	10507920	human speed and thinking and you know, and there's other things you can do on coordination, you
10507920	10511680	know, they can kind of like share latent space, attend to each other's context. There's basically
10511680	10515360	this huge range of possibilities of things you can do. The 100 million thing is more, I mean,
10515360	10519120	another illustration of this is, you know, if you kind of, I run the math in my series,
10519120	10522960	and it's basically, you know, 27, 28, you have this automated AI researcher,
10524080	10528640	you're going to be able to generate an entire internet's worth of tokens every single day.
10528640	10532560	So it's clearly sort of a huge amount of like intellectual work they can do.
10532560	10538960	I think the analogous thing there is today we generate more patents in a year than during the
10538960	10542960	actual physics revolution in the early 20th century, they were generating across like half a
10542960	10547520	century or something. And are you making more physics progress in a year today than you made?
10547520	10552080	So yeah, you're going to generate all these tokens. Are you generating as much
10553200	10558160	codified knowledge as humanity has been able to generate in the initial creation of the internet?
10558160	10562080	Internet tokens are usually final output, right? A lot of these tokens, if we talk, we talked about
10562080	10566480	the unhobbling, right? And I think of a kind of like, you know, a GPDN token is sort of like one
10566480	10569920	token of my internal monologue, right? And so that's how I do this math on human equivalents,
10569920	10573840	you know, it's like 100 tokens a minute, and then, you know, humans working for X hours and,
10573840	10578800	you know, what is the equivalent there? I think this goes back to something we were talking about
10578800	10584400	earlier where, well, I haven't seen the huge revenues from people often ask this question,
10584400	10589040	that if you took GPD for back 10 years and you show people this and they think this is going to
10589040	10593840	automate, this is already automated, half the jobs. And so there's a sort of a modus ponens,
10593840	10597600	modus tolens here where part of the explanation is like, oh, it's like just on the verge,
10597600	10601920	you need to do these unhobblings. And part of that is probably true. But there is another lesson
10601920	10608800	to learn there, which is that just looking at face value outside of abilities, there's probably more
10608800	10613040	sort of hobblings that you don't realize that are hidden behind the scenes. I think the same will
10613040	10617680	be true of the AGI that you have running as AI researchers. I think a lot of things basically
10617680	10621760	agree, right? I think my story here is like, you know, I talk about, I think there's going to be
10621760	10625520	some long tail, right? And so part, you know, maybe it's like, you know, 26, 27, you know,
10625520	10628560	like the proto-automated engineer, and it's really good at engineering. It doesn't have the
10628560	10632800	research intuition yet. You don't quite know how to put them to work. But, you know, the sort of
10632800	10636880	even the underlying pace of AI progress is already so fast, right? In three years from not being able
10636880	10641760	to do any kind of like math at all to now crushing, crushing these math competitions. And so you have
10641760	10645840	the initial thing in like 26, 27, maybe the sort of automated, it's an automated research engineer,
10645840	10649600	speeds you up by 2x, you go through a lot more progress in that year. By the end of the year,
10649600	10653120	you figured out like the remaining kind of unhobblings, you've like got a smarter model,
10653120	10656720	and you know, maybe then that thing, or maybe it's two years, you know, and that thing, just like
10656720	10660640	that thing really can do automate 100%. And again, you know, they don't need to be doing
10660640	10663680	everything. They don't need to be making coffee, you know, they don't need to like, you know,
10663680	10667680	maybe there's a bunch of, you know, tacit knowledge and a bunch of other fields. But you know,
10668320	10672560	AI researchers at AI labs really know the job of an AI researcher. And it's in some sense,
10672560	10676640	it's a sort of, there's lots of clear metrics, it's all virtual, there's code, it's things you
10676640	10682160	can kind of develop and train for. So I mean, another thing is how do you actually manage a
10682240	10689120	million AI researchers? Humans, the sort of comparative ability we have that we've been
10689120	10694160	especially trained for is like working in teams. And despite this fact, we have, for thousands of
10694160	10699200	years, we've been learning about how we work together in groups. And despite this, management
10699200	10704720	is a clusterfock, right? It's like most companies are badly managed. It's really hard to do this
10704720	10715120	stuff. For AIs, the sort of like, we talk about AGI, but it'll be some bespoke set of abilities,
10715120	10720000	some of which will be higher than humans, some of which will be at human level. And so it'll be
10720000	10726160	some bundle and we'll need to figure out how to put these bundles together with their human
10726160	10732560	overseers, with the equipment and everything. And the idea that as soon as you get the bundle,
10732560	10737760	you'll figure out how to get, like just shove millions of them together and manage them.
10737760	10744560	I'm just very skeptical of like any other revolution, technological revolution in history
10744560	10749600	has been very piecemeal, much more piecemeal than you would expect on paper. If you just thought
10749600	10754800	about what is the industrial revolution? Well, we dig up coal that powers the steam engines,
10754800	10758720	you use the steam engines to run these railroads, that helps us get more coal out. And there's
10758720	10763600	sort of like factorial store, you can tell, where in like a six hours, you can be pumping
10764240	10769120	thousands of times more coal. But in real life, it takes centuries often, right? In fact, the
10769120	10776000	electrification, there's this famous study about how to initially to electrify factories.
10777200	10783760	It was decades after electricity to change from the pull, pulleys and water wheel based
10783760	10788240	system that we had for steam engines to one that's works with more spread out electrical
10788240	10791120	motors and everything. I think this will be the same kind of thing. It might take like decades
10791120	10794800	to actually get millions of AI researchers to work together. Okay, great. This is great.
10794800	10798480	Okay, so a few responses to that. First of all, I mean, I totally agree with the kind of like
10798480	10802960	real world bottlenecks type of thing. I think this is sort of, you know, I think it's easy to under
10802960	10806640	rate, you know, basically what we're doing is we're removing the labor constraint, we automate
10806640	10810480	labor and we like kind of exploit technology. But you know, there's still lots of other bottlenecks
10810480	10813680	in the world. And so I think it's part of why the story is it kind of like starts pretty narrow at
10813680	10817440	the thing where you don't have these bottlenecks. And then only over time as we let it kind of
10817440	10821760	expand to sort of broader areas. AI, this is part of why I think it's like initially this sort of
10821760	10825520	AI research explosion, right? It's like AI research doesn't run into these real world bottlenecks.
10825520	10829520	It doesn't require, you know, like plow a field or dig up coal. It's just you're just doing AI
10829520	10833840	research. The other thing, you know, the other thing about like in your model, AI research,
10833840	10838320	it's not complicated like about flipping a burger, it's just AI research.
10839520	10844480	I mean, this is because people make these arguments like, oh, you know, AGI won't do anything
10844480	10847520	because it can't flip a burger. I'm like, yeah, it won't be able to flip a burger, but it's going
10847520	10851680	to be able to do algorithmic progress, you know, and then, and then, and then when it does algorithmic
10851680	10858560	progress, it'll figure out how to flip a burger. You know, look, the, the, the, sorry, the other
10858560	10861840	thing is about, you know, again, these are the sort of quantities are lower bound, right? So
10861840	10865840	it's like, this is just like, we can definitely run 100 million of these. Probably what will happen
10865840	10870240	is one of the first things we're going to try to figure out is how to like, again, run like,
10870240	10874560	you know, translate quantity into quality, right? And so it's like, even at the baseline rate of
10874560	10878240	progress, you're like quickly getting smarter and smarter systems, right? If we said it was like,
10878240	10881680	you know, four years between the preschooler and the high schooler, right? So I think, you know,
10881680	10885280	pretty quickly, you know, there's probably some like simple algorithmic changes you find, you know,
10885280	10889280	if instead of one Alec Radford, you have 100, you know, you don't even need 100 million. And then,
10889280	10893040	and then you get even smarter systems. And now these systems are, you know, they're capable of
10893040	10896560	sort of creative, complicated behavior, you don't understand. Maybe there's some way to like use
10896640	10899920	all this test time compute in a more unified way rather than all these parallel copies.
10901440	10905680	And, you know, so there won't just be quantitatively superhuman, they'll pretty quickly become
10905680	10909280	qualitatively superhuman. You know, it's sort of like, it looked like, you know, you're a high
10909280	10913200	school student, you're like trying to wrap yourself, wrap your mind around kind of standard physics.
10913200	10916960	And then there's some like super smart professor who is like, quantum physics, it all makes sense
10916960	10920880	to him. And you're just like, what is going on? And sort of, I think pretty quickly, you kind of
10920880	10925520	enter that regime, just given even the underlying pace of AI progress, but even more quickly than
10925520	10929600	that, because you have the sort of accelerated force of now this automated AI research.
10929600	10935920	I agree that over time, you would, I'm not denying that ASI is, I think that's possible.
10935920	10936560	Because the time is just not that much, you know?
10936560	10940400	I'm just like, you know, how is this happening in a year? Like you've, okay, first of all.
10940400	10943600	So I think the story is sort of like, basically, I think it's a little bit more continuous,
10943600	10947040	you know, right? Like, I think already, you know, like I talked about, you know, 25, 26,
10947040	10949600	you're basically going to have models as good as a college graduate. And I, you know,
10950240	10953520	I don't, I don't know where the unhobbling is going to be. But I think it's plausible that even
10953520	10957840	then you have kind of the proto-automated engineer. So there's, I think there is a bit of like a smear,
10957840	10961440	kind of an AGI smear or whatever, where it's like, there's sort of unhobblings that you're
10961440	10964640	missing. There's kind of like ways of connecting them you're missing. There's like some level
10964640	10968000	intelligence you're missing. But then at some point, you are going to get the thing that is like
10968000	10973680	an 100% automated Alec Radford. And once you have that, you know, things really take off, I think.
10974560	10979520	Yeah. Okay. So let's go back to the unhobblings. Is there, we're going to get a bunch of models
10979520	10984400	by the end of the year. Is there something, let's suppose we didn't get some capacity by the end of
10984400	10988800	the year. Is there some such capacity, which lacking would suggest that AI progress is going to take
10988800	10992800	longer than you are projecting? Yeah. I mean, I think there's, there's two kind of key things.
10992800	10995680	There's the unhobbling and there's the data wall, right? I think we should talk about the data wall
10995680	10999840	for a moment. I think the data wall is, you know, even though kind of like all this stuff has been
10999840	11002800	about, you know, crazy AI progress, I think the data wall is actually sort of underrated. I think
11002800	11006880	there's like a real scenario where we're just stagnating. You know, because we've been running
11006880	11010400	this tailwind of just like, it's really easy to bootstrap and you just do unsupervised learning
11010400	11014480	next token prediction that learns these amazing world models, like bam, you know, great model,
11014480	11017760	and you just got to buy some more compute, you know, do some simple efficiency changes,
11018960	11022080	you know, and, and again, like so much of deep learning, all these like big gains on
11022080	11025680	efficiency have been like pretty dumb things, right? Like, you know, you add a normalization layer,
11025680	11029760	you know, you know, you fix the scaling laws, you know, and these already have been huge things,
11029760	11034320	let alone kind of like obvious ways in which these models aren't good yet. Anyway, so data
11034320	11039120	wall big deal, you know, I don't know, some like put some numbers on this, you know,
11039760	11043920	some like you do common crawl, you know, online is like, you know, 30 trillion tokens,
11043920	11047840	llama three trained on 15 trillion tokens. So you're basically already using all the data.
11047840	11051600	And then, you know, you can get somewhat further by repeating it. So there's an academic paper by,
11051600	11056320	you know, Boaz Barak and some others that does scaling laws for this. And they're basically
11056320	11060880	like, yeah, you can repeat it sometime. After 16 times of reputation, just like returns basically
11060880	11064720	go to zero, you're just completely screwed. And so I don't know, say you can get another 10x on
11064720	11069120	data from, say like llama three, and GP four, you know, llama three is already kind of like
11069120	11072640	at the limit of all the data, you know, maybe you can get 10x more by repeating data.
11073680	11078080	You know, I don't know, maybe that's like at most 100x better model than GP for, which is like,
11078080	11082000	you know, 100x effective compute from GP four is, you know, not that much, you know, if you do half
11082000	11085360	in order magnitude a year of compute half in order magnitude a year of algorithmic progress,
11085360	11089760	you know, that's kind of like two years from GP four. So, you know, GP four finished pretreading
11089760	11096240	in 22, you know, 24. So I think one thing that really matters, I think we won't quite know by
11096240	11100560	end of the year, but you know, 25, 26, are we cracking the data wall?
11102400	11108640	Okay, so suppose we had three orders of magnitude less data in common crawl on the internet than
11108640	11114720	we just happen to have now. And for decades, the internet, other things, we've been rapidly
11114720	11121120	increasing the stock of data that humanity has. Is it your view that for contingent reasons,
11121120	11128560	we just happen to have enough data to train models that are just powerful enough at 4.5 level,
11128560	11136240	where they can kick off the self play RL loop? Or is it just that we, you know, if it had been
11136240	11140880	three rooms higher, then it would progress would have been slightly faster. In that world, we
11140880	11143920	would have been looking back at like, oh, how hard it would have been to like kick off the RL
11144000	11148240	explosion with just 4.5, but we would have figured it out. And then so in this world, we would have
11148240	11152880	gotten to GP three and then we'd have to kick us on sort of RL explosion. But we would have still
11152880	11156880	figured it out. The sort of the we didn't just like luck out on the amount of data we happen to
11156880	11160480	have in the world. I mean, three rooms is pretty rough, right? Like three rooms, if less data means
11160480	11164400	like six rooms smaller, six rooms, less compute model and scale scaling laws, you know, that's
11164400	11168880	it's basically like capping out at like GP two, but I think that would be really rough. I think
11168880	11173360	you do make an interesting point about the contingency. You know, I guess earlier, we were
11173360	11177040	talking about the sort of like when in the sort of human trajectory, are you able to learn from
11177040	11181120	yourself? And so, you know, if we go with that analogy, again, like if you'd only gotten the
11181120	11184160	preschooler model, it can't learn from itself. You know, if you only got in the elementary
11184160	11188000	school or model, can't learn from itself. And you know, maybe GP for, you know, smart high
11188000	11191520	school is really where it starts. Ideally, you have a somewhat better model than it really is
11191520	11196240	able to kind of like learn from itself or learn by itself. So yeah, I think there's an interesting,
11196240	11201280	I mean, I think maybe one room less data, I would be like more iffy, but maybe still doable.
11202080	11204720	Yeah, I think it would feel chiller if we had, you know, like one or two.
11204720	11209040	It would be an interesting exercise to get probably distributions of HEI contingent on
11209040	11214160	across like data. Yeah, okay. I think the thing that makes me skeptical of this story
11214160	11219440	is that the things it totally makes sense for free training works so well. Yeah. These other
11219440	11226160	things, their stories of in principle, why they ought to work like a humans can learn this way
11226240	11231440	and so on. Yes. And maybe they're true, but I worry that a lot of this case is based on
11231440	11237280	sort of first principles with evaluation of how learning happens that fundamentally,
11237280	11240800	we don't understand how humans learn and maybe there's some key thing we're missing.
11240800	11244720	Yeah. On the sort of sample efficiency, yeah, humans actually, maybe there's,
11245920	11249680	you say, well, the fact that these things are way of a less sample efficient in terms of learning
11249680	11253680	than humans are suggests that there's a lot of room for improvement. Yeah. Another perspective
11253680	11258640	is that we are just on the wrong path altogether, right? That's why there's a sample inefficient
11258640	11263920	when it comes to pre-training. Yeah. So, yeah, I mean, I'm just like, there's a lot of like
11264720	11267920	first principles arguments stack on top of each other where you get these unhoplings and then
11267920	11272640	you get to HEI. Yeah. Then you, because of these reasons why you can stack all these things on
11272640	11276720	top of each other, you get to ASI. Yeah. And I'm worried that there's too many steps of this.
11276720	11283520	Yeah. Sort of first principles thinking. I mean, we'll see, right? I mean, on the sort
11283520	11287840	of sample efficiency thing, again, sort of first principles, but I think, again, there's this
11287840	11293280	clear sort of missing middle. And so, you know, and sort of like, you know, people hadn't been
11293280	11297840	trying. Now people are really trying, you know, and so it's sort of, you know, I think often again
11297840	11301680	in deep learning, something like the obvious thing works. And there's a lot of details to get
11301680	11304880	right. So it might take some time, but it's now people are really trying. So I think we get a
11304880	11312880	lot of signal in the next couple of years. You know, on a hobbling, I mean, what is the signal
11312960	11316320	on hobbling that I think would be interesting? I think, I think the question is basically like,
11316320	11320080	are you making progress on this test time compute thing, right? Like is this thing able to think
11320080	11323440	longer horizon than just a couple hundred tokens, right? That was unlocked by chain of thought.
11323920	11329120	And on that point in particular, the many people who have longer timelines have come on the podcast
11329120	11335920	have made the point that the way to train this long horizon RL, it's not, I mean, earlier talking
11335920	11340320	about like, well, they can think for five minutes, but not for longer. But it's not because they
11340320	11345040	can't physically output an hours or the tokens. It's just really, at least from what I understand
11345040	11348640	what they say, right? Like even like Gemini has like a million in context and the million of context
11348640	11352400	is actually great for consumption. And it solves one important on hobbling, which is the sort of
11352400	11358000	onboarding problem, right? Which is, you know, a new coworker, you know, in your first five minutes,
11358000	11361920	like a new smart high school intern first five minutes, not useful at all, a month in, you know,
11361920	11365760	much more useful, right? Because they've like looked at the mono repo and understand how the
11365760	11369520	code works. And they've read your internal docs. And so being able to put that in context, great,
11369520	11373360	solve this onboarding problem. Yeah, but they're not good at sort of the production of a million
11373360	11379840	tokens yet. Yeah, right. But on the production of a million tokens, there's no public evidence that
11379840	11385920	there's some easy loss function where you can GP for has gotten a lot better since it's actually
11385920	11390960	so the GP for gains since launch, I think are a huge indicator that there's like, you know, so
11390960	11395040	you talked about this with John on the podcast, John said this was mostly post training gains.
11395120	11400080	Right. You know, if you look at the sort of LMS scores, you know, it's like 100 ELO or something,
11400080	11404000	it's like a bigger gap than between Claude III, Opus and Claude III, Haiku. And the price difference
11404000	11409600	between those is 60X. But it's not more agentic. It's like better in the same channel. Right? Like,
11409600	11414000	you know, it went from like, you know, 40% to 70% math. The crux is like whether or like be able to
11414000	11417440	like, but I think I think it indicates that clearly there's stuff to be done on hobbling.
11418240	11421840	I think yeah, I think I think the interesting question is like this time of year from now,
11421840	11425920	you know, is there a model that is able to think for like, you know, a few thousand tokens
11425920	11430000	coherently, cohesively, agentically. And I think probably there's, you know,
11430000	11433840	again, this is what I'd feel better if we had an humor to more data, because it's like the scaling
11433840	11438480	just gives you this sort of like tailwind, right? We're like, for example, tools, right tools, I
11438480	11441840	think, you know, talking to people who try to make things work with tools, you know, actually
11441840	11446080	sort of GP for is really when tools start to work. And it's like, you can kind of make them work with
11446080	11451440	GP 3.5, but it's just really tough. And so it's just like having GP for you can kind of help it
11451440	11457120	learn tools in a much easier way. And so just a bit more tailwind from scaling. And then and then
11457120	11462080	yeah, and does does I don't know if it'll work, but it's a key question. Okay, I think it's a good
11462080	11467680	place to sort of close that part where we know what the crux is and what the progress, what
11467680	11473920	what evidence that would look like on the AGI to super intelligence. Maybe it's a case that the
11473920	11478000	games are really easier right now, and you can just sort of let loose and Alec Ratt for giving
11478080	11482320	a compute budget, and he comes out the other end with something that is an additive,
11483600	11486800	like change as part of the code, this is compute multiplier changes to the part.
11488320	11494400	What other parts of the world? Maybe there here's an interesting way to ask this. Yeah, how many
11494400	11501280	other domains in the world are like this, where you think you could get the equivalent of in one
11501280	11507040	year, you just throw enough intelligence across multiple instances, and you just come out the
11507120	11514720	other end with something that is remarkably decades centuries ahead. Yeah, like you start off with
11514720	11520400	no flight and then the right brother is a million instances of GPT six, and you come out the other
11520400	11527120	end with starlink. Yeah. Like is that your model of how things work? I think I think you're exaggerating
11527120	11530400	the timelines a little bit, but but you know, I think you know, a decade's worth of progress in
11530400	11535200	a year or something. I think that's a reasonable prompt. So I think this is where, you know,
11535840	11539760	basically the sort of automated AI researcher comes in because it gives you this enormous tail
11539760	11543600	headwind on all the other stuff, right? So it's like, you know, you automate AI research with
11543600	11547280	your sort of automated Alec Radford's, you come out the other end, you've done another five booms,
11547280	11552080	you have a thing that is like vastly smarter, not only is it vastly smarter, you like, you know,
11552080	11555120	you've been able to make it good at everything else, right? You're like, you're solving robotics,
11555120	11558560	the robots are important, right? Because like, for a lot of other things, you do actually need to
11558560	11562560	like try things in the physical world. I mean, I don't know, maybe you can do a lot in simulation,
11562560	11566080	those are the really quick worlds. I don't know if you saw the like last Nvidia GTC, you know,
11566080	11569760	it was all about the like digital twins and just like having all your manufacturing processes and
11569760	11573840	simulation, like, I don't know, like, again, if you have these like, you know, super intelligent,
11573840	11577040	like cognitive workers, like, can they just like make simulations of everything, you know,
11577040	11581600	kind of off the float style, and then, and then, you know, make a lot of progress and simulation
11581600	11587440	possible. But I also just think you're going to get the robots. Again, I agree about like,
11587440	11592160	there are a lot of real world bottlenecks, right? And so, you know, I don't know, it's quite possible
11592160	11595520	that we're going to have, you know, crazy drone forms, but also, you know, like lawyers and
11595520	11600640	doctors still need to be humans because of like, you know, regulation. But, you know, I think,
11600640	11604400	you know, you kind of start narrowly, you broaden, and then the world's in which you kind of let them
11604400	11607520	loose, which again, because of I think these competitive pressures, we will have to let them
11607520	11613600	loose in some degree on, you know, various national security applications. I think like,
11613600	11617760	quite rapid progress is possible. The other thing, though, is it's sort of, you know,
11617760	11620880	basically in the sort of an explosion after there's kind of two components, there's the A,
11620880	11624400	right in the production function, like growth of technology, and that's massively accelerated
11624400	11628240	by, you know, you have a billion super intelligent scientists and engineers and technicians,
11628240	11632720	you know, superbly competent and everything. You also just automated labor, right? And so,
11632720	11636480	it's like, even without the whole technological explosion thing, you have this industrial explosion,
11636480	11639840	at least if you let them let them loose, which is like, now you can just build, you know,
11639840	11643840	you can cover Nevada and like, you know, you start with one robot factory is producing more robots,
11643840	11648640	and basically this like just the cumulative process because you've taken labor out of the equation.
11648880	11654720	Yeah. That's super interesting. Yeah. Although when you increase the
11655360	11662400	K or the L without increasing the A, you can look at the Soviet Union or China where they
11662400	11667920	rapidly increase inputs. Yeah. And that does have the effect of being geopolitically game changing,
11667920	11673440	where you, it is remarkable, like you go to Shanghai over a set of these crazy cities in
11673440	11676800	a decade. Right. Right. I mean, the closest thing to like, people talk about 30% growth
11676800	11683200	rates or whatever. Yeah. 10%. It's totally possible. Yeah. But without productivity gains,
11683200	11687360	it's not like the industrial revolution, where like, you're from the perspective of
11687360	11690480	you're looking at a system from the outside, your goods have gotten cheaper, or they can
11690480	11695680	manufacture more things. But, you know, it's not like the next century is coming at you.
11695680	11698560	Yeah. It's both. It's both. So it's, you know, both that are important. The other thing I'll say
11698560	11702480	is like, all this stuff, I think the magnitudes are really, really important, right? So,
11703280	11707680	you know, we talked about a 10x of research effort, or maybe 10, 30x over a decade,
11707680	11711920	you know, even without any kind of like self-improvement type loop, you know, we talk,
11711920	11715520	the sort of, even in the sort of Jeep before the AGI story, we're talking about an order of magnitude
11715520	11719280	of effective compute increase a year, right? Half an order of magnitude of compute, half an order
11719280	11723680	of magnitude of algorithmic progress that sort of translates into effective compute. And so,
11724560	11728480	you're doing a 10x a year, right? Basically on your labor force, right? So it's like,
11728480	11732080	it's a radically different world if you're doing a 10x or 30x in a century versus a
11732080	11736160	10x a year on your labor force. So the magnitudes really matter. They also really matter on the
11736160	11739840	sort of intelligence explosion, right? So like just the automated AI research part. So, you know,
11739840	11743920	one story you could tell there is like, well, ideas get harder to find, right? Algorithmic
11743920	11747120	progress is going to get harder. Yeah, right now you have the easy wins, but in like four or five
11747120	11751120	years, there'll be fewer easy wins. And so the sort of automated AI researchers are just going
11751120	11754560	to be what's necessary to just keep it going, right? Because it's gotten harder. But that's
11754560	11757920	sort of, it's like a really weird knife edge assumption economics where you assume it's just
11758000	11762320	enough. But isn't that the equilibrium story you were just telling with why the economy as a whole
11762320	11766400	has 2% economic growth? Because you just pursued on the equal, I guess you're saying by the time
11766400	11770240	you get to the equilibrium here is it's like way faster, at least, you know, and it's at least,
11770240	11773280	and it depends on the sort of exponents, but it's basically it's the increase that like,
11773280	11777200	suppose you need to like 10x effective research effort in AI research in the last,
11777200	11780480	you know, four or five years to keep the pace of progress, we're not just getting a 10x, you're
11780480	11783680	getting, you know, a million x or a hundred thousand x, there's just the magnitudes really
11783680	11787600	matter. And the magnitude is just basically, you know, one, one way to think about this is that
11787600	11791520	you have kind of two exponentials, you have your sort of like normal economy that's growing at,
11791520	11796000	you know, 2% a year, and you have a really AI economy, and that's going at like 10x a year.
11796000	11799920	And it's starting out really small, but sort of eventually it's going to, it's just, it's, it's,
11799920	11803920	it's way faster and eventually it's going to overtake, right? And even if you have, you can
11803920	11807520	almost sort of just do the simple revenue extrapolation, right? If you think your AI economy,
11807520	11811440	you know, that has some growth rate, I mean, it's very simplistic way and so on. But there's,
11811520	11815040	there's this sort of 10x a year process and that will eventually kind of like,
11815040	11819200	you're going to transition the sort of whole economy from, as it broadens from the sort of,
11819200	11824320	you know, 2% a year to the sort of much faster growing process. And I don't know, I think that's
11824320	11829600	very like consistent with historical change, you know, stories, right? There's this sort of like,
11830480	11834880	you know, there's this sort of long run hyperbolic trend, you know, manifested in the sort of like,
11834880	11838640	sort of change in growth mode and the austral, you know, revolution, but there's just this
11838640	11843040	long run hyperbolic trend. And, you know, now you have this sort of, now you have that another
11843040	11846480	sort of change in growth mode. Yeah, yeah. I mean, that was one of the questions I asked Tyler,
11846480	11853600	when I had him on the podcast, is that you do go from the fact that after 1776, you go from a regime
11853600	11858880	of negligible economic growth, 2% is really interesting. It shows that, I mean, from the
11858880	11864160	perspective of somebody in the Middle Ages or before, 2% is equivalent to the sort of 10%.
11864960	11867760	I guess you're projecting even higher for the AI economy, but
11867760	11871200	yeah, I mean, I think again, and it's all this stuff, you know, I have a lot of uncertainty,
11871200	11874480	right? So a lot of the time I'm trying to kind of tell the modal story. I think it's important
11874480	11878800	to be kind of concrete and visceral. And I, you know, I have, I have a lot of uncertainty
11878800	11882720	basically over how the 2030s play out. And basically, the thing I know is it's going to be
11882720	11887760	fucking crazy. But, but, you know, exactly what, you know, where the bottlenecks are and so on,
11887760	11893120	I think that will be kind of like. So let's talk through the numbers here. You hundreds of millions
11893120	11902160	of AI researchers. So right now, GPT-40 turbo is like 15 bucks for a million tokens outputted.
11902160	11907360	And a human thinks 150 tokens a minute or something. And if you do the math on that,
11907360	11914880	I think it's for an hour's worth of human output. You, it's like 10 cents or something.
11915760	11920240	Now, cheaper than a human worker. Cheaper than a human worker. But it can't do the job yet.
11920240	11923920	That's right. That's right. But by the time you're talking about models that are trained on the 10
11923920	11930160	gigawatt cluster, then you have something that is four orders of magnitude more expensive via
11930160	11935520	inference, three orders of magnitude, something like that. So that's like $100 an hour of labor.
11935520	11941760	And now you're having hundreds of millions of such laborers. Is there enough compute to do
11941760	11946000	with the model that is a thousand times bigger, this kind of labor?
11946000	11949840	Great. Okay. Great question. So I actually don't think inference costs for sort of frontier
11949840	11953280	models are necessarily going to go up that much. So I mean, one historical data point.
11953280	11956320	But isn't the test time sort of thing that it will go up even higher?
11956320	11959200	I mean, we're just doing per token, right? And then I'm just saying, you know, if,
11959200	11963440	suppose each model token was the same as sort of a human token thing at 100 tokens a minute.
11963440	11966640	So it's like, yeah, it'll use more. But the sort of, if you just, the token calculations
11966640	11973200	is already pricing that in the, the question is like per token pricing, right? And so like GPT-3
11973200	11979120	went at launch was like actually more expensive than GPT-4 now. And so over just like, you know,
11979120	11982480	fast increases in capability gains, inference costs has remained constant.
11983840	11987440	That's sort of wild. And I think it's worth appreciating. And I think it gestures that
11987440	11992080	sort of an underlying pace of algorithmic progress. I think there's a sort of like more
11992080	11995840	theoretically grounded way to why, why inference costs would stay constant. And it's the fourth
11995840	11999840	following story, right? So on Chichilly scaling laws, right, you, you know,
11999840	12004000	half of the additional compute you allocate to bigger models and half of it you allocate to more
12004000	12008720	data, right? But also if we go with the sort of basic story of half an order a year more
12008720	12012560	compute and half an order of magnitude a year of algorithmic progress, you're also kind of like
12012560	12016320	you're saving half an order of magnitude a year. And so that kind of would exactly compensate for
12016320	12020240	making the model bigger. The caveat on that is, you know, obviously not all training efficiencies
12020240	12024560	are also inference efficiencies, you know, bunch of the time they are separately, you can find
12024560	12028480	inference efficiencies. So I don't know, given this historical trend, given the sort of like,
12028480	12035360	you know, baseline sort of theoretical reason, you know, I don't know, I, I think it's not crazy
12035520	12038800	baseline assumption that actually these models, frontier models are not necessarily going to get
12038800	12044480	more expensive per token. Oh, really? Yeah. Like, okay, that's, that's wild. We'll see, we'll see.
12044480	12047600	I mean, the other thing, you know, maybe they get, you know, even if they get like 10x more
12047600	12050720	expensive than, you know, you have 10 million instead of 100 million, you know, so it's like,
12050720	12055360	it's not really, you know, like, but okay, so part of the internal explosion is that each of them has
12055360	12063520	to run experiments that are gb4 size and the result of, so that takes up a bunch of compute.
12063760	12067840	Then you're going to consolidate the results of the experiments and what is the synthesized
12068720	12071120	weight. I mean, you have a much bigger influence street anyway than your training.
12071120	12073840	Sure. Okay. But I think the experiment compute is a constraint.
12073840	12079040	Yeah. Okay. I'm going back to maybe a sort of bigger fundamental thing we're talking about here.
12080400	12088400	We're projecting, in a series, you say we should denominate the probability of getting to AGI in
12088400	12094240	terms of orders of magnitude of affected compute effective here, accounting for the fact that
12094240	12098000	there's a compute quote unquote compute multiplier if you have better algorithms.
12099520	12107200	And I'm not sure that it makes sense to be confident that this is a sensible way to project
12107200	12112240	progress. It might be, but I'm just like, I have a lot of uncertainty about it. It seems similar to
12112240	12116400	somebody trying to project when we're going to get to the moon and they're like looking at the
12116400	12121520	Apollo program in the four fifties or something and they're like, we have some amount of effective
12121520	12129200	jet fuel. And if we get more efficient engines, then we have more effective jet fuel. And so
12129200	12133200	we're going to like probability of getting to the moon based on the amount of effective jet fuel we
12133200	12138800	have. And I don't deny that jet fuel is important to launch rockets, but that seems like an odd
12138800	12143760	way to denominate when you're going to get to the moon. Yeah. Yeah. So I mean, I think these cases
12143760	12146880	are pretty different. I don't know. I didn't, I don't think there was a sort of clear, I don't
12146880	12150080	know how rocket science works, but I didn't, I didn't get the impression that there's some
12150080	12155520	clear scaling behavior with like, you know, the amount of jet fuel. I think the, I think in AI,
12155520	12159680	you know, I mean, first of all, the scaling laws, you know, they've just helped, right? And so if
12159680	12163120	you, a friend of mine pointed this out, and I think it's a great point, if you kind of concatenate
12163120	12167440	both the sort of original Kaplan scaling laws paper that I think went from 10 to the negative
12167440	12173040	nine to 10 petaflop days, and then, you know, concatenate additional compute to from there to
12173120	12176800	kind of GP four, you assume some algorithmic progress, you know, it's like the scaling laws
12176800	12180640	have held, you know, like probably over 15 ooms, you know, I know it was rough, probably maybe
12180640	12185120	even more held for a lot of ooms. They held for the specific loss function, which they're training
12185120	12191520	on, which is training max token. Whereas the, the, the progress you are forecasting will be
12191520	12196640	required for further progress in capabilities. Yeah. It was specifically, we know that scaling
12196640	12200400	can't work because of the data wall. And so there's some new thing that has to happen. And I'm not
12200480	12205680	sure whether the, you can extrapolate that same scaling curve to tell us whether these
12205680	12209440	hobblings will also, like it's, is this not on the same graph? The hobblings are just a separate
12209440	12213600	thing. Yeah, exactly. So this is, this is sort of like, you know, it's, yeah. So I mean, a few,
12213600	12218880	a few things here, right? Okay. So the, on the, on the effective compute scaling, the, you know,
12218880	12222080	in some sense, I think it's like people center the scaling laws because they're easy to explain
12222080	12227040	and the sort of like, why, why is scaling matter? The scaling laws like came way after people, at
12227040	12230560	least, you know, like Dario, Ilya realized that scaling mattered. And I think, you know, I think
12230560	12234800	that almost more important than the sort of loss curve is just like, just in general, make, you
12234800	12238080	know, there's this great quote from Dario on your, on your, on your, on your podcasts, it's just like,
12238080	12241520	you know, Ilya was like, the models, they just want to learn, you know, you make them bigger,
12241520	12246320	they learn more. And, and that just applied just across domains, generally, you know, all the
12246320	12250560	capabilities. And so, um, and you can look at this in benchmarks. Again, like you say,
12250560	12254560	headwind data wall, I'm sort of bracketing that and talking about that separately.
12254640	12258480	The other thing is on hobblings, right? If you just put them on the effective compute graph,
12258480	12261360	these on hobblings would be kind of huge, right? So like, I think,
12261360	12264080	What does it even mean? Like, what is it? What is on the y axis here?
12264960	12269520	Like, say MLPR on this benchmark or whatever, right? And so, you know, like, you know, we mentioned
12269520	12273920	the sort of, you know, the LMSYS differences, you know, RLHF, you know, again, as good as 100x
12273920	12277360	small chain of thought, right? You know, just going from this prompting change, a simple argument
12277360	12281600	change can be like 10x effective increase, compute increases on like math benchmarks.
12281600	12284960	I think this is like, you know, I think this is useful to illustrate that on hobblings are
12284960	12289040	large. Um, but I think they're like, I kind of think of them as like slightly separate things.
12289040	12293120	And the kind of the way I think about is that like, at a per token level, I think GP four is
12293120	12298560	not that far away from like a token of my internal monologue, right? Even like 3.5 to four took us
12298560	12302400	kind of from like the bottom of the human range, the top of the human range on like a lot of,
12302400	12306400	you know, on a lot of, uh, you know, kind of like high school tests. And so it's like a few more
12306400	12311040	3.5 to four jumps per token basis, like per token intelligence. And then you've got to unlock the
12311040	12315440	test time, you've got to solve the onboarding problem, make it use a computer. Um, and then
12315440	12321440	you're getting real close. Um, I'm reminded of, again, the story might be wrong, but I think it
12321440	12325600	is strikingly plausible. I agree. And so I think actually, I mean, the other thing I'll say is
12325600	12330320	like, you know, I say this 2027 timeline, I think it's unlikely, but I do think there's worlds
12330320	12334320	that are like AGI next year. And that's basically if the test time compute overhang is really easy
12334320	12338400	to crack. If it's really easy to crack, then you do like four rooms of test and compute, you know,
12338400	12342320	from a few hundred tokens to a few million tokens, you know, quickly. And then, you know, again,
12342320	12346720	maybe it's maybe only takes one or two 3.5 to four jumps per token, like one or two of those
12346720	12351120	jumps for token plus uses test time compute. And you basically have the proto automated engineer.
12351600	12359280	Um, so I'm reminded of, uh, uh, Stephen Pinker releases his book on, um, what is it? The Better
12359280	12363920	Angels of Our Nature. And it's like a couple of years ago or something. And he says the secular
12364000	12369520	decline in violence and war and everything. And you can just like plot the line from the end of
12369520	12373280	World War Two. And in fact, before World War Two, then these are just aberrations, whatever.
12373280	12379920	And basically, as soon as it happens, Ukraine, Gaza, the, everything is like, so
12379920	12391040	I think this is a sort of thing that happens in history where you see history align and you're
12391040	12395440	like, oh my gosh. And then just like, as soon as you make that prediction, who was that famous
12395440	12399280	author? So yeah, this, you know, again, people are predicting deep learning will hold a wall
12399280	12403840	every year. Maybe one year, they're right. But it's like gone a long way and hasn't hit a wall.
12403840	12408480	And I don't have that much more to go. And, and you know, so yeah, I guess I think this is a sort
12408480	12414400	of plausible story. And let's just run with it and see what it implies. Yeah. So we were talk
12414400	12420880	in your series, you talk about a lineman from the perspective of this is not about
12420880	12426080	some doomer scheme to get the point zero and personal probability distribution,
12426080	12430160	where things don't go off the rails. It's more about just controlling the systems,
12430160	12435920	making sure they do what we intend them to do. If that's the case, and we're going to be in the
12435920	12442320	sort of geopolitical conflict with China, and part of that will involve and what we're worried
12442320	12448800	about is them making the CCP bots that go out and take the red flag of Mao across the galaxies
12448800	12457280	or something. Then shouldn't we be worried about alignment as something that if in the wrong hands,
12457280	12464400	this is the thing that enables brainwashing, sort of dictatorial control. This seems like a
12464400	12468160	worrying thing. This should be part of this sort of algorithmic secrets we keep hidden, right?
12468160	12472000	The how to align these models, because that's also something the CCP can use to control their
12472000	12475280	models. I mean, I think in the world where you get the Democratic coalition, yeah, I mean,
12475280	12479360	also just alignment is often dual use, right? Like RLHF, it's like alignment team developed,
12479360	12483440	it was great, it was a big win for alignment, but it's also obviously makes these models useful.
12486240	12491520	But yeah, so yeah, alignment enables the CCP bots. Alignment also is what you need to get the
12491520	12498480	sort of whatever USAIs, follow the Constitution and disobey unlawful orders and respect separation
12498480	12504320	of powers and checks and balances. So yeah, you need alignment for whatever you want to do. It's
12504320	12508240	the sort of underlying technique. Tell me what you make of this take. I'm going to stream with
12508240	12514080	this a little bit. Okay. So fundamentally, there's many different ways the future could go. Yeah.
12514080	12520080	There's one path in which the LA's are type crazy AI's with nanobots take the future and
12520080	12525200	determine everything to gray goo or paper clips. And the more you solve alignment, the more that
12525200	12530160	path of the decision tree is circumscribed. And then so the more you solve alignment,
12530160	12533840	the more it is just different humans and divisions they have. And of course,
12533920	12537440	we know from history that things don't turn out the way you expect. So it's not like you can decide
12537440	12541280	the future, but it will appear. It's part of the beauty of it, right? You want these mechanisms,
12541280	12545920	the error correction, pluralism. But from the perspective of anybody who's looking at the system,
12545920	12551280	it will be like, I can control where this thing is going to end up. And so the more you solve
12551280	12558000	alignment and the more you circumscribe the different futures that are the results of AI will,
12558000	12563520	the more that accentuates the conflict between humans and their visions of the future. And so
12563520	12567600	in the world where alignment is solved and the world in which alignment is solved is the one,
12567600	12570720	is the world in which you have the most sort of human conflict over where to take AI.
12571280	12574880	Yeah. I mean, by removing the worlds in which the AI's take over, then like, you know,
12574880	12578240	the remaining worlds are the ones where it's like the humans decide what happens. And then
12578240	12582320	as we talked about, there's a whole lot of, yeah, a whole lot of worlds and how that could go.
12582320	12586480	And I worry. So when you think about alignment, and this is just controlling these things,
12586880	12594160	just think a little forward. And there's worlds in which hopefully, you know, human
12594160	12598960	descendants or some version of things in the future merge with super intelligences and they
12598960	12605520	have the rules of their own, but they're in some sort of law and market based order. I worry about
12605520	12611520	if you have things that are conscious and should be treated with rights. If you read about what
12611520	12615840	alignment schemes actually are, and then you read these books about what actually happened
12615920	12620720	during the Cultural Revolution, what happened when Stalin took over Russia, and you have
12621840	12626960	very strong monitoring from different instances where one, everybody's tasked with watching
12626960	12632560	each other. You have brainwashing, you have red teaming, where you have the spice stuff
12632560	12636320	you were talking about, where you try to convince somebody you're on like a defector and you see
12636320	12641920	if they defect with you. And if they do, then you realize they're an enemy. And listen, maybe
12641920	12648560	I'm stretching the analogy too far, but the way, like the ease of these alignment techniques
12648560	12653040	actually map on to something you could have read about during like mouse culture revolution
12653040	12657760	is a little bit troubling. Yeah, I mean, look, I think sentient AI is a whole other topic. I
12657760	12661200	don't know if we want to talk about it. I agree that like it's going to be very important how we
12661200	12666080	treat them. You know, in terms of like what you're actually programming these systems to do, again,
12666080	12670560	it's like alignment is just, it's a technical, it's a technical problem, a technical solution,
12670560	12675760	enables the CCP bots. I mean, in some sense, I think the, you know, I almost feel like the
12675760	12678800	sort of model and also about talking about checks and balances is sort of, you know, like the Federal
12678800	12682560	Reserve or Supreme Court justices. And there's a funny way in which they're kind of this like
12682560	12685840	very dedicated order, you know, Supreme Court justices, and it's amazing, they're actually
12685840	12690480	quite high quality, right? And they like really smart people, they really believe in the Constitution,
12690480	12694000	they love the Constitution, they believe in their principles, they have, you know, these, these,
12694000	12697920	these wonderful, you know, back, you know, and yeah, they have different persuasions, but they
12697920	12701920	have sort of, I think very sincere kind of debates about what is the meaning of the Constitution,
12701920	12706000	you know, what is the best actuation of these principles? You know, I guess, you know, by the
12706000	12710160	way, recommendation sort of skittish or arguments is like the best podcast, you know, when I run
12710160	12714000	out of high quality content on the Internet. I mean, I think there's going to be a process of
12714000	12717120	like figuring out what the Constitution should be. I think, you know, this Constitution has like
12717120	12720480	worked for a long time, you start with that, maybe eventually things change enough that you want
12720480	12724400	added to that. But anyway, you want them to like, you know, for example, for the checks and balances,
12724400	12728160	they like, they really love the Constitution, and they believe in it, and they take it really
12728160	12732640	seriously. And like, look, at some point, yeah, you are going to have like AI police and AI military,
12732640	12737360	but I think sort of like, you know, being able to ensure that they like, you know, believe in it
12737360	12740800	in the way that like a Supreme Court justice does, or like in the way that like a federal reserve
12740800	12746480	job, you know, official takes their job really seriously. Yeah. And I guess a big open question
12746480	12750400	is whether if you do the project or something like the project, the other important thing is
12750400	12754240	like a bunch of different factions need their own AIs, right? And so it's, it's really important
12754240	12757440	that like each political party gets to like, have their own, you know, and like, whatever
12757440	12760560	creeds it, you might totally disagree with their values, but it's like, it's really important
12760560	12764480	that they get to like, have their own kind of like super intelligence. And, and again, I think
12764480	12768320	it's that these sort of like classical liberal processes play out, including like, different
12768320	12772240	people of different persuasions and so on. And I don't know, maybe the advisors might not make
12772240	12776320	them, you know, wise, they might not follow the advice or whatever, but I think it's important.
12776320	12780960	Okay. So speaking of alignment, you seem pretty optimistic. So let's run, run through
12781600	12786800	the source of the optimism. Yeah. I think there you laid out different worlds in which we could
12786800	12791360	get AI. Yeah. There's one that you think is low probability of next year, where a GPT-4 plus
12791360	12796400	scaffolding plus unhoplings gets you to AGI. Not GPT-4, you know, like, oh, sorry, sorry. It's
12796400	12801680	a GP-5. Yeah. Yeah. And there's ones where it takes much longer. There's ones where it's
12801680	12807760	something that's a couple years. Yeah. In a modal world. Yeah. So GPT-4 seems pretty aligned in
12807760	12811600	the sense that I don't expect it to go off the rails. Yeah. Maybe with scaffolding things might
12811600	12817600	change. Looks pretty good. Yeah. Exactly. So the, and maybe you will keep turning at, there's cranks
12817600	12823440	to keep going up and one of the cranks gets you to ASI. Yeah. Is there any point at which the
12823440	12828880	sharp left turn happens? Is it when you start, is it the case that you think plausibly when they
12829120	12833520	act more like agents, this is the thing to worry about? Yeah. Is there anything qualitatively
12833520	12836640	that you expect to change with regards to the enlightenment perspective at these cranks?
12836640	12839760	Yeah. So I don't know if I believe in this concept of sharp left turn, but I do think there's
12839760	12843520	basically, I think there's important qualitative changes that happen between now and kind of like
12843520	12847440	somewhat super human systems, kind of like early on the intelligence explosion. And then important
12847440	12851680	qualitative changes that happen from like early in intelligence explosion to kind of like true
12851680	12855600	super intelligence and all its power and might. And let's talk about both of those.
12856480	12859920	And so, okay. So the first part of the problem is one, we're going to have to solve ourselves,
12859920	12862880	right? We have to kind of have to align the like initial AI and the intelligence explosion,
12862880	12866880	you know, the sort of automated out of Bradford. I think there's kind of like, I mean, two important
12866880	12873040	things that change from GBD4, right? So one of them is, if you believe the story on like,
12873040	12877600	you know, synthetic data or L or self play to get past the data wall, and if you believe this on
12877600	12881120	hobbling story, you know, at the end, you're going to have things, you know, they're agents,
12881120	12886240	right? Including they do long term plans, right? They have long, long, you know, they're somehow
12886240	12889280	they're able to act over long horizons, right? But you need that, right? That's the sort of
12889280	12895200	prerequisite to be able to do the sort of automated AI research. And so, you know, I think
12895200	12898640	there's basically, you know, I basically think sort of pre training is sort of alignment neutral
12898640	12901920	in the sense of like, it has all these representations that has good representations
12901920	12906480	that you know, as representations of doing bad things, you know, but but there's, there's,
12906480	12911040	it's not like, you know, scheming against you or whatever. I think the sort of misalignment can
12911040	12915360	arise once you're doing more kind of long horizon training, right? And so you're training, you know,
12915360	12918960	again, two simplified example, but to kind of illustrate, you know, you're training an AI to
12918960	12923920	make money. And, you know, if you're just doing that with reinforcement learning, you know, it's,
12923920	12928960	you know, it might learn to commit fraud or lie or to see you for seek power, simply because those
12928960	12932320	are successful strategies in the real world, right? So maybe, you know, RL is basically it
12932320	12936560	explores, maybe it figures out like, oh, it tries to like hack, and then it gets some money and that
12936560	12939680	made more money, you know, and then if that's successful, if that gets reward, that's just
12939680	12943440	reinforced. So basically, I think there's sort of more serious misalignments, kind of like
12943440	12948320	misaligned long term goals that could arise between now and or that sort of necessarily
12948320	12951760	have to be able to arise if you're able to get long horizon system, that's one.
12952880	12956560	What you want to do in that situation is you want to add side constraints, right? So you want to add,
12956560	12961920	you know, don't lie, don't deceive, don't commit fraud. And so how do you add those side constraints,
12961920	12965520	right? The sort of basic idea you might have is like RLHF, right? You're kind of like, yeah,
12965520	12969120	it has this goal of like, you know, make money or whatever, but you're watching what it's doing,
12969120	12972320	it starts trying to like, you know, lie or deceive or fraud or whatever,
12972320	12975600	break the law, you're just kind of like, thumbs down, don't do that, you anti reinforce that.
12976480	12980160	The sort of critical issue that comes in is that these eye systems are getting superhuman,
12980160	12983920	right? And they're going to be able to do things that are too complex for humans to evaluate,
12983920	12988000	right? So again, even early on, you know, in the intelligence explosion, the automated AI
12988000	12991200	researchers and engineers, you know, they might write millions, you know, billions,
12991200	12994480	trillions of lines of complicated code, you know, they might be doing all sorts of stuff,
12994480	12999040	you just like, don't understand anymore. And so, you know, in the million lines of code,
12999040	13002560	you know, is it somewhere kind of like, you know, hacking, hacking, or like exultating itself,
13002560	13005920	or like, you know, trying to go for the nukes or whatever, you know, like, you don't know anymore,
13005920	13010400	right? And so this sort of like, thumbs up, thumbs down, pure RLHF doesn't fully work anymore.
13011200	13015280	Second part of the picture, and maybe talk more about this first part of the picture,
13015280	13017920	I think it's going to be like, there's a hard technical problem of what do you do,
13017920	13021280	sort of post RLHF, but I think it's a solvable problem. And it's like, you know,
13021280	13024720	there's various things in bullish on, I think there's like ways in which deep learning has
13024720	13028720	shaped out favorably. The second part of the problem is you're going from your like initial
13028720	13031920	systems and intelligence explosion to like super intelligence, and you know, it's like,
13031920	13036400	many ooms, it ends up being like, by the end of it, you have a thing that's vastly smarter than humans.
13038000	13041760	I think the intelligence explosion is really scary from an alignment point of view,
13041760	13045040	because basically, if you have this rapid intelligence explosion, you know, less than a
13045040	13048720	year or two years or whatever, you're going say in the period of a year from systems where like,
13048720	13052640	you know, failure would be bad, but it's not kind of strapped back to like, you know, saying a bad
13052640	13058320	word, it's like, you know, it's, it's something goes awry to like, you know, failure is like,
13058320	13061920	you know, it extra traded itself, it starts hacking the military can do really bad things.
13061920	13064960	You're going less than a year from sort of a world in which like, you know,
13064960	13068560	it's some descendant of current systems, and you kind of understand it, and it's like, you know,
13068560	13071840	it has good properties. There's something that potentially has a very sort of alien and different
13071840	13076160	architecture, right? After having gone through another decade of imal advances. I think one
13076160	13081280	example there that's very salient to me is legible and faithful chain of thought, right? So a lot
13081280	13084160	of the time when we're talking about these things, we're talking about, you know, it has tokens of
13084160	13088880	thinking, and then it uses many tokens of thinking. And, you know, maybe we bootstrap ourselves by,
13088880	13092320	you know, it's pre-trained, it learns to think in English, and we do something else on top,
13092320	13098240	so it can do the sort of longer chains of thought. And so, you know, it's very plausible to me that
13098320	13101600	like, for the initial automated alignment researchers, you know, we don't need to do any
13101600	13105600	complicated mechanistic interpretability, and just like literally you read what they're thinking,
13105600	13112960	which is great. You know, it's like huge advantage, right? However, I'm very likely not the most
13112960	13116800	efficient way to do it, right? There's like probably some way to have a recurrent architecture.
13116800	13119840	It's all internal states. There's a much more efficient way to do it. That's what you get
13119840	13124880	by the end of the year. You know, you're going this year from like RLHF plus plus some extension
13124880	13131200	works to like, it's vastly superhuman. It's like, you know, it's to us like, you know, an expert
13131200	13135440	in the field might be to like an elementary school or middle schooler. And so, you know,
13135440	13139920	I think it's this sort of incredibly sort of like, hairy period for alignment.
13140960	13145040	Thing you do have is you have the automated AI researchers, right? And so, you can use the
13145040	13151440	automated AI researchers to also do alignment. And so, in this world, why are we optimistic
13151440	13158160	that the project is being run by people who are thinking, I think, so here's something to think
13158160	13166240	about. The open AI starts off with people who are very explicitly thinking about exactly these
13166240	13171760	kinds of things, right? But are they still there? No, no, but you're still here. Here's the thing.
13171760	13175520	No, no, even the people who are there, even like the current leadership is like exactly these things
13175520	13181840	that can find them in interviews in their blog posts talking about. And what happens is when,
13181840	13186880	as you were talking about, when some sort of trivial, and Yon talked about it, this is not just
13186880	13193440	you, Yon talked about in his tweet thread, when there is some trade off that has to be made with
13193440	13196800	we need to do this flashy release this week and not next week, because whatever Google
13196800	13200960	IO is the next week, so we're going to get it. And then the trade off is made in favor of
13201680	13210880	the less the more careless decision. When we have the government or the national security advisor,
13210880	13215520	the military or whatever, which is much less familiar with this kind of discourse is a
13215520	13219360	naturally thinking in this way about how I'm worried the chain of thought isn't faithful and
13219360	13223760	how do we think about the features that are represented here? Why should it be optimistic
13223760	13229200	that a project run by people like that will be thoughtful about these kinds of considerations?
13229440	13242640	I mean, they might not be. I agree. I think a few thoughts, right? First of all, I think the private
13242640	13246800	world, even if they sort of nominally care is extremely tough for alignment, a couple of reasons.
13246800	13250240	One, you just have the race between the sort of commercial labs, right? And it's like, you don't
13250240	13253600	have any headroom there to like be like, actually, we're going to hold back for three months, like
13253600	13257600	get this right. And we're going to dedicate 90% of our compute to automated alignment research
13257600	13262080	instead of just like pushing the next zoom. The other thing, though, is like in the private world,
13262080	13265280	you know, China has stolen your age, China has your secrets, they're right on your tails,
13265280	13270560	you're in this fever struggle, no room at all for maneuver. They're like the way it's like
13270560	13274080	absolutely essential to get alignment right and you get it during this intelligence explosion,
13274080	13277040	you get it right, because you need to have that room to maneuver and you need to have that clear
13277040	13283680	lead. And, you know, again, maybe you've made the deal or whatever, but I think you're an incredibly
13283680	13288320	tough space, tough spot if you don't have this clearly. So I think the sort of private world
13288320	13291680	is kind of rough there on like whether people will take it seriously, you know, I don't know,
13291680	13296480	I have some faith in sort of sort of normal mechanisms of a liberal society, sort of if
13296480	13300080	alignment is an issue, which, you know, we don't fully know yet, but sort of the science will
13300080	13304000	develop, we're going to get better measurements of alignment, you know, and the case will be
13304000	13308880	clear and obvious. I worry that there's, you know, I worry about worlds where evidence is
13308880	13313200	ambiguous. And I think a lot of a lot of the most scary kind of intelligence explosion scenarios are
13313200	13317760	worlds in which evidence is ambiguous. But again, it's sort of like, I, if evidence is ambiguous,
13317760	13321040	then that's the world in which you really want the safety margins. And that's also the world in
13321040	13324240	which kind of like running the intelligence explosion is sort of like, you know, running a war,
13324240	13328320	right? It's like, the evidence is ambiguous, we have to make these really tough trade offs.
13328320	13331680	And you like, you better have a really good chain of command for that. And it's not just like, you
13331680	13336160	know, you're going at, well, let's go, you know, it's cool. Yeah. Let's talk a little bit about
13336160	13342640	Germany. We're making the analogy to World War Two. And you made a really interesting point,
13342640	13357360	many hours ago. The fact that throughout history, World War Two is not unique, at least when you
13357360	13366480	think in proportion to the size of the population. But these other sorts of catastrophes where
13366480	13372480	some significant portion of the population has been killed off. After that, the nation recovers
13372560	13378560	and they get back to their heights. And so what's interesting after World War Two
13379200	13383600	is that Germany especially, and maybe Europe as a whole, obviously they experienced
13383600	13389600	fast economic growth in the direct aftermath because of catch-up growth. But subsequently,
13389600	13395040	we just don't think of Germany as, we're not talking about Germany potentially launching an
13395040	13398560	intelligence explosion and they're going to get into the AI table. We were talking about
13398560	13401440	Iran and North Korea and Russia. We didn't talk about Germany, right?
13401440	13402640	Well, because they're allies.
13402640	13409040	Yeah. But so what happened? I mean, World War Two and now it didn't like come back
13409040	13410720	over the seven years war or something, right?
13410720	13414320	Yeah. Yeah. Yeah. I mean, look, I'm generally very bearish on Germany. I think in this context,
13414320	13417040	I'm kind of like, you know, it's a little bit, you know, I think you're underrating a little bit.
13417040	13420160	I think it's probably still one of the, you know, top five most important countries in the world.
13421200	13425200	You know, I mean, Europe overall, you know, it still has, I mean, it's a GDP that's like
13425200	13430480	close to the United States the size of the GDP, you know, and there's things actually that Germany
13430480	13435040	is kind of good at, right? Like state capacity, right? Like, you know, the, you know, the roads
13435040	13439520	are good and they're clean and they're well maintained and, you know, in some sense, the sort
13439520	13443520	of, a lot of this is the sort of flip side of things that I think are bad about Germany, right?
13443520	13446560	So in the US, it's a little bit like there's a bit more of a sort of Wild West feeling to the
13446560	13451600	United States, right? And it includes the kind of like crazy bursts of creativity. It includes like,
13451600	13456960	you know, political candidates that are sort of, you know, there's a much broader spectrum and,
13456960	13460480	you know, much, you know, like both in Obama and Trump as somebody you just wouldn't see in the sort
13460480	13464800	of much more confined kind of German political debate. You know, I wrote this blog post at some
13464800	13469680	point, Europe's political stupor about this. But anyway, and so there's this sort of punctilious
13469680	13473440	sort of rule following that is like good in terms of like, you know, keeping your kind of state
13473440	13481120	capacity functioning. But that is also, you know, I think I kind of think there's a sort of very
13481120	13486320	constrained view of the world in some sense. You know, and that includes kind of, you know,
13486320	13490080	I think after World War Two, there's a real backlash against anything like elite, you know,
13490080	13495920	and, you know, again, no, you know, no elite high schools or elite colleges and sort of
13495920	13497600	what my is that the law?
13497600	13499680	excellence isn't cherished, you know, there's a yeah.
13499680	13506800	Why is that the logical intellectual thing to rebel against if what if you're trying to
13506800	13510560	overcorrect from the Nazis? Yeah, was it because the Nazis were very much into elitism?
13511200	13514240	I don't understand why that's a logical sort of counter reaction.
13514240	13517840	I know, maybe it was sort of a counter reaction against the sort of like whole like Aryan race
13517840	13521520	and sort of that sort of thing. I mean, I also just think there was a certain amount in what
13521520	13526240	amount certain, I mean, look at sort of World War One, end of World War One versus end of World
13526240	13531280	War Two for Germany, right? And sort of, you know, a common narrative is that the piece of Versailles,
13531280	13536160	you know, was too strict on Germany. You know, the piece imposed after World War Two was like
13536160	13540160	much more strict, right? It was a complete, you know, the whole country was destroyed,
13540160	13543920	you know, it was, you know, and all the main, most of the major cities, you know, over half of
13543920	13548320	the housing stock had been destroyed, right? Like, you know, in some birth cohorts, you know,
13548320	13552960	like 40% of the men had died. Half the population displaced. Oh, yeah. I mean,
13552960	13557360	almost 20 million people are displaced, huge, crazy, right? You know, like,
13557360	13561440	and the borders are way smaller than the Versailles borders. Yeah, exactly. And sort of
13561440	13566480	complete imposition of a new political system and, and, you know, on both sides, you know, and
13566480	13573840	yeah, so it was, but in some sense that worked out better than the post-World War One piece,
13574400	13578240	where then there was this kind of resurgence of German nationalism and, you know, in some sense,
13578240	13581120	the thing that has been a pattern. So it's sort of like, it's unclear if you want to wake the
13581120	13585280	sleeping beast. I do think that at this point, you know, it's gotten a bit too sleepy. Yeah.
13587440	13590560	I do think it's an interesting point about we underrate the American political system. Yeah.
13590640	13596080	I've been making the same correction myself. Yeah. There's, there was this book about
13596080	13601840	burdened by a Chinese economist called China's World View. And overall, I wasn't a big fan,
13601840	13608480	but they made a really interesting point in there, which was the way in which candidates rise up
13608480	13616080	through the Chinese hierarchy for politics, for administration, in some sense, that selects for
13616080	13618960	you're not going to get some Marjorie Taylor Greene or somebody running some.
13620160	13624560	Don't get that in Germany either. Right. Yeah. But you're, he explicitly made the point in the
13624560	13628320	book that that also means we're never going to get a Henry Kissinger or Barack Obama. Right.
13628320	13633280	In China. We're going to get like, by the time they end up in charge of the, the Politburo,
13633280	13636960	the Politburo, there'll be like some 60 year old Democrat who's never like ruffled any feathers.
13636960	13639920	Yeah. Yeah. Yeah. I mean, I think, I think there's something really important about the sort of
13639920	13644800	like very raucous political debate. And I mean, yeah, in general, kind of like, you know, there's
13644800	13648320	the sense in which in America, you know, lots of people live in their kind of like own world.
13648320	13652560	I mean, like we live in this kind of bizarre little like bubble in San Francisco and people,
13653360	13658480	you know, and, and, but I think that's important for the sort of evolution of ideas,
13658480	13663280	error correction and that sort of thing. You know, there's other ways in which the
13663280	13667920	German system is more functional. Yeah. But it's interesting that there's major mistakes,
13667920	13670800	right? Like the sort of defense spending, right? And you know, then, you know, Russia
13670800	13675440	made Ukraine and, and you're like, wow, what did we do? Right? No, that's a really good point,
13675440	13681840	right? The main issues, there's everybody agrees, but exactly. Yeah. So consensus blob kind of thing.
13681840	13685920	Right. And on the China point, you know, just having this experience of like reading German
13685920	13689600	newspapers, and I think how much, you know, how much more poorly I would understand the sort of
13689600	13695520	German debate and sort of the sort of state of mind from just kind of afar. I worry a lot about,
13695520	13700480	you know, or I think it is interesting just how kind of impenetrable China is to me.
13700800	13704640	It's a billion people, right? And like, you know, almost everything else is really globalized.
13704640	13708480	You have a globalized internet and I kind of, I kind of a sense what's happening in the UK.
13708480	13711040	You know, I probably, even if I didn't read German newspapers, just sort of would have a
13711040	13715840	sense of what's happening in Germany. But I really don't feel like I have a sense of what like,
13717120	13720960	you know, what is the state of mind or what are the state of political debate, you know,
13720960	13725520	of a sort of average Chinese person or like an average Chinese leader. And yeah, I think that,
13725520	13729680	that I find that distance kind of worrying. And I, you know, and there's, you know, there's
13729680	13732880	some people who do this and they do really great work where they kind of go through the like party
13732880	13737360	documents and the party speeches. And it seems to require a kind of a lot of interpretive ability
13737360	13741120	where there's like very specific words and mentoring that like mean we'll have one connotation,
13741120	13745840	not the other connotation. But yeah, I think it's sort of interesting given how globalized
13745840	13749520	everything is. And like, I mean, now we have basically perfect translation machines and it's
13749520	13754960	still so impenetrable. That's really interesting. I've been, I should, I'm sort of ashamed almost
13754960	13760240	that I haven't done this yet. I think many months ago, when Alexi interviewed me on his
13760240	13764400	YouTube channel, I said, I'm meaning to go to China to actually see for myself what's going on.
13764400	13769520	And actually I'm, I should, so by the way, if anybody listening has a lot of context on China,
13769520	13772560	if I went to China, who could introduce me to people, please email me.
13773120	13776320	You got to do some pods and you got to find some of the Chinese AI researchers, man.
13776320	13780160	I know. I was thinking at some point, again, this is the fact that I'm...
13780160	13783360	Can I speak freely, but you know, I don't know if they can speak freely, but...
13783360	13787360	I was thinking of there's, so they had these papers and on the paper, they'll say who's a
13787360	13793360	co-author. It's funny because, well, I was thinking of just emailing, cold emailing everybody,
13793360	13796800	like, here's my calendar. Let's just talk. I just want to see what is the vibe. Even
13796800	13800000	they don't tell me anything. I'm just like, what kind of person is this? How westernized are they?
13801200	13808080	But as I was saying this, I just remembered that in fact, by Dan's, according to mutual
13808080	13813520	friends we have at Google, they cold emailed every single person on the Gemini paper and said,
13813520	13818640	if you come work for By Dan's, we'll make you an allied engineer, you'll report directly to the CTO,
13818640	13822240	and in fact, this actually... That's how the secrets go over, right?
13822240	13826560	Right. No, I meant to ask this earlier, but suppose they hired what...
13826560	13830160	If there's only a hundred or so people, or maybe less, we're working on the key algorithmic secrets.
13830960	13834720	If they hired one such person, is all the alpha gone that these labs have?
13835360	13839040	If this person was intentional about it, they could get a lot. I mean, they couldn't get the sort
13839040	13842000	of like... I mean, actually, you could probably just also exfiltrate the code. They could get a
13842000	13846240	lot of the key ideas. Again, up until recently stuff was published, but they could get a lot
13846240	13850240	of the key ideas if they tried. I think there's a lot of people who don't actually look around
13850240	13856160	to see what the other teams are doing, but I think you can. But yeah, I mean, they could. It's scary.
13856880	13861200	Right. I think the project makes more sense there where you can't just recruit a Manhattan
13861200	13865920	project engineer and then just get... I mean, these are secrets that can be used for like
13865920	13869520	probably every training around the future that'll be like, maybe are the key to the data wall that
13869520	13874000	are like, they can't go on or they can't go on that are like, they're going to be worth giving
13874000	13877120	sort of like the multipliers on compute, hundreds of billions, trillions of dollars,
13877920	13881680	and all it takes is China to offer a hundred million dollars to somebody and be like,
13881680	13888160	yeah, come work for us. And then... I mean, yeah, I'm really uncertain on how
13888160	13893680	sort of seriously China is taking AGI right now. One anecdote that was relate to me on
13893680	13897920	the topic of the anecdotes, by another sort of like kind of researcher in the field was
13897920	13901520	at some point they were at a conference with somebody, Chinese AI researcher,
13901520	13904640	and he was talking to him and he was like, I think it's really good that you're here and like,
13904640	13909920	we got to have the international coordination stuff. And apparently this guy said that I'm the
13909920	13913840	kind of most senior most person that they're going to let leave the country to come to things like
13913840	13919520	this. Wait, what's the takeaway? As in they're not letting really senior
13919520	13923600	AI researchers leave the country. Interesting. Kind of classic, you know, Eastern Block move.
13923600	13929040	Yeah. I don't know if this is true, but it's what I heard. It's interesting. So I thought the point
13929040	13934800	you made earlier about being exposed to German newspapers and also to, because earlier you
13934800	13940960	were interested in economics and law and national security, you have the variety in intellectual
13940960	13944800	diet there has exposed you to thinking about the geopolitical question here and why is others
13945360	13948240	talking about AI. I mean, this is the first episode I've done about this where we've talked
13948240	13952080	about things like this, which is now that I think about it weird to give that this is an obvious
13952080	13957040	thing in retrospect, I should have been thinking about. Anyways, so that's one thing we've been
13957040	13962000	missing. What are you missing? And national security you're thinking about so you can't say
13962000	13967520	national security. What like perspective are you probably underexposed to as a result?
13967520	13970720	And China, I guess you mentioned. Yeah. So I think the China one is an important one.
13972480	13975760	I mean, I think another one would be a sort of very Tyler Cowan-esque take, which is like,
13975760	13980000	you're not exposed to how, like, how will a normal person in America, like, you know,
13980000	13985040	both like use AI, you know, probably not, you know, and that being kind of like bottlenecks
13985040	13988720	to the fusion of these things. I'm overrating the revenue because I'm kind of like, ah, you know,
13988720	13992640	everyone has to stop adopting it, but you know, kind of like, you know, Joe Schmo engineer at a
13992640	13996800	company, you know, like, ah, will they, will they be able to integrate it? And also the reaction
13996800	14000480	to it, right? You know, I mean, I think this was a question again, hours ago, where it was
14001920	14006640	about like, you know, won't people kind of rebel against this? Yeah. And they won't want to do the
14006640	14012480	project. I don't know, maybe they will. Yeah. Here's a political reaction that I didn't anticipate.
14012480	14017520	Yeah. So Tucker Carlson was recently on the Joe Rogan episode. I already told you about this
14017520	14023680	part. I'm just gonna tell the story again. So Tucker Carlson is on Joe Rogan. Yeah. And they
14023680	14029520	start talking about World War II and Tucker says, well, listen, I'm going to say something that my
14029520	14034640	fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously
14034640	14040880	immoral that we use them on Nagasaki and Hiroshima. And then he says, in fact, nuclear weapons are
14040880	14047280	always immoral, except when we would use them on data centers. In fact, it would be immoral not to
14047280	14051760	use them on data centers because look, we're, DC people in Silicon Valley, these fucking nerds
14051760	14058720	are making super intelligent. And they say that it could enslave humanity. We made machines to
14058720	14064080	serve humanity, not to enslave humanity. And they're just going on and making these machines.
14064080	14072640	And so we should of course be nuking the data centers. And that is definitely not a political
14072640	14079120	reaction in 2024. I was expecting. I mean, who knows? It's gonna be crazy. It's gonna be crazy.
14079200	14084720	The thing we learned with COVID is that also the left, right reactions that you would anticipate
14084720	14090080	just based on hunches, it completely flipped. Initially, like kind of the right is like, you
14090080	14093920	know, it's like so contingent. And then, and then, and then, and the right left was like, this is
14093920	14098000	racist. And then it flipped, you know, the left was really into the code. Yeah. And the whole
14098000	14102720	thing also is just like so blunt and crude. And so, yeah, I think, I think probably in general,
14102720	14106320	you know, I think people are really under, you know, people like to make sort of complicated
14106400	14111040	technocratic AI policy proposals. And I think, especially if things go kind of fairly rapidly
14111040	14116800	on the last AGI, you know, there might not actually be that much space for kind of like
14116800	14120640	complicated kind of like, you know, clever proposals that might just be kind of much
14120640	14127120	cruder reactions. Yeah. Look, and then also when you mentioned the spies and national security
14127120	14132880	getting involved and everything, and you can talk about that in the abstract, but now that we're
14132880	14136640	living in San Francisco and we know many of the people who are doing the top AI research
14137680	14140560	is also a little scary to think about people I personally know and friends with.
14141520	14146240	It's not unfeasible if they have secrets in their head that are worth $100 billion or something,
14146240	14151280	kidnapping, assassination, sabotage. It's scary. Oh, they're family, or yeah, it's really bad.
14151280	14154160	Yeah, yeah. I mean, this is to the point on security, you know, like right now, it's just
14154160	14160160	really foreign. But, you know, at some point, as it becomes like really serious, it's, you know,
14160160	14166960	you're going to want the security cards. Yeah. Yeah. So presumably you have thought about the
14166960	14171200	fact that people in China will be listening to this and will be reading your series. Yeah.
14171840	14180320	And somehow you made the trade off that it's better to let the whole world know. Yeah. And
14180320	14184640	also including China and make them up to AGI, which is part of the thing you're worried about
14184640	14190480	is China making up to AGI than to stay silent. Yeah. I'm just curious, walk me through how you've
14190480	14194640	thought about that trade off. Yeah, I actually, look, I think this is a tough trade off. I thought
14194640	14199120	about this a bunch, you know, I think, you know, I think people on the PRC will read this.
14204880	14208320	I think, you know, I think there's some extent to which sort of cat is out of the bag, you know,
14208320	14212160	this is like not, you know, AGI being a thing people are thinking about very seriously is not
14212160	14215520	new anymore. There's sort of, you know, a lot of these takes are kind of old or, you know, I've had,
14215520	14219760	I had, you know, similar views a year ago, might not have written it up a year ago, in part because
14219760	14224640	I think this cat wasn't out of the bag enough. You know, I think the other thing is
14228160	14232960	I think to be able to manage this challenge, you know, I think much broader swaths in society
14232960	14235920	will need to wake up, right? And if we're going to get the project, you know, we actually need
14236000	14240560	sort of like, you know, abroad by partisan understanding, the challenges facing us. And
14241440	14245920	so, you know, I think it's a tough trade off, but I think the sort of need to wake up people in the
14245920	14251680	United States in the sort of Western world and the Democratic coalition is ultimately imperative.
14251680	14255360	And, you know, I think my hope is more people here will read it than the PRC.
14257600	14260880	You know, and I think people sometimes underrate the importance of just kind of like writing it
14261360	14266160	laying out the strategic picture. And, you know, I think you've done actually a great service to
14266160	14274000	sort of mankind in some sense by, you know, with your podcast. And, you know, I think it's overall
14274000	14278880	been good. Okay, so by the way, you know, on the topic of, you know, Germany, you know, we were
14278880	14282720	talking at some point about kind of immigration story, right? Like you have a kind of interesting
14282720	14289040	story you haven't told. And I think you should tell. So a couple of years ago, I was in college and
14289360	14295760	I was 20. Yeah, I was about to turn 21. Yeah, I think it was, yeah, you came from India when you
14295760	14301760	were really right. Yeah. So I was eight or eight or nine. I lived in India and then we moved around
14301760	14308240	all over the place. But because of the backlog for Indians, the green card backlog, yeah, it's
14309680	14314080	we were we've been in the queue for like decades, even though you came at eight, you're still on
14314080	14319600	the H1B. Yeah. And when you're 21, you get kicked off the queue and you had to restart the process.
14319600	14323360	So I'm on my dad's, my dad's a doctor and I'm on his H1B as it depended. But when you're 21,
14323360	14328240	you get kicked off. Yeah. And so I'm 20 and I just like kind of dawns on me that this is my situation.
14328240	14332640	Yeah. And you're completely screwed. Right. And so I also had experience that my dad,
14333200	14337680	yeah, we've like moved all around the country. They have to prove that him as a doctor is like,
14337680	14343040	you can't get native talent. Yeah. And you can't start up. Yeah. So where can you not get like
14343040	14347920	even getting the H1B for you would have been like 20% lottery. So if you're lucky, you're in this
14347920	14350240	time. And they had to prove that they can't get native talent, which means like for him,
14350240	14354000	I'm like, we lived in North Dakota for three years, West Virginia for three years, Maryland,
14354000	14359040	West Texas. Yeah. And so kind of dawn on me, this is my situation. Is that turn 21, I'll be like
14359600	14363760	on this lottery, even if I get the lottery, I'll be a fucking code monkey for the rest of my life
14363760	14369040	because this thing isn't going to let up. Yeah. Can't do a startup. Exactly. And so at the same time,
14369120	14372640	I had been reading for the last year, I've been super obsessed with Paul Graham essays.
14373360	14377600	My plan at the time was to make a startup or something. I was super excited about that.
14377600	14381840	And it just occurred to me that I couldn't do this. Yeah. That like, this is just not in the
14381840	14388800	cars for me. Yeah. And so I was kind of depressed about it. I remember I kind of just, I was in
14388800	14393200	a daze through finals, because I had like, it just occurred to me and I was really like
14394160	14401360	anxious about it. Yeah. And I remember thinking to myself at the time that if somehow I end up
14401360	14406400	getting my green card before I turn 21, there's no fucking way I'm turning, becoming a code monkey
14406400	14411920	because the thing that I've, like this feeling of dread that I have is this realization that
14412640	14417920	I'm just going to have to be a code monkey. And I realize that's my default path. Yeah. If I,
14417920	14421680	if I hadn't sort of made a proactive effort not to do that, I would have graduated college as a
14421680	14424800	computer science student and I would have just done that. And that's the thing I was super scared
14424800	14431200	about. Yeah. So that was an important sort of realization for me. Anyway, so COVID happened
14431200	14436640	because of that, since there weren't foreigners coming, the backlog cleared fast. And by the skin
14436640	14441440	of my teeth, like a few months before I turned 21, extremely contingent reasons, I ended up getting
14441440	14446240	a green card because I got a green card. I could, you know, the whole podcast, right? Exactly.
14446240	14451200	I graduated college and I was like bumming around and I got, it was like, I graduated
14451200	14455200	just a semester early. I'm going to like do this podcast, see what happens. And it was,
14455200	14461200	it hadn't, it didn't have a green card. And it only existed because, yeah, it's actually,
14461200	14465360	because I think it's hard. It's probably, it's, you know, what is the impact of like immigration
14465360	14469040	reform? Like what is the impact of clearing, you know, like whatever 50,000 green cards in
14469040	14473920	the backlog? And you're such like an amazing example of like, you know, all of this is only
14474000	14478960	possible. And it's, yeah, it's, I mean, it's just incredibly tragic that this is so dysfunctional.
14478960	14484560	Yeah, yeah, yeah. No, it's insane. I'm glad you did it. I'm glad you kind of like, you know,
14484560	14489600	tried the, you know, the, the, the unusual path. Well, yeah, but I could only do it.
14490240	14494880	Obviously, I was extremely fortunate that I got the green card. I was like,
14496000	14500640	I had a little bit of saved up money and I got a small grant out of college. Thanks to the
14501280	14505040	future fund to like do this for basically the equivalent of six months.
14505040	14510560	And so it turned out really well. And then at each time when I was like, oh, okay, podcast,
14510560	14515280	come on, like, I wasted a few months on this, let's now go do something real. Something big would
14515280	14520880	happen. I would, Jeff Bezos would, huh? You kept with it. Yeah. Yeah. But there would always be
14520880	14524160	just like the moment I'm about to quit the podcast, something like Jeff Bezos will say
14524160	14528080	there's something nice about me on Twitter. The daily episodes gets like half a million views,
14528080	14531840	you know, and then now this is my career, but it was a sort of very,
14532880	14536080	looking back on it, incredibly contingent that things worked out the right way.
14536080	14539440	Yeah. I mean, look, if, if the AGI stuff goes down, you know, it'll be,
14540160	14543920	it'll be the most important kind of like, you know, source of, it'll be how,
14543920	14548640	maybe most of the people who kind of end up feeling the AGI are sure about it.
14548640	14553600	Yeah. Yeah. Yeah. Yeah. Also very much, you're very linked with the story in many ways. First,
14554160	14560560	the, I got like a $20,000 grant from a future fund right out of college. Yeah.
14560560	14566080	And that sustained me for six months or however long it was. Yeah. And without that,
14566080	14568480	I wouldn't, it was kind of crazy. Yeah. 10 grand or what was it?
14568480	14573120	It was, no, it just, it's tiny, but you know, it goes to show kind of how small grants can go.
14573120	14577680	Yeah. It's sort of the immersion ventures too. Yeah. Exactly. The immersion ventures and the,
14578800	14581920	well, the last year I've been in San Francisco, we've just been
14582160	14586640	in close contact the entire time and just bouncing ideas back and forth.
14586640	14591520	We're just basically the alpha I have, I think people would be surprised by how much I got from
14591520	14596080	you, Sholto, Trent and a couple others. I mean, it's been, it's been an absolute pleasure.
14596080	14598240	Yeah. Likewise. Likewise. It's been super fun. Yeah.
14599840	14604080	Okay. So some random questions for you. Yeah. If you could convert to Mormonism.
14604080	14607120	Yeah. And you could really believe it. Yeah. Would you do it? Would you push the button?
14607680	14613200	Yeah. Well, okay. Okay. Before I answer that question, one sort of observation about the
14613200	14616480	Mormons. So there's actually, there's an article that actually made a big impact on me.
14616480	14619360	Yeah. I think it was by McKick Hop and at some point, you know, in the Atlantic or whatever
14619360	14624400	about the Mormons. And I think the thing he kind of, you know, and I think he even was like
14624400	14627840	interviewed Romney and so on. And I think the thing I thought was really interesting in this
14627840	14632320	article was he kind of talked about how the experience of kind of growing up different,
14632320	14635360	you know, growing up very unusual, especially if you grow up Mormon outside of Utah, you know,
14635360	14638960	like the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird.
14640080	14645360	How that kind of got people prepared for being willing to be kind of outside of the norm later
14645360	14649120	on. And like, you know, Romney, you know, was willing to kind of take stands alone, you know,
14649120	14653840	in his party, because he believed, you know, what he believed is true. And I don't, I mean,
14653840	14656880	probably not to the same way, but I feel a little bit like this from kind of having grown up in
14656880	14660560	Germany, you know, and really not having like this sort of German system and having been kind
14660560	14664880	of an outsider or something. I think there's a certain amount in which kind of, yeah, growing
14664880	14669680	up in an outsider gives you kind of unusual strength later on to be kind of like willing to
14669680	14674400	say what you think. And so that is one thing I really appreciate about the Mormons, at least the
14674400	14677680	ones that grew up outside of Utah. I think, you know, the fertility rates, they're good, they're
14677680	14682080	important. They're going down as well, right? This is the thing that really clinched the kind of
14682080	14686400	fertility decline story for you. Even the Mormons. Yeah, even the Mormons, right? You're like, oh,
14686400	14689520	this is like a good sort of good story. The Mormons will replace everybody. Well, no, I don't know
14689520	14692400	if it's good, but it's like, at least, you know, at least come on, you know, like at least some
14692400	14695600	people will maintain high, you know, but it's no, no, you know, even the Mormons and sort of
14695600	14699360	basically, once these religious subgroups have high fertility rates, once they kind of grow big
14699360	14703840	enough, they become, they're too close in contact with sort of normal society and become normalized,
14703840	14708000	Mormon fertility rates drop from, I don't remember the exact numbers, maybe like four to two in the
14708000	14712560	course of 10, 20 years. Anyway, so it's like, you know, now people point to the Amish or whatever,
14712560	14716000	but I'm just like, it's probably just not scalable. And if you grow big enough, then there's just like,
14716000	14720080	you know, the sort of like, you know, the sort of like overwhelming force of modernity kind of gets
14720080	14726080	you. Yeah. No, if I could convert to Mormonism, look, I think there's something, I don't believe
14726080	14729280	it, right? If I believed it, I obviously would convert to Mormonism, right? Because it's, you
14729280	14731840	gotta, you gotta, but you can choose a world in which you do believe it.
14737200	14740640	I think there's something really valuable and kind of believing in something greater than
14740640	14748560	yourself and believing and having a certain amount of faith. You do, right? And you know,
14749280	14754000	you know, there's a, you know, feeling some sort of duty to the thing greater than yourself.
14754000	14757920	Yeah. And you know, maybe my version of this is somewhat different. You know, I think I feel
14757920	14761920	some sort of duty to like, I feel like there's some sort of historical weight on like how this
14761920	14765600	might play out. And I feel some sort of duty to like make that go well. I feel some sort of duty
14765600	14773600	to, you know, our country, to the national security of the United States. And, you know,
14773600	14776720	I think, I think that, I think it can be a force for a lot of good.
14777280	14780720	I, the, going back to the opening, I think just,
14784320	14790800	the thing that's especially impressive about that is, look, there's people who, at the company,
14790800	14797600	who have through years and decades of building up savings from working in tech have probably
14797600	14804640	10 civilians, liquid, more than that in terms of their equity. And the person, very many people
14804640	14811040	were concerned about the clusters and the Middle East and the secrets leaking to China and all
14811040	14817120	these things. But the person who actually made a hassle about it, and I think hassling people is
14817120	14823760	so underrated, I think that one person who made a hassle about it is the 22 year old who has less
14823840	14830240	than a year at the company who doesn't have savings built up. Who isn't like a solidified member of the,
14831760	14836080	I think that's a sort of like, maybe, maybe it's me being naive and, you know, not knowing how big
14836080	14840480	companies work. And, you know, but like, there's a, you know, I think sometimes a bit of a speech
14840480	14844560	deontologist, you know, I kind of believe in saying what you think. Sometimes friends tell me I
14844560	14853520	should be more of a speech consequentialist. No, I think I really think the amount of people who
14853600	14858880	when they have the opportunity to talk to the person will just bring up the thing. I've been
14858880	14861760	with you in multiple contexts, and I guess I shouldn't reveal who the person is or what the
14861760	14867360	context was, but I've just been like very impressed that the dinner begins and by the end, somebody
14867360	14874080	who has a major voice in how things go is seriously thinking about a worldview they would have found
14874080	14879920	incredibly alien before the dinner or something. And I've been impressed that like, just like,
14880480	14887520	give them the spiel and hassle them. I mean, look, I just, I think, I think I feel this stuff
14887520	14890800	pretty viscerally now. You know, I think there's a time, you know, there's a time when I thought
14890800	14894720	about the stuff a lot, but it was kind of like econ models and like, you know, kind of like these
14894720	14898240	sort of theoretical abstractions and, you know, you talk about human brain size or whatever.
14898240	14903760	Right. And I think, you know, since, I think since at least last year, you know, I feel like,
14903760	14909120	you know, I feel like I can see it, you know, and I just, I feel it. And I think I can like,
14909120	14913920	you know, I can sort of see the cluster that I can see the kind of rough combination of algorithms
14913920	14919120	and the people that be involved and how this is going to play out. And, you know, I think,
14919120	14922240	look, we'll see how it plays out. There's many ways this could be wrong. There's many ways it
14922240	14928400	could go. But I think this could get very real. Yeah. Should we talk about what you're up to next?
14928400	14933040	Sure. Yeah. Okay. So you're starting an investment firm anchor investments from
14933120	14936560	Nat Friedman, Daniel Gross, Patrick Lawson, John Collison.
14938640	14943120	First of all, why is this thing to do? You believe the AGI is coming in a few years?
14944800	14949520	Why the investment firm? A good question, fair question. Wait, so I mean, a couple things. One
14949520	14952720	is just, you know, I think we talked about this earlier, but it's like the screen doesn't go blank,
14952720	14955600	you know, when sort of AGI is different intelligence happens, I think people really
14955600	14958560	underrate the sort of basically the sort of decade after it, you have the intelligence
14958560	14962560	explosion, that's maybe the most sort of wild period. But I think the decade after is also
14962560	14966720	going to be wild. And you know, this combination of human institutions, but super intelligence,
14966720	14969920	you have crazy kind of geopolitical things going on, you have the sort of broadening of
14969920	14974720	this explosive growth. And basically, yeah, I think it's going to be a really important period.
14974720	14977200	I think capital will really matter, you know, eventually, you know, like, you know, going to
14977200	14981840	go to the stars, you know, going to go to the galaxies. So anyway, so part of the answer is
14981840	14985600	just like, look, I think done, done, right, there's a lot of money to be made, you know,
14985600	14989040	I think if AGI were priced in tomorrow, you could maybe make 100x, probably you can make
14989040	14995040	even way more than that because of the sequencing. And, and, and, you know, capital matters.
14996080	15001840	I think the other reason is just, you know, some amount of freedom and independence. And I think,
15001840	15006960	you know, you know, I think there's some people who are very smart about this AGI stuff and who
15006960	15010800	are kind of like see it coming. But I think almost all of them, you know, are kind of,
15010800	15014000	you know, constrained in various ways right there in the labs, you know, they're in some,
15014000	15018560	you know, some other position where they can't really talk about the stuff. And, you know,
15018560	15021680	in some sense, I've really admired sort of the thing you've done, which is I think it's really
15021680	15024880	important that there's sort of voices of reason on this stuff publicly, or people who are in
15024880	15028560	positions to kind of advise important actors and so on. And so I think there's a, you know,
15029200	15032400	basically the thing this investment firm will be, will be kind of like, you know, a brain trust
15032400	15035760	on AI, it's going to be all about situational awareness. We're going to have the best situational
15035760	15038720	awareness in the business, you know, we're going to have way more situational business than any of
15038720	15041920	the people who manage money in New York. We're definitely going to, you know, we're going to do
15041920	15046240	great on investing. But it's the same sort of situational awareness that I think is going to
15046240	15052480	be important for understanding what's happening, being a voice of reason publicly and, and, and
15052480	15059440	sort of being able to be in a position to advise. Yeah. I, there was the book about Peter Thiel.
15059440	15065120	Yeah. They had an interesting quote about his hedge fund. I think it got terrible return. So
15065680	15067840	this isn't the example. Right, right. That's the, that's the sort of
15067840	15071920	bare case, right? It's like two theoretical and sure. Yeah. But they had an interesting quote
15072000	15076640	that it's, it's, that it's like basically a think tank inside of a hedge fund.
15076640	15083200	Yeah. So we're trying to build. Right. Yeah. So presumably you've thought about the ways in which
15083200	15086880	these kinds of things can blow. There's a very, there's a lot of interesting business history
15086880	15094400	books about people who got the pieces right, but timed it wrong. Yeah. Where they, they buy that
15094400	15098240	internet's going to be a big deal. Yeah. They sell at the wrong time and buy the wrong time
15098320	15102400	in the dotcom boom. And so they miss out on the gains, even though they're right about the,
15102400	15106400	anyways, yeah. What, what is that trick to preventing that kind of thing?
15106400	15110000	Yeah. I mean, look, obviously you can't, you know, not blowing up as sort of like, you know,
15110000	15115040	task number one and two or whatever. I mean, you know, I think this investment firm, it is going
15115040	15118800	to just be betting on AGI, you know, betting on AGI and super intelligence before the decade
15118800	15122560	is out, taking that seriously, making the bets you would make, you know, if you took that seriously.
15122560	15127280	So, you know, I think if that's wrong, you know, firm is not going to do that well. The thing you
15127280	15130400	have to be resistant to is like, you have to be able to resist and get, you know, one or a couple
15130400	15134080	or a few kind of individual calls, right? You know, it's like AI stagnates for a year because of the
15134080	15138720	data wall or like, you know, you got, you got the call wrong on like when revenue would go up. And
15138720	15143040	so anyway, that's pretty critical. You have to get timing right. I do think in general that the
15143040	15146960	sort of sequence of bets on the way to AGI is actually pretty critical. And I think a thing
15146960	15152480	people underrate. So, all right. I mean, yeah. So like, where does the story start, right? So like,
15152560	15158160	obviously, the sort of only bet over the last year was NVIDIA. And, you know, it's obvious now,
15159520	15164000	very few people did it. This is sort of also, you know, a classic debate I and a friend had with
15164000	15167760	another colleague of ours, where this colleague was really into TSM, you know, TSMC. And he was
15167760	15171680	just kind of like, well, you know, like, these tabs are going to be so valuable. And also like,
15171680	15174720	NVIDIA, there's just a lot of videos in credit risk, right? It's like, maybe somebody else makes
15174720	15179680	better GPUs. That was basically right. But sort of only NVIDIA had the AI beta, right? Because
15179680	15183440	only NVIDIA was kind of like a large fraction AI. The next few doublings would just like
15183440	15187440	meaningfully explode their revenue. Whereas TSMC was, you know, a couple percent AI. So,
15187440	15190400	you know, even though there's going to be a few doublings of AI, not going to make that big of
15190400	15194720	an impact. All right. So it's sort of like, the only place to find the AI beta basically was NVIDIA
15194720	15201520	for a while. You know, now it's broadening, right? So now TSM is like, you know, 20 percent AI by
15201520	15205280	like 27 or something is what they're saying. One more doubling, it'll be kind of like a large
15205280	15208480	fraction of what they're doing. And, you know, there's a whole, you know, whole stack, you know,
15208480	15213360	there's like, you know, there's people making memory and COAS and, you know, power, you know,
15213360	15216880	utility companies are starting to get excited about AI. And they're like, oh, it'll, you know,
15217520	15221600	power production in the United States will grow, you know, not 2.5 percent, 5 percent of the next
15221600	15228560	five years. And I'm like, no, it'll grow more. You know, at some point, you know, you know,
15228560	15232080	like a Google or something becomes interesting, you know, people are excited about them with AI
15232080	15235440	because it's like, oh, you know, AI revenue will be, you know, 10 billion or tens of billions.
15235440	15238880	And I'm kind of like, ah, I don't really care about them before then. I care about it, you know,
15238880	15242560	once it, you know, once you get the AI beta, right? And so at some point, you know, Google
15242560	15246400	will get, you know, $100 billion of revenue from AI. Probably their stock will explode,
15246400	15248960	you know, they're going to become, you know, 5 trillion, 10 trillion dollar company.
15249680	15252400	Anyway, so the timing there is very important. You have to get the timing right. You have to
15252400	15255520	get the sequence right. You know, at some point, actually, I think like, you know, there's going
15255520	15260240	to be real tailwind to equities from real interest rates, right? So basically in these sort of
15260320	15264960	explosive growths worlds, you would expect real interest rates to go up a lot, both on the sort
15264960	15269520	of like, you know, a basic both sides of the equation, right? On the supply side or on the
15269520	15275280	sort of demand for money side, because, you know, people are going to be making these crazy investments,
15275280	15278240	you know, initially in clusters and then in the robo factories or whatever, right? And so they're
15278240	15283760	going to be borrowing like crazy. They want all this capital, higher AI. And then on the sort of
15283760	15288800	like consumer saving side, right, to like, you know, to give up all this capital, you know,
15288800	15292160	this sort of like Euler equation, standard sort of intratemporal transfer, you know,
15293280	15295840	trade off of consumption. So standard.
15298880	15302000	Some of our friends have a paper on this, you know, basically, if you expect, you know,
15302000	15305280	if consumers expect real growth rates to be higher, you know, interest rates are going to be
15305280	15308480	higher because they're less willing to give up consumption, you know, consumption in the
15310240	15312720	less willing to give up consumption day for consumption in the future.
15313280	15316480	Anyway, so at some point, real interest rates will go up if sort of ADA is greater than one,
15316480	15320720	that actually means equities, you know, higher growth rate expectations mean equities go down
15320720	15323200	because the sort of interest rate effect outweighs the growth rate effect.
15323840	15327200	And so, you know, at some point, there's like big, the big bond short, you got to get that right,
15327200	15330880	you got to get it right, that, you know, nationalization, you know, like, you got, you know,
15330880	15333600	anyway, so there's this whole sequence of things, you got to get that right.
15333600	15336800	And the unknown unknowns, unknown unknowns. Yeah. And so you've, look, you've got to be
15336800	15340080	really, really careful about your like overall like this positioning, right? And because, you
15340080	15343280	know, you know, if you expect these kind of crazy events to play out, there's going to be crazy
15343280	15347920	things you didn't see. You know, you do also want to make the sort of kind of bets that are tailored
15347920	15351760	to your scenarios in the sense of like, you know, you want to find bets that are bets on the tails,
15351760	15356240	right? You know, I don't think anyone is expecting, you know, interest rates to go above,
15356240	15360240	you know, 10% like real interest rates. But, you know, I think there's at least a serious chance
15360240	15364320	of that, you know, before the decade is out. And so, you know, maybe there's some like cheap
15364320	15368880	insurance you can buy on that, you know, very silly question. In these worlds,
15369680	15374240	are financial markets where you make these kinds of bets going to be respected? And
15375120	15379680	like, you know, like, is my fidelity account going to mean anything when we have their 50%
15379680	15383280	economic growth? Like, who's, who's like, we got to respect his property rights?
15383280	15386400	That's pretty deep into it. The bond short, the sort of 50 and 52nd hour growth, that's pretty
15386400	15389440	deep into it. I mean, again, there's this whole sequence of things. But yeah, no, I think property
15389440	15394240	rights will be instructed again, in the sort of modal world, the project. Yeah. At some point,
15394240	15397040	at some point, there's going to be figuring out the property rights for the galaxies, you know,
15397040	15403440	and that'll be interesting. So there's an interesting question about
15405440	15410240	going back to your strategy about, well, the 30s will really matter a lot about how the rest of
15410240	15415280	the future goes. And you want to be in a position of influence by that point, because of capital.
15416480	15421200	It's worth considering, as far as I know, there's probably a whole bunch of literature on this,
15421200	15427600	I'm just riffing. But the, the landed gentry during the, before the beginning of the industrial
15427600	15433920	revolution, I'm not sure if they were able to leverage their position in a sort of georgist
15434560	15443840	or pickety type sense in order to accrue the returns that were realized through the industrial
15443840	15447920	revolution. And I don't know what happened. At some point, they were just wearing the
15448000	15454320	landed gentry. But I'd be concerned that even if you make great investment calls,
15454320	15458640	you'll be like the guy who owned a lot of land, farmland before the industrial revolution,
15458640	15462640	and like the guy who's actually going to make a bunch of money is the one with the C mentioned,
15462640	15466080	even if he doesn't make that much money, most of the benefits are sort of widely diffused and so
15466080	15471280	forth. I mean, I think that the analog is like you sell your land, you put it all and sort of
15471280	15476560	that, you know, that the people who are building the new industry. I think the, I mean, I think
15476560	15481840	the sort of like real depreciating asset, you know, for me is human capital, right? Yeah, no,
15481840	15485520	look, I'm serious, right? It's like, you know, there's something about like, you know, I don't
15485520	15488000	know, it was like valedictorian of Columbia, you know, the thing that made you special is
15488000	15491520	you're smart, right? But actually, like, you know, that might not matter in like four years,
15491520	15495520	you know, because it's actually automatable. Right. And so anyway, a friend joke that the
15495520	15500000	sort of investment firm is perfectly hedged for me. It's like, you know, either like AGI this
15500000	15504640	decade, and yeah, your human capital is depreciated, but you've turned that into financial capital,
15504640	15508480	or you know, like no AGI this decade, in which case, maybe the firm doesn't do that well,
15508480	15510800	but you know, you're still in your 20s and you're still smart.
15512640	15518320	Excellent. And what's your story for why AGI hasn't been priced in the story?
15519360	15523600	Financial markets are supposed to be very efficient to say very hard to get an edge here.
15525360	15530480	Naively, you just say, well, I've looked at these scaling curves and they imply that we're
15530480	15534400	going to be buying much more computed energy than the analysts realize.
15535280	15537520	Shouldn't those analysts be broke by now? What's going on?
15539280	15546800	Yeah. I mean, I used to be a true EMH guy. I was an economist, you know. I think the thing I,
15547600	15552080	you know, changed my mind on is that I think there can be kind of groups of people, smart people,
15552080	15555200	you know, who are, you know, say they're in San Francisco, who do just have
15556080	15560960	off over the rest of society and kind of seeing the future. And so like COVID, right? Like,
15560960	15565520	I think there's just honestly kind of similar group of people who just saw that and called it
15565520	15570080	completely correctly. And, you know, they showed at the market they did really well.
15571280	15583200	You know, a bunch of other sort of things like that. So, you know, why is AGI not priced in?
15583200	15587360	You know, it's sort of, you know, why hasn't the government nationalized the labs yet, right?
15587360	15590720	It's like, you know, this, you know, society hasn't priced it in yet and sort of it hasn't
15590720	15596080	completely diffused. And, you know, again, it might be wrong, right? But I just think sort of,
15597680	15600880	you know, not that many people take these ideas seriously yet. Yeah. Yeah.
15601760	15607360	Yeah. A couple of other sort of ideas that I was playing around with with regards to
15607360	15614480	reading it a chance to talk about, but the systems competition, there's a very interesting,
15616320	15618480	one of my favorite books about World War II is the Victor Davis Hansen
15621760	15628000	summary of everything. And he explains why the Allies made better decisions than the Axis.
15628000	15632240	Why did they? And so obviously, there were some decisions that the Axis made that were pretty
15632240	15635440	like Blitzkrieg, whatever. That was sort of by accident, though.
15635520	15637840	In what sense? That they just had the infrastructure left over?
15637840	15642080	Well, no, I mean, the sort of, I think, I mean, I don't, I mean, I think sort of my read of it is
15642080	15645440	Blitzkrieg wasn't kind of some like a genius strategy. It was just kind of, it was like more
15645440	15650080	like their hand was forced. I mean, this is sort of the very Adam Tuzi and story of World War II,
15650080	15653680	right? But it was, you know, there's sort of this long war versus short war. I think it's
15653680	15657680	actually kind of an important concept. I think sort of Germany realized that if they were in a
15657680	15662560	long war, including the United States, you know, they would not be able to compete industrially.
15662560	15667360	So their only path to victory was like make it a short war, right? And that, that sort of worked
15667360	15671360	much more spectacularly than they thought, right? And sort of take over France and take over much
15671360	15675200	of Europe. And so then, you know, the decision to invade the Soviet Union, it was, you know,
15675760	15680240	it was, it was, um, look, if it was, it was about the Western front in some sense, because it was
15680240	15683920	like, we've got to get the resources. You know, we don't, we're actually, we don't actually have a
15683920	15687520	bunch of the stuff we need, like, you know, oil and so on. You know, Auschwitz was actually just
15687520	15690640	this giant chemical plant to make kind of like synthetic oil and a bunch of these things was
15690640	15695920	the largest industrial project in Nazi Germany. And so, you know, and sort of they thought, well,
15695920	15699360	you know, we completely crushed them in World War I, you know, it'll be easy, we'll invade them,
15699360	15703440	we'll get the resources, and then we can fight on the Western front. And even during the sort of
15703440	15707040	whole invasion of the Soviet Union, even though kind of like a large amount of the sort of,
15707040	15710400	you know, the sort of deaths happened there, you know, like a large fraction of German industrial
15710400	15714880	production was actually, you know, like planes and naval, you know, and so on, those directed,
15714880	15718640	you know, towards the Western front and towards the, you know, the Western allies.
15718800	15721440	Well, and then so the point that Hansen was making was,
15722000	15725280	by the way, I think this concept of like long war and short war is kind of interesting and with
15725280	15730160	respect to thinking about the China competition, which is like, you know, I worry a lot about kind
15730160	15735920	of, you know, the decline of sort of American, like late in American industrial capacity, you know,
15735920	15742240	like, I think China builds like 200 times more ships than we do right now. You know, some crazy
15742240	15746000	way. And so it's like, maybe we have this superiority, say in the non AI worlds, we have
15746000	15749600	the superiority in military material, kind of like win a short war, at least, you know,
15749600	15753760	kind of defend Taiwan in some sense. But like if it actually goes on, you know, it's like maybe
15753760	15759200	China is much better able to mobilize, mobilize industrial resources in a way that like we just
15759200	15764080	don't have that same ability anymore. I think this is also relevant to the AI thing in the sense of
15764080	15768640	like, if it comes down to sort of a game about building, right, including like, maybe AGI takes
15768640	15772320	the trillion dollar cluster, not the hundred billion dollar cluster, maybe, or even maybe AGI
15772320	15776240	takes the, you know, is on the hundred billion dollar cluster. But, you know, it really matters
15776240	15779520	if you can run, you know, 10x, you can do one more order of magnitude of compute for your super
15779520	15785040	intelligence or whatever. That, you know, maybe right now they're behind, but they just have this
15785040	15789360	sort of like raw, late in industrial capacity to help build us. And that matters both in the
15789360	15793600	run up to AGI and after, right, where it's like, you have this super intelligence on your cluster,
15793600	15797920	now it's time to kind of like expand the explosive growth. And, you know, like, will we let the
15797920	15802400	ROA factories run wild? Like, maybe not. But like, maybe China will. Or like, you know, will we,
15802400	15805200	will, yeah, will we produce the, how many, how many of the drones will we produce?
15806240	15809360	And I think, yeah, so there's some sort of like outbuilding in the industrial explosion that I
15809360	15813360	worked on. You've got to be one of the few people in the world who is both concerned about alignment,
15813360	15818720	but also wants to make sure that we'll let the ROA factories proceed once we get the ASI to beat
15818720	15824880	out China. Like, it's all, it's all part of the picture. Yeah, yeah, yeah.
15825120	15831840	And by the way, speaking of the ASIs and the robot factories, one of the interesting things,
15831840	15836800	RoboArmy's too. Yeah, one of the interesting things, there's this question of what you do
15836800	15842480	with industrial scale intelligence. And obviously, it's not chatbots, but it's a, I think it's very
15842480	15852000	hard to predict. Yeah, yeah. But the history of oil is very interesting. We're in the, I think it's
15852000	15858160	in the 1860s that we figure out how to refine oil, some geologist. And so then standard oil
15858160	15864720	got started, there's this huge boom. It changes American politics, entire legislators are getting
15864720	15871360	bought out by oil interest and presidents are getting elected based on the divisions about
15871360	15876720	oil and breaking them up and everything. And all of this has happened. The world has never
15876800	15884320	revolutionized before the car has been invented. And so when the light bulb was invented, I think
15884320	15889840	it was like 50 years after oil refining had been discovered, majority of standard oil's history
15889840	15894960	is before the car is invented. Carousel lamps. Exactly. So it's just used for lighting.
15894960	15898800	Then they thought oil would just no longer be relevant. Yeah, yeah. So there was a concern
15898800	15905200	that standard oil would go to brain corrupt when the light bulb was invented. But then
15906000	15910320	there's sort of, you realize that there's immense amount of compressed energy here.
15910960	15915280	You're going to have billions of gallons of this stuff a year. And it's hard to
15915280	15920960	sort of predict in advance what you can do with that. And then later on, it turns out transportation
15921520	15928480	cars with, that's what it's used for. Anyways, with intelligence, maybe one answer is the
15928480	15933440	intelligence explosion. But even after that, so you have all these ASIs and you have enough
15933440	15938480	compute, especially the compute they'll build to run hundreds of millions of GPUs will hum.
15938480	15942560	Yeah. But what are we doing with that? And it's very hard to predict in advance. I think it would
15942560	15949360	be very interesting to figure out what the Jupiter brains will be doing. So look, there's
15949360	15955840	situational awareness of where things stand now. And we've gotten a good dose of that.
15958160	15960480	Obviously, a lot of the things we're talking about now, you couldn't have
15961200	15968720	prejudged many years back in the past. And part of your role implies that things will accelerate
15968720	15975200	because of AI getting the process. But many other things that are unpredictable fundamentally,
15975200	15979200	basically how people will react, how the political system will react, how foreign adversaries will
15979200	15986720	react. Those things will become evident over time. So the situational awareness is not just
15986800	15992000	knowing where the picture stands now, but being in a position to react appropriately to new information,
15992960	15996640	to change your worldview as a result, to change your recommendations as a result.
15997680	16003440	What is the appropriate way to think about situational awareness as a continuous process
16003440	16008720	rather than as a one-time thing you realized? Yep. No, I think this is great. Look, I think
16008720	16012640	there's a sort of mental flexibility and willing to change your mind. That's really important.
16012640	16016480	I actually think this is sort of like how a lot of brains have been broken in the AGI debate,
16017040	16022320	the tumors who actually, I think we're really prescient on AGI thinking about the stuff a decade
16022320	16026320	ago, but they haven't actually updated on the empirical realities of deep learning. They're
16026320	16030320	sort of like, the proposals are really kind of even unworkable. This doesn't really make sense.
16031040	16033920	There's people who come in with sort of a predefined ideology. They're just kind of like,
16033920	16038000	the EX a little bit. They like to shitpost about technology, but they're not actually thinking
16038000	16042160	through. I mean, either the sort of stagnationists who think this stuff is only going to be a
16042160	16045760	chatbot, and so of course it isn't risky, or they're just not thinking through the kind of like
16045760	16050880	actually immense national security implications and how that's going to go. I actually think
16050880	16054640	there's kind of a risk in kind of like having written this stuff down and put it online.
16057120	16060080	I think this sometimes happens to people as a sort of calcification of the worldview,
16060080	16064400	because now they've publicly articulated this position. Maybe there's some evidence against
16064400	16069600	it, but they're clinging to it. I actually want to give the big disclaimer on like,
16069600	16073040	I think it's really valuable to paint this sort of very concrete and visceral picture.
16074400	16078960	I think this is currently my best guess on how this decade will go. I think if it goes
16078960	16085440	anywhere like this, it will be wild, but given the rapid pace of progress, we're going to keep
16085440	16091920	getting a lot more information. I think it's important to sort of keep your head on straight
16091920	16099440	about that. I feel like the most important thing here is that, and this relates to some of the
16099440	16106160	stuff we've talked about and the world being surprisingly small and so on. I feel like I
16106160	16109200	used to have this worldview of like, look, there's important things happening in the world, but there's
16109200	16112720	like people who are taking care of it, and there's like the people in government, and there's again,
16112720	16118960	even like AI labs have idealized, and people are on it. Surely there must be on it, right?
16118960	16122560	And I think just some of this personal experience, even seeing how kind of COVID went,
16124160	16127680	people aren't necessarily, there's not some, not that somebody else is just kind of on it and
16127760	16134720	making sure this goes well, however it goes. You know, the thing that I think will really
16134720	16139680	matter is that there are sort of good people who take this stuff as seriously as it deserves,
16139680	16143920	and who are willing to kind of take the implication seriously, who are willing to, you know,
16143920	16147680	who have situational awareness, are willing to change their minds, are willing to sort of
16147680	16154000	stare the picture in the face, and you know, I'm counting on those good people.
16154000	16157920	All right, that's a great place to close Leopold.
16157920	16161360	Thanks so much, Tarkash. This is the absolute joy.
16161360	16166320	Hey everybody, I hope you enjoyed that episode with Leopold. There's actually one more riff
16166320	16171120	about German history that he had after a break, and it was pretty interesting, so I didn't want
16171120	16176400	to cut it out, so I've just included it after this outro. You can advertise on the show now,
16176400	16181200	so if you're interested, you can reach out at the forum in the description below.
16181280	16185680	Other than that, the most helpful thing you can do is to share the episode if you enjoyed it.
16185680	16190720	Send it to group chats, Twitter, wherever else you think people who might like this episode
16190720	16196000	might congregate, and other than that, I guess here's this riff on Frederick the Great. See you
16196000	16200640	on the next one. I mean, I think the actual funny thing is, you know, a lot of the sort of German
16200640	16205120	history stuff we've talked about is sort of like not actually stuff I learned in Germany,
16205120	16207600	it's sort of like stuff that I learned after, and there's actually, you know,
16207600	16211040	a funny thing where I kind of would go back to Germany over Christmas or whatever. Suddenly
16211040	16214160	I understand the street names, you know, it's like, you know, Gneisenau and Scharnhorst,
16214160	16218160	and they're all these like Prussian military reformers, and you're like finally understood,
16218160	16221440	you know, Sansa C, and you're like, Frederick, you know, Frederick the Great is this really
16221440	16229440	interesting figure, where, so he's this sort of, in some sense, kind of like gay lover of arts,
16230000	16236080	right, where he, you know, he hates speaking German, he only wants to speak French, you know,
16236080	16240000	he like plays the flute, he composes, he has all the sort of great, you know, artists of his day,
16240000	16247520	you know, over at Sansa C, and he actually had this sort of like really tough upbringing, where
16247520	16255040	his father was this sort of like really stern sort of Prussian military man, and he had had a,
16256080	16259840	Frederick the Great as sort of a 17-year-old or whatever, he basically had a male lover,
16260400	16267840	and what his father did was imprison his son, and then I think hang his male lover in front of him,
16268480	16272160	and again, his father was this kind of very stern Prussian guy, he was this kind of gay,
16272160	16276560	you know, lover of arts, but then later on Frederick the Great turns out to be this like,
16276560	16282880	you know, one of the most kind of like, you know, successful kind of Prussian conquerors,
16282880	16286640	right, like he gets Silesia, he wins the Seven Years War, you know, also, you know,
16286640	16290320	amazing military strategists, you know, amazing military strategy at the time consisted of like,
16290320	16294400	he was able to like flank the army, and that was crazy, you know, and that was brilliant,
16294400	16298320	and then they like almost lose the Seven Years War, and at the very end, you know, the sort of,
16299120	16303280	the Russian Tsar changes, and he's like, ah, I'm actually kind of a Prussian stan, you know,
16303280	16307360	I think I'm like, I'm into this stuff, and then he lets, you know, let's Frederick the Great lose,
16307360	16315200	and he had, let's their army be okay, and anyway, sort of like, yeah, kind of bizarre,
16315200	16317200	interesting figure in German history.
