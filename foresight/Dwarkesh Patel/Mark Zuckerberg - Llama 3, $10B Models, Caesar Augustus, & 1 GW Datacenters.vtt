WEBVTT

00:00.000 --> 00:01.440
That's not even a question for me,

00:01.440 --> 00:04.080
whether we're going to go take a swing at building the next thing.

00:04.080 --> 00:06.640
I'm just incapable of not doing that.

00:06.640 --> 00:09.840
There's a bunch of times when we wanted to launch features

00:09.840 --> 00:12.480
and then Apple's just like, nope, you're not launching that.

00:12.480 --> 00:13.360
I was like, that sucks.

00:14.160 --> 00:18.960
Are we set up for that with AI, where you're going to get a handful of companies

00:18.960 --> 00:22.240
that run these closed models that are going to be in control of the APIs

00:22.240 --> 00:24.480
and therefore are going to be able to tell you what you can build?

00:24.480 --> 00:29.360
Then when you start getting into building a data center that's like 300 megawatts

00:29.360 --> 00:31.920
or 500 megawatts or a gigawatt,

00:31.920 --> 00:34.320
just no one has built a single gigawatt data center yet.

00:34.320 --> 00:37.200
But from wherever you sit, there's going to be some actor who you don't trust.

00:37.200 --> 00:39.600
If they're the ones who have like the super strong AI,

00:39.600 --> 00:43.040
I think that that's potentially a much bigger risk.

00:43.920 --> 00:45.280
Mark, welcome to the podcast.

00:45.280 --> 00:47.200
Hey, thanks for having me, big fan of your podcast.

00:47.200 --> 00:48.560
Oh, thank you. That's very nice of you to say.

00:49.920 --> 00:54.000
Okay, so let's start by talking about the releases that will go out

00:54.000 --> 00:55.040
when this interview goes out.

00:55.920 --> 00:57.920
Tell me about the models, tell me about meta AI,

00:57.920 --> 00:59.440
what's new, what's exciting about them?

00:59.440 --> 01:02.640
Yeah, sure. I think the main thing that most people in the world

01:02.640 --> 01:04.240
are going to see is the new version of meta AI.

01:06.640 --> 01:10.400
The most important thing about what we're doing is the upgrade to the model.

01:10.400 --> 01:11.600
We're rolling out Lama 3.

01:11.600 --> 01:14.960
We're doing it both as open source for the dev community,

01:14.960 --> 01:17.120
and it is now going to be powering meta AI.

01:18.880 --> 01:20.880
There's a lot that I'm sure we'll go into around Lama 3,

01:20.880 --> 01:24.160
but I think the bottom line on this is that with Lama 3,

01:24.160 --> 01:28.080
we now think that meta AI is the most intelligent AI assistant

01:28.080 --> 01:30.000
that people can use that's freely available.

01:30.720 --> 01:33.760
We're also integrating Google and Bing for real-time knowledge.

01:34.480 --> 01:37.200
We're going to make it a lot more prominent across our apps.

01:37.200 --> 01:40.960
So basically, at the top of WhatsApp and Instagram

01:40.960 --> 01:45.680
and Facebook and Messenger, you'll just be able to use the search box

01:45.680 --> 01:47.280
right there to ask it any question.

01:48.080 --> 01:50.720
And there's a bunch of new creation features that we added

01:50.720 --> 01:52.720
that I think are pretty cool that I think people enjoy.

01:54.400 --> 01:56.480
And I think animations is a good one.

01:57.200 --> 01:59.360
You can basically just take any image and animate it.

01:59.360 --> 02:03.520
But I think one that people are going to find pretty wild is

02:04.400 --> 02:07.680
it now generates high-quality images so quickly.

02:07.680 --> 02:09.360
I don't know if you've gotten a chance to play with this,

02:09.360 --> 02:12.880
that it actually generates it as you're typing and updates it in real-time.

02:12.880 --> 02:16.880
So you're typing your query and it's honing in on...

02:16.880 --> 02:22.480
And it's like, okay, here, show me a picture of a cow in a field

02:22.560 --> 02:25.920
with mountains in the background and just like eating macadamia nuts,

02:25.920 --> 02:30.080
drinking beer and it's updating the image in real-time.

02:30.800 --> 02:31.520
It's pretty wild.

02:31.520 --> 02:32.720
I think people are going to enjoy that.

02:33.920 --> 02:37.200
So yeah, that's what most people are going to see in the world.

02:37.200 --> 02:39.360
We're rolling that out, not everywhere,

02:39.360 --> 02:41.760
but we're starting in a handful of countries

02:41.760 --> 02:44.080
and we'll do more over the coming weeks and months.

02:45.520 --> 02:47.760
So that I think is going to be a pretty big deal.

02:48.880 --> 02:50.560
And I'm really excited to get that in people's hands.

02:51.040 --> 02:53.680
It's a big step forward for Met AI.

02:55.440 --> 02:57.680
But I think if you want to get under the hood a bit,

02:58.480 --> 03:01.520
the Llama 3 stuff is obviously the most technically interesting.

03:01.520 --> 03:04.240
So we're basically, for the first version,

03:04.240 --> 03:08.720
we're training three versions, an 8 billion and a 70 billion,

03:08.720 --> 03:13.360
which we're releasing today and a 405 billion dense model,

03:13.360 --> 03:16.080
which is still training, so we're not releasing that today.

03:16.480 --> 03:23.040
But the 8 and 70, I mean, I'm pretty excited about how they turned out.

03:23.040 --> 03:26.800
I mean, they're leading for their scale.

03:28.800 --> 03:32.880
You know, it's, I mean, we'll release a blog post with all the benchmarks

03:32.880 --> 03:35.200
so people can check it out themselves and obviously it's open source

03:35.200 --> 03:36.720
so people get a chance to play with it.

03:37.840 --> 03:40.080
We have a roadmap of new releases coming

03:40.960 --> 03:44.640
that are going to bring multi-modality, more multi-linguality,

03:45.600 --> 03:47.520
bigger context windows to those as well.

03:48.720 --> 03:51.520
And then, you know, hopefully sometime later in the year,

03:51.520 --> 03:56.320
we'll get to roll out the 405, which I think is, in training,

03:56.320 --> 04:01.040
it's still training, but for where it is right now in training,

04:01.040 --> 04:08.720
it is already at around 85 mmlu and just,

04:08.720 --> 04:10.640
we expect that it's going to have leading benchmarks

04:10.640 --> 04:12.800
on a bunch of the benchmarks.

04:12.800 --> 04:14.640
So, I'm pretty excited about all of that.

04:14.640 --> 04:18.480
I mean, the 70 billion is great too.

04:18.480 --> 04:19.440
I mean, we're releasing that today.

04:19.440 --> 04:23.200
It's around 82 mmlu and has leading scores on math and reasoning.

04:23.200 --> 04:25.280
So, I mean, it's, I think just getting this in people's hands

04:25.280 --> 04:26.480
is going to be pretty wild.

04:26.480 --> 04:27.040
Oh, interesting.

04:27.040 --> 04:28.240
Yeah, that's the first time hearing this benchmark.

04:28.240 --> 04:28.960
That's super impressive.

04:28.960 --> 04:35.840
Yeah, and the 8 billion is nearly as powerful

04:35.840 --> 04:38.240
as the biggest version of Llama 2 that we released.

04:38.240 --> 04:41.120
So, it's like the smallest Llama 3 is basically as powerful

04:41.200 --> 04:43.360
as the biggest Llama 2.

04:43.360 --> 04:45.680
Okay, so before we dig into these models,

04:45.680 --> 04:47.280
I actually want to go back in time.

04:47.840 --> 04:51.520
2022 is, I'm assuming, when you started acquiring these H100s,

04:52.560 --> 04:54.960
or you can tell me when, where you're like,

04:54.960 --> 04:56.400
stock price is getting hammered.

04:56.400 --> 04:58.400
People are like, what's happening with all this capex?

04:58.400 --> 04:59.920
People aren't buying the metaverse.

04:59.920 --> 05:01.360
And presumably, you're spending that capex

05:01.360 --> 05:02.720
to get these H100s.

05:02.720 --> 05:05.040
How, back then, how did you know to get the H100s?

05:05.040 --> 05:06.560
How did you know we'll need the GPUs?

05:07.760 --> 05:10.160
I think it was because we were working on Reels.

05:10.160 --> 05:15.440
So, we got into this situation where we always

05:16.160 --> 05:19.520
want to have enough capacity to build something

05:19.520 --> 05:23.280
that we can't quite see that were on the horizon yet.

05:24.160 --> 05:26.640
And we got into this position with Reels,

05:26.640 --> 05:30.800
where we needed more GPUs to train the models.

05:30.800 --> 05:33.920
It was this big evolution for our services,

05:33.920 --> 05:35.760
where instead of just ranking content from people

05:35.760 --> 05:39.200
who you follow, or your friends, and whatever pages you follow,

05:41.040 --> 05:45.040
we made this big push to basically start recommending

05:45.600 --> 05:47.280
what we call unconnected content,

05:47.280 --> 05:49.520
to basically connect content from people

05:49.520 --> 05:50.800
or pages that you're not following.

05:50.800 --> 05:55.760
So, now, kind of the corpus of kind of content candidates

05:55.760 --> 05:57.840
that we could potentially show you expanded from,

05:57.840 --> 05:59.680
you know, on the order of thousands

05:59.680 --> 06:02.480
to on the order of hundreds of millions.

06:02.480 --> 06:04.240
So, completely different infrastructure.

06:04.960 --> 06:08.720
And we started working on doing that,

06:08.720 --> 06:12.960
and we were constrained on basically the infrastructure

06:12.960 --> 06:15.600
that we had to catch up to what TikTok was doing

06:15.600 --> 06:16.880
as quickly as we would have wanted to.

06:17.760 --> 06:19.040
So, I basically looked at that, and I was like,

06:19.040 --> 06:22.560
hey, we have to make sure that we're never in this situation again.

06:22.560 --> 06:26.320
So, let's order enough GPUs to do what we need to do

06:26.320 --> 06:28.640
on Reels and ranking content and feed,

06:28.640 --> 06:30.480
but let's also double that, right?

06:30.480 --> 06:32.800
Because, again, like our normal principle is,

06:32.800 --> 06:34.240
there's going to be something on the horizon

06:34.240 --> 06:35.040
that we can't see yet.

06:35.040 --> 06:35.840
Did you know it would be AI?

06:36.400 --> 06:38.480
Well, we thought it would be,

06:39.440 --> 06:40.320
we thought it was going to be something

06:40.320 --> 06:42.400
that I had to do with training large models, right?

06:42.400 --> 06:44.000
I mean, but at the time, I thought it was probably

06:44.000 --> 06:46.080
going to be more something that I had to do with content.

06:46.080 --> 06:46.800
But I don't know.

06:46.800 --> 06:49.520
I mean, it's almost just the pattern matching

06:49.520 --> 06:53.680
and running the company is there's always another thing, right?

06:53.680 --> 06:55.920
So, I'm not even sure I had, at that time,

06:55.920 --> 06:58.480
I was so deep in just, you know, trying to get,

06:58.480 --> 07:00.800
you know, the recommendations working for Reels

07:00.800 --> 07:02.560
and other content, because I mean,

07:02.560 --> 07:04.960
that's just such a big unlock for Instagram and Facebook

07:04.960 --> 07:06.720
to now being able to show people content

07:06.720 --> 07:08.240
that's interesting to them that they're from people

07:08.240 --> 07:09.840
that they're not even following.

07:09.840 --> 07:15.280
But, yeah, that ended up being a very good decision

07:15.280 --> 07:16.000
in retrospect.

07:16.000 --> 07:16.960
Yeah, yeah.

07:16.960 --> 07:18.320
Okay, and it came from being behind.

07:18.320 --> 07:20.640
So, then it wasn't like I was, you know,

07:20.640 --> 07:22.160
it wasn't like, oh, I was so far ahead.

07:22.160 --> 07:23.440
Actually, most of the times, I think,

07:23.440 --> 07:25.680
where we kind of make some decision

07:25.680 --> 07:28.800
that ends up seeming good is because we messed something up

07:28.800 --> 07:30.640
before and just didn't want to repeat the mistake.

07:31.280 --> 07:32.160
This is a total detour,

07:32.160 --> 07:33.840
but actually, I want to ask about this while we're on this.

07:33.840 --> 07:35.680
We'll get back to AI in a second.

07:36.480 --> 07:38.240
So, you didn't suffer one billion,

07:38.240 --> 07:39.520
but presumably there's some amount

07:39.520 --> 07:40.720
you would have sold for, right?

07:40.720 --> 07:42.480
Did you write down in your head, like,

07:42.480 --> 07:45.120
I think the actual valuation of Facebook at the time is this

07:45.120 --> 07:46.960
and they're not actually getting the valuation right?

07:46.960 --> 07:49.040
Like, the average $5 trillion, of course, you would have sold.

07:49.040 --> 07:52.000
So, like, how did you think about that choice?

07:52.720 --> 07:53.520
Yeah, I don't know.

07:53.520 --> 07:56.080
I mean, look, I think some of these things are just personal.

07:58.160 --> 08:01.040
I don't know at the time that I was sophisticated enough

08:01.040 --> 08:02.080
to do that analysis,

08:02.080 --> 08:05.280
but I had all these people around me who were making

08:05.280 --> 08:09.120
all these arguments for how, like, a billion dollars was,

08:09.120 --> 08:11.200
you know, it's like, here's the revenue that we need to make

08:11.200 --> 08:12.640
and here's how big we need to be

08:12.640 --> 08:14.640
and, like, it's clearly so many years in the future.

08:14.640 --> 08:16.560
Like, and it was, it was very far ahead

08:16.560 --> 08:18.080
of where we were at the time.

08:18.080 --> 08:23.040
And I don't know, I didn't really have the financial sophistication

08:23.040 --> 08:26.240
to really even engage with that kind of debate.

08:26.240 --> 08:29.520
I just, I think I sort of deep down believed

08:29.520 --> 08:30.640
in what we were doing.

08:30.640 --> 08:32.000
And I did some analysis.

08:33.840 --> 08:38.640
I was like, okay, well, what would I go do if I wasn't doing this?

08:38.640 --> 08:41.360
It's like, well, I really like building things

08:41.360 --> 08:43.120
and I like helping people communicate

08:43.120 --> 08:46.560
and I like understanding what's going on with people

08:46.560 --> 08:48.000
and the dynamics between people.

08:48.000 --> 08:50.080
So, I think if I sold this company,

08:50.080 --> 08:51.920
I'd just go build another company like this.

08:51.920 --> 08:54.480
And I kind of like the one I have.

08:54.480 --> 08:58.560
So, so, I mean, you know, what's, why, why, right?

08:58.560 --> 09:00.960
But I don't know.

09:00.960 --> 09:04.080
I think a lot of the biggest bets that people make

09:05.520 --> 09:08.240
are often just based on conviction and values.

09:09.200 --> 09:13.200
Not, it's actually usually very hard to do the analyses

09:13.200 --> 09:14.560
trying to connect the dots forward.

09:14.560 --> 09:15.120
Yeah.

09:15.120 --> 09:18.160
So, you've had Facebook AI research for a long time.

09:19.280 --> 09:21.760
Now it's become seemingly central to your company.

09:23.120 --> 09:27.040
At what point did making AGI or whatever,

09:27.040 --> 09:28.640
however you consider that mission,

09:28.640 --> 09:29.440
at what point is that like,

09:29.440 --> 09:31.520
this is a Cree priority of what Meta is doing?

09:32.800 --> 09:32.960
Yeah.

09:32.960 --> 09:34.960
I mean, it's been a big deal for a while.

09:34.960 --> 09:38.720
So, we started fair about 10 years ago.

09:38.720 --> 09:44.320
And the idea was that along the way to general intelligence

09:44.320 --> 09:46.640
or AI, like full AI, whatever you want to call it,

09:47.360 --> 09:49.280
there can be all these different innovations

09:49.280 --> 09:51.680
and that's going to just improve everything that we do.

09:51.680 --> 09:55.440
So, we didn't kind of conceive it as a product.

09:55.440 --> 09:57.520
It was more kind of a research group.

09:58.080 --> 10:01.280
And over the last 10 years,

10:01.280 --> 10:03.360
it has created a lot of different things

10:03.360 --> 10:06.320
that have basically improved all of our products

10:07.040 --> 10:09.840
and advanced the field and allowed other people in the field

10:09.840 --> 10:11.760
to create things that have improved our products too.

10:11.760 --> 10:13.040
So, I think that that's been great.

10:13.600 --> 10:18.400
But there's obviously a big change in the last few years

10:18.400 --> 10:20.560
when ChatGPT comes out,

10:21.200 --> 10:23.520
the diffusion models or an image creation come out.

10:24.000 --> 10:25.760
I mean, this is some pretty wild stuff

10:25.760 --> 10:27.920
that I think is pretty clearly going to affect

10:27.920 --> 10:32.320
how people interact with every app that's out there.

10:34.400 --> 10:39.600
At that point, we started a second group, the GenAI group,

10:40.480 --> 10:44.400
with the goal of basically bringing that stuff into our products,

10:44.400 --> 10:46.480
so building leading foundation models

10:46.480 --> 10:48.880
that would sort of power all these different products.

10:49.440 --> 10:51.920
And initially, when we started doing that,

10:53.760 --> 10:55.120
the theory at first was, hey,

10:55.680 --> 10:58.560
a lot of the stuff that we're doing is pretty social, right?

10:58.560 --> 11:01.920
So, it's helping people interact with creators,

11:01.920 --> 11:04.880
helping people interact with businesses.

11:04.880 --> 11:08.160
So, the businesses can sell things or do customer support

11:08.160 --> 11:11.200
or basic assistant functionality for,

11:12.160 --> 11:14.880
you know, whether it's for our apps or the smart glasses

11:14.880 --> 11:16.960
or VR or like all these different things.

11:17.600 --> 11:21.600
So, initially, it wasn't completely clear

11:21.600 --> 11:25.280
that you were going to need kind of full AGI

11:26.080 --> 11:27.680
to be able to support those use cases.

11:27.680 --> 11:29.520
But then through working on them,

11:29.520 --> 11:31.520
I think it's actually become clear that you do, right?

11:31.520 --> 11:32.480
In all these subtle ways.

11:32.480 --> 11:35.680
So, for example, for Llama 2, when we were working on it,

11:35.680 --> 11:37.200
we didn't prioritize coding.

11:37.200 --> 11:39.280
And the reason why we didn't prioritize coding

11:39.280 --> 11:41.520
is because people aren't going to ask MetaAI

11:41.520 --> 11:43.360
a lot of coding questions in WhatsApp.

11:43.360 --> 11:44.400
Now they will, right?

11:44.400 --> 11:44.880
Well, I don't know.

11:44.880 --> 11:46.960
I'm not sure that WhatsApp is like the UI

11:46.960 --> 11:48.960
that people are going to be doing a lot of coding questions.

11:48.960 --> 11:51.120
So, we're like, all right, look, in terms of the things that,

11:51.200 --> 11:52.880
you know, or Facebook or Instagram

11:52.880 --> 11:54.400
or, you know, those different services,

11:54.400 --> 11:58.160
maybe the website, right, meta.ai that we're launching, I think.

11:58.160 --> 12:01.440
But the thing that was sort of, I think,

12:01.440 --> 12:06.160
has been a somewhat surprising result over the last 18 months

12:06.160 --> 12:10.720
is that it turns out that coding is important

12:10.720 --> 12:12.480
for a lot of domains, not just coding, right?

12:12.480 --> 12:15.440
So, even if people aren't asking coding questions to the models,

12:16.160 --> 12:20.080
training the models on coding helps them just be more rigorous

12:20.080 --> 12:23.360
and answer the question and kind of help reason

12:23.360 --> 12:25.280
across a lot of different types of domains.

12:25.280 --> 12:26.400
Okay, so that's one example where it's like,

12:26.400 --> 12:27.360
all right, so for Llama 3,

12:27.360 --> 12:29.680
we're like really focused on training it with a lot of coding

12:29.680 --> 12:30.320
because it's like, all right,

12:30.320 --> 12:32.160
that's going to make it better on all these things,

12:32.160 --> 12:33.520
even if people aren't answering,

12:33.520 --> 12:35.280
aren't asking primarily coding questions.

12:36.080 --> 12:37.680
Reasoning, I think, is another example.

12:38.320 --> 12:41.600
It's like, okay, yeah, maybe you want to chat with a creator

12:41.600 --> 12:42.960
or, you know, you're a business

12:42.960 --> 12:45.200
and you're trying to interact with a customer.

12:45.200 --> 12:46.720
You know, that interaction is not just like,

12:46.720 --> 12:49.200
okay, the person sends you a message

12:49.200 --> 12:50.640
and you just reply, right?

12:50.640 --> 12:53.040
It's like a multi-step interaction

12:53.040 --> 12:54.560
where you're trying to think through

12:54.560 --> 12:56.400
how do I accomplish the person's goals

12:56.400 --> 12:58.720
and, you know, a lot of times when a customer comes,

12:58.720 --> 13:00.640
they don't necessarily know exactly

13:00.640 --> 13:02.640
what they're looking for or how to ask their questions.

13:02.640 --> 13:04.960
So, it's not really the job of the AI

13:04.960 --> 13:06.400
to just respond to the question.

13:06.400 --> 13:08.640
It's like, you need to kind of think about it more holistically.

13:08.640 --> 13:10.400
It really becomes a reasoning problem, right?

13:10.400 --> 13:11.520
So, if someone else, you know,

13:11.520 --> 13:14.160
solves reasoning or makes good advances on reasoning,

13:14.160 --> 13:17.040
and we're sitting here with a basic chat bot,

13:17.040 --> 13:18.480
then, like, our product is lame

13:18.480 --> 13:20.240
compared to what other people are building.

13:20.240 --> 13:21.520
So, it's like, it's okay.

13:21.520 --> 13:23.360
So, at the end of the day, we've got,

13:23.360 --> 13:25.520
we, you know, we basically realized

13:25.520 --> 13:27.200
we've got to solve general intelligence

13:28.320 --> 13:31.040
and we just kind of upped the ante and the investment

13:31.040 --> 13:32.480
to make sure that we could do that.

13:32.480 --> 13:38.480
So, the version of Lama that's going to solve

13:38.480 --> 13:40.960
all these use cases for users,

13:40.960 --> 13:43.040
is that the version that will be powerful enough

13:43.040 --> 13:45.840
to, like, replace a programmer you might have in this building?

13:46.720 --> 13:47.920
I mean, I just think that all this stuff

13:47.920 --> 13:49.360
is going to be progressive over time.

13:49.360 --> 13:50.960
But, in case, Lama 10.

13:53.920 --> 13:56.400
I mean, I think that there's a lot baked into that question.

13:56.400 --> 13:58.800
I'm not sure that we're replacing people

13:58.800 --> 14:02.080
as much as giving people tools to do more stuff.

14:02.080 --> 14:03.920
Is a programmer in this building 10x more productive

14:03.920 --> 14:04.320
after Lama 10?

14:04.320 --> 14:05.120
I would have more.

14:05.120 --> 14:08.320
But no, I mean, look, I'm not,

14:08.320 --> 14:09.520
I don't believe that there's, like,

14:09.520 --> 14:12.560
a single threshold of intelligence for humanity,

14:12.560 --> 14:14.560
because, I mean, people have different skills.

14:14.560 --> 14:16.400
And at some point, I think that AI is going to be,

14:17.520 --> 14:21.280
is probably going to surpass people at most of those things,

14:21.280 --> 14:23.040
depending on how powerful the models are.

14:23.040 --> 14:26.960
But I think it's progressive.

14:26.960 --> 14:28.400
And I don't think AGI is one thing.

14:28.400 --> 14:30.960
I think it's, you're basically adding different capabilities.

14:30.960 --> 14:35.360
So, multimodality is kind of a key one that we're focused on now,

14:35.360 --> 14:38.000
initially with photos and images and text,

14:38.000 --> 14:39.360
but eventually with videos.

14:39.360 --> 14:41.280
And then, because we're so focused on the metaverse,

14:41.360 --> 14:43.440
kind of 3D type stuff is important.

14:45.040 --> 14:47.680
One modality that I'm pretty focused on that I haven't seen

14:47.680 --> 14:50.560
as many other people in the industry focus on this

14:50.560 --> 14:53.680
is sort of like emotional understanding.

14:53.680 --> 14:56.320
Like, I mean, so much of the human brain

14:56.320 --> 14:59.200
is just dedicated to understanding people

14:59.200 --> 15:02.240
and kind of like understanding your expressions and emotions.

15:02.240 --> 15:04.800
And I think that that's like its own whole modality, right?

15:04.800 --> 15:06.400
That, I mean, you could say, okay,

15:06.400 --> 15:07.840
maybe it's just video or image,

15:07.840 --> 15:10.880
but it's like clearly a very specialized version of those too.

15:10.880 --> 15:12.880
So, there's all these different capabilities

15:12.880 --> 15:17.680
that I think you wanna basically train the models to focus on,

15:17.680 --> 15:19.840
as well as getting a lot better at reasoning,

15:19.840 --> 15:21.040
getting a lot better at memory,

15:21.040 --> 15:23.040
which I think is kind of its own whole thing.

15:23.040 --> 15:24.480
It's, I mean, I don't think we're gonna be,

15:24.480 --> 15:26.720
you know, primarily shoving context

15:26.720 --> 15:31.120
or kind of things into a query context window in the future

15:31.120 --> 15:32.880
to ask more complicated questions.

15:32.880 --> 15:35.360
I think that there'll be kind of different stores of memory

15:35.360 --> 15:36.480
or different custom models

15:36.480 --> 15:39.760
that are maybe more personalized to people.

15:39.760 --> 15:41.440
But I don't know, I think that these are all

15:41.440 --> 15:42.640
just different capabilities.

15:42.640 --> 15:44.400
And then obviously making them big and small,

15:44.400 --> 15:46.800
we care about both because, you know, we wanna,

15:46.800 --> 15:49.200
you know, if you're running something like meta AI,

15:49.200 --> 15:52.480
then we have the ability to, that's pretty server-based,

15:52.480 --> 15:54.320
but we also want it running on smart glasses.

15:54.320 --> 15:56.720
And, you know, there's not a lot of space in smart glasses.

15:56.720 --> 16:00.080
So, you wanna have something that's very efficient for that.

16:00.080 --> 16:03.120
What is the use case that if you're doing tens of billions

16:03.120 --> 16:04.000
of dollars worth of inference,

16:04.000 --> 16:05.600
or even eventually hundreds of billions of dollars

16:05.600 --> 16:09.120
worth of inference, using intelligence in an industrial scale,

16:09.120 --> 16:10.240
what is the use case?

16:10.240 --> 16:11.360
Is it simulations?

16:11.360 --> 16:13.120
Is it the AIs that will be in the metaverse?

16:13.120 --> 16:15.520
What will we be using the data centers for?

16:19.120 --> 16:21.200
I mean, our bet is that it's gonna,

16:21.200 --> 16:23.440
this is basically gonna change all of the products, right?

16:23.440 --> 16:27.760
So, I think that there's gonna be a kind of meta AI

16:27.760 --> 16:29.440
general assistant product.

16:29.440 --> 16:32.480
And I think that that will shift from something

16:32.480 --> 16:34.080
that feels more like a chat bot

16:34.080 --> 16:35.600
where it's like you just ask a question

16:35.600 --> 16:37.440
and it kind of formulates an answer

16:37.440 --> 16:38.720
to things where you're increasingly

16:38.720 --> 16:40.320
giving it more complicated tasks

16:40.320 --> 16:41.600
and that goes away and does them.

16:42.400 --> 16:44.480
So, that's gonna take a lot of inference.

16:44.480 --> 16:46.240
It's gonna take a lot of compute in other ways too.

16:47.920 --> 16:50.880
Then I think that there's a big part of what we're gonna do

16:50.880 --> 16:56.560
that is like interacting with other agents for other people.

16:56.560 --> 16:58.400
So, whether it's businesses or creators,

17:00.160 --> 17:01.760
I guess a big part of my theory on this

17:01.760 --> 17:04.080
is that there's not just gonna be like one singular AI

17:04.080 --> 17:05.280
that you interact with,

17:05.280 --> 17:08.960
because I think every business is gonna like want an AI

17:08.960 --> 17:10.240
that represents their interests.

17:10.240 --> 17:12.800
They're not gonna like wanna primarily interact with you

17:12.800 --> 17:16.240
through an AI that is gonna sell their competitors' customers.

17:16.240 --> 17:18.160
So, sorry, their competitors' products.

17:19.040 --> 17:25.280
So, yeah, so I think creators is gonna be a big one.

17:25.280 --> 17:28.640
I mean, there are about 200 million creators on our platforms.

17:28.640 --> 17:30.480
They all basically have the pattern where

17:31.600 --> 17:33.040
they want to engage their community,

17:33.040 --> 17:34.560
but they're limited by hours in the day

17:34.560 --> 17:36.800
and their community generally wants to engage them,

17:36.800 --> 17:38.800
but they don't have, they're limited by hours in the day.

17:39.520 --> 17:43.840
So, if you could create something where an AI could basically,

17:44.400 --> 17:46.640
that creator can basically own the AI

17:46.640 --> 17:48.080
and train it in the way that they want

17:49.920 --> 17:51.600
and can engage their community,

17:51.600 --> 17:53.760
I think that that's gonna be super powerful too.

17:53.760 --> 17:56.240
So, I think that there's gonna be a ton of engagement

17:56.240 --> 17:57.200
across all these things.

17:59.440 --> 18:01.120
But these are just the consumer use cases.

18:01.120 --> 18:02.800
I mean, I think when you think about stuff like,

18:03.680 --> 18:06.640
I mean, I run our foundation,

18:07.440 --> 18:09.120
Chan Zuckerberg Initiative with my wife,

18:09.120 --> 18:11.200
and we're doing a bunch of stuff on science,

18:11.200 --> 18:13.760
and there's obviously a lot of AI work

18:13.760 --> 18:16.880
that I think is gonna advance science and healthcare

18:16.880 --> 18:17.680
and all these things too.

18:17.680 --> 18:19.200
So, I think that it's like,

18:19.200 --> 18:21.040
this is, I think, an end up affecting

18:21.040 --> 18:25.360
basically every area of the products and the economy.

18:25.360 --> 18:27.200
The thing you mentioned about an AI

18:27.200 --> 18:29.040
that can just go out and do something for you

18:29.040 --> 18:31.520
that's multi-step, is that a bigger model?

18:31.520 --> 18:34.240
Is that you'll make, like, Lama 4 will still,

18:34.240 --> 18:35.840
there'll be a version that's still 70B,

18:35.840 --> 18:38.000
but will just be, you'll just train it on the right data,

18:38.000 --> 18:39.840
and that will be super powerful.

18:39.840 --> 18:41.280
Like, what does the progression look like?

18:41.280 --> 18:43.600
Is it scaling? Is it just same size,

18:43.600 --> 18:45.680
but different banks like you were talking about?

18:49.200 --> 18:51.600
I don't know that we know the answer to that.

18:51.600 --> 18:55.840
So, I think one thing that seems to be a pattern

18:55.840 --> 18:58.000
is that you have the Lama,

18:58.000 --> 18:59.680
sorry, the Lama model,

18:59.680 --> 19:04.000
and then you build some kind of other

19:04.000 --> 19:06.320
application-specific code around it, right?

19:06.320 --> 19:08.640
So, some of it is the fine-tuning for the use case,

19:08.640 --> 19:12.560
but some of it is just like logic for, okay, how,

19:14.320 --> 19:16.240
like, how Met AI should integrate,

19:17.040 --> 19:19.280
that should work with tools like Google or Bing

19:19.280 --> 19:20.560
to bring in real-time knowledge.

19:20.560 --> 19:22.080
I mean, that's not part of the base Lama model.

19:22.080 --> 19:22.960
That's like part of it.

19:22.960 --> 19:25.840
Okay, so, for Lama 2, we had some of that,

19:26.640 --> 19:29.440
and it was a little more kind of hand-engineered.

19:29.440 --> 19:31.440
And then part of our goal for Lama 3

19:32.080 --> 19:34.480
was to bring more of that into the model itself.

19:35.120 --> 19:36.320
And, but for Lama 3,

19:36.320 --> 19:39.440
as we start getting into more of these agent-like behaviors,

19:40.000 --> 19:42.880
I think some of that is going to be more hand-engineered.

19:42.880 --> 19:44.960
And then I think our goal for Lama 4

19:44.960 --> 19:46.800
will be to bring more of that into the model.

19:46.800 --> 19:50.080
So, I think at each point, like at each step along the way,

19:50.080 --> 19:53.280
you kind of have a sense of what's going to be possible

19:53.280 --> 19:54.000
on the horizon.

19:54.000 --> 19:56.080
You start messing with it and hacking around it.

19:56.880 --> 19:59.360
And then I think that that helps you hone your intuition

19:59.680 --> 20:01.520
for what you want to try to train

20:01.520 --> 20:03.440
into the next version of the model itself.

20:03.440 --> 20:03.920
Interesting.

20:03.920 --> 20:04.960
Which makes it more general,

20:04.960 --> 20:07.200
because obviously anything that you're hand-coding

20:07.200 --> 20:10.240
is, you know, you can unlock some use cases,

20:10.240 --> 20:12.320
but it's just inherently brittle and non-general.

20:13.520 --> 20:14.400
Hey, everybody.

20:14.400 --> 20:16.720
Real quick, I want to tell you about a tool

20:16.720 --> 20:19.120
that I wish more applications used.

20:19.120 --> 20:21.920
So, obviously, you've noticed every single company

20:21.920 --> 20:25.120
is trying to add an AI chatbot to their website.

20:25.120 --> 20:28.480
But as a user, I usually find them really annoying

20:28.480 --> 20:31.600
because they give these long, generic, often useless answers.

20:32.320 --> 20:35.360
Command Bar is a user assistant that you can just embed

20:35.360 --> 20:36.880
into your website or application.

20:37.440 --> 20:40.640
And it feels like you're talking to a friendly human support

20:40.640 --> 20:43.440
agent who is browsing with you and for you.

20:44.000 --> 20:47.520
And it's much more personalized than a regular chatbot.

20:47.520 --> 20:49.440
It can actually look up users' history

20:49.440 --> 20:51.600
and respond differently based on that.

20:51.600 --> 20:54.560
It can use APIs to perform actions.

20:54.560 --> 20:58.080
It can even practically nudge users to explore new features.

20:58.560 --> 21:00.160
One thing that I think is really cool

21:00.160 --> 21:02.640
is that instead of just outputting text,

21:02.640 --> 21:05.680
Command Bar can kind of just say, here, let me show you

21:05.680 --> 21:07.920
and start browsing alongside the user.

21:08.560 --> 21:11.120
Anyways, they're in a bunch of great products already.

21:11.120 --> 21:14.960
You can learn more about them at commandbar.com.

21:15.520 --> 21:17.520
Thanks to them for sponsoring this episode.

21:17.520 --> 21:18.560
And now back to Mark.

21:19.120 --> 21:20.560
What do you say into the model itself?

21:20.560 --> 21:24.080
You train it on the thing that you want in the model itself?

21:24.080 --> 21:25.920
What do you mean by into the model itself?

21:25.920 --> 21:29.520
Well, I think the example that I gave for Llama 2,

21:29.520 --> 21:37.040
where for Llama 2, the tool use was very specific.

21:37.920 --> 21:40.800
Whereas Llama 3 has the ability to have much better tool use.

21:40.800 --> 21:44.080
So we don't have to hand code all the stuff

21:44.080 --> 21:47.280
to have it use Google to go do a search.

21:48.160 --> 21:49.680
It just kind of can do that.

21:51.840 --> 21:54.800
And similarly for coding and kind of running code

21:54.800 --> 21:56.640
and a bunch of stuff like that.

21:59.040 --> 22:01.040
But I think once you kind of get that capability,

22:01.920 --> 22:04.960
then you get a peek of, okay, well, what can we start doing next?

22:04.960 --> 22:07.760
Okay, well, I don't necessarily want to wait until Llama 4 is around

22:07.760 --> 22:09.280
to start building those capabilities.

22:09.280 --> 22:10.640
So let's start hacking around it.

22:10.640 --> 22:13.040
And so you do a bunch of hand coding

22:13.040 --> 22:16.320
and that makes the products better for the interim.

22:16.320 --> 22:18.240
But then that also helps show the way

22:18.240 --> 22:21.280
of what we want to try to build into the next version of the model.

22:21.280 --> 22:23.600
What is the community fine tune of Llama 3

22:23.600 --> 22:24.640
you're most excited by?

22:24.640 --> 22:26.400
Maybe not the one that will be most useful to you,

22:26.400 --> 22:28.320
but Jess, you'll just enjoy playing it with the most.

22:29.760 --> 22:31.200
They like fine tune it on antiquity

22:31.200 --> 22:32.960
and you'll just be like talking to Virgil or something.

22:32.960 --> 22:34.320
What are you excited about?

22:34.320 --> 22:34.880
I don't know.

22:36.320 --> 22:38.400
I mean, I think the nature of the stuff is it's like,

22:39.600 --> 22:41.200
you get surprised, right?

22:41.200 --> 22:44.960
So I think like any specific thing that I sort of

22:46.560 --> 22:49.520
thought would be valuable, we'd probably be building, right?

22:49.520 --> 22:54.960
So, but I think you'll get distilled versions.

22:54.960 --> 22:57.120
I think you'll get kind of smaller versions.

22:57.120 --> 23:01.760
I mean, one thing that I think is 8 billion,

23:01.760 --> 23:05.440
I don't think is quite small enough for a bunch of use cases, right?

23:05.440 --> 23:09.680
I think like over time, I'd love to get a billion parameter model

23:09.680 --> 23:11.680
or a 2 billion parameter model

23:11.680 --> 23:14.720
or even like a, I don't know, maybe like a 500 million parameter model

23:14.720 --> 23:15.680
and see what you can do with that.

23:15.680 --> 23:19.520
Because I mean, as they start getting, if with 8 billion parameters

23:19.520 --> 23:23.440
we're basically nearly as powerful as the largest llama 2 model,

23:23.440 --> 23:25.360
then with a billion parameters,

23:25.360 --> 23:27.200
you should be able to do something that's interesting, right?

23:27.200 --> 23:30.720
And faster, good for classification

23:30.720 --> 23:33.200
or a lot of kind of like basic things that people do

23:33.200 --> 23:37.920
before kind of understanding the intent of a user query

23:37.920 --> 23:39.600
and feeding it to the most powerful model

23:39.600 --> 23:42.320
to kind of hone what the prompt should be.

23:44.000 --> 23:44.480
So I don't know.

23:44.480 --> 23:46.640
I think that's one thing that maybe the community can help fill in.

23:46.640 --> 23:49.200
But I mean, we'll also, we're also thinking about getting around

23:49.200 --> 23:51.520
to distilling some of these ourselves,

23:51.520 --> 23:55.840
but right now the GPUs are pegged training the 405.

23:55.840 --> 23:57.840
So what, okay, so you have all these GPUs,

24:00.080 --> 24:02.080
I think 350,000 by the end of the year.

24:02.080 --> 24:02.960
That's the whole fleet.

24:02.960 --> 24:10.000
I mean, we built two, I think it's like 22, 24,000 clusters

24:10.000 --> 24:12.320
that are kind of the single clusters that we have

24:12.320 --> 24:13.440
for training the big models.

24:13.840 --> 24:16.000
I mean, obviously across a lot of the stuff that we do,

24:16.000 --> 24:19.120
a lot of our stuff goes towards training like reels models

24:19.120 --> 24:22.160
and like Facebook news feed and Instagram feed.

24:22.160 --> 24:23.840
And then inference is a huge thing for us

24:23.840 --> 24:25.360
because we serve a ton of people, right?

24:25.360 --> 24:32.320
So our ratio of inference compute required to training

24:32.320 --> 24:34.640
is probably much higher than most other companies

24:34.640 --> 24:36.800
that are doing this stuff just because of the sheer volume

24:36.800 --> 24:38.800
of the community that we're serving.

24:38.800 --> 24:39.840
Yeah, yeah.

24:39.840 --> 24:42.240
That was really interesting in the material they shared with me before

24:42.240 --> 24:45.200
that you trained it on more data than is computer optimal

24:45.200 --> 24:47.680
just for training because the inference is such a big deal

24:47.680 --> 24:49.360
for you guys and also for the community

24:49.360 --> 24:50.960
that it makes sense to just have this thing

24:50.960 --> 24:52.800
and have a trillion to tokens in there.

24:52.800 --> 24:53.520
Yeah, yeah.

24:53.520 --> 24:55.920
Although, and one of the interesting things about it

24:55.920 --> 24:58.320
that we saw even with the 70 billion is we thought

24:58.320 --> 25:02.240
it would get more saturated at, you know,

25:02.240 --> 25:04.720
it's like we trained on around 15 trillion tokens.

25:04.720 --> 25:05.520
Yeah.

25:05.520 --> 25:07.920
We, I guess our prediction going in was that

25:08.560 --> 25:10.720
it was going to ask some to it more,

25:10.720 --> 25:13.920
but even by the end it was still learning, right?

25:13.920 --> 25:17.200
It's like we probably could have fed it more tokens

25:17.200 --> 25:19.040
and it would have gotten somewhat better.

25:19.040 --> 25:21.200
But I mean, at some point, you know, you're running a company

25:21.200 --> 25:24.240
you need to do these meta reasoning questions of like,

25:24.240 --> 25:26.400
all right, how do I want to spend our GPUs

25:26.400 --> 25:29.040
on like training this 70 billion model further?

25:29.040 --> 25:31.280
Do we want to kind of get on with it

25:31.280 --> 25:33.840
so we can start testing hypotheses for Llama 4?

25:33.840 --> 25:36.800
So we kind of needed to make that call.

25:36.800 --> 25:38.000
And I think we got it,

25:38.000 --> 25:39.280
I think we got to a reasonable balance

25:39.760 --> 25:41.200
for this version of the 70 billion.

25:42.880 --> 25:44.080
There will be others in the future

25:44.080 --> 25:45.760
where, you know, 70 billion multimodal one

25:45.760 --> 25:47.760
that'll come over the next period.

25:47.760 --> 25:51.280
But yeah, I mean, that was fascinating

25:51.280 --> 25:53.280
that you could just, that it's the architectures

25:53.280 --> 25:55.600
at this point can just take so much data.

25:55.600 --> 25:56.400
Yeah, that's really interesting.

25:56.400 --> 25:58.240
So what is this imply by future models?

25:58.960 --> 26:02.320
You mentioned that the Llama 3 8B is better

26:02.320 --> 26:03.520
than the Llama 270B?

26:03.520 --> 26:04.720
No, no, no, it's nearly as good.

26:04.720 --> 26:05.440
Okay.

26:05.440 --> 26:06.320
I don't overstep.

26:06.320 --> 26:07.440
But does that mean like the Llama 4?

26:07.440 --> 26:08.320
The same order of magnitude.

26:08.400 --> 26:09.280
Does that mean like the Llama 4?

26:09.280 --> 26:10.800
70B will be as good as the Llama 3?

26:10.800 --> 26:11.760
4 or 5B?

26:11.760 --> 26:15.120
I mean, this is one of the great questions, right?

26:15.120 --> 26:17.920
That I think no one knows is basically,

26:19.680 --> 26:22.800
you know, it's one of the trickiest things in the world

26:22.800 --> 26:25.280
to plan around is when you have an exponential curve,

26:25.280 --> 26:26.880
how long does it keep going for?

26:27.520 --> 26:32.080
And I think it's likely enough that it will keep going,

26:32.080 --> 26:36.240
that it is worth investing the tens or, you know,

26:36.320 --> 26:39.360
100 billion plus in building the infrastructure

26:39.360 --> 26:42.480
to assume that if that kind of keeps going,

26:42.480 --> 26:44.720
you're going to get some really amazing things

26:44.720 --> 26:46.480
that are just going to make amazing products.

26:47.040 --> 26:50.640
But I don't think anyone in the industry can really tell you

26:51.600 --> 26:55.200
that it will continue scaling at that rate for sure, right?

26:55.200 --> 26:58.640
In general, in history, you hit bottlenecks at certain points.

26:58.640 --> 27:00.800
And now there's so much energy on this

27:00.800 --> 27:03.920
that maybe those bottlenecks get knocked over pretty quickly.

27:03.920 --> 27:08.080
But I don't know. I think that's an interesting question.

27:08.080 --> 27:11.120
What does the world look like where there aren't these bottlenecks?

27:11.120 --> 27:14.240
Suppose like progress just continues at this pace,

27:14.240 --> 27:17.520
which seems like plausible, like zooming out.

27:17.520 --> 27:19.840
Well, they're going to be different bottlenecks.

27:20.560 --> 27:22.800
Right. So if not training, then like, oh, yeah, go ahead.

27:23.680 --> 27:27.600
Well, I think at some point, over the last few years,

27:27.600 --> 27:31.200
I think there was this issue of GPU production.

27:31.200 --> 27:33.760
Yeah. Right. So even companies that had the models,

27:35.200 --> 27:37.120
sorry, that had the money to pay for the GPUs,

27:38.800 --> 27:40.640
couldn't necessarily get as many as they wanted

27:40.640 --> 27:43.120
because there were all these supply constraints.

27:43.120 --> 27:45.680
Now I think that's sort of getting less.

27:46.240 --> 27:50.320
So now I think you're seeing a bunch of companies think about,

27:50.320 --> 27:52.640
wow, we should just like really invest a lot of money

27:52.640 --> 27:53.840
in building out these things.

27:53.840 --> 27:56.960
And I think that that will go for some period of time.

27:57.840 --> 28:02.400
I think there's a, there is a capital question of like, okay,

28:03.280 --> 28:06.640
at what point does it stop being worth it to put the capital in?

28:06.640 --> 28:09.040
But I actually think before we hit that,

28:09.040 --> 28:11.120
you're going to run into energy constraints.

28:11.120 --> 28:14.080
Right. Because I just, I mean,

28:14.080 --> 28:19.120
I don't think anyone's built a gigawatt single training cluster yet.

28:19.120 --> 28:21.600
Right. And then you run into these things

28:21.600 --> 28:23.200
that just end up being slower in the world.

28:23.200 --> 28:26.400
Like getting energy permitted

28:26.400 --> 28:31.040
is like a very heavily regulated government function.

28:31.040 --> 28:34.320
Right. So you're going from on the one hand software,

28:34.320 --> 28:36.240
which is somewhat regulated.

28:36.240 --> 28:38.320
I'd argue that it is more regulated

28:38.320 --> 28:41.360
than I think a lot of people in the tech community feel,

28:41.360 --> 28:42.480
although it's obviously different.

28:42.480 --> 28:43.680
If you're starting a small company,

28:43.680 --> 28:45.680
maybe you feel that less if you're a big company,

28:45.680 --> 28:47.360
you know, we just interact with people,

28:47.360 --> 28:49.440
but different governments and regulators are,

28:49.440 --> 28:52.560
you know, we have kind of lots of rules

28:52.560 --> 28:53.600
that we need to kind of follow

28:53.600 --> 28:55.280
and make sure we do a good job with around the world.

28:56.720 --> 28:58.800
But I think that there's no doubt that like energy,

28:58.800 --> 29:02.720
and if you're talking about building large new power plants

29:02.720 --> 29:05.520
or large buildouts and then building transmission lines

29:05.520 --> 29:09.920
that cross other private or public land,

29:09.920 --> 29:11.760
that is just a heavily regulated thing.

29:11.760 --> 29:14.480
So you're talking about many years of lead time.

29:14.480 --> 29:18.800
So if we wanted to stand up to some like massive facility

29:18.800 --> 29:23.040
to power that, I think that that is,

29:24.080 --> 29:26.800
that's a very long-term project, right?

29:26.800 --> 29:29.600
And so I don't know, I think that that's,

29:29.600 --> 29:30.960
I think people will do it,

29:30.960 --> 29:33.280
but I don't think that this is like something

29:33.280 --> 29:35.440
that can be quite as magical as just like,

29:35.440 --> 29:37.760
okay, you get a level of AI and you get a bunch of capital

29:37.760 --> 29:39.200
and you put it in and then like all of a sudden

29:39.200 --> 29:41.440
the models are just going to kind of like interest,

29:41.440 --> 29:43.840
like I think you do hit different bottlenecks along the way.

29:43.840 --> 29:46.320
Yeah. Is there something, a project,

29:46.320 --> 29:48.160
maybe I realized maybe not,

29:48.160 --> 29:51.360
that even a company like Meta doesn't have the resources for,

29:51.360 --> 29:54.640
like if your R&D budget or CapEx budget was 10x what it is now,

29:54.640 --> 29:56.800
then you could pursue it, like it's in the back of your mind,

29:56.800 --> 30:00.000
but Meta today, maybe you could like,

30:00.000 --> 30:01.680
even you can't even issue a stock or bond for it,

30:01.680 --> 30:03.520
it's like just 10x bigger than your budget.

30:03.520 --> 30:05.280
Well, I think energy is one piece, right?

30:06.400 --> 30:10.160
I think we would probably build out bigger clusters

30:10.160 --> 30:15.840
than we currently can if we could get the energy to do it.

30:15.840 --> 30:21.920
So I think that's fundamentally money bottlenecked in the limit,

30:21.920 --> 30:23.040
like if you had a trillion dollars.

30:23.040 --> 30:24.640
I think it's time, right?

30:26.320 --> 30:28.720
Well, if you look at it in terms of,

30:28.720 --> 30:31.520
but it depends on how far the exponential curves go, right?

30:31.520 --> 30:34.160
Like I think a number of companies are working on,

30:34.160 --> 30:36.800
you know, right now I think a lot of data centers

30:36.800 --> 30:39.120
are on the order of 50 megawatts or 100 megawatts,

30:39.120 --> 30:41.360
or like a big one might be 150 megawatts.

30:41.360 --> 30:44.160
Okay, so you take a whole data center and you fill it up with

30:44.240 --> 30:46.160
just all the stuff that you need to do for training

30:46.160 --> 30:47.520
and you build the biggest cluster you can.

30:47.520 --> 30:49.680
I think that's kind of,

30:49.680 --> 30:51.760
I think a bunch of companies are running at stuff like that.

30:53.200 --> 30:57.920
But then when you start getting into building a data center

30:57.920 --> 31:02.640
that's like 300 megawatts or 500 megawatts or a gigawatt,

31:02.640 --> 31:06.320
I mean, just no one has built a single gigawatt data center yet.

31:06.320 --> 31:07.600
So I think it will happen, right?

31:07.600 --> 31:08.800
I mean, this is only a matter of time,

31:08.800 --> 31:11.840
but it's not going to be like next year, right?

31:12.400 --> 31:16.240
I think that some of these things will take, I don't know,

31:17.040 --> 31:19.120
some number of years to build out.

31:19.120 --> 31:20.880
And then the question is, okay, well, if you,

31:22.400 --> 31:24.240
I mean, just to, I guess, put this in perspective,

31:25.440 --> 31:29.520
I think a gigawatt, it's like around the size of like

31:30.160 --> 31:32.400
a meaningful nuclear power plant

31:32.400 --> 31:34.800
only going towards training a model.

31:34.800 --> 31:36.400
Didn't Amazon do this?

31:36.400 --> 31:38.960
There's like, they have a 950 megawatt thing.

31:38.960 --> 31:40.560
Yeah, I'm not exactly sure what you did.

31:41.120 --> 31:42.240
What they did, you'd have to ask them.

31:43.840 --> 31:45.280
But it doesn't have to be in the same place, right?

31:45.280 --> 31:47.200
If distributed training works, it can be distributed.

31:47.200 --> 31:48.080
That I think is a big question.

31:48.080 --> 31:48.400
Yeah.

31:48.400 --> 31:49.920
Right, is basically how that's going to work.

31:49.920 --> 31:51.120
And I do think in the future,

31:52.160 --> 31:56.560
it seems quite possible that more of what we call training

31:56.560 --> 32:01.680
for these big models is actually more along the lines

32:02.240 --> 32:04.880
of inference generating synthetic data

32:04.880 --> 32:06.560
to then go feed into the model.

32:06.560 --> 32:08.560
So I don't know what that ratio is going to be,

32:08.560 --> 32:12.400
but I consider the generation of synthetic data

32:12.400 --> 32:14.480
to be more inference than training today.

32:14.480 --> 32:16.800
But obviously, if you're doing it in order to train a model,

32:16.800 --> 32:18.720
it's part of the broader training process.

32:19.280 --> 32:23.440
So I don't know, that's an open question,

32:23.440 --> 32:25.440
is to kind of where, what the balance of that

32:25.440 --> 32:26.480
and how that plays out.

32:26.480 --> 32:29.760
If that's the case, would that potentially also

32:29.760 --> 32:31.200
be the case with Lama 3?

32:31.200 --> 32:34.000
And maybe like Lama 4 onwards, where you put this out

32:34.000 --> 32:35.840
and if somebody has a ton of compute,

32:35.840 --> 32:37.680
then using the models that you've put out,

32:37.760 --> 32:39.920
you can just keep making these things arbitrarily smarter.

32:41.120 --> 32:44.720
Some Kuwait or UAE or some random country has a ton of compute,

32:45.600 --> 32:48.240
and they can just actually just use Lama 4

32:48.240 --> 32:49.440
to just make something much smarter.

32:52.240 --> 32:55.600
I do think that there are going to be dynamics like that,

32:55.600 --> 33:00.960
but I also think that there is a fundamental limitation

33:01.520 --> 33:05.760
on kind of the network architecture,

33:06.320 --> 33:08.080
the kind of model architecture.

33:08.080 --> 33:11.360
So I think like a 70 billion model

33:12.320 --> 33:14.560
that kind of we trained with the Lama 3 architecture

33:14.560 --> 33:16.800
can get better, it can keep going.

33:16.800 --> 33:20.160
Like I was saying, we felt like if we kept on feeding it

33:20.160 --> 33:24.080
more data or rotated the high value tokens through again,

33:24.080 --> 33:26.160
then it would continue getting better.

33:26.880 --> 33:31.520
But, and we've seen a bunch of other people around the world,

33:32.480 --> 33:35.280
you know, different companies basically take the Lama 2

33:35.920 --> 33:38.240
70 billion base, like take that model architecture

33:38.240 --> 33:39.200
and then build a new model.

33:41.360 --> 33:44.160
It's still the case that when you make a generational improvement

33:44.160 --> 33:47.200
to the kind of Lama 3 70 billion or the Lama 3 405,

33:47.200 --> 33:49.920
there's nothing open source anything like that today, right?

33:49.920 --> 33:53.120
Like it's not, I think that that's like,

33:53.120 --> 33:54.960
it's a big step function

33:54.960 --> 33:57.120
and what people are going to be able to build on top of

33:57.120 --> 33:59.760
that I don't think can go infinitely from there.

33:59.840 --> 34:02.480
I think it can, there can be some optimization in that

34:02.480 --> 34:03.920
until you get to the next step function.

34:04.800 --> 34:05.680
Yeah. Okay.

34:05.680 --> 34:08.720
So let's zoom out a little bit from specific models

34:08.720 --> 34:12.000
and even the many years lead times you would need

34:12.000 --> 34:13.920
to get energy approvals and so on.

34:13.920 --> 34:16.240
Like big picture, these next couple of decades,

34:16.240 --> 34:17.280
what's happening with AI?

34:18.080 --> 34:20.000
Does it feel like another technology,

34:20.000 --> 34:21.440
like metaverse or social,

34:21.440 --> 34:23.520
or does it feel like a fundamentally different thing

34:23.520 --> 34:24.720
in the course of human history?

34:25.600 --> 34:30.000
I think it's going to be pretty fundamental.

34:30.000 --> 34:33.280
I think it's going to be more like the creation

34:33.280 --> 34:36.160
of computing in the first place, right?

34:36.160 --> 34:41.840
So you'll get all these new apps in the same way

34:42.640 --> 34:45.440
that when you got the web or you got mobile phones,

34:45.440 --> 34:48.800
you got like people basically rethought all these experiences

34:48.800 --> 34:50.400
and a lot of things that weren't possible

34:50.400 --> 34:51.760
before now became possible.

34:52.080 --> 34:53.440
Something that will happen,

34:53.440 --> 34:56.240
but I think it's a much lower level innovation.

34:56.240 --> 34:59.040
It's going to be more like going from

34:59.040 --> 35:02.240
people didn't have computers to people have computers,

35:02.240 --> 35:03.200
is my sense.

35:05.360 --> 35:09.440
But it's also, it's, I don't know,

35:09.440 --> 35:14.400
it's very hard to reason about exactly how this goes.

35:14.400 --> 35:17.440
I tend to think that, you know,

35:17.440 --> 35:19.120
in like the cosmic scale, obviously,

35:19.360 --> 35:22.480
it'll happen quickly over a couple of decades or something.

35:22.480 --> 35:25.680
But I do think that there is some set of people

35:25.680 --> 35:27.680
who are afraid of like, you know,

35:27.680 --> 35:30.080
it really just kind of spins and goes from being

35:30.080 --> 35:32.960
like somewhat intelligent to extremely intelligent overnight.

35:32.960 --> 35:34.880
And I just think that there's all these physical constraints

35:34.880 --> 35:37.360
that make that, so that that's unlikely to happen.

35:37.360 --> 35:41.280
I just don't, I don't really see that playing out.

35:41.280 --> 35:42.480
So I think you'll have,

35:42.480 --> 35:44.720
I think we'll have time to kind of acclimate a bit,

35:44.720 --> 35:46.320
but it will really change.

35:46.320 --> 35:49.760
The way that we work and give people all these creative tools

35:49.760 --> 35:52.880
to do different things that they, yeah.

35:52.880 --> 35:54.560
I think it's going to be,

35:54.560 --> 35:56.720
it's going to really enable people to do

35:56.720 --> 35:59.120
the things that they want a lot more, as is my view.

36:00.400 --> 36:01.840
Okay, so maybe not overnight,

36:01.840 --> 36:04.560
but is it your view that like on a cosmic scale,

36:04.560 --> 36:08.000
if you think like humans evolved and then like AI happened

36:08.000 --> 36:10.000
and then they like went out through the galaxy

36:10.000 --> 36:13.200
or maybe it takes many decades, maybe it takes a century,

36:13.200 --> 36:14.720
but like, you know,

36:14.960 --> 36:16.320
is that like the grand scheme

36:16.320 --> 36:18.000
of what's happening right now in history?

36:19.600 --> 36:20.640
Sorry, in what sense?

36:20.640 --> 36:22.800
I mean, in the sense that there were other technologies

36:22.800 --> 36:24.160
like computers and even like fire,

36:24.160 --> 36:26.800
but like the AI happening is as significant

36:26.800 --> 36:28.640
as like humans evolving in the first place.

36:29.600 --> 36:31.200
I think that's tricky.

36:31.200 --> 36:35.040
I think people like to, you know,

36:35.040 --> 36:37.360
the history of humanity, I think has been

36:38.240 --> 36:40.640
people basically, you know,

36:40.640 --> 36:42.400
thinking that certain things

36:43.360 --> 36:51.200
of humanity are like really unique in different ways.

36:51.840 --> 36:55.600
And then coming to grips with the fact

36:55.600 --> 36:56.480
that that's not true,

36:56.480 --> 36:59.280
but humanity is actually still super special, right?

36:59.280 --> 37:03.920
So it's like we thought that the earth

37:03.920 --> 37:05.200
was the center of the universe.

37:05.200 --> 37:08.400
And it's like, it's not, but like humans

37:08.400 --> 37:09.840
are still pretty awesome, right?

37:09.840 --> 37:11.280
And pretty unique.

37:12.480 --> 37:14.720
I think that another bias that people tend to have

37:15.520 --> 37:18.000
is thinking that intelligence is somehow

37:20.160 --> 37:23.360
kind of fundamentally connected to life.

37:24.000 --> 37:26.960
And it's not actually clear that it is, right?

37:26.960 --> 37:28.960
I think like people think that,

37:31.520 --> 37:32.880
I mean, I don't know that we have

37:32.880 --> 37:34.800
a clear enough definition of consciousness

37:34.800 --> 37:39.920
or life to kind of fully interrogate this,

37:39.920 --> 37:43.280
but there's all this science fiction about,

37:43.280 --> 37:44.880
okay, you create intelligence

37:44.880 --> 37:47.520
and now it like starts taking on all these human

37:47.520 --> 37:50.320
like behaviors and things like that.

37:50.320 --> 37:52.560
But I actually think that the current incarnation

37:52.560 --> 37:54.400
of all this stuff at least kind of feels

37:54.400 --> 37:55.440
like it's going in a direction

37:55.440 --> 37:57.760
where intelligence can be pretty separated

37:57.760 --> 38:01.200
from consciousness and agency and things like that,

38:01.200 --> 38:05.040
that I think just makes it a super valuable tool.

38:05.040 --> 38:06.000
So I don't know.

38:06.000 --> 38:08.160
I mean, obviously it's very difficult to predict

38:08.160 --> 38:10.320
what direction the stuff goes in over time,

38:10.320 --> 38:14.160
which is why I don't think anyone should be dogmatic

38:14.160 --> 38:17.360
about how they plan to develop it or what they plan to do.

38:17.360 --> 38:19.680
I think you want to kind of look at like each release.

38:19.680 --> 38:21.920
You know, it's like, we're obviously very pro open source,

38:21.920 --> 38:23.520
but I haven't committed that we're going to like release

38:23.520 --> 38:24.880
every single thing that we do.

38:24.880 --> 38:29.120
But it's basically, I'm just generally very inclined

38:29.120 --> 38:30.320
to thinking that open sourcing it

38:30.320 --> 38:32.640
is going to be good for the community

38:32.640 --> 38:33.840
and also good for us, right?

38:33.840 --> 38:36.320
Because we'll benefit from the innovations.

38:36.560 --> 38:40.080
But if at some point like there's some qualitative change

38:40.080 --> 38:42.000
in what the thing is capable of,

38:42.000 --> 38:44.720
and we feel like it's just not responsible to open source it,

38:44.720 --> 38:48.640
then we won't, but so I don't know.

38:48.640 --> 38:50.640
It's all very difficult to predict.

38:50.640 --> 38:51.600
Yeah.

38:51.600 --> 38:53.520
What is a kind of qualitative change,

38:53.520 --> 38:54.800
like a specific thing?

38:54.800 --> 38:57.920
You're training lamify, lamaphore, and you've seen this,

38:57.920 --> 38:59.360
and like, you know what?

38:59.360 --> 39:00.560
I'm not sure about open sourcing it.

39:00.560 --> 39:06.800
I think that that, it's a little hard to answer that

39:06.800 --> 39:10.160
in the abstract because there are negative behaviors

39:10.800 --> 39:14.320
that any product can exhibit that as long as you can mitigate it,

39:14.960 --> 39:17.440
it's like, it's okay, right?

39:17.440 --> 39:20.640
So, I mean, there's bad things about social media

39:20.640 --> 39:22.080
that we work to mitigate, right?

39:22.080 --> 39:24.000
There's bad things about llama two

39:24.000 --> 39:26.000
that we spend a lot of time trying to make sure

39:26.000 --> 39:29.440
that it's not like, you know, helping people commit violent acts

39:29.440 --> 39:30.400
or things like that, right?

39:30.400 --> 39:34.480
I mean, that doesn't mean that it's like a kind of autonomous

39:34.480 --> 39:35.760
or intelligent agent.

39:35.760 --> 39:37.840
It just means that it's learned a lot about the world,

39:37.840 --> 39:39.280
and it can answer a set of questions

39:39.280 --> 39:41.840
that we think it would be unhelpful for it to answer.

39:43.440 --> 39:47.200
So, I don't know.

39:47.200 --> 39:51.680
I think the question isn't really what behaviors would it show.

39:51.680 --> 39:54.000
It's what things would we not be able to mitigate

39:54.000 --> 39:58.480
after it shows that, and I don't know.

39:59.120 --> 40:01.520
I think that there's so many ways

40:01.520 --> 40:03.440
in which something can be good or bad

40:03.440 --> 40:05.360
that it's hard to actually enumerate them all up front.

40:05.360 --> 40:10.320
If you even look at what we've had to deal with in social media

40:10.320 --> 40:12.880
and the different types of harms, we've basically gotten to.

40:12.880 --> 40:16.240
It's like, there's like 18 or 19 categories of harmful things

40:16.240 --> 40:19.840
that people do, and we've basically built AI systems

40:19.840 --> 40:22.080
to try to go identify what those things are

40:22.080 --> 40:23.520
that people are doing and try to make sure

40:23.520 --> 40:26.400
that that doesn't happen on our network as much as possible.

40:26.400 --> 40:28.400
So, yeah, I think you can...

40:28.400 --> 40:30.000
Over time, I think you'll be able to break down

40:31.600 --> 40:33.200
this into more of a taxonomy, too,

40:33.200 --> 40:36.000
and I think this is a thing that we spend time researching, too,

40:36.000 --> 40:37.360
because we want to make sure that we understand that.

40:38.240 --> 40:40.080
So, one of the things I asked Mark

40:40.080 --> 40:44.000
is what industrial-scale use of LLMs would look like.

40:44.000 --> 40:46.000
You see this in previous technological revolutions

40:46.000 --> 40:48.400
where, at first, they're thinking in a very small-scale way

40:48.400 --> 40:49.440
about what's enabled,

40:49.440 --> 40:52.080
and I think that's what chatbots might be for LLMs.

40:52.080 --> 40:53.920
And I think the large-scale use case

40:53.920 --> 40:56.480
might look something like what V7 Go is.

40:56.480 --> 40:58.400
And, by the way, it's made by V7 Labs

40:58.400 --> 40:59.680
who's sponsoring this episode.

41:00.240 --> 41:02.160
So, it's like a spreadsheet.

41:02.160 --> 41:06.160
You put in raw information, like documents, images, whatever,

41:06.160 --> 41:07.600
and they become rows,

41:07.600 --> 41:11.120
and the columns are populated by an LLM of your choice.

41:11.120 --> 41:13.680
And, in fact, I used it to prepare for Mark,

41:13.680 --> 41:16.160
so I fed in a bunch of blog posts and papers

41:16.160 --> 41:17.920
from Metas AI Research,

41:17.920 --> 41:20.080
and, as you can see, if you're on YouTube,

41:20.080 --> 41:23.280
it summarizes and extracts exactly the information I want

41:23.360 --> 41:24.320
as columns.

41:24.320 --> 41:26.480
And, obviously, mine is a small use case,

41:26.480 --> 41:29.440
but you can imagine, for example, a company like FedEx

41:29.440 --> 41:32.080
has to process half a million documents a day.

41:32.080 --> 41:34.080
Obviously, a chatbot can't do that.

41:34.080 --> 41:35.280
A spreadsheet can,

41:35.280 --> 41:37.440
because this is just like a fire hose of intelligence

41:37.440 --> 41:38.480
in there, right?

41:38.480 --> 41:40.160
Anyways, you can learn more about them

41:40.160 --> 41:42.560
at v7labs.com slash go,

41:42.560 --> 41:44.160
or the link in the description.

41:44.160 --> 41:45.280
Back to Mark.

41:45.280 --> 41:45.840
Yeah.

41:45.840 --> 41:48.480
Like, it seems to me it would be a good idea.

41:48.480 --> 41:50.000
I would be disappointed in a future

41:50.000 --> 41:51.680
where AI systems aren't broadly deployed

41:51.680 --> 41:53.040
and everybody doesn't have access to them.

41:54.000 --> 41:54.960
At the same time,

41:54.960 --> 41:57.120
I want to better understand the mitigations,

41:58.000 --> 42:00.480
because if the mitigation is the fine-tuning,

42:00.480 --> 42:02.320
well, the whole thing about open weights

42:02.320 --> 42:05.680
is that you can then remove the fine-tuning,

42:05.680 --> 42:07.920
which is often superficial on top of these capabilities.

42:07.920 --> 42:10.240
Like, if it's like talking on Slack

42:10.240 --> 42:12.000
with a biology researcher,

42:12.000 --> 42:14.000
and again, I think models are very far from this.

42:14.000 --> 42:15.120
Right now, they're like Google search,

42:16.160 --> 42:17.920
but I can show them my Petri disk

42:17.920 --> 42:18.560
and they can next lane.

42:18.560 --> 42:21.680
Like, here's why your smallpox sample didn't grow.

42:21.680 --> 42:22.400
Here's what to change.

42:23.600 --> 42:24.720
How do you mitigate that?

42:24.720 --> 42:27.040
Because somebody can just fine-tune that in there, right?

42:27.840 --> 42:30.640
Yeah. I mean, that's true.

42:30.640 --> 42:32.800
I think a lot of people will basically use

42:32.800 --> 42:34.880
the off-the-shelf model,

42:34.880 --> 42:38.320
and some people who have basically bad faith

42:38.320 --> 42:40.560
are going to try to strip out all the bad stuff,

42:40.560 --> 42:41.600
so I do think that that's an issue.

42:44.720 --> 42:46.720
The flip side of this is that,

42:46.720 --> 42:47.920
and this is one of the reasons

42:47.920 --> 42:51.280
why I'm kind of philosophically so pro-open source,

42:52.160 --> 42:56.720
is I do think that a concentration of AI in the future

42:57.840 --> 43:00.240
has the potential to be as dangerous

43:00.880 --> 43:03.760
as kind of it being widespread.

43:03.760 --> 43:04.560
So, I think a lot of people,

43:05.360 --> 43:07.120
they think about the questions of,

43:07.120 --> 43:08.400
okay, well, if we can do this stuff,

43:08.400 --> 43:10.080
is it bad for it to be out wild?

43:10.080 --> 43:12.640
Like, just kind of widely available.

43:14.800 --> 43:16.720
I think another version of this is like,

43:16.720 --> 43:19.360
okay, well, it's probably also pretty bad

43:20.000 --> 43:24.800
for one institution to have an AI

43:24.800 --> 43:27.920
that is way more powerful than everyone else's AI, right?

43:27.920 --> 43:29.680
So, if you look at, like, I guess,

43:29.680 --> 43:32.240
one security analogy that I think of is,

43:34.240 --> 43:38.160
you know, it doesn't take AI to basically,

43:38.160 --> 43:40.560
okay, there's security holes in so many different things,

43:41.200 --> 43:44.720
and if you could travel back in time a year or two years,

43:44.720 --> 43:46.880
right, it's like, that's not AI,

43:46.880 --> 43:48.240
it's like you just, let's say you just have,

43:48.320 --> 43:50.240
like, one year or two years more knowledge

43:50.240 --> 43:51.520
of the security holes,

43:51.520 --> 43:53.760
it's pretty much hack into, like, any system, right?

43:53.760 --> 43:56.000
So, it's not that far-fetched to believe

43:56.800 --> 44:00.240
that a very intelligent AI would probably be able

44:00.240 --> 44:03.520
to identify some holes and basically be,

44:03.520 --> 44:04.800
like, a human who could potentially

44:04.800 --> 44:06.000
go back in time a year or two

44:06.000 --> 44:07.200
and compromise all these systems.

44:07.200 --> 44:09.840
Okay, so how have we dealt with that as a society?

44:09.840 --> 44:13.200
Well, one big part is open-source software

44:13.200 --> 44:14.720
that makes it so that when improvements

44:14.720 --> 44:16.080
are made to the software,

44:16.080 --> 44:17.680
it doesn't just kind of get stuck

44:17.680 --> 44:19.440
in one company's products,

44:19.440 --> 44:22.080
but it can kind of be broadly deployed

44:22.080 --> 44:23.440
to a lot of different systems,

44:23.440 --> 44:26.080
whether it's banks or hospitals or government stuff,

44:26.080 --> 44:28.000
and, like, just everyone can kind of,

44:28.000 --> 44:29.520
like, as the software gets hardened,

44:30.160 --> 44:31.840
which happens because more people can see it

44:31.840 --> 44:33.040
and more people can bang on it,

44:33.680 --> 44:35.680
and there are standards on how this stuff works,

44:37.040 --> 44:39.920
the world can kind of get upgraded together pretty quickly.

44:39.920 --> 44:43.680
And I kind of think that a world where AI

44:43.680 --> 44:45.440
is very widely deployed

44:45.760 --> 44:49.680
in a way where it's gotten hardened progressively over time

44:50.880 --> 44:54.240
is one where all the different systems will be in check

44:54.240 --> 44:56.880
in a way that seems like it is fundamentally

44:56.880 --> 44:58.000
more healthy to me

44:58.000 --> 45:00.160
than one where this is more concentrated.

45:00.160 --> 45:02.960
So there are risks on all sides,

45:02.960 --> 45:07.440
but I think that that's one risk that I think people,

45:07.440 --> 45:09.120
I don't hear them talking about quite as much.

45:09.120 --> 45:11.520
I think, like, there's sort of the risk of, like,

45:11.520 --> 45:13.520
okay, well, what if the AI system does something bad?

45:14.000 --> 45:18.400
I am more, like, you know, I stay up at night more worrying,

45:18.400 --> 45:22.240
well, what if, like, some actor that, whatever.

45:22.240 --> 45:23.360
It's like, from wherever you sit,

45:23.360 --> 45:25.360
there's going to be some actor who you don't trust

45:25.360 --> 45:27.520
if they're the ones who have, like, the super strong AI,

45:27.520 --> 45:29.360
whether it's some, like, other government

45:29.360 --> 45:32.880
that is sort of, like, an opponent of our country

45:32.880 --> 45:35.680
or some company that you don't trust or whatever it is.

45:37.680 --> 45:43.120
Like, I think that that's potentially a much bigger risk

45:43.200 --> 45:47.360
as in they could, like, overthrow our government

45:47.360 --> 45:50.080
because they have a weapon that, like, nobody else has.

45:50.080 --> 45:51.600
Cause a lot of mayhem.

45:51.600 --> 45:54.880
Right, it's, I think it's, like, I mean,

45:54.880 --> 45:57.360
I think the intuition is that this stuff ends up being

45:57.360 --> 46:00.960
pretty kind of important and valuable

46:00.960 --> 46:03.920
for both kind of economic and kind of security

46:03.920 --> 46:04.640
and other things.

46:04.640 --> 46:08.000
And I don't know, I just think, yeah, if, like,

46:08.000 --> 46:10.880
if someone who you don't trust or is an adversary of you

46:10.960 --> 46:12.960
gets something that is more powerful,

46:12.960 --> 46:15.120
then I think that that could be an issue.

46:15.120 --> 46:17.360
And I think probably the best way to mitigate that

46:17.360 --> 46:20.400
is to have good open source AI

46:20.400 --> 46:22.640
that basically becomes the standard

46:23.440 --> 46:26.160
and in a lot of ways kind of can become the leader.

46:26.160 --> 46:30.080
And in that way, it just ensures that it's a much more

46:30.080 --> 46:32.960
kind of even and balanced playing field.

46:32.960 --> 46:34.400
Yeah, that seems plausible to me.

46:34.400 --> 46:36.800
And if that works out, that would be the future I prefer.

46:37.920 --> 46:40.560
I guess I want to understand, like, mechanistically

46:40.640 --> 46:44.080
how if somebody was going to cause mayhem with AI systems,

46:44.080 --> 46:46.480
how the fact that there are other open source systems

46:46.480 --> 46:48.080
in the world prevents that?

46:48.080 --> 46:49.920
Like the specific example of, like,

46:49.920 --> 46:51.200
somebody coming with a bio weapon,

46:51.840 --> 46:53.280
is it just that we'll do a bunch of, like,

46:53.280 --> 46:55.040
R&D in the rest of the world to, like,

46:55.040 --> 46:56.400
figure out vaccines really fast?

46:56.400 --> 46:57.280
Like, what's happening?

46:57.280 --> 46:58.480
Would you take, like, the computer,

46:58.480 --> 47:00.640
the security one that I was talking about?

47:00.640 --> 47:02.400
I think someone with a weaker AI

47:02.400 --> 47:04.640
trying to hack into a system that is, like,

47:04.640 --> 47:07.040
protected by a stronger AI will succeed less.

47:07.760 --> 47:10.560
Right, so I think that that's, I mean,

47:10.560 --> 47:11.760
that's, like, in terms of software security.

47:11.760 --> 47:13.360
How do you know everything in the world is like that?

47:13.360 --> 47:15.040
Like, what if bio weapons aren't like that?

47:16.080 --> 47:17.920
No, I mean, I don't know that everything

47:17.920 --> 47:18.800
in the world is like that.

47:21.520 --> 47:24.160
I think that that's, I guess,

47:24.160 --> 47:26.080
one of the, bio weapons are one of the areas

47:26.080 --> 47:28.320
where I think the people who are most worried

47:28.320 --> 47:29.920
about this stuff are focused.

47:29.920 --> 47:33.200
And I think that that's, I think that makes

47:33.200 --> 47:34.320
a lot of sense to think about that.

47:35.200 --> 47:38.800
The, and I think that there are certain mitigations.

47:38.800 --> 47:41.600
You can try to not train certain knowledge

47:41.600 --> 47:42.880
into the model, right?

47:42.880 --> 47:45.760
There's different things, but, yeah,

47:45.760 --> 47:48.400
I mean, it's some level, I mean,

47:48.400 --> 47:50.880
if you get a sufficiently bad actor

47:50.880 --> 47:54.560
and you don't have other AI that can sort of balance them

47:54.560 --> 47:57.840
and understand what's going on and what the threats are,

47:57.840 --> 48:00.320
then that could be a risk.

48:00.320 --> 48:01.440
So I think that that's one of the things

48:01.440 --> 48:02.560
that we need to watch out for.

48:02.960 --> 48:06.640
Is there something you could see

48:06.640 --> 48:08.000
in the deployment of these systems

48:08.000 --> 48:12.160
where you observe like you're training Lama 4

48:12.160 --> 48:13.680
and it's like, it lied to you

48:13.680 --> 48:15.520
because you thought you were noticing or something.

48:15.520 --> 48:18.160
And you're like, whoa, what's going on here?

48:18.160 --> 48:20.480
Not that this is probably not likely

48:20.480 --> 48:21.440
with the Lama 4.0 system,

48:21.440 --> 48:23.360
but is there something you can imagine like that

48:23.360 --> 48:26.480
where you'd be really concerned about deceptiveness

48:26.480 --> 48:28.960
and if billions of copies of things are out in the wild?

48:29.840 --> 48:34.400
Yeah, I mean, I think that that's not necessarily,

48:34.400 --> 48:37.680
I mean, right now, we see a lot of hallucinations, right?

48:37.680 --> 48:39.360
So I think it's more that.

48:41.200 --> 48:42.640
I think it's an interesting question

48:42.640 --> 48:43.680
how you would tell the difference

48:43.680 --> 48:46.320
between a hallucination and deception.

48:46.320 --> 48:47.520
But yeah, I mean, look, I mean,

48:47.520 --> 48:49.920
I think there's a lot of risks and things to think about.

48:49.920 --> 48:55.120
The flip side of all this is that there are also a lot of,

48:56.080 --> 48:59.040
I try to, in running our company at least,

49:00.000 --> 49:05.120
balance what I think of as these longer term theoretical risks

49:07.440 --> 49:10.640
with what I actually think are quite real risks that exist today.

49:10.640 --> 49:14.400
So like when you talk about deception,

49:14.400 --> 49:16.080
the form of that that I worry about most

49:16.080 --> 49:18.560
is people using this to generate misinformation

49:18.560 --> 49:19.920
and then like pump that through

49:19.920 --> 49:21.840
whether it's our networks or others.

49:21.920 --> 49:25.680
So the way that we've basically combated

49:25.680 --> 49:27.840
a lot of this type of harmful content

49:28.400 --> 49:30.000
is by building AI systems

49:30.000 --> 49:32.240
that are smarter than the adversarial ones.

49:32.240 --> 49:33.920
And I guess this is part of,

49:33.920 --> 49:35.920
this kind of informs part of my theory on this, right?

49:35.920 --> 49:38.000
Is if you look at like the different types of harm

49:38.000 --> 49:41.680
that people do or try to do through social networks,

49:44.320 --> 49:46.800
there are ones that are not very adversarial.

49:46.800 --> 49:51.520
So for example, like hate speech,

49:51.520 --> 49:53.920
I would say is not super adversarial

49:53.920 --> 49:56.240
in the sense that like people aren't getting

49:57.280 --> 49:59.440
better at being racist, right?

49:59.440 --> 50:01.920
They're just like, it's, you just like, okay,

50:01.920 --> 50:04.560
if you kind of, that's one where I think the AIs

50:04.560 --> 50:07.680
are generally just getting way more sophisticated,

50:07.680 --> 50:09.600
faster than people are at those issues.

50:09.600 --> 50:11.360
So we have, and we have issues both ways.

50:11.360 --> 50:14.240
It's like people do bad things

50:14.240 --> 50:16.480
that whether they're trying to incite violence or something.

50:18.240 --> 50:19.840
But we also have a lot of false positives, right?

50:19.920 --> 50:22.240
So where we basically censor stuff that we shouldn't,

50:22.240 --> 50:25.280
and I think understandably make a lot of people annoyed.

50:25.280 --> 50:27.920
So I think having an AI that just gets increasingly

50:27.920 --> 50:30.560
precise on that, that's gonna be good over time.

50:30.560 --> 50:31.680
But let me give you another example,

50:31.680 --> 50:34.320
which is like nation states trying to interfere in elections.

50:34.880 --> 50:37.200
That's an example where they're absolutely,

50:37.200 --> 50:38.800
they have cutting edge technology

50:38.800 --> 50:41.440
and absolutely get better each year.

50:41.440 --> 50:44.640
So we block some technique, they learn what we did,

50:44.640 --> 50:46.640
they come at us with a different technique, right?

50:46.640 --> 50:52.240
It's not like a person trying to say mean things, right?

50:52.240 --> 50:55.120
It's like, they're basically, they have a goal,

50:55.120 --> 50:56.960
they're sophisticated, they have a lot of technology.

50:58.320 --> 51:00.320
In those cases, I still think the ability

51:00.320 --> 51:04.800
to kind of have RAI systems grow in sophistication

51:04.800 --> 51:07.680
at a faster rate than theirs have, it's an arms race,

51:07.680 --> 51:10.640
but I think we're at least currently winning that arms race.

51:12.240 --> 51:13.920
So I don't know, I think that that's,

51:13.920 --> 51:15.040
but this is like a lot of the stuff

51:15.040 --> 51:17.200
that I spend time thinking about is like, okay,

51:18.240 --> 51:22.240
yes, it is possible that whether it's llama four

51:22.240 --> 51:24.240
or llama five or llama six, yeah,

51:24.240 --> 51:26.480
we need to think about what behaviors we're observing

51:26.480 --> 51:27.200
and it's not just us.

51:27.200 --> 51:28.800
And part of the reason why you make this open source

51:28.800 --> 51:30.960
is that there are a lot of other people who study this too.

51:30.960 --> 51:34.240
So yeah, we wanna see what other people are observing,

51:34.240 --> 51:36.720
what we're observing, what we can mitigate,

51:36.720 --> 51:38.480
and then we'll make our assessment

51:38.480 --> 51:41.680
on whether we can make it open source.

51:41.680 --> 51:44.960
But I think for the foreseeable future,

51:44.960 --> 51:47.200
I'm optimistic we will be able to.

51:47.200 --> 51:51.040
And in the near term, I don't wanna take our eye off the ball

51:51.040 --> 51:52.960
of what our actual bad things

51:52.960 --> 51:54.960
that people are trying to use the models for today,

51:54.960 --> 51:56.160
even if they're not existential,

51:56.160 --> 52:00.560
but they're like pretty bad kind of day-to-day harms

52:00.560 --> 52:03.120
that we are familiar with in running our services.

52:04.880 --> 52:06.400
That's actually a lot of what we have to,

52:06.400 --> 52:07.680
I think, spend our time on as well.

52:07.680 --> 52:08.720
Yeah, yeah.

52:08.720 --> 52:11.440
Actually, I found the synthetic data thing really curious.

52:12.640 --> 52:14.640
I'm actually interested in why you don't think,

52:15.440 --> 52:16.720
like current models, it makes sense

52:16.720 --> 52:18.080
why there might be an asymptote

52:18.080 --> 52:20.480
with just doing the synthetic data again and again.

52:20.480 --> 52:22.320
If it gets smarter and uses the kind of techniques

52:22.320 --> 52:24.320
you talk about in the paper or the blog post

52:24.320 --> 52:26.880
that's coming out on the day this will be released

52:26.880 --> 52:30.080
where it goes to the thought chain

52:30.080 --> 52:32.480
that is the most correct.

52:33.040 --> 52:35.680
Why this wouldn't like lead to a loop

52:35.680 --> 52:37.200
that, of course, it wouldn't be overnight,

52:37.200 --> 52:38.800
but over many months or years of training,

52:38.800 --> 52:40.320
potentially, with a smarter model,

52:40.320 --> 52:41.920
it gets smarter, makes better output,

52:41.920 --> 52:43.040
gets smarter, and so forth.

52:45.280 --> 52:47.200
Well, I think it could within the parameter

52:47.200 --> 52:49.520
of whatever the model architecture is.

52:49.520 --> 52:53.920
It's just that at some level, I don't know,

52:54.560 --> 52:58.640
I think today is eight billion parameter models.

52:59.280 --> 53:02.560
I just don't think you're going to be able to get to be as good

53:02.560 --> 53:06.160
as the state-of-the-art multi-hundred billion

53:06.240 --> 53:08.720
parameter models that are incorporating new research

53:08.720 --> 53:10.080
into the architecture itself.

53:12.640 --> 53:14.160
But those will be open source as well, right?

53:14.880 --> 53:16.160
Well, yeah, but I think that that's,

53:17.200 --> 53:20.880
I mean, subject to all the questions

53:20.880 --> 53:21.760
that we just talked about.

53:21.760 --> 53:23.920
Yes, I mean, we would hope that that'll be the case,

53:23.920 --> 53:26.880
but I think that at each point, I don't know,

53:26.880 --> 53:29.360
it's like when you're building software,

53:29.360 --> 53:31.920
there's like a ton of stuff that you can do with software,

53:31.920 --> 53:33.840
but then at some level, you're constrained

53:33.840 --> 53:35.920
by the chips that it's running on.

53:36.240 --> 53:39.600
Right? So there are always going to be different

53:40.400 --> 53:41.680
physical constraints.

53:41.680 --> 53:44.960
And it's like how big are the models is going to be constrained

53:44.960 --> 53:49.840
by how much energy you can get and use for inference.

53:50.800 --> 53:56.160
So I guess I'm simultaneously very optimistic

53:56.160 --> 53:58.160
that this stuff will continue to improve quickly.

53:58.960 --> 54:05.600
And also a little more measured than I think

54:05.680 --> 54:09.520
some people are about kind of it's,

54:10.480 --> 54:12.800
I just don't think the runaway case

54:12.800 --> 54:15.040
is like a particularly likely one.

54:16.080 --> 54:18.320
I think it makes sense to keep your options open.

54:18.320 --> 54:19.440
Like there's so much we don't know.

54:20.480 --> 54:22.080
There's a case in which like it's really important

54:22.080 --> 54:23.040
to keep the balance of power.

54:23.040 --> 54:25.200
So when nobody becomes like a technology or a dictator,

54:25.200 --> 54:26.480
there's a case in which like,

54:26.480 --> 54:28.640
you don't want to open source the architecture

54:28.640 --> 54:32.240
because like China can use it to catch up to America's AIs

54:32.240 --> 54:33.840
and like there is an intelligence explosion

54:33.920 --> 54:34.800
and they like win that.

54:35.840 --> 54:37.120
Yeah, a lot of things are impossible.

54:37.120 --> 54:38.640
Just like keeping your options open,

54:38.640 --> 54:40.800
considering all of them seems reasonable.

54:40.800 --> 54:40.960
Yeah.

54:42.480 --> 54:43.520
Let's talk about some other things.

54:43.520 --> 54:44.080
Go for it.

54:44.080 --> 54:48.560
Okay. Metaverse, what time period in human history

54:48.560 --> 54:50.640
would you be most interested in going into?

54:50.640 --> 54:52.640
A 100,000 BCE to now.

54:52.640 --> 54:53.680
You just want to see what it was like.

54:53.680 --> 54:54.720
Well, that's through the past.

54:54.720 --> 54:55.120
Huh?

54:55.120 --> 54:55.840
It has to be the past.

54:55.840 --> 54:56.800
Oh yeah, it has to be the past.

55:01.760 --> 55:02.400
I don't know.

55:02.400 --> 55:04.400
I mean, I have the periods of time that I'm interested.

55:04.400 --> 55:06.240
I mean, I'm really interested in American history

55:06.240 --> 55:10.320
and classical history and I'm really interested

55:10.320 --> 55:11.520
in the history of science too.

55:11.520 --> 55:15.920
So I actually think seeing and trying to understand more

55:18.720 --> 55:20.960
about how some of the big advances came about.

55:20.960 --> 55:24.400
I mean, all we have are like somewhat limited writings

55:24.400 --> 55:25.920
about some of that stuff.

55:25.920 --> 55:27.520
I'm not sure the metaverse is going to let you do that

55:27.520 --> 55:29.440
because I mean, it's, you know, we can't,

55:29.520 --> 55:32.640
it's going to be hard to kind of go back in time

55:32.640 --> 55:34.560
for things that we don't have records of.

55:34.560 --> 55:39.040
But I'm actually not sure that going back in time

55:39.040 --> 55:41.520
is going to be that important thing for them.

55:41.520 --> 55:43.440
I mean, I think it's going to be cool for history classes

55:43.440 --> 55:46.960
and stuff, but that's probably not the use case

55:46.960 --> 55:49.600
that I'm most excited about for the metaverse overall.

55:49.600 --> 55:52.960
I mean, the main thing is just the ability

55:52.960 --> 55:55.280
to feel present with people no matter where you are.

55:55.280 --> 55:56.640
I think that's going to be killer.

55:56.640 --> 56:01.360
I mean, there's, I mean, in the AI conversation

56:01.360 --> 56:04.560
that we were having, I mean, it's, you know,

56:04.560 --> 56:06.640
so much of it is about physical constraints

56:06.640 --> 56:09.120
that kind of underlie all of this, right?

56:09.120 --> 56:12.320
And you want to move, I mean, one lesson of technology

56:12.320 --> 56:15.600
is you want to move things from the physical constraint realm

56:15.600 --> 56:17.200
into software as much as possible

56:17.200 --> 56:21.120
because software is so much easier to build and evolve.

56:21.120 --> 56:22.800
And like you can democratize it more

56:22.800 --> 56:25.520
because like not everyone is going to have a data center,

56:25.520 --> 56:28.800
but like a lot of people can kind of write code

56:28.800 --> 56:30.880
and take open source code and modify it.

56:33.280 --> 56:35.840
The metaverse version of this is I think

56:35.840 --> 56:38.480
enabling realistic digital presence

56:39.600 --> 56:43.760
is going to be just an absolutely huge difference

56:43.760 --> 56:48.000
for making it so that people don't feel

56:48.000 --> 56:49.840
like they have to physically be together

56:49.840 --> 56:50.880
for as many things.

56:51.520 --> 56:52.720
Now, I mean, I think that there are going to be things

56:52.720 --> 56:54.240
that are better about being physically together.

56:56.240 --> 56:58.000
So it's not, I mean, these things aren't binary,

56:58.000 --> 56:58.960
it's not going to be like, okay,

56:58.960 --> 57:00.960
now it's, you don't need to do that anymore.

57:00.960 --> 57:07.200
But overall, I mean, I think that this,

57:07.200 --> 57:09.760
it's just going to be really powerful for socializing,

57:09.760 --> 57:12.640
for feeling connected with people, for working,

57:13.600 --> 57:18.000
for, I don't know, parts of industry, for medicine,

57:18.000 --> 57:19.920
for like so many things.

57:19.920 --> 57:21.040
I want to go back to something you said

57:21.040 --> 57:22.240
at the beginning of the conversation

57:22.240 --> 57:24.960
where you didn't sell the company for a billion dollars

57:24.960 --> 57:26.640
and like the metaverse, you knew we were going to do this

57:26.640 --> 57:29.520
even though the market was hammering you for it.

57:29.520 --> 57:30.960
And then I'm actually curious,

57:30.960 --> 57:32.720
like what is the source of that edge?

57:32.720 --> 57:35.280
And you said like, oh, values, I have this intuition,

57:35.280 --> 57:36.800
but like everybody says that, right?

57:37.360 --> 57:39.200
If you had to say something that's specific to you,

57:39.200 --> 57:41.200
what is, how would you express what that is?

57:41.200 --> 57:43.280
Like why were you so convinced about the metaverse?

57:49.920 --> 57:51.440
Well, I think that those are different questions.

57:51.440 --> 57:56.560
So what, I mean, what are the things that kind of power me?

57:58.640 --> 57:59.920
I think we've talked about a bunch of things.

57:59.920 --> 58:03.200
So it's, I mean, I just really like building things.

58:05.200 --> 58:09.280
I specifically like building things around how people communicate

58:09.280 --> 58:11.680
and sort of understanding how people express themselves

58:11.680 --> 58:12.800
and how people work, right?

58:12.800 --> 58:14.000
And when I was in college, I was,

58:14.000 --> 58:16.560
I was studying computer science and psychology.

58:16.560 --> 58:17.760
I think a lot of other people in the industry

58:17.760 --> 58:19.440
started studying computer science, right?

58:19.440 --> 58:23.600
So it's always been sort of the intersection

58:23.600 --> 58:25.120
of those two things for me.

58:25.120 --> 58:31.920
But I think it's also sort of this like really deep drive.

58:31.920 --> 58:35.760
I don't know how to explain it, but I just feel like in,

58:35.760 --> 58:39.280
like constitutionally, like I'm doing something wrong

58:39.280 --> 58:41.920
if I'm not building something new, right?

58:41.920 --> 58:46.560
And so I think that there's like

58:49.760 --> 58:52.480
even when we're putting together the business case for,

58:53.440 --> 58:57.360
you know, investing like $100 billion in AI

58:57.360 --> 58:59.360
or some huge amount in the metaverse.

58:59.360 --> 59:01.520
So it's like, yeah, I mean, we have plans

59:01.520 --> 59:03.840
that I think make it pretty clear that if our stuff works,

59:03.840 --> 59:05.360
it'll be a good investment.

59:05.360 --> 59:08.320
But like, you can't know for certain from the outset.

59:08.960 --> 59:12.000
And there's all these arguments that people have,

59:12.000 --> 59:14.880
you know, whether it's like, you know, with advisors

59:14.880 --> 59:17.680
or different folks, it's like, well, how, how could you,

59:17.760 --> 59:20.320
like it's how are you confident enough to do this?

59:20.320 --> 59:24.880
And it's like, well, the day I stop trying to build new things,

59:25.520 --> 59:26.240
I'm just done.

59:26.240 --> 59:28.240
I'm going to go build new things somewhere else, right?

59:28.240 --> 59:32.880
It's like, it's like, it is, I'm fundamentally incapable

59:33.520 --> 59:38.560
of running something or in my own life

59:38.560 --> 59:41.520
and like not trying to build new things

59:41.520 --> 59:42.880
that I think are interesting.

59:42.880 --> 59:45.040
It's like, that's not even a question for me, right?

59:45.040 --> 59:47.680
It's like whether, like whether we're going to go take a swing

59:47.680 --> 59:49.040
at like building the next thing.

59:49.040 --> 59:52.960
It's like, it's like, I'm just incapable of not doing that.

59:54.800 --> 59:59.760
And I don't know, and I'm kind of like this

59:59.760 --> 01:00:01.760
in like all the different aspects of my life, right?

01:00:01.760 --> 01:00:04.320
It's like we built this like, you know,

01:00:04.320 --> 01:00:08.720
family built this ranch in Kauai and like, I just like

01:00:10.240 --> 01:00:11.840
worked to like design all these buildings.

01:00:11.840 --> 01:00:15.200
I'm like kind of trying to like, we started raising cattle

01:00:15.200 --> 01:00:16.560
and I'm like, all right, well, I want to make

01:00:16.560 --> 01:00:18.160
like the best cattle in the world, right?

01:00:18.160 --> 01:00:20.240
So it's like, how do we like, how do we architect this

01:00:20.240 --> 01:00:22.560
so that way we can figure this out and like and build

01:00:23.200 --> 01:00:25.200
and call the stuff up that we need to try to do that?

01:00:26.640 --> 01:00:27.840
So I don't know, that's me.

01:00:29.600 --> 01:00:30.800
What was the other part of the question?

01:00:32.080 --> 01:00:34.880
Look, Metta is just a really amazing tech company, right?

01:00:34.880 --> 01:00:36.960
They have all these great software engineers

01:00:36.960 --> 01:00:39.840
and even they work with Stripe to handle payments.

01:00:39.840 --> 01:00:41.360
And I think that's just a really notable fact.

01:00:41.840 --> 01:00:45.200
That Stripe's ability to engineer these checkout experiences

01:00:45.200 --> 01:00:48.720
is so good that big companies like Ford, Zoom, Metta,

01:00:48.720 --> 01:00:52.000
even OpenAI, they work with Stripe to handle payments

01:00:52.000 --> 01:00:53.840
because just think about how many different possibilities

01:00:53.840 --> 01:00:54.880
you have to handle.

01:00:54.880 --> 01:00:56.880
If you're in a different country, you'll pay a different way

01:00:56.880 --> 01:00:58.480
and if you're buying a certain kind of item

01:00:58.480 --> 01:01:00.400
that might affect how you decide to pay.

01:01:00.400 --> 01:01:03.840
And Stripe is able to test these fine-grained optimizations

01:01:03.840 --> 01:01:06.400
across tens of billions of transactions a day

01:01:06.400 --> 01:01:08.560
to figure out what will convert people

01:01:08.560 --> 01:01:11.280
and obviously conversion means more revenue for you.

01:01:11.280 --> 01:01:13.680
And look, I'm not a big company like Metta or anything,

01:01:13.680 --> 01:01:15.280
but I've been using Stripe since long

01:01:15.280 --> 01:01:16.800
before they were advertisers.

01:01:16.800 --> 01:01:20.080
Stripe Atlas was just the easiest way for me to set up an LLC

01:01:20.080 --> 01:01:22.160
and they have these payments and invoicing features

01:01:22.160 --> 01:01:26.000
that make it super convenient for me to get money from advertisers.

01:01:26.000 --> 01:01:28.080
And obviously without that, it would have been much harder

01:01:28.080 --> 01:01:30.000
for me to earn money from the podcast.

01:01:30.000 --> 01:01:31.600
And so it's been great for me.

01:01:31.600 --> 01:01:33.680
Go to stripe.com to learn more.

01:01:33.680 --> 01:01:35.440
Thanks to them for sponsoring the episode.

01:01:35.440 --> 01:01:36.240
Now back to Mark.

01:01:37.120 --> 01:01:39.280
I'm not sure, but I'm actually curious about something else

01:01:39.280 --> 01:01:44.400
which is, so the 19-year-old Mark reads a bunch

01:01:44.400 --> 01:01:47.280
of like antiquity in classics, high school, college.

01:01:47.920 --> 01:01:49.440
What important lesson did you learn from it?

01:01:49.440 --> 01:01:50.560
Not just interesting things you found,

01:01:50.560 --> 01:01:53.200
but like there aren't that many tokens you consumed

01:01:53.200 --> 01:01:54.160
by the time you're 19.

01:01:54.160 --> 01:01:55.680
A bunch of them were about the classics.

01:01:55.680 --> 01:01:56.880
Clearly that was important in some way.

01:01:56.880 --> 01:01:58.240
And that many tokens you consumed.

01:02:05.680 --> 01:02:06.080
I don't know.

01:02:06.080 --> 01:02:07.120
That's a good question.

01:02:07.120 --> 01:02:08.320
I mean, one of the things that I thought

01:02:08.320 --> 01:02:17.440
was really fascinating is, so when Augustus was first,

01:02:18.000 --> 01:02:24.480
so he became emperor, and he was trying to establish peace.

01:02:24.480 --> 01:02:30.400
And there was no real conception of peace at the time.

01:02:30.400 --> 01:02:33.040
Like the people's understanding of peace was,

01:02:34.000 --> 01:02:36.400
it is the temporary time between when you're

01:02:36.400 --> 01:02:38.960
and amuse will inevitably attack you again,

01:02:38.960 --> 01:02:40.240
so you get like a short rest.

01:02:40.800 --> 01:02:43.200
And he had this view, which is like, look,

01:02:43.200 --> 01:02:46.720
like we want to change the economy from instead of being

01:02:46.720 --> 01:02:50.080
so mercenary and like in kind of militaristic

01:02:50.960 --> 01:02:53.920
to like actually this positive something.

01:02:54.640 --> 01:02:57.760
It's like a very novel idea at the time.

01:03:02.400 --> 01:03:02.960
I don't know.

01:03:02.960 --> 01:03:04.560
I think that there's like something that's

01:03:04.560 --> 01:03:06.080
just really fundamental about that.

01:03:06.080 --> 01:03:09.280
It's like in terms of the bounds on like what people

01:03:09.280 --> 01:03:12.960
can conceive at the time of like what are rational ways to work.

01:03:13.680 --> 01:03:17.280
And I don't know, I mean, going back to like,

01:03:17.280 --> 01:03:19.600
I mean, this applies to both the metaverse and the AI stuff,

01:03:19.600 --> 01:03:22.400
but like a lot of investors and just different people

01:03:22.400 --> 01:03:24.880
just can't wrap their head around why we would open source this.

01:03:25.440 --> 01:03:29.440
And it's like, I don't understand.

01:03:29.440 --> 01:03:30.320
It's like open source.

01:03:30.320 --> 01:03:32.160
That must just be like the temporary time

01:03:32.160 --> 01:03:34.080
between which you're making things proprietary.

01:03:34.080 --> 01:03:35.200
Right.

01:03:35.200 --> 01:03:37.840
And it's, but I actually think it's like

01:03:38.480 --> 01:03:44.160
this very profound thing in tech that has actually,

01:03:44.160 --> 01:03:46.240
it creates a lot of winners.

01:03:46.240 --> 01:03:46.560
Right.

01:03:46.560 --> 01:03:50.720
And it's and so I don't want to strain the analogy too much,

01:03:50.720 --> 01:03:54.960
but I do think that there's a lot of times,

01:03:54.960 --> 01:03:56.480
I think ways where you can

01:03:59.360 --> 01:04:01.760
that are just like models for building things

01:04:02.000 --> 01:04:05.520
that people can't even like they just like often can't wrap

01:04:05.520 --> 01:04:08.240
their head around how that would be a valuable thing

01:04:08.240 --> 01:04:11.760
for people to go do, or like a reasonable state of the world

01:04:11.760 --> 01:04:17.680
that it's, I mean, it's, I think that there's more reasonable

01:04:17.680 --> 01:04:18.880
things than people think.

01:04:18.880 --> 01:04:20.720
That's super fascinating.

01:04:20.720 --> 01:04:22.800
Can I give you my answer when I was thinking

01:04:22.800 --> 01:04:24.480
what you might have gotten from it?

01:04:24.480 --> 01:04:28.160
This is probably totally off, but just how young

01:04:28.160 --> 01:04:30.960
some of these people are who have very important roles

01:04:31.840 --> 01:04:32.480
in the empire.

01:04:32.480 --> 01:04:34.720
Like Caesar Augustus, like by the time he's 19,

01:04:34.720 --> 01:04:37.120
he's actually incredibly one of the most prominent people

01:04:37.120 --> 01:04:38.320
in Roman politics.

01:04:38.320 --> 01:04:39.760
And he's like leading battles and forming

01:04:39.760 --> 01:04:41.040
the second prime emirate.

01:04:41.040 --> 01:04:42.800
I wonder if you're like the 19 year old is like,

01:04:42.800 --> 01:04:44.000
I can actually do this because like

01:04:44.000 --> 01:04:46.800
I think that's an interesting example,

01:04:46.800 --> 01:04:50.240
both from a lot of history and American history.

01:04:50.240 --> 01:04:50.720
Yeah.

01:04:50.720 --> 01:04:55.040
I mean, it's, I mean, one of my favorite quotes is,

01:04:55.040 --> 01:04:57.680
it's this Picasso quote that all children are artists

01:04:57.680 --> 01:04:59.520
and the challenge is how do you remain an artist

01:04:59.520 --> 01:05:00.560
when you grow up?

01:05:00.560 --> 01:05:03.280
And it's like basically I think because when you're younger,

01:05:04.080 --> 01:05:09.680
I think it's just easier to have kind of wild ideas

01:05:09.680 --> 01:05:11.360
and you're not, you know, you have no,

01:05:12.560 --> 01:05:15.200
there are all these analogies to the innovators dilemma

01:05:15.200 --> 01:05:18.640
that exist in your life as well as your company

01:05:18.640 --> 01:05:19.840
or whatever you've built, right?

01:05:19.840 --> 01:05:22.080
So, you know, you're kind of earlier on your trajectory.

01:05:22.080 --> 01:05:24.960
It's easier to pivot and take in new ideas

01:05:24.960 --> 01:05:26.800
without it disrupting other commitments

01:05:26.800 --> 01:05:29.040
that you've made to different things.

01:05:29.040 --> 01:05:31.920
And so, I don't know.

01:05:31.920 --> 01:05:34.560
I think that's an interesting part of running a company

01:05:34.560 --> 01:05:36.960
is like how do you kind of stay dynamic?

01:05:38.640 --> 01:05:40.400
Going back to the investors in open source,

01:05:41.440 --> 01:05:42.880
the $10 billion model,

01:05:42.880 --> 01:05:45.840
suppose it's totally safe, you've done these evaluations

01:05:45.840 --> 01:05:47.280
and unlike in this case,

01:05:47.280 --> 01:05:48.960
the evaluators can also fine tune the model,

01:05:49.680 --> 01:05:51.360
which hopefully will be the case in future models.

01:05:52.560 --> 01:05:54.560
Would you open source that, the $10 billion model?

01:05:55.360 --> 01:05:57.280
Well, I mean, as long as it's helping us, then yeah.

01:05:57.280 --> 01:05:59.360
But would it like to $10 billion of R&D

01:05:59.360 --> 01:06:00.960
and then now it's like open source or anything?

01:06:00.960 --> 01:06:02.880
Well, I think here's, I think a question,

01:06:02.880 --> 01:06:06.640
which we'll have to evaluate this as time goes on too, but

01:06:10.320 --> 01:06:12.960
we have a long history of open sourcing software, right?

01:06:12.960 --> 01:06:15.520
We don't tend to open source our product, right?

01:06:15.520 --> 01:06:18.880
So, it's not like we don't take like the code for Instagram

01:06:18.880 --> 01:06:19.760
and make it open source,

01:06:19.760 --> 01:06:22.880
but we take like a lot of the low level infrastructure

01:06:23.440 --> 01:06:25.600
and we make that open source, right?

01:06:26.080 --> 01:06:29.520
Probably the biggest one in our history was open compute project

01:06:29.520 --> 01:06:34.640
where we took the designs for kind of all of our servers

01:06:34.640 --> 01:06:36.240
and network switches and data centers

01:06:36.240 --> 01:06:38.880
and made it open source and ended up being super helpful

01:06:38.880 --> 01:06:41.520
because, you know, I mean, a lot of people can design servers,

01:06:41.520 --> 01:06:43.520
but now like the industry standardized on our design,

01:06:43.520 --> 01:06:45.280
which meant that the supply chains

01:06:46.160 --> 01:06:47.920
basically all got built out around our design,

01:06:47.920 --> 01:06:48.720
the volumes went up,

01:06:48.720 --> 01:06:51.600
so it got cheaper for everyone and saved us billions of dollars.

01:06:51.600 --> 01:06:53.360
So, awesome, right?

01:06:53.440 --> 01:06:56.240
Okay, so there's multiple ways where open source,

01:06:56.240 --> 01:06:57.920
I think, could be helpful for us.

01:06:57.920 --> 01:07:01.440
One is if people figure out how to run the models more cheaply.

01:07:01.440 --> 01:07:04.640
Well, we're going to be spending tens or like 100 billion dollars

01:07:04.640 --> 01:07:07.040
or more over time on all this stuff.

01:07:07.680 --> 01:07:10.160
So, if we can do that 10% more effectively,

01:07:10.160 --> 01:07:12.720
we're saving billions or tens of billions of dollars.

01:07:12.720 --> 01:07:14.320
Okay, that's probably worth a lot by itself,

01:07:15.920 --> 01:07:17.840
especially if there's other competitive models out there.

01:07:17.840 --> 01:07:19.760
It's not like our thing is like

01:07:19.760 --> 01:07:21.760
be giving away some kind of crazy advantage.

01:07:22.720 --> 01:07:25.600
So, is there a view that the trading will be commodified?

01:07:29.040 --> 01:07:30.880
I think there's a bunch of ways that this could play out.

01:07:30.880 --> 01:07:31.840
That's one.

01:07:31.840 --> 01:07:38.560
The other is that, so commodity kind of implies

01:07:39.120 --> 01:07:40.480
that it's going to get very cheap

01:07:40.480 --> 01:07:43.040
because there's lots of options.

01:07:43.040 --> 01:07:44.560
The other direction that this could go in

01:07:45.360 --> 01:07:47.040
is qualitative improvements.

01:07:47.040 --> 01:07:49.360
So, you mentioned fine-tuning, right?

01:07:49.360 --> 01:07:51.760
It's like right now, it's pretty limited,

01:07:51.760 --> 01:07:55.120
what you can do with fine-tuning major other models out there.

01:07:55.120 --> 01:07:56.400
And there are some options,

01:07:56.400 --> 01:07:58.400
but generally not for the biggest models.

01:07:59.840 --> 01:08:01.280
So, I think being able to do that

01:08:01.280 --> 01:08:06.160
and be able to kind of do different app-specific things

01:08:06.160 --> 01:08:07.440
or use case-specific things

01:08:07.440 --> 01:08:09.120
or build them into specific tool chains,

01:08:10.880 --> 01:08:15.600
I think will not only enable kind of more efficient development,

01:08:15.600 --> 01:08:17.520
it could enable qualitatively different things.

01:08:18.480 --> 01:08:19.680
Here's one analogy on this.

01:08:22.160 --> 01:08:24.240
So, one thing that I think generally sucks

01:08:24.240 --> 01:08:25.520
about the mobile ecosystem

01:08:26.080 --> 01:08:30.160
is that you have these two gatekeeper companies,

01:08:30.160 --> 01:08:31.200
Apple and Google,

01:08:31.200 --> 01:08:33.200
that can tell you what you're allowed to build.

01:08:33.200 --> 01:08:35.440
And there are lots of times in our history,

01:08:35.440 --> 01:08:36.800
so there's the economic version of that,

01:08:36.800 --> 01:08:38.080
which is like, all right, we build something in there,

01:08:38.080 --> 01:08:39.600
just like, I'm going to take a bunch of your money.

01:08:39.600 --> 01:08:43.520
But then there's the qualitative version,

01:08:43.520 --> 01:08:46.080
which is actually what kind of upsets me more,

01:08:46.160 --> 01:08:47.520
which is there's a bunch of times

01:08:47.520 --> 01:08:50.160
when we've launched or wanted to launch features,

01:08:50.720 --> 01:08:53.280
and then Apple's just like, nope, you're not launching that.

01:08:53.280 --> 01:08:55.520
So, it's like, that sucks, right?

01:08:55.520 --> 01:08:59.440
And so, the question is, what is,

01:08:59.440 --> 01:09:04.560
like, are we kind of set up for a world like that with AI,

01:09:04.560 --> 01:09:07.440
where like, you're going to get a handful of companies

01:09:07.440 --> 01:09:08.720
that run these closed models

01:09:08.720 --> 01:09:10.640
that are going to be in control of the APIs,

01:09:10.640 --> 01:09:11.840
and therefore are going to be able to tell you

01:09:11.840 --> 01:09:12.640
what you can build.

01:09:13.520 --> 01:09:15.520
Well, for one, I can say, for us,

01:09:16.480 --> 01:09:18.800
it is worth it to go build a model ourselves

01:09:18.800 --> 01:09:20.960
to make sure that we're not in that position, right?

01:09:20.960 --> 01:09:23.680
Like, I don't want any of those other companies

01:09:23.680 --> 01:09:24.800
telling us what we can build.

01:09:26.000 --> 01:09:28.000
But from an open source perspective,

01:09:28.000 --> 01:09:29.920
I think a lot of developers don't want those companies

01:09:29.920 --> 01:09:31.200
telling them what they can build either.

01:09:32.560 --> 01:09:35.680
So, the question is, what is the ecosystem

01:09:35.680 --> 01:09:36.880
that gets built out around that?

01:09:36.880 --> 01:09:38.400
What are interesting new things?

01:09:38.400 --> 01:09:40.080
How much does that improve our products?

01:09:42.560 --> 01:09:43.920
I think that there's a lot of cases

01:09:43.920 --> 01:09:46.000
where if this ends up being like, you know,

01:09:46.000 --> 01:09:50.160
like our databases or caching systems or architecture,

01:09:50.720 --> 01:09:52.800
we'll get valuable contributions from the community

01:09:52.800 --> 01:09:54.000
that will make our stuff better,

01:09:54.000 --> 01:09:56.320
and then our app-specific work that we do

01:09:56.320 --> 01:09:57.920
will still be so differentiated

01:09:57.920 --> 01:09:59.280
that it won't really matter, right?

01:09:59.280 --> 01:10:01.360
It's like, we'll be able to do what we do,

01:10:01.360 --> 01:10:02.800
we'll benefit in all the systems,

01:10:02.800 --> 01:10:04.320
ours and the communities will be better

01:10:04.320 --> 01:10:05.280
because it's open source.

01:10:06.080 --> 01:10:10.320
There is one world where maybe it's not that.

01:10:10.320 --> 01:10:11.520
I mean, maybe the model just ends up

01:10:11.520 --> 01:10:13.200
being more of the product itself.

01:10:13.280 --> 01:10:17.760
In that case, then I think it's a trickier economic

01:10:17.760 --> 01:10:20.000
calculation about whether you open source that

01:10:20.000 --> 01:10:23.200
because then you are kind of commoditizing yourself a lot.

01:10:23.840 --> 01:10:24.800
From what I can see so far,

01:10:24.800 --> 01:10:25.920
it doesn't seem like we're in that zone.

01:10:26.560 --> 01:10:28.480
Do you expect to earn significant revenue

01:10:28.480 --> 01:10:30.640
from licensing your model to the cloud providers?

01:10:30.640 --> 01:10:32.720
So, they have to pay you a fee to actually serve the model?

01:10:36.160 --> 01:10:38.640
We want to have an arrangement like that,

01:10:38.640 --> 01:10:40.800
but I don't know how significant it'll be.

01:10:40.800 --> 01:10:44.640
And we have this, this is basically our license for Lama.

01:10:46.800 --> 01:10:49.040
In a lot of ways, it's like a very permissive

01:10:49.040 --> 01:10:51.680
open source license, except that we have a limit

01:10:51.680 --> 01:10:53.440
for the largest companies using it.

01:10:53.440 --> 01:10:56.240
And this is why we put that limit in,

01:10:56.240 --> 01:10:58.640
is we're not trying to prevent them from using it.

01:10:58.640 --> 01:10:59.920
We just want them to come talk to us

01:10:59.920 --> 01:11:01.120
because if they're going to just basically

01:11:01.120 --> 01:11:03.680
take what we built and resell it and make money off of it,

01:11:03.680 --> 01:11:06.320
then it's like, okay, well, if you're like,

01:11:07.200 --> 01:11:09.200
Microsoft Azure or Amazon,

01:11:09.680 --> 01:11:11.200
yeah, if you're going to reselling the model,

01:11:11.200 --> 01:11:12.720
then we should have some revenue share on that.

01:11:12.720 --> 01:11:14.800
So, just come talk to us before you go do that.

01:11:14.800 --> 01:11:15.760
And that's how that's played out.

01:11:15.760 --> 01:11:19.520
So, for Lama 2, it's, I mean, we basically just have deals

01:11:19.520 --> 01:11:22.320
with all these major cloud companies,

01:11:22.320 --> 01:11:26.480
and Lama 2 is available as a hosted service on all those clouds.

01:11:28.480 --> 01:11:31.200
I assume that as we release bigger and bigger models,

01:11:31.200 --> 01:11:32.240
that'll become a bigger thing.

01:11:32.240 --> 01:11:33.440
It's not the main thing that we're doing,

01:11:33.440 --> 01:11:35.040
but I just think if others are,

01:11:35.040 --> 01:11:36.640
if those companies are going to be selling our models,

01:11:36.640 --> 01:11:38.240
it makes sense that we should, you know,

01:11:38.240 --> 01:11:39.600
share the upside of that somehow.

01:11:39.600 --> 01:11:42.480
Yeah. With regards to the other open source dangers,

01:11:42.480 --> 01:11:44.720
I think I have a genuine legion of points

01:11:44.720 --> 01:11:45.920
about the balance of power stuff,

01:11:46.960 --> 01:11:49.280
and potentially like the harms you can get rid of

01:11:49.280 --> 01:11:51.280
because we have better alignment techniques or something.

01:11:52.240 --> 01:11:54.320
I wish there was some sort of framework that Meta had,

01:11:54.320 --> 01:11:56.320
like other labs have this where they say like,

01:11:56.320 --> 01:11:58.400
if we see this is a concrete thing,

01:11:58.400 --> 01:12:00.720
then that's a no-go on the open source,

01:12:00.720 --> 01:12:03.200
or like even potentially on deployment,

01:12:03.200 --> 01:12:06.080
just like writing it down so like the company is ready for it,

01:12:06.720 --> 01:12:08.800
people have expectations around it and so forth.

01:12:08.800 --> 01:12:10.640
Yeah. No, I think that that's a fair point

01:12:10.640 --> 01:12:12.000
on the existential risk side.

01:12:12.000 --> 01:12:14.800
Right now, we focus more on the types of risks

01:12:14.800 --> 01:12:17.760
that we see today, which are more of these content risks.

01:12:17.760 --> 01:12:23.120
So, you know, we have lines on, we don't want the model to be

01:12:24.080 --> 01:12:27.040
basically doing things that are helping people commit violence

01:12:27.040 --> 01:12:30.320
or fraud or, you know, just harming people in different ways.

01:12:30.320 --> 01:12:35.200
So in practice for today's models,

01:12:35.200 --> 01:12:37.920
and I would guess the next generation

01:12:37.920 --> 01:12:39.600
and maybe even the generation after that,

01:12:40.320 --> 01:12:44.800
I think while it is somewhat more maybe intellectually interesting

01:12:44.800 --> 01:12:47.040
to talk about the existential risks,

01:12:47.600 --> 01:12:53.840
I actually think the real harms that need more energy being mitigated

01:12:53.840 --> 01:12:57.840
are things that are going to like have someone take a model

01:12:57.840 --> 01:13:00.960
and do something to hurt a person with today's parameters

01:13:00.960 --> 01:13:04.240
of and kind of the types of kind of more mundane harms

01:13:04.320 --> 01:13:07.280
that we see today, like people kind of committing fraud

01:13:07.280 --> 01:13:08.720
against each other or things like that.

01:13:08.720 --> 01:13:13.360
So that, I just don't want to short change that.

01:13:13.360 --> 01:13:15.280
I think we have a responsibility

01:13:15.280 --> 01:13:16.960
to make sure we do a good job on that.

01:13:16.960 --> 01:13:18.880
Yeah, Meta's a big company, you can handle both.

01:13:18.880 --> 01:13:22.960
Yeah. Okay, so as far as the open source goes,

01:13:22.960 --> 01:13:25.520
I'm actually curious if you think the impact of the open source

01:13:25.520 --> 01:13:28.320
from PyTorch, React, open compute, these things

01:13:28.320 --> 01:13:29.840
has been bigger for the world

01:13:29.840 --> 01:13:32.000
than even the social media aspects of Meta.

01:13:32.000 --> 01:13:33.600
Because I like talk to people who use these services

01:13:33.600 --> 01:13:34.800
and think like it's plausible

01:13:34.800 --> 01:13:36.560
because a big part of the internet runs on these things.

01:13:38.960 --> 01:13:40.160
It's an interesting question.

01:13:40.160 --> 01:13:43.360
I mean, I think almost half the world uses our...

01:13:43.360 --> 01:13:44.800
Yeah, that's an interesting point.

01:13:47.360 --> 01:13:49.760
So I think it's hard to beat that.

01:13:49.760 --> 01:13:52.880
But no, I think open sources,

01:13:54.560 --> 01:13:57.360
it's really powerful as a new way of building things.

01:13:57.360 --> 01:14:01.600
And yeah, I mean, it's possible.

01:14:01.600 --> 01:14:03.920
I mean, it's maybe one of these things where...

01:14:06.320 --> 01:14:09.520
I don't know, like Bell Labs, where they...

01:14:09.520 --> 01:14:12.880
It's like they were working on the transistor

01:14:12.880 --> 01:14:15.360
because they wanted to enable long distance calling.

01:14:15.920 --> 01:14:17.280
And they did.

01:14:17.280 --> 01:14:19.120
And it ended up being really profitable for them

01:14:19.120 --> 01:14:21.120
that they were able to enable long distance calling.

01:14:21.120 --> 01:14:25.440
And if you ask them five to 10 years out from that,

01:14:27.280 --> 01:14:29.600
what was the most useful thing that they invented?

01:14:29.600 --> 01:14:31.440
It's like, okay, well, we enable long distance calling

01:14:31.440 --> 01:14:33.040
and now all these people are long distance calling.

01:14:33.040 --> 01:14:34.400
But if you ask 100 years later,

01:14:34.400 --> 01:14:35.760
maybe it's a different question.

01:14:35.760 --> 01:14:40.320
So I think that that's true of a lot of the things

01:14:40.320 --> 01:14:41.280
that we're building, right?

01:14:41.280 --> 01:14:45.280
Reality Labs, some of the AI stuff, some of the open source stuff.

01:14:45.280 --> 01:14:48.480
I think it's like the specific products evolve

01:14:48.480 --> 01:14:50.400
and to some degree come and go.

01:14:50.400 --> 01:14:54.080
But I think like the advances for humanity persist.

01:14:54.080 --> 01:14:57.360
And that's like a cool part of what we all get to do.

01:14:58.080 --> 01:14:59.920
By when will the Lama models be trained

01:14:59.920 --> 01:15:01.040
on your own custom silicon?

01:15:06.480 --> 01:15:08.320
Soon, not Lama 4.

01:15:09.760 --> 01:15:14.640
The approach that we took is first we basically built

01:15:14.640 --> 01:15:16.560
custom silicon that could handle inference

01:15:17.200 --> 01:15:20.000
for our ranking and recommendation type stuff.

01:15:20.000 --> 01:15:22.560
So reels, newsfeed, ads.

01:15:23.200 --> 01:15:26.800
And that was consuming a lot of GPUs.

01:15:28.000 --> 01:15:30.800
But when we were able to move that to our own silicon,

01:15:30.800 --> 01:15:34.720
we now were able to use the more expensive Nvidia GPUs

01:15:35.440 --> 01:15:36.480
only for training.

01:15:37.120 --> 01:15:44.480
So at some point, we will hopefully have silicon ourselves

01:15:44.480 --> 01:15:47.600
that we can be using for probably first training

01:15:47.600 --> 01:15:49.680
some of the simpler things that eventually training

01:15:50.560 --> 01:15:53.200
these like really large models.

01:15:53.840 --> 01:15:58.160
But in the meantime, I'd say the program is going quite well

01:15:58.160 --> 01:15:59.920
and we're just rolling it out methodically

01:15:59.920 --> 01:16:01.600
and have a long-term roadmap for it.

01:16:02.800 --> 01:16:03.760
Final question.

01:16:03.760 --> 01:16:05.120
This is sort of the out of left field,

01:16:05.120 --> 01:16:08.640
but if you were made CEO of Google+, could you have made it work?

01:16:08.640 --> 01:16:10.240
Google+, oof.

01:16:11.440 --> 01:16:12.800
Well, I don't know.

01:16:14.160 --> 01:16:14.720
I don't know.

01:16:14.720 --> 01:16:18.800
That's a very difficult, very difficult counterfactual.

01:16:19.440 --> 01:16:21.040
Okay, then the real final question will be

01:16:21.040 --> 01:16:22.480
when Gemini was launched,

01:16:22.480 --> 01:16:24.720
was there any chance that somebody in the office

01:16:24.720 --> 01:16:26.080
uttered Karthica Dalinda Est?

01:16:27.600 --> 01:16:29.520
No, I think we're tamer now.

01:16:30.720 --> 01:16:31.520
Cool, cool.

01:16:31.520 --> 01:16:32.160
Awesome, Mark.

01:16:35.680 --> 01:16:36.800
Yeah, I don't know.

01:16:36.800 --> 01:16:37.360
It's a good question.

01:16:38.720 --> 01:16:41.120
The problem is there was no CEO of Google+,

01:16:41.120 --> 01:16:43.120
it was just like a division within a company.

01:16:43.840 --> 01:16:46.160
I think it's like, and you asked before about

01:16:46.160 --> 01:16:48.560
what are the kind of scarcest commodities,

01:16:48.560 --> 01:16:50.240
but you asked about it in terms of dollars.

01:16:50.800 --> 01:16:52.800
And I actually think for most companies,

01:16:53.680 --> 01:16:57.280
it's, of this scale at least, it's focus, right?

01:16:57.280 --> 01:16:58.400
It's like when you're a startup,

01:16:58.400 --> 01:17:00.000
maybe you're more constrained on capital.

01:17:01.120 --> 01:17:03.200
You know, you just are working on one idea

01:17:03.200 --> 01:17:05.280
and you might not have all the resources.

01:17:05.280 --> 01:17:07.360
I think you cross some threshold at some point

01:17:07.360 --> 01:17:09.600
where the nature of what you're doing,

01:17:09.600 --> 01:17:11.120
you're building multiple things

01:17:11.120 --> 01:17:13.120
and you're creating more value across them,

01:17:13.120 --> 01:17:17.360
but you become more constrained on what can you direct

01:17:17.360 --> 01:17:19.200
and to go well.

01:17:19.200 --> 01:17:21.920
And like, there's always the cases

01:17:21.920 --> 01:17:24.000
where something just random awesome happens

01:17:24.000 --> 01:17:25.840
in the organization, I don't even know about it.

01:17:25.840 --> 01:17:28.160
And those are, that's great.

01:17:28.160 --> 01:17:29.520
But like, but I think in general,

01:17:30.640 --> 01:17:33.680
the organization's capacity is largely limited

01:17:33.680 --> 01:17:38.480
by what like the CEO and the management team

01:17:38.480 --> 01:17:41.440
are able to kind of oversee and kind of manage.

01:17:42.480 --> 01:17:45.360
I think that that's just been a big focus for us.

01:17:45.360 --> 01:17:49.600
It's like, all right, keep the, as I guess Ben Horowitz says,

01:17:49.600 --> 01:17:51.920
keep the main thing, the main thing, right?

01:17:51.920 --> 01:17:57.440
And try to kind of stay focused on your key priorities.

01:17:58.000 --> 01:17:59.600
Yeah, all right, awesome.

01:17:59.600 --> 01:18:00.400
That was excellent, Mark.

01:18:00.400 --> 01:18:01.520
Thanks so much. That was a lot of fun.

01:18:01.520 --> 01:18:02.720
Yeah, really fun.

01:18:02.720 --> 01:18:03.440
Thanks for having me.

01:18:03.440 --> 01:18:04.160
Yeah, absolutely.

01:18:04.960 --> 01:18:05.840
Hey, everybody.

01:18:05.840 --> 01:18:07.920
I hope you enjoyed that episode with Mark.

01:18:07.920 --> 01:18:09.760
As you can see, I'm now doing ads.

01:18:09.760 --> 01:18:12.800
So if you're interested in advertising on the podcast,

01:18:12.800 --> 01:18:14.160
go to the link in the description.

01:18:14.800 --> 01:18:17.520
Otherwise, as you know, the most helpful thing you can do

01:18:17.520 --> 01:18:20.240
is just share the podcast with people who you think

01:18:20.240 --> 01:18:21.040
might enjoy it.

01:18:21.040 --> 01:18:23.040
You know, your friends, group chats, Twitter,

01:18:23.680 --> 01:18:24.560
I guess threads.

01:18:25.120 --> 01:18:27.680
Yeah, I hope you enjoyed and I'll see you on the next one.

