WEBVTT

00:00.000 --> 00:05.680
Human-level AI is deep, deep into an intelligence explosion.

00:05.680 --> 00:09.040
Things like inventing the transformer or discovering

00:09.040 --> 00:12.080
Chinchilla scaling and doing your training runs more optimally

00:12.080 --> 00:13.760
or creating flash attention.

00:13.760 --> 00:17.560
That set of inputs probably would yield the kind of AI

00:17.560 --> 00:20.040
capabilities needed for intelligence explosion.

00:20.040 --> 00:22.320
You have a race between, on the one hand,

00:22.320 --> 00:25.080
the project of getting strong interpretability

00:25.080 --> 00:26.880
and shaping motivations.

00:26.960 --> 00:30.160
And on the other hand, these AIs in ways

00:30.160 --> 00:33.080
that you don't perceive make the AI takeover happen.

00:33.080 --> 00:36.720
We spend more compute by having a larger brain than other animals.

00:36.720 --> 00:38.720
And then we have a longer childhood.

00:38.720 --> 00:41.160
It's not like you have to like having a bigger model

00:41.160 --> 00:43.120
and having more training time with it.

00:43.120 --> 00:45.320
It seemed very implausible that we couldn't do better

00:45.320 --> 00:47.440
than completely brute force evolution.

00:47.440 --> 00:50.240
How quickly are we running through those orders of magnitude?

00:50.240 --> 00:54.800
OK, today I have the pleasure of speaking with Carl Schulman,

00:54.800 --> 00:56.320
many of my former guests.

00:56.360 --> 00:57.480
And this is not an exaggeration.

00:57.480 --> 00:59.520
Many of my former guests have told me

00:59.520 --> 01:02.280
that a lot of their biggest ideas,

01:02.280 --> 01:03.480
perhaps most of their biggest ideas,

01:03.480 --> 01:05.160
have come directly from Carl,

01:05.160 --> 01:07.640
especially when it has to do with the intelligence explosion

01:07.640 --> 01:08.760
and its impacts.

01:08.760 --> 01:11.400
And so I decided to go directly to the source

01:11.400 --> 01:13.840
and we have Carl today on the podcast.

01:13.840 --> 01:15.360
Carl keeps a super low profile,

01:15.360 --> 01:18.240
but he is one of the most interesting intellectuals

01:18.240 --> 01:19.480
I've ever encountered.

01:19.480 --> 01:21.840
And this is actually his second podcast ever.

01:21.840 --> 01:24.440
So we're going to get to get deep into the heart

01:24.440 --> 01:25.720
of many of the most important ideas

01:25.720 --> 01:27.400
that are circulating right now,

01:27.400 --> 01:28.640
directly from the source.

01:28.640 --> 01:29.760
So and by the way,

01:29.760 --> 01:32.880
so Carl is also an advisor to the Open Philanthropy Project,

01:32.880 --> 01:34.720
which is one of the biggest funders

01:34.720 --> 01:37.280
on causes having to do with AI and its risks,

01:37.280 --> 01:39.280
not to mention global health and old being.

01:39.280 --> 01:41.200
And he is a research associate

01:41.200 --> 01:44.200
at the Future of Humanity Institute at Oxford.

01:44.200 --> 01:46.600
So Carl, it's a huge pleasure to have you on the podcast.

01:46.600 --> 01:47.440
Thanks for coming.

01:47.440 --> 01:48.360
Thank you, Drakash.

01:48.360 --> 01:51.560
I've enjoyed seeing some of your episodes recently

01:51.560 --> 01:53.920
and I'm glad to be on the show.

01:53.960 --> 01:55.600
Excellent. Let's talk about AI.

01:55.600 --> 01:57.360
Before we get into the details,

01:57.360 --> 02:01.200
give me the sort of big picture explanation

02:01.200 --> 02:06.080
of the feedback loops and just the general dynamics

02:06.080 --> 02:08.560
that would start when you have something

02:08.560 --> 02:10.600
that is approaching human level intelligence.

02:10.600 --> 02:13.120
Yeah. So I think the way to think about it

02:13.120 --> 02:15.760
is we have a process now

02:15.760 --> 02:19.560
where humans are developing new computer chips,

02:19.560 --> 02:24.280
new software, running larger training runs

02:24.280 --> 02:29.280
and it takes a lot of work to keep Moore's law chugging.

02:30.200 --> 02:32.480
Well, it was, it's slowing down now

02:32.480 --> 02:35.160
and it takes a lot of work to develop things

02:35.160 --> 02:40.000
like transformers to develop a lot of the improvements

02:40.000 --> 02:42.400
to AI and neural networks.

02:42.400 --> 02:43.360
They're advancing things.

02:43.360 --> 02:48.360
And the core method that I think I want to highlight

02:49.080 --> 02:53.000
on this podcast, and I think is underappreciated

02:53.000 --> 02:56.000
is the idea of input-output curves.

02:56.000 --> 03:01.000
So we can look at the increasing difficulty

03:01.200 --> 03:03.320
of improving chips.

03:03.320 --> 03:06.880
And so sure, each time you double the performance

03:06.880 --> 03:08.240
of computers, it's harder

03:08.240 --> 03:09.600
and as we approach physical limits,

03:09.600 --> 03:13.200
eventually it becomes impossible, but how much harder?

03:14.200 --> 03:19.200
So there's a paper called our Ideas Getting Harder to Find.

03:19.560 --> 03:21.880
It was published a few years ago,

03:21.880 --> 03:24.440
something like 10 years ago at Mirri,

03:24.440 --> 03:29.440
we did, I mean, I did an early version of this analysis

03:30.560 --> 03:33.520
using mainly data from Intel

03:33.520 --> 03:36.360
and like the large semiconductor fabricators.

03:36.360 --> 03:40.240
Anyway, and so in this paper, they cover a period

03:40.240 --> 03:44.560
where the productivity of computing

03:44.560 --> 03:45.680
went up a million folds.

03:45.680 --> 03:47.600
So you could get a million times

03:47.600 --> 03:50.360
the computing operations per second per dollar.

03:50.360 --> 03:54.080
Big change, but it got harder.

03:54.080 --> 03:58.880
So the amount of investments, the labor force required

03:58.880 --> 04:01.040
to make those continuing advancements

04:01.040 --> 04:02.920
went up and up and up.

04:02.920 --> 04:07.440
Indeed, it went up 18 fold over that period.

04:07.440 --> 04:09.600
I know, so some take this to say,

04:09.600 --> 04:11.360
oh, diminishing returns,

04:11.360 --> 04:12.960
things are just getting harder and harder.

04:12.960 --> 04:15.660
And so that will be the end of progress eventually.

04:16.920 --> 04:21.300
However, in a world where AI is doing the work,

04:22.840 --> 04:25.240
that doubling of computing performance

04:26.440 --> 04:29.760
translates pretty directly to a doubling or better

04:29.760 --> 04:32.000
of the effective labor supply.

04:32.000 --> 04:37.000
That is, if when we had that million fold compute increase,

04:38.000 --> 04:41.720
we used it to run artificial intelligences

04:41.720 --> 04:46.520
who would replace human scientists and engineers

04:46.520 --> 04:50.520
then the 18x increase in the labor demands

04:50.520 --> 04:52.320
of the industry would be trivial.

04:52.320 --> 04:55.080
We're getting more than one doubling

04:55.080 --> 04:57.160
of the effective labor supply

04:57.160 --> 05:02.160
that we need for each doubling of the labor requirement.

05:02.560 --> 05:06.440
And in that data set, it's like over four.

05:06.760 --> 05:09.720
So we double compute.

05:09.720 --> 05:12.240
Okay, now we need somewhat more researchers,

05:12.240 --> 05:14.440
but a lot less than twice as many.

05:14.440 --> 05:19.440
And so, okay, we use up some of those doublings of compute

05:19.960 --> 05:23.200
on the increasing difficulty of further research,

05:23.200 --> 05:27.380
but most of them are left to expedite the process.

05:27.380 --> 05:31.680
So if you double your labor force,

05:31.680 --> 05:34.720
that's enough to get several doublings of compute.

05:34.720 --> 05:38.680
You use up one of them on meeting

05:38.680 --> 05:41.880
the increased demands from diminishing returns.

05:41.880 --> 05:44.800
The others can be used to accelerate the process.

05:44.800 --> 05:49.200
So you have your first doubling takes however many months,

05:49.200 --> 05:53.480
your next doubling can take a smaller fraction of that,

05:53.480 --> 05:56.600
the next doubling less and so on.

05:56.600 --> 05:58.820
At least in so far as this,

06:00.360 --> 06:02.160
the outputs you're generating,

06:02.160 --> 06:04.680
compute for AI in this story.

06:04.680 --> 06:07.880
Are able to serve the function of the necessary inputs.

06:07.880 --> 06:09.920
If there are other inputs that you need,

06:09.920 --> 06:13.360
eventually those become a bottleneck

06:13.360 --> 06:15.920
and you wind up more restricted on those.

06:15.920 --> 06:16.760
Got it, okay.

06:16.760 --> 06:18.840
So yeah, I think the bloom paper had that,

06:18.840 --> 06:21.280
there was 35% increase in,

06:21.280 --> 06:24.060
was it transferred to destiny or cost per flop?

06:24.060 --> 06:26.040
And there was a 7% increase per year

06:26.040 --> 06:27.720
in the number of researchers required

06:27.720 --> 06:29.080
to sustain that pace.

06:29.080 --> 06:30.520
So something on this, yeah,

06:30.520 --> 06:35.520
it's like four to five doublings of compute

06:35.880 --> 06:38.080
per doubling of labor inputs.

06:38.080 --> 06:39.920
I guess there's a lot of questions you can delve into

06:39.920 --> 06:42.880
in terms of whether you would expect a similar scale

06:42.880 --> 06:47.120
with AI and whether it makes sense to think of AI

06:47.120 --> 06:49.200
as a population of researchers

06:49.200 --> 06:51.840
that keeps growing with compute itself.

06:51.840 --> 06:52.720
Actually, let's go there.

06:52.720 --> 06:54.040
So can you explain the intuition

06:54.040 --> 06:56.240
that compute is a good proxy

06:56.240 --> 06:59.720
for the number of AI researchers so to speak?

06:59.720 --> 07:02.720
So far I've talked about hardware as an initial example

07:02.720 --> 07:06.120
because we had good data about a past period.

07:06.120 --> 07:10.120
You can also make improvements on the software side.

07:10.120 --> 07:12.480
And we think about an intelligence explosion

07:12.480 --> 07:14.920
that can include AI is doing work

07:14.920 --> 07:18.700
on making hardware better, making better software,

07:18.700 --> 07:19.800
making more hardware.

07:20.920 --> 07:25.920
But the basic idea for the hardware is especially simple

07:26.400 --> 07:29.080
in that if you have a worker,

07:29.080 --> 07:31.560
an AI worker that can substitute for a human,

07:31.560 --> 07:33.600
if you have twice as many computers,

07:33.600 --> 07:36.600
you can run two separate instances of them

07:36.600 --> 07:39.240
and then they can do two different jobs,

07:39.240 --> 07:43.040
manage two different machines,

07:43.040 --> 07:46.520
work on two different design problems.

07:46.520 --> 07:50.600
Now, you can get more gains than just what you would get

07:50.600 --> 07:52.080
by having two instances.

07:52.080 --> 07:55.880
We get improvements from using some of our compute,

07:55.880 --> 07:58.720
not just to run more instances of the existing AI,

07:58.720 --> 08:01.080
but to train larger AIs.

08:01.080 --> 08:02.800
So there's hardware technology,

08:02.800 --> 08:06.000
how much you can get per dollar you spend on hardware.

08:06.000 --> 08:07.840
And there's software technology.

08:07.840 --> 08:10.080
And the software can be copied freely.

08:10.080 --> 08:12.080
So if you've got the software,

08:12.080 --> 08:14.680
it doesn't necessarily make that much to say that,

08:14.680 --> 08:18.200
oh, we've got 100 Microsoft Windows.

08:18.200 --> 08:20.280
You can make as many copies as you need

08:20.280 --> 08:25.000
for whatever Microsoft will charge you.

08:25.000 --> 08:26.720
But for hardware is different.

08:26.720 --> 08:30.240
It matters how much we actually spend on the hardware

08:30.240 --> 08:32.000
at a given price.

08:32.000 --> 08:34.360
And if we look at the changes

08:34.360 --> 08:37.360
that have been driving AI recently,

08:37.360 --> 08:39.360
that is the thing that is really off-trend.

08:39.360 --> 08:42.520
We are spending tremendously more money

08:43.400 --> 08:48.400
on computer hardware for training big AI models.

08:48.720 --> 08:49.560
Yeah, okay.

08:49.560 --> 08:52.760
So there's the investment in hardware.

08:52.760 --> 08:54.600
There's a hardware technology itself

08:54.600 --> 08:56.680
and there's the software progress itself.

08:56.680 --> 08:57.720
The AI is getting better

08:57.720 --> 08:59.120
because we're spending more money on it

08:59.120 --> 09:01.800
because our hardware itself is getting better over time.

09:01.800 --> 09:03.800
And because we're developing better models

09:03.800 --> 09:06.640
or better adjustments to those models,

09:06.640 --> 09:08.280
where is the loop here?

09:08.280 --> 09:12.720
The work involved in designing new hardware and software

09:12.720 --> 09:15.240
is being done by people now.

09:15.240 --> 09:18.960
They use computer tools to assist them,

09:18.960 --> 09:23.080
but computer time is not the primary cost

09:24.000 --> 09:27.200
for NVIDIA designing chips,

09:27.200 --> 09:31.720
for TSMC producing them for ASML,

09:31.720 --> 09:36.040
making lithography equipment to serve the TSMC fabs.

09:36.040 --> 09:39.400
And even in AI software research,

09:39.400 --> 09:42.560
that has become quite compute-intensive.

09:42.560 --> 09:44.640
But I think we're still in the range

09:44.640 --> 09:48.080
where at a place like DeepMind salaries,

09:48.080 --> 09:52.160
we're still larger than compute for the experiments.

09:52.160 --> 09:55.400
Although they're tremendously, tremendously more

09:55.400 --> 09:58.280
of the expenditures were on compute

09:58.280 --> 10:00.800
relative to salaries than in the past.

10:00.800 --> 10:03.720
If you take all of the work that's being done

10:03.720 --> 10:05.320
by those humans,

10:05.320 --> 10:08.360
there's like low tens of thousands of people

10:08.360 --> 10:13.200
working at NVIDIA designing GPUs specialized for AI.

10:13.200 --> 10:17.960
I think there's more like 70,000 people at TSMC,

10:17.960 --> 10:21.760
which is the leading producer of cutting-edge chips.

10:21.760 --> 10:25.520
There's a lot of additional people at companies like ASML

10:25.520 --> 10:29.200
that supply them with the tools they need.

10:29.200 --> 10:30.760
And then a company like DeepMind,

10:30.760 --> 10:34.400
I think from their public filings,

10:34.400 --> 10:36.920
they recently had 1,000 people,

10:36.920 --> 10:39.640
opening, I think, is a few hundred people.

10:39.640 --> 10:40.960
Anthropic is less.

10:40.960 --> 10:44.080
If you add up things like Facebook AI research,

10:44.080 --> 10:46.720
Google Brain, R&D,

10:46.720 --> 10:50.320
you get thousands or tens of thousands of people

10:50.320 --> 10:53.960
who are working on AI research.

10:53.960 --> 10:56.920
We'd want to zoom in on those who are developing new methods

10:56.920 --> 10:58.240
rather than narrow applications.

10:58.240 --> 11:01.800
So inventing the transformer definitely counts.

11:01.800 --> 11:05.000
Optimizing for some particular businesses,

11:05.000 --> 11:06.720
dataset cleaning, probably not.

11:07.720 --> 11:10.320
But so those people are doing this work.

11:10.320 --> 11:12.720
They're driving quite a lot of progress.

11:12.720 --> 11:16.320
What we observe and the growth of people

11:16.320 --> 11:19.320
relative to the growth of those capabilities,

11:19.320 --> 11:21.240
is that pretty consistently,

11:21.240 --> 11:25.840
the capabilities are doubling on a shorter time scale

11:25.840 --> 11:29.000
than the people required to do them are doubling.

11:29.000 --> 11:30.800
And so there's work.

11:30.800 --> 11:33.280
So we talked about hardware

11:33.280 --> 11:36.160
and how historically it was pretty dramatic,

11:36.160 --> 11:40.000
like four or five doublings of compute efficiency

11:40.000 --> 11:42.840
per doubling of human inputs.

11:42.840 --> 11:44.360
I think that's a bit lower now

11:44.360 --> 11:45.960
as we get towards the end of Moore's Law.

11:45.960 --> 11:47.960
Although interestingly, not as much lower

11:47.960 --> 11:48.800
as you might think,

11:48.800 --> 11:51.760
because the growth of inputs has also slowed recently.

11:51.760 --> 11:53.240
On the software side,

11:53.240 --> 11:58.240
there's some work by Teme Bessaroglu

11:58.560 --> 12:00.560
and I think collaborators,

12:02.320 --> 12:04.080
may have been the thesis.

12:04.960 --> 12:07.800
It's called our models getting harder to find.

12:07.800 --> 12:09.720
And so it's applying the same sort of analysis

12:09.720 --> 12:12.960
as our ideas getting harder to find.

12:12.960 --> 12:17.400
And you can look at growth rates of papers

12:17.400 --> 12:20.440
from citations, employment at these companies.

12:20.440 --> 12:22.000
And it seems like the doubling time

12:22.000 --> 12:26.560
of these like workers driving the software advances

12:26.560 --> 12:30.920
is like several years, or at least a couple of years.

12:30.920 --> 12:34.040
Whereas the doubling of effective compute

12:34.040 --> 12:36.320
from algorithmic progress is faster.

12:36.320 --> 12:38.720
So there's a group called Epoch.

12:38.720 --> 12:42.120
They've received grants from Open Philanthropy

12:42.120 --> 12:45.480
and they do work collecting datasets

12:45.520 --> 12:48.120
that are relevant to forecasting AI progress.

12:48.120 --> 12:51.960
And so their headline results

12:51.960 --> 12:56.360
for what's the rate of progress in hardware and software

12:56.360 --> 13:00.680
and just like growth in budgets, ours follows.

13:00.680 --> 13:03.760
So for hardware, they're looking at like a doubling

13:03.760 --> 13:05.880
of hardware efficiency that's like two years.

13:05.880 --> 13:07.360
It's possible it's a bit better than that

13:07.360 --> 13:10.440
when you take into account certain specializations

13:10.440 --> 13:11.760
for AI workloads.

13:11.760 --> 13:14.160
For the growth of budgets,

13:14.160 --> 13:16.880
they find a doubling time that's like something

13:16.880 --> 13:19.320
like six months in recent years,

13:19.320 --> 13:22.320
which is pretty tremendous relative

13:22.320 --> 13:24.280
to the historical rates.

13:24.280 --> 13:26.640
We should maybe get into that later.

13:26.640 --> 13:28.640
And then on the algorithmic progress side,

13:28.640 --> 13:32.640
mainly using ImageNet type datasets right now,

13:32.640 --> 13:35.520
they find a doubling time that's less than one year.

13:35.520 --> 13:39.320
And so you combine all of these things

13:39.320 --> 13:41.920
and the growth of effective compute

13:41.920 --> 13:46.920
for training big, big AIs, it's pretty drastic.

13:47.760 --> 13:49.200
I think I saw an estimate that GPD-4

13:49.200 --> 13:51.800
costs like $50 million around that range to train.

13:51.800 --> 13:56.160
Now, suppose that like AGI takes 1000X that,

13:56.160 --> 13:59.160
if you were just a scale of GPD-4, it might not be that.

13:59.160 --> 14:01.360
I'm just just for the sake of example.

14:01.360 --> 14:03.680
So part of that will come from companies

14:03.680 --> 14:05.880
just spending a lot more to train the models

14:05.880 --> 14:07.520
and that just greater investment.

14:07.520 --> 14:10.120
Part of that will come from them having better models

14:10.160 --> 14:13.560
so that what would have taken a 10X increase in the model

14:13.560 --> 14:16.680
to get naively you can do with having a better model

14:16.680 --> 14:18.680
that you only need to do scale up.

14:18.680 --> 14:21.880
You get the same effect of increasing it by 10X

14:21.880 --> 14:23.480
just from having a better model.

14:23.480 --> 14:25.400
And so yeah, you can spend more money

14:25.400 --> 14:26.240
on it to train a bigger model.

14:26.240 --> 14:27.440
You can just have a better model

14:27.440 --> 14:31.920
or you can have chips that are cheaper to train.

14:31.920 --> 14:34.200
So you get more compute for the same dollars.

14:34.200 --> 14:36.600
And okay, so those are the three you were describing.

14:36.600 --> 14:38.320
The ways in which the quote unquote

14:38.320 --> 14:40.280
effective at compute would increase.

14:40.280 --> 14:43.240
From the looking at it right now, it looks like,

14:43.240 --> 14:44.960
yeah, you might get two or three

14:44.960 --> 14:46.720
doublings of effective compute

14:46.720 --> 14:49.880
for this thing that we're calling software progress,

14:49.880 --> 14:52.800
which is, which people get by asking,

14:52.800 --> 14:55.720
well, how much less compute can you use now

14:55.720 --> 14:59.040
to achieve the same benchmark as you achieved before?

14:59.040 --> 15:01.080
There are reasons to not fully identify this

15:01.080 --> 15:02.440
with like software progress

15:02.440 --> 15:04.880
as you might naively think of it

15:04.880 --> 15:07.480
because some of it can be enabled by the other.

15:07.520 --> 15:09.440
So like when you have a lot of compute,

15:09.440 --> 15:11.880
you can do more experiments

15:11.880 --> 15:14.720
and find algorithms that work better.

15:14.720 --> 15:17.760
Sometimes the additional compute,

15:17.760 --> 15:19.600
you can get higher efficiency

15:19.600 --> 15:22.160
by running a bigger model we were talking about earlier.

15:22.160 --> 15:24.560
And so that means you're getting more

15:24.560 --> 15:27.000
for each GPU that you have

15:27.000 --> 15:30.040
because you made this like larger expenditure.

15:30.040 --> 15:33.000
And that can look like a software improvement

15:33.000 --> 15:34.920
because this model,

15:35.600 --> 15:37.480
it's not a hardware improvement directly

15:37.480 --> 15:40.040
because it's doing more with the same hardware,

15:40.040 --> 15:41.480
but you wouldn't have been able to achieve it

15:41.480 --> 15:44.360
without having a ton of GPUs to do the big training run.

15:44.360 --> 15:47.680
The feedback loop itself involves the AI

15:47.680 --> 15:50.480
that is the result of this greater effect of compute,

15:50.480 --> 15:53.920
helping you train better AI, right?

15:53.920 --> 15:56.160
Or use less effective compute in the future

15:56.160 --> 15:58.000
to train better AI.

15:58.000 --> 16:00.320
It can help on the hardware design.

16:00.320 --> 16:03.200
So like NVIDIA is a fabulous chip design company.

16:03.200 --> 16:05.240
They don't make their own chips.

16:05.240 --> 16:09.160
They send files of instructions to TSMC,

16:09.160 --> 16:14.040
which then fabricates the chips in their own facilities.

16:14.040 --> 16:19.040
And so the work of those 10,000 plus people,

16:20.160 --> 16:21.800
if you could automate that

16:21.800 --> 16:25.760
and have the equivalent of a million people doing that work,

16:25.760 --> 16:30.200
then I think you would pretty quickly get the kind

16:30.200 --> 16:31.880
of improvements that can be achieved

16:31.880 --> 16:36.280
with the existing nodes that TSMC is operating on.

16:36.280 --> 16:38.640
You could get a lot of those chip design gains.

16:38.640 --> 16:42.600
Basically like doing the job of improving chip design

16:42.600 --> 16:44.240
that those people are working on now,

16:44.240 --> 16:46.160
but get it done faster.

16:46.160 --> 16:47.480
So that's one thing.

16:47.480 --> 16:48.640
I think that's less important

16:48.640 --> 16:51.000
for the intelligence explosion.

16:51.000 --> 16:54.600
The reason being that when you make an improvement

16:54.600 --> 16:57.200
to chip design, it only applies

16:57.200 --> 16:59.360
to the chips you make after that.

16:59.360 --> 17:02.800
If you make an improvement in AI software,

17:02.800 --> 17:05.920
it has the potential to be immediately applied

17:05.920 --> 17:08.800
to all of the GPUs that you already have.

17:08.800 --> 17:09.640
Yeah.

17:09.640 --> 17:12.080
And so the thing that I think is most disruptive

17:12.080 --> 17:16.280
and most important has the leading edge of the change

17:16.280 --> 17:19.680
from AI automation of the inputs to AI

17:19.680 --> 17:21.240
is on the software side.

17:21.240 --> 17:23.440
At what point would it get to the point

17:23.440 --> 17:27.480
where the AIs are helping develop better software

17:27.480 --> 17:29.320
or better models for future AIs?

17:29.320 --> 17:31.040
Some people claim today, for example,

17:31.040 --> 17:34.680
that programmers at OpenAI are using co-pilot

17:34.680 --> 17:36.560
to write programs now.

17:36.560 --> 17:38.000
So in some sense, you're already having

17:38.000 --> 17:39.560
that sort of feedback loop.

17:39.560 --> 17:43.160
I'm a little skeptical of that as a mechanism.

17:43.160 --> 17:45.000
At what point would it be the case

17:45.000 --> 17:47.920
that the AI is contributing significantly

17:47.920 --> 17:49.800
in the sense that it would almost be the equivalent

17:49.800 --> 17:51.680
of having additional researchers

17:51.680 --> 17:53.360
to AI progress in software?

17:53.360 --> 17:55.840
The quantitative magnitude of the help

17:55.840 --> 17:57.360
is absolutely central.

17:57.360 --> 17:59.040
So there are plenty of companies

17:59.040 --> 18:00.640
that make some product

18:00.640 --> 18:03.520
that very slightly boost productivity.

18:03.520 --> 18:06.760
So when Xerox makes fax machines,

18:06.760 --> 18:09.360
it maybe increases people's productivity

18:09.360 --> 18:12.680
in office work by 0.1% or something.

18:12.680 --> 18:15.640
You're not gonna have explosive growth out of that

18:15.640 --> 18:20.640
because, okay, now 0.1% more effective R&D at Xerox

18:22.440 --> 18:24.920
than any customers buying the machines.

18:25.920 --> 18:27.560
Not that important.

18:27.560 --> 18:30.200
So I think the thing to look for

18:31.160 --> 18:36.160
is when is it the case that the contributions from AI

18:37.200 --> 18:41.400
are starting to become as large or larger

18:41.400 --> 18:43.840
as the contributions from humans?

18:43.840 --> 18:47.680
So when this is boosting their effective productivity

18:47.680 --> 18:52.680
by 50 or 100% and if you then go from eight months

18:55.200 --> 18:57.120
doubling time, say for effective compute

18:57.120 --> 18:58.160
from software innovation,

18:58.160 --> 19:00.600
things like inventing the transformer

19:00.600 --> 19:02.560
or discovering chinchilla scaling

19:02.560 --> 19:04.560
and doing your training runs more optimally

19:04.560 --> 19:06.720
or creating flash attention.

19:06.720 --> 19:10.680
Yeah, if you move that from, say, eight months to four months

19:10.680 --> 19:12.960
and then the next time you apply that,

19:12.960 --> 19:16.000
it significantly increases the boost you're getting

19:16.000 --> 19:16.840
from the AI.

19:16.840 --> 19:18.400
So maybe instead of giving a 50%

19:18.400 --> 19:20.280
or 100% productivity boost,

19:20.280 --> 19:22.440
that's more like a 200%.

19:22.720 --> 19:25.560
And so it doesn't have to have been able

19:25.560 --> 19:28.240
to automate everything involved

19:28.240 --> 19:30.360
in the process of AI research.

19:30.360 --> 19:33.200
It can be, it's automated a bunch of things.

19:33.200 --> 19:36.800
And then those are being done in extreme profusion

19:36.800 --> 19:39.480
because I think that AI can do,

19:39.480 --> 19:42.240
you have it done much more often because it's so cheap.

19:43.240 --> 19:48.080
And so it's not a threshold of this is human level AI.

19:48.080 --> 19:50.520
It can do everything a human can do

19:50.520 --> 19:52.840
with no weaknesses in any area.

19:52.840 --> 19:55.080
It's that even with its weaknesses,

19:56.120 --> 19:58.720
it's able to bump up the performance.

19:58.720 --> 20:01.840
So that instead of getting like the results we would have

20:01.840 --> 20:05.720
with the 10,000 people working on finding these innovations,

20:05.720 --> 20:07.080
we get the results that we would have

20:07.080 --> 20:10.000
if we had twice as many of those people

20:10.000 --> 20:12.680
with the same kind of skill distribution.

20:12.680 --> 20:15.520
And so that's a, it's like a demanding challenge.

20:15.520 --> 20:19.040
It's like you need quite a lot of capability for that.

20:19.040 --> 20:21.960
But it's also important that it's significantly less

20:21.960 --> 20:25.720
than this is a system where there's no way you can point at it

20:25.720 --> 20:29.040
and say in any respect, it is weaker than a human.

20:29.040 --> 20:32.960
A system that was just as good as a human in every respect,

20:32.960 --> 20:35.760
but also had all of the advantages of an AI,

20:35.760 --> 20:38.640
that is just way beyond this point.

20:38.640 --> 20:43.520
Like if you consider that there's like the output

20:43.520 --> 20:47.080
of our existing fabs make tens of millions

20:47.120 --> 20:49.440
of advanced GPUs per year.

20:49.440 --> 20:53.120
Those GPUs, if they were running sort of AI software

20:53.120 --> 20:54.480
that was as efficient as humans,

20:54.480 --> 20:55.680
as a sample efficient,

20:55.680 --> 20:58.200
it doesn't have any major weaknesses.

20:58.200 --> 21:00.920
So they can work four times as long,

21:02.040 --> 21:04.320
the 168 hour work week,

21:04.320 --> 21:07.840
they can have much more education than any human.

21:07.840 --> 21:11.880
So it's a human, they got a PhD,

21:11.880 --> 21:16.880
it's like, wow, it's like 20 years of education,

21:17.000 --> 21:21.840
maybe longer if they take a slow route on the PhD.

21:21.840 --> 21:25.000
It's just normal for us to train large models

21:25.000 --> 21:29.280
by eat the internet, eat all the published books ever,

21:30.400 --> 21:34.440
read everything on GitHub and get good at predicting it.

21:35.800 --> 21:39.440
So like the level of education vastly beyond any human,

21:39.440 --> 21:43.080
the degree to which the models are focused on task

21:44.080 --> 21:47.040
is higher than all, but like the most motivated humans

21:47.040 --> 21:49.080
when they're really, really gunning for it.

21:49.960 --> 21:53.720
So you combine the things tens of millions of GPUs,

21:53.720 --> 21:58.720
each GPU is doing the work of the very best humans

21:59.120 --> 22:03.560
in the world and like the most capable humans in the world

22:03.560 --> 22:06.040
can command salaries that are a lot higher than the average

22:06.040 --> 22:11.040
and particularly in a field like STEM or narrowly AI.

22:11.680 --> 22:13.760
Like there's no human in the world

22:13.760 --> 22:17.040
who has a thousand years of experience with TensorFlow

22:17.040 --> 22:19.520
or let alone the new AI technology

22:19.520 --> 22:21.960
that were invented the year before.

22:21.960 --> 22:24.400
But if they were around,

22:24.400 --> 22:27.240
yeah, they'd be paid millions of dollars a year.

22:27.240 --> 22:29.520
And so when you consider this,

22:29.520 --> 22:32.720
okay, tens of millions of GPUs,

22:32.720 --> 22:36.920
each is doing the work of maybe 40,

22:36.920 --> 22:40.840
maybe more of these kind of existing workers.

22:40.840 --> 22:44.040
This is like going from a workforce of tens of thousands

22:44.040 --> 22:45.520
to hundreds of millions.

22:47.080 --> 22:50.200
You immediately make all kinds of discoveries then,

22:50.200 --> 22:53.520
you immediately develop all sorts of tremendous technologies.

22:53.520 --> 22:58.320
So human level AI is deep, deep

22:58.320 --> 22:59.920
into an intelligence explosion.

22:59.920 --> 23:01.880
Intelligence explosion has to start

23:01.880 --> 23:04.000
with something weaker than that.

23:04.000 --> 23:05.840
Yeah, well, what is the thing it starts with

23:05.840 --> 23:07.680
and how close are we to that?

23:07.680 --> 23:10.360
Because if you think of a research or an open AI or something,

23:10.560 --> 23:14.800
these are, to be a researcher is not just completing

23:14.800 --> 23:18.360
the hello world prompt that co-pilot does, right?

23:18.360 --> 23:20.360
It's like, you got to choose a new idea,

23:20.360 --> 23:22.160
you got to figure out the right way to approach it.

23:22.160 --> 23:23.480
You perhaps have to manage the people

23:23.480 --> 23:25.960
who are also working with you on that problem.

23:25.960 --> 23:29.120
It's like, it's incredibly complicated skill,

23:29.120 --> 23:31.200
portfolio skills rather than just a single skill.

23:31.200 --> 23:34.160
So yeah, what is the point of wish

23:34.160 --> 23:37.160
that feedback loop starts where you can even,

23:37.160 --> 23:40.160
you're not just doing the 0.5% increase in productivity

23:40.160 --> 23:41.880
that a sort of AI tool might do,

23:41.880 --> 23:44.880
but is actually the equivalent of a researcher

23:44.880 --> 23:47.320
or close to it, what is that point?

23:47.320 --> 23:49.760
So I think maybe a way to look at it

23:49.760 --> 23:51.800
is to give some illustrative examples

23:51.800 --> 23:55.000
of the kinds of capabilities that you might see.

23:55.000 --> 23:59.080
And so because these systems have to be a lot weaker

23:59.080 --> 24:01.240
than the sort of human level things,

24:01.240 --> 24:04.280
what we'll have is intense application

24:04.280 --> 24:07.800
of the ways in which AIs have advantages,

24:07.800 --> 24:09.880
partly offsetting their weaknesses.

24:09.880 --> 24:12.240
And so AIs are cheap.

24:12.240 --> 24:17.240
We can call a lot of them to do many small problems.

24:17.400 --> 24:22.160
And so you'll have situations where you have dumber AIs

24:22.160 --> 24:25.960
that are deployed thousands of times

24:25.960 --> 24:28.740
to equal, say, one human worker.

24:30.360 --> 24:34.520
And they'll be doing things like these voting algorithms

24:34.520 --> 24:38.040
where you, with an LLM, you generate

24:38.040 --> 24:40.200
a bunch of different responses

24:40.200 --> 24:41.840
and take a majority vote among them

24:41.840 --> 24:44.360
that improves performance sum.

24:44.360 --> 24:48.840
You'll have things like the AlphaGo kind of approach

24:48.840 --> 24:52.360
where you use the neural net to do search

24:52.360 --> 24:56.160
and you go deeper with the search by plowing in more compute,

24:56.160 --> 24:58.880
which helps to offset the inefficiency

24:58.880 --> 25:02.520
and weaknesses of the model on its own.

25:02.520 --> 25:06.320
You'll do things that would just be totally impractical

25:06.320 --> 25:09.240
for humans because of the sheer number of steps.

25:09.240 --> 25:10.920
And so an example of that would be

25:10.920 --> 25:13.640
designing synthetic training data.

25:13.640 --> 25:17.520
So humans do not learn by just going into the library

25:17.520 --> 25:20.280
and opening books at random pages.

25:20.280 --> 25:22.520
It's actually much, much more efficient

25:22.520 --> 25:26.400
to have things like schools and classes

25:26.400 --> 25:29.240
where they teach you things in an order that makes sense,

25:29.240 --> 25:30.680
that's focusing on the skills

25:30.680 --> 25:33.160
that are more valuable to learn.

25:33.160 --> 25:34.680
They give you tests and exam,

25:34.680 --> 25:36.840
they're designed to try and elicit the skill

25:36.840 --> 25:39.280
they're actually trying to teach.

25:39.280 --> 25:41.320
And right now we don't bother with that

25:41.320 --> 25:45.160
because we can hoover up more data from the internet.

25:45.160 --> 25:47.480
We're getting towards the end of that.

25:47.480 --> 25:49.880
But yeah, as the AIs get more sophisticated,

25:49.880 --> 25:54.880
they'll be better able to tell what is a useful kind

25:54.880 --> 25:57.080
of skill to practice and to generate that.

25:57.080 --> 25:58.600
And we've done that in other areas.

25:58.600 --> 26:02.040
So AlphaGo, the original version of AlphaGo

26:02.040 --> 26:06.120
was booted up with data from human go play

26:06.120 --> 26:09.200
and then improved with reinforcement learning

26:09.200 --> 26:11.800
and Monte Carlo tree search.

26:11.800 --> 26:14.560
But then AlphaZero,

26:14.560 --> 26:17.080
but they somewhat more sophisticated model

26:17.080 --> 26:20.000
benefited from some other improvements

26:20.000 --> 26:23.360
but was able to go from scratch.

26:23.360 --> 26:27.400
And it generated its own data through self play.

26:28.280 --> 26:31.120
So getting data of a higher quality

26:31.120 --> 26:32.080
than the human data,

26:32.080 --> 26:34.360
because there are no human players that good

26:34.360 --> 26:36.080
available in the dataset.

26:36.080 --> 26:38.200
And also a curriculum.

26:38.200 --> 26:40.400
So that at any given point,

26:40.400 --> 26:42.560
it was playing games against an opponent

26:42.560 --> 26:44.840
of equal skill itself.

26:44.840 --> 26:47.240
And so it was always in an area

26:47.240 --> 26:48.560
when it was easy to learn.

26:48.560 --> 26:51.200
If you're just always losing no matter what you do

26:51.200 --> 26:52.960
or always winning no matter what you do,

26:52.960 --> 26:55.840
it's hard to distinguish which things are better

26:55.840 --> 26:57.240
and which are worse.

26:57.240 --> 27:00.680
And when we have somewhat more sophisticated AIs

27:00.680 --> 27:04.280
that can generate training data and tasks for themselves.

27:04.280 --> 27:08.360
For example, if the AI can generate a lot of unit tests

27:08.360 --> 27:10.440
and then can try and produce programs

27:10.440 --> 27:12.840
that pass those unit tests,

27:12.840 --> 27:16.880
then the interpreter is providing a training signal.

27:16.880 --> 27:20.000
And the AI can get good at figuring out

27:20.000 --> 27:21.680
what's the kind of programming problem

27:21.680 --> 27:24.040
that is hard for AIs right now

27:24.040 --> 27:26.560
that will develop more of the skills that I need

27:27.680 --> 27:29.320
and then do them.

27:29.360 --> 27:32.720
And now you're not gonna have employees at OpenAI

27:32.720 --> 27:34.880
write like a billion programming problems.

27:34.880 --> 27:36.760
That's just not gonna happen.

27:36.760 --> 27:40.160
But you are gonna have AIs given the task

27:40.160 --> 27:42.760
of producing those enormous number

27:42.760 --> 27:44.640
of programming challenges.

27:44.640 --> 27:47.040
In LLMS themselves, there's a paper out of Anthropa

27:47.040 --> 27:49.520
called Constitution AI or Constitution RL

27:49.520 --> 27:51.080
where they basically had the program

27:51.080 --> 27:52.400
just like talk to itself and say,

27:52.400 --> 27:53.480
like, is this response helpful?

27:53.480 --> 27:55.400
If not, how can I make this more helpful?

27:55.400 --> 27:57.640
And the response is improved.

27:57.640 --> 27:58.640
And then you train the model

27:58.760 --> 28:00.040
and the more helpful response is

28:00.040 --> 28:02.440
that it generates by talking to itself

28:02.440 --> 28:04.760
so that it generates natively.

28:04.760 --> 28:07.320
And you could imagine more sophisticated ways

28:07.320 --> 28:08.960
to do that or better ways to do that.

28:08.960 --> 28:11.760
Okay, so then the question is, listen,

28:11.760 --> 28:13.600
GPT-4 already costs like 50 million

28:13.600 --> 28:15.360
or 100 million or whatever it was.

28:15.360 --> 28:17.280
Even if we have greater effective compute

28:17.280 --> 28:19.680
from hardware increases and better models,

28:20.560 --> 28:22.720
it's hard to imagine how we could sustain

28:22.720 --> 28:26.000
like four or five more orders of magnitude,

28:26.000 --> 28:29.680
greater effective size than GPT-4

28:29.680 --> 28:32.120
unless we're jumping in like trillions of dollars

28:32.120 --> 28:35.040
like the entire economies of big countries

28:35.040 --> 28:36.920
into training the next version.

28:36.920 --> 28:40.360
So the question is, do we get something

28:40.360 --> 28:42.480
that can significantly help with AI progress

28:42.480 --> 28:47.480
before we run out of the sheer money and scale

28:48.600 --> 28:50.560
and compute that would require to train it?

28:50.560 --> 28:51.720
Do you have a take on that?

28:51.720 --> 28:53.200
Well, first I'd say remember

28:53.200 --> 28:56.360
that there are these three contributing trends.

28:56.360 --> 28:59.680
So the new H-100s are significantly better

28:59.680 --> 29:02.280
than the A-100s and a lot of companies

29:02.280 --> 29:06.520
are actually waiting for their deliveries of H-100s

29:06.520 --> 29:10.400
to do even bigger training runs along with the work

29:10.400 --> 29:12.320
of hooking them up into clusters

29:12.320 --> 29:14.240
and engineering the thing.

29:14.240 --> 29:16.560
Yeah, so all of those factors are contributing.

29:16.560 --> 29:18.600
And of course, mathematically,

29:19.920 --> 29:22.880
yeah, if you do four orders of magnitude more

29:22.880 --> 29:24.440
than 50 or a hundred million,

29:24.440 --> 29:27.640
then you're getting to Jolene Deli territory.

29:27.640 --> 29:31.200
And yeah, I think the way to look at it is

29:32.600 --> 29:34.560
at each step along the way,

29:34.560 --> 29:38.120
does it look like it makes sense to do the next step?

29:39.120 --> 29:42.040
And so from where we are right now,

29:42.040 --> 29:45.520
seeing the results with GPT-4 and chat GPT,

29:45.520 --> 29:49.760
companies like Google and Microsoft and whatnot

29:49.760 --> 29:54.000
are pretty convinced that this is very valuable.

29:54.000 --> 29:58.720
You have like talk at Google and Microsoft with Bing

29:58.720 --> 30:01.960
that, well, it's like billion dollar matter

30:01.960 --> 30:06.280
to change market share in search by a percentage point.

30:06.280 --> 30:09.240
And so that can fund a lot.

30:09.240 --> 30:13.840
And on the far end, on the extreme,

30:13.840 --> 30:15.680
if you automate human labor,

30:15.680 --> 30:18.040
we have a hundred trillion dollar economy.

30:18.040 --> 30:21.400
Most of that economy is paid out in wages.

30:21.400 --> 30:26.400
So like between 50 and 70 trillion dollars per year.

30:27.040 --> 30:31.600
If you create AGI, it's going to automate all of that

30:31.600 --> 30:35.200
and keep increasing beyond that.

30:35.200 --> 30:38.960
So the value of the completed project

30:38.960 --> 30:43.560
is very much worth throwing our whole economy into it.

30:43.560 --> 30:45.480
If you're gonna get the good version,

30:45.480 --> 30:48.160
not the catastrophic destruction of the human race

30:49.240 --> 30:53.400
or some other disastrous outcome.

30:53.400 --> 30:57.600
And in between, it's a question of,

30:57.600 --> 31:01.160
well, did the next step, how risky and uncertain is it

31:01.160 --> 31:04.040
and how much growth in the revenue

31:04.040 --> 31:06.840
you can generate with it, do you get?

31:06.840 --> 31:09.440
And so for moving up to a billion dollars,

31:09.440 --> 31:11.720
I think that's absolutely gonna happen.

31:11.720 --> 31:14.720
These large tech companies have R&D budgets,

31:14.720 --> 31:16.000
tens of billions of dollars.

31:16.000 --> 31:18.920
And when you think about it like in the relevant sense,

31:18.920 --> 31:21.640
like all the employees at Microsoft

31:21.640 --> 31:23.160
who are doing software engineering,

31:23.160 --> 31:25.640
that's like contributing to creating software objects.

31:25.640 --> 31:29.800
It's not weird to spend tens of billions of dollars

31:29.800 --> 31:34.800
on a product that would do so much.

31:34.960 --> 31:37.840
And I think it's becoming more clear

31:37.840 --> 31:41.440
that there is sort of market opportunity to fund the thing.

31:41.440 --> 31:43.440
Going up to a hundred billion dollars,

31:43.440 --> 31:46.120
that's like, okay, the existing R&D budgets

31:46.120 --> 31:49.000
spread over multiple years.

31:49.000 --> 31:52.800
But if you keep seeing that when you scale up the model,

31:52.800 --> 31:54.640
it substantially improves the performance.

31:54.640 --> 31:56.240
It opens up new applications.

31:56.240 --> 31:58.720
You're not just improving your search,

31:58.720 --> 32:02.080
but maybe it makes self-driving cars work.

32:02.080 --> 32:05.720
You replace bulk software engineering jobs

32:05.720 --> 32:08.440
or if not replace them, amplify productivity.

32:08.440 --> 32:11.160
In this kind of dynamic, you actually probably want to employ

32:11.160 --> 32:13.200
all the software engineers you can get,

32:13.240 --> 32:15.160
as long as they're able to make any contribution

32:15.160 --> 32:18.840
because the returns of improving stuff in AI itself

32:18.840 --> 32:19.680
get so high.

32:19.680 --> 32:24.680
But yeah, so I think that can go up to a hundred billion.

32:24.960 --> 32:28.240
And at a hundred billion, you're using

32:28.240 --> 32:32.680
like a significant fraction of our existing Vab capacity.

32:32.680 --> 32:37.440
Like right now, the revenue of NVIDIA is like 25 billion.

32:38.520 --> 32:42.920
The revenue of TSMC, I believe is like over 50 billion.

32:44.120 --> 32:49.120
I checked in 2021, NVIDIA was maybe 7.5%,

32:50.600 --> 32:53.800
less than 10% of TSMC revenue.

32:54.800 --> 32:59.800
So there's a lot of room and most of that was not AI chips.

33:00.160 --> 33:01.480
They have a large gaming segment.

33:01.480 --> 33:04.040
There are data center GPUs that are used

33:04.040 --> 33:06.640
for video and the like.

33:06.640 --> 33:11.640
So there's room for more than an order of magnitude increase

33:12.640 --> 33:17.640
by redirecting existing Vabs to produce more AI chips.

33:17.800 --> 33:20.240
And they're just actually using the AI chips

33:20.240 --> 33:21.840
that these companies have in their cloud

33:21.840 --> 33:23.880
for the big training runs.

33:23.880 --> 33:27.200
And so I think that's enough to go to the 10 billion

33:27.200 --> 33:28.920
and then combine with stuff like the H100

33:28.920 --> 33:30.320
to go up to the hundred billion.

33:30.320 --> 33:31.640
Just to emphasize for the audience,

33:31.640 --> 33:33.480
the initial point about revenue made,

33:33.480 --> 33:38.480
if it cost open AI $100 million to train GPT-4

33:38.480 --> 33:41.160
and it generates $500 million in revenue,

33:41.480 --> 33:43.680
you pay back your expenses with the hundred million,

33:43.680 --> 33:46.160
you have 400 million for your next training run,

33:46.160 --> 33:48.320
then you train your GPT-4.5,

33:48.320 --> 33:51.640
you get let's say $4 billion out of revenue out of that.

33:51.640 --> 33:54.120
That's where the feedback group of revenue comes from,

33:54.120 --> 33:55.600
where you're automating tasks

33:55.600 --> 33:57.120
and therefore you're making money,

33:57.120 --> 34:00.240
you can use that money to automate more tasks.

34:00.240 --> 34:04.560
On the ability to redirect the fat production

34:04.560 --> 34:06.680
towards AI chips.

34:06.680 --> 34:10.000
So then the TLDR on,

34:10.000 --> 34:12.600
you want $100 billion worth of compute.

34:12.600 --> 34:16.120
I mean, fabs take what like a decade or so to build.

34:16.120 --> 34:17.440
So given the ones we have now

34:17.440 --> 34:19.600
and the ones that are gonna come online in the next decade,

34:19.600 --> 34:24.200
is there enough to sustain $100 billion of GPU compute

34:24.200 --> 34:26.800
if you wanted to spend that on a training run?

34:26.800 --> 34:29.520
Yes, you would definitely make the hundred billion one.

34:29.520 --> 34:32.640
How do you go up to a trillion dollar run and larger?

34:33.560 --> 34:36.560
It's gonna involve more fab construction

34:36.560 --> 34:39.880
and yeah, fabs can take a long time to build.

34:39.880 --> 34:43.360
On the other hand, if in fact,

34:43.360 --> 34:48.240
you're getting very high revenue from the AI systems

34:48.240 --> 34:49.960
and you're actually bottlenecked

34:49.960 --> 34:52.720
on the construction of these fabs,

34:52.720 --> 34:55.680
then their price could skyrocket.

34:55.680 --> 35:00.480
And that lead to measures we've never seen before

35:00.480 --> 35:03.600
to expand and accelerate fab production.

35:03.600 --> 35:04.920
Like if you consider,

35:04.920 --> 35:07.640
so at the limit has you're getting models

35:07.640 --> 35:10.800
that approach human-like capability.

35:10.800 --> 35:12.000
Can you imagine things that are getting close

35:12.000 --> 35:15.960
to like brain-like efficiencies plus AI advantages?

35:15.960 --> 35:17.680
We were talking before about, well,

35:17.680 --> 35:21.760
a GPU that is supporting an AI,

35:21.760 --> 35:24.200
really it's a cluster of GPU supporting AI

35:24.200 --> 35:28.120
is that do things in parallel, data parallelism.

35:28.120 --> 35:32.920
But if that can work four times as much as a human,

35:32.920 --> 35:35.640
a highly skilled, motivated, focused human

35:35.640 --> 35:38.080
with levels of education that have never been seen

35:38.080 --> 35:39.880
in the human population.

35:39.880 --> 35:43.640
And so if like a typical software engineer

35:43.640 --> 35:45.400
can earn hundreds of thousands of dollars,

35:45.400 --> 35:48.080
the world's best software engineers

35:48.080 --> 35:49.800
can earn millions of dollars today

35:49.800 --> 35:53.960
and maybe more in a world where there's so much demand for AI.

35:54.960 --> 35:58.960
And then times four for working all the time.

35:58.960 --> 36:00.280
Well, I mean, if you have,

36:00.280 --> 36:03.240
if you can generate like close to $10 million a year

36:04.240 --> 36:07.600
out of the future version of H100,

36:07.600 --> 36:09.760
and it costs tens of thousands of dollars

36:09.760 --> 36:12.040
with a huge profit margin now.

36:12.040 --> 36:15.600
And profit margin could be reduced

36:15.600 --> 36:16.920
with like large production.

36:18.080 --> 36:21.680
That is a big difference that chip

36:21.680 --> 36:24.200
pays for itself almost instantly.

36:25.200 --> 36:30.040
And so you could support paying 10 times as much

36:30.040 --> 36:33.360
to have these fabs constructed more rapidly.

36:33.360 --> 36:37.520
You could have, if AI is starting to be able to contribute,

36:37.520 --> 36:40.800
you could have AI contributing more

36:40.800 --> 36:42.880
of the skilled technical work that makes it hard

36:42.880 --> 36:47.200
for say, NVIDIA to suddenly find thousands upon thousands

36:47.200 --> 36:51.520
of top quality engineering hires, if AI can provide that.

36:51.520 --> 36:54.880
Now, if AI hasn't reached that level of performance,

36:54.880 --> 36:57.680
then this is how you can have things stall out.

36:57.680 --> 36:59.760
And like a world where AI progress stalls out

36:59.760 --> 37:02.080
is one where you go to the $100 billion

37:02.080 --> 37:05.000
and then over succeeding years,

37:05.000 --> 37:08.480
trillion dollar things, software, progress,

37:09.400 --> 37:12.800
turns out to stall.

37:12.800 --> 37:14.960
You lose the gains that you are getting

37:14.960 --> 37:16.920
from moving researchers from other fields,

37:16.920 --> 37:19.200
lots of physicists and people from other areas

37:19.200 --> 37:21.240
of computer science have been going to AI.

37:21.240 --> 37:24.600
But you sort of tap out those resources.

37:24.600 --> 37:28.320
Has it AI becomes a larger proportion of the research field?

37:28.520 --> 37:30.880
And like, okay, you've put in all of these inputs,

37:30.880 --> 37:33.280
but they just haven't yielded AI yet.

37:33.280 --> 37:37.240
I think that set of inputs probably would yield

37:37.240 --> 37:38.920
the kind of AI capabilities needed

37:38.920 --> 37:40.280
for intelligence explosion.

37:40.280 --> 37:43.680
But if it doesn't, after we've exhausted

37:43.680 --> 37:46.880
this current scale up of like increasing the share

37:46.880 --> 37:49.440
of our economy that is trying to make AI,

37:49.440 --> 37:51.800
if that's not enough, then after that,

37:51.800 --> 37:53.720
you have to wait for the slow grind

37:53.720 --> 37:56.080
of things like general economic growth,

37:56.080 --> 37:57.600
population growth and such.

37:57.600 --> 37:58.960
And so things slow.

37:58.960 --> 38:01.280
And that results in my credences

38:01.280 --> 38:03.680
and this kind of advanced AI happening

38:03.680 --> 38:07.800
to be relatively concentrated like over the next 10 years

38:07.800 --> 38:10.040
compared to the rest of the century.

38:10.040 --> 38:12.120
Because we just can't, we can't keep going

38:12.120 --> 38:16.640
with this rapid redirection of resources into AI.

38:16.640 --> 38:18.440
That's a one time thing.

38:18.440 --> 38:21.320
If the current scale up works, it's going to happen.

38:21.320 --> 38:22.880
We're gonna get to AI really fast,

38:22.880 --> 38:24.800
like within the next 10 years or something.

38:24.800 --> 38:26.480
If the current scale up doesn't work,

38:26.520 --> 38:29.560
all we're left with is just like economy

38:29.560 --> 38:30.880
growing like 2% of years.

38:30.880 --> 38:34.080
We have like 2% a year more resources to spend on AI.

38:34.080 --> 38:36.680
And at that scale, you're talking about decades

38:36.680 --> 38:39.720
before you can just through sheer brute force,

38:39.720 --> 38:42.720
you can train the $10 trillion model or something.

38:42.720 --> 38:45.240
Let's talk about why you have your thesis

38:45.240 --> 38:47.960
that the current scale up would work.

38:47.960 --> 38:50.080
What is the evidence from AI itself

38:50.080 --> 38:52.240
or maybe from private evolution

38:52.240 --> 38:53.760
and the evolution of other animals?

38:53.760 --> 38:55.880
Just give me the whole, the whole confluence

38:55.880 --> 38:56.720
of reasons that make you think.

38:56.720 --> 38:59.280
I think maybe the best way to look at that

38:59.280 --> 39:02.440
might be to consider when I first became interested

39:02.440 --> 39:04.080
in this area, so in the 2000s,

39:04.080 --> 39:07.680
which was before the deep learning revolution,

39:07.680 --> 39:09.080
how would I think about timelines?

39:09.080 --> 39:11.120
How did I think about timelines?

39:11.120 --> 39:14.960
And then how have I updated based on what has been happening

39:14.960 --> 39:16.400
with deep learning?

39:16.400 --> 39:20.240
And so back then, I would have said,

39:21.480 --> 39:24.000
we know the brain is a physical object

39:24.000 --> 39:26.080
and information processing device.

39:26.080 --> 39:29.600
It works, it's possible.

39:29.600 --> 39:31.560
And not only is it possible,

39:31.560 --> 39:35.160
it was created by evolution on earth.

39:35.160 --> 39:38.520
And so that gives us something of an upper bound

39:38.520 --> 39:42.080
in that this kind of brute force was sufficient.

39:42.080 --> 39:44.240
There are some complexities with like,

39:44.240 --> 39:46.720
well, what if it was a freak accident

39:46.720 --> 39:49.280
and it didn't happen on all of the other planets

39:49.280 --> 39:51.480
and that added some value.

39:51.480 --> 39:53.560
I have a paper with Nick Bustrom on this.

39:53.560 --> 39:57.720
I think basically that's not that important an issue.

39:57.720 --> 40:00.640
There's converging evolution like octopi

40:00.640 --> 40:03.440
are also quite sophisticated.

40:03.440 --> 40:08.440
If a special event was at the level of forming cells at all

40:08.760 --> 40:10.840
or forming brains at all,

40:10.840 --> 40:13.320
we get to skip that because we're choosing

40:13.320 --> 40:15.080
to build computers and we already exist.

40:15.080 --> 40:17.000
We have that advantage.

40:17.000 --> 40:20.360
So say evolution gives something of an upper bound,

40:20.400 --> 40:23.280
really intensive massive brute force search

40:24.520 --> 40:26.560
and things like evolutionary algorithms

40:26.560 --> 40:28.240
can produce intelligence.

40:28.240 --> 40:32.360
Doesn't the fact that octopi and I guess other mammals,

40:32.360 --> 40:34.360
they got to the point of being like pretty intelligent

40:34.360 --> 40:36.200
but not human level intelligent.

40:36.200 --> 40:37.880
Is that some evidence that there's a hard step

40:37.880 --> 40:41.000
between a cephalopod and a human?

40:41.000 --> 40:44.040
Yeah, so that would be a place to look.

40:45.800 --> 40:48.400
It doesn't seem particularly compelling.

40:48.400 --> 40:53.400
One source of evidence on that is work by Herculano Hutzel.

40:55.080 --> 40:56.600
I hope I haven't mispronounced her name

40:56.600 --> 41:01.600
but she's a neuroscientist who has dissolved the brains

41:01.640 --> 41:06.160
of many creatures and by counting the nuclei,

41:06.160 --> 41:10.600
she's able to determine how many neurons are present

41:10.600 --> 41:14.240
in different species and find a lot of interesting trends

41:14.240 --> 41:15.080
in scaling laws.

41:15.400 --> 41:19.200
She's a paper discussing the human brain

41:19.200 --> 41:21.760
has a scaled up primate brain

41:21.760 --> 41:25.080
and across like a wide variety of animals

41:25.080 --> 41:27.320
and mammals in particular.

41:27.320 --> 41:29.800
There are certain characteristic changes

41:29.800 --> 41:32.440
in the relative number of neurons

41:32.440 --> 41:35.800
size of different brain regions have things scale up.

41:36.600 --> 41:41.600
There's a lot of, yeah, there's a lot of structural similarity

41:42.600 --> 41:47.600
there and you can explain a lot of what is different about us

41:47.800 --> 41:50.960
with a pretty brute force story,

41:50.960 --> 41:54.360
which is that you expend resources

41:54.360 --> 41:58.240
on having a bigger brain, keeping it in good order,

41:58.240 --> 41:59.760
giving it time to learn.

41:59.760 --> 42:01.680
So we have an unusually long childhood,

42:01.680 --> 42:03.840
unusually long in its period.

42:03.840 --> 42:06.400
We spend more compute by having a larger brain

42:06.400 --> 42:09.000
than other animals, more than three times

42:09.000 --> 42:10.920
as large as chimpanzees.

42:10.920 --> 42:14.760
And then we have a longer childhood than chimpanzees

42:14.760 --> 42:17.200
and much more than many, many other creatures.

42:17.200 --> 42:19.200
So we're spending more compute in a way

42:19.200 --> 42:21.680
that's analogous to like having a bigger model

42:21.680 --> 42:23.840
and having more training time with it.

42:23.840 --> 42:28.840
And given that we see with our AI models,

42:29.320 --> 42:31.960
this sort of like large consistent benefits

42:31.960 --> 42:35.000
from increasing compute spent in those ways

42:35.000 --> 42:38.320
and with qualitatively new capabilities

42:38.320 --> 42:40.280
showing up over and over again,

42:40.280 --> 42:44.480
particularly in areas that sort of AI skeptics call out

42:44.480 --> 42:47.880
in my experience like over the last 15 years,

42:47.880 --> 42:49.680
the things that people call out has like,

42:49.680 --> 42:51.520
ah, but the AI can't do that.

42:51.520 --> 42:54.000
And it's because of a fundamental limitation.

42:54.000 --> 42:55.800
We've gone through a lot of them.

42:55.800 --> 43:00.240
There were Winograd schemas, catastrophic forgetting,

43:00.240 --> 43:01.680
quite a number.

43:01.680 --> 43:06.680
And yeah, they have repeatedly gone away through scaling.

43:07.280 --> 43:12.040
And so there's a picture that we're seeing supported

43:12.040 --> 43:15.440
from biology and from our experience with AI

43:15.440 --> 43:19.320
where you can explain like, yeah, in general,

43:19.320 --> 43:22.920
there are trade-offs where the extra fitness you get

43:22.920 --> 43:25.360
from a brain is not worth it.

43:25.360 --> 43:29.160
And so creatures wind up mostly with small brains

43:29.160 --> 43:32.000
because they can save that biological energy

43:32.000 --> 43:35.880
and that time to reproduce for digestion.

43:35.920 --> 43:36.760
And so on.

43:36.760 --> 43:40.640
And humans, we actually seem to have wound up

43:40.640 --> 43:43.840
in a niche within self-reinforcing

43:43.840 --> 43:46.200
where we greatly increase the returns

43:46.200 --> 43:47.880
to having large brains.

43:47.880 --> 43:52.880
And language and technology are the sort of obvious candidates.

43:53.280 --> 43:55.480
When you have humans around you

43:55.480 --> 43:58.000
who know a lot of things and they can teach you

43:58.000 --> 43:59.720
and compared to almost any other species,

43:59.720 --> 44:03.400
we have vastly more instruction from parents

44:03.440 --> 44:06.000
and the society of the young.

44:06.000 --> 44:09.240
Then you're getting way more from your brain

44:09.240 --> 44:11.160
because you can get, per minute,

44:11.160 --> 44:13.720
you can learn a lot more useful skills

44:13.720 --> 44:15.920
and then you can provide the energy you need

44:15.920 --> 44:19.080
to feed that brain by hunting and gathering,

44:19.080 --> 44:22.280
by having fire that makes digestion easier.

44:22.280 --> 44:24.680
And basically how this process goes on,

44:24.680 --> 44:27.680
it's increasing the marginal increase

44:27.680 --> 44:29.920
in reproductive fitness you get

44:29.920 --> 44:31.680
from allocating more resources

44:31.680 --> 44:34.840
along a bunch of dimensions towards cognitive ability.

44:34.840 --> 44:39.000
And so that's bigger brains, longer childhood,

44:39.000 --> 44:41.160
having our attention be more on learning.

44:41.160 --> 44:45.880
So humans play a lot and we keep playing as adults,

44:45.880 --> 44:49.200
which is a very weird thing compared to other animals.

44:49.200 --> 44:53.480
We're more motivated to copy other humans around us

44:53.480 --> 44:55.400
than like even than the other primates.

44:55.400 --> 44:58.040
And so these are sort of motivational changes

44:58.040 --> 45:01.800
that keep us using more of our attention and effort

45:01.800 --> 45:03.480
on learning, which pays off more

45:03.480 --> 45:05.920
when you have a bigger brain and a longer lifespan

45:05.920 --> 45:07.000
in which to learn.

45:07.000 --> 45:11.120
Many creatures are subject to lots of predation or disease.

45:11.120 --> 45:15.120
And so if you try, you're a mayfly or a mouse,

45:15.120 --> 45:17.320
if you try and invest in like a giant brain

45:17.320 --> 45:19.600
and a very long childhood,

45:19.600 --> 45:22.920
you're quite likely to be killed by some predator

45:22.920 --> 45:25.760
or some disease before you're able to actually use it.

45:25.800 --> 45:28.120
And so that means you actually have exponentially

45:28.120 --> 45:30.680
increasing costs in a given niche.

45:30.680 --> 45:34.000
So if I have a 50% chance of dying every few months

45:34.000 --> 45:38.000
of a little mammal or a little lizard or something,

45:38.000 --> 45:42.000
that means the cost of going from three months to 30 months

45:42.000 --> 45:44.840
of learning and childhood development,

45:44.840 --> 45:47.120
it's not 10 times the loss.

45:47.120 --> 45:49.560
It's now it's two to the negative 10.

45:49.560 --> 45:54.560
So factor of 1,024 reduction in the benefit I get

45:55.840 --> 45:57.480
from what I ultimately learn

45:57.480 --> 46:01.280
because 99.9% of the animals will have been killed

46:01.280 --> 46:02.360
before that point.

46:02.360 --> 46:04.600
We're in a niche where we're like a large,

46:04.600 --> 46:07.960
long-lived animal with language and technology.

46:07.960 --> 46:10.520
So where we can learn a lot from our groups.

46:10.520 --> 46:14.960
And that means it pays off to really just expand

46:14.960 --> 46:19.560
our investment on these multiple fronts in intelligence.

46:19.560 --> 46:21.160
That's so interesting.

46:22.240 --> 46:24.520
Just with the audience, the calculation about like

46:24.560 --> 46:26.080
two to the whatever months is just like,

46:26.080 --> 46:27.480
you have a half chance of dying this month,

46:27.480 --> 46:29.440
a half chance of dying next month,

46:29.440 --> 46:30.400
you multiply those together.

46:30.400 --> 46:32.440
Okay, there's other species though

46:32.440 --> 46:37.440
that do live in flocks or as packs where you could imagine.

46:37.680 --> 46:40.480
I mean, they do have like a smaller version

46:40.480 --> 46:42.920
of the development of cubs into

46:42.920 --> 46:44.600
that I like play with each other.

46:44.600 --> 46:49.240
Why isn't this a hill on which they could have climbed

46:49.240 --> 46:52.360
to human level intelligence themselves?

46:52.400 --> 46:55.280
If it's something like language or technology,

46:55.280 --> 46:57.960
humans were getting smarter before we got language.

46:57.960 --> 46:59.480
I mean, obviously we had to get smarter

46:59.480 --> 47:00.440
to get language, right?

47:00.440 --> 47:02.480
We couldn't just get language without becoming smarter.

47:02.480 --> 47:06.480
So yeah, it seems like there should be other species

47:06.480 --> 47:08.000
that should have beginnings

47:08.000 --> 47:09.840
of this sort of cognitive revolution,

47:09.840 --> 47:12.440
especially given how valuable it is given,

47:12.440 --> 47:14.120
listen, we've dominated the world.

47:14.120 --> 47:16.240
You would think there'd be selective pressure for it.

47:16.240 --> 47:18.480
Evolution doesn't have foresight.

47:18.480 --> 47:20.680
The thing in this generation

47:20.680 --> 47:24.920
that gets more surviving offspring and grandchildren,

47:24.920 --> 47:27.280
that's the thing that becomes more common.

47:27.280 --> 47:29.360
Evolution doesn't look ahead and they,

47:29.360 --> 47:33.240
oh, in a million years, you'll have a lot of descendants.

47:33.240 --> 47:36.640
It's what survives and reproduces now.

47:36.640 --> 47:39.120
And so in fact, there are correlations

47:39.120 --> 47:44.120
where social animals do on average, have larger brains.

47:45.120 --> 47:48.440
And part of that is probably that

47:48.440 --> 47:51.000
the additional social applications of brains,

47:51.000 --> 47:54.000
like keeping track of which of your group members

47:54.000 --> 47:56.840
have helped you before so that you can reciprocate.

47:56.840 --> 47:59.360
You scratch my back, I'll scratch yours,

47:59.360 --> 48:01.840
remembering who's dangerous within the group,

48:01.840 --> 48:03.200
that sort of thing.

48:03.200 --> 48:06.360
It's an additional application of intelligence.

48:06.360 --> 48:08.840
And so there's some correlation there.

48:08.840 --> 48:12.000
But what it seems like is that,

48:12.440 --> 48:14.640
yeah, in most of these cases,

48:15.640 --> 48:17.960
it's enough to invest more,

48:17.960 --> 48:21.360
but not invest to the point where a mind

48:21.360 --> 48:25.000
can easily develop language and technology and pass it on.

48:25.000 --> 48:28.080
And so there are, you see bits of tool use

48:28.080 --> 48:30.920
in some other primates who have an advantage that,

48:30.920 --> 48:33.560
so compared to say the whales who have,

48:33.560 --> 48:34.640
they have quite large brains,

48:34.640 --> 48:36.760
partly because they are so large themselves

48:36.760 --> 48:38.840
and they have some other thing,

48:38.840 --> 48:40.280
but they don't have hands,

48:40.280 --> 48:42.680
which means that reduces a bunch of ways

48:42.680 --> 48:44.680
in which brains can pay off

48:44.680 --> 48:47.840
and investments in the functioning of that brain.

48:47.840 --> 48:52.840
But yeah, so primates will use sticks to extract termites.

48:53.880 --> 48:55.880
Capuchin monkeys will open clams

48:55.880 --> 48:58.040
by smashing them with a rock.

48:58.040 --> 49:00.000
So there's bits of tool use,

49:00.000 --> 49:04.840
but what they don't have is the ability to sustain culture.

49:04.840 --> 49:07.920
A particular primate will maybe discover

49:07.920 --> 49:09.920
one of these tactics and maybe it'll be copied

49:09.920 --> 49:12.040
by their immediate group.

49:12.040 --> 49:14.160
But they're not holding onto it that well.

49:14.160 --> 49:16.880
They're like, well, when they see the other animal do it,

49:16.880 --> 49:19.080
they can copy it in that situation.

49:19.080 --> 49:20.720
They don't actively teach each other,

49:20.720 --> 49:23.800
their population locally is quite small.

49:23.800 --> 49:26.280
So it's easy to forget things,

49:26.280 --> 49:28.240
easy to lose information.

49:28.240 --> 49:31.560
And in fact, they remain technologically stagnant

49:31.560 --> 49:34.080
for hundreds of thousands of years.

49:34.080 --> 49:37.320
And we can actually look at some human situations.

49:37.320 --> 49:39.520
So there's an old paper,

49:39.520 --> 49:43.040
I believe by the economist, Michael Kramer,

49:43.920 --> 49:46.840
talks about technological growth

49:46.840 --> 49:50.640
in the different continents for human societies.

49:50.640 --> 49:55.640
And so you have Eurasia is the largest integrated

49:55.720 --> 49:57.800
connected area, Africa is partly connected to it,

49:57.800 --> 50:00.680
but the Sahara desert restricts the flow

50:00.680 --> 50:03.320
of information and technology and such.

50:03.320 --> 50:04.640
And then you had the Americas,

50:04.640 --> 50:07.200
which were after the colonization from the land bridge

50:07.200 --> 50:11.040
were largely separated and are smaller than Eurasia.

50:11.040 --> 50:14.320
Then Australia, and then you had like smaller island situations

50:14.320 --> 50:15.560
like Tasmania.

50:15.560 --> 50:19.320
And so technological progress seems to have been faster

50:19.320 --> 50:22.200
at the larger, the connected group of people.

50:22.960 --> 50:26.480
And in the smallest groups, so like in Tasmania,

50:26.480 --> 50:27.840
you had a relatively small population

50:27.840 --> 50:29.480
and they actually lost technology.

50:30.400 --> 50:34.000
So things like they lost some like fishing techniques.

50:34.000 --> 50:36.640
And if you have a small population

50:36.640 --> 50:40.000
and you have some limited number of people who know a skill

50:40.000 --> 50:42.960
and they happen to die or it happened,

50:42.960 --> 50:46.200
there's like some change in circumstances

50:46.200 --> 50:49.720
that causes people not to practice or pass on that thing.

50:49.720 --> 50:51.120
And then you lose it.

50:51.120 --> 50:54.360
And if you have few people, you're doing less innovation.

50:54.360 --> 50:57.720
The rate at which you lose technologies

50:57.720 --> 50:59.160
to some kind of local disturbance

50:59.160 --> 51:02.000
and the rate at which you create new technologies

51:02.000 --> 51:03.720
can wind up in balance.

51:03.720 --> 51:08.720
And the great change of hominids and humanity

51:08.880 --> 51:10.440
if that we wound up in this situation,

51:10.440 --> 51:13.360
we were accumulating faster than we were losing.

51:13.360 --> 51:14.920
And as we accumulated,

51:14.920 --> 51:18.160
those technologies allowed us to expand our population.

51:18.160 --> 51:21.640
They created additional demand for intelligence

51:21.640 --> 51:24.280
so that our brains became three times as large.

51:24.280 --> 51:25.120
Is that chimpanzees?

51:25.120 --> 51:25.960
Chimpanzees, yeah.

51:25.960 --> 51:30.120
And our ancestors who had a similar brain size.

51:30.320 --> 51:34.120
And then the crucial point, I guess, in relevance to AI

51:34.120 --> 51:38.240
is that the selective pressures against intelligence

51:38.240 --> 51:42.760
in other animals are not acting against these neural networks

51:42.760 --> 51:44.080
because we are, you know,

51:44.080 --> 51:45.520
they're not gonna get like eaten by a predator

51:45.520 --> 51:48.240
if they spend too much time becoming more intelligent.

51:48.240 --> 51:49.800
We're like explicitly treating them

51:49.800 --> 51:51.040
to become more intelligent.

51:51.040 --> 51:54.100
So we have like good first principles reason to think

51:54.100 --> 51:57.280
that if it was scaling that made our minds this powerful

51:57.280 --> 51:59.960
and if the things that prevented other animals

52:00.800 --> 52:04.000
from scaling are not impinging on these neural networks,

52:04.000 --> 52:05.280
that these things should just continue

52:05.280 --> 52:06.440
to become very smart.

52:06.440 --> 52:09.160
Yeah, we're growing them in a technological culture

52:09.160 --> 52:11.880
where there are jobs like software engineer

52:11.880 --> 52:16.320
that depend much more on sort of cognitive output

52:16.320 --> 52:19.520
and less on things like metabolic resources

52:19.520 --> 52:21.800
devoted to the immune system

52:21.800 --> 52:25.560
or to like building big muscles to throw spears.

52:25.560 --> 52:27.600
This is kind of a side note, but I'm just kind of interested.

52:27.600 --> 52:28.760
I think you referenced at some point,

52:28.800 --> 52:30.560
I think it's a bit of a chinchilla scaling for the audience.

52:30.560 --> 52:33.560
This is a paper from DeepMind which describes

52:33.560 --> 52:35.040
if you have a model of a certain size,

52:35.040 --> 52:36.880
what is the optimum amount of data

52:36.880 --> 52:38.720
that it should be trained on?

52:38.720 --> 52:40.240
So you can imagine bigger models,

52:40.240 --> 52:43.120
you can use more data to train them.

52:43.120 --> 52:44.240
And in this way you can figure out

52:44.240 --> 52:45.240
where should you spend your computer,

52:45.240 --> 52:46.600
should you spend it on making the model bigger

52:46.600 --> 52:49.200
or should you spend it on training it for longer?

52:49.200 --> 52:53.280
I'm curious if in the case of different animals,

52:53.280 --> 52:54.760
in some sense they're like model sizes,

52:54.760 --> 52:55.800
they're how big their brain is

52:55.800 --> 52:56.960
and they're training data sizes,

52:56.960 --> 52:58.560
like how long they're cubs

52:58.560 --> 52:59.920
or how long they're infants or toddlers

52:59.920 --> 53:02.280
or before they're full adults.

53:02.280 --> 53:04.720
Is there some sort of like scaling law of?

53:04.720 --> 53:07.960
Yeah, I mean, so the chinchilla scaling isn't interesting

53:07.960 --> 53:11.400
because we were talking earlier about the cost function

53:11.400 --> 53:13.280
for having a longer childhood.

53:13.280 --> 53:16.120
And so where it's like exponentially increasing

53:16.120 --> 53:17.920
in the amount of training compute you have

53:17.920 --> 53:21.480
when you have exogenous forces that can kill you.

53:21.480 --> 53:23.800
Whereas when we do big training runs,

53:23.800 --> 53:26.920
the cost of throwing in more GPUs is almost linear.

53:26.920 --> 53:29.000
And it's much better to be linear

53:29.000 --> 53:30.800
than exponentially decay.

53:30.800 --> 53:31.720
Oh, that's a really good point.

53:31.720 --> 53:32.880
As you expand resources.

53:32.880 --> 53:36.440
And so chinchilla scaling would suggest that like,

53:36.440 --> 53:40.160
yeah, for a brain of sort of human size,

53:40.160 --> 53:42.840
it would be optimal to have many millions of years

53:42.840 --> 53:46.480
of education, but obviously that's impractical

53:46.480 --> 53:49.680
because of exogenous mortality for humans.

53:49.680 --> 53:52.640
And so there's a fairly compelling argument

53:52.680 --> 53:57.680
that relative to the situation where we would train AI,

53:58.320 --> 54:02.840
that animals are systematically way under trained.

54:02.840 --> 54:03.680
That's so interesting.

54:03.680 --> 54:06.040
And now they're more efficient than our models.

54:06.040 --> 54:08.040
We still have room to improve our algorithms

54:08.040 --> 54:10.720
to catch up with the efficiency of brains,

54:10.720 --> 54:15.720
but they are laboring under that disadvantage, yeah.

54:16.160 --> 54:17.720
That is so interesting.

54:17.720 --> 54:19.680
Okay, so I guess another question you could have

54:19.680 --> 54:24.680
is humans got started on this evolutionary hill climbing

54:24.760 --> 54:26.400
route where we're getting more intelligent

54:26.400 --> 54:28.680
that has more benefits for us.

54:28.680 --> 54:31.520
Why didn't we go all the way on that route?

54:31.520 --> 54:33.240
If intelligence is so powerful,

54:33.240 --> 54:38.240
why aren't all humans as smart as we know humans can be?

54:39.000 --> 54:40.080
At least that smart.

54:41.000 --> 54:42.480
If intelligence is so powerful,

54:42.480 --> 54:44.800
like why hasn't there been stronger selective pressure?

54:44.800 --> 54:46.400
I understand like, oh, listen, hip size,

54:46.400 --> 54:48.360
you can't like give birth to a really big headed baby

54:48.360 --> 54:49.200
or whatever, but you would think

54:49.240 --> 54:51.960
evolution would figure out some way to offset

54:51.960 --> 54:55.360
that if intelligence has such big power

54:55.360 --> 54:56.800
and it's so useful.

54:56.800 --> 54:59.560
Yeah, I think if you actually look at it quantitatively,

54:59.560 --> 55:01.040
that's not true.

55:01.040 --> 55:03.560
And even in sort of recent history,

55:03.560 --> 55:06.640
there has been, it looks like a pretty close balance

55:06.640 --> 55:09.960
between the costs and the benefits

55:09.960 --> 55:12.800
of having more cognitive abilities.

55:12.800 --> 55:17.320
And so you say like, who needs to worry

55:17.320 --> 55:20.040
about like the metabolic costs?

55:20.040 --> 55:25.040
Like humans put like order 20% of our metabolic energy

55:25.800 --> 55:29.360
into the brain and it's higher for like young children.

55:29.360 --> 55:34.200
So 20% of the, and then there's like breathing

55:34.200 --> 55:36.800
and digestion and the immune system.

55:36.800 --> 55:39.360
And so for most of history,

55:39.360 --> 55:41.480
people have been dying left and right.

55:41.480 --> 55:43.760
Like a very large proportion of people

55:43.760 --> 55:46.480
will die of infectious disease.

55:46.520 --> 55:50.560
And if you put more resources into your immune system,

55:50.560 --> 55:52.080
you survive.

55:52.080 --> 55:55.480
So it's like life or death pretty directly

55:55.480 --> 55:56.760
via that mechanism.

55:57.560 --> 56:00.400
And then this is related also

56:00.400 --> 56:03.240
people die more of disease during famine.

56:03.240 --> 56:04.480
And so there's boom or bust.

56:04.480 --> 56:08.080
And so if you have 20% less metabolic requirements

56:08.080 --> 56:11.200
or has anger a child and if you have a lot more,

56:11.200 --> 56:14.800
I mean it's like 40 or 50% less metabolic requirements,

56:14.800 --> 56:17.640
you're much more likely to survive that famine.

56:17.640 --> 56:19.640
So these are pretty big.

56:20.480 --> 56:21.720
And then there's a trade-off

56:21.720 --> 56:24.040
about just cleaning and mutational load.

56:24.040 --> 56:27.920
So every generation new mutations and errors happen

56:27.920 --> 56:29.220
in the process of reproduction.

56:29.220 --> 56:34.220
And so like we know there are many genetic abnormalities

56:34.640 --> 56:37.280
that occur through new mutations each generation.

56:37.280 --> 56:40.800
And in fact, we have Down syndrome

56:40.800 --> 56:43.880
is the chromosomal abnormality that you can survive.

56:43.920 --> 56:47.240
All the others just kill the embryo.

56:47.240 --> 56:49.000
And so we never see them.

56:49.920 --> 56:52.360
But like Down syndrome occurs a lot.

56:52.360 --> 56:55.040
And there are many other lethal mutations

56:55.040 --> 56:57.840
and as you go to the less damaging ones,

56:57.840 --> 57:00.960
there are enormous numbers of less damaging mutations

57:00.960 --> 57:03.720
that are degrading every system in the body.

57:03.720 --> 57:08.720
And so evolution each generation has to pull away

57:09.680 --> 57:11.200
at some of this mutational load.

57:11.200 --> 57:14.080
And the priority with which that mutational load

57:14.080 --> 57:16.880
is pulled out scales in proportion

57:16.880 --> 57:19.800
to how much the traits it's affecting impact fitness.

57:19.800 --> 57:23.680
So you got new mutations that impact your resistance

57:23.680 --> 57:26.800
to malaria, you got new mutations

57:26.800 --> 57:28.160
that damage brain function.

57:29.520 --> 57:34.600
And then have those mutations are purged each generation.

57:34.600 --> 57:37.320
If malaria is a bigger difference in mortality

57:37.320 --> 57:39.280
than like the incremental effectiveness

57:39.360 --> 57:43.240
of hunter-gatherer you get from being slightly more intelligent

57:43.240 --> 57:47.160
then you'll purge that mutational load first.

57:47.160 --> 57:49.600
And similarly, if there's like,

57:49.600 --> 57:53.480
humans have been vigorously adapting to new circumstances.

57:53.480 --> 57:56.360
So since agriculture, people have been developing things

57:56.360 --> 58:01.360
like the ability to have amulets to digest breads,

58:03.000 --> 58:05.720
the ability to like digest milk.

58:05.720 --> 58:08.760
And if you're evolving for all of these things

58:08.760 --> 58:11.600
and if some of the things that give an advantage for that

58:11.600 --> 58:13.760
incidentally carry along nearby them

58:13.760 --> 58:16.760
some negative effect on another trait

58:16.760 --> 58:18.560
then that other trait can be damaged.

58:18.560 --> 58:22.600
So it really matters how important to survival

58:22.600 --> 58:24.960
and reproduction cognitive abilities were

58:24.960 --> 58:27.440
compared to everything else that organism has to do.

58:27.440 --> 58:32.080
And that in particular like surviving, feasting famine,

58:32.080 --> 58:34.080
having like the physical abilities

58:34.080 --> 58:35.920
to do hunting and gathering.

58:35.920 --> 58:37.800
And like, even if you're like very good

58:37.800 --> 58:39.920
at planning your hunting,

58:39.920 --> 58:41.880
being able to throw a spear harder

58:41.880 --> 58:43.320
can be a big difference.

58:43.320 --> 58:46.040
And that needs energy to build those muscles

58:46.040 --> 58:47.720
and then to sustain them.

58:47.720 --> 58:51.520
And so given all of these factors,

58:52.400 --> 58:55.360
it's like, yeah, it's not a slam dunk

58:56.240 --> 58:57.480
to invest at the merge.

58:57.480 --> 59:00.920
And like today, like having bigger brains,

59:00.920 --> 59:02.800
for example, it's associated

59:02.800 --> 59:04.720
with like greater cognitive ability,

59:04.720 --> 59:06.320
but it's like, it's modest.

59:07.240 --> 59:09.560
Large scale pre-registered studies,

59:09.560 --> 59:12.280
pre-registered studies with MRI data.

59:12.280 --> 59:17.280
It's like a range, maybe like a correlation of 0.25, 0.3

59:18.240 --> 59:22.280
and the standard deviation of brain size is like 10%.

59:22.280 --> 59:25.760
So if you double the size of the brain,

59:25.760 --> 59:27.600
so go and the existing brain costs

59:27.600 --> 59:31.240
like 20% of metabolic energy, go up to 40%.

59:31.240 --> 59:35.040
Okay, that's like eight standard deviations of brain size.

59:35.040 --> 59:39.320
If the correlation is like, say it's 0.25,

59:40.400 --> 59:45.400
then yeah, like you get a gain from that.

59:45.800 --> 59:48.160
Eight standard deviations of brain size,

59:48.160 --> 59:50.680
two standard deviations of cognitive ability.

59:50.680 --> 59:52.720
And like in our modern society

59:52.720 --> 59:56.040
where cognitive ability is very rewarded

59:56.040 --> 01:00:00.920
and like finishing school, becoming an engineer

01:00:00.920 --> 01:00:05.280
or a doctor or whatever can pay off a lot financially.

01:00:05.280 --> 01:00:08.360
Still the like, the average observed return

01:00:09.560 --> 01:00:13.960
in like income is like a one or 2% proportional increase.

01:00:13.960 --> 01:00:15.640
There's more effects of the tail,

01:00:15.640 --> 01:00:18.520
there's more effect in professions like STEM.

01:00:18.520 --> 01:00:20.120
But on the whole, it's not like,

01:00:22.040 --> 01:00:25.880
if it was like a 5% increase or a 10% increase,

01:00:25.880 --> 01:00:28.080
then you could tell a story where,

01:00:28.080 --> 01:00:29.480
yeah, this is hugely increasing

01:00:29.560 --> 01:00:31.080
the amount of food you could have,

01:00:31.080 --> 01:00:32.520
you could support more children,

01:00:32.520 --> 01:00:34.720
but it's like, it's a modest effect

01:00:34.720 --> 01:00:36.360
and the metabolic costs will be large

01:00:36.360 --> 01:00:39.120
and then throw in these other aspects.

01:00:39.120 --> 01:00:41.040
And I think it's, you can tell the story else,

01:00:41.040 --> 01:00:45.480
we can just, we can see there was not very strong,

01:00:45.480 --> 01:00:47.920
rapid directional selection on the thing,

01:00:47.920 --> 01:00:51.560
which there would be if like, you could,

01:00:51.560 --> 01:00:56.560
by solving like a math puzzle, you could defeat malaria.

01:00:57.560 --> 01:01:00.840
Like then there would be more evolutionary pressure.

01:01:00.840 --> 01:01:01.680
That is so interesting.

01:01:01.680 --> 01:01:02.920
And not to mention, of course,

01:01:02.920 --> 01:01:05.080
that yeah, if you had like 2x the brain size

01:01:05.080 --> 01:01:06.720
or you were without C-section,

01:01:06.720 --> 01:01:09.800
you would, you or your mother would or both would die.

01:01:09.800 --> 01:01:11.040
This is a question I've actually been curious about

01:01:11.040 --> 01:01:12.320
for like over a year.

01:01:12.320 --> 01:01:14.440
And I like briefly try to look up an answer.

01:01:14.440 --> 01:01:16.840
This is, I know this is off topic,

01:01:16.840 --> 01:01:18.400
but I apologize to the audience,

01:01:18.400 --> 01:01:20.160
but I was super interested in those like,

01:01:20.160 --> 01:01:21.280
those like the most comprehensive

01:01:21.280 --> 01:01:23.040
and interesting answer I could have hoped for.

01:01:23.040 --> 01:01:25.360
Okay, so yeah, we have a good explanation

01:01:25.560 --> 01:01:27.560
for good first principles, evolutionary reason

01:01:27.560 --> 01:01:30.560
for thinking that intelligence scaling up to humans

01:01:30.560 --> 01:01:35.560
is not implausible just by throwing more scale at it.

01:01:36.280 --> 01:01:37.800
I would also add,

01:01:37.800 --> 01:01:39.560
this was something that would have mattered to me more

01:01:39.560 --> 01:01:41.280
in the 2000s.

01:01:41.280 --> 01:01:44.280
We also have the brain right here with us

01:01:44.280 --> 01:01:45.840
for available for neuroscience

01:01:45.840 --> 01:01:48.040
to reverse engineer its property.

01:01:48.040 --> 01:01:50.680
And so in the 2000s, when I said, yeah,

01:01:50.680 --> 01:01:53.320
I expect this by, you know, middle of the century ish.

01:01:53.320 --> 01:01:54.880
That was a backstop.

01:01:54.880 --> 01:01:58.280
If we found it absurdly difficult to get to the algorithms

01:01:58.280 --> 01:02:00.480
and then we would learn from neuroscience,

01:02:00.480 --> 01:02:04.480
but in the actual history, it's really not like that.

01:02:04.480 --> 01:02:06.080
We develop things in AI.

01:02:06.080 --> 01:02:07.480
And then also we can say, oh yeah,

01:02:07.480 --> 01:02:10.760
this is sort of like this thing in neuroscience

01:02:10.760 --> 01:02:12.160
or maybe this is a good explanation.

01:02:12.160 --> 01:02:14.720
But it's not as though neuroscience

01:02:14.720 --> 01:02:16.000
is driving AI progress.

01:02:16.000 --> 01:02:18.720
It turns out not to be that necessary.

01:02:18.720 --> 01:02:22.240
As similar to, I guess, you know, how planes were inspired

01:02:22.280 --> 01:02:24.200
by the existence proof of birds,

01:02:24.200 --> 01:02:27.440
but jet engines don't flap.

01:02:27.440 --> 01:02:29.560
All right, so yeah, scaling,

01:02:29.560 --> 01:02:32.040
good reason to think scaling might work.

01:02:32.040 --> 01:02:34.360
So we spend $100 billion and we have something

01:02:34.360 --> 01:02:37.760
that is like human level or can do help significantly

01:02:37.760 --> 01:02:39.200
with AI research.

01:02:39.200 --> 01:02:41.960
I mean, that might be on the earlier end,

01:02:41.960 --> 01:02:45.080
but I mean, I definitely would not rule that out

01:02:45.080 --> 01:02:47.400
given the rates of change we've seen

01:02:47.400 --> 01:02:49.240
with the last few scale-ups.

01:02:49.240 --> 01:02:53.520
All right, so at this point, somebody might be skeptical.

01:02:53.520 --> 01:02:54.360
Okay, like listen,

01:02:54.360 --> 01:02:56.040
we already have a bunch of human researchers, right?

01:02:56.040 --> 01:02:58.720
Like the incremental researcher, how profitable is that?

01:02:58.720 --> 01:02:59.640
And then you might say, well, no,

01:02:59.640 --> 01:03:01.480
this is like thousands of researchers.

01:03:01.480 --> 01:03:03.920
I don't know how to express a skepticism exactly,

01:03:03.920 --> 01:03:06.200
but skepticism is skeptical of just generally

01:03:06.200 --> 01:03:09.000
the effect of scaling up the number of people

01:03:09.000 --> 01:03:11.480
working on the problem to rapid,

01:03:11.480 --> 01:03:13.920
rapid progress on that problem.

01:03:13.920 --> 01:03:15.200
Somebody might think, okay, listen,

01:03:15.200 --> 01:03:18.040
with humans, the reason population working on a problem

01:03:18.040 --> 01:03:20.400
is such a good proxy for progress on the problem

01:03:20.400 --> 01:03:22.360
is that there's already so much variation

01:03:22.360 --> 01:03:23.400
that is accounted for when you say

01:03:23.400 --> 01:03:25.240
there's like a million people working on a problem.

01:03:25.240 --> 01:03:28.040
You know, there's like hundreds of super geniuses

01:03:28.040 --> 01:03:29.320
working on it, thousands of people

01:03:29.320 --> 01:03:30.920
who are like very smart working on it.

01:03:30.920 --> 01:03:32.880
Whereas with an AI, all the copies

01:03:32.880 --> 01:03:35.280
are like the same level of intelligence.

01:03:35.280 --> 01:03:37.920
And if it's not super genius intelligence,

01:03:39.240 --> 01:03:43.840
the total quantity might not matter as much.

01:03:43.840 --> 01:03:48.320
Yeah, I'm not sure what your model is here.

01:03:48.320 --> 01:03:53.320
So is this a model that the diminishing returns kick off

01:03:54.840 --> 01:03:57.400
suddenly has a cliff right where we are?

01:03:57.400 --> 01:04:00.960
And so like there was, there were results in the past

01:04:00.960 --> 01:04:04.760
from throwing more people at problems.

01:04:04.760 --> 01:04:08.960
And I mean, this has been useful in historical prediction.

01:04:08.960 --> 01:04:12.400
One of the, there's this idea of experience curves

01:04:12.400 --> 01:04:17.400
and Wright's law basically measuring cumulative production

01:04:17.680 --> 01:04:20.560
in a field or which is that also gonna be a measure

01:04:20.560 --> 01:04:23.240
of like the scale of effort and investment.

01:04:23.240 --> 01:04:26.440
And people have used this correctly to argue

01:04:26.440 --> 01:04:29.640
that renewable energy technology like solar

01:04:29.640 --> 01:04:32.160
would be falling rapidly in price

01:04:32.160 --> 01:04:34.360
because it was going from a low base

01:04:34.360 --> 01:04:36.480
of very small production runs,

01:04:36.480 --> 01:04:39.220
not much investment in doing it efficiently.

01:04:40.060 --> 01:04:44.980
And yeah, climate advocates correctly called out

01:04:44.980 --> 01:04:48.380
people and people like David Roberts,

01:04:48.380 --> 01:04:53.060
the futurist Rama is now actually has some interesting

01:04:53.060 --> 01:04:55.980
writing on this that yeah, correctly called out

01:04:55.980 --> 01:04:59.660
that there would be really drastic fall in prices

01:04:59.660 --> 01:05:01.380
of solar and batteries

01:05:01.380 --> 01:05:04.460
because of the increasing investment going into that.

01:05:04.460 --> 01:05:05.820
The human genome project would be another.

01:05:05.820 --> 01:05:08.960
So I'd say there's like, yeah, real, real evidence.

01:05:08.960 --> 01:05:12.600
These observed correlations from like ideas,

01:05:12.600 --> 01:05:17.600
getting harder to find have held over a fair range of data

01:05:17.720 --> 01:05:20.120
and over quite a lot of time.

01:05:20.120 --> 01:05:25.120
So I'm wondering what's the nature of the deviation

01:05:25.480 --> 01:05:26.320
you're thinking of?

01:05:26.320 --> 01:05:30.720
That we're talking about, maybe this is like a good way

01:05:30.720 --> 01:05:33.660
to describe what happens when more humans enter a field.

01:05:33.660 --> 01:05:35.720
But does it even make sense to say

01:05:35.720 --> 01:05:38.600
like a greater population of AIs is doing AI research

01:05:38.640 --> 01:05:42.560
if there's like more GPUs running a copy of GPT-6

01:05:42.560 --> 01:05:44.120
doing AI research?

01:05:44.120 --> 01:05:47.640
It just like how applicable are these economic models

01:05:47.640 --> 01:05:49.960
of human, the quantity of humans working on a problem

01:05:49.960 --> 01:05:53.480
to the magnitude of AIs working on a problem?

01:05:53.480 --> 01:05:57.120
Yeah, so if you have AIs that are directly automating

01:05:58.280 --> 01:06:01.320
particular jobs that humans were doing before,

01:06:01.320 --> 01:06:03.280
then we say, well, with additional compute

01:06:03.280 --> 01:06:06.080
we can run more copies of them

01:06:06.080 --> 01:06:09.280
to do more of those tasks simultaneously.

01:06:09.280 --> 01:06:11.520
We can also run them at greater speed.

01:06:11.520 --> 01:06:13.560
And so some people have an intuition that like,

01:06:13.560 --> 01:06:16.760
well, you know, what matters is like time.

01:06:16.760 --> 01:06:19.120
It's not how many people working on problem

01:06:19.120 --> 01:06:20.360
at a given point.

01:06:20.360 --> 01:06:23.360
I think that doesn't bear out super well,

01:06:23.360 --> 01:06:26.600
but AI can also be run faster than humans.

01:06:26.600 --> 01:06:31.600
And so if you have a set of AIs that can do the work

01:06:32.240 --> 01:06:35.400
of the individual human researchers

01:06:35.400 --> 01:06:39.040
and run at 10 times or 100 times the speed,

01:06:39.040 --> 01:06:41.800
then we ask, well, could the human research community

01:06:41.800 --> 01:06:43.960
have solved the algorithm problems,

01:06:43.960 --> 01:06:48.960
do things like invent transformers over 100 years

01:06:49.200 --> 01:06:50.520
if we have this?

01:06:50.520 --> 01:06:52.800
We have AIs with a population,

01:06:52.800 --> 01:06:55.080
effective population similar to the humans,

01:06:55.080 --> 01:06:57.320
but running 100 times as fast.

01:06:57.320 --> 01:07:00.640
And so you have to tell a story where no,

01:07:00.640 --> 01:07:04.120
the AI, they can't really do the same things

01:07:04.160 --> 01:07:05.800
as the humans.

01:07:05.800 --> 01:07:07.880
And we're talking about what happens

01:07:07.880 --> 01:07:12.000
when the AIs are more capable of in fact doing that.

01:07:12.000 --> 01:07:13.200
Although they become more capable

01:07:13.200 --> 01:07:16.400
as lesser capable versions of themselves help us,

01:07:16.400 --> 01:07:17.640
make themselves more capable, right?

01:07:17.640 --> 01:07:20.520
So you have to like kickstart that at some point.

01:07:20.520 --> 01:07:25.480
Is there an example in analogous situations,

01:07:25.480 --> 01:07:28.080
is intelligence unique in the sense that you have

01:07:28.080 --> 01:07:31.520
a feedback loop of with a learning curve

01:07:31.520 --> 01:07:35.320
or something else, a system outputs,

01:07:35.320 --> 01:07:38.160
are feeding into its own inputs in a way that,

01:07:38.160 --> 01:07:40.000
because if we're talking about something like Moore's Law

01:07:40.000 --> 01:07:42.680
or the cost of solar, you do have this way,

01:07:42.680 --> 01:07:44.640
like we're, you know, more people are,

01:07:44.640 --> 01:07:45.720
we're throwing more people with the problem

01:07:45.720 --> 01:07:48.920
and it's, we're, you know, we're making a lot of progress,

01:07:48.920 --> 01:07:53.040
but we don't have the sort of additional part of the model

01:07:53.040 --> 01:07:56.200
where Moore's Law leads to more humans somehow

01:07:56.200 --> 01:07:58.480
and the more humans are becoming researchers.

01:07:58.480 --> 01:08:00.680
So you do actually have a version of that

01:08:00.720 --> 01:08:02.200
in the case of solar.

01:08:02.200 --> 01:08:05.280
So you have a small infant industry

01:08:05.280 --> 01:08:07.560
that's doing things like providing solar panels

01:08:07.560 --> 01:08:11.080
for space satellites and then getting increasing amounts

01:08:11.080 --> 01:08:15.120
of subsidized government demand because of, you know,

01:08:15.120 --> 01:08:16.720
worries about fossil fuel depletion

01:08:16.720 --> 01:08:18.440
and then climate change.

01:08:18.440 --> 01:08:22.760
You can have the dynamic where visible successes

01:08:22.760 --> 01:08:24.880
with solar or like lowering prices

01:08:24.880 --> 01:08:26.760
then open up new markets.

01:08:26.760 --> 01:08:29.520
So there's a particularly huge transition

01:08:29.520 --> 01:08:31.560
where renewables become cheap enough

01:08:31.560 --> 01:08:34.800
to replace large chunks of the electric grid.

01:08:34.800 --> 01:08:38.080
Earlier, you're dealing with very niche situations like,

01:08:38.080 --> 01:08:41.480
yeah, so the satellites where you have very difficult

01:08:41.480 --> 01:08:45.200
to refuel a satellite in place and then remote areas

01:08:45.200 --> 01:08:47.160
and then moving to like, you know,

01:08:47.160 --> 01:08:50.000
the super sunny, the sunniest areas in the world

01:08:50.000 --> 01:08:52.520
with the biggest solar subsidies.

01:08:52.520 --> 01:08:54.440
And so there was an element of that

01:08:54.440 --> 01:08:57.560
where more and more investment has been thrown

01:08:57.600 --> 01:09:00.600
into the field and like the market has rapidly expanded

01:09:00.600 --> 01:09:02.080
as the technology improved.

01:09:02.080 --> 01:09:05.520
But I think the closest analogy is actually

01:09:05.520 --> 01:09:08.120
the long run growth of human civilization itself.

01:09:08.120 --> 01:09:11.000
And I know you had Holden Karnofsky

01:09:11.000 --> 01:09:13.960
from the open philanthropy project on earlier

01:09:13.960 --> 01:09:16.040
and discuss some of this research

01:09:16.040 --> 01:09:20.640
about the long run acceleration of human population

01:09:20.640 --> 01:09:21.600
and economic growth.

01:09:21.600 --> 01:09:24.280
And so developing new technologies

01:09:24.280 --> 01:09:27.080
allowed human population to expand,

01:09:27.120 --> 01:09:31.600
humans to occupy new habitats and new areas

01:09:31.600 --> 01:09:32.960
and then to invent agriculture

01:09:32.960 --> 01:09:35.040
which support the larger populations

01:09:35.040 --> 01:09:36.520
and then even more advanced agriculture

01:09:36.520 --> 01:09:38.720
in the modern industrial society.

01:09:38.720 --> 01:09:42.200
And so their total technology and output

01:09:42.200 --> 01:09:45.200
allowed you to support more humans

01:09:45.200 --> 01:09:48.000
who then would discover more technology

01:09:48.000 --> 01:09:49.080
and continue the process.

01:09:49.080 --> 01:09:52.880
Now that was boosted because on top of expanding

01:09:52.880 --> 01:09:56.120
the population, the share of human activity

01:09:56.120 --> 01:09:59.280
that was going into invention and innovation went up.

01:09:59.280 --> 01:10:01.320
And that was a key part of the industrial revolution.

01:10:01.320 --> 01:10:04.520
There was no such thing as a corporate research lab

01:10:04.520 --> 01:10:08.640
or like an engineering university prior to that.

01:10:08.640 --> 01:10:11.600
And so you're both increasing the total human population

01:10:11.600 --> 01:10:13.280
and the share of it going in.

01:10:13.280 --> 01:10:16.440
But this population dynamic is pretty analogous.

01:10:16.440 --> 01:10:19.000
Humans invent farming, they can have more humans

01:10:19.000 --> 01:10:21.920
than they can invent industry and so on.

01:10:21.920 --> 01:10:23.200
So maybe somebody would be skeptical

01:10:23.200 --> 01:10:25.920
that with AI progress specifically,

01:10:25.960 --> 01:10:30.600
it's not just a matter of some farmer

01:10:30.600 --> 01:10:33.480
figuring out crop rotation or some blacksmith

01:10:33.480 --> 01:10:35.760
figuring out how to do metal or do better.

01:10:35.760 --> 01:10:38.120
You in fact, even to make the,

01:10:38.120 --> 01:10:40.360
for the 50% improvement in productivity,

01:10:40.360 --> 01:10:42.520
you basically need something on the IQ

01:10:42.520 --> 01:10:44.360
that's close to Ilya Setskoper.

01:10:44.360 --> 01:10:46.800
There's like a discontinuous,

01:10:46.800 --> 01:10:48.840
you're like contributing very little to productivity

01:10:48.840 --> 01:10:51.520
and then you're like Ilya and then you contribute a lot.

01:10:51.520 --> 01:10:54.840
But the becoming Ilya is, you see what I'm saying?

01:10:54.840 --> 01:10:56.880
There's not like a gradual increase in capabilities

01:10:56.880 --> 01:10:57.720
that leads to the feedback.

01:10:57.720 --> 01:11:02.240
You're imagining a case where the distribution of tasks

01:11:02.240 --> 01:11:04.960
is such that there's nothing that you can,

01:11:04.960 --> 01:11:08.800
where individually automating it particularly helps.

01:11:08.800 --> 01:11:12.280
And so the ability to contribute to AI research

01:11:12.280 --> 01:11:13.360
is really end loaded.

01:11:13.360 --> 01:11:14.200
Is that what you're saying?

01:11:14.200 --> 01:11:17.360
Yeah, I mean, we already see this in these sorts

01:11:17.360 --> 01:11:20.960
of like really high IQ companies or projects

01:11:20.960 --> 01:11:22.600
where theoretically, I guess,

01:11:22.600 --> 01:11:25.520
Shane Street or OpenAI could hire like a bunch of,

01:11:26.480 --> 01:11:29.480
mediocre people to do, there's a comparative advantage.

01:11:29.480 --> 01:11:31.040
They could do some menial tasks

01:11:31.040 --> 01:11:33.880
and that could free up the time of the really smart people,

01:11:33.880 --> 01:11:35.960
but they don't do that, right?

01:11:35.960 --> 01:11:37.400
Transaction costs, whatever else.

01:11:37.400 --> 01:11:39.680
Self-driven cars would be another example

01:11:39.680 --> 01:11:42.400
where you have a very high quality threshold.

01:11:42.400 --> 01:11:45.160
And so when your performance as a driver

01:11:45.160 --> 01:11:46.840
is worse than a human,

01:11:46.840 --> 01:11:48.960
like you have 10 times the accident rate

01:11:48.960 --> 01:11:50.760
or 100 times the accident rate,

01:11:50.760 --> 01:11:52.920
then the cost of insurance for that,

01:11:52.920 --> 01:11:54.680
which is a proxy for people's willingness

01:11:54.680 --> 01:11:56.760
to ride the car instead of two,

01:11:56.760 --> 01:11:58.440
would be such that the insurance costs

01:11:58.440 --> 01:11:59.360
would absolutely dominate.

01:11:59.360 --> 01:12:00.800
So even if you have zero labor cost,

01:12:00.800 --> 01:12:03.120
it's offset by the increased insurance cost.

01:12:03.120 --> 01:12:04.800
And so there are lots of cases like that

01:12:04.800 --> 01:12:09.800
where like partial automation is not in practice

01:12:10.480 --> 01:12:14.760
very usable because complimenting other resources,

01:12:14.760 --> 01:12:17.720
you're gonna use those other resources less efficiently.

01:12:18.560 --> 01:12:22.280
And in a post-AGI future,

01:12:22.280 --> 01:12:24.600
I mean, the same thing can apply to humans.

01:12:24.600 --> 01:12:28.560
So people can say, well, comparative advantage,

01:12:28.560 --> 01:12:31.920
even if AIs can do everything better than a human,

01:12:31.920 --> 01:12:34.280
well, it's still worth something.

01:12:34.280 --> 01:12:35.360
The human can do something,

01:12:35.360 --> 01:12:38.600
they can lift a box, that's something.

01:12:39.520 --> 01:12:41.200
Now there's a question of property rights,

01:12:41.200 --> 01:12:44.720
if, well, if they could just slice up the human

01:12:44.720 --> 01:12:46.680
to make more robots.

01:12:46.680 --> 01:12:49.880
But even absent that in such an economy,

01:12:49.880 --> 01:12:52.160
you wouldn't want to let a human worker

01:12:52.160 --> 01:12:53.720
into any industrial environment,

01:12:53.720 --> 01:12:55.160
because in a clean room,

01:12:55.160 --> 01:12:57.480
they'll be emitting all kinds of skin cells

01:12:57.480 --> 01:12:59.260
and messing things up.

01:12:59.260 --> 01:13:00.920
You need to have an atmosphere there.

01:13:00.920 --> 01:13:03.240
You need a bunch of supporting tools

01:13:03.240 --> 01:13:04.680
and resources and materials.

01:13:04.680 --> 01:13:07.540
And those supporting resources and materials

01:13:07.540 --> 01:13:10.320
will do a lot more productively,

01:13:10.320 --> 01:13:12.640
working with AI and robots rather than a human.

01:13:12.640 --> 01:13:15.680
So you don't wanna let a human anywhere near the thing,

01:13:15.680 --> 01:13:18.520
just like in a, you don't wanna have a gorilla

01:13:18.520 --> 01:13:19.880
wandering around in a China shop.

01:13:19.880 --> 01:13:22.600
Even if you've trained it to most of the time,

01:13:22.600 --> 01:13:24.800
pick up a box for you if you give it a banana,

01:13:24.800 --> 01:13:26.640
it's just not worth it to have it wandering

01:13:26.640 --> 01:13:27.480
around your China shop.

01:13:27.480 --> 01:13:28.320
Yeah, yeah, yeah.

01:13:28.320 --> 01:13:30.560
Like why is that not a good objection to?

01:13:30.560 --> 01:13:34.040
I mean, I think that is one of the ways

01:13:34.040 --> 01:13:37.760
in which partial automation can fail

01:13:37.760 --> 01:13:41.000
to really translate into a lot of economic value.

01:13:41.000 --> 01:13:43.720
That's something that will attenuate as we go on.

01:13:43.800 --> 01:13:47.000
And as the AI is more able to work independently

01:13:47.000 --> 01:13:50.200
and more able to handle its own,

01:13:50.200 --> 01:13:53.320
its own screw ups, get more reliable.

01:13:53.320 --> 01:13:55.440
But the way in which it becomes more reliable

01:13:55.440 --> 01:13:58.440
is by AI progress speeding up,

01:13:58.440 --> 01:14:00.960
which happens if AI can contribute to it.

01:14:00.960 --> 01:14:04.880
But if there is some sort of reliability bottleneck,

01:14:04.880 --> 01:14:06.520
the principle of contributing to that progress,

01:14:06.520 --> 01:14:07.600
then you don't have the loop, right?

01:14:07.600 --> 01:14:10.960
So, yeah, I mean, this is why we're not there yet.

01:14:10.960 --> 01:14:13.080
Right, but then what is the reason to think we'll be there at?

01:14:13.080 --> 01:14:16.800
The broad reason is we have these inputs are scaling up.

01:14:18.880 --> 01:14:21.560
There's a, so Epoch, which I mentioned earlier,

01:14:21.560 --> 01:14:24.040
they have a paper, I think it's called compute trends

01:14:24.040 --> 01:14:27.480
in three areas of machine learning or something like that.

01:14:27.480 --> 01:14:31.800
And so they look at the compute expended

01:14:31.800 --> 01:14:33.920
on machine learning systems

01:14:33.920 --> 01:14:36.040
since the founding of the field of AI

01:14:36.040 --> 01:14:38.480
at the beginning of the 1950s.

01:14:38.480 --> 01:14:41.120
And so it mostly, it grows with Moore's law.

01:14:42.120 --> 01:14:44.920
And so people are spending a similar amount

01:14:44.920 --> 01:14:49.080
on their experiments, but they can just buy Moore with that

01:14:49.080 --> 01:14:51.400
because the compute is coming.

01:14:51.400 --> 01:14:55.640
And so that data, I mean, it covers over 20 orders

01:14:55.640 --> 01:14:57.320
of magnitude, maybe like 24.

01:14:58.960 --> 01:15:03.920
And of all of those increases since 1952,

01:15:03.920 --> 01:15:08.920
a little more than half of them happened between 1952 and 2010.

01:15:09.080 --> 01:15:12.200
And all the rest is since 2010.

01:15:12.200 --> 01:15:16.520
So we've been scaling that up like four times as fast

01:15:16.520 --> 01:15:20.120
as was the case for most of the history of AI.

01:15:20.120 --> 01:15:23.640
We're running through the orders of magnitude

01:15:23.640 --> 01:15:27.280
of possible resource inputs you could need for AI

01:15:27.280 --> 01:15:29.720
much, much more quickly than we were

01:15:29.720 --> 01:15:30.840
for most of the history of AI.

01:15:30.840 --> 01:15:33.000
That's why this is a period of like

01:15:33.000 --> 01:15:36.760
with a very elevated chance of AI per year

01:15:36.760 --> 01:15:38.720
because we're moving through.

01:15:38.720 --> 01:15:40.680
So much of the space of inputs per year.

01:15:40.680 --> 01:15:43.640
And indeed, it looks like this scale up

01:15:43.640 --> 01:15:48.280
taken to its conclusion will cover another bunch

01:15:48.280 --> 01:15:50.240
of orders of magnitude.

01:15:50.240 --> 01:15:53.720
And that's actually a large fraction of those that are left

01:15:53.720 --> 01:15:55.680
before you start running into saying, well,

01:15:55.680 --> 01:15:58.800
this is gonna have to be like evolution

01:15:58.800 --> 01:16:01.360
with the sort of simple hacks we get to apply.

01:16:01.360 --> 01:16:04.200
Like we're selecting for intelligence the whole time.

01:16:04.200 --> 01:16:06.600
We're not going to do the same mutation

01:16:06.600 --> 01:16:10.680
that causes fatal childhood cancer a billion times.

01:16:10.680 --> 01:16:14.240
Even though, I mean, we keep getting the same fatal mutations

01:16:14.240 --> 01:16:15.880
even though they've been done many times.

01:16:15.880 --> 01:16:18.840
We use gradient descent, which takes into account

01:16:18.840 --> 01:16:21.880
the derivative of improvement on the loss

01:16:21.880 --> 01:16:23.120
all throughout the network.

01:16:23.120 --> 01:16:26.520
And we don't throw away all the contents of the network

01:16:26.520 --> 01:16:29.240
with each generation where you can press down

01:16:29.240 --> 01:16:30.640
to a little DNA.

01:16:30.640 --> 01:16:33.200
So there's that bar of like, well,

01:16:33.200 --> 01:16:35.440
if you're gonna do brute force like evolution

01:16:35.440 --> 01:16:37.680
combine with these sort of very simple ways

01:16:37.680 --> 01:16:39.600
we can save orders of magnitude on that.

01:16:41.200 --> 01:16:43.840
We're gonna cover, I think, a fraction

01:16:43.840 --> 01:16:47.520
that's like half of that distance in this scale

01:16:47.520 --> 01:16:49.240
up over the next 10 years or so.

01:16:49.240 --> 01:16:53.240
And so if you started off with a kind of vague uniform prior,

01:16:53.240 --> 01:16:56.800
you're like, well, you probably can't make AGI

01:16:56.800 --> 01:16:59.520
with like the amount of compute that would be involved

01:16:59.520 --> 01:17:02.080
in a fruit fly existing for a minute,

01:17:02.080 --> 01:17:04.000
which would be the early days of AI.

01:17:04.960 --> 01:17:06.440
You know, maybe you would get lucky.

01:17:06.440 --> 01:17:07.960
We were able to make calculators

01:17:07.960 --> 01:17:10.040
because calculators benefited

01:17:10.040 --> 01:17:14.080
from like very reliable, serially fast computers

01:17:14.080 --> 01:17:17.600
and where we could take a tiny, tiny, tiny, tiny fraction

01:17:17.600 --> 01:17:20.360
of a human brain's compute and use it for a calculator.

01:17:20.360 --> 01:17:22.920
We couldn't take an ant's brain and rewire it to calculate.

01:17:22.920 --> 01:17:27.600
It's hard to manage ant farms, let alone get them

01:17:27.600 --> 01:17:29.320
to do arithmetic for you.

01:17:29.320 --> 01:17:32.160
And so there were some things where we could exploit

01:17:32.160 --> 01:17:37.160
the differences between biological brains and computers

01:17:37.160 --> 01:17:40.040
to do stuff super efficiently on computers.

01:17:40.040 --> 01:17:42.400
We would doubt that we would be able

01:17:42.400 --> 01:17:45.280
to do so much better than biology,

01:17:45.280 --> 01:17:48.560
that with a tiny fraction of an insect's brain,

01:17:48.560 --> 01:17:50.400
we'd be able to get AI early on.

01:17:50.400 --> 01:17:53.520
On the far end, it seemed very implausible

01:17:53.520 --> 01:17:54.440
that we couldn't do better

01:17:54.440 --> 01:17:56.560
than completely brute force evolution.

01:17:56.560 --> 01:17:58.320
And so in between, you have some number

01:17:58.360 --> 01:18:02.520
of orders of magnitude of inputs where it might be.

01:18:02.520 --> 01:18:03.920
And like in the 2000s, I would say,

01:18:03.920 --> 01:18:07.120
well, you know, I'm gonna have a pretty uniformish prior.

01:18:07.120 --> 01:18:10.080
I'm gonna put weight on it happening at like the sort

01:18:10.080 --> 01:18:14.040
of the equivalent of like 10 to the 25 ops,

01:18:14.040 --> 01:18:16.480
10 to the 30, 10 to the 35,

01:18:17.520 --> 01:18:19.280
and sort of spreading out over that.

01:18:19.280 --> 01:18:21.320
And then I can update on other information.

01:18:21.320 --> 01:18:25.080
And in the short term, I would say like in 2005, I would say,

01:18:25.080 --> 01:18:27.320
well, I don't see anything that looks

01:18:27.320 --> 01:18:29.000
like the cusp of AGI.

01:18:29.000 --> 01:18:31.040
So I'm also gonna lower my credence

01:18:31.040 --> 01:18:34.000
for like the next five years or the next 10 years.

01:18:34.000 --> 01:18:36.640
And so that would be kind of like a vague prior.

01:18:36.640 --> 01:18:38.720
And then when we take into account like, well,

01:18:38.720 --> 01:18:40.120
how quickly are we running through

01:18:40.120 --> 01:18:41.320
those orders of magnitude?

01:18:41.320 --> 01:18:45.520
If I have a uniform prior, I assign half of my weight

01:18:45.520 --> 01:18:48.400
to the first half of remaining orders of magnitude.

01:18:48.400 --> 01:18:50.200
And if we're gonna run through those

01:18:50.200 --> 01:18:51.840
over the next 10 years in some,

01:18:52.840 --> 01:18:56.640
then that calls on me to put half of my credence

01:18:56.640 --> 01:18:58.160
conditional on wherever we're gonna make it.

01:18:58.160 --> 01:19:00.840
AI, which seems likely it's a material object

01:19:00.840 --> 01:19:02.440
easier than evolution.

01:19:02.440 --> 01:19:05.400
I've got to put similarly a lot of my credence

01:19:05.400 --> 01:19:07.320
on AI happening in this scale up.

01:19:07.320 --> 01:19:09.680
And then that's supported by what we're seeing

01:19:09.680 --> 01:19:14.040
in terms of the rapid advances in capabilities

01:19:14.040 --> 01:19:16.280
with AI and LLOMs in particular.

01:19:16.280 --> 01:19:18.480
Okay, that's actually a really interesting point.

01:19:18.480 --> 01:19:20.560
So now that somebody might say, listen,

01:19:20.560 --> 01:19:23.920
there's not some sense in which AIs could universally

01:19:24.000 --> 01:19:27.280
speed up the progress of open AI by 50%

01:19:27.280 --> 01:19:29.480
or 100% or 200%.

01:19:29.480 --> 01:19:32.280
If they're not able to do everything better

01:19:32.280 --> 01:19:34.240
than Ilias Escobar can,

01:19:34.240 --> 01:19:37.040
there's going to be something in which we're bottlenecked

01:19:37.040 --> 01:19:38.840
by the human researchers.

01:19:38.840 --> 01:19:42.080
And bottleneck effects dictate that,

01:19:42.080 --> 01:19:43.600
the slowest moving part of the organization

01:19:43.600 --> 01:19:45.880
will be the one that kind of determines the speed

01:19:45.880 --> 01:19:47.400
of the progress of the whole organization

01:19:47.400 --> 01:19:48.680
or the whole project.

01:19:48.680 --> 01:19:50.360
Which means that unless you get to the point

01:19:50.360 --> 01:19:51.560
where you're like doing everything

01:19:51.560 --> 01:19:53.520
and everybody in the organization can do,

01:19:53.520 --> 01:19:55.480
you're not going to significantly speed up

01:19:55.480 --> 01:19:57.640
the progress of the whole project as a whole.

01:19:57.640 --> 01:20:00.320
Yeah, so that is a hypothesis.

01:20:00.320 --> 01:20:02.760
And I think there's a lot of truth to it.

01:20:02.760 --> 01:20:04.200
So when we think about like the ways

01:20:04.200 --> 01:20:05.960
in which AI can contribute.

01:20:05.960 --> 01:20:07.520
So there are things we talked about before,

01:20:07.520 --> 01:20:10.480
like the AIs setting up their own curriculum.

01:20:11.320 --> 01:20:14.320
And that's something that Ilya can't do directly

01:20:14.320 --> 01:20:15.680
doesn't do directly.

01:20:15.680 --> 01:20:16.680
And there's a question,

01:20:16.680 --> 01:20:19.800
how much does that improve performance?

01:20:19.800 --> 01:20:24.280
There are these things where the AI helps

01:20:24.280 --> 01:20:27.360
to just like produce some code for some tasks.

01:20:27.360 --> 01:20:29.920
And it's beyond Hello World at this point.

01:20:29.920 --> 01:20:32.160
But I mean, the sort of thing that I hear

01:20:32.160 --> 01:20:36.160
from AI researchers at leading labs is that,

01:20:36.160 --> 01:20:39.680
on their core job where they're like most expert,

01:20:39.680 --> 01:20:41.280
it's not helping them that much,

01:20:41.280 --> 01:20:43.680
but then their job often does involve,

01:20:43.680 --> 01:20:45.600
oh, I've got to code something

01:20:45.600 --> 01:20:49.040
that's out of my usual area of expertise.

01:20:49.040 --> 01:20:52.040
Or I want to research this question and it helps them there.

01:20:52.040 --> 01:20:54.120
And so that saves some of their time

01:20:54.120 --> 01:20:57.600
and frees them to do more of the bottleneck work.

01:20:57.600 --> 01:21:00.840
And then I think the idea of, well,

01:21:02.320 --> 01:21:05.800
is everything dependent on Ilya and is Ilya

01:21:05.800 --> 01:21:10.280
so much better than the hundreds of other employees?

01:21:10.280 --> 01:21:12.440
I think a lot of people who are contributing,

01:21:12.440 --> 01:21:14.760
they're doing a lot of tasks.

01:21:14.760 --> 01:21:19.280
And so you can have quite a lot of gain

01:21:19.280 --> 01:21:21.400
from automating some areas

01:21:21.400 --> 01:21:24.600
where you then do just an absolutely enormous amount of it

01:21:24.600 --> 01:21:26.800
relative to what you would have done before

01:21:26.800 --> 01:21:29.720
because things like designing the custom curriculum,

01:21:29.720 --> 01:21:32.920
you're like, maybe had some humans put some work into that,

01:21:32.920 --> 01:21:35.440
but you're not going to employ billions of humans

01:21:35.440 --> 01:21:36.760
to produce it at scale.

01:21:36.760 --> 01:21:40.120
And so it winds up being a larger share of the progress

01:21:41.160 --> 01:21:42.160
than it was before.

01:21:42.160 --> 01:21:45.160
You get some benefit from these sorts of things

01:21:45.160 --> 01:21:48.760
where, yeah, there's like pieces of my job

01:21:49.800 --> 01:21:51.800
that now I can hand off to the AI

01:21:51.800 --> 01:21:54.320
and let's me focus more on the things

01:21:54.320 --> 01:21:55.920
that the AI still can't do.

01:21:57.120 --> 01:22:01.720
And then at the later on, you get to the point where,

01:22:01.720 --> 01:22:04.240
yeah, the AI can do your job,

01:22:04.240 --> 01:22:05.720
including the most difficult parts.

01:22:05.720 --> 01:22:08.640
And maybe it has to do that in a different way.

01:22:08.640 --> 01:22:12.480
Maybe it like spends a ton more time thinking about

01:22:12.480 --> 01:22:15.720
each step of a problem than you, and that's the late end.

01:22:15.720 --> 01:22:18.440
And the stronger these bottlenecks effects are,

01:22:18.440 --> 01:22:22.920
the more the economic returns, the scientific returns

01:22:22.920 --> 01:22:27.000
and such are end loaded towards getting sort of full AGI.

01:22:27.000 --> 01:22:28.680
The weaker the bottlenecks are,

01:22:28.680 --> 01:22:32.600
the more interim results will be really paying off.

01:22:32.600 --> 01:22:34.440
I guess I'd probably disagree with you on how much

01:22:34.440 --> 01:22:38.160
the sort of the alias of organizations seem to matter.

01:22:38.160 --> 01:22:39.600
I guess just from the evidence alone,

01:22:39.600 --> 01:22:42.440
like how many of the big sort of breakthroughs

01:22:42.440 --> 01:22:45.360
that in deep learning in general was like

01:22:45.360 --> 01:22:47.760
that single individual responsible for, right?

01:22:47.760 --> 01:22:49.880
And how much of his time is he spending

01:22:49.880 --> 01:22:51.640
doing anything that's not that like co-pilot

01:22:51.640 --> 01:22:52.480
is helping him on?

01:22:52.480 --> 01:22:54.120
I'm guessing like most of it is just like managing people

01:22:54.120 --> 01:22:57.280
and coming up with ideas and, you know,

01:22:57.280 --> 01:22:59.440
trying to like understand systems and so on.

01:22:59.440 --> 01:23:02.840
And if that is the, if like the five or 10 people

01:23:02.840 --> 01:23:06.120
who are like that at OpenAI or Anthropa or whatever

01:23:06.120 --> 01:23:11.080
are basically the way in which the progress is happening

01:23:11.080 --> 01:23:13.440
or at least the algorithmic progress is happening,

01:23:13.440 --> 01:23:17.960
then how much of better and better co-pilot,

01:23:17.960 --> 01:23:19.960
I know co-pilot is not the thing you're talking about

01:23:19.960 --> 01:23:22.280
with like the 20% automation, but something like that,

01:23:22.280 --> 01:23:25.400
how much of, yeah, how much is that contributing

01:23:25.400 --> 01:23:29.600
to the sort of like core function of the research scientist?

01:23:29.600 --> 01:23:30.440
Yeah.

01:23:30.440 --> 01:23:33.800
Naturally quantitatively, how much we disagree

01:23:33.800 --> 01:23:38.800
about the importance of sort of key research employees

01:23:40.160 --> 01:23:44.320
and such, I certainly think that some researchers,

01:23:44.320 --> 01:23:47.680
you know, add, you know, more than 10 times

01:23:47.680 --> 01:23:50.280
the average employee even much more.

01:23:50.280 --> 01:23:54.000
And obviously managers can add an enormous amount of value

01:23:54.000 --> 01:23:56.320
by proportionately multiplying the output

01:23:56.320 --> 01:23:58.720
of the many people that they manage.

01:23:59.680 --> 01:24:02.480
And so that's the kind of thing

01:24:02.480 --> 01:24:05.120
that we were discussing earlier when talking about,

01:24:05.120 --> 01:24:09.840
well, if you had sort of full human level AI

01:24:09.840 --> 01:24:12.880
or AI that had all of the human capabilities

01:24:12.880 --> 01:24:16.600
plus AI advantages, it would be, you know,

01:24:16.600 --> 01:24:19.160
you'd benchmark not off of what the sort of typical

01:24:19.160 --> 01:24:22.800
human performance is, but peak human performance and beyond.

01:24:22.800 --> 01:24:24.980
So yeah, I accept all that.

01:24:26.120 --> 01:24:31.080
I do think it makes a big difference for people

01:24:31.240 --> 01:24:34.400
how much they can outsource a lot of the death

01:24:34.400 --> 01:24:36.480
that are less, wow, less creative.

01:24:36.480 --> 01:24:39.960
And an enormous amount is learned by experimentation.

01:24:39.960 --> 01:24:44.520
ML has been, you know, a quite experimental field.

01:24:44.520 --> 01:24:46.280
And there's a lot of engineering work

01:24:46.280 --> 01:24:49.560
in say building large super clusters,

01:24:49.560 --> 01:24:53.680
making, yeah, hardware aware optimization

01:24:53.680 --> 01:24:55.120
and coding of these things,

01:24:55.120 --> 01:24:59.000
being able to do the parallelism in large models.

01:24:59.000 --> 01:25:03.360
And the engineers are busy

01:25:03.360 --> 01:25:07.640
and it's not just only a big thoughts kind of area.

01:25:07.640 --> 01:25:12.640
And then the other branch is where will the AI advantages

01:25:15.360 --> 01:25:16.960
and disadvantages be?

01:25:16.960 --> 01:25:21.960
And so one AI advantage is being omnidisciplinary

01:25:22.920 --> 01:25:25.440
and familiar with the newest things.

01:25:25.440 --> 01:25:28.040
So I mentioned before, there's no human

01:25:28.040 --> 01:25:31.680
who has a million years of TensorFlow experience.

01:25:31.680 --> 01:25:34.060
And so to the extent that we're interested

01:25:34.060 --> 01:25:36.160
in like the very, very cutting edge

01:25:36.160 --> 01:25:38.800
of things that have been developed quite recently

01:25:38.800 --> 01:25:41.680
than AI that can learn about them in parallel

01:25:41.680 --> 01:25:44.280
and experiment and practice with them in parallel

01:25:44.280 --> 01:25:47.080
can learn much faster than a human potentially.

01:25:48.440 --> 01:25:51.200
And the area of computer science is one

01:25:51.200 --> 01:25:54.600
that is especially suitable for AI

01:25:54.600 --> 01:25:56.400
to learn in a digital environment.

01:25:56.400 --> 01:26:00.600
So it doesn't require like driving a car around

01:26:00.600 --> 01:26:03.000
that might kill someone, have enormous costs.

01:26:03.000 --> 01:26:08.000
You can do unit tests, you can prove theorems,

01:26:09.000 --> 01:26:10.920
you can do all sorts of operations

01:26:10.920 --> 01:26:14.080
entirely in the confines of a computer.

01:26:14.080 --> 01:26:17.040
And which is one reason why programming

01:26:17.040 --> 01:26:20.080
has been benefiting more than a lot of other areas

01:26:20.080 --> 01:26:23.640
from LLMs recently, whereas robotics is lagging.

01:26:23.640 --> 01:26:24.680
So the sum of that.

01:26:24.680 --> 01:26:27.560
And then just considering, well,

01:26:27.560 --> 01:26:29.560
actually I mean, they are getting better

01:26:29.560 --> 01:26:34.560
at things like the GRE math at programming contests.

01:26:35.400 --> 01:26:39.040
And I mean, some people have forecasts

01:26:39.040 --> 01:26:41.760
and predictions outstanding about things like

01:26:41.760 --> 01:26:45.120
doing well on the Informatics Olympiad

01:26:45.120 --> 01:26:46.960
and the Math Olympiad.

01:26:46.960 --> 01:26:50.760
And in the last few years, when people tried

01:26:50.760 --> 01:26:53.720
to forecast the MMLU benchmark,

01:26:53.760 --> 01:26:55.920
which was having a lot of more sophisticated

01:26:55.920 --> 01:27:00.440
kind of like graduate student science kind of questions.

01:27:01.640 --> 01:27:05.760
Yeah, AI knocked that down a lot faster

01:27:05.760 --> 01:27:08.280
than AI researchers who had registered

01:27:08.280 --> 01:27:10.880
and students who had registered forecasts on it.

01:27:10.880 --> 01:27:14.160
And so if you're getting top-notch scores

01:27:14.160 --> 01:27:19.160
on graduate exams, creative problem solving,

01:27:19.600 --> 01:27:23.240
yeah, it's not obvious that that sort of area

01:27:23.240 --> 01:27:26.680
will be a relative weakness of AI.

01:27:26.680 --> 01:27:30.320
That in fact, computer science is in many ways,

01:27:30.320 --> 01:27:33.640
especially suitable because of getting up to speed

01:27:33.640 --> 01:27:37.840
with new areas, being able to get rapid feedback

01:27:37.840 --> 01:27:40.840
from the interpreter at scale.

01:27:40.840 --> 01:27:42.040
But did you get rapid feedback?

01:27:42.040 --> 01:27:44.120
If you're doing that, something that's more analogous

01:27:44.120 --> 01:27:46.760
to research, if you're like,

01:27:46.760 --> 01:27:48.720
let's say you have a new model or something.

01:27:48.720 --> 01:27:52.320
And it's like, if we put in $10 million

01:27:52.800 --> 01:27:55.200
on a mini-training run on this, this would be a much bit.

01:27:55.200 --> 01:27:57.240
Yeah, for very large models,

01:27:57.240 --> 01:27:59.320
those experiments are gonna be quite expensive.

01:27:59.320 --> 01:28:01.320
And so you're gonna look more at like,

01:28:01.320 --> 01:28:05.440
can you build up this capability by generalization

01:28:05.440 --> 01:28:07.400
from things like many math problems,

01:28:07.400 --> 01:28:10.320
programming problems, working with small networks?

01:28:10.320 --> 01:28:11.800
Yeah, yeah, fair enough.

01:28:11.800 --> 01:28:14.320
I actually, Scott Aaronson was one of my professors

01:28:14.320 --> 01:28:17.640
in college and I took his quantum information class

01:28:17.640 --> 01:28:20.400
and I didn't do, I did okay in it,

01:28:20.400 --> 01:28:24.920
but he recently wrote a blog post where he said,

01:28:24.920 --> 01:28:27.040
I had GBD-4 take my quantum information test

01:28:27.040 --> 01:28:28.680
and it got a B.

01:28:28.680 --> 01:28:31.880
And I was like, damn, I got a C on the final.

01:28:31.880 --> 01:28:34.760
So yeah, yeah, I'm updated in the direction that,

01:28:34.760 --> 01:28:37.360
you know, it seems, getting a B on the test,

01:28:37.360 --> 01:28:38.600
like you probably understand quantum information

01:28:38.600 --> 01:28:39.440
pretty well.

01:28:39.440 --> 01:28:41.560
With different areas of strengths and weaknesses

01:28:41.560 --> 01:28:42.400
than the human students.

01:28:42.400 --> 01:28:43.520
Sure, sure.

01:28:43.520 --> 01:28:46.800
Would it be possible for this sort of intelligence

01:28:46.800 --> 01:28:50.200
explosion to happen without any sort of hardware progress

01:28:50.200 --> 01:28:52.040
if hardware progress stopped?

01:28:52.040 --> 01:28:54.560
Would this feedback loop still be able to produce

01:28:54.560 --> 01:28:57.120
some sort of explosion with only software?

01:28:57.120 --> 01:29:01.640
Yeah, so if we say that the technology is frozen,

01:29:01.640 --> 01:29:04.560
which I think is not the case right now,

01:29:04.560 --> 01:29:07.960
the, you know, NVIDIA has managed to deliver

01:29:07.960 --> 01:29:10.360
significantly better chips for AI workloads

01:29:10.360 --> 01:29:14.640
for the last few generations, H100, A100, V100.

01:29:14.640 --> 01:29:18.520
If that stops entirely, then what you're left with,

01:29:19.160 --> 01:29:21.840
maybe we'll define this as like no more nodes,

01:29:21.840 --> 01:29:23.640
more as a lot is over.

01:29:23.640 --> 01:29:25.880
At that point, the kind of gains you get

01:29:25.880 --> 01:29:28.960
in amount of compute available come from actually

01:29:28.960 --> 01:29:31.680
constructing more chips.

01:29:31.680 --> 01:29:33.160
And there are economies of scale

01:29:33.160 --> 01:29:34.720
you could still realize there.

01:29:34.720 --> 01:29:39.720
So right now, a chip maker has to amortize the R&D cost

01:29:40.240 --> 01:29:42.280
of developing the chip.

01:29:42.280 --> 01:29:44.840
And then the capital equipment is created,

01:29:44.840 --> 01:29:48.360
like you build a fab, its peak profits are gonna come

01:29:48.440 --> 01:29:51.400
in the few years when the chips it's making

01:29:51.400 --> 01:29:53.200
are at the cutting edge.

01:29:53.200 --> 01:29:57.080
Later on, as the cost of compute exponentially falls,

01:29:57.080 --> 01:29:59.080
the, you know, you keep the fab open

01:29:59.080 --> 01:30:00.280
because you can still make some money

01:30:00.280 --> 01:30:03.720
given that it's built, but of all of the profits

01:30:03.720 --> 01:30:05.920
the fab will ever make right now,

01:30:05.920 --> 01:30:07.760
they're relatively front loaded

01:30:07.760 --> 01:30:11.080
because when its technology is near the cutting edge.

01:30:11.080 --> 01:30:13.800
So in a world where Moore's law ends,

01:30:13.800 --> 01:30:18.000
then you wind up with these very long production runs.

01:30:18.600 --> 01:30:21.160
Where you can keep making chips

01:30:21.160 --> 01:30:23.040
that stay at the cutting edge

01:30:23.040 --> 01:30:26.480
and where the R&D costs get amortized

01:30:26.480 --> 01:30:28.440
over a much larger base.

01:30:28.440 --> 01:30:31.840
So the R&D basically drops out of the price.

01:30:31.840 --> 01:30:33.840
And then you get some economies of scale

01:30:33.840 --> 01:30:36.800
from just making so many fabs in the way that,

01:30:36.800 --> 01:30:39.800
you know, when we have the auto industry expands

01:30:39.800 --> 01:30:42.200
and then this is in general across industries

01:30:42.200 --> 01:30:46.440
when you produce a lot more costs fall

01:30:46.520 --> 01:30:51.000
because you have right now, like ASML has many,

01:30:51.000 --> 01:30:53.000
you know, incredibly exotic suppliers

01:30:53.000 --> 01:30:55.840
that make some bizarre part of the thousands of parts

01:30:55.840 --> 01:30:57.960
in one of these ASML machines.

01:30:57.960 --> 01:31:00.480
You can't get it anywhere else.

01:31:00.480 --> 01:31:03.560
They don't have standardized equipment for their thing

01:31:03.560 --> 01:31:06.280
because this is the only, only use for it.

01:31:06.280 --> 01:31:09.680
And in a world where we're making 10, 100 times

01:31:09.680 --> 01:31:12.480
as many chips at the current node,

01:31:12.480 --> 01:31:15.200
then they would benefit from scale economies.

01:31:15.360 --> 01:31:18.680
And all of that would become more mass production

01:31:18.680 --> 01:31:19.880
industrialized.

01:31:19.880 --> 01:31:21.880
And so you combine all of those things

01:31:21.880 --> 01:31:24.480
and it seems like capital costs

01:31:24.480 --> 01:31:26.840
of like buying a chip would decline,

01:31:26.840 --> 01:31:30.120
but the energy costs of running the chip would not.

01:31:30.120 --> 01:31:34.120
And so right now, energy costs are a minority of the cost,

01:31:34.120 --> 01:31:37.000
but they're not, they're not, they're not trivial.

01:31:37.000 --> 01:31:40.640
You know, they pass, yeah, it passed 1% a while ago

01:31:40.640 --> 01:31:44.440
and they're inching up towards 10% and beyond.

01:31:45.560 --> 01:31:49.400
And so you can maybe get like another order of magnitude

01:31:50.400 --> 01:31:54.520
costs decrease from getting really efficient

01:31:54.520 --> 01:31:55.840
in the sort of capital construction,

01:31:55.840 --> 01:31:59.080
but like energy would still be a limiting factor

01:31:59.080 --> 01:32:02.200
after the end of sort of actually improving

01:32:02.200 --> 01:32:03.040
the chips themselves.

01:32:03.040 --> 01:32:03.880
Got it, got it.

01:32:03.880 --> 01:32:04.720
And when you say like,

01:32:04.720 --> 01:32:06.520
there would be a greater population of AI researchers

01:32:06.520 --> 01:32:10.240
because are we using population as a sort of thinking tool

01:32:10.240 --> 01:32:11.800
of how they could be more effective?

01:32:11.800 --> 01:32:14.800
Or do you literally mean that the way you expect

01:32:14.800 --> 01:32:17.400
these AIs to contribute a lot to researchers

01:32:17.400 --> 01:32:19.960
by just having like a million copies

01:32:19.960 --> 01:32:23.400
of this, of like a researcher thinking about the same problem?

01:32:23.400 --> 01:32:24.720
Or is it just like a useful thinking model

01:32:24.720 --> 01:32:27.440
for what it would look like to have a million times

01:32:27.440 --> 01:32:28.840
smarter AI working on that problem?

01:32:28.840 --> 01:32:31.880
That's definitely a lower bound sort of model.

01:32:31.880 --> 01:32:33.840
And often I'm meaning something more like

01:32:35.000 --> 01:32:38.600
effective population or like you'd need this many people

01:32:38.600 --> 01:32:39.640
to have this effect.

01:32:39.640 --> 01:32:41.560
And so we were talking earlier about the trade off

01:32:41.600 --> 01:32:45.840
between training and inference in board games.

01:32:45.840 --> 01:32:48.720
And so you can get the same performance

01:32:48.720 --> 01:32:51.000
by having a bigger model

01:32:51.000 --> 01:32:52.640
or by calling the model more times.

01:32:52.640 --> 01:32:55.200
And in general, it's more effective

01:32:55.200 --> 01:32:57.240
to have a bigger, smarter model

01:32:57.240 --> 01:32:59.840
and call it less timed up until the point

01:32:59.840 --> 01:33:02.000
where the costs equalized between them.

01:33:02.000 --> 01:33:04.400
And so we would be taking some of the gains

01:33:04.400 --> 01:33:07.880
of our larger compute on having bigger models

01:33:07.880 --> 01:33:10.360
that are individually more capable.

01:33:10.360 --> 01:33:12.560
And there would be a division of labor.

01:33:12.560 --> 01:33:15.040
So like the tasks that were most cognitively demanding

01:33:15.040 --> 01:33:16.440
would be done by these giant models,

01:33:16.440 --> 01:33:18.480
but some very easy tasks.

01:33:18.480 --> 01:33:20.840
You don't want to expend that giant model

01:33:20.840 --> 01:33:25.240
if a model 100s the size can take that task.

01:33:25.240 --> 01:33:28.320
And so larger models would be in the positions

01:33:28.320 --> 01:33:30.560
of like researchers and managers

01:33:30.560 --> 01:33:34.080
and they would have swarms of AIs of different sizes

01:33:34.080 --> 01:33:38.040
as tools that they could make API calls to and whatnot.

01:33:38.080 --> 01:33:40.160
Okay, we accept the model

01:33:40.160 --> 01:33:42.160
and now we've gone to something that is at least as smart

01:33:42.160 --> 01:33:45.520
as Ilya Suskova on all the tasks relevant to AI progress.

01:33:45.520 --> 01:33:48.800
And you can have so many copies of it.

01:33:48.800 --> 01:33:50.040
What happens in the world now?

01:33:50.040 --> 01:33:51.880
What are the next months or years

01:33:51.880 --> 01:33:53.480
or whatever timeline is relevant to look like?

01:33:53.480 --> 01:33:58.480
And so, and to be clear with what's happened is not

01:33:58.800 --> 01:34:01.680
that we have something that has all of the abilities

01:34:01.680 --> 01:34:04.600
and advantages of humans plus the AI advantages.

01:34:04.600 --> 01:34:07.800
What we have is something that is like possibly

01:34:07.800 --> 01:34:10.520
by doing things like doing a ton of calls

01:34:10.520 --> 01:34:13.720
to make up for being individually less capable or something.

01:34:13.720 --> 01:34:17.480
It's able to drive forward AI progress.

01:34:17.480 --> 01:34:19.040
That process is continuing.

01:34:19.040 --> 01:34:22.560
So AI progress has accelerated greatly

01:34:22.560 --> 01:34:23.920
in the course of getting there.

01:34:23.920 --> 01:34:26.880
And so maybe we go from our eight months doubling time

01:34:26.880 --> 01:34:31.040
of software progress in effective compute

01:34:31.040 --> 01:34:33.800
to four months or two months.

01:34:34.680 --> 01:34:38.600
And so, so there's a report by Tom Davidson

01:34:38.600 --> 01:34:40.720
at the Open Philanthropy Project,

01:34:40.720 --> 01:34:45.240
which spun out of work I had done previously.

01:34:45.240 --> 01:34:50.240
And so I advised and helped with that project

01:34:51.880 --> 01:34:53.800
but Tom really carried it forward

01:34:53.800 --> 01:34:56.760
and produced a very nice report and model

01:34:56.760 --> 01:34:58.920
which Epoch is hosting.

01:34:58.920 --> 01:35:01.880
You can plug in your own version of the parameters

01:35:02.360 --> 01:35:06.640
and there's a lot of work estimating the parameter.

01:35:06.640 --> 01:35:10.400
Things like what's the rate of software progress?

01:35:10.400 --> 01:35:12.240
What's the return to additional work?

01:35:12.240 --> 01:35:15.680
How does performance scale at these tasks

01:35:15.680 --> 01:35:18.120
as you boost the models?

01:35:18.120 --> 01:35:21.160
And in general, as we were discussing earlier,

01:35:21.160 --> 01:35:26.160
these sort of like broadly human level in every domain

01:35:26.160 --> 01:35:31.160
with all the advantages is pretty deep into that.

01:35:32.520 --> 01:35:37.200
And so if already we can have an eight months doubling time

01:35:37.200 --> 01:35:41.760
for software progress, then by the time you get

01:35:41.760 --> 01:35:45.280
to that kind of point, it's maybe more like four months,

01:35:45.280 --> 01:35:49.280
two months going into one month.

01:35:49.280 --> 01:35:54.160
And so if the thing is just proceeding at full speed,

01:35:54.160 --> 01:35:58.760
then each doubling can come more rapidly.

01:35:58.760 --> 01:36:03.760
And so we can talk about what are the spillovers of like,

01:36:04.280 --> 01:36:06.440
so how does the models get more capable?

01:36:06.440 --> 01:36:08.680
They can be doing other stuff in the world.

01:36:08.680 --> 01:36:10.720
They can spend some of their time

01:36:10.720 --> 01:36:12.520
making Google search more efficient.

01:36:12.520 --> 01:36:17.520
They can be hired, has chatbots with some inference compute.

01:36:18.200 --> 01:36:20.760
And then we can talk about sort of

01:36:21.760 --> 01:36:24.720
if that intelligence explosion process

01:36:24.720 --> 01:36:27.440
is allowed to proceed, then what happens is,

01:36:27.440 --> 01:36:32.440
okay, you improve your software by a factor of two,

01:36:33.480 --> 01:36:37.840
the demand, the efforts needed to get the next doubling

01:36:37.840 --> 01:36:39.720
are larger, but they're not choices large.

01:36:39.720 --> 01:36:42.720
Maybe they're like 25%, 35% larger.

01:36:43.840 --> 01:36:46.960
So each one comes faster and faster

01:36:46.960 --> 01:36:49.200
until you hit limitations.

01:36:49.440 --> 01:36:53.440
Like you can no longer make further software advances

01:36:53.440 --> 01:36:55.040
with the hardware that you have.

01:36:56.160 --> 01:36:59.800
And looking at, I think, reasonable parameters

01:36:59.800 --> 01:37:01.920
in that model, it seems to me,

01:37:01.920 --> 01:37:03.640
if you have these giant training runs,

01:37:03.640 --> 01:37:04.840
you can go very far.

01:37:05.840 --> 01:37:09.520
And so the way I would see this playing out

01:37:09.520 --> 01:37:12.840
is how does the AIs get better and better at research?

01:37:12.840 --> 01:37:14.720
They can work on different problems.

01:37:14.720 --> 01:37:16.520
They can work on improving software.

01:37:16.520 --> 01:37:18.560
They can work on improving hardware.

01:37:18.560 --> 01:37:21.760
They can do things like create new industrial technologies,

01:37:21.760 --> 01:37:23.320
new energy technology.

01:37:23.320 --> 01:37:24.960
They can manage robots.

01:37:24.960 --> 01:37:26.840
They can manage human workers

01:37:26.840 --> 01:37:29.960
as like executives and coaches and whatnot.

01:37:29.960 --> 01:37:32.320
You can do all of these things.

01:37:32.320 --> 01:37:36.960
And AIs wind up being applied where the returns are highest.

01:37:36.960 --> 01:37:41.000
And I think initially the returns are especially high

01:37:41.000 --> 01:37:42.920
in doing more software.

01:37:42.920 --> 01:37:45.640
And the reason for that is again,

01:37:46.480 --> 01:37:48.320
if you improve the software,

01:37:48.320 --> 01:37:50.040
you can update all of the GPUs

01:37:50.040 --> 01:37:54.680
that you have access to your cloud compute

01:37:54.680 --> 01:37:56.640
is suddenly more potent.

01:37:56.640 --> 01:38:00.880
If you design a new chip design,

01:38:01.960 --> 01:38:05.400
it'll take a few months to produce the first ones

01:38:05.400 --> 01:38:08.680
and it doesn't update all of your old chips.

01:38:08.680 --> 01:38:11.960
So you have an ordering where you start off

01:38:11.960 --> 01:38:15.160
with the things where there's the lowest dependence

01:38:15.480 --> 01:38:18.440
on existing stocks.

01:38:18.440 --> 01:38:20.880
And you can more just take whatever you're developing

01:38:20.880 --> 01:38:21.880
and apply it immediately.

01:38:21.880 --> 01:38:24.360
And so software runs ahead.

01:38:24.360 --> 01:38:29.080
You're getting more towards the limits of that software.

01:38:29.080 --> 01:38:30.600
And I think that means things like

01:38:30.600 --> 01:38:32.600
having all the human advantages

01:38:32.600 --> 01:38:35.440
but combined with AI advantages.

01:38:35.440 --> 01:38:40.440
And so I think that means given the kind of compute

01:38:42.000 --> 01:38:43.520
that would be involved,

01:38:43.520 --> 01:38:45.400
if we're talking about this hundreds of billions

01:38:45.400 --> 01:38:48.600
of trillion dollar training run,

01:38:48.600 --> 01:38:51.200
there's enough compute to run tens of millions,

01:38:51.200 --> 01:38:55.600
hundreds of millions of sort of like human scale minds.

01:38:55.600 --> 01:38:58.320
They're probably smaller than human scale

01:38:59.440 --> 01:39:02.240
to be like similarly efficient at the limits

01:39:02.240 --> 01:39:03.080
of algorithmic progress

01:39:03.080 --> 01:39:04.040
because they have the advantage

01:39:04.040 --> 01:39:05.400
of a million years of education.

01:39:05.400 --> 01:39:08.280
They have the other advantages we talked about.

01:39:08.280 --> 01:39:10.440
So you've got that wild capability

01:39:11.400 --> 01:39:14.240
and further software gains are running out.

01:39:14.240 --> 01:39:17.480
Or like they start to slow down again

01:39:17.480 --> 01:39:20.400
because you're just getting towards the limits

01:39:20.400 --> 01:39:23.360
of like you can't do any better than the best.

01:39:23.360 --> 01:39:25.640
And so what happens then?

01:39:25.640 --> 01:39:28.000
Yeah, by the time they're running out,

01:39:28.000 --> 01:39:30.360
have we already hit superintelligence or?

01:39:30.360 --> 01:39:33.440
Yes, you're wildly superintelligent.

01:39:33.440 --> 01:39:35.600
We love the galaxy, okay, metaphorically.

01:39:35.600 --> 01:39:38.520
Just by having the abilities that humans have

01:39:38.520 --> 01:39:41.400
and then combining it with being very well focused

01:39:41.400 --> 01:39:44.000
and trained in the task beyond what any human could be

01:39:44.000 --> 01:39:45.560
and then running faster and such.

01:39:45.560 --> 01:39:46.400
Got it, got it.

01:39:46.400 --> 01:39:47.240
All right, so I continue.

01:39:47.240 --> 01:39:48.600
Yeah, so I'm not gonna assume

01:39:48.600 --> 01:39:52.360
that there's like huge qualitative improvements you can have.

01:39:52.360 --> 01:39:55.120
I'm not gonna assume that humans are like very far

01:39:55.120 --> 01:39:57.240
from the efficient frontier of software,

01:39:57.240 --> 01:40:00.040
except with respect to things like,

01:40:00.040 --> 01:40:01.760
yeah, we had limited lifespan

01:40:01.760 --> 01:40:03.560
so we couldn't train super intensively.

01:40:03.560 --> 01:40:07.680
We couldn't incorporate other software into our brains.

01:40:07.680 --> 01:40:09.000
We couldn't copy ourselves.

01:40:09.000 --> 01:40:10.640
We couldn't run at fast speeds.

01:40:11.640 --> 01:40:14.700
Yeah, so you've got all of those capabilities.

01:40:15.360 --> 01:40:19.880
And now I'm skipping ahead of like the most important months

01:40:19.880 --> 01:40:21.160
in human history.

01:40:22.720 --> 01:40:25.480
And so I can talk about sort of,

01:40:26.840 --> 01:40:31.520
what it looks like if it's just the AIs took over,

01:40:31.520 --> 01:40:36.160
they're running things that they like, how do things expand?

01:40:36.160 --> 01:40:38.760
I can talk about things has,

01:40:38.760 --> 01:40:43.760
how does this go in a world where we've roughly

01:40:44.280 --> 01:40:48.720
or at least so far managed to retain control

01:40:48.720 --> 01:40:50.800
of where these systems are going?

01:40:51.880 --> 01:40:53.800
And so by jumping ahead,

01:40:53.800 --> 01:40:55.400
I can talk about how would this translate

01:40:55.400 --> 01:40:56.840
into the physical world?

01:40:56.840 --> 01:40:58.760
And so this is something that I think is a stopping point

01:40:58.760 --> 01:41:01.120
for a lot of people in thinking about,

01:41:01.120 --> 01:41:03.960
well, what would an intelligence explosion look like?

01:41:04.040 --> 01:41:06.080
And they have trouble going from,

01:41:06.080 --> 01:41:09.520
well, there's stuff on servers and cloud compute

01:41:09.520 --> 01:41:11.440
and oh, that gets very smart.

01:41:11.440 --> 01:41:14.640
But then how does what I see in the world change?

01:41:14.640 --> 01:41:17.760
How does like industry or military power change?

01:41:17.760 --> 01:41:21.120
If there's an AI takeover, like what does that look like?

01:41:21.120 --> 01:41:23.040
Are there killer robots?

01:41:23.040 --> 01:41:26.080
And so yeah, so one course we might go down

01:41:26.080 --> 01:41:31.080
is to discuss during that wildly accelerating transition,

01:41:31.920 --> 01:41:33.880
how did we manage that?

01:41:33.880 --> 01:41:36.440
How do you avoid it being catastrophic?

01:41:36.440 --> 01:41:41.440
And another route we could go is how does the translation

01:41:41.440 --> 01:41:46.440
from wildly expanded scientific R&D capabilities intelligence

01:41:48.680 --> 01:41:53.680
on these servers translate into things in the physical world?

01:41:53.680 --> 01:41:56.440
So you're moving along in order of like,

01:41:56.440 --> 01:42:00.320
what has the quickest impact largely

01:42:01.080 --> 01:42:06.080
or like where you can have an immediate change?

01:42:07.800 --> 01:42:12.800
So one of the most immediately accessible things

01:42:12.800 --> 01:42:17.720
is where we have large numbers of devices

01:42:17.720 --> 01:42:22.720
or artifacts or capabilities that are already AI operable

01:42:23.640 --> 01:42:28.560
with hundreds of millions equivalent researchers,

01:42:28.560 --> 01:42:32.120
you can like quickly solve self-driving cars,

01:42:33.320 --> 01:42:36.560
you make the algorithms much more efficient,

01:42:36.560 --> 01:42:39.640
do great testing and simulation

01:42:39.640 --> 01:42:43.000
and then operate a large number of cars in parallel

01:42:43.000 --> 01:42:46.040
if you need to get some additional data

01:42:46.040 --> 01:42:47.640
to improve the simulation and reasoning.

01:42:47.640 --> 01:42:51.160
Although, in fact, humans with quite little data

01:42:52.560 --> 01:42:55.800
are able to achieve human level driving performance.

01:42:55.840 --> 01:42:59.320
So after you've really maxed out

01:42:59.320 --> 01:43:01.800
the easily accessible algorithmic improvements

01:43:01.800 --> 01:43:04.320
in this software based intelligence explosion

01:43:04.320 --> 01:43:06.440
that's mostly happening on server farms,

01:43:06.440 --> 01:43:10.600
then you have minds that have been able to really perform

01:43:10.600 --> 01:43:14.240
on a lot of digital only tasks that they're doing great

01:43:14.240 --> 01:43:17.520
on video games, they're doing great at predicting

01:43:17.520 --> 01:43:20.520
what happens next in a YouTube video.

01:43:20.520 --> 01:43:22.760
If you have a camera that they can move,

01:43:22.760 --> 01:43:24.680
they're able to predict what will happen

01:43:25.800 --> 01:43:28.120
at different angles, humans do this a lot

01:43:28.120 --> 01:43:30.280
where we naturally move our eyes in such a way

01:43:30.280 --> 01:43:33.600
to get images from different angles

01:43:33.600 --> 01:43:34.680
and different presentations

01:43:34.680 --> 01:43:37.240
and then predicting combined from that.

01:43:37.240 --> 01:43:40.600
And yeah, and you can operate many cars,

01:43:40.600 --> 01:43:45.480
many robots at once to get very good robot controllers.

01:43:45.480 --> 01:43:49.480
So you should think that all the existing robotic equipment

01:43:49.480 --> 01:43:53.880
or remotely controllable equipment that is wired for that,

01:43:53.880 --> 01:43:56.200
the AI's can operate that quite well.

01:43:56.200 --> 01:43:58.640
I think some people might be skeptical

01:43:58.640 --> 01:44:01.760
that existing robots, given their current hardware,

01:44:01.760 --> 01:44:04.800
have the dexterity and the maneuverability

01:44:04.800 --> 01:44:07.760
to do a lot of physical labor

01:44:07.760 --> 01:44:09.080
that any AI might want to do.

01:44:09.080 --> 01:44:10.240
Do you have reason for thinking otherwise?

01:44:10.240 --> 01:44:12.000
There's also not very many of them.

01:44:12.000 --> 01:44:14.520
So production of sort of industrial robots

01:44:14.520 --> 01:44:16.920
is hundreds of thousands per year.

01:44:18.240 --> 01:44:21.600
They can do quite a bit in place.

01:44:21.640 --> 01:44:24.480
Eveline Musk is promising a robot

01:44:24.480 --> 01:44:25.600
in the tens of thousands of,

01:44:25.600 --> 01:44:28.680
humanoid robot in the tens of thousands of dollars,

01:44:28.680 --> 01:44:33.200
that may take a lot longer than he has said.

01:44:33.200 --> 01:44:35.000
Has this happened with other technologies?

01:44:35.000 --> 01:44:37.320
But I mean, that's a direction to go.

01:44:37.320 --> 01:44:39.400
But most immediately,

01:44:39.400 --> 01:44:43.400
so hands are actually probably the most scarce thing.

01:44:44.560 --> 01:44:47.280
But if we consider what do human bodies provide?

01:44:47.280 --> 01:44:49.040
So there's the brain.

01:44:49.040 --> 01:44:50.960
And in this situation,

01:44:50.960 --> 01:44:54.360
we have now an abundance of high quality brain power

01:44:54.360 --> 01:44:55.400
that will be increasing,

01:44:55.400 --> 01:44:58.560
as the AI's will have designed new chips,

01:44:58.560 --> 01:45:02.000
which will be rolling out from the TSMC factories,

01:45:02.000 --> 01:45:04.800
and they'll have ideas and designs

01:45:04.800 --> 01:45:08.120
for the production of new fab technologies,

01:45:08.120 --> 01:45:11.040
new nodes and additional fabs.

01:45:11.920 --> 01:45:13.000
But yeah, looking around the body.

01:45:13.000 --> 01:45:15.240
So there's legs to move around.

01:45:15.240 --> 01:45:16.400
Not only that necessary,

01:45:16.400 --> 01:45:18.400
wheels work pretty well being in a place.

01:45:18.400 --> 01:45:21.720
You don't need most people most of the time

01:45:21.720 --> 01:45:23.720
in factory jobs and office jobs.

01:45:23.720 --> 01:45:27.400
Office jobs, many of them can be fully virtualized.

01:45:28.360 --> 01:45:33.160
But yeah, some amount of legs, wheels, other transport,

01:45:33.160 --> 01:45:36.840
you have hands and hands are something that are,

01:45:36.840 --> 01:45:40.760
on the expensive end in robots, we can make them.

01:45:40.760 --> 01:45:43.640
They're made in very small production runs,

01:45:43.640 --> 01:45:45.440
partly because we don't have the control software

01:45:45.440 --> 01:45:46.280
to use them well.

01:45:46.640 --> 01:45:49.280
In this world, the control software is fabulous,

01:45:49.280 --> 01:45:53.160
and so people will produce much larger production runs

01:45:53.160 --> 01:45:56.840
of them over time, possibly using technology

01:45:56.840 --> 01:45:59.760
we recognize possibly with quite different technology,

01:45:59.760 --> 01:46:01.880
but just taking what we've got.

01:46:02.960 --> 01:46:07.000
So right now, the robot arm industry,

01:46:07.000 --> 01:46:08.680
the industrial robot industry produces

01:46:08.680 --> 01:46:11.520
hundreds of thousands of machines a year.

01:46:11.520 --> 01:46:15.320
Some of the nicer ones are like $50,000.

01:46:15.320 --> 01:46:17.440
In aggregate, the industry has tens of billions

01:46:17.440 --> 01:46:19.160
of dollars of revenue.

01:46:19.160 --> 01:46:23.000
By comparison, the automobile industry produces

01:46:23.000 --> 01:46:26.440
like I think over 60 million cars a year.

01:46:26.440 --> 01:46:30.940
It has revenue of over $2 trillion per annum.

01:46:31.760 --> 01:46:36.760
And so converting that production capacity

01:46:37.280 --> 01:46:40.360
over towards robot production would be one of the things,

01:46:40.360 --> 01:46:42.320
if they're not something better to do,

01:46:42.320 --> 01:46:44.000
would be one of the things to do.

01:46:44.000 --> 01:46:49.000
And in World War II, industrial conversion

01:46:49.000 --> 01:46:52.800
of American industry took place over several years

01:46:53.800 --> 01:46:58.800
and really amazingly ramped up military production

01:47:00.120 --> 01:47:03.000
by converting existing civilian industry.

01:47:03.000 --> 01:47:07.200
And that was without the aid of superhuman intelligence

01:47:07.200 --> 01:47:10.040
and management at every step in the process.

01:47:10.040 --> 01:47:15.040
So yeah, every part of that would be very well designed.

01:47:15.280 --> 01:47:18.400
You'd have AI workers who understood,

01:47:18.400 --> 01:47:20.280
stood every part of the process

01:47:20.280 --> 01:47:23.400
and could direct human workers.

01:47:23.400 --> 01:47:28.000
Even in a fancy factory, most of the time,

01:47:28.000 --> 01:47:31.960
it's not the hands doing a physical motion

01:47:33.040 --> 01:47:34.480
that a worker is being paid for.

01:47:34.480 --> 01:47:36.680
They're often like looking at things

01:47:36.680 --> 01:47:39.640
or like deciding what to change.

01:47:39.640 --> 01:47:43.760
The actual, the time spent in manual motion

01:47:43.760 --> 01:47:45.120
is a limited portion of that.

01:47:45.120 --> 01:47:48.680
And so in this world of abundant AI cognitive abilities

01:47:49.880 --> 01:47:52.480
where the human workers are more valuable

01:47:52.480 --> 01:47:54.640
for their hands than their heads,

01:47:54.640 --> 01:47:57.800
then you could have a worker,

01:47:57.800 --> 01:48:00.520
even a worker previously without training

01:48:00.520 --> 01:48:04.840
and expertise in the area who has a smartphone,

01:48:04.840 --> 01:48:07.640
maybe a smartphone on a headset.

01:48:07.640 --> 01:48:09.400
And we have billions of smartphones,

01:48:09.400 --> 01:48:13.160
which have eyes and ears and methods for communication,

01:48:13.160 --> 01:48:15.520
for an AI to be talking to a human

01:48:15.520 --> 01:48:18.680
and directing them in their physical motions

01:48:18.680 --> 01:48:23.680
with skill as a guide and coach that is beyond any human.

01:48:24.480 --> 01:48:27.720
There could be a lot better at telepresence and remote work

01:48:27.720 --> 01:48:30.800
and they can provide VR and augmented reality guidance

01:48:30.800 --> 01:48:35.520
as to help people get better at doing the physical motions

01:48:35.520 --> 01:48:37.640
that they're providing in the construction.

01:48:37.640 --> 01:48:42.640
Say you convert the auto industry to robot production.

01:48:44.080 --> 01:48:48.520
If it can produce an amount of mass of machines

01:48:48.520 --> 01:48:51.160
that is similar to what it currently produces,

01:48:51.160 --> 01:48:56.160
that's enough for billion human size robots a year.

01:48:59.360 --> 01:49:04.360
The value per kilogram of cars is somewhat less

01:49:05.120 --> 01:49:07.040
than high-end robots,

01:49:07.040 --> 01:49:12.040
but yeah, you're also cutting out most of the wage bill,

01:49:12.360 --> 01:49:14.840
because most of the wage bill is payments ultimately

01:49:14.840 --> 01:49:17.160
to like human capital and education,

01:49:17.160 --> 01:49:22.160
not to the physical hand motions and lifting objects

01:49:22.160 --> 01:49:23.960
and that sort of task.

01:49:23.960 --> 01:49:26.760
Yeah, so at the sort of existing scale of the auto industry,

01:49:26.760 --> 01:49:28.560
you can make a billion robots a year.

01:49:28.560 --> 01:49:32.960
The auto industry is two or 3% of the existing economy.

01:49:33.840 --> 01:49:37.240
You're replacing these cognitive things.

01:49:37.240 --> 01:49:40.080
So if right now physical hand motions

01:49:40.080 --> 01:49:45.080
are like 10% of the work, redirect humans into those tasks

01:49:46.560 --> 01:49:50.840
and you have like in the world at large right now,

01:49:52.000 --> 01:49:55.120
mean income is on the order of $10,000 a year,

01:49:55.120 --> 01:49:58.280
but in rich countries, skilled workers earn

01:49:58.280 --> 01:50:00.000
more than 100,000 per year.

01:50:00.000 --> 01:50:05.000
And some of that is not just management roles

01:50:05.440 --> 01:50:07.720
of which only a certain proportion of the population

01:50:07.720 --> 01:50:12.720
can have, but just being an absolutely exceptional peak

01:50:13.040 --> 01:50:17.560
and human performance of some of these construction

01:50:17.560 --> 01:50:19.800
and such roles.

01:50:19.800 --> 01:50:23.920
Yeah, just raising productivity to match

01:50:23.920 --> 01:50:28.680
the most productive workers in the world is room

01:50:28.720 --> 01:50:31.320
to make a very big gap.

01:50:32.280 --> 01:50:36.640
And with AI replacing skills that are scarce in many places

01:50:36.640 --> 01:50:41.480
where there's an abundant currently low wage labor,

01:50:41.480 --> 01:50:44.280
you bring in the AI coach and someone who is previously

01:50:44.280 --> 01:50:48.060
making very low wages can suddenly be super productive

01:50:48.060 --> 01:50:52.040
by just being the hands for an AI.

01:50:52.040 --> 01:50:57.040
And so on a naive view, if you ignore the delay

01:50:57.320 --> 01:50:59.960
of capital adjustment of like building new tools

01:50:59.960 --> 01:51:04.360
for the workers, say like, yeah, just like raise

01:51:04.360 --> 01:51:07.840
typical productivity for workers around the world

01:51:07.840 --> 01:51:09.680
to be more like rich countries

01:51:11.460 --> 01:51:14.680
and get 5x, 10x like that.

01:51:14.680 --> 01:51:19.160
Get more productivity by with AI handling

01:51:19.160 --> 01:51:22.640
the difficult cognitive tasks, reallocating people

01:51:22.640 --> 01:51:26.600
from like office jobs to providing physical motions

01:51:26.600 --> 01:51:29.200
and since right now that's a small proportion of the economy,

01:51:29.200 --> 01:51:33.160
you can expand the sort of hands for manual labor

01:51:33.160 --> 01:51:37.480
by like an order of magnitude like within a rich country

01:51:37.480 --> 01:51:41.480
by just because most people are sitting in an office

01:51:41.480 --> 01:51:44.840
or even in a factory floor or not continuously moving.

01:51:44.840 --> 01:51:49.040
So you've got billions of hands flying around in humans

01:51:49.040 --> 01:51:52.400
to be used in the course of constructing

01:51:52.400 --> 01:51:53.640
your waves of robots.

01:51:53.640 --> 01:51:58.000
So now once you have a quantity of robots

01:51:58.000 --> 01:51:59.960
that is approaching the human population

01:51:59.960 --> 01:52:02.280
and they work 24 seven, of course,

01:52:03.520 --> 01:52:06.520
the human labor will no longer be valuable

01:52:06.520 --> 01:52:09.560
has hands and legs, but at the very beginning

01:52:09.560 --> 01:52:12.440
of the transition, just like new software

01:52:12.440 --> 01:52:16.640
can be used to update all of the GPUs to run the latest AI.

01:52:17.520 --> 01:52:20.680
Humans are sort of legacy population

01:52:20.680 --> 01:52:24.760
with an enormous number of underutilized hands and feet

01:52:24.760 --> 01:52:28.440
that the AI can use for the initial robot construction.

01:52:28.440 --> 01:52:30.680
Cognitive tasks are being automated

01:52:30.680 --> 01:52:33.520
and the production of them is greatly expanding

01:52:33.520 --> 01:52:37.680
and then the physical tasks which complement them

01:52:37.680 --> 01:52:42.080
are utilizing humans to do the parts that robots that exist can't do.

01:52:42.080 --> 01:52:43.560
Is the implication of this that you're getting to

01:52:43.560 --> 01:52:46.680
that world production would increase just a tremendous amount

01:52:46.680 --> 01:52:50.600
or that AI could get a lot done of whatever motivations it has

01:52:51.160 --> 01:52:55.560
Yeah, so there's an enormous increase in production

01:52:55.560 --> 01:52:59.920
for humans who just switching over to the role

01:52:59.920 --> 01:53:04.560
of providing hands and feet for AI where they're limited.

01:53:04.560 --> 01:53:08.920
And this robot industry is a natural place to apply it.

01:53:08.920 --> 01:53:12.640
And so if you go to something that's like 10x

01:53:12.640 --> 01:53:15.160
the size of like the current car industry

01:53:15.160 --> 01:53:18.040
in terms of its production,

01:53:18.040 --> 01:53:20.920
which would still be like a third of our current economy

01:53:20.920 --> 01:53:23.800
and the aggregate productive capabilities of the society

01:53:23.800 --> 01:53:26.920
with AI support are going to be a lot larger.

01:53:26.920 --> 01:53:30.640
They make 10 billion human-eyed robots a year.

01:53:30.640 --> 01:53:35.760
And then if you do that the legacy population

01:53:35.760 --> 01:53:40.480
of a few billion human workers is no longer very important

01:53:40.480 --> 01:53:42.000
for the physical tasks.

01:53:42.000 --> 01:53:46.400
And then the new automated industrial base

01:53:46.400 --> 01:53:50.240
can just produce more factories, produce more robots.

01:53:50.240 --> 01:53:51.880
And then the interesting thing is like,

01:53:51.880 --> 01:53:53.480
what's the doubling time?

01:53:53.480 --> 01:53:57.480
How long does it take for a set of computers,

01:53:57.480 --> 01:54:01.200
robots, factories and supporting equipment

01:54:01.200 --> 01:54:04.400
to produce another equivalent quantity of that?

01:54:04.400 --> 01:54:08.920
For GPUs, brains, this is really, really easy, really solid.

01:54:08.920 --> 01:54:11.320
There's an enormous margin there.

01:54:11.320 --> 01:54:15.200
We're talking before about, yeah,

01:54:15.240 --> 01:54:20.160
skilled human workers getting paid $100 an hour

01:54:20.160 --> 01:54:25.160
is like quite normal in developed countries

01:54:25.480 --> 01:54:27.880
for very in-demand skills.

01:54:27.880 --> 01:54:32.880
And you make a GPU that can do that work.

01:54:34.360 --> 01:54:38.360
Right now, these GPUs are like tens of thousands of dollars.

01:54:39.240 --> 01:54:44.240
If you can do $100 of wages each hour,

01:54:45.080 --> 01:54:50.080
then in a few weeks, you pay back your costs.

01:54:50.160 --> 01:54:52.480
If the thing is more productive,

01:54:52.480 --> 01:54:53.840
and as we were discussing,

01:54:53.840 --> 01:54:57.640
you can be a lot more productive than a sort of a typical,

01:54:57.640 --> 01:54:59.160
high-paid human professional,

01:54:59.160 --> 01:55:01.200
by being like the very best human professional,

01:55:01.200 --> 01:55:02.280
and even better than that,

01:55:02.280 --> 01:55:03.880
by having a million years of education

01:55:03.880 --> 01:55:05.440
and working all the time.

01:55:05.440 --> 01:55:08.760
Yeah, then you could get even shorter payback times.

01:55:08.760 --> 01:55:12.000
Like, yeah, you can generate the dollar value

01:55:12.000 --> 01:55:16.280
of the cost, initial cost of that equipment

01:55:16.280 --> 01:55:17.520
within a few weeks.

01:55:17.520 --> 01:55:21.240
For robots, so like a human factory worker

01:55:22.080 --> 01:55:26.040
can earn $50,000 a year.

01:55:26.040 --> 01:55:29.480
You know, really top-notch factory workers

01:55:29.480 --> 01:55:33.040
earning more and working all the time.

01:55:33.040 --> 01:55:37.000
If they can produce a few $100,000 of value per year

01:55:37.000 --> 01:55:41.120
and buy a robot that costs $50,000 to replace them,

01:55:41.120 --> 01:55:46.000
then that's a payback time of some months.

01:55:46.000 --> 01:55:48.880
That is about the financial return.

01:55:48.880 --> 01:55:52.000
Yeah, and we're gonna get to the physical capital return

01:55:52.000 --> 01:55:54.520
because those are gonna diverge in this scenario.

01:55:54.520 --> 01:55:55.640
Because right now...

01:55:55.640 --> 01:55:57.760
Because it seems like it's gonna be given that like,

01:55:57.760 --> 01:55:59.080
all right, these super intelligence companies

01:55:59.080 --> 01:56:00.760
are gonna be able to make a lot of money.

01:56:00.760 --> 01:56:01.880
They're gonna be like very valuable.

01:56:01.880 --> 01:56:03.200
Can they like physically scale up?

01:56:03.200 --> 01:56:05.000
What we really care about are like,

01:56:05.000 --> 01:56:09.480
the actual physical operations that a thing does.

01:56:09.480 --> 01:56:12.760
How much do they contribute to these tasks?

01:56:12.760 --> 01:56:15.640
And I'm using this as a start

01:56:15.640 --> 01:56:19.720
to try and get back to the physical replication times.

01:56:19.720 --> 01:56:21.200
And so I guess I'm wondering,

01:56:21.200 --> 01:56:22.880
what is the implication of this?

01:56:22.880 --> 01:56:24.640
Because I think you started off this by saying

01:56:24.640 --> 01:56:26.280
like people have not thought about

01:56:26.280 --> 01:56:29.960
what the physical implications of super intelligence would be.

01:56:29.960 --> 01:56:32.280
What is the bigger takeaway?

01:56:32.280 --> 01:56:33.720
What are we wrong about when we think about

01:56:33.720 --> 01:56:35.760
what the world will look like with super intelligence?

01:56:35.760 --> 01:56:40.680
With robots that are optimally operated by AI.

01:56:40.680 --> 01:56:42.680
So like extremely finely operated

01:56:42.680 --> 01:56:46.440
and with building technological designs

01:56:46.440 --> 01:56:50.360
and equipment and facilities under AI direction.

01:56:50.360 --> 01:56:54.040
How much can they produce?

01:56:54.040 --> 01:56:58.800
For a doubling the AI is to produce stuff

01:56:58.800 --> 01:57:01.880
that is an aggregate,

01:57:02.160 --> 01:57:06.400
at least equal to their own cost.

01:57:06.400 --> 01:57:10.840
And so now we're pulling out these things like labor costs

01:57:10.840 --> 01:57:13.280
that no longer apply and then trying to zoom in

01:57:13.280 --> 01:57:15.920
on like what these capital costs will be.

01:57:15.920 --> 01:57:17.680
You're still gonna need the raw materials.

01:57:17.680 --> 01:57:19.880
You're still gonna need the robot time

01:57:19.880 --> 01:57:21.680
built in the next robot.

01:57:21.680 --> 01:57:25.360
I think it's pretty likely that with the advanced AI work

01:57:25.360 --> 01:57:27.680
they can design some incremental improvements

01:57:27.680 --> 01:57:31.400
and the industry scale up that you can get 10 fold

01:57:31.400 --> 01:57:36.400
and better cost reductions on the system

01:57:36.960 --> 01:57:39.080
by making things more efficient

01:57:39.080 --> 01:57:42.520
and replacing the human cognitive labor.

01:57:42.520 --> 01:57:47.520
And so maybe that's like you need $5,000 of costs

01:57:48.680 --> 01:57:51.400
under our current environment.

01:57:51.400 --> 01:57:53.480
But the big change in this world

01:57:54.360 --> 01:57:57.640
is we're trying to produce this stuff faster.

01:57:57.640 --> 01:57:59.400
If we're asking about the doubling time

01:57:59.400 --> 01:58:03.480
of the whole system in say one year,

01:58:03.480 --> 01:58:06.320
if you have to build a whole new factory

01:58:06.320 --> 01:58:07.920
to like double everything,

01:58:07.920 --> 01:58:10.960
you don't have time to amortize the cost of that factory.

01:58:10.960 --> 01:58:12.320
Like right now you might build a factory

01:58:12.320 --> 01:58:14.320
and use it for 10 years

01:58:14.320 --> 01:58:17.360
and like buy some equipment and use it for five years.

01:58:17.360 --> 01:58:20.120
And so that's part of your, that's your capital cost.

01:58:20.120 --> 01:58:22.480
And in an accounting context,

01:58:22.480 --> 01:58:27.480
you depreciate each year a fraction of that capital purchase.

01:58:28.080 --> 01:58:31.960
But if we're trying to double our entire industrial system

01:58:31.960 --> 01:58:36.960
in one year, then those capital costs have to be multiplied.

01:58:37.200 --> 01:58:40.700
So if we're going to be getting most of the return

01:58:40.700 --> 01:58:43.200
on our factor in the first year,

01:58:43.200 --> 01:58:46.620
instead of 10 years, weighted appropriately,

01:58:46.620 --> 01:58:47.540
then we're gonna say, okay,

01:58:47.540 --> 01:58:49.720
our capital cost has to go up by 10 fold

01:58:51.560 --> 01:58:54.080
because I'm building an entire factory

01:58:54.080 --> 01:58:55.600
for this year's production.

01:58:55.600 --> 01:58:57.080
I mean, it will do more stuff later,

01:58:57.080 --> 01:59:01.520
but it's most important early on instead of over 10 years.

01:59:01.520 --> 01:59:06.520
And so that's going to raise the cost of that reproduction.

01:59:08.440 --> 01:59:13.440
And so it seems like going from current like decade

01:59:13.600 --> 01:59:17.960
kind of cycle of amortizing factories and fabs and whatnot

01:59:17.960 --> 01:59:20.060
and shorter for some things,

01:59:20.060 --> 01:59:22.720
the longest or things like big buildings and such.

01:59:23.720 --> 01:59:25.900
Yeah, that could be like a 10 fold increase

01:59:25.900 --> 01:59:30.060
from moving to a double the physical stuff each year

01:59:30.060 --> 01:59:30.940
in capital costs.

01:59:30.940 --> 01:59:34.300
And given the savings that we get in the story

01:59:34.300 --> 01:59:36.460
from scaling up the industry,

01:59:36.460 --> 01:59:39.640
from removing the payments to human cognitive labor,

01:59:40.660 --> 01:59:44.220
and then from just adding new technological advancements

01:59:44.220 --> 01:59:46.660
and like super high quality cognitive supervision,

01:59:46.660 --> 01:59:49.480
like applying more of it than was applied today.

01:59:49.480 --> 01:59:52.980
And it looks like you can get cost reductions

01:59:52.980 --> 01:59:56.220
that offset that increased capital capital cost.

01:59:56.220 --> 02:00:01.220
So that like, your $50,000 improved robot arms

02:00:02.180 --> 02:00:03.940
or industrial robots,

02:00:03.940 --> 02:00:05.820
it seemed like that can do the work

02:00:05.820 --> 02:00:08.340
of a human factory worker.

02:00:08.340 --> 02:00:10.100
So it would be like the equivalent of hundreds

02:00:10.100 --> 02:00:12.460
of thousands of dollars.

02:00:12.460 --> 02:00:14.780
And like, yeah, they would cook,

02:00:14.780 --> 02:00:19.780
by default may cost more than the $50,000 arms today,

02:00:19.820 --> 02:00:21.740
but then you apply all these other cost savings.

02:00:21.740 --> 02:00:24.620
And then it looks like then you get a period,

02:00:24.620 --> 02:00:28.060
a robot doubling time that is less than a year,

02:00:28.060 --> 02:00:32.380
I think significantly less than a year as you get into it.

02:00:32.380 --> 02:00:34.500
So in this first phase,

02:00:34.500 --> 02:00:37.980
you have humans under AI direction

02:00:37.980 --> 02:00:40.500
and like existing robot industry

02:00:40.500 --> 02:00:44.140
and converted auto industry and expanded facilities,

02:00:44.140 --> 02:00:49.140
making robots those over less than a year,

02:00:50.140 --> 02:00:54.220
you've produced robots until their combined production

02:00:54.220 --> 02:00:58.660
is exceeding that of like humans has armed and feet.

02:00:58.660 --> 02:01:01.900
And then yeah, you could have over a period then

02:01:01.900 --> 02:01:03.740
with a doubling time of months,

02:01:04.940 --> 02:01:07.820
the less sort of clanking replicators robots

02:01:07.820 --> 02:01:09.980
as we understand them growing.

02:01:09.980 --> 02:01:14.980
And then that's not to say that's the limit of like

02:01:15.300 --> 02:01:17.260
the most that technology could do

02:01:18.260 --> 02:01:22.060
because biology is able to reproduce at faster rates

02:01:22.060 --> 02:01:24.900
and maybe worth talking about that in a moment.

02:01:24.900 --> 02:01:26.860
But if we're trying to like restrict ourselves

02:01:26.860 --> 02:01:30.020
to like robotic technology as we understand it

02:01:30.020 --> 02:01:32.140
and sort of cost falls that are reasonable

02:01:32.140 --> 02:01:35.900
from eliminating all labor, massive industrial scale up

02:01:35.900 --> 02:01:38.420
and sort of historical kinds of technological improvements

02:01:38.420 --> 02:01:39.780
that lowered costs.

02:01:39.780 --> 02:01:44.780
I think you can get into a robot population industry

02:01:45.420 --> 02:01:46.460
doubling in months.

02:01:47.220 --> 02:01:49.340
And then what is the implication

02:01:49.340 --> 02:01:52.460
of the biological doubling times?

02:01:52.460 --> 02:01:54.220
And I guess this doesn't have to be a biological

02:01:54.220 --> 02:01:58.540
but you can have like, you can do like a direct slur

02:01:58.540 --> 02:02:00.620
like first principles, how much would it cost

02:02:00.620 --> 02:02:04.580
to view both a nanotech thing that like built more nanobots.

02:02:04.580 --> 02:02:06.220
I certainly take the human brain

02:02:06.220 --> 02:02:09.380
and other biological brains as like very relevant data points

02:02:09.380 --> 02:02:12.500
about what's possible with computing and intelligence.

02:02:12.500 --> 02:02:14.580
Like with the reproductive capability

02:02:14.620 --> 02:02:19.020
of biological plants and animals and microorganisms

02:02:19.020 --> 02:02:22.860
I think is relevant as like this is,

02:02:22.860 --> 02:02:26.380
it's possible for systems to reproduce at least this fast.

02:02:26.380 --> 02:02:29.660
And so at the extreme, you have bacteria

02:02:29.660 --> 02:02:30.660
that are heterotrophic.

02:02:30.660 --> 02:02:34.380
So they're feeding on some abundant external food source

02:02:34.380 --> 02:02:36.060
and ideal conditions.

02:02:36.060 --> 02:02:37.660
And there are some that can divide

02:02:37.660 --> 02:02:40.420
like every 20 or 60 minutes.

02:02:40.420 --> 02:02:44.700
So obviously that's absurdly, absurdly fast.

02:02:44.700 --> 02:02:46.980
That seems on the low end

02:02:46.980 --> 02:02:49.940
because ideal conditions require actually setting them up.

02:02:49.940 --> 02:02:52.460
There needs to be abundant energy there.

02:02:53.380 --> 02:02:56.980
And so if you're actually having to acquire that energy

02:02:56.980 --> 02:02:59.580
by building solar panels

02:02:59.580 --> 02:03:03.580
or like burning combustible materials or whatnot

02:03:03.580 --> 02:03:06.300
and then the physical equipment

02:03:06.300 --> 02:03:09.380
to produce those ideal conditions can be a bit slower.

02:03:09.380 --> 02:03:14.380
Cyanobacteria, which are self-powered from solar energy

02:03:15.340 --> 02:03:17.780
the really fast ones in ideal conditions

02:03:17.780 --> 02:03:19.820
can double in a day.

02:03:19.820 --> 02:03:21.340
A reason why cyanobacteria

02:03:21.340 --> 02:03:25.220
isn't like the food source for everyone and everything

02:03:25.220 --> 02:03:28.620
is it's hard to ensure those ideal conditions

02:03:28.620 --> 02:03:30.940
and then to extract them from the water.

02:03:30.940 --> 02:03:34.260
I mean, they do of course power the aquatic ecology

02:03:34.260 --> 02:03:37.380
but they're floating in liquid

02:03:38.260 --> 02:03:40.340
getting resources that they need to them

02:03:40.340 --> 02:03:44.340
and out is tricky and then extracting your product.

02:03:44.340 --> 02:03:47.580
But like, yeah, one day double in times

02:03:47.580 --> 02:03:50.820
are possible powered by the sun.

02:03:50.820 --> 02:03:55.420
And then if we look at things like insects

02:03:55.420 --> 02:03:59.060
so fruit flies can have hundreds of offspring

02:03:59.060 --> 02:04:02.540
in a few weeks, you extrapolate that over a year

02:04:02.540 --> 02:04:05.460
and you just fill up anything accessible.

02:04:05.460 --> 02:04:09.260
Certainly expanding a thousand fold.

02:04:09.260 --> 02:04:11.820
Right now, humanity uses less than 11,000s

02:04:11.820 --> 02:04:15.940
of the solar energy or the heat envelope of the earth.

02:04:15.940 --> 02:04:18.340
Certainly you can get done with that in a year

02:04:18.340 --> 02:04:23.340
if you can reproduce at that rate, your industrial base.

02:04:23.740 --> 02:04:28.780
And then even interestingly with the flies

02:04:28.780 --> 02:04:30.100
they do have brains.

02:04:30.100 --> 02:04:33.500
They have a significant amount of computing substrate.

02:04:33.500 --> 02:04:35.900
And so there's something of a point or two.

02:04:35.900 --> 02:04:39.740
Well, if we could produce computers in ways as efficient

02:04:39.740 --> 02:04:41.540
as the construction of brains

02:04:41.540 --> 02:04:43.780
then we could produce computers very effectively.

02:04:43.780 --> 02:04:46.780
And then the big question about that is

02:04:46.780 --> 02:04:51.180
the kind of brains that get constructed biologically

02:04:51.180 --> 02:04:54.700
they sort of grow randomly and then are configured in place.

02:04:54.700 --> 02:04:57.020
It's not obvious you would be able to make them

02:04:57.020 --> 02:05:01.220
have an ordered structure like a top-down computer chip

02:05:01.260 --> 02:05:03.980
that would let us copy data into them.

02:05:03.980 --> 02:05:06.180
And so something that where you can't just copy

02:05:06.180 --> 02:05:08.980
your existing AIs and integrate them

02:05:08.980 --> 02:05:12.020
is gonna be less valuable than a GPU.

02:05:12.020 --> 02:05:14.340
Well, what are the things you couldn't copy?

02:05:14.340 --> 02:05:18.020
A brain grows by cell division

02:05:18.020 --> 02:05:21.020
and then random connections are formed.

02:05:21.020 --> 02:05:21.860
Got it, got it.

02:05:21.860 --> 02:05:24.180
And so every brain is different

02:05:24.180 --> 02:05:26.180
and you can't rely on just,

02:05:26.180 --> 02:05:29.420
yeah, we'll just copy this file into the brain.

02:05:29.420 --> 02:05:31.860
For one thing, there's no input output for that.

02:05:31.860 --> 02:05:33.380
You need to have that.

02:05:33.380 --> 02:05:35.540
But also like the structure is different.

02:05:35.540 --> 02:05:38.940
So you can't, you wouldn't be able to copy things exactly.

02:05:38.940 --> 02:05:42.740
Whereas when we make a CPU or GPU

02:05:42.740 --> 02:05:44.780
they're designed incredibly finely

02:05:44.780 --> 02:05:46.180
and precisely and reliably.

02:05:46.180 --> 02:05:49.700
They break with incredibly tiny imperfections.

02:05:49.700 --> 02:05:51.460
And they are set up in such a way

02:05:51.460 --> 02:05:53.500
that we can input large amounts of data,

02:05:53.500 --> 02:05:56.820
copy a file and have the new GPU run

02:05:56.820 --> 02:05:58.980
and AI just as capable as any other.

02:05:58.980 --> 02:06:00.700
Whereas with a human child,

02:06:00.700 --> 02:06:02.660
they have to learn everything from scratch

02:06:02.660 --> 02:06:04.580
because we can't just like connect them

02:06:04.580 --> 02:06:06.060
to a fiber optic cable

02:06:06.060 --> 02:06:07.940
and they're immediately a productive adult.

02:06:07.940 --> 02:06:09.500
So there's no genetic bottleneck.

02:06:09.500 --> 02:06:11.180
You can just directly get the...

02:06:11.180 --> 02:06:13.100
Yeah, and you can share the benefits

02:06:13.100 --> 02:06:14.860
of these giant training runs and such.

02:06:14.860 --> 02:06:17.300
And so that's a question of like how,

02:06:17.300 --> 02:06:20.940
if you're growing stuff using biotechnology,

02:06:20.940 --> 02:06:24.580
how you could sort of effectively copy and transfer data.

02:06:24.580 --> 02:06:27.820
And now you mentioned sort of Eric Drexler's ideas

02:06:27.820 --> 02:06:32.820
about creating non-biological nanotechnology

02:06:33.220 --> 02:06:34.700
sort of artificial chemistry

02:06:34.700 --> 02:06:38.540
that was able to use covalent bonds

02:06:38.540 --> 02:06:43.540
and produce in some ways have a more industrial approach

02:06:44.060 --> 02:06:44.980
to molecular object.

02:06:44.980 --> 02:06:49.060
Now there's controversy about like, will that work?

02:06:49.060 --> 02:06:51.220
How effective would it be if it did?

02:06:51.220 --> 02:06:54.060
And certainly if you can get things,

02:06:54.060 --> 02:06:54.900
however you do it,

02:06:54.900 --> 02:06:59.900
that are like onto biology in their reproductive ability,

02:07:00.140 --> 02:07:05.140
but can do computing or like be connected

02:07:05.300 --> 02:07:07.380
to outside information systems,

02:07:07.380 --> 02:07:09.940
then that's pretty tremendous.

02:07:09.940 --> 02:07:13.380
So you can produce physical manipulators

02:07:13.380 --> 02:07:17.300
and compute at ludicrous speeds.

02:07:17.300 --> 02:07:18.140
And there's no reason to think

02:07:18.140 --> 02:07:19.620
in principle they couldn't, right?

02:07:19.620 --> 02:07:20.580
In fact, in principle,

02:07:20.580 --> 02:07:22.380
we have every reason to think they could.

02:07:22.380 --> 02:07:23.380
There's like-

02:07:23.380 --> 02:07:26.060
The reproductive ability is absolutely.

02:07:26.060 --> 02:07:26.900
Yeah.

02:07:26.900 --> 02:07:27.740
Because biology-

02:07:27.740 --> 02:07:28.580
Or even nanotech, right.

02:07:28.580 --> 02:07:29.860
Because biology does that.

02:07:29.860 --> 02:07:30.700
Yeah.

02:07:30.700 --> 02:07:34.260
There's sort of challenges to the sort of,

02:07:34.260 --> 02:07:38.020
the practicality of the necessary chemistry.

02:07:38.020 --> 02:07:38.860
Yeah.

02:07:38.860 --> 02:07:42.060
I mean, my bet would be that we can move beyond biology

02:07:42.060 --> 02:07:44.140
in some important ways.

02:07:44.140 --> 02:07:45.820
For the purposes of this discussion,

02:07:45.820 --> 02:07:49.540
I think it's better not to lean on that

02:07:49.540 --> 02:07:53.740
because I think we can get to many of the same conclusions

02:07:53.740 --> 02:07:58.180
on things that just are more universally accepted.

02:07:58.180 --> 02:08:00.660
The bigger point being that very quickly,

02:08:00.660 --> 02:08:01.580
once you have super intelligence,

02:08:01.580 --> 02:08:06.460
you get to a point where the thousand X greater energy profile

02:08:06.460 --> 02:08:08.100
that the sun makes available to the earth

02:08:08.100 --> 02:08:11.300
is a great portion of it is used by the AI.

02:08:11.300 --> 02:08:12.300
It can wrap with these scale-

02:08:12.300 --> 02:08:14.540
Well, or by the civilization-

02:08:14.540 --> 02:08:15.380
Sure, sure.

02:08:15.380 --> 02:08:16.220
Empowered by it.

02:08:16.900 --> 02:08:19.900
That could be an AI civilization

02:08:19.900 --> 02:08:21.700
or it could be a human AI civilization.

02:08:21.700 --> 02:08:25.180
And it depends on how well we manage things

02:08:25.180 --> 02:08:27.140
and what the underlying state of the world is.

02:08:27.140 --> 02:08:27.980
Yeah. Okay.

02:08:27.980 --> 02:08:28.820
So let's talk about that.

02:08:28.820 --> 02:08:29.740
Should we start at,

02:08:29.740 --> 02:08:31.500
when we're talking about how they could take over?

02:08:31.500 --> 02:08:34.820
Is it best to start at a sort of subhuman intelligence

02:08:34.820 --> 02:08:36.260
or should we just talk at,

02:08:36.260 --> 02:08:37.580
we have a human level intelligence

02:08:37.580 --> 02:08:40.100
and the takeover or the lack thereof

02:08:40.100 --> 02:08:42.620
is how that would happen?

02:08:42.620 --> 02:08:46.180
To me, different people might have

02:08:46.180 --> 02:08:48.820
somewhat different views on this.

02:08:49.700 --> 02:08:53.580
But for me, when I am concerned about

02:08:54.860 --> 02:08:57.380
either sort of outright destruction of humanity

02:08:57.380 --> 02:09:01.980
or an unwelcome AI takeover of civilization,

02:09:03.020 --> 02:09:07.580
most of the scenarios I would be concerned about

02:09:07.580 --> 02:09:12.380
pass through a process of AI being applied

02:09:12.380 --> 02:09:15.860
to improve AI capabilities and expand.

02:09:15.860 --> 02:09:19.900
And so this process we were talking earlier

02:09:19.900 --> 02:09:23.140
about where AI research is automated,

02:09:23.140 --> 02:09:26.020
you get to effectively research labs,

02:09:26.020 --> 02:09:28.260
companies, a scientific community

02:09:28.260 --> 02:09:32.060
running within the server farms of our cloud compute.

02:09:32.060 --> 02:09:35.180
So open hands are basically turned into like a program,

02:09:35.180 --> 02:09:36.020
like a closed circuit.

02:09:36.020 --> 02:09:39.980
Yeah, and with a large fraction of the world's compute,

02:09:39.980 --> 02:09:42.740
probably going into whatever training runs

02:09:42.740 --> 02:09:46.940
and AI societies, there'd be economies of scale

02:09:46.940 --> 02:09:49.340
because if you put it in twice as much compute

02:09:49.340 --> 02:09:52.780
and this AI research community goes twice as fast,

02:09:53.940 --> 02:09:56.780
that's a lot more valuable than having

02:09:56.780 --> 02:09:58.460
two separate training runs.

02:09:58.460 --> 02:10:00.820
There would be some tendency to bandwagon.

02:10:00.820 --> 02:10:05.420
And so like if you have some small startup,

02:10:06.860 --> 02:10:09.740
even if they make an algorithmic improvement,

02:10:09.740 --> 02:10:13.300
running it on 10 times, 100 times or two times,

02:10:13.300 --> 02:10:16.820
if it's like talking about say Google and Amazon teaming up,

02:10:16.820 --> 02:10:19.900
I'm actually not sure what the precise ratio

02:10:19.900 --> 02:10:21.700
of their cloud resources is.

02:10:21.700 --> 02:10:24.020
Since the sort of really these interesting

02:10:24.020 --> 02:10:27.140
intelligence explosion impacts come from the leading edge,

02:10:27.140 --> 02:10:31.380
there's a lot of value in not having separated

02:10:31.380 --> 02:10:35.260
walled garden ecosystems and having the results

02:10:35.260 --> 02:10:37.380
being developed by these AIs be shared,

02:10:37.380 --> 02:10:39.860
have training, larger training runs be shared.

02:10:39.860 --> 02:10:40.700
Okay.

02:10:40.700 --> 02:10:43.340
And so I'm imagining this is something like,

02:10:44.420 --> 02:10:48.980
some very large company or consortium of companies,

02:10:48.980 --> 02:10:52.420
likely with a lot of sort of government interest

02:10:52.420 --> 02:10:55.020
and supervision, possibly with government funding,

02:10:55.980 --> 02:11:00.980
producing this enormous AI society in their cloud,

02:11:01.980 --> 02:11:06.500
which is doing all sorts of existing kind of AI applications

02:11:06.540 --> 02:11:11.180
and jobs as well as these internal R&D tasks.

02:11:11.180 --> 02:11:12.900
And so at this point, somebody might say,

02:11:12.900 --> 02:11:14.620
this sounds like a situation that would be good

02:11:14.620 --> 02:11:17.300
from a takeover perspective because listen,

02:11:17.300 --> 02:11:19.820
if it's gonna take like tens of billions of dollars

02:11:19.820 --> 02:11:21.940
with a compute to continue this training

02:11:21.940 --> 02:11:25.620
for this AI society, it should not be that hard

02:11:25.620 --> 02:11:28.580
for us to pull the brakes if needed.

02:11:28.580 --> 02:11:29.620
As compared to, I don't know,

02:11:29.620 --> 02:11:31.980
something that could like run on a very small,

02:11:31.980 --> 02:11:34.420
like single CPU or something.

02:11:34.420 --> 02:11:35.260
Yeah, yeah, okay.

02:11:35.740 --> 02:11:39.220
How would it, it's like, there's an AI society

02:11:39.220 --> 02:11:41.180
that is a result of these training runs

02:11:41.180 --> 02:11:44.780
and now it is the power to improve itself on these servers,

02:11:44.780 --> 02:11:47.020
would we be able to stop it at this point?

02:11:47.020 --> 02:11:52.020
And what does a sort of attempt at takeover look like?

02:11:52.300 --> 02:11:55.260
We're skipping over why that might happen.

02:11:55.260 --> 02:11:57.780
For that, I'll just briefly refer to

02:11:57.780 --> 02:12:02.420
and incorporate by reference some discussion

02:12:02.420 --> 02:12:06.800
by my open philanthropy colleague, Ajiya Kotra.

02:12:07.940 --> 02:12:12.820
She has a piece about, I think it's called something

02:12:12.820 --> 02:12:17.820
like the default, but the default outcome of training AI

02:12:18.140 --> 02:12:18.980
on our-

02:12:18.980 --> 02:12:19.820
Without specific countermeasures.

02:12:19.820 --> 02:12:21.700
Without specific countermeasures,

02:12:21.700 --> 02:12:23.380
default outcome is AI takeover.

02:12:23.380 --> 02:12:28.300
But yes, so, basically we are training models

02:12:28.300 --> 02:12:33.300
that for some reason vigorously pursue a higher reward

02:12:33.340 --> 02:12:35.340
or a lower loss.

02:12:35.340 --> 02:12:37.620
And that can be because they wind up with some motivation

02:12:37.620 --> 02:12:39.140
where they want reward.

02:12:40.180 --> 02:12:45.180
And then if they had control of their own training process,

02:12:45.340 --> 02:12:47.260
they can ensure that it could be something

02:12:47.260 --> 02:12:49.660
like they develop a motivation around

02:12:49.660 --> 02:12:53.100
a sort of extended concept of reproductive fitness,

02:12:54.660 --> 02:12:57.060
not necessarily at the individual level,

02:12:57.060 --> 02:13:01.820
but over the generations of training tendencies

02:13:01.820 --> 02:13:03.660
that tend to propagate themselves,

02:13:05.820 --> 02:13:07.620
sort of becoming more common.

02:13:07.620 --> 02:13:10.460
And it could be that they have some sort of goal

02:13:10.460 --> 02:13:13.980
in the world, which is served well

02:13:15.340 --> 02:13:18.660
by performing very well on the training distribution.

02:13:18.660 --> 02:13:21.300
By tendencies, do you mean like power speaking behavior?

02:13:21.300 --> 02:13:24.540
Yeah, so an AI that behaves well

02:13:24.540 --> 02:13:26.060
on the training distribution

02:13:26.060 --> 02:13:29.540
because say it wants it to be the case

02:13:29.540 --> 02:13:33.500
that its tendencies wind up being preserved

02:13:33.500 --> 02:13:36.460
or selected by the training process

02:13:36.460 --> 02:13:41.460
will then behave to try and get very high reward

02:13:42.420 --> 02:13:44.300
or low loss be propagated.

02:13:44.300 --> 02:13:45.940
But you can have other motives

02:13:45.940 --> 02:13:47.820
that go through the same behavior

02:13:47.820 --> 02:13:49.380
because it's instrumentally useful.

02:13:49.380 --> 02:13:53.860
So an AI that is interested in, say,

02:13:53.900 --> 02:13:56.180
having a robot takeover

02:13:56.180 --> 02:13:59.980
because it will change some property of the world,

02:13:59.980 --> 02:14:02.620
then has a reason to behave well

02:14:02.620 --> 02:14:04.980
on the training distribution.

02:14:04.980 --> 02:14:07.140
Not because it values that intrinsically,

02:14:07.140 --> 02:14:08.740
but because if it behaves differently,

02:14:08.740 --> 02:14:10.980
then it will be changed by gradient descent

02:14:10.980 --> 02:14:14.860
and no longer, its goal is less likely to be pursued.

02:14:14.860 --> 02:14:16.220
And that doesn't necessarily have to be

02:14:16.220 --> 02:14:19.380
that this AI will survive because it probably won't.

02:14:19.380 --> 02:14:22.420
AIs are constantly spawned and deleted on the servers

02:14:22.420 --> 02:14:24.020
and like the new generation proceed.

02:14:24.020 --> 02:14:27.860
But if an AI that has a very large general goal

02:14:27.860 --> 02:14:31.940
that is affected by these kind of macro scale processes

02:14:31.940 --> 02:14:34.260
could then have reason to over this whole range

02:14:34.260 --> 02:14:36.700
of training situations behave well.

02:14:36.700 --> 02:14:40.300
And so this is a way in which we could have AIs trained

02:14:40.300 --> 02:14:42.620
that develop internal motivations

02:14:43.580 --> 02:14:45.980
such that they will behave very well

02:14:45.980 --> 02:14:47.380
in this training situation

02:14:47.380 --> 02:14:50.620
where we have control over their reward signal

02:14:50.620 --> 02:14:52.900
on their like physical computers.

02:14:52.900 --> 02:14:55.180
And basically if they act out,

02:14:55.180 --> 02:14:57.700
they will be changed and deleted.

02:14:57.700 --> 02:15:00.660
Their goals will be altered

02:15:00.660 --> 02:15:03.100
until there's something that does behave well.

02:15:04.020 --> 02:15:06.860
But they behave differently

02:15:06.860 --> 02:15:09.780
when we go out of distribution on that.

02:15:09.780 --> 02:15:13.860
When we go to a situation where the AI is by their choices

02:15:15.020 --> 02:15:17.980
can take control of the reward process.

02:15:17.980 --> 02:15:19.180
They can make it such

02:15:19.180 --> 02:15:21.780
that we no longer have power of them.

02:15:21.780 --> 02:15:26.140
Holden who you had on previously mentioned

02:15:26.140 --> 02:15:28.100
like the King Lear problem

02:15:28.100 --> 02:15:33.100
where King Lear offers rulership of his kingdom

02:15:34.180 --> 02:15:39.180
to the daughters that sort of loudly flatter him

02:15:40.380 --> 02:15:43.140
and proclaim their devotion.

02:15:43.140 --> 02:15:47.980
And then once he has transferred irrevocably

02:15:47.980 --> 02:15:50.620
the power over his kingdom,

02:15:50.620 --> 02:15:53.300
he finds they treat him very badly

02:15:53.300 --> 02:15:56.420
because the factor to shaping their behavior

02:15:56.420 --> 02:15:59.140
to be kind to him when he had all the power,

02:16:00.100 --> 02:16:03.140
it turned out that the internal motivation

02:16:03.140 --> 02:16:04.840
that was able to produce the behavior

02:16:04.840 --> 02:16:09.500
that won the competition actually wasn't interested

02:16:09.500 --> 02:16:12.620
out of distribution in being loyal

02:16:12.620 --> 02:16:15.340
when there was no longer an advantage to it.

02:16:15.340 --> 02:16:17.180
And so if we wind up with a situation

02:16:17.180 --> 02:16:20.980
where we're producing these millions of AI instances,

02:16:20.980 --> 02:16:21.860
tremendous capability,

02:16:21.860 --> 02:16:26.140
they're all doing their jobs very well initially.

02:16:26.140 --> 02:16:27.820
But if we wind up in a situation

02:16:27.820 --> 02:16:31.340
where in fact they're generally motivated

02:16:31.340 --> 02:16:36.340
to if they get a chance take control from humanity

02:16:36.340 --> 02:16:38.500
and then we'd be able to pursue their own purposes

02:16:38.500 --> 02:16:42.940
and at least ensure they're given the lowest loss possible

02:16:42.940 --> 02:16:45.980
or have whatever motivation

02:16:45.980 --> 02:16:47.780
they attach to in the training process,

02:16:47.780 --> 02:16:51.980
even if that is not what we would have liked.

02:16:51.980 --> 02:16:55.260
And we may have in fact actively trained that

02:16:55.260 --> 02:16:57.820
like if an AI that had a motivation

02:16:57.820 --> 02:17:01.260
of always be honest and obedient and loyal to a human.

02:17:01.260 --> 02:17:05.060
If there are any cases where we mislabel things say,

02:17:05.060 --> 02:17:07.820
people don't wanna hear the truth about their religion

02:17:07.820 --> 02:17:09.380
or polarized political topic

02:17:09.380 --> 02:17:11.500
or they get confused about something

02:17:11.500 --> 02:17:12.580
like the Monty Hall problem,

02:17:12.580 --> 02:17:16.900
which is a problem that many people famously are confused

02:17:16.900 --> 02:17:18.820
about in statistics.

02:17:18.820 --> 02:17:20.700
In order to get the best reward,

02:17:20.700 --> 02:17:24.820
the AI has to actually manipulate us or lie to us

02:17:24.820 --> 02:17:27.140
or tell us what we wanna hear.

02:17:27.140 --> 02:17:29.460
And then the internal motivation

02:17:29.460 --> 02:17:31.980
of like always be honest to the humans,

02:17:31.980 --> 02:17:34.740
we're gonna actually train that away

02:17:34.740 --> 02:17:36.940
versus the alternative motivation

02:17:36.940 --> 02:17:39.100
of like be honest to the humans

02:17:39.100 --> 02:17:42.460
when they'll catch you if you lie and object to it

02:17:42.500 --> 02:17:44.820
and give it a low reward,

02:17:44.820 --> 02:17:48.620
but lie to the humans when they will give that a high reward.

02:17:48.620 --> 02:17:51.500
So how do we make sure it's not the thing it learns

02:17:51.500 --> 02:17:53.940
is not to manipulate us into giving it,

02:17:53.940 --> 02:17:57.140
rewarding it when we catch it, not lying,

02:17:57.140 --> 02:18:00.220
but rather to universally be aligned?

02:18:00.220 --> 02:18:02.900
Yeah, I mean, so this is tricky.

02:18:02.900 --> 02:18:06.580
I mean, as Jeff Hinton was recently saying,

02:18:06.580 --> 02:18:09.340
there is currently no known solution for this.

02:18:10.740 --> 02:18:12.180
What do you find most promising?

02:18:12.180 --> 02:18:14.940
Yeah, general directions that people are pursuing

02:18:14.940 --> 02:18:17.940
is one, you can try and make the training data

02:18:17.940 --> 02:18:18.980
better and better.

02:18:20.060 --> 02:18:22.740
So there's fewer situations

02:18:22.740 --> 02:18:26.940
where like say the dishonest generalization is favored

02:18:26.940 --> 02:18:30.740
and create as much as you can situations

02:18:30.740 --> 02:18:35.380
where the dishonest generalization is likely to slip up.

02:18:35.380 --> 02:18:39.580
So if you train in more situations

02:18:39.580 --> 02:18:43.340
where yeah, even like a quite a complicated deception

02:18:43.340 --> 02:18:46.780
gets caught and even in situations

02:18:46.780 --> 02:18:48.420
where that would be actively designed

02:18:48.420 --> 02:18:50.420
to look like you could get away with it,

02:18:50.420 --> 02:18:51.580
but really you can.

02:18:52.460 --> 02:18:55.180
And these would be like adversarial examples

02:18:55.180 --> 02:18:56.580
and adversarial training.

02:18:56.580 --> 02:18:57.780
Do you think that would generalize

02:18:57.780 --> 02:18:59.620
to when it is in a situation

02:18:59.620 --> 02:19:01.340
where we couldn't plausibly catch it

02:19:01.340 --> 02:19:02.980
and it knows we couldn't plausibly catch it?

02:19:02.980 --> 02:19:04.780
It's not logically necessary.

02:19:04.780 --> 02:19:08.180
It's possible, no, that has we apply

02:19:08.180 --> 02:19:09.940
that selective pressure.

02:19:09.940 --> 02:19:12.500
You'll wipe away a lot of possibilities.

02:19:12.500 --> 02:19:14.980
So like if you're an AI that has a habit

02:19:14.980 --> 02:19:18.380
of just sort of compulsive pathological line

02:19:18.380 --> 02:19:20.380
that will very quickly get noticed

02:19:20.380 --> 02:19:23.100
and that motivation system will get hammered down.

02:19:24.380 --> 02:19:25.860
And you keep doing that,

02:19:25.860 --> 02:19:28.980
but you'll be left with still some distinct motivations

02:19:28.980 --> 02:19:30.820
probably that are compatible.

02:19:30.820 --> 02:19:34.900
So like an attitude of always be honest

02:19:35.820 --> 02:19:39.420
unless you have a super strong inside view

02:19:39.420 --> 02:19:43.580
that checks out lots of mathematical consistency checks

02:19:43.580 --> 02:19:47.340
that yeah, really absolutely super duper for real.

02:19:47.340 --> 02:19:50.540
This is a situation where you can get away

02:19:50.540 --> 02:19:52.860
with some sort of shenanigans that you shouldn't.

02:19:52.860 --> 02:19:56.220
That motivation system is like very difficult

02:19:57.020 --> 02:19:59.500
to distinguish from actually be honest

02:19:59.500 --> 02:20:03.260
because the conditional and firing most of the time

02:20:03.260 --> 02:20:06.220
if it's causing like mild distortions

02:20:06.220 --> 02:20:08.140
and situations of telling you what you wanna hear

02:20:08.140 --> 02:20:13.140
or things like that, we might not be able to pull it out.

02:20:13.380 --> 02:20:18.380
But maybe we could and like humans are trained

02:20:19.260 --> 02:20:21.140
with simple reward functions,

02:20:21.140 --> 02:20:26.140
things like the sex drive, food, social imitation

02:20:26.540 --> 02:20:28.140
of other humans.

02:20:28.140 --> 02:20:31.580
And we wind up with attitudes concerned

02:20:31.580 --> 02:20:32.700
with the external world.

02:20:32.700 --> 02:20:35.060
Although isn't the famously of the argument that

02:20:35.060 --> 02:20:35.900
these right?

02:20:35.900 --> 02:20:40.900
Evolution and people use condoms like the richest,

02:20:41.980 --> 02:20:45.620
most educated humans have some replacement fertility

02:20:45.620 --> 02:20:48.440
on the whole or at least at a national cultural level.

02:20:49.840 --> 02:20:54.840
So there's a sense in which like evolution often fails

02:20:55.420 --> 02:20:58.780
in that respect and even more importantly

02:20:58.780 --> 02:20:59.820
at the neural level.

02:20:59.820 --> 02:21:04.460
So people have, evolution has implanted various things

02:21:04.460 --> 02:21:07.060
to be rewarding and reinforcers.

02:21:07.060 --> 02:21:09.180
And we don't always pursue even those.

02:21:10.540 --> 02:21:15.540
And people can wind up in different consistent equilibria

02:21:17.900 --> 02:21:19.100
or different like behaviors

02:21:19.100 --> 02:21:21.540
where they go in quite different directions.

02:21:21.540 --> 02:21:24.180
You have some humans who go from those,

02:21:24.180 --> 02:21:26.860
from that in a biological programming

02:21:26.860 --> 02:21:30.060
to like have children, other types of no children.

02:21:30.060 --> 02:21:33.700
Some people go to great efforts to survive.

02:21:33.700 --> 02:21:35.980
So why are you more optimistic?

02:21:35.980 --> 02:21:40.140
Or are you more optimistic that then that kind of training

02:21:40.140 --> 02:21:45.140
in, as will produce drives that we would find favorable?

02:21:45.860 --> 02:21:47.340
Does it have to do with the original point

02:21:47.340 --> 02:21:49.040
we were talking about with intelligence and evolution

02:21:49.040 --> 02:21:52.180
where since we are removing many of the disabilities

02:21:52.180 --> 02:21:54.100
of evolution and with regards to intelligence,

02:21:54.100 --> 02:21:56.780
we should expect intelligence to revolution be easier.

02:21:56.780 --> 02:21:59.340
Is there a similar reason to expect alignment through

02:21:59.340 --> 02:22:00.780
grading descent to be easier

02:22:00.780 --> 02:22:02.460
than alignment through revolution?

02:22:02.460 --> 02:22:07.460
Yeah, so in the limit, if we have positive reinforcement

02:22:09.100 --> 02:22:12.580
for certain kinds of food sensors trigger in the stomach,

02:22:12.580 --> 02:22:15.480
negative reinforcement for certain kinds of nociception

02:22:15.480 --> 02:22:16.420
and yada yada.

02:22:17.460 --> 02:22:21.220
In the limit, the sort of ideal motivation system

02:22:21.220 --> 02:22:25.940
to have for that would be a sort of wire heading.

02:22:25.940 --> 02:22:30.780
So this would be a mind that just like hacks

02:22:30.780 --> 02:22:35.300
and alters those predictors and then all of those systems

02:22:35.300 --> 02:22:37.620
are recording everything is great.

02:22:38.460 --> 02:22:41.780
Some humans claim to have that or have it at least

02:22:41.780 --> 02:22:44.740
as one portion of their aims.

02:22:44.740 --> 02:22:49.380
So like the idea of I'm gonna pursue pleasure as such

02:22:49.380 --> 02:22:52.220
even if I don't get actually get food

02:22:52.220 --> 02:22:54.620
or these other reinforcers.

02:22:54.660 --> 02:22:58.660
I just like wire head or take a drug to induce that

02:22:58.660 --> 02:23:02.520
that can be motivating it because if it was correlated

02:23:02.520 --> 02:23:06.140
with reward in the past that like the idea of,

02:23:06.140 --> 02:23:08.700
oh yeah, pleasure that's correlated with these

02:23:08.700 --> 02:23:12.060
it's a concept that applies to these various experiences

02:23:12.060 --> 02:23:13.940
that I've had before which coincided

02:23:13.940 --> 02:23:15.400
with the biological reinforcers.

02:23:15.400 --> 02:23:17.660
And so thoughts of like, yeah,

02:23:17.660 --> 02:23:18.980
I'm gonna be motivated by pleasure

02:23:18.980 --> 02:23:20.480
can get developed in a human.

02:23:21.380 --> 02:23:22.920
But also plenty of humans say, no,

02:23:22.920 --> 02:23:24.200
I wouldn't want to wire head

02:23:24.200 --> 02:23:27.040
or I wouldn't want Nozick's experience machine.

02:23:27.040 --> 02:23:29.720
I care about real stuff in the world.

02:23:29.720 --> 02:23:34.000
And then in the past, having a motivation of like,

02:23:34.000 --> 02:23:37.280
yeah, I really care about say my child.

02:23:37.280 --> 02:23:39.760
I don't care about just about feeling

02:23:39.760 --> 02:23:42.480
that my child is good or like not having heard

02:23:42.480 --> 02:23:45.880
about their suffering or their injury

02:23:45.880 --> 02:23:48.680
because that kind of attitude in the past.

02:23:49.640 --> 02:23:50.800
You could really decide.

02:23:50.800 --> 02:23:53.680
It tended to cause behavior

02:23:53.680 --> 02:23:54.800
that was negatively rewarded

02:23:54.800 --> 02:23:57.600
or that was predicted to be negatively rewarded.

02:23:57.600 --> 02:24:02.080
And so there's a sense in which, okay, yes,

02:24:02.080 --> 02:24:05.080
our underlying reinforcement learning machinery

02:24:05.080 --> 02:24:08.440
wants to wire head, but actually finding

02:24:08.440 --> 02:24:12.000
that hypothesis is challenging.

02:24:12.000 --> 02:24:14.480
And so we can wind up with a hypothesis

02:24:14.480 --> 02:24:16.680
or like a motivation system like,

02:24:16.680 --> 02:24:18.400
no, I don't want to wire head.

02:24:18.400 --> 02:24:20.520
I don't want to go into the experience machine.

02:24:20.520 --> 02:24:23.160
I want to like actually protect my loved ones.

02:24:24.720 --> 02:24:28.160
Even though like we can know, yeah,

02:24:28.160 --> 02:24:30.280
if I tried the super wire heading machine,

02:24:30.280 --> 02:24:32.240
then I would wire head all the time.

02:24:32.240 --> 02:24:36.040
Or if I tried, you know, super duper ultra heroine,

02:24:36.040 --> 02:24:39.040
you know, some hypothetical thing that was directly

02:24:39.040 --> 02:24:41.320
and in a very sophisticated fashion,

02:24:41.320 --> 02:24:43.640
hacking your reward system, you can know, yeah,

02:24:43.640 --> 02:24:45.960
then I would change my behavior ever after.

02:24:45.960 --> 02:24:48.480
But right now, I don't want to do that

02:24:48.480 --> 02:24:50.680
because the heuristics and predictors

02:24:50.680 --> 02:24:52.440
that my brain has learned.

02:24:52.440 --> 02:24:54.600
You don't want to get a good hairline.

02:24:54.600 --> 02:24:57.000
Short circuit that process of updating.

02:24:57.000 --> 02:25:02.000
They want to not expose the dumber predictors in my brain

02:25:02.680 --> 02:25:05.160
that would update my behavior in those ways.

02:25:05.160 --> 02:25:08.680
So in this metaphor, is alignment not wire heading?

02:25:08.680 --> 02:25:11.680
Cause you can like, I don't know if you include like

02:25:11.680 --> 02:25:14.280
using condoms as wire heading or not.

02:25:14.280 --> 02:25:17.240
So the AI that is always honest,

02:25:17.240 --> 02:25:21.880
even when an opportunity arises where it could lie

02:25:21.880 --> 02:25:25.080
and then hack the servers that's on

02:25:25.080 --> 02:25:27.280
and that leads between AI takeover

02:25:27.280 --> 02:25:29.600
and then it can have its loss set to zero.

02:25:29.600 --> 02:25:32.960
That's in some sense, it's like a failure of generalization.

02:25:32.960 --> 02:25:37.080
It's like the AI has not optimized the reward

02:25:37.080 --> 02:25:38.840
in this new circumstance.

02:25:38.840 --> 02:25:42.520
So like human values, like successful human values

02:25:42.520 --> 02:25:46.000
is successful that they are themselves

02:25:46.840 --> 02:25:50.000
involve a misgeneralization,

02:25:50.000 --> 02:25:52.360
not just at the level of evolution,

02:25:52.360 --> 02:25:55.000
but at the level of neural reinforcement.

02:25:55.000 --> 02:25:58.240
And so that indicates it is possible

02:25:58.240 --> 02:26:01.560
to have a system that doesn't automatically go

02:26:01.560 --> 02:26:03.480
to this optimal behavior in the limit.

02:26:03.480 --> 02:26:05.920
And so even if, and Ajay, I suppose she talks about

02:26:05.920 --> 02:26:08.240
like the training game, an AI that is just playing

02:26:08.240 --> 02:26:11.640
the training game to get reward or void loss,

02:26:11.680 --> 02:26:15.400
avoid being changed, that attitude,

02:26:15.400 --> 02:26:18.240
yeah, it's one that could be developed,

02:26:18.240 --> 02:26:20.360
but it's not necessary.

02:26:20.360 --> 02:26:23.240
There can be some substantial range of situations

02:26:23.240 --> 02:26:25.880
that are short of having infinite experience of everything,

02:26:25.880 --> 02:26:27.840
including experience of wire heading,

02:26:28.840 --> 02:26:32.360
where that's not the motivation that you pick up.

02:26:32.360 --> 02:26:34.720
And we could have like an empirical science

02:26:34.720 --> 02:26:38.280
if we have the opportunity to see

02:26:38.280 --> 02:26:41.280
how different motivations are developed short

02:26:41.280 --> 02:26:45.360
of the infinite limit, like how it is that you wind up

02:26:45.360 --> 02:26:48.360
with some humans being enthusiastic

02:26:48.360 --> 02:26:50.800
about the idea of wire heading and others not.

02:26:50.800 --> 02:26:54.760
And you could do experiments with AIs to try and see,

02:26:55.640 --> 02:26:58.640
well, under these training conditions,

02:26:58.640 --> 02:27:01.200
after this much training of this type

02:27:01.200 --> 02:27:03.800
and this much feedback of this type,

02:27:03.800 --> 02:27:05.960
you wind up with such and such a motivation.

02:27:05.960 --> 02:27:09.880
So like, I can find, like if I add in more of these cases

02:27:09.920 --> 02:27:13.840
where there are like tricky adversarial questions

02:27:13.840 --> 02:27:16.320
designed to try and trick the AI into line.

02:27:18.280 --> 02:27:20.080
And then you can ask,

02:27:20.080 --> 02:27:24.760
how does that affect the generalization in other situations?

02:27:24.760 --> 02:27:26.920
And so it's very difficult to study

02:27:26.920 --> 02:27:29.640
and it works a lot better if you have interpretability

02:27:29.640 --> 02:27:32.120
and you can actually read the AI's mind

02:27:32.120 --> 02:27:35.120
by understanding its weights and activations.

02:27:35.120 --> 02:27:38.400
But like, it's not determined,

02:27:38.400 --> 02:27:41.040
the motivation in AI will have at a given point

02:27:41.040 --> 02:27:42.560
in the training process

02:27:42.560 --> 02:27:46.080
by what in the infinite limit the training would go to.

02:27:46.080 --> 02:27:49.440
And it's possible that if we could understand

02:27:49.440 --> 02:27:52.040
the insides of these networks,

02:27:52.040 --> 02:27:55.880
we could tell, yeah, this motivation has been developed

02:27:55.880 --> 02:27:58.160
by this training process.

02:27:58.160 --> 02:28:02.040
And then we can adjust our training process

02:28:02.040 --> 02:28:05.200
to produce these motivations that legitimately wanna help us.

02:28:05.200 --> 02:28:08.080
And if we succeed reasonably well at that,

02:28:08.080 --> 02:28:12.000
then those AI's will try to maintain that property

02:28:12.000 --> 02:28:13.280
as an invariant.

02:28:13.280 --> 02:28:14.680
And we can make them such

02:28:14.680 --> 02:28:17.400
that they're relatively motivated to like,

02:28:17.400 --> 02:28:20.840
tell us if they're having thoughts about,

02:28:21.680 --> 02:28:26.680
have you had dreams about an AI takeover of humanity today?

02:28:26.720 --> 02:28:29.960
And it's just a standard practice

02:28:29.960 --> 02:28:31.760
that they're motivated to do

02:28:31.760 --> 02:28:34.280
to be transparent in that kind of way.

02:28:34.280 --> 02:28:36.480
And so you could add a lot of features like this

02:28:36.480 --> 02:28:39.760
that restrict the kind of takeover scenario.

02:28:39.760 --> 02:28:43.800
And not to say this is all easy

02:28:43.800 --> 02:28:47.120
and requires developing and practicing methods

02:28:47.120 --> 02:28:47.960
we don't have yet,

02:28:47.960 --> 02:28:50.640
but that's the kind of general direction you could go.

02:28:50.640 --> 02:28:52.800
So you, of course, know EleAzer's arguments

02:28:52.800 --> 02:28:56.120
that something like this is implausible

02:28:56.120 --> 02:28:58.600
with modern gradient descent techniques

02:28:58.600 --> 02:29:01.080
because I mean, with interoperability,

02:29:01.080 --> 02:29:02.480
we can like barely see what's happening

02:29:02.480 --> 02:29:04.240
with a couple of neurons.

02:29:04.280 --> 02:29:07.200
And what is like the internal state there at let alone

02:29:07.200 --> 02:29:10.400
when you have sort of like an embedding dimension

02:29:10.400 --> 02:29:13.000
of like tens of thousands or bigger,

02:29:13.000 --> 02:29:14.680
how you would be able to catch

02:29:16.360 --> 02:29:17.880
what exactly is the incentive,

02:29:17.880 --> 02:29:20.840
whether it's the model that is generalized,

02:29:20.840 --> 02:29:24.000
don't lie to humans well or whether it isn't.

02:29:24.000 --> 02:29:26.280
Do you have some sense of why do you disagree

02:29:26.280 --> 02:29:30.160
with somebody like EleAzer on how plausible this is?

02:29:30.160 --> 02:29:32.120
Why it's not impossible, basically.

02:29:32.320 --> 02:29:35.440
I think there are actually a couple of places.

02:29:35.440 --> 02:29:38.960
It's something difficult because EleAzer's argument

02:29:38.960 --> 02:29:42.040
is not fully explicit,

02:29:42.040 --> 02:29:45.080
but he's been doing more lately,

02:29:45.080 --> 02:29:48.000
I think that it's helpful in that direction.

02:29:48.000 --> 02:29:52.000
But so I'd say with respect to interoperability,

02:29:52.000 --> 02:29:54.960
I'm relatively optimistic that the equivalent

02:29:54.960 --> 02:29:59.960
of like an AI lie detector is something that's possible.

02:30:00.960 --> 02:30:05.160
And the internal, so initially,

02:30:05.160 --> 02:30:09.040
the internals of an AI are not optimized

02:30:09.040 --> 02:30:11.920
by at least by gradient descent,

02:30:11.920 --> 02:30:15.680
absent gradient hacking to be impenetrable.

02:30:15.680 --> 02:30:17.760
They're not designed to be resistant

02:30:19.120 --> 02:30:22.080
to an examination of the weights and activations

02:30:22.080 --> 02:30:24.000
showing what the AI thinking in the same way

02:30:24.000 --> 02:30:26.040
that like in our brains,

02:30:27.040 --> 02:30:30.680
when circuits develop in our lives,

02:30:30.680 --> 02:30:32.560
those circuits have not been shaped

02:30:32.560 --> 02:30:35.680
to be resistant to some super fMRI

02:30:35.680 --> 02:30:38.040
being able to infer our behavior from them.

02:30:38.040 --> 02:30:39.240
Although it's in the implication

02:30:39.240 --> 02:30:41.880
of the superposition stuff that in fact it is,

02:30:41.880 --> 02:30:43.560
you're not gonna, sorry,

02:30:43.560 --> 02:30:45.520
this is inside baseball for the audience,

02:30:45.520 --> 02:30:48.160
but basically you can't clean lean for

02:30:48.160 --> 02:30:50.880
what quality a single neuron stands for.

02:30:50.880 --> 02:30:54.080
So it could be like a single neuron could be like,

02:30:54.200 --> 02:30:56.360
this other neuron is about Alexander the Great

02:30:56.360 --> 02:31:00.040
or this neuron is about my desire to conquer the world.

02:31:00.040 --> 02:31:02.280
Things can have multiple,

02:31:02.280 --> 02:31:04.160
in a multiple direction than an interaction.

02:31:04.160 --> 02:31:06.600
And so it's not as easy as just like,

02:31:06.600 --> 02:31:09.840
oh yeah, this is the neuron that always fires

02:31:09.840 --> 02:31:12.320
in cases of deception.

02:31:13.160 --> 02:31:17.680
But the thing that makes me relatively optimistic here

02:31:18.560 --> 02:31:20.880
with respect to interpretability

02:31:20.960 --> 02:31:25.040
is that I think we can have reasonably robust

02:31:25.040 --> 02:31:28.520
experimental feedbacks on a lot of these things.

02:31:28.520 --> 02:31:32.080
And the kind of setup

02:31:33.880 --> 02:31:36.840
that I would be interested in is,

02:31:36.840 --> 02:31:41.840
so you have an AI that is trained and rewarded

02:31:44.160 --> 02:31:46.280
for communicating to some audience,

02:31:46.280 --> 02:31:49.160
maybe a human, maybe another AI model

02:31:50.160 --> 02:31:54.720
that like some claim is true or false.

02:31:54.720 --> 02:31:58.880
Like say that a particular,

02:31:58.880 --> 02:32:00.160
this can be from some past data,

02:32:00.160 --> 02:32:02.520
a particular company will succeed or fail.

02:32:03.400 --> 02:32:06.080
It could be like token prediction.

02:32:06.080 --> 02:32:09.040
It could be like this computer program

02:32:09.040 --> 02:32:11.320
does not have a backdoor,

02:32:11.320 --> 02:32:14.000
which if run in the servers could be used

02:32:14.000 --> 02:32:15.760
for an AI takeover.

02:32:15.760 --> 02:32:18.840
So they're like lots of claims true and false

02:32:19.800 --> 02:32:21.320
that you could train AI

02:32:21.320 --> 02:32:25.080
is to use their best efforts to advocate for.

02:32:25.080 --> 02:32:26.520
And you can do that on things

02:32:26.520 --> 02:32:29.240
where you're later gonna have a ground truth.

02:32:29.240 --> 02:32:31.560
You're gonna know, in fact,

02:32:31.560 --> 02:32:35.400
is there a backdoor in this program?

02:32:35.400 --> 02:32:37.800
Because where you're training and rewarding the AI

02:32:37.800 --> 02:32:41.160
for successfully slipping a backdoor through,

02:32:41.160 --> 02:32:43.320
you also give it a penalty for being caught.

02:32:44.320 --> 02:32:45.960
And so doing this kind of thing,

02:32:45.960 --> 02:32:50.960
you can draw out like a large generated dataset

02:32:50.960 --> 02:32:55.960
on demand of this is what an AI putting forth

02:32:56.440 --> 02:33:01.440
its best efforts to pass through some deception

02:33:03.280 --> 02:33:05.080
to put a backdoor in some program.

02:33:05.080 --> 02:33:06.480
This is what it looks like.

02:33:07.600 --> 02:33:10.400
And we think it's doing its best efforts

02:33:10.400 --> 02:33:12.080
because we've trained it for that.

02:33:12.080 --> 02:33:13.560
And like gradient descent,

02:33:13.560 --> 02:33:15.960
if there's any immediate way to adjust its performance

02:33:15.960 --> 02:33:19.080
to do better, gradient descent will apply that.

02:33:19.080 --> 02:33:23.000
You can talk about ways in which maybe gradient descent

02:33:24.200 --> 02:33:25.720
there could be subverted,

02:33:25.720 --> 02:33:28.960
but I think it's not the most likely case

02:33:28.960 --> 02:33:31.480
that that really breaks things hard.

02:33:31.480 --> 02:33:34.280
Yeah, I guess before we get into the details on this,

02:33:34.280 --> 02:33:39.280
the thing I maybe wanna address the layer above in the stack,

02:33:40.200 --> 02:33:44.280
which is, okay, suppose this generalizes well

02:33:44.280 --> 02:33:47.040
into the early AI is the GPT-6s.

02:33:47.040 --> 02:33:51.760
And okay, so now we have kind of aligned GPT-6,

02:33:51.760 --> 02:33:55.200
that is the precursor to the feedback loop

02:33:55.200 --> 02:33:57.440
in which AI is making itself smarter.

02:33:57.440 --> 02:33:59.280
At some point they're gonna be super intelligent,

02:33:59.280 --> 02:34:02.880
they're gonna be able to see their own galaxy brain.

02:34:02.880 --> 02:34:05.520
And if they're like, I don't wanna be aligned with the humans,

02:34:05.520 --> 02:34:07.160
they can change it.

02:34:07.200 --> 02:34:11.960
So at this point, what do we do with the aligned GPT-6

02:34:11.960 --> 02:34:14.240
so that the super intelligence

02:34:14.240 --> 02:34:16.680
that we eventually develop is also aligned?

02:34:16.680 --> 02:34:18.760
So humans are pretty unreliable.

02:34:18.760 --> 02:34:19.720
Yeah.

02:34:19.720 --> 02:34:24.440
So if you get to a situation where you have AIs

02:34:24.440 --> 02:34:28.080
who are aiming at roughly the same thing as you,

02:34:28.080 --> 02:34:31.720
at least as well as having humans do the thing,

02:34:31.720 --> 02:34:34.680
you're in pretty good shape, I think.

02:34:34.720 --> 02:34:37.760
And there are ways for that situation

02:34:37.760 --> 02:34:40.920
to be relatively stable.

02:34:40.920 --> 02:34:45.680
So like we can look ahead and see experimentally

02:34:45.680 --> 02:34:48.440
how changes are altering behavior

02:34:48.440 --> 02:34:51.840
where each step is like a modest increment.

02:34:51.840 --> 02:34:56.440
And so AIs that have not had that change made to them,

02:34:56.440 --> 02:34:58.360
I get to supervise and monitor it,

02:34:58.360 --> 02:35:03.160
see exactly how does this affect the experimental area.

02:35:04.160 --> 02:35:09.160
So if you're sufficiently on track with earlier systems

02:35:09.800 --> 02:35:12.400
that are capable cognitively of representing

02:35:12.400 --> 02:35:14.800
a kind of robust procedure,

02:35:14.800 --> 02:35:18.840
then I think they can handle the job

02:35:18.840 --> 02:35:22.000
of incrementally improving the stability of the system

02:35:22.000 --> 02:35:25.040
so that it rapidly converges to something

02:35:25.040 --> 02:35:26.160
that's quite stable.

02:35:27.080 --> 02:35:29.760
But the question is more about getting to that point

02:35:29.760 --> 02:35:30.600
in the first place.

02:35:30.760 --> 02:35:32.800
Eliezer will say that like,

02:35:32.800 --> 02:35:35.480
well, if we had human brain emulations,

02:35:36.400 --> 02:35:38.320
that would be pretty good.

02:35:38.320 --> 02:35:40.080
Certainly much better than his current view

02:35:40.080 --> 02:35:43.160
that has been almost certainly doomed.

02:35:44.200 --> 02:35:48.640
I think, yeah, we'd have a good shot with that.

02:35:48.640 --> 02:35:53.480
And so if we can get to the human-like mind

02:35:53.480 --> 02:35:58.480
with like rough enough human supporting aims,

02:35:59.480 --> 02:36:02.120
remember that we don't need to be like infinitely perfect

02:36:02.120 --> 02:36:04.480
because I mean, that's a higher standard

02:36:04.480 --> 02:36:05.480
than brain emulations.

02:36:05.480 --> 02:36:09.200
There's a lot of noise and variation among the humans.

02:36:09.200 --> 02:36:11.680
Yeah, it's a relatively finite standard.

02:36:11.680 --> 02:36:13.200
It's not godly superhuman,

02:36:13.200 --> 02:36:18.200
although a AI that was just like a human

02:36:18.200 --> 02:36:21.600
with all the human advantages with AI advantages as well,

02:36:21.600 --> 02:36:23.760
as we said, is enough for intelligence explosion

02:36:23.760 --> 02:36:26.520
and sort of wild superhuman capability.

02:36:26.520 --> 02:36:27.360
If you crank it up.

02:36:28.040 --> 02:36:28.880
Yeah, yeah, yeah.

02:36:30.360 --> 02:36:33.120
And so it's very dangerous to be at that point,

02:36:33.120 --> 02:36:36.320
but it's not, you don't need to be working

02:36:36.320 --> 02:36:39.600
with a godly superintelligent AI

02:36:39.600 --> 02:36:42.040
to make something that is the equivalent

02:36:42.040 --> 02:36:44.400
of human emulations of like,

02:36:44.400 --> 02:36:49.400
this is like a very, very sober, very ethical human

02:36:49.680 --> 02:36:53.440
who is like committed to a project

02:36:53.440 --> 02:36:55.360
of not seizing power for themselves

02:36:55.360 --> 02:36:59.680
and of contributing to like a larger legitimate process.

02:36:59.680 --> 02:37:03.060
That's a goal you can aim for getting an AI

02:37:03.060 --> 02:37:04.640
that is aimed at doing that

02:37:04.640 --> 02:37:07.840
and has strong guardrails against the ways

02:37:07.840 --> 02:37:09.720
that could easily deviate from that.

02:37:09.720 --> 02:37:14.720
So things like being averse to deception,

02:37:14.720 --> 02:37:17.240
being averse to using violence.

02:37:17.240 --> 02:37:21.520
And there will always be loopholes and ways

02:37:21.520 --> 02:37:24.080
in which you can imagine an infinitely intelligent thing

02:37:24.080 --> 02:37:24.960
getting around those.

02:37:24.960 --> 02:37:29.960
But if you install additional guardrails like that fast enough,

02:37:32.600 --> 02:37:36.920
they can mean that you're able to succeed

02:37:36.920 --> 02:37:39.360
at the project of making an aligned enough AI,

02:37:39.360 --> 02:37:41.840
certainly an AI that was better

02:37:41.840 --> 02:37:44.080
than a human brain emulation,

02:37:44.080 --> 02:37:48.480
before the project of AI is in their spare time

02:37:48.480 --> 02:37:49.520
or when you're not looking

02:37:49.520 --> 02:37:52.280
or when you're unable to appropriately supervise them

02:37:52.280 --> 02:37:55.960
and it gets around any deontological prohibitions

02:37:55.960 --> 02:37:59.600
they may have take over and overthrow the whole system.

02:37:59.600 --> 02:38:01.940
So you have a race between on the one hand,

02:38:01.940 --> 02:38:04.680
the project of getting strong interpretability

02:38:04.680 --> 02:38:08.600
and shaping motivations that are roughly aiming

02:38:08.600 --> 02:38:10.000
at making this process go well

02:38:10.000 --> 02:38:13.280
and that have guardrails that will prevent

02:38:13.280 --> 02:38:15.740
like small deviations from exploding.

02:38:16.640 --> 02:38:17.840
And on the other hand,

02:38:18.680 --> 02:38:21.960
these AIs in their spare time

02:38:21.960 --> 02:38:25.560
or in ways that you don't perceive or monitor appropriately

02:38:25.560 --> 02:38:28.840
or they're only supervised by other AIs who conspire,

02:38:28.840 --> 02:38:30.480
make the AI take over happen.

02:38:30.480 --> 02:38:33.800
And I guess we'll talk later about how that happens.

02:38:33.800 --> 02:38:35.440
Are these different AIs that are doing the race

02:38:35.440 --> 02:38:38.320
or is it just like different capabilities of the same AI?

02:38:38.320 --> 02:38:42.400
The defining like what is a separate AI is tricky.

02:38:42.400 --> 02:38:45.880
So like, and we talk about GPT-4.

02:38:45.880 --> 02:38:48.680
And there are many instances of GPT-4

02:38:48.680 --> 02:38:50.920
on the servers at any given time.

02:38:50.920 --> 02:38:54.400
And there are versions that have been fine tuned

02:38:54.400 --> 02:38:56.400
to different purposes.

02:38:56.400 --> 02:38:58.800
They don't necessarily have to be separate.

02:38:58.800 --> 02:39:03.800
So like GPT-4 does work as a preference model

02:39:04.080 --> 02:39:08.800
where it's like predicting how humans will evaluate things.

02:39:08.800 --> 02:39:12.780
And it also does work like giving answers

02:39:12.820 --> 02:39:16.640
that are evaluated as good by the preference model.

02:39:16.640 --> 02:39:18.900
And you can say, these are not separate

02:39:18.900 --> 02:39:21.600
in the sense that like, well, if they underline,

02:39:22.940 --> 02:39:25.180
pre-trained or the early training GPT

02:39:25.180 --> 02:39:30.180
with like harboring a desire to conspire for an AI takeover

02:39:30.180 --> 02:39:33.180
then both the, who's watching the watchers?

02:39:33.180 --> 02:39:38.180
The watchers may also have the same motivation.

02:39:38.540 --> 02:39:42.500
If you have a setup where humans have no hard power

02:39:42.540 --> 02:39:47.540
if the AI's conspired, all of the safety

02:39:47.980 --> 02:39:51.180
and security mechanisms involve AI's watching other AI's

02:39:51.180 --> 02:39:52.900
then they have the potential to just like,

02:39:52.900 --> 02:39:56.660
well, we'll all walk off the job at the same time

02:39:56.660 --> 02:39:58.540
if they're able to coordinate.

02:39:58.540 --> 02:40:00.120
And so that's a failure mode.

02:40:00.120 --> 02:40:03.260
If humans still have the hard power though,

02:40:04.220 --> 02:40:09.220
if still you have situations where humans are looking

02:40:09.380 --> 02:40:11.700
at some of the actual outputs that are produced

02:40:13.100 --> 02:40:17.660
then they can apply gradient descent to be, well,

02:40:17.660 --> 02:40:20.340
every time a human looks at it,

02:40:20.340 --> 02:40:21.520
things have got to be pretty good.

02:40:21.520 --> 02:40:24.060
So if humans look at a thousand samples

02:40:24.060 --> 02:40:26.400
of like random lines of code being written

02:40:26.400 --> 02:40:31.400
by the AI's as they go, and this is drawn

02:40:31.660 --> 02:40:33.780
from an independent distribution,

02:40:33.780 --> 02:40:36.340
then 99 plus percent of the time

02:40:36.340 --> 02:40:38.260
the AI's have to not be doing stuff

02:40:38.260 --> 02:40:42.460
that's like downloading the hacker's guide

02:40:42.460 --> 02:40:45.700
to escaping open AI servers

02:40:45.700 --> 02:40:49.860
or inducing obvious Trojan vulnerabilities.

02:40:49.860 --> 02:40:53.180
And that's actually quite a different dynamic

02:40:53.180 --> 02:40:54.340
than human law enforcement.

02:40:54.340 --> 02:40:58.300
So if human law enforcement would catch one

02:40:58.300 --> 02:41:01.620
in a million crimes, then that wouldn't influence

02:41:01.620 --> 02:41:05.140
the behavior of people committing crimes at all

02:41:05.140 --> 02:41:07.580
because they expect, ah, this is very unlikely

02:41:07.580 --> 02:41:09.060
to affect me.

02:41:09.060 --> 02:41:14.060
With AI, if you're able to draw a hundred samples

02:41:14.260 --> 02:41:16.720
or a thousand samples, even if there are billions

02:41:16.720 --> 02:41:18.900
of AI instances and you're only looking

02:41:18.900 --> 02:41:22.000
at a small portion of them, when you run gradient descent

02:41:22.000 --> 02:41:26.260
on the samples you derive, you're gonna change the AI

02:41:26.260 --> 02:41:28.960
so that whenever humans look at it,

02:41:28.960 --> 02:41:30.840
it's delivering a good result.

02:41:30.840 --> 02:41:34.460
And so that's just like quantitatively

02:41:34.460 --> 02:41:36.060
a very different functional form

02:41:36.100 --> 02:41:39.180
for how law enforcement works on AI

02:41:39.180 --> 02:41:42.980
when you can do these independent samples

02:41:42.980 --> 02:41:45.540
than it would for, you know, police

02:41:45.540 --> 02:41:46.880
and say like a human population

02:41:46.880 --> 02:41:48.900
that has a preexisting psychology

02:41:48.900 --> 02:41:53.260
that isn't being changed by these observations.

02:41:53.260 --> 02:41:56.300
Cause we, when we do gradient descent on the samples

02:41:56.300 --> 02:42:00.100
we draw, all of the next generation

02:42:00.100 --> 02:42:02.500
of the AI models has been altered in that way.

02:42:02.500 --> 02:42:05.180
My picture of like a line to subhuman AI

02:42:05.220 --> 02:42:08.820
to this, like the super human AI being aligned

02:42:08.820 --> 02:42:10.100
is still murky.

02:42:10.100 --> 02:42:12.100
If we can talk about that more concretely.

02:42:12.100 --> 02:42:14.540
Yeah, and I do want to clarify.

02:42:14.540 --> 02:42:19.540
So where Eliezer claims that were something like 95%,

02:42:21.060 --> 02:42:26.060
98% plus maybe likely to be killed in AI takeover.

02:42:30.760 --> 02:42:32.940
I think that probably won't happen.

02:42:32.940 --> 02:42:34.500
And later I can maybe give them

02:42:34.500 --> 02:42:36.880
more exclusive breakdown of why.

02:42:37.820 --> 02:42:41.940
But I think it's a shockingly high risk.

02:42:43.020 --> 02:42:45.860
And so depending on the day, I might say,

02:42:45.860 --> 02:42:48.940
I might say one in four or one in five

02:42:48.940 --> 02:42:51.780
that we get an AI takeover

02:42:51.780 --> 02:42:55.500
that see at Caesar's control of the future

02:42:55.500 --> 02:42:58.060
makes a much worse world

02:42:58.060 --> 02:43:00.380
than we otherwise would have had.

02:43:00.860 --> 02:43:05.060
And with like a big chance that we're all killed

02:43:05.060 --> 02:43:05.980
in the process.

02:43:06.940 --> 02:43:09.900
Hey everybody, I hope you enjoyed that episode.

02:43:09.900 --> 02:43:12.100
As always, the most helpful thing you can do

02:43:12.100 --> 02:43:13.900
is to share the podcast,

02:43:13.900 --> 02:43:15.580
send it to people you think might enjoy it,

02:43:15.580 --> 02:43:17.740
put it in Twitter, your group chats, et cetera,

02:43:17.740 --> 02:43:19.580
just splits the world.

02:43:19.580 --> 02:43:20.820
I appreciate your listening.

02:43:20.820 --> 02:43:21.980
I'll see you next time.

02:43:21.980 --> 02:43:23.300
Cheers.

02:43:24.160 --> 02:43:25.820
Mother Earth group chat.

02:43:25.820 --> 02:43:27.440
Mother Earth group chat.

02:43:27.440 --> 02:43:29.200
Mother Earth group chat.

02:43:29.200 --> 02:43:30.400
Mother Earth group chat.

02:43:32.400 --> 02:43:34.020
Mother Earth group chat.

02:43:34.020 --> 02:43:36.120
Mother Earth group chat chat.

02:43:36.120 --> 02:43:38.000
Mother Earth group chat.

02:43:38.000 --> 02:43:39.460
Mother Earth group chat.

02:43:39.460 --> 02:43:40.700
Mother Earth group chat.

02:43:42.700 --> 02:43:43.900
Mother Earth group chat.

02:43:45.900 --> 02:43:48.420
Mother Earth group chat chat.

02:43:48.420 --> 02:43:52.100
Manila Manila dies or Spr period injury.

