1
00:00:00,000 --> 00:00:03,000
LLMs are very good at memorizing static programs.

2
00:00:03,000 --> 00:00:05,880
If you scale up the size of your database,

3
00:00:05,880 --> 00:00:09,160
you are not increasing the intelligence of the system

4
00:00:09,160 --> 00:00:09,640
one bit.

5
00:00:09,640 --> 00:00:11,520
I feel like you're using words like memorization, which

6
00:00:11,520 --> 00:00:12,960
we would never use for human children.

7
00:00:12,960 --> 00:00:16,120
If they can just solve any arbitrary algebraic problem,

8
00:00:16,120 --> 00:00:17,920
you wouldn't say they've memorized algebra.

9
00:00:17,920 --> 00:00:19,360
They'd say they've learned algebra.

10
00:00:19,360 --> 00:00:20,880
So you've got a million dollar price pool,

11
00:00:20,880 --> 00:00:23,800
and there's a $500,000 price for the first team that

12
00:00:23,800 --> 00:00:25,920
can get to the 85% benchmark.

13
00:00:25,920 --> 00:00:29,000
If ARC survives three months from here, we'll pull up the price.

14
00:00:29,000 --> 00:00:32,120
Open AI basically set back progress towards HGI

15
00:00:32,120 --> 00:00:33,800
by probably like five to 10 years.

16
00:00:33,800 --> 00:00:35,880
They caused this complete closing down

17
00:00:35,880 --> 00:00:37,320
of frontier research publishing.

18
00:00:37,320 --> 00:00:41,400
And now LLMs have sucked the oxygen out of the room,

19
00:00:41,400 --> 00:00:44,000
like everyone is just doing LLMs.

20
00:00:44,000 --> 00:00:46,880
OK, today I have the pleasure to speak

21
00:00:46,880 --> 00:00:51,600
with Francois Chollet, who is a AI researcher at Google

22
00:00:51,600 --> 00:00:53,280
and creator of Keras.

23
00:00:53,280 --> 00:00:56,360
And he's launching a prize in collaboration

24
00:00:56,360 --> 00:00:58,560
with Mike Canouf, the co-founder of Xavier,

25
00:00:58,560 --> 00:01:00,280
who we'll also be talking to in a second,

26
00:01:00,280 --> 00:01:03,840
a million dollar prize to solve the ARC benchmark

27
00:01:03,840 --> 00:01:04,720
that he created.

28
00:01:04,720 --> 00:01:06,960
So first question, what is the ARC benchmark,

29
00:01:06,960 --> 00:01:08,440
and why do we even need this prize?

30
00:01:08,440 --> 00:01:10,120
Why won't the biggest LLM we have in a year

31
00:01:10,120 --> 00:01:11,800
be able to just saturate it?

32
00:01:11,800 --> 00:01:12,360
Sure.

33
00:01:12,360 --> 00:01:15,120
So ARC is intended as a kind of IQ test

34
00:01:15,120 --> 00:01:16,640
for machine intelligence.

35
00:01:16,640 --> 00:01:20,080
And what makes it different from most LLM benchmarks out there

36
00:01:20,080 --> 00:01:23,800
is that it's designed to be resistant to memorization.

37
00:01:23,800 --> 00:01:25,880
So if you look at the way LLMs work,

38
00:01:25,880 --> 00:01:29,280
they're basically this big interpolative memory.

39
00:01:29,280 --> 00:01:31,480
And the way you scale up their capabilities

40
00:01:31,480 --> 00:01:35,080
is by trying to cram as much knowledge and patterns

41
00:01:35,080 --> 00:01:36,880
as possible into them.

42
00:01:36,880 --> 00:01:41,440
And by contrast, ARC does not require a lot of knowledge

43
00:01:41,440 --> 00:01:42,240
at all.

44
00:01:42,240 --> 00:01:43,960
It's designed to only require what's

45
00:01:43,960 --> 00:01:48,360
known as core knowledge, which is basic knowledge about things

46
00:01:48,360 --> 00:01:52,360
like elementary physics, objectness, counting,

47
00:01:52,360 --> 00:01:54,640
that sort of thing, the sort of knowledge

48
00:01:54,640 --> 00:01:59,200
that any four-year-old or five-year-old possesses.

49
00:01:59,200 --> 00:02:02,600
But what's interesting is that each puzzle in ARC

50
00:02:02,600 --> 00:02:06,080
is novel, is something that you've probably not encountered

51
00:02:06,080 --> 00:02:09,680
before, even if you've memorized the entire internet.

52
00:02:09,680 --> 00:02:15,800
And that's what makes ARC challenging for LLMs.

53
00:02:15,800 --> 00:02:19,400
And so far, LLMs have not been doing very well on it.

54
00:02:19,400 --> 00:02:21,720
In fact, the approaches that are working well

55
00:02:21,720 --> 00:02:24,400
are more towards discrete program search, program

56
00:02:24,400 --> 00:02:25,760
synthesis.

57
00:02:25,760 --> 00:02:28,320
So first of all, I'll make a comment

58
00:02:28,320 --> 00:02:30,720
that I'm glad that as a skeptic of LLM,

59
00:02:30,720 --> 00:02:35,640
you have put out yourself a benchmark that is it accurate

60
00:02:35,640 --> 00:02:38,880
to say that, suppose that the biggest model we have in a year

61
00:02:38,880 --> 00:02:41,800
is able to get 80% on this, then your view would be

62
00:02:41,800 --> 00:02:43,880
we are on track to AGI with LLMs.

63
00:02:43,880 --> 00:02:45,240
How would you think about that?

64
00:02:45,240 --> 00:02:47,400
Right.

65
00:02:47,400 --> 00:02:48,840
I'm pretty skeptical that we're going

66
00:02:48,840 --> 00:02:51,600
to see LLM do 80% in a year.

67
00:02:51,600 --> 00:02:53,720
That said, if we do see it, you would also

68
00:02:53,720 --> 00:02:56,120
have to look at how this was achieved.

69
00:02:56,120 --> 00:03:00,320
If you just train the model and millions or billions

70
00:03:00,320 --> 00:03:02,440
of puzzles similar to ARC so that you're

71
00:03:02,440 --> 00:03:07,440
relying on the ability to have some overlap between the tasks

72
00:03:07,440 --> 00:03:08,800
that you train on and the tasks that you're

73
00:03:08,800 --> 00:03:10,520
going to see at this time, then you're still

74
00:03:10,520 --> 00:03:12,480
using memorization.

75
00:03:12,480 --> 00:03:14,440
And maybe it can work.

76
00:03:14,440 --> 00:03:17,520
Hopefully, ARC is going to be good enough

77
00:03:17,560 --> 00:03:20,520
that it's going to be resistant to this sort of attempt

78
00:03:20,520 --> 00:03:22,280
at brute forcing.

79
00:03:22,280 --> 00:03:23,280
But you never know.

80
00:03:23,280 --> 00:03:24,560
Maybe it could happen.

81
00:03:24,560 --> 00:03:26,320
I'm not saying it's not going to happen.

82
00:03:26,320 --> 00:03:28,200
ARC is not a perfect benchmark.

83
00:03:28,200 --> 00:03:30,200
Maybe it has flaws.

84
00:03:30,200 --> 00:03:33,120
Maybe it could be hacked in that way.

85
00:03:33,120 --> 00:03:37,120
So I guess I'm curious about what would GPTI

86
00:03:37,120 --> 00:03:40,760
have to do that you're very confident that it's

87
00:03:40,760 --> 00:03:42,160
on the path to AGI?

88
00:03:42,160 --> 00:03:44,720
What would make me change my mind about LLMs

89
00:03:44,720 --> 00:03:49,920
is basically, if I start seeing a critical mass of cases

90
00:03:49,920 --> 00:03:52,120
where you show the model with something

91
00:03:52,120 --> 00:03:55,080
it has not seen before, a task that's actually

92
00:03:55,080 --> 00:03:57,920
novel from the perspective of its training data, something

93
00:03:57,920 --> 00:03:59,960
that's not in the training data, and if it can actually

94
00:03:59,960 --> 00:04:02,880
adapt on the fly.

95
00:04:02,880 --> 00:04:03,960
And this is true for LLMs.

96
00:04:03,960 --> 00:04:06,080
But really, this would catch my attention

97
00:04:06,080 --> 00:04:08,840
with any for any AI technique out there.

98
00:04:08,840 --> 00:04:13,600
If I can see the ability to adapt to novelty on the fly

99
00:04:13,600 --> 00:04:15,600
to pick up new skills efficiently,

100
00:04:15,600 --> 00:04:18,040
then I would be extremely interested.

101
00:04:18,040 --> 00:04:21,440
I would think this is on the path to AGI.

102
00:04:21,440 --> 00:04:24,680
So the advantage they have is that they do get to see everything.

103
00:04:24,680 --> 00:04:27,640
Maybe I'll take issue with how much they are relying on that.

104
00:04:27,640 --> 00:04:29,040
But let's suppose that they are relying.

105
00:04:29,040 --> 00:04:32,200
Obviously, they're relying on that more than humans do.

106
00:04:32,200 --> 00:04:35,560
To the extent that they do have so much indistribution,

107
00:04:35,560 --> 00:04:37,840
to the extent that we have trouble distinguishing

108
00:04:37,840 --> 00:04:41,520
whether an example is indistribution or not,

109
00:04:41,520 --> 00:04:43,240
well, if they have everything in distribution,

110
00:04:43,240 --> 00:04:45,280
then they can do everything that we can do.

111
00:04:45,280 --> 00:04:47,720
Maybe it's not indistribution for us.

112
00:04:47,720 --> 00:04:50,160
Why is it so crucial that it has to be out of distribution

113
00:04:50,160 --> 00:04:51,520
for them?

114
00:04:51,520 --> 00:04:52,920
Why can't we just leverage the fact

115
00:04:52,920 --> 00:04:54,440
that they do get to see everything?

116
00:04:54,440 --> 00:04:55,680
Right.

117
00:04:55,680 --> 00:04:57,560
You're asking basically what's the difference

118
00:04:57,560 --> 00:04:59,280
between actual intelligence, which

119
00:04:59,280 --> 00:05:01,920
is the ability to adapt to things you've not been prepared

120
00:05:01,920 --> 00:05:06,040
for, and pure memorization, like reciting what you've seen

121
00:05:06,040 --> 00:05:06,920
before.

122
00:05:06,920 --> 00:05:10,120
And it's not just some semantic difference.

123
00:05:10,120 --> 00:05:13,840
The big difference is that you can never

124
00:05:13,840 --> 00:05:18,520
pre-train on everything that you might see at test time,

125
00:05:18,520 --> 00:05:20,840
because the world changes all the time.

126
00:05:20,840 --> 00:05:24,400
So it's not just the fact that the space of possible tasks

127
00:05:24,400 --> 00:05:25,280
is infinite.

128
00:05:25,280 --> 00:05:28,080
And even if you're trained on millions of them,

129
00:05:28,080 --> 00:05:30,120
you've only seen zero person out of the total space.

130
00:05:30,120 --> 00:05:34,720
It's also the fact that the world is changing every day.

131
00:05:34,720 --> 00:05:37,800
This is why we, the human species,

132
00:05:37,840 --> 00:05:40,480
developed intelligence in the first place.

133
00:05:40,480 --> 00:05:44,720
If there was a shifting as a distribution for the world,

134
00:05:44,720 --> 00:05:47,080
for the universe, for our lives, then we would not

135
00:05:47,080 --> 00:05:48,400
need intelligence at all.

136
00:05:48,400 --> 00:05:51,840
In fact, many creatures, many insects, for instance,

137
00:05:51,840 --> 00:05:53,440
do not have intelligence.

138
00:05:53,440 --> 00:05:58,440
Instead, what they have is they have in their connectome,

139
00:05:58,440 --> 00:06:01,120
in their genes, hard-coded programs,

140
00:06:01,120 --> 00:06:03,800
behavioral programs that map some stimuli

141
00:06:03,800 --> 00:06:05,320
to appropriate response.

142
00:06:05,320 --> 00:06:07,720
And they can actually navigate their lives

143
00:06:07,720 --> 00:06:11,160
to environments in a way that's very evolutionary fit.

144
00:06:11,160 --> 00:06:14,400
That way, without needing to learn anything.

145
00:06:14,400 --> 00:06:17,560
And while if our environment was static enough,

146
00:06:17,560 --> 00:06:19,920
predictable enough, what would have happened

147
00:06:19,920 --> 00:06:21,960
is that evolution would have found

148
00:06:21,960 --> 00:06:24,400
the perfect behavioral program, a hard-coded,

149
00:06:24,400 --> 00:06:25,720
static behavioral program.

150
00:06:25,720 --> 00:06:28,480
We'd have written it into our genes.

151
00:06:28,480 --> 00:06:30,880
We would have a hard-coded brain connectome.

152
00:06:30,880 --> 00:06:32,240
And that's what we were running on.

153
00:06:32,240 --> 00:06:33,440
But no, that's not what happened.

154
00:06:33,440 --> 00:06:36,240
Instead, we have general intelligence.

155
00:06:36,280 --> 00:06:40,360
We are born with extremely little knowledge about the world.

156
00:06:40,360 --> 00:06:43,760
But we are born with the ability to learn very efficiently

157
00:06:43,760 --> 00:06:45,960
and to adapt in the face of things

158
00:06:45,960 --> 00:06:47,720
that we've never seen before.

159
00:06:47,720 --> 00:06:48,920
And that's what makes us unique.

160
00:06:48,920 --> 00:06:51,960
And that's what is really, really challenging

161
00:06:51,960 --> 00:06:53,640
to recreate in machines.

162
00:06:53,640 --> 00:06:55,240
I want to wrap it all in that a little bit.

163
00:06:55,240 --> 00:06:57,760
But before I do that, maybe I'm going

164
00:06:57,760 --> 00:06:59,920
to overlay some examples of what an arc-like challenge looks

165
00:06:59,920 --> 00:07:01,760
like for the YouTube audience.

166
00:07:01,760 --> 00:07:03,800
But maybe for people listening on audio,

167
00:07:03,800 --> 00:07:06,680
can you just describe what an example arc challenge

168
00:07:06,680 --> 00:07:07,160
will look like?

169
00:07:07,160 --> 00:07:07,640
Sure.

170
00:07:07,640 --> 00:07:11,800
So one arc puzzle, it looks kind of like an IQ test puzzle.

171
00:07:11,800 --> 00:07:15,480
You've got a number of demonstration input-adput pairs.

172
00:07:15,480 --> 00:07:19,200
So one pair is made of two grids.

173
00:07:19,200 --> 00:07:21,800
So one grid shows you an input.

174
00:07:21,800 --> 00:07:24,040
And the second grid shows you what

175
00:07:24,040 --> 00:07:27,840
you should produce as a response to that input.

176
00:07:27,840 --> 00:07:30,600
And you get a couple pairs like this

177
00:07:30,600 --> 00:07:32,480
to demonstrate the nature of the task,

178
00:07:32,480 --> 00:07:35,080
to demonstrate what you're supposed to do with your inputs.

179
00:07:35,080 --> 00:07:39,160
And then you get a new test input.

180
00:07:39,160 --> 00:07:43,000
And your job is to produce the corresponding test outputs.

181
00:07:43,000 --> 00:07:44,920
You look at the demonstration pairs.

182
00:07:44,920 --> 00:07:48,640
And from that, you figure out what you're supposed to do.

183
00:07:48,640 --> 00:07:51,240
And you show that you've understood it on this new test

184
00:07:51,240 --> 00:07:52,800
pair.

185
00:07:52,800 --> 00:07:57,960
And importantly, in order to the knowledge basis

186
00:07:57,960 --> 00:08:01,640
that you need, in order to approach these challenges,

187
00:08:01,640 --> 00:08:03,400
is you just need core knowledge.

188
00:08:03,400 --> 00:08:06,200
And core knowledge is basically the knowledge

189
00:08:06,200 --> 00:08:10,760
of what makes an object, basic counting, basic geometry,

190
00:08:10,760 --> 00:08:13,400
topology, symmetries, that sort of thing.

191
00:08:13,400 --> 00:08:15,360
So extremely basic knowledge.

192
00:08:15,360 --> 00:08:17,640
LLMs for sure possess such knowledge.

193
00:08:17,640 --> 00:08:21,480
Any child possesses such knowledge.

194
00:08:21,480 --> 00:08:24,800
And what's really interesting is that each puzzle is new.

195
00:08:24,800 --> 00:08:26,040
So it's not something that you're

196
00:08:26,040 --> 00:08:31,000
going to find elsewhere on the internet, for instance.

197
00:08:31,040 --> 00:08:34,360
And that means that whether it's as a human or as a machine,

198
00:08:34,360 --> 00:08:37,520
every puzzle you have to approach it from scratch.

199
00:08:37,520 --> 00:08:39,520
You have to actually reason your way through it.

200
00:08:39,520 --> 00:08:43,000
You cannot just fetch the response from your memory.

201
00:08:43,000 --> 00:08:47,640
So the core knowledge, one contention here

202
00:08:47,640 --> 00:08:52,600
is we are only now getting multimodal models who,

203
00:08:52,600 --> 00:08:54,520
because of the data that are trained on,

204
00:08:54,520 --> 00:08:57,280
are trained to do spatial reasoning.

205
00:08:57,280 --> 00:08:59,280
Whereas, obviously, not only humans,

206
00:08:59,280 --> 00:09:01,760
but for billions of years of revolution,

207
00:09:01,760 --> 00:09:04,200
we've had our ancestors have had to learn

208
00:09:04,200 --> 00:09:08,600
how to understand abstract, physical, and spatial properties

209
00:09:08,600 --> 00:09:10,600
and recognize the patterns there.

210
00:09:10,600 --> 00:09:14,920
And so one view would be, in the next year,

211
00:09:14,920 --> 00:09:17,400
as we gain models that are multimodal native,

212
00:09:17,400 --> 00:09:20,840
that isn't just a second class that is an add-on,

213
00:09:20,840 --> 00:09:23,600
but the multimodal capability is a priority.

214
00:09:23,600 --> 00:09:26,600
That it will understand these kinds of patterns

215
00:09:26,600 --> 00:09:28,480
because that's something we see natively.

216
00:09:28,480 --> 00:09:31,800
Whereas, right now, what Arc sees is some JSON string

217
00:09:31,800 --> 00:09:35,600
of 100100, and it's supposed to recognize a pattern there.

218
00:09:35,600 --> 00:09:39,720
And even if you showed a sequence of these kinds of numbers,

219
00:09:39,720 --> 00:09:41,600
it would have a challenge making sense

220
00:09:41,600 --> 00:09:44,240
of what kind of question you're asking it.

221
00:09:44,240 --> 00:09:46,080
So why want it to be the case that,

222
00:09:46,080 --> 00:09:47,480
as soon as we get multimodal models,

223
00:09:47,480 --> 00:09:49,440
which we're on the path to unlock right now,

224
00:09:49,440 --> 00:09:50,360
they're going to be so much better

225
00:09:50,360 --> 00:09:52,000
at Arc-type spatial reasoning?

226
00:09:52,000 --> 00:09:53,400
That's an incredibly cool question,

227
00:09:53,400 --> 00:09:54,800
so I guess we're going to see the answer

228
00:09:54,800 --> 00:09:55,920
within a few months.

229
00:09:55,920 --> 00:09:59,160
But my answer to that is Arc grids,

230
00:09:59,160 --> 00:10:02,240
they're just discrete 2D grids of symbols.

231
00:10:02,240 --> 00:10:05,280
They're pretty small, like it's not like...

232
00:10:05,280 --> 00:10:08,280
If you flatten an image as a sequence of pixels,

233
00:10:08,280 --> 00:10:10,200
for instance, then you get something

234
00:10:10,200 --> 00:10:12,120
that's actually very, very difficult to parse.

235
00:10:12,120 --> 00:10:15,200
But that's not true for Arc because the grids are very small.

236
00:10:15,200 --> 00:10:17,120
You only have 10 possible symbols.

237
00:10:17,120 --> 00:10:18,840
So there's these 2D grids that are actually

238
00:10:18,840 --> 00:10:21,360
very easy to flatten as sequences.

239
00:10:21,360 --> 00:10:23,480
And transformers, LLMs, they're very good

240
00:10:23,480 --> 00:10:24,560
at processing the sequences.

241
00:10:24,560 --> 00:10:29,240
In fact, you can show that LLMs do fine

242
00:10:29,240 --> 00:10:32,240
with processing Arc-like data

243
00:10:32,240 --> 00:10:37,240
by simply fine-tuning LLMs on some subsets of the tasks

244
00:10:38,440 --> 00:10:42,160
and then trying to test it on small variations

245
00:10:42,160 --> 00:10:43,240
of these tasks.

246
00:10:43,240 --> 00:10:46,240
And you see that, yeah, the LLMs can encode

247
00:10:46,240 --> 00:10:49,440
just fine solution programs for tasks

248
00:10:49,440 --> 00:10:50,800
that they've seen before.

249
00:10:50,800 --> 00:10:52,560
So it does not really have a problem

250
00:10:52,560 --> 00:10:57,560
parsing the input or figuring out the program.

251
00:10:57,640 --> 00:11:01,400
The reason why LLMs don't do well on Arc

252
00:11:01,400 --> 00:11:04,640
is really just the unfamiliarity aspect.

253
00:11:04,640 --> 00:11:07,560
The fact that each new task is different

254
00:11:07,560 --> 00:11:09,240
from every other task.

255
00:11:09,240 --> 00:11:11,880
You cannot, basically, you cannot memorize

256
00:11:11,880 --> 00:11:13,840
the solution programs in advance.

257
00:11:13,840 --> 00:11:16,640
You have to synthesize a new solution program

258
00:11:16,640 --> 00:11:18,480
on the fly for each new task.

259
00:11:18,480 --> 00:11:20,800
And that's really what LLMs are struggling with.

260
00:11:20,840 --> 00:11:22,480
So before I do more devil's advocate,

261
00:11:22,480 --> 00:11:24,200
I just want to step back and explain

262
00:11:24,200 --> 00:11:27,240
why I'm especially interested in having this conversation.

263
00:11:27,240 --> 00:11:29,480
And obviously the million dollar Arc prize,

264
00:11:29,480 --> 00:11:31,920
I'm excited to actually play with it myself.

265
00:11:31,920 --> 00:11:35,400
And hopefully the Vesuvius challenge,

266
00:11:35,400 --> 00:11:39,760
which was Nat Friedman's prize for solving decoding scrolls,

267
00:11:39,760 --> 00:11:42,000
the winner of that, decoding the scrolls from

268
00:11:42,000 --> 00:11:43,600
that were buried in the volcanoes

269
00:11:43,600 --> 00:11:46,760
in the Herculaneum library that was solved

270
00:11:46,760 --> 00:11:48,680
by a 22 year old who was listening

271
00:11:48,680 --> 00:11:49,840
to the podcast, Luke Farator.

272
00:11:49,840 --> 00:11:51,800
So hopefully somebody listening will find

273
00:11:51,800 --> 00:11:54,240
this challenge intriguing and find a solution.

274
00:11:54,240 --> 00:11:58,200
So I'm, and the reason I've had on recently

275
00:11:58,200 --> 00:12:01,680
a lot of people who are bullish on LLMs

276
00:12:01,680 --> 00:12:03,560
and I've had discussions with them

277
00:12:03,560 --> 00:12:05,640
before interviewing you about how do we explain the fact

278
00:12:05,640 --> 00:12:07,440
that LLMs don't seem to be natively performing

279
00:12:07,440 --> 00:12:08,800
that well on Arc.

280
00:12:08,800 --> 00:12:12,440
And I found their explanations somewhat contrived

281
00:12:12,440 --> 00:12:15,120
and I'll try out some of the reasons on you.

282
00:12:15,120 --> 00:12:17,640
But it is actually an intriguing fact

283
00:12:17,840 --> 00:12:18,960
that they actually, these are,

284
00:12:18,960 --> 00:12:21,120
some of these problems are relatively straightforward

285
00:12:21,120 --> 00:12:22,400
for humans to understand.

286
00:12:22,400 --> 00:12:24,360
And they do struggle with them

287
00:12:24,360 --> 00:12:25,920
if you just input them natively.

288
00:12:25,920 --> 00:12:27,640
All of them are very easy for humans.

289
00:12:27,640 --> 00:12:29,760
Like any smart human should be able

290
00:12:29,760 --> 00:12:32,560
to do 90%, 95% on Arc.

291
00:12:32,560 --> 00:12:33,400
Smart human.

292
00:12:33,400 --> 00:12:35,360
A smart human, but even a five year old.

293
00:12:35,360 --> 00:12:37,320
So with very, very little knowledge,

294
00:12:37,320 --> 00:12:40,280
they could definitely do over 50%.

295
00:12:40,280 --> 00:12:44,560
So let's talk about that because you,

296
00:12:45,480 --> 00:12:48,480
I agree that smart humans will do very well on this test,

297
00:12:48,480 --> 00:12:53,480
but the average human will probably do mediocre.

298
00:12:53,680 --> 00:12:54,640
Not really.

299
00:12:54,640 --> 00:12:56,920
So we actually tried with average humans,

300
00:12:56,920 --> 00:12:58,280
the score about 85.

301
00:12:58,280 --> 00:13:00,640
That was with Amazon Mechanical Turk workers, right?

302
00:13:00,640 --> 00:13:02,800
I honestly don't know the demographic profile

303
00:13:02,800 --> 00:13:04,040
of Amazon Mechanical Turk workers,

304
00:13:04,040 --> 00:13:07,320
but imagine just interacting with the platform

305
00:13:07,320 --> 00:13:09,080
that Amazon has set up to do remote work.

306
00:13:09,080 --> 00:13:11,920
That's not the median human across the planet, I'm guessing.

307
00:13:12,120 --> 00:13:14,680
I mean, the broader point here being that,

308
00:13:14,680 --> 00:13:17,200
so we see the spectrum in humans

309
00:13:17,200 --> 00:13:20,160
where humans obviously have AGI,

310
00:13:20,160 --> 00:13:22,000
but even within humans, you see a spectrum

311
00:13:22,000 --> 00:13:24,200
where some people are relatively dumber

312
00:13:24,200 --> 00:13:27,920
and they'll do perform work on IQ like tests.

313
00:13:27,920 --> 00:13:29,480
For example, Raven's progressive matrices.

314
00:13:29,480 --> 00:13:31,680
If you look at how the average person performs on that

315
00:13:31,680 --> 00:13:32,920
and you look at the quick kind of questions

316
00:13:32,920 --> 00:13:34,400
that is this sort of midtermists,

317
00:13:34,400 --> 00:13:35,360
half of people will get it right,

318
00:13:35,360 --> 00:13:36,360
half of people will get it wrong.

319
00:13:36,360 --> 00:13:37,800
Some of them are like pretty trivial.

320
00:13:37,800 --> 00:13:40,560
For us, we might think like this was kind of trivial.

321
00:13:40,560 --> 00:13:42,200
And so humans have AGI,

322
00:13:42,200 --> 00:13:45,200
but from relatively small tweaks,

323
00:13:45,200 --> 00:13:47,440
you can go from somebody who misses

324
00:13:47,440 --> 00:13:48,920
these kinds of basic IQ test questions

325
00:13:48,920 --> 00:13:50,240
to somebody who gets them all right,

326
00:13:50,240 --> 00:13:51,560
which suggests that actually,

327
00:13:51,560 --> 00:13:54,480
if these models are doing natively,

328
00:13:54,480 --> 00:13:56,240
we'll talk about some of the previous performances

329
00:13:56,240 --> 00:13:57,120
that people have tried with these models,

330
00:13:57,120 --> 00:13:59,000
but somebody with a Jack Cole

331
00:13:59,000 --> 00:14:02,800
with a 240 million parameter model got 35%.

332
00:14:02,800 --> 00:14:05,080
Doesn't that suggest that they're on this spectrum

333
00:14:05,080 --> 00:14:06,480
that clearly exists within humans

334
00:14:06,480 --> 00:14:08,560
and they're gonna be saturated at pretty soon?

335
00:14:08,560 --> 00:14:11,360
Yeah, so that's a bunch of interesting points here.

336
00:14:11,360 --> 00:14:16,360
So there is indeed a branch of LLM approaches

337
00:14:16,800 --> 00:14:19,960
suspended by Jack Cole that are doing quite well,

338
00:14:19,960 --> 00:14:23,000
that are in fact a state of the art.

339
00:14:23,000 --> 00:14:25,520
But you have to look at what's going on there.

340
00:14:25,520 --> 00:14:26,360
So there are two things.

341
00:14:26,360 --> 00:14:29,480
The first thing is that to guess these numbers,

342
00:14:29,480 --> 00:14:31,840
you need to pre-train your LLM

343
00:14:31,840 --> 00:14:34,520
on millions of generated art tasks.

344
00:14:34,520 --> 00:14:37,840
And of course, if you compare that to a five-year-old child

345
00:14:37,880 --> 00:14:39,680
looking at art for the first time,

346
00:14:39,680 --> 00:14:41,400
the child has never done like you did before,

347
00:14:41,400 --> 00:14:44,040
has never seen something like an art task before.

348
00:14:44,040 --> 00:14:46,160
The only overlap between what they know

349
00:14:46,160 --> 00:14:49,320
and what they have to do in the test is core knowledge,

350
00:14:49,320 --> 00:14:51,560
is knowing about like counting and objects

351
00:14:51,560 --> 00:14:53,040
and symmetries and things like that.

352
00:14:53,040 --> 00:14:56,160
And still, they're gonna do really well

353
00:14:56,160 --> 00:14:57,880
and they're gonna do much better than the LLM

354
00:14:57,880 --> 00:15:00,560
trained on millions of similar tasks.

355
00:15:00,560 --> 00:15:03,400
And the second thing that's something to note

356
00:15:03,400 --> 00:15:07,360
about the Jack Cole approach is one thing

357
00:15:07,360 --> 00:15:10,520
that's really critical to making the model work at all

358
00:15:10,520 --> 00:15:12,560
is test time fine tuning.

359
00:15:12,560 --> 00:15:14,320
And that's something that's really missing, by the way,

360
00:15:14,320 --> 00:15:19,320
from LLM approaches right now is that, you know,

361
00:15:19,360 --> 00:15:21,680
most of the time when you're using an LLM,

362
00:15:21,680 --> 00:15:23,760
it's just doing static inference.

363
00:15:23,760 --> 00:15:26,920
The model is frozen and you're just prompting it

364
00:15:26,920 --> 00:15:28,280
and then you're getting an answer.

365
00:15:28,280 --> 00:15:31,400
So the model is not actually learning anything on the fly.

366
00:15:31,400 --> 00:15:35,880
Its state is not adapting to the task at hand.

367
00:15:36,120 --> 00:15:38,280
What Jack Cole is actually doing is that

368
00:15:38,280 --> 00:15:41,960
for every test problem is on the fly,

369
00:15:41,960 --> 00:15:46,960
is fine tuning a version of the LLM for that task.

370
00:15:47,080 --> 00:15:48,920
And that's really what's unlocking performance.

371
00:15:48,920 --> 00:15:51,720
If you don't do that, you get like 1%, 2%.

372
00:15:51,720 --> 00:15:55,000
So basically something completely negligible.

373
00:15:55,000 --> 00:15:57,080
And if you do test time fine tuning

374
00:15:57,080 --> 00:15:58,880
and you add a bunch of tricks on top,

375
00:15:58,880 --> 00:16:01,080
then you end up with interesting performance numbers.

376
00:16:01,080 --> 00:16:04,240
So I think what he's doing is trying to address

377
00:16:04,240 --> 00:16:07,080
one of the key limitations of LLMs today,

378
00:16:07,080 --> 00:16:08,680
which is the lack of active inference,

379
00:16:08,680 --> 00:16:11,600
is actually adding active inference to LLMs.

380
00:16:11,600 --> 00:16:13,360
And that's working extremely well actually.

381
00:16:13,360 --> 00:16:14,800
So that's fascinating to me.

382
00:16:14,800 --> 00:16:17,200
That there's so many interesting rabbit holes there.

383
00:16:18,360 --> 00:16:20,120
Should I take them in sequence or deal with them all at once?

384
00:16:20,120 --> 00:16:21,200
Let me just start.

385
00:16:21,200 --> 00:16:24,480
So the point you made about the fact

386
00:16:24,480 --> 00:16:26,880
that you need to unlock the adapter compute

387
00:16:26,880 --> 00:16:30,640
slash test time compute, a lot of the scale maximalist,

388
00:16:30,640 --> 00:16:31,800
I think this will be interesting rabbit hole

389
00:16:31,800 --> 00:16:32,720
to explore with you,

390
00:16:32,720 --> 00:16:35,040
because a lot of the scaling maximalist

391
00:16:35,040 --> 00:16:37,600
have your broader perspective in the sense

392
00:16:37,600 --> 00:16:40,400
that they think that in addition to scaling,

393
00:16:40,400 --> 00:16:41,960
you need these kinds of things,

394
00:16:41,960 --> 00:16:44,080
like unlocking adaptive compute

395
00:16:44,080 --> 00:16:47,400
or doing some sort of RL to get the system to working.

396
00:16:47,400 --> 00:16:49,920
And their perspective is that this is a relatively

397
00:16:49,920 --> 00:16:52,320
straightforward thing that will be added atop

398
00:16:52,320 --> 00:16:55,440
the representations that a scaled up model

399
00:16:55,440 --> 00:16:57,880
has greater access to.

400
00:16:57,880 --> 00:17:00,400
No, it's not just a technical detail.

401
00:17:00,400 --> 00:17:01,840
It's not a straightforward thing.

402
00:17:01,840 --> 00:17:03,240
It is everything.

403
00:17:03,240 --> 00:17:05,120
It is the important part.

404
00:17:05,120 --> 00:17:08,800
And the scale maximalist argument,

405
00:17:08,800 --> 00:17:13,080
you know, it boils down to, you know,

406
00:17:13,080 --> 00:17:15,320
these people, they refer to scaling loss,

407
00:17:15,320 --> 00:17:17,440
which is this empirical relationship

408
00:17:17,440 --> 00:17:19,720
that you can draw between how much compute

409
00:17:19,720 --> 00:17:21,000
you spend on training a model

410
00:17:21,000 --> 00:17:23,600
and the performance you're getting on benchmarks, right?

411
00:17:23,600 --> 00:17:26,320
And the key question here, of course, is,

412
00:17:26,320 --> 00:17:28,160
well, how do you measure performance?

413
00:17:28,160 --> 00:17:31,000
What it is that you're actually improving

414
00:17:31,000 --> 00:17:33,080
by adding more compute and more data?

415
00:17:33,080 --> 00:17:35,480
And, well, it's benchmark performance, right?

416
00:17:35,480 --> 00:17:38,400
And the thing is, the way you measure performance

417
00:17:38,400 --> 00:17:41,360
is not a technical detail.

418
00:17:41,360 --> 00:17:46,160
It's not an afterthought because it's gonna narrow down

419
00:17:46,160 --> 00:17:47,840
the set of questions that you're asking.

420
00:17:47,840 --> 00:17:50,400
And so, accordingly, it's gonna narrow down

421
00:17:50,400 --> 00:17:53,280
the set of answers that you're looking for.

422
00:17:53,280 --> 00:17:56,840
If you look at the benchmarks we're using for LMS,

423
00:17:56,840 --> 00:17:59,640
they're all memorization-based benchmarks.

424
00:17:59,640 --> 00:18:01,880
Like, sometimes they are literally just knowledge-based,

425
00:18:01,880 --> 00:18:03,440
like a school test.

426
00:18:03,440 --> 00:18:04,760
And even if you look at the ones

427
00:18:04,760 --> 00:18:08,920
that are, you know, explicitly about reasoning,

428
00:18:08,920 --> 00:18:10,640
you realize, if you look closely,

429
00:18:10,640 --> 00:18:13,000
that it's, in order to solve them,

430
00:18:13,000 --> 00:18:18,000
it's enough to memorize a finite set of reasoning patterns.

431
00:18:19,080 --> 00:18:20,520
And then you just reapply them.

432
00:18:20,520 --> 00:18:22,840
They're like static programs.

433
00:18:22,840 --> 00:18:25,600
LMS are very good at memorizing static programs,

434
00:18:25,600 --> 00:18:26,560
small static programs.

435
00:18:26,560 --> 00:18:31,560
And they've got this sort of like bank of solution programs.

436
00:18:31,760 --> 00:18:33,720
And when you give them a new puzzle,

437
00:18:33,720 --> 00:18:37,240
they can just fetch the appropriate program, apply it.

438
00:18:37,240 --> 00:18:39,160
And it's looking like it's reasoning,

439
00:18:39,160 --> 00:18:41,000
but really it's not doing any sort of

440
00:18:41,000 --> 00:18:42,640
on-the-flight program synthesis.

441
00:18:42,640 --> 00:18:45,280
All it's doing is program fetching.

442
00:18:45,280 --> 00:18:47,400
So you can actually solve all these benchmarks

443
00:18:47,400 --> 00:18:48,680
with memorization.

444
00:18:48,680 --> 00:18:51,800
And so, what you're scaling up here,

445
00:18:51,800 --> 00:18:53,360
like if you look at the models,

446
00:18:53,360 --> 00:18:56,680
they are big parametric curves

447
00:18:56,680 --> 00:18:58,320
fitted to a data distribution,

448
00:18:58,320 --> 00:18:59,480
which I call in descent.

449
00:18:59,480 --> 00:19:03,600
So they're basically these big interpolative databases,

450
00:19:03,600 --> 00:19:04,960
interpolative memories.

451
00:19:04,960 --> 00:19:08,560
And of course, if you scale up the size of your database

452
00:19:08,560 --> 00:19:11,400
and you cram into it more knowledge,

453
00:19:11,400 --> 00:19:13,640
more patterns and so on,

454
00:19:13,640 --> 00:19:16,760
you are gonna be increasing its performance

455
00:19:16,760 --> 00:19:19,400
as measured by a memorization benchmark.

456
00:19:19,400 --> 00:19:20,960
That's kind of obvious.

457
00:19:20,960 --> 00:19:22,440
But as you're doing it,

458
00:19:22,440 --> 00:19:25,080
you are not increasing the intelligence

459
00:19:25,080 --> 00:19:26,760
of the system one bit.

460
00:19:26,760 --> 00:19:28,640
You are increasing the skill of the system.

461
00:19:28,640 --> 00:19:30,600
You are increasing its usefulness,

462
00:19:30,600 --> 00:19:34,360
its scope of applicability, but not its intelligence

463
00:19:34,360 --> 00:19:36,640
because skill is not intelligence.

464
00:19:36,640 --> 00:19:38,480
And that's the fundamental confusion

465
00:19:39,480 --> 00:19:42,120
that people run into is that

466
00:19:42,120 --> 00:19:44,160
they're confusing skill and intelligence.

467
00:19:44,160 --> 00:19:45,640
Yeah, there's a lot of fascinating things

468
00:19:45,640 --> 00:19:46,480
to talk about here.

469
00:19:46,480 --> 00:19:50,640
So skill, intelligence, interpolation.

470
00:19:50,640 --> 00:19:52,240
I mean, okay, so the thing about

471
00:19:52,240 --> 00:19:56,720
they're fitting some manifold into that maps the input data,

472
00:19:56,720 --> 00:19:58,440
there's a reductionist way to talk about what happens

473
00:19:58,440 --> 00:19:59,960
in the human brain that says

474
00:19:59,960 --> 00:20:03,200
that it's just axons firing at each other.

475
00:20:03,200 --> 00:20:05,880
But we don't care about the reductionist explanation

476
00:20:05,880 --> 00:20:06,720
of what's happening.

477
00:20:06,720 --> 00:20:11,200
We care about what the sort of meta at the

478
00:20:11,200 --> 00:20:13,840
macroscopic level, what happens when these things combine.

479
00:20:13,840 --> 00:20:15,960
As far as the interpolation goes,

480
00:20:15,960 --> 00:20:19,120
so okay, let's look at one of the benchmarks here.

481
00:20:19,120 --> 00:20:22,680
There's one benchmark that does great school math

482
00:20:22,680 --> 00:20:26,720
and these are problems that like a smart high schooler

483
00:20:26,720 --> 00:20:28,480
would be able to solve.

484
00:20:28,480 --> 00:20:31,760
It's called GSM 8K and these models get 95% on these.

485
00:20:31,760 --> 00:20:33,200
Like basically they always nail it.

486
00:20:33,200 --> 00:20:34,320
That's memorization benchmark.

487
00:20:34,320 --> 00:20:35,520
Okay, let's talk about what that means.

488
00:20:35,520 --> 00:20:38,480
So here's one question about from that benchmark.

489
00:20:38,480 --> 00:20:40,120
So 30 students are in a class,

490
00:20:40,120 --> 00:20:41,880
one fifth of them are 12 year olds,

491
00:20:41,880 --> 00:20:45,040
one third are 13 year old, one 10th are 11 year olds.

492
00:20:45,040 --> 00:20:48,320
How many of them are not 11, 12 or 13 years old?

493
00:20:48,320 --> 00:20:50,440
So I agree, like this is not rocket science, right?

494
00:20:50,440 --> 00:20:53,200
You can write down on paper how you go through this problem

495
00:20:53,200 --> 00:20:54,600
and a high school kid,

496
00:20:54,600 --> 00:20:56,640
at least a smart high school kid should be able to solve it.

497
00:20:56,640 --> 00:20:58,920
Now, when you say memorization,

498
00:20:58,920 --> 00:21:03,000
it still has to reason through how to think about fractions

499
00:21:03,000 --> 00:21:05,000
and what is the context of the whole problem

500
00:21:05,000 --> 00:21:07,840
and then combining the different calculations it's doing.

501
00:21:07,840 --> 00:21:10,200
It depends how you want to define reasoning,

502
00:21:10,200 --> 00:21:12,200
but there are two definitions you can use.

503
00:21:12,200 --> 00:21:17,200
So one is I have available a set of program templates.

504
00:21:17,680 --> 00:21:21,200
It's like the structure of the puzzle,

505
00:21:21,200 --> 00:21:22,960
which can also generate its solution.

506
00:21:22,960 --> 00:21:25,600
And I'm just gonna identify the right template,

507
00:21:25,600 --> 00:21:26,960
which is in my memory.

508
00:21:27,800 --> 00:21:29,840
I'm gonna input the new values into the template,

509
00:21:29,840 --> 00:21:31,840
run the program, get the solution.

510
00:21:31,840 --> 00:21:33,480
And you could say this is reasoning.

511
00:21:33,480 --> 00:21:35,400
And I say, yeah, sure, okay.

512
00:21:35,400 --> 00:21:37,560
But another definition you can use is reasoning

513
00:21:37,560 --> 00:21:41,280
is the ability to, when you're faced with a puzzle,

514
00:21:41,280 --> 00:21:44,480
given that you don't have already a program in memory

515
00:21:44,480 --> 00:21:49,480
to solve it, you must synthesize on-the-fly a new program

516
00:21:49,680 --> 00:21:52,960
based on bits of pieces of existing programs that you have.

517
00:21:52,960 --> 00:21:55,480
You have to do on-the-fly program synthesis.

518
00:21:55,480 --> 00:21:57,280
And it's actually dramatically harder

519
00:21:57,280 --> 00:22:00,040
than just fetching the right memorized program

520
00:22:00,040 --> 00:22:01,320
and replying it.

521
00:22:01,320 --> 00:22:04,560
So I think maybe we are overestimating

522
00:22:04,560 --> 00:22:06,840
the extent to which humans are so sample efficient.

523
00:22:06,840 --> 00:22:10,200
They also don't need training in this way

524
00:22:10,200 --> 00:22:12,760
where they have to drill in these kinds

525
00:22:12,800 --> 00:22:16,760
of pathways of reasoning through certain kinds of problems.

526
00:22:16,760 --> 00:22:19,200
So let's take math, for example.

527
00:22:19,200 --> 00:22:21,240
It's not like you can just show a baby

528
00:22:21,240 --> 00:22:22,840
the axioms of set theory.

529
00:22:22,840 --> 00:22:24,080
And now they know math, right?

530
00:22:24,080 --> 00:22:25,680
So when they're growing up,

531
00:22:25,680 --> 00:22:28,120
you had to do years of teaching them pre-algebra.

532
00:22:28,120 --> 00:22:30,360
Then you got to do a year of teaching them doing drills

533
00:22:30,360 --> 00:22:32,240
and going through the same kind of problem in algebra,

534
00:22:32,240 --> 00:22:35,040
then geometry, pre-calculus, calculus.

535
00:22:35,040 --> 00:22:36,520
Absolutely, so training?

536
00:22:36,520 --> 00:22:37,880
Yeah, but isn't that like the same kind of thing

537
00:22:37,880 --> 00:22:40,080
where you can't just see one example

538
00:22:40,080 --> 00:22:41,960
and now you have the program or whatever.

539
00:22:41,960 --> 00:22:42,800
You actually had to drill it.

540
00:22:42,800 --> 00:22:43,800
These models also had to drill it

541
00:22:43,800 --> 00:22:45,080
with a bunch of returning data.

542
00:22:45,080 --> 00:22:48,320
Sure, I mean, in order to do on-the-fly program synthesis,

543
00:22:48,320 --> 00:22:51,920
you actually need building blocks to work from.

544
00:22:51,920 --> 00:22:55,440
So knowledge and memory are actually tremendously important

545
00:22:55,440 --> 00:22:56,280
in the process.

546
00:22:56,280 --> 00:23:00,560
I'm not saying it's memory versus reasoning.

547
00:23:00,560 --> 00:23:04,640
In order to do effective reasoning, you need memory.

548
00:23:04,640 --> 00:23:07,680
But it sounds like it's compatible with your story

549
00:23:07,680 --> 00:23:10,440
that through seeing a lot of different kinds of examples,

550
00:23:10,440 --> 00:23:11,960
these things can learn to reason

551
00:23:11,960 --> 00:23:13,800
within the context of those examples.

552
00:23:13,800 --> 00:23:16,160
And we can also see within bigger and bigger models.

553
00:23:16,160 --> 00:23:18,800
So that was an example of a high school level math problem.

554
00:23:19,880 --> 00:23:22,120
Let's say a model that's smaller than GPT-3

555
00:23:22,120 --> 00:23:23,680
couldn't do that at all.

556
00:23:23,680 --> 00:23:24,760
As these models get bigger,

557
00:23:24,760 --> 00:23:26,400
they seem to be able to pick up bigger and bigger.

558
00:23:26,400 --> 00:23:28,400
It's not really a size issue.

559
00:23:28,400 --> 00:23:30,600
It's more like a training data issue in this case.

560
00:23:30,600 --> 00:23:33,680
Well, bigger models can pick up these kinds of circuits

561
00:23:33,680 --> 00:23:36,040
which smaller models apparently don't do a good job

562
00:23:36,040 --> 00:23:37,280
of doing this even if you were to train them

563
00:23:37,280 --> 00:23:38,240
on this kind of data.

564
00:23:38,240 --> 00:23:39,080
Doesn't that just suggest

565
00:23:39,080 --> 00:23:40,400
that you have bigger and bigger models?

566
00:23:40,400 --> 00:23:42,240
They can pick up bigger and bigger pathways

567
00:23:42,240 --> 00:23:44,040
or more general ways of reasoning.

568
00:23:44,040 --> 00:23:44,960
Absolutely.

569
00:23:44,960 --> 00:23:46,400
But then isn't that intelligence?

570
00:23:46,400 --> 00:23:47,600
No, no, it's not.

571
00:23:47,600 --> 00:23:49,520
If you scale up your database

572
00:23:49,520 --> 00:23:52,280
and you keep adding to it more knowledge,

573
00:23:52,280 --> 00:23:53,640
more program templates,

574
00:23:53,640 --> 00:23:55,560
then sure it becomes more and more skillful.

575
00:23:55,560 --> 00:23:57,400
You can apply it to more and more tasks.

576
00:23:57,400 --> 00:24:01,120
But general intelligence is not tasks with six skills

577
00:24:01,120 --> 00:24:03,400
scaled up to many skills.

578
00:24:03,400 --> 00:24:06,640
Because there is an infinite space of possible skills.

579
00:24:06,680 --> 00:24:09,200
General intelligence is the ability to approach

580
00:24:09,200 --> 00:24:11,040
any problem, any skill,

581
00:24:11,040 --> 00:24:13,960
and very quickly master it using valid or data.

582
00:24:13,960 --> 00:24:16,920
Because this is what makes you able to face

583
00:24:16,920 --> 00:24:18,240
anything you might ever encounter.

584
00:24:18,240 --> 00:24:19,360
This is what makes,

585
00:24:20,800 --> 00:24:22,600
this is the definition of generality.

586
00:24:22,600 --> 00:24:25,640
Like generality is now specifically scaled up.

587
00:24:25,640 --> 00:24:29,120
It is the ability to apply your mind

588
00:24:29,120 --> 00:24:31,480
to anything at all, to arbitrary things.

589
00:24:31,480 --> 00:24:33,040
And this requires, fundamentally,

590
00:24:33,040 --> 00:24:35,040
it requires the ability to adapt,

591
00:24:35,080 --> 00:24:37,080
to learn on the fly efficiently.

592
00:24:37,080 --> 00:24:41,280
So, my claim is that by doing this free training

593
00:24:41,280 --> 00:24:42,680
on bigger and bigger models,

594
00:24:42,680 --> 00:24:44,440
you are gaining that capacity

595
00:24:44,440 --> 00:24:46,760
to then generalize very efficiently.

596
00:24:46,760 --> 00:24:48,120
Let me give you an example.

597
00:24:48,120 --> 00:24:49,280
Let me give you an example.

598
00:24:49,280 --> 00:24:51,240
So, your own company, Google,

599
00:24:51,240 --> 00:24:54,240
in their paper on Gemini 1.5,

600
00:24:54,240 --> 00:24:55,920
they had this very interesting example

601
00:24:55,920 --> 00:25:00,520
where they would give, in context,

602
00:25:00,520 --> 00:25:01,840
they would give the model,

603
00:25:01,840 --> 00:25:04,360
the grammar book and the dictionary

604
00:25:04,360 --> 00:25:07,520
of a language that has less than 200 living speakers.

605
00:25:07,520 --> 00:25:09,480
So, it's not in the free training data.

606
00:25:09,480 --> 00:25:11,520
And you just give them the dictionary

607
00:25:11,520 --> 00:25:14,120
and it basically is able to speak this language

608
00:25:14,120 --> 00:25:15,080
and translate to it,

609
00:25:15,080 --> 00:25:17,640
including the complex and organic ways

610
00:25:17,640 --> 00:25:20,200
in which languages are structured.

611
00:25:20,200 --> 00:25:21,760
So, a human, if you showed me a dictionary

612
00:25:21,760 --> 00:25:22,720
from English to Spanish,

613
00:25:22,720 --> 00:25:24,560
I'm not gonna be able to pick up the

614
00:25:24,560 --> 00:25:25,840
how to structure sentences

615
00:25:25,840 --> 00:25:28,040
and how to say things in Spanish.

616
00:25:28,040 --> 00:25:30,280
The fact that because of the representations

617
00:25:30,280 --> 00:25:33,120
that it has gained through this free training,

618
00:25:33,120 --> 00:25:35,240
it is able to now extremely efficiently

619
00:25:35,240 --> 00:25:36,360
learn a new language.

620
00:25:36,360 --> 00:25:38,600
Doesn't that show that this kind of free training

621
00:25:38,600 --> 00:25:41,240
actually does increase your ability to learn new tasks?

622
00:25:41,240 --> 00:25:43,200
If you're right, if you were right,

623
00:25:43,200 --> 00:25:45,440
LLMs would do really well on arch puzzles

624
00:25:45,440 --> 00:25:47,680
because arch puzzles are not complex.

625
00:25:47,680 --> 00:25:49,880
Each one of them requires very little knowledge.

626
00:25:49,880 --> 00:25:52,280
Each one of them is very low on complexity.

627
00:25:52,280 --> 00:25:54,600
You don't need to think very hard about it.

628
00:25:54,600 --> 00:25:56,520
They're actually extremely obvious for humans,

629
00:25:56,520 --> 00:25:58,240
like even children can do them.

630
00:25:58,240 --> 00:26:02,920
But LLMs cannot, even LLMs that have, you know,

631
00:26:02,920 --> 00:26:05,360
100,000 times more knowledge than you do.

632
00:26:05,360 --> 00:26:06,400
They still cannot.

633
00:26:06,400 --> 00:26:09,560
And the only thing that makes arch special

634
00:26:09,560 --> 00:26:11,680
is that it was designed with this intent

635
00:26:11,680 --> 00:26:13,080
to resist memorization.

636
00:26:13,080 --> 00:26:14,320
This is the only thing.

637
00:26:14,320 --> 00:26:18,960
And this is the huge blocker for LLM performance, right?

638
00:26:18,960 --> 00:26:23,960
And so, you know, I think if you look at LLMs closely,

639
00:26:25,240 --> 00:26:28,280
it's pretty obvious that they're not really like

640
00:26:28,280 --> 00:26:30,600
synthesizing new programs on the fly

641
00:26:30,600 --> 00:26:33,160
to solve the tasks that they're faced with.

642
00:26:33,160 --> 00:26:34,640
They're very much replying things

643
00:26:34,640 --> 00:26:36,640
that they've stored in memory.

644
00:26:36,640 --> 00:26:39,280
For instance, one thing that's very striking

645
00:26:39,280 --> 00:26:42,440
is LLMs can solve a CISA cipher,

646
00:26:42,440 --> 00:26:43,760
you know, like a CISA cipher,

647
00:26:43,760 --> 00:26:48,760
like transposing letters to code a message.

648
00:26:49,200 --> 00:26:52,600
And well, that's a very complex algorithm, right?

649
00:26:52,600 --> 00:26:54,920
But it comes up quite a bit on the internet.

650
00:26:54,920 --> 00:26:56,440
So they've basically memorized it.

651
00:26:56,440 --> 00:26:59,680
And what's really interesting is that they can do it

652
00:26:59,800 --> 00:27:02,760
for a transposition length of like three or five

653
00:27:02,760 --> 00:27:04,360
because there are very, very common numbers

654
00:27:04,360 --> 00:27:05,880
in examples provided on the internet.

655
00:27:05,880 --> 00:27:09,000
But if you try to do it with an arbitrary number,

656
00:27:09,000 --> 00:27:11,120
like nine, it's gonna fail.

657
00:27:11,120 --> 00:27:14,520
Because it does not encode the generalized form

658
00:27:14,520 --> 00:27:16,520
of the algorithm, but only specific cases.

659
00:27:16,520 --> 00:27:20,040
It does memorize specific cases of the algorithm, right?

660
00:27:20,040 --> 00:27:23,160
And if it could actually synthesize on the fly

661
00:27:23,160 --> 00:27:26,840
the solver algorithm, then the value of N

662
00:27:26,840 --> 00:27:28,520
would not matter at all

663
00:27:28,520 --> 00:27:30,880
because it does not increase the problem complexity.

664
00:27:30,880 --> 00:27:32,440
I think this is true of humans as well,

665
00:27:32,440 --> 00:27:34,360
where what was the study that-

666
00:27:34,360 --> 00:27:37,080
Humans use memorization pattern matching all the time,

667
00:27:37,080 --> 00:27:39,880
of course, but humans are not limited

668
00:27:39,880 --> 00:27:41,440
to memorization pattern matching.

669
00:27:41,440 --> 00:27:43,160
They have this very unique ability

670
00:27:43,160 --> 00:27:45,560
to adapt to new situations on the fly.

671
00:27:45,560 --> 00:27:48,160
This is exactly what enables you to navigate

672
00:27:49,160 --> 00:27:50,760
every new day in your life.

673
00:27:50,760 --> 00:27:51,600
I'm forgetting the details,

674
00:27:51,600 --> 00:27:54,120
but there was some study that chess grandmasters

675
00:27:54,120 --> 00:27:56,840
will perform very well within the context of the moves that-

676
00:27:56,840 --> 00:27:59,560
Excellent example, because chess at the highest level

677
00:27:59,560 --> 00:28:02,000
is all about memorization, chess memorization.

678
00:28:02,000 --> 00:28:02,960
Okay, sure, we can leave that aside.

679
00:28:02,960 --> 00:28:05,520
What is your explanation for the original question of

680
00:28:05,520 --> 00:28:10,520
why in context the GPT- sorry, Gemini 1.5

681
00:28:11,800 --> 00:28:13,520
was able to learn a language,

682
00:28:13,520 --> 00:28:15,600
including the complex grammar structure?

683
00:28:15,600 --> 00:28:17,400
Doesn't that show that they can pick up new knowledge?

684
00:28:17,400 --> 00:28:20,120
I would assume that it has simply mined

685
00:28:20,120 --> 00:28:23,880
from its extremely extensive and imaginably vast

686
00:28:23,880 --> 00:28:27,400
training data, it has mined the required template

687
00:28:27,400 --> 00:28:28,880
and then it's just reusing it.

688
00:28:28,880 --> 00:28:31,360
We know that they have a very poor ability

689
00:28:31,360 --> 00:28:34,840
to synthesize new program templates like this on the fly

690
00:28:34,840 --> 00:28:36,640
or even adapt existing ones.

691
00:28:36,640 --> 00:28:38,760
They're very much limited to fetching.

692
00:28:38,760 --> 00:28:40,880
Suppose there's a programmer at Google,

693
00:28:40,880 --> 00:28:42,880
they go into the office in the morning.

694
00:28:42,880 --> 00:28:44,280
At what point are they doing something

695
00:28:44,280 --> 00:28:47,760
that 100% cannot be due to fetching some template

696
00:28:47,760 --> 00:28:50,520
that even if they, suppose they were an LLM,

697
00:28:50,520 --> 00:28:52,120
they could not do if they had fetched some template

698
00:28:52,120 --> 00:28:52,960
from their program.

699
00:28:53,000 --> 00:28:53,920
At what point do they have to use

700
00:28:53,920 --> 00:28:55,680
this so-called extreme generalization capability?

701
00:28:55,680 --> 00:28:57,560
Forget about Google software developers.

702
00:28:57,560 --> 00:29:00,400
Every human, every day of their lives

703
00:29:00,400 --> 00:29:03,920
is full of novel things that they've not been prepared for.

704
00:29:03,920 --> 00:29:07,880
You cannot navigate your life based on memorization alone.

705
00:29:07,880 --> 00:29:08,720
It's impossible.

706
00:29:08,720 --> 00:29:11,440
I'm sort of denying the premise that they're,

707
00:29:11,440 --> 00:29:13,040
you also agree they're not doing like,

708
00:29:13,040 --> 00:29:14,600
quote-unquote memorization.

709
00:29:14,600 --> 00:29:17,400
It seems like you're saying they're less capable

710
00:29:17,400 --> 00:29:19,440
of generalization, but I'm just curious of like,

711
00:29:19,440 --> 00:29:21,640
the kind of generalization they do,

712
00:29:21,960 --> 00:29:24,240
if you get into the office

713
00:29:24,240 --> 00:29:25,680
and you try to do this kind of generalization,

714
00:29:25,680 --> 00:29:26,640
you're gonna fail at your job.

715
00:29:26,640 --> 00:29:28,640
But what is the first point, you're a programmer.

716
00:29:28,640 --> 00:29:30,640
What is the first point when you try to do that generalization,

717
00:29:30,640 --> 00:29:32,120
you would lose your job

718
00:29:32,120 --> 00:29:34,640
because you can't do the extreme generalization?

719
00:29:34,640 --> 00:29:36,120
I don't have any specific examples,

720
00:29:36,120 --> 00:29:41,120
but literally like, take this situation for instance,

721
00:29:41,120 --> 00:29:43,680
you've never been here in this room.

722
00:29:43,680 --> 00:29:46,400
Maybe you've been in this city a few times, I don't know,

723
00:29:46,400 --> 00:29:49,280
but there's a fair amount of novelty.

724
00:29:49,280 --> 00:29:51,880
You've never been interviewing me.

725
00:29:51,880 --> 00:29:53,800
There's a fair amount of novelty

726
00:29:53,800 --> 00:29:55,880
every hour of every day in your life.

727
00:29:55,880 --> 00:29:58,920
And it's in fact, by and large,

728
00:29:58,920 --> 00:30:02,280
more novelty than any LLM could handle.

729
00:30:02,280 --> 00:30:04,840
Like if you just put a LLM in a robot,

730
00:30:04,840 --> 00:30:06,200
it could not be doing all the things

731
00:30:06,200 --> 00:30:08,800
that you've been doing today, right?

732
00:30:08,800 --> 00:30:11,600
Or take on like cell driving cars, for instance.

733
00:30:11,600 --> 00:30:15,320
You take a cell driving car operating in the barrier.

734
00:30:15,320 --> 00:30:18,000
Do you think you could just drop it in New York City

735
00:30:18,000 --> 00:30:21,240
or drop it in London where people drive on the left?

736
00:30:21,240 --> 00:30:22,360
No, it's gonna fail.

737
00:30:22,360 --> 00:30:24,880
So not only can you drop, not like,

738
00:30:24,880 --> 00:30:29,880
make it generalize to a change of rules of driving rules,

739
00:30:31,600 --> 00:30:34,480
but you can not even make it generalize to a new city.

740
00:30:34,480 --> 00:30:38,200
It needs to be trained on each specific environment.

741
00:30:38,200 --> 00:30:41,680
I mean, I agree that self-driving cars aren't AGI.

742
00:30:41,680 --> 00:30:42,840
But it's the same type of model,

743
00:30:42,840 --> 00:30:44,120
they're transformers as well.

744
00:30:44,120 --> 00:30:45,640
I mean, I don't know,

745
00:30:46,240 --> 00:30:48,040
they also have brains with neurons in them,

746
00:30:48,040 --> 00:30:49,880
but they're less intelligent because they're small.

747
00:30:49,880 --> 00:30:50,720
It's not the same architecture.

748
00:30:50,720 --> 00:30:51,560
We can get into that.

749
00:30:51,560 --> 00:30:56,560
But so I still don't understand like a concrete thing of,

750
00:30:57,560 --> 00:30:58,920
we also need training.

751
00:30:58,920 --> 00:31:00,000
That's why education exists.

752
00:31:00,000 --> 00:31:02,200
That's why we had to spend the first 18 years of our life

753
00:31:02,200 --> 00:31:03,280
doing drills.

754
00:31:03,280 --> 00:31:06,160
We have a memory, but we are not a memory.

755
00:31:06,160 --> 00:31:08,120
We are not limited to just a memory.

756
00:31:08,120 --> 00:31:10,000
I'm denying the premise that that's necessarily

757
00:31:10,000 --> 00:31:11,120
the only thing these models are doing.

758
00:31:11,120 --> 00:31:13,720
And I'm still not sure what is the task

759
00:31:13,720 --> 00:31:17,000
that a remote worker would have to,

760
00:31:17,000 --> 00:31:19,200
suppose you do some remote work with an LLM

761
00:31:19,200 --> 00:31:20,480
and they're programmer,

762
00:31:20,480 --> 00:31:22,040
what is the first point that you realize

763
00:31:22,040 --> 00:31:23,760
this is not a human, this is an LLM?

764
00:31:23,760 --> 00:31:26,080
What about they just send them a knock puzzle

765
00:31:26,080 --> 00:31:27,280
and see how they do?

766
00:31:27,280 --> 00:31:29,240
No, like part of their job, you know?

767
00:31:29,240 --> 00:31:32,440
But you have to deal with novelty all the time.

768
00:31:32,440 --> 00:31:34,720
Okay, so if you, is there a world in which

769
00:31:34,720 --> 00:31:36,560
all the programmers are replaced?

770
00:31:36,560 --> 00:31:38,360
And then we're still saying,

771
00:31:38,360 --> 00:31:40,120
but they're only doing memorization

772
00:31:40,120 --> 00:31:41,440
late in programming tasks,

773
00:31:41,440 --> 00:31:43,160
but they're still producing a trillion dollars

774
00:31:44,160 --> 00:31:46,600
worth of output in the form of code.

775
00:31:46,600 --> 00:31:48,400
Software development is actually a pretty good example

776
00:31:48,400 --> 00:31:51,640
of a job where you're dealing with novelty all the time.

777
00:31:51,640 --> 00:31:53,720
Or if you're not, well, I'm not sure what you're doing.

778
00:31:53,720 --> 00:31:57,600
So I personally use Genetic VI very little

779
00:31:57,600 --> 00:31:59,720
in my software development job.

780
00:31:59,720 --> 00:32:03,080
And before LLMs, I think I was also using

781
00:32:03,080 --> 00:32:05,000
Stack Overflow very little.

782
00:32:05,000 --> 00:32:07,440
You know, some people maybe are just copy-pasting stuff

783
00:32:07,440 --> 00:32:08,280
from Stack Overflow,

784
00:32:08,280 --> 00:32:10,680
or nowadays copy-pasting stuff from an LLM.

785
00:32:11,680 --> 00:32:14,960
Personally, I try to focus on problem-solving.

786
00:32:14,960 --> 00:32:16,920
The syntax is just a technical detail.

787
00:32:16,920 --> 00:32:19,360
What's really important is the problem-solving.

788
00:32:19,360 --> 00:32:23,960
Like the essence of programming is engineering

789
00:32:23,960 --> 00:32:27,360
mental models, like mental representations

790
00:32:27,360 --> 00:32:29,480
of the problem you're trying to solve.

791
00:32:29,480 --> 00:32:32,520
But you can, you know, we have many,

792
00:32:32,520 --> 00:32:34,200
people can interact with these systems themselves

793
00:32:34,200 --> 00:32:36,200
and you can go to chat GPT and say,

794
00:32:36,200 --> 00:32:38,640
here's a specification of the kind of program I want.

795
00:32:38,640 --> 00:32:39,560
They'll build it for you.

796
00:32:39,640 --> 00:32:41,800
As long as there are many examples of this program

797
00:32:41,800 --> 00:32:43,600
on like GitHub and Stack Overflow and so on,

798
00:32:43,600 --> 00:32:47,320
sure, they will fetch the program for you from their memory.

799
00:32:47,320 --> 00:32:49,160
But you can change arbitrary details.

800
00:32:49,160 --> 00:32:52,320
You can say I need it to work on this different kind of server.

801
00:32:52,320 --> 00:32:55,920
If that were true, there would be no software engineers today.

802
00:32:55,920 --> 00:32:58,280
I agree. We're not at a full AGI yet,

803
00:32:58,280 --> 00:33:00,360
in the sense that these models have,

804
00:33:00,360 --> 00:33:02,560
let's say, less than a trillion parameters.

805
00:33:02,560 --> 00:33:04,200
A human brain has somewhere on the order

806
00:33:04,200 --> 00:33:05,960
of 10 to 30 trillion synapses.

807
00:33:05,960 --> 00:33:08,480
I mean, if you were just doing some naive math,

808
00:33:08,520 --> 00:33:11,120
you're like at least 10x under parameterized.

809
00:33:11,120 --> 00:33:12,640
So I agree we're not there yet,

810
00:33:12,640 --> 00:33:17,280
but I'm sort of confused on why we're not on the spectrum,

811
00:33:17,280 --> 00:33:19,400
where yes, I agree that there's many kinds

812
00:33:19,400 --> 00:33:20,880
of generalization they can do,

813
00:33:20,880 --> 00:33:22,760
but it seems like they're on this kind of smooth spectrum

814
00:33:22,760 --> 00:33:24,160
that we see even within humans,

815
00:33:24,160 --> 00:33:27,360
where some humans would have a hard time doing an ARC type test.

816
00:33:27,360 --> 00:33:28,800
We see that based on the performance

817
00:33:28,800 --> 00:33:31,040
on progressive Ravens matrices type IQ tests.

818
00:33:31,040 --> 00:33:34,040
I'm not a fan of IQ tests because for the most part,

819
00:33:34,040 --> 00:33:37,720
you can train on IQ tests and get better at them.

820
00:33:37,760 --> 00:33:39,880
So they have very much memorization based.

821
00:33:39,880 --> 00:33:42,360
And this is actually the main pitfall

822
00:33:42,360 --> 00:33:45,880
that ARC tries not to fall far.

823
00:33:45,880 --> 00:33:46,760
I'm still not confused.

824
00:33:46,760 --> 00:33:49,880
So if all remote jobs are automated

825
00:33:49,880 --> 00:33:52,240
in the next five years, let's say,

826
00:33:52,240 --> 00:33:54,800
at least that don't require you to be like sort of a service.

827
00:33:54,800 --> 00:33:56,000
It's not like a salesperson

828
00:33:56,000 --> 00:33:57,480
where you want the human to be talking,

829
00:33:57,480 --> 00:33:58,840
but like programming or whatever.

830
00:33:58,840 --> 00:34:02,840
In that world, would you say that that's not possible

831
00:34:02,840 --> 00:34:05,600
because a lot of what a programmer needs to do,

832
00:34:05,600 --> 00:34:07,640
definitely requires things that would not be

833
00:34:07,640 --> 00:34:08,720
in any free training corpus?

834
00:34:08,720 --> 00:34:10,040
Sure. I mean, in five years,

835
00:34:10,040 --> 00:34:11,560
there will be more software engineers

836
00:34:11,560 --> 00:34:13,200
than there are today and not too well.

837
00:34:13,200 --> 00:34:14,280
But I just want to understand.

838
00:34:14,280 --> 00:34:16,040
So I'm still not sure.

839
00:34:16,040 --> 00:34:18,160
I mean, I know how to, I studied computer science.

840
00:34:18,160 --> 00:34:20,120
I think if I had become a code monkey out of college,

841
00:34:20,120 --> 00:34:22,040
like what would I be doing?

842
00:34:22,040 --> 00:34:23,320
I go to my job.

843
00:34:23,320 --> 00:34:26,360
What is the first thing my boss tells me something to do?

844
00:34:26,360 --> 00:34:30,280
When does he realize I'm an LLM if I was an LLM?

845
00:34:30,280 --> 00:34:32,560
Probably on the first day, you know?

846
00:34:32,560 --> 00:34:33,400
Again,

847
00:34:33,680 --> 00:34:38,680
if it were true that LLMs could generalize

848
00:34:40,000 --> 00:34:41,560
to novel problems like this

849
00:34:41,560 --> 00:34:45,040
and you can actually develop software

850
00:34:45,040 --> 00:34:46,680
to solve a problem they've never seen before,

851
00:34:46,680 --> 00:34:48,720
you would not need software engineers anymore.

852
00:34:48,720 --> 00:34:51,360
In practice, if I look at how people are using LLMs

853
00:34:51,360 --> 00:34:53,160
in their software engineering job today,

854
00:34:53,160 --> 00:34:56,640
they're using it as a stack of a flow replacement.

855
00:34:56,640 --> 00:35:00,840
So they're using it as a way to copy paste code snippets

856
00:35:00,840 --> 00:35:03,080
to perform very common actions.

857
00:35:03,080 --> 00:35:06,600
And what they actually need is a database of code snippets.

858
00:35:06,600 --> 00:35:09,560
They don't actually need any of the abilities

859
00:35:09,560 --> 00:35:10,880
that actually make them software engineers.

860
00:35:10,880 --> 00:35:13,560
I mean, when we talk about interpolating

861
00:35:13,560 --> 00:35:15,240
between stack overflow databases,

862
00:35:15,240 --> 00:35:16,600
if you look at the kinds of math problems

863
00:35:16,600 --> 00:35:20,040
or coding problems, maybe to say that they're,

864
00:35:21,520 --> 00:35:22,960
maybe let's step back on interpolation

865
00:35:22,960 --> 00:35:24,640
and let me ask the question this way.

866
00:35:24,640 --> 00:35:26,000
Why can't creativity,

867
00:35:26,000 --> 00:35:28,280
why isn't creativity just interpolation

868
00:35:28,280 --> 00:35:31,400
in a higher dimension where if a bigger model

869
00:35:31,400 --> 00:35:33,320
can learn a more complex manifold,

870
00:35:33,320 --> 00:35:34,800
we're gonna use the ML language.

871
00:35:34,800 --> 00:35:38,240
And if you look at read a biography of a scientist,

872
00:35:38,240 --> 00:35:40,360
it doesn't feel like they're not zero shot

873
00:35:40,360 --> 00:35:41,280
in new scientific theories.

874
00:35:41,280 --> 00:35:43,160
They're playing with existing ideas.

875
00:35:43,160 --> 00:35:45,200
They're trying to juxtapose them in their head.

876
00:35:45,200 --> 00:35:49,720
They try out some like slightly ever in the tree

877
00:35:49,720 --> 00:35:51,600
of intellectual descendants,

878
00:35:51,600 --> 00:35:53,680
they try out a different evolutionary path.

879
00:35:53,680 --> 00:35:55,960
You sort of run the experiment there

880
00:35:55,960 --> 00:35:57,920
in terms of publishing the paper, whatever.

881
00:35:57,920 --> 00:35:59,480
It seems like a similar kind of thing humans are doing.

882
00:35:59,480 --> 00:36:01,800
There's like at a higher level of generalization.

883
00:36:01,800 --> 00:36:04,040
And what you see across bigger and bigger models

884
00:36:04,040 --> 00:36:05,360
is they seem to be approaching

885
00:36:05,360 --> 00:36:06,960
higher and higher level of generalization

886
00:36:06,960 --> 00:36:10,360
where GPT-2 couldn't do a great school level math problem

887
00:36:10,360 --> 00:36:11,440
that requires more generalization

888
00:36:11,440 --> 00:36:13,640
that it has capability for, even that skill.

889
00:36:13,640 --> 00:36:15,760
Then GPT-3 and 4 can.

890
00:36:15,760 --> 00:36:16,600
So not quite.

891
00:36:16,600 --> 00:36:19,960
So GPT-4 has a higher degree of skill

892
00:36:19,960 --> 00:36:21,520
and higher range of skills.

893
00:36:21,520 --> 00:36:22,360
Because it's-

894
00:36:22,360 --> 00:36:23,200
I don't want to get into semantics here,

895
00:36:23,200 --> 00:36:24,040
but I think-

896
00:36:24,040 --> 00:36:24,880
The same degree of generalization.

897
00:36:24,880 --> 00:36:25,720
I don't want to get into semantics here,

898
00:36:25,720 --> 00:36:28,640
but the question of why can't creativity

899
00:36:28,640 --> 00:36:32,800
be just interpolation on a higher dimension?

900
00:36:32,800 --> 00:36:35,320
I think interpolation can be creative, absolutely.

901
00:36:35,320 --> 00:36:36,920
And you know, to your point,

902
00:36:36,920 --> 00:36:39,160
I do think that on some level,

903
00:36:39,160 --> 00:36:41,640
humans also do a lot of memorization,

904
00:36:41,640 --> 00:36:43,440
a lot of reciting, a lot of pattern matching,

905
00:36:43,440 --> 00:36:45,120
a lot of interpolation as well.

906
00:36:45,120 --> 00:36:47,640
So it's very much a spectrum

907
00:36:48,920 --> 00:36:51,760
between pattern matching and true reasoning.

908
00:36:51,760 --> 00:36:52,600
It's a spectrum.

909
00:36:52,600 --> 00:36:57,480
And humans are never really at one end of the spectrum.

910
00:36:57,480 --> 00:36:59,720
They're never really doing pure pattern matching

911
00:36:59,720 --> 00:37:00,560
or pure reasoning.

912
00:37:00,560 --> 00:37:02,920
They're usually doing some mixture of both.

913
00:37:02,920 --> 00:37:06,400
Even if you're doing something that seems very reasoning heavy,

914
00:37:06,400 --> 00:37:08,880
like proving a mathematical theorem,

915
00:37:08,880 --> 00:37:09,960
as you're doing it, sure,

916
00:37:09,960 --> 00:37:12,480
you're doing quite a bit of discrete search in your mind,

917
00:37:12,480 --> 00:37:14,360
quite a bit of actual reasoning.

918
00:37:14,360 --> 00:37:18,120
But you're also very much guided by intuition,

919
00:37:18,120 --> 00:37:19,280
guided by pattern matching,

920
00:37:19,280 --> 00:37:22,960
guided by the shape of proofs that you've seen before,

921
00:37:22,960 --> 00:37:25,000
by your knowledge of mathematics.

922
00:37:25,000 --> 00:37:26,840
So it's never really,

923
00:37:26,880 --> 00:37:28,200
you know, all of our thoughts,

924
00:37:28,200 --> 00:37:31,760
everything we do is a mixture of this sort of like

925
00:37:31,760 --> 00:37:34,280
interpolative memorization based thinking,

926
00:37:34,280 --> 00:37:38,600
this sort of like type one thinking and type two thinking.

927
00:37:40,280 --> 00:37:43,200
Why are bigger models more sample efficient?

928
00:37:43,200 --> 00:37:47,600
Because they have more reusable building blocks

929
00:37:47,600 --> 00:37:52,120
that they can lean on to pick up new patterns

930
00:37:52,120 --> 00:37:52,960
in their train data.

931
00:37:52,960 --> 00:37:54,680
And does that pattern keep continuing

932
00:37:54,680 --> 00:37:56,040
as you keep getting bigger and bigger?

933
00:37:56,040 --> 00:37:58,280
To the extent that the new patterns

934
00:37:58,280 --> 00:38:00,520
you're giving the model to learn

935
00:38:00,520 --> 00:38:03,000
are good match for what it has learned before.

936
00:38:03,000 --> 00:38:05,240
If you present something that is actually novel,

937
00:38:05,240 --> 00:38:07,600
that is not in a state of distribution like an arc puzzle,

938
00:38:07,600 --> 00:38:09,240
for instance, it will fail.

939
00:38:09,240 --> 00:38:10,240
Let me make this claim.

940
00:38:10,240 --> 00:38:12,360
The program synthesis I think is a very,

941
00:38:12,360 --> 00:38:14,120
very useful intuition pump.

942
00:38:14,120 --> 00:38:15,600
Why can't it be the case that what's happening

943
00:38:15,600 --> 00:38:19,440
in the transformer is the early layers are doing the,

944
00:38:19,440 --> 00:38:22,240
figuring out how to represent the inputting tokens.

945
00:38:22,240 --> 00:38:24,600
And what the middle layers do is this kind of program search,

946
00:38:24,600 --> 00:38:27,440
program synthesis, where they combine the inputs

947
00:38:27,440 --> 00:38:31,080
to all the circuits in the model

948
00:38:31,080 --> 00:38:33,920
where they go from the low level representation

949
00:38:33,920 --> 00:38:35,240
to a higher level representation

950
00:38:35,240 --> 00:38:36,320
near the middle of the model.

951
00:38:36,320 --> 00:38:39,680
They use these programs, they combine these concepts,

952
00:38:39,680 --> 00:38:42,600
then what comes out at the other end is the reasoning

953
00:38:42,600 --> 00:38:45,160
based on that high level intelligence.

954
00:38:45,160 --> 00:38:46,640
Possibly, why not?

955
00:38:47,720 --> 00:38:50,840
But if these models were actually capable

956
00:38:50,840 --> 00:38:54,440
of synthesizing novel programs,

957
00:38:54,480 --> 00:38:57,360
however simple they should be able to do arc

958
00:38:57,360 --> 00:38:59,400
because for any arc task,

959
00:38:59,400 --> 00:39:02,320
if you write down the solution program in Python,

960
00:39:02,320 --> 00:39:05,600
it's not a complex program, it's extremely simple

961
00:39:05,600 --> 00:39:07,880
and humans can figure it out.

962
00:39:07,880 --> 00:39:10,240
So why can LLMs not do it?

963
00:39:10,240 --> 00:39:12,880
Okay, I think that's a fair point.

964
00:39:12,880 --> 00:39:15,480
And if I turn the question around to you,

965
00:39:15,480 --> 00:39:18,960
so suppose that it's the case that in a year,

966
00:39:18,960 --> 00:39:22,200
a multimodal model can solve arc,

967
00:39:22,240 --> 00:39:25,520
let's say get 80%, whatever the average human would get,

968
00:39:25,520 --> 00:39:27,200
then AGI?

969
00:39:27,200 --> 00:39:28,360
Quite possibly, yes.

970
00:39:28,360 --> 00:39:30,960
I think if you start, so honestly,

971
00:39:30,960 --> 00:39:34,720
what I would like to see is an LLM type model

972
00:39:34,720 --> 00:39:36,760
solving arc at like 80%,

973
00:39:36,760 --> 00:39:39,880
but after having only been trained

974
00:39:39,880 --> 00:39:42,840
on core knowledge related stuff.

975
00:39:42,840 --> 00:39:45,320
But human kids, I don't think we're necessarily

976
00:39:45,320 --> 00:39:47,360
just trading on, it's not just that we have

977
00:39:47,360 --> 00:39:48,800
in our show is object permanence.

978
00:39:48,800 --> 00:39:50,600
Okay, let me rephrase that.

979
00:39:50,600 --> 00:39:55,600
Only trained on information that is not explicitly

980
00:39:55,720 --> 00:39:58,960
trying to anticipate what's gonna be in the arc test set.

981
00:39:58,960 --> 00:40:01,280
But isn't the whole point of arc that you can't,

982
00:40:01,280 --> 00:40:03,640
sort of, it's a new type of intelligence

983
00:40:03,640 --> 00:40:04,480
every single time?

984
00:40:04,480 --> 00:40:05,320
Yes, that is the point.

985
00:40:05,320 --> 00:40:07,880
So if arc were perfect, flawless benchmark,

986
00:40:07,880 --> 00:40:10,680
it would be impossible to anticipate within the test set.

987
00:40:10,680 --> 00:40:14,160
And arc was released more than four years ago

988
00:40:14,160 --> 00:40:16,640
and so far it's been resistant to memorization.

989
00:40:16,640 --> 00:40:20,840
So I think it has, to some extent, passed a test of time.

990
00:40:20,840 --> 00:40:22,960
But I don't think it's perfect.

991
00:40:22,960 --> 00:40:26,200
I think if you try to make by hand

992
00:40:27,120 --> 00:40:29,240
hundreds of thousands of arc tasks

993
00:40:29,240 --> 00:40:31,800
and then you try to multiply them

994
00:40:32,760 --> 00:40:35,000
by programmatically generating variations

995
00:40:35,000 --> 00:40:38,520
and then you end up with maybe hundreds of millions of tasks.

996
00:40:38,520 --> 00:40:41,040
Just by brute forcing the task space,

997
00:40:41,040 --> 00:40:43,680
there will be enough overlap between what you're trained on

998
00:40:43,680 --> 00:40:45,440
and what's in the test set that you can actually score

999
00:40:45,440 --> 00:40:46,280
very highly.

1000
00:40:46,280 --> 00:40:49,600
So, you know, with enough scale, you can always cheat.

1001
00:40:49,600 --> 00:40:51,240
If you can do this for every single thing

1002
00:40:51,240 --> 00:40:52,880
that supposedly requires intelligence,

1003
00:40:52,880 --> 00:40:53,920
then what good is intelligence?

1004
00:40:53,920 --> 00:40:55,560
Apparently you can just brute force intelligence.

1005
00:40:55,560 --> 00:40:59,920
If the world, if your life, were a static distribution,

1006
00:40:59,920 --> 00:41:01,520
then sure, you could just brute force

1007
00:41:01,520 --> 00:41:03,720
the space of possible behaviors.

1008
00:41:03,720 --> 00:41:07,960
You could like, you know, the way we think about intelligence,

1009
00:41:07,960 --> 00:41:09,960
there are several metaphors, I like to use,

1010
00:41:09,960 --> 00:41:12,640
but one of them is you can think of intelligence

1011
00:41:12,640 --> 00:41:17,000
as a past finding algorithm in future situation space.

1012
00:41:17,000 --> 00:41:19,160
Like, I don't know if you're familiar with game development,

1013
00:41:19,160 --> 00:41:23,400
like RTS game development, but you have a map, right?

1014
00:41:23,400 --> 00:41:26,000
And you have, it's like a 2D map.

1015
00:41:26,000 --> 00:41:28,800
And you have partial information about it.

1016
00:41:28,800 --> 00:41:31,880
Like there is some fog of war on your map.

1017
00:41:31,880 --> 00:41:34,040
There are areas that you haven't explored yet.

1018
00:41:34,040 --> 00:41:35,080
You know nothing about them.

1019
00:41:35,080 --> 00:41:36,640
And then there are areas that you've explored,

1020
00:41:36,640 --> 00:41:39,920
but you only know how they were like in the past.

1021
00:41:39,920 --> 00:41:41,520
You don't know how they are like today.

1022
00:41:41,520 --> 00:41:46,760
And now, instead of thinking about a 2D map,

1023
00:41:46,760 --> 00:41:50,040
think about the space of possible future situations

1024
00:41:50,040 --> 00:41:52,640
that you might encounter and how they're connected to each other.

1025
00:41:52,640 --> 00:41:54,440
Intelligence is a past finding algorithm.

1026
00:41:54,440 --> 00:41:57,400
So once you set a goal, it will tell you

1027
00:41:57,400 --> 00:42:00,680
how to get there optimally.

1028
00:42:00,680 --> 00:42:05,240
But of course, it's constrained by the information you have.

1029
00:42:05,240 --> 00:42:09,240
It cannot pass find in an area that you know nothing about.

1030
00:42:09,240 --> 00:42:12,640
It cannot also anticipate changes.

1031
00:42:12,640 --> 00:42:20,040
And the thing is, if you had complete information about the map,

1032
00:42:20,040 --> 00:42:22,360
then you could solve the past finding problem

1033
00:42:22,360 --> 00:42:26,040
by simply memorizing every possible path, every mapping

1034
00:42:26,040 --> 00:42:31,320
from point A to point B. You could solve the problem

1035
00:42:31,320 --> 00:42:32,400
with pure memory.

1036
00:42:32,400 --> 00:42:35,040
But the reason you cannot do that in real life

1037
00:42:35,040 --> 00:42:37,120
is because you don't actually know what's

1038
00:42:37,120 --> 00:42:38,920
going to happen in the future.

1039
00:42:39,800 --> 00:42:41,600
I feel like you're using words like memorization, which

1040
00:42:41,600 --> 00:42:43,000
we would never use for human children.

1041
00:42:43,000 --> 00:42:46,680
If your kid learns to do algebra and then now learns

1042
00:42:46,680 --> 00:42:48,760
to do calculus, you wouldn't say they memorized calculus.

1043
00:42:48,760 --> 00:42:52,160
If they can just solve any arbitrary algebraic problem,

1044
00:42:52,160 --> 00:42:54,000
you wouldn't say they memorized algebra.

1045
00:42:54,000 --> 00:42:55,080
They say they've learned algebra.

1046
00:42:55,080 --> 00:42:58,160
Humans are never redoing pure memorization or pure reasoning.

1047
00:42:58,160 --> 00:42:59,520
But that's only because you're semantically

1048
00:42:59,520 --> 00:43:01,160
labeling when the human does the skill.

1049
00:43:01,160 --> 00:43:03,560
It's a memorization when the exact same skill is done by the LLM

1050
00:43:03,560 --> 00:43:04,880
as you can measure by these benchmarks.

1051
00:43:04,880 --> 00:43:06,640
And you can just plug in any sort of math problem.

1052
00:43:06,680 --> 00:43:08,760
Sometimes humans are doing the exact same as the LLM

1053
00:43:08,760 --> 00:43:10,840
is doing, which is just, for instance,

1054
00:43:10,840 --> 00:43:12,640
I know if you learn to add numbers,

1055
00:43:12,640 --> 00:43:14,800
you're memorizing an algorithm.

1056
00:43:14,800 --> 00:43:15,960
You're memorizing a program.

1057
00:43:15,960 --> 00:43:17,640
And then you can reapply it.

1058
00:43:17,640 --> 00:43:21,720
You are not synthesizing on the fly the addition program.

1059
00:43:21,720 --> 00:43:24,000
So obviously at some point, some human had to figure out

1060
00:43:24,000 --> 00:43:24,600
how to do addition.

1061
00:43:24,600 --> 00:43:27,000
But the way a kid learns it is not

1062
00:43:27,000 --> 00:43:30,160
that they figure out from the actions of set theory

1063
00:43:30,160 --> 00:43:30,800
how to do addition.

1064
00:43:30,800 --> 00:43:32,120
I think what you're learning in school

1065
00:43:32,120 --> 00:43:33,960
is mostly memorization.

1066
00:43:34,000 --> 00:43:37,080
So my claim is that, listen, these models

1067
00:43:37,080 --> 00:43:40,880
are vastly underparameterized relative to how many flops

1068
00:43:40,880 --> 00:43:43,240
or how many parameters you have in the human brain.

1069
00:43:43,240 --> 00:43:45,800
And so yeah, they're not going to be coming up

1070
00:43:45,800 --> 00:43:48,800
with new theorems like the smartest humans can.

1071
00:43:48,800 --> 00:43:51,120
But most humans can't do that either.

1072
00:43:51,120 --> 00:43:52,880
What most humans do, it sounds like it's

1073
00:43:52,880 --> 00:43:55,040
similar to what you were calling memorization, which

1074
00:43:55,040 --> 00:44:00,120
is memorizing skills or memorizing techniques

1075
00:44:00,120 --> 00:44:01,080
that you've learned.

1076
00:44:01,080 --> 00:44:03,560
And so it sounds like it's compatible.

1077
00:44:03,680 --> 00:44:04,720
Tell me if this is wrong.

1078
00:44:04,720 --> 00:44:07,600
Is it compatible in your world if all the remote workers

1079
00:44:07,600 --> 00:44:09,960
are gone, but they're doing skills

1080
00:44:09,960 --> 00:44:12,120
which we can potentially make synthetic data of?

1081
00:44:12,120 --> 00:44:15,440
So we record everybody's screen and every single remote worker

1082
00:44:15,440 --> 00:44:16,160
screen.

1083
00:44:16,160 --> 00:44:18,680
We sort of understand the skills they're performing there.

1084
00:44:18,680 --> 00:44:20,680
And now we've trained a model that can do all this.

1085
00:44:20,680 --> 00:44:22,360
All the remote workers are unemployed.

1086
00:44:22,360 --> 00:44:23,760
We're generating trillions of dollars

1087
00:44:23,760 --> 00:44:26,600
to economic activity for AI remote workers.

1088
00:44:26,600 --> 00:44:28,920
In that world, are we still in the memorization regime?

1089
00:44:28,920 --> 00:44:30,320
So sure.

1090
00:44:30,360 --> 00:44:33,840
With memorization, you can automate almost anything

1091
00:44:33,840 --> 00:44:35,880
as long as it's a static distribution,

1092
00:44:35,880 --> 00:44:38,000
as long as you don't have to deal with change.

1093
00:44:38,000 --> 00:44:41,000
Are most jobs part of such a static distribution?

1094
00:44:41,000 --> 00:44:44,400
Potentially, there are lots of things that you can automate.

1095
00:44:44,400 --> 00:44:46,880
And LLMs are an excellent tool for automation.

1096
00:44:46,880 --> 00:44:48,240
And I think that's true.

1097
00:44:48,240 --> 00:44:50,520
But you have to understand that automation is not

1098
00:44:50,520 --> 00:44:51,440
the same as intelligence.

1099
00:44:51,440 --> 00:44:53,800
I'm not saying that LLMs are useless.

1100
00:44:53,800 --> 00:44:57,000
I've been a huge proponent of deep learning for many years.

1101
00:44:57,000 --> 00:44:58,920
And for many years, I've been saying two things.

1102
00:44:58,920 --> 00:45:01,680
I've been saying that if you keep scaling up deep learning,

1103
00:45:01,680 --> 00:45:03,200
it will keep paying off.

1104
00:45:03,200 --> 00:45:04,720
And at the same time, I've been saying,

1105
00:45:04,720 --> 00:45:06,200
if you keep scaling up deep learning,

1106
00:45:06,200 --> 00:45:08,640
this will not lead to a GI.

1107
00:45:08,640 --> 00:45:10,680
So we can automate more and more things.

1108
00:45:10,680 --> 00:45:12,520
And yes, this is economically valuable.

1109
00:45:12,520 --> 00:45:14,480
And yes, potentially, there are many jobs.

1110
00:45:14,480 --> 00:45:15,840
You could automate a way like this.

1111
00:45:15,840 --> 00:45:17,880
And that would be economically valuable.

1112
00:45:17,880 --> 00:45:20,080
But you're still not going to have intelligence.

1113
00:45:20,080 --> 00:45:22,400
So you can ask, OK, so what does it

1114
00:45:22,400 --> 00:45:24,680
matter if we can generate all this economic value?

1115
00:45:24,680 --> 00:45:26,280
Maybe we don't need intelligence after all.

1116
00:45:26,280 --> 00:45:28,440
Well, you need intelligence the moment

1117
00:45:28,480 --> 00:45:32,120
you have to deal with change, with novelty, with uncertainty.

1118
00:45:32,120 --> 00:45:34,000
As long as you are in a space that

1119
00:45:34,000 --> 00:45:37,240
can be exactly described in advance,

1120
00:45:37,240 --> 00:45:41,600
you can just automate your pure memorization.

1121
00:45:41,600 --> 00:45:44,120
In fact, you can always solve any problem.

1122
00:45:44,120 --> 00:45:48,760
You can always display arbitrary levels of skills

1123
00:45:48,760 --> 00:45:54,640
on any task without leveraging any intelligence whatsoever,

1124
00:45:54,640 --> 00:45:58,840
as long as it is possible to describe the problem

1125
00:45:58,840 --> 00:46:01,320
and its solution very, very precisely.

1126
00:46:01,320 --> 00:46:03,200
But when they do deal with novelty,

1127
00:46:03,200 --> 00:46:05,200
then you just call it interpolation, right?

1128
00:46:05,200 --> 00:46:08,240
And so interpolation is not enough

1129
00:46:08,240 --> 00:46:09,920
to deal with all kinds of novelty

1130
00:46:09,920 --> 00:46:13,360
if it were, then LLMs would be a GI.

1131
00:46:13,360 --> 00:46:14,360
Well, I agree they're not a GI.

1132
00:46:14,360 --> 00:46:16,520
I'm just trying to figure out how do we figure out

1133
00:46:16,520 --> 00:46:17,320
we're on the path to a GI.

1134
00:46:17,320 --> 00:46:20,680
And I think a sort of crux here is maybe

1135
00:46:20,680 --> 00:46:23,840
that it seems to me that these things are on a spectrum

1136
00:46:23,880 --> 00:46:26,640
and we're clearly covering the earliest part of the spectrum

1137
00:46:26,640 --> 00:46:27,480
with LLMs.

1138
00:46:27,480 --> 00:46:28,320
I think so.

1139
00:46:28,320 --> 00:46:29,440
And oh, OK, interesting.

1140
00:46:29,440 --> 00:46:31,520
But here's another sort of thing

1141
00:46:31,520 --> 00:46:34,240
that I think is evidence for this, grokking, right?

1142
00:46:34,240 --> 00:46:36,760
So clearly, even within deep learning,

1143
00:46:36,760 --> 00:46:39,560
there's a difference between the memorization regime

1144
00:46:39,560 --> 00:46:42,640
and the generalization regime, where at first they'll just

1145
00:46:42,640 --> 00:46:46,800
memorize the data set of if you're doing modular addition,

1146
00:46:46,800 --> 00:46:47,920
how to add digits.

1147
00:46:47,920 --> 00:46:50,120
And then at some point, if you keep training on that,

1148
00:46:50,120 --> 00:46:51,280
they'll learn the skill.

1149
00:46:51,280 --> 00:46:53,360
So the fact that there is that distinction

1150
00:46:53,360 --> 00:46:56,400
suggests that the generalized circuit, the deep learning

1151
00:46:56,400 --> 00:46:59,560
can learn, there is a regime in enters where it generalizes.

1152
00:46:59,560 --> 00:47:01,120
If you have an over-parameterized model,

1153
00:47:01,120 --> 00:47:02,960
which you don't have in comparison to all the tasks

1154
00:47:02,960 --> 00:47:04,720
we want these models to do right now.

1155
00:47:04,720 --> 00:47:06,320
Grokking is a very, very old phenomenon.

1156
00:47:06,320 --> 00:47:09,160
We've been observing it for decades.

1157
00:47:09,160 --> 00:47:13,480
It's basically an instance of the minimum description length

1158
00:47:13,480 --> 00:47:16,680
principle, where, sure, given a problem,

1159
00:47:16,680 --> 00:47:22,760
you can just memorize a point-wise input-to-output mapping,

1160
00:47:22,800 --> 00:47:24,160
which is completely overfit.

1161
00:47:24,160 --> 00:47:26,000
So it does not generalize at all,

1162
00:47:26,000 --> 00:47:29,560
but it solves the problem on the train data.

1163
00:47:29,560 --> 00:47:33,320
And from there, you can actually keep proving it,

1164
00:47:33,320 --> 00:47:36,240
keep making your mapping simpler and simpler and more

1165
00:47:36,240 --> 00:47:37,320
compressed.

1166
00:47:37,320 --> 00:47:40,520
And at some point, it will start generalizing.

1167
00:47:40,520 --> 00:47:44,280
And so that's something called the minimum description

1168
00:47:44,280 --> 00:47:45,040
length principle.

1169
00:47:45,040 --> 00:47:48,520
It's this idea that the program that will generalize best

1170
00:47:48,520 --> 00:47:51,280
is the shortest, right?

1171
00:47:51,280 --> 00:47:54,520
And it doesn't mean that you're doing anything

1172
00:47:54,520 --> 00:47:55,840
other than memorization, but you're

1173
00:47:55,840 --> 00:47:58,480
doing memorization plus regularization.

1174
00:47:58,480 --> 00:48:00,680
Right, AKA generalization.

1175
00:48:00,680 --> 00:48:03,640
Yeah, and that is absolutely, at least to generalization.

1176
00:48:03,640 --> 00:48:05,560
Right, and then so you do that within one skill,

1177
00:48:05,560 --> 00:48:07,720
but then the pattern you see here of meta-learning

1178
00:48:07,720 --> 00:48:10,760
is that it's more efficient to store a program that can perform

1179
00:48:10,760 --> 00:48:13,240
many skills rather than one skill, which is what we might

1180
00:48:13,240 --> 00:48:14,520
call fluid intelligence.

1181
00:48:14,520 --> 00:48:16,040
And so as you get bigger and bigger models,

1182
00:48:16,040 --> 00:48:18,720
you would expect it to go up this hierarchy of generalization

1183
00:48:18,720 --> 00:48:20,880
where it generalizes to a skill, then it generalizes

1184
00:48:21,080 --> 00:48:22,040
multiple skills.

1185
00:48:22,040 --> 00:48:23,400
That's correct, that's correct.

1186
00:48:23,400 --> 00:48:27,200
And you know, LLMs, they're not infinitely large.

1187
00:48:27,200 --> 00:48:29,560
They have only a fixed number of parameters.

1188
00:48:29,560 --> 00:48:32,440
And so they have to compress their knowledge

1189
00:48:32,440 --> 00:48:33,720
as much as possible.

1190
00:48:33,720 --> 00:48:35,560
And in practice, so LLMs are mostly

1191
00:48:35,560 --> 00:48:40,400
storing reusable bits of programs, like vector programs.

1192
00:48:40,400 --> 00:48:42,800
And because they have this need for compression,

1193
00:48:42,800 --> 00:48:44,960
it means that every time they're learning a new program,

1194
00:48:44,960 --> 00:48:47,120
they're going to try to express it

1195
00:48:47,120 --> 00:48:50,160
in terms of existing bits and pieces of programs

1196
00:48:50,160 --> 00:48:52,440
that they've already learned before, right?

1197
00:48:52,440 --> 00:48:54,560
Isn't this the generalization?

1198
00:48:54,560 --> 00:48:55,760
Absolutely.

1199
00:48:55,760 --> 00:48:57,200
Oh, wait, so.

1200
00:48:57,200 --> 00:48:59,360
This is why, you know, clearly LLMs

1201
00:48:59,360 --> 00:49:01,400
have some degree of generalization.

1202
00:49:01,400 --> 00:49:03,880
And this is precisely why, it's because they have to compress.

1203
00:49:03,880 --> 00:49:05,680
And why is that intrinsically limited?

1204
00:49:05,680 --> 00:49:07,760
Why can't you just go, at some point,

1205
00:49:07,760 --> 00:49:09,600
it has to learn a higher level of generalization,

1206
00:49:09,600 --> 00:49:11,360
a higher level, and then the highest level

1207
00:49:11,360 --> 00:49:12,440
is the fluid intelligence.

1208
00:49:12,440 --> 00:49:15,080
It's intrinsically limited because the substrate

1209
00:49:15,080 --> 00:49:18,360
of your model is a big parametric curve.

1210
00:49:18,360 --> 00:49:21,960
And all you can do with this is local generalization.

1211
00:49:21,960 --> 00:49:25,480
If you want to go beyond this towards broader

1212
00:49:25,480 --> 00:49:27,520
or even extreme generalization, you

1213
00:49:27,520 --> 00:49:29,840
have to move to a different type of model.

1214
00:49:29,840 --> 00:49:34,000
And my paradigm of choice is discrete program search,

1215
00:49:34,000 --> 00:49:35,040
program synthesis.

1216
00:49:35,040 --> 00:49:37,280
So and if you want to understand that,

1217
00:49:37,280 --> 00:49:41,520
you can sort of like compare and contrast it with deep learning.

1218
00:49:41,520 --> 00:49:45,400
So in deep learning, your model is a parametric curve,

1219
00:49:45,400 --> 00:49:47,080
a differentiable parametric curve.

1220
00:49:47,080 --> 00:49:49,640
In program synthesis, your model

1221
00:49:49,640 --> 00:49:52,560
is a discrete graph of operators.

1222
00:49:52,560 --> 00:49:55,080
So you've got like a set of logical operators,

1223
00:49:55,080 --> 00:49:57,400
like a domain-specific language.

1224
00:49:57,400 --> 00:49:59,800
You're picking instances of it.

1225
00:49:59,800 --> 00:50:01,960
You're structuring that into a graph.

1226
00:50:01,960 --> 00:50:03,240
That's a program.

1227
00:50:03,240 --> 00:50:05,480
And that's actually very similar to like a program

1228
00:50:05,480 --> 00:50:09,400
you might write in Python or C++ and so on.

1229
00:50:09,400 --> 00:50:11,880
And in deep learning, your learning engine,

1230
00:50:11,880 --> 00:50:13,600
because we are doing much learning here,

1231
00:50:13,600 --> 00:50:16,800
like we're trying to automatically learn these models.

1232
00:50:16,800 --> 00:50:21,320
In deep learning, your learning engine is quite in the sense.

1233
00:50:21,320 --> 00:50:24,440
And quite in the sense is very compute efficient,

1234
00:50:24,440 --> 00:50:27,080
because you have this very strong, informative feedback

1235
00:50:27,080 --> 00:50:29,760
signal about where the solution is.

1236
00:50:29,760 --> 00:50:31,760
So you can get to the solution very quickly.

1237
00:50:31,760 --> 00:50:35,040
But it is very data inefficient, meaning

1238
00:50:35,040 --> 00:50:36,840
that in order to make it work, you

1239
00:50:36,840 --> 00:50:39,640
need a dense sampling of the operating space.

1240
00:50:39,640 --> 00:50:41,800
You need a dense sampling of the data distribution.

1241
00:50:41,800 --> 00:50:44,400
And then you're limited to only generalizing

1242
00:50:44,400 --> 00:50:46,080
within that data distribution.

1243
00:50:46,080 --> 00:50:48,160
And the reason why you have this limitation

1244
00:50:48,160 --> 00:50:50,000
is because your model is a curve.

1245
00:50:50,000 --> 00:50:53,680
And meanwhile, if you look at discrete program search,

1246
00:50:53,680 --> 00:50:56,840
the learning engine is combinatorial search.

1247
00:50:56,840 --> 00:50:58,920
You're just trying a bunch of programs

1248
00:50:58,920 --> 00:51:01,800
until you find one that actually miss your spec.

1249
00:51:01,800 --> 00:51:04,240
This process is extremely data efficient.

1250
00:51:04,240 --> 00:51:06,120
You can learn a generalizable program

1251
00:51:06,120 --> 00:51:08,480
from just one example, two examples, which

1252
00:51:08,480 --> 00:51:10,720
is why it works so well on Arc, by the way.

1253
00:51:10,720 --> 00:51:14,400
But the big limitation is that it's extremely compute

1254
00:51:14,400 --> 00:51:17,320
inefficient, because you're running into combinatorial

1255
00:51:17,320 --> 00:51:18,800
explosion, of course.

1256
00:51:18,800 --> 00:51:22,320
And so you can sort of see here how

1257
00:51:22,320 --> 00:51:24,880
the planning and discrete program search,

1258
00:51:24,880 --> 00:51:29,680
they have very complementary strengths and limitations

1259
00:51:29,680 --> 00:51:30,240
as well.

1260
00:51:30,240 --> 00:51:33,680
Every limitation of deep learning has a strength,

1261
00:51:33,680 --> 00:51:37,440
a corresponding strength in program synthesis and inversely.

1262
00:51:37,440 --> 00:51:40,640
And I think the path forward is going to be to merge the two,

1263
00:51:40,640 --> 00:51:42,040
to basically start doing.

1264
00:51:42,080 --> 00:51:44,240
So another way you can think about it

1265
00:51:44,240 --> 00:51:48,760
is, so these parametric curves, train with ground descent,

1266
00:51:48,760 --> 00:51:51,080
there are great fits for everything

1267
00:51:51,080 --> 00:51:55,600
that's system one type thinking, like pattern cognition,

1268
00:51:55,600 --> 00:51:58,520
intuition, memorization, and so on.

1269
00:51:58,520 --> 00:52:02,320
And discrete program search is a great fit

1270
00:52:02,320 --> 00:52:06,000
for type two thinking, system two thinking.

1271
00:52:06,000 --> 00:52:08,960
For instance, planning, reasoning,

1272
00:52:08,960 --> 00:52:11,360
quickly figuring out a generalizable model,

1273
00:52:11,360 --> 00:52:14,000
that matches just one or two examples,

1274
00:52:14,000 --> 00:52:16,000
like for an archbishop, for instance.

1275
00:52:16,000 --> 00:52:20,640
And I think humans are never doing pure system one

1276
00:52:20,640 --> 00:52:21,600
or pure system two.

1277
00:52:21,600 --> 00:52:24,600
They're always mixing and matching both.

1278
00:52:24,600 --> 00:52:27,040
And right now, we have all the tools for system one.

1279
00:52:27,040 --> 00:52:29,320
We have almost nothing for system two.

1280
00:52:29,320 --> 00:52:32,080
The way forward is to create a hybrid system.

1281
00:52:32,080 --> 00:52:34,160
And I think the form it's going to take

1282
00:52:34,160 --> 00:52:37,240
is it's going to be mostly system two.

1283
00:52:37,240 --> 00:52:40,560
So the outer structure is going to be a discrete program

1284
00:52:40,560 --> 00:52:42,040
search system.

1285
00:52:42,040 --> 00:52:44,680
But you're going to fix the fundamental limitation

1286
00:52:44,680 --> 00:52:46,840
of discrete program search, which is combinator explosion.

1287
00:52:46,840 --> 00:52:49,440
You're going to fix it with deep learning.

1288
00:52:49,440 --> 00:52:52,400
You're going to leverage deep learning to guide,

1289
00:52:52,400 --> 00:52:55,480
to provide intuition in program space,

1290
00:52:55,480 --> 00:52:57,640
to guide the program search.

1291
00:52:57,640 --> 00:53:00,960
And I think that's very similar to what you see,

1292
00:53:00,960 --> 00:53:03,800
for instance, when you're playing chess

1293
00:53:03,800 --> 00:53:06,480
or when you're trying to prove a theorem,

1294
00:53:06,520 --> 00:53:11,400
is that it's mostly a reasoning thing,

1295
00:53:11,400 --> 00:53:13,600
but you start out with some intuition

1296
00:53:13,600 --> 00:53:15,440
about the shape of the solution.

1297
00:53:15,440 --> 00:53:18,040
And that's very much something you can get

1298
00:53:18,040 --> 00:53:19,640
via a deep learning model.

1299
00:53:19,640 --> 00:53:23,360
Deep learning models, they're very much like intuition machines.

1300
00:53:23,360 --> 00:53:25,360
They're pattern matching machines.

1301
00:53:25,360 --> 00:53:30,160
So you start from this shape of the solution,

1302
00:53:30,160 --> 00:53:33,760
and then you're going to do actual explicit discrete

1303
00:53:33,760 --> 00:53:35,160
program search.

1304
00:53:35,160 --> 00:53:38,120
But you're not going to do it via brute force.

1305
00:53:38,120 --> 00:53:42,560
You're not going to try things kind of like randomly.

1306
00:53:42,560 --> 00:53:45,920
You're actually going to ask another deep learning model

1307
00:53:45,920 --> 00:53:46,960
for suggestions.

1308
00:53:46,960 --> 00:53:49,840
Like, here's the best likely next step.

1309
00:53:49,840 --> 00:53:52,040
Here's where in the graph you should be going.

1310
00:53:52,040 --> 00:53:54,560
And you can also use yet another deep learning model

1311
00:53:54,560 --> 00:53:57,720
for feedback about, well, here's what I had so far.

1312
00:53:57,720 --> 00:53:58,760
Is it looking good?

1313
00:53:58,760 --> 00:54:01,240
Should I just backtrack and try something new?

1314
00:54:01,240 --> 00:54:06,040
So I think discrete program search is going to be the key,

1315
00:54:06,040 --> 00:54:08,200
but you want to make it dramatically better,

1316
00:54:08,200 --> 00:54:09,840
all those of magnitude more efficient,

1317
00:54:09,840 --> 00:54:11,120
by leveraging deep learning.

1318
00:54:11,120 --> 00:54:13,520
And by the way, another thing that you can use deep learning

1319
00:54:13,520 --> 00:54:16,640
is, of course, things like common sense knowledge,

1320
00:54:16,640 --> 00:54:18,920
and knowledge in general.

1321
00:54:18,920 --> 00:54:20,560
And I think you're going to end up

1322
00:54:20,560 --> 00:54:22,560
with this sort of system where you

1323
00:54:22,560 --> 00:54:27,360
have this on-the-fly synthesis engine that

1324
00:54:27,360 --> 00:54:29,400
can adapt to new situations.

1325
00:54:29,440 --> 00:54:31,160
But the way it adapts is that it's

1326
00:54:31,160 --> 00:54:35,960
going to fetch from a bank of patterns,

1327
00:54:35,960 --> 00:54:39,160
modules that could be themselves,

1328
00:54:39,160 --> 00:54:42,240
curves that could be differentiable modules,

1329
00:54:42,240 --> 00:54:44,560
and some others that could be algorithmic in nature.

1330
00:54:44,560 --> 00:54:48,760
It's going to assemble them via this process that's

1331
00:54:48,760 --> 00:54:50,360
intuition-guided.

1332
00:54:50,360 --> 00:54:52,640
And it's going to give you, for every new situation you

1333
00:54:52,640 --> 00:54:54,240
might be faced with, it's going to give you

1334
00:54:54,240 --> 00:54:57,400
with a generalizable model that was synthesized

1335
00:54:57,400 --> 00:55:00,600
using very, very little data.

1336
00:55:00,600 --> 00:55:02,520
Something like this would sort of arc.

1337
00:55:02,520 --> 00:55:05,640
That's actually a really interesting prompt,

1338
00:55:05,640 --> 00:55:08,720
because I think an interesting crux here

1339
00:55:08,720 --> 00:55:11,440
is when I talk to my friends who are extremely

1340
00:55:11,440 --> 00:55:17,480
optimistic about LLMs and expect AGI within the next couple

1341
00:55:17,480 --> 00:55:20,520
of years, they also, in some sense,

1342
00:55:20,520 --> 00:55:23,760
agree that scaling is not all you need,

1343
00:55:23,760 --> 00:55:26,600
but that the rest of the progress is undergirded

1344
00:55:26,640 --> 00:55:28,640
and enabled by scaling.

1345
00:55:28,640 --> 00:55:31,520
But still, you need to add the system

1346
00:55:31,520 --> 00:55:34,920
to the test time compute atop these models.

1347
00:55:34,920 --> 00:55:36,880
And their perspective is that it's relatively

1348
00:55:36,880 --> 00:55:38,960
straightforward to do that, because you

1349
00:55:38,960 --> 00:55:41,600
have this library of representations

1350
00:55:41,600 --> 00:55:43,560
that you built up from free training,

1351
00:55:43,560 --> 00:55:46,800
but it's almost talking like, it's just

1352
00:55:46,800 --> 00:55:48,640
like skimming through textbooks.

1353
00:55:48,640 --> 00:55:52,120
You need some more deliberate way in which it engages

1354
00:55:52,120 --> 00:55:53,480
with the material it learns.

1355
00:55:53,480 --> 00:55:56,520
In-context learning is extremely sample-efficient.

1356
00:55:56,520 --> 00:55:59,000
But to actually distill that into the weights,

1357
00:55:59,000 --> 00:56:01,480
you need the model to talk through the things that sees

1358
00:56:01,480 --> 00:56:03,000
and then add it back to the weights.

1359
00:56:03,000 --> 00:56:05,640
As far as the system 2 goes, they talk about adding some kind

1360
00:56:05,640 --> 00:56:08,280
of RL setup so that it is encouraged

1361
00:56:08,280 --> 00:56:12,640
to proceed on the reasoning traces that end up being correct.

1362
00:56:12,640 --> 00:56:14,720
And they think this is relatively straightforward stuff

1363
00:56:14,720 --> 00:56:16,560
that will be added within the next couple of years.

1364
00:56:16,560 --> 00:56:17,840
That's an empirical question.

1365
00:56:17,840 --> 00:56:18,880
So I think we'll see.

1366
00:56:18,880 --> 00:56:20,440
Your intuition, I assume, is not that.

1367
00:56:20,440 --> 00:56:21,120
I'm curious.

1368
00:56:21,120 --> 00:56:24,840
My intuition is, in fact, this whole system

1369
00:56:24,840 --> 00:56:27,160
2 architecture is the hard part.

1370
00:56:27,160 --> 00:56:29,040
It's the very hard and non-obvious part.

1371
00:56:29,040 --> 00:56:32,800
Scaling up the interpolative memory is the easy part.

1372
00:56:32,800 --> 00:56:37,080
All you need is, like, it's literally just a big curve.

1373
00:56:37,080 --> 00:56:38,120
All you need is more data.

1374
00:56:38,120 --> 00:56:39,560
It's representation of a data set,

1375
00:56:39,560 --> 00:56:41,840
interpolative representation of data set.

1376
00:56:41,840 --> 00:56:42,840
That's the easy part.

1377
00:56:42,840 --> 00:56:45,960
The hard part is the architecture of intelligence.

1378
00:56:45,960 --> 00:56:48,600
Memory and intelligence are separate components.

1379
00:56:48,600 --> 00:56:49,480
We have the memory.

1380
00:56:49,480 --> 00:56:51,040
We don't have the intelligence yet.

1381
00:56:51,040 --> 00:56:53,400
And I agree with you that, well, having the memory

1382
00:56:53,400 --> 00:56:54,920
is actually very useful.

1383
00:56:54,920 --> 00:56:57,080
And if you just had the intelligence,

1384
00:56:57,080 --> 00:56:59,080
but it was not hooked up to an extensive memory,

1385
00:56:59,080 --> 00:57:01,320
it would not be that useful, because it would not

1386
00:57:01,320 --> 00:57:04,240
have enough material to work from.

1387
00:57:04,240 --> 00:57:04,960
Yeah.

1388
00:57:04,960 --> 00:57:07,720
The alternative hypothesis here that former guest Trenton

1389
00:57:07,720 --> 00:57:11,360
Brickin advanced is that intelligence

1390
00:57:11,360 --> 00:57:14,840
is just hierarchically associated memory

1391
00:57:14,840 --> 00:57:18,000
where higher-level patterns, when Sherlock Holmes goes

1392
00:57:18,000 --> 00:57:20,600
into a crime scene, and he's extremely sample-efficient,

1393
00:57:20,600 --> 00:57:22,360
he can just look at a few clues and figure out

1394
00:57:22,400 --> 00:57:23,920
who was a murderer, and the way he's

1395
00:57:23,920 --> 00:57:26,800
able to do that is he has learned higher-level

1396
00:57:26,800 --> 00:57:28,080
sort of associations.

1397
00:57:28,080 --> 00:57:30,280
It's memory in some fundamental sense.

1398
00:57:30,280 --> 00:57:33,960
But so here's one way to ask the question.

1399
00:57:33,960 --> 00:57:37,400
In the brain, supposedly we do program synthesis,

1400
00:57:37,400 --> 00:57:40,400
but it is just synapses connected to one another,

1401
00:57:40,400 --> 00:57:43,080
each other, and so physically it's

1402
00:57:43,080 --> 00:57:45,560
got to be that you just query the right circuit, right?

1403
00:57:45,560 --> 00:57:46,720
You are, yeah, yeah, yeah.

1404
00:57:46,720 --> 00:57:48,320
It's a matter of degree.

1405
00:57:48,320 --> 00:57:51,800
But if you can learn it, if training in the environment

1406
00:57:52,360 --> 00:57:53,960
human ancestors are trained in means

1407
00:57:53,960 --> 00:57:55,680
you learn those circuits, training

1408
00:57:55,680 --> 00:57:57,520
on the same kinds of outputs that humans produce,

1409
00:57:57,520 --> 00:58:00,080
which to replicate require these kinds of circuits,

1410
00:58:00,080 --> 00:58:03,480
wouldn't that train the same kind of whatever humans have?

1411
00:58:03,480 --> 00:58:05,040
You know, it's a matter of degree.

1412
00:58:07,720 --> 00:58:09,560
If you have a system that has a memory

1413
00:58:09,560 --> 00:58:13,680
and is only capable of doing local generalization from that,

1414
00:58:13,680 --> 00:58:16,760
it's not going to be very adaptable.

1415
00:58:16,760 --> 00:58:19,160
To be really general, you need the memory

1416
00:58:19,160 --> 00:58:23,200
plus the ability to search to quite some depth,

1417
00:58:23,200 --> 00:58:26,880
to achieve broader even extramuralization.

1418
00:58:26,880 --> 00:58:31,600
You know, like one of my favorite psychologists,

1419
00:58:31,600 --> 00:58:35,320
so Jean Piaget was the founder of the Elemental Psychology.

1420
00:58:35,320 --> 00:58:37,720
He had a very good quote about intelligence.

1421
00:58:37,720 --> 00:58:40,960
He said, intelligence is what you use when you don't know what

1422
00:58:40,960 --> 00:58:42,000
to do.

1423
00:58:42,000 --> 00:58:46,480
And it's like, as a human living your life,

1424
00:58:46,480 --> 00:58:48,760
in most situations you already know what to do,

1425
00:58:48,760 --> 00:58:50,680
because you've been in this situation before.

1426
00:58:50,680 --> 00:58:53,440
You already have the answer, right?

1427
00:58:53,440 --> 00:58:55,560
And you're only going to need to use intelligence

1428
00:58:55,560 --> 00:58:59,280
when you're faced with novelty, with something you didn't expect,

1429
00:58:59,280 --> 00:59:01,240
with something that you weren't prepared for,

1430
00:59:01,240 --> 00:59:04,960
either by your own experience, your own life experience,

1431
00:59:04,960 --> 00:59:07,480
or by your evolutionary history.

1432
00:59:07,480 --> 00:59:11,560
Like, this day that you're living right now is different

1433
00:59:11,560 --> 00:59:14,840
in some important ways from every day you've lived before,

1434
00:59:14,840 --> 00:59:17,440
but it's also different from any day ever lived

1435
00:59:17,440 --> 00:59:18,880
by any of your ancestors.

1436
00:59:18,880 --> 00:59:22,600
And still, you're capable of being functional, right?

1437
00:59:22,600 --> 00:59:23,480
How is it possible?

1438
00:59:23,480 --> 00:59:25,760
I'm not denying that generalization is extremely important,

1439
00:59:25,760 --> 00:59:28,960
and is the basis for intelligence.

1440
00:59:28,960 --> 00:59:30,600
That's not the correct, the correct is like,

1441
00:59:30,600 --> 00:59:32,120
how much of that is happening in the models?

1442
00:59:32,120 --> 00:59:35,000
But, okay, let me ask a separate question.

1443
00:59:35,000 --> 00:59:38,520
We might keep going in the circle here.

1444
00:59:38,520 --> 00:59:41,000
The differences in intelligence between humans,

1445
00:59:41,000 --> 00:59:43,480
maybe the intelligence tests because of reasons

1446
00:59:43,480 --> 00:59:44,600
you mentioned are not measuring it well,

1447
00:59:44,600 --> 00:59:46,080
but clearly there's differences in intelligence

1448
00:59:46,080 --> 00:59:47,080
between different humans.

1449
00:59:47,680 --> 00:59:49,640
What is your explanation for what's going on there?

1450
00:59:49,640 --> 00:59:52,160
Because I think that's sort of compatible with my story

1451
00:59:52,160 --> 00:59:53,720
that there's a spectrum of generality

1452
00:59:53,720 --> 00:59:56,520
and that these models are climbing up to a human level,

1453
00:59:56,520 --> 00:59:58,280
and even some humans haven't even climbed up

1454
00:59:58,280 --> 01:00:02,400
to the Einstein level or the Francois level, but.

1455
01:00:02,400 --> 01:00:04,440
That's a great question, you know.

1456
01:00:04,440 --> 01:00:07,960
There is extensive evidence that intelligence,

1457
01:00:07,960 --> 01:00:11,000
difference in intelligence are mostly genetic in nature,

1458
01:00:11,000 --> 01:00:11,840
right?

1459
01:00:11,840 --> 01:00:14,400
Meaning that if you take someone who is not very intelligent,

1460
01:00:14,400 --> 01:00:17,880
there is no amount of training, of like training data,

1461
01:00:17,880 --> 01:00:19,720
you can expose that person to that would

1462
01:00:21,120 --> 01:00:22,880
make them become Einstein.

1463
01:00:22,880 --> 01:00:25,360
And this kind of points to the fact

1464
01:00:25,360 --> 01:00:28,560
that you really need a better architecture,

1465
01:00:28,560 --> 01:00:30,240
you need a better algorithm,

1466
01:00:30,240 --> 01:00:34,040
and more training data is not in fact all you need.

1467
01:00:34,040 --> 01:00:35,920
I think I agree with that.

1468
01:00:35,920 --> 01:00:39,000
I think what, maybe the way I might phrase it is that

1469
01:00:39,000 --> 01:00:42,240
the people who are smarter have in ML language

1470
01:00:42,280 --> 01:00:45,680
better initializations, the neural wiring,

1471
01:00:45,680 --> 01:00:48,040
if you just look at, it's more efficient,

1472
01:00:48,040 --> 01:00:51,400
they have maybe greater density of firing.

1473
01:00:51,400 --> 01:00:53,200
And so as some part of the story is scaling,

1474
01:00:53,200 --> 01:00:55,320
there is some correlation between brain size

1475
01:00:55,320 --> 01:00:56,600
and intelligence.

1476
01:00:56,600 --> 01:01:00,160
And we also see within the context of quote unquote,

1477
01:01:00,160 --> 01:01:01,360
scaling that people talk about

1478
01:01:01,360 --> 01:01:04,400
within the context of LLMs, architectural improvements,

1479
01:01:04,400 --> 01:01:07,920
where a model like Gemini 1.5 flash

1480
01:01:07,920 --> 01:01:10,720
is performs as well as GPT-4 did

1481
01:01:10,720 --> 01:01:12,560
when GPT-4 was released a year ago,

1482
01:01:12,560 --> 01:01:15,320
but is 57 times cheaper on output.

1483
01:01:15,320 --> 01:01:17,480
So the part of the scaling story

1484
01:01:17,480 --> 01:01:19,240
is that the architectural improvements are,

1485
01:01:19,240 --> 01:01:21,640
we're in like extremely low hanging fruit territory

1486
01:01:21,640 --> 01:01:23,160
when it comes to those.

1487
01:01:23,160 --> 01:01:27,560
Okay, we're back now with the co-founder of Zapier,

1488
01:01:27,560 --> 01:01:31,040
Mike Canouf, we had to restart a few times there.

1489
01:01:31,040 --> 01:01:32,880
And you're funding this prize

1490
01:01:32,880 --> 01:01:35,800
and you're running this prize with Francois.

1491
01:01:35,800 --> 01:01:38,760
And so tell me about how this came together,

1492
01:01:39,720 --> 01:01:41,760
what more prompted you guys to launch this prize?

1493
01:01:41,760 --> 01:01:44,240
Yeah, I guess I've been sort of like AI curious

1494
01:01:44,240 --> 01:01:46,680
for 13 years, I co-founded Zapier,

1495
01:01:46,680 --> 01:01:48,520
been running it for the last 13 years.

1496
01:01:48,520 --> 01:01:51,240
And I think I first got introduced to your work

1497
01:01:51,240 --> 01:01:54,560
and during COVID, I kind of went down the rabbit hole,

1498
01:01:54,560 --> 01:01:56,320
we had a lot of free time.

1499
01:01:56,320 --> 01:01:58,960
And it was right after you published your

1500
01:01:58,960 --> 01:01:59,960
on measure of intelligence paper,

1501
01:01:59,960 --> 01:02:01,840
you sort of introduced the concept of AGI,

1502
01:02:01,840 --> 01:02:03,400
this like efficiency of skill acquisition

1503
01:02:03,400 --> 01:02:06,280
is like the right definition and the arc puzzles.

1504
01:02:06,320 --> 01:02:09,040
But I don't think the first Kaggle contest was done yet.

1505
01:02:09,040 --> 01:02:10,480
I think it was still running.

1506
01:02:10,480 --> 01:02:12,600
And so I kind of, it was interesting,

1507
01:02:12,600 --> 01:02:14,880
but I just parked the idea.

1508
01:02:14,880 --> 01:02:16,920
And my bigger fish to fry at Zapier

1509
01:02:16,920 --> 01:02:18,480
were in this middle of this big turnaround

1510
01:02:18,480 --> 01:02:21,240
of trying to get to our second product.

1511
01:02:21,240 --> 01:02:24,040
And then it was January, 2022,

1512
01:02:24,040 --> 01:02:25,680
when the chain of thought paper came out

1513
01:02:25,680 --> 01:02:28,560
that really like awoken me to sort of the progress.

1514
01:02:28,560 --> 01:02:30,560
I gave a whole presentation to the Zapier

1515
01:02:30,560 --> 01:02:31,880
on like the GPT-3 paper events.

1516
01:02:31,880 --> 01:02:33,720
I'd sort of felt like I had priced in everything

1517
01:02:33,720 --> 01:02:35,720
that Elms could do and that paper was

1518
01:02:35,760 --> 01:02:37,360
really shocking to me in terms of

1519
01:02:37,360 --> 01:02:39,240
these latent capabilities that Elms have

1520
01:02:39,240 --> 01:02:41,520
that I didn't expect that they had.

1521
01:02:41,520 --> 01:02:45,600
And so I actually gave up my exact team role at Zapier.

1522
01:02:45,600 --> 01:02:46,720
I was running half the company at that point

1523
01:02:46,720 --> 01:02:48,800
and I went back to be an individual contributor

1524
01:02:48,800 --> 01:02:50,880
and just to go do AI research

1525
01:02:50,880 --> 01:02:52,480
alongside Brian, my co-founder.

1526
01:02:53,920 --> 01:02:56,560
And all of that led me to back towards arc.

1527
01:02:56,560 --> 01:02:57,800
I was looking into it again

1528
01:02:57,800 --> 01:03:02,120
and I had sort of expected to see this saturation effect

1529
01:03:02,120 --> 01:03:05,680
that MMLU has, that GMSK 8K has.

1530
01:03:05,680 --> 01:03:07,840
And when I looked at the scores and the progress

1531
01:03:07,840 --> 01:03:11,160
since the last four years, I was really again, shocked to see

1532
01:03:11,160 --> 01:03:13,440
actually we've made very little objective progress

1533
01:03:13,440 --> 01:03:16,640
towards it and it felt very,

1534
01:03:16,640 --> 01:03:18,280
it felt like a really, really important Eval.

1535
01:03:18,280 --> 01:03:19,640
And as I sort of spent the last year

1536
01:03:19,640 --> 01:03:21,120
asking people, quizzing people about it

1537
01:03:21,120 --> 01:03:22,880
and sort of my network and community,

1538
01:03:23,760 --> 01:03:26,040
very few people even knew it existed.

1539
01:03:26,040 --> 01:03:29,000
And that felt like, okay, if it's right

1540
01:03:29,000 --> 01:03:31,440
that this is a really, really like globally

1541
01:03:31,440 --> 01:03:34,840
singularly unique EGI Eval.

1542
01:03:34,840 --> 01:03:36,440
And it's different from every other Eval that exists

1543
01:03:36,440 --> 01:03:40,200
that are more narrowly measures AI skill.

1544
01:03:40,200 --> 01:03:42,120
Like more people should know about this thing.

1545
01:03:42,120 --> 01:03:44,400
I had my own ideas on how to beat the arc as well.

1546
01:03:44,400 --> 01:03:46,320
So like I was working on nights and weekends on that

1547
01:03:46,320 --> 01:03:49,600
and I flew up to meet Francois earlier this year

1548
01:03:49,600 --> 01:03:51,480
to sort of quiz him, show him my ideas.

1549
01:03:51,480 --> 01:03:54,160
And ultimately I was like, well,

1550
01:03:54,160 --> 01:03:56,480
why don't you think more people know about arc?

1551
01:03:56,480 --> 01:03:57,440
I think you should actually answer that.

1552
01:03:57,440 --> 01:03:59,280
I think it's a really interesting question.

1553
01:03:59,280 --> 01:04:01,320
Like why don't you think more people know about arc?

1554
01:04:01,320 --> 01:04:05,000
Sure, you know, I think benchmarks that gain traction

1555
01:04:05,000 --> 01:04:06,520
in the research community are benchmarks

1556
01:04:06,520 --> 01:04:08,680
that are already fairly tractable

1557
01:04:08,680 --> 01:04:11,600
because the dynamic that you see is that some research group

1558
01:04:11,600 --> 01:04:13,720
is gonna make some initial breakthrough

1559
01:04:13,720 --> 01:04:16,760
and then this is gonna catch the attention of everyone else.

1560
01:04:16,760 --> 01:04:18,480
And so you're gonna get follow-up papers

1561
01:04:18,480 --> 01:04:22,200
with people trying to beat the first team and so on.

1562
01:04:22,200 --> 01:04:24,520
And for arc, this has not really happened

1563
01:04:24,520 --> 01:04:26,360
because arc is actually very hard

1564
01:04:26,360 --> 01:04:27,800
for existing AI techniques.

1565
01:04:27,800 --> 01:04:30,920
Kind of arc requires you to try new ideas.

1566
01:04:31,000 --> 01:04:33,280
And that's very much the point, by the way.

1567
01:04:33,280 --> 01:04:35,280
Like the point is not that, yeah,

1568
01:04:35,280 --> 01:04:37,640
you should just be able to apply existing technology

1569
01:04:37,640 --> 01:04:38,480
and solve arc.

1570
01:04:38,480 --> 01:04:42,920
The point is that existing technology has reached a plateau

1571
01:04:42,920 --> 01:04:44,800
and if you want to go beyond that,

1572
01:04:44,800 --> 01:04:47,760
if you want to start being able to tackle problems

1573
01:04:47,760 --> 01:04:50,800
that you haven't memorized, that you haven't seen before,

1574
01:04:50,800 --> 01:04:52,440
you need to try new ideas.

1575
01:04:52,440 --> 01:04:57,440
And arc is not just meant to be this sort of like measure

1576
01:04:58,440 --> 01:05:01,000
of how close we are to a GI.

1577
01:05:01,000 --> 01:05:04,360
It's also meant to be a source of inspiration.

1578
01:05:04,360 --> 01:05:06,680
Like I want researchers to look at these puzzles

1579
01:05:06,680 --> 01:05:09,080
and be like, hey, it's really strange

1580
01:05:09,080 --> 01:05:11,560
that these puzzles are so simple

1581
01:05:11,560 --> 01:05:15,320
and most humans can just do them very quickly.

1582
01:05:15,320 --> 01:05:18,400
Why is it so hard for existing AI systems?

1583
01:05:18,400 --> 01:05:20,800
Why is it so hard for LLMs and so on?

1584
01:05:20,800 --> 01:05:23,480
And it's true for LLMs, but arc was actually released

1585
01:05:23,480 --> 01:05:25,480
before LLMs were really a thing.

1586
01:05:25,520 --> 01:05:28,880
And the only thing that made it special at the time

1587
01:05:28,880 --> 01:05:32,240
was that it was designed to be a resistance to memorization.

1588
01:05:32,240 --> 01:05:34,800
And the fact that it has survived LLMs

1589
01:05:34,800 --> 01:05:37,480
and Genia in general so well,

1590
01:05:37,480 --> 01:05:38,680
kind of shows that yes,

1591
01:05:38,680 --> 01:05:40,800
it is actually resistant to memorization.

1592
01:05:40,800 --> 01:05:42,280
This is what nerds night me

1593
01:05:42,280 --> 01:05:44,200
because I went and took a bunch of the puzzles myself.

1594
01:05:44,200 --> 01:05:45,720
I've showed it to all my friends and family too

1595
01:05:45,720 --> 01:05:48,560
and they're all like, oh yeah, this is like super easy.

1596
01:05:49,480 --> 01:05:51,240
Are you sure AI can't solve this?

1597
01:05:51,240 --> 01:05:54,160
Like that's the reaction in the same one for me as well.

1598
01:05:54,160 --> 01:05:55,440
And the more you dig in, you're like, okay,

1599
01:05:55,440 --> 01:05:57,320
yep, there's not just empirical evidence

1600
01:05:57,320 --> 01:05:58,720
over the last four years that it's unbeaten,

1601
01:05:58,720 --> 01:06:01,800
but there's theoretical like concepts behind why.

1602
01:06:02,640 --> 01:06:04,200
And I completely agree at this point

1603
01:06:04,200 --> 01:06:06,240
that like new ideas basically are needed to be dark.

1604
01:06:06,240 --> 01:06:08,000
And there's a lot of current trends in the world

1605
01:06:08,000 --> 01:06:09,240
that are actually, I think,

1606
01:06:09,240 --> 01:06:12,280
working against that happening basically.

1607
01:06:12,280 --> 01:06:13,320
I think we're actually less likely

1608
01:06:13,320 --> 01:06:14,880
to generate new ideas right now.

1609
01:06:15,840 --> 01:06:17,680
You know, I think one of the kind of trends

1610
01:06:17,680 --> 01:06:19,480
is the closing up frontier research, right?

1611
01:06:19,480 --> 01:06:22,720
The GP4 paper from Open AI had no technical details shared.

1612
01:06:22,720 --> 01:06:24,480
The Gemini paper had no technical details shared

1613
01:06:24,520 --> 01:06:27,160
and like the longer context part of that work.

1614
01:06:27,160 --> 01:06:30,200
And yet that open innovation and open progress and sharing

1615
01:06:30,200 --> 01:06:32,120
is what got us to transformers in the first place.

1616
01:06:32,120 --> 01:06:35,120
That's what got us to LMS in the first place.

1617
01:06:35,120 --> 01:06:37,920
So it's kind of disappointing a little bit actually

1618
01:06:37,920 --> 01:06:40,000
that like so much frontier work has gone closed.

1619
01:06:40,000 --> 01:06:42,560
It's really making a bet that like these individual labs

1620
01:06:42,560 --> 01:06:43,760
are going to have the breakthrough

1621
01:06:43,760 --> 01:06:46,240
and not the ecosystem is going to have the breakthrough.

1622
01:06:46,240 --> 01:06:48,360
And I think sort of the internet open source has shown

1623
01:06:48,360 --> 01:06:50,360
that that's like the most powerful innovation ecosystem

1624
01:06:50,360 --> 01:06:52,480
that's ever existed probably in the entire world.

1625
01:06:52,480 --> 01:06:54,080
I think that's actually really sad

1626
01:06:54,080 --> 01:06:57,720
that frontier research is no longer being published.

1627
01:06:57,720 --> 01:06:59,960
If you look back, you know, four years ago,

1628
01:07:01,280 --> 01:07:03,080
well, everything was just openly shared

1629
01:07:03,080 --> 01:07:05,880
like all the state of the art results were published

1630
01:07:05,880 --> 01:07:07,160
and this is no longer the case.

1631
01:07:07,160 --> 01:07:08,400
And it's very much, you know,

1632
01:07:08,400 --> 01:07:11,400
Open AI single-handedly changed the game.

1633
01:07:11,400 --> 01:07:16,400
And I think Open AI basically set back progress towards HGI

1634
01:07:17,560 --> 01:07:20,160
by quite a few years, probably like five to 10 years

1635
01:07:20,160 --> 01:07:21,000
for two reasons.

1636
01:07:21,000 --> 01:07:25,640
And one is that, well, they cause this complete closing down

1637
01:07:25,640 --> 01:07:28,200
of research, frontier research publishing,

1638
01:07:28,200 --> 01:07:33,200
but also they trigger this initial burst of hype

1639
01:07:34,480 --> 01:07:35,520
around LLMS.

1640
01:07:35,520 --> 01:07:39,640
And now LLMS have sucked the oxygen out of the room

1641
01:07:39,640 --> 01:07:43,320
like everything, everyone is just doing LLMS.

1642
01:07:43,320 --> 01:07:47,080
And I see LLMS as a more often off-ramp

1643
01:07:47,080 --> 01:07:49,840
on the path to HGI actually.

1644
01:07:49,880 --> 01:07:51,960
And all these new resources,

1645
01:07:51,960 --> 01:07:54,160
they're actually going to LLMS instead

1646
01:07:54,160 --> 01:07:56,920
of everything else they could be going to.

1647
01:07:56,920 --> 01:07:59,720
And, you know, if you look further into the past

1648
01:07:59,720 --> 01:08:02,760
to like 2015, 2016,

1649
01:08:02,760 --> 01:08:05,520
there were like a thousand times fewer people

1650
01:08:05,520 --> 01:08:07,280
doing AI back then.

1651
01:08:07,280 --> 01:08:10,740
And yet I feel like the rate of progress was higher

1652
01:08:10,740 --> 01:08:14,480
because people were exploring more directions.

1653
01:08:14,480 --> 01:08:16,400
The world felt more open-ended.

1654
01:08:16,400 --> 01:08:18,400
Like you could just go and try,

1655
01:08:18,400 --> 01:08:20,400
like have a cool idea of a launch

1656
01:08:20,400 --> 01:08:22,360
and try it and get some interesting results.

1657
01:08:22,360 --> 01:08:24,440
So there was this energy.

1658
01:08:24,440 --> 01:08:27,440
And now everyone is very much doing some variation

1659
01:08:27,440 --> 01:08:28,800
of the same thing.

1660
01:08:28,800 --> 01:08:32,720
And the big labs also tried their hand on arc,

1661
01:08:32,720 --> 01:08:34,560
but because they got bad results,

1662
01:08:34,560 --> 01:08:35,840
they didn't publish anything.

1663
01:08:35,840 --> 01:08:39,520
Like, you know, people only publish positive results.

1664
01:08:39,520 --> 01:08:43,840
I wonder how much effort people have put into

1665
01:08:43,840 --> 01:08:46,400
trying to prompt or scaffold,

1666
01:08:46,400 --> 01:08:48,920
do some sort of maybe Devon type approach

1667
01:08:48,920 --> 01:08:52,280
into getting the frontier models

1668
01:08:52,280 --> 01:08:54,280
and the frontier models of today, not just a year ago,

1669
01:08:54,280 --> 01:08:55,440
because a lot of post-training

1670
01:08:55,440 --> 01:08:57,120
has gone into making them better.

1671
01:08:57,120 --> 01:09:00,000
So Claude Friropas or GPT-40

1672
01:09:00,000 --> 01:09:02,880
into getting good solutions on arc.

1673
01:09:04,760 --> 01:09:06,760
I hope that one of the things this episode does

1674
01:09:06,760 --> 01:09:09,200
is get people to try out this open competition

1675
01:09:09,200 --> 01:09:12,920
where they have to put in an open source model to compete.

1676
01:09:12,920 --> 01:09:14,880
But also to like figure out if they're,

1677
01:09:14,880 --> 01:09:17,880
maybe the like capability is latent in Claude Opus

1678
01:09:17,880 --> 01:09:20,240
and just see if you can show that.

1679
01:09:20,240 --> 01:09:21,920
I think that would be super interesting.

1680
01:09:21,920 --> 01:09:23,240
So let's talk about the prize.

1681
01:09:23,240 --> 01:09:25,760
How much do you win if you solve it?

1682
01:09:25,760 --> 01:09:27,920
You know, get whatever percent on arc.

1683
01:09:27,920 --> 01:09:29,680
How much do you get if you get the best of vision,

1684
01:09:29,680 --> 01:09:30,600
but don't crack it?

1685
01:09:30,600 --> 01:09:31,600
So we got a million dollar,

1686
01:09:31,600 --> 01:09:32,600
actually a little over a million dollars

1687
01:09:32,600 --> 01:09:33,640
is the price pool.

1688
01:09:33,640 --> 01:09:35,880
We're running the contest on an annual basis.

1689
01:09:35,880 --> 01:09:37,520
We're gonna, we're starting it today

1690
01:09:37,520 --> 01:09:39,800
through the middle of November.

1691
01:09:39,800 --> 01:09:41,840
And the goal is to get 85%.

1692
01:09:41,840 --> 01:09:43,320
That's the lower bound and human average

1693
01:09:43,320 --> 01:09:44,920
that you guys talked about earlier.

1694
01:09:44,920 --> 01:09:48,080
And there's a $500,000 prize for the first team

1695
01:09:48,080 --> 01:09:50,560
that can get to the 85% benchmark.

1696
01:09:50,560 --> 01:09:51,720
We're also gonna run,

1697
01:09:51,720 --> 01:09:54,000
we don't expect that to happen this year actually.

1698
01:09:54,000 --> 01:09:57,040
One of the early statisticians that's up here

1699
01:09:57,040 --> 01:09:59,360
giving this line that has always stuck with me

1700
01:09:59,360 --> 01:10:01,280
that the longer it takes, the longer it takes.

1701
01:10:01,280 --> 01:10:04,600
So my prior is that like arc is gonna take years to solve.

1702
01:10:05,360 --> 01:10:06,240
And so we're gonna keep to,

1703
01:10:06,240 --> 01:10:08,840
we're also gonna break down and do a progress price this year.

1704
01:10:08,840 --> 01:10:10,960
So there's a $100,000 progress price,

1705
01:10:10,960 --> 01:10:13,760
which we will pay out to the top scores.

1706
01:10:13,760 --> 01:10:18,640
So $50,000 is gonna go to the top objective scores this year

1707
01:10:18,640 --> 01:10:19,600
on the Kaggle leaderboard,

1708
01:10:19,600 --> 01:10:21,320
which we're hosting it on Kaggle.

1709
01:10:21,320 --> 01:10:23,160
And then we're gonna have a $50,000 pot set

1710
01:10:23,160 --> 01:10:26,200
for a paper award for the best paper

1711
01:10:26,200 --> 01:10:28,720
that explains conceptually the scores

1712
01:10:28,720 --> 01:10:30,160
that they were able to achieve.

1713
01:10:30,160 --> 01:10:31,600
And one of the I think interesting things

1714
01:10:31,600 --> 01:10:33,680
we're also gonna be doing is,

1715
01:10:33,680 --> 01:10:35,880
we're gonna be requiring that in order to win the prize money

1716
01:10:35,880 --> 01:10:38,040
that you put the solution or your paper

1717
01:10:38,040 --> 01:10:39,360
out into public domain.

1718
01:10:40,320 --> 01:10:41,960
The reason for this is,

1719
01:10:41,960 --> 01:10:43,360
typically with contests,

1720
01:10:43,360 --> 01:10:45,320
you see a lot of like closed up sharing.

1721
01:10:45,320 --> 01:10:46,600
People are kind of private secret.

1722
01:10:46,600 --> 01:10:47,720
They wanna hold their alpha to themselves

1723
01:10:47,720 --> 01:10:49,120
during the contest period.

1724
01:10:49,120 --> 01:10:52,080
And because we expect it's gonna be multiple years,

1725
01:10:52,080 --> 01:10:53,160
we wanna enter a game here.

1726
01:10:53,160 --> 01:10:56,520
So the plan is at the end of November,

1727
01:10:56,520 --> 01:10:58,520
we will award the $100,000 prize money

1728
01:10:58,520 --> 01:10:59,920
to the top progress prize

1729
01:10:59,920 --> 01:11:03,440
and then use the downtime between December, January, February

1730
01:11:03,440 --> 01:11:06,840
to share out all the knowledge from the top scores

1731
01:11:06,840 --> 01:11:08,200
and the approaches folks were taking

1732
01:11:08,200 --> 01:11:10,080
in order to re-baseline the community

1733
01:11:10,080 --> 01:11:11,720
up to whatever the state of the art is

1734
01:11:11,720 --> 01:11:13,360
and then run the contest again next year.

1735
01:11:13,360 --> 01:11:16,400
And keep doing that on a yearly basis until we get 85%.

1736
01:11:16,400 --> 01:11:17,760
I'll give some people some context

1737
01:11:17,760 --> 01:11:20,440
on why I think this prize is very interesting.

1738
01:11:20,440 --> 01:11:22,720
I was having conversations with my friends

1739
01:11:22,720 --> 01:11:26,360
who are very much believers in models as they exist today.

1740
01:11:26,360 --> 01:11:28,200
And first of all, it was intriguing to me

1741
01:11:28,200 --> 01:11:29,960
that they didn't know about ARC.

1742
01:11:29,960 --> 01:11:32,240
These are experienced ML researchers.

1743
01:11:32,240 --> 01:11:35,560
And so you show them this happened a couple of nights ago.

1744
01:11:35,560 --> 01:11:38,240
We went to dinner and I showed them an example problem.

1745
01:11:38,240 --> 01:11:39,160
And they said, of course,

1746
01:11:39,160 --> 01:11:41,040
an LLM would be able to solve something like this.

1747
01:11:41,040 --> 01:11:42,280
And then we take a screenshot of it.

1748
01:11:42,280 --> 01:11:44,240
We just put it into our chat GPT app

1749
01:11:44,240 --> 01:11:45,520
and it doesn't get the pattern.

1750
01:11:45,520 --> 01:11:48,600
And so I think it's very interesting.

1751
01:11:48,600 --> 01:11:49,720
Like it is a notable fact.

1752
01:11:49,720 --> 01:11:51,600
I was sort of playing devil's advocate against you

1753
01:11:51,600 --> 01:11:52,440
on these kinds of questions.

1754
01:11:52,440 --> 01:11:53,920
But this is a very intriguing fact.

1755
01:11:53,920 --> 01:11:56,800
And I'm extreme, I think this prize is extremely interesting

1756
01:11:56,800 --> 01:11:58,240
because we're gonna learn,

1757
01:11:58,240 --> 01:12:01,240
we're gonna learn something fascinating one way or another.

1758
01:12:01,240 --> 01:12:03,920
So with regards to the 85%,

1759
01:12:03,960 --> 01:12:04,920
separate from this prize,

1760
01:12:04,920 --> 01:12:07,640
I'd be very curious if somebody could replicate that result

1761
01:12:07,640 --> 01:12:11,640
because obviously in psychology and other kinds of fields,

1762
01:12:11,640 --> 01:12:15,000
which this result seems to be analogous to

1763
01:12:15,000 --> 01:12:18,920
when you run test on some small sample of people,

1764
01:12:18,920 --> 01:12:20,200
often they're hard to replicate.

1765
01:12:20,200 --> 01:12:22,000
So I'd be very curious if you try to replicate this,

1766
01:12:22,000 --> 01:12:25,440
how, what does an average human perform on ARC?

1767
01:12:25,440 --> 01:12:27,360
Ask for the difficulty on how long it will take

1768
01:12:27,360 --> 01:12:28,920
to crack this benchmark.

1769
01:12:28,920 --> 01:12:31,160
It's very interesting because the other benchmarks

1770
01:12:31,200 --> 01:12:34,040
that are now fully saturated like MMLU math,

1771
01:12:34,040 --> 01:12:36,400
actually the people who made them,

1772
01:12:36,400 --> 01:12:39,680
Dan Hendricks and Colin Burns who did MMLU and math,

1773
01:12:39,680 --> 01:12:41,840
I think they were grad students or college students

1774
01:12:41,840 --> 01:12:43,160
when they made it.

1775
01:12:43,160 --> 01:12:45,400
And the goal when they made it just a couple of years ago

1776
01:12:45,400 --> 01:12:47,960
was that this will be a test of AGI.

1777
01:12:47,960 --> 01:12:49,240
And of course it got totally saturated.

1778
01:12:49,240 --> 01:12:52,480
And I know you all argue that these are test memorization,

1779
01:12:52,480 --> 01:12:54,120
but I think the pattern we've seen,

1780
01:12:54,120 --> 01:12:57,120
in fact, Epoch AI has a very interesting graph

1781
01:12:57,120 --> 01:12:59,440
that I'll sort of overlay for the YouTube version here

1782
01:12:59,480 --> 01:13:02,200
where you see this almost exponential

1783
01:13:02,200 --> 01:13:06,120
where it gets 5%, 10%, 30%, 40%

1784
01:13:06,120 --> 01:13:07,880
as you increase the compute across models

1785
01:13:07,880 --> 01:13:09,920
and then it just shoots up.

1786
01:13:09,920 --> 01:13:12,640
And in the GPT-4 technical report,

1787
01:13:12,640 --> 01:13:14,160
they had this interesting graph

1788
01:13:14,160 --> 01:13:16,240
of the human eval problem set,

1789
01:13:16,240 --> 01:13:18,400
which was 22 coding problems.

1790
01:13:18,400 --> 01:13:22,200
And they had to graph it on the mean log pass curve,

1791
01:13:22,200 --> 01:13:24,960
basically because early on in training

1792
01:13:24,960 --> 01:13:28,200
or even smaller models can have the right idea

1793
01:13:28,200 --> 01:13:29,760
of how to solve this problem,

1794
01:13:29,760 --> 01:13:31,720
but it takes a lot of reliability

1795
01:13:31,720 --> 01:13:34,000
to make sure they stay on track to solve the whole problem.

1796
01:13:34,000 --> 01:13:36,120
And so you really wanna up wait the signal

1797
01:13:36,120 --> 01:13:38,000
where they get it right at least some of the time,

1798
01:13:38,000 --> 01:13:39,720
maybe one in a hundred times or one in a thousand.

1799
01:13:39,720 --> 01:13:41,560
And then so they go from like one in a thousand,

1800
01:13:41,560 --> 01:13:42,480
one in a hundred, one in 10,

1801
01:13:42,480 --> 01:13:44,620
and then they just like totally saturated.

1802
01:13:44,620 --> 01:13:46,280
I guess the question I have,

1803
01:13:46,280 --> 01:13:47,160
this is all leading up to,

1804
01:13:47,160 --> 01:13:49,900
is why won't the same thing happen with ARC

1805
01:13:49,900 --> 01:13:53,480
where people had to try really hard, bigger models.

1806
01:13:54,400 --> 01:13:56,240
And now they figured out these techniques

1807
01:13:56,280 --> 01:13:57,120
that Jack Cole has figured out

1808
01:13:57,120 --> 01:14:00,120
with only a 240 million parameter language model

1809
01:14:00,120 --> 01:14:02,440
that can get 35%.

1810
01:14:02,440 --> 01:14:03,560
Shouldn't we see the same pattern we saw

1811
01:14:03,560 --> 01:14:04,480
across all these other benchmarks

1812
01:14:04,480 --> 01:14:05,880
where you just like sort of eke out.

1813
01:14:05,880 --> 01:14:07,560
And then once you get the general idea,

1814
01:14:07,560 --> 01:14:09,800
then you just go all the way to a hundred.

1815
01:14:09,800 --> 01:14:10,920
That's an empirical question.

1816
01:14:10,920 --> 01:14:12,840
So we'll see in practice what happens.

1817
01:14:13,840 --> 01:14:16,560
But what Jack Cole is doing is actually very unique.

1818
01:14:16,560 --> 01:14:19,680
It's not just pre-training an LLM and then prompting it,

1819
01:14:19,680 --> 01:14:21,960
he's actually trying to do active inference.

1820
01:14:21,960 --> 01:14:23,160
He's doing a test time, right?

1821
01:14:23,160 --> 01:14:24,000
He's doing like test time functioning.

1822
01:14:24,000 --> 01:14:25,680
Exactly, test time functioning.

1823
01:14:25,680 --> 01:14:27,680
And this is actually trying to lift

1824
01:14:27,680 --> 01:14:29,680
one of the key limitations of LLMs,

1825
01:14:29,680 --> 01:14:31,520
which is that at inference time,

1826
01:14:31,520 --> 01:14:32,680
they cannot learn anything new.

1827
01:14:32,680 --> 01:14:35,520
They cannot adapt on the flight what they're seeing.

1828
01:14:35,520 --> 01:14:38,720
And he's actually trying to learn.

1829
01:14:38,720 --> 01:14:40,800
So what he's doing is effectively

1830
01:14:40,800 --> 01:14:42,960
a form of program synthesis.

1831
01:14:44,080 --> 01:14:46,920
Because the LLM contains a lot of useful building blocks,

1832
01:14:46,920 --> 01:14:48,680
like programming building blocks,

1833
01:14:48,680 --> 01:14:52,080
and by finding it on the task at test time,

1834
01:14:52,080 --> 01:14:54,600
you are trying to assemble these building blocks

1835
01:14:54,600 --> 01:14:57,680
into the right pattern that matches the task.

1836
01:14:57,680 --> 01:15:00,560
This is exactly what program synthesis is about.

1837
01:15:00,560 --> 01:15:03,640
And the way we contrast this approach

1838
01:15:03,640 --> 01:15:05,920
with discrete program search is that

1839
01:15:05,920 --> 01:15:07,760
in discrete program search,

1840
01:15:07,760 --> 01:15:10,240
so you're trying to assemble a program

1841
01:15:10,240 --> 01:15:12,160
from a set of primitives.

1842
01:15:12,160 --> 01:15:13,520
You have very few primitives.

1843
01:15:13,520 --> 01:15:15,800
So people working on discrete program search on Arc,

1844
01:15:15,800 --> 01:15:18,800
for instance, they tend to work with DSLs that have like

1845
01:15:18,800 --> 01:15:21,920
100 to 200 primitive programs.

1846
01:15:21,920 --> 01:15:23,640
So very small DSL,

1847
01:15:23,640 --> 01:15:26,720
but then they're trying to combine these primitives

1848
01:15:26,720 --> 01:15:29,160
into very complex programs.

1849
01:15:29,160 --> 01:15:32,280
So there's very deep depths of search.

1850
01:15:32,280 --> 01:15:34,600
And on the other hand,

1851
01:15:34,600 --> 01:15:37,120
if you look at what Jack is doing with LLMs,

1852
01:15:37,120 --> 01:15:42,120
is that he's got this sort of like vector program database,

1853
01:15:43,200 --> 01:15:47,440
DSL of millions of building blocks in the LLM

1854
01:15:47,440 --> 01:15:50,240
that are mined by pre-training the LLM,

1855
01:15:50,240 --> 01:15:52,720
not just on a ton of programming problems,

1856
01:15:52,720 --> 01:15:56,400
but also on millions of generated Arc-like tasks.

1857
01:15:56,400 --> 01:15:59,680
So you have an extraordinarily large DSL.

1858
01:15:59,680 --> 01:16:03,000
And then the fun tuning is very, very shallow

1859
01:16:03,000 --> 01:16:04,880
recombination of these primitives.

1860
01:16:04,880 --> 01:16:08,560
So discrete program search, very deep recombination,

1861
01:16:08,560 --> 01:16:12,000
very small set of primitive programs.

1862
01:16:12,000 --> 01:16:14,160
And the LLM approach is the same,

1863
01:16:14,160 --> 01:16:17,260
but on the complete opposite end of that spectrum,

1864
01:16:17,260 --> 01:16:19,400
where you scale up the memorization

1865
01:16:19,400 --> 01:16:23,560
by a massive factor and you're doing very, very shallow search,

1866
01:16:23,560 --> 01:16:25,400
but they are the same thing,

1867
01:16:25,400 --> 01:16:27,400
just different ends of the spectrum.

1868
01:16:27,400 --> 01:16:31,160
And I think where you're gonna get the most value

1869
01:16:32,200 --> 01:16:36,200
for your compute cycles is gonna be somewhere in between.

1870
01:16:36,200 --> 01:16:40,680
You want to leverage memorization to build up a richer,

1871
01:16:40,680 --> 01:16:43,800
more useful bank of primitive programs.

1872
01:16:43,800 --> 01:16:46,080
And you don't want them to be hard-coded

1873
01:16:46,080 --> 01:16:48,400
like what we saw for the typical artist.

1874
01:16:48,400 --> 01:16:51,200
You want them to be learned from examples.

1875
01:16:51,200 --> 01:16:55,840
But then you also want to do some degree of deep search.

1876
01:16:55,840 --> 01:16:58,160
As long as you're only doing very shallow search,

1877
01:16:58,160 --> 01:17:00,120
you are limited to local generalization.

1878
01:17:00,120 --> 01:17:01,960
If you want to generalize further,

1879
01:17:01,960 --> 01:17:06,960
more broadly, this depth of search is gonna be critical.

1880
01:17:07,240 --> 01:17:11,840
I might argue that the reason that he had to rely so heavily

1881
01:17:11,840 --> 01:17:16,840
on the synthetic data was because he used a 240 million

1882
01:17:16,840 --> 01:17:19,600
parameter model because the Kaggle competition at the time

1883
01:17:19,600 --> 01:17:22,520
required him to use a P100 GPU,

1884
01:17:22,520 --> 01:17:26,400
which has like a 10th or something of the flops of an H100.

1885
01:17:26,400 --> 01:17:28,880
And so obviously he can't use,

1886
01:17:28,880 --> 01:17:32,400
if you believe that sort of scaling will solve

1887
01:17:32,400 --> 01:17:33,760
this kind of reasoning,

1888
01:17:33,760 --> 01:17:36,560
then there you can just rely on the generalization,

1889
01:17:36,560 --> 01:17:38,400
whereas if you're using a much smaller,

1890
01:17:38,400 --> 01:17:39,920
for context for the listeners, by the way,

1891
01:17:39,920 --> 01:17:41,520
the frontier models today are literally

1892
01:17:41,520 --> 01:17:43,080
a thousand X bigger than that.

1893
01:17:43,080 --> 01:17:46,280
And so for your competition,

1894
01:17:46,280 --> 01:17:47,960
from what I remember,

1895
01:17:47,960 --> 01:17:50,520
the submission you'll have to submit

1896
01:17:50,520 --> 01:17:53,360
can't make any API calls, can't go online,

1897
01:17:53,360 --> 01:17:57,640
and has to run on NVIDIA Tesla T4.

1898
01:17:57,640 --> 01:17:58,480
P100.

1899
01:17:58,480 --> 01:17:59,320
P100.

1900
01:17:59,320 --> 01:18:00,160
Oh, is it P100?

1901
01:18:00,160 --> 01:18:01,000
Yeah.

1902
01:18:01,000 --> 01:18:02,320
Okay, so again, it's like significantly less powerful.

1903
01:18:02,320 --> 01:18:03,760
There's a 12-hour runtime limit, basically.

1904
01:18:03,760 --> 01:18:06,280
There's a forcing function of efficiency in the eval.

1905
01:18:06,280 --> 01:18:09,600
But here's the thing, you only have 100 test tasks.

1906
01:18:09,600 --> 01:18:12,040
So the amount of computing available for each task

1907
01:18:12,040 --> 01:18:13,040
is actually quite a bit,

1908
01:18:13,080 --> 01:18:14,720
especially if you contrast that

1909
01:18:14,720 --> 01:18:16,640
with the simplicity of each task.

1910
01:18:16,640 --> 01:18:19,600
So it would be seven minutes per task, basically,

1911
01:18:19,600 --> 01:18:22,520
which for, people have tried to do these estimates

1912
01:18:22,520 --> 01:18:24,600
of how many flops does a human brain have.

1913
01:18:24,600 --> 01:18:26,080
And you can take them with a grain of salt,

1914
01:18:26,080 --> 01:18:28,240
but as a sort of anchor,

1915
01:18:28,240 --> 01:18:31,360
it's basically the amount of flops an H100 has.

1916
01:18:31,360 --> 01:18:32,960
And I guess maybe you would argue with that,

1917
01:18:32,960 --> 01:18:35,400
well, a human brain can solve this question

1918
01:18:35,400 --> 01:18:36,560
in faster than 7.2 minutes.

1919
01:18:36,560 --> 01:18:38,040
So even with a tenth of the compute,

1920
01:18:38,040 --> 01:18:40,560
you should be able to do it in seven minutes.

1921
01:18:40,560 --> 01:18:43,040
Obviously, we have less memory than, you know,

1922
01:18:43,040 --> 01:18:45,880
like petabytes of fast access memory in the brain.

1923
01:18:45,880 --> 01:18:48,840
And with these, you know, 29 or whatever gigabytes

1924
01:18:48,840 --> 01:18:50,400
in this H100.

1925
01:18:50,400 --> 01:18:52,480
Anyway, I guess the broader question I'm asking is,

1926
01:18:55,360 --> 01:18:58,560
I wish there was a way to also test this prize

1927
01:18:58,560 --> 01:19:01,120
with some sort of scaffolding on the biggest models

1928
01:19:01,120 --> 01:19:04,000
as a way to test whether scaling is the path

1929
01:19:04,000 --> 01:19:07,880
to get to solving ARC.

1930
01:19:07,880 --> 01:19:08,720
Absolutely.

1931
01:19:08,720 --> 01:19:09,920
So in the context of the computation,

1932
01:19:09,920 --> 01:19:12,320
we want to see how much progress we can do

1933
01:19:12,320 --> 01:19:13,800
with limited resources.

1934
01:19:13,800 --> 01:19:16,320
But you're entirely right that it's a super interesting

1935
01:19:16,320 --> 01:19:17,360
open question.

1936
01:19:17,360 --> 01:19:20,600
What could the biggest model out there actually do on ARC?

1937
01:19:20,600 --> 01:19:24,760
So we want to actually also make available a private

1938
01:19:24,760 --> 01:19:29,480
sort of like one-off track where you can submit to us a VM

1939
01:19:29,480 --> 01:19:32,000
and so you can put on it any model you want.

1940
01:19:32,000 --> 01:19:34,560
Like you can take one of the largest open source models

1941
01:19:34,560 --> 01:19:37,200
out there and find you need to do whatever you want

1942
01:19:37,200 --> 01:19:39,880
and just give us an image.

1943
01:19:39,880 --> 01:19:42,960
And then we run it on the H100 for like 24 hours

1944
01:19:42,960 --> 01:19:44,680
or something and you see what you get.

1945
01:19:44,680 --> 01:19:47,760
I think it's worth pointing out that there's two different

1946
01:19:47,760 --> 01:19:48,600
test sets.

1947
01:19:48,600 --> 01:19:50,760
There is a public test set that's in the public

1948
01:19:50,760 --> 01:19:53,720
GitHub repository that anyone can use to train, you know,

1949
01:19:53,720 --> 01:19:56,400
put it in an open API call, whatever you'd like to do.

1950
01:19:56,400 --> 01:19:57,720
And then there's the private test set,

1951
01:19:57,720 --> 01:19:59,320
which is the 100 that is actually measuring

1952
01:19:59,320 --> 01:20:00,720
the state of the art.

1953
01:20:00,720 --> 01:20:03,400
So I think it is pretty open and interesting to have folks

1954
01:20:03,400 --> 01:20:05,920
attempt to at least use the public test set and go try it.

1955
01:20:05,920 --> 01:20:09,160
Now, there is an asterisk on any score that's reported on

1956
01:20:09,160 --> 01:20:11,320
against the public test set because it is public.

1957
01:20:11,320 --> 01:20:13,320
It could have leaked into the training data.

1958
01:20:13,320 --> 01:20:15,240
And this is actually what people are already doing.

1959
01:20:15,240 --> 01:20:18,760
Like you can already try to prompt one of the best models

1960
01:20:18,760 --> 01:20:22,880
like the latest Jaminar, the latest GPT-4 with tasks

1961
01:20:22,880 --> 01:20:24,360
from the public evaluation set.

1962
01:20:24,360 --> 01:20:27,400
And you know, again, the primary set, these tasks

1963
01:20:27,400 --> 01:20:30,400
are available as JSON files on GitHub.

1964
01:20:30,400 --> 01:20:32,440
These models are also trained on GitHub.

1965
01:20:32,440 --> 01:20:34,800
So they're actually trained on these tasks.

1966
01:20:35,720 --> 01:20:38,960
And yeah, that kind of creates uncertainty about

1967
01:20:38,960 --> 01:20:40,960
if they can actually solve some of the tasks,

1968
01:20:40,960 --> 01:20:43,960
is that because they memorized the answer or not.

1969
01:20:43,960 --> 01:20:47,080
You know, maybe you would be better off trying to create

1970
01:20:47,080 --> 01:20:53,080
your own private, arc-like, very novel test set.

1971
01:20:53,120 --> 01:20:54,720
Don't make the task difficult.

1972
01:20:54,720 --> 01:20:55,760
Don't make them complex.

1973
01:20:55,760 --> 01:20:57,160
Make them very obvious for humans.

1974
01:20:57,160 --> 01:21:00,840
But make sure to make them original as much as possible.

1975
01:21:00,840 --> 01:21:02,480
Make them unique, different.

1976
01:21:02,480 --> 01:21:06,720
And see how much your GPT-4 and so on GPT-5 does on them.

1977
01:21:06,720 --> 01:21:08,720
Well, they're having tests on whether these models

1978
01:21:08,720 --> 01:21:11,040
are being overtrained on these benchmarks.

1979
01:21:11,040 --> 01:21:14,120
Scale recently did this where on the GSM-

1980
01:21:14,120 --> 01:21:14,960
That's really interesting.

1981
01:21:14,960 --> 01:21:17,560
AK, they basically replicated the benchmark

1982
01:21:17,560 --> 01:21:18,840
with different questions.

1983
01:21:18,840 --> 01:21:20,480
And so some of the models actually were extremely

1984
01:21:20,480 --> 01:21:24,000
overfit on the benchmark like Mistral and so forth.

1985
01:21:24,000 --> 01:21:28,600
And but the frontier models, Claude and GPT actually did

1986
01:21:28,600 --> 01:21:30,760
as well on their novel benchmark that they did

1987
01:21:30,760 --> 01:21:32,160
on the specific questions that were

1988
01:21:32,160 --> 01:21:35,160
in the existing public benchmark.

1989
01:21:35,160 --> 01:21:37,880
So I would be relatively optimistic about them

1990
01:21:37,880 --> 01:21:40,320
just sort of training on the JSON.

1991
01:21:40,320 --> 01:21:44,080
I was joking with Mike that you should allow API access

1992
01:21:44,080 --> 01:21:49,080
but sort of keep an even more private validation set

1993
01:21:49,160 --> 01:21:51,440
of these arc questions.

1994
01:21:51,440 --> 01:21:53,760
And so allow API access, people can sort of play

1995
01:21:53,760 --> 01:21:56,400
with GPT-4 scaffolding to enter into this contest.

1996
01:21:56,400 --> 01:21:58,960
And if it turns out maybe later on you run the validation

1997
01:21:58,960 --> 01:22:01,440
set on the API and if it performs worse

1998
01:22:01,440 --> 01:22:03,880
than the test set that you allowed the API access

1999
01:22:03,880 --> 01:22:07,160
to originally, that means that open AI is training

2000
01:22:07,160 --> 01:22:09,760
on your API calls and you like go public with this

2001
01:22:09,760 --> 01:22:10,760
and show them like, oh my God,

2002
01:22:10,760 --> 01:22:13,080
they've like leaked your data.

2003
01:22:13,080 --> 01:22:15,680
We do want to make, we want to evolve the arc data set.

2004
01:22:15,680 --> 01:22:17,560
Like that is a goal that we want to do.

2005
01:22:17,560 --> 01:22:19,520
I think Francois you mentioned, you know, it's not perfect.

2006
01:22:19,520 --> 01:22:22,080
Yeah, no, arc is not perfect for perfect benchmark.

2007
01:22:22,080 --> 01:22:24,080
I mean, I made it like four years ago

2008
01:22:24,080 --> 01:22:26,720
over four years ago, almost five now.

2009
01:22:26,720 --> 01:22:28,840
This was in a time before LMS.

2010
01:22:28,840 --> 01:22:31,800
And I think we learned a lot actually since

2011
01:22:31,800 --> 01:22:34,200
about what potential flaws there might be.

2012
01:22:34,200 --> 01:22:37,560
I think there is some redundancy in the set of tasks

2013
01:22:37,560 --> 01:22:40,200
which is of course against the goals of the benchmark.

2014
01:22:40,200 --> 01:22:42,680
Every task is supposed to be unique in practice.

2015
01:22:42,680 --> 01:22:44,000
That's not quite true.

2016
01:22:44,000 --> 01:22:47,080
I think there's also, every task is supposed

2017
01:22:47,080 --> 01:22:49,760
to be very novel, but in practice, they might not be.

2018
01:22:49,760 --> 01:22:52,440
They might be structurally similar to something

2019
01:22:52,440 --> 01:22:54,680
that you might find online somewhere.

2020
01:22:54,680 --> 01:22:56,480
So we want to keep iterating

2021
01:22:56,480 --> 01:23:00,240
and release an arc two version later this year.

2022
01:23:00,240 --> 01:23:01,720
And I think when we do that,

2023
01:23:01,720 --> 01:23:06,720
we're gonna want to make the old private test set available.

2024
01:23:06,800 --> 01:23:08,720
So maybe we won't be releasing it publicly,

2025
01:23:08,720 --> 01:23:13,400
but what we could do is just create a test server

2026
01:23:13,400 --> 01:23:16,520
where you can query, get a task, you submit a solution,

2027
01:23:16,520 --> 01:23:18,360
and of course you can use whatever frontier model

2028
01:23:18,360 --> 01:23:19,640
you want there.

2029
01:23:19,640 --> 01:23:22,720
So that way, because you actually have to query this API,

2030
01:23:22,760 --> 01:23:26,160
you're making sure that no one is gonna buy accident train

2031
01:23:26,160 --> 01:23:27,000
on this data.

2032
01:23:27,000 --> 01:23:29,800
It's unlike like the current public article

2033
01:23:29,800 --> 01:23:31,240
which is literally on GitHub.

2034
01:23:31,240 --> 01:23:33,160
So there's no question about whether the models

2035
01:23:33,160 --> 01:23:34,000
are actually trained on it.

2036
01:23:34,000 --> 01:23:36,440
Yes, they are because they're trained on GitHub.

2037
01:23:36,440 --> 01:23:39,520
So by sort of like gating access

2038
01:23:39,520 --> 01:23:42,520
to querying this API with a various issue.

2039
01:23:42,520 --> 01:23:44,120
And then we would see, you know,

2040
01:23:44,120 --> 01:23:46,920
for people who actually wanna try

2041
01:23:46,920 --> 01:23:48,120
whatever technique they have in mind

2042
01:23:48,120 --> 01:23:50,760
using whatever resources they want,

2043
01:23:50,760 --> 01:23:52,600
that would be a way for them to get an answer.

2044
01:23:52,640 --> 01:23:54,160
I wonder what might happen.

2045
01:23:54,160 --> 01:23:55,320
I'm not sure.

2046
01:23:55,320 --> 01:23:58,520
One answer is that they come up with a whole new algorithm

2047
01:23:58,520 --> 01:24:02,320
for AI with some explicit program synthesis

2048
01:24:02,320 --> 01:24:03,840
that now we're on a new track.

2049
01:24:03,840 --> 01:24:06,680
And another is they did something hacky

2050
01:24:06,680 --> 01:24:10,320
with the existing models in a way that actually is valid,

2051
01:24:10,320 --> 01:24:12,800
which reveals that movie intelligence is more

2052
01:24:12,800 --> 01:24:15,320
of getting things to the right part of the distribution,

2053
01:24:15,320 --> 01:24:16,640
but then it can reason.

2054
01:24:16,640 --> 01:24:19,760
And in that world, I guess that will be interesting.

2055
01:24:19,760 --> 01:24:21,440
And maybe that'll indicate that, you know,

2056
01:24:21,480 --> 01:24:23,240
you had to do something hacky with current models

2057
01:24:23,240 --> 01:24:24,080
as they get better,

2058
01:24:24,080 --> 01:24:25,840
you won't have to do something hacky.

2059
01:24:27,120 --> 01:24:29,200
I'm also gonna be very curious to see

2060
01:24:29,200 --> 01:24:30,720
how these multimodal models,

2061
01:24:30,720 --> 01:24:33,840
if they will perform natively much better at arc like tests.

2062
01:24:33,840 --> 01:24:35,360
If arc survives three months from here,

2063
01:24:35,360 --> 01:24:37,000
we'll blow up the price.

2064
01:24:37,000 --> 01:24:39,000
I think we're about to make a really important moment

2065
01:24:39,000 --> 01:24:41,640
of like contact with reality by blowing up the price,

2066
01:24:41,640 --> 01:24:43,360
putting a much big price pool against it.

2067
01:24:43,360 --> 01:24:44,400
We're gonna learn really quickly

2068
01:24:44,400 --> 01:24:46,600
if there's like low hanging fruit of ideas.

2069
01:24:46,600 --> 01:24:47,920
Again, I think new ideas are needed.

2070
01:24:47,920 --> 01:24:49,000
I think anyone listening this

2071
01:24:49,000 --> 01:24:51,160
might have the idea in their head.

2072
01:24:51,160 --> 01:24:53,520
And I'd encourage everyone to like give it a try.

2073
01:24:53,520 --> 01:24:55,520
And I think as time goes on,

2074
01:24:55,520 --> 01:24:56,920
that adds strength to the argument

2075
01:24:56,920 --> 01:24:59,280
that like we sort of stall that in progress

2076
01:24:59,280 --> 01:25:00,920
and that new ideas are necessary to be dark.

2077
01:25:00,920 --> 01:25:03,600
Yeah, that's the point of having a money price

2078
01:25:03,600 --> 01:25:06,120
is that you attract more people,

2079
01:25:06,120 --> 01:25:07,720
you get them to try to solve it.

2080
01:25:07,720 --> 01:25:09,960
And if there's a easy way to hack the benchmark

2081
01:25:09,960 --> 01:25:11,480
that reveals that the benchmark is valid,

2082
01:25:11,480 --> 01:25:12,640
then you're gonna know about it.

2083
01:25:12,640 --> 01:25:13,520
In fact, that was the point

2084
01:25:13,520 --> 01:25:18,200
of the original Carol competition back in 2020 for arc.

2085
01:25:19,040 --> 01:25:20,280
I was running this competition

2086
01:25:20,320 --> 01:25:22,440
because I had released this dataset

2087
01:25:22,440 --> 01:25:26,640
and I wanted to know if it was hackable, if you could cheat.

2088
01:25:26,640 --> 01:25:28,800
So there was a small money price at the time,

2089
01:25:28,800 --> 01:25:30,520
there was like 20K.

2090
01:25:30,520 --> 01:25:32,360
And this was right around the same time

2091
01:25:32,360 --> 01:25:34,720
as GPT-3 was released.

2092
01:25:34,720 --> 01:25:37,560
So people of course tried GPT-3 on the public data,

2093
01:25:37,560 --> 01:25:38,480
it scored zero.

2094
01:25:39,560 --> 01:25:42,560
But I think what the first context

2095
01:25:42,560 --> 01:25:45,240
the first context taught us is that

2096
01:25:45,240 --> 01:25:48,160
there is no obvious shortcuts, right?

2097
01:25:49,160 --> 01:25:50,680
And well, now there's more money,

2098
01:25:50,680 --> 01:25:53,960
there's gonna be more people looking into it.

2099
01:25:53,960 --> 01:25:56,120
Well, we're gonna find out,

2100
01:25:56,120 --> 01:25:58,720
we're gonna see if the benchmark is gonna survive.

2101
01:25:58,720 --> 01:26:02,400
And you know, if we end up with a solution

2102
01:26:02,400 --> 01:26:05,840
that is not like trying to brute force

2103
01:26:05,840 --> 01:26:07,400
the space of possible arc tasks

2104
01:26:07,400 --> 01:26:09,920
that's just trained on core knowledge,

2105
01:26:09,920 --> 01:26:13,800
I don't think it's necessarily gonna be in and by itself, AGI,

2106
01:26:13,800 --> 01:26:16,440
but it's probably gonna be a huge milestone

2107
01:26:16,440 --> 01:26:18,040
on the way to AGI.

2108
01:26:18,120 --> 01:26:23,120
Because what it represents is the ability to synthesize,

2109
01:26:25,800 --> 01:26:28,560
task a problem solving program

2110
01:26:28,560 --> 01:26:31,920
from just two or three examples.

2111
01:26:31,920 --> 01:26:35,080
And that alone is a new way to program.

2112
01:26:35,080 --> 01:26:38,000
It's an entirely new paradigm for software development

2113
01:26:38,000 --> 01:26:39,760
where you can start programming

2114
01:26:39,760 --> 01:26:41,720
potentially quite complex programs

2115
01:26:41,720 --> 01:26:44,000
that will generalize very well.

2116
01:26:44,000 --> 01:26:46,680
And instead of programming them by coming up

2117
01:26:46,720 --> 01:26:49,920
with the shape of the program in your mind

2118
01:26:49,920 --> 01:26:52,080
and then tapping it up,

2119
01:26:52,080 --> 01:26:54,800
you're actually just showing the computer

2120
01:26:54,800 --> 01:26:55,960
what add what you want

2121
01:26:55,960 --> 01:26:58,520
and you let the computer figure it out.

2122
01:26:58,520 --> 01:27:00,400
I think that's what is extremely powerful.

2123
01:27:00,400 --> 01:27:03,360
I wanna riff a little bit on what kinds of solutions

2124
01:27:03,360 --> 01:27:04,200
might be possible here

2125
01:27:04,200 --> 01:27:06,400
and which you would consider sort of defeating

2126
01:27:06,400 --> 01:27:09,320
the purpose of arc and which are sort of valid.

2127
01:27:10,600 --> 01:27:14,000
Here's one I'll mention which is my friends

2128
01:27:14,000 --> 01:27:16,480
that Ryan and Buck stayed up last night

2129
01:27:16,480 --> 01:27:17,840
because I told them about this

2130
01:27:17,840 --> 01:27:19,240
and they were like, oh, of course,

2131
01:27:19,240 --> 01:27:20,080
I was gonna solve this.

2132
01:27:20,080 --> 01:27:20,920
Thank you for spreading the word.

2133
01:27:20,920 --> 01:27:21,760
Of course, I was gonna solve this.

2134
01:27:21,760 --> 01:27:23,560
And then so they were trying to prompt,

2135
01:27:23,560 --> 01:27:25,400
I think Claude, Opus on this

2136
01:27:25,400 --> 01:27:29,640
and they say they got 25% on the public arc test.

2137
01:27:30,520 --> 01:27:33,320
And what they did was have other examples

2138
01:27:33,320 --> 01:27:34,760
of some of the arc tests

2139
01:27:34,760 --> 01:27:36,760
and in context explain the reasoning

2140
01:27:36,760 --> 01:27:39,520
of why you went from one output to another output

2141
01:27:39,520 --> 01:27:41,400
and then now you have the current problem.

2142
01:27:41,400 --> 01:27:44,960
And I think also maybe expressing the JSON in a way

2143
01:27:44,960 --> 01:27:48,240
that is more amenable to the tokenizer.

2144
01:27:48,240 --> 01:27:51,880
And another thing was using the code interpreter.

2145
01:27:51,880 --> 01:27:54,080
So I'm curious actually,

2146
01:27:54,080 --> 01:27:55,800
if you think the code interpreter,

2147
01:27:55,800 --> 01:27:58,320
which keeps getting better as these models get smarter

2148
01:27:58,320 --> 01:28:00,640
is just the program synthesis right there

2149
01:28:00,640 --> 01:28:02,200
because what they were able to do

2150
01:28:02,200 --> 01:28:06,280
was the actual output of the cells, the JSON output,

2151
01:28:06,280 --> 01:28:08,640
they got through the code interpreter,

2152
01:28:08,640 --> 01:28:10,880
like write the Python program that gets right up here.

2153
01:28:10,880 --> 01:28:13,240
Do you think that the program synthesis

2154
01:28:13,240 --> 01:28:14,440
kind of researcher talking about

2155
01:28:14,440 --> 01:28:16,800
will look like just using the code interpreter

2156
01:28:16,800 --> 01:28:17,760
in large language models?

2157
01:28:17,760 --> 01:28:20,320
I think whatever solution we see that will score well

2158
01:28:20,320 --> 01:28:24,560
is gonna probably need to leverage some aspects

2159
01:28:24,560 --> 01:28:27,040
from deep learning models and LLMs in particular.

2160
01:28:27,040 --> 01:28:30,040
We've shown already that LLMs can do quite well,

2161
01:28:30,040 --> 01:28:32,400
that's basically the jack code approach.

2162
01:28:32,400 --> 01:28:35,080
We've also shown that pure discrete program search

2163
01:28:35,080 --> 01:28:37,480
from a small DSL does very, very well

2164
01:28:37,480 --> 01:28:39,120
before jack code, this was the state of the art.

2165
01:28:39,120 --> 01:28:41,320
In fact, it's still extremely close to the state of the art.

2166
01:28:41,440 --> 01:28:44,160
And there's no deep learning involved at all in these models.

2167
01:28:44,160 --> 01:28:48,240
So we have two approaches that have basically no overlap

2168
01:28:48,240 --> 01:28:49,280
that are doing quite well.

2169
01:28:49,280 --> 01:28:53,640
And they're very much at two opposite ends of one spectrum,

2170
01:28:53,640 --> 01:28:56,840
where on one end you have these extremely large banks

2171
01:28:56,840 --> 01:28:58,800
of millions of vector programs,

2172
01:28:58,800 --> 01:29:01,120
but very, very shallow recombination,

2173
01:29:01,120 --> 01:29:02,720
like simplistic recombination.

2174
01:29:02,720 --> 01:29:05,840
And on the other end, you have very simplistic DSLs,

2175
01:29:05,840 --> 01:29:08,840
very simple, like 100 or 200 primitives,

2176
01:29:08,840 --> 01:29:11,920
but very deep, very sophisticated program search.

2177
01:29:12,840 --> 01:29:15,160
The solution is gonna be somewhere in between, right?

2178
01:29:15,160 --> 01:29:19,720
So the people are gonna be winning the art competition

2179
01:29:19,720 --> 01:29:21,840
and we're gonna be making the most progress

2180
01:29:21,840 --> 01:29:23,960
towards near-term NGR are gonna be users

2181
01:29:23,960 --> 01:29:27,000
that manage to merge the deep learning paradigm

2182
01:29:27,000 --> 01:29:28,800
and the discrete program search paradigm

2183
01:29:28,800 --> 01:29:31,560
into one elegant way.

2184
01:29:31,560 --> 01:29:33,280
And you know, you ask like,

2185
01:29:33,280 --> 01:29:36,560
what would be legitimate and what would be cheating,

2186
01:29:36,560 --> 01:29:37,400
for instance?

2187
01:29:37,840 --> 01:29:41,080
You wanna add a code interpreter to the system.

2188
01:29:41,080 --> 01:29:42,760
I think that's great, that's sort of legitimate.

2189
01:29:42,760 --> 01:29:45,080
The part that would be cheating is try to

2190
01:29:47,000 --> 01:29:49,240
anticipate what might be in the test set,

2191
01:29:49,240 --> 01:29:52,320
like brute force the space of possible tasks

2192
01:29:52,320 --> 01:29:55,160
and then train a memorization system on it.

2193
01:29:55,160 --> 01:29:57,720
And then rely on the fact that you're generating

2194
01:29:57,720 --> 01:29:59,880
so many tasks, like millions and millions and millions,

2195
01:29:59,880 --> 01:30:02,160
that inevitably there's gonna be some overlap

2196
01:30:02,160 --> 01:30:04,720
between what you're generating and what's in the test set.

2197
01:30:04,760 --> 01:30:07,960
I think that's defeating the purpose of benchmark

2198
01:30:07,960 --> 01:30:09,840
because then you can just solve it with that

2199
01:30:09,840 --> 01:30:13,280
and you need to adapt just by fetching a memorized solution.

2200
01:30:13,280 --> 01:30:15,640
So hopefully arc will resist to that,

2201
01:30:15,640 --> 01:30:18,160
but you know, nothing, no benchmark is necessarily perfect.

2202
01:30:18,160 --> 01:30:20,160
So maybe there's a way to hack it

2203
01:30:20,160 --> 01:30:22,200
and I guess we are gonna get an answer very soon.

2204
01:30:22,200 --> 01:30:24,120
Although I think some amount of fine tuning is valid

2205
01:30:24,120 --> 01:30:27,520
because these models don't natively think in terms of,

2206
01:30:27,520 --> 01:30:28,840
especially the language models alone,

2207
01:30:28,840 --> 01:30:31,080
which the open source models that they would have to use

2208
01:30:31,080 --> 01:30:33,200
to be competitive here compete here.

2209
01:30:33,800 --> 01:30:34,920
They're like natively language,

2210
01:30:34,920 --> 01:30:38,080
so they need to be able to think in this kind of...

2211
01:30:38,080 --> 01:30:38,920
Yes.

2212
01:30:38,920 --> 01:30:39,760
The arc type way.

2213
01:30:39,760 --> 01:30:41,480
You want to input corner ledge,

2214
01:30:41,480 --> 01:30:44,440
like arc like corner ledge into the model,

2215
01:30:44,440 --> 01:30:47,280
but surely you don't need tens of millions of tasks

2216
01:30:47,280 --> 01:30:50,080
to do this, like corner ledge is extremely basic.

2217
01:30:50,080 --> 01:30:52,960
If you look at some of these arc type questions,

2218
01:30:54,360 --> 01:30:56,480
I actually do think they rely a little bit

2219
01:30:56,480 --> 01:30:59,640
on things I have seen throughout my life.

2220
01:30:59,640 --> 01:31:02,800
And for the same, like for example,

2221
01:31:02,800 --> 01:31:04,640
like something bounces off a wall

2222
01:31:04,640 --> 01:31:06,160
and comes back and you see that pattern.

2223
01:31:06,160 --> 01:31:07,480
It's like I played arcade games

2224
01:31:07,480 --> 01:31:09,600
and I've seen like pong or something.

2225
01:31:09,600 --> 01:31:11,440
And I think for example, when you see the Flynn effect

2226
01:31:11,440 --> 01:31:14,120
and people's intelligence has measured on

2227
01:31:14,120 --> 01:31:15,560
very advanced progressive matrices

2228
01:31:15,560 --> 01:31:17,360
increasing on these kinds of questions,

2229
01:31:17,360 --> 01:31:19,360
it's probably a simpler story where since now,

2230
01:31:19,360 --> 01:31:21,160
since childhood, we actually see these sorts of patterns

2231
01:31:21,160 --> 01:31:23,400
in TV and whatever, spatial patterns.

2232
01:31:23,400 --> 01:31:26,200
And so I don't think this is sort of core knowledge.

2233
01:31:26,200 --> 01:31:29,240
I think actually this is also part of the quote unquote

2234
01:31:29,240 --> 01:31:31,720
trying tuning that humans have as they grow up

2235
01:31:31,720 --> 01:31:34,080
of seeing different kinds of spatial patterns

2236
01:31:34,080 --> 01:31:35,480
and trying to pattern match to them.

2237
01:31:35,480 --> 01:31:37,800
I would definitely file that under core knowledge.

2238
01:31:37,800 --> 01:31:40,600
Like core knowledge includes basic physics,

2239
01:31:40,600 --> 01:31:43,160
for instance, bouncing or trajectories,

2240
01:31:43,160 --> 01:31:44,400
that would be included.

2241
01:31:44,400 --> 01:31:45,800
But yeah, I think you're entirely right.

2242
01:31:45,800 --> 01:31:47,160
The reason why as a human,

2243
01:31:47,160 --> 01:31:49,040
you're able to quickly figure out the solution

2244
01:31:49,040 --> 01:31:51,840
is because you have this set of building blocks,

2245
01:31:51,840 --> 01:31:54,160
this set of patterns in your mind that you can recombine.

2246
01:31:54,160 --> 01:31:57,240
Is core knowledge required to attain intelligence?

2247
01:31:57,240 --> 01:31:58,680
Any algorithm you have,

2248
01:31:58,760 --> 01:32:00,760
does the core knowledge have to be in some sense hard coded

2249
01:32:00,760 --> 01:32:03,760
or can even the core knowledge be learned through intelligence?

2250
01:32:03,760 --> 01:32:05,040
Core knowledge can be learned.

2251
01:32:05,040 --> 01:32:07,200
And I think in the case of humans,

2252
01:32:07,200 --> 01:32:09,600
some amount of core knowledge is something

2253
01:32:09,600 --> 01:32:10,640
that you're born with.

2254
01:32:10,640 --> 01:32:13,640
Like we're actually born with a small amount of knowledge

2255
01:32:13,640 --> 01:32:15,560
about the world we're gonna live in.

2256
01:32:15,560 --> 01:32:17,080
We're not blank slates.

2257
01:32:17,080 --> 01:32:20,640
But most core knowledge is acquired through experience.

2258
01:32:20,640 --> 01:32:22,000
But the thing with core knowledge

2259
01:32:22,000 --> 01:32:25,320
that it's not gonna be acquired like for instance in school,

2260
01:32:25,320 --> 01:32:27,520
it's actually acquired very, very early

2261
01:32:27,520 --> 01:32:30,400
in the first like three to four years of your life.

2262
01:32:30,400 --> 01:32:31,440
And by age four,

2263
01:32:31,440 --> 01:32:34,560
you have all the core knowledge you're gonna need as an adult.

2264
01:32:34,560 --> 01:32:36,040
Okay, interesting.

2265
01:32:36,040 --> 01:32:37,800
So I mean, on the price itself,

2266
01:32:37,800 --> 01:32:40,960
I'm super excited to see both the open source versions

2267
01:32:40,960 --> 01:32:44,120
of maybe with a Lama 70B or something

2268
01:32:44,120 --> 01:32:46,520
what people can score in the competition itself.

2269
01:32:46,520 --> 01:32:50,520
Then if to sort of test specifically

2270
01:32:50,520 --> 01:32:51,600
the scaling hypothesis,

2271
01:32:51,600 --> 01:32:53,560
I'm very curious to see if you can prompt

2272
01:32:53,560 --> 01:32:55,120
on the public version of ARC,

2273
01:32:55,120 --> 01:32:56,080
which I guess when we compare,

2274
01:32:56,080 --> 01:32:58,640
you will be able to submit to this competition itself.

2275
01:32:58,640 --> 01:33:00,080
But I'd be very curious to see how,

2276
01:33:00,080 --> 01:33:02,800
if people can sort of crack that and get ARC working there

2277
01:33:02,800 --> 01:33:04,640
and if that would update your reviews on AGI.

2278
01:33:04,640 --> 01:33:05,640
It's gonna be motivating.

2279
01:33:05,640 --> 01:33:06,760
We're gonna keep running the contest

2280
01:33:06,760 --> 01:33:09,040
until somebody puts a reproducible open source version

2281
01:33:09,040 --> 01:33:09,880
into public domain.

2282
01:33:09,880 --> 01:33:13,760
So even if somebody privately beats the ARC eval,

2283
01:33:13,760 --> 01:33:14,880
we're gonna still keep the price money

2284
01:33:14,880 --> 01:33:16,080
until someone can reproduce it

2285
01:33:16,080 --> 01:33:18,560
and put the public reproducible version out there.

2286
01:33:18,560 --> 01:33:19,400
Yeah, exactly.

2287
01:33:19,400 --> 01:33:22,280
Like the goal is to accelerate progress towards AGI.

2288
01:33:22,280 --> 01:33:24,280
And a key part of that is that

2289
01:33:24,280 --> 01:33:26,160
any sort of meaningful bits of progress

2290
01:33:26,160 --> 01:33:28,800
needs to be shared, needs to be public.

2291
01:33:28,800 --> 01:33:30,440
So everyone can know about it

2292
01:33:30,440 --> 01:33:32,160
and can try to iterate on it.

2293
01:33:32,160 --> 01:33:33,720
If there's no sharing, there's no progress.

2294
01:33:33,720 --> 01:33:35,040
What I'm especially curious about

2295
01:33:35,040 --> 01:33:36,640
is sort of disaggregating the bets

2296
01:33:36,640 --> 01:33:39,840
of like, can we make an open version of this

2297
01:33:39,840 --> 01:33:43,320
versus is this a thing that's just possible with scaling?

2298
01:33:43,320 --> 01:33:45,680
And we can, I guess test both of them

2299
01:33:45,680 --> 01:33:47,840
based on the public and the private version.

2300
01:33:47,840 --> 01:33:50,680
We're making contact with reality as well with this, right?

2301
01:33:50,680 --> 01:33:51,520
We're gonna learn a lot, I think,

2302
01:33:51,520 --> 01:33:52,920
about what the actual limits of the compute

2303
01:33:52,920 --> 01:33:53,920
where if someone showed up and said,

2304
01:33:53,920 --> 01:33:55,120
hey, here's a closed source model

2305
01:33:55,120 --> 01:33:57,240
that like I'm getting 50 plus percent on,

2306
01:33:57,240 --> 01:33:58,720
I think that would probably update us on like,

2307
01:33:58,720 --> 01:34:00,400
okay, perhaps we should increase the amount of compute

2308
01:34:00,400 --> 01:34:01,600
that we give on the private test set

2309
01:34:01,600 --> 01:34:03,760
in order to balance some of the decisions

2310
01:34:03,760 --> 01:34:04,960
that initially are somewhat arbitrary

2311
01:34:04,960 --> 01:34:07,240
in order to learn about, okay, what do people want?

2312
01:34:07,240 --> 01:34:08,240
What does progress look like?

2313
01:34:08,240 --> 01:34:10,480
And I think both of us are sort of committed

2314
01:34:10,480 --> 01:34:12,640
to evolving it over time in order to be the best,

2315
01:34:12,640 --> 01:34:14,360
or the closest to perfect as we can get it.

2316
01:34:14,360 --> 01:34:15,840
Awesome, and where can people go to learn more

2317
01:34:15,840 --> 01:34:18,280
about the prize and maybe give their hand at it?

2318
01:34:18,280 --> 01:34:19,320
ARCPrize.org.

2319
01:34:19,320 --> 01:34:21,000
Which goes live today, so.

2320
01:34:21,000 --> 01:34:21,840
It's live now.

2321
01:34:21,880 --> 01:34:23,640
$70 million is on this line, people.

2322
01:34:23,640 --> 01:34:24,480
Good luck.

2323
01:34:24,480 --> 01:34:25,320
Thank you guys for coming on the podcast.

2324
01:34:25,320 --> 01:34:26,840
It was super fun to go through all the cruxes

2325
01:34:26,840 --> 01:34:28,920
on intelligence and get a different perspective,

2326
01:34:28,920 --> 01:34:30,880
and also to announce a prize here.

2327
01:34:30,880 --> 01:34:31,800
So this is awesome.

2328
01:34:31,800 --> 01:34:32,960
Thank you for helping break news.

2329
01:34:32,960 --> 01:34:33,960
Thank you, Finest.

