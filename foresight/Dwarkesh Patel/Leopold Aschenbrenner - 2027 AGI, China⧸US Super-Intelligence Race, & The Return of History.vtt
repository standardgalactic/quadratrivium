WEBVTT

00:00.000 --> 00:03.920
What will be at stake will not just be equal products, but whether the liberal democracy

00:03.920 --> 00:08.320
survives, whether the CCP survives, what the world order for the next century will be.

00:08.320 --> 00:12.640
The CCP is going to have an all-out effort to infiltrate American AI labs, billions of dollars,

00:12.640 --> 00:16.560
thousands of people. The CCP is going to try to outbuild us. People don't realize how intense

00:16.560 --> 00:20.000
state-level espionage can be. When we have literal superintelligence on our cluster,

00:20.000 --> 00:23.840
and they can stuxnet the Chinese data centers, you really think they'll be a private company,

00:23.840 --> 00:27.680
and the government would be like, oh my god, what is going on? I do think it is incredibly

00:27.680 --> 00:31.200
important that these clusters are in the United States. I mean, would you do the Manhattan Project

00:31.200 --> 00:35.600
in the UAE, right? 2023 was the sort of moment for me where it went from kind of AGI as a sort of

00:35.600 --> 00:39.840
theoretical abstract thing, and you'd make the models to like, I see it, I feel it. I can see the

00:39.840 --> 00:43.120
cluster where it's strained on, like the rough combination of algorithms, the people, like how

00:43.120 --> 00:46.720
it's happening. And I think, you know, most of the world is not, you know, most of the people feel it

00:46.720 --> 00:52.720
are like right here, you know, right? Okay, today I'm chatting with my friend, Leopold Aschenbrenner.

00:52.720 --> 00:57.280
He grew up in Germany, graduated valedictorian of Columbia when he was 19,

00:58.000 --> 01:03.680
and then he had a very interesting Gaffier, which we'll talk about, and then he was on the

01:03.680 --> 01:10.880
OpenAI Superalignment team, made a recent piece, and now he, with some anchor investments from

01:10.880 --> 01:16.640
Patrick and John Collison and Daniel Gross and Nat Friedman, is launching an investment firm.

01:16.640 --> 01:21.520
So Leopold, I know you're off to a slow start, but life is long, and I wouldn't worry about it

01:21.520 --> 01:26.720
too much. You'll make up for it in due time. But thanks for coming on the podcast.

01:26.720 --> 01:31.200
Thank you. You know, I first discovered your podcast when your best episode had, you know,

01:31.200 --> 01:34.960
like a couple of hundred views. And so it's just been, it's been amazing to follow your

01:34.960 --> 01:39.920
trajectory. And it's a delight to be on. Yeah, yeah. Well, I think in the shelter in Trenton

01:39.920 --> 01:45.200
episode, I mentioned that a lot of the things I've learned about AI, I've learned from talking with

01:45.200 --> 01:49.440
them. And the third part of this triumvirate, probably the most significant in terms of the

01:49.440 --> 01:53.520
things that I've learned about AI has been you will go out of the stuff on the record now.

01:53.520 --> 01:57.920
Great. Okay, first thing I had to get on record, tell me about the trillion dollar cluster.

01:58.560 --> 02:02.880
But by the way, I should mention, so the context of this podcast is today, there's,

02:02.880 --> 02:06.960
you're releasing a series called Situational Awareness. We're going to get into it. First

02:06.960 --> 02:11.520
question about that is tell me about the trillion dollar cluster. Yeah. So, you know,

02:11.520 --> 02:15.520
unlike basically most things that have come out of Silicon Valley recently, you know, AI is kind of

02:15.520 --> 02:20.400
this industrial process. You know, the next model doesn't just require, you know, some code,

02:20.400 --> 02:24.640
it's, it's, it's building a giant new cluster. You know, now it's building giant new power plants,

02:24.640 --> 02:30.000
you know, pretty soon it's going to be building giant new fabs. And, you know, since that should

02:30.000 --> 02:33.680
be tea, this kind of extraordinary sort of techno capital acceleration has been set into motion.

02:33.680 --> 02:38.160
I mean, basically, you know, exactly a year ago today, you know, Nvidia had their first kind of

02:38.160 --> 02:41.920
blockbuster earnings call, right, where it like went out 25% after hours and everyone was like,

02:41.920 --> 02:46.960
oh my god, AI, it's a thing. You know, I mean, I think within a year, you know, you know, and

02:46.960 --> 02:50.640
Nvidia, Nvidia data center revenue has gone from like, you know, a few billion a quarter to like,

02:50.640 --> 02:55.040
you know, 20, 25 billion a quarter now and, you know, continue to go up, like, you know,

02:55.040 --> 03:01.040
big tech capex, skyrocketing. And, you know, it's funny because it's both there's this sort of,

03:01.040 --> 03:04.720
this kind of crazy scramble going on, but in some sense, it's just the sort of continuation of

03:04.720 --> 03:07.920
straight lines on a graph, right? There's this kind of like long run trend, basically almost a

03:07.920 --> 03:11.840
decade of sort of training compute of the sort of largest AI systems growing by about, you know,

03:11.840 --> 03:17.360
half an order of magnitude, you know, 0.5 booms a year. And you just kind of play that forward,

03:17.360 --> 03:23.120
right? So, you know, GPT-4, you know, rumored or reported to have finished pre-training in 2022,

03:23.120 --> 03:27.360
you know, the sort of cluster size there was rumored to be about, you know, 25,000 H100s,

03:27.360 --> 03:32.640
you know, sorry, A100s on semi-analysis, you know, that's roughly, you know, if you do the

03:32.640 --> 03:36.880
math on that, it's maybe like a $500 million cluster, you know, it's very roughly 10 megawatts.

03:37.600 --> 03:44.480
And, you know, just play that forward half a new year, right? So, then 2024, that's a cluster,

03:44.480 --> 03:48.800
that's, you know, 100 megawatts, that's like 100,000 H100 equivalents, you know, that's,

03:50.720 --> 03:54.960
you know, costs in the billions, you know, play it forward, you know, two more years, 2026,

03:54.960 --> 03:59.440
that's a cluster, that's a gigawatt, you know, that's sort of a large nuclear reactor size,

03:59.440 --> 04:02.560
it's like the power of the Hoover Dam, you know, that costs tens of billions of dollars,

04:02.560 --> 04:06.320
that's like a million H100 equivalents, you know, 2028, that's a cluster, that's 10 gigawatts,

04:06.320 --> 04:11.600
right? That's more power than kind of like most U.S. states. That's, you know, like 10 million H100

04:11.600 --> 04:17.680
equivalents, you know, costs hundreds of billions of dollars. And then 2030, trillion-dollar cluster,

04:18.400 --> 04:23.680
100 gigawatts, over 20% of U.S. electricity production, you know, 100 million H100 equivalents.

04:24.320 --> 04:27.840
And that's just the training cluster, right? That's like the one largest training cluster,

04:27.840 --> 04:30.400
you know, and then there's more inference GPUs as well, right? Most of, you know, once there's

04:30.400 --> 04:36.000
products, most of them are going to be inference GPUs. And so, you know, U.S. power

04:36.000 --> 04:40.960
production has barely grown for like, you know, decades, and now we're really in for a ride.

04:40.960 --> 04:47.040
So, I mean, when I had Zuck on the podcast, he was claiming, not a plateau, per se, but

04:47.920 --> 04:52.400
that AI progress would be bottlenecked by specifically this constraint on energy,

04:52.400 --> 04:56.880
and specifically like, oh, gigawatt data centers are going to build another three gorgeous dam or

04:56.880 --> 05:02.320
something. I know that there's companies, according to public reports, who are planning

05:02.320 --> 05:07.120
things on the scale of a gigawatt data center. 10 gigawatt data center, who's going to be able

05:07.120 --> 05:11.200
to build that? I mean, the 100 gigawatt center, like a state, where are you getting,

05:11.200 --> 05:14.640
are you going to pump that into one physical data center? How is this going to be possible?

05:15.280 --> 05:18.400
Yeah, you know. What is Zuck missing? I mean, you know, I don't know. I think to 10 gigawatt,

05:18.400 --> 05:21.520
you know, like six months ago, you know, 10 gigawatt was the taco town. I mean, I think,

05:22.080 --> 05:25.040
I feel like now, you know, people have moved on, you know, 10 gigawatt is happening. I mean,

05:25.040 --> 05:29.760
I know there's the information report on OpenAI and Microsoft planning a $100 billion

05:30.400 --> 05:34.560
cluster. Is that the gigawatt or is that the 10 gigawatt? I mean, I don't know. But, you know,

05:34.560 --> 05:38.320
if you try to like map out how expensive would the 10 gigawatt cluster be, you know, that's

05:38.320 --> 05:42.880
maybe a couple hundred billion. So it's sort of on that scale. And they're planning it. They're

05:42.880 --> 05:49.760
working on it. You know, so it's not just sort of my crazy take. I mean, AMD, AMD, I think,

05:49.760 --> 05:54.640
forecasted a $400 billion AI accelerator market by 27. You know, I think it's, you know, and AI

05:54.640 --> 05:59.120
accelerators are only part of the expenditures. It's sort of, you know, I think sort of a trillion

05:59.120 --> 06:03.280
dollars of sort of like total AI investment by 2027 is sort of like, we're very much in track

06:03.280 --> 06:06.800
on it. I think the trillion dollar cluster is going to take a bit more sort of acceleration.

06:06.800 --> 06:11.360
But, you know, we saw how much sort of chat GPT unleashed, right? And so like every generation,

06:11.360 --> 06:13.760
you know, the models are going to be kind of crazy and people, it's going to shift the

06:13.760 --> 06:17.600
overtune window. And then, and then, you know, obviously the revenue comes in, right? So these

06:17.600 --> 06:21.120
are forward looking investments. The question is, do they pay off? Right? And so if we sort of

06:21.120 --> 06:26.560
estimated the, you know, the GPT four cluster at around 500 million, by the way, that's, that's

06:26.560 --> 06:29.360
sort of a common mistake people make is they say, you know, people say like a hundred million

06:29.360 --> 06:33.120
dollars before, but that's just the rental price, right? They're like, ah, you rent the cluster

06:33.120 --> 06:35.840
for three months. But it's, you know, if you're building the biggest cluster, you got to like,

06:35.840 --> 06:38.400
you got to build the whole cluster, you got to pay for the whole cluster, you can't just rent it

06:38.400 --> 06:41.520
for three. But I mean, the really, you know, once, once you're trying to get into the sort of

06:41.520 --> 06:44.320
hundreds of billions, eventually you got to get to like a hundred billion a year. I mean,

06:44.320 --> 06:47.120
I think this is where it gets really interesting for the big tech companies, right? Because like,

06:47.120 --> 06:51.200
their revenues are on order, you know, hundreds of billions, right? So it's like 10 billion fine,

06:51.200 --> 06:56.160
you know, and it'll pay off the, you know, 2024 size training cluster. But, you know, really

06:56.160 --> 06:59.600
when sort of big tech, it'll be gangbusters is a hundred billion a year. And so the question is

06:59.600 --> 07:04.880
sort of how feasible is a hundred billion a year from AI revenue? And, you know, it's a lot more

07:04.880 --> 07:08.800
than right now. But I think, you know, if you sort of believe in the trajectory of the AI systems,

07:08.800 --> 07:13.600
as I do, which we'll probably talk about, it's not that crazy, right? So there's, I think there's

07:13.600 --> 07:18.640
like 300 million, you know, ish Microsoft office subscribers, right? And so they have co-pilot

07:18.640 --> 07:22.160
now and I don't know what they're selling it for. But, you know, suppose you sold some sort of AI

07:22.160 --> 07:26.560
add-on for a hundred bucks a month, and you sold that to, you know, a third of Microsoft office

07:26.560 --> 07:29.680
subscribers subscribe to that. That'd be a hundred billion right there. You know, a hundred dollars

07:29.680 --> 07:32.880
a month is, you know, a lot. That's a lot. Yeah. It's a lot. It's a lot. For a third of office

07:32.880 --> 07:35.760
subscribers. Yeah. But it's, but it's, you know, for the average dollars worker, it's like a few

07:35.760 --> 07:39.360
hours of productivity a month. And it's, you know, kind of like, you have to be expecting pretty lame

07:39.360 --> 07:44.000
AI progress to not hit like, you know, some few hours of productivity a month of, of, of, of, yeah.

07:44.000 --> 07:49.040
Okay, sure. So let's assume all this. Yeah. What happens in the next few years in terms of

07:50.000 --> 07:55.440
what is the one gigawatt training, the AI that's trained on the one gigawatt data center? What

07:55.440 --> 07:58.960
can it do with the one on the 10 gigawatt data center? Just map out the next few years of AI

07:58.960 --> 08:03.440
progress for me. Yeah. I think probably the sort of 10 gigawatt range is sort of my best guess

08:03.440 --> 08:06.800
for when you get the sort of true AGI. I mean, yeah, I think it's sort of like one gigawatt

08:06.800 --> 08:09.760
data center. And again, I think actually compute is overrated and we're going to talk about that.

08:09.760 --> 08:13.600
But what we will talk about compute right now. So, you know, I think so 25, 26, we're going to get

08:13.600 --> 08:19.520
models that are, you know, basically smarter than most college graduates. I think sort of the

08:19.520 --> 08:22.400
practice, a lot of the economic usefulness, I think really depends on sort of, you know,

08:22.400 --> 08:25.680
sort of on hobbling. Basically, it's, you know, the models are kind of, you know,

08:25.680 --> 08:28.640
they're smart, but they're limited, right? They're, you know, there's chat bot, you know,

08:28.640 --> 08:31.520
and things like being able to use a computer, things like being able to do kind of like a

08:31.520 --> 08:37.440
genetic long horizon tasks. Yeah. And then I think by 27, 28, you know, if you extrapolate the trends

08:37.440 --> 08:40.800
and, you know, we'll talk about that more later. And I talked about in the series, I think we hit,

08:40.800 --> 08:44.240
you know, basically, you know, like as smart as the smartest experts, I think the on hobbling

08:44.240 --> 08:49.600
trajectory kind of points to, you know, looks much more like an agent than a chat bot and much

08:49.600 --> 08:53.200
more almost like basically a drop in remote worker, right? So it's not like, I think basically, I

08:53.200 --> 08:56.320
mean, I think this is the sort of question on the economic returns. I think a lot of the,

08:56.320 --> 08:59.920
a lot of the intermediate AI systems could be really useful, but, you know, it actually just

08:59.920 --> 09:03.920
takes a lot of slap to integrate them, right? Like GVD4, you know, whatever, 4.5, you know,

09:03.920 --> 09:06.880
probably there's a lot you could do with them in a business use case. But, you know, you really

09:06.880 --> 09:10.320
got to change your workflows to make them useful. And it's just like, there's a lot of, you know,

09:10.320 --> 09:14.000
it's a very Tyler Cowan-esque take. It takes a long time to diffuse. You know, it's like, you

09:14.000 --> 09:20.640
know, we're an SF and so we missed that or whatever. But I think in some sense, you know,

09:20.640 --> 09:24.880
the way a lot of these systems won't be integrated is, is you kind of get this sort of sonic boom

09:24.880 --> 09:29.040
where it's, you know, the sort of intermediate systems could have done it, but it would have taken

09:29.040 --> 09:33.040
slap. And before you do the slap to integrate them, you get much more powerful systems, much

09:33.040 --> 09:36.480
more powerful systems that are sort of on hobbled. And so they're this agent and there's drop in

09:36.560 --> 09:40.400
remote worker. And, you know, and then you're kind of interacting with them like a coworker,

09:40.400 --> 09:44.480
right? You know, you can take do zoom calls with them and you're slacking them. And you're like,

09:44.480 --> 09:48.000
ah, can you do this project? And then they go off and they, you know, go away for a week and write

09:48.000 --> 09:52.960
a first draft and get feedback on them and, you know, run tests on their code. And then they come

09:52.960 --> 09:57.440
back and then you see it and you tell them a little bit more things or, you know, and that'll

09:57.440 --> 10:02.480
be much easier to integrate. And so, you know, it might be that actually you need a bit of overkill

10:02.480 --> 10:05.280
to make the sort of transition easy and to really harvest the games.

10:05.280 --> 10:07.920
What do you mean by the overkill? Overkill on the model capabilities?

10:07.920 --> 10:11.120
Yeah. Yeah. So basically intermediate models could do it, but it would take a lot of slap.

10:11.120 --> 10:14.400
I see. And so then, you know, the like, actually it's just the drop in remote worker kind of AGI

10:14.400 --> 10:17.680
that can automate, you know, cognitive tasks that actually just ends up kind of like,

10:17.680 --> 10:20.640
you know, basically it's your like, you know, the intermediate models would have made the

10:20.640 --> 10:24.320
software engineer more productive, but, you know, will the software engineer adopted? And then the,

10:24.320 --> 10:28.400
you know, 27 model is, well, you know, you just don't need the software engineer. You can literally

10:28.400 --> 10:31.280
interact with it like a software engineer and it'll do the work of a software engineer.

10:32.000 --> 10:37.920
So the last episode I did was with John Shulman. Yeah. And I was asking about basically this and

10:37.920 --> 10:42.400
one of the questions I asked is, we have these models that have been coming out in the last year

10:42.400 --> 10:47.200
and none of them seem to have significantly surpassed GPT-4 and certainly not in the

10:47.200 --> 10:51.680
agentic way in which they are interacting with as a co-worker, you know, the brag that they

10:51.680 --> 10:56.880
got a few extra points on MMLU or something. And even GPT-4.0, it's cool that they can talk

10:56.880 --> 11:02.160
like Scarlett Johansson or something, but like... And honestly, I'm going to use that.

11:03.040 --> 11:07.920
Oh, I guess not anymore, not anymore. Okay, but the whole co-worker thing. So

11:08.800 --> 11:11.840
this is going to be a wrong question, but you can address it in any order. But

11:12.880 --> 11:17.120
the, it makes sense to me why they'd be good at answering questions. They have a bunch of

11:17.680 --> 11:22.240
data about how to complete Wikipedia text or whatever. Where is the equivalent training

11:22.240 --> 11:28.160
data that enables it to understand what's going on in the Zoom call? How does this connect with

11:28.160 --> 11:32.560
what they were talking about in the Slack? What is the cohesive project that they're going after

11:33.200 --> 11:36.880
based on all this context that I have? Where is that training data coming from?

11:37.680 --> 11:42.480
Yeah. So I think a really key question for sort of AI progress in the next few years is sort of

11:42.480 --> 11:46.800
how hard is it to do, sort of unlock the test time compute overhang? So, you know, right now,

11:46.800 --> 11:51.680
GPT-4.0 answers a question and, you know, it kind of can do a few hundred tokens of kind of chain

11:51.680 --> 11:54.880
of thought. And that's already a huge improvement, right? Sort of like, this is a big on hobbling

11:54.880 --> 12:00.000
before, you know, answer a math question, it's just shotgun. And, you know, if you try to kind of

12:00.000 --> 12:02.640
like answer a math question by saying the first thing that came to mind, you know, you wouldn't

12:02.640 --> 12:08.080
be very good. So, you know, GP-4 thinks for a few hundred tokens. And, you know, if I thought for a

12:08.080 --> 12:10.800
few hundred, you know, if I think at like a hundred tokens a minute, and I thought for a few minutes-

12:10.800 --> 12:12.240
You're thinking much more than a hundred tokens.

12:12.240 --> 12:16.960
I don't know. If I thought for like a hundred tokens a minute, you know, it's like what GP-4

12:16.960 --> 12:20.240
does, maybe it's like, you know, it's equivalent to me thinking for three minutes or whatever, right?

12:22.480 --> 12:26.960
You know, suppose GP-4 could think for millions of tokens, right? That's sort of plus four rooms,

12:26.960 --> 12:30.160
plus four days of magnitude on test time compute, just like on one problem.

12:30.960 --> 12:34.240
It can't do it right now. It kind of gets stuck, right? Like write some code, even if, you know,

12:34.240 --> 12:36.800
you can do a little bit of iterative debugging, but eventually just kind of like,

12:36.800 --> 12:40.480
it can't, it kind of gets stuck in something, it can't correct its errors and so on.

12:40.480 --> 12:45.040
And, you know, in a sense, there's this big overhang, right? And like other areas of ML,

12:45.040 --> 12:49.040
you know, there's this great paper on AlphaGo, right? Where you can trade off train time and

12:49.040 --> 12:52.320
test time compute. And if you can use, you know, four rooms, more test time compute, that's almost

12:52.320 --> 12:56.000
like, you know, a three and a half room bigger model. Just because, again, like you can, you know,

12:56.000 --> 13:00.560
if a hundred tokens a minute, a few million tokens, that's a few months of sort of working time.

13:00.560 --> 13:03.600
There's a lot more you can do in a few months of working time than, and then right now. So the

13:03.600 --> 13:09.360
question is, how hard is it to unlock that? And I think the, you know, the sort of short

13:09.360 --> 13:14.720
timelines AI world is if it's not that hard. And the reason it might not be that hard is that,

13:15.360 --> 13:18.960
you know, there's only really a few extra tokens you need to learn, right? You need to kind of

13:19.040 --> 13:23.040
learn error correction tokens, the tokens where you're like, ah, I think I made a mistake. Let me

13:23.040 --> 13:25.920
think about that again. You need to learn the kind of planning tokens. That's kind of like, I'm going

13:25.920 --> 13:30.080
to start by making a plan. Here's my plan of attack. And then I'm going to write a draft. And I'm

13:30.080 --> 13:33.040
going to like, now I'm going to critique my draft. I'm going to think about it. And so it's not,

13:33.040 --> 13:37.440
it's not things that models can do right now. But, you know, the question is how hard is that?

13:38.240 --> 13:41.040
And in some sense, also, you know, there's sort of two paths to agents, right? You know,

13:41.680 --> 13:44.720
when Cholto was on your podcast, you know, he talked about kind of scaling,

13:44.800 --> 13:49.280
leading to more nines of reliability. And so that's one path. I think the other path is

13:49.280 --> 13:53.760
this sort of like unhobbling path where you, it needs to learn this kind of like system to

13:53.760 --> 13:58.000
process. And if it can learn this sort of system to process, it can just use kind of millions of

13:58.000 --> 14:04.720
tokens and think for them and be cohesive and be coherent. You know, one analogy. So when you drive,

14:04.720 --> 14:09.120
here's an analogy, when you drive, right? Okay, you're driving. And, you know, most of the time,

14:09.120 --> 14:12.320
you're kind of an autopilot, right? You're just kind of driving and you're doing well. And then,

14:13.280 --> 14:17.040
but sometimes you hit like a weird construction zone or a weird intersection, you know, and then I

14:17.040 --> 14:20.320
sometimes like, you know, my passenger seat, my girlfriend, I'm kind of like, ah, be quiet for

14:20.320 --> 14:23.680
a moment. I need to like figure out what's going on, right? And that's sort of like, you know,

14:23.680 --> 14:27.520
you go from autopilot to like the system to is jumping in and you're thinking about how to do

14:27.520 --> 14:31.520
it. And so the scaling scaling is improving that system one autopilot. And I think it's sort of,

14:31.520 --> 14:36.000
it's the brute force way to get to kind of agents who just improve that system. But if you can get

14:36.000 --> 14:41.680
that system to working, then, you know, I think you could like quite quickly jump, you know,

14:41.680 --> 14:45.600
to sort of this like more identified, you know, test time, compute, overhang is unlocked.

14:46.400 --> 14:53.520
What's the reason to think that this is an easy win in the sense that, oh, you just get the,

14:53.520 --> 14:59.040
there's like some loss function that easily enables you to train it to enable the system to

14:59.040 --> 15:02.720
thinking. Yeah, there's not a lot of animals that have system to thinking, you know, it like took

15:02.720 --> 15:06.640
a long time for evolution to give us system to thinking. Yeah, pre-training it like, listen,

15:06.640 --> 15:10.880
I get it, you got like trillions of tokens of internet text, I get that like, yeah, you like

15:10.880 --> 15:15.200
match that and you get all these, all this free training capabilities. What's the reason

15:15.200 --> 15:20.000
to think that this is an easy and hobbling? Yeah, so, okay, a bunch of things. So

15:21.840 --> 15:26.320
first of all, free training is magical, right? And it's, and it's, and it gave us this huge

15:26.320 --> 15:31.440
advantage for, for, for models of general intelligence, because, you know, you could,

15:31.440 --> 15:34.800
you just predict the next token, but predicting the next token, I mean, it's sort of a common

15:34.800 --> 15:38.480
misconception. But what it does is lets this model learn these incredibly rich representations,

15:38.480 --> 15:41.120
right? Like these sort of representation learning properties are the magic of deep

15:41.120 --> 15:44.640
learning. You have these models, and instead of learning just kind of like, you know, whatever,

15:44.640 --> 15:47.520
statistical artifacts or whatever, it learns sort of these models of the world. You know,

15:47.520 --> 15:51.040
that's also why they can kind of like generalize, right? Because it learned the right representations.

15:52.320 --> 15:56.400
And so, you know, you train these models and you have this sort of like raw bundle of capabilities

15:56.400 --> 16:01.280
that's really useful. It's sort of this almost unformed raw mass. And sort of the unhobbling

16:01.280 --> 16:05.360
we've done over sort of like GP2 to GP4 was, was you kind of took this sort of like raw mass,

16:05.440 --> 16:08.880
and then you like RLHF it into a really good chat bot. And that was a huge win, right? Like,

16:08.880 --> 16:12.880
you know, going, going, you know, an RL, you know, in the original, I think it's truck GPT

16:12.880 --> 16:17.200
paper, you know, RLHF versus non RLHF model, it's like 100x model size win on sort of human

16:17.200 --> 16:21.040
preference rating, you know, it started to be able to do like simple chain of thought and so on.

16:21.040 --> 16:25.280
But you still have a disadvantage of all these kind of like raw capabilities. And I think there's

16:25.280 --> 16:28.880
still like a huge amount that you're not doing with them. And by the way, I think the sort of

16:28.880 --> 16:32.000
this pre training advantage is also sort of the difference to robotics, right? Where I think

16:32.000 --> 16:36.480
robotics, you know, you know, I think people used to say it was a hardware problem, but I think

16:36.480 --> 16:40.320
the hardware stuff is getting solved. But the thing we have right now is you don't have this

16:40.320 --> 16:44.160
huge advantage of being able to bootstrap yourself with pre training, you don't have all this sort

16:44.160 --> 16:47.840
of unsupervised learning you can do, you have to start right away with the sort of RL self play

16:47.840 --> 16:55.040
and so on. Alright, so now the question is why, you know, why might some of this unhobbling and

16:55.040 --> 17:01.200
RL and so on work? And again, there's sort of this advantage of bootstrapping, right? So I, you

17:01.280 --> 17:04.960
know, your Twitter bio is being pre trained, right? You're actually not being pre trained

17:04.960 --> 17:08.640
anymore. You're not being pre trained anymore. You are pre trained in like grade school and high

17:08.640 --> 17:13.840
school. At some point you transition to be able being able to like learn by yourself. Right?

17:14.720 --> 17:19.040
You weren't able to do that in elementary school. I don't know middle school probably high school

17:19.040 --> 17:24.480
maybe when sort of started some guidance. You know, college, you know, you're smart, you can kind

17:24.480 --> 17:28.640
of teach yourself. And then sort of models are just starting to enter that regime. Right? And so

17:28.720 --> 17:32.320
it's sort of like, it's a little bit probably a little bit more scaling. And then you got to

17:32.320 --> 17:37.760
figure out what goes on top and it won't be trivial, right? So a lot of a lot of deep learning is

17:37.760 --> 17:42.000
sort of like, you know, it sort of seems very obvious in retrospect. And there's sort of this

17:42.000 --> 17:46.080
some obvious cluster of ideas, right? There's sort of some kind of like thing that seems a little

17:46.080 --> 17:49.120
dumb, but there's kind of works, but there's a lot of details you have to get right. So I'm not

17:49.120 --> 17:51.600
saying this, you know, we're going to get this, you know, next month or whatever, I think it's

17:51.600 --> 17:55.200
going to take a while to like really figure out a while for you is like half a year or something.

17:55.920 --> 18:02.000
I don't know. I think it's between six months and three years, you know. But you know, I think

18:02.000 --> 18:06.640
it's possible. And I think there's, you know, I think, and this is, I think it's also very related

18:06.640 --> 18:10.960
to the sort of issue of the data wall. But I mean, I think the, you know, one intuition on the sort

18:10.960 --> 18:15.760
of like learning, learning, learning by yourself, right, is sort of pre-training is kind of the

18:15.760 --> 18:20.560
words are flying by, right? You know, and, and, or it's like, you know, the teacher is lecturing

18:20.560 --> 18:23.920
to you. And the models, you know, the words are flying by, you know, they're taking, they're

18:23.920 --> 18:28.640
just getting a little bit from it. But that's sort of not what you do when you learn from yourself,

18:28.640 --> 18:32.320
right? When you learn by yourself, you know, so you're reading a dense math textbook,

18:32.320 --> 18:34.720
you're not just kind of like skimming through it once, you know, you wouldn't learn that much

18:34.720 --> 18:38.560
from it. I mean, some word cells just give them through reading, reread and reread the math textbook,

18:38.560 --> 18:42.480
and then they memorize, you know, like, you just repeated the data, then they memorize.

18:42.480 --> 18:45.760
What you do is you kind of like, you read a page, kind of think about it, you have some internal

18:45.760 --> 18:50.480
monologue going on, you have a conversational study buddy, you try a practice problem, you know,

18:50.480 --> 18:54.240
you fail a bunch of times. At some point, it clicks, and you're like, this made sense,

18:54.240 --> 18:58.400
then you read a few more pages. And so we've kind of bootstrapped our way to being, being able to

18:58.400 --> 19:03.600
do that now with models, or like just starting to be able to do that. And then the question is,

19:03.600 --> 19:08.640
you know, being able to like, read it, think about it, you know, try problems. And the question is,

19:08.640 --> 19:12.000
can you, you know, all this sort of self place, synthetic data, RL is kind of like making that

19:12.000 --> 19:18.480
thing work. So basically translate, translate translating like in context, like right now,

19:18.480 --> 19:22.160
there's like in context learning, right, super sample efficient. There's that, you know, in the

19:22.160 --> 19:27.120
Gemini paper, right, it just like learns a language in context. And then you're pre training, not at

19:27.120 --> 19:33.040
all sample efficient. But you know, what humans do is they kind of like, they do in context learning,

19:33.040 --> 19:36.400
you read a book, you think about it until eventually it clicks. But then you somehow

19:36.400 --> 19:40.880
distill that back into the weights. And in some sense, that's sort of like what RL is trying to

19:40.880 --> 19:46.560
do. And like when RL is super finicky, but when RL works, RL is kind of magical, because it's sort

19:46.640 --> 19:51.040
of the best possible data for the model. It's like when you try a practice problem, and you know,

19:51.040 --> 19:55.280
and then you fail, and at some point you kind of figure it out in a way that makes sense to you,

19:55.280 --> 19:58.400
that's sort of like the best possible data for you, because like the way you would have solved the

19:58.400 --> 20:03.680
problem. And that's sort of, that's what RL is. Rather than just, you know, you kind of read how

20:03.680 --> 20:07.680
somebody else solved the problem and doesn't, you know, initially click. Yeah, by the way, if that

20:07.680 --> 20:12.000
takes sounds familiar, because it was like part of the question I asked on showman, that goes to

20:12.000 --> 20:15.920
illustrate the thing I said in the intro, where like a bunch of the things I've learned about AI,

20:16.000 --> 20:21.840
just like, we do these dinners before the interviews, and I'm like, oh, what should I ask

20:21.840 --> 20:28.880
on showman? What should I ask Dario? Okay, suppose this is the way things go, and we get these

20:28.880 --> 20:33.600
in hobblings. Yeah. And the scaling, right? So it's like, you have this baseline, just enormous

20:33.600 --> 20:38.320
force of scaling, right? Where it's like GP2 to GP4, you know, GP2, it could kind of like, it was

20:38.320 --> 20:42.160
amazing, right? It could string together plausible senses. But you know, it could, it could barely

20:42.160 --> 20:46.960
do anything. It's kind of like preschooler. And then GP4 is, you know, it's writing code, it like,

20:46.960 --> 20:50.320
you know, can do hard math. And so it's sort of like smart high school. And so this big jump,

20:50.320 --> 20:53.120
and you know, in sort of the essay series, I go through and kind of count the order's magnitude

20:53.120 --> 20:58.560
of compute scale up with algorithmic progress. And so sort of scaling alone, you know, sort of by

20:58.560 --> 21:05.040
2728 is going to do another kind of preschool to high school jump on top of GP4. And so that'll

21:05.040 --> 21:09.200
already be just like at a per token level, just incredibly smart, they'll get you some more reliability.

21:09.200 --> 21:12.480
And then you add these on hobblings that make it look much less like a chat bot, more like this

21:12.480 --> 21:18.240
agent, like a drop in remote worker. And, you know, that's when things really get gone.

21:18.240 --> 21:23.840
Okay, yeah. I want to ask you more questions about this. I think, yeah, let's zoom out. Okay,

21:23.840 --> 21:30.720
so suppose you're right about this. Yeah. And I guess you this is because of the 2027 cluster,

21:30.720 --> 21:36.400
we've got 10 gigawatt, 2027 10 gigawatt something 28 is the 10 gigawatt. Okay, so you'll be pulled

21:36.480 --> 21:43.200
for it. And so I guess that's like 5.5 level by 2027, like whatever that's called, right?

21:44.240 --> 21:48.640
What does the world look like at that point? You have these remote workers who can replace people.

21:49.600 --> 21:53.680
What is the reaction to that in terms of the economy, politics, geopolitics?

21:54.960 --> 22:01.280
Yeah, so, you know, I think 2023 was kind of a really interesting year to experience as somebody

22:01.280 --> 22:04.720
who is like, you know, really following the ice stuff where, you know, before that,

22:04.880 --> 22:13.680
what were you doing in 2023? I mean, open AI. And, and, and, you know, kind of went, you know,

22:13.680 --> 22:16.960
I mean, I was, I was been thinking about this and, you know, like talking to a lot of people,

22:16.960 --> 22:19.440
you know, in the years before, and it was this kind of weird thing, you know, you almost didn't

22:19.440 --> 22:23.520
want to talk about AI or AGI, you know, it's kind of a dirty word, right? And then 2023, you know,

22:23.520 --> 22:27.840
people saw chat, GPT for the first time in such a before, and it just like exploded, right? It

22:27.840 --> 22:32.000
triggered this kind of like, you know, you know, a huge sort of capital expenditures from all these

22:32.000 --> 22:38.640
firms and, and, and, and, you know, the explosion of revenue from NVIDIA and so on. And, you know,

22:38.640 --> 22:41.680
things have been quiet since then. But, you know, the next thing has been in the oven. And I sort

22:41.680 --> 22:45.760
of expect sort of every generation, these kind of like G forces to intensify, right? It's like,

22:45.760 --> 22:50.400
people see the models. There's like, you know, people haven't counted them. So they're going to

22:50.400 --> 22:53.920
be surprised. And it'll be kind of crazy. And then, you know, revenue is going to accelerate,

22:53.920 --> 22:57.120
you know, suppose you do hit the 10 billion, you know, end of this year, suppose it like just

22:57.120 --> 23:00.320
continues on this sort of doubling trajectory of, you know, like every six months of revenue

23:00.400 --> 23:03.600
doubling, you know, it's like, you're not actually that far from 100 billion, you know,

23:03.600 --> 23:06.960
maybe that's like 26. And so, you know, at some point, you know, like, you know,

23:06.960 --> 23:09.680
sort of what happened to NVIDIA is going to happen to big tech, you know, like their stocks,

23:09.680 --> 23:15.840
their, you know, that's going to explode. And I mean, I think a lot more people are going to feel

23:15.840 --> 23:21.760
it, right? I mean, I think the, I think 2023 was the sort of moment for me where it went from

23:21.760 --> 23:25.360
kind of AGI is a sort of theoretical abstract thing, and you'd make the models to like,

23:25.360 --> 23:29.360
I see it, I feel it. And like, I see the path, I see where it's going. I like,

23:30.000 --> 23:33.280
I think I can see the cluster where it's strained on, like the rough combination of algorithms,

23:33.280 --> 23:36.400
the people, like how it's happening. And I think, you know, most of the world is not,

23:36.400 --> 23:40.640
you know, most of the people feel it are like right here, right? But, but, you know, I think a

23:40.640 --> 23:46.960
lot more of the world is going to start feeling it. And I think that's going to start being kind of

23:46.960 --> 23:51.760
intense. Okay. So, right now, who feels that you can, you go on Twitter and there's these

23:51.760 --> 23:55.040
GPT wrapper companies like, whoa, GPT Floreau is going to change our business.

23:55.040 --> 23:57.760
I mean, I'm so, so bearish on the wrapper companies, right? Because like, they're the ones

23:57.760 --> 24:00.560
that are going to be like, the wrapper companies are betting on stagnation, right? The wrapper

24:00.560 --> 24:03.280
companies are betting like, you have these intermediate models and take so much left

24:03.280 --> 24:06.160
to integrate them. And I'm kind of like, I'm really bearish because I'm like,

24:06.160 --> 24:08.800
we're just going to sonic boom you, you know, and we're going to get the unhauled ones, we're

24:08.800 --> 24:12.000
going to get the drop in remote worker. And then, you know, your stuff is not going to matter.

24:12.000 --> 24:19.520
Okay. Sure. Sure. So that's done. Now, who, so the SF is paying attention now, or this crowd

24:19.520 --> 24:25.200
here is paying attention. Who is going to be paying attention in 2026, 2027? And, but,

24:25.200 --> 24:27.840
presumably this is, these are years in which the hundreds of billions of CapEx is being

24:27.840 --> 24:32.000
spent on the eye. I mean, I think the, the national security state is going to be starting

24:32.000 --> 24:36.400
to pay a lot of attention. And I, you know, I hope we get to talk about that.

24:36.400 --> 24:40.080
Okay. Let's talk about it now. What happens? Yeah. Like, well, what is the sort of political

24:40.080 --> 24:43.840
reaction immediately? Yeah. And even like internationally, like what people see like

24:43.840 --> 24:47.040
right now, I don't know if like Xi Jinping like reads the news and sees like,

24:47.120 --> 24:51.280
yeah, I don't know. Oh my God, like MMLU score on that. What are you doing about this comrade?

24:54.320 --> 24:58.240
So what happens when the, like what, what are the, he's like sees a remote replacement and it

24:58.240 --> 25:01.120
has a hundred billion dollars in revenue. There's a lot of businesses that have a hundred billion

25:01.120 --> 25:04.720
dollars in revenue and people don't like aren't staying up all night talking about it.

25:05.680 --> 25:10.160
The question, I think the question is when, when does the CCP and when does the sort of

25:10.160 --> 25:15.040
American national security establishment realize that superintelligence is going to be

25:15.040 --> 25:18.320
absolutely decisive for national power, right? And this is where, you know, the sort of intelligence

25:18.320 --> 25:21.280
explosion stuff comes in, which, you know, we should also talk about later, you know, it's sort

25:21.280 --> 25:24.560
of like, you know, you have AGI, you have this sort of drop in remote worker that can replace,

25:24.560 --> 25:27.920
you know, you or me, at least that sort of remote jobs, you know, cognitive jobs.

25:29.600 --> 25:34.960
And then, you know, I think fairly quickly, you know, I mean, by default, you know, you

25:34.960 --> 25:37.520
turn the crank, you know, one or two more times, you know, and then you get a thing that's

25:37.520 --> 25:40.720
smarter than humans. But I think even, even more than just turning the cramp a few more times,

25:41.600 --> 25:47.040
you know, I think one of the first jobs to be automated is going to be that of sort of an AI

25:47.040 --> 25:52.400
researcher engineer. And if you can automate AI research, you know, I think things can start

25:52.400 --> 25:56.640
going very fast. You know, right now, there's already this trend of, you know, half in order of

25:56.640 --> 26:00.240
magnitude a year of algorithmic progress, you know, suppose, you know, at this point, you're

26:00.240 --> 26:04.880
going to have GPU fleets in the tens of millions for inference, you know, or more. And you're going

26:04.880 --> 26:09.840
to be able to run like 100 million human human equivalents of these sort of automated AI researchers.

26:09.920 --> 26:14.000
And if you can do that, you know, you can maybe do, you know, a decade's worth of sort of ML

26:14.000 --> 26:20.080
research progress in a year, you know, get the some sort of 10x speed up. And if you can do that,

26:20.080 --> 26:24.720
I think you can make the jump to kind of like AI that is vastly smarter than humans, you know,

26:24.720 --> 26:29.040
within a year, a couple years. And then, you know, that broadens, right? So you have this,

26:29.040 --> 26:33.280
you have this sort of initial acceleration of AI research that broadens to like you apply R&D to

26:33.280 --> 26:38.320
a bunch of other fields of technology. And the sort of like extremes, you know, at this point,

26:38.320 --> 26:42.560
you have like a billion, just super intelligent researchers, engineers, technicians, everything,

26:42.560 --> 26:46.080
you're superbly competent, all the things, you know, they're going to figure out robotics.

26:46.080 --> 26:49.280
Or we talked about it being a software problem. Well, you know, you have, you have a billion of

26:50.320 --> 26:53.680
super smart, smarter than the smartest human researchers, AI researchers on your cluster,

26:53.680 --> 26:56.320
you know, at some point during the intelligence explosion, they're going to be able to figure

26:56.320 --> 27:02.320
out robotics, you know, and then again, that expands. And, you know, I think if you play this

27:02.320 --> 27:10.880
picture forward, I think it is fairly unlike any other technology in that it will, I think,

27:10.880 --> 27:16.160
you know, a couple years of lead could be utterly decisive in say like military competition, right?

27:16.160 --> 27:19.760
You know, if you look at like go for one, right? Go for one, you know, like the Western coalition

27:19.760 --> 27:23.520
forces, you know, they had, you know, like a hundred to one kill ratio, right? And that was like,

27:23.520 --> 27:27.040
they had better sensors on their tanks, you know, and they had, they had better, you know, more

27:27.040 --> 27:31.600
precision precision missiles, right? Like GPS, and they had, you know, stealth, and they had sort

27:31.680 --> 27:36.400
of a few, you know, maybe 20, 30 years of technological lead, right? And they, you know,

27:36.400 --> 27:42.800
just completely crushed them. Super intelligence applied to sort of broad fields of R&D. And

27:42.800 --> 27:45.760
then, you know, the sort of industrial explosion as well, you have the robots, you're just making lots

27:45.760 --> 27:49.920
of material, you know, I think that could compress, I mean, basically compress kind of like a century

27:49.920 --> 27:53.840
worth of technological progress since the last decade. And that means that, you know, a couple

27:53.840 --> 27:58.720
years could mean a sort of go for one style, like, you know, advantage in military affairs.

27:58.960 --> 28:05.120
And, you know, including, like, you know, a decisive advantage that even like preempts

28:05.120 --> 28:08.320
nukes, right? Suppose, like, you know, how do you find the stealth and nuclear submarines?

28:08.320 --> 28:11.200
Like right now, that's a problem of like, you have sensors, you have the software,

28:11.200 --> 28:14.720
like tech where they are, you know, you can do that, you can find them, you have kind of like

28:14.720 --> 28:18.160
millions or billions of like mosquito-like, you know, size drones, and that, you know,

28:18.160 --> 28:22.080
they take out the nuclear submarines, they take out the mobile launchers, they take out the other

28:22.080 --> 28:27.440
nukes. And anyway, so I think enormously destabilizing, enormously important for national power,

28:28.880 --> 28:33.520
and at some point, I think people are going to realize that, not yet, but they will. And when

28:33.520 --> 28:39.760
they will, I think there will be sort of, you know, I don't think it'll just be the sort of AI

28:39.760 --> 28:44.480
researchers in charge. And, you know, I think on the, you know, the CCP is going to, you know,

28:44.480 --> 28:48.000
have sort of an all-out effort to like infiltrate American AI labs, right? You know, like billions

28:48.000 --> 28:51.600
of dollars, thousands of people, you know, full force of the sort of, you know, Ministry of State

28:51.600 --> 28:55.120
Security. CCP is going to try to, you know, like outbuild us, right? Like they, you know, their,

28:55.120 --> 28:59.520
you know, power in China, you know, like the electric grid, you know, they added a U.S. is,

28:59.520 --> 29:03.440
you know, a complete, like they added as much power in the last decade as like sort of entire

29:03.440 --> 29:06.880
U.S. electric grid. So like the 100 gigawatts cluster, at least the 100 gigawatts is going

29:06.880 --> 29:11.120
to be a lot easier for them to get. And so I think sort of, you know, by this point, I think it's

29:11.120 --> 29:16.720
going to be like an extremely intense sort of international competition. Okay, so in this picture,

29:18.800 --> 29:23.840
one thing I'm uncertain about is whether it's more like what you say, where it's more of an

29:23.840 --> 29:31.680
implosion of you have developed an AGI and then you make it into an AI researcher. And for a while,

29:31.680 --> 29:37.760
a year or something, you're only using this ability to make hundreds of millions of other AI

29:37.760 --> 29:43.600
researchers. And then like the thing that comes out of this really frenetic process is a super

29:43.600 --> 29:47.920
intelligence. And then that goes out in the world and is developing robotics and helping you take

29:47.920 --> 29:50.800
over other countries and whatever. It's a little bit more, you know, it's a little bit more kind

29:50.800 --> 29:53.600
of like, you know, it's not like, you know, on and off, it's a little bit more gradual, but it's

29:53.600 --> 29:57.200
sort of like it's an explosion that starts narrowly. It's can do cognitive jobs, you know, the highest

29:57.200 --> 30:02.080
RI use for cognitive jobs is make the AI better, like solve robotics, you know, and as as as you

30:02.080 --> 30:05.680
know, you solve robotics, now you can do R&D and, you know, like biology and other technology.

30:06.560 --> 30:09.680
You know, initially you start with the factory workers, you know, they're wearing the glasses

30:09.680 --> 30:12.800
and the the AirPods, you know, and the AI is instructing them, right? Because, you know,

30:12.800 --> 30:16.160
you kind of make any worker into a skilled technician, and then you have the robots come in.

30:16.160 --> 30:20.960
And anyway, so it sort of expands, this process expands. Metas revans are a compliment to their

30:20.960 --> 30:24.800
llama. Well, you know, whatever, like, you know, the fabs in the US, the constrained skilled workers,

30:24.800 --> 30:27.840
right? You have, you have, even if you don't have robots that you have the cognitive super

30:27.840 --> 30:30.640
intelligence and, you know, it can kind of make them all into skilled workers immediately. But

30:30.640 --> 30:34.160
that's, you know, it's a very brief period, you know, robots will come soon. Sure. Okay. Okay, so

30:34.160 --> 30:39.680
suppose this is actually how the tech progresses in the United States, maybe because these companies

30:39.680 --> 30:43.360
are already experiencing hundreds of billions of dollars of revenue. At this point, you know,

30:43.360 --> 30:46.640
companies are barring, you know, hundreds of billions of more in the corporate debt markets,

30:46.640 --> 30:51.200
you know. But why is a CCP bureaucrat, some 60 year old guy, he looks at this and he's like,

30:51.200 --> 30:56.480
oh, it's like, co-pilot has gotten better now. Why are they now? I mean, this is much more than

30:56.480 --> 31:03.120
co-pilot has gotten better now. I mean, to them, like, yeah, because to shift the production of

31:03.120 --> 31:10.320
an entire country, to dislocate energy that is otherwise being used for consumer goods or

31:10.320 --> 31:18.160
something and to make it that all feed into the data centers. What part of this whole story is

31:18.160 --> 31:23.280
you realize the super intelligence is coming soon, right? And I guess you realize it, maybe I realize

31:23.280 --> 31:27.600
it. I'm not sure how much I realize it, but will the will the national security apparatus in the

31:27.600 --> 31:31.920
United States and the CCP realize it? Yeah, I mean, look, I think in some sense, this is a really

31:31.920 --> 31:36.960
key question. I think we have sort of a few more years of mid game, basically, and where you have

31:36.960 --> 31:43.280
a few more 2023s, and that just starts updating more and more people. And I think the trend lines

31:43.280 --> 31:50.320
will become clear. I think you will see some amount of the sort of COVID dynamic, right?

31:50.320 --> 31:57.760
Like COVID was February of 2020. It honestly feels a lot like today, where it feels like this

31:57.760 --> 32:03.600
utterly crazy thing is about, is impending, is coming. You kind of see the exponential and yet

32:03.600 --> 32:07.040
most of the world just doesn't realize, right? The mayor of New York is like, go out to the shows,

32:07.040 --> 32:15.840
and this is just Asian racism or whatever. But at some point, the exponential, at some point,

32:15.840 --> 32:20.960
people saw it. And then just kind of crazy radical reactions came.

32:20.960 --> 32:24.720
Right. Okay, so by the way, what were you doing during COVID? February?

32:26.000 --> 32:27.280
Like freshman, sophomore, what?

32:28.080 --> 32:31.760
Junior? But still, like, what were you, like 17-year-old junior or something?

32:33.760 --> 32:37.120
And then, like, did you short the market or something?

32:37.120 --> 32:37.680
Yeah, yeah, yeah.

32:37.680 --> 32:40.400
Okay. Did you sell at the right time?

32:40.400 --> 32:40.800
Yeah.

32:40.800 --> 32:46.400
Okay. Yeah, so there will be like a March 2020 moment, the thing that was COVID, but here.

32:47.920 --> 32:52.240
Now, then you can make the analogy that you make in the series that this will then

32:53.200 --> 32:58.240
cause the reaction of like, we got to do the Manhattan Project for America here.

32:58.240 --> 33:02.720
I wonder what the politics of this will be like, because the difference here is,

33:02.720 --> 33:08.400
it's not just like, we need the bomb to beat the Nazis. It's, we're building this thing that's

33:08.400 --> 33:12.400
making all our entry prices rise a bunch, and it's automating a bunch of our jobs.

33:12.400 --> 33:15.360
And the climate change stuff, like people are going to be like, oh my god, it's making climate

33:15.360 --> 33:20.400
change worse. And it's helping big tech. Like, politically, this doesn't seem like a dynamic

33:20.960 --> 33:25.680
where the national security apparatus or the president is like, we have to step on the gas

33:25.680 --> 33:27.120
here and like, make sure America wins.

33:29.440 --> 33:32.800
Yeah. I mean, again, I think a lot of this really depends on sort of how much people

33:32.800 --> 33:34.560
are feeling it, how much people are seeing it.

33:38.080 --> 33:41.600
You know, I think there's a thing where, you know, kind of basically our generation, right?

33:41.600 --> 33:46.400
We're kind of so used to kind of basically peace and like, you know, the world, you know,

33:46.400 --> 33:53.040
American hegemony and nothing matters. But, you know, the sort of like extremely intense and

33:53.040 --> 33:57.600
these extraordinary things happening in the world and like intense international competition is

33:57.600 --> 34:01.840
like very much the historical norm. Like in some sense, it's like, you know, sort of this,

34:01.840 --> 34:07.520
there's this sort of 20 year very unique period, but like, you know, the history of the world is

34:07.520 --> 34:12.000
like, you know, you know, like in World War II, right, it was like 50% of GDP went to, you know,

34:12.080 --> 34:15.600
like, you know, war prodigy and production, you know, the US borrowed over 60% of GDP, you know,

34:15.600 --> 34:21.040
and in, you know, I think Germany and Japan over 100%, World War I, you know, UK, Japan,

34:21.040 --> 34:29.200
sorry, UK, France, Germany all borrowed over 100% of GDP. And, you know, I think the sort of,

34:30.400 --> 34:33.840
much more was on the line, right? Like, you know, and, you know, people talk about World War I

34:33.840 --> 34:38.480
being so destructive and you know, like 20 million Soviet soldiers dying and like 20% of Poland.

34:38.480 --> 34:41.040
But, you know, that was just the sort of like, that happened all the time, right? You know,

34:41.040 --> 34:45.680
like seven years war, you know, like whatever 20, 30% of Prussia died, you know, like 30 years war,

34:45.680 --> 34:50.160
you know, like, I think, like, you know, up to 50% of like large swath of Germany died.

34:52.400 --> 35:00.880
And, you know, I think the question is, will these sort of like, will people see that the

35:00.880 --> 35:04.480
stakes here are really, really high and that basically is sort of like history is actually back.

35:06.160 --> 35:09.120
And I think, you know, I think the American national security state thinks

35:09.840 --> 35:13.200
very seriously about stuff like this. They think very seriously about competition with China. I

35:13.200 --> 35:16.960
think China very much thinks of itself on this historical mission and your nation, the Chinese

35:16.960 --> 35:21.600
nation, a lot about national power, I think a lot about like the world order. And then, you know,

35:23.040 --> 35:27.040
I think there's a real question on timing, right? Like, do they, do they start taking this seriously,

35:27.040 --> 35:30.240
right? Like when the intelligence explosion is already happening, like quite late, or do they

35:30.240 --> 35:33.840
start taking this seriously, like two years earlier on that matters a lot for how things play out.

35:33.840 --> 35:37.840
But at some point, they will, and at some point, they will realize that this will be sort of

35:37.840 --> 35:44.800
utterly decisive for, you know, not just kind of like some proxy war somewhere, but, you know,

35:44.800 --> 35:48.800
like whether liberal democracy can continue to thrive, whether, you know, whether the CCP will

35:48.800 --> 35:54.480
continue existing. And I think that will activate sort of forces that we haven't seen in a long time.

35:56.080 --> 35:59.520
The great conflict, the great power conflict thing definitely seems compelling.

36:00.080 --> 36:04.400
I think just all kinds of different things seem much more likely when you think from a historical

36:04.480 --> 36:08.160
perspective, when you zoom out beyond the liberal democracy that we've been living in,

36:08.720 --> 36:14.400
had the pleasure to live in America, let's say 80 years, including dictatorships, including all

36:14.960 --> 36:19.520
obviously war, famine, whatever. I was reading the Gullig Archipelago, and one of the chapters

36:19.520 --> 36:24.560
begins with Sojenitsyn saying, if you would have told Russian citizens under the czars that because

36:24.560 --> 36:29.440
of all these new technologies, we wouldn't see some great Russian revival or becomes a great power,

36:29.440 --> 36:36.240
and the citizens are made wealthy. But instead, what you would see is tens of millions of Soviet

36:36.240 --> 36:42.080
citizens tortured by millions of beasts in the worst possible ways, and that this is what would

36:42.080 --> 36:47.360
be the result of the 20th century, they wouldn't have believed you, they'd have called you a slanderer.

36:47.360 --> 36:52.000
Yeah, and you know, the, you know, the possibilities for dictatorship with super

36:52.000 --> 36:56.000
intelligence are sort of even crazier, right? I think, you know, imagine you have a perfectly

36:56.080 --> 37:00.720
loyal military and security force, right? That's it. No more, no more rebellions, right? No more

37:00.720 --> 37:05.120
popular uprisings, you know, perfectly loyal, you know, you have, you know, perfect lie

37:05.120 --> 37:08.800
detection, you know, you have surveillance of everybody, you know, you can perfectly figure

37:08.800 --> 37:12.240
out who's the dissenter, weed them out, you know, no Gorbachev would have ever risen to power,

37:12.240 --> 37:16.320
who had some doubts about the system, you know, no military coup would have ever happened.

37:16.320 --> 37:19.920
And I think you, I mean, you know, I think there's a real way in which,

37:20.000 --> 37:27.840
you know, part of why things have worked out is that, you know, ideas can evolve and, you know,

37:27.840 --> 37:31.760
there's sort of like some, some sense in which sort of time heals a lot of wounds and time, you

37:31.760 --> 37:35.520
know, and solves, solves, you know, a lot of debates and a lot of people had really strong

37:35.520 --> 37:38.640
convictions, but you know, a lot of those have been overturned by time because there's been this

37:38.640 --> 37:42.000
continued pluralism and evolution. I think there's a way in which kind of like, you know,

37:42.000 --> 37:45.840
if you take a CCP like approach to kind of like truth, truth is what the party says,

37:45.840 --> 37:48.960
when you supercharge that with super intelligence, I think there's a way in which that could just

37:48.960 --> 37:53.600
be like locked in and trying for, you know, a long time. And I think the possibilities are pretty

37:53.600 --> 37:59.760
terrifying. You know, your point about, you know, history and sort of like living in America for

37:59.760 --> 38:04.320
the past eight years, you know, I think this is one of the things I sort of took away from growing

38:04.320 --> 38:08.160
up in Germany is a lot of the stuff feels more visceral, right? Like, you know, my mother grew

38:08.160 --> 38:11.760
up in the former East, my father in the former West, they like met shortly after the wall fell,

38:11.760 --> 38:15.440
right? Like the end of the Cold War was the sort of extremely pivotal moment for me because it's,

38:15.440 --> 38:18.960
you know, it's the reason I exist, right? And then, you know, growing up in Berlin and, you know,

38:19.600 --> 38:25.760
former wall, you know, my great grandmother, who is still alive, is very important in my life.

38:25.760 --> 38:30.160
You know, she was born in 34, you know, grew up, you know, during the Nazi era, during, you know,

38:30.160 --> 38:34.000
all that, you know, then World War II, you know, like South of the firebombing of Dresden from the

38:34.000 --> 38:38.240
sort of, you know, country cottage or whatever were, you know, the day as kids were, you know,

38:38.240 --> 38:42.160
then, and then, you know, then spends most of her life in sort of the East German Communistic

38:42.160 --> 38:46.080
leadership. You know, she'd tell me about, you know, in like 54 when there's like the popular

38:46.080 --> 38:50.080
uprising, you know, in Soviet tanks came in, you know, her husband was telling her to get home

38:50.080 --> 38:55.440
really quickly, you know, get off off the streets, you know, had a, had a son who, who tried to,

38:55.440 --> 38:59.600
you know, ride a motorcycle across, across the Iron Curtain and then was put in the Stasi prison

38:59.600 --> 39:05.920
for a while. You know, and then finally, you know, when she's almost 60, you know, it was the first

39:05.920 --> 39:14.080
time she lives in, you know, a free country and, and a wealthy country. And, you know, when I was

39:14.080 --> 39:18.640
a kid, she was, she, the thing she always really didn't want me to do was like get involved in

39:18.640 --> 39:23.360
politics because like joining a political party was just, you know, it was a very bad connotations

39:23.360 --> 39:30.240
for her. Anyway, and she sort of raised me when I was young, you know, and so it, you know, it

39:30.240 --> 39:35.280
doesn't feel that long ago. It feels very close. Yeah. So I wonder when we're talking today about

39:35.920 --> 39:41.520
the CCP, listen, the people in China who will be doing the pro, their version of the project

39:41.520 --> 39:48.880
will be AI researchers who are somewhat westernized, who interact with either got educated in the West

39:48.880 --> 39:58.080
or have colleagues in the West. Are they going to sign up for the, the CCP project that's going to

39:58.080 --> 40:03.600
hand over control to Xi Jinping? What's your sense on, I mean, you're just like fundamentally,

40:03.600 --> 40:06.640
they're just people, right? Like, can't you like convince them about the dangers of super

40:06.640 --> 40:11.200
intelligence? Will they be in charge though? I mean, since this is, I mean, this is also the case,

40:11.200 --> 40:16.400
you know, uh, you know, in the US or whatever, this is sort of like rapidly depreciating influence

40:16.400 --> 40:20.400
of the lab employees. Like right now, the sort of AI lab employees have so much power, right over

40:20.400 --> 40:24.240
this, you know, like, they're going to get automated and then you saw this November event, so much

40:24.240 --> 40:27.280
power, right? But both, I mean, both they're going to get automated and they're going to lose all

40:27.280 --> 40:30.640
their power. And it'll just be, you know, kind of like a few people in charge with their sort of

40:30.720 --> 40:36.080
armies of automated eyes. But also, you know, it's sort of like the politicians and the generals

40:36.080 --> 40:38.560
and the sort of national security state, you know, a lot, you know, it's, I mean, there's sort of,

40:38.560 --> 40:41.520
this is the sort of some of these classic scenes from the, you know, the Oppenheimer movies, you

40:41.520 --> 40:44.960
know, the scientists built it and then it was kind of, you know, and the bomb was shipped away and

40:44.960 --> 40:49.200
it was out of their hands. You know, I actually, yeah, I think, I actually think it's good for

40:49.200 --> 40:54.240
like lab employees to be aware of this is like, you have a lot of power now, but you know, maybe

40:54.240 --> 40:59.520
not for that long and, you know, use it wisely. Yeah, I do, I do think they would benefit from

40:59.520 --> 41:03.040
some more, you know, organs of representative democracy. What do you mean by that? Oh, I mean,

41:03.040 --> 41:06.400
I, you know, in the sort of the, in the open AI board events, you know, employee at power was

41:06.400 --> 41:10.240
exercising a very sort of direct democracy way. And I feel like that's how some of how that went

41:10.240 --> 41:13.520
about, you know, I think it really highlighted the benefits of representative democracy and having

41:13.520 --> 41:19.040
some deliverative organs. Interesting. Yeah. Well, let's go back to the 100 billion revenue,

41:19.040 --> 41:23.920
whatever, and so these companies, Nala cluster, yeah, the companies are deploying, we're trying

41:23.920 --> 41:28.080
to build clusters that are this big. Yeah. Where are they building it? Because if you say it's

41:28.080 --> 41:32.640
the amount of energy that would be required for a small or medium sized US state, is it then Colorado

41:32.640 --> 41:35.840
gets no power and it's happening in the United States or is it happening somewhere else? Oh,

41:35.840 --> 41:38.960
I mean, I think that, I mean, in some sense, this is the thing that I always find funny is, you know,

41:38.960 --> 41:42.080
you talk about Colorado gets no power, you know, the easy way to get the power would be like, you

41:42.080 --> 41:46.080
know, displaced, less economically useful stuff, you know, it's like, whatever, buy up the aluminum

41:46.080 --> 41:49.440
smelting plant and, you know, that has a gigalot and, you know, we're going to replace it with the

41:49.440 --> 41:52.880
data center because that's important. I mean, that's not actually happening because a lot of

41:52.880 --> 41:56.640
these power contracts are really sort of long-term locked in, you know, there's obviously people

41:56.720 --> 42:00.320
don't like things like this. And so it sort of, it seems like in practice, what it's, what it's

42:00.320 --> 42:04.160
requiring, at least right now is building new power. The, that might change. And I think that

42:04.160 --> 42:07.120
that's when things get really interesting when it's like, no, we're just dedicating all of the

42:07.120 --> 42:11.840
power to the AGI. Okay, so right now it's building new power, 10 gigawatt, I think quite doable.

42:12.640 --> 42:15.120
You know, it's like a few percent of like US natural gas production.

42:16.880 --> 42:19.680
You know, I mean, when you have the 10 gigawatt training cluster, you have a lot more in

42:19.680 --> 42:23.200
friends. So that starts getting more, you know, I think 100 gigawatt, that starts getting pretty

42:23.200 --> 42:26.880
wild. You know, that's, you know, again, it's like over 20% of US electricity production.

42:28.320 --> 42:31.920
I think it's pretty doable, especially if you're willing to go for like natural gas.

42:32.880 --> 42:37.040
I do, I do think, I do think it is incredibly important, incredibly important that these

42:37.040 --> 42:39.840
clusters are in the United States. And why does it matter? It's in the US?

42:42.640 --> 42:47.840
I mean, look, I think there's some people who are, you know, trying to build clusters elsewhere. And

42:47.840 --> 42:51.200
you know, there's like a lot of free flowing Middle Eastern money that's trying to build clusters

42:51.200 --> 42:56.640
elsewhere. I think this comes back to the sort of like national security question we talked about

42:56.640 --> 43:00.320
earlier. Like would you, I mean, would you do the Manhattan Project and the UAE, right? And I think,

43:00.320 --> 43:04.000
I think basically like putting, putting the clusters, you know, I think you can put them in the US,

43:04.000 --> 43:08.000
you can put them in sort of like ally democracies. But I think once you put them in kind of like

43:08.000 --> 43:11.520
dictatorships, authoritarian dictatorships, you kind of create this, you know, irreversible security

43:11.520 --> 43:16.400
risk, right? So I mean, one cluster is there, much easier for them to exfiltrate the weights.

43:16.400 --> 43:20.080
You know, they can like literally steal the AGI, the superintelligence. It's like they got a copy of

43:20.080 --> 43:24.240
the, you know, of the atomic bomb, you know, and they just got the direct replica of that.

43:24.240 --> 43:28.000
And it makes it much easier to them. I mean, we're ties to China, you can ship that to China.

43:28.000 --> 43:31.760
So that's a huge risk. Another thing is they can just seize the compute, right? Like maybe right

43:31.760 --> 43:34.720
now they just think of this, I mean, in general, I think people, you know, I think the issue here

43:34.720 --> 43:37.840
is people are thinking of this as they, you know, chat, GBT, big tech product clusters. But I think

43:37.840 --> 43:42.320
the cluster is being planned now, you know, three to five years out, like it will be the like AGI

43:42.320 --> 43:46.000
superintelligence clusters. And so anyway, so like when things get hot, you know, they might just

43:46.000 --> 43:50.640
seize the compute. And I don't know, suppose we put like, you know, 25% of the compute capacity

43:50.640 --> 43:54.240
in the sort of Middle Eastern decaderships, well, they seize that. And now it's sort of a ratio

43:54.240 --> 43:58.160
of compute of three to one, and you know, still have some more, but even like, even, even only,

43:58.160 --> 44:02.240
only 25% of compute there, like, I think it starts getting pretty hairy, you know, I think three to

44:02.240 --> 44:06.960
one is like, not that great of a ratio, you can do a lot with that amount of compute. And then look,

44:06.960 --> 44:09.760
even, even if they don't actually do this, right, even they don't actually seize the compute, even

44:09.760 --> 44:13.760
they actually don't steal the weights. There's just a lot of implicit leverage you get, right?

44:13.760 --> 44:20.880
They get, they get the seat at the AGI table. And, you know, I don't know why we're giving

44:20.880 --> 44:26.400
authoritarian dictatorships the seat at the AGI table. Okay, so there's going to be a lot of

44:26.400 --> 44:30.800
compute in the Middle East, if these deals go through. First of all, who's who is it? Just

44:30.800 --> 44:34.000
like every single big tech company is just trying to figure out where they're going to be.

44:34.000 --> 44:40.400
Okay, okay. Well, I guess there's reports, I think Microsoft or yeah, which we'll get into.

44:40.480 --> 44:44.720
So the UAE gets a bunch of compute because we're building the clusters there.

44:45.760 --> 44:50.960
And why, so let's say they have 25% of, why does a compute ratio matter?

44:53.440 --> 44:56.640
If it's about them being able to kick off the intelligence explosion,

44:56.640 --> 45:00.880
isn't it just some threshold where you have 100 million AI researchers or you don't?

45:00.880 --> 45:04.560
I mean, you can do a lot with, you know, 33 million extremely smart scientists.

45:05.280 --> 45:08.560
And, you know, and again, a lot of the stuff, you know, so first of all, it's like,

45:08.640 --> 45:11.760
you know, that might be enough to build the crazy bio weapons, right? And then you're in a

45:11.760 --> 45:15.280
situation where like now, wow, we've just like, they stole the weights, they seized the compute,

45:15.280 --> 45:19.280
now they can make, you know, they can build these crazy new WMDs that, you know,

45:19.280 --> 45:22.560
will be possible super intelligence. And then you just kind of like proliferated the stuff.

45:22.560 --> 45:26.800
And, you know, it'll be really powerful. And also, I mean, I think, you know,

45:27.840 --> 45:31.520
three acts on compute isn't actually that much. And so the, you know, the,

45:33.840 --> 45:36.240
you know, I think a thing I worry a lot about is

45:38.560 --> 45:42.800
I think everything, I think the riskiest situation is if we're in some sort of like

45:42.800 --> 45:47.200
really tight neck, feverish international struggle, right? If we're like really close

45:47.200 --> 45:52.000
with the CCP and we're like months apart. I think the situation we want to be in,

45:52.000 --> 45:55.040
we could be in, if we played our cards, right, is a little bit more like, you know, the US,

45:55.040 --> 45:59.200
you know, building the atomic bomb versus the German project way behind, you know, years behind.

46:00.560 --> 46:03.520
And if we have that, I think we just have so much more wiggle room, like to get safety,

46:03.520 --> 46:06.400
right? We're going to be building like, you know, there's going to be these crazy new WMDs,

46:06.400 --> 46:09.520
you know, things that completely undermine, you know, nuclear deterrence, you know,

46:10.240 --> 46:14.960
intense competition. And that's so much easier to deal with if, you know, you're like, you know,

46:14.960 --> 46:17.440
it's not just, you know, you don't have somebody right on your tails, you got to go,

46:17.440 --> 46:21.840
go, go, you got to go maximum speed, you have no wiggle room. You're worried that at any time

46:21.840 --> 46:24.800
they can overtake you. I mean, they can also just try to outbuild you, right? Like they might,

46:24.800 --> 46:28.720
they might literally win, like China might literally win if they can steal the weights,

46:28.720 --> 46:33.440
because they can outbuild you. And they maybe have less caution, both, you know, good and bad

46:33.440 --> 46:38.560
caution, you know, kind of like whatever unreasonable regulations we have. Or you're just

46:38.560 --> 46:41.760
in this really tight race. And I think it is that sort of like, if you're in this really tight race,

46:41.760 --> 46:44.880
this sort of feverish struggle, I think that's when sort of there's the greatest peril of

46:44.880 --> 46:50.160
self-destruction. So then presumably the companies that are trying to build clusters in

46:50.160 --> 46:53.920
the Middle East realize this, what is it? Is it just that it's impossible to do this in America?

46:53.920 --> 46:57.520
And if you want American companies to do this at all, then you do it in Middle East or not at all.

46:57.520 --> 46:59.920
And then you just like, I'm trying to build a three gorgeous dam cluster.

46:59.920 --> 47:02.640
I mean, there's a few reasons. One of them is just like, people aren't thinking about this as

47:02.640 --> 47:06.160
the AGI superintelligence cluster. They're just like, ah, you know, like cool clusters for my,

47:06.160 --> 47:11.120
you know, for my chat. So they're building in the plans right now are clusters, which

47:11.120 --> 47:14.160
are ones that are like, because if you're doing once we're inference, presumably you

47:14.160 --> 47:16.880
could like spread them out across the country or something. But the ones they're building,

47:16.880 --> 47:21.440
they realize we're going to do one training run in this thing we're building.

47:21.440 --> 47:24.960
I just think it's harder to distinguish between inference and training compute. And so people

47:24.960 --> 47:27.920
can claim it's training compute, but I think they might realize that actually, you know,

47:27.920 --> 47:31.120
this is going to be useful for, yeah, sorry, they might say it's inference compute. And

47:31.120 --> 47:32.480
actually it's useful for training compute too.

47:32.480 --> 47:34.960
Because of the synthetic data and things like that.

47:34.960 --> 47:38.000
Yeah. The future of training, you know, like RL looks a lot like inference, for example, right?

47:38.000 --> 47:41.840
Or, or you just kind of like end up connecting them, you know, in time, you know, it's like

47:41.840 --> 47:45.360
a lot raw material, you know, it's like, you know, it's, it's, it's placing your uranium refinement

47:45.360 --> 47:46.400
facilities there. Sure.

47:46.400 --> 47:49.520
Anyway, so a few reasons, right? One is just like, they don't think about this as the AGI cluster.

47:49.520 --> 47:54.000
Another is just like easy money from the Middle East, right? Another one is like, you know,

47:55.520 --> 47:58.720
people saying, some people think that, you know, you can't do it in the U.S.

47:58.800 --> 48:03.600
And, you know, I think we actually face this sort of real system competition here, because

48:03.600 --> 48:06.640
again, some people think it's only autocracies that can do this, that can kind of like top

48:06.640 --> 48:11.520
down, mobilize the sort of industrial capacity, the power, you know, get the stuff done fast.

48:11.520 --> 48:14.320
And again, this is the sort of thing, you know, we haven't faced in a while.

48:15.040 --> 48:18.640
But, you know, during the Cold War, like we really, there was this sort of intense system

48:18.640 --> 48:22.160
competition, right? Like East West Germany was this, right? Like West Germany kind of like

48:22.160 --> 48:27.600
liberal democratic capitalism versus kind of, you know, communist state planned. And, you know,

48:27.600 --> 48:32.000
now it's obvious that the sort of, you know, the free world would win. But, you know, even as

48:32.000 --> 48:36.080
late as like 61, you know, Paul Samuelson was predicting that the Soviet Union would outgrow

48:36.080 --> 48:40.320
the United States because they were able to sort of mobilize industry better. And so yeah,

48:40.320 --> 48:45.120
there's some people who, you know, shippost about loving America by day, but then in private,

48:45.120 --> 48:49.280
they're betting against America. They're betting against the liberal order. And I think, I basically

48:49.280 --> 48:52.480
just think it's a bad bet. And the reason I think it's a bad bet is I think this stuff is just

48:52.480 --> 48:56.080
really possible in the U.S. And so there's make it possible in the U.S. There's some amount that

48:56.160 --> 48:59.200
we have to get our act together, right? So I think there's basically two paths to doing it in the

48:59.200 --> 49:03.520
U.S. One is you just got to be willing to do natural gas. And there's ample natural gas, right?

49:03.520 --> 49:07.040
You put your cluster in West Texas, you put it in, you know, Southwest Pennsylvania by the,

49:07.040 --> 49:12.160
you know, Marcello Shale. Ten gigawatt cluster is super easy. 100 gigawatt cluster also pretty

49:12.160 --> 49:16.160
doable. You know, I think, you know, natural gas production in the United States is, you know,

49:16.160 --> 49:19.520
almost doubled in a decade. If you do that, you know, one more time over the next, you know,

49:20.080 --> 49:23.760
seven years or whatever, you know, you could power multiple trillion dollar data centers.

49:24.000 --> 49:28.080
But the issue there is, you know, a lot of people have sort of these made these climate

49:28.080 --> 49:31.280
commitments and not just government. It's actually the private companies themselves, right? The

49:31.280 --> 49:35.280
Microsoft, the Amazons and so on. And these climate commitments, so they won't do natural gas.

49:36.240 --> 49:39.120
And, you know, I admire the climate commitments, but I think at some point, you know,

49:39.760 --> 49:42.560
the national interest and national security kind of is more important.

49:43.840 --> 49:47.120
The other path is like, you know, you can do this sort of green energy mega projects, right? You

49:47.120 --> 49:53.760
do the solar and the batteries and the, you know, the SMRs and geothermal. But if we want to do that,

49:53.760 --> 49:57.840
there needs to be sort of a sort of broad, deregulatory push, right? So, like, you can't

49:57.840 --> 50:01.360
have permitting take a decade, right? So you got to reform FERC. You got to, like, have, you know,

50:01.360 --> 50:05.760
blanket NEPA exemptions for this stuff. You know, there's like Nain state level regulations,

50:05.760 --> 50:09.040
you know, that are like, yeah, you could build, you know, you build the solar panels and batteries

50:09.040 --> 50:12.800
next to your data center, but it'll still take years because, you know, you actually have to

50:12.800 --> 50:18.000
hook it up to the state electrical grid, you know, and you'll have to, like, use governmental

50:18.000 --> 50:21.920
powers to create rights of way to kind of, like, you know, have multiple clusters and connect them,

50:21.920 --> 50:25.920
you know, and have thick cables, basically. And so, look, I mean, ideally, we do both,

50:25.920 --> 50:29.120
right? Ideally, we do natural gas and the broad deregulatory agenda. I think we have to do at

50:29.120 --> 50:33.360
least one. And then I think this possible stuff is just possible in the United States.

50:33.360 --> 50:37.920
Yeah. I think a good analogy for this, by the way, before the conversation I was reading,

50:38.640 --> 50:42.240
there's a good book about World War II industrial mobilization in the United States called

50:42.240 --> 50:48.400
Freedom's Forge. Yeah. And I guess when we think back on that period, especially if you're from,

50:48.400 --> 50:52.560
if you read, like, the Patrick Hallison Fast and the Progress Study stuff, it's like,

50:52.560 --> 50:58.080
the hat state capacity back then, and people just got you done, but now it's a cluster.

50:58.080 --> 51:02.880
Wasn't it all the case? No, so it was really interesting. So you have people who are from

51:02.880 --> 51:09.040
the Detroit auto industry side, like Knudsen, who are running mobilization for the United States,

51:09.120 --> 51:13.040
and they were extremely incompetent. Yeah. But then at the same time, you had

51:13.040 --> 51:17.840
labor organization agitation, which is actually very analogous to the climate pledges and

51:17.840 --> 51:24.960
climate change concern we have today, where they would have these strikes while literally into 1941,

51:25.520 --> 51:31.920
that would cost millions of man hours worth of time when we're trying to make tens of millions,

51:31.920 --> 51:36.000
sorry, tens of thousands of planes a month or something. And they would just

51:36.640 --> 51:43.440
debilitate factories for, you know, trivial, like pennies on the dollar kind of concessions from

51:43.440 --> 51:50.080
capital. And it was concerns that, oh, the auto companies are trying to use the pretext of a

51:50.080 --> 51:57.440
potential war to actually prevent paying labor that money deserves. And so the climate changes

51:57.440 --> 52:00.800
today, like, you think, ah, fuck, America's fucked, like, we're not going to be able to build this

52:00.800 --> 52:05.120
shit. Like, if you, if you look at Nipah or something, but I didn't realize how debilitating

52:05.120 --> 52:08.720
labor was in like World War Two, right? It was just, you know, before at the, you know,

52:08.720 --> 52:12.240
it's sort of like 39 or whatever, the American military was in total shambles, right? You read

52:12.240 --> 52:15.360
about it and it reads a little bit like, you know, the German military today, right? It's like, you

52:15.360 --> 52:19.440
know, military expenditures, I think were less than 2% of GDP, you know, all the European countries

52:19.440 --> 52:23.920
had gone even in peacetime, you know, like above 10% of GDP, sort of this like rapid mobilization,

52:23.920 --> 52:27.280
there's nothing, you know, like, we're making kind of like no planes, there's no military

52:27.280 --> 52:30.800
contracts, everything had been starved during the Great Depression. But there was this latent

52:30.800 --> 52:35.120
capacity. And, you know, at some point, the United States got their act together. I mean,

52:35.120 --> 52:39.280
the thing I'll say is I think, you know, the supplies are the other way around too, to basically

52:39.280 --> 52:42.640
to China, right? And I think sometimes people are, you know, they kind of count them out a little

52:42.640 --> 52:46.560
bit and they're like the export controls and so on. And, you know, they're able to make seven

52:46.560 --> 52:50.160
nanometer chips now. I think there's a question of like, how many could they make? But, you know,

52:50.160 --> 52:53.040
I think there's at least a possibility that they're going to be able to mature that ability and make

52:53.040 --> 52:57.680
a lot of seven nanometer chips. And there's a lot of latent industrial capacity in China,

52:57.760 --> 53:01.360
and they are able to like, you know, build a lot of power fast. And maybe that isn't activated for

53:01.360 --> 53:06.240
AI yet. But at some point, you know, the same way the United States and like, you know, a lot of

53:06.240 --> 53:09.120
people in the US and the United States government is going to wake up, you know, at some point,

53:09.120 --> 53:16.320
the CCP is going to wake up. Yeah. Okay. Going back to the question of presumably companies,

53:16.320 --> 53:20.400
if are they blind to the fact that there's going to be some sort of, well, okay, so

53:20.400 --> 53:24.080
they realize that there's going, they realize scaling is a thing, right? Obviously, their whole

53:24.080 --> 53:29.360
plans are continued on scaling. And so they understand that we're going to be in 2020 building

53:29.360 --> 53:34.960
the 10 gigawatt data centers. And at this point, the people who can keep up our big tech just

53:34.960 --> 53:40.880
potentially at like the edge of their capabilities, then sovereign wealth fund, fund of things, and

53:40.880 --> 53:48.400
also big major countries like America, China, whatever. So what's their plan? If you look at

53:48.400 --> 53:54.000
like these AI labs, what's their plan given this landscape? Do they not want the leverage

53:54.000 --> 53:58.240
of having being in the United States? I mean, I think, I don't know, I think, I mean, one thing

53:58.240 --> 54:03.760
the Middle East does offer is capital, but it's like America has plenty of capital, right? It's

54:03.760 --> 54:06.720
like, you know, we have trillion dollar companies, like what are these Middle Eastern states, they're

54:06.720 --> 54:09.920
kind of like trillion dollar oil companies, and we have trillion dollar companies, and we have

54:09.920 --> 54:13.200
very deep financial markets, and it's like, you know, Microsoft could issue hundreds of billions

54:13.200 --> 54:17.440
of dollars of bonds, and they can pay for these clusters. I mean, look, I think another argument

54:17.440 --> 54:21.760
being made, and I think it's worth taking seriously is an argument that look, if we don't work with

54:21.760 --> 54:27.200
the UAE or with these Middle Eastern countries, they're just going to go to China, right? And so,

54:27.200 --> 54:30.160
you know, we, you know, they're going to build data centers, they're going to pour money into AI,

54:30.160 --> 54:36.240
regardless, and if we don't work with them, you know, they'll just support China. And look, I mean,

54:36.240 --> 54:41.280
I think, I think there's some merit to the argument, and in the sense that I think we should

54:41.280 --> 54:43.920
be doing basically benefit sharing with them, right? I think we should talk about this later,

54:43.920 --> 54:47.120
but I think basically sort of on the road to AGI, there should be kind of like two tiers of

54:47.120 --> 54:50.640
coalitions should be the sort of narrow coalition of democracies, that's sort of the

54:50.640 --> 54:54.560
coalition that's developing AGI. And then there should be a broader coalition where we kind of

54:54.560 --> 54:58.000
go to other countries, including, you know, dictatorships, and we're willing to offer them,

54:59.680 --> 55:02.160
you know, we're willing to offer them some of the benefits of the AI, some of the sharing.

55:02.160 --> 55:06.080
And so it's like, look, if, if, if the UAE wants to use AI products, if they want to run, you know,

55:06.080 --> 55:10.160
meta recommendation engines, if they want to run, you know, like the last generation models,

55:10.160 --> 55:14.320
that's fine. I think by default, they just like wouldn't have had this seat at the AGI table,

55:14.320 --> 55:17.440
right? And so it's like, yeah, they have some money, but a lot of people have money.

55:17.680 --> 55:23.200
And, you know, the only reason they're getting this sort of coursey at the AGI table, the only

55:23.200 --> 55:28.240
reason we're giving these dictators will have this enormous amount of leverage over this extremely

55:28.240 --> 55:34.000
national security relevant technology is because we're, you know, we're kind of getting them excited

55:34.000 --> 55:40.080
and offering it to them. You know, I think the other who like who specifically is doing this,

55:40.080 --> 55:43.440
like just the companies who are going there to fundraise or like this is the AGI is happening

55:43.520 --> 55:46.880
and you can find it or you can't. It's been reported that it's been reported that, you know,

55:46.880 --> 55:50.560
Sam is trying to raise, you know, seven trillion or whatever for a chip project. And, you know,

55:50.560 --> 55:54.080
it's unclear how many of the clusters will be there and so on. But it's, you know, definitely,

55:54.080 --> 55:57.600
definitely stuff is happening. I mean, look, I think another reason I'm a little bit,

55:57.600 --> 56:01.120
at least suspicious of this argument of like, look, if the US doesn't work with them, they'll

56:01.120 --> 56:06.640
go to China is, you know, I've heard heard from multiple people. And this wasn't, you know,

56:06.640 --> 56:10.320
from a time at open AI and I haven't seen the memo, but I have heard from multiple people

56:10.400 --> 56:15.120
that, you know, at some point several years ago, open AI leadership had sort of laid out a plan

56:15.120 --> 56:19.440
to fund and sell AGI by starting a bidding war between the governments of, you know, the United

56:19.440 --> 56:24.080
States, China and Russia. And so, you know, it's kind of surprising to me that they're willing to

56:24.080 --> 56:28.240
sell AGI to the Chinese and Russian governments. But also there's something that sort of feels a bit

56:28.240 --> 56:31.760
eerily familiar about kind of starting this bidding war and then kind of like playing them

56:31.760 --> 56:37.040
off each other. And well, you know, if you don't do this, China will do it. So anyway, interesting.

56:37.040 --> 56:42.720
Okay, so that's pretty fucked up. But given that, that's, okay, so suppose that you were right

56:42.720 --> 56:47.920
about, we ended up in this place because we got the way one of our friends put it is that the

56:47.920 --> 56:54.240
Middle East has like no other place in the world, billions of dollars or trillions of dollars up

56:54.240 --> 57:02.240
for persuasion. And the Microsoft board, it's only, it's only the dictator.

57:02.240 --> 57:06.640
Yeah. But so let's say you're right, that you shouldn't have gotten them excited about AGI in

57:06.640 --> 57:11.920
the first place. But now we're in a place where they are excited about AGI. And they're like,

57:11.920 --> 57:15.440
fuck, we want us to have GPT-5 where we're going to be off building super intelligence.

57:15.440 --> 57:19.920
This Atoms for Peace thing doesn't work for us. And if you're in this place,

57:21.920 --> 57:24.640
don't they already have the leverage? Aren't you like, and as you might as well just say-

57:24.640 --> 57:27.920
I don't think, I think the UAE on its own is not competitive, right? It's like, I mean,

57:27.920 --> 57:30.800
they're already export controlled. Like, you know, we're not, you know, there's like,

57:30.800 --> 57:34.080
you're not actually supposed to ship NVIDIA chips over there, right? You know, it's not like they

57:34.080 --> 57:36.640
have any of the leading AI labs. You know, it's like they have money, but you know,

57:36.640 --> 57:38.480
it's actually hard to just translate money into like-

57:38.480 --> 57:42.320
But the other things you've been saying about laying out your vision is very much there's

57:42.320 --> 57:47.120
this almost industrial process of you put in the compute and then you put in the algorithms.

57:47.120 --> 57:50.640
Yes. You add that up and you get AGI on the other end.

57:50.640 --> 57:56.160
Yes. If it's something more like that, then the case for somebody being able to catch up

57:56.160 --> 57:58.400
rapidly seems more compelling than if it's some bespoke.

57:58.400 --> 58:01.600
Well, well, if they can steal the algorithms and if they can steal the weights.

58:01.600 --> 58:05.040
That's really, that's really where sort of, I mean, we should talk about this. This is really

58:05.040 --> 58:11.920
important. And I think, you know. So like right now, how easy would it be for a foreign actor to

58:11.920 --> 58:17.840
steal the things that are like, not the things that are released about Scarlett Johansson's voice,

58:17.840 --> 58:20.400
but the RL things are talking about the unhobblings.

58:20.400 --> 58:24.240
I mean, I mean, all extremely easy, right? You know, I, you know, deep mind even like,

58:24.240 --> 58:27.600
you know, they don't make a claim that it's hard, right? Deep mind put out there,

58:27.600 --> 58:30.800
like whatever, frontier safety, something, and they like lay out security levels,

58:30.800 --> 58:33.600
and they, you know, security level zero to four and four is this new resilient,

58:33.600 --> 58:36.480
resistant to state actors. And they say we're at level zero, right?

58:36.480 --> 58:39.840
And then, you know, I mean, just recently, there was like an indictment of a guy who just

58:39.840 --> 58:43.680
like stole the code, a bunch of like really important AI code and went to China with it.

58:43.680 --> 58:47.280
And, you know, all he had to do to steal the code was, you know, copy the code and put it

58:47.280 --> 58:51.520
into Apple notes and then export it as PDF. And that got past their monitoring, right? And, you know,

58:51.520 --> 58:54.800
Google is the best security of any of the iLabs probably because they have the, you know, the

58:54.800 --> 58:58.080
Google infrastructure. I mean, I think, I don't know, roughly, I would think of this as like,

58:58.080 --> 59:02.160
you know, security of a startup, right? And like, what does security of a startup look like,

59:02.160 --> 59:05.840
right? You know, it's not that good. It's easy to steal.

59:05.840 --> 59:12.000
So even if that's the case, a lot of your posts is making the argument that, you know,

59:12.000 --> 59:15.360
why are we going to get the intelligence explosion? Because if we have somebody with

59:15.360 --> 59:19.280
the intuition of an Alec Radford, to be able to come up with all these ideas,

59:19.280 --> 59:24.080
that intuition is extremely valuable and you scale that up. But if it's a matter of these,

59:24.800 --> 59:32.160
if it's just in the code, that, like, if it's just the intuition, then that's not

59:32.160 --> 59:35.920
going to be just in the code, right? And also because of export controls, these countries

59:35.920 --> 59:40.480
are going to have slightly different hardware. You're going to have to make different trade-offs

59:40.480 --> 59:45.840
and probably rewrite things to be able to be compatible with that, including all these things.

59:45.840 --> 59:49.280
Is it just a matter of getting the right pen drive and you plug it into the gigawatt data center

59:49.280 --> 59:51.600
and exit the three gorgeous dam and then you're off to the races?

59:52.800 --> 59:55.920
I mean, like, there's a few different things, right? So one threat model is just stealing

59:55.920 --> 01:00:00.080
the weights themselves. And the weights one is sort of particularly insane, right? Because they

01:00:00.080 --> 01:00:04.560
can just like steal the literal like end product, right? Just like make a replica of the atomic bomb

01:00:04.560 --> 01:00:08.240
and then they're just like ready to go. And, you know, I think that one just is, you know,

01:00:08.240 --> 01:00:12.080
extremely important around the time we have AGI and superintelligence, right? Because it's,

01:00:12.080 --> 01:00:16.480
you know, China can build a big cluster. By default, we'd have a big lead, right? Because

01:00:16.480 --> 01:00:19.360
we have the better scientists, but we make the superintelligence, they just steal it,

01:00:19.360 --> 01:00:23.600
they're off to the races. Weights are a little bit less important right now. Because, you know,

01:00:23.600 --> 01:00:29.360
who cares if they steal the GPT-4 weights, right? Like whatever. And so, you know, we still have

01:00:29.360 --> 01:00:32.640
to get started on weight security now. Because, you know, look, if we think AGI by 27, you know,

01:00:32.640 --> 01:00:35.600
this stuff is going to take a while. And it, you know, it doesn't, you know, it's not just going

01:00:35.600 --> 01:00:38.800
to be like, oh, we do some access control. It's going to, you know, if you actually want to be

01:00:38.800 --> 01:00:43.680
resistant to sort of Chinese espionage, you know, it needs to be much more intense. The thing,

01:00:43.760 --> 01:00:46.720
though, that I think, you know, people aren't paying enough attention to is the secrets,

01:00:46.720 --> 01:00:52.240
as you say. And, you know, I think this is, you know, the compute stuff is sexy. You know,

01:00:52.240 --> 01:00:55.440
we talk about it. But, you know, I think that, you know, I think people underrate the secrets

01:00:56.560 --> 01:00:59.840
because they're, you know, I think they're, you know, the half an order of magnitude a year,

01:00:59.840 --> 01:01:02.960
just by default, sort of algorithmic progress, that's huge. You know, if we have a few-year

01:01:02.960 --> 01:01:07.600
lead by default, you know, that's 10, 30x, 100x bigger cluster, if we protected them.

01:01:08.640 --> 01:01:11.760
And then there's this additional layer of the data wall, right? And so, we have to get

01:01:11.760 --> 01:01:14.720
through the data wall. That means we actually have to figure out some sort of basic new paradigm,

01:01:14.720 --> 01:01:19.440
sort of the AlphaGo step two, right? AlphaGo step one is learns from human imitation. AlphaGo step

01:01:19.440 --> 01:01:23.760
two is the sort of self play RL. And everyone's working on that right now. And maybe we're going

01:01:23.760 --> 01:01:31.280
to crack it. And, you know, if China can't steal that, then they, you know, then they're stuck.

01:01:31.280 --> 01:01:36.880
If they can't steal it, they're off to the races. But whatever that thing is, is it like literally,

01:01:36.880 --> 01:01:40.160
I can write down on the back of a napkin, because if it's that easy, then why is it that hard for

01:01:40.160 --> 01:01:43.520
them to figure it out? And if it's more about the intuitions, then don't you just have to hire

01:01:43.520 --> 01:01:46.960
Alec Radford? Like, what are you copying now? Well, I think there's a few layers to this, right?

01:01:46.960 --> 01:01:53.440
So, I think at the top is kind of like sort of the, you know, fundamental approach, right? And

01:01:53.440 --> 01:01:57.280
sort of like, I don't know, on pre-training, it might be, you know, like, you know, unsupervised

01:01:57.280 --> 01:02:00.640
learning, next token protection, train on the entire internet. You actually get a lot of

01:02:00.640 --> 01:02:04.960
juice out of that already. That one's very quick to communicate. Then there's like,

01:02:04.960 --> 01:02:07.760
there's a lot of details that matter. And you were talking about this earlier, right? It's like,

01:02:07.760 --> 01:02:11.200
probably the way that thing people are going to figure out is going to be like somewhat

01:02:11.200 --> 01:02:14.880
obvious, or there's going to be some kind of like clear, you know, not that complicated thing,

01:02:14.880 --> 01:02:17.200
that'll work. But there's going to be a lot of details to getting that right.

01:02:17.200 --> 01:02:22.960
But if that's true, then again, why are we even, why do we think that getting state-level security

01:02:22.960 --> 01:02:26.320
in these stars will prevent China from catching up? If it's just like, oh, we know some sort of

01:02:26.320 --> 01:02:31.040
self-play RL, we require it to get past the data wall. And if it's as easy as you say,

01:02:31.040 --> 01:02:32.720
in some fundamental sense. Well, I don't know if it's that easy. I mean, again, like, yeah.

01:02:32.720 --> 01:02:36.160
But it's going to be solved by 2027, you say, like, right? It's like, not that hard.

01:02:36.160 --> 01:02:39.600
I just think, you know, the US and the sort of, I mean, all the leading antelabs in the United

01:02:39.600 --> 01:02:42.800
States, and they have this huge lead. I mean, by default, you know, China actually has some good

01:02:42.800 --> 01:02:46.080
LLMs. You know, why do they have good LLMs? They're just using the sort of open source code, right?

01:02:46.080 --> 01:02:50.720
You know, llama or whatever. And so the, the, I think people really underrate the sort of,

01:02:50.720 --> 01:02:54.800
both the sort of divergence on algorithmic progress and the lead the US would have by default,

01:02:54.800 --> 01:02:56.960
because by the, you know, all this stuff was published until recently, right?

01:02:56.960 --> 01:03:00.000
Like Chinchilla scaling laws were published, you know, there's a bunch of MOE papers,

01:03:00.000 --> 01:03:03.600
there's, you know, transformers and, you know, all that stuff was published. And so that's why

01:03:03.600 --> 01:03:06.960
open source is good. That's why China can make some good models. That stuff is now, I mean,

01:03:06.960 --> 01:03:11.200
at least they're not publishing it anymore. And, you know, if we actually kept it secret,

01:03:11.200 --> 01:03:15.440
it would be this huge edge. To your point about sort of like some tacit knowledge,

01:03:15.440 --> 01:03:18.240
now like Bradford, you know, there's, there's another layer at the bottom that is something

01:03:18.240 --> 01:03:21.600
about like, you know, large scale engineering work to make these big training ones work.

01:03:21.600 --> 01:03:25.840
I think that is a little bit more tacit knowledge. So I think that, but I think China will be able

01:03:25.840 --> 01:03:28.640
to figure that out. That's like sort of engineering stuff. They're going to figure out how to

01:03:28.640 --> 01:03:32.000
figure that out, but not how to get the RL thing working.

01:03:32.240 --> 01:03:37.680
I mean, look, I don't know, Germany during World War II, you know, they went down the wrong path,

01:03:37.680 --> 01:03:41.200
they did heavy water, and that was wrong. And there's actually, there's an amazing anecdote in

01:03:41.920 --> 01:03:45.360
the making of the atomic bomb on this, right? So, so secrecy is actually one of the most

01:03:45.360 --> 01:03:49.600
contentious issues, you know, early on as well. And, you know, part of it was sort of,

01:03:50.240 --> 01:03:53.760
you know, zillard or whatever really thought, you know, the sort of nuclear chain reaction was

01:03:53.760 --> 01:03:58.560
possible. And so an atomic bomb was possible when you went around and it was like, this is going to

01:03:58.560 --> 01:04:02.400
be of enormous strategic importance, military importance. And a lot of people didn't believe

01:04:02.400 --> 01:04:05.200
it or they're kind of like, well, maybe this is possible, but you know, I'm going to act as

01:04:05.200 --> 01:04:10.880
it's not possible. And, you know, science should be open and all these things. And anyway, and so

01:04:10.880 --> 01:04:15.520
these early days, so there had been some sort of incorrect measurements made on graphite as a

01:04:15.520 --> 01:04:20.080
moderator and that Germany had. And so they thought, you know, graphite was not going to work, we

01:04:20.080 --> 01:04:26.960
have to do heavy water. But then Fermi made some new measurements on graphite. And they indicated

01:04:26.960 --> 01:04:31.680
that graphite would work, you know, this is really important. And then, you know, zillard kind of

01:04:31.680 --> 01:04:36.000
assaulted Fermi with the kind of another secrecy appeal. And Fermi was just kind of, he was pissed

01:04:36.000 --> 01:04:39.600
off, you know, at a temper tantrum, you know, he was like, he thought it was absurd, you know,

01:04:39.600 --> 01:04:43.920
like, come on, this is crazy. But, you know, you know, zillard persisted, I think they

01:04:43.920 --> 01:04:50.320
roped in another guy, Pegram, and then Fermi didn't publish it. And, you know, that was just in time,

01:04:50.320 --> 01:04:54.560
because Fermi not publishing it meant that the Nazis didn't figure out graphite would work,

01:04:54.560 --> 01:04:58.160
they went down this path of heavy water. And that was the wrong path. That was one of the sort,

01:04:58.160 --> 01:05:01.280
you know, this is a key reason why the sort of German project didn't work out. They were kind

01:05:01.280 --> 01:05:08.160
of way behind. And, you know, I think we face a similar situation on are we are we just going to

01:05:08.160 --> 01:05:12.240
instantly leak the sort of how do we get past the data wall? What's the next paradigm? Or are we not?

01:05:12.240 --> 01:05:17.920
So and the reason this would matter is if there's like a being one year ahead would be a huge

01:05:17.920 --> 01:05:21.840
advantage in the world where it's like you deploy AI over time, and then just like, God,

01:05:21.840 --> 01:05:25.440
they're going to catch up anyway. I mean, I interviewed Richard Rhodes, the guy who wrote

01:05:25.440 --> 01:05:33.440
the making an atomic bomb. Yeah. And one of the anecdotes he had was when, so they'd realized

01:05:33.440 --> 01:05:39.360
America had the bomb, obviously we dropped it in Japan. And Beria goes, the guy who ran the NKBD,

01:05:40.400 --> 01:05:45.440
just a famously ruthless guy, just evil. And he goes to, I forgot the name of the guy, the Soviet

01:05:45.440 --> 01:05:51.200
scientist was running their version of the Mandan project. He says, comrade, you will get us

01:05:51.200 --> 01:05:55.280
the American bomb. Yeah. And the guy says, well, listen, their implosion device actually is not

01:05:55.280 --> 01:05:59.920
optimal. We should make it a different way. And Beria says, no, you will get us the American bomb

01:05:59.920 --> 01:06:07.200
or your family will be camped us. But the thing that's relevant about that anecdote is actually

01:06:07.840 --> 01:06:11.120
the Soviets would have had a better bomb if they hadn't copied the American design, at least

01:06:11.120 --> 01:06:15.600
initially. And which suggests that often in history, this is something that's not just for the

01:06:15.600 --> 01:06:22.320
Manhattan project, but there's this pattern of parallel invention where, because the tech tree

01:06:22.320 --> 01:06:26.240
implies that the certain thing is next, in this case, self play, RL, whatever.

01:06:28.160 --> 01:06:31.040
Then people are just like working on that. And like people are going to figure out around the same

01:06:31.040 --> 01:06:35.680
time. There's not, there's not going to be that much gap in who gets it first. It wasn't like

01:06:35.680 --> 01:06:39.200
famously the bunch of people were invented something like the light bulb around the same time and so

01:06:39.200 --> 01:06:43.760
for it. So, but is it just that like, yeah, that might be true, but it'll be the one year or the

01:06:43.760 --> 01:06:46.640
six months or whatever. Two years makes all the difference. I don't know if it'll be two years

01:06:46.640 --> 01:06:50.240
though. I mean, I actually, I mean, I actually think if we locked down the labs, we have, we have

01:06:50.240 --> 01:06:53.520
much better scientists, we're way ahead, it would be two years. But even, I think, even, I think,

01:06:53.520 --> 01:06:56.800
I think whether you, I think, yeah, I think even six months a year would make a huge difference.

01:06:56.800 --> 01:07:00.240
And this gets back to the sort of intelligence explosion. It's like a year might be the difference

01:07:00.240 --> 01:07:05.440
between, you know, a system that's sort of like human level and a system that is like vastly super

01:07:05.440 --> 01:07:09.520
human, right? It might be like five, five ooms, you know, even on the current pace, right? We went

01:07:09.600 --> 01:07:13.760
from, you know, I think on the math benchmark recently, right? Like, you know, three years ago

01:07:13.760 --> 01:07:19.280
on the math benchmark, we, you know, that that was, you know, this is a sort of really difficult

01:07:19.280 --> 01:07:23.600
high school competition math problems. You know, we were at, you know, a few percent couldn't solve

01:07:23.600 --> 01:07:27.840
anything. Now it's solved. And that was sort of at the normal pace of AI progress. You didn't have

01:07:27.840 --> 01:07:31.520
sort of a billion super intelligent resources, researchers. So like a year is a huge difference.

01:07:31.520 --> 01:07:35.120
And then particularly after super intelligence, right? Once this is applied to sort of lots of

01:07:35.120 --> 01:07:38.480
elements of R&D, once you get the sort of like industrial explosion with the robots and so on,

01:07:39.200 --> 01:07:42.880
you know, I think a year, you know, a couple years might be kind of like decades worth of

01:07:42.880 --> 01:07:47.200
technological progress and might, you know, again, it's like go for one, right? 20, 30 years of

01:07:47.200 --> 01:07:51.680
technological lead, totally decisive. You know, I think it really matters. The other reason it

01:07:51.680 --> 01:07:56.560
really matters is, you know, suppose, suppose they steal the weight, suppose they steal the

01:07:56.560 --> 01:08:00.800
algorithms and, you know, they're close on our tails. Suppose we still pull out ahead, right?

01:08:00.800 --> 01:08:05.200
We just kind of, we were a little bit faster, you know, we're three months ahead. I think the

01:08:05.200 --> 01:08:09.360
sort of like world in which we're really neck and neck, you know, you only have a three-month lead

01:08:09.360 --> 01:08:14.000
are incredibly dangerous, right? And we're in this like feverish struggle where like if they get ahead,

01:08:14.000 --> 01:08:19.840
they get to dominate, you know, sort of maybe they'd get a decisive advantage. They're about in

01:08:19.840 --> 01:08:24.160
clusters like crazy. They're willing to throw all caution to the wind. We have to keep up. There's

01:08:24.160 --> 01:08:28.320
some crazy new WMDs popping up. And then we're going to be in the situation where it's like,

01:08:28.320 --> 01:08:31.920
you know, crazy new military technology, crazy new WMDs, you know, like deterrence,

01:08:31.920 --> 01:08:35.680
mutually-disturbed interaction, like keeps changing, you know, every few weeks. And it's like,

01:08:35.680 --> 01:08:40.240
you know, completely unstable volatile situation. That is incredibly dangerous. So it's, I think,

01:08:40.240 --> 01:08:42.880
I think, you know, both, both from just the technologies are dangerous from the alignment

01:08:42.880 --> 01:08:45.840
point of view. You know, I think it might be really important during the intelligence explosion to

01:08:45.840 --> 01:08:50.240
have the sort of six-month, you know, wiggle room to be like, look, we're going to like

01:08:50.240 --> 01:08:53.040
dedicate more compute to alignment during this period because we have to get it right. We're

01:08:53.040 --> 01:08:58.400
feeling uneasy about how it's going. And so I think in some sense that like one of the most

01:08:58.400 --> 01:09:02.320
important inputs, so whether we will kind of destroy ourselves or whether we will get through

01:09:02.320 --> 01:09:06.000
this just incredibly crazy period is whether we have that buffer.

01:09:08.000 --> 01:09:14.960
Why, so before we go further object level in this, I think it's very much worth noting that

01:09:14.960 --> 01:09:22.400
almost nobody, at least nobody I talk to, thinks about the geopolitical implications of AI. And I

01:09:22.400 --> 01:09:26.720
think I have some object level disagreements that we'll get into, but or at least things I want

01:09:26.800 --> 01:09:33.120
to iron out. I may not disagree in the end. But the basic premise that obviously if you

01:09:33.120 --> 01:09:37.920
keep scaling and obviously if people realize that this is where intelligence is headed,

01:09:37.920 --> 01:09:44.000
it's not just going to be like the same world where like what model are we deploying tomorrow

01:09:44.000 --> 01:09:49.200
and what is the latest like people on Twitter like, oh, there are the GPT-4O is going to

01:09:49.200 --> 01:09:53.120
shake your expectations or whatever. You know, COVID is really interesting because

01:09:53.360 --> 01:10:02.720
before a year or something, when March 2020 hit, it became clear to the world like president,

01:10:02.720 --> 01:10:07.360
CEO, media, average person, there's other things happening in the world right now. But the main

01:10:07.360 --> 01:10:13.760
thing we as a world are dealing with right now is COVID. Soon on AGI. Yeah. Okay. And then so this

01:10:13.760 --> 01:10:17.520
is the quiet period. You know, if you want to go on vacation, you know, you want to like, you want

01:10:18.480 --> 01:10:24.320
maybe like now is the last time you can have some kids. You know, my girlfriend sometimes

01:10:24.320 --> 01:10:29.200
complains that, you know, when I'm like, you know, off doing work or whatever, she's like,

01:10:29.200 --> 01:10:33.200
I'm not spending time with her. She's like, you know, she threatens to replace me with like,

01:10:33.200 --> 01:10:37.120
you know, GPT-6 or whatever. And I'm like, you know, GPT-6 will also be too busy doing AI research.

01:10:39.760 --> 01:10:44.400
Okay. Anyway, so what's the answer to the question of why, why aren't other people talking

01:10:44.400 --> 01:10:47.280
about being national security? I made this mistake with COVID, right? So I, you know,

01:10:47.280 --> 01:10:52.800
February of 2020, and I, you know, I thought just it was going to sweep the world and all the hospitals

01:10:52.800 --> 01:10:57.040
would collapse and it would be crazy. And then, and then, you know, and then it'd be over. And you

01:10:57.040 --> 01:10:59.680
know, a lot of people thought this kind of the beginning of COVID, they shut down their offices

01:10:59.680 --> 01:11:03.680
a month or whatever. I think the thing I just really didn't price in was the sidal reaction,

01:11:03.680 --> 01:11:09.760
right? And, and within weeks, you know, Congress spent over 10% of GDP on like COVID measures,

01:11:09.760 --> 01:11:15.520
right? The entire country was shut down. It was crazy. And so I don't know, I didn't price it in

01:11:15.520 --> 01:11:20.640
with COVID sufficiently. I don't know, why do people underrate it? I mean, I think there's,

01:11:20.640 --> 01:11:24.800
there's a, there's a sort of way in which being kind of in the trenches actually kind of, I think

01:11:26.720 --> 01:11:29.680
gives you a less clear picture of the trend lines. You actually have to zoom out that much

01:11:29.680 --> 01:11:34.080
only like a few years, right? But you know, you're in the trenches, you're like trying to get the

01:11:34.080 --> 01:11:37.360
next model to work, you know, there's always something that's hard, you know, for example,

01:11:37.360 --> 01:11:40.880
you might underrate algorithmic progress because you're like, ah, things are hard right now or,

01:11:40.880 --> 01:11:44.000
you know, data wall or whatever. But, you know, you zoom out just a few years and you actually

01:11:44.000 --> 01:11:47.600
try to like count up how much algorithmic progress made in the last, you know, last few years. And

01:11:47.600 --> 01:11:53.120
it's, it's enormous. But I also just don't think people think about this stuff. Like I think smart

01:11:53.120 --> 01:11:58.160
people really underrate espionage, right? And, you know, I think part of the security issue is I

01:11:58.160 --> 01:12:02.560
think people don't realize like how intense state level espionage can be, right? Like, you know,

01:12:02.640 --> 01:12:06.800
you know, this is really company had had software that could just zero click hack any

01:12:06.800 --> 01:12:10.160
iPhone, right? They just put in your number and then it's just like straight download of

01:12:10.160 --> 01:12:14.560
everything, right? Like the United States infiltrated an air gap, atomic weapons program,

01:12:14.560 --> 01:12:19.840
right? Wild, you know, like, yeah, you know, the, you know, you know, intelligence agencies have

01:12:19.840 --> 01:12:24.720
just stockpiles of zero days, you know, when things get really hot, you know, I don't know,

01:12:24.720 --> 01:12:28.080
maybe we'll send special forces, right? To like, you know, get go to the data center or something

01:12:28.080 --> 01:12:32.400
that's, you know, or, you know, I mean, China does this, they threaten people's families, right?

01:12:32.400 --> 01:12:34.880
And they're like, look, if you don't cooperate, if you don't give us the Intel,

01:12:37.360 --> 01:12:41.120
there's a there's a good book, you know, along the lines of the gulag develop, you know, the

01:12:41.120 --> 01:12:47.280
inside the aquarium, which is by a Soviet GRU de facto, GRU was like military intelligence,

01:12:47.280 --> 01:12:54.720
Ilya recommended this book to me. And, you know, I think reading that is just kind of like shocked

01:12:54.800 --> 01:12:58.480
at how intense sort of state level espionage is the whole book was about like, they go to these

01:12:58.480 --> 01:13:02.080
European countries, and they try to like get all the technology and recruit all these people to get

01:13:02.080 --> 01:13:07.840
the technology. I mean, yeah, maybe one anecdote, you know, so when so the spot, you know, this

01:13:07.840 --> 01:13:12.480
eventual defector, you know, so he's being trained, he goes to the kind of GRU spy academy. And so

01:13:12.480 --> 01:13:16.560
then to graduate from the spy academy, sort of before you're sent abroad, you kind of had to

01:13:16.560 --> 01:13:22.480
pass a test to show that you can do this. And the test was, you know, you had to in Moscow recruit

01:13:22.560 --> 01:13:26.400
a Soviet scientist and recruit them to give you information sort of like you would do in the

01:13:26.400 --> 01:13:33.600
foreign country. But of course, for whomever you recruited, the penalty for giving away sort of

01:13:33.600 --> 01:13:40.000
secret information was death. And so to graduate from the Soviet spy, the GRU spy academy, you had

01:13:40.000 --> 01:13:47.280
to condemn a country man to death. States do this stuff. I started reading the book on

01:13:47.360 --> 01:13:51.680
because I saw it in the series. Yeah. And I was actually wondering the fact that you use this

01:13:51.680 --> 01:13:57.280
anecdote. Yeah. And then you're like enough of a book recommended by Ilya. Is this some sort of

01:13:58.080 --> 01:14:03.600
is this some sort of Easter egg? We'll leave that for an exercise for the reader.

01:14:05.360 --> 01:14:08.160
Okay, so the beatings will continue until them are all improved.

01:14:11.360 --> 01:14:15.840
So suppose that we live in the world in which these secrets are locked down,

01:14:16.400 --> 01:14:21.040
but China still realizes that this progress is happening in America.

01:14:23.040 --> 01:14:27.920
In that world, especially if they realize, and I guess it's a very interesting open question,

01:14:27.920 --> 01:14:31.440
it probably won't be locked down. Okay, but we're probably gonna live in the bad world.

01:14:31.440 --> 01:14:36.400
Yeah, it's gonna be really bad. Why are you so confident that they won't be locked down? I mean,

01:14:36.400 --> 01:14:39.520
I'm not confident that won't be locked down, but I think it's just it's not happening.

01:14:40.480 --> 01:14:47.520
And so tomorrow, the lab leaders get the message. How hard like, what do they have to do?

01:14:47.520 --> 01:14:51.600
They get the more security guards, they like air gap the, what do they do?

01:14:51.600 --> 01:14:56.480
So again, I think basically it's, I think people, there's kind of like two two reactions there,

01:14:56.480 --> 01:15:03.440
which is like, it's, we're already secure, not. And there's, fatalism, it's impossible.

01:15:03.440 --> 01:15:06.160
I think the thing you need to do is you kind of got to stay ahead of the curve of basically

01:15:06.160 --> 01:15:10.400
how EGI pillows the CCP, right? So like right now, you've got to be resistant to kind of like

01:15:10.400 --> 01:15:15.760
normal economic espionage. They're not, right? I mean, I probably wouldn't be talking about the

01:15:15.760 --> 01:15:19.360
stuff that the labs were, right? Because I wouldn't want to wake them up more, the CCP,

01:15:19.360 --> 01:15:23.440
but they're not, you know, this is like, this stuff is like really trivial for them to do right

01:15:23.440 --> 01:15:27.440
now. I mean, it's also anyway, so they're not resistant to that. I think it would be possible

01:15:27.440 --> 01:15:31.440
for private company to be resistant to it, right? So, you know, both of us have, you know, friends

01:15:31.440 --> 01:15:34.880
in the kind of like quantitative trading world, right? And, you know, I think actually those

01:15:34.880 --> 01:15:39.440
secrets are shaped kind of similarly where it's like, you know, you know, they've said, you know,

01:15:39.440 --> 01:15:42.960
yeah, if I got on a call for an hour or with somebody from a competitive firm, I could,

01:15:42.960 --> 01:15:47.440
most of our alpha would be gone. And that's sort of like, that's the like list of details of like

01:15:47.440 --> 01:15:50.880
really how to, how to make, you're gonna have to worry about that pretty soon. You're gonna have to

01:15:50.880 --> 01:15:55.360
worry about that pretty soon. Yeah. Well, anyway, and so, so all alpha could be gone. But in fact,

01:15:55.360 --> 01:15:59.360
their alpha persists, right? And, you know, often, often for many years and decades. And so this

01:15:59.360 --> 01:16:02.640
doesn't seem to happen. And so I think there's like, you know, I think there's a lot you could go

01:16:02.720 --> 01:16:06.000
if you went from kind of current startup security, you know, you just got to look through the window

01:16:06.000 --> 01:16:10.800
and you can look at the slides. You know, it's kind of like, you know, you know, good private

01:16:10.800 --> 01:16:14.160
sector security hedge funds, you know, the way Google treats, you know, customer data or whatever.

01:16:16.400 --> 01:16:21.040
That'd be good right now. The issue is, you know, basically the CCP will also get more AI

01:16:21.040 --> 01:16:28.000
filled. And at some point, we're going to face kind of the full force of, you know, the Ministry

01:16:28.000 --> 01:16:31.280
of State Security. And again, you're talking about smart people underrating espionage and

01:16:31.280 --> 01:16:34.960
sort of insane capabilities of states. I mean, this stuff is wild, right? You know, they can get

01:16:34.960 --> 01:16:38.080
like, you know, there's papers about, you know, you can find out the location of like where you

01:16:38.080 --> 01:16:41.280
are on a video game map, just from sounds, right? Like states can do a lot with like

01:16:41.280 --> 01:16:46.000
electromagnetic emanations, you know, like, you know, at some point, like you got to be working

01:16:46.000 --> 01:16:49.440
from a sketch, like your cluster needs to be air gapped and basically be a military base. It's

01:16:49.440 --> 01:16:52.640
like, you know, you need to have, you know, intense kind of security clearance procedures

01:16:52.640 --> 01:16:56.720
for employees, you know, they have to be like, you know, all their shit is monitored, you know,

01:16:56.720 --> 01:17:01.280
they're, you know, they basically have security guards, you know, it's, you know, you can't use

01:17:01.280 --> 01:17:05.040
any kind of like, you know, other dependencies, it's all got to be like intensely vetted, you know,

01:17:05.040 --> 01:17:12.000
all your hardware has to be intensely vetted. And, you know, I think basically, if they actually

01:17:12.000 --> 01:17:15.760
really face the full force of state level espionage, I don't really think this is the thing private

01:17:15.760 --> 01:17:18.800
companies can do both. I mean, empirically, right, like, you know, Microsoft recently had

01:17:18.800 --> 01:17:22.160
executives emails hacked by Russian hackers, and, you know, government emails, they've hosted

01:17:22.160 --> 01:17:26.880
hacked by government actors. But also, you know, it's basically there's just a lot of stuff that

01:17:26.880 --> 01:17:29.920
only kind of, you know, the people behind the security curtains know and only they deal with.

01:17:31.360 --> 01:17:35.440
And so, you know, I think it's actually kind of resist the sort of full force of espionage,

01:17:35.440 --> 01:17:38.480
you're going to need the government. Anyway, so I think basically we could, we could do it by

01:17:38.480 --> 01:17:42.960
always being ahead of the curve. I think we're just going to always be behind the curve. And I

01:17:42.960 --> 01:17:47.760
think, you know, maybe unless we get the sort of government project. Okay, so going back to the

01:17:47.760 --> 01:17:52.960
naive perspective of we're very much coming at this from there's going to be a race in the CCP,

01:17:52.960 --> 01:17:58.320
we must win. And listen, I understand like bad people are in charge of the Chinese government,

01:17:58.320 --> 01:18:04.800
like the CCP and everything. But just stepping back in a sort of galactic perspective, humanity

01:18:04.800 --> 01:18:10.560
is developing AGI. And do we want to come at this from the perspective of we need to feed China to

01:18:10.560 --> 01:18:16.320
this, our super intelligent Jupiter brain descendants will know who China like China will

01:18:16.400 --> 01:18:21.680
be something like distant memory that they have America to. So shouldn't it be a more the initial

01:18:21.680 --> 01:18:27.120
approach just come to them like, listen, we this is super intelligence. This is something like

01:18:27.120 --> 01:18:34.720
we come from a cooperative perspective. Why why immediately sort of rush into it from a hawkish

01:18:34.720 --> 01:18:37.760
competitive perspective. I mean, look, I mean, one thing I want to say is like a lot of the

01:18:37.760 --> 01:18:42.480
stuff I talk about in the series is, you know, is sort of primarily, you know, descriptive,

01:18:42.560 --> 01:18:46.480
right. And so I think that on the China stuff, it's like, you know, yeah, and some ideal world,

01:18:46.480 --> 01:18:50.720
you know, we, we, you know, it's just all, you know, merry go around and cooperation. But again,

01:18:50.720 --> 01:18:56.320
it's sort of, I think, I think people wake up to AGI. I think the the issue particular on sort of

01:18:56.320 --> 01:18:59.920
like, can we make a deal? Can we make an international treaty? I think it really relates to sort of

01:18:59.920 --> 01:19:04.800
what is the stability of sort of international arms control dreamers, right. And so we did very

01:19:04.800 --> 01:19:10.000
successful arms control on nuclear weapons in the 80s, right. And the reason it was successful

01:19:10.080 --> 01:19:13.040
is because the sort of new equilibrium was stable, right. So you take go down from, you know,

01:19:13.040 --> 01:19:18.320
whatever, 60,000 nukes to 10,000 nukes. You know, when you have 10,000 nukes, you know,

01:19:18.320 --> 01:19:22.160
basically breakout, breakout doesn't matter that much, right? Suppose the other guy now try to

01:19:22.160 --> 01:19:25.360
make 20,000 nukes. Well, it's like, who cares, right? You know, like, it's still mutually

01:19:25.360 --> 01:19:29.120
assured destruction. Suppose a rogue state kind of went from zero nukes to one nukes. It's like,

01:19:29.120 --> 01:19:32.160
who cares, we still have way more nukes than you. I mean, it's still not ideal for destabilization,

01:19:32.160 --> 01:19:36.400
but it's, you know, it'd be very different if the arms control agreement had been zero nukes,

01:19:36.400 --> 01:19:40.080
right? Because if it had been zero nukes, then it's just like one rogue state makes one nuk.

01:19:40.080 --> 01:19:45.280
The whole thing is destabilized, breakout is very easy. You know, your adversary state

01:19:45.280 --> 01:19:49.120
starts making nukes. And so basically, when, when you're going to sort of like very low levels of

01:19:49.120 --> 01:19:52.560
arms, or when you're going to kind of in your sort of very dynamic technological situation,

01:19:54.080 --> 01:19:57.520
arms control is really tough because, because breakout is easy, you know, there's, there's,

01:19:57.520 --> 01:20:02.080
I mean, there's some other sort of stories about this in sort of like 1920s, 1930s, you know, it's

01:20:02.080 --> 01:20:06.480
like, you know, all the European states had done disarmament and Germany was, was kind of did this

01:20:06.480 --> 01:20:10.640
like crash program to build the Luftwaffe. And that was able to like massively destabilize things,

01:20:10.640 --> 01:20:13.840
because not that, you know, they were the first, they were able to like pretty easily build kind

01:20:13.840 --> 01:20:16.960
of a modern, you know, Air Force, because the others didn't really have one. And that, you know,

01:20:16.960 --> 01:20:21.600
that really destabilized things. And so I think the issue with EGI and super intelligence is the

01:20:21.600 --> 01:20:25.920
explosiveness of it, right? So if you have an intelligence explosion, if you're able to go from

01:20:25.920 --> 01:20:30.240
kind of EGI to super intelligence, if that super intelligence is decisive, like either, you know,

01:20:30.240 --> 01:20:33.760
like a year after, because you've developed some crazy WMD, or because you have some like,

01:20:33.760 --> 01:20:38.240
you know, super hacking ability that lets you, you kind of, you know, completely deactivate the

01:20:38.240 --> 01:20:42.800
sort of enemy arsenal. That means like, suppose, suppose you're trying to like put in a break,

01:20:42.800 --> 01:20:46.720
you know, like we both, we're both going to like cooperate, and we're going to go slower, you know,

01:20:46.720 --> 01:20:50.640
on the cusp of EGI or whatever, you know, it's going to be such an enormous incentive

01:20:50.640 --> 01:20:53.840
to kind of race ahead to break out. And we're just going to do an intelligence explosion. If we

01:20:53.840 --> 01:20:59.200
can get three months ahead, we win. I think that makes it basically, I think any sort of arms

01:20:59.280 --> 01:21:03.760
control agreement that comes as a situation where it's close, very unstable.

01:21:04.400 --> 01:21:11.120
That's really interesting. This is very analogous to kind of a debate I had with Rose on the podcast

01:21:11.120 --> 01:21:16.640
where he argued for nuclear disarmament. But if some country tries to break out and starts

01:21:16.640 --> 01:21:22.320
developing nuclear weapons, the six months or whatever that you would get is enough to get

01:21:22.320 --> 01:21:26.560
international consensus and invade the country and prevent them from getting nukes. And I thought

01:21:26.560 --> 01:21:34.160
that was sort of, that's not a stable clue. But on this, right? So like maybe it's a bit easier

01:21:34.160 --> 01:21:37.920
because you have EGI and so like you can monitor the other person's cluster or something like

01:21:37.920 --> 01:21:41.920
data centers, you can see them from space actually, you can see the energy draw they're

01:21:41.920 --> 01:21:45.280
getting. There's a lot of things as you were saying, there's a lot of ways to get information

01:21:45.280 --> 01:21:51.520
from an environment if you're really dedicated. And also because unlike a nukes, the data centers are

01:21:52.160 --> 01:21:58.240
nukes, you have obviously the submarines, planes, you have bunkers, mountains, whatever, you have

01:21:58.240 --> 01:22:02.080
them in so many different places. A data center that you're 100 gigawatt data center, we can blow

01:22:02.080 --> 01:22:06.320
that shit up if you're like we're concerned, right? Like just some cruise missile or something.

01:22:06.320 --> 01:22:08.080
It's like very vulnerable to sabotage.

01:22:08.080 --> 01:22:11.760
I mean, that gets to the sort of, I mean, that gets to the sort of insane vulnerability of this

01:22:11.760 --> 01:22:15.280
period post superintelligence, right? Because basically, I think so you have the intelligence

01:22:15.280 --> 01:22:18.640
explosion, you have these like vastly superhuman things on your cluster, but you're like, you

01:22:18.640 --> 01:22:21.760
haven't done the industrial explosion yet, you don't have your robots yet, you haven't kind of,

01:22:21.760 --> 01:22:25.440
you haven't covered the desert in like robot factories yet. And that is the sort of crazy

01:22:25.440 --> 01:22:30.080
moment where, you know, say the United States is ahead, the CCP is somewhat behind. There's

01:22:30.080 --> 01:22:33.680
actually an enormous incentive for first strike, right? Because if they can take out your data

01:22:33.680 --> 01:22:38.640
center, they know you're about to have just this command and decisive lead, they know if we can

01:22:38.640 --> 01:22:43.280
just take out this data center, you know, then we can stop it and they might get desperate.

01:22:43.280 --> 01:22:47.760
And, you know, so I think basically we're going to get into a position, it's actually, I think it's

01:22:47.760 --> 01:22:51.520
going to be pretty hard to defend early on. I think we're basically going to be in a position

01:22:51.520 --> 01:22:54.960
where protecting data centers with like the threat of nuclear retaliation. It's like, maybe

01:22:54.960 --> 01:22:58.480
sounds kind of crazy though, you know, and this is the inverse of the LAZR. We got to

01:22:59.200 --> 01:23:04.080
the data centers. Nuclear deterrence for data centers. I mean, this is a, you know, Berlin,

01:23:04.080 --> 01:23:08.320
you know, in the like late fifties, early sixties, both Eisenhower and Kennedy multiple times kind

01:23:08.320 --> 01:23:12.320
of made the threat of full on nuclear war against the Soviets, if they tried to encroach on West

01:23:12.320 --> 01:23:16.560
Berlin, sort of insane. It's kind of insane that that went well. But basically, I think that's

01:23:16.560 --> 01:23:20.000
going to be the only option for the data centers. It's a terrible option. This whole scheme is

01:23:20.000 --> 01:23:24.480
terrible, right? Like being, being, being in this like neck and neck race, sort of at this

01:23:24.480 --> 01:23:28.960
point is terrible. And, you know, it's also, you know, I think I have some uncertainty basically

01:23:28.960 --> 01:23:32.080
on how easy that decisive advantage will be. I'm pretty confident that if you have super

01:23:32.080 --> 01:23:35.200
intelligence, you have two years, you have the robots, you're able to get that 30 year lead.

01:23:35.840 --> 01:23:39.280
Look, then you're in this like go for one situation. You have your like, you know,

01:23:39.280 --> 01:23:42.640
millions or billions of like mosquito size drones that can just take it out. I think there's even

01:23:42.640 --> 01:23:45.920
a possibility you can kind of get a decisive advantage earlier. So, you know, there's these

01:23:45.920 --> 01:23:49.600
stores, you know, about these as well about, you know, like colonization and like the sort of

01:23:49.600 --> 01:23:54.720
1500s where it was, you know, these like a few hundred kind of spanyards were able to like topple

01:23:54.720 --> 01:23:58.240
the Aztec empire, you know, a couple, I think a couple other empires as well, you know, each of

01:23:58.240 --> 01:24:02.000
these had a few million people. And it was not like God like technological advantage. It was some

01:24:02.000 --> 01:24:05.520
technological advantage. It was, I mean, it was some amount of disease. And then it was kind of

01:24:05.520 --> 01:24:10.400
like cunning strategic play. And so I think there's a, there's a possibility that even sort of early

01:24:10.400 --> 01:24:14.080
on, you know, it's you haven't gone through the full industrial explosion yet, we have super

01:24:14.080 --> 01:24:17.920
intelligence, but you know, you're able to kind of like manipulate the imposing generals, claim

01:24:17.920 --> 01:24:21.760
your ally with them, then you have, you have some, you know, you have sort of like some crazy new

01:24:21.760 --> 01:24:25.520
bioweapons, maybe, maybe there's even some way to like pretty easily get a paradigm that like

01:24:25.520 --> 01:24:30.160
deactivates enemy nukes. Anyway, so I think this stuff could get pretty wild. Here's what I think

01:24:30.160 --> 01:24:36.000
we should do. I really don't want this volatile period. And so a deal with China would be nice.

01:24:36.000 --> 01:24:38.320
It's going to be really tough if you're in this unstable equilibrium.

01:24:38.720 --> 01:24:43.840
I think basically we want to get in a position where it is clear that the United States, that a

01:24:43.840 --> 01:24:47.600
sort of coalition of democratic allies will win. It's clear the United States would be clear to

01:24:47.600 --> 01:24:51.360
China, you know, that will require having locked down the secrets that will require having built

01:24:51.360 --> 01:24:54.560
the 100 gigawatt cluster in the United States and having done the natural gas and doing what's

01:24:54.560 --> 01:24:59.280
necessary. And then when it is clear that the democratic coalition is well ahead, then you go

01:24:59.280 --> 01:25:04.240
to China and then you offer them a deal. And you know, China will know they're going to win. This

01:25:04.480 --> 01:25:10.080
is going to be, they're very scared of what's going to happen. We're going to know we're going to

01:25:10.080 --> 01:25:12.800
win, but we're also very scared of what's going to happen because we really want to avoid this

01:25:12.800 --> 01:25:18.560
kind of like breakneck race right at the end and where things could really go awry. And,

01:25:19.840 --> 01:25:22.960
you know, and then, and so then we offer them a deal. I think there's an incentive to come to

01:25:22.960 --> 01:25:26.320
the table. I think there's a sort of more stable arrangement you can do. It's a sort of an Adams

01:25:26.320 --> 01:25:29.680
for peace arrangement. And we're like, look, we're going to respect you. We're not, we're not going

01:25:29.680 --> 01:25:32.720
to like, we're not going to use super intelligence against you. You can do what you want. You're

01:25:32.720 --> 01:25:36.400
going to get your like, you're going to get your slice of the galaxy. We're going to like,

01:25:36.400 --> 01:25:39.520
we're going to benefit share with you. We're going to have some like computer agreement where

01:25:39.520 --> 01:25:42.400
it's like, there's some ratio of compute that you're allowed to have. And that's like enforced

01:25:42.400 --> 01:25:47.680
with your like composing AI's or whatever. And we're just not going to do, we're just not going

01:25:47.680 --> 01:25:52.880
to do this kind of like volatile sort of WMD arms race to the death. We're good. And sort of it's

01:25:52.880 --> 01:25:57.440
like a new world order that's US led, that sort of democratic led, but that respects China. Let's

01:25:57.440 --> 01:26:04.320
then do what they want. Okay. There's so much to, there's so much there. First on the galaxies

01:26:04.320 --> 01:26:07.920
thing, I think it's just a funny anecdote. So I want to kind of want to tell it. And this,

01:26:07.920 --> 01:26:11.040
we were at an event and I'm respecting Chad, I'm house rules here. I'm not revealing anything

01:26:11.040 --> 01:26:16.400
about it, but we're talking to somebody or a Leopold was talking to somebody influential.

01:26:16.400 --> 01:26:23.440
Afterwards, that person asked the group, Leopold told me that he wants, he's not going to spend

01:26:23.440 --> 01:26:29.200
any money on consumption until he's ready to buy galaxies. And he goes, the guy goes,

01:26:29.920 --> 01:26:36.720
I honestly don't know if you meant galaxies, like the brand of private plane galaxy or the physical

01:26:36.720 --> 01:26:41.360
galaxies. And there was an actual debate. Like he went away to the restroom and there was an

01:26:41.360 --> 01:26:48.080
actual debate among people who are very influential about, oh, they can't amend galaxies. And the

01:26:48.080 --> 01:26:52.400
other people who knew you better be like, no, he means galaxies. I mean the galaxy. I mean the

01:26:52.720 --> 01:26:57.200
galaxies. I mean, I think it'd be interesting. I mean, I think there's, I mean, there's two

01:26:57.200 --> 01:27:00.800
ways to buy the galaxies. One is like at some point, you know, it's like post-superintelligence,

01:27:00.800 --> 01:27:04.960
you know, they're so crazy. But by the way, I love, okay, so what happens is he's on the ground,

01:27:04.960 --> 01:27:08.480
I'm laughing my ass off. I'm not even saying I think people were like, having this debate.

01:27:08.480 --> 01:27:14.400
And then Leopold comes back and the guy, somebody's like, oh, Leopold, we're having this debate

01:27:14.400 --> 01:27:20.640
about whether you meant you want to buy the galaxy or you want to buy the other thing.

01:27:20.640 --> 01:27:26.240
And Leopold assumes they must mean not the Friday play in the galaxy versus the actual galaxy.

01:27:26.240 --> 01:27:30.480
But do you want to buy the property rights with the galaxy or do you just send out the probes right

01:27:30.480 --> 01:27:44.080
now? Oh my god. All right. Back to China. There's a whole bunch of things that I could ask about

01:27:44.800 --> 01:27:48.560
that plan about whether you're going to get credible, promised. You will get some part of

01:27:48.640 --> 01:27:51.760
galaxies, whether they care about that. I mean, you have AIs to help you enforce stuff.

01:27:51.760 --> 01:27:56.000
Okay, sure. We'll leave that aside. That's a different rabbit hole. The thing I want to ask is,

01:27:56.560 --> 01:28:00.880
but it has to be the thing we need. The only way this is possible is if we lock it down.

01:28:00.880 --> 01:28:03.760
I see. If we don't lock it down, we are in this fever struggle.

01:28:05.040 --> 01:28:12.160
Greatest peril mankind will have ever seen. So, but given the fact that in during this period,

01:28:12.160 --> 01:28:16.960
instead of just taking their chances and they don't really understand how this AI governance scheme

01:28:16.960 --> 01:28:19.680
is going to work, whether they're going to check, whether we had to actually get the galaxies,

01:28:20.880 --> 01:28:24.000
the data centers, they can't be built underground. They have to be built above ground.

01:28:24.000 --> 01:28:28.400
Taiwan is right off the coast of us. They need the chips from there. Why aren't we just going to

01:28:28.400 --> 01:28:32.480
invade? Listen, we don't want like worst case scenario is they win the super intelligence,

01:28:32.480 --> 01:28:37.920
which they're on track to do anyways. Wouldn't this instigate them to either invade Taiwan or blow

01:28:37.920 --> 01:28:41.680
up the data center in Arizona or something like that? Yeah. I mean, look, I mean, talked about

01:28:41.680 --> 01:28:45.040
the data center one and then you probably have to like threaten nuclear retaliation to protect

01:28:45.120 --> 01:28:48.640
that. They might also just blow it up. There's also maybe ways they can do it without sort of

01:28:48.640 --> 01:28:52.800
attribution, right? Like you. Stuxnet. Stuxnet. Stuxnet. Yeah. I mean, this is part of, we'll

01:28:52.800 --> 01:28:56.240
talk about this later, but you know, I think, look, I think we need to be working on the Stuxnet

01:28:56.240 --> 01:29:01.760
for the Chinese project, but the. But by the way, for the audience. I mean, Taiwan, the Taiwan

01:29:01.760 --> 01:29:08.160
thing, the, you know, I talk about, you know, AGI about, you know, 27 or whatever. Do you know

01:29:08.160 --> 01:29:13.520
about the like terrible 20s? No. Okay. Well, I mean, sort of in this sort of Taiwan watcher circles,

01:29:13.520 --> 01:29:17.360
people often talk about like the late 2020s as like maximum period of risk for Taiwan,

01:29:17.360 --> 01:29:20.640
because it's sort of like, you know, military modernization cycles and basically extreme

01:29:20.640 --> 01:29:24.160
fiscal tightening on the military budget in the United States over the last decade or two

01:29:24.960 --> 01:29:29.120
has meant that sort of we're in this kind of like, you know, trough in, in, in the late 20s of like,

01:29:29.120 --> 01:29:32.400
you know, basically overall naval capacity. And, you know, that's sort of when China is saying

01:29:32.400 --> 01:29:35.760
they want to be ready. So it's already kind of like, it's kind of pitching, you know,

01:29:35.760 --> 01:29:39.600
there's some sort of like, you know, parallel timeline there. Yeah. Look, it looks appealing

01:29:39.600 --> 01:29:43.120
to invade Taiwan. I mean, maybe not because they, you know, basically remote cutoff of

01:29:43.120 --> 01:29:48.800
the chips. And so then it doesn't mean they get the chips, but it just means they, they,

01:29:49.760 --> 01:29:54.160
you know, it's just, it's, you know, the machines are deactivated. But look, I mean, imagine if

01:29:54.160 --> 01:29:58.640
during the Cold War, you know, all of the world's uranium deposits had been in Berlin,

01:29:58.640 --> 01:30:02.720
you know, and Berlin was already, I mean, almost multiple times it was caused a nuclear war.

01:30:02.720 --> 01:30:09.600
So God help us all. Well, the Groves had a plan after the after the war, that the plan was that

01:30:09.680 --> 01:30:14.320
America would go around the world and getting the rights to every single uranium deposit,

01:30:14.320 --> 01:30:16.800
because they didn't realize how much uranium there was in the world. And they thought this

01:30:16.800 --> 01:30:20.160
was the thing that was feasible. Not realizing, of course, that there's like huge deposits in

01:30:20.160 --> 01:30:25.760
the Soviet Union itself. Right. Okay. East Germany too. There's a, there's always,

01:30:25.760 --> 01:30:29.040
there's a lot of East German workers who kind of got screwed and got cancer.

01:30:30.880 --> 01:30:35.680
Okay. So the framing we've been talking about that we've been assuming, and I'm not sure I buy

01:30:35.680 --> 01:30:41.680
yet, is that the United States, this is our leverage, this is our data center,

01:30:41.680 --> 01:30:45.680
the China is the competitor. Right now, obviously, that's not the way things are progressing.

01:30:45.680 --> 01:30:49.280
Private companies control these AIs. They're deploying them. It's a market-based thing.

01:30:51.840 --> 01:30:56.000
Why will it be the case that it's like the United States, it has this leverage,

01:30:56.000 --> 01:30:58.400
or is doing this thing versus China is doing this thing?

01:30:59.920 --> 01:31:03.520
Yeah. I mean, look, look on the, on the project, you know, I mean, there's sort of descriptive

01:31:03.600 --> 01:31:06.960
and prescriptive claims or sort of normative positive claims. I think the main thing I'm trying

01:31:06.960 --> 01:31:10.720
to say is, you know, you know, look, we're at, we're at these SF parties or whatever.

01:31:10.720 --> 01:31:14.560
And I think people talk about AGI and they're always just talking about the private AI labs.

01:31:14.560 --> 01:31:17.280
And I think I just really want to challenge that assumption. It just seems like,

01:31:18.080 --> 01:31:21.840
seems pretty likely to me, you know, as we've talked about, for reasons we've talked about,

01:31:21.840 --> 01:31:25.360
that look like the national security state is going to get involved.

01:31:25.360 --> 01:31:28.800
And, you know, I think there's a lot of ways this could look like, right? Is it,

01:31:28.800 --> 01:31:32.400
is it like nationalization? Is it a public-private partnership? Is it a kind of defense

01:31:32.400 --> 01:31:36.080
contra-elect or like a relationship? Is it a sort of government project that soaks up all the people?

01:31:37.360 --> 01:31:44.080
And so there's a spectrum there. But I think people are just vastly underrating the chances of

01:31:44.080 --> 01:31:48.080
this more or less looking like a government project. And look, I mean, look, if, if,

01:31:49.520 --> 01:31:52.400
you know, it's sort of like, you know, do you think, do you think like we all have literal,

01:31:52.400 --> 01:31:55.520
like, you know, when we have like literal superintelligence on our cluster, right? And it's

01:31:55.520 --> 01:31:59.040
like, you know, you have 100 billion, they're like, sorry, you have a billion like superintelligence

01:31:59.040 --> 01:32:02.880
scientists that they can like hack everything. They can like stuxnet the Chinese data centers,

01:32:02.880 --> 01:32:05.520
you know, they're starting to build the robo armies, you know, you like, you really think

01:32:05.520 --> 01:32:09.520
they'll be like a private company and the government would be like, oh my God, what is going on? You

01:32:09.520 --> 01:32:16.080
know, like, yeah. Suppose there's no China. Suppose there's people like Iran, North Korea,

01:32:16.080 --> 01:32:20.080
who theoretically at some point will be able to do superintelligence, but they're not on our heels

01:32:20.080 --> 01:32:24.480
and don't have the ability to be on our heels. In that world, are you advocating for the national

01:32:24.480 --> 01:32:30.160
project or do you prefer the private path forward? Yeah. So I mean, two responses to this. One is,

01:32:30.160 --> 01:32:34.160
I mean, you still have like Russia, you still have these other countries, you know, you've got to have

01:32:34.160 --> 01:32:38.560
Russia proof security, right? It's like, you can't, you can't just have Russia steal all your stuff.

01:32:38.560 --> 01:32:41.520
And like, maybe their clusters aren't going to be as big, but like, they're still going to be able

01:32:41.520 --> 01:32:46.160
to make the crazy bio weapons and the, you know, the mosquito-sized drones, you know, and so on.

01:32:46.160 --> 01:32:51.840
And so, I mean, I think, I think, I think the security component is just actually a pretty

01:32:51.920 --> 01:32:56.320
large component of the project in the sense of like, I currently do not see another way

01:32:56.320 --> 01:33:00.880
where we don't kind of like instantly proliferate this to everybody. And so, yeah. So I think it's

01:33:00.880 --> 01:33:03.920
sort of like, you still have to deal with Russia, you know, Iran, North Korea, and you know, like,

01:33:03.920 --> 01:33:06.960
you know, Saudi and Iran are going to be trying to get it because they want to screw each other,

01:33:06.960 --> 01:33:09.520
and you know, Pakistan and India, because they want to screw each other. There's like this enormous

01:33:09.520 --> 01:33:14.000
destabilization still. That said, look, I agree with you. If, if, you know, if, you know, by some,

01:33:14.000 --> 01:33:18.000
somehow things are shaking out differently, and like, you know, AGI would have been in 2005,

01:33:18.960 --> 01:33:23.200
you know, sort of like unparalleled, you know, American hegemony. I think there would have

01:33:23.200 --> 01:33:28.640
been more scope for less government involvement. But again, you know, as we're talking about

01:33:28.640 --> 01:33:32.000
earlier, I think that would have been sort of this like very unique moment in history. And I

01:33:32.000 --> 01:33:35.280
think basically, you know, almost all other moments in history, there would have been the

01:33:35.280 --> 01:33:42.640
sort of great power competitor. So, okay, so let's get into this debate. So I, my position here

01:33:42.640 --> 01:33:47.360
is if you look at the people who are involved in the Manhattan Project itself, many of them

01:33:47.360 --> 01:33:52.320
regretted their participation, as you said. Now we can infer from that that we should sort of

01:33:53.040 --> 01:34:00.320
start off with a cautious approach to the nationalized ASI project. Then you might say, well,

01:34:00.320 --> 01:34:04.720
listen, obviously they're super. Did they regret their participation because of the project or

01:34:04.720 --> 01:34:08.720
because of the technology itself? I think people will regret it. But I think it's, it's, it's about

01:34:08.720 --> 01:34:13.760
the nature of the technology, and it's not about project. I think they also probably had a sense

01:34:13.760 --> 01:34:18.160
that different decisions would have been made if it wasn't some concerted effort that everybody

01:34:18.160 --> 01:34:23.360
had agreed to participate in. That if it wasn't in the context of this, we need to race to beat

01:34:23.360 --> 01:34:28.080
Germany and Japan, you might not develop, so that's a technology part, but also like you wouldn't

01:34:28.080 --> 01:34:32.160
actually like hit them. It's like the sort of, the destructive potential, the sort of,

01:34:32.160 --> 01:34:35.760
you know, military potential, it's not, it's not because of the project, it is because of the

01:34:35.760 --> 01:34:42.080
technology. And that will unfold regardless. You know, I think this underrates the power of

01:34:42.960 --> 01:34:47.200
Imagine you go through like the 20th century in like, you know, a decade. You know, it's just

01:34:47.200 --> 01:34:51.360
the sort of, the sort of, yes, great technological progress. Let's just actually run to that example.

01:34:51.360 --> 01:34:54.880
So suppose you actually, there was some reason that the 20th century would be run through in one

01:34:54.880 --> 01:35:00.560
decade. Do you think the cause of that should have been, should have been like the technologies

01:35:00.560 --> 01:35:04.560
that happened through the 20th century shouldn't have been privatized? That it should have been a

01:35:04.560 --> 01:35:12.480
more sort of concerted government led project. You know, look, there is a history of just dual

01:35:12.480 --> 01:35:16.640
use technologies, right? And so I think AI in some sense is going to be dual use in the same way.

01:35:16.640 --> 01:35:20.400
And so there's going to be lots of civilian uses of it, right? Like nuclear energy, it's like

01:35:20.400 --> 01:35:23.040
itself, right? It was like, you know, there's the government project developed the military

01:35:23.040 --> 01:35:26.400
angle of it. And then, you know, it was like, you know, then the government worked with private

01:35:26.400 --> 01:35:29.440
companies, there's a sort of like real like flourishing of nuclear energy until you know,

01:35:29.440 --> 01:35:35.200
the environmentalists stopped it. You know, planes, right? Like Boeing, right? Actually,

01:35:35.200 --> 01:35:38.720
you know, the Manhattan project wasn't the biggest defense R&D project during World War II. It was

01:35:38.720 --> 01:35:42.320
the B-29 bomber, right? Because they needed the bomber that had long enough range to reach Japan

01:35:43.360 --> 01:35:47.360
to destroy their cities. And then, you know, Boeing made some Boeing made that,

01:35:47.360 --> 01:35:51.600
B Boeing made the B-47, made the B-52, you know, the plane the US military uses today.

01:35:51.600 --> 01:35:57.280
And then they use that technology later on to, you know, build the 707 and the sort of the

01:35:58.000 --> 01:36:04.240
later on mean this context because in the other, like I get what it means after a war to privatize,

01:36:04.240 --> 01:36:09.520
but if you have the government has ASI, maybe just let me back up and explain my concern.

01:36:09.520 --> 01:36:13.760
So you have the only institution in our society, which has a monopoly on violence.

01:36:15.040 --> 01:36:20.160
And then we're going to give the, give it some, in a way that's not broadly deployed,

01:36:20.160 --> 01:36:25.200
access to the ASI, the counterfactual, and this may be sound silly, but listen,

01:36:25.200 --> 01:36:30.160
we're going to go through higher and higher levels of intelligence. Private companies will

01:36:30.160 --> 01:36:35.280
be required by regulation to increase their security, but they'll still be private companies

01:36:35.280 --> 01:36:39.040
and they're deployed this and they're going to release the AGI, now McDonald's and JP

01:36:39.040 --> 01:36:42.320
Morgan and some random startup are now more effective organizations because they have a bunch

01:36:42.320 --> 01:36:47.040
of AGI workers. And it'll be sort of like the industrial revolution in the sense that the

01:36:47.040 --> 01:36:52.160
benefits were widely diffused. If you don't end up in a situation like that, then the,

01:36:53.120 --> 01:36:57.280
I mean, even backing up, like, what is it we're trying to, why do we want to win against China?

01:36:57.280 --> 01:37:02.560
We want to win against China because we don't want a top down authoritarian system to win.

01:37:03.120 --> 01:37:09.200
Now, if the way to beat that is that the most important technology that humanity will have

01:37:09.200 --> 01:37:15.840
has to be controlled by a top down government, like, what was the point? Like, maybe, so let's

01:37:15.840 --> 01:37:19.520
like run our cards with privatization. That's the way we get to the classic liberal

01:37:19.520 --> 01:37:23.920
market based system we want for the ESIs. Yeah. All right. So a lot of talk about here. Yeah.

01:37:23.920 --> 01:37:27.040
I think, yeah, maybe I'll start a bit about like actually looking at what the private world would

01:37:27.040 --> 01:37:30.640
look like. And I think this is part of where the sort of there's no alternative comes from.

01:37:30.640 --> 01:37:33.920
And then let's look like, look at like what the government project looks like, what checks and

01:37:33.920 --> 01:37:38.880
balances look like and so on. All right. Private world. And first of all, okay, so right, like a

01:37:38.880 --> 01:37:41.680
lot of people right now talk about open source. And I think there's this sort of misconception

01:37:41.680 --> 01:37:45.440
that like AGI development is going to be like, Oh, it's going to be some like beautiful decentralized

01:37:45.440 --> 01:37:48.880
thing. And you know, like, you know, some giddy community of coders who gets to like, you know,

01:37:48.880 --> 01:37:52.320
collaborate on it. That's not how it's going to look like, right? You know, it's, you know,

01:37:52.320 --> 01:37:55.200
$100 billion trillion dollar cluster. It's not going to be that many people that have it. The

01:37:55.200 --> 01:37:58.480
algorithms, you know, it's like right now, open source is kind of good, because people just use

01:37:58.480 --> 01:38:01.760
the stuff that was published. And so they basically, you know, the algorithms were published, or, you

01:38:01.760 --> 01:38:05.120
know, as Mistral, they just kind of like leave deep mind and take all the secrets with them. And

01:38:05.120 --> 01:38:10.800
they just kind of replicate it. That's not going to continue being the case. And so, you know,

01:38:10.800 --> 01:38:14.720
the sort of like open source, I mean, also people say stuff like, you know, 1026 flops,

01:38:14.720 --> 01:38:18.080
it will be in my phone, you know, it's no, it won't, you know, it's like Moore's Law is really

01:38:18.080 --> 01:38:21.600
slow. I mean, AI chips are getting better. But like, you know, the $100 billion computer will

01:38:21.600 --> 01:38:26.480
not cost, you know, like $1,000, you know, within your lifetime or whatever, aside from me. So anyway,

01:38:26.480 --> 01:38:32.720
so it's going to be, it's going to be like two or three, you know, big players on the private world.

01:38:33.840 --> 01:38:40.400
And so look, a few things. So first of all, you know, you talk about the sort of like, you know,

01:38:40.480 --> 01:38:44.560
enormous power that sort of superintelligence will have and the government will have.

01:38:46.080 --> 01:38:50.400
I think it's pretty plausible that the alternative world is that like one AI company has that power,

01:38:50.400 --> 01:38:52.880
right? And specifically, if we're talking about lead, you know, it's like, what, I don't know,

01:38:52.880 --> 01:38:56.800
open AI has a six month lead. And then, you know, so then you're not talking, you're talking about

01:38:56.800 --> 01:39:01.120
basically, you know, the most powerful weapon ever. And it's, you know, you're kind of making this

01:39:01.120 --> 01:39:05.600
like radical bet on like a private company CEO is the benevolent dictator. No, no, this is not

01:39:05.600 --> 01:39:09.760
necessarily like any other thing that's privatized, we don't account on them being benevolent. We

01:39:09.760 --> 01:39:15.520
just look to think of, for example, somebody who manufactures industrial fertilizer, right?

01:39:15.520 --> 01:39:20.880
This is the person with this factory, if they went back to an ancient civilization, they could

01:39:20.880 --> 01:39:26.320
like blow up Rome, they could probably blow up Washington DC. And I think in their series,

01:39:26.320 --> 01:39:30.800
you talk about Tyler Cowan's phrase of muddling through. And I think even with privatization,

01:39:30.800 --> 01:39:35.120
people sort of underrate that there are actually a lot of private actors who have the ability to

01:39:36.080 --> 01:39:41.200
there's a lot of people who control the water supply or whatever. And we can count on cooperation

01:39:41.200 --> 01:39:45.760
and market based incentives to basically keep a balance of power. I get that things are proceeding

01:39:45.760 --> 01:39:49.200
really fast. But we have a lot of historical evidence that this is the thing that works best.

01:39:49.200 --> 01:39:54.160
So look, I mean, what do we do with nukes, right? The way we keep the sort of nukes in check is not

01:39:54.160 --> 01:39:57.360
like, you know, a sort of beefed up second amendment where like each state has their own

01:39:57.360 --> 01:40:01.520
like little nuclear arsenal and like, you know, Dario and Sam have their own little nuclear arsenal.

01:40:01.520 --> 01:40:09.520
No, no, it's like, it's institutions, it's constitutions, it's laws, it's courts. And so I

01:40:09.520 --> 01:40:12.720
don't actually, I'm not sure that this, you know, I'm not sure that the sort of balance of power

01:40:12.720 --> 01:40:17.040
analogy holds. In fact, you know, sort of the government having the biggest guns was sort of

01:40:17.040 --> 01:40:20.800
like an enormous civilizational achievement, right? Like Landfrieden in the sort of Holy Roman

01:40:20.800 --> 01:40:24.320
Empire, right? You know, if somebody from the town over kind of committed a crime on you,

01:40:24.320 --> 01:40:28.320
you know, you didn't kind of start a sort of a, you know, a big battle between the two towns.

01:40:28.320 --> 01:40:32.640
No, you take it to a court of the Holy Roman Empire and they would decide. And it's a big

01:40:32.640 --> 01:40:36.480
achievement. Now, the thing about, you know, the industrial fertilizer, I think the key difference

01:40:36.480 --> 01:40:40.800
is kind of speed and often Stephen's balance issues, right? So it's like 20th century and,

01:40:40.800 --> 01:40:47.280
you know, 10 years in a few years. That is an incredibly scary period. And it is incredibly

01:40:47.280 --> 01:40:51.200
scary, you know, because it's, you know, you're going through just this sort of enormous array of

01:40:51.200 --> 01:40:55.680
destructive technology and the sort of like enormous amount of like, you know, basically

01:40:55.680 --> 01:40:59.360
military event. I mean, you would have gone from, you know, kind of like, you know, you know,

01:40:59.360 --> 01:41:04.000
bayonets and horses to kind of like tank armies and fighter jets in like a couple years and then

01:41:04.000 --> 01:41:07.760
from, you know, like, you know, and then to like, you know, nukes and, you know, ICBMs and stuff,

01:41:07.760 --> 01:41:14.000
you know, it's just like in a matter of years. And so it is sort of that speed that creates,

01:41:14.000 --> 01:41:16.960
I think basically the way I think about it is there's going to be this initial just incredibly

01:41:16.960 --> 01:41:21.200
volatile and incredibly dangerous period. And somehow we have to make it through that. That's

01:41:21.200 --> 01:41:26.240
going to be incredibly challenging. That's where you need the kind of government project.

01:41:26.240 --> 01:41:30.000
If you can make it through that, then you kind of go to like, now we can, now, you know, the

01:41:30.000 --> 01:41:33.360
situation has been stabilized, you know, we don't face this imminent national security threat.

01:41:33.360 --> 01:41:36.880
You know, it's like, yes, there were kind of WMDs that came along the way, but either we've managed

01:41:36.880 --> 01:41:40.400
to kind of like have a sort of stable offense, defense balance, right? Like I think bioweapons

01:41:40.400 --> 01:41:44.080
initially are a huge issue, right? Like an attacker can just create like a thousand different

01:41:44.080 --> 01:41:47.200
static, you know, viruses and spread them. And it's like going to be really hard for you to kind

01:41:47.200 --> 01:41:50.480
of like make a defense against each, but maybe at some point you figure out the kind of like,

01:41:50.560 --> 01:41:53.840
you know, universal defense against every possible virus. And then you're in a stable

01:41:53.840 --> 01:41:57.280
situation and on the offense, defense balance, or you do the thing, you know, you do with planes

01:41:57.280 --> 01:42:00.160
where it's there's like, you know, there's certain capabilities that the private sector

01:42:00.160 --> 01:42:03.840
isn't allowed to have. And you've like figured out what's going on, restrict those. And then you

01:42:03.840 --> 01:42:07.840
can kind of like let, let, you know, you let this sort of civilian, civilian uses.

01:42:07.840 --> 01:42:14.640
So I'm skeptical of this because well, there's sorry, I mean, the other important thing is,

01:42:14.640 --> 01:42:18.160
so I talked about the sort of, you know, maybe it's like, it's, it's a, you know,

01:42:18.960 --> 01:42:21.920
it's, you know, it's one company with all this power. And I think it's like, I think it is

01:42:21.920 --> 01:42:25.760
unprecedented because it's like the industrial fertilizer guy cannot overthrow the US government.

01:42:25.760 --> 01:42:29.280
I think it's quite plausible that like the AI company with super intelligence can overthrow

01:42:29.280 --> 01:42:32.240
the multiple companies, right? And I buy that one of them could be ahead.

01:42:32.240 --> 01:42:36.080
So it's not obvious that it'll be multiple. I think it's again, if there's like a six monthly,

01:42:36.080 --> 01:42:39.680
maybe, maybe there's two or three, but if there's two or three, then what you have is just like

01:42:39.680 --> 01:42:43.200
the crazy race between these two or three companies, you know, it's like, you know, whatever,

01:42:43.200 --> 01:42:46.400
Demis and Sam, they're just like, I don't want to let the other one win.

01:42:46.400 --> 01:42:50.320
And, and they're both developing their nuclear arsenals in the road. It's just like, also like,

01:42:50.320 --> 01:42:52.800
come on, the government is not going to let these people, you know, are they going to let,

01:42:52.800 --> 01:42:56.080
like, you know, is Dario going to be the one developing the kind of like, you know,

01:42:56.080 --> 01:42:59.680
you know, super hacking Stuxnet and like deploying against the Chinese data center.

01:42:59.680 --> 01:43:03.760
The other issue though, is it won't just, if it's two or three, it won't just be two or three.

01:43:03.760 --> 01:43:07.200
There'll be two or three and it'll be China and Russia and North Korea because the private

01:43:07.200 --> 01:43:11.040
and the private lab world, there's no way they'll have security that is good enough.

01:43:11.040 --> 01:43:15.120
I think we're also assuming that somehow if you nationalize it, like the security just,

01:43:15.760 --> 01:43:20.880
especially in the world, where it did this stuff is priced in by the CCP,

01:43:20.880 --> 01:43:25.520
that now you've like got it nailed down. And I'm not sure why we would expect that to be the case.

01:43:25.520 --> 01:43:27.760
But on this, government's the only one who does this stuff.

01:43:27.760 --> 01:43:32.080
So if it's not Sam or Dario, who's, we don't want to trust them to be benevolent,

01:43:32.080 --> 01:43:40.160
dictator or whatever. So by here, we're counting on, if it's, because you can

01:43:40.160 --> 01:43:43.120
cause a coup, the same capabilities are going to be true of the government project, right?

01:43:43.120 --> 01:43:49.680
And so the modal president in 2020, 2025, but Donald Trump will be the person that you don't

01:43:49.680 --> 01:43:55.280
trust Sam or Dario to have these capabilities. And why, okay, I agree that like I'm worried

01:43:55.280 --> 01:44:00.480
if the Sam or Dario have a one year lead on ASI in that, in that world, then I'm like concerned

01:44:00.480 --> 01:44:05.360
about this being privatized. But in that exact same world, I'm very concerned about Donald

01:44:05.360 --> 01:44:09.120
Trump having the capability. And potentially if we're living in a world where the takeoff

01:44:09.120 --> 01:44:12.960
is slower than you anticipate, in that world, I'm like very much I want the private companies.

01:44:12.960 --> 01:44:17.280
So like in no part of this matrix, is it obviously true that the government

01:44:17.280 --> 01:44:19.920
led project is better than the private project? Let's talk about the government

01:44:19.920 --> 01:44:23.600
project a little bit and checks and balances. In some sense, I think my argument is a sort

01:44:23.600 --> 01:44:28.720
of brookian argument, which is like American checks and balances have held for over 200 years

01:44:28.720 --> 01:44:33.600
and through crazy technological revolutions. The US military could kill like every civilian

01:44:33.600 --> 01:44:36.240
in the United States. But you're going to make that argument, the private public

01:44:36.240 --> 01:44:40.640
balance of power itself for hundreds of years. But yeah, why has it held? Because the government

01:44:40.640 --> 01:44:45.600
has the biggest guns and has never before has a single CEO or a random nonprofit board

01:44:45.600 --> 01:44:50.400
had the ability to launch nukes. And so again, it's like, you know, what is the track record

01:44:50.400 --> 01:44:53.120
of the government checks and balances versus the track record of the private company checks

01:44:53.120 --> 01:44:57.120
and balances? Well, the iLab, you know, like first stress test, you know, went really badly,

01:44:57.120 --> 01:45:02.480
you know, that didn't really work, you know? I mean, even worse in the sort of private

01:45:02.480 --> 01:45:07.040
company world. So it's both like, it is like the two private companies and the CCP and they just

01:45:07.040 --> 01:45:11.200
like instantly have all the shit. And then it's, you know, they probably won't have good enough

01:45:11.200 --> 01:45:14.960
internal control. So it's like, not just like the random CEO, but it's like, you know, rogue

01:45:14.960 --> 01:45:18.240
employees that can kind of like use these super intelligences to do whatever they want.

01:45:18.240 --> 01:45:22.720
And this won't be true of the government? Like the rogue employees won't exist on the project?

01:45:22.720 --> 01:45:27.360
Well, the government actually like, you know, has decades of experience and like actually

01:45:27.360 --> 01:45:30.400
really cares about the stuff. I mean, it's like they deal, they deal with nukes, they deal with

01:45:30.400 --> 01:45:34.160
really powerful technology. And it's, you know, this is like, this is the stuff that the national

01:45:34.160 --> 01:45:37.680
security state cares about. You know, again, to the go, let's talk about the government checks

01:45:37.680 --> 01:45:41.440
and balances a little bit. So, you know, what are checks and balances in the government world?

01:45:41.440 --> 01:45:43.920
First of all, I think it's actually quite important that you have some amount of

01:45:43.920 --> 01:45:47.040
international coalition. And I talked about these sort of two tiers before. Basically,

01:45:47.040 --> 01:45:50.240
I think the inner tier is a sort of modeled on the cool record agreement, right? This was like

01:45:50.240 --> 01:45:55.840
Churchill and Roosevelt, they kind of agreed secretly. We're going to like pull our efforts

01:45:55.840 --> 01:45:58.880
on nukes, but we're not going to use them against each other. And we're not going to use them

01:45:58.880 --> 01:46:02.640
against anyone else with their consent. And I think basically look, bring in, bring in the UK,

01:46:02.720 --> 01:46:06.000
they have deep mind bringing in the kind of like Southeast Asian states who have the chip supply

01:46:06.000 --> 01:46:10.560
chain, bring in some more of kind of like NATO close democratic allies for, you know, talent and

01:46:10.560 --> 01:46:14.240
industrial resources. And you have the sort of like, you know, so you have, you have those checks

01:46:14.240 --> 01:46:19.440
and balances in terms of like more international countries at the table. Sorry, somewhat separately,

01:46:19.440 --> 01:46:22.640
but then you have the sort of second tier of coalitions, which is the sort of Adams for peace

01:46:22.640 --> 01:46:26.000
thing, where you go to a bunch of countries, including like the UAE, and you're like, look,

01:46:26.000 --> 01:46:29.600
we're going to basically like, you know, there's a deal similar to like the NPT stuff where it's

01:46:30.080 --> 01:46:33.920
you're not allowed to like do the crazy military stuff, but we're going to share the civilian

01:46:33.920 --> 01:46:38.240
applications. We're in fact going to help you and share the benefits and, you know, sort of kind

01:46:38.240 --> 01:46:42.240
of like this new sort of post superintelligence world order. All right, US checks and balances,

01:46:42.240 --> 01:46:45.360
right? So obviously, Congress is going to have to be involved, right? Appropriate in

01:46:45.360 --> 01:46:49.600
trillions of dollars. I think probably ideally you have Congress needs to kind of like confirm

01:46:49.600 --> 01:46:53.200
whoever's running this. So you have Congress, you have like different factions of the government,

01:46:53.200 --> 01:46:56.800
you have the courts, I expect the First Amendment to continue being really important. And maybe that,

01:46:56.800 --> 01:46:59.520
I think that sounds kind of crazy to people, but I actually think, again, I think these are

01:47:00.160 --> 01:47:04.160
institutions that have held this test of time in a really sort of powerful way.

01:47:05.040 --> 01:47:08.480
You know, eventually, you know, this is why honestly, alignment is important is like,

01:47:08.480 --> 01:47:12.800
you know, the AIs, you program the AIs to follow the Constitution. And it's like, you know,

01:47:12.800 --> 01:47:18.480
why does the military work? It's like generals, you know, are not allowed to follow unlawful

01:47:18.480 --> 01:47:22.320
orders or not allowed to follow unconstitutional orders. You have the same thing for the AIs.

01:47:22.320 --> 01:47:26.640
So what's wrong with this argument? When you say, listen, maybe you have a point in the world

01:47:26.720 --> 01:47:29.840
where we have extremely fast takeoff. It's like one year from AGI to ASI.

01:47:29.840 --> 01:47:33.920
Yeah. And then you have the like, years after ASI, where you have this like,

01:47:33.920 --> 01:47:39.040
extraordinary explosion. Maybe you have a point. We don't know, you have these arguments,

01:47:39.040 --> 01:47:41.760
we'll like get into the weeds on them about why that's a more likely world, but like maybe

01:47:41.760 --> 01:47:46.720
that's not the world we live in. And in the other world, I'm like, very on the side of making sure

01:47:46.720 --> 01:47:54.560
that these things are privately held. Now, when you nationalize, that's a one way function,

01:47:54.560 --> 01:48:00.400
you can't go back. Why not wait until we have more evidence on which of those worlds we live in?

01:48:02.080 --> 01:48:05.920
And I think like rushing on the nationalization might be a bad idea while we're not sure.

01:48:06.560 --> 01:48:10.400
And okay, I'll just respond to that first. I mean, I don't, I don't expect us to nationalize

01:48:10.400 --> 01:48:13.840
tomorrow. If anything, I expected to be kind of with COVID where it's like kind of too late,

01:48:13.840 --> 01:48:17.360
like ideally you nationalize it early enough to like actually lock stuff down,

01:48:17.360 --> 01:48:20.240
it'll probably be kind of chaotic and like, you know, you're going to be trying to like do this

01:48:20.240 --> 01:48:23.520
crash program to lock stuff down and it'll be kind of late and it'll be kind of clear what's

01:48:23.520 --> 01:48:26.800
happening. We're not going to nationalize when it's not clear what's happening. I think the whole

01:48:28.080 --> 01:48:31.360
historically these institutions have held up well. First of all, they've actually

01:48:31.360 --> 01:48:37.040
almost broken a bunch of times. It's like, this is the argument that some people who are saying

01:48:37.040 --> 01:48:40.080
that we shouldn't be that concerned about nuclear war say, where it's like, listen,

01:48:40.080 --> 01:48:44.400
we have the nuke for 80 years and like we've been fine so far, so the risk must be low.

01:48:44.400 --> 01:48:49.120
And then the answer to that is no, actually, it is a really high risk. And the reason we've avoided

01:48:49.120 --> 01:48:53.680
it is like people have gone through a lot of effort to make sure that this thing doesn't happen.

01:48:53.680 --> 01:49:00.400
I don't think that giving government ASI without knowing what that implies is going through the

01:49:00.400 --> 01:49:05.440
lot of effort. And I think the base rate, like you can talk about America, I think America is

01:49:05.440 --> 01:49:10.320
very exceptional, not just in terms of dictatorship, but in terms of every other country in history

01:49:10.320 --> 01:49:15.360
has had a complete drawdown of wealth because of war, revolution, something. America is very

01:49:15.360 --> 01:49:18.960
unique in not having that. And the historical base rate, we're talking about Greek power

01:49:18.960 --> 01:49:22.000
competition. I think that has a really big, that's something we haven't been thinking about

01:49:22.000 --> 01:49:26.240
the last 80 years, but it's really big. Dictatorship is also something that is just

01:49:26.240 --> 01:49:33.680
DD, false state of mankind. And I think relying on institutions, which in an ASI world,

01:49:35.360 --> 01:49:40.240
it's fundamentally right now, if the government tried to overthrow, it's much harder if you

01:49:40.240 --> 01:49:45.760
don't have the ASI, right? Like there's people who have AR-15s and there's like things that

01:49:45.760 --> 01:49:48.960
had to make it harder. If government could crush the AR-15s. No, I think it'd actually be pretty

01:49:48.960 --> 01:49:51.840
hard. The reason it was Vietnam and Afghanistan were pretty hard. They're just a new whole country.

01:49:52.960 --> 01:49:56.240
Yeah, yeah, I agree, but like I'm... They could. I mean similar with ASI.

01:49:57.920 --> 01:50:01.200
Yeah, I think it's just like easier if you have what you're talking about. But there are institutions,

01:50:01.200 --> 01:50:04.800
there are constitutions, there are legal restraints, there are courts, there are checks and balances.

01:50:05.680 --> 01:50:09.600
The crazy bet is the bet which are like private company CEOs. The same thing, by the way,

01:50:09.600 --> 01:50:12.800
isn't the same thing true of nukes where we have these institutional agreements about

01:50:12.800 --> 01:50:16.960
non-polar aspiration and whatever. And we're still very concerned about that being broken

01:50:16.960 --> 01:50:19.360
and somebody getting nukes and like you should stay up that night worrying about that.

01:50:19.360 --> 01:50:24.720
It's a precarious situation, but ASI is going to be a really precarious situation as well. And like

01:50:24.720 --> 01:50:27.040
given how precarious nukes are, we've done pretty well.

01:50:27.040 --> 01:50:29.040
And so what does privatization in this world even mean?

01:50:29.040 --> 01:50:30.240
I mean, I think the other thing is...

01:50:30.240 --> 01:50:31.120
Like what happened after?

01:50:31.120 --> 01:50:33.280
I mean, the other thing, you know, because we're talking about like whether the government

01:50:33.280 --> 01:50:36.640
project is good or not. And it's like, I have very mixed feelings about this as well.

01:50:37.200 --> 01:50:39.200
Again, I think my primary argument is like,

01:50:40.720 --> 01:50:43.120
you know, if you're at the point where this thing has like

01:50:44.240 --> 01:50:47.920
vastly superhuman hacking capabilities, if you're at the point where this thing can develop,

01:50:47.920 --> 01:50:51.200
you know, bio weapons, you know, like in crazy bio weapons, ones that are like targeted,

01:50:51.200 --> 01:50:54.240
you know, can kill everybody but the hand Chinese or, you know, that, you know,

01:50:55.280 --> 01:50:58.800
would wipe out, you know, entire countries, where you're talking about like building Robo

01:50:58.800 --> 01:51:02.400
Armors, you're talking about kind of like drone swarms that are, you know, again,

01:51:02.400 --> 01:51:04.400
the mosquito sized drones that could take it out, you know,

01:51:07.600 --> 01:51:10.800
the United States national security state is going to be intimately involved with this.

01:51:10.800 --> 01:51:13.600
And this will, you know, the labs, whether, you know, and I think, again, the government,

01:51:13.600 --> 01:51:16.640
a lot of what I think is the government project looks like, it is basically a joint venture

01:51:16.640 --> 01:51:20.400
between like, you know, the cloud providers between some of the labs and the government.

01:51:20.400 --> 01:51:25.120
And so I think there is no world in which the government isn't intimately involved in this

01:51:25.120 --> 01:51:28.640
like crazy period. The very least, basically, you know, like the intelligence agencies need

01:51:28.640 --> 01:51:31.600
to be running security for these labs. So they're already kind of like, they're controlling

01:51:31.600 --> 01:51:34.960
everything, they're controlling access to everything. Then they're going to be like,

01:51:34.960 --> 01:51:38.400
probably again, if we're in this like really volatile international situation, like a lot

01:51:38.400 --> 01:51:41.760
of the initial applications, it'll, it'll suck. It's not what I want to use ASI for,

01:51:41.760 --> 01:51:46.880
will be like trying to somehow stabilize this crazy situation. Somehow we need to prevent

01:51:46.880 --> 01:51:51.040
like proliferation of like some crazy new WMDs and like the undermining of mutually assured

01:51:51.040 --> 01:51:57.840
destruction to kind of like, you know, North Korea and Russia and China. And so I think,

01:51:57.920 --> 01:52:02.960
you know, I basically think your world, you know, I think there's much more spectrum than

01:52:02.960 --> 01:52:06.080
you're acknowledging here. And I think basically the world in which it's private labs is like

01:52:06.080 --> 01:52:09.280
extremely heavy government involvement. And really what we're debating is like, you know,

01:52:09.280 --> 01:52:12.800
what form of government project, but it is going to look much more like, you know,

01:52:12.800 --> 01:52:17.280
the national security state than anything it does look like, like a startup as it is right now.

01:52:17.280 --> 01:52:18.480
And I think the, yeah.

01:52:18.480 --> 01:52:23.120
Look, I think something like that makes sense. I would be, if it's like the Manhattan Project,

01:52:23.200 --> 01:52:27.040
then I'm very worried where it's like, this is part of the U.S. military.

01:52:28.480 --> 01:52:31.920
Where I've hit some more like, listen, you got to talk to Jake Sullivan before you

01:52:31.920 --> 01:52:33.120
like run the next training one.

01:52:33.120 --> 01:52:36.960
It's like Lockheed Martin, Skunkward's part of the U.S. military. It's like, they call the shops.

01:52:37.520 --> 01:52:40.560
Yeah, I don't think that's great. I think that's, I think that's bad. I think it would be bad if

01:52:40.560 --> 01:52:42.800
that happened with ASI. And like, what is it? What is the scenario?

01:52:44.400 --> 01:52:45.520
What is the alternative?

01:52:45.520 --> 01:52:48.640
Okay. So it's closer to my end of the spectrum where,

01:52:48.640 --> 01:52:52.640
yeah, you do have to talk to Jake Sullivan before you can launch the next training cluster.

01:52:52.640 --> 01:52:55.680
But there's many companies who are still going for it.

01:52:55.680 --> 01:52:58.720
And the government will be intimately involved in the security.

01:53:00.720 --> 01:53:01.920
But the like, three different companies are trying to-

01:53:01.920 --> 01:53:03.360
Is Dario launching the Stuxnet attack?

01:53:04.080 --> 01:53:06.640
Yeah. What are you, what are you, launching, launching. Okay.

01:53:07.520 --> 01:53:10.560
So Dario's activating the Chinese data centers.

01:53:10.560 --> 01:53:12.960
I think this is similar to the story you could tell about, there's a lot of,

01:53:12.960 --> 01:53:17.440
like literally the big tech right now. I think Satya, if you wanted to, he probably

01:53:17.440 --> 01:53:20.000
like could get his engineers like, what are the zero days in Windows?

01:53:20.960 --> 01:53:25.040
And like, well, how do we get infiltrate the president's computer so that like-

01:53:25.040 --> 01:53:25.920
Maybe shut down.

01:53:26.560 --> 01:53:28.720
No, no, no. Like right now I'm saying Satya could do that, right?

01:53:28.720 --> 01:53:29.120
Because he knows-

01:53:29.120 --> 01:53:30.000
Maybe shut down.

01:53:30.000 --> 01:53:30.720
What do you mean?

01:53:30.720 --> 01:53:31.840
Government wouldn't let them do that.

01:53:33.280 --> 01:53:36.000
Yeah. I think there's a story you could tell where like they could pull off a coup,

01:53:36.000 --> 01:53:37.040
whatever. But like, I think there's like-

01:53:37.040 --> 01:53:38.560
Maybe not pull off a coup.

01:53:38.560 --> 01:53:38.800
Okay.

01:53:38.800 --> 01:53:40.320
Maybe not pull off, come on.

01:53:40.320 --> 01:53:43.120
Okay. Fine, fine, fine. I agree. I'm just saying like something closer to,

01:53:43.920 --> 01:53:49.520
so what's wrong with the scenario where you, the government is, there's like multiple

01:53:49.520 --> 01:53:54.880
companies going for it, but the AI is still broadly deployed and alignment works in the

01:53:54.880 --> 01:53:58.240
sense that you can make sure that it's not, the system level prompt is like,

01:53:58.240 --> 01:54:02.320
you can't help people make bio weapons or something, but these are still broadly deployed.

01:54:02.320 --> 01:54:03.120
So that-

01:54:03.120 --> 01:54:05.280
I mean, I expect the AIs to be broadly deployed. I mean, first of all-

01:54:05.280 --> 01:54:06.240
Even if it's a government project?

01:54:06.240 --> 01:54:08.720
Yeah. I mean, look, I think first of all, like, I think the matters of the world,

01:54:08.720 --> 01:54:11.520
you know, open sourcing their eyes, you know, that are two years behind or whatever.

01:54:11.520 --> 01:54:15.760
Yeah. Super valuable role. They're gonna like, you know, and so there's gonna be some question

01:54:15.760 --> 01:54:19.600
of like, either the offense, defense balance is fine. And so like, even if they open sourced

01:54:19.600 --> 01:54:22.880
two year old AIs, it's fine. Or it's like, there's some restrictions on the most extreme dual use

01:54:22.880 --> 01:54:26.080
capabilities, like, you know, you don't let private companies sell kind of crazy weapons.

01:54:26.800 --> 01:54:30.720
And that's great. And that will help with the diffusion. And, you know, after the government

01:54:30.720 --> 01:54:33.760
project, you know, there's gonna be this initial tense period, hopefully that's stabilized.

01:54:33.760 --> 01:54:36.320
And then look, yeah, like Boeing, they're gonna go out and they're gonna like make,

01:54:37.280 --> 01:54:41.040
do all the flourishing civilian applications and, you know, like nuclear energy, you know,

01:54:41.040 --> 01:54:45.360
it'll like all the civilian applications will have their day. I think part of my argument here is that-

01:54:45.360 --> 01:54:49.200
And how does that proceed, right? Because in the other world, there's existing stocks of capital

01:54:49.200 --> 01:54:52.720
that are worth a lot. Yeah, the clusters, they'll be still be Google clusters.

01:54:52.720 --> 01:54:56.880
And so Google, because they got the contract from the government, they'll be the ones that control

01:54:56.880 --> 01:55:01.040
the ASI. But like, why are they trading with anybody else? Why is there a random start up again?

01:55:01.040 --> 01:55:04.000
It'll be the same. It'll be the same companies that would be doing it anyway. But in this,

01:55:04.000 --> 01:55:07.600
in this world, they're just contracting with the government or like their DPA for all their compute

01:55:07.600 --> 01:55:13.360
goes to the government. And, but in the world, it's very natural. It's like sort of how

01:55:14.320 --> 01:55:18.320
after you get the ASI and we're building the robot armies and building fusion reactors or whatever,

01:55:18.800 --> 01:55:22.080
that the, that's- Only the government will get to build robot armies.

01:55:22.800 --> 01:55:26.160
Yeah, that one worried. Or like the fusion reactors and stuff.

01:55:26.160 --> 01:55:28.800
That's what we do with this, because- It's the same situation we have today.

01:55:28.800 --> 01:55:31.920
Because if you already have the robot armies and everything, like the existing society doesn't

01:55:31.920 --> 01:55:35.200
have some leverage where it makes sense to the government to- But they don't have that today.

01:55:35.200 --> 01:55:38.560
Yeah, they get in the sense that there's like a lot of capital that the government wants and

01:55:38.560 --> 01:55:41.360
there's other things like, why was Boeing privatized after?

01:55:41.360 --> 01:55:44.080
Government has the biggest guns. And the way we regulate is institutions,

01:55:44.080 --> 01:55:45.440
constitutions, legal restraints.

01:55:45.440 --> 01:55:48.560
Oh, because tell me what privatization should look like in the ASI world afterwards.

01:55:48.560 --> 01:55:51.440
Afterwards. Like the Boeing example, right? It's like, you have this government-

01:55:51.440 --> 01:55:54.080
Who gets it? Like Google, Microsoft, and last year-

01:55:54.080 --> 01:55:56.160
And who are they selling it to? Like they already have the robot factory.

01:55:56.160 --> 01:55:58.240
And then look at- Why are they selling it to us? Like they already have the,

01:55:58.240 --> 01:56:01.680
they don't need like our, this is chum change in the ASI world.

01:56:01.680 --> 01:56:05.920
Because we didn't get like the ASI broadly deployed throughout this takeoff.

01:56:05.920 --> 01:56:09.520
So we don't have the robot. We don't have like the fusion reactors and whatever advanced,

01:56:09.520 --> 01:56:11.840
decades of advanced science that you were talking about.

01:56:11.840 --> 01:56:13.840
So like it just, what are they trading with us for?

01:56:14.800 --> 01:56:16.000
Trading with whom for?

01:56:16.000 --> 01:56:17.600
Everybody who was not part of the project.

01:56:17.600 --> 01:56:19.360
They've got that technology that's decades ahead.

01:56:19.360 --> 01:56:21.040
Yeah. I mean, look, that's a whole nother issue of like,

01:56:21.040 --> 01:56:24.240
how does like economic distribution work or whatever? I don't know. That'll be rough.

01:56:25.280 --> 01:56:26.560
Yeah. I think-

01:56:26.560 --> 01:56:28.800
I'm just saying, I don't, I don't, basically I'm kind of like,

01:56:28.800 --> 01:56:30.880
I don't see the alternative. The alternative is,

01:56:30.880 --> 01:56:34.400
you like overturn a 500 year civilizational achievement of land fleeting.

01:56:34.400 --> 01:56:38.000
You basically instantly leak the stuff to the CCP.

01:56:38.000 --> 01:56:42.240
And either you like barely scrape out ahead and, but you're in this fever struggle,

01:56:42.240 --> 01:56:44.000
you're like proliferating crazy WMDs.

01:56:44.000 --> 01:56:45.920
It's just like enormously dangerous situation,

01:56:45.920 --> 01:56:47.120
enormously dangerous on alignment,

01:56:47.120 --> 01:56:49.280
because you're in this kind of like crazy race at the end.

01:56:49.280 --> 01:56:51.760
And you don't have the ability to like take six months to get alignment.

01:56:51.760 --> 01:56:55.760
Right. The alternative is, you know,

01:56:55.760 --> 01:56:58.560
alternative is like you aren't actually bundling your efforts to kind of like win

01:56:58.560 --> 01:57:00.080
the race against the authoritarian powers.

01:57:00.160 --> 01:57:02.480
You know, yeah. And so,

01:57:05.440 --> 01:57:09.520
you know, I don't like it. You know, I wish,

01:57:09.520 --> 01:57:11.760
I wish the thing we use the ASI for is to like,

01:57:11.760 --> 01:57:14.160
you know, cure the diseases and do all the good in the world.

01:57:14.160 --> 01:57:18.560
But it is my prediction that sort of like by the, in the end game,

01:57:21.520 --> 01:57:24.320
what will be at stake will not just be kind of cool products,

01:57:24.320 --> 01:57:27.840
but what will be at stake is like whether liberal democracy survives,

01:57:27.840 --> 01:57:32.320
like whether the CCP survives, like what the world order for the next century will be.

01:57:32.320 --> 01:57:34.000
And when that is at stake,

01:57:34.000 --> 01:57:37.680
forces will be activated that are sort of way beyond what we're talking about now.

01:57:37.680 --> 01:57:41.840
And like, you know, in the sort of like crazy race at the end,

01:57:42.400 --> 01:57:45.360
like the sort of national security implications will be the most important,

01:57:45.360 --> 01:57:48.000
you know, sort of like, you know, World War II, it's like, yeah, you know,

01:57:48.640 --> 01:57:49.840
nuclear energy, how did it stay?

01:57:49.840 --> 01:57:53.040
But in the initial kind of period, when, you know,

01:57:53.040 --> 01:57:54.560
when this technology was first discovered,

01:57:54.560 --> 01:57:56.480
you had to stabilize the situation, you had to get nukes,

01:57:56.480 --> 01:57:57.280
you had to do it right.

01:57:58.560 --> 01:58:00.960
And then, and then the civilian applications out there.

01:58:00.960 --> 01:58:03.760
I think of closer analogy to what this is, because nuclear,

01:58:03.760 --> 01:58:05.920
I agree that nuclear energy is the thing that happens in Iran,

01:58:05.920 --> 01:58:06.960
and it's like dual use in that way.

01:58:06.960 --> 01:58:09.600
But it's, it's something that happened like literally a decade after nuclear weapons were

01:58:09.600 --> 01:58:14.080
developed, whereas with AI, like the immediately all the applications are unlocked.

01:58:14.080 --> 01:58:17.680
And it's closer to literally, I mean, this is analogy people are supposed to make in

01:58:17.680 --> 01:58:23.280
the context of AGI is like, assume your society had 100 million more John Wayne Neumanns.

01:58:23.280 --> 01:58:25.680
And I don't think like, if that was literally what happened,

01:58:25.760 --> 01:58:28.720
if tomorrow you just have 100 million more of them, the approach should have been,

01:58:28.720 --> 01:58:30.240
well, some of them will convert to ISIS.

01:58:30.240 --> 01:58:32.400
And we need to like be really careful about that.

01:58:32.400 --> 01:58:35.680
And then like, oh, you know, like what if a bunch of them are born in China?

01:58:35.680 --> 01:58:38.880
And then we like, if we got to nationalize the John Wayne Neumanns,

01:58:38.880 --> 01:58:41.440
I'm like, no, I think it'll be generally a good thing.

01:58:41.440 --> 01:58:44.960
And I'd be concerned about one power had getting like all the John Wayne Neumanns.

01:58:44.960 --> 01:58:48.240
I mean, I think the issue is the sort of like bottling up in the sort of intensely short

01:58:48.240 --> 01:58:50.880
period of time, like this enormous sort of like, you know,

01:58:52.000 --> 01:58:55.120
unfolding of technological progress of an industrial explosion.

01:58:55.120 --> 01:58:57.280
And I think we do worry about the 100 million John Wayne Neumanns.

01:58:57.280 --> 01:58:58.320
It's like rise of China.

01:58:58.320 --> 01:58:59.920
Why are we worried about the rise of China?

01:58:59.920 --> 01:59:03.440
Because it's like 100 billion people and they're able to do a lot of industry

01:59:03.440 --> 01:59:04.640
and do a lot of technology.

01:59:04.640 --> 01:59:07.680
And but it's just like, you know, the rise of China times like, you know, 100,

01:59:07.680 --> 01:59:09.120
because it's not just 101 billion people.

01:59:09.120 --> 01:59:13.280
It's like a billion super intelligent, crazy, you know, crazy things.

01:59:13.280 --> 01:59:15.360
And in like, you know, a very short period.

01:59:16.160 --> 01:59:20.800
Let's start practically, because if the goal is we need to beat China,

01:59:20.800 --> 01:59:21.840
part of that is protecting.

01:59:21.840 --> 01:59:22.800
I mean, that's one of the goals, right?

01:59:22.800 --> 01:59:23.440
Yeah, I agree.

01:59:23.440 --> 01:59:23.760
I agree.

01:59:23.760 --> 01:59:24.880
One of the goals is to beat China.

01:59:24.960 --> 01:59:28.080
And also manage this incredibly crazy, scary period.

01:59:28.080 --> 01:59:28.240
Yeah.

01:59:28.240 --> 01:59:28.560
Right.

01:59:28.560 --> 01:59:31.760
So part of that is making sure we're not leaking algorithmic secrets to them.

01:59:31.760 --> 01:59:32.240
Yep.

01:59:32.240 --> 01:59:33.680
Part of that is a cluster.

01:59:34.320 --> 01:59:35.600
I mean, building the trillion dollar cluster.

01:59:35.600 --> 01:59:35.920
That's right.

01:59:35.920 --> 01:59:36.000
Right.

01:59:36.000 --> 01:59:36.240
Yeah.

01:59:36.240 --> 01:59:40.000
But like your whole point, the Microsoft can release corporate bonds that are.

01:59:40.000 --> 01:59:41.760
I think Microsoft can do the like hundreds of billions.

01:59:41.760 --> 01:59:41.920
Yeah.

01:59:41.920 --> 01:59:45.120
I think I think the trillion dollar cluster is closer to a national effort.

01:59:45.840 --> 01:59:49.200
I thought that your earlier point was that American capital markets are deep.

01:59:49.200 --> 01:59:49.600
They're good.

01:59:49.600 --> 01:59:50.160
They're pretty good.

01:59:50.160 --> 01:59:52.400
I mean, I think the trillion, I think it's possible it's private.

01:59:52.400 --> 01:59:52.960
It's possible.

01:59:53.280 --> 01:59:57.520
But it's going to be like, you know, by the way, at this point, we have a AGI that's

01:59:57.520 --> 01:59:58.880
rapidly accelerating productivity.

01:59:58.880 --> 02:00:01.840
I think the trillion dollar cluster is going to be planned before before the AGI.

02:00:02.880 --> 02:00:06.880
I think it's sort of like you get the AGI on the like 10 gigawatt cluster, like intelligent.

02:00:06.880 --> 02:00:09.600
Maybe you have like one more year where you're kind of doing some final on hobbling to fully

02:00:09.600 --> 02:00:10.240
unlock it.

02:00:10.240 --> 02:00:11.840
Then you have the intelligence explosion.

02:00:11.840 --> 02:00:14.000
And meanwhile, the like trillion dollar cluster is almost finished.

02:00:14.000 --> 02:00:16.880
And then you like, and then you do your super intelligence on your trillion dollar cluster,

02:00:16.880 --> 02:00:18.480
or you run it on your trillion dollar cluster.

02:00:18.480 --> 02:00:21.040
And by the way, you have not just your trillion dollar cluster, but like, you know,

02:00:21.120 --> 02:00:23.600
hundreds of millions of GPUs on inference clusters everywhere.

02:00:23.600 --> 02:00:27.200
And this isn't result, like, I think private, in this world, I think private companies have

02:00:27.200 --> 02:00:28.800
the capital and can raise capital to do it.

02:00:28.800 --> 02:00:31.120
I think you will need the government force to do it fast.

02:00:31.120 --> 02:00:35.520
Well, I was just about to ask, like, wouldn't it be the, like, we know company companies are

02:00:35.520 --> 02:00:41.440
on track to be able to do this and China, if they're unhindered by climate pledges or whatever.

02:00:41.440 --> 02:00:42.400
Well, that's part of what I'm saying.

02:00:42.400 --> 02:00:48.160
So if that's the case, if it really matters that we beat China, there's all kinds of

02:00:48.160 --> 02:00:49.360
practical difficulties of like,

02:00:49.920 --> 02:00:53.680
will the AI researchers actually join the AI effort?

02:00:53.680 --> 02:00:58.480
If they do, there's going to be three different teams at least who are currently doing

02:00:58.480 --> 02:01:02.480
private pre-training on different, different companies.

02:01:02.480 --> 02:01:06.880
Now who decides, at some point, you're going to have the, you're like YOLO, the hyperparameters

02:01:06.880 --> 02:01:11.040
of the trillion dollar cluster, who decides that?

02:01:11.040 --> 02:01:15.920
Just like merging extremely complicated research and development processes

02:01:15.920 --> 02:01:17.360
across very different organizations.

02:01:18.320 --> 02:01:21.280
This is how it's supposed to speed up America against the Chinese.

02:01:21.280 --> 02:01:22.480
Like, why don't we just let...

02:01:22.480 --> 02:01:23.520
Brain and deep mind merge.

02:01:23.520 --> 02:01:24.720
And it was like a little messy, but it was fine.

02:01:24.720 --> 02:01:25.520
It was pretty messy.

02:01:25.520 --> 02:01:28.560
And it was also the same company and also much earlier on in the process.

02:01:28.560 --> 02:01:29.520
I mean, pretty similar, right?

02:01:29.520 --> 02:01:32.880
Same code, different code bases and like lots of different infrastructure and different teams.

02:01:32.880 --> 02:01:35.920
And it was like, you know, it wasn't like, it wasn't the smoothest of all processes,

02:01:35.920 --> 02:01:37.520
but, you know, deep mind is doing, I think, very well.

02:01:37.520 --> 02:01:41.520
I mean, look, you give the example of COVID and the COVID example is like, listen,

02:01:41.520 --> 02:01:45.040
we woke up to it, maybe it was late, but then we deployed all this money.

02:01:45.040 --> 02:01:48.320
And COVID response to government was a clusterfuck over.

02:01:48.320 --> 02:01:51.520
And like the only part of it that was worked is I agree Warp Speed was like enabled by the

02:01:51.520 --> 02:01:51.840
government.

02:01:51.840 --> 02:01:55.920
It was literally just giving the permission that you can actually do.

02:01:55.920 --> 02:01:58.240
Well, it was also taking, making like the big

02:01:58.240 --> 02:01:59.120
commitments or whatever.

02:01:59.120 --> 02:01:59.520
But I agree.

02:01:59.520 --> 02:02:01.760
But it was like fundamentally it was like a private sector led effort.

02:02:01.760 --> 02:02:02.080
Yeah.

02:02:02.080 --> 02:02:03.440
That was the only part of COVID that worked.

02:02:03.440 --> 02:02:06.080
I mean, I think, I think, again, I think the project will look closer to operation

02:02:06.080 --> 02:02:06.640
Warp Speed.

02:02:06.640 --> 02:02:09.600
And it's not even, I mean, I think, I think you'll have all the companies involved

02:02:09.600 --> 02:02:10.640
in the government project.

02:02:10.640 --> 02:02:12.800
I'm not that sold that merging is that difficult.

02:02:12.800 --> 02:02:16.240
You know, you have one, okay, you select one code base and you know, you run free

02:02:16.240 --> 02:02:18.480
training on like GPUs with, you know, one code base.

02:02:18.480 --> 02:02:21.360
And then you do the sort of second RL step on the, you know, the other code base with

02:02:21.360 --> 02:02:23.200
TPU is that I think it's fine.

02:02:24.880 --> 02:02:26.880
I mean, to the topic of like, will people sign up for it?

02:02:26.880 --> 02:02:28.160
It wouldn't sign up for it today.

02:02:28.160 --> 02:02:29.680
I think this would be kind of crazy to people.

02:02:30.640 --> 02:02:33.120
But also, you know, I mean, this is part of the like secrets thing, you know, people

02:02:33.120 --> 02:02:37.120
gather parties or whatever, you know, you know this, you know, I don't think anyone

02:02:37.120 --> 02:02:40.880
has really gotten up in front of these people and been like, look, you know, the thing you're

02:02:40.880 --> 02:02:46.000
building is the most important thing for like the national security of the United States

02:02:46.000 --> 02:02:49.280
for like weather, you know, like, you know, the free world will have another century ahead

02:02:49.280 --> 02:02:49.600
of it.

02:02:49.600 --> 02:02:53.840
Like this is the thing you're doing is really important, like for your country, for democracy.

02:02:55.280 --> 02:02:57.920
And, you know, don't talk about the secrets.

02:02:57.920 --> 02:03:01.280
And it's not just about, you know, deep mind or whatever, it's about, it's about, you know,

02:03:01.280 --> 02:03:02.320
these really important things.

02:03:03.760 --> 02:03:06.000
And so, you know, I don't know, like, again, we're talking about the Manhattan project,

02:03:06.000 --> 02:03:06.080
right?

02:03:06.080 --> 02:03:07.840
This stuff was really contentious initially.

02:03:08.800 --> 02:03:11.600
But, you know, at some point, it was like clear that this stuff was coming.

02:03:11.600 --> 02:03:15.440
It was clear that there was like sort of a real sort of like exigency on the military

02:03:15.440 --> 02:03:16.880
national security front.

02:03:16.880 --> 02:03:21.520
And, you know, I think a lot of people come around on the like weather will be competent.

02:03:21.520 --> 02:03:22.320
I agree.

02:03:22.320 --> 02:03:25.280
I mean, this is again, where it's like a lot of the stuff is more like predictive in the

02:03:25.280 --> 02:03:27.280
sense, I think this is like reasonably likely.

02:03:27.280 --> 02:03:28.720
And I think not enough people are thinking about it.

02:03:28.720 --> 02:03:31.600
You know, like a lot of people think about like AI lab politics or whatever.

02:03:32.640 --> 02:03:35.200
But like nobody has a plan for the project, you know, it's like, you know,

02:03:35.520 --> 02:03:36.800
they think you're pessimistic about it.

02:03:36.800 --> 02:03:38.160
And like, well, you don't have a plan for it.

02:03:38.160 --> 02:03:40.800
We need to do it very soon because AGI is upon us.

02:03:40.800 --> 02:03:45.760
Then fuck, the only capable competent technical institutions capable of making AI right now

02:03:45.760 --> 02:03:46.560
are private companies.

02:03:46.560 --> 02:03:48.480
And they're going to play that leading role.

02:03:48.480 --> 02:03:49.920
It'll be a sort of a partnership basically.

02:03:49.920 --> 02:03:52.720
But the other thing is like, you know, again, we talked about World War II and, you know,

02:03:52.720 --> 02:03:56.240
American unpreparedness, the veneer of World War II is complete, you know, complete shambles,

02:03:56.240 --> 02:03:56.640
right?

02:03:56.640 --> 02:03:58.800
And so there is a sort of like very company.

02:03:58.800 --> 02:04:03.120
I think America has a very deep bench of just like incredibly competent managerial talent.

02:04:03.120 --> 02:04:06.880
You know, I think that, you know, there's a lot of really dedicated people.

02:04:06.880 --> 02:04:11.440
And, you know, I think basically a sort of operational warp speed, public-private partnership,

02:04:11.440 --> 02:04:14.560
something like that, you know, is sort of what I imagine it would look like.

02:04:14.560 --> 02:04:14.880
Yeah.

02:04:14.880 --> 02:04:20.000
I mean, the recruiting the talent is an interesting question because the same sort of thing where

02:04:22.320 --> 02:04:24.640
initially for the Manhattan Project, you had to convince people,

02:04:24.640 --> 02:04:26.960
we've got to beat the Nazis and you got to get on board.

02:04:26.960 --> 02:04:31.200
I think a lot of them maybe regretted how much they accelerated the bomb.

02:04:31.200 --> 02:04:36.480
And I want, I think this is generally a thing of the war where...

02:04:36.480 --> 02:04:38.320
I mean, I think they're also wrong to regret it, but...

02:04:40.560 --> 02:04:41.600
Yeah, I mean, why?

02:04:42.400 --> 02:04:43.520
What's the reason for regretting it?

02:04:44.080 --> 02:04:48.640
I think there's a world in which you don't have, the way in which nuclear weapons were

02:04:48.640 --> 02:04:53.840
developed after the war was pretty explosive because there was a precedent that you actually

02:04:53.840 --> 02:04:55.360
can use nuclear weapons.

02:04:55.360 --> 02:04:58.800
Then because of the race that was set up, you immediately go to the H bomb.

02:04:59.760 --> 02:05:03.600
I mean, I think my view is, again, this is related to the view on AI and maybe some of

02:05:03.600 --> 02:05:05.680
our disagreement is like, that was inevitable.

02:05:05.680 --> 02:05:11.680
Like, of course, there was this world war and then obviously there was the cold war right after.

02:05:11.680 --> 02:05:17.760
Of course, the military and technology angle of this would be pursued with ferocious intensity.

02:05:17.760 --> 02:05:20.000
And I don't really think there's a world in which that doesn't happen,

02:05:20.000 --> 02:05:21.920
where it's like, ah, we're all not going to build nukes.

02:05:21.920 --> 02:05:24.000
And also just like nukes went really well.

02:05:24.000 --> 02:05:25.520
I think that could have gone terribly, right?

02:05:26.080 --> 02:05:31.440
Again, I think this is not physically possible with nukes, this pocket nukes for everybody,

02:05:31.440 --> 02:05:35.920
but I think WMDs that are proliferated and democratized and all the countries have it.

02:05:36.560 --> 02:05:41.360
The US leading on nukes and then building this new world order that was US-led,

02:05:41.360 --> 02:05:44.880
or at least a few great powers, and a non-proliferation regime for nukes,

02:05:44.880 --> 02:05:50.080
a partnership and a deal that's like, look, no military application of nuclear technology,

02:05:50.080 --> 02:05:51.920
but we're going to help you with the civilian technology.

02:05:51.920 --> 02:05:53.840
We're going to enforce safety norms on the rest of the world.

02:05:53.920 --> 02:05:57.040
That worked. It worked. And it could have gone so much worse.

02:05:58.000 --> 02:05:59.440
So we're zooming on.

02:05:59.440 --> 02:06:00.560
I don't know if you're talking about Nagasaki.

02:06:00.560 --> 02:06:02.720
You know, I mean, this is, I mean, I say this a bit in the piece,

02:06:02.720 --> 02:06:05.920
but it's like actually the A-bomb, you know, like the A-bomb on Hiroshima and Nagasaki was just like,

02:06:05.920 --> 02:06:07.200
you know, the sort of firebombing.

02:06:08.560 --> 02:06:13.760
I think the thing that really changed the game was like the super, you know, the H-bombs and ICBMs.

02:06:13.760 --> 02:06:16.480
And then I think that's really when it took it to like a whole new level.

02:06:16.480 --> 02:06:22.800
I think part of me thinks when you say, we'll tell the people that for the free world to survive,

02:06:22.800 --> 02:06:24.080
we need to pursue this project.

02:06:24.720 --> 02:06:27.760
It sounds similar to World War II is,

02:06:28.800 --> 02:06:31.920
so World War II is a sad story, obviously in this fact that it happened,

02:06:31.920 --> 02:06:34.400
but also like the victory is sad in the sense that

02:06:35.120 --> 02:06:38.880
Britain goes in to protect Poland.

02:06:38.880 --> 02:06:44.880
And at the end, the USSR, which is, you know, as your family knows,

02:06:45.920 --> 02:06:50.800
is incredibly brutal, ends up occupying half of Europe.

02:06:50.800 --> 02:06:56.880
And part of protecting the free world, that's why I got to rush the AI.

02:06:56.880 --> 02:06:59.920
And like, if we end up with the American AI Leviathan,

02:06:59.920 --> 02:07:01.920
I think there's a world where we look back on this,

02:07:01.920 --> 02:07:09.680
where it has the same sort of twisted irony that Britain going into World War II had about trying to protect Poland.

02:07:11.280 --> 02:07:13.840
Look, I mean, I think there's going to be a lot of unfortunate things that happen.

02:07:13.840 --> 02:07:15.840
I'm just hoping we make it through.

02:07:15.840 --> 02:07:19.520
I mean, to the point of it's like, I really don't think the pitch will only be the sort of like,

02:07:19.520 --> 02:07:23.040
you know, the race, I think the race will be sort of a backdrop to it.

02:07:23.040 --> 02:07:26.800
I think the sort of general like, look, it's important that democracy shape this technology.

02:07:26.800 --> 02:07:30.400
We can't just like leak this stuff to, you know, North Korea is going to be important.

02:07:30.400 --> 02:07:33.440
I think also for the just safety, including alignment,

02:07:33.440 --> 02:07:37.520
including the sort of like creation of new WMDs, I'm not currently sold.

02:07:37.520 --> 02:07:38.480
There's another path, right?

02:07:38.480 --> 02:07:41.760
So it's like, if you just have the breakneck grace, both internationally,

02:07:41.760 --> 02:07:44.400
because you're just instantly leaking all the stuff, including the weights,

02:07:45.120 --> 02:07:48.560
and just, you know, the commercial race, you know, Demis and Dario and Sam, you know,

02:07:48.560 --> 02:07:49.920
just kind of like, they all want to be first.

02:07:51.280 --> 02:07:52.800
And then it's incredibly rough for safety.

02:07:52.800 --> 02:07:54.800
And then you say, okay, safety regulation.

02:07:54.800 --> 02:07:57.680
But, you know, it's sort of like, you know, the safety regulation that people talk about,

02:07:57.680 --> 02:08:01.520
it's like, oh, well, NIST, and they take years and they figure out what the expert consensus is,

02:08:01.520 --> 02:08:03.920
and then they write what's going to happen to the project as well.

02:08:03.920 --> 02:08:07.920
But I think, I mean, I think the sort of alignment angle during the intelligence explosion,

02:08:07.920 --> 02:08:10.640
it's going to, you know, it's not a process of like years of bureaucracy,

02:08:10.640 --> 02:08:12.080
and then you can kind of write some standards.

02:08:12.720 --> 02:08:16.160
I think it looks much more like basically a war and like you have a fog of war.

02:08:16.160 --> 02:08:18.480
It's like, look, it's like, is it safe to do the next oom?

02:08:18.480 --> 02:08:21.280
You know, and it's like, ah, you know, like, you know, we're like three ooms into the

02:08:21.280 --> 02:08:22.080
intelligence explosion.

02:08:22.080 --> 02:08:23.840
We don't really understand what's going on anymore.

02:08:25.040 --> 02:08:29.600
You know, the, you know, like a bunch of our like generalization scaling curves are like,

02:08:29.600 --> 02:08:30.960
kind of looking not great.

02:08:30.960 --> 02:08:33.680
You know, some of our like automated AI researchers that are doing alignment are

02:08:33.680 --> 02:08:35.680
saying it's fine, but we don't quite trust them.

02:08:35.680 --> 02:08:39.280
In this test, you know, the like, the eyes started doing naughty things and,

02:08:39.280 --> 02:08:41.680
ah, but then we like hammered it out and then it was fine.

02:08:41.680 --> 02:08:43.760
And like, ah, should we, should we go ahead?

02:08:43.760 --> 02:08:45.280
Should we take, you know, another six months?

02:08:45.360 --> 02:08:47.680
Also, by the way, you know, like China just stole the weights.

02:08:47.680 --> 02:08:49.760
Are we, you know, they're about to like deploy the rumor army.

02:08:49.760 --> 02:08:50.400
Like what do we do?

02:08:50.400 --> 02:08:52.960
I think it's this, I think it is this crazy situation.

02:08:54.480 --> 02:08:59.840
And, um, you know, basically you, you were lying much more on kind of like a sane chain

02:08:59.840 --> 02:09:04.160
of command than you are on sort of some like, you know, the Libertive Regulatory Scheme.

02:09:04.160 --> 02:09:06.960
I wish you had, you were able to do the Libertive Regulatory Scheme.

02:09:06.960 --> 02:09:08.720
And this is the thing about the private companies too.

02:09:08.720 --> 02:09:13.280
I don't think, you know, they all claim they're going to do safety, but

02:09:13.680 --> 02:09:17.680
I think it's really rough when you're in the commercial race and they're startups,

02:09:17.680 --> 02:09:20.640
you know, and startups, startups or startups, you know,

02:09:20.640 --> 02:09:22.400
I think they're not fit to handle WMDs.

02:09:23.600 --> 02:09:25.440
Yeah, I'm coming closer to your position.

02:09:26.960 --> 02:09:32.160
But part of me also, so with the responsible scaling policies,

02:09:32.160 --> 02:09:35.920
I was told that people who are advancing that, that the way to think about this,

02:09:35.920 --> 02:09:38.000
because they know I'm like a libertarian type of person.

02:09:38.000 --> 02:09:38.560
Yeah, yeah, yeah.

02:09:38.560 --> 02:09:45.440
And the way they approached me about it was that fundamentally this is a way to protect

02:09:46.080 --> 02:09:51.120
market-based development of AGI in the sense that if you didn't have this at all,

02:09:51.120 --> 02:09:54.880
then you would have the sort of misuse and then you would have to be nationalized.

02:09:54.880 --> 02:09:59.120
And the RSPs are a way to make sure that through this deployment,

02:09:59.120 --> 02:10:01.040
you can still have a market-based order.

02:10:01.040 --> 02:10:04.480
But then there's these safeguards that make sure that things don't go off the rails.

02:10:05.200 --> 02:10:12.560
And I wonder if it seems like your story seems self-consistent,

02:10:13.200 --> 02:10:18.400
but it does feel, I know this was never your position, so I'm not looping you into this,

02:10:18.400 --> 02:10:22.960
but sort of modern Bailey almost in the sense of...

02:10:23.760 --> 02:10:26.000
Well, look, here's what I think about RSP-type stuff

02:10:26.000 --> 02:10:28.160
or sort of safety regulation that's happening now.

02:10:28.160 --> 02:10:30.720
I think they're important for helping us figure out what world we're in

02:10:30.720 --> 02:10:33.200
and like flashing the warning signs when we're close, right?

02:10:33.200 --> 02:10:38.800
And so the story we've been telling is sort of what I think the modal version of this decade is,

02:10:38.800 --> 02:10:40.640
but it's like, I think there's lots of ways it could be wrong.

02:10:40.640 --> 02:10:42.480
I really... We should talk about the data a while more.

02:10:42.480 --> 02:10:45.360
I think there's like, again, I think there's a world where the stuff stagnates, right?

02:10:45.360 --> 02:10:46.720
There's a world where we don't have AGI.

02:10:47.920 --> 02:10:51.440
And so basically the RSP thing is preserving the optionality,

02:10:51.440 --> 02:10:54.160
let's see how this stuff goes, but we need to be prepared.

02:10:54.160 --> 02:10:57.520
Like if the red lights start flashing, if we're getting the automated eye researcher,

02:10:57.520 --> 02:11:00.160
then it's like, and it's crunch time, and then it's time to go.

02:11:00.160 --> 02:11:04.480
I think, okay, I can be on the same page on that, that we should have a very,

02:11:04.480 --> 02:11:07.520
very strong prior on a proceeding in a market-based way,

02:11:07.520 --> 02:11:12.640
unless you're right about what the explosion looks like, the intelligence explosion.

02:11:12.640 --> 02:11:18.400
And so like, I don't move yet, but in that world where like really does seem like

02:11:18.400 --> 02:11:24.000
Alec Radford can be automated, and that is the only bottleneck to getting TSI.

02:11:24.000 --> 02:11:25.200
Okay, I think we can leave it at that.

02:11:26.160 --> 02:11:30.240
I can, yeah, I am somewhat of the way there.

02:11:30.240 --> 02:11:32.880
Okay, okay. I hope it goes well.

02:11:34.000 --> 02:11:36.240
It's gonna be, ah, very stressful.

02:11:36.240 --> 02:11:37.680
And again, right now is the chill time.

02:11:39.760 --> 02:11:41.040
Enjoy your vacation a lot less.

02:11:41.600 --> 02:11:45.680
It's funny to look out over, just like, this is San Francisco.

02:11:45.680 --> 02:11:46.960
Yeah, yeah, yeah.

02:11:46.960 --> 02:11:49.120
Open the eyes right there, you know, anthropics there.

02:11:49.120 --> 02:11:51.120
I mean, again, this is kind of like, you know, it's like,

02:11:51.120 --> 02:11:54.960
you guys have this enormous power over how it's gonna go for the next couple of years,

02:11:54.960 --> 02:11:56.560
and that power is depreciating.

02:11:57.680 --> 02:11:58.320
Who's you guys?

02:11:58.880 --> 02:12:00.080
Like, you know, people at labs.

02:12:00.080 --> 02:12:00.640
Yeah, yeah, yeah.

02:12:01.760 --> 02:12:02.960
But it is a sort of crazy world.

02:12:02.960 --> 02:12:05.120
And you're talking about like, you know, I feel like you talk about like,

02:12:05.120 --> 02:12:06.560
oh, maybe they'll nationalize too soon.

02:12:06.560 --> 02:12:11.120
It's like, you know, almost nobody like really like feels it, sees what's happening.

02:12:11.120 --> 02:12:14.240
And it's, I think this is the thing that I find stressful about all the stuff is like,

02:12:14.240 --> 02:12:15.440
look, maybe I'm wrong.

02:12:15.440 --> 02:12:18.560
Like if I'm right, we're in this crazy situation where there's like, you know,

02:12:18.560 --> 02:12:20.720
like a few hundred guys that are like paying attention.

02:12:22.800 --> 02:12:24.480
And it's daunting.

02:12:24.960 --> 02:12:26.880
I went to Washington a few months ago.

02:12:26.880 --> 02:12:30.800
And I was talking to some people who are doing AI policy stuff there.

02:12:30.800 --> 02:12:33.200
And I was asking them how likely they think nationalization is.

02:12:34.080 --> 02:12:37.920
And they said, oh, you know, like, it's really hard to nationalize stuff.

02:12:37.920 --> 02:12:39.200
It's been a long time since we've done it.

02:12:39.200 --> 02:12:43.520
There's these very specific procedural constraints on what kinds of things can be nationalized.

02:12:44.400 --> 02:12:49.760
And then I was asked, well, like ASI, so that means because there's,

02:12:49.760 --> 02:12:53.280
there's constraints at a defense production act or whatever that won't be nationalized.

02:12:53.600 --> 02:12:54.800
The Supreme Court would overturn that.

02:12:55.760 --> 02:12:58.320
And they're like, yeah, I guess that would be nationalized.

02:13:00.720 --> 02:13:04.080
That's the short summary of my post or my view on the project.

02:13:10.320 --> 02:13:14.160
Okay. So before we go further on the ASF, let's just back off.

02:13:16.000 --> 02:13:16.960
We began the conversation.

02:13:16.960 --> 02:13:17.920
I think people will be confused.

02:13:17.920 --> 02:13:20.640
You graduated valedictorian of Columbia when you were 19.

02:13:20.640 --> 02:13:22.480
So you got to college when you were 15.

02:13:23.040 --> 02:13:25.600
And you were in Germany, then you got to college at 15.

02:13:27.040 --> 02:13:28.000
How the fuck did that happen?

02:13:29.520 --> 02:13:30.960
I really wanted out of Germany.

02:13:34.160 --> 02:13:36.240
I went to kind of a German public school.

02:13:36.240 --> 02:13:38.160
It was not a good environment for me.

02:13:41.040 --> 02:13:41.600
In what sense?

02:13:41.600 --> 02:13:42.720
There's just like no peers.

02:13:43.440 --> 02:13:45.600
Yeah, look, I mean, it was, yeah, it was, you know,

02:13:46.560 --> 02:13:49.440
there's, I mean, there's also just a sense in which sort of like,

02:13:49.440 --> 02:13:51.200
there's this particular sort of German cultural sense.

02:13:51.200 --> 02:13:53.360
I think in the US, you know, there's all these like amazing high schools

02:13:53.360 --> 02:13:55.040
and like sort of an appreciation of excellence.

02:13:55.040 --> 02:13:58.800
And in Germany, there's really this sort of like Paul Poppy syndrome of us, right?

02:13:58.800 --> 02:14:01.920
Where it's, you know, you're the curious kid in class and you want to learn more

02:14:01.920 --> 02:14:03.920
instead of the teacher being like, ah, that's great.

02:14:03.920 --> 02:14:05.440
They're like, they kind of resent you for it.

02:14:05.440 --> 02:14:06.640
And they're like trying to crush you.

02:14:07.920 --> 02:14:10.000
I mean, there's also like, there's no kind of like elite universities

02:14:10.000 --> 02:14:11.520
for undergraduate, which is kind of crazy.

02:14:13.520 --> 02:14:15.680
So, you know, the sort of, you know, there's sort of like,

02:14:16.240 --> 02:14:20.080
basically like the meritocracy was kind of crushed in Germany at some point.

02:14:21.200 --> 02:14:24.240
Also, I mean, there's a sort of incredible sense of complacency,

02:14:25.920 --> 02:14:27.760
you know, across the board.

02:14:27.760 --> 02:14:30.400
I mean, one of the things that always puzzles me is like, you know,

02:14:31.120 --> 02:14:34.080
even just going to a US college was just kind of like radical act.

02:14:34.080 --> 02:14:36.880
And like, you know, it doesn't seem radical to anyone here because it's like,

02:14:36.880 --> 02:14:39.840
ah, this is obviously the thing you do and you can go to Columbia, you go to Columbia.

02:14:39.840 --> 02:14:41.920
But it's, you know, it is very unusual.

02:14:41.920 --> 02:14:44.720
And it's, it's, it's wild to me because it's like, you know,

02:14:44.720 --> 02:14:45.920
this is where stuff is happening.

02:14:45.920 --> 02:14:47.520
You can get so much of a better education.

02:14:47.520 --> 02:14:50.960
And, you know, like America is where, you know, it's where, where, where,

02:14:50.960 --> 02:14:53.920
where all the stuff is and people don't do it.

02:14:53.920 --> 02:14:59.520
And, and so, um, yeah, anyway, so I, you know, I know I skipped a few grades and,

02:14:59.520 --> 02:15:03.120
and, you know, I think, um, at the time it seemed very normal to me to kind of like

02:15:03.120 --> 02:15:05.280
go to college and come to America.

02:15:05.280 --> 02:15:10.080
I think, um, you know, now one of my sisters is now like turning 15, you know.

02:15:10.080 --> 02:15:12.080
And so then I, you know, and I look at her and I'm like,

02:15:12.880 --> 02:15:14.080
now I understand how my mother.

02:15:14.880 --> 02:15:19.040
And as you get to college, you're like presumably the only 15 year old.

02:15:19.040 --> 02:15:19.840
Yeah, yeah.

02:15:19.840 --> 02:15:22.000
As it was just like normal for you to be a 15 year old.

02:15:22.000 --> 02:15:23.520
Like, what was the initial years?

02:15:23.520 --> 02:15:24.400
It felt so normal at the time.

02:15:24.400 --> 02:15:25.040
You know, I didn't, yeah.

02:15:25.040 --> 02:15:27.440
So yeah, it's like, now I understand why my mother's worried.

02:15:27.440 --> 02:15:30.080
And, you know, I think, you know, I worked, I worked on my parents for a while.

02:15:30.080 --> 02:15:32.160
You know, eventually I was, you know, I persuaded them.

02:15:32.160 --> 02:15:34.240
No, but yeah, it felt, felt very normal at the time.

02:15:34.240 --> 02:15:34.800
And it was great.

02:15:34.800 --> 02:15:37.200
It was also great because I, you know, I actually really like college, right?

02:15:37.840 --> 02:15:39.840
And in some sense it sort of came at the right time for me.

02:15:40.480 --> 02:15:45.360
Where, you know, I, I mean, I, you know, for example, I really appreciate the sort of like

02:15:45.360 --> 02:15:48.240
liberal arts education and, you know, like the core curriculum and reading sort of

02:15:48.240 --> 02:15:50.880
core works of political philosophy and, and literature.

02:15:50.880 --> 02:15:52.400
And you did what you can.

02:15:52.400 --> 02:15:56.000
And I mean, my majors were math and statistics and economics.

02:15:57.200 --> 02:16:00.800
But, you know, Columbia has a sort of pretty heavy core curriculum and liberal arts education.

02:16:00.800 --> 02:16:03.200
And honestly, like, you know, I shouldn't have done all the majors.

02:16:03.200 --> 02:16:05.920
I should have just, I mean, the best courses were sort of the courses where it's like,

02:16:05.920 --> 02:16:08.320
there's some amazing professor and it's some history class.

02:16:08.400 --> 02:16:13.200
And it's, I mean, that's, that's honestly the thing I would recommend people spend their time

02:16:13.200 --> 02:16:13.920
on in college.

02:16:14.800 --> 02:16:16.560
Was there one professor or class that stood out that way?

02:16:17.280 --> 02:16:23.040
I mean, if you, there's like a class by Richard Betz on war, peace and strategy.

02:16:23.920 --> 02:16:25.600
Adam too is obviously fantastic.

02:16:27.040 --> 02:16:28.800
And, you know, has written very riveting books.

02:16:29.600 --> 02:16:30.160
Yeah.

02:16:30.160 --> 02:16:31.440
You should have them on the podcast, by the way.

02:16:31.440 --> 02:16:31.920
I've tried.

02:16:31.920 --> 02:16:32.240
Okay.

02:16:32.240 --> 02:16:32.800
Try it.

02:16:32.800 --> 02:16:34.160
I think you try it for me.

02:16:34.160 --> 02:16:35.760
Yeah, you gotta give it on the pod.

02:16:35.760 --> 02:16:36.160
Yeah.

02:16:36.160 --> 02:16:37.040
Oh, it'd be so good.

02:16:38.640 --> 02:16:38.960
Okay.

02:16:38.960 --> 02:16:46.320
So then in a couple of years, we were talking to Tyler Cowan recently and he said that when,

02:16:46.320 --> 02:16:52.000
the way we, he first encountered you was you wrote this paper on economic growth and existential

02:16:52.000 --> 02:16:57.440
risk and he said, I, when I found, read it, I couldn't believe that a 17 year old had written

02:16:57.440 --> 02:16:57.760
it.

02:16:57.760 --> 02:17:01.040
I thought if this was a MIT dissertation, I'd be impressed.

02:17:01.040 --> 02:17:06.160
So you were like, how did you go from your, I guess we were the junior of them,

02:17:07.040 --> 02:17:12.000
you're writing, you're writing, you know, pretty novel economic papers.

02:17:13.600 --> 02:17:15.680
Why did you get interested in this, this kind of thing?

02:17:15.680 --> 02:17:17.280
And what was the process to get in that?

02:17:18.880 --> 02:17:19.280
I don't know.

02:17:19.280 --> 02:17:21.040
I just, you know, I get interested in things in some sense.

02:17:21.040 --> 02:17:23.280
It's sort of like, it feels very natural to me.

02:17:23.280 --> 02:17:24.560
It's like, I get excited about a thing.

02:17:24.560 --> 02:17:25.200
I read about it.

02:17:25.200 --> 02:17:25.920
I immerse myself.

02:17:25.920 --> 02:17:29.120
I think I can, you know, I can learn information very quickly and understand it.

02:17:30.240 --> 02:17:35.200
The, I mean, I think to the paper, I mean, I think one actual, at least for the way I work,

02:17:35.200 --> 02:17:39.200
I feel like sort of moments of peak productivity matter much more than sort of average productivity.

02:17:39.200 --> 02:17:42.080
I think there's some jobs, you know, like CUO or something, you know, like average

02:17:42.080 --> 02:17:43.360
productivity really matters.

02:17:43.360 --> 02:17:47.520
But I think there's sort of a, I often feel like I have periods of like, you know,

02:17:47.520 --> 02:17:50.560
there's some, there's a couple months where there's sort of nephrolessence and I'm like,

02:17:50.560 --> 02:17:53.520
you know, and the other times I'm sort of computing stuff in the background.

02:17:53.520 --> 02:17:56.400
And at some point, you know, like writing the series, this is also kind of similar.

02:17:56.400 --> 02:18:00.400
And it's just like you, you write it and, and it's, it's like, it's really flowing.

02:18:00.400 --> 02:18:02.800
And that's sort of what ends up mattering.

02:18:02.880 --> 02:18:06.480
I think even for CEOs, it might be the case that the peak productivity is very important.

02:18:06.480 --> 02:18:11.520
There's one of our following chat-off-house rules, one of our friends in a group chat

02:18:11.520 --> 02:18:17.040
has pointed out how many famous CEOs and founders have been bipolar manic,

02:18:17.920 --> 02:18:21.840
which is very much the peak, like the call option on your productivity is the most

02:18:21.840 --> 02:18:25.520
important thing and you get it by just increasing the volatility through bipolar.

02:18:27.120 --> 02:18:28.560
Okay. So that's interesting.

02:18:28.560 --> 02:18:30.800
And so you get interested in economics first.

02:18:30.800 --> 02:18:31.840
First of all, why economics?

02:18:31.840 --> 02:18:33.520
Like you could read about anything at this move.

02:18:33.520 --> 02:18:36.560
Like you, if you wanted, you know, you kind of got a slow start on them.

02:18:39.120 --> 02:18:42.560
You reached it all these years on the econ, there's an alternative world where you're

02:18:42.560 --> 02:18:45.600
like on the super alignment team at 17 instead of 21 or whatever it was.

02:18:51.760 --> 02:18:53.920
I mean, in some sense, I'm still doing economics, right?

02:18:53.920 --> 02:18:55.760
You know, what is, what is straight lines on a graph?

02:18:55.760 --> 02:18:59.040
I'm looking at the log-log thoughts and like figuring out what the trends are

02:18:59.040 --> 02:19:02.240
and like thinking about the feedback loops and equilibrium arms control dynamics.

02:19:02.240 --> 02:19:06.560
And, you know, it's, I think it is a sort of a way of thinking that I find very useful.

02:19:07.680 --> 02:19:12.640
And, you know, like what, you know, Dario and Ilya seeing scaling early in some sense,

02:19:12.640 --> 02:19:13.920
that is a sort of very economic way.

02:19:13.920 --> 02:19:16.400
And also the sort of physics, kind of like empirical physics, you know,

02:19:16.400 --> 02:19:17.200
a lot of them are physicists.

02:19:17.200 --> 02:19:19.840
I think the economists usually can't code well enough and that's their issue.

02:19:19.840 --> 02:19:21.920
But I think it's that sort of way of thinking.

02:19:22.880 --> 02:19:27.600
I mean, the other thing is, you know, I thought they were sort of, you know, I thought of a lot

02:19:27.600 --> 02:19:30.000
of the sort of like core ideas of economics.

02:19:30.000 --> 02:19:30.960
I thought we're just beautiful.

02:19:32.160 --> 02:19:34.640
And, you know, in some sense, I feel like I was a little duped, you know,

02:19:34.640 --> 02:19:37.040
where it's like actually econ academia is kind of decadent now.

02:19:37.040 --> 02:19:40.080
You know, I think that, you know, for example, the paper I wrote, you know,

02:19:40.080 --> 02:19:43.120
it's sort of, I think the takeaway, you know, it's a long paper,

02:19:43.120 --> 02:19:44.480
it's 100 pages of math or whatever.

02:19:44.480 --> 02:19:48.560
I think the core takeaway I can, you know, kind of give the core intuition for in like,

02:19:48.560 --> 02:19:50.320
you know, 30 seconds and it makes sense.

02:19:50.320 --> 02:19:52.480
And it's, and it's like, you don't actually need the math.

02:19:52.480 --> 02:19:56.320
I think that's the sort of the best pieces of economics are like that where you do the work,

02:19:56.400 --> 02:20:00.960
but you do the work to kind of uncover insights that weren't obvious to you before.

02:20:00.960 --> 02:20:04.640
Once, once you've done the work, it's like some sort of like mechanism falls out of it

02:20:04.640 --> 02:20:08.880
that like makes a lot of crisp intuitive sense that like explains some facts about the world

02:20:08.880 --> 02:20:10.160
that you can then use in arguments.

02:20:10.160 --> 02:20:13.120
And I think, you know, I think, you know, like a lot of econ one-on-one like this,

02:20:13.120 --> 02:20:13.680
and it's great.

02:20:13.680 --> 02:20:18.240
A lot of econ in the, you know, the 50s and the 60s, you know, was like this.

02:20:18.240 --> 02:20:21.040
And, you know, Chad Jones papers are often like this.

02:20:21.040 --> 02:20:22.960
I really like Chad Jones papers for this.

02:20:22.960 --> 02:20:29.600
You know, I think, you know, why did I ultimately not pursue econ academia was number of reasons.

02:20:29.600 --> 02:20:30.640
One of them was Tyler Cowan.

02:20:33.280 --> 02:20:36.240
You know, he kind of took me aside and he was kind of like, look, I think you're one of the

02:20:36.240 --> 02:20:39.280
like top young economists I've ever met, but also you should probably not go to grad school.

02:20:39.280 --> 02:20:39.840
Oh, interesting.

02:20:39.840 --> 02:20:41.040
Yeah, I didn't realize that.

02:20:41.040 --> 02:20:41.280
Well, yeah.

02:20:41.280 --> 02:20:44.800
And it was, it was, it was good because he kind of introduced me to the, you know,

02:20:44.800 --> 02:20:48.880
I don't know, like the Twitter weirdos or just like, you know, and I think the takeaway from that

02:20:48.880 --> 02:20:52.000
was kind of, you know, got to move out last one more time.

02:20:52.080 --> 02:20:53.840
Wait, Tyler introduced you to the Twitter weirdos?

02:20:53.840 --> 02:20:54.320
A little bit.

02:20:54.320 --> 02:20:54.480
Yeah.

02:20:54.480 --> 02:20:55.840
Or just kind of like the sort of brought, you know,

02:20:55.840 --> 02:20:59.920
Like the 60 year old, the old economist in GCT that Twitter.

02:20:59.920 --> 02:21:00.480
Yeah.

02:21:00.480 --> 02:21:03.840
Well, you know, I had been, I had, so I went from Germany, you know, completely, you know,

02:21:03.840 --> 02:21:08.080
on the periphery to kind of like, you know, in the U.S. elite institution and sort of got some

02:21:08.080 --> 02:21:12.720
vibe of like sort of, you know, meritocratic elite, you know, U.S. society.

02:21:12.720 --> 02:21:16.240
And then sort of, yeah, basically this sort of like, there was a sort of directory then to

02:21:16.240 --> 02:21:18.640
being like, look, I, you know, find the true American spirit.

02:21:18.640 --> 02:21:19.440
I got to come out here.

02:21:19.920 --> 02:21:23.200
The other reason I didn't become an economist was because, or at least econ academia,

02:21:23.200 --> 02:21:25.760
was because I think sort of econ academia has become a bit decadent.

02:21:25.760 --> 02:21:29.040
And maybe it's just ideas getting harder to find and maybe it's sort of things, you know,

02:21:29.040 --> 02:21:31.360
and the sort of beautiful, simple things have been discovered.

02:21:31.360 --> 02:21:33.200
But you know, like what are econ papers these days?

02:21:33.200 --> 02:21:38.480
You know, it's like, you know, it's like 200 pages of like empirical analyses on what happened

02:21:38.480 --> 02:21:41.840
when, you know, like Wisconsin bought, you know, 100,000 more textbooks on like educational

02:21:41.840 --> 02:21:42.320
outcomes.

02:21:42.320 --> 02:21:44.080
And I'm really happy that work happened.

02:21:44.080 --> 02:21:46.800
I think it's important work, but I think it is not in government and covering these

02:21:46.800 --> 02:21:50.160
sort of like fundamental insights and sort of mechanisms in society.

02:21:51.440 --> 02:21:54.720
Or, you know, it's like, even the theory work is kind of like, here's a really complicated

02:21:54.720 --> 02:21:58.800
model and the model spits out, you know, if the Fed does X, you know, then Y happens,

02:21:58.800 --> 02:22:00.640
you have no idea what that hat, why that happened?

02:22:00.640 --> 02:22:03.680
Because it's like gazillion parameters and they're all calibrated in some way.

02:22:03.680 --> 02:22:05.280
And it's some computer simulation.

02:22:05.280 --> 02:22:07.360
You have no idea about the validity, you know, yeah.

02:22:07.360 --> 02:22:10.880
So I think, I think the sort of, you know, the most important insights are the ones where

02:22:10.880 --> 02:22:12.320
you have to do a lot of work to get them.

02:22:12.320 --> 02:22:14.240
But then there's this crisp intuition.

02:22:14.560 --> 02:22:14.720
Yeah.

02:22:14.720 --> 02:22:21.040
The P versus NP of, that's really interesting.

02:22:21.040 --> 02:22:26.480
So just going back to your time in college, you say that peak productivity kind of explains

02:22:26.480 --> 02:22:31.840
the, this paper and things, but the valedictorian, that's getting straight A's or whatever is very

02:22:31.840 --> 02:22:35.680
much average productivity phenomenon.

02:22:35.680 --> 02:22:36.320
Right.

02:22:36.320 --> 02:22:40.400
So there's one award for the highest GPA, which I won, but the valedictorian is like among the

02:22:40.400 --> 02:22:43.600
people which have the highest GPA and then like selected by faculty.

02:22:43.680 --> 02:22:44.000
Okay.

02:22:44.000 --> 02:22:44.480
Yeah.

02:22:44.480 --> 02:22:47.040
So it's just not, but it's not just peak productivity.

02:22:47.040 --> 02:22:49.920
It's just, it's just, I generally just love this stuff.

02:22:49.920 --> 02:22:53.120
You know, I just, I was curious and I thought it was really interesting and I love learning

02:22:53.120 --> 02:22:57.920
about it and, and I love kind of like, it made sense to me and, you know, it was very natural.

02:22:57.920 --> 02:23:01.840
And so, you know, I think I'm, you know, I'm not, you know, I think one of my faults is

02:23:01.840 --> 02:23:03.440
I'm not that good at eating glass or whatever.

02:23:03.440 --> 02:23:04.880
I think there's some people who are very good at it.

02:23:04.880 --> 02:23:09.200
I think the sort of like, the sort of moments of peak productivity come when I, you know,

02:23:09.200 --> 02:23:13.040
I'm just really excited and engaged and, and, and, and love it.

02:23:13.040 --> 02:23:18.080
And, you know, I, I, you know, if you take like courses, you know, that's what you got in college.

02:23:18.080 --> 02:23:21.120
And it's, it's, it's, it's the Bruce Banner code and Avengers.

02:23:21.760 --> 02:23:23.120
You know, I'm always angry.

02:23:24.080 --> 02:23:24.960
I'm always excited.

02:23:24.960 --> 02:23:25.920
I'm always curious.

02:23:25.920 --> 02:23:27.200
That's why I'm always the equitability.

02:23:29.040 --> 02:23:32.560
So it's interesting, by the way, when you were in college, I was also in college.

02:23:32.560 --> 02:23:38.480
I think you were, despite being a year younger than me, I think you're ahead in college than

02:23:38.480 --> 02:23:40.080
me or at least two years, maybe two years ahead.

02:23:40.720 --> 02:23:44.240
And we met around this time.

02:23:44.240 --> 02:23:45.360
Yeah, yeah, yeah.

02:23:45.360 --> 02:23:48.800
We also met, I think through the Tyler Cowan universe.

02:23:48.800 --> 02:23:49.520
Yeah, yeah, yeah.

02:23:49.520 --> 02:23:52.480
And it's very insane how small the world is.

02:23:52.480 --> 02:23:54.000
I think I, did I reach out to you?

02:23:54.000 --> 02:23:54.800
I must have.

02:23:54.800 --> 02:23:54.960
Yeah.

02:23:54.960 --> 02:24:00.400
About when I had a couple of videos and they had a couple hundred views or something.

02:24:00.400 --> 02:24:00.960
Yeah.

02:24:00.960 --> 02:24:02.160
It's a small world.

02:24:02.160 --> 02:24:04.080
I mean, this is the crazy thing about the eye world, right?

02:24:04.080 --> 02:24:08.560
It's kind of like, it's the same few people at the kind of SF parties and they're the ones,

02:24:09.040 --> 02:24:12.400
running the models at DeepMind and OpenAI and Anthropic.

02:24:12.400 --> 02:24:16.800
And I mean, I think some other friends of ours have mentioned this,

02:24:16.800 --> 02:24:20.480
who are now later in their career and very successful, that they actually met all the

02:24:20.480 --> 02:24:23.040
people who are also kind of very successful in Silicon Valley now.

02:24:23.040 --> 02:24:26.800
Like when they're in their 20s or when they're really 20.

02:24:29.120 --> 02:24:31.680
I mean, look, I actually think, why is it a small world?

02:24:32.320 --> 02:24:36.560
I mean, I think one of the things is some amount of some sort of agency.

02:24:37.520 --> 02:24:42.640
And I think in a funny way, this is a thing I sort of took away from the sort of Germany

02:24:42.640 --> 02:24:46.560
experience where it was, I mean, look, I, I, it was crushing.

02:24:46.560 --> 02:24:47.760
I really didn't like it.

02:24:47.760 --> 02:24:52.080
And it was like, it was such an unusual move to kind of skip grades and such an unusual move to

02:24:52.080 --> 02:24:52.880
come to the United States.

02:24:52.880 --> 02:24:55.840
And, you know, a lot of these things I did were kind of unusual moves.

02:24:55.840 --> 02:25:02.640
And, you know, there's some amount where like, just like, just trying to do it.

02:25:02.640 --> 02:25:04.000
And then it was fine.

02:25:04.000 --> 02:25:04.560
And it worked.

02:25:05.680 --> 02:25:08.800
That kind of reinforced, like, you know, you don't, you don't just have to kind of conform

02:25:08.800 --> 02:25:10.080
to what the opportune window is.

02:25:10.080 --> 02:25:12.640
You can just kind of like try to do the thing, the thing that seems right to you.

02:25:13.680 --> 02:25:16.800
And like, you know, most people can be wrong and I know things like that.

02:25:16.800 --> 02:25:20.240
And I think that was kind of a, you know, valuable kind of like early experience,

02:25:20.240 --> 02:25:21.200
those sort of formative.

02:25:21.840 --> 02:25:22.160
Okay.

02:25:22.160 --> 02:25:23.760
So after college, what did you do?

02:25:24.400 --> 02:25:27.360
I did econ research for a little bit, you know, Oxford and stuff.

02:25:27.360 --> 02:25:29.520
And then, then I worked at Future Fund.

02:25:30.320 --> 02:25:30.880
Yeah.

02:25:30.880 --> 02:25:31.200
Okay.

02:25:31.200 --> 02:25:32.880
So, and so tell me about it.

02:25:35.440 --> 02:25:39.440
Future Fund was, you know, it was a foundation that was, you know, funded by

02:25:39.440 --> 02:25:40.320
San Bank and Freed.

02:25:40.320 --> 02:25:42.800
I mean, we were our own thing, you know, we were based in the Bay.

02:25:44.080 --> 02:25:46.560
You know, at the time, this was in sort of early 22.

02:25:48.160 --> 02:25:50.960
It was, it was this just like incredibly exciting opportunity, right?

02:25:50.960 --> 02:25:54.480
It was basically like a startup, you know, foundation, which is like, you know, it doesn't

02:25:54.480 --> 02:25:57.680
come along that, that often that, you know, we thought we'd be able to give away billions

02:25:57.680 --> 02:26:01.120
of dollars, you know, thought we'd be able to kind of like, you know, remake how philanthropy

02:26:01.120 --> 02:26:05.040
is done, you know, from first principles, thought we'd be able to have, you know,

02:26:05.040 --> 02:26:08.880
this like great impact, you know, we, the causes we focused on were, you know,

02:26:08.880 --> 02:26:15.040
biosecurity, you know, AI, you know, finding exceptional talent and putting them to work

02:26:15.040 --> 02:26:15.840
on hard problems.

02:26:17.360 --> 02:26:20.000
And, you know, like a lot of the stuff we did, I was, I was really excited about,

02:26:20.000 --> 02:26:23.120
you know, like academics who would, you know, usually take six months would send us emails

02:26:23.120 --> 02:26:24.080
like, ah, you know, this is great.

02:26:24.080 --> 02:26:27.360
This is so quick and, you know, and straightforward, you know, in general,

02:26:27.360 --> 02:26:30.560
I feel like I've often find that with like, you know, a little bit of encouragement, a little

02:26:30.560 --> 02:26:34.080
bit of sort of empowerment, kind of like removing excuses, making the process easy,

02:26:34.080 --> 02:26:36.560
you know, you can kind of like get people to do great things.

02:26:37.760 --> 02:26:42.800
I think on the future front, the thing is context for people who might not realize,

02:26:43.680 --> 02:26:48.320
not only were you guys planning on deploying billions of dollars, but it was a team of four

02:26:48.320 --> 02:26:48.800
people.

02:26:48.800 --> 02:26:49.600
Yeah, yeah, yeah.

02:26:49.600 --> 02:26:55.600
So you at 18 are on a team of four people that is in charge of deploying billions of dollars.

02:26:55.600 --> 02:26:56.080
Yeah.

02:26:56.080 --> 02:26:58.240
I mean, just, I mean, yeah, I'm a future fund, you know, the,

02:26:59.360 --> 02:27:01.840
yeah, I mean, the, you know, so that was, that was sort of the heyday, right?

02:27:02.480 --> 02:27:05.680
And then obviously, you know, when, when in sort of, you know, November of 22,

02:27:07.520 --> 02:27:10.880
you know, it was kind of revealed that Sam was this, you know, giant fraud.

02:27:10.880 --> 02:27:13.280
And from one day to the next, you know, the whole thing collapsed.

02:27:15.040 --> 02:27:16.480
That was just really tough.

02:27:16.480 --> 02:27:18.400
I mean, you know, obviously it was devastating.

02:27:18.400 --> 02:27:21.280
It was devastating, obviously for the people at their money on FTX,

02:27:21.840 --> 02:27:26.400
you know, closer to home, you know, all the, you know, all these grantees, you know,

02:27:26.400 --> 02:27:28.960
we'd wanted to help them and we thought they were doing amazing projects.

02:27:28.960 --> 02:27:32.400
And so, but instead of helping them, we ended up saddling them with like a giant problem.

02:27:33.920 --> 02:27:35.920
You know, personally, it was, you know, it was a startup, right?

02:27:35.920 --> 02:27:38.480
And so I, you know, I'd worked 70 hour weeks every week for, you know,

02:27:38.480 --> 02:27:41.200
basically a year on this to kind of build this up, you know, we're a tiny team.

02:27:42.480 --> 02:27:45.680
And then from one day to the next, it was all gone and not just gone.

02:27:45.680 --> 02:27:47.360
It was associated with this giant fraud.

02:27:49.200 --> 02:27:50.960
And so, you know, that was incredibly tough.

02:27:51.920 --> 02:27:52.640
Yeah.

02:27:52.640 --> 02:27:54.960
And then were there any signs early on that

02:27:56.000 --> 02:27:56.800
SPF was?

02:27:58.240 --> 02:27:58.480
Yeah.

02:27:58.480 --> 02:28:00.640
And like, obviously I didn't know he was a fraud and the whole, you know,

02:28:01.600 --> 02:28:03.440
I would have never worked there, you know.

02:28:04.400 --> 02:28:06.480
And, you know, we weren't, you know, we were a separate thing.

02:28:06.480 --> 02:28:07.920
We weren't working with the business.

02:28:09.280 --> 02:28:11.360
I mean, I think, I do think there were some takeaways for me.

02:28:11.360 --> 02:28:16.480
I think one takeaway was, you know, I think there's a, I had this tendency,

02:28:16.480 --> 02:28:18.720
I think people in general have this tendency to kind of like, you know,

02:28:18.720 --> 02:28:21.120
give successful CEOs a pass on their behavior.

02:28:21.120 --> 02:28:23.440
Because, you know, there's successful CEOs and that's how they are.

02:28:23.440 --> 02:28:25.440
And that's just the successful CEO things.

02:28:25.440 --> 02:28:30.720
And, you know, I didn't know Sandbank Manfredo was a fraud, but I knew SPF.

02:28:30.720 --> 02:28:33.200
And I knew he was extremely risk-taking, right?

02:28:33.200 --> 02:28:35.520
I knew he, he was narcissistic.

02:28:37.840 --> 02:28:40.400
He didn't tolerate this agreement well, you know, sort of by the end,

02:28:40.400 --> 02:28:42.640
he and I just like didn't get along well.

02:28:42.640 --> 02:28:45.520
And sort of, I think the reason for that was like, there's some biosecurity grants

02:28:45.520 --> 02:28:48.320
he really liked because they're kind of cool and flashy.

02:28:48.320 --> 02:28:50.880
And at some point I'd kind of run the numbers and it didn't really seem

02:28:50.880 --> 02:28:51.760
that cost effective.

02:28:51.760 --> 02:28:54.480
And I pointed that out and he was pretty unhappy about that.

02:28:55.520 --> 02:28:56.640
And so I knew his character.

02:28:58.640 --> 02:29:01.520
And I think, you know, I feel like one takeaway for me was,

02:29:03.440 --> 02:29:07.280
was, you know, like, I think it's really worth paying attention to people's character,

02:29:07.280 --> 02:29:09.680
including like people you work for and successful CEOs.

02:29:10.880 --> 02:29:13.760
And, you know, that can save you a lot of pain down the line.

02:29:14.720 --> 02:29:18.160
Okay, so after that, I picked some clothes and you're out.

02:29:18.480 --> 02:29:28.000
And then you got into, you went to OpenAI, the Super Alignment team had just started.

02:29:28.000 --> 02:29:30.080
I think you were like part of the initial team.

02:29:30.640 --> 02:29:33.680
And so what was the original idea?

02:29:33.680 --> 02:29:35.440
What was compelling about that for you to join?

02:29:36.080 --> 02:29:36.880
Yeah, totally.

02:29:37.920 --> 02:29:39.920
So, I mean, what was the goal of the Super Alignment team?

02:29:40.960 --> 02:29:46.560
You know, the Alignment team at OpenAI, you know, at other labs, sort of like several years ago,

02:29:46.560 --> 02:29:49.040
kind of had done sort of basic research and they developed RLHF,

02:29:49.040 --> 02:29:51.040
Reinforcement Learning from Human Feedback.

02:29:51.040 --> 02:29:54.880
And that was sort of a, you know, ended up being really successful technique

02:29:54.880 --> 02:29:56.960
for controlling sort of current generation of AI models.

02:29:59.280 --> 02:30:02.080
What we were trying to do was basically kind of be the basic research

02:30:02.080 --> 02:30:04.560
bet to figure out what is the successor to RLHF.

02:30:04.560 --> 02:30:06.640
And the reason that we needed that is, you know, basically, you know,

02:30:06.640 --> 02:30:09.280
RLHF probably won't scale to superhuman systems.

02:30:09.280 --> 02:30:12.560
RLHF relies on sort of human raiders who kind of thumbs up, thumbs down, you know,

02:30:12.560 --> 02:30:15.120
like the model said something, it looks fine, it looks good to me.

02:30:15.200 --> 02:30:17.440
At some point, you know, the superhuman models, the superintelligence,

02:30:17.440 --> 02:30:20.320
it's going to write, you know, a million lines of, you know, crazy complex code,

02:30:20.320 --> 02:30:22.080
you don't know at all what's going on anymore.

02:30:22.080 --> 02:30:24.560
And so how do you kind of steer and control these systems?

02:30:24.560 --> 02:30:25.760
How do you hide side constraints?

02:30:26.560 --> 02:30:31.360
You know, the reason I joined was I thought this was an important problem

02:30:31.360 --> 02:30:33.680
and I thought it was just a really solvable problem, right?

02:30:33.680 --> 02:30:36.880
I thought this was basically, you know, there's, I think there's a, I still do.

02:30:36.880 --> 02:30:41.200
I mean, even more so do I think there's a lot of just really promising sort of ML research

02:30:41.200 --> 02:30:44.240
on alignment on sort of aligning superhuman systems.

02:30:45.440 --> 02:30:50.320
And maybe we should talk about that a bit more later, but so, and then it was so solvable,

02:30:50.320 --> 02:30:51.120
you solved it in a year.

02:30:55.120 --> 02:30:58.800
Anyway, so look, opening, I wanted to do this like really ambitious effort on alignment and,

02:30:58.800 --> 02:31:01.840
you know, Elliot was backing it and, you know, I liked a lot of the people there.

02:31:01.840 --> 02:31:04.640
And so I was, you know, I was really excited and I was kind of like, you know,

02:31:04.640 --> 02:31:07.760
I think there was a lot of people sort of on alignment.

02:31:07.760 --> 02:31:09.680
There's always a lot of people kind of making hay about it.

02:31:09.680 --> 02:31:13.680
And, you know, I appreciate people highlighting the importance of the problem.

02:31:13.680 --> 02:31:15.840
And I was just really into like, let's just try to solve it.

02:31:15.840 --> 02:31:17.920
And let's do the ambitious effort, you know, let's do the, you know,

02:31:17.920 --> 02:31:20.160
operation warp speed for solving alignment.

02:31:20.160 --> 02:31:22.880
And it seemed like an amazing opportunity to do so.

02:31:23.760 --> 02:31:26.880
Okay. And now basically the team doesn't exist.

02:31:26.880 --> 02:31:28.720
I think the head of it has left.

02:31:28.720 --> 02:31:31.520
Both heads of it have left, Jan and Ilya.

02:31:31.520 --> 02:31:32.800
That's the news of last week.

02:31:34.080 --> 02:31:35.600
What happened? Why did the thing break down?

02:31:37.040 --> 02:31:40.240
I think OpenAI sort of decided to take things in a somewhat different direction.

02:31:41.200 --> 02:31:42.320
Meaning what?

02:31:43.520 --> 02:31:46.320
I mean, that super alignment isn't the best way to frame the...

02:31:47.440 --> 02:31:49.920
No, I mean, look, obviously, sort of after the November board events,

02:31:49.920 --> 02:31:51.120
you know, there were personnel changes.

02:31:51.120 --> 02:31:54.320
I think Ilya leaving was just incredibly tragic for OpenAI.

02:31:54.320 --> 02:31:58.000
And, you know, I think some amount of repartilization,

02:31:58.000 --> 02:32:00.000
I think some amount of, you know, I mean,

02:32:00.000 --> 02:32:02.240
there's been some reporting on the Superalignment Compute Commitment.

02:32:02.240 --> 02:32:04.160
You know, there's this 20% compute commitment as part of,

02:32:04.160 --> 02:32:05.520
you know, how a lot of people were recruited.

02:32:05.520 --> 02:32:07.920
You know, it's like, we're going to do this ambitious effort on alignment.

02:32:07.920 --> 02:32:11.680
And, you know, some amount of, you know,

02:32:11.680 --> 02:32:14.240
not keeping that and deciding to go in a different direction.

02:32:15.440 --> 02:32:18.160
Okay, so now Jan has left, Ilya has left.

02:32:19.280 --> 02:32:20.720
So this team itself has dissolved,

02:32:20.720 --> 02:32:24.960
but you were the sort of first person who left or was forced to leave.

02:32:24.960 --> 02:32:28.960
You were the information reported that you were fired for leaking.

02:32:28.960 --> 02:32:30.320
But what happened? Was this accurate?

02:32:31.360 --> 02:32:35.520
Yeah. Look, why don't I tell you what they claim I leaked

02:32:35.520 --> 02:32:36.640
and you can tell me what you think.

02:32:37.280 --> 02:32:40.480
Yeah. So OpenAI did claim to employees that I was fired for leaking.

02:32:41.120 --> 02:32:44.240
And, you know, I and others have sort of pushed them to say what the leak is.

02:32:44.240 --> 02:32:45.760
And so here's their response in full.

02:32:47.280 --> 02:32:52.960
You know, sometime last year, I had written a sort of brainstorming document on preparedness

02:32:52.960 --> 02:32:56.320
on safety and security measures we need in the future on the Path to AGI.

02:32:57.120 --> 02:32:59.920
And I shared that with three external researchers for feedback.

02:32:59.920 --> 02:33:01.280
So that's it. That's the leak.

02:33:02.640 --> 02:33:03.920
You know, I think for context,

02:33:03.920 --> 02:33:07.360
it was totally normal at OpenAI at the time to share sort of safety ideas

02:33:07.360 --> 02:33:08.880
with external researchers for feedback.

02:33:09.760 --> 02:33:10.880
You know, it happened all the time.

02:33:11.680 --> 02:33:14.880
You know, the doc was sort of my idea is, you know, before I shared it,

02:33:15.520 --> 02:33:17.520
I reviewed it for anything sensitive.

02:33:19.440 --> 02:33:21.680
The internal version had a reference to a future cluster,

02:33:21.680 --> 02:33:23.600
but I redacted that for the external copy.

02:33:24.480 --> 02:33:28.160
You know, there's a link in there to some slides of mine, internal slides.

02:33:28.880 --> 02:33:31.520
You know, that was a dead link to the external people I shared it with.

02:33:31.520 --> 02:33:32.880
You know, the slides weren't shared with them.

02:33:33.520 --> 02:33:37.440
And so, obviously, I pressed them to sort of tell me,

02:33:37.440 --> 02:33:39.600
what is the confidential information in this document?

02:33:40.160 --> 02:33:46.800
And what they came back with was a line in the doc about planning for AGI by 2728,

02:33:46.800 --> 02:33:48.480
and not setting timelines for preparedness.

02:33:50.720 --> 02:33:54.320
You know, I wrote this doc, you know, a couple months after the super alignment

02:33:54.320 --> 02:33:57.120
announcement, we had put out, you know, this sort of four-year planning horizon.

02:33:57.120 --> 02:33:59.280
I didn't think that planning horizon was sensitive.

02:33:59.280 --> 02:34:02.240
You know, it's the sort of thing Sam says publicly all the time.

02:34:03.680 --> 02:34:06.400
Hey, I think sort of John said it on the podcast a couple weeks ago.

02:34:07.200 --> 02:34:08.800
Anyway, so that's it.

02:34:08.800 --> 02:34:09.280
That's it?

02:34:09.280 --> 02:34:14.800
So that seems pretty thin for, if the cause was leaking, that seems pretty thin.

02:34:14.800 --> 02:34:15.840
Was there anything else to it?

02:34:16.960 --> 02:34:19.600
Yeah, I mean, so that was the leaking claim.

02:34:19.600 --> 02:34:22.800
I mean, you can say a bit more about sort of what happened in the final.

02:34:22.800 --> 02:34:23.040
Yeah.

02:34:24.240 --> 02:34:28.560
So one thing was last year, I had written a memo,

02:34:28.560 --> 02:34:30.880
internal memo, about opening iSecurity.

02:34:30.880 --> 02:34:32.480
I thought it was, you know, egregiously insufficient.

02:34:32.480 --> 02:34:34.560
You know, I thought it wasn't sufficient to protect, you know,

02:34:34.560 --> 02:34:37.840
the theft of model weights or key algorithmic secrets from foreign actors.

02:34:38.640 --> 02:34:40.080
So I wrote this memo.

02:34:40.080 --> 02:34:42.880
I shared it with a few colleagues, a couple members of leadership,

02:34:43.520 --> 02:34:44.960
who sort of mostly said it was helpful.

02:34:46.640 --> 02:34:50.560
But then, you know, a couple weeks later, a sort of major security incident occurred.

02:34:51.600 --> 02:34:54.560
And that prompted me to share the memo with a couple members of the board.

02:34:54.560 --> 02:34:59.120
And so after I did that, you know, days later, it was made very clear to me that leadership

02:34:59.120 --> 02:35:01.680
was very unhappy with me having shared this memo with the board.

02:35:02.800 --> 02:35:06.480
You know, apparently the board had hassled leadership about security.

02:35:07.440 --> 02:35:10.240
And then I got sort of an official HR warning for this memo,

02:35:10.880 --> 02:35:12.240
you know, for sharing it with the board.

02:35:13.440 --> 02:35:16.720
The HR person told me it was racist to worry about CCPS view.

02:35:18.480 --> 02:35:20.240
And they said it was sort of unconstructive.

02:35:21.920 --> 02:35:24.640
And, you know, look, I think I probably wasn't that my most diplomatic,

02:35:24.640 --> 02:35:26.640
you know, I definitely could have been more politically savvy.

02:35:27.520 --> 02:35:29.920
But, you know, I thought it was a really, really important issue.

02:35:29.920 --> 02:35:32.960
And, you know, the security incident had been really worried.

02:35:34.160 --> 02:35:37.200
Anyway, and so I guess the reason I bring this up is when I was fired,

02:35:37.200 --> 02:35:40.880
it was sort of made very explicit that the security memo is a major reason for my being fired.

02:35:41.600 --> 02:35:43.120
You know, I think it was something like, you know,

02:35:43.120 --> 02:35:46.240
the reason that this is a firing and not a warning is because of the security memo.

02:35:47.760 --> 02:35:49.280
But you were sharing it with the board?

02:35:50.160 --> 02:35:52.080
The warning I'd gotten for the security memo.

02:35:52.080 --> 02:35:57.280
Hmm. Anyway, and I mean, some other, you know, what might also be helpful context

02:35:57.280 --> 02:35:59.360
is the sort of questions they asked me when they fired me.

02:35:59.360 --> 02:36:03.920
So, you know, this was a bit over a month ago, I was pulled aside for a chat with a lawyer,

02:36:03.920 --> 02:36:05.840
you know, that quickly turned very adversarial.

02:36:06.560 --> 02:36:15.120
And, you know, the questions were all about my views on AI progress on AGI on the level

02:36:15.120 --> 02:36:21.520
security appropriate for AGI on, you know, whether government should be involved in AGI on

02:36:23.040 --> 02:36:29.520
whether I and super alignment were loyal to the company on, you know, what I was up to

02:36:29.520 --> 02:36:31.600
during the opening of board events, you know, things like that.

02:36:32.160 --> 02:36:35.200
And, you know, then they, you know, chatted to a couple of my colleagues,

02:36:35.200 --> 02:36:37.040
and then they came back and told me I was fired.

02:36:37.760 --> 02:36:40.320
And, you know, they'd gone through all of my digital artifacts from the time at my,

02:36:40.320 --> 02:36:42.320
you know, time at opening messages docs.

02:36:42.320 --> 02:36:43.920
And that's when they found, you know, the leak.

02:36:45.680 --> 02:36:48.640
Yeah. And so anyway, so the main claim they made was a leaking allegation.

02:36:48.640 --> 02:36:50.080
You know, that's what they told employees.

02:36:51.040 --> 02:36:52.800
They, you know, the security memo.

02:36:54.160 --> 02:36:56.160
There's a couple of other allegations they threw in.

02:36:56.720 --> 02:37:00.160
One thing they said was that I was unforthcoming during the investigation,

02:37:00.160 --> 02:37:02.560
because I didn't initially remember who I'd shared the doc with,

02:37:02.560 --> 02:37:06.800
the sort of preparedness brainstorming doc, only that I had sort of spoken to some external

02:37:06.800 --> 02:37:08.080
researchers about these ideas.

02:37:08.720 --> 02:37:11.120
And, you know, look, the doc was over six months old.

02:37:11.120 --> 02:37:12.320
You know, I spent the day on it.

02:37:12.960 --> 02:37:15.280
You know, as a Google doc, I shared with my opening email.

02:37:15.280 --> 02:37:17.760
It wasn't a, you know, screenshot or anything I was trying to hide.

02:37:18.560 --> 02:37:20.640
It simply didn't stick because it was such a non-issue.

02:37:22.240 --> 02:37:26.240
And then they also claim that I was engaging on policy in a way that they didn't like.

02:37:27.120 --> 02:37:31.440
And so what they cited there was that I had spoken to a couple of external researchers,

02:37:31.440 --> 02:37:35.920
you know, somebody got a think tank about my view that AGI would become a government project,

02:37:35.920 --> 02:37:36.800
you know, as we discussed.

02:37:37.440 --> 02:37:40.560
You know, in fact, I was speaking to lots of sort of people in the field about that at the time.

02:37:40.560 --> 02:37:42.320
I thought it was a really important thing to think about.

02:37:43.520 --> 02:37:47.280
Anyway, and so they found, you know, they found a DM that I'd written to like a friendly colleague,

02:37:47.280 --> 02:37:51.440
you know, five or six months ago, where I relayed this and, you know, they cited that.

02:37:52.640 --> 02:37:56.000
And, you know, I had thought it was well within open-eyed norms to kind of talk about

02:37:56.000 --> 02:37:59.200
high-level issues on the future of AGI with the external people in the field.

02:38:00.080 --> 02:38:01.760
So anyway, so that's what they alleged.

02:38:01.760 --> 02:38:02.480
That's what happened.

02:38:04.240 --> 02:38:07.840
You know, I've spoken to kind of a few dozen former colleagues about this, you know,

02:38:07.840 --> 02:38:11.760
since I think the sort of universal reaction is kind of like, you know, that's insane.

02:38:13.520 --> 02:38:15.760
I was sort of surprised as well.

02:38:15.760 --> 02:38:19.200
You know, I had been promoted just a few months before.

02:38:20.640 --> 02:38:24.480
I think, you know, I think Ilya's comment for the promotion case at the time was something like,

02:38:24.480 --> 02:38:25.680
you know, Leopold's amazing.

02:38:25.680 --> 02:38:26.560
We're lucky to have him.

02:38:29.600 --> 02:38:33.040
But look, I mean, I think the thing I understand, and I think in some sense is reasonable,

02:38:33.040 --> 02:38:34.720
is like, you know, I think I ruffled some feathers.

02:38:34.720 --> 02:38:37.040
And, you know, I think I was probably kind of annoying at times.

02:38:37.040 --> 02:38:41.360
You know, it's like, I security stuff, and I kind of like repeatedly raised that,

02:38:41.360 --> 02:38:43.200
and maybe not always in the most diplomatic way.

02:38:43.360 --> 02:38:48.880
You know, I didn't sign the employee letter during the board events, you know, despite

02:38:48.880 --> 02:38:50.400
pressure to do so.

02:38:51.760 --> 02:38:54.000
And you were one of like eight people or something.

02:38:54.000 --> 02:38:55.360
Not that many people.

02:38:55.360 --> 02:38:59.040
I guess the, I think the sort of two senior most people didn't sign were Andrey,

02:38:59.040 --> 02:39:07.520
and yeah, I mean, on the letter, by the way, I, by the time on sort of Monday morning,

02:39:07.520 --> 02:39:10.080
when that letter was going around, I think probably it was appropriate for the board

02:39:10.160 --> 02:39:10.560
to resign.

02:39:10.560 --> 02:39:13.680
I think they'd kind of like lost too much credibility and trust with the employees.

02:39:15.280 --> 02:39:17.280
But I thought the letter had a bunch of issues.

02:39:17.280 --> 02:39:20.240
I mean, I think one of them was it just didn't call for an independent board.

02:39:20.240 --> 02:39:22.960
I think it's sort of like basics of corporate governance to have an independent board.

02:39:23.600 --> 02:39:27.680
Anyway, you know, it's other things, you know, I am in sort of other discussions,

02:39:27.680 --> 02:39:31.680
I press leadership for sort of opening eye to abide by its public commitments.

02:39:32.960 --> 02:39:37.520
You know, I raised a bunch of tough questions about whether it was consistent with the

02:39:37.520 --> 02:39:41.280
opening I mission and consistent with the national interests to sort of partner with

02:39:41.280 --> 02:39:44.160
authoritarian dictatorships to build the core infrastructure for AGI.

02:39:45.920 --> 02:39:48.560
So, you know, look, you know, it's a free country, right?

02:39:49.360 --> 02:39:50.560
That's what I love about this country.

02:39:50.560 --> 02:39:51.520
You know, we talked about it.

02:39:52.400 --> 02:39:55.040
And so they have no obligation to keep me on staff.

02:39:56.960 --> 02:40:00.800
And, you know, I think in some sense, I think it would have been perfectly reasonable

02:40:00.800 --> 02:40:04.800
for them to come to me and say, look, you know, we're taking the company in a different direction.

02:40:04.800 --> 02:40:06.320
You know, we disagree with your point of view.

02:40:07.280 --> 02:40:10.800
You know, we don't trust you enough to sort of tow the company line anymore.

02:40:10.800 --> 02:40:15.440
And, you know, thank you so much for your work at Open AI, but I think it's time to part ways.

02:40:15.440 --> 02:40:16.400
I think that would have made sense.

02:40:16.400 --> 02:40:20.720
I think, you know, we did start sort of materially diverging on sort of views on important issues.

02:40:20.720 --> 02:40:24.240
I'd come in very excited and aligned with Open AI, but that sort of changed over time.

02:40:24.880 --> 02:40:29.840
And look, I think there would have been a very amicable way to part ways.

02:40:29.840 --> 02:40:33.360
And I think it's a bit of a shame that it sort of this is the way it went down.

02:40:34.320 --> 02:40:39.200
You know, all that being said, I think, you know, I really want to emphasize

02:40:40.160 --> 02:40:42.400
there's just a lot of really incredible people at Open AI.

02:40:42.400 --> 02:40:45.360
And it was an incredible privilege to work with them.

02:40:45.360 --> 02:40:48.640
And, you know, overall, I'm just extremely grateful for my time there.

02:40:49.600 --> 02:40:53.120
When you left, now that there's now there's been reporting about

02:40:54.560 --> 02:41:01.120
an NDA that former employees have to sign in order to have access to their vested equity.

02:41:01.760 --> 02:41:03.680
Did you sign some such NDA?

02:41:04.480 --> 02:41:07.280
No. My situation is a little different.

02:41:07.280 --> 02:41:09.200
And that is sort of basically right before my cliff.

02:41:10.080 --> 02:41:11.920
But then, you know, they still offered me the equity.

02:41:13.600 --> 02:41:16.640
But I didn't want to sign a nondisparagement, you know, freedom is priceless.

02:41:16.640 --> 02:41:18.320
And how much was how much was the equity?

02:41:18.320 --> 02:41:20.400
It's like close to a million dollars.

02:41:21.120 --> 02:41:25.280
So it was definitely a thing you were you and others aware of that this is like

02:41:25.920 --> 02:41:29.040
a choice that Open AI is explicitly offering you.

02:41:29.040 --> 02:41:29.680
Yeah.

02:41:29.680 --> 02:41:34.560
And presumably the person on Open AI staff knew that we're offering them equity,

02:41:34.560 --> 02:41:37.760
but they had to sign this NDA that has these conditions that you can't,

02:41:38.320 --> 02:41:43.680
for example, give the kind of statements about your thoughts on AGI and Open AI that

02:41:43.680 --> 02:41:45.360
you're giving on this podcast right now.

02:41:45.360 --> 02:41:46.800
Like, I don't know what the whole situation is.

02:41:46.800 --> 02:41:51.200
I certainly think sort of vested equity is pretty rough if you're conditioning that on NDA.

02:41:51.200 --> 02:41:53.920
It might be a somewhat different situation if it's a sort of separate agreement.

02:41:53.920 --> 02:41:54.400
Right.

02:41:54.400 --> 02:41:59.360
But an Open AI employee who had signed it presumably could not give the podcast that you're giving today.

02:42:00.640 --> 02:42:01.920
Quite plausibly not.

02:42:01.920 --> 02:42:02.560
Yeah.

02:42:02.560 --> 02:42:02.960
Yeah.

02:42:02.960 --> 02:42:03.440
I don't know.

02:42:04.800 --> 02:42:05.200
Okay.

02:42:05.200 --> 02:42:12.400
So analyzing the situation here, I guess if you were to, yeah, the board thing is really tough

02:42:12.400 --> 02:42:16.480
because if you were trying to defend them, you would say, well, listen,

02:42:16.480 --> 02:42:19.040
you were just kind of going outside the regular chain of command.

02:42:19.040 --> 02:42:24.080
And maybe there's a point there, although the way in which the person from HR thinks

02:42:24.080 --> 02:42:27.760
that you have an adversarial relationship with or you're supposed to have an adversarial

02:42:27.760 --> 02:42:34.560
relationship with the board where to give the board some information, which is relevant to

02:42:36.320 --> 02:42:40.080
whether Open AI is fulfilling its mission and whether it can do that in a better way

02:42:40.080 --> 02:42:45.760
is part of the leak as if the board is that is supposed to ensure that Open AI is following

02:42:45.760 --> 02:42:47.680
its mission as some sort of external actor.

02:42:48.240 --> 02:42:49.200
That seems pretty...

02:42:49.200 --> 02:42:53.040
I mean, I think, I mean, to be clear, the leak allegation was just that sort of document

02:42:53.040 --> 02:42:53.840
I changed the feedback.

02:42:53.840 --> 02:42:55.840
This is just sort of a separate thing that they cited and they said,

02:42:55.840 --> 02:42:57.920
I wouldn't have been fired if not for the security memo.

02:42:57.920 --> 02:42:59.760
They said you wouldn't have been fired.

02:42:59.760 --> 02:43:03.280
They said the reason this is a firing and not a warning is because of the warning you had

02:43:03.280 --> 02:43:04.480
gotten for the security memo.

02:43:04.480 --> 02:43:11.280
Oh, before you left, the incidents with the board happened, were Sam was fired and then

02:43:11.280 --> 02:43:13.280
Rihard, a CEO, and now he's on the board.

02:43:14.240 --> 02:43:20.320
Now, Ilya and Yan, who are the heads of the super lineman team and Ilya, who is a co-founder of

02:43:20.320 --> 02:43:25.680
Open AI, obviously the most significant in terms of stash or a member of Open AI for

02:43:25.680 --> 02:43:27.920
a research perspective, they've left.

02:43:27.920 --> 02:43:31.680
It seems like, especially with regards to super lineman stuff and just generally the

02:43:31.680 --> 02:43:36.160
Open AI, a lot of the sort of personnel drama has happened over the last few months.

02:43:36.800 --> 02:43:37.680
What's going on?

02:43:37.680 --> 02:43:38.960
Yeah, there's a lot of drama.

02:43:41.760 --> 02:43:43.280
Yeah, so why is there so much drama?

02:43:45.360 --> 02:43:49.120
You know, I think there would be a lot less drama all Open AI claim to be with sort of

02:43:49.120 --> 02:43:51.440
building chat, GPT or building business software.

02:43:51.760 --> 02:43:55.360
I think where a lot of the drama comes from is, you know, Open AI really believes they're

02:43:55.360 --> 02:43:56.720
building AGI, right?

02:43:56.720 --> 02:44:02.640
And it's not just, you know, a claim they make for marketing purposes, you know, whatever,

02:44:02.640 --> 02:44:06.080
you know, there's this report that Sam is raising, you know, $7 trillion for chips and

02:44:06.080 --> 02:44:09.200
it's like, that stuff only makes sense if you really believe in AGI.

02:44:10.400 --> 02:44:13.680
And so I think what gets people sometimes is sort of the cognitive dissonance between

02:44:13.680 --> 02:44:17.440
sort of really believing in AGI, but then sort of not taking some of the other implications

02:44:17.440 --> 02:44:20.320
seriously, you know, it's, this is going to be incredible.

02:44:20.480 --> 02:44:23.520
This is going to be incredibly powerful technology, both for good and for bad.

02:44:23.520 --> 02:44:27.600
And that implicates really important issues like the national security issues we spoke

02:44:27.600 --> 02:44:30.480
about, like, you know, are you protecting the secrets from the CCP?

02:44:30.480 --> 02:44:34.640
Like, you know, does America control the core AGI infrastructure or does it, you know, a

02:44:34.640 --> 02:44:37.120
Middle Eastern dictator control the core AGI infrastructure?

02:44:39.360 --> 02:44:44.640
And then I mean, I think the thing that, you know, really gets people is the sort of tendency

02:44:44.640 --> 02:44:49.440
to kind of then make commitments and sort of like, you know, they say they take these

02:44:49.440 --> 02:44:53.120
issues really seriously, they make big commitments on them, but then sort of frequently don't

02:44:53.120 --> 02:44:56.800
follow through, right? So, you know, again, as mentioned, there was this commitment around

02:44:56.800 --> 02:45:00.720
super alignment compute, you know, sort of 20% of compute for this long term safety research

02:45:00.720 --> 02:45:04.640
effort. And I think, you know, you and I could have a totally reasonable debate about what is

02:45:04.640 --> 02:45:09.600
the appropriate level of compute for super alignment. But that's not really the issue.

02:45:09.600 --> 02:45:13.280
The issue is that this commitment was made and it was used to recruit people and, you know,

02:45:13.280 --> 02:45:18.560
it was, it was very public. And it was made because, you know, there's a recognition that

02:45:18.560 --> 02:45:21.520
there would always be something more urgent than a long term safety research effort, you know,

02:45:21.520 --> 02:45:26.160
like some new product or whatever. And then in fact, they just, you know, really didn't keep the

02:45:26.160 --> 02:45:29.920
commitment. And so, you know, there was always something more urgent than long term safety

02:45:29.920 --> 02:45:34.320
research. I mean, I think, I think another example of this is, you know, when I raised these issues

02:45:34.320 --> 02:45:39.680
about security, you know, they would, they would tell me, you know, securities are number one priority.

02:45:41.120 --> 02:45:46.720
But then, you know, invariably, when, when it came time to sort of invest serious resources,

02:45:46.720 --> 02:45:50.800
when it came time to make trade offs, to sort of take some pretty basic measures,

02:45:51.840 --> 02:45:55.840
security would not be prioritized. And so, yeah, I think it's the cognitive dissonance,

02:45:55.840 --> 02:46:00.000
and I think it's the sort of unreliability that causes a bunch of the drama.

02:46:00.960 --> 02:46:08.320
So let's zoom out, talk about the part, a big part of the story, and also a big motivation

02:46:08.320 --> 02:46:12.480
of the way in which it must proceed with regards to geopolitics and everything,

02:46:12.480 --> 02:46:18.160
is that once you have the AGI, pretty soon after you proceed to ASI, because superintelligence,

02:46:18.160 --> 02:46:25.520
because you have these AGI's, which can function as researchers into further AI progress. And with

02:46:25.520 --> 02:46:32.560
a matter of years, maybe less, you go to something that is like superintelligence. And at the high,

02:46:32.560 --> 02:46:36.160
and then from there, then you can do up in according to your story, do all this research

02:46:36.160 --> 02:46:40.800
and development and robotics and pocket nukes and whatever other crazy shit.

02:46:41.600 --> 02:46:55.360
But at a high level, it's not clear to me this input-output model of research is how things

02:46:55.360 --> 02:47:01.120
actually happen in research. We can look at economy-wide, right? Patrick Hollis and others

02:47:01.120 --> 02:47:05.920
have made this point that from compared to 100 years ago, we have 100x more researchers in the

02:47:05.920 --> 02:47:11.520
world. It's not like progress is happening 100x faster. So it's clearly not the case that you

02:47:11.520 --> 02:47:16.880
can just pump in more population into research and you get higher research on the other end.

02:47:18.000 --> 02:47:20.400
I don't know why it would be different for the AI researchers themselves.

02:47:20.400 --> 02:47:23.920
Okay, great. So this is getting into some good stuff. I have a classic disagreement

02:47:23.920 --> 02:47:29.840
I have with Patrick and others. So obviously, inputs matter. So it's like United States

02:47:29.840 --> 02:47:35.840
produces a lot more scientific and technological progress than Liechtenstein or Switzerland.

02:47:36.160 --> 02:47:40.160
And even if I made Patrick Hollis and dictator of Liechtenstein or Switzerland,

02:47:40.160 --> 02:47:44.320
and Patrick Hollis was able to implement his Utopia of Ideal Institutions,

02:47:44.320 --> 02:47:48.160
keeping the talent pool fixed. He's not able to do some crazy high school immigration thing or

02:47:49.200 --> 02:47:52.320
whatever, some crazy genetic breeding scheme or whatever he wants to do.

02:47:54.160 --> 02:47:58.080
Keeping the talent pool fixed, but amazing institutions. I claim that still,

02:47:58.080 --> 02:48:01.280
even if you made Patrick Hollis and dictator of Switzerland, maybe you get some factor,

02:48:01.280 --> 02:48:04.560
but Switzerland is not going to be able to outcompete the United States in scientific

02:48:04.560 --> 02:48:09.440
and technological parts. Obviously, magnitudes matter. Okay. No, I actually, I'm not sure I

02:48:09.440 --> 02:48:14.160
agree with this. There's been many examples in history where you have small groups of people

02:48:14.160 --> 02:48:18.400
who are part of like Bell Labs or Skunkworks or something. There's a couple hundred researchers

02:48:18.400 --> 02:48:22.800
open AI, right? Couple hundred researchers, they do highly selected though, right? You know,

02:48:22.800 --> 02:48:26.800
it's like, it's like saying, you know, that's part of Patrick Hollis and his dictator is going

02:48:26.800 --> 02:48:30.720
to do a good job of this. Well, yes, if you can highly select all the best AI researchers in

02:48:30.720 --> 02:48:33.920
the world, you might only need a few hundred, but if you, you know, that's, that's the talent

02:48:33.920 --> 02:48:37.280
pool. It's like you have the, you know, 300 best AI researchers in the world.

02:48:37.280 --> 02:48:41.120
But, but there's, there has been, it's not a case that from a hundred years to now, there haven't

02:48:41.120 --> 02:48:45.360
been, the population has increased massively. A lot of the, in fact, you would expect the density

02:48:45.360 --> 02:48:49.600
of talent to have increased in the sense that malnutrition and other kinds of debilitation,

02:48:49.600 --> 02:48:54.320
poverty, whatever, that have debilitated past talent at the same sort of level is no longer

02:48:54.320 --> 02:48:58.000
debilitated in the same way. To the 100X point, right? So I don't know if it's 100X. I think it's

02:48:58.000 --> 02:49:02.240
easy to inflate these things, probably at least 10X. And so people are sometimes like, ah, you

02:49:02.240 --> 02:49:05.840
know, like, you know, come on, ideas haven't gotten them much harder to find, you know, why would

02:49:05.840 --> 02:49:10.160
you have needed this 10X increase in research effort? Whereas to me, I think this is an extremely

02:49:10.160 --> 02:49:14.000
natural story. And why is it a natural story? It's a straight line on a log log plot. This is sort

02:49:14.000 --> 02:49:18.320
of a, you know, deep learning researchers dream, right? What is this log log plot on the X axis?

02:49:18.320 --> 02:49:24.080
You have log cumulative research effort on the Y axis. You have some log GDP or ooms of algorithmic

02:49:24.160 --> 02:49:28.800
progress, or, you know, log transistors per square inch, or, you know, in the sort of

02:49:28.800 --> 02:49:33.120
experience curve for solar, kind of like, you know, whatever the log of, you know, the price for a

02:49:33.120 --> 02:49:37.600
gigawatt of solar. And it's extremely natural for that to be a straight line. You know, this is

02:49:37.600 --> 02:49:41.920
sort of a class, yeah, it's a classic. And, you know, it's basically the first thing is very easy,

02:49:41.920 --> 02:49:45.120
then basically, you know, you have to have log increments of cumulative research effort to

02:49:45.120 --> 02:49:50.480
find the next thing. And so, you know, in some sense, I think this is a natural story. Now,

02:49:50.480 --> 02:49:54.880
one objection kind of people then make is like, oh, you know, isn't it suspicious, right? That

02:49:54.880 --> 02:49:59.920
like ideas, you know, well, we increased research effort 10x, and ideas also just got 10x harder

02:49:59.920 --> 02:50:05.520
defined. And so it perfectly, you know, equilibrates. And to there, I say, you know, it's just, it's an

02:50:05.520 --> 02:50:10.080
equilibrium. It's an dodges equilibrium, right? So it's like, you know, isn't it a coincidence that

02:50:10.080 --> 02:50:14.640
supply equals demand, you know, on the market clears, right? And that's, and the same thing here,

02:50:14.640 --> 02:50:18.640
right? So it's, you know, ideas getting, how much ideas have gotten harder to find as a function

02:50:18.640 --> 02:50:23.440
of how much progress you've made. And then, you know, what the overall growth rate has been is a

02:50:23.440 --> 02:50:27.600
function of how much ideas have gotten harder to find in ratio to how much you've been able to,

02:50:27.600 --> 02:50:30.640
like, increase research effort, what is the sort of growth, the log cumulative research effort.

02:50:30.640 --> 02:50:34.400
So in some sense, I think the story is sort of like, fairly natural. And you see this, you see

02:50:34.400 --> 02:50:37.760
this not just economy wide, you see it in kind of experience curve for all sorts of individual

02:50:37.760 --> 02:50:43.040
technologies. So I think there's some process like this. And it's totally plausible that, you know,

02:50:43.040 --> 02:50:46.560
institutions have gotten worse by some factor. Obviously, there's some sort of exponent of

02:50:46.560 --> 02:50:50.800
diminishing returns on more people, right? So like serial time is better than just paralyzing.

02:50:51.840 --> 02:50:54.400
But still, I think it's like, clearly inputs matter.

02:50:55.280 --> 02:51:02.640
Yeah, I agree. But if the coefficient of how fast they diminish as you grow, the input

02:51:02.640 --> 02:51:07.600
is high enough, then the, and the abstract, the fact that inputs matter isn't that relevant.

02:51:07.600 --> 02:51:11.360
Okay, so I mean, we're talking at a very high level, but just like take it down to the actual

02:51:11.360 --> 02:51:18.720
concrete thing here. Open AI has a staff of at most low hundreds who are directly involved

02:51:18.720 --> 02:51:23.760
in the algorithmic progress in future models. If it was really the case that you could just

02:51:23.760 --> 02:51:28.480
arbitrarily scale this number, and you can have much faster algorithmic progress, and that would

02:51:28.480 --> 02:51:34.400
result in much higher, much better AI store open AI basically, then it's not clear why open AI

02:51:34.400 --> 02:51:38.320
doesn't just go out and hire every single person with 150 IQ, of which there are hundreds of

02:51:38.320 --> 02:51:45.520
thousands in the world. And my, my story there is there's transaction costs to managing all these

02:51:45.520 --> 02:51:51.440
people that don't just go away if you have a bunch of AI's that there, these tasks aren't easy to

02:51:51.440 --> 02:51:57.520
parallelize. And I think you, I'm not sure how you would explain the fact of like, why does an

02:51:57.520 --> 02:52:01.280
open AI go on a recruiting binge of every single genius in the world?

02:52:01.280 --> 02:52:04.160
All right, great. So let's talk about the open AI example, and let's talk about the automated

02:52:04.160 --> 02:52:07.600
AI researchers. So I mean, in the open AI case, I mean, just, you know, just kind of like look

02:52:07.600 --> 02:52:11.200
at the inflation of like AI researcher salaries over the last year. I mean, I think like, I don't

02:52:11.200 --> 02:52:15.040
know, I don't know what it is, you know, 4x, 5x is kind of crazy. So they're clearly really trying

02:52:15.040 --> 02:52:18.880
to recruit the best AI researchers in the world. And, you know, I don't know, it's,

02:52:18.880 --> 02:52:23.120
they do find the best AI researchers in the world. I think my response to your thing is like, you know,

02:52:23.120 --> 02:52:26.400
almost all of these 150 IQ people, you know, if you just hire them tomorrow, they wouldn't be

02:52:26.400 --> 02:52:30.960
good AI researchers, they wouldn't be an Alec Radford. But they're willing to make investments

02:52:30.960 --> 02:52:35.440
that take years to pan out of the four. The data centers they're buying right now will come

02:52:35.440 --> 02:52:40.720
online in 2026 or something. Why wouldn't they be able to make every 150 IQ person? Some of them

02:52:40.720 --> 02:52:44.320
won't work out. Some of them won't have the traits we like. But some of them by 2026 will be amazing

02:52:44.320 --> 02:52:48.480
AI researchers. Why aren't they making that bet? Yeah. And so sometimes this happens, right? Like,

02:52:48.480 --> 02:52:51.280
smart physicists have been really good at AI research, you know, it's like all the anthropocopies

02:52:51.280 --> 02:52:56.240
co-found. But like, if you talk to, I've had Daria in the podcast, they have this very careful

02:52:56.240 --> 02:53:00.240
policy of like, we're not going to just hire arbitrarily, we're going to be extremely selective.

02:53:01.520 --> 02:53:05.680
Training is not as easily scalable, right? So training is very hard. You know, if you just

02:53:05.680 --> 02:53:09.920
hired, you know, 100,000 people, it's like, you, I mean, you couldn't train them all. If you're

02:53:09.920 --> 02:53:13.120
really hard to train them all, you know, you wouldn't be doing any AI research. Like, you know,

02:53:13.120 --> 02:53:16.400
there's, there's huge costs to bringing on a new person training them. This is very different

02:53:16.400 --> 02:53:19.280
with the AIs, right? And I think this is, it's really important to talk about the sort of like

02:53:19.280 --> 02:53:23.200
advantages the AIs will have. So it's like, you know, training, right? It's like, what does it take

02:53:23.200 --> 02:53:26.800
to be an Alec Radford? You know, we need to be in a really good engineer, right? The AIs,

02:53:26.800 --> 02:53:29.520
they're going to be an amazing engineer. They're going to be amazing at coding. You can just train

02:53:29.520 --> 02:53:33.840
them to do that. They need to have, you know, not just be a good engineer, but have really good

02:53:33.840 --> 02:53:37.520
research intuitions and like really understand deep learning. And this is stuff that, you know,

02:53:37.520 --> 02:53:41.440
Alec Radford, or, you know, somebody like him has acquired over years of research over just like

02:53:41.440 --> 02:53:46.880
being deeply immersed in deep learning, having tried lots of things himself and failed. The AIs,

02:53:46.880 --> 02:53:49.920
you know, they're going to be able to read every research paper I've written, every experiment

02:53:49.920 --> 02:53:53.120
ever run at the lab, you know, like gain the intuitions from all of this, they're going to be

02:53:53.120 --> 02:53:56.800
able to learn in parallel from all of each other's experiment, you know, experiences.

02:53:57.920 --> 02:54:00.000
You know, I don't know what else, you know, it's like, what does it take to be an Alec

02:54:00.000 --> 02:54:03.840
Radford? Well, there's a, there's a sort of cultural acclimation aspect of it, right? You know,

02:54:03.840 --> 02:54:08.080
if you hire somebody new, there's like politicking, maybe they don't fit in. Well, in the AI case,

02:54:08.080 --> 02:54:12.080
you just make replicas, right? There's a like motivation aspect for it, right? So it's like,

02:54:12.080 --> 02:54:15.920
you know, Alec, you know, they could just like duplicate Alec Radford. And before I run every

02:54:15.920 --> 02:54:19.760
experiment, I haven't spent like, you know, a decade's worth of human time, like double checking

02:54:19.760 --> 02:54:22.800
the code and thinking really careful, be careful about it. I mean, first of all,

02:54:22.800 --> 02:54:27.520
on how that many Alec Radfords, and you know, he wouldn't care. And he would not be motivated.

02:54:27.520 --> 02:54:30.800
But you know, the AI is it can just be like, look, I have a hundred million of you guys,

02:54:30.800 --> 02:54:34.400
I'm just going to put you on just like really making sure this code is correct. There are no

02:54:34.400 --> 02:54:39.840
bugs. This experiment is thought through every hyperparameter is correct. Final thing I'll say

02:54:39.840 --> 02:54:44.080
is, you know, the 100 million human equivalent AI researchers, that is just a way to visualize it.

02:54:44.080 --> 02:54:47.200
So that doesn't mean you're going to have literally 100 million copies. You know,

02:54:47.200 --> 02:54:51.440
so there's tradeoffs you can make between serial speed and in parallel. So you might make the

02:54:51.440 --> 02:54:55.520
tradeoff is look, we're going to run them at, you know, 10x 100x serial speed. It's going to

02:54:55.520 --> 02:54:59.280
result in fewer tokens overall, because it's sort of inherent tradeoffs. But you know, then we have,

02:54:59.280 --> 02:55:03.520
I don't know what the numbers would be, but then we have, you know, 100,000 of them running at 100x

02:55:03.520 --> 02:55:07.920
human speed and thinking and you know, and there's other things you can do on coordination, you

02:55:07.920 --> 02:55:11.680
know, they can kind of like share latent space, attend to each other's context. There's basically

02:55:11.680 --> 02:55:15.360
this huge range of possibilities of things you can do. The 100 million thing is more, I mean,

02:55:15.360 --> 02:55:19.120
another illustration of this is, you know, if you kind of, I run the math in my series,

02:55:19.120 --> 02:55:22.960
and it's basically, you know, 27, 28, you have this automated AI researcher,

02:55:24.080 --> 02:55:28.640
you're going to be able to generate an entire internet's worth of tokens every single day.

02:55:28.640 --> 02:55:32.560
So it's clearly sort of a huge amount of like intellectual work they can do.

02:55:32.560 --> 02:55:38.960
I think the analogous thing there is today we generate more patents in a year than during the

02:55:38.960 --> 02:55:42.960
actual physics revolution in the early 20th century, they were generating across like half a

02:55:42.960 --> 02:55:47.520
century or something. And are you making more physics progress in a year today than you made?

02:55:47.520 --> 02:55:52.080
So yeah, you're going to generate all these tokens. Are you generating as much

02:55:53.200 --> 02:55:58.160
codified knowledge as humanity has been able to generate in the initial creation of the internet?

02:55:58.160 --> 02:56:02.080
Internet tokens are usually final output, right? A lot of these tokens, if we talk, we talked about

02:56:02.080 --> 02:56:06.480
the unhobbling, right? And I think of a kind of like, you know, a GPDN token is sort of like one

02:56:06.480 --> 02:56:09.920
token of my internal monologue, right? And so that's how I do this math on human equivalents,

02:56:09.920 --> 02:56:13.840
you know, it's like 100 tokens a minute, and then, you know, humans working for X hours and,

02:56:13.840 --> 02:56:18.800
you know, what is the equivalent there? I think this goes back to something we were talking about

02:56:18.800 --> 02:56:24.400
earlier where, well, I haven't seen the huge revenues from people often ask this question,

02:56:24.400 --> 02:56:29.040
that if you took GPD for back 10 years and you show people this and they think this is going to

02:56:29.040 --> 02:56:33.840
automate, this is already automated, half the jobs. And so there's a sort of a modus ponens,

02:56:33.840 --> 02:56:37.600
modus tolens here where part of the explanation is like, oh, it's like just on the verge,

02:56:37.600 --> 02:56:41.920
you need to do these unhobblings. And part of that is probably true. But there is another lesson

02:56:41.920 --> 02:56:48.800
to learn there, which is that just looking at face value outside of abilities, there's probably more

02:56:48.800 --> 02:56:53.040
sort of hobblings that you don't realize that are hidden behind the scenes. I think the same will

02:56:53.040 --> 02:56:57.680
be true of the AGI that you have running as AI researchers. I think a lot of things basically

02:56:57.680 --> 02:57:01.760
agree, right? I think my story here is like, you know, I talk about, I think there's going to be

02:57:01.760 --> 02:57:05.520
some long tail, right? And so part, you know, maybe it's like, you know, 26, 27, you know,

02:57:05.520 --> 02:57:08.560
like the proto-automated engineer, and it's really good at engineering. It doesn't have the

02:57:08.560 --> 02:57:12.800
research intuition yet. You don't quite know how to put them to work. But, you know, the sort of

02:57:12.800 --> 02:57:16.880
even the underlying pace of AI progress is already so fast, right? In three years from not being able

02:57:16.880 --> 02:57:21.760
to do any kind of like math at all to now crushing, crushing these math competitions. And so you have

02:57:21.760 --> 02:57:25.840
the initial thing in like 26, 27, maybe the sort of automated, it's an automated research engineer,

02:57:25.840 --> 02:57:29.600
speeds you up by 2x, you go through a lot more progress in that year. By the end of the year,

02:57:29.600 --> 02:57:33.120
you figured out like the remaining kind of unhobblings, you've like got a smarter model,

02:57:33.120 --> 02:57:36.720
and you know, maybe then that thing, or maybe it's two years, you know, and that thing, just like

02:57:36.720 --> 02:57:40.640
that thing really can do automate 100%. And again, you know, they don't need to be doing

02:57:40.640 --> 02:57:43.680
everything. They don't need to be making coffee, you know, they don't need to like, you know,

02:57:43.680 --> 02:57:47.680
maybe there's a bunch of, you know, tacit knowledge and a bunch of other fields. But you know,

02:57:48.320 --> 02:57:52.560
AI researchers at AI labs really know the job of an AI researcher. And it's in some sense,

02:57:52.560 --> 02:57:56.640
it's a sort of, there's lots of clear metrics, it's all virtual, there's code, it's things you

02:57:56.640 --> 02:58:02.160
can kind of develop and train for. So I mean, another thing is how do you actually manage a

02:58:02.240 --> 02:58:09.120
million AI researchers? Humans, the sort of comparative ability we have that we've been

02:58:09.120 --> 02:58:14.160
especially trained for is like working in teams. And despite this fact, we have, for thousands of

02:58:14.160 --> 02:58:19.200
years, we've been learning about how we work together in groups. And despite this, management

02:58:19.200 --> 02:58:24.720
is a clusterfock, right? It's like most companies are badly managed. It's really hard to do this

02:58:24.720 --> 02:58:35.120
stuff. For AIs, the sort of like, we talk about AGI, but it'll be some bespoke set of abilities,

02:58:35.120 --> 02:58:40.000
some of which will be higher than humans, some of which will be at human level. And so it'll be

02:58:40.000 --> 02:58:46.160
some bundle and we'll need to figure out how to put these bundles together with their human

02:58:46.160 --> 02:58:52.560
overseers, with the equipment and everything. And the idea that as soon as you get the bundle,

02:58:52.560 --> 02:58:57.760
you'll figure out how to get, like just shove millions of them together and manage them.

02:58:57.760 --> 02:59:04.560
I'm just very skeptical of like any other revolution, technological revolution in history

02:59:04.560 --> 02:59:09.600
has been very piecemeal, much more piecemeal than you would expect on paper. If you just thought

02:59:09.600 --> 02:59:14.800
about what is the industrial revolution? Well, we dig up coal that powers the steam engines,

02:59:14.800 --> 02:59:18.720
you use the steam engines to run these railroads, that helps us get more coal out. And there's

02:59:18.720 --> 02:59:23.600
sort of like factorial store, you can tell, where in like a six hours, you can be pumping

02:59:24.240 --> 02:59:29.120
thousands of times more coal. But in real life, it takes centuries often, right? In fact, the

02:59:29.120 --> 02:59:36.000
electrification, there's this famous study about how to initially to electrify factories.

02:59:37.200 --> 02:59:43.760
It was decades after electricity to change from the pull, pulleys and water wheel based

02:59:43.760 --> 02:59:48.240
system that we had for steam engines to one that's works with more spread out electrical

02:59:48.240 --> 02:59:51.120
motors and everything. I think this will be the same kind of thing. It might take like decades

02:59:51.120 --> 02:59:54.800
to actually get millions of AI researchers to work together. Okay, great. This is great.

02:59:54.800 --> 02:59:58.480
Okay, so a few responses to that. First of all, I mean, I totally agree with the kind of like

02:59:58.480 --> 03:00:02.960
real world bottlenecks type of thing. I think this is sort of, you know, I think it's easy to under

03:00:02.960 --> 03:00:06.640
rate, you know, basically what we're doing is we're removing the labor constraint, we automate

03:00:06.640 --> 03:00:10.480
labor and we like kind of exploit technology. But you know, there's still lots of other bottlenecks

03:00:10.480 --> 03:00:13.680
in the world. And so I think it's part of why the story is it kind of like starts pretty narrow at

03:00:13.680 --> 03:00:17.440
the thing where you don't have these bottlenecks. And then only over time as we let it kind of

03:00:17.440 --> 03:00:21.760
expand to sort of broader areas. AI, this is part of why I think it's like initially this sort of

03:00:21.760 --> 03:00:25.520
AI research explosion, right? It's like AI research doesn't run into these real world bottlenecks.

03:00:25.520 --> 03:00:29.520
It doesn't require, you know, like plow a field or dig up coal. It's just you're just doing AI

03:00:29.520 --> 03:00:33.840
research. The other thing, you know, the other thing about like in your model, AI research,

03:00:33.840 --> 03:00:38.320
it's not complicated like about flipping a burger, it's just AI research.

03:00:39.520 --> 03:00:44.480
I mean, this is because people make these arguments like, oh, you know, AGI won't do anything

03:00:44.480 --> 03:00:47.520
because it can't flip a burger. I'm like, yeah, it won't be able to flip a burger, but it's going

03:00:47.520 --> 03:00:51.680
to be able to do algorithmic progress, you know, and then, and then, and then when it does algorithmic

03:00:51.680 --> 03:00:58.560
progress, it'll figure out how to flip a burger. You know, look, the, the, the, sorry, the other

03:00:58.560 --> 03:01:01.840
thing is about, you know, again, these are the sort of quantities are lower bound, right? So

03:01:01.840 --> 03:01:05.840
it's like, this is just like, we can definitely run 100 million of these. Probably what will happen

03:01:05.840 --> 03:01:10.240
is one of the first things we're going to try to figure out is how to like, again, run like,

03:01:10.240 --> 03:01:14.560
you know, translate quantity into quality, right? And so it's like, even at the baseline rate of

03:01:14.560 --> 03:01:18.240
progress, you're like quickly getting smarter and smarter systems, right? If we said it was like,

03:01:18.240 --> 03:01:21.680
you know, four years between the preschooler and the high schooler, right? So I think, you know,

03:01:21.680 --> 03:01:25.280
pretty quickly, you know, there's probably some like simple algorithmic changes you find, you know,

03:01:25.280 --> 03:01:29.280
if instead of one Alec Radford, you have 100, you know, you don't even need 100 million. And then,

03:01:29.280 --> 03:01:33.040
and then you get even smarter systems. And now these systems are, you know, they're capable of

03:01:33.040 --> 03:01:36.560
sort of creative, complicated behavior, you don't understand. Maybe there's some way to like use

03:01:36.640 --> 03:01:39.920
all this test time compute in a more unified way rather than all these parallel copies.

03:01:41.440 --> 03:01:45.680
And, you know, so there won't just be quantitatively superhuman, they'll pretty quickly become

03:01:45.680 --> 03:01:49.280
qualitatively superhuman. You know, it's sort of like, it looked like, you know, you're a high

03:01:49.280 --> 03:01:53.200
school student, you're like trying to wrap yourself, wrap your mind around kind of standard physics.

03:01:53.200 --> 03:01:56.960
And then there's some like super smart professor who is like, quantum physics, it all makes sense

03:01:56.960 --> 03:02:00.880
to him. And you're just like, what is going on? And sort of, I think pretty quickly, you kind of

03:02:00.880 --> 03:02:05.520
enter that regime, just given even the underlying pace of AI progress, but even more quickly than

03:02:05.520 --> 03:02:09.600
that, because you have the sort of accelerated force of now this automated AI research.

03:02:09.600 --> 03:02:15.920
I agree that over time, you would, I'm not denying that ASI is, I think that's possible.

03:02:15.920 --> 03:02:16.560
Because the time is just not that much, you know?

03:02:16.560 --> 03:02:20.400
I'm just like, you know, how is this happening in a year? Like you've, okay, first of all.

03:02:20.400 --> 03:02:23.600
So I think the story is sort of like, basically, I think it's a little bit more continuous,

03:02:23.600 --> 03:02:27.040
you know, right? Like, I think already, you know, like I talked about, you know, 25, 26,

03:02:27.040 --> 03:02:29.600
you're basically going to have models as good as a college graduate. And I, you know,

03:02:30.240 --> 03:02:33.520
I don't, I don't know where the unhobbling is going to be. But I think it's plausible that even

03:02:33.520 --> 03:02:37.840
then you have kind of the proto-automated engineer. So there's, I think there is a bit of like a smear,

03:02:37.840 --> 03:02:41.440
kind of an AGI smear or whatever, where it's like, there's sort of unhobblings that you're

03:02:41.440 --> 03:02:44.640
missing. There's kind of like ways of connecting them you're missing. There's like some level

03:02:44.640 --> 03:02:48.000
intelligence you're missing. But then at some point, you are going to get the thing that is like

03:02:48.000 --> 03:02:53.680
an 100% automated Alec Radford. And once you have that, you know, things really take off, I think.

03:02:54.560 --> 03:02:59.520
Yeah. Okay. So let's go back to the unhobblings. Is there, we're going to get a bunch of models

03:02:59.520 --> 03:03:04.400
by the end of the year. Is there something, let's suppose we didn't get some capacity by the end of

03:03:04.400 --> 03:03:08.800
the year. Is there some such capacity, which lacking would suggest that AI progress is going to take

03:03:08.800 --> 03:03:12.800
longer than you are projecting? Yeah. I mean, I think there's, there's two kind of key things.

03:03:12.800 --> 03:03:15.680
There's the unhobbling and there's the data wall, right? I think we should talk about the data wall

03:03:15.680 --> 03:03:19.840
for a moment. I think the data wall is, you know, even though kind of like all this stuff has been

03:03:19.840 --> 03:03:22.800
about, you know, crazy AI progress, I think the data wall is actually sort of underrated. I think

03:03:22.800 --> 03:03:26.880
there's like a real scenario where we're just stagnating. You know, because we've been running

03:03:26.880 --> 03:03:30.400
this tailwind of just like, it's really easy to bootstrap and you just do unsupervised learning

03:03:30.400 --> 03:03:34.480
next token prediction that learns these amazing world models, like bam, you know, great model,

03:03:34.480 --> 03:03:37.760
and you just got to buy some more compute, you know, do some simple efficiency changes,

03:03:38.960 --> 03:03:42.080
you know, and, and again, like so much of deep learning, all these like big gains on

03:03:42.080 --> 03:03:45.680
efficiency have been like pretty dumb things, right? Like, you know, you add a normalization layer,

03:03:45.680 --> 03:03:49.760
you know, you know, you fix the scaling laws, you know, and these already have been huge things,

03:03:49.760 --> 03:03:54.320
let alone kind of like obvious ways in which these models aren't good yet. Anyway, so data

03:03:54.320 --> 03:03:59.120
wall big deal, you know, I don't know, some like put some numbers on this, you know,

03:03:59.760 --> 03:04:03.920
some like you do common crawl, you know, online is like, you know, 30 trillion tokens,

03:04:03.920 --> 03:04:07.840
llama three trained on 15 trillion tokens. So you're basically already using all the data.

03:04:07.840 --> 03:04:11.600
And then, you know, you can get somewhat further by repeating it. So there's an academic paper by,

03:04:11.600 --> 03:04:16.320
you know, Boaz Barak and some others that does scaling laws for this. And they're basically

03:04:16.320 --> 03:04:20.880
like, yeah, you can repeat it sometime. After 16 times of reputation, just like returns basically

03:04:20.880 --> 03:04:24.720
go to zero, you're just completely screwed. And so I don't know, say you can get another 10x on

03:04:24.720 --> 03:04:29.120
data from, say like llama three, and GP four, you know, llama three is already kind of like

03:04:29.120 --> 03:04:32.640
at the limit of all the data, you know, maybe you can get 10x more by repeating data.

03:04:33.680 --> 03:04:38.080
You know, I don't know, maybe that's like at most 100x better model than GP for, which is like,

03:04:38.080 --> 03:04:42.000
you know, 100x effective compute from GP four is, you know, not that much, you know, if you do half

03:04:42.000 --> 03:04:45.360
in order magnitude a year of compute half in order magnitude a year of algorithmic progress,

03:04:45.360 --> 03:04:49.760
you know, that's kind of like two years from GP four. So, you know, GP four finished pretreading

03:04:49.760 --> 03:04:56.240
in 22, you know, 24. So I think one thing that really matters, I think we won't quite know by

03:04:56.240 --> 03:05:00.560
end of the year, but you know, 25, 26, are we cracking the data wall?

03:05:02.400 --> 03:05:08.640
Okay, so suppose we had three orders of magnitude less data in common crawl on the internet than

03:05:08.640 --> 03:05:14.720
we just happen to have now. And for decades, the internet, other things, we've been rapidly

03:05:14.720 --> 03:05:21.120
increasing the stock of data that humanity has. Is it your view that for contingent reasons,

03:05:21.120 --> 03:05:28.560
we just happen to have enough data to train models that are just powerful enough at 4.5 level,

03:05:28.560 --> 03:05:36.240
where they can kick off the self play RL loop? Or is it just that we, you know, if it had been

03:05:36.240 --> 03:05:40.880
three rooms higher, then it would progress would have been slightly faster. In that world, we

03:05:40.880 --> 03:05:43.920
would have been looking back at like, oh, how hard it would have been to like kick off the RL

03:05:44.000 --> 03:05:48.240
explosion with just 4.5, but we would have figured it out. And then so in this world, we would have

03:05:48.240 --> 03:05:52.880
gotten to GP three and then we'd have to kick us on sort of RL explosion. But we would have still

03:05:52.880 --> 03:05:56.880
figured it out. The sort of the we didn't just like luck out on the amount of data we happen to

03:05:56.880 --> 03:06:00.480
have in the world. I mean, three rooms is pretty rough, right? Like three rooms, if less data means

03:06:00.480 --> 03:06:04.400
like six rooms smaller, six rooms, less compute model and scale scaling laws, you know, that's

03:06:04.400 --> 03:06:08.880
it's basically like capping out at like GP two, but I think that would be really rough. I think

03:06:08.880 --> 03:06:13.360
you do make an interesting point about the contingency. You know, I guess earlier, we were

03:06:13.360 --> 03:06:17.040
talking about the sort of like when in the sort of human trajectory, are you able to learn from

03:06:17.040 --> 03:06:21.120
yourself? And so, you know, if we go with that analogy, again, like if you'd only gotten the

03:06:21.120 --> 03:06:24.160
preschooler model, it can't learn from itself. You know, if you only got in the elementary

03:06:24.160 --> 03:06:28.000
school or model, can't learn from itself. And you know, maybe GP for, you know, smart high

03:06:28.000 --> 03:06:31.520
school is really where it starts. Ideally, you have a somewhat better model than it really is

03:06:31.520 --> 03:06:36.240
able to kind of like learn from itself or learn by itself. So yeah, I think there's an interesting,

03:06:36.240 --> 03:06:41.280
I mean, I think maybe one room less data, I would be like more iffy, but maybe still doable.

03:06:42.080 --> 03:06:44.720
Yeah, I think it would feel chiller if we had, you know, like one or two.

03:06:44.720 --> 03:06:49.040
It would be an interesting exercise to get probably distributions of HEI contingent on

03:06:49.040 --> 03:06:54.160
across like data. Yeah, okay. I think the thing that makes me skeptical of this story

03:06:54.160 --> 03:06:59.440
is that the things it totally makes sense for free training works so well. Yeah. These other

03:06:59.440 --> 03:07:06.160
things, their stories of in principle, why they ought to work like a humans can learn this way

03:07:06.240 --> 03:07:11.440
and so on. Yes. And maybe they're true, but I worry that a lot of this case is based on

03:07:11.440 --> 03:07:17.280
sort of first principles with evaluation of how learning happens that fundamentally,

03:07:17.280 --> 03:07:20.800
we don't understand how humans learn and maybe there's some key thing we're missing.

03:07:20.800 --> 03:07:24.720
Yeah. On the sort of sample efficiency, yeah, humans actually, maybe there's,

03:07:25.920 --> 03:07:29.680
you say, well, the fact that these things are way of a less sample efficient in terms of learning

03:07:29.680 --> 03:07:33.680
than humans are suggests that there's a lot of room for improvement. Yeah. Another perspective

03:07:33.680 --> 03:07:38.640
is that we are just on the wrong path altogether, right? That's why there's a sample inefficient

03:07:38.640 --> 03:07:43.920
when it comes to pre-training. Yeah. So, yeah, I mean, I'm just like, there's a lot of like

03:07:44.720 --> 03:07:47.920
first principles arguments stack on top of each other where you get these unhoplings and then

03:07:47.920 --> 03:07:52.640
you get to HEI. Yeah. Then you, because of these reasons why you can stack all these things on

03:07:52.640 --> 03:07:56.720
top of each other, you get to ASI. Yeah. And I'm worried that there's too many steps of this.

03:07:56.720 --> 03:08:03.520
Yeah. Sort of first principles thinking. I mean, we'll see, right? I mean, on the sort

03:08:03.520 --> 03:08:07.840
of sample efficiency thing, again, sort of first principles, but I think, again, there's this

03:08:07.840 --> 03:08:13.280
clear sort of missing middle. And so, you know, and sort of like, you know, people hadn't been

03:08:13.280 --> 03:08:17.840
trying. Now people are really trying, you know, and so it's sort of, you know, I think often again

03:08:17.840 --> 03:08:21.680
in deep learning, something like the obvious thing works. And there's a lot of details to get

03:08:21.680 --> 03:08:24.880
right. So it might take some time, but it's now people are really trying. So I think we get a

03:08:24.880 --> 03:08:32.880
lot of signal in the next couple of years. You know, on a hobbling, I mean, what is the signal

03:08:32.960 --> 03:08:36.320
on hobbling that I think would be interesting? I think, I think the question is basically like,

03:08:36.320 --> 03:08:40.080
are you making progress on this test time compute thing, right? Like is this thing able to think

03:08:40.080 --> 03:08:43.440
longer horizon than just a couple hundred tokens, right? That was unlocked by chain of thought.

03:08:43.920 --> 03:08:49.120
And on that point in particular, the many people who have longer timelines have come on the podcast

03:08:49.120 --> 03:08:55.920
have made the point that the way to train this long horizon RL, it's not, I mean, earlier talking

03:08:55.920 --> 03:09:00.320
about like, well, they can think for five minutes, but not for longer. But it's not because they

03:09:00.320 --> 03:09:05.040
can't physically output an hours or the tokens. It's just really, at least from what I understand

03:09:05.040 --> 03:09:08.640
what they say, right? Like even like Gemini has like a million in context and the million of context

03:09:08.640 --> 03:09:12.400
is actually great for consumption. And it solves one important on hobbling, which is the sort of

03:09:12.400 --> 03:09:18.000
onboarding problem, right? Which is, you know, a new coworker, you know, in your first five minutes,

03:09:18.000 --> 03:09:21.920
like a new smart high school intern first five minutes, not useful at all, a month in, you know,

03:09:21.920 --> 03:09:25.760
much more useful, right? Because they've like looked at the mono repo and understand how the

03:09:25.760 --> 03:09:29.520
code works. And they've read your internal docs. And so being able to put that in context, great,

03:09:29.520 --> 03:09:33.360
solve this onboarding problem. Yeah, but they're not good at sort of the production of a million

03:09:33.360 --> 03:09:39.840
tokens yet. Yeah, right. But on the production of a million tokens, there's no public evidence that

03:09:39.840 --> 03:09:45.920
there's some easy loss function where you can GP for has gotten a lot better since it's actually

03:09:45.920 --> 03:09:50.960
so the GP for gains since launch, I think are a huge indicator that there's like, you know, so

03:09:50.960 --> 03:09:55.040
you talked about this with John on the podcast, John said this was mostly post training gains.

03:09:55.120 --> 03:10:00.080
Right. You know, if you look at the sort of LMS scores, you know, it's like 100 ELO or something,

03:10:00.080 --> 03:10:04.000
it's like a bigger gap than between Claude III, Opus and Claude III, Haiku. And the price difference

03:10:04.000 --> 03:10:09.600
between those is 60X. But it's not more agentic. It's like better in the same channel. Right? Like,

03:10:09.600 --> 03:10:14.000
you know, it went from like, you know, 40% to 70% math. The crux is like whether or like be able to

03:10:14.000 --> 03:10:17.440
like, but I think I think it indicates that clearly there's stuff to be done on hobbling.

03:10:18.240 --> 03:10:21.840
I think yeah, I think I think the interesting question is like this time of year from now,

03:10:21.840 --> 03:10:25.920
you know, is there a model that is able to think for like, you know, a few thousand tokens

03:10:25.920 --> 03:10:30.000
coherently, cohesively, agentically. And I think probably there's, you know,

03:10:30.000 --> 03:10:33.840
again, this is what I'd feel better if we had an humor to more data, because it's like the scaling

03:10:33.840 --> 03:10:38.480
just gives you this sort of like tailwind, right? We're like, for example, tools, right tools, I

03:10:38.480 --> 03:10:41.840
think, you know, talking to people who try to make things work with tools, you know, actually

03:10:41.840 --> 03:10:46.080
sort of GP for is really when tools start to work. And it's like, you can kind of make them work with

03:10:46.080 --> 03:10:51.440
GP 3.5, but it's just really tough. And so it's just like having GP for you can kind of help it

03:10:51.440 --> 03:10:57.120
learn tools in a much easier way. And so just a bit more tailwind from scaling. And then and then

03:10:57.120 --> 03:11:02.080
yeah, and does does I don't know if it'll work, but it's a key question. Okay, I think it's a good

03:11:02.080 --> 03:11:07.680
place to sort of close that part where we know what the crux is and what the progress, what

03:11:07.680 --> 03:11:13.920
what evidence that would look like on the AGI to super intelligence. Maybe it's a case that the

03:11:13.920 --> 03:11:18.000
games are really easier right now, and you can just sort of let loose and Alec Ratt for giving

03:11:18.080 --> 03:11:22.320
a compute budget, and he comes out the other end with something that is an additive,

03:11:23.600 --> 03:11:26.800
like change as part of the code, this is compute multiplier changes to the part.

03:11:28.320 --> 03:11:34.400
What other parts of the world? Maybe there here's an interesting way to ask this. Yeah, how many

03:11:34.400 --> 03:11:41.280
other domains in the world are like this, where you think you could get the equivalent of in one

03:11:41.280 --> 03:11:47.040
year, you just throw enough intelligence across multiple instances, and you just come out the

03:11:47.120 --> 03:11:54.720
other end with something that is remarkably decades centuries ahead. Yeah, like you start off with

03:11:54.720 --> 03:12:00.400
no flight and then the right brother is a million instances of GPT six, and you come out the other

03:12:00.400 --> 03:12:07.120
end with starlink. Yeah. Like is that your model of how things work? I think I think you're exaggerating

03:12:07.120 --> 03:12:10.400
the timelines a little bit, but but you know, I think you know, a decade's worth of progress in

03:12:10.400 --> 03:12:15.200
a year or something. I think that's a reasonable prompt. So I think this is where, you know,

03:12:15.840 --> 03:12:19.760
basically the sort of automated AI researcher comes in because it gives you this enormous tail

03:12:19.760 --> 03:12:23.600
headwind on all the other stuff, right? So it's like, you know, you automate AI research with

03:12:23.600 --> 03:12:27.280
your sort of automated Alec Radford's, you come out the other end, you've done another five booms,

03:12:27.280 --> 03:12:32.080
you have a thing that is like vastly smarter, not only is it vastly smarter, you like, you know,

03:12:32.080 --> 03:12:35.120
you've been able to make it good at everything else, right? You're like, you're solving robotics,

03:12:35.120 --> 03:12:38.560
the robots are important, right? Because like, for a lot of other things, you do actually need to

03:12:38.560 --> 03:12:42.560
like try things in the physical world. I mean, I don't know, maybe you can do a lot in simulation,

03:12:42.560 --> 03:12:46.080
those are the really quick worlds. I don't know if you saw the like last Nvidia GTC, you know,

03:12:46.080 --> 03:12:49.760
it was all about the like digital twins and just like having all your manufacturing processes and

03:12:49.760 --> 03:12:53.840
simulation, like, I don't know, like, again, if you have these like, you know, super intelligent,

03:12:53.840 --> 03:12:57.040
like cognitive workers, like, can they just like make simulations of everything, you know,

03:12:57.040 --> 03:13:01.600
kind of off the float style, and then, and then, you know, make a lot of progress and simulation

03:13:01.600 --> 03:13:07.440
possible. But I also just think you're going to get the robots. Again, I agree about like,

03:13:07.440 --> 03:13:12.160
there are a lot of real world bottlenecks, right? And so, you know, I don't know, it's quite possible

03:13:12.160 --> 03:13:15.520
that we're going to have, you know, crazy drone forms, but also, you know, like lawyers and

03:13:15.520 --> 03:13:20.640
doctors still need to be humans because of like, you know, regulation. But, you know, I think,

03:13:20.640 --> 03:13:24.400
you know, you kind of start narrowly, you broaden, and then the world's in which you kind of let them

03:13:24.400 --> 03:13:27.520
loose, which again, because of I think these competitive pressures, we will have to let them

03:13:27.520 --> 03:13:33.600
loose in some degree on, you know, various national security applications. I think like,

03:13:33.600 --> 03:13:37.760
quite rapid progress is possible. The other thing, though, is it's sort of, you know,

03:13:37.760 --> 03:13:40.880
basically in the sort of an explosion after there's kind of two components, there's the A,

03:13:40.880 --> 03:13:44.400
right in the production function, like growth of technology, and that's massively accelerated

03:13:44.400 --> 03:13:48.240
by, you know, you have a billion super intelligent scientists and engineers and technicians,

03:13:48.240 --> 03:13:52.720
you know, superbly competent and everything. You also just automated labor, right? And so,

03:13:52.720 --> 03:13:56.480
it's like, even without the whole technological explosion thing, you have this industrial explosion,

03:13:56.480 --> 03:13:59.840
at least if you let them let them loose, which is like, now you can just build, you know,

03:13:59.840 --> 03:14:03.840
you can cover Nevada and like, you know, you start with one robot factory is producing more robots,

03:14:03.840 --> 03:14:08.640
and basically this like just the cumulative process because you've taken labor out of the equation.

03:14:08.880 --> 03:14:14.720
Yeah. That's super interesting. Yeah. Although when you increase the

03:14:15.360 --> 03:14:22.400
K or the L without increasing the A, you can look at the Soviet Union or China where they

03:14:22.400 --> 03:14:27.920
rapidly increase inputs. Yeah. And that does have the effect of being geopolitically game changing,

03:14:27.920 --> 03:14:33.440
where you, it is remarkable, like you go to Shanghai over a set of these crazy cities in

03:14:33.440 --> 03:14:36.800
a decade. Right. Right. I mean, the closest thing to like, people talk about 30% growth

03:14:36.800 --> 03:14:43.200
rates or whatever. Yeah. 10%. It's totally possible. Yeah. But without productivity gains,

03:14:43.200 --> 03:14:47.360
it's not like the industrial revolution, where like, you're from the perspective of

03:14:47.360 --> 03:14:50.480
you're looking at a system from the outside, your goods have gotten cheaper, or they can

03:14:50.480 --> 03:14:55.680
manufacture more things. But, you know, it's not like the next century is coming at you.

03:14:55.680 --> 03:14:58.560
Yeah. It's both. It's both. So it's, you know, both that are important. The other thing I'll say

03:14:58.560 --> 03:15:02.480
is like, all this stuff, I think the magnitudes are really, really important, right? So,

03:15:03.280 --> 03:15:07.680
you know, we talked about a 10x of research effort, or maybe 10, 30x over a decade,

03:15:07.680 --> 03:15:11.920
you know, even without any kind of like self-improvement type loop, you know, we talk,

03:15:11.920 --> 03:15:15.520
the sort of, even in the sort of Jeep before the AGI story, we're talking about an order of magnitude

03:15:15.520 --> 03:15:19.280
of effective compute increase a year, right? Half an order of magnitude of compute, half an order

03:15:19.280 --> 03:15:23.680
of magnitude of algorithmic progress that sort of translates into effective compute. And so,

03:15:24.560 --> 03:15:28.480
you're doing a 10x a year, right? Basically on your labor force, right? So it's like,

03:15:28.480 --> 03:15:32.080
it's a radically different world if you're doing a 10x or 30x in a century versus a

03:15:32.080 --> 03:15:36.160
10x a year on your labor force. So the magnitudes really matter. They also really matter on the

03:15:36.160 --> 03:15:39.840
sort of intelligence explosion, right? So like just the automated AI research part. So, you know,

03:15:39.840 --> 03:15:43.920
one story you could tell there is like, well, ideas get harder to find, right? Algorithmic

03:15:43.920 --> 03:15:47.120
progress is going to get harder. Yeah, right now you have the easy wins, but in like four or five

03:15:47.120 --> 03:15:51.120
years, there'll be fewer easy wins. And so the sort of automated AI researchers are just going

03:15:51.120 --> 03:15:54.560
to be what's necessary to just keep it going, right? Because it's gotten harder. But that's

03:15:54.560 --> 03:15:57.920
sort of, it's like a really weird knife edge assumption economics where you assume it's just

03:15:58.000 --> 03:16:02.320
enough. But isn't that the equilibrium story you were just telling with why the economy as a whole

03:16:02.320 --> 03:16:06.400
has 2% economic growth? Because you just pursued on the equal, I guess you're saying by the time

03:16:06.400 --> 03:16:10.240
you get to the equilibrium here is it's like way faster, at least, you know, and it's at least,

03:16:10.240 --> 03:16:13.280
and it depends on the sort of exponents, but it's basically it's the increase that like,

03:16:13.280 --> 03:16:17.200
suppose you need to like 10x effective research effort in AI research in the last,

03:16:17.200 --> 03:16:20.480
you know, four or five years to keep the pace of progress, we're not just getting a 10x, you're

03:16:20.480 --> 03:16:23.680
getting, you know, a million x or a hundred thousand x, there's just the magnitudes really

03:16:23.680 --> 03:16:27.600
matter. And the magnitude is just basically, you know, one, one way to think about this is that

03:16:27.600 --> 03:16:31.520
you have kind of two exponentials, you have your sort of like normal economy that's growing at,

03:16:31.520 --> 03:16:36.000
you know, 2% a year, and you have a really AI economy, and that's going at like 10x a year.

03:16:36.000 --> 03:16:39.920
And it's starting out really small, but sort of eventually it's going to, it's just, it's, it's,

03:16:39.920 --> 03:16:43.920
it's way faster and eventually it's going to overtake, right? And even if you have, you can

03:16:43.920 --> 03:16:47.520
almost sort of just do the simple revenue extrapolation, right? If you think your AI economy,

03:16:47.520 --> 03:16:51.440
you know, that has some growth rate, I mean, it's very simplistic way and so on. But there's,

03:16:51.520 --> 03:16:55.040
there's this sort of 10x a year process and that will eventually kind of like,

03:16:55.040 --> 03:16:59.200
you're going to transition the sort of whole economy from, as it broadens from the sort of,

03:16:59.200 --> 03:17:04.320
you know, 2% a year to the sort of much faster growing process. And I don't know, I think that's

03:17:04.320 --> 03:17:09.600
very like consistent with historical change, you know, stories, right? There's this sort of like,

03:17:10.480 --> 03:17:14.880
you know, there's this sort of long run hyperbolic trend, you know, manifested in the sort of like,

03:17:14.880 --> 03:17:18.640
sort of change in growth mode and the austral, you know, revolution, but there's just this

03:17:18.640 --> 03:17:23.040
long run hyperbolic trend. And, you know, now you have this sort of, now you have that another

03:17:23.040 --> 03:17:26.480
sort of change in growth mode. Yeah, yeah. I mean, that was one of the questions I asked Tyler,

03:17:26.480 --> 03:17:33.600
when I had him on the podcast, is that you do go from the fact that after 1776, you go from a regime

03:17:33.600 --> 03:17:38.880
of negligible economic growth, 2% is really interesting. It shows that, I mean, from the

03:17:38.880 --> 03:17:44.160
perspective of somebody in the Middle Ages or before, 2% is equivalent to the sort of 10%.

03:17:44.960 --> 03:17:47.760
I guess you're projecting even higher for the AI economy, but

03:17:47.760 --> 03:17:51.200
yeah, I mean, I think again, and it's all this stuff, you know, I have a lot of uncertainty,

03:17:51.200 --> 03:17:54.480
right? So a lot of the time I'm trying to kind of tell the modal story. I think it's important

03:17:54.480 --> 03:17:58.800
to be kind of concrete and visceral. And I, you know, I have, I have a lot of uncertainty

03:17:58.800 --> 03:18:02.720
basically over how the 2030s play out. And basically, the thing I know is it's going to be

03:18:02.720 --> 03:18:07.760
fucking crazy. But, but, you know, exactly what, you know, where the bottlenecks are and so on,

03:18:07.760 --> 03:18:13.120
I think that will be kind of like. So let's talk through the numbers here. You hundreds of millions

03:18:13.120 --> 03:18:22.160
of AI researchers. So right now, GPT-40 turbo is like 15 bucks for a million tokens outputted.

03:18:22.160 --> 03:18:27.360
And a human thinks 150 tokens a minute or something. And if you do the math on that,

03:18:27.360 --> 03:18:34.880
I think it's for an hour's worth of human output. You, it's like 10 cents or something.

03:18:35.760 --> 03:18:40.240
Now, cheaper than a human worker. Cheaper than a human worker. But it can't do the job yet.

03:18:40.240 --> 03:18:43.920
That's right. That's right. But by the time you're talking about models that are trained on the 10

03:18:43.920 --> 03:18:50.160
gigawatt cluster, then you have something that is four orders of magnitude more expensive via

03:18:50.160 --> 03:18:55.520
inference, three orders of magnitude, something like that. So that's like $100 an hour of labor.

03:18:55.520 --> 03:19:01.760
And now you're having hundreds of millions of such laborers. Is there enough compute to do

03:19:01.760 --> 03:19:06.000
with the model that is a thousand times bigger, this kind of labor?

03:19:06.000 --> 03:19:09.840
Great. Okay. Great question. So I actually don't think inference costs for sort of frontier

03:19:09.840 --> 03:19:13.280
models are necessarily going to go up that much. So I mean, one historical data point.

03:19:13.280 --> 03:19:16.320
But isn't the test time sort of thing that it will go up even higher?

03:19:16.320 --> 03:19:19.200
I mean, we're just doing per token, right? And then I'm just saying, you know, if,

03:19:19.200 --> 03:19:23.440
suppose each model token was the same as sort of a human token thing at 100 tokens a minute.

03:19:23.440 --> 03:19:26.640
So it's like, yeah, it'll use more. But the sort of, if you just, the token calculations

03:19:26.640 --> 03:19:33.200
is already pricing that in the, the question is like per token pricing, right? And so like GPT-3

03:19:33.200 --> 03:19:39.120
went at launch was like actually more expensive than GPT-4 now. And so over just like, you know,

03:19:39.120 --> 03:19:42.480
fast increases in capability gains, inference costs has remained constant.

03:19:43.840 --> 03:19:47.440
That's sort of wild. And I think it's worth appreciating. And I think it gestures that

03:19:47.440 --> 03:19:52.080
sort of an underlying pace of algorithmic progress. I think there's a sort of like more

03:19:52.080 --> 03:19:55.840
theoretically grounded way to why, why inference costs would stay constant. And it's the fourth

03:19:55.840 --> 03:19:59.840
following story, right? So on Chichilly scaling laws, right, you, you know,

03:19:59.840 --> 03:20:04.000
half of the additional compute you allocate to bigger models and half of it you allocate to more

03:20:04.000 --> 03:20:08.720
data, right? But also if we go with the sort of basic story of half an order a year more

03:20:08.720 --> 03:20:12.560
compute and half an order of magnitude a year of algorithmic progress, you're also kind of like

03:20:12.560 --> 03:20:16.320
you're saving half an order of magnitude a year. And so that kind of would exactly compensate for

03:20:16.320 --> 03:20:20.240
making the model bigger. The caveat on that is, you know, obviously not all training efficiencies

03:20:20.240 --> 03:20:24.560
are also inference efficiencies, you know, bunch of the time they are separately, you can find

03:20:24.560 --> 03:20:28.480
inference efficiencies. So I don't know, given this historical trend, given the sort of like,

03:20:28.480 --> 03:20:35.360
you know, baseline sort of theoretical reason, you know, I don't know, I, I think it's not crazy

03:20:35.520 --> 03:20:38.800
baseline assumption that actually these models, frontier models are not necessarily going to get

03:20:38.800 --> 03:20:44.480
more expensive per token. Oh, really? Yeah. Like, okay, that's, that's wild. We'll see, we'll see.

03:20:44.480 --> 03:20:47.600
I mean, the other thing, you know, maybe they get, you know, even if they get like 10x more

03:20:47.600 --> 03:20:50.720
expensive than, you know, you have 10 million instead of 100 million, you know, so it's like,

03:20:50.720 --> 03:20:55.360
it's not really, you know, like, but okay, so part of the internal explosion is that each of them has

03:20:55.360 --> 03:21:03.520
to run experiments that are gb4 size and the result of, so that takes up a bunch of compute.

03:21:03.760 --> 03:21:07.840
Then you're going to consolidate the results of the experiments and what is the synthesized

03:21:08.720 --> 03:21:11.120
weight. I mean, you have a much bigger influence street anyway than your training.

03:21:11.120 --> 03:21:13.840
Sure. Okay. But I think the experiment compute is a constraint.

03:21:13.840 --> 03:21:19.040
Yeah. Okay. I'm going back to maybe a sort of bigger fundamental thing we're talking about here.

03:21:20.400 --> 03:21:28.400
We're projecting, in a series, you say we should denominate the probability of getting to AGI in

03:21:28.400 --> 03:21:34.240
terms of orders of magnitude of affected compute effective here, accounting for the fact that

03:21:34.240 --> 03:21:38.000
there's a compute quote unquote compute multiplier if you have better algorithms.

03:21:39.520 --> 03:21:47.200
And I'm not sure that it makes sense to be confident that this is a sensible way to project

03:21:47.200 --> 03:21:52.240
progress. It might be, but I'm just like, I have a lot of uncertainty about it. It seems similar to

03:21:52.240 --> 03:21:56.400
somebody trying to project when we're going to get to the moon and they're like looking at the

03:21:56.400 --> 03:22:01.520
Apollo program in the four fifties or something and they're like, we have some amount of effective

03:22:01.520 --> 03:22:09.200
jet fuel. And if we get more efficient engines, then we have more effective jet fuel. And so

03:22:09.200 --> 03:22:13.200
we're going to like probability of getting to the moon based on the amount of effective jet fuel we

03:22:13.200 --> 03:22:18.800
have. And I don't deny that jet fuel is important to launch rockets, but that seems like an odd

03:22:18.800 --> 03:22:23.760
way to denominate when you're going to get to the moon. Yeah. Yeah. So I mean, I think these cases

03:22:23.760 --> 03:22:26.880
are pretty different. I don't know. I didn't, I don't think there was a sort of clear, I don't

03:22:26.880 --> 03:22:30.080
know how rocket science works, but I didn't, I didn't get the impression that there's some

03:22:30.080 --> 03:22:35.520
clear scaling behavior with like, you know, the amount of jet fuel. I think the, I think in AI,

03:22:35.520 --> 03:22:39.680
you know, I mean, first of all, the scaling laws, you know, they've just helped, right? And so if

03:22:39.680 --> 03:22:43.120
you, a friend of mine pointed this out, and I think it's a great point, if you kind of concatenate

03:22:43.120 --> 03:22:47.440
both the sort of original Kaplan scaling laws paper that I think went from 10 to the negative

03:22:47.440 --> 03:22:53.040
nine to 10 petaflop days, and then, you know, concatenate additional compute to from there to

03:22:53.120 --> 03:22:56.800
kind of GP four, you assume some algorithmic progress, you know, it's like the scaling laws

03:22:56.800 --> 03:23:00.640
have held, you know, like probably over 15 ooms, you know, I know it was rough, probably maybe

03:23:00.640 --> 03:23:05.120
even more held for a lot of ooms. They held for the specific loss function, which they're training

03:23:05.120 --> 03:23:11.520
on, which is training max token. Whereas the, the, the progress you are forecasting will be

03:23:11.520 --> 03:23:16.640
required for further progress in capabilities. Yeah. It was specifically, we know that scaling

03:23:16.640 --> 03:23:20.400
can't work because of the data wall. And so there's some new thing that has to happen. And I'm not

03:23:20.480 --> 03:23:25.680
sure whether the, you can extrapolate that same scaling curve to tell us whether these

03:23:25.680 --> 03:23:29.440
hobblings will also, like it's, is this not on the same graph? The hobblings are just a separate

03:23:29.440 --> 03:23:33.600
thing. Yeah, exactly. So this is, this is sort of like, you know, it's, yeah. So I mean, a few,

03:23:33.600 --> 03:23:38.880
a few things here, right? Okay. So the, on the, on the effective compute scaling, the, you know,

03:23:38.880 --> 03:23:42.080
in some sense, I think it's like people center the scaling laws because they're easy to explain

03:23:42.080 --> 03:23:47.040
and the sort of like, why, why is scaling matter? The scaling laws like came way after people, at

03:23:47.040 --> 03:23:50.560
least, you know, like Dario, Ilya realized that scaling mattered. And I think, you know, I think

03:23:50.560 --> 03:23:54.800
that almost more important than the sort of loss curve is just like, just in general, make, you

03:23:54.800 --> 03:23:58.080
know, there's this great quote from Dario on your, on your, on your, on your podcasts, it's just like,

03:23:58.080 --> 03:24:01.520
you know, Ilya was like, the models, they just want to learn, you know, you make them bigger,

03:24:01.520 --> 03:24:06.320
they learn more. And, and that just applied just across domains, generally, you know, all the

03:24:06.320 --> 03:24:10.560
capabilities. And so, um, and you can look at this in benchmarks. Again, like you say,

03:24:10.560 --> 03:24:14.560
headwind data wall, I'm sort of bracketing that and talking about that separately.

03:24:14.640 --> 03:24:18.480
The other thing is on hobblings, right? If you just put them on the effective compute graph,

03:24:18.480 --> 03:24:21.360
these on hobblings would be kind of huge, right? So like, I think,

03:24:21.360 --> 03:24:24.080
What does it even mean? Like, what is it? What is on the y axis here?

03:24:24.960 --> 03:24:29.520
Like, say MLPR on this benchmark or whatever, right? And so, you know, like, you know, we mentioned

03:24:29.520 --> 03:24:33.920
the sort of, you know, the LMSYS differences, you know, RLHF, you know, again, as good as 100x

03:24:33.920 --> 03:24:37.360
small chain of thought, right? You know, just going from this prompting change, a simple argument

03:24:37.360 --> 03:24:41.600
change can be like 10x effective increase, compute increases on like math benchmarks.

03:24:41.600 --> 03:24:44.960
I think this is like, you know, I think this is useful to illustrate that on hobblings are

03:24:44.960 --> 03:24:49.040
large. Um, but I think they're like, I kind of think of them as like slightly separate things.

03:24:49.040 --> 03:24:53.120
And the kind of the way I think about is that like, at a per token level, I think GP four is

03:24:53.120 --> 03:24:58.560
not that far away from like a token of my internal monologue, right? Even like 3.5 to four took us

03:24:58.560 --> 03:25:02.400
kind of from like the bottom of the human range, the top of the human range on like a lot of,

03:25:02.400 --> 03:25:06.400
you know, on a lot of, uh, you know, kind of like high school tests. And so it's like a few more

03:25:06.400 --> 03:25:11.040
3.5 to four jumps per token basis, like per token intelligence. And then you've got to unlock the

03:25:11.040 --> 03:25:15.440
test time, you've got to solve the onboarding problem, make it use a computer. Um, and then

03:25:15.440 --> 03:25:21.440
you're getting real close. Um, I'm reminded of, again, the story might be wrong, but I think it

03:25:21.440 --> 03:25:25.600
is strikingly plausible. I agree. And so I think actually, I mean, the other thing I'll say is

03:25:25.600 --> 03:25:30.320
like, you know, I say this 2027 timeline, I think it's unlikely, but I do think there's worlds

03:25:30.320 --> 03:25:34.320
that are like AGI next year. And that's basically if the test time compute overhang is really easy

03:25:34.320 --> 03:25:38.400
to crack. If it's really easy to crack, then you do like four rooms of test and compute, you know,

03:25:38.400 --> 03:25:42.320
from a few hundred tokens to a few million tokens, you know, quickly. And then, you know, again,

03:25:42.320 --> 03:25:46.720
maybe it's maybe only takes one or two 3.5 to four jumps per token, like one or two of those

03:25:46.720 --> 03:25:51.120
jumps for token plus uses test time compute. And you basically have the proto automated engineer.

03:25:51.600 --> 03:25:59.280
Um, so I'm reminded of, uh, uh, Stephen Pinker releases his book on, um, what is it? The Better

03:25:59.280 --> 03:26:03.920
Angels of Our Nature. And it's like a couple of years ago or something. And he says the secular

03:26:04.000 --> 03:26:09.520
decline in violence and war and everything. And you can just like plot the line from the end of

03:26:09.520 --> 03:26:13.280
World War Two. And in fact, before World War Two, then these are just aberrations, whatever.

03:26:13.280 --> 03:26:19.920
And basically, as soon as it happens, Ukraine, Gaza, the, everything is like, so

03:26:19.920 --> 03:26:31.040
I think this is a sort of thing that happens in history where you see history align and you're

03:26:31.040 --> 03:26:35.440
like, oh my gosh. And then just like, as soon as you make that prediction, who was that famous

03:26:35.440 --> 03:26:39.280
author? So yeah, this, you know, again, people are predicting deep learning will hold a wall

03:26:39.280 --> 03:26:43.840
every year. Maybe one year, they're right. But it's like gone a long way and hasn't hit a wall.

03:26:43.840 --> 03:26:48.480
And I don't have that much more to go. And, and you know, so yeah, I guess I think this is a sort

03:26:48.480 --> 03:26:54.400
of plausible story. And let's just run with it and see what it implies. Yeah. So we were talk

03:26:54.400 --> 03:27:00.880
in your series, you talk about a lineman from the perspective of this is not about

03:27:00.880 --> 03:27:06.080
some doomer scheme to get the point zero and personal probability distribution,

03:27:06.080 --> 03:27:10.160
where things don't go off the rails. It's more about just controlling the systems,

03:27:10.160 --> 03:27:15.920
making sure they do what we intend them to do. If that's the case, and we're going to be in the

03:27:15.920 --> 03:27:22.320
sort of geopolitical conflict with China, and part of that will involve and what we're worried

03:27:22.320 --> 03:27:28.800
about is them making the CCP bots that go out and take the red flag of Mao across the galaxies

03:27:28.800 --> 03:27:37.280
or something. Then shouldn't we be worried about alignment as something that if in the wrong hands,

03:27:37.280 --> 03:27:44.400
this is the thing that enables brainwashing, sort of dictatorial control. This seems like a

03:27:44.400 --> 03:27:48.160
worrying thing. This should be part of this sort of algorithmic secrets we keep hidden, right?

03:27:48.160 --> 03:27:52.000
The how to align these models, because that's also something the CCP can use to control their

03:27:52.000 --> 03:27:55.280
models. I mean, I think in the world where you get the Democratic coalition, yeah, I mean,

03:27:55.280 --> 03:27:59.360
also just alignment is often dual use, right? Like RLHF, it's like alignment team developed,

03:27:59.360 --> 03:28:03.440
it was great, it was a big win for alignment, but it's also obviously makes these models useful.

03:28:06.240 --> 03:28:11.520
But yeah, so yeah, alignment enables the CCP bots. Alignment also is what you need to get the

03:28:11.520 --> 03:28:18.480
sort of whatever USAIs, follow the Constitution and disobey unlawful orders and respect separation

03:28:18.480 --> 03:28:24.320
of powers and checks and balances. So yeah, you need alignment for whatever you want to do. It's

03:28:24.320 --> 03:28:28.240
the sort of underlying technique. Tell me what you make of this take. I'm going to stream with

03:28:28.240 --> 03:28:34.080
this a little bit. Okay. So fundamentally, there's many different ways the future could go. Yeah.

03:28:34.080 --> 03:28:40.080
There's one path in which the LA's are type crazy AI's with nanobots take the future and

03:28:40.080 --> 03:28:45.200
determine everything to gray goo or paper clips. And the more you solve alignment, the more that

03:28:45.200 --> 03:28:50.160
path of the decision tree is circumscribed. And then so the more you solve alignment,

03:28:50.160 --> 03:28:53.840
the more it is just different humans and divisions they have. And of course,

03:28:53.920 --> 03:28:57.440
we know from history that things don't turn out the way you expect. So it's not like you can decide

03:28:57.440 --> 03:29:01.280
the future, but it will appear. It's part of the beauty of it, right? You want these mechanisms,

03:29:01.280 --> 03:29:05.920
the error correction, pluralism. But from the perspective of anybody who's looking at the system,

03:29:05.920 --> 03:29:11.280
it will be like, I can control where this thing is going to end up. And so the more you solve

03:29:11.280 --> 03:29:18.000
alignment and the more you circumscribe the different futures that are the results of AI will,

03:29:18.000 --> 03:29:23.520
the more that accentuates the conflict between humans and their visions of the future. And so

03:29:23.520 --> 03:29:27.600
in the world where alignment is solved and the world in which alignment is solved is the one,

03:29:27.600 --> 03:29:30.720
is the world in which you have the most sort of human conflict over where to take AI.

03:29:31.280 --> 03:29:34.880
Yeah. I mean, by removing the worlds in which the AI's take over, then like, you know,

03:29:34.880 --> 03:29:38.240
the remaining worlds are the ones where it's like the humans decide what happens. And then

03:29:38.240 --> 03:29:42.320
as we talked about, there's a whole lot of, yeah, a whole lot of worlds and how that could go.

03:29:42.320 --> 03:29:46.480
And I worry. So when you think about alignment, and this is just controlling these things,

03:29:46.880 --> 03:29:54.160
just think a little forward. And there's worlds in which hopefully, you know, human

03:29:54.160 --> 03:29:58.960
descendants or some version of things in the future merge with super intelligences and they

03:29:58.960 --> 03:30:05.520
have the rules of their own, but they're in some sort of law and market based order. I worry about

03:30:05.520 --> 03:30:11.520
if you have things that are conscious and should be treated with rights. If you read about what

03:30:11.520 --> 03:30:15.840
alignment schemes actually are, and then you read these books about what actually happened

03:30:15.920 --> 03:30:20.720
during the Cultural Revolution, what happened when Stalin took over Russia, and you have

03:30:21.840 --> 03:30:26.960
very strong monitoring from different instances where one, everybody's tasked with watching

03:30:26.960 --> 03:30:32.560
each other. You have brainwashing, you have red teaming, where you have the spice stuff

03:30:32.560 --> 03:30:36.320
you were talking about, where you try to convince somebody you're on like a defector and you see

03:30:36.320 --> 03:30:41.920
if they defect with you. And if they do, then you realize they're an enemy. And listen, maybe

03:30:41.920 --> 03:30:48.560
I'm stretching the analogy too far, but the way, like the ease of these alignment techniques

03:30:48.560 --> 03:30:53.040
actually map on to something you could have read about during like mouse culture revolution

03:30:53.040 --> 03:30:57.760
is a little bit troubling. Yeah, I mean, look, I think sentient AI is a whole other topic. I

03:30:57.760 --> 03:31:01.200
don't know if we want to talk about it. I agree that like it's going to be very important how we

03:31:01.200 --> 03:31:06.080
treat them. You know, in terms of like what you're actually programming these systems to do, again,

03:31:06.080 --> 03:31:10.560
it's like alignment is just, it's a technical, it's a technical problem, a technical solution,

03:31:10.560 --> 03:31:15.760
enables the CCP bots. I mean, in some sense, I think the, you know, I almost feel like the

03:31:15.760 --> 03:31:18.800
sort of model and also about talking about checks and balances is sort of, you know, like the Federal

03:31:18.800 --> 03:31:22.560
Reserve or Supreme Court justices. And there's a funny way in which they're kind of this like

03:31:22.560 --> 03:31:25.840
very dedicated order, you know, Supreme Court justices, and it's amazing, they're actually

03:31:25.840 --> 03:31:30.480
quite high quality, right? And they like really smart people, they really believe in the Constitution,

03:31:30.480 --> 03:31:34.000
they love the Constitution, they believe in their principles, they have, you know, these, these,

03:31:34.000 --> 03:31:37.920
these wonderful, you know, back, you know, and yeah, they have different persuasions, but they

03:31:37.920 --> 03:31:41.920
have sort of, I think very sincere kind of debates about what is the meaning of the Constitution,

03:31:41.920 --> 03:31:46.000
you know, what is the best actuation of these principles? You know, I guess, you know, by the

03:31:46.000 --> 03:31:50.160
way, recommendation sort of skittish or arguments is like the best podcast, you know, when I run

03:31:50.160 --> 03:31:54.000
out of high quality content on the Internet. I mean, I think there's going to be a process of

03:31:54.000 --> 03:31:57.120
like figuring out what the Constitution should be. I think, you know, this Constitution has like

03:31:57.120 --> 03:32:00.480
worked for a long time, you start with that, maybe eventually things change enough that you want

03:32:00.480 --> 03:32:04.400
added to that. But anyway, you want them to like, you know, for example, for the checks and balances,

03:32:04.400 --> 03:32:08.160
they like, they really love the Constitution, and they believe in it, and they take it really

03:32:08.160 --> 03:32:12.640
seriously. And like, look, at some point, yeah, you are going to have like AI police and AI military,

03:32:12.640 --> 03:32:17.360
but I think sort of like, you know, being able to ensure that they like, you know, believe in it

03:32:17.360 --> 03:32:20.800
in the way that like a Supreme Court justice does, or like in the way that like a federal reserve

03:32:20.800 --> 03:32:26.480
job, you know, official takes their job really seriously. Yeah. And I guess a big open question

03:32:26.480 --> 03:32:30.400
is whether if you do the project or something like the project, the other important thing is

03:32:30.400 --> 03:32:34.240
like a bunch of different factions need their own AIs, right? And so it's, it's really important

03:32:34.240 --> 03:32:37.440
that like each political party gets to like, have their own, you know, and like, whatever

03:32:37.440 --> 03:32:40.560
creeds it, you might totally disagree with their values, but it's like, it's really important

03:32:40.560 --> 03:32:44.480
that they get to like, have their own kind of like super intelligence. And, and again, I think

03:32:44.480 --> 03:32:48.320
it's that these sort of like classical liberal processes play out, including like, different

03:32:48.320 --> 03:32:52.240
people of different persuasions and so on. And I don't know, maybe the advisors might not make

03:32:52.240 --> 03:32:56.320
them, you know, wise, they might not follow the advice or whatever, but I think it's important.

03:32:56.320 --> 03:33:00.960
Okay. So speaking of alignment, you seem pretty optimistic. So let's run, run through

03:33:01.600 --> 03:33:06.800
the source of the optimism. Yeah. I think there you laid out different worlds in which we could

03:33:06.800 --> 03:33:11.360
get AI. Yeah. There's one that you think is low probability of next year, where a GPT-4 plus

03:33:11.360 --> 03:33:16.400
scaffolding plus unhoplings gets you to AGI. Not GPT-4, you know, like, oh, sorry, sorry. It's

03:33:16.400 --> 03:33:21.680
a GP-5. Yeah. Yeah. And there's ones where it takes much longer. There's ones where it's

03:33:21.680 --> 03:33:27.760
something that's a couple years. Yeah. In a modal world. Yeah. So GPT-4 seems pretty aligned in

03:33:27.760 --> 03:33:31.600
the sense that I don't expect it to go off the rails. Yeah. Maybe with scaffolding things might

03:33:31.600 --> 03:33:37.600
change. Looks pretty good. Yeah. Exactly. So the, and maybe you will keep turning at, there's cranks

03:33:37.600 --> 03:33:43.440
to keep going up and one of the cranks gets you to ASI. Yeah. Is there any point at which the

03:33:43.440 --> 03:33:48.880
sharp left turn happens? Is it when you start, is it the case that you think plausibly when they

03:33:49.120 --> 03:33:53.520
act more like agents, this is the thing to worry about? Yeah. Is there anything qualitatively

03:33:53.520 --> 03:33:56.640
that you expect to change with regards to the enlightenment perspective at these cranks?

03:33:56.640 --> 03:33:59.760
Yeah. So I don't know if I believe in this concept of sharp left turn, but I do think there's

03:33:59.760 --> 03:34:03.520
basically, I think there's important qualitative changes that happen between now and kind of like

03:34:03.520 --> 03:34:07.440
somewhat super human systems, kind of like early on the intelligence explosion. And then important

03:34:07.440 --> 03:34:11.680
qualitative changes that happen from like early in intelligence explosion to kind of like true

03:34:11.680 --> 03:34:15.600
super intelligence and all its power and might. And let's talk about both of those.

03:34:16.480 --> 03:34:19.920
And so, okay. So the first part of the problem is one, we're going to have to solve ourselves,

03:34:19.920 --> 03:34:22.880
right? We have to kind of have to align the like initial AI and the intelligence explosion,

03:34:22.880 --> 03:34:26.880
you know, the sort of automated out of Bradford. I think there's kind of like, I mean, two important

03:34:26.880 --> 03:34:33.040
things that change from GBD4, right? So one of them is, if you believe the story on like,

03:34:33.040 --> 03:34:37.600
you know, synthetic data or L or self play to get past the data wall, and if you believe this on

03:34:37.600 --> 03:34:41.120
hobbling story, you know, at the end, you're going to have things, you know, they're agents,

03:34:41.120 --> 03:34:46.240
right? Including they do long term plans, right? They have long, long, you know, they're somehow

03:34:46.240 --> 03:34:49.280
they're able to act over long horizons, right? But you need that, right? That's the sort of

03:34:49.280 --> 03:34:55.200
prerequisite to be able to do the sort of automated AI research. And so, you know, I think

03:34:55.200 --> 03:34:58.640
there's basically, you know, I basically think sort of pre training is sort of alignment neutral

03:34:58.640 --> 03:35:01.920
in the sense of like, it has all these representations that has good representations

03:35:01.920 --> 03:35:06.480
that you know, as representations of doing bad things, you know, but but there's, there's,

03:35:06.480 --> 03:35:11.040
it's not like, you know, scheming against you or whatever. I think the sort of misalignment can

03:35:11.040 --> 03:35:15.360
arise once you're doing more kind of long horizon training, right? And so you're training, you know,

03:35:15.360 --> 03:35:18.960
again, two simplified example, but to kind of illustrate, you know, you're training an AI to

03:35:18.960 --> 03:35:23.920
make money. And, you know, if you're just doing that with reinforcement learning, you know, it's,

03:35:23.920 --> 03:35:28.960
you know, it might learn to commit fraud or lie or to see you for seek power, simply because those

03:35:28.960 --> 03:35:32.320
are successful strategies in the real world, right? So maybe, you know, RL is basically it

03:35:32.320 --> 03:35:36.560
explores, maybe it figures out like, oh, it tries to like hack, and then it gets some money and that

03:35:36.560 --> 03:35:39.680
made more money, you know, and then if that's successful, if that gets reward, that's just

03:35:39.680 --> 03:35:43.440
reinforced. So basically, I think there's sort of more serious misalignments, kind of like

03:35:43.440 --> 03:35:48.320
misaligned long term goals that could arise between now and or that sort of necessarily

03:35:48.320 --> 03:35:51.760
have to be able to arise if you're able to get long horizon system, that's one.

03:35:52.880 --> 03:35:56.560
What you want to do in that situation is you want to add side constraints, right? So you want to add,

03:35:56.560 --> 03:36:01.920
you know, don't lie, don't deceive, don't commit fraud. And so how do you add those side constraints,

03:36:01.920 --> 03:36:05.520
right? The sort of basic idea you might have is like RLHF, right? You're kind of like, yeah,

03:36:05.520 --> 03:36:09.120
it has this goal of like, you know, make money or whatever, but you're watching what it's doing,

03:36:09.120 --> 03:36:12.320
it starts trying to like, you know, lie or deceive or fraud or whatever,

03:36:12.320 --> 03:36:15.600
break the law, you're just kind of like, thumbs down, don't do that, you anti reinforce that.

03:36:16.480 --> 03:36:20.160
The sort of critical issue that comes in is that these eye systems are getting superhuman,

03:36:20.160 --> 03:36:23.920
right? And they're going to be able to do things that are too complex for humans to evaluate,

03:36:23.920 --> 03:36:28.000
right? So again, even early on, you know, in the intelligence explosion, the automated AI

03:36:28.000 --> 03:36:31.200
researchers and engineers, you know, they might write millions, you know, billions,

03:36:31.200 --> 03:36:34.480
trillions of lines of complicated code, you know, they might be doing all sorts of stuff,

03:36:34.480 --> 03:36:39.040
you just like, don't understand anymore. And so, you know, in the million lines of code,

03:36:39.040 --> 03:36:42.560
you know, is it somewhere kind of like, you know, hacking, hacking, or like exultating itself,

03:36:42.560 --> 03:36:45.920
or like, you know, trying to go for the nukes or whatever, you know, like, you don't know anymore,

03:36:45.920 --> 03:36:50.400
right? And so this sort of like, thumbs up, thumbs down, pure RLHF doesn't fully work anymore.

03:36:51.200 --> 03:36:55.280
Second part of the picture, and maybe talk more about this first part of the picture,

03:36:55.280 --> 03:36:57.920
I think it's going to be like, there's a hard technical problem of what do you do,

03:36:57.920 --> 03:37:01.280
sort of post RLHF, but I think it's a solvable problem. And it's like, you know,

03:37:01.280 --> 03:37:04.720
there's various things in bullish on, I think there's like ways in which deep learning has

03:37:04.720 --> 03:37:08.720
shaped out favorably. The second part of the problem is you're going from your like initial

03:37:08.720 --> 03:37:11.920
systems and intelligence explosion to like super intelligence, and you know, it's like,

03:37:11.920 --> 03:37:16.400
many ooms, it ends up being like, by the end of it, you have a thing that's vastly smarter than humans.

03:37:18.000 --> 03:37:21.760
I think the intelligence explosion is really scary from an alignment point of view,

03:37:21.760 --> 03:37:25.040
because basically, if you have this rapid intelligence explosion, you know, less than a

03:37:25.040 --> 03:37:28.720
year or two years or whatever, you're going say in the period of a year from systems where like,

03:37:28.720 --> 03:37:32.640
you know, failure would be bad, but it's not kind of strapped back to like, you know, saying a bad

03:37:32.640 --> 03:37:38.320
word, it's like, you know, it's, it's something goes awry to like, you know, failure is like,

03:37:38.320 --> 03:37:41.920
you know, it extra traded itself, it starts hacking the military can do really bad things.

03:37:41.920 --> 03:37:44.960
You're going less than a year from sort of a world in which like, you know,

03:37:44.960 --> 03:37:48.560
it's some descendant of current systems, and you kind of understand it, and it's like, you know,

03:37:48.560 --> 03:37:51.840
it has good properties. There's something that potentially has a very sort of alien and different

03:37:51.840 --> 03:37:56.160
architecture, right? After having gone through another decade of imal advances. I think one

03:37:56.160 --> 03:38:01.280
example there that's very salient to me is legible and faithful chain of thought, right? So a lot

03:38:01.280 --> 03:38:04.160
of the time when we're talking about these things, we're talking about, you know, it has tokens of

03:38:04.160 --> 03:38:08.880
thinking, and then it uses many tokens of thinking. And, you know, maybe we bootstrap ourselves by,

03:38:08.880 --> 03:38:12.320
you know, it's pre-trained, it learns to think in English, and we do something else on top,

03:38:12.320 --> 03:38:18.240
so it can do the sort of longer chains of thought. And so, you know, it's very plausible to me that

03:38:18.320 --> 03:38:21.600
like, for the initial automated alignment researchers, you know, we don't need to do any

03:38:21.600 --> 03:38:25.600
complicated mechanistic interpretability, and just like literally you read what they're thinking,

03:38:25.600 --> 03:38:32.960
which is great. You know, it's like huge advantage, right? However, I'm very likely not the most

03:38:32.960 --> 03:38:36.800
efficient way to do it, right? There's like probably some way to have a recurrent architecture.

03:38:36.800 --> 03:38:39.840
It's all internal states. There's a much more efficient way to do it. That's what you get

03:38:39.840 --> 03:38:44.880
by the end of the year. You know, you're going this year from like RLHF plus plus some extension

03:38:44.880 --> 03:38:51.200
works to like, it's vastly superhuman. It's like, you know, it's to us like, you know, an expert

03:38:51.200 --> 03:38:55.440
in the field might be to like an elementary school or middle schooler. And so, you know,

03:38:55.440 --> 03:38:59.920
I think it's this sort of incredibly sort of like, hairy period for alignment.

03:39:00.960 --> 03:39:05.040
Thing you do have is you have the automated AI researchers, right? And so, you can use the

03:39:05.040 --> 03:39:11.440
automated AI researchers to also do alignment. And so, in this world, why are we optimistic

03:39:11.440 --> 03:39:18.160
that the project is being run by people who are thinking, I think, so here's something to think

03:39:18.160 --> 03:39:26.240
about. The open AI starts off with people who are very explicitly thinking about exactly these

03:39:26.240 --> 03:39:31.760
kinds of things, right? But are they still there? No, no, but you're still here. Here's the thing.

03:39:31.760 --> 03:39:35.520
No, no, even the people who are there, even like the current leadership is like exactly these things

03:39:35.520 --> 03:39:41.840
that can find them in interviews in their blog posts talking about. And what happens is when,

03:39:41.840 --> 03:39:46.880
as you were talking about, when some sort of trivial, and Yon talked about it, this is not just

03:39:46.880 --> 03:39:53.440
you, Yon talked about in his tweet thread, when there is some trade off that has to be made with

03:39:53.440 --> 03:39:56.800
we need to do this flashy release this week and not next week, because whatever Google

03:39:56.800 --> 03:40:00.960
IO is the next week, so we're going to get it. And then the trade off is made in favor of

03:40:01.680 --> 03:40:10.880
the less the more careless decision. When we have the government or the national security advisor,

03:40:10.880 --> 03:40:15.520
the military or whatever, which is much less familiar with this kind of discourse is a

03:40:15.520 --> 03:40:19.360
naturally thinking in this way about how I'm worried the chain of thought isn't faithful and

03:40:19.360 --> 03:40:23.760
how do we think about the features that are represented here? Why should it be optimistic

03:40:23.760 --> 03:40:29.200
that a project run by people like that will be thoughtful about these kinds of considerations?

03:40:29.440 --> 03:40:42.640
I mean, they might not be. I agree. I think a few thoughts, right? First of all, I think the private

03:40:42.640 --> 03:40:46.800
world, even if they sort of nominally care is extremely tough for alignment, a couple of reasons.

03:40:46.800 --> 03:40:50.240
One, you just have the race between the sort of commercial labs, right? And it's like, you don't

03:40:50.240 --> 03:40:53.600
have any headroom there to like be like, actually, we're going to hold back for three months, like

03:40:53.600 --> 03:40:57.600
get this right. And we're going to dedicate 90% of our compute to automated alignment research

03:40:57.600 --> 03:41:02.080
instead of just like pushing the next zoom. The other thing, though, is like in the private world,

03:41:02.080 --> 03:41:05.280
you know, China has stolen your age, China has your secrets, they're right on your tails,

03:41:05.280 --> 03:41:10.560
you're in this fever struggle, no room at all for maneuver. They're like the way it's like

03:41:10.560 --> 03:41:14.080
absolutely essential to get alignment right and you get it during this intelligence explosion,

03:41:14.080 --> 03:41:17.040
you get it right, because you need to have that room to maneuver and you need to have that clear

03:41:17.040 --> 03:41:23.680
lead. And, you know, again, maybe you've made the deal or whatever, but I think you're an incredibly

03:41:23.680 --> 03:41:28.320
tough space, tough spot if you don't have this clearly. So I think the sort of private world

03:41:28.320 --> 03:41:31.680
is kind of rough there on like whether people will take it seriously, you know, I don't know,

03:41:31.680 --> 03:41:36.480
I have some faith in sort of sort of normal mechanisms of a liberal society, sort of if

03:41:36.480 --> 03:41:40.080
alignment is an issue, which, you know, we don't fully know yet, but sort of the science will

03:41:40.080 --> 03:41:44.000
develop, we're going to get better measurements of alignment, you know, and the case will be

03:41:44.000 --> 03:41:48.880
clear and obvious. I worry that there's, you know, I worry about worlds where evidence is

03:41:48.880 --> 03:41:53.200
ambiguous. And I think a lot of a lot of the most scary kind of intelligence explosion scenarios are

03:41:53.200 --> 03:41:57.760
worlds in which evidence is ambiguous. But again, it's sort of like, I, if evidence is ambiguous,

03:41:57.760 --> 03:42:01.040
then that's the world in which you really want the safety margins. And that's also the world in

03:42:01.040 --> 03:42:04.240
which kind of like running the intelligence explosion is sort of like, you know, running a war,

03:42:04.240 --> 03:42:08.320
right? It's like, the evidence is ambiguous, we have to make these really tough trade offs.

03:42:08.320 --> 03:42:11.680
And you like, you better have a really good chain of command for that. And it's not just like, you

03:42:11.680 --> 03:42:16.160
know, you're going at, well, let's go, you know, it's cool. Yeah. Let's talk a little bit about

03:42:16.160 --> 03:42:22.640
Germany. We're making the analogy to World War Two. And you made a really interesting point,

03:42:22.640 --> 03:42:37.360
many hours ago. The fact that throughout history, World War Two is not unique, at least when you

03:42:37.360 --> 03:42:46.480
think in proportion to the size of the population. But these other sorts of catastrophes where

03:42:46.480 --> 03:42:52.480
some significant portion of the population has been killed off. After that, the nation recovers

03:42:52.560 --> 03:42:58.560
and they get back to their heights. And so what's interesting after World War Two

03:42:59.200 --> 03:43:03.600
is that Germany especially, and maybe Europe as a whole, obviously they experienced

03:43:03.600 --> 03:43:09.600
fast economic growth in the direct aftermath because of catch-up growth. But subsequently,

03:43:09.600 --> 03:43:15.040
we just don't think of Germany as, we're not talking about Germany potentially launching an

03:43:15.040 --> 03:43:18.560
intelligence explosion and they're going to get into the AI table. We were talking about

03:43:18.560 --> 03:43:21.440
Iran and North Korea and Russia. We didn't talk about Germany, right?

03:43:21.440 --> 03:43:22.640
Well, because they're allies.

03:43:22.640 --> 03:43:29.040
Yeah. But so what happened? I mean, World War Two and now it didn't like come back

03:43:29.040 --> 03:43:30.720
over the seven years war or something, right?

03:43:30.720 --> 03:43:34.320
Yeah. Yeah. Yeah. I mean, look, I'm generally very bearish on Germany. I think in this context,

03:43:34.320 --> 03:43:37.040
I'm kind of like, you know, it's a little bit, you know, I think you're underrating a little bit.

03:43:37.040 --> 03:43:40.160
I think it's probably still one of the, you know, top five most important countries in the world.

03:43:41.200 --> 03:43:45.200
You know, I mean, Europe overall, you know, it still has, I mean, it's a GDP that's like

03:43:45.200 --> 03:43:50.480
close to the United States the size of the GDP, you know, and there's things actually that Germany

03:43:50.480 --> 03:43:55.040
is kind of good at, right? Like state capacity, right? Like, you know, the, you know, the roads

03:43:55.040 --> 03:43:59.520
are good and they're clean and they're well maintained and, you know, in some sense, the sort

03:43:59.520 --> 03:44:03.520
of, a lot of this is the sort of flip side of things that I think are bad about Germany, right?

03:44:03.520 --> 03:44:06.560
So in the US, it's a little bit like there's a bit more of a sort of Wild West feeling to the

03:44:06.560 --> 03:44:11.600
United States, right? And it includes the kind of like crazy bursts of creativity. It includes like,

03:44:11.600 --> 03:44:16.960
you know, political candidates that are sort of, you know, there's a much broader spectrum and,

03:44:16.960 --> 03:44:20.480
you know, much, you know, like both in Obama and Trump as somebody you just wouldn't see in the sort

03:44:20.480 --> 03:44:24.800
of much more confined kind of German political debate. You know, I wrote this blog post at some

03:44:24.800 --> 03:44:29.680
point, Europe's political stupor about this. But anyway, and so there's this sort of punctilious

03:44:29.680 --> 03:44:33.440
sort of rule following that is like good in terms of like, you know, keeping your kind of state

03:44:33.440 --> 03:44:41.120
capacity functioning. But that is also, you know, I think I kind of think there's a sort of very

03:44:41.120 --> 03:44:46.320
constrained view of the world in some sense. You know, and that includes kind of, you know,

03:44:46.320 --> 03:44:50.080
I think after World War Two, there's a real backlash against anything like elite, you know,

03:44:50.080 --> 03:44:55.920
and, you know, again, no, you know, no elite high schools or elite colleges and sort of

03:44:55.920 --> 03:44:57.600
what my is that the law?

03:44:57.600 --> 03:44:59.680
excellence isn't cherished, you know, there's a yeah.

03:44:59.680 --> 03:45:06.800
Why is that the logical intellectual thing to rebel against if what if you're trying to

03:45:06.800 --> 03:45:10.560
overcorrect from the Nazis? Yeah, was it because the Nazis were very much into elitism?

03:45:11.200 --> 03:45:14.240
I don't understand why that's a logical sort of counter reaction.

03:45:14.240 --> 03:45:17.840
I know, maybe it was sort of a counter reaction against the sort of like whole like Aryan race

03:45:17.840 --> 03:45:21.520
and sort of that sort of thing. I mean, I also just think there was a certain amount in what

03:45:21.520 --> 03:45:26.240
amount certain, I mean, look at sort of World War One, end of World War One versus end of World

03:45:26.240 --> 03:45:31.280
War Two for Germany, right? And sort of, you know, a common narrative is that the piece of Versailles,

03:45:31.280 --> 03:45:36.160
you know, was too strict on Germany. You know, the piece imposed after World War Two was like

03:45:36.160 --> 03:45:40.160
much more strict, right? It was a complete, you know, the whole country was destroyed,

03:45:40.160 --> 03:45:43.920
you know, it was, you know, and all the main, most of the major cities, you know, over half of

03:45:43.920 --> 03:45:48.320
the housing stock had been destroyed, right? Like, you know, in some birth cohorts, you know,

03:45:48.320 --> 03:45:52.960
like 40% of the men had died. Half the population displaced. Oh, yeah. I mean,

03:45:52.960 --> 03:45:57.360
almost 20 million people are displaced, huge, crazy, right? You know, like,

03:45:57.360 --> 03:46:01.440
and the borders are way smaller than the Versailles borders. Yeah, exactly. And sort of

03:46:01.440 --> 03:46:06.480
complete imposition of a new political system and, and, you know, on both sides, you know, and

03:46:06.480 --> 03:46:13.840
yeah, so it was, but in some sense that worked out better than the post-World War One piece,

03:46:14.400 --> 03:46:18.240
where then there was this kind of resurgence of German nationalism and, you know, in some sense,

03:46:18.240 --> 03:46:21.120
the thing that has been a pattern. So it's sort of like, it's unclear if you want to wake the

03:46:21.120 --> 03:46:25.280
sleeping beast. I do think that at this point, you know, it's gotten a bit too sleepy. Yeah.

03:46:27.440 --> 03:46:30.560
I do think it's an interesting point about we underrate the American political system. Yeah.

03:46:30.640 --> 03:46:36.080
I've been making the same correction myself. Yeah. There's, there was this book about

03:46:36.080 --> 03:46:41.840
burdened by a Chinese economist called China's World View. And overall, I wasn't a big fan,

03:46:41.840 --> 03:46:48.480
but they made a really interesting point in there, which was the way in which candidates rise up

03:46:48.480 --> 03:46:56.080
through the Chinese hierarchy for politics, for administration, in some sense, that selects for

03:46:56.080 --> 03:46:58.960
you're not going to get some Marjorie Taylor Greene or somebody running some.

03:47:00.160 --> 03:47:04.560
Don't get that in Germany either. Right. Yeah. But you're, he explicitly made the point in the

03:47:04.560 --> 03:47:08.320
book that that also means we're never going to get a Henry Kissinger or Barack Obama. Right.

03:47:08.320 --> 03:47:13.280
In China. We're going to get like, by the time they end up in charge of the, the Politburo,

03:47:13.280 --> 03:47:16.960
the Politburo, there'll be like some 60 year old Democrat who's never like ruffled any feathers.

03:47:16.960 --> 03:47:19.920
Yeah. Yeah. Yeah. I mean, I think, I think there's something really important about the sort of

03:47:19.920 --> 03:47:24.800
like very raucous political debate. And I mean, yeah, in general, kind of like, you know, there's

03:47:24.800 --> 03:47:28.320
the sense in which in America, you know, lots of people live in their kind of like own world.

03:47:28.320 --> 03:47:32.560
I mean, like we live in this kind of bizarre little like bubble in San Francisco and people,

03:47:33.360 --> 03:47:38.480
you know, and, and, but I think that's important for the sort of evolution of ideas,

03:47:38.480 --> 03:47:43.280
error correction and that sort of thing. You know, there's other ways in which the

03:47:43.280 --> 03:47:47.920
German system is more functional. Yeah. But it's interesting that there's major mistakes,

03:47:47.920 --> 03:47:50.800
right? Like the sort of defense spending, right? And you know, then, you know, Russia

03:47:50.800 --> 03:47:55.440
made Ukraine and, and you're like, wow, what did we do? Right? No, that's a really good point,

03:47:55.440 --> 03:48:01.840
right? The main issues, there's everybody agrees, but exactly. Yeah. So consensus blob kind of thing.

03:48:01.840 --> 03:48:05.920
Right. And on the China point, you know, just having this experience of like reading German

03:48:05.920 --> 03:48:09.600
newspapers, and I think how much, you know, how much more poorly I would understand the sort of

03:48:09.600 --> 03:48:15.520
German debate and sort of the sort of state of mind from just kind of afar. I worry a lot about,

03:48:15.520 --> 03:48:20.480
you know, or I think it is interesting just how kind of impenetrable China is to me.

03:48:20.800 --> 03:48:24.640
It's a billion people, right? And like, you know, almost everything else is really globalized.

03:48:24.640 --> 03:48:28.480
You have a globalized internet and I kind of, I kind of a sense what's happening in the UK.

03:48:28.480 --> 03:48:31.040
You know, I probably, even if I didn't read German newspapers, just sort of would have a

03:48:31.040 --> 03:48:35.840
sense of what's happening in Germany. But I really don't feel like I have a sense of what like,

03:48:37.120 --> 03:48:40.960
you know, what is the state of mind or what are the state of political debate, you know,

03:48:40.960 --> 03:48:45.520
of a sort of average Chinese person or like an average Chinese leader. And yeah, I think that,

03:48:45.520 --> 03:48:49.680
that I find that distance kind of worrying. And I, you know, and there's, you know, there's

03:48:49.680 --> 03:48:52.880
some people who do this and they do really great work where they kind of go through the like party

03:48:52.880 --> 03:48:57.360
documents and the party speeches. And it seems to require a kind of a lot of interpretive ability

03:48:57.360 --> 03:49:01.120
where there's like very specific words and mentoring that like mean we'll have one connotation,

03:49:01.120 --> 03:49:05.840
not the other connotation. But yeah, I think it's sort of interesting given how globalized

03:49:05.840 --> 03:49:09.520
everything is. And like, I mean, now we have basically perfect translation machines and it's

03:49:09.520 --> 03:49:14.960
still so impenetrable. That's really interesting. I've been, I should, I'm sort of ashamed almost

03:49:14.960 --> 03:49:20.240
that I haven't done this yet. I think many months ago, when Alexi interviewed me on his

03:49:20.240 --> 03:49:24.400
YouTube channel, I said, I'm meaning to go to China to actually see for myself what's going on.

03:49:24.400 --> 03:49:29.520
And actually I'm, I should, so by the way, if anybody listening has a lot of context on China,

03:49:29.520 --> 03:49:32.560
if I went to China, who could introduce me to people, please email me.

03:49:33.120 --> 03:49:36.320
You got to do some pods and you got to find some of the Chinese AI researchers, man.

03:49:36.320 --> 03:49:40.160
I know. I was thinking at some point, again, this is the fact that I'm...

03:49:40.160 --> 03:49:43.360
Can I speak freely, but you know, I don't know if they can speak freely, but...

03:49:43.360 --> 03:49:47.360
I was thinking of there's, so they had these papers and on the paper, they'll say who's a

03:49:47.360 --> 03:49:53.360
co-author. It's funny because, well, I was thinking of just emailing, cold emailing everybody,

03:49:53.360 --> 03:49:56.800
like, here's my calendar. Let's just talk. I just want to see what is the vibe. Even

03:49:56.800 --> 03:50:00.000
they don't tell me anything. I'm just like, what kind of person is this? How westernized are they?

03:50:01.200 --> 03:50:08.080
But as I was saying this, I just remembered that in fact, by Dan's, according to mutual

03:50:08.080 --> 03:50:13.520
friends we have at Google, they cold emailed every single person on the Gemini paper and said,

03:50:13.520 --> 03:50:18.640
if you come work for By Dan's, we'll make you an allied engineer, you'll report directly to the CTO,

03:50:18.640 --> 03:50:22.240
and in fact, this actually... That's how the secrets go over, right?

03:50:22.240 --> 03:50:26.560
Right. No, I meant to ask this earlier, but suppose they hired what...

03:50:26.560 --> 03:50:30.160
If there's only a hundred or so people, or maybe less, we're working on the key algorithmic secrets.

03:50:30.960 --> 03:50:34.720
If they hired one such person, is all the alpha gone that these labs have?

03:50:35.360 --> 03:50:39.040
If this person was intentional about it, they could get a lot. I mean, they couldn't get the sort

03:50:39.040 --> 03:50:42.000
of like... I mean, actually, you could probably just also exfiltrate the code. They could get a

03:50:42.000 --> 03:50:46.240
lot of the key ideas. Again, up until recently stuff was published, but they could get a lot

03:50:46.240 --> 03:50:50.240
of the key ideas if they tried. I think there's a lot of people who don't actually look around

03:50:50.240 --> 03:50:56.160
to see what the other teams are doing, but I think you can. But yeah, I mean, they could. It's scary.

03:50:56.880 --> 03:51:01.200
Right. I think the project makes more sense there where you can't just recruit a Manhattan

03:51:01.200 --> 03:51:05.920
project engineer and then just get... I mean, these are secrets that can be used for like

03:51:05.920 --> 03:51:09.520
probably every training around the future that'll be like, maybe are the key to the data wall that

03:51:09.520 --> 03:51:14.000
are like, they can't go on or they can't go on that are like, they're going to be worth giving

03:51:14.000 --> 03:51:17.120
sort of like the multipliers on compute, hundreds of billions, trillions of dollars,

03:51:17.920 --> 03:51:21.680
and all it takes is China to offer a hundred million dollars to somebody and be like,

03:51:21.680 --> 03:51:28.160
yeah, come work for us. And then... I mean, yeah, I'm really uncertain on how

03:51:28.160 --> 03:51:33.680
sort of seriously China is taking AGI right now. One anecdote that was relate to me on

03:51:33.680 --> 03:51:37.920
the topic of the anecdotes, by another sort of like kind of researcher in the field was

03:51:37.920 --> 03:51:41.520
at some point they were at a conference with somebody, Chinese AI researcher,

03:51:41.520 --> 03:51:44.640
and he was talking to him and he was like, I think it's really good that you're here and like,

03:51:44.640 --> 03:51:49.920
we got to have the international coordination stuff. And apparently this guy said that I'm the

03:51:49.920 --> 03:51:53.840
kind of most senior most person that they're going to let leave the country to come to things like

03:51:53.840 --> 03:51:59.520
this. Wait, what's the takeaway? As in they're not letting really senior

03:51:59.520 --> 03:52:03.600
AI researchers leave the country. Interesting. Kind of classic, you know, Eastern Block move.

03:52:03.600 --> 03:52:09.040
Yeah. I don't know if this is true, but it's what I heard. It's interesting. So I thought the point

03:52:09.040 --> 03:52:14.800
you made earlier about being exposed to German newspapers and also to, because earlier you

03:52:14.800 --> 03:52:20.960
were interested in economics and law and national security, you have the variety in intellectual

03:52:20.960 --> 03:52:24.800
diet there has exposed you to thinking about the geopolitical question here and why is others

03:52:25.360 --> 03:52:28.240
talking about AI. I mean, this is the first episode I've done about this where we've talked

03:52:28.240 --> 03:52:32.080
about things like this, which is now that I think about it weird to give that this is an obvious

03:52:32.080 --> 03:52:37.040
thing in retrospect, I should have been thinking about. Anyways, so that's one thing we've been

03:52:37.040 --> 03:52:42.000
missing. What are you missing? And national security you're thinking about so you can't say

03:52:42.000 --> 03:52:47.520
national security. What like perspective are you probably underexposed to as a result?

03:52:47.520 --> 03:52:50.720
And China, I guess you mentioned. Yeah. So I think the China one is an important one.

03:52:52.480 --> 03:52:55.760
I mean, I think another one would be a sort of very Tyler Cowan-esque take, which is like,

03:52:55.760 --> 03:53:00.000
you're not exposed to how, like, how will a normal person in America, like, you know,

03:53:00.000 --> 03:53:05.040
both like use AI, you know, probably not, you know, and that being kind of like bottlenecks

03:53:05.040 --> 03:53:08.720
to the fusion of these things. I'm overrating the revenue because I'm kind of like, ah, you know,

03:53:08.720 --> 03:53:12.640
everyone has to stop adopting it, but you know, kind of like, you know, Joe Schmo engineer at a

03:53:12.640 --> 03:53:16.800
company, you know, like, ah, will they, will they be able to integrate it? And also the reaction

03:53:16.800 --> 03:53:20.480
to it, right? You know, I mean, I think this was a question again, hours ago, where it was

03:53:21.920 --> 03:53:26.640
about like, you know, won't people kind of rebel against this? Yeah. And they won't want to do the

03:53:26.640 --> 03:53:32.480
project. I don't know, maybe they will. Yeah. Here's a political reaction that I didn't anticipate.

03:53:32.480 --> 03:53:37.520
Yeah. So Tucker Carlson was recently on the Joe Rogan episode. I already told you about this

03:53:37.520 --> 03:53:43.680
part. I'm just gonna tell the story again. So Tucker Carlson is on Joe Rogan. Yeah. And they

03:53:43.680 --> 03:53:49.520
start talking about World War II and Tucker says, well, listen, I'm going to say something that my

03:53:49.520 --> 03:53:54.640
fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously

03:53:54.640 --> 03:54:00.880
immoral that we use them on Nagasaki and Hiroshima. And then he says, in fact, nuclear weapons are

03:54:00.880 --> 03:54:07.280
always immoral, except when we would use them on data centers. In fact, it would be immoral not to

03:54:07.280 --> 03:54:11.760
use them on data centers because look, we're, DC people in Silicon Valley, these fucking nerds

03:54:11.760 --> 03:54:18.720
are making super intelligent. And they say that it could enslave humanity. We made machines to

03:54:18.720 --> 03:54:24.080
serve humanity, not to enslave humanity. And they're just going on and making these machines.

03:54:24.080 --> 03:54:32.640
And so we should of course be nuking the data centers. And that is definitely not a political

03:54:32.640 --> 03:54:39.120
reaction in 2024. I was expecting. I mean, who knows? It's gonna be crazy. It's gonna be crazy.

03:54:39.200 --> 03:54:44.720
The thing we learned with COVID is that also the left, right reactions that you would anticipate

03:54:44.720 --> 03:54:50.080
just based on hunches, it completely flipped. Initially, like kind of the right is like, you

03:54:50.080 --> 03:54:53.920
know, it's like so contingent. And then, and then, and then, and the right left was like, this is

03:54:53.920 --> 03:54:58.000
racist. And then it flipped, you know, the left was really into the code. Yeah. And the whole

03:54:58.000 --> 03:55:02.720
thing also is just like so blunt and crude. And so, yeah, I think, I think probably in general,

03:55:02.720 --> 03:55:06.320
you know, I think people are really under, you know, people like to make sort of complicated

03:55:06.400 --> 03:55:11.040
technocratic AI policy proposals. And I think, especially if things go kind of fairly rapidly

03:55:11.040 --> 03:55:16.800
on the last AGI, you know, there might not actually be that much space for kind of like

03:55:16.800 --> 03:55:20.640
complicated kind of like, you know, clever proposals that might just be kind of much

03:55:20.640 --> 03:55:27.120
cruder reactions. Yeah. Look, and then also when you mentioned the spies and national security

03:55:27.120 --> 03:55:32.880
getting involved and everything, and you can talk about that in the abstract, but now that we're

03:55:32.880 --> 03:55:36.640
living in San Francisco and we know many of the people who are doing the top AI research

03:55:37.680 --> 03:55:40.560
is also a little scary to think about people I personally know and friends with.

03:55:41.520 --> 03:55:46.240
It's not unfeasible if they have secrets in their head that are worth $100 billion or something,

03:55:46.240 --> 03:55:51.280
kidnapping, assassination, sabotage. It's scary. Oh, they're family, or yeah, it's really bad.

03:55:51.280 --> 03:55:54.160
Yeah, yeah. I mean, this is to the point on security, you know, like right now, it's just

03:55:54.160 --> 03:56:00.160
really foreign. But, you know, at some point, as it becomes like really serious, it's, you know,

03:56:00.160 --> 03:56:06.960
you're going to want the security cards. Yeah. Yeah. So presumably you have thought about the

03:56:06.960 --> 03:56:11.200
fact that people in China will be listening to this and will be reading your series. Yeah.

03:56:11.840 --> 03:56:20.320
And somehow you made the trade off that it's better to let the whole world know. Yeah. And

03:56:20.320 --> 03:56:24.640
also including China and make them up to AGI, which is part of the thing you're worried about

03:56:24.640 --> 03:56:30.480
is China making up to AGI than to stay silent. Yeah. I'm just curious, walk me through how you've

03:56:30.480 --> 03:56:34.640
thought about that trade off. Yeah, I actually, look, I think this is a tough trade off. I thought

03:56:34.640 --> 03:56:39.120
about this a bunch, you know, I think, you know, I think people on the PRC will read this.

03:56:44.880 --> 03:56:48.320
I think, you know, I think there's some extent to which sort of cat is out of the bag, you know,

03:56:48.320 --> 03:56:52.160
this is like not, you know, AGI being a thing people are thinking about very seriously is not

03:56:52.160 --> 03:56:55.520
new anymore. There's sort of, you know, a lot of these takes are kind of old or, you know, I've had,

03:56:55.520 --> 03:56:59.760
I had, you know, similar views a year ago, might not have written it up a year ago, in part because

03:56:59.760 --> 03:57:04.640
I think this cat wasn't out of the bag enough. You know, I think the other thing is

03:57:08.160 --> 03:57:12.960
I think to be able to manage this challenge, you know, I think much broader swaths in society

03:57:12.960 --> 03:57:15.920
will need to wake up, right? And if we're going to get the project, you know, we actually need

03:57:16.000 --> 03:57:20.560
sort of like, you know, abroad by partisan understanding, the challenges facing us. And

03:57:21.440 --> 03:57:25.920
so, you know, I think it's a tough trade off, but I think the sort of need to wake up people in the

03:57:25.920 --> 03:57:31.680
United States in the sort of Western world and the Democratic coalition is ultimately imperative.

03:57:31.680 --> 03:57:35.360
And, you know, I think my hope is more people here will read it than the PRC.

03:57:37.600 --> 03:57:40.880
You know, and I think people sometimes underrate the importance of just kind of like writing it

03:57:41.360 --> 03:57:46.160
laying out the strategic picture. And, you know, I think you've done actually a great service to

03:57:46.160 --> 03:57:54.000
sort of mankind in some sense by, you know, with your podcast. And, you know, I think it's overall

03:57:54.000 --> 03:57:58.880
been good. Okay, so by the way, you know, on the topic of, you know, Germany, you know, we were

03:57:58.880 --> 03:58:02.720
talking at some point about kind of immigration story, right? Like you have a kind of interesting

03:58:02.720 --> 03:58:09.040
story you haven't told. And I think you should tell. So a couple of years ago, I was in college and

03:58:09.360 --> 03:58:15.760
I was 20. Yeah, I was about to turn 21. Yeah, I think it was, yeah, you came from India when you

03:58:15.760 --> 03:58:21.760
were really right. Yeah. So I was eight or eight or nine. I lived in India and then we moved around

03:58:21.760 --> 03:58:28.240
all over the place. But because of the backlog for Indians, the green card backlog, yeah, it's

03:58:29.680 --> 03:58:34.080
we were we've been in the queue for like decades, even though you came at eight, you're still on

03:58:34.080 --> 03:58:39.600
the H1B. Yeah. And when you're 21, you get kicked off the queue and you had to restart the process.

03:58:39.600 --> 03:58:43.360
So I'm on my dad's, my dad's a doctor and I'm on his H1B as it depended. But when you're 21,

03:58:43.360 --> 03:58:48.240
you get kicked off. Yeah. And so I'm 20 and I just like kind of dawns on me that this is my situation.

03:58:48.240 --> 03:58:52.640
Yeah. And you're completely screwed. Right. And so I also had experience that my dad,

03:58:53.200 --> 03:58:57.680
yeah, we've like moved all around the country. They have to prove that him as a doctor is like,

03:58:57.680 --> 03:59:03.040
you can't get native talent. Yeah. And you can't start up. Yeah. So where can you not get like

03:59:03.040 --> 03:59:07.920
even getting the H1B for you would have been like 20% lottery. So if you're lucky, you're in this

03:59:07.920 --> 03:59:10.240
time. And they had to prove that they can't get native talent, which means like for him,

03:59:10.240 --> 03:59:14.000
I'm like, we lived in North Dakota for three years, West Virginia for three years, Maryland,

03:59:14.000 --> 03:59:19.040
West Texas. Yeah. And so kind of dawn on me, this is my situation. Is that turn 21, I'll be like

03:59:19.600 --> 03:59:23.760
on this lottery, even if I get the lottery, I'll be a fucking code monkey for the rest of my life

03:59:23.760 --> 03:59:29.040
because this thing isn't going to let up. Yeah. Can't do a startup. Exactly. And so at the same time,

03:59:29.120 --> 03:59:32.640
I had been reading for the last year, I've been super obsessed with Paul Graham essays.

03:59:33.360 --> 03:59:37.600
My plan at the time was to make a startup or something. I was super excited about that.

03:59:37.600 --> 03:59:41.840
And it just occurred to me that I couldn't do this. Yeah. That like, this is just not in the

03:59:41.840 --> 03:59:48.800
cars for me. Yeah. And so I was kind of depressed about it. I remember I kind of just, I was in

03:59:48.800 --> 03:59:53.200
a daze through finals, because I had like, it just occurred to me and I was really like

03:59:54.160 --> 04:00:01.360
anxious about it. Yeah. And I remember thinking to myself at the time that if somehow I end up

04:00:01.360 --> 04:00:06.400
getting my green card before I turn 21, there's no fucking way I'm turning, becoming a code monkey

04:00:06.400 --> 04:00:11.920
because the thing that I've, like this feeling of dread that I have is this realization that

04:00:12.640 --> 04:00:17.920
I'm just going to have to be a code monkey. And I realize that's my default path. Yeah. If I,

04:00:17.920 --> 04:00:21.680
if I hadn't sort of made a proactive effort not to do that, I would have graduated college as a

04:00:21.680 --> 04:00:24.800
computer science student and I would have just done that. And that's the thing I was super scared

04:00:24.800 --> 04:00:31.200
about. Yeah. So that was an important sort of realization for me. Anyway, so COVID happened

04:00:31.200 --> 04:00:36.640
because of that, since there weren't foreigners coming, the backlog cleared fast. And by the skin

04:00:36.640 --> 04:00:41.440
of my teeth, like a few months before I turned 21, extremely contingent reasons, I ended up getting

04:00:41.440 --> 04:00:46.240
a green card because I got a green card. I could, you know, the whole podcast, right? Exactly.

04:00:46.240 --> 04:00:51.200
I graduated college and I was like bumming around and I got, it was like, I graduated

04:00:51.200 --> 04:00:55.200
just a semester early. I'm going to like do this podcast, see what happens. And it was,

04:00:55.200 --> 04:01:01.200
it hadn't, it didn't have a green card. And it only existed because, yeah, it's actually,

04:01:01.200 --> 04:01:05.360
because I think it's hard. It's probably, it's, you know, what is the impact of like immigration

04:01:05.360 --> 04:01:09.040
reform? Like what is the impact of clearing, you know, like whatever 50,000 green cards in

04:01:09.040 --> 04:01:13.920
the backlog? And you're such like an amazing example of like, you know, all of this is only

04:01:14.000 --> 04:01:18.960
possible. And it's, yeah, it's, I mean, it's just incredibly tragic that this is so dysfunctional.

04:01:18.960 --> 04:01:24.560
Yeah, yeah, yeah. No, it's insane. I'm glad you did it. I'm glad you kind of like, you know,

04:01:24.560 --> 04:01:29.600
tried the, you know, the, the, the unusual path. Well, yeah, but I could only do it.

04:01:30.240 --> 04:01:34.880
Obviously, I was extremely fortunate that I got the green card. I was like,

04:01:36.000 --> 04:01:40.640
I had a little bit of saved up money and I got a small grant out of college. Thanks to the

04:01:41.280 --> 04:01:45.040
future fund to like do this for basically the equivalent of six months.

04:01:45.040 --> 04:01:50.560
And so it turned out really well. And then at each time when I was like, oh, okay, podcast,

04:01:50.560 --> 04:01:55.280
come on, like, I wasted a few months on this, let's now go do something real. Something big would

04:01:55.280 --> 04:02:00.880
happen. I would, Jeff Bezos would, huh? You kept with it. Yeah. Yeah. But there would always be

04:02:00.880 --> 04:02:04.160
just like the moment I'm about to quit the podcast, something like Jeff Bezos will say

04:02:04.160 --> 04:02:08.080
there's something nice about me on Twitter. The daily episodes gets like half a million views,

04:02:08.080 --> 04:02:11.840
you know, and then now this is my career, but it was a sort of very,

04:02:12.880 --> 04:02:16.080
looking back on it, incredibly contingent that things worked out the right way.

04:02:16.080 --> 04:02:19.440
Yeah. I mean, look, if, if the AGI stuff goes down, you know, it'll be,

04:02:20.160 --> 04:02:23.920
it'll be the most important kind of like, you know, source of, it'll be how,

04:02:23.920 --> 04:02:28.640
maybe most of the people who kind of end up feeling the AGI are sure about it.

04:02:28.640 --> 04:02:33.600
Yeah. Yeah. Yeah. Yeah. Also very much, you're very linked with the story in many ways. First,

04:02:34.160 --> 04:02:40.560
the, I got like a $20,000 grant from a future fund right out of college. Yeah.

04:02:40.560 --> 04:02:46.080
And that sustained me for six months or however long it was. Yeah. And without that,

04:02:46.080 --> 04:02:48.480
I wouldn't, it was kind of crazy. Yeah. 10 grand or what was it?

04:02:48.480 --> 04:02:53.120
It was, no, it just, it's tiny, but you know, it goes to show kind of how small grants can go.

04:02:53.120 --> 04:02:57.680
Yeah. It's sort of the immersion ventures too. Yeah. Exactly. The immersion ventures and the,

04:02:58.800 --> 04:03:01.920
well, the last year I've been in San Francisco, we've just been

04:03:02.160 --> 04:03:06.640
in close contact the entire time and just bouncing ideas back and forth.

04:03:06.640 --> 04:03:11.520
We're just basically the alpha I have, I think people would be surprised by how much I got from

04:03:11.520 --> 04:03:16.080
you, Sholto, Trent and a couple others. I mean, it's been, it's been an absolute pleasure.

04:03:16.080 --> 04:03:18.240
Yeah. Likewise. Likewise. It's been super fun. Yeah.

04:03:19.840 --> 04:03:24.080
Okay. So some random questions for you. Yeah. If you could convert to Mormonism.

04:03:24.080 --> 04:03:27.120
Yeah. And you could really believe it. Yeah. Would you do it? Would you push the button?

04:03:27.680 --> 04:03:33.200
Yeah. Well, okay. Okay. Before I answer that question, one sort of observation about the

04:03:33.200 --> 04:03:36.480
Mormons. So there's actually, there's an article that actually made a big impact on me.

04:03:36.480 --> 04:03:39.360
Yeah. I think it was by McKick Hop and at some point, you know, in the Atlantic or whatever

04:03:39.360 --> 04:03:44.400
about the Mormons. And I think the thing he kind of, you know, and I think he even was like

04:03:44.400 --> 04:03:47.840
interviewed Romney and so on. And I think the thing I thought was really interesting in this

04:03:47.840 --> 04:03:52.320
article was he kind of talked about how the experience of kind of growing up different,

04:03:52.320 --> 04:03:55.360
you know, growing up very unusual, especially if you grow up Mormon outside of Utah, you know,

04:03:55.360 --> 04:03:58.960
like the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird.

04:04:00.080 --> 04:04:05.360
How that kind of got people prepared for being willing to be kind of outside of the norm later

04:04:05.360 --> 04:04:09.120
on. And like, you know, Romney, you know, was willing to kind of take stands alone, you know,

04:04:09.120 --> 04:04:13.840
in his party, because he believed, you know, what he believed is true. And I don't, I mean,

04:04:13.840 --> 04:04:16.880
probably not to the same way, but I feel a little bit like this from kind of having grown up in

04:04:16.880 --> 04:04:20.560
Germany, you know, and really not having like this sort of German system and having been kind

04:04:20.560 --> 04:04:24.880
of an outsider or something. I think there's a certain amount in which kind of, yeah, growing

04:04:24.880 --> 04:04:29.680
up in an outsider gives you kind of unusual strength later on to be kind of like willing to

04:04:29.680 --> 04:04:34.400
say what you think. And so that is one thing I really appreciate about the Mormons, at least the

04:04:34.400 --> 04:04:37.680
ones that grew up outside of Utah. I think, you know, the fertility rates, they're good, they're

04:04:37.680 --> 04:04:42.080
important. They're going down as well, right? This is the thing that really clinched the kind of

04:04:42.080 --> 04:04:46.400
fertility decline story for you. Even the Mormons. Yeah, even the Mormons, right? You're like, oh,

04:04:46.400 --> 04:04:49.520
this is like a good sort of good story. The Mormons will replace everybody. Well, no, I don't know

04:04:49.520 --> 04:04:52.400
if it's good, but it's like, at least, you know, at least come on, you know, like at least some

04:04:52.400 --> 04:04:55.600
people will maintain high, you know, but it's no, no, you know, even the Mormons and sort of

04:04:55.600 --> 04:04:59.360
basically, once these religious subgroups have high fertility rates, once they kind of grow big

04:04:59.360 --> 04:05:03.840
enough, they become, they're too close in contact with sort of normal society and become normalized,

04:05:03.840 --> 04:05:08.000
Mormon fertility rates drop from, I don't remember the exact numbers, maybe like four to two in the

04:05:08.000 --> 04:05:12.560
course of 10, 20 years. Anyway, so it's like, you know, now people point to the Amish or whatever,

04:05:12.560 --> 04:05:16.000
but I'm just like, it's probably just not scalable. And if you grow big enough, then there's just like,

04:05:16.000 --> 04:05:20.080
you know, the sort of like, you know, the sort of like overwhelming force of modernity kind of gets

04:05:20.080 --> 04:05:26.080
you. Yeah. No, if I could convert to Mormonism, look, I think there's something, I don't believe

04:05:26.080 --> 04:05:29.280
it, right? If I believed it, I obviously would convert to Mormonism, right? Because it's, you

04:05:29.280 --> 04:05:31.840
gotta, you gotta, but you can choose a world in which you do believe it.

04:05:37.200 --> 04:05:40.640
I think there's something really valuable and kind of believing in something greater than

04:05:40.640 --> 04:05:48.560
yourself and believing and having a certain amount of faith. You do, right? And you know,

04:05:49.280 --> 04:05:54.000
you know, there's a, you know, feeling some sort of duty to the thing greater than yourself.

04:05:54.000 --> 04:05:57.920
Yeah. And you know, maybe my version of this is somewhat different. You know, I think I feel

04:05:57.920 --> 04:06:01.920
some sort of duty to like, I feel like there's some sort of historical weight on like how this

04:06:01.920 --> 04:06:05.600
might play out. And I feel some sort of duty to like make that go well. I feel some sort of duty

04:06:05.600 --> 04:06:13.600
to, you know, our country, to the national security of the United States. And, you know,

04:06:13.600 --> 04:06:16.720
I think, I think that, I think it can be a force for a lot of good.

04:06:17.280 --> 04:06:20.720
I, the, going back to the opening, I think just,

04:06:24.320 --> 04:06:30.800
the thing that's especially impressive about that is, look, there's people who, at the company,

04:06:30.800 --> 04:06:37.600
who have through years and decades of building up savings from working in tech have probably

04:06:37.600 --> 04:06:44.640
10 civilians, liquid, more than that in terms of their equity. And the person, very many people

04:06:44.640 --> 04:06:51.040
were concerned about the clusters and the Middle East and the secrets leaking to China and all

04:06:51.040 --> 04:06:57.120
these things. But the person who actually made a hassle about it, and I think hassling people is

04:06:57.120 --> 04:07:03.760
so underrated, I think that one person who made a hassle about it is the 22 year old who has less

04:07:03.840 --> 04:07:10.240
than a year at the company who doesn't have savings built up. Who isn't like a solidified member of the,

04:07:11.760 --> 04:07:16.080
I think that's a sort of like, maybe, maybe it's me being naive and, you know, not knowing how big

04:07:16.080 --> 04:07:20.480
companies work. And, you know, but like, there's a, you know, I think sometimes a bit of a speech

04:07:20.480 --> 04:07:24.560
deontologist, you know, I kind of believe in saying what you think. Sometimes friends tell me I

04:07:24.560 --> 04:07:33.520
should be more of a speech consequentialist. No, I think I really think the amount of people who

04:07:33.600 --> 04:07:38.880
when they have the opportunity to talk to the person will just bring up the thing. I've been

04:07:38.880 --> 04:07:41.760
with you in multiple contexts, and I guess I shouldn't reveal who the person is or what the

04:07:41.760 --> 04:07:47.360
context was, but I've just been like very impressed that the dinner begins and by the end, somebody

04:07:47.360 --> 04:07:54.080
who has a major voice in how things go is seriously thinking about a worldview they would have found

04:07:54.080 --> 04:07:59.920
incredibly alien before the dinner or something. And I've been impressed that like, just like,

04:08:00.480 --> 04:08:07.520
give them the spiel and hassle them. I mean, look, I just, I think, I think I feel this stuff

04:08:07.520 --> 04:08:10.800
pretty viscerally now. You know, I think there's a time, you know, there's a time when I thought

04:08:10.800 --> 04:08:14.720
about the stuff a lot, but it was kind of like econ models and like, you know, kind of like these

04:08:14.720 --> 04:08:18.240
sort of theoretical abstractions and, you know, you talk about human brain size or whatever.

04:08:18.240 --> 04:08:23.760
Right. And I think, you know, since, I think since at least last year, you know, I feel like,

04:08:23.760 --> 04:08:29.120
you know, I feel like I can see it, you know, and I just, I feel it. And I think I can like,

04:08:29.120 --> 04:08:33.920
you know, I can sort of see the cluster that I can see the kind of rough combination of algorithms

04:08:33.920 --> 04:08:39.120
and the people that be involved and how this is going to play out. And, you know, I think,

04:08:39.120 --> 04:08:42.240
look, we'll see how it plays out. There's many ways this could be wrong. There's many ways it

04:08:42.240 --> 04:08:48.400
could go. But I think this could get very real. Yeah. Should we talk about what you're up to next?

04:08:48.400 --> 04:08:53.040
Sure. Yeah. Okay. So you're starting an investment firm anchor investments from

04:08:53.120 --> 04:08:56.560
Nat Friedman, Daniel Gross, Patrick Lawson, John Collison.

04:08:58.640 --> 04:09:03.120
First of all, why is this thing to do? You believe the AGI is coming in a few years?

04:09:04.800 --> 04:09:09.520
Why the investment firm? A good question, fair question. Wait, so I mean, a couple things. One

04:09:09.520 --> 04:09:12.720
is just, you know, I think we talked about this earlier, but it's like the screen doesn't go blank,

04:09:12.720 --> 04:09:15.600
you know, when sort of AGI is different intelligence happens, I think people really

04:09:15.600 --> 04:09:18.560
underrate the sort of basically the sort of decade after it, you have the intelligence

04:09:18.560 --> 04:09:22.560
explosion, that's maybe the most sort of wild period. But I think the decade after is also

04:09:22.560 --> 04:09:26.720
going to be wild. And you know, this combination of human institutions, but super intelligence,

04:09:26.720 --> 04:09:29.920
you have crazy kind of geopolitical things going on, you have the sort of broadening of

04:09:29.920 --> 04:09:34.720
this explosive growth. And basically, yeah, I think it's going to be a really important period.

04:09:34.720 --> 04:09:37.200
I think capital will really matter, you know, eventually, you know, like, you know, going to

04:09:37.200 --> 04:09:41.840
go to the stars, you know, going to go to the galaxies. So anyway, so part of the answer is

04:09:41.840 --> 04:09:45.600
just like, look, I think done, done, right, there's a lot of money to be made, you know,

04:09:45.600 --> 04:09:49.040
I think if AGI were priced in tomorrow, you could maybe make 100x, probably you can make

04:09:49.040 --> 04:09:55.040
even way more than that because of the sequencing. And, and, and, you know, capital matters.

04:09:56.080 --> 04:10:01.840
I think the other reason is just, you know, some amount of freedom and independence. And I think,

04:10:01.840 --> 04:10:06.960
you know, you know, I think there's some people who are very smart about this AGI stuff and who

04:10:06.960 --> 04:10:10.800
are kind of like see it coming. But I think almost all of them, you know, are kind of,

04:10:10.800 --> 04:10:14.000
you know, constrained in various ways right there in the labs, you know, they're in some,

04:10:14.000 --> 04:10:18.560
you know, some other position where they can't really talk about the stuff. And, you know,

04:10:18.560 --> 04:10:21.680
in some sense, I've really admired sort of the thing you've done, which is I think it's really

04:10:21.680 --> 04:10:24.880
important that there's sort of voices of reason on this stuff publicly, or people who are in

04:10:24.880 --> 04:10:28.560
positions to kind of advise important actors and so on. And so I think there's a, you know,

04:10:29.200 --> 04:10:32.400
basically the thing this investment firm will be, will be kind of like, you know, a brain trust

04:10:32.400 --> 04:10:35.760
on AI, it's going to be all about situational awareness. We're going to have the best situational

04:10:35.760 --> 04:10:38.720
awareness in the business, you know, we're going to have way more situational business than any of

04:10:38.720 --> 04:10:41.920
the people who manage money in New York. We're definitely going to, you know, we're going to do

04:10:41.920 --> 04:10:46.240
great on investing. But it's the same sort of situational awareness that I think is going to

04:10:46.240 --> 04:10:52.480
be important for understanding what's happening, being a voice of reason publicly and, and, and

04:10:52.480 --> 04:10:59.440
sort of being able to be in a position to advise. Yeah. I, there was the book about Peter Thiel.

04:10:59.440 --> 04:11:05.120
Yeah. They had an interesting quote about his hedge fund. I think it got terrible return. So

04:11:05.680 --> 04:11:07.840
this isn't the example. Right, right. That's the, that's the sort of

04:11:07.840 --> 04:11:11.920
bare case, right? It's like two theoretical and sure. Yeah. But they had an interesting quote

04:11:12.000 --> 04:11:16.640
that it's, it's, that it's like basically a think tank inside of a hedge fund.

04:11:16.640 --> 04:11:23.200
Yeah. So we're trying to build. Right. Yeah. So presumably you've thought about the ways in which

04:11:23.200 --> 04:11:26.880
these kinds of things can blow. There's a very, there's a lot of interesting business history

04:11:26.880 --> 04:11:34.400
books about people who got the pieces right, but timed it wrong. Yeah. Where they, they buy that

04:11:34.400 --> 04:11:38.240
internet's going to be a big deal. Yeah. They sell at the wrong time and buy the wrong time

04:11:38.320 --> 04:11:42.400
in the dotcom boom. And so they miss out on the gains, even though they're right about the,

04:11:42.400 --> 04:11:46.400
anyways, yeah. What, what is that trick to preventing that kind of thing?

04:11:46.400 --> 04:11:50.000
Yeah. I mean, look, obviously you can't, you know, not blowing up as sort of like, you know,

04:11:50.000 --> 04:11:55.040
task number one and two or whatever. I mean, you know, I think this investment firm, it is going

04:11:55.040 --> 04:11:58.800
to just be betting on AGI, you know, betting on AGI and super intelligence before the decade

04:11:58.800 --> 04:12:02.560
is out, taking that seriously, making the bets you would make, you know, if you took that seriously.

04:12:02.560 --> 04:12:07.280
So, you know, I think if that's wrong, you know, firm is not going to do that well. The thing you

04:12:07.280 --> 04:12:10.400
have to be resistant to is like, you have to be able to resist and get, you know, one or a couple

04:12:10.400 --> 04:12:14.080
or a few kind of individual calls, right? You know, it's like AI stagnates for a year because of the

04:12:14.080 --> 04:12:18.720
data wall or like, you know, you got, you got the call wrong on like when revenue would go up. And

04:12:18.720 --> 04:12:23.040
so anyway, that's pretty critical. You have to get timing right. I do think in general that the

04:12:23.040 --> 04:12:26.960
sort of sequence of bets on the way to AGI is actually pretty critical. And I think a thing

04:12:26.960 --> 04:12:32.480
people underrate. So, all right. I mean, yeah. So like, where does the story start, right? So like,

04:12:32.560 --> 04:12:38.160
obviously, the sort of only bet over the last year was NVIDIA. And, you know, it's obvious now,

04:12:39.520 --> 04:12:44.000
very few people did it. This is sort of also, you know, a classic debate I and a friend had with

04:12:44.000 --> 04:12:47.760
another colleague of ours, where this colleague was really into TSM, you know, TSMC. And he was

04:12:47.760 --> 04:12:51.680
just kind of like, well, you know, like, these tabs are going to be so valuable. And also like,

04:12:51.680 --> 04:12:54.720
NVIDIA, there's just a lot of videos in credit risk, right? It's like, maybe somebody else makes

04:12:54.720 --> 04:12:59.680
better GPUs. That was basically right. But sort of only NVIDIA had the AI beta, right? Because

04:12:59.680 --> 04:13:03.440
only NVIDIA was kind of like a large fraction AI. The next few doublings would just like

04:13:03.440 --> 04:13:07.440
meaningfully explode their revenue. Whereas TSMC was, you know, a couple percent AI. So,

04:13:07.440 --> 04:13:10.400
you know, even though there's going to be a few doublings of AI, not going to make that big of

04:13:10.400 --> 04:13:14.720
an impact. All right. So it's sort of like, the only place to find the AI beta basically was NVIDIA

04:13:14.720 --> 04:13:21.520
for a while. You know, now it's broadening, right? So now TSM is like, you know, 20 percent AI by

04:13:21.520 --> 04:13:25.280
like 27 or something is what they're saying. One more doubling, it'll be kind of like a large

04:13:25.280 --> 04:13:28.480
fraction of what they're doing. And, you know, there's a whole, you know, whole stack, you know,

04:13:28.480 --> 04:13:33.360
there's like, you know, there's people making memory and COAS and, you know, power, you know,

04:13:33.360 --> 04:13:36.880
utility companies are starting to get excited about AI. And they're like, oh, it'll, you know,

04:13:37.520 --> 04:13:41.600
power production in the United States will grow, you know, not 2.5 percent, 5 percent of the next

04:13:41.600 --> 04:13:48.560
five years. And I'm like, no, it'll grow more. You know, at some point, you know, you know,

04:13:48.560 --> 04:13:52.080
like a Google or something becomes interesting, you know, people are excited about them with AI

04:13:52.080 --> 04:13:55.440
because it's like, oh, you know, AI revenue will be, you know, 10 billion or tens of billions.

04:13:55.440 --> 04:13:58.880
And I'm kind of like, ah, I don't really care about them before then. I care about it, you know,

04:13:58.880 --> 04:14:02.560
once it, you know, once you get the AI beta, right? And so at some point, you know, Google

04:14:02.560 --> 04:14:06.400
will get, you know, $100 billion of revenue from AI. Probably their stock will explode,

04:14:06.400 --> 04:14:08.960
you know, they're going to become, you know, 5 trillion, 10 trillion dollar company.

04:14:09.680 --> 04:14:12.400
Anyway, so the timing there is very important. You have to get the timing right. You have to

04:14:12.400 --> 04:14:15.520
get the sequence right. You know, at some point, actually, I think like, you know, there's going

04:14:15.520 --> 04:14:20.240
to be real tailwind to equities from real interest rates, right? So basically in these sort of

04:14:20.320 --> 04:14:24.960
explosive growths worlds, you would expect real interest rates to go up a lot, both on the sort

04:14:24.960 --> 04:14:29.520
of like, you know, a basic both sides of the equation, right? On the supply side or on the

04:14:29.520 --> 04:14:35.280
sort of demand for money side, because, you know, people are going to be making these crazy investments,

04:14:35.280 --> 04:14:38.240
you know, initially in clusters and then in the robo factories or whatever, right? And so they're

04:14:38.240 --> 04:14:43.760
going to be borrowing like crazy. They want all this capital, higher AI. And then on the sort of

04:14:43.760 --> 04:14:48.800
like consumer saving side, right, to like, you know, to give up all this capital, you know,

04:14:48.800 --> 04:14:52.160
this sort of like Euler equation, standard sort of intratemporal transfer, you know,

04:14:53.280 --> 04:14:55.840
trade off of consumption. So standard.

04:14:58.880 --> 04:15:02.000
Some of our friends have a paper on this, you know, basically, if you expect, you know,

04:15:02.000 --> 04:15:05.280
if consumers expect real growth rates to be higher, you know, interest rates are going to be

04:15:05.280 --> 04:15:08.480
higher because they're less willing to give up consumption, you know, consumption in the

04:15:10.240 --> 04:15:12.720
less willing to give up consumption day for consumption in the future.

04:15:13.280 --> 04:15:16.480
Anyway, so at some point, real interest rates will go up if sort of ADA is greater than one,

04:15:16.480 --> 04:15:20.720
that actually means equities, you know, higher growth rate expectations mean equities go down

04:15:20.720 --> 04:15:23.200
because the sort of interest rate effect outweighs the growth rate effect.

04:15:23.840 --> 04:15:27.200
And so, you know, at some point, there's like big, the big bond short, you got to get that right,

04:15:27.200 --> 04:15:30.880
you got to get it right, that, you know, nationalization, you know, like, you got, you know,

04:15:30.880 --> 04:15:33.600
anyway, so there's this whole sequence of things, you got to get that right.

04:15:33.600 --> 04:15:36.800
And the unknown unknowns, unknown unknowns. Yeah. And so you've, look, you've got to be

04:15:36.800 --> 04:15:40.080
really, really careful about your like overall like this positioning, right? And because, you

04:15:40.080 --> 04:15:43.280
know, you know, if you expect these kind of crazy events to play out, there's going to be crazy

04:15:43.280 --> 04:15:47.920
things you didn't see. You know, you do also want to make the sort of kind of bets that are tailored

04:15:47.920 --> 04:15:51.760
to your scenarios in the sense of like, you know, you want to find bets that are bets on the tails,

04:15:51.760 --> 04:15:56.240
right? You know, I don't think anyone is expecting, you know, interest rates to go above,

04:15:56.240 --> 04:16:00.240
you know, 10% like real interest rates. But, you know, I think there's at least a serious chance

04:16:00.240 --> 04:16:04.320
of that, you know, before the decade is out. And so, you know, maybe there's some like cheap

04:16:04.320 --> 04:16:08.880
insurance you can buy on that, you know, very silly question. In these worlds,

04:16:09.680 --> 04:16:14.240
are financial markets where you make these kinds of bets going to be respected? And

04:16:15.120 --> 04:16:19.680
like, you know, like, is my fidelity account going to mean anything when we have their 50%

04:16:19.680 --> 04:16:23.280
economic growth? Like, who's, who's like, we got to respect his property rights?

04:16:23.280 --> 04:16:26.400
That's pretty deep into it. The bond short, the sort of 50 and 52nd hour growth, that's pretty

04:16:26.400 --> 04:16:29.440
deep into it. I mean, again, there's this whole sequence of things. But yeah, no, I think property

04:16:29.440 --> 04:16:34.240
rights will be instructed again, in the sort of modal world, the project. Yeah. At some point,

04:16:34.240 --> 04:16:37.040
at some point, there's going to be figuring out the property rights for the galaxies, you know,

04:16:37.040 --> 04:16:43.440
and that'll be interesting. So there's an interesting question about

04:16:45.440 --> 04:16:50.240
going back to your strategy about, well, the 30s will really matter a lot about how the rest of

04:16:50.240 --> 04:16:55.280
the future goes. And you want to be in a position of influence by that point, because of capital.

04:16:56.480 --> 04:17:01.200
It's worth considering, as far as I know, there's probably a whole bunch of literature on this,

04:17:01.200 --> 04:17:07.600
I'm just riffing. But the, the landed gentry during the, before the beginning of the industrial

04:17:07.600 --> 04:17:13.920
revolution, I'm not sure if they were able to leverage their position in a sort of georgist

04:17:14.560 --> 04:17:23.840
or pickety type sense in order to accrue the returns that were realized through the industrial

04:17:23.840 --> 04:17:27.920
revolution. And I don't know what happened. At some point, they were just wearing the

04:17:28.000 --> 04:17:34.320
landed gentry. But I'd be concerned that even if you make great investment calls,

04:17:34.320 --> 04:17:38.640
you'll be like the guy who owned a lot of land, farmland before the industrial revolution,

04:17:38.640 --> 04:17:42.640
and like the guy who's actually going to make a bunch of money is the one with the C mentioned,

04:17:42.640 --> 04:17:46.080
even if he doesn't make that much money, most of the benefits are sort of widely diffused and so

04:17:46.080 --> 04:17:51.280
forth. I mean, I think that the analog is like you sell your land, you put it all and sort of

04:17:51.280 --> 04:17:56.560
that, you know, that the people who are building the new industry. I think the, I mean, I think

04:17:56.560 --> 04:18:01.840
the sort of like real depreciating asset, you know, for me is human capital, right? Yeah, no,

04:18:01.840 --> 04:18:05.520
look, I'm serious, right? It's like, you know, there's something about like, you know, I don't

04:18:05.520 --> 04:18:08.000
know, it was like valedictorian of Columbia, you know, the thing that made you special is

04:18:08.000 --> 04:18:11.520
you're smart, right? But actually, like, you know, that might not matter in like four years,

04:18:11.520 --> 04:18:15.520
you know, because it's actually automatable. Right. And so anyway, a friend joke that the

04:18:15.520 --> 04:18:20.000
sort of investment firm is perfectly hedged for me. It's like, you know, either like AGI this

04:18:20.000 --> 04:18:24.640
decade, and yeah, your human capital is depreciated, but you've turned that into financial capital,

04:18:24.640 --> 04:18:28.480
or you know, like no AGI this decade, in which case, maybe the firm doesn't do that well,

04:18:28.480 --> 04:18:30.800
but you know, you're still in your 20s and you're still smart.

04:18:32.640 --> 04:18:38.320
Excellent. And what's your story for why AGI hasn't been priced in the story?

04:18:39.360 --> 04:18:43.600
Financial markets are supposed to be very efficient to say very hard to get an edge here.

04:18:45.360 --> 04:18:50.480
Naively, you just say, well, I've looked at these scaling curves and they imply that we're

04:18:50.480 --> 04:18:54.400
going to be buying much more computed energy than the analysts realize.

04:18:55.280 --> 04:18:57.520
Shouldn't those analysts be broke by now? What's going on?

04:18:59.280 --> 04:19:06.800
Yeah. I mean, I used to be a true EMH guy. I was an economist, you know. I think the thing I,

04:19:07.600 --> 04:19:12.080
you know, changed my mind on is that I think there can be kind of groups of people, smart people,

04:19:12.080 --> 04:19:15.200
you know, who are, you know, say they're in San Francisco, who do just have

04:19:16.080 --> 04:19:20.960
off over the rest of society and kind of seeing the future. And so like COVID, right? Like,

04:19:20.960 --> 04:19:25.520
I think there's just honestly kind of similar group of people who just saw that and called it

04:19:25.520 --> 04:19:30.080
completely correctly. And, you know, they showed at the market they did really well.

04:19:31.280 --> 04:19:43.200
You know, a bunch of other sort of things like that. So, you know, why is AGI not priced in?

04:19:43.200 --> 04:19:47.360
You know, it's sort of, you know, why hasn't the government nationalized the labs yet, right?

04:19:47.360 --> 04:19:50.720
It's like, you know, this, you know, society hasn't priced it in yet and sort of it hasn't

04:19:50.720 --> 04:19:56.080
completely diffused. And, you know, again, it might be wrong, right? But I just think sort of,

04:19:57.680 --> 04:20:00.880
you know, not that many people take these ideas seriously yet. Yeah. Yeah.

04:20:01.760 --> 04:20:07.360
Yeah. A couple of other sort of ideas that I was playing around with with regards to

04:20:07.360 --> 04:20:14.480
reading it a chance to talk about, but the systems competition, there's a very interesting,

04:20:16.320 --> 04:20:18.480
one of my favorite books about World War II is the Victor Davis Hansen

04:20:21.760 --> 04:20:28.000
summary of everything. And he explains why the Allies made better decisions than the Axis.

04:20:28.000 --> 04:20:32.240
Why did they? And so obviously, there were some decisions that the Axis made that were pretty

04:20:32.240 --> 04:20:35.440
like Blitzkrieg, whatever. That was sort of by accident, though.

04:20:35.520 --> 04:20:37.840
In what sense? That they just had the infrastructure left over?

04:20:37.840 --> 04:20:42.080
Well, no, I mean, the sort of, I think, I mean, I don't, I mean, I think sort of my read of it is

04:20:42.080 --> 04:20:45.440
Blitzkrieg wasn't kind of some like a genius strategy. It was just kind of, it was like more

04:20:45.440 --> 04:20:50.080
like their hand was forced. I mean, this is sort of the very Adam Tuzi and story of World War II,

04:20:50.080 --> 04:20:53.680
right? But it was, you know, there's sort of this long war versus short war. I think it's

04:20:53.680 --> 04:20:57.680
actually kind of an important concept. I think sort of Germany realized that if they were in a

04:20:57.680 --> 04:21:02.560
long war, including the United States, you know, they would not be able to compete industrially.

04:21:02.560 --> 04:21:07.360
So their only path to victory was like make it a short war, right? And that, that sort of worked

04:21:07.360 --> 04:21:11.360
much more spectacularly than they thought, right? And sort of take over France and take over much

04:21:11.360 --> 04:21:15.200
of Europe. And so then, you know, the decision to invade the Soviet Union, it was, you know,

04:21:15.760 --> 04:21:20.240
it was, it was, um, look, if it was, it was about the Western front in some sense, because it was

04:21:20.240 --> 04:21:23.920
like, we've got to get the resources. You know, we don't, we're actually, we don't actually have a

04:21:23.920 --> 04:21:27.520
bunch of the stuff we need, like, you know, oil and so on. You know, Auschwitz was actually just

04:21:27.520 --> 04:21:30.640
this giant chemical plant to make kind of like synthetic oil and a bunch of these things was

04:21:30.640 --> 04:21:35.920
the largest industrial project in Nazi Germany. And so, you know, and sort of they thought, well,

04:21:35.920 --> 04:21:39.360
you know, we completely crushed them in World War I, you know, it'll be easy, we'll invade them,

04:21:39.360 --> 04:21:43.440
we'll get the resources, and then we can fight on the Western front. And even during the sort of

04:21:43.440 --> 04:21:47.040
whole invasion of the Soviet Union, even though kind of like a large amount of the sort of,

04:21:47.040 --> 04:21:50.400
you know, the sort of deaths happened there, you know, like a large fraction of German industrial

04:21:50.400 --> 04:21:54.880
production was actually, you know, like planes and naval, you know, and so on, those directed,

04:21:54.880 --> 04:21:58.640
you know, towards the Western front and towards the, you know, the Western allies.

04:21:58.800 --> 04:22:01.440
Well, and then so the point that Hansen was making was,

04:22:02.000 --> 04:22:05.280
by the way, I think this concept of like long war and short war is kind of interesting and with

04:22:05.280 --> 04:22:10.160
respect to thinking about the China competition, which is like, you know, I worry a lot about kind

04:22:10.160 --> 04:22:15.920
of, you know, the decline of sort of American, like late in American industrial capacity, you know,

04:22:15.920 --> 04:22:22.240
like, I think China builds like 200 times more ships than we do right now. You know, some crazy

04:22:22.240 --> 04:22:26.000
way. And so it's like, maybe we have this superiority, say in the non AI worlds, we have

04:22:26.000 --> 04:22:29.600
the superiority in military material, kind of like win a short war, at least, you know,

04:22:29.600 --> 04:22:33.760
kind of defend Taiwan in some sense. But like if it actually goes on, you know, it's like maybe

04:22:33.760 --> 04:22:39.200
China is much better able to mobilize, mobilize industrial resources in a way that like we just

04:22:39.200 --> 04:22:44.080
don't have that same ability anymore. I think this is also relevant to the AI thing in the sense of

04:22:44.080 --> 04:22:48.640
like, if it comes down to sort of a game about building, right, including like, maybe AGI takes

04:22:48.640 --> 04:22:52.320
the trillion dollar cluster, not the hundred billion dollar cluster, maybe, or even maybe AGI

04:22:52.320 --> 04:22:56.240
takes the, you know, is on the hundred billion dollar cluster. But, you know, it really matters

04:22:56.240 --> 04:22:59.520
if you can run, you know, 10x, you can do one more order of magnitude of compute for your super

04:22:59.520 --> 04:23:05.040
intelligence or whatever. That, you know, maybe right now they're behind, but they just have this

04:23:05.040 --> 04:23:09.360
sort of like raw, late in industrial capacity to help build us. And that matters both in the

04:23:09.360 --> 04:23:13.600
run up to AGI and after, right, where it's like, you have this super intelligence on your cluster,

04:23:13.600 --> 04:23:17.920
now it's time to kind of like expand the explosive growth. And, you know, like, will we let the

04:23:17.920 --> 04:23:22.400
ROA factories run wild? Like, maybe not. But like, maybe China will. Or like, you know, will we,

04:23:22.400 --> 04:23:25.200
will, yeah, will we produce the, how many, how many of the drones will we produce?

04:23:26.240 --> 04:23:29.360
And I think, yeah, so there's some sort of like outbuilding in the industrial explosion that I

04:23:29.360 --> 04:23:33.360
worked on. You've got to be one of the few people in the world who is both concerned about alignment,

04:23:33.360 --> 04:23:38.720
but also wants to make sure that we'll let the ROA factories proceed once we get the ASI to beat

04:23:38.720 --> 04:23:44.880
out China. Like, it's all, it's all part of the picture. Yeah, yeah, yeah.

04:23:45.120 --> 04:23:51.840
And by the way, speaking of the ASIs and the robot factories, one of the interesting things,

04:23:51.840 --> 04:23:56.800
RoboArmy's too. Yeah, one of the interesting things, there's this question of what you do

04:23:56.800 --> 04:24:02.480
with industrial scale intelligence. And obviously, it's not chatbots, but it's a, I think it's very

04:24:02.480 --> 04:24:12.000
hard to predict. Yeah, yeah. But the history of oil is very interesting. We're in the, I think it's

04:24:12.000 --> 04:24:18.160
in the 1860s that we figure out how to refine oil, some geologist. And so then standard oil

04:24:18.160 --> 04:24:24.720
got started, there's this huge boom. It changes American politics, entire legislators are getting

04:24:24.720 --> 04:24:31.360
bought out by oil interest and presidents are getting elected based on the divisions about

04:24:31.360 --> 04:24:36.720
oil and breaking them up and everything. And all of this has happened. The world has never

04:24:36.800 --> 04:24:44.320
revolutionized before the car has been invented. And so when the light bulb was invented, I think

04:24:44.320 --> 04:24:49.840
it was like 50 years after oil refining had been discovered, majority of standard oil's history

04:24:49.840 --> 04:24:54.960
is before the car is invented. Carousel lamps. Exactly. So it's just used for lighting.

04:24:54.960 --> 04:24:58.800
Then they thought oil would just no longer be relevant. Yeah, yeah. So there was a concern

04:24:58.800 --> 04:25:05.200
that standard oil would go to brain corrupt when the light bulb was invented. But then

04:25:06.000 --> 04:25:10.320
there's sort of, you realize that there's immense amount of compressed energy here.

04:25:10.960 --> 04:25:15.280
You're going to have billions of gallons of this stuff a year. And it's hard to

04:25:15.280 --> 04:25:20.960
sort of predict in advance what you can do with that. And then later on, it turns out transportation

04:25:21.520 --> 04:25:28.480
cars with, that's what it's used for. Anyways, with intelligence, maybe one answer is the

04:25:28.480 --> 04:25:33.440
intelligence explosion. But even after that, so you have all these ASIs and you have enough

04:25:33.440 --> 04:25:38.480
compute, especially the compute they'll build to run hundreds of millions of GPUs will hum.

04:25:38.480 --> 04:25:42.560
Yeah. But what are we doing with that? And it's very hard to predict in advance. I think it would

04:25:42.560 --> 04:25:49.360
be very interesting to figure out what the Jupiter brains will be doing. So look, there's

04:25:49.360 --> 04:25:55.840
situational awareness of where things stand now. And we've gotten a good dose of that.

04:25:58.160 --> 04:26:00.480
Obviously, a lot of the things we're talking about now, you couldn't have

04:26:01.200 --> 04:26:08.720
prejudged many years back in the past. And part of your role implies that things will accelerate

04:26:08.720 --> 04:26:15.200
because of AI getting the process. But many other things that are unpredictable fundamentally,

04:26:15.200 --> 04:26:19.200
basically how people will react, how the political system will react, how foreign adversaries will

04:26:19.200 --> 04:26:26.720
react. Those things will become evident over time. So the situational awareness is not just

04:26:26.800 --> 04:26:32.000
knowing where the picture stands now, but being in a position to react appropriately to new information,

04:26:32.960 --> 04:26:36.640
to change your worldview as a result, to change your recommendations as a result.

04:26:37.680 --> 04:26:43.440
What is the appropriate way to think about situational awareness as a continuous process

04:26:43.440 --> 04:26:48.720
rather than as a one-time thing you realized? Yep. No, I think this is great. Look, I think

04:26:48.720 --> 04:26:52.640
there's a sort of mental flexibility and willing to change your mind. That's really important.

04:26:52.640 --> 04:26:56.480
I actually think this is sort of like how a lot of brains have been broken in the AGI debate,

04:26:57.040 --> 04:27:02.320
the tumors who actually, I think we're really prescient on AGI thinking about the stuff a decade

04:27:02.320 --> 04:27:06.320
ago, but they haven't actually updated on the empirical realities of deep learning. They're

04:27:06.320 --> 04:27:10.320
sort of like, the proposals are really kind of even unworkable. This doesn't really make sense.

04:27:11.040 --> 04:27:13.920
There's people who come in with sort of a predefined ideology. They're just kind of like,

04:27:13.920 --> 04:27:18.000
the EX a little bit. They like to shitpost about technology, but they're not actually thinking

04:27:18.000 --> 04:27:22.160
through. I mean, either the sort of stagnationists who think this stuff is only going to be a

04:27:22.160 --> 04:27:25.760
chatbot, and so of course it isn't risky, or they're just not thinking through the kind of like

04:27:25.760 --> 04:27:30.880
actually immense national security implications and how that's going to go. I actually think

04:27:30.880 --> 04:27:34.640
there's kind of a risk in kind of like having written this stuff down and put it online.

04:27:37.120 --> 04:27:40.080
I think this sometimes happens to people as a sort of calcification of the worldview,

04:27:40.080 --> 04:27:44.400
because now they've publicly articulated this position. Maybe there's some evidence against

04:27:44.400 --> 04:27:49.600
it, but they're clinging to it. I actually want to give the big disclaimer on like,

04:27:49.600 --> 04:27:53.040
I think it's really valuable to paint this sort of very concrete and visceral picture.

04:27:54.400 --> 04:27:58.960
I think this is currently my best guess on how this decade will go. I think if it goes

04:27:58.960 --> 04:28:05.440
anywhere like this, it will be wild, but given the rapid pace of progress, we're going to keep

04:28:05.440 --> 04:28:11.920
getting a lot more information. I think it's important to sort of keep your head on straight

04:28:11.920 --> 04:28:19.440
about that. I feel like the most important thing here is that, and this relates to some of the

04:28:19.440 --> 04:28:26.160
stuff we've talked about and the world being surprisingly small and so on. I feel like I

04:28:26.160 --> 04:28:29.200
used to have this worldview of like, look, there's important things happening in the world, but there's

04:28:29.200 --> 04:28:32.720
like people who are taking care of it, and there's like the people in government, and there's again,

04:28:32.720 --> 04:28:38.960
even like AI labs have idealized, and people are on it. Surely there must be on it, right?

04:28:38.960 --> 04:28:42.560
And I think just some of this personal experience, even seeing how kind of COVID went,

04:28:44.160 --> 04:28:47.680
people aren't necessarily, there's not some, not that somebody else is just kind of on it and

04:28:47.760 --> 04:28:54.720
making sure this goes well, however it goes. You know, the thing that I think will really

04:28:54.720 --> 04:28:59.680
matter is that there are sort of good people who take this stuff as seriously as it deserves,

04:28:59.680 --> 04:29:03.920
and who are willing to kind of take the implication seriously, who are willing to, you know,

04:29:03.920 --> 04:29:07.680
who have situational awareness, are willing to change their minds, are willing to sort of

04:29:07.680 --> 04:29:14.000
stare the picture in the face, and you know, I'm counting on those good people.

04:29:14.000 --> 04:29:17.920
All right, that's a great place to close Leopold.

04:29:17.920 --> 04:29:21.360
Thanks so much, Tarkash. This is the absolute joy.

04:29:21.360 --> 04:29:26.320
Hey everybody, I hope you enjoyed that episode with Leopold. There's actually one more riff

04:29:26.320 --> 04:29:31.120
about German history that he had after a break, and it was pretty interesting, so I didn't want

04:29:31.120 --> 04:29:36.400
to cut it out, so I've just included it after this outro. You can advertise on the show now,

04:29:36.400 --> 04:29:41.200
so if you're interested, you can reach out at the forum in the description below.

04:29:41.280 --> 04:29:45.680
Other than that, the most helpful thing you can do is to share the episode if you enjoyed it.

04:29:45.680 --> 04:29:50.720
Send it to group chats, Twitter, wherever else you think people who might like this episode

04:29:50.720 --> 04:29:56.000
might congregate, and other than that, I guess here's this riff on Frederick the Great. See you

04:29:56.000 --> 04:30:00.640
on the next one. I mean, I think the actual funny thing is, you know, a lot of the sort of German

04:30:00.640 --> 04:30:05.120
history stuff we've talked about is sort of like not actually stuff I learned in Germany,

04:30:05.120 --> 04:30:07.600
it's sort of like stuff that I learned after, and there's actually, you know,

04:30:07.600 --> 04:30:11.040
a funny thing where I kind of would go back to Germany over Christmas or whatever. Suddenly

04:30:11.040 --> 04:30:14.160
I understand the street names, you know, it's like, you know, Gneisenau and Scharnhorst,

04:30:14.160 --> 04:30:18.160
and they're all these like Prussian military reformers, and you're like finally understood,

04:30:18.160 --> 04:30:21.440
you know, Sansa C, and you're like, Frederick, you know, Frederick the Great is this really

04:30:21.440 --> 04:30:29.440
interesting figure, where, so he's this sort of, in some sense, kind of like gay lover of arts,

04:30:30.000 --> 04:30:36.080
right, where he, you know, he hates speaking German, he only wants to speak French, you know,

04:30:36.080 --> 04:30:40.000
he like plays the flute, he composes, he has all the sort of great, you know, artists of his day,

04:30:40.000 --> 04:30:47.520
you know, over at Sansa C, and he actually had this sort of like really tough upbringing, where

04:30:47.520 --> 04:30:55.040
his father was this sort of like really stern sort of Prussian military man, and he had had a,

04:30:56.080 --> 04:30:59.840
Frederick the Great as sort of a 17-year-old or whatever, he basically had a male lover,

04:31:00.400 --> 04:31:07.840
and what his father did was imprison his son, and then I think hang his male lover in front of him,

04:31:08.480 --> 04:31:12.160
and again, his father was this kind of very stern Prussian guy, he was this kind of gay,

04:31:12.160 --> 04:31:16.560
you know, lover of arts, but then later on Frederick the Great turns out to be this like,

04:31:16.560 --> 04:31:22.880
you know, one of the most kind of like, you know, successful kind of Prussian conquerors,

04:31:22.880 --> 04:31:26.640
right, like he gets Silesia, he wins the Seven Years War, you know, also, you know,

04:31:26.640 --> 04:31:30.320
amazing military strategists, you know, amazing military strategy at the time consisted of like,

04:31:30.320 --> 04:31:34.400
he was able to like flank the army, and that was crazy, you know, and that was brilliant,

04:31:34.400 --> 04:31:38.320
and then they like almost lose the Seven Years War, and at the very end, you know, the sort of,

04:31:39.120 --> 04:31:43.280
the Russian Tsar changes, and he's like, ah, I'm actually kind of a Prussian stan, you know,

04:31:43.280 --> 04:31:47.360
I think I'm like, I'm into this stuff, and then he lets, you know, let's Frederick the Great lose,

04:31:47.360 --> 04:31:55.200
and he had, let's their army be okay, and anyway, sort of like, yeah, kind of bizarre,

04:31:55.200 --> 04:31:57.200
interesting figure in German history.

