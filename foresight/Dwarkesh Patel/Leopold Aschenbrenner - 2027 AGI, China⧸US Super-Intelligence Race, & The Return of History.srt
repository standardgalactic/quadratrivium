1
00:00:00,000 --> 00:00:03,920
What will be at stake will not just be equal products, but whether the liberal democracy

2
00:00:03,920 --> 00:00:08,320
survives, whether the CCP survives, what the world order for the next century will be.

3
00:00:08,320 --> 00:00:12,640
The CCP is going to have an all-out effort to infiltrate American AI labs, billions of dollars,

4
00:00:12,640 --> 00:00:16,560
thousands of people. The CCP is going to try to outbuild us. People don't realize how intense

5
00:00:16,560 --> 00:00:20,000
state-level espionage can be. When we have literal superintelligence on our cluster,

6
00:00:20,000 --> 00:00:23,840
and they can stuxnet the Chinese data centers, you really think they'll be a private company,

7
00:00:23,840 --> 00:00:27,680
and the government would be like, oh my god, what is going on? I do think it is incredibly

8
00:00:27,680 --> 00:00:31,200
important that these clusters are in the United States. I mean, would you do the Manhattan Project

9
00:00:31,200 --> 00:00:35,600
in the UAE, right? 2023 was the sort of moment for me where it went from kind of AGI as a sort of

10
00:00:35,600 --> 00:00:39,840
theoretical abstract thing, and you'd make the models to like, I see it, I feel it. I can see the

11
00:00:39,840 --> 00:00:43,120
cluster where it's strained on, like the rough combination of algorithms, the people, like how

12
00:00:43,120 --> 00:00:46,720
it's happening. And I think, you know, most of the world is not, you know, most of the people feel it

13
00:00:46,720 --> 00:00:52,720
are like right here, you know, right? Okay, today I'm chatting with my friend, Leopold Aschenbrenner.

14
00:00:52,720 --> 00:00:57,280
He grew up in Germany, graduated valedictorian of Columbia when he was 19,

15
00:00:58,000 --> 00:01:03,680
and then he had a very interesting Gaffier, which we'll talk about, and then he was on the

16
00:01:03,680 --> 00:01:10,880
OpenAI Superalignment team, made a recent piece, and now he, with some anchor investments from

17
00:01:10,880 --> 00:01:16,640
Patrick and John Collison and Daniel Gross and Nat Friedman, is launching an investment firm.

18
00:01:16,640 --> 00:01:21,520
So Leopold, I know you're off to a slow start, but life is long, and I wouldn't worry about it

19
00:01:21,520 --> 00:01:26,720
too much. You'll make up for it in due time. But thanks for coming on the podcast.

20
00:01:26,720 --> 00:01:31,200
Thank you. You know, I first discovered your podcast when your best episode had, you know,

21
00:01:31,200 --> 00:01:34,960
like a couple of hundred views. And so it's just been, it's been amazing to follow your

22
00:01:34,960 --> 00:01:39,920
trajectory. And it's a delight to be on. Yeah, yeah. Well, I think in the shelter in Trenton

23
00:01:39,920 --> 00:01:45,200
episode, I mentioned that a lot of the things I've learned about AI, I've learned from talking with

24
00:01:45,200 --> 00:01:49,440
them. And the third part of this triumvirate, probably the most significant in terms of the

25
00:01:49,440 --> 00:01:53,520
things that I've learned about AI has been you will go out of the stuff on the record now.

26
00:01:53,520 --> 00:01:57,920
Great. Okay, first thing I had to get on record, tell me about the trillion dollar cluster.

27
00:01:58,560 --> 00:02:02,880
But by the way, I should mention, so the context of this podcast is today, there's,

28
00:02:02,880 --> 00:02:06,960
you're releasing a series called Situational Awareness. We're going to get into it. First

29
00:02:06,960 --> 00:02:11,520
question about that is tell me about the trillion dollar cluster. Yeah. So, you know,

30
00:02:11,520 --> 00:02:15,520
unlike basically most things that have come out of Silicon Valley recently, you know, AI is kind of

31
00:02:15,520 --> 00:02:20,400
this industrial process. You know, the next model doesn't just require, you know, some code,

32
00:02:20,400 --> 00:02:24,640
it's, it's, it's building a giant new cluster. You know, now it's building giant new power plants,

33
00:02:24,640 --> 00:02:30,000
you know, pretty soon it's going to be building giant new fabs. And, you know, since that should

34
00:02:30,000 --> 00:02:33,680
be tea, this kind of extraordinary sort of techno capital acceleration has been set into motion.

35
00:02:33,680 --> 00:02:38,160
I mean, basically, you know, exactly a year ago today, you know, Nvidia had their first kind of

36
00:02:38,160 --> 00:02:41,920
blockbuster earnings call, right, where it like went out 25% after hours and everyone was like,

37
00:02:41,920 --> 00:02:46,960
oh my god, AI, it's a thing. You know, I mean, I think within a year, you know, you know, and

38
00:02:46,960 --> 00:02:50,640
Nvidia, Nvidia data center revenue has gone from like, you know, a few billion a quarter to like,

39
00:02:50,640 --> 00:02:55,040
you know, 20, 25 billion a quarter now and, you know, continue to go up, like, you know,

40
00:02:55,040 --> 00:03:01,040
big tech capex, skyrocketing. And, you know, it's funny because it's both there's this sort of,

41
00:03:01,040 --> 00:03:04,720
this kind of crazy scramble going on, but in some sense, it's just the sort of continuation of

42
00:03:04,720 --> 00:03:07,920
straight lines on a graph, right? There's this kind of like long run trend, basically almost a

43
00:03:07,920 --> 00:03:11,840
decade of sort of training compute of the sort of largest AI systems growing by about, you know,

44
00:03:11,840 --> 00:03:17,360
half an order of magnitude, you know, 0.5 booms a year. And you just kind of play that forward,

45
00:03:17,360 --> 00:03:23,120
right? So, you know, GPT-4, you know, rumored or reported to have finished pre-training in 2022,

46
00:03:23,120 --> 00:03:27,360
you know, the sort of cluster size there was rumored to be about, you know, 25,000 H100s,

47
00:03:27,360 --> 00:03:32,640
you know, sorry, A100s on semi-analysis, you know, that's roughly, you know, if you do the

48
00:03:32,640 --> 00:03:36,880
math on that, it's maybe like a $500 million cluster, you know, it's very roughly 10 megawatts.

49
00:03:37,600 --> 00:03:44,480
And, you know, just play that forward half a new year, right? So, then 2024, that's a cluster,

50
00:03:44,480 --> 00:03:48,800
that's, you know, 100 megawatts, that's like 100,000 H100 equivalents, you know, that's,

51
00:03:50,720 --> 00:03:54,960
you know, costs in the billions, you know, play it forward, you know, two more years, 2026,

52
00:03:54,960 --> 00:03:59,440
that's a cluster, that's a gigawatt, you know, that's sort of a large nuclear reactor size,

53
00:03:59,440 --> 00:04:02,560
it's like the power of the Hoover Dam, you know, that costs tens of billions of dollars,

54
00:04:02,560 --> 00:04:06,320
that's like a million H100 equivalents, you know, 2028, that's a cluster, that's 10 gigawatts,

55
00:04:06,320 --> 00:04:11,600
right? That's more power than kind of like most U.S. states. That's, you know, like 10 million H100

56
00:04:11,600 --> 00:04:17,680
equivalents, you know, costs hundreds of billions of dollars. And then 2030, trillion-dollar cluster,

57
00:04:18,400 --> 00:04:23,680
100 gigawatts, over 20% of U.S. electricity production, you know, 100 million H100 equivalents.

58
00:04:24,320 --> 00:04:27,840
And that's just the training cluster, right? That's like the one largest training cluster,

59
00:04:27,840 --> 00:04:30,400
you know, and then there's more inference GPUs as well, right? Most of, you know, once there's

60
00:04:30,400 --> 00:04:36,000
products, most of them are going to be inference GPUs. And so, you know, U.S. power

61
00:04:36,000 --> 00:04:40,960
production has barely grown for like, you know, decades, and now we're really in for a ride.

62
00:04:40,960 --> 00:04:47,040
So, I mean, when I had Zuck on the podcast, he was claiming, not a plateau, per se, but

63
00:04:47,920 --> 00:04:52,400
that AI progress would be bottlenecked by specifically this constraint on energy,

64
00:04:52,400 --> 00:04:56,880
and specifically like, oh, gigawatt data centers are going to build another three gorgeous dam or

65
00:04:56,880 --> 00:05:02,320
something. I know that there's companies, according to public reports, who are planning

66
00:05:02,320 --> 00:05:07,120
things on the scale of a gigawatt data center. 10 gigawatt data center, who's going to be able

67
00:05:07,120 --> 00:05:11,200
to build that? I mean, the 100 gigawatt center, like a state, where are you getting,

68
00:05:11,200 --> 00:05:14,640
are you going to pump that into one physical data center? How is this going to be possible?

69
00:05:15,280 --> 00:05:18,400
Yeah, you know. What is Zuck missing? I mean, you know, I don't know. I think to 10 gigawatt,

70
00:05:18,400 --> 00:05:21,520
you know, like six months ago, you know, 10 gigawatt was the taco town. I mean, I think,

71
00:05:22,080 --> 00:05:25,040
I feel like now, you know, people have moved on, you know, 10 gigawatt is happening. I mean,

72
00:05:25,040 --> 00:05:29,760
I know there's the information report on OpenAI and Microsoft planning a $100 billion

73
00:05:30,400 --> 00:05:34,560
cluster. Is that the gigawatt or is that the 10 gigawatt? I mean, I don't know. But, you know,

74
00:05:34,560 --> 00:05:38,320
if you try to like map out how expensive would the 10 gigawatt cluster be, you know, that's

75
00:05:38,320 --> 00:05:42,880
maybe a couple hundred billion. So it's sort of on that scale. And they're planning it. They're

76
00:05:42,880 --> 00:05:49,760
working on it. You know, so it's not just sort of my crazy take. I mean, AMD, AMD, I think,

77
00:05:49,760 --> 00:05:54,640
forecasted a $400 billion AI accelerator market by 27. You know, I think it's, you know, and AI

78
00:05:54,640 --> 00:05:59,120
accelerators are only part of the expenditures. It's sort of, you know, I think sort of a trillion

79
00:05:59,120 --> 00:06:03,280
dollars of sort of like total AI investment by 2027 is sort of like, we're very much in track

80
00:06:03,280 --> 00:06:06,800
on it. I think the trillion dollar cluster is going to take a bit more sort of acceleration.

81
00:06:06,800 --> 00:06:11,360
But, you know, we saw how much sort of chat GPT unleashed, right? And so like every generation,

82
00:06:11,360 --> 00:06:13,760
you know, the models are going to be kind of crazy and people, it's going to shift the

83
00:06:13,760 --> 00:06:17,600
overtune window. And then, and then, you know, obviously the revenue comes in, right? So these

84
00:06:17,600 --> 00:06:21,120
are forward looking investments. The question is, do they pay off? Right? And so if we sort of

85
00:06:21,120 --> 00:06:26,560
estimated the, you know, the GPT four cluster at around 500 million, by the way, that's, that's

86
00:06:26,560 --> 00:06:29,360
sort of a common mistake people make is they say, you know, people say like a hundred million

87
00:06:29,360 --> 00:06:33,120
dollars before, but that's just the rental price, right? They're like, ah, you rent the cluster

88
00:06:33,120 --> 00:06:35,840
for three months. But it's, you know, if you're building the biggest cluster, you got to like,

89
00:06:35,840 --> 00:06:38,400
you got to build the whole cluster, you got to pay for the whole cluster, you can't just rent it

90
00:06:38,400 --> 00:06:41,520
for three. But I mean, the really, you know, once, once you're trying to get into the sort of

91
00:06:41,520 --> 00:06:44,320
hundreds of billions, eventually you got to get to like a hundred billion a year. I mean,

92
00:06:44,320 --> 00:06:47,120
I think this is where it gets really interesting for the big tech companies, right? Because like,

93
00:06:47,120 --> 00:06:51,200
their revenues are on order, you know, hundreds of billions, right? So it's like 10 billion fine,

94
00:06:51,200 --> 00:06:56,160
you know, and it'll pay off the, you know, 2024 size training cluster. But, you know, really

95
00:06:56,160 --> 00:06:59,600
when sort of big tech, it'll be gangbusters is a hundred billion a year. And so the question is

96
00:06:59,600 --> 00:07:04,880
sort of how feasible is a hundred billion a year from AI revenue? And, you know, it's a lot more

97
00:07:04,880 --> 00:07:08,800
than right now. But I think, you know, if you sort of believe in the trajectory of the AI systems,

98
00:07:08,800 --> 00:07:13,600
as I do, which we'll probably talk about, it's not that crazy, right? So there's, I think there's

99
00:07:13,600 --> 00:07:18,640
like 300 million, you know, ish Microsoft office subscribers, right? And so they have co-pilot

100
00:07:18,640 --> 00:07:22,160
now and I don't know what they're selling it for. But, you know, suppose you sold some sort of AI

101
00:07:22,160 --> 00:07:26,560
add-on for a hundred bucks a month, and you sold that to, you know, a third of Microsoft office

102
00:07:26,560 --> 00:07:29,680
subscribers subscribe to that. That'd be a hundred billion right there. You know, a hundred dollars

103
00:07:29,680 --> 00:07:32,880
a month is, you know, a lot. That's a lot. Yeah. It's a lot. It's a lot. For a third of office

104
00:07:32,880 --> 00:07:35,760
subscribers. Yeah. But it's, but it's, you know, for the average dollars worker, it's like a few

105
00:07:35,760 --> 00:07:39,360
hours of productivity a month. And it's, you know, kind of like, you have to be expecting pretty lame

106
00:07:39,360 --> 00:07:44,000
AI progress to not hit like, you know, some few hours of productivity a month of, of, of, of, yeah.

107
00:07:44,000 --> 00:07:49,040
Okay, sure. So let's assume all this. Yeah. What happens in the next few years in terms of

108
00:07:50,000 --> 00:07:55,440
what is the one gigawatt training, the AI that's trained on the one gigawatt data center? What

109
00:07:55,440 --> 00:07:58,960
can it do with the one on the 10 gigawatt data center? Just map out the next few years of AI

110
00:07:58,960 --> 00:08:03,440
progress for me. Yeah. I think probably the sort of 10 gigawatt range is sort of my best guess

111
00:08:03,440 --> 00:08:06,800
for when you get the sort of true AGI. I mean, yeah, I think it's sort of like one gigawatt

112
00:08:06,800 --> 00:08:09,760
data center. And again, I think actually compute is overrated and we're going to talk about that.

113
00:08:09,760 --> 00:08:13,600
But what we will talk about compute right now. So, you know, I think so 25, 26, we're going to get

114
00:08:13,600 --> 00:08:19,520
models that are, you know, basically smarter than most college graduates. I think sort of the

115
00:08:19,520 --> 00:08:22,400
practice, a lot of the economic usefulness, I think really depends on sort of, you know,

116
00:08:22,400 --> 00:08:25,680
sort of on hobbling. Basically, it's, you know, the models are kind of, you know,

117
00:08:25,680 --> 00:08:28,640
they're smart, but they're limited, right? They're, you know, there's chat bot, you know,

118
00:08:28,640 --> 00:08:31,520
and things like being able to use a computer, things like being able to do kind of like a

119
00:08:31,520 --> 00:08:37,440
genetic long horizon tasks. Yeah. And then I think by 27, 28, you know, if you extrapolate the trends

120
00:08:37,440 --> 00:08:40,800
and, you know, we'll talk about that more later. And I talked about in the series, I think we hit,

121
00:08:40,800 --> 00:08:44,240
you know, basically, you know, like as smart as the smartest experts, I think the on hobbling

122
00:08:44,240 --> 00:08:49,600
trajectory kind of points to, you know, looks much more like an agent than a chat bot and much

123
00:08:49,600 --> 00:08:53,200
more almost like basically a drop in remote worker, right? So it's not like, I think basically, I

124
00:08:53,200 --> 00:08:56,320
mean, I think this is the sort of question on the economic returns. I think a lot of the,

125
00:08:56,320 --> 00:08:59,920
a lot of the intermediate AI systems could be really useful, but, you know, it actually just

126
00:08:59,920 --> 00:09:03,920
takes a lot of slap to integrate them, right? Like GVD4, you know, whatever, 4.5, you know,

127
00:09:03,920 --> 00:09:06,880
probably there's a lot you could do with them in a business use case. But, you know, you really

128
00:09:06,880 --> 00:09:10,320
got to change your workflows to make them useful. And it's just like, there's a lot of, you know,

129
00:09:10,320 --> 00:09:14,000
it's a very Tyler Cowan-esque take. It takes a long time to diffuse. You know, it's like, you

130
00:09:14,000 --> 00:09:20,640
know, we're an SF and so we missed that or whatever. But I think in some sense, you know,

131
00:09:20,640 --> 00:09:24,880
the way a lot of these systems won't be integrated is, is you kind of get this sort of sonic boom

132
00:09:24,880 --> 00:09:29,040
where it's, you know, the sort of intermediate systems could have done it, but it would have taken

133
00:09:29,040 --> 00:09:33,040
slap. And before you do the slap to integrate them, you get much more powerful systems, much

134
00:09:33,040 --> 00:09:36,480
more powerful systems that are sort of on hobbled. And so they're this agent and there's drop in

135
00:09:36,560 --> 00:09:40,400
remote worker. And, you know, and then you're kind of interacting with them like a coworker,

136
00:09:40,400 --> 00:09:44,480
right? You know, you can take do zoom calls with them and you're slacking them. And you're like,

137
00:09:44,480 --> 00:09:48,000
ah, can you do this project? And then they go off and they, you know, go away for a week and write

138
00:09:48,000 --> 00:09:52,960
a first draft and get feedback on them and, you know, run tests on their code. And then they come

139
00:09:52,960 --> 00:09:57,440
back and then you see it and you tell them a little bit more things or, you know, and that'll

140
00:09:57,440 --> 00:10:02,480
be much easier to integrate. And so, you know, it might be that actually you need a bit of overkill

141
00:10:02,480 --> 00:10:05,280
to make the sort of transition easy and to really harvest the games.

142
00:10:05,280 --> 00:10:07,920
What do you mean by the overkill? Overkill on the model capabilities?

143
00:10:07,920 --> 00:10:11,120
Yeah. Yeah. So basically intermediate models could do it, but it would take a lot of slap.

144
00:10:11,120 --> 00:10:14,400
I see. And so then, you know, the like, actually it's just the drop in remote worker kind of AGI

145
00:10:14,400 --> 00:10:17,680
that can automate, you know, cognitive tasks that actually just ends up kind of like,

146
00:10:17,680 --> 00:10:20,640
you know, basically it's your like, you know, the intermediate models would have made the

147
00:10:20,640 --> 00:10:24,320
software engineer more productive, but, you know, will the software engineer adopted? And then the,

148
00:10:24,320 --> 00:10:28,400
you know, 27 model is, well, you know, you just don't need the software engineer. You can literally

149
00:10:28,400 --> 00:10:31,280
interact with it like a software engineer and it'll do the work of a software engineer.

150
00:10:32,000 --> 00:10:37,920
So the last episode I did was with John Shulman. Yeah. And I was asking about basically this and

151
00:10:37,920 --> 00:10:42,400
one of the questions I asked is, we have these models that have been coming out in the last year

152
00:10:42,400 --> 00:10:47,200
and none of them seem to have significantly surpassed GPT-4 and certainly not in the

153
00:10:47,200 --> 00:10:51,680
agentic way in which they are interacting with as a co-worker, you know, the brag that they

154
00:10:51,680 --> 00:10:56,880
got a few extra points on MMLU or something. And even GPT-4.0, it's cool that they can talk

155
00:10:56,880 --> 00:11:02,160
like Scarlett Johansson or something, but like... And honestly, I'm going to use that.

156
00:11:03,040 --> 00:11:07,920
Oh, I guess not anymore, not anymore. Okay, but the whole co-worker thing. So

157
00:11:08,800 --> 00:11:11,840
this is going to be a wrong question, but you can address it in any order. But

158
00:11:12,880 --> 00:11:17,120
the, it makes sense to me why they'd be good at answering questions. They have a bunch of

159
00:11:17,680 --> 00:11:22,240
data about how to complete Wikipedia text or whatever. Where is the equivalent training

160
00:11:22,240 --> 00:11:28,160
data that enables it to understand what's going on in the Zoom call? How does this connect with

161
00:11:28,160 --> 00:11:32,560
what they were talking about in the Slack? What is the cohesive project that they're going after

162
00:11:33,200 --> 00:11:36,880
based on all this context that I have? Where is that training data coming from?

163
00:11:37,680 --> 00:11:42,480
Yeah. So I think a really key question for sort of AI progress in the next few years is sort of

164
00:11:42,480 --> 00:11:46,800
how hard is it to do, sort of unlock the test time compute overhang? So, you know, right now,

165
00:11:46,800 --> 00:11:51,680
GPT-4.0 answers a question and, you know, it kind of can do a few hundred tokens of kind of chain

166
00:11:51,680 --> 00:11:54,880
of thought. And that's already a huge improvement, right? Sort of like, this is a big on hobbling

167
00:11:54,880 --> 00:12:00,000
before, you know, answer a math question, it's just shotgun. And, you know, if you try to kind of

168
00:12:00,000 --> 00:12:02,640
like answer a math question by saying the first thing that came to mind, you know, you wouldn't

169
00:12:02,640 --> 00:12:08,080
be very good. So, you know, GP-4 thinks for a few hundred tokens. And, you know, if I thought for a

170
00:12:08,080 --> 00:12:10,800
few hundred, you know, if I think at like a hundred tokens a minute, and I thought for a few minutes-

171
00:12:10,800 --> 00:12:12,240
You're thinking much more than a hundred tokens.

172
00:12:12,240 --> 00:12:16,960
I don't know. If I thought for like a hundred tokens a minute, you know, it's like what GP-4

173
00:12:16,960 --> 00:12:20,240
does, maybe it's like, you know, it's equivalent to me thinking for three minutes or whatever, right?

174
00:12:22,480 --> 00:12:26,960
You know, suppose GP-4 could think for millions of tokens, right? That's sort of plus four rooms,

175
00:12:26,960 --> 00:12:30,160
plus four days of magnitude on test time compute, just like on one problem.

176
00:12:30,960 --> 00:12:34,240
It can't do it right now. It kind of gets stuck, right? Like write some code, even if, you know,

177
00:12:34,240 --> 00:12:36,800
you can do a little bit of iterative debugging, but eventually just kind of like,

178
00:12:36,800 --> 00:12:40,480
it can't, it kind of gets stuck in something, it can't correct its errors and so on.

179
00:12:40,480 --> 00:12:45,040
And, you know, in a sense, there's this big overhang, right? And like other areas of ML,

180
00:12:45,040 --> 00:12:49,040
you know, there's this great paper on AlphaGo, right? Where you can trade off train time and

181
00:12:49,040 --> 00:12:52,320
test time compute. And if you can use, you know, four rooms, more test time compute, that's almost

182
00:12:52,320 --> 00:12:56,000
like, you know, a three and a half room bigger model. Just because, again, like you can, you know,

183
00:12:56,000 --> 00:13:00,560
if a hundred tokens a minute, a few million tokens, that's a few months of sort of working time.

184
00:13:00,560 --> 00:13:03,600
There's a lot more you can do in a few months of working time than, and then right now. So the

185
00:13:03,600 --> 00:13:09,360
question is, how hard is it to unlock that? And I think the, you know, the sort of short

186
00:13:09,360 --> 00:13:14,720
timelines AI world is if it's not that hard. And the reason it might not be that hard is that,

187
00:13:15,360 --> 00:13:18,960
you know, there's only really a few extra tokens you need to learn, right? You need to kind of

188
00:13:19,040 --> 00:13:23,040
learn error correction tokens, the tokens where you're like, ah, I think I made a mistake. Let me

189
00:13:23,040 --> 00:13:25,920
think about that again. You need to learn the kind of planning tokens. That's kind of like, I'm going

190
00:13:25,920 --> 00:13:30,080
to start by making a plan. Here's my plan of attack. And then I'm going to write a draft. And I'm

191
00:13:30,080 --> 00:13:33,040
going to like, now I'm going to critique my draft. I'm going to think about it. And so it's not,

192
00:13:33,040 --> 00:13:37,440
it's not things that models can do right now. But, you know, the question is how hard is that?

193
00:13:38,240 --> 00:13:41,040
And in some sense, also, you know, there's sort of two paths to agents, right? You know,

194
00:13:41,680 --> 00:13:44,720
when Cholto was on your podcast, you know, he talked about kind of scaling,

195
00:13:44,800 --> 00:13:49,280
leading to more nines of reliability. And so that's one path. I think the other path is

196
00:13:49,280 --> 00:13:53,760
this sort of like unhobbling path where you, it needs to learn this kind of like system to

197
00:13:53,760 --> 00:13:58,000
process. And if it can learn this sort of system to process, it can just use kind of millions of

198
00:13:58,000 --> 00:14:04,720
tokens and think for them and be cohesive and be coherent. You know, one analogy. So when you drive,

199
00:14:04,720 --> 00:14:09,120
here's an analogy, when you drive, right? Okay, you're driving. And, you know, most of the time,

200
00:14:09,120 --> 00:14:12,320
you're kind of an autopilot, right? You're just kind of driving and you're doing well. And then,

201
00:14:13,280 --> 00:14:17,040
but sometimes you hit like a weird construction zone or a weird intersection, you know, and then I

202
00:14:17,040 --> 00:14:20,320
sometimes like, you know, my passenger seat, my girlfriend, I'm kind of like, ah, be quiet for

203
00:14:20,320 --> 00:14:23,680
a moment. I need to like figure out what's going on, right? And that's sort of like, you know,

204
00:14:23,680 --> 00:14:27,520
you go from autopilot to like the system to is jumping in and you're thinking about how to do

205
00:14:27,520 --> 00:14:31,520
it. And so the scaling scaling is improving that system one autopilot. And I think it's sort of,

206
00:14:31,520 --> 00:14:36,000
it's the brute force way to get to kind of agents who just improve that system. But if you can get

207
00:14:36,000 --> 00:14:41,680
that system to working, then, you know, I think you could like quite quickly jump, you know,

208
00:14:41,680 --> 00:14:45,600
to sort of this like more identified, you know, test time, compute, overhang is unlocked.

209
00:14:46,400 --> 00:14:53,520
What's the reason to think that this is an easy win in the sense that, oh, you just get the,

210
00:14:53,520 --> 00:14:59,040
there's like some loss function that easily enables you to train it to enable the system to

211
00:14:59,040 --> 00:15:02,720
thinking. Yeah, there's not a lot of animals that have system to thinking, you know, it like took

212
00:15:02,720 --> 00:15:06,640
a long time for evolution to give us system to thinking. Yeah, pre-training it like, listen,

213
00:15:06,640 --> 00:15:10,880
I get it, you got like trillions of tokens of internet text, I get that like, yeah, you like

214
00:15:10,880 --> 00:15:15,200
match that and you get all these, all this free training capabilities. What's the reason

215
00:15:15,200 --> 00:15:20,000
to think that this is an easy and hobbling? Yeah, so, okay, a bunch of things. So

216
00:15:21,840 --> 00:15:26,320
first of all, free training is magical, right? And it's, and it's, and it gave us this huge

217
00:15:26,320 --> 00:15:31,440
advantage for, for, for models of general intelligence, because, you know, you could,

218
00:15:31,440 --> 00:15:34,800
you just predict the next token, but predicting the next token, I mean, it's sort of a common

219
00:15:34,800 --> 00:15:38,480
misconception. But what it does is lets this model learn these incredibly rich representations,

220
00:15:38,480 --> 00:15:41,120
right? Like these sort of representation learning properties are the magic of deep

221
00:15:41,120 --> 00:15:44,640
learning. You have these models, and instead of learning just kind of like, you know, whatever,

222
00:15:44,640 --> 00:15:47,520
statistical artifacts or whatever, it learns sort of these models of the world. You know,

223
00:15:47,520 --> 00:15:51,040
that's also why they can kind of like generalize, right? Because it learned the right representations.

224
00:15:52,320 --> 00:15:56,400
And so, you know, you train these models and you have this sort of like raw bundle of capabilities

225
00:15:56,400 --> 00:16:01,280
that's really useful. It's sort of this almost unformed raw mass. And sort of the unhobbling

226
00:16:01,280 --> 00:16:05,360
we've done over sort of like GP2 to GP4 was, was you kind of took this sort of like raw mass,

227
00:16:05,440 --> 00:16:08,880
and then you like RLHF it into a really good chat bot. And that was a huge win, right? Like,

228
00:16:08,880 --> 00:16:12,880
you know, going, going, you know, an RL, you know, in the original, I think it's truck GPT

229
00:16:12,880 --> 00:16:17,200
paper, you know, RLHF versus non RLHF model, it's like 100x model size win on sort of human

230
00:16:17,200 --> 00:16:21,040
preference rating, you know, it started to be able to do like simple chain of thought and so on.

231
00:16:21,040 --> 00:16:25,280
But you still have a disadvantage of all these kind of like raw capabilities. And I think there's

232
00:16:25,280 --> 00:16:28,880
still like a huge amount that you're not doing with them. And by the way, I think the sort of

233
00:16:28,880 --> 00:16:32,000
this pre training advantage is also sort of the difference to robotics, right? Where I think

234
00:16:32,000 --> 00:16:36,480
robotics, you know, you know, I think people used to say it was a hardware problem, but I think

235
00:16:36,480 --> 00:16:40,320
the hardware stuff is getting solved. But the thing we have right now is you don't have this

236
00:16:40,320 --> 00:16:44,160
huge advantage of being able to bootstrap yourself with pre training, you don't have all this sort

237
00:16:44,160 --> 00:16:47,840
of unsupervised learning you can do, you have to start right away with the sort of RL self play

238
00:16:47,840 --> 00:16:55,040
and so on. Alright, so now the question is why, you know, why might some of this unhobbling and

239
00:16:55,040 --> 00:17:01,200
RL and so on work? And again, there's sort of this advantage of bootstrapping, right? So I, you

240
00:17:01,280 --> 00:17:04,960
know, your Twitter bio is being pre trained, right? You're actually not being pre trained

241
00:17:04,960 --> 00:17:08,640
anymore. You're not being pre trained anymore. You are pre trained in like grade school and high

242
00:17:08,640 --> 00:17:13,840
school. At some point you transition to be able being able to like learn by yourself. Right?

243
00:17:14,720 --> 00:17:19,040
You weren't able to do that in elementary school. I don't know middle school probably high school

244
00:17:19,040 --> 00:17:24,480
maybe when sort of started some guidance. You know, college, you know, you're smart, you can kind

245
00:17:24,480 --> 00:17:28,640
of teach yourself. And then sort of models are just starting to enter that regime. Right? And so

246
00:17:28,720 --> 00:17:32,320
it's sort of like, it's a little bit probably a little bit more scaling. And then you got to

247
00:17:32,320 --> 00:17:37,760
figure out what goes on top and it won't be trivial, right? So a lot of a lot of deep learning is

248
00:17:37,760 --> 00:17:42,000
sort of like, you know, it sort of seems very obvious in retrospect. And there's sort of this

249
00:17:42,000 --> 00:17:46,080
some obvious cluster of ideas, right? There's sort of some kind of like thing that seems a little

250
00:17:46,080 --> 00:17:49,120
dumb, but there's kind of works, but there's a lot of details you have to get right. So I'm not

251
00:17:49,120 --> 00:17:51,600
saying this, you know, we're going to get this, you know, next month or whatever, I think it's

252
00:17:51,600 --> 00:17:55,200
going to take a while to like really figure out a while for you is like half a year or something.

253
00:17:55,920 --> 00:18:02,000
I don't know. I think it's between six months and three years, you know. But you know, I think

254
00:18:02,000 --> 00:18:06,640
it's possible. And I think there's, you know, I think, and this is, I think it's also very related

255
00:18:06,640 --> 00:18:10,960
to the sort of issue of the data wall. But I mean, I think the, you know, one intuition on the sort

256
00:18:10,960 --> 00:18:15,760
of like learning, learning, learning by yourself, right, is sort of pre-training is kind of the

257
00:18:15,760 --> 00:18:20,560
words are flying by, right? You know, and, and, or it's like, you know, the teacher is lecturing

258
00:18:20,560 --> 00:18:23,920
to you. And the models, you know, the words are flying by, you know, they're taking, they're

259
00:18:23,920 --> 00:18:28,640
just getting a little bit from it. But that's sort of not what you do when you learn from yourself,

260
00:18:28,640 --> 00:18:32,320
right? When you learn by yourself, you know, so you're reading a dense math textbook,

261
00:18:32,320 --> 00:18:34,720
you're not just kind of like skimming through it once, you know, you wouldn't learn that much

262
00:18:34,720 --> 00:18:38,560
from it. I mean, some word cells just give them through reading, reread and reread the math textbook,

263
00:18:38,560 --> 00:18:42,480
and then they memorize, you know, like, you just repeated the data, then they memorize.

264
00:18:42,480 --> 00:18:45,760
What you do is you kind of like, you read a page, kind of think about it, you have some internal

265
00:18:45,760 --> 00:18:50,480
monologue going on, you have a conversational study buddy, you try a practice problem, you know,

266
00:18:50,480 --> 00:18:54,240
you fail a bunch of times. At some point, it clicks, and you're like, this made sense,

267
00:18:54,240 --> 00:18:58,400
then you read a few more pages. And so we've kind of bootstrapped our way to being, being able to

268
00:18:58,400 --> 00:19:03,600
do that now with models, or like just starting to be able to do that. And then the question is,

269
00:19:03,600 --> 00:19:08,640
you know, being able to like, read it, think about it, you know, try problems. And the question is,

270
00:19:08,640 --> 00:19:12,000
can you, you know, all this sort of self place, synthetic data, RL is kind of like making that

271
00:19:12,000 --> 00:19:18,480
thing work. So basically translate, translate translating like in context, like right now,

272
00:19:18,480 --> 00:19:22,160
there's like in context learning, right, super sample efficient. There's that, you know, in the

273
00:19:22,160 --> 00:19:27,120
Gemini paper, right, it just like learns a language in context. And then you're pre training, not at

274
00:19:27,120 --> 00:19:33,040
all sample efficient. But you know, what humans do is they kind of like, they do in context learning,

275
00:19:33,040 --> 00:19:36,400
you read a book, you think about it until eventually it clicks. But then you somehow

276
00:19:36,400 --> 00:19:40,880
distill that back into the weights. And in some sense, that's sort of like what RL is trying to

277
00:19:40,880 --> 00:19:46,560
do. And like when RL is super finicky, but when RL works, RL is kind of magical, because it's sort

278
00:19:46,640 --> 00:19:51,040
of the best possible data for the model. It's like when you try a practice problem, and you know,

279
00:19:51,040 --> 00:19:55,280
and then you fail, and at some point you kind of figure it out in a way that makes sense to you,

280
00:19:55,280 --> 00:19:58,400
that's sort of like the best possible data for you, because like the way you would have solved the

281
00:19:58,400 --> 00:20:03,680
problem. And that's sort of, that's what RL is. Rather than just, you know, you kind of read how

282
00:20:03,680 --> 00:20:07,680
somebody else solved the problem and doesn't, you know, initially click. Yeah, by the way, if that

283
00:20:07,680 --> 00:20:12,000
takes sounds familiar, because it was like part of the question I asked on showman, that goes to

284
00:20:12,000 --> 00:20:15,920
illustrate the thing I said in the intro, where like a bunch of the things I've learned about AI,

285
00:20:16,000 --> 00:20:21,840
just like, we do these dinners before the interviews, and I'm like, oh, what should I ask

286
00:20:21,840 --> 00:20:28,880
on showman? What should I ask Dario? Okay, suppose this is the way things go, and we get these

287
00:20:28,880 --> 00:20:33,600
in hobblings. Yeah. And the scaling, right? So it's like, you have this baseline, just enormous

288
00:20:33,600 --> 00:20:38,320
force of scaling, right? Where it's like GP2 to GP4, you know, GP2, it could kind of like, it was

289
00:20:38,320 --> 00:20:42,160
amazing, right? It could string together plausible senses. But you know, it could, it could barely

290
00:20:42,160 --> 00:20:46,960
do anything. It's kind of like preschooler. And then GP4 is, you know, it's writing code, it like,

291
00:20:46,960 --> 00:20:50,320
you know, can do hard math. And so it's sort of like smart high school. And so this big jump,

292
00:20:50,320 --> 00:20:53,120
and you know, in sort of the essay series, I go through and kind of count the order's magnitude

293
00:20:53,120 --> 00:20:58,560
of compute scale up with algorithmic progress. And so sort of scaling alone, you know, sort of by

294
00:20:58,560 --> 00:21:05,040
2728 is going to do another kind of preschool to high school jump on top of GP4. And so that'll

295
00:21:05,040 --> 00:21:09,200
already be just like at a per token level, just incredibly smart, they'll get you some more reliability.

296
00:21:09,200 --> 00:21:12,480
And then you add these on hobblings that make it look much less like a chat bot, more like this

297
00:21:12,480 --> 00:21:18,240
agent, like a drop in remote worker. And, you know, that's when things really get gone.

298
00:21:18,240 --> 00:21:23,840
Okay, yeah. I want to ask you more questions about this. I think, yeah, let's zoom out. Okay,

299
00:21:23,840 --> 00:21:30,720
so suppose you're right about this. Yeah. And I guess you this is because of the 2027 cluster,

300
00:21:30,720 --> 00:21:36,400
we've got 10 gigawatt, 2027 10 gigawatt something 28 is the 10 gigawatt. Okay, so you'll be pulled

301
00:21:36,480 --> 00:21:43,200
for it. And so I guess that's like 5.5 level by 2027, like whatever that's called, right?

302
00:21:44,240 --> 00:21:48,640
What does the world look like at that point? You have these remote workers who can replace people.

303
00:21:49,600 --> 00:21:53,680
What is the reaction to that in terms of the economy, politics, geopolitics?

304
00:21:54,960 --> 00:22:01,280
Yeah, so, you know, I think 2023 was kind of a really interesting year to experience as somebody

305
00:22:01,280 --> 00:22:04,720
who is like, you know, really following the ice stuff where, you know, before that,

306
00:22:04,880 --> 00:22:13,680
what were you doing in 2023? I mean, open AI. And, and, and, you know, kind of went, you know,

307
00:22:13,680 --> 00:22:16,960
I mean, I was, I was been thinking about this and, you know, like talking to a lot of people,

308
00:22:16,960 --> 00:22:19,440
you know, in the years before, and it was this kind of weird thing, you know, you almost didn't

309
00:22:19,440 --> 00:22:23,520
want to talk about AI or AGI, you know, it's kind of a dirty word, right? And then 2023, you know,

310
00:22:23,520 --> 00:22:27,840
people saw chat, GPT for the first time in such a before, and it just like exploded, right? It

311
00:22:27,840 --> 00:22:32,000
triggered this kind of like, you know, you know, a huge sort of capital expenditures from all these

312
00:22:32,000 --> 00:22:38,640
firms and, and, and, and, you know, the explosion of revenue from NVIDIA and so on. And, you know,

313
00:22:38,640 --> 00:22:41,680
things have been quiet since then. But, you know, the next thing has been in the oven. And I sort

314
00:22:41,680 --> 00:22:45,760
of expect sort of every generation, these kind of like G forces to intensify, right? It's like,

315
00:22:45,760 --> 00:22:50,400
people see the models. There's like, you know, people haven't counted them. So they're going to

316
00:22:50,400 --> 00:22:53,920
be surprised. And it'll be kind of crazy. And then, you know, revenue is going to accelerate,

317
00:22:53,920 --> 00:22:57,120
you know, suppose you do hit the 10 billion, you know, end of this year, suppose it like just

318
00:22:57,120 --> 00:23:00,320
continues on this sort of doubling trajectory of, you know, like every six months of revenue

319
00:23:00,400 --> 00:23:03,600
doubling, you know, it's like, you're not actually that far from 100 billion, you know,

320
00:23:03,600 --> 00:23:06,960
maybe that's like 26. And so, you know, at some point, you know, like, you know,

321
00:23:06,960 --> 00:23:09,680
sort of what happened to NVIDIA is going to happen to big tech, you know, like their stocks,

322
00:23:09,680 --> 00:23:15,840
their, you know, that's going to explode. And I mean, I think a lot more people are going to feel

323
00:23:15,840 --> 00:23:21,760
it, right? I mean, I think the, I think 2023 was the sort of moment for me where it went from

324
00:23:21,760 --> 00:23:25,360
kind of AGI is a sort of theoretical abstract thing, and you'd make the models to like,

325
00:23:25,360 --> 00:23:29,360
I see it, I feel it. And like, I see the path, I see where it's going. I like,

326
00:23:30,000 --> 00:23:33,280
I think I can see the cluster where it's strained on, like the rough combination of algorithms,

327
00:23:33,280 --> 00:23:36,400
the people, like how it's happening. And I think, you know, most of the world is not,

328
00:23:36,400 --> 00:23:40,640
you know, most of the people feel it are like right here, right? But, but, you know, I think a

329
00:23:40,640 --> 00:23:46,960
lot more of the world is going to start feeling it. And I think that's going to start being kind of

330
00:23:46,960 --> 00:23:51,760
intense. Okay. So, right now, who feels that you can, you go on Twitter and there's these

331
00:23:51,760 --> 00:23:55,040
GPT wrapper companies like, whoa, GPT Floreau is going to change our business.

332
00:23:55,040 --> 00:23:57,760
I mean, I'm so, so bearish on the wrapper companies, right? Because like, they're the ones

333
00:23:57,760 --> 00:24:00,560
that are going to be like, the wrapper companies are betting on stagnation, right? The wrapper

334
00:24:00,560 --> 00:24:03,280
companies are betting like, you have these intermediate models and take so much left

335
00:24:03,280 --> 00:24:06,160
to integrate them. And I'm kind of like, I'm really bearish because I'm like,

336
00:24:06,160 --> 00:24:08,800
we're just going to sonic boom you, you know, and we're going to get the unhauled ones, we're

337
00:24:08,800 --> 00:24:12,000
going to get the drop in remote worker. And then, you know, your stuff is not going to matter.

338
00:24:12,000 --> 00:24:19,520
Okay. Sure. Sure. So that's done. Now, who, so the SF is paying attention now, or this crowd

339
00:24:19,520 --> 00:24:25,200
here is paying attention. Who is going to be paying attention in 2026, 2027? And, but,

340
00:24:25,200 --> 00:24:27,840
presumably this is, these are years in which the hundreds of billions of CapEx is being

341
00:24:27,840 --> 00:24:32,000
spent on the eye. I mean, I think the, the national security state is going to be starting

342
00:24:32,000 --> 00:24:36,400
to pay a lot of attention. And I, you know, I hope we get to talk about that.

343
00:24:36,400 --> 00:24:40,080
Okay. Let's talk about it now. What happens? Yeah. Like, well, what is the sort of political

344
00:24:40,080 --> 00:24:43,840
reaction immediately? Yeah. And even like internationally, like what people see like

345
00:24:43,840 --> 00:24:47,040
right now, I don't know if like Xi Jinping like reads the news and sees like,

346
00:24:47,120 --> 00:24:51,280
yeah, I don't know. Oh my God, like MMLU score on that. What are you doing about this comrade?

347
00:24:54,320 --> 00:24:58,240
So what happens when the, like what, what are the, he's like sees a remote replacement and it

348
00:24:58,240 --> 00:25:01,120
has a hundred billion dollars in revenue. There's a lot of businesses that have a hundred billion

349
00:25:01,120 --> 00:25:04,720
dollars in revenue and people don't like aren't staying up all night talking about it.

350
00:25:05,680 --> 00:25:10,160
The question, I think the question is when, when does the CCP and when does the sort of

351
00:25:10,160 --> 00:25:15,040
American national security establishment realize that superintelligence is going to be

352
00:25:15,040 --> 00:25:18,320
absolutely decisive for national power, right? And this is where, you know, the sort of intelligence

353
00:25:18,320 --> 00:25:21,280
explosion stuff comes in, which, you know, we should also talk about later, you know, it's sort

354
00:25:21,280 --> 00:25:24,560
of like, you know, you have AGI, you have this sort of drop in remote worker that can replace,

355
00:25:24,560 --> 00:25:27,920
you know, you or me, at least that sort of remote jobs, you know, cognitive jobs.

356
00:25:29,600 --> 00:25:34,960
And then, you know, I think fairly quickly, you know, I mean, by default, you know, you

357
00:25:34,960 --> 00:25:37,520
turn the crank, you know, one or two more times, you know, and then you get a thing that's

358
00:25:37,520 --> 00:25:40,720
smarter than humans. But I think even, even more than just turning the cramp a few more times,

359
00:25:41,600 --> 00:25:47,040
you know, I think one of the first jobs to be automated is going to be that of sort of an AI

360
00:25:47,040 --> 00:25:52,400
researcher engineer. And if you can automate AI research, you know, I think things can start

361
00:25:52,400 --> 00:25:56,640
going very fast. You know, right now, there's already this trend of, you know, half in order of

362
00:25:56,640 --> 00:26:00,240
magnitude a year of algorithmic progress, you know, suppose, you know, at this point, you're

363
00:26:00,240 --> 00:26:04,880
going to have GPU fleets in the tens of millions for inference, you know, or more. And you're going

364
00:26:04,880 --> 00:26:09,840
to be able to run like 100 million human human equivalents of these sort of automated AI researchers.

365
00:26:09,920 --> 00:26:14,000
And if you can do that, you know, you can maybe do, you know, a decade's worth of sort of ML

366
00:26:14,000 --> 00:26:20,080
research progress in a year, you know, get the some sort of 10x speed up. And if you can do that,

367
00:26:20,080 --> 00:26:24,720
I think you can make the jump to kind of like AI that is vastly smarter than humans, you know,

368
00:26:24,720 --> 00:26:29,040
within a year, a couple years. And then, you know, that broadens, right? So you have this,

369
00:26:29,040 --> 00:26:33,280
you have this sort of initial acceleration of AI research that broadens to like you apply R&D to

370
00:26:33,280 --> 00:26:38,320
a bunch of other fields of technology. And the sort of like extremes, you know, at this point,

371
00:26:38,320 --> 00:26:42,560
you have like a billion, just super intelligent researchers, engineers, technicians, everything,

372
00:26:42,560 --> 00:26:46,080
you're superbly competent, all the things, you know, they're going to figure out robotics.

373
00:26:46,080 --> 00:26:49,280
Or we talked about it being a software problem. Well, you know, you have, you have a billion of

374
00:26:50,320 --> 00:26:53,680
super smart, smarter than the smartest human researchers, AI researchers on your cluster,

375
00:26:53,680 --> 00:26:56,320
you know, at some point during the intelligence explosion, they're going to be able to figure

376
00:26:56,320 --> 00:27:02,320
out robotics, you know, and then again, that expands. And, you know, I think if you play this

377
00:27:02,320 --> 00:27:10,880
picture forward, I think it is fairly unlike any other technology in that it will, I think,

378
00:27:10,880 --> 00:27:16,160
you know, a couple years of lead could be utterly decisive in say like military competition, right?

379
00:27:16,160 --> 00:27:19,760
You know, if you look at like go for one, right? Go for one, you know, like the Western coalition

380
00:27:19,760 --> 00:27:23,520
forces, you know, they had, you know, like a hundred to one kill ratio, right? And that was like,

381
00:27:23,520 --> 00:27:27,040
they had better sensors on their tanks, you know, and they had, they had better, you know, more

382
00:27:27,040 --> 00:27:31,600
precision precision missiles, right? Like GPS, and they had, you know, stealth, and they had sort

383
00:27:31,680 --> 00:27:36,400
of a few, you know, maybe 20, 30 years of technological lead, right? And they, you know,

384
00:27:36,400 --> 00:27:42,800
just completely crushed them. Super intelligence applied to sort of broad fields of R&D. And

385
00:27:42,800 --> 00:27:45,760
then, you know, the sort of industrial explosion as well, you have the robots, you're just making lots

386
00:27:45,760 --> 00:27:49,920
of material, you know, I think that could compress, I mean, basically compress kind of like a century

387
00:27:49,920 --> 00:27:53,840
worth of technological progress since the last decade. And that means that, you know, a couple

388
00:27:53,840 --> 00:27:58,720
years could mean a sort of go for one style, like, you know, advantage in military affairs.

389
00:27:58,960 --> 00:28:05,120
And, you know, including, like, you know, a decisive advantage that even like preempts

390
00:28:05,120 --> 00:28:08,320
nukes, right? Suppose, like, you know, how do you find the stealth and nuclear submarines?

391
00:28:08,320 --> 00:28:11,200
Like right now, that's a problem of like, you have sensors, you have the software,

392
00:28:11,200 --> 00:28:14,720
like tech where they are, you know, you can do that, you can find them, you have kind of like

393
00:28:14,720 --> 00:28:18,160
millions or billions of like mosquito-like, you know, size drones, and that, you know,

394
00:28:18,160 --> 00:28:22,080
they take out the nuclear submarines, they take out the mobile launchers, they take out the other

395
00:28:22,080 --> 00:28:27,440
nukes. And anyway, so I think enormously destabilizing, enormously important for national power,

396
00:28:28,880 --> 00:28:33,520
and at some point, I think people are going to realize that, not yet, but they will. And when

397
00:28:33,520 --> 00:28:39,760
they will, I think there will be sort of, you know, I don't think it'll just be the sort of AI

398
00:28:39,760 --> 00:28:44,480
researchers in charge. And, you know, I think on the, you know, the CCP is going to, you know,

399
00:28:44,480 --> 00:28:48,000
have sort of an all-out effort to like infiltrate American AI labs, right? You know, like billions

400
00:28:48,000 --> 00:28:51,600
of dollars, thousands of people, you know, full force of the sort of, you know, Ministry of State

401
00:28:51,600 --> 00:28:55,120
Security. CCP is going to try to, you know, like outbuild us, right? Like they, you know, their,

402
00:28:55,120 --> 00:28:59,520
you know, power in China, you know, like the electric grid, you know, they added a U.S. is,

403
00:28:59,520 --> 00:29:03,440
you know, a complete, like they added as much power in the last decade as like sort of entire

404
00:29:03,440 --> 00:29:06,880
U.S. electric grid. So like the 100 gigawatts cluster, at least the 100 gigawatts is going

405
00:29:06,880 --> 00:29:11,120
to be a lot easier for them to get. And so I think sort of, you know, by this point, I think it's

406
00:29:11,120 --> 00:29:16,720
going to be like an extremely intense sort of international competition. Okay, so in this picture,

407
00:29:18,800 --> 00:29:23,840
one thing I'm uncertain about is whether it's more like what you say, where it's more of an

408
00:29:23,840 --> 00:29:31,680
implosion of you have developed an AGI and then you make it into an AI researcher. And for a while,

409
00:29:31,680 --> 00:29:37,760
a year or something, you're only using this ability to make hundreds of millions of other AI

410
00:29:37,760 --> 00:29:43,600
researchers. And then like the thing that comes out of this really frenetic process is a super

411
00:29:43,600 --> 00:29:47,920
intelligence. And then that goes out in the world and is developing robotics and helping you take

412
00:29:47,920 --> 00:29:50,800
over other countries and whatever. It's a little bit more, you know, it's a little bit more kind

413
00:29:50,800 --> 00:29:53,600
of like, you know, it's not like, you know, on and off, it's a little bit more gradual, but it's

414
00:29:53,600 --> 00:29:57,200
sort of like it's an explosion that starts narrowly. It's can do cognitive jobs, you know, the highest

415
00:29:57,200 --> 00:30:02,080
RI use for cognitive jobs is make the AI better, like solve robotics, you know, and as as as you

416
00:30:02,080 --> 00:30:05,680
know, you solve robotics, now you can do R&D and, you know, like biology and other technology.

417
00:30:06,560 --> 00:30:09,680
You know, initially you start with the factory workers, you know, they're wearing the glasses

418
00:30:09,680 --> 00:30:12,800
and the the AirPods, you know, and the AI is instructing them, right? Because, you know,

419
00:30:12,800 --> 00:30:16,160
you kind of make any worker into a skilled technician, and then you have the robots come in.

420
00:30:16,160 --> 00:30:20,960
And anyway, so it sort of expands, this process expands. Metas revans are a compliment to their

421
00:30:20,960 --> 00:30:24,800
llama. Well, you know, whatever, like, you know, the fabs in the US, the constrained skilled workers,

422
00:30:24,800 --> 00:30:27,840
right? You have, you have, even if you don't have robots that you have the cognitive super

423
00:30:27,840 --> 00:30:30,640
intelligence and, you know, it can kind of make them all into skilled workers immediately. But

424
00:30:30,640 --> 00:30:34,160
that's, you know, it's a very brief period, you know, robots will come soon. Sure. Okay. Okay, so

425
00:30:34,160 --> 00:30:39,680
suppose this is actually how the tech progresses in the United States, maybe because these companies

426
00:30:39,680 --> 00:30:43,360
are already experiencing hundreds of billions of dollars of revenue. At this point, you know,

427
00:30:43,360 --> 00:30:46,640
companies are barring, you know, hundreds of billions of more in the corporate debt markets,

428
00:30:46,640 --> 00:30:51,200
you know. But why is a CCP bureaucrat, some 60 year old guy, he looks at this and he's like,

429
00:30:51,200 --> 00:30:56,480
oh, it's like, co-pilot has gotten better now. Why are they now? I mean, this is much more than

430
00:30:56,480 --> 00:31:03,120
co-pilot has gotten better now. I mean, to them, like, yeah, because to shift the production of

431
00:31:03,120 --> 00:31:10,320
an entire country, to dislocate energy that is otherwise being used for consumer goods or

432
00:31:10,320 --> 00:31:18,160
something and to make it that all feed into the data centers. What part of this whole story is

433
00:31:18,160 --> 00:31:23,280
you realize the super intelligence is coming soon, right? And I guess you realize it, maybe I realize

434
00:31:23,280 --> 00:31:27,600
it. I'm not sure how much I realize it, but will the will the national security apparatus in the

435
00:31:27,600 --> 00:31:31,920
United States and the CCP realize it? Yeah, I mean, look, I think in some sense, this is a really

436
00:31:31,920 --> 00:31:36,960
key question. I think we have sort of a few more years of mid game, basically, and where you have

437
00:31:36,960 --> 00:31:43,280
a few more 2023s, and that just starts updating more and more people. And I think the trend lines

438
00:31:43,280 --> 00:31:50,320
will become clear. I think you will see some amount of the sort of COVID dynamic, right?

439
00:31:50,320 --> 00:31:57,760
Like COVID was February of 2020. It honestly feels a lot like today, where it feels like this

440
00:31:57,760 --> 00:32:03,600
utterly crazy thing is about, is impending, is coming. You kind of see the exponential and yet

441
00:32:03,600 --> 00:32:07,040
most of the world just doesn't realize, right? The mayor of New York is like, go out to the shows,

442
00:32:07,040 --> 00:32:15,840
and this is just Asian racism or whatever. But at some point, the exponential, at some point,

443
00:32:15,840 --> 00:32:20,960
people saw it. And then just kind of crazy radical reactions came.

444
00:32:20,960 --> 00:32:24,720
Right. Okay, so by the way, what were you doing during COVID? February?

445
00:32:26,000 --> 00:32:27,280
Like freshman, sophomore, what?

446
00:32:28,080 --> 00:32:31,760
Junior? But still, like, what were you, like 17-year-old junior or something?

447
00:32:33,760 --> 00:32:37,120
And then, like, did you short the market or something?

448
00:32:37,120 --> 00:32:37,680
Yeah, yeah, yeah.

449
00:32:37,680 --> 00:32:40,400
Okay. Did you sell at the right time?

450
00:32:40,400 --> 00:32:40,800
Yeah.

451
00:32:40,800 --> 00:32:46,400
Okay. Yeah, so there will be like a March 2020 moment, the thing that was COVID, but here.

452
00:32:47,920 --> 00:32:52,240
Now, then you can make the analogy that you make in the series that this will then

453
00:32:53,200 --> 00:32:58,240
cause the reaction of like, we got to do the Manhattan Project for America here.

454
00:32:58,240 --> 00:33:02,720
I wonder what the politics of this will be like, because the difference here is,

455
00:33:02,720 --> 00:33:08,400
it's not just like, we need the bomb to beat the Nazis. It's, we're building this thing that's

456
00:33:08,400 --> 00:33:12,400
making all our entry prices rise a bunch, and it's automating a bunch of our jobs.

457
00:33:12,400 --> 00:33:15,360
And the climate change stuff, like people are going to be like, oh my god, it's making climate

458
00:33:15,360 --> 00:33:20,400
change worse. And it's helping big tech. Like, politically, this doesn't seem like a dynamic

459
00:33:20,960 --> 00:33:25,680
where the national security apparatus or the president is like, we have to step on the gas

460
00:33:25,680 --> 00:33:27,120
here and like, make sure America wins.

461
00:33:29,440 --> 00:33:32,800
Yeah. I mean, again, I think a lot of this really depends on sort of how much people

462
00:33:32,800 --> 00:33:34,560
are feeling it, how much people are seeing it.

463
00:33:38,080 --> 00:33:41,600
You know, I think there's a thing where, you know, kind of basically our generation, right?

464
00:33:41,600 --> 00:33:46,400
We're kind of so used to kind of basically peace and like, you know, the world, you know,

465
00:33:46,400 --> 00:33:53,040
American hegemony and nothing matters. But, you know, the sort of like extremely intense and

466
00:33:53,040 --> 00:33:57,600
these extraordinary things happening in the world and like intense international competition is

467
00:33:57,600 --> 00:34:01,840
like very much the historical norm. Like in some sense, it's like, you know, sort of this,

468
00:34:01,840 --> 00:34:07,520
there's this sort of 20 year very unique period, but like, you know, the history of the world is

469
00:34:07,520 --> 00:34:12,000
like, you know, you know, like in World War II, right, it was like 50% of GDP went to, you know,

470
00:34:12,080 --> 00:34:15,600
like, you know, war prodigy and production, you know, the US borrowed over 60% of GDP, you know,

471
00:34:15,600 --> 00:34:21,040
and in, you know, I think Germany and Japan over 100%, World War I, you know, UK, Japan,

472
00:34:21,040 --> 00:34:29,200
sorry, UK, France, Germany all borrowed over 100% of GDP. And, you know, I think the sort of,

473
00:34:30,400 --> 00:34:33,840
much more was on the line, right? Like, you know, and, you know, people talk about World War I

474
00:34:33,840 --> 00:34:38,480
being so destructive and you know, like 20 million Soviet soldiers dying and like 20% of Poland.

475
00:34:38,480 --> 00:34:41,040
But, you know, that was just the sort of like, that happened all the time, right? You know,

476
00:34:41,040 --> 00:34:45,680
like seven years war, you know, like whatever 20, 30% of Prussia died, you know, like 30 years war,

477
00:34:45,680 --> 00:34:50,160
you know, like, I think, like, you know, up to 50% of like large swath of Germany died.

478
00:34:52,400 --> 00:35:00,880
And, you know, I think the question is, will these sort of like, will people see that the

479
00:35:00,880 --> 00:35:04,480
stakes here are really, really high and that basically is sort of like history is actually back.

480
00:35:06,160 --> 00:35:09,120
And I think, you know, I think the American national security state thinks

481
00:35:09,840 --> 00:35:13,200
very seriously about stuff like this. They think very seriously about competition with China. I

482
00:35:13,200 --> 00:35:16,960
think China very much thinks of itself on this historical mission and your nation, the Chinese

483
00:35:16,960 --> 00:35:21,600
nation, a lot about national power, I think a lot about like the world order. And then, you know,

484
00:35:23,040 --> 00:35:27,040
I think there's a real question on timing, right? Like, do they, do they start taking this seriously,

485
00:35:27,040 --> 00:35:30,240
right? Like when the intelligence explosion is already happening, like quite late, or do they

486
00:35:30,240 --> 00:35:33,840
start taking this seriously, like two years earlier on that matters a lot for how things play out.

487
00:35:33,840 --> 00:35:37,840
But at some point, they will, and at some point, they will realize that this will be sort of

488
00:35:37,840 --> 00:35:44,800
utterly decisive for, you know, not just kind of like some proxy war somewhere, but, you know,

489
00:35:44,800 --> 00:35:48,800
like whether liberal democracy can continue to thrive, whether, you know, whether the CCP will

490
00:35:48,800 --> 00:35:54,480
continue existing. And I think that will activate sort of forces that we haven't seen in a long time.

491
00:35:56,080 --> 00:35:59,520
The great conflict, the great power conflict thing definitely seems compelling.

492
00:36:00,080 --> 00:36:04,400
I think just all kinds of different things seem much more likely when you think from a historical

493
00:36:04,480 --> 00:36:08,160
perspective, when you zoom out beyond the liberal democracy that we've been living in,

494
00:36:08,720 --> 00:36:14,400
had the pleasure to live in America, let's say 80 years, including dictatorships, including all

495
00:36:14,960 --> 00:36:19,520
obviously war, famine, whatever. I was reading the Gullig Archipelago, and one of the chapters

496
00:36:19,520 --> 00:36:24,560
begins with Sojenitsyn saying, if you would have told Russian citizens under the czars that because

497
00:36:24,560 --> 00:36:29,440
of all these new technologies, we wouldn't see some great Russian revival or becomes a great power,

498
00:36:29,440 --> 00:36:36,240
and the citizens are made wealthy. But instead, what you would see is tens of millions of Soviet

499
00:36:36,240 --> 00:36:42,080
citizens tortured by millions of beasts in the worst possible ways, and that this is what would

500
00:36:42,080 --> 00:36:47,360
be the result of the 20th century, they wouldn't have believed you, they'd have called you a slanderer.

501
00:36:47,360 --> 00:36:52,000
Yeah, and you know, the, you know, the possibilities for dictatorship with super

502
00:36:52,000 --> 00:36:56,000
intelligence are sort of even crazier, right? I think, you know, imagine you have a perfectly

503
00:36:56,080 --> 00:37:00,720
loyal military and security force, right? That's it. No more, no more rebellions, right? No more

504
00:37:00,720 --> 00:37:05,120
popular uprisings, you know, perfectly loyal, you know, you have, you know, perfect lie

505
00:37:05,120 --> 00:37:08,800
detection, you know, you have surveillance of everybody, you know, you can perfectly figure

506
00:37:08,800 --> 00:37:12,240
out who's the dissenter, weed them out, you know, no Gorbachev would have ever risen to power,

507
00:37:12,240 --> 00:37:16,320
who had some doubts about the system, you know, no military coup would have ever happened.

508
00:37:16,320 --> 00:37:19,920
And I think you, I mean, you know, I think there's a real way in which,

509
00:37:20,000 --> 00:37:27,840
you know, part of why things have worked out is that, you know, ideas can evolve and, you know,

510
00:37:27,840 --> 00:37:31,760
there's sort of like some, some sense in which sort of time heals a lot of wounds and time, you

511
00:37:31,760 --> 00:37:35,520
know, and solves, solves, you know, a lot of debates and a lot of people had really strong

512
00:37:35,520 --> 00:37:38,640
convictions, but you know, a lot of those have been overturned by time because there's been this

513
00:37:38,640 --> 00:37:42,000
continued pluralism and evolution. I think there's a way in which kind of like, you know,

514
00:37:42,000 --> 00:37:45,840
if you take a CCP like approach to kind of like truth, truth is what the party says,

515
00:37:45,840 --> 00:37:48,960
when you supercharge that with super intelligence, I think there's a way in which that could just

516
00:37:48,960 --> 00:37:53,600
be like locked in and trying for, you know, a long time. And I think the possibilities are pretty

517
00:37:53,600 --> 00:37:59,760
terrifying. You know, your point about, you know, history and sort of like living in America for

518
00:37:59,760 --> 00:38:04,320
the past eight years, you know, I think this is one of the things I sort of took away from growing

519
00:38:04,320 --> 00:38:08,160
up in Germany is a lot of the stuff feels more visceral, right? Like, you know, my mother grew

520
00:38:08,160 --> 00:38:11,760
up in the former East, my father in the former West, they like met shortly after the wall fell,

521
00:38:11,760 --> 00:38:15,440
right? Like the end of the Cold War was the sort of extremely pivotal moment for me because it's,

522
00:38:15,440 --> 00:38:18,960
you know, it's the reason I exist, right? And then, you know, growing up in Berlin and, you know,

523
00:38:19,600 --> 00:38:25,760
former wall, you know, my great grandmother, who is still alive, is very important in my life.

524
00:38:25,760 --> 00:38:30,160
You know, she was born in 34, you know, grew up, you know, during the Nazi era, during, you know,

525
00:38:30,160 --> 00:38:34,000
all that, you know, then World War II, you know, like South of the firebombing of Dresden from the

526
00:38:34,000 --> 00:38:38,240
sort of, you know, country cottage or whatever were, you know, the day as kids were, you know,

527
00:38:38,240 --> 00:38:42,160
then, and then, you know, then spends most of her life in sort of the East German Communistic

528
00:38:42,160 --> 00:38:46,080
leadership. You know, she'd tell me about, you know, in like 54 when there's like the popular

529
00:38:46,080 --> 00:38:50,080
uprising, you know, in Soviet tanks came in, you know, her husband was telling her to get home

530
00:38:50,080 --> 00:38:55,440
really quickly, you know, get off off the streets, you know, had a, had a son who, who tried to,

531
00:38:55,440 --> 00:38:59,600
you know, ride a motorcycle across, across the Iron Curtain and then was put in the Stasi prison

532
00:38:59,600 --> 00:39:05,920
for a while. You know, and then finally, you know, when she's almost 60, you know, it was the first

533
00:39:05,920 --> 00:39:14,080
time she lives in, you know, a free country and, and a wealthy country. And, you know, when I was

534
00:39:14,080 --> 00:39:18,640
a kid, she was, she, the thing she always really didn't want me to do was like get involved in

535
00:39:18,640 --> 00:39:23,360
politics because like joining a political party was just, you know, it was a very bad connotations

536
00:39:23,360 --> 00:39:30,240
for her. Anyway, and she sort of raised me when I was young, you know, and so it, you know, it

537
00:39:30,240 --> 00:39:35,280
doesn't feel that long ago. It feels very close. Yeah. So I wonder when we're talking today about

538
00:39:35,920 --> 00:39:41,520
the CCP, listen, the people in China who will be doing the pro, their version of the project

539
00:39:41,520 --> 00:39:48,880
will be AI researchers who are somewhat westernized, who interact with either got educated in the West

540
00:39:48,880 --> 00:39:58,080
or have colleagues in the West. Are they going to sign up for the, the CCP project that's going to

541
00:39:58,080 --> 00:40:03,600
hand over control to Xi Jinping? What's your sense on, I mean, you're just like fundamentally,

542
00:40:03,600 --> 00:40:06,640
they're just people, right? Like, can't you like convince them about the dangers of super

543
00:40:06,640 --> 00:40:11,200
intelligence? Will they be in charge though? I mean, since this is, I mean, this is also the case,

544
00:40:11,200 --> 00:40:16,400
you know, uh, you know, in the US or whatever, this is sort of like rapidly depreciating influence

545
00:40:16,400 --> 00:40:20,400
of the lab employees. Like right now, the sort of AI lab employees have so much power, right over

546
00:40:20,400 --> 00:40:24,240
this, you know, like, they're going to get automated and then you saw this November event, so much

547
00:40:24,240 --> 00:40:27,280
power, right? But both, I mean, both they're going to get automated and they're going to lose all

548
00:40:27,280 --> 00:40:30,640
their power. And it'll just be, you know, kind of like a few people in charge with their sort of

549
00:40:30,720 --> 00:40:36,080
armies of automated eyes. But also, you know, it's sort of like the politicians and the generals

550
00:40:36,080 --> 00:40:38,560
and the sort of national security state, you know, a lot, you know, it's, I mean, there's sort of,

551
00:40:38,560 --> 00:40:41,520
this is the sort of some of these classic scenes from the, you know, the Oppenheimer movies, you

552
00:40:41,520 --> 00:40:44,960
know, the scientists built it and then it was kind of, you know, and the bomb was shipped away and

553
00:40:44,960 --> 00:40:49,200
it was out of their hands. You know, I actually, yeah, I think, I actually think it's good for

554
00:40:49,200 --> 00:40:54,240
like lab employees to be aware of this is like, you have a lot of power now, but you know, maybe

555
00:40:54,240 --> 00:40:59,520
not for that long and, you know, use it wisely. Yeah, I do, I do think they would benefit from

556
00:40:59,520 --> 00:41:03,040
some more, you know, organs of representative democracy. What do you mean by that? Oh, I mean,

557
00:41:03,040 --> 00:41:06,400
I, you know, in the sort of the, in the open AI board events, you know, employee at power was

558
00:41:06,400 --> 00:41:10,240
exercising a very sort of direct democracy way. And I feel like that's how some of how that went

559
00:41:10,240 --> 00:41:13,520
about, you know, I think it really highlighted the benefits of representative democracy and having

560
00:41:13,520 --> 00:41:19,040
some deliverative organs. Interesting. Yeah. Well, let's go back to the 100 billion revenue,

561
00:41:19,040 --> 00:41:23,920
whatever, and so these companies, Nala cluster, yeah, the companies are deploying, we're trying

562
00:41:23,920 --> 00:41:28,080
to build clusters that are this big. Yeah. Where are they building it? Because if you say it's

563
00:41:28,080 --> 00:41:32,640
the amount of energy that would be required for a small or medium sized US state, is it then Colorado

564
00:41:32,640 --> 00:41:35,840
gets no power and it's happening in the United States or is it happening somewhere else? Oh,

565
00:41:35,840 --> 00:41:38,960
I mean, I think that, I mean, in some sense, this is the thing that I always find funny is, you know,

566
00:41:38,960 --> 00:41:42,080
you talk about Colorado gets no power, you know, the easy way to get the power would be like, you

567
00:41:42,080 --> 00:41:46,080
know, displaced, less economically useful stuff, you know, it's like, whatever, buy up the aluminum

568
00:41:46,080 --> 00:41:49,440
smelting plant and, you know, that has a gigalot and, you know, we're going to replace it with the

569
00:41:49,440 --> 00:41:52,880
data center because that's important. I mean, that's not actually happening because a lot of

570
00:41:52,880 --> 00:41:56,640
these power contracts are really sort of long-term locked in, you know, there's obviously people

571
00:41:56,720 --> 00:42:00,320
don't like things like this. And so it sort of, it seems like in practice, what it's, what it's

572
00:42:00,320 --> 00:42:04,160
requiring, at least right now is building new power. The, that might change. And I think that

573
00:42:04,160 --> 00:42:07,120
that's when things get really interesting when it's like, no, we're just dedicating all of the

574
00:42:07,120 --> 00:42:11,840
power to the AGI. Okay, so right now it's building new power, 10 gigawatt, I think quite doable.

575
00:42:12,640 --> 00:42:15,120
You know, it's like a few percent of like US natural gas production.

576
00:42:16,880 --> 00:42:19,680
You know, I mean, when you have the 10 gigawatt training cluster, you have a lot more in

577
00:42:19,680 --> 00:42:23,200
friends. So that starts getting more, you know, I think 100 gigawatt, that starts getting pretty

578
00:42:23,200 --> 00:42:26,880
wild. You know, that's, you know, again, it's like over 20% of US electricity production.

579
00:42:28,320 --> 00:42:31,920
I think it's pretty doable, especially if you're willing to go for like natural gas.

580
00:42:32,880 --> 00:42:37,040
I do, I do think, I do think it is incredibly important, incredibly important that these

581
00:42:37,040 --> 00:42:39,840
clusters are in the United States. And why does it matter? It's in the US?

582
00:42:42,640 --> 00:42:47,840
I mean, look, I think there's some people who are, you know, trying to build clusters elsewhere. And

583
00:42:47,840 --> 00:42:51,200
you know, there's like a lot of free flowing Middle Eastern money that's trying to build clusters

584
00:42:51,200 --> 00:42:56,640
elsewhere. I think this comes back to the sort of like national security question we talked about

585
00:42:56,640 --> 00:43:00,320
earlier. Like would you, I mean, would you do the Manhattan Project and the UAE, right? And I think,

586
00:43:00,320 --> 00:43:04,000
I think basically like putting, putting the clusters, you know, I think you can put them in the US,

587
00:43:04,000 --> 00:43:08,000
you can put them in sort of like ally democracies. But I think once you put them in kind of like

588
00:43:08,000 --> 00:43:11,520
dictatorships, authoritarian dictatorships, you kind of create this, you know, irreversible security

589
00:43:11,520 --> 00:43:16,400
risk, right? So I mean, one cluster is there, much easier for them to exfiltrate the weights.

590
00:43:16,400 --> 00:43:20,080
You know, they can like literally steal the AGI, the superintelligence. It's like they got a copy of

591
00:43:20,080 --> 00:43:24,240
the, you know, of the atomic bomb, you know, and they just got the direct replica of that.

592
00:43:24,240 --> 00:43:28,000
And it makes it much easier to them. I mean, we're ties to China, you can ship that to China.

593
00:43:28,000 --> 00:43:31,760
So that's a huge risk. Another thing is they can just seize the compute, right? Like maybe right

594
00:43:31,760 --> 00:43:34,720
now they just think of this, I mean, in general, I think people, you know, I think the issue here

595
00:43:34,720 --> 00:43:37,840
is people are thinking of this as they, you know, chat, GBT, big tech product clusters. But I think

596
00:43:37,840 --> 00:43:42,320
the cluster is being planned now, you know, three to five years out, like it will be the like AGI

597
00:43:42,320 --> 00:43:46,000
superintelligence clusters. And so anyway, so like when things get hot, you know, they might just

598
00:43:46,000 --> 00:43:50,640
seize the compute. And I don't know, suppose we put like, you know, 25% of the compute capacity

599
00:43:50,640 --> 00:43:54,240
in the sort of Middle Eastern decaderships, well, they seize that. And now it's sort of a ratio

600
00:43:54,240 --> 00:43:58,160
of compute of three to one, and you know, still have some more, but even like, even, even only,

601
00:43:58,160 --> 00:44:02,240
only 25% of compute there, like, I think it starts getting pretty hairy, you know, I think three to

602
00:44:02,240 --> 00:44:06,960
one is like, not that great of a ratio, you can do a lot with that amount of compute. And then look,

603
00:44:06,960 --> 00:44:09,760
even, even if they don't actually do this, right, even they don't actually seize the compute, even

604
00:44:09,760 --> 00:44:13,760
they actually don't steal the weights. There's just a lot of implicit leverage you get, right?

605
00:44:13,760 --> 00:44:20,880
They get, they get the seat at the AGI table. And, you know, I don't know why we're giving

606
00:44:20,880 --> 00:44:26,400
authoritarian dictatorships the seat at the AGI table. Okay, so there's going to be a lot of

607
00:44:26,400 --> 00:44:30,800
compute in the Middle East, if these deals go through. First of all, who's who is it? Just

608
00:44:30,800 --> 00:44:34,000
like every single big tech company is just trying to figure out where they're going to be.

609
00:44:34,000 --> 00:44:40,400
Okay, okay. Well, I guess there's reports, I think Microsoft or yeah, which we'll get into.

610
00:44:40,480 --> 00:44:44,720
So the UAE gets a bunch of compute because we're building the clusters there.

611
00:44:45,760 --> 00:44:50,960
And why, so let's say they have 25% of, why does a compute ratio matter?

612
00:44:53,440 --> 00:44:56,640
If it's about them being able to kick off the intelligence explosion,

613
00:44:56,640 --> 00:45:00,880
isn't it just some threshold where you have 100 million AI researchers or you don't?

614
00:45:00,880 --> 00:45:04,560
I mean, you can do a lot with, you know, 33 million extremely smart scientists.

615
00:45:05,280 --> 00:45:08,560
And, you know, and again, a lot of the stuff, you know, so first of all, it's like,

616
00:45:08,640 --> 00:45:11,760
you know, that might be enough to build the crazy bio weapons, right? And then you're in a

617
00:45:11,760 --> 00:45:15,280
situation where like now, wow, we've just like, they stole the weights, they seized the compute,

618
00:45:15,280 --> 00:45:19,280
now they can make, you know, they can build these crazy new WMDs that, you know,

619
00:45:19,280 --> 00:45:22,560
will be possible super intelligence. And then you just kind of like proliferated the stuff.

620
00:45:22,560 --> 00:45:26,800
And, you know, it'll be really powerful. And also, I mean, I think, you know,

621
00:45:27,840 --> 00:45:31,520
three acts on compute isn't actually that much. And so the, you know, the,

622
00:45:33,840 --> 00:45:36,240
you know, I think a thing I worry a lot about is

623
00:45:38,560 --> 00:45:42,800
I think everything, I think the riskiest situation is if we're in some sort of like

624
00:45:42,800 --> 00:45:47,200
really tight neck, feverish international struggle, right? If we're like really close

625
00:45:47,200 --> 00:45:52,000
with the CCP and we're like months apart. I think the situation we want to be in,

626
00:45:52,000 --> 00:45:55,040
we could be in, if we played our cards, right, is a little bit more like, you know, the US,

627
00:45:55,040 --> 00:45:59,200
you know, building the atomic bomb versus the German project way behind, you know, years behind.

628
00:46:00,560 --> 00:46:03,520
And if we have that, I think we just have so much more wiggle room, like to get safety,

629
00:46:03,520 --> 00:46:06,400
right? We're going to be building like, you know, there's going to be these crazy new WMDs,

630
00:46:06,400 --> 00:46:09,520
you know, things that completely undermine, you know, nuclear deterrence, you know,

631
00:46:10,240 --> 00:46:14,960
intense competition. And that's so much easier to deal with if, you know, you're like, you know,

632
00:46:14,960 --> 00:46:17,440
it's not just, you know, you don't have somebody right on your tails, you got to go,

633
00:46:17,440 --> 00:46:21,840
go, go, you got to go maximum speed, you have no wiggle room. You're worried that at any time

634
00:46:21,840 --> 00:46:24,800
they can overtake you. I mean, they can also just try to outbuild you, right? Like they might,

635
00:46:24,800 --> 00:46:28,720
they might literally win, like China might literally win if they can steal the weights,

636
00:46:28,720 --> 00:46:33,440
because they can outbuild you. And they maybe have less caution, both, you know, good and bad

637
00:46:33,440 --> 00:46:38,560
caution, you know, kind of like whatever unreasonable regulations we have. Or you're just

638
00:46:38,560 --> 00:46:41,760
in this really tight race. And I think it is that sort of like, if you're in this really tight race,

639
00:46:41,760 --> 00:46:44,880
this sort of feverish struggle, I think that's when sort of there's the greatest peril of

640
00:46:44,880 --> 00:46:50,160
self-destruction. So then presumably the companies that are trying to build clusters in

641
00:46:50,160 --> 00:46:53,920
the Middle East realize this, what is it? Is it just that it's impossible to do this in America?

642
00:46:53,920 --> 00:46:57,520
And if you want American companies to do this at all, then you do it in Middle East or not at all.

643
00:46:57,520 --> 00:46:59,920
And then you just like, I'm trying to build a three gorgeous dam cluster.

644
00:46:59,920 --> 00:47:02,640
I mean, there's a few reasons. One of them is just like, people aren't thinking about this as

645
00:47:02,640 --> 00:47:06,160
the AGI superintelligence cluster. They're just like, ah, you know, like cool clusters for my,

646
00:47:06,160 --> 00:47:11,120
you know, for my chat. So they're building in the plans right now are clusters, which

647
00:47:11,120 --> 00:47:14,160
are ones that are like, because if you're doing once we're inference, presumably you

648
00:47:14,160 --> 00:47:16,880
could like spread them out across the country or something. But the ones they're building,

649
00:47:16,880 --> 00:47:21,440
they realize we're going to do one training run in this thing we're building.

650
00:47:21,440 --> 00:47:24,960
I just think it's harder to distinguish between inference and training compute. And so people

651
00:47:24,960 --> 00:47:27,920
can claim it's training compute, but I think they might realize that actually, you know,

652
00:47:27,920 --> 00:47:31,120
this is going to be useful for, yeah, sorry, they might say it's inference compute. And

653
00:47:31,120 --> 00:47:32,480
actually it's useful for training compute too.

654
00:47:32,480 --> 00:47:34,960
Because of the synthetic data and things like that.

655
00:47:34,960 --> 00:47:38,000
Yeah. The future of training, you know, like RL looks a lot like inference, for example, right?

656
00:47:38,000 --> 00:47:41,840
Or, or you just kind of like end up connecting them, you know, in time, you know, it's like

657
00:47:41,840 --> 00:47:45,360
a lot raw material, you know, it's like, you know, it's, it's, it's placing your uranium refinement

658
00:47:45,360 --> 00:47:46,400
facilities there. Sure.

659
00:47:46,400 --> 00:47:49,520
Anyway, so a few reasons, right? One is just like, they don't think about this as the AGI cluster.

660
00:47:49,520 --> 00:47:54,000
Another is just like easy money from the Middle East, right? Another one is like, you know,

661
00:47:55,520 --> 00:47:58,720
people saying, some people think that, you know, you can't do it in the U.S.

662
00:47:58,800 --> 00:48:03,600
And, you know, I think we actually face this sort of real system competition here, because

663
00:48:03,600 --> 00:48:06,640
again, some people think it's only autocracies that can do this, that can kind of like top

664
00:48:06,640 --> 00:48:11,520
down, mobilize the sort of industrial capacity, the power, you know, get the stuff done fast.

665
00:48:11,520 --> 00:48:14,320
And again, this is the sort of thing, you know, we haven't faced in a while.

666
00:48:15,040 --> 00:48:18,640
But, you know, during the Cold War, like we really, there was this sort of intense system

667
00:48:18,640 --> 00:48:22,160
competition, right? Like East West Germany was this, right? Like West Germany kind of like

668
00:48:22,160 --> 00:48:27,600
liberal democratic capitalism versus kind of, you know, communist state planned. And, you know,

669
00:48:27,600 --> 00:48:32,000
now it's obvious that the sort of, you know, the free world would win. But, you know, even as

670
00:48:32,000 --> 00:48:36,080
late as like 61, you know, Paul Samuelson was predicting that the Soviet Union would outgrow

671
00:48:36,080 --> 00:48:40,320
the United States because they were able to sort of mobilize industry better. And so yeah,

672
00:48:40,320 --> 00:48:45,120
there's some people who, you know, shippost about loving America by day, but then in private,

673
00:48:45,120 --> 00:48:49,280
they're betting against America. They're betting against the liberal order. And I think, I basically

674
00:48:49,280 --> 00:48:52,480
just think it's a bad bet. And the reason I think it's a bad bet is I think this stuff is just

675
00:48:52,480 --> 00:48:56,080
really possible in the U.S. And so there's make it possible in the U.S. There's some amount that

676
00:48:56,160 --> 00:48:59,200
we have to get our act together, right? So I think there's basically two paths to doing it in the

677
00:48:59,200 --> 00:49:03,520
U.S. One is you just got to be willing to do natural gas. And there's ample natural gas, right?

678
00:49:03,520 --> 00:49:07,040
You put your cluster in West Texas, you put it in, you know, Southwest Pennsylvania by the,

679
00:49:07,040 --> 00:49:12,160
you know, Marcello Shale. Ten gigawatt cluster is super easy. 100 gigawatt cluster also pretty

680
00:49:12,160 --> 00:49:16,160
doable. You know, I think, you know, natural gas production in the United States is, you know,

681
00:49:16,160 --> 00:49:19,520
almost doubled in a decade. If you do that, you know, one more time over the next, you know,

682
00:49:20,080 --> 00:49:23,760
seven years or whatever, you know, you could power multiple trillion dollar data centers.

683
00:49:24,000 --> 00:49:28,080
But the issue there is, you know, a lot of people have sort of these made these climate

684
00:49:28,080 --> 00:49:31,280
commitments and not just government. It's actually the private companies themselves, right? The

685
00:49:31,280 --> 00:49:35,280
Microsoft, the Amazons and so on. And these climate commitments, so they won't do natural gas.

686
00:49:36,240 --> 00:49:39,120
And, you know, I admire the climate commitments, but I think at some point, you know,

687
00:49:39,760 --> 00:49:42,560
the national interest and national security kind of is more important.

688
00:49:43,840 --> 00:49:47,120
The other path is like, you know, you can do this sort of green energy mega projects, right? You

689
00:49:47,120 --> 00:49:53,760
do the solar and the batteries and the, you know, the SMRs and geothermal. But if we want to do that,

690
00:49:53,760 --> 00:49:57,840
there needs to be sort of a sort of broad, deregulatory push, right? So, like, you can't

691
00:49:57,840 --> 00:50:01,360
have permitting take a decade, right? So you got to reform FERC. You got to, like, have, you know,

692
00:50:01,360 --> 00:50:05,760
blanket NEPA exemptions for this stuff. You know, there's like Nain state level regulations,

693
00:50:05,760 --> 00:50:09,040
you know, that are like, yeah, you could build, you know, you build the solar panels and batteries

694
00:50:09,040 --> 00:50:12,800
next to your data center, but it'll still take years because, you know, you actually have to

695
00:50:12,800 --> 00:50:18,000
hook it up to the state electrical grid, you know, and you'll have to, like, use governmental

696
00:50:18,000 --> 00:50:21,920
powers to create rights of way to kind of, like, you know, have multiple clusters and connect them,

697
00:50:21,920 --> 00:50:25,920
you know, and have thick cables, basically. And so, look, I mean, ideally, we do both,

698
00:50:25,920 --> 00:50:29,120
right? Ideally, we do natural gas and the broad deregulatory agenda. I think we have to do at

699
00:50:29,120 --> 00:50:33,360
least one. And then I think this possible stuff is just possible in the United States.

700
00:50:33,360 --> 00:50:37,920
Yeah. I think a good analogy for this, by the way, before the conversation I was reading,

701
00:50:38,640 --> 00:50:42,240
there's a good book about World War II industrial mobilization in the United States called

702
00:50:42,240 --> 00:50:48,400
Freedom's Forge. Yeah. And I guess when we think back on that period, especially if you're from,

703
00:50:48,400 --> 00:50:52,560
if you read, like, the Patrick Hallison Fast and the Progress Study stuff, it's like,

704
00:50:52,560 --> 00:50:58,080
the hat state capacity back then, and people just got you done, but now it's a cluster.

705
00:50:58,080 --> 00:51:02,880
Wasn't it all the case? No, so it was really interesting. So you have people who are from

706
00:51:02,880 --> 00:51:09,040
the Detroit auto industry side, like Knudsen, who are running mobilization for the United States,

707
00:51:09,120 --> 00:51:13,040
and they were extremely incompetent. Yeah. But then at the same time, you had

708
00:51:13,040 --> 00:51:17,840
labor organization agitation, which is actually very analogous to the climate pledges and

709
00:51:17,840 --> 00:51:24,960
climate change concern we have today, where they would have these strikes while literally into 1941,

710
00:51:25,520 --> 00:51:31,920
that would cost millions of man hours worth of time when we're trying to make tens of millions,

711
00:51:31,920 --> 00:51:36,000
sorry, tens of thousands of planes a month or something. And they would just

712
00:51:36,640 --> 00:51:43,440
debilitate factories for, you know, trivial, like pennies on the dollar kind of concessions from

713
00:51:43,440 --> 00:51:50,080
capital. And it was concerns that, oh, the auto companies are trying to use the pretext of a

714
00:51:50,080 --> 00:51:57,440
potential war to actually prevent paying labor that money deserves. And so the climate changes

715
00:51:57,440 --> 00:52:00,800
today, like, you think, ah, fuck, America's fucked, like, we're not going to be able to build this

716
00:52:00,800 --> 00:52:05,120
shit. Like, if you, if you look at Nipah or something, but I didn't realize how debilitating

717
00:52:05,120 --> 00:52:08,720
labor was in like World War Two, right? It was just, you know, before at the, you know,

718
00:52:08,720 --> 00:52:12,240
it's sort of like 39 or whatever, the American military was in total shambles, right? You read

719
00:52:12,240 --> 00:52:15,360
about it and it reads a little bit like, you know, the German military today, right? It's like, you

720
00:52:15,360 --> 00:52:19,440
know, military expenditures, I think were less than 2% of GDP, you know, all the European countries

721
00:52:19,440 --> 00:52:23,920
had gone even in peacetime, you know, like above 10% of GDP, sort of this like rapid mobilization,

722
00:52:23,920 --> 00:52:27,280
there's nothing, you know, like, we're making kind of like no planes, there's no military

723
00:52:27,280 --> 00:52:30,800
contracts, everything had been starved during the Great Depression. But there was this latent

724
00:52:30,800 --> 00:52:35,120
capacity. And, you know, at some point, the United States got their act together. I mean,

725
00:52:35,120 --> 00:52:39,280
the thing I'll say is I think, you know, the supplies are the other way around too, to basically

726
00:52:39,280 --> 00:52:42,640
to China, right? And I think sometimes people are, you know, they kind of count them out a little

727
00:52:42,640 --> 00:52:46,560
bit and they're like the export controls and so on. And, you know, they're able to make seven

728
00:52:46,560 --> 00:52:50,160
nanometer chips now. I think there's a question of like, how many could they make? But, you know,

729
00:52:50,160 --> 00:52:53,040
I think there's at least a possibility that they're going to be able to mature that ability and make

730
00:52:53,040 --> 00:52:57,680
a lot of seven nanometer chips. And there's a lot of latent industrial capacity in China,

731
00:52:57,760 --> 00:53:01,360
and they are able to like, you know, build a lot of power fast. And maybe that isn't activated for

732
00:53:01,360 --> 00:53:06,240
AI yet. But at some point, you know, the same way the United States and like, you know, a lot of

733
00:53:06,240 --> 00:53:09,120
people in the US and the United States government is going to wake up, you know, at some point,

734
00:53:09,120 --> 00:53:16,320
the CCP is going to wake up. Yeah. Okay. Going back to the question of presumably companies,

735
00:53:16,320 --> 00:53:20,400
if are they blind to the fact that there's going to be some sort of, well, okay, so

736
00:53:20,400 --> 00:53:24,080
they realize that there's going, they realize scaling is a thing, right? Obviously, their whole

737
00:53:24,080 --> 00:53:29,360
plans are continued on scaling. And so they understand that we're going to be in 2020 building

738
00:53:29,360 --> 00:53:34,960
the 10 gigawatt data centers. And at this point, the people who can keep up our big tech just

739
00:53:34,960 --> 00:53:40,880
potentially at like the edge of their capabilities, then sovereign wealth fund, fund of things, and

740
00:53:40,880 --> 00:53:48,400
also big major countries like America, China, whatever. So what's their plan? If you look at

741
00:53:48,400 --> 00:53:54,000
like these AI labs, what's their plan given this landscape? Do they not want the leverage

742
00:53:54,000 --> 00:53:58,240
of having being in the United States? I mean, I think, I don't know, I think, I mean, one thing

743
00:53:58,240 --> 00:54:03,760
the Middle East does offer is capital, but it's like America has plenty of capital, right? It's

744
00:54:03,760 --> 00:54:06,720
like, you know, we have trillion dollar companies, like what are these Middle Eastern states, they're

745
00:54:06,720 --> 00:54:09,920
kind of like trillion dollar oil companies, and we have trillion dollar companies, and we have

746
00:54:09,920 --> 00:54:13,200
very deep financial markets, and it's like, you know, Microsoft could issue hundreds of billions

747
00:54:13,200 --> 00:54:17,440
of dollars of bonds, and they can pay for these clusters. I mean, look, I think another argument

748
00:54:17,440 --> 00:54:21,760
being made, and I think it's worth taking seriously is an argument that look, if we don't work with

749
00:54:21,760 --> 00:54:27,200
the UAE or with these Middle Eastern countries, they're just going to go to China, right? And so,

750
00:54:27,200 --> 00:54:30,160
you know, we, you know, they're going to build data centers, they're going to pour money into AI,

751
00:54:30,160 --> 00:54:36,240
regardless, and if we don't work with them, you know, they'll just support China. And look, I mean,

752
00:54:36,240 --> 00:54:41,280
I think, I think there's some merit to the argument, and in the sense that I think we should

753
00:54:41,280 --> 00:54:43,920
be doing basically benefit sharing with them, right? I think we should talk about this later,

754
00:54:43,920 --> 00:54:47,120
but I think basically sort of on the road to AGI, there should be kind of like two tiers of

755
00:54:47,120 --> 00:54:50,640
coalitions should be the sort of narrow coalition of democracies, that's sort of the

756
00:54:50,640 --> 00:54:54,560
coalition that's developing AGI. And then there should be a broader coalition where we kind of

757
00:54:54,560 --> 00:54:58,000
go to other countries, including, you know, dictatorships, and we're willing to offer them,

758
00:54:59,680 --> 00:55:02,160
you know, we're willing to offer them some of the benefits of the AI, some of the sharing.

759
00:55:02,160 --> 00:55:06,080
And so it's like, look, if, if, if the UAE wants to use AI products, if they want to run, you know,

760
00:55:06,080 --> 00:55:10,160
meta recommendation engines, if they want to run, you know, like the last generation models,

761
00:55:10,160 --> 00:55:14,320
that's fine. I think by default, they just like wouldn't have had this seat at the AGI table,

762
00:55:14,320 --> 00:55:17,440
right? And so it's like, yeah, they have some money, but a lot of people have money.

763
00:55:17,680 --> 00:55:23,200
And, you know, the only reason they're getting this sort of coursey at the AGI table, the only

764
00:55:23,200 --> 00:55:28,240
reason we're giving these dictators will have this enormous amount of leverage over this extremely

765
00:55:28,240 --> 00:55:34,000
national security relevant technology is because we're, you know, we're kind of getting them excited

766
00:55:34,000 --> 00:55:40,080
and offering it to them. You know, I think the other who like who specifically is doing this,

767
00:55:40,080 --> 00:55:43,440
like just the companies who are going there to fundraise or like this is the AGI is happening

768
00:55:43,520 --> 00:55:46,880
and you can find it or you can't. It's been reported that it's been reported that, you know,

769
00:55:46,880 --> 00:55:50,560
Sam is trying to raise, you know, seven trillion or whatever for a chip project. And, you know,

770
00:55:50,560 --> 00:55:54,080
it's unclear how many of the clusters will be there and so on. But it's, you know, definitely,

771
00:55:54,080 --> 00:55:57,600
definitely stuff is happening. I mean, look, I think another reason I'm a little bit,

772
00:55:57,600 --> 00:56:01,120
at least suspicious of this argument of like, look, if the US doesn't work with them, they'll

773
00:56:01,120 --> 00:56:06,640
go to China is, you know, I've heard heard from multiple people. And this wasn't, you know,

774
00:56:06,640 --> 00:56:10,320
from a time at open AI and I haven't seen the memo, but I have heard from multiple people

775
00:56:10,400 --> 00:56:15,120
that, you know, at some point several years ago, open AI leadership had sort of laid out a plan

776
00:56:15,120 --> 00:56:19,440
to fund and sell AGI by starting a bidding war between the governments of, you know, the United

777
00:56:19,440 --> 00:56:24,080
States, China and Russia. And so, you know, it's kind of surprising to me that they're willing to

778
00:56:24,080 --> 00:56:28,240
sell AGI to the Chinese and Russian governments. But also there's something that sort of feels a bit

779
00:56:28,240 --> 00:56:31,760
eerily familiar about kind of starting this bidding war and then kind of like playing them

780
00:56:31,760 --> 00:56:37,040
off each other. And well, you know, if you don't do this, China will do it. So anyway, interesting.

781
00:56:37,040 --> 00:56:42,720
Okay, so that's pretty fucked up. But given that, that's, okay, so suppose that you were right

782
00:56:42,720 --> 00:56:47,920
about, we ended up in this place because we got the way one of our friends put it is that the

783
00:56:47,920 --> 00:56:54,240
Middle East has like no other place in the world, billions of dollars or trillions of dollars up

784
00:56:54,240 --> 00:57:02,240
for persuasion. And the Microsoft board, it's only, it's only the dictator.

785
00:57:02,240 --> 00:57:06,640
Yeah. But so let's say you're right, that you shouldn't have gotten them excited about AGI in

786
00:57:06,640 --> 00:57:11,920
the first place. But now we're in a place where they are excited about AGI. And they're like,

787
00:57:11,920 --> 00:57:15,440
fuck, we want us to have GPT-5 where we're going to be off building super intelligence.

788
00:57:15,440 --> 00:57:19,920
This Atoms for Peace thing doesn't work for us. And if you're in this place,

789
00:57:21,920 --> 00:57:24,640
don't they already have the leverage? Aren't you like, and as you might as well just say-

790
00:57:24,640 --> 00:57:27,920
I don't think, I think the UAE on its own is not competitive, right? It's like, I mean,

791
00:57:27,920 --> 00:57:30,800
they're already export controlled. Like, you know, we're not, you know, there's like,

792
00:57:30,800 --> 00:57:34,080
you're not actually supposed to ship NVIDIA chips over there, right? You know, it's not like they

793
00:57:34,080 --> 00:57:36,640
have any of the leading AI labs. You know, it's like they have money, but you know,

794
00:57:36,640 --> 00:57:38,480
it's actually hard to just translate money into like-

795
00:57:38,480 --> 00:57:42,320
But the other things you've been saying about laying out your vision is very much there's

796
00:57:42,320 --> 00:57:47,120
this almost industrial process of you put in the compute and then you put in the algorithms.

797
00:57:47,120 --> 00:57:50,640
Yes. You add that up and you get AGI on the other end.

798
00:57:50,640 --> 00:57:56,160
Yes. If it's something more like that, then the case for somebody being able to catch up

799
00:57:56,160 --> 00:57:58,400
rapidly seems more compelling than if it's some bespoke.

800
00:57:58,400 --> 00:58:01,600
Well, well, if they can steal the algorithms and if they can steal the weights.

801
00:58:01,600 --> 00:58:05,040
That's really, that's really where sort of, I mean, we should talk about this. This is really

802
00:58:05,040 --> 00:58:11,920
important. And I think, you know. So like right now, how easy would it be for a foreign actor to

803
00:58:11,920 --> 00:58:17,840
steal the things that are like, not the things that are released about Scarlett Johansson's voice,

804
00:58:17,840 --> 00:58:20,400
but the RL things are talking about the unhobblings.

805
00:58:20,400 --> 00:58:24,240
I mean, I mean, all extremely easy, right? You know, I, you know, deep mind even like,

806
00:58:24,240 --> 00:58:27,600
you know, they don't make a claim that it's hard, right? Deep mind put out there,

807
00:58:27,600 --> 00:58:30,800
like whatever, frontier safety, something, and they like lay out security levels,

808
00:58:30,800 --> 00:58:33,600
and they, you know, security level zero to four and four is this new resilient,

809
00:58:33,600 --> 00:58:36,480
resistant to state actors. And they say we're at level zero, right?

810
00:58:36,480 --> 00:58:39,840
And then, you know, I mean, just recently, there was like an indictment of a guy who just

811
00:58:39,840 --> 00:58:43,680
like stole the code, a bunch of like really important AI code and went to China with it.

812
00:58:43,680 --> 00:58:47,280
And, you know, all he had to do to steal the code was, you know, copy the code and put it

813
00:58:47,280 --> 00:58:51,520
into Apple notes and then export it as PDF. And that got past their monitoring, right? And, you know,

814
00:58:51,520 --> 00:58:54,800
Google is the best security of any of the iLabs probably because they have the, you know, the

815
00:58:54,800 --> 00:58:58,080
Google infrastructure. I mean, I think, I don't know, roughly, I would think of this as like,

816
00:58:58,080 --> 00:59:02,160
you know, security of a startup, right? And like, what does security of a startup look like,

817
00:59:02,160 --> 00:59:05,840
right? You know, it's not that good. It's easy to steal.

818
00:59:05,840 --> 00:59:12,000
So even if that's the case, a lot of your posts is making the argument that, you know,

819
00:59:12,000 --> 00:59:15,360
why are we going to get the intelligence explosion? Because if we have somebody with

820
00:59:15,360 --> 00:59:19,280
the intuition of an Alec Radford, to be able to come up with all these ideas,

821
00:59:19,280 --> 00:59:24,080
that intuition is extremely valuable and you scale that up. But if it's a matter of these,

822
00:59:24,800 --> 00:59:32,160
if it's just in the code, that, like, if it's just the intuition, then that's not

823
00:59:32,160 --> 00:59:35,920
going to be just in the code, right? And also because of export controls, these countries

824
00:59:35,920 --> 00:59:40,480
are going to have slightly different hardware. You're going to have to make different trade-offs

825
00:59:40,480 --> 00:59:45,840
and probably rewrite things to be able to be compatible with that, including all these things.

826
00:59:45,840 --> 00:59:49,280
Is it just a matter of getting the right pen drive and you plug it into the gigawatt data center

827
00:59:49,280 --> 00:59:51,600
and exit the three gorgeous dam and then you're off to the races?

828
00:59:52,800 --> 00:59:55,920
I mean, like, there's a few different things, right? So one threat model is just stealing

829
00:59:55,920 --> 01:00:00,080
the weights themselves. And the weights one is sort of particularly insane, right? Because they

830
01:00:00,080 --> 01:00:04,560
can just like steal the literal like end product, right? Just like make a replica of the atomic bomb

831
01:00:04,560 --> 01:00:08,240
and then they're just like ready to go. And, you know, I think that one just is, you know,

832
01:00:08,240 --> 01:00:12,080
extremely important around the time we have AGI and superintelligence, right? Because it's,

833
01:00:12,080 --> 01:00:16,480
you know, China can build a big cluster. By default, we'd have a big lead, right? Because

834
01:00:16,480 --> 01:00:19,360
we have the better scientists, but we make the superintelligence, they just steal it,

835
01:00:19,360 --> 01:00:23,600
they're off to the races. Weights are a little bit less important right now. Because, you know,

836
01:00:23,600 --> 01:00:29,360
who cares if they steal the GPT-4 weights, right? Like whatever. And so, you know, we still have

837
01:00:29,360 --> 01:00:32,640
to get started on weight security now. Because, you know, look, if we think AGI by 27, you know,

838
01:00:32,640 --> 01:00:35,600
this stuff is going to take a while. And it, you know, it doesn't, you know, it's not just going

839
01:00:35,600 --> 01:00:38,800
to be like, oh, we do some access control. It's going to, you know, if you actually want to be

840
01:00:38,800 --> 01:00:43,680
resistant to sort of Chinese espionage, you know, it needs to be much more intense. The thing,

841
01:00:43,760 --> 01:00:46,720
though, that I think, you know, people aren't paying enough attention to is the secrets,

842
01:00:46,720 --> 01:00:52,240
as you say. And, you know, I think this is, you know, the compute stuff is sexy. You know,

843
01:00:52,240 --> 01:00:55,440
we talk about it. But, you know, I think that, you know, I think people underrate the secrets

844
01:00:56,560 --> 01:00:59,840
because they're, you know, I think they're, you know, the half an order of magnitude a year,

845
01:00:59,840 --> 01:01:02,960
just by default, sort of algorithmic progress, that's huge. You know, if we have a few-year

846
01:01:02,960 --> 01:01:07,600
lead by default, you know, that's 10, 30x, 100x bigger cluster, if we protected them.

847
01:01:08,640 --> 01:01:11,760
And then there's this additional layer of the data wall, right? And so, we have to get

848
01:01:11,760 --> 01:01:14,720
through the data wall. That means we actually have to figure out some sort of basic new paradigm,

849
01:01:14,720 --> 01:01:19,440
sort of the AlphaGo step two, right? AlphaGo step one is learns from human imitation. AlphaGo step

850
01:01:19,440 --> 01:01:23,760
two is the sort of self play RL. And everyone's working on that right now. And maybe we're going

851
01:01:23,760 --> 01:01:31,280
to crack it. And, you know, if China can't steal that, then they, you know, then they're stuck.

852
01:01:31,280 --> 01:01:36,880
If they can't steal it, they're off to the races. But whatever that thing is, is it like literally,

853
01:01:36,880 --> 01:01:40,160
I can write down on the back of a napkin, because if it's that easy, then why is it that hard for

854
01:01:40,160 --> 01:01:43,520
them to figure it out? And if it's more about the intuitions, then don't you just have to hire

855
01:01:43,520 --> 01:01:46,960
Alec Radford? Like, what are you copying now? Well, I think there's a few layers to this, right?

856
01:01:46,960 --> 01:01:53,440
So, I think at the top is kind of like sort of the, you know, fundamental approach, right? And

857
01:01:53,440 --> 01:01:57,280
sort of like, I don't know, on pre-training, it might be, you know, like, you know, unsupervised

858
01:01:57,280 --> 01:02:00,640
learning, next token protection, train on the entire internet. You actually get a lot of

859
01:02:00,640 --> 01:02:04,960
juice out of that already. That one's very quick to communicate. Then there's like,

860
01:02:04,960 --> 01:02:07,760
there's a lot of details that matter. And you were talking about this earlier, right? It's like,

861
01:02:07,760 --> 01:02:11,200
probably the way that thing people are going to figure out is going to be like somewhat

862
01:02:11,200 --> 01:02:14,880
obvious, or there's going to be some kind of like clear, you know, not that complicated thing,

863
01:02:14,880 --> 01:02:17,200
that'll work. But there's going to be a lot of details to getting that right.

864
01:02:17,200 --> 01:02:22,960
But if that's true, then again, why are we even, why do we think that getting state-level security

865
01:02:22,960 --> 01:02:26,320
in these stars will prevent China from catching up? If it's just like, oh, we know some sort of

866
01:02:26,320 --> 01:02:31,040
self-play RL, we require it to get past the data wall. And if it's as easy as you say,

867
01:02:31,040 --> 01:02:32,720
in some fundamental sense. Well, I don't know if it's that easy. I mean, again, like, yeah.

868
01:02:32,720 --> 01:02:36,160
But it's going to be solved by 2027, you say, like, right? It's like, not that hard.

869
01:02:36,160 --> 01:02:39,600
I just think, you know, the US and the sort of, I mean, all the leading antelabs in the United

870
01:02:39,600 --> 01:02:42,800
States, and they have this huge lead. I mean, by default, you know, China actually has some good

871
01:02:42,800 --> 01:02:46,080
LLMs. You know, why do they have good LLMs? They're just using the sort of open source code, right?

872
01:02:46,080 --> 01:02:50,720
You know, llama or whatever. And so the, the, I think people really underrate the sort of,

873
01:02:50,720 --> 01:02:54,800
both the sort of divergence on algorithmic progress and the lead the US would have by default,

874
01:02:54,800 --> 01:02:56,960
because by the, you know, all this stuff was published until recently, right?

875
01:02:56,960 --> 01:03:00,000
Like Chinchilla scaling laws were published, you know, there's a bunch of MOE papers,

876
01:03:00,000 --> 01:03:03,600
there's, you know, transformers and, you know, all that stuff was published. And so that's why

877
01:03:03,600 --> 01:03:06,960
open source is good. That's why China can make some good models. That stuff is now, I mean,

878
01:03:06,960 --> 01:03:11,200
at least they're not publishing it anymore. And, you know, if we actually kept it secret,

879
01:03:11,200 --> 01:03:15,440
it would be this huge edge. To your point about sort of like some tacit knowledge,

880
01:03:15,440 --> 01:03:18,240
now like Bradford, you know, there's, there's another layer at the bottom that is something

881
01:03:18,240 --> 01:03:21,600
about like, you know, large scale engineering work to make these big training ones work.

882
01:03:21,600 --> 01:03:25,840
I think that is a little bit more tacit knowledge. So I think that, but I think China will be able

883
01:03:25,840 --> 01:03:28,640
to figure that out. That's like sort of engineering stuff. They're going to figure out how to

884
01:03:28,640 --> 01:03:32,000
figure that out, but not how to get the RL thing working.

885
01:03:32,240 --> 01:03:37,680
I mean, look, I don't know, Germany during World War II, you know, they went down the wrong path,

886
01:03:37,680 --> 01:03:41,200
they did heavy water, and that was wrong. And there's actually, there's an amazing anecdote in

887
01:03:41,920 --> 01:03:45,360
the making of the atomic bomb on this, right? So, so secrecy is actually one of the most

888
01:03:45,360 --> 01:03:49,600
contentious issues, you know, early on as well. And, you know, part of it was sort of,

889
01:03:50,240 --> 01:03:53,760
you know, zillard or whatever really thought, you know, the sort of nuclear chain reaction was

890
01:03:53,760 --> 01:03:58,560
possible. And so an atomic bomb was possible when you went around and it was like, this is going to

891
01:03:58,560 --> 01:04:02,400
be of enormous strategic importance, military importance. And a lot of people didn't believe

892
01:04:02,400 --> 01:04:05,200
it or they're kind of like, well, maybe this is possible, but you know, I'm going to act as

893
01:04:05,200 --> 01:04:10,880
it's not possible. And, you know, science should be open and all these things. And anyway, and so

894
01:04:10,880 --> 01:04:15,520
these early days, so there had been some sort of incorrect measurements made on graphite as a

895
01:04:15,520 --> 01:04:20,080
moderator and that Germany had. And so they thought, you know, graphite was not going to work, we

896
01:04:20,080 --> 01:04:26,960
have to do heavy water. But then Fermi made some new measurements on graphite. And they indicated

897
01:04:26,960 --> 01:04:31,680
that graphite would work, you know, this is really important. And then, you know, zillard kind of

898
01:04:31,680 --> 01:04:36,000
assaulted Fermi with the kind of another secrecy appeal. And Fermi was just kind of, he was pissed

899
01:04:36,000 --> 01:04:39,600
off, you know, at a temper tantrum, you know, he was like, he thought it was absurd, you know,

900
01:04:39,600 --> 01:04:43,920
like, come on, this is crazy. But, you know, you know, zillard persisted, I think they

901
01:04:43,920 --> 01:04:50,320
roped in another guy, Pegram, and then Fermi didn't publish it. And, you know, that was just in time,

902
01:04:50,320 --> 01:04:54,560
because Fermi not publishing it meant that the Nazis didn't figure out graphite would work,

903
01:04:54,560 --> 01:04:58,160
they went down this path of heavy water. And that was the wrong path. That was one of the sort,

904
01:04:58,160 --> 01:05:01,280
you know, this is a key reason why the sort of German project didn't work out. They were kind

905
01:05:01,280 --> 01:05:08,160
of way behind. And, you know, I think we face a similar situation on are we are we just going to

906
01:05:08,160 --> 01:05:12,240
instantly leak the sort of how do we get past the data wall? What's the next paradigm? Or are we not?

907
01:05:12,240 --> 01:05:17,920
So and the reason this would matter is if there's like a being one year ahead would be a huge

908
01:05:17,920 --> 01:05:21,840
advantage in the world where it's like you deploy AI over time, and then just like, God,

909
01:05:21,840 --> 01:05:25,440
they're going to catch up anyway. I mean, I interviewed Richard Rhodes, the guy who wrote

910
01:05:25,440 --> 01:05:33,440
the making an atomic bomb. Yeah. And one of the anecdotes he had was when, so they'd realized

911
01:05:33,440 --> 01:05:39,360
America had the bomb, obviously we dropped it in Japan. And Beria goes, the guy who ran the NKBD,

912
01:05:40,400 --> 01:05:45,440
just a famously ruthless guy, just evil. And he goes to, I forgot the name of the guy, the Soviet

913
01:05:45,440 --> 01:05:51,200
scientist was running their version of the Mandan project. He says, comrade, you will get us

914
01:05:51,200 --> 01:05:55,280
the American bomb. Yeah. And the guy says, well, listen, their implosion device actually is not

915
01:05:55,280 --> 01:05:59,920
optimal. We should make it a different way. And Beria says, no, you will get us the American bomb

916
01:05:59,920 --> 01:06:07,200
or your family will be camped us. But the thing that's relevant about that anecdote is actually

917
01:06:07,840 --> 01:06:11,120
the Soviets would have had a better bomb if they hadn't copied the American design, at least

918
01:06:11,120 --> 01:06:15,600
initially. And which suggests that often in history, this is something that's not just for the

919
01:06:15,600 --> 01:06:22,320
Manhattan project, but there's this pattern of parallel invention where, because the tech tree

920
01:06:22,320 --> 01:06:26,240
implies that the certain thing is next, in this case, self play, RL, whatever.

921
01:06:28,160 --> 01:06:31,040
Then people are just like working on that. And like people are going to figure out around the same

922
01:06:31,040 --> 01:06:35,680
time. There's not, there's not going to be that much gap in who gets it first. It wasn't like

923
01:06:35,680 --> 01:06:39,200
famously the bunch of people were invented something like the light bulb around the same time and so

924
01:06:39,200 --> 01:06:43,760
for it. So, but is it just that like, yeah, that might be true, but it'll be the one year or the

925
01:06:43,760 --> 01:06:46,640
six months or whatever. Two years makes all the difference. I don't know if it'll be two years

926
01:06:46,640 --> 01:06:50,240
though. I mean, I actually, I mean, I actually think if we locked down the labs, we have, we have

927
01:06:50,240 --> 01:06:53,520
much better scientists, we're way ahead, it would be two years. But even, I think, even, I think,

928
01:06:53,520 --> 01:06:56,800
I think whether you, I think, yeah, I think even six months a year would make a huge difference.

929
01:06:56,800 --> 01:07:00,240
And this gets back to the sort of intelligence explosion. It's like a year might be the difference

930
01:07:00,240 --> 01:07:05,440
between, you know, a system that's sort of like human level and a system that is like vastly super

931
01:07:05,440 --> 01:07:09,520
human, right? It might be like five, five ooms, you know, even on the current pace, right? We went

932
01:07:09,600 --> 01:07:13,760
from, you know, I think on the math benchmark recently, right? Like, you know, three years ago

933
01:07:13,760 --> 01:07:19,280
on the math benchmark, we, you know, that that was, you know, this is a sort of really difficult

934
01:07:19,280 --> 01:07:23,600
high school competition math problems. You know, we were at, you know, a few percent couldn't solve

935
01:07:23,600 --> 01:07:27,840
anything. Now it's solved. And that was sort of at the normal pace of AI progress. You didn't have

936
01:07:27,840 --> 01:07:31,520
sort of a billion super intelligent resources, researchers. So like a year is a huge difference.

937
01:07:31,520 --> 01:07:35,120
And then particularly after super intelligence, right? Once this is applied to sort of lots of

938
01:07:35,120 --> 01:07:38,480
elements of R&D, once you get the sort of like industrial explosion with the robots and so on,

939
01:07:39,200 --> 01:07:42,880
you know, I think a year, you know, a couple years might be kind of like decades worth of

940
01:07:42,880 --> 01:07:47,200
technological progress and might, you know, again, it's like go for one, right? 20, 30 years of

941
01:07:47,200 --> 01:07:51,680
technological lead, totally decisive. You know, I think it really matters. The other reason it

942
01:07:51,680 --> 01:07:56,560
really matters is, you know, suppose, suppose they steal the weight, suppose they steal the

943
01:07:56,560 --> 01:08:00,800
algorithms and, you know, they're close on our tails. Suppose we still pull out ahead, right?

944
01:08:00,800 --> 01:08:05,200
We just kind of, we were a little bit faster, you know, we're three months ahead. I think the

945
01:08:05,200 --> 01:08:09,360
sort of like world in which we're really neck and neck, you know, you only have a three-month lead

946
01:08:09,360 --> 01:08:14,000
are incredibly dangerous, right? And we're in this like feverish struggle where like if they get ahead,

947
01:08:14,000 --> 01:08:19,840
they get to dominate, you know, sort of maybe they'd get a decisive advantage. They're about in

948
01:08:19,840 --> 01:08:24,160
clusters like crazy. They're willing to throw all caution to the wind. We have to keep up. There's

949
01:08:24,160 --> 01:08:28,320
some crazy new WMDs popping up. And then we're going to be in the situation where it's like,

950
01:08:28,320 --> 01:08:31,920
you know, crazy new military technology, crazy new WMDs, you know, like deterrence,

951
01:08:31,920 --> 01:08:35,680
mutually-disturbed interaction, like keeps changing, you know, every few weeks. And it's like,

952
01:08:35,680 --> 01:08:40,240
you know, completely unstable volatile situation. That is incredibly dangerous. So it's, I think,

953
01:08:40,240 --> 01:08:42,880
I think, you know, both, both from just the technologies are dangerous from the alignment

954
01:08:42,880 --> 01:08:45,840
point of view. You know, I think it might be really important during the intelligence explosion to

955
01:08:45,840 --> 01:08:50,240
have the sort of six-month, you know, wiggle room to be like, look, we're going to like

956
01:08:50,240 --> 01:08:53,040
dedicate more compute to alignment during this period because we have to get it right. We're

957
01:08:53,040 --> 01:08:58,400
feeling uneasy about how it's going. And so I think in some sense that like one of the most

958
01:08:58,400 --> 01:09:02,320
important inputs, so whether we will kind of destroy ourselves or whether we will get through

959
01:09:02,320 --> 01:09:06,000
this just incredibly crazy period is whether we have that buffer.

960
01:09:08,000 --> 01:09:14,960
Why, so before we go further object level in this, I think it's very much worth noting that

961
01:09:14,960 --> 01:09:22,400
almost nobody, at least nobody I talk to, thinks about the geopolitical implications of AI. And I

962
01:09:22,400 --> 01:09:26,720
think I have some object level disagreements that we'll get into, but or at least things I want

963
01:09:26,800 --> 01:09:33,120
to iron out. I may not disagree in the end. But the basic premise that obviously if you

964
01:09:33,120 --> 01:09:37,920
keep scaling and obviously if people realize that this is where intelligence is headed,

965
01:09:37,920 --> 01:09:44,000
it's not just going to be like the same world where like what model are we deploying tomorrow

966
01:09:44,000 --> 01:09:49,200
and what is the latest like people on Twitter like, oh, there are the GPT-4O is going to

967
01:09:49,200 --> 01:09:53,120
shake your expectations or whatever. You know, COVID is really interesting because

968
01:09:53,360 --> 01:10:02,720
before a year or something, when March 2020 hit, it became clear to the world like president,

969
01:10:02,720 --> 01:10:07,360
CEO, media, average person, there's other things happening in the world right now. But the main

970
01:10:07,360 --> 01:10:13,760
thing we as a world are dealing with right now is COVID. Soon on AGI. Yeah. Okay. And then so this

971
01:10:13,760 --> 01:10:17,520
is the quiet period. You know, if you want to go on vacation, you know, you want to like, you want

972
01:10:18,480 --> 01:10:24,320
maybe like now is the last time you can have some kids. You know, my girlfriend sometimes

973
01:10:24,320 --> 01:10:29,200
complains that, you know, when I'm like, you know, off doing work or whatever, she's like,

974
01:10:29,200 --> 01:10:33,200
I'm not spending time with her. She's like, you know, she threatens to replace me with like,

975
01:10:33,200 --> 01:10:37,120
you know, GPT-6 or whatever. And I'm like, you know, GPT-6 will also be too busy doing AI research.

976
01:10:39,760 --> 01:10:44,400
Okay. Anyway, so what's the answer to the question of why, why aren't other people talking

977
01:10:44,400 --> 01:10:47,280
about being national security? I made this mistake with COVID, right? So I, you know,

978
01:10:47,280 --> 01:10:52,800
February of 2020, and I, you know, I thought just it was going to sweep the world and all the hospitals

979
01:10:52,800 --> 01:10:57,040
would collapse and it would be crazy. And then, and then, you know, and then it'd be over. And you

980
01:10:57,040 --> 01:10:59,680
know, a lot of people thought this kind of the beginning of COVID, they shut down their offices

981
01:10:59,680 --> 01:11:03,680
a month or whatever. I think the thing I just really didn't price in was the sidal reaction,

982
01:11:03,680 --> 01:11:09,760
right? And, and within weeks, you know, Congress spent over 10% of GDP on like COVID measures,

983
01:11:09,760 --> 01:11:15,520
right? The entire country was shut down. It was crazy. And so I don't know, I didn't price it in

984
01:11:15,520 --> 01:11:20,640
with COVID sufficiently. I don't know, why do people underrate it? I mean, I think there's,

985
01:11:20,640 --> 01:11:24,800
there's a, there's a sort of way in which being kind of in the trenches actually kind of, I think

986
01:11:26,720 --> 01:11:29,680
gives you a less clear picture of the trend lines. You actually have to zoom out that much

987
01:11:29,680 --> 01:11:34,080
only like a few years, right? But you know, you're in the trenches, you're like trying to get the

988
01:11:34,080 --> 01:11:37,360
next model to work, you know, there's always something that's hard, you know, for example,

989
01:11:37,360 --> 01:11:40,880
you might underrate algorithmic progress because you're like, ah, things are hard right now or,

990
01:11:40,880 --> 01:11:44,000
you know, data wall or whatever. But, you know, you zoom out just a few years and you actually

991
01:11:44,000 --> 01:11:47,600
try to like count up how much algorithmic progress made in the last, you know, last few years. And

992
01:11:47,600 --> 01:11:53,120
it's, it's enormous. But I also just don't think people think about this stuff. Like I think smart

993
01:11:53,120 --> 01:11:58,160
people really underrate espionage, right? And, you know, I think part of the security issue is I

994
01:11:58,160 --> 01:12:02,560
think people don't realize like how intense state level espionage can be, right? Like, you know,

995
01:12:02,640 --> 01:12:06,800
you know, this is really company had had software that could just zero click hack any

996
01:12:06,800 --> 01:12:10,160
iPhone, right? They just put in your number and then it's just like straight download of

997
01:12:10,160 --> 01:12:14,560
everything, right? Like the United States infiltrated an air gap, atomic weapons program,

998
01:12:14,560 --> 01:12:19,840
right? Wild, you know, like, yeah, you know, the, you know, you know, intelligence agencies have

999
01:12:19,840 --> 01:12:24,720
just stockpiles of zero days, you know, when things get really hot, you know, I don't know,

1000
01:12:24,720 --> 01:12:28,080
maybe we'll send special forces, right? To like, you know, get go to the data center or something

1001
01:12:28,080 --> 01:12:32,400
that's, you know, or, you know, I mean, China does this, they threaten people's families, right?

1002
01:12:32,400 --> 01:12:34,880
And they're like, look, if you don't cooperate, if you don't give us the Intel,

1003
01:12:37,360 --> 01:12:41,120
there's a there's a good book, you know, along the lines of the gulag develop, you know, the

1004
01:12:41,120 --> 01:12:47,280
inside the aquarium, which is by a Soviet GRU de facto, GRU was like military intelligence,

1005
01:12:47,280 --> 01:12:54,720
Ilya recommended this book to me. And, you know, I think reading that is just kind of like shocked

1006
01:12:54,800 --> 01:12:58,480
at how intense sort of state level espionage is the whole book was about like, they go to these

1007
01:12:58,480 --> 01:13:02,080
European countries, and they try to like get all the technology and recruit all these people to get

1008
01:13:02,080 --> 01:13:07,840
the technology. I mean, yeah, maybe one anecdote, you know, so when so the spot, you know, this

1009
01:13:07,840 --> 01:13:12,480
eventual defector, you know, so he's being trained, he goes to the kind of GRU spy academy. And so

1010
01:13:12,480 --> 01:13:16,560
then to graduate from the spy academy, sort of before you're sent abroad, you kind of had to

1011
01:13:16,560 --> 01:13:22,480
pass a test to show that you can do this. And the test was, you know, you had to in Moscow recruit

1012
01:13:22,560 --> 01:13:26,400
a Soviet scientist and recruit them to give you information sort of like you would do in the

1013
01:13:26,400 --> 01:13:33,600
foreign country. But of course, for whomever you recruited, the penalty for giving away sort of

1014
01:13:33,600 --> 01:13:40,000
secret information was death. And so to graduate from the Soviet spy, the GRU spy academy, you had

1015
01:13:40,000 --> 01:13:47,280
to condemn a country man to death. States do this stuff. I started reading the book on

1016
01:13:47,360 --> 01:13:51,680
because I saw it in the series. Yeah. And I was actually wondering the fact that you use this

1017
01:13:51,680 --> 01:13:57,280
anecdote. Yeah. And then you're like enough of a book recommended by Ilya. Is this some sort of

1018
01:13:58,080 --> 01:14:03,600
is this some sort of Easter egg? We'll leave that for an exercise for the reader.

1019
01:14:05,360 --> 01:14:08,160
Okay, so the beatings will continue until them are all improved.

1020
01:14:11,360 --> 01:14:15,840
So suppose that we live in the world in which these secrets are locked down,

1021
01:14:16,400 --> 01:14:21,040
but China still realizes that this progress is happening in America.

1022
01:14:23,040 --> 01:14:27,920
In that world, especially if they realize, and I guess it's a very interesting open question,

1023
01:14:27,920 --> 01:14:31,440
it probably won't be locked down. Okay, but we're probably gonna live in the bad world.

1024
01:14:31,440 --> 01:14:36,400
Yeah, it's gonna be really bad. Why are you so confident that they won't be locked down? I mean,

1025
01:14:36,400 --> 01:14:39,520
I'm not confident that won't be locked down, but I think it's just it's not happening.

1026
01:14:40,480 --> 01:14:47,520
And so tomorrow, the lab leaders get the message. How hard like, what do they have to do?

1027
01:14:47,520 --> 01:14:51,600
They get the more security guards, they like air gap the, what do they do?

1028
01:14:51,600 --> 01:14:56,480
So again, I think basically it's, I think people, there's kind of like two two reactions there,

1029
01:14:56,480 --> 01:15:03,440
which is like, it's, we're already secure, not. And there's, fatalism, it's impossible.

1030
01:15:03,440 --> 01:15:06,160
I think the thing you need to do is you kind of got to stay ahead of the curve of basically

1031
01:15:06,160 --> 01:15:10,400
how EGI pillows the CCP, right? So like right now, you've got to be resistant to kind of like

1032
01:15:10,400 --> 01:15:15,760
normal economic espionage. They're not, right? I mean, I probably wouldn't be talking about the

1033
01:15:15,760 --> 01:15:19,360
stuff that the labs were, right? Because I wouldn't want to wake them up more, the CCP,

1034
01:15:19,360 --> 01:15:23,440
but they're not, you know, this is like, this stuff is like really trivial for them to do right

1035
01:15:23,440 --> 01:15:27,440
now. I mean, it's also anyway, so they're not resistant to that. I think it would be possible

1036
01:15:27,440 --> 01:15:31,440
for private company to be resistant to it, right? So, you know, both of us have, you know, friends

1037
01:15:31,440 --> 01:15:34,880
in the kind of like quantitative trading world, right? And, you know, I think actually those

1038
01:15:34,880 --> 01:15:39,440
secrets are shaped kind of similarly where it's like, you know, you know, they've said, you know,

1039
01:15:39,440 --> 01:15:42,960
yeah, if I got on a call for an hour or with somebody from a competitive firm, I could,

1040
01:15:42,960 --> 01:15:47,440
most of our alpha would be gone. And that's sort of like, that's the like list of details of like

1041
01:15:47,440 --> 01:15:50,880
really how to, how to make, you're gonna have to worry about that pretty soon. You're gonna have to

1042
01:15:50,880 --> 01:15:55,360
worry about that pretty soon. Yeah. Well, anyway, and so, so all alpha could be gone. But in fact,

1043
01:15:55,360 --> 01:15:59,360
their alpha persists, right? And, you know, often, often for many years and decades. And so this

1044
01:15:59,360 --> 01:16:02,640
doesn't seem to happen. And so I think there's like, you know, I think there's a lot you could go

1045
01:16:02,720 --> 01:16:06,000
if you went from kind of current startup security, you know, you just got to look through the window

1046
01:16:06,000 --> 01:16:10,800
and you can look at the slides. You know, it's kind of like, you know, you know, good private

1047
01:16:10,800 --> 01:16:14,160
sector security hedge funds, you know, the way Google treats, you know, customer data or whatever.

1048
01:16:16,400 --> 01:16:21,040
That'd be good right now. The issue is, you know, basically the CCP will also get more AI

1049
01:16:21,040 --> 01:16:28,000
filled. And at some point, we're going to face kind of the full force of, you know, the Ministry

1050
01:16:28,000 --> 01:16:31,280
of State Security. And again, you're talking about smart people underrating espionage and

1051
01:16:31,280 --> 01:16:34,960
sort of insane capabilities of states. I mean, this stuff is wild, right? You know, they can get

1052
01:16:34,960 --> 01:16:38,080
like, you know, there's papers about, you know, you can find out the location of like where you

1053
01:16:38,080 --> 01:16:41,280
are on a video game map, just from sounds, right? Like states can do a lot with like

1054
01:16:41,280 --> 01:16:46,000
electromagnetic emanations, you know, like, you know, at some point, like you got to be working

1055
01:16:46,000 --> 01:16:49,440
from a sketch, like your cluster needs to be air gapped and basically be a military base. It's

1056
01:16:49,440 --> 01:16:52,640
like, you know, you need to have, you know, intense kind of security clearance procedures

1057
01:16:52,640 --> 01:16:56,720
for employees, you know, they have to be like, you know, all their shit is monitored, you know,

1058
01:16:56,720 --> 01:17:01,280
they're, you know, they basically have security guards, you know, it's, you know, you can't use

1059
01:17:01,280 --> 01:17:05,040
any kind of like, you know, other dependencies, it's all got to be like intensely vetted, you know,

1060
01:17:05,040 --> 01:17:12,000
all your hardware has to be intensely vetted. And, you know, I think basically, if they actually

1061
01:17:12,000 --> 01:17:15,760
really face the full force of state level espionage, I don't really think this is the thing private

1062
01:17:15,760 --> 01:17:18,800
companies can do both. I mean, empirically, right, like, you know, Microsoft recently had

1063
01:17:18,800 --> 01:17:22,160
executives emails hacked by Russian hackers, and, you know, government emails, they've hosted

1064
01:17:22,160 --> 01:17:26,880
hacked by government actors. But also, you know, it's basically there's just a lot of stuff that

1065
01:17:26,880 --> 01:17:29,920
only kind of, you know, the people behind the security curtains know and only they deal with.

1066
01:17:31,360 --> 01:17:35,440
And so, you know, I think it's actually kind of resist the sort of full force of espionage,

1067
01:17:35,440 --> 01:17:38,480
you're going to need the government. Anyway, so I think basically we could, we could do it by

1068
01:17:38,480 --> 01:17:42,960
always being ahead of the curve. I think we're just going to always be behind the curve. And I

1069
01:17:42,960 --> 01:17:47,760
think, you know, maybe unless we get the sort of government project. Okay, so going back to the

1070
01:17:47,760 --> 01:17:52,960
naive perspective of we're very much coming at this from there's going to be a race in the CCP,

1071
01:17:52,960 --> 01:17:58,320
we must win. And listen, I understand like bad people are in charge of the Chinese government,

1072
01:17:58,320 --> 01:18:04,800
like the CCP and everything. But just stepping back in a sort of galactic perspective, humanity

1073
01:18:04,800 --> 01:18:10,560
is developing AGI. And do we want to come at this from the perspective of we need to feed China to

1074
01:18:10,560 --> 01:18:16,320
this, our super intelligent Jupiter brain descendants will know who China like China will

1075
01:18:16,400 --> 01:18:21,680
be something like distant memory that they have America to. So shouldn't it be a more the initial

1076
01:18:21,680 --> 01:18:27,120
approach just come to them like, listen, we this is super intelligence. This is something like

1077
01:18:27,120 --> 01:18:34,720
we come from a cooperative perspective. Why why immediately sort of rush into it from a hawkish

1078
01:18:34,720 --> 01:18:37,760
competitive perspective. I mean, look, I mean, one thing I want to say is like a lot of the

1079
01:18:37,760 --> 01:18:42,480
stuff I talk about in the series is, you know, is sort of primarily, you know, descriptive,

1080
01:18:42,560 --> 01:18:46,480
right. And so I think that on the China stuff, it's like, you know, yeah, and some ideal world,

1081
01:18:46,480 --> 01:18:50,720
you know, we, we, you know, it's just all, you know, merry go around and cooperation. But again,

1082
01:18:50,720 --> 01:18:56,320
it's sort of, I think, I think people wake up to AGI. I think the the issue particular on sort of

1083
01:18:56,320 --> 01:18:59,920
like, can we make a deal? Can we make an international treaty? I think it really relates to sort of

1084
01:18:59,920 --> 01:19:04,800
what is the stability of sort of international arms control dreamers, right. And so we did very

1085
01:19:04,800 --> 01:19:10,000
successful arms control on nuclear weapons in the 80s, right. And the reason it was successful

1086
01:19:10,080 --> 01:19:13,040
is because the sort of new equilibrium was stable, right. So you take go down from, you know,

1087
01:19:13,040 --> 01:19:18,320
whatever, 60,000 nukes to 10,000 nukes. You know, when you have 10,000 nukes, you know,

1088
01:19:18,320 --> 01:19:22,160
basically breakout, breakout doesn't matter that much, right? Suppose the other guy now try to

1089
01:19:22,160 --> 01:19:25,360
make 20,000 nukes. Well, it's like, who cares, right? You know, like, it's still mutually

1090
01:19:25,360 --> 01:19:29,120
assured destruction. Suppose a rogue state kind of went from zero nukes to one nukes. It's like,

1091
01:19:29,120 --> 01:19:32,160
who cares, we still have way more nukes than you. I mean, it's still not ideal for destabilization,

1092
01:19:32,160 --> 01:19:36,400
but it's, you know, it'd be very different if the arms control agreement had been zero nukes,

1093
01:19:36,400 --> 01:19:40,080
right? Because if it had been zero nukes, then it's just like one rogue state makes one nuk.

1094
01:19:40,080 --> 01:19:45,280
The whole thing is destabilized, breakout is very easy. You know, your adversary state

1095
01:19:45,280 --> 01:19:49,120
starts making nukes. And so basically, when, when you're going to sort of like very low levels of

1096
01:19:49,120 --> 01:19:52,560
arms, or when you're going to kind of in your sort of very dynamic technological situation,

1097
01:19:54,080 --> 01:19:57,520
arms control is really tough because, because breakout is easy, you know, there's, there's,

1098
01:19:57,520 --> 01:20:02,080
I mean, there's some other sort of stories about this in sort of like 1920s, 1930s, you know, it's

1099
01:20:02,080 --> 01:20:06,480
like, you know, all the European states had done disarmament and Germany was, was kind of did this

1100
01:20:06,480 --> 01:20:10,640
like crash program to build the Luftwaffe. And that was able to like massively destabilize things,

1101
01:20:10,640 --> 01:20:13,840
because not that, you know, they were the first, they were able to like pretty easily build kind

1102
01:20:13,840 --> 01:20:16,960
of a modern, you know, Air Force, because the others didn't really have one. And that, you know,

1103
01:20:16,960 --> 01:20:21,600
that really destabilized things. And so I think the issue with EGI and super intelligence is the

1104
01:20:21,600 --> 01:20:25,920
explosiveness of it, right? So if you have an intelligence explosion, if you're able to go from

1105
01:20:25,920 --> 01:20:30,240
kind of EGI to super intelligence, if that super intelligence is decisive, like either, you know,

1106
01:20:30,240 --> 01:20:33,760
like a year after, because you've developed some crazy WMD, or because you have some like,

1107
01:20:33,760 --> 01:20:38,240
you know, super hacking ability that lets you, you kind of, you know, completely deactivate the

1108
01:20:38,240 --> 01:20:42,800
sort of enemy arsenal. That means like, suppose, suppose you're trying to like put in a break,

1109
01:20:42,800 --> 01:20:46,720
you know, like we both, we're both going to like cooperate, and we're going to go slower, you know,

1110
01:20:46,720 --> 01:20:50,640
on the cusp of EGI or whatever, you know, it's going to be such an enormous incentive

1111
01:20:50,640 --> 01:20:53,840
to kind of race ahead to break out. And we're just going to do an intelligence explosion. If we

1112
01:20:53,840 --> 01:20:59,200
can get three months ahead, we win. I think that makes it basically, I think any sort of arms

1113
01:20:59,280 --> 01:21:03,760
control agreement that comes as a situation where it's close, very unstable.

1114
01:21:04,400 --> 01:21:11,120
That's really interesting. This is very analogous to kind of a debate I had with Rose on the podcast

1115
01:21:11,120 --> 01:21:16,640
where he argued for nuclear disarmament. But if some country tries to break out and starts

1116
01:21:16,640 --> 01:21:22,320
developing nuclear weapons, the six months or whatever that you would get is enough to get

1117
01:21:22,320 --> 01:21:26,560
international consensus and invade the country and prevent them from getting nukes. And I thought

1118
01:21:26,560 --> 01:21:34,160
that was sort of, that's not a stable clue. But on this, right? So like maybe it's a bit easier

1119
01:21:34,160 --> 01:21:37,920
because you have EGI and so like you can monitor the other person's cluster or something like

1120
01:21:37,920 --> 01:21:41,920
data centers, you can see them from space actually, you can see the energy draw they're

1121
01:21:41,920 --> 01:21:45,280
getting. There's a lot of things as you were saying, there's a lot of ways to get information

1122
01:21:45,280 --> 01:21:51,520
from an environment if you're really dedicated. And also because unlike a nukes, the data centers are

1123
01:21:52,160 --> 01:21:58,240
nukes, you have obviously the submarines, planes, you have bunkers, mountains, whatever, you have

1124
01:21:58,240 --> 01:22:02,080
them in so many different places. A data center that you're 100 gigawatt data center, we can blow

1125
01:22:02,080 --> 01:22:06,320
that shit up if you're like we're concerned, right? Like just some cruise missile or something.

1126
01:22:06,320 --> 01:22:08,080
It's like very vulnerable to sabotage.

1127
01:22:08,080 --> 01:22:11,760
I mean, that gets to the sort of, I mean, that gets to the sort of insane vulnerability of this

1128
01:22:11,760 --> 01:22:15,280
period post superintelligence, right? Because basically, I think so you have the intelligence

1129
01:22:15,280 --> 01:22:18,640
explosion, you have these like vastly superhuman things on your cluster, but you're like, you

1130
01:22:18,640 --> 01:22:21,760
haven't done the industrial explosion yet, you don't have your robots yet, you haven't kind of,

1131
01:22:21,760 --> 01:22:25,440
you haven't covered the desert in like robot factories yet. And that is the sort of crazy

1132
01:22:25,440 --> 01:22:30,080
moment where, you know, say the United States is ahead, the CCP is somewhat behind. There's

1133
01:22:30,080 --> 01:22:33,680
actually an enormous incentive for first strike, right? Because if they can take out your data

1134
01:22:33,680 --> 01:22:38,640
center, they know you're about to have just this command and decisive lead, they know if we can

1135
01:22:38,640 --> 01:22:43,280
just take out this data center, you know, then we can stop it and they might get desperate.

1136
01:22:43,280 --> 01:22:47,760
And, you know, so I think basically we're going to get into a position, it's actually, I think it's

1137
01:22:47,760 --> 01:22:51,520
going to be pretty hard to defend early on. I think we're basically going to be in a position

1138
01:22:51,520 --> 01:22:54,960
where protecting data centers with like the threat of nuclear retaliation. It's like, maybe

1139
01:22:54,960 --> 01:22:58,480
sounds kind of crazy though, you know, and this is the inverse of the LAZR. We got to

1140
01:22:59,200 --> 01:23:04,080
the data centers. Nuclear deterrence for data centers. I mean, this is a, you know, Berlin,

1141
01:23:04,080 --> 01:23:08,320
you know, in the like late fifties, early sixties, both Eisenhower and Kennedy multiple times kind

1142
01:23:08,320 --> 01:23:12,320
of made the threat of full on nuclear war against the Soviets, if they tried to encroach on West

1143
01:23:12,320 --> 01:23:16,560
Berlin, sort of insane. It's kind of insane that that went well. But basically, I think that's

1144
01:23:16,560 --> 01:23:20,000
going to be the only option for the data centers. It's a terrible option. This whole scheme is

1145
01:23:20,000 --> 01:23:24,480
terrible, right? Like being, being, being in this like neck and neck race, sort of at this

1146
01:23:24,480 --> 01:23:28,960
point is terrible. And, you know, it's also, you know, I think I have some uncertainty basically

1147
01:23:28,960 --> 01:23:32,080
on how easy that decisive advantage will be. I'm pretty confident that if you have super

1148
01:23:32,080 --> 01:23:35,200
intelligence, you have two years, you have the robots, you're able to get that 30 year lead.

1149
01:23:35,840 --> 01:23:39,280
Look, then you're in this like go for one situation. You have your like, you know,

1150
01:23:39,280 --> 01:23:42,640
millions or billions of like mosquito size drones that can just take it out. I think there's even

1151
01:23:42,640 --> 01:23:45,920
a possibility you can kind of get a decisive advantage earlier. So, you know, there's these

1152
01:23:45,920 --> 01:23:49,600
stores, you know, about these as well about, you know, like colonization and like the sort of

1153
01:23:49,600 --> 01:23:54,720
1500s where it was, you know, these like a few hundred kind of spanyards were able to like topple

1154
01:23:54,720 --> 01:23:58,240
the Aztec empire, you know, a couple, I think a couple other empires as well, you know, each of

1155
01:23:58,240 --> 01:24:02,000
these had a few million people. And it was not like God like technological advantage. It was some

1156
01:24:02,000 --> 01:24:05,520
technological advantage. It was, I mean, it was some amount of disease. And then it was kind of

1157
01:24:05,520 --> 01:24:10,400
like cunning strategic play. And so I think there's a, there's a possibility that even sort of early

1158
01:24:10,400 --> 01:24:14,080
on, you know, it's you haven't gone through the full industrial explosion yet, we have super

1159
01:24:14,080 --> 01:24:17,920
intelligence, but you know, you're able to kind of like manipulate the imposing generals, claim

1160
01:24:17,920 --> 01:24:21,760
your ally with them, then you have, you have some, you know, you have sort of like some crazy new

1161
01:24:21,760 --> 01:24:25,520
bioweapons, maybe, maybe there's even some way to like pretty easily get a paradigm that like

1162
01:24:25,520 --> 01:24:30,160
deactivates enemy nukes. Anyway, so I think this stuff could get pretty wild. Here's what I think

1163
01:24:30,160 --> 01:24:36,000
we should do. I really don't want this volatile period. And so a deal with China would be nice.

1164
01:24:36,000 --> 01:24:38,320
It's going to be really tough if you're in this unstable equilibrium.

1165
01:24:38,720 --> 01:24:43,840
I think basically we want to get in a position where it is clear that the United States, that a

1166
01:24:43,840 --> 01:24:47,600
sort of coalition of democratic allies will win. It's clear the United States would be clear to

1167
01:24:47,600 --> 01:24:51,360
China, you know, that will require having locked down the secrets that will require having built

1168
01:24:51,360 --> 01:24:54,560
the 100 gigawatt cluster in the United States and having done the natural gas and doing what's

1169
01:24:54,560 --> 01:24:59,280
necessary. And then when it is clear that the democratic coalition is well ahead, then you go

1170
01:24:59,280 --> 01:25:04,240
to China and then you offer them a deal. And you know, China will know they're going to win. This

1171
01:25:04,480 --> 01:25:10,080
is going to be, they're very scared of what's going to happen. We're going to know we're going to

1172
01:25:10,080 --> 01:25:12,800
win, but we're also very scared of what's going to happen because we really want to avoid this

1173
01:25:12,800 --> 01:25:18,560
kind of like breakneck race right at the end and where things could really go awry. And,

1174
01:25:19,840 --> 01:25:22,960
you know, and then, and so then we offer them a deal. I think there's an incentive to come to

1175
01:25:22,960 --> 01:25:26,320
the table. I think there's a sort of more stable arrangement you can do. It's a sort of an Adams

1176
01:25:26,320 --> 01:25:29,680
for peace arrangement. And we're like, look, we're going to respect you. We're not, we're not going

1177
01:25:29,680 --> 01:25:32,720
to like, we're not going to use super intelligence against you. You can do what you want. You're

1178
01:25:32,720 --> 01:25:36,400
going to get your like, you're going to get your slice of the galaxy. We're going to like,

1179
01:25:36,400 --> 01:25:39,520
we're going to benefit share with you. We're going to have some like computer agreement where

1180
01:25:39,520 --> 01:25:42,400
it's like, there's some ratio of compute that you're allowed to have. And that's like enforced

1181
01:25:42,400 --> 01:25:47,680
with your like composing AI's or whatever. And we're just not going to do, we're just not going

1182
01:25:47,680 --> 01:25:52,880
to do this kind of like volatile sort of WMD arms race to the death. We're good. And sort of it's

1183
01:25:52,880 --> 01:25:57,440
like a new world order that's US led, that sort of democratic led, but that respects China. Let's

1184
01:25:57,440 --> 01:26:04,320
then do what they want. Okay. There's so much to, there's so much there. First on the galaxies

1185
01:26:04,320 --> 01:26:07,920
thing, I think it's just a funny anecdote. So I want to kind of want to tell it. And this,

1186
01:26:07,920 --> 01:26:11,040
we were at an event and I'm respecting Chad, I'm house rules here. I'm not revealing anything

1187
01:26:11,040 --> 01:26:16,400
about it, but we're talking to somebody or a Leopold was talking to somebody influential.

1188
01:26:16,400 --> 01:26:23,440
Afterwards, that person asked the group, Leopold told me that he wants, he's not going to spend

1189
01:26:23,440 --> 01:26:29,200
any money on consumption until he's ready to buy galaxies. And he goes, the guy goes,

1190
01:26:29,920 --> 01:26:36,720
I honestly don't know if you meant galaxies, like the brand of private plane galaxy or the physical

1191
01:26:36,720 --> 01:26:41,360
galaxies. And there was an actual debate. Like he went away to the restroom and there was an

1192
01:26:41,360 --> 01:26:48,080
actual debate among people who are very influential about, oh, they can't amend galaxies. And the

1193
01:26:48,080 --> 01:26:52,400
other people who knew you better be like, no, he means galaxies. I mean the galaxy. I mean the

1194
01:26:52,720 --> 01:26:57,200
galaxies. I mean, I think it'd be interesting. I mean, I think there's, I mean, there's two

1195
01:26:57,200 --> 01:27:00,800
ways to buy the galaxies. One is like at some point, you know, it's like post-superintelligence,

1196
01:27:00,800 --> 01:27:04,960
you know, they're so crazy. But by the way, I love, okay, so what happens is he's on the ground,

1197
01:27:04,960 --> 01:27:08,480
I'm laughing my ass off. I'm not even saying I think people were like, having this debate.

1198
01:27:08,480 --> 01:27:14,400
And then Leopold comes back and the guy, somebody's like, oh, Leopold, we're having this debate

1199
01:27:14,400 --> 01:27:20,640
about whether you meant you want to buy the galaxy or you want to buy the other thing.

1200
01:27:20,640 --> 01:27:26,240
And Leopold assumes they must mean not the Friday play in the galaxy versus the actual galaxy.

1201
01:27:26,240 --> 01:27:30,480
But do you want to buy the property rights with the galaxy or do you just send out the probes right

1202
01:27:30,480 --> 01:27:44,080
now? Oh my god. All right. Back to China. There's a whole bunch of things that I could ask about

1203
01:27:44,800 --> 01:27:48,560
that plan about whether you're going to get credible, promised. You will get some part of

1204
01:27:48,640 --> 01:27:51,760
galaxies, whether they care about that. I mean, you have AIs to help you enforce stuff.

1205
01:27:51,760 --> 01:27:56,000
Okay, sure. We'll leave that aside. That's a different rabbit hole. The thing I want to ask is,

1206
01:27:56,560 --> 01:28:00,880
but it has to be the thing we need. The only way this is possible is if we lock it down.

1207
01:28:00,880 --> 01:28:03,760
I see. If we don't lock it down, we are in this fever struggle.

1208
01:28:05,040 --> 01:28:12,160
Greatest peril mankind will have ever seen. So, but given the fact that in during this period,

1209
01:28:12,160 --> 01:28:16,960
instead of just taking their chances and they don't really understand how this AI governance scheme

1210
01:28:16,960 --> 01:28:19,680
is going to work, whether they're going to check, whether we had to actually get the galaxies,

1211
01:28:20,880 --> 01:28:24,000
the data centers, they can't be built underground. They have to be built above ground.

1212
01:28:24,000 --> 01:28:28,400
Taiwan is right off the coast of us. They need the chips from there. Why aren't we just going to

1213
01:28:28,400 --> 01:28:32,480
invade? Listen, we don't want like worst case scenario is they win the super intelligence,

1214
01:28:32,480 --> 01:28:37,920
which they're on track to do anyways. Wouldn't this instigate them to either invade Taiwan or blow

1215
01:28:37,920 --> 01:28:41,680
up the data center in Arizona or something like that? Yeah. I mean, look, I mean, talked about

1216
01:28:41,680 --> 01:28:45,040
the data center one and then you probably have to like threaten nuclear retaliation to protect

1217
01:28:45,120 --> 01:28:48,640
that. They might also just blow it up. There's also maybe ways they can do it without sort of

1218
01:28:48,640 --> 01:28:52,800
attribution, right? Like you. Stuxnet. Stuxnet. Stuxnet. Yeah. I mean, this is part of, we'll

1219
01:28:52,800 --> 01:28:56,240
talk about this later, but you know, I think, look, I think we need to be working on the Stuxnet

1220
01:28:56,240 --> 01:29:01,760
for the Chinese project, but the. But by the way, for the audience. I mean, Taiwan, the Taiwan

1221
01:29:01,760 --> 01:29:08,160
thing, the, you know, I talk about, you know, AGI about, you know, 27 or whatever. Do you know

1222
01:29:08,160 --> 01:29:13,520
about the like terrible 20s? No. Okay. Well, I mean, sort of in this sort of Taiwan watcher circles,

1223
01:29:13,520 --> 01:29:17,360
people often talk about like the late 2020s as like maximum period of risk for Taiwan,

1224
01:29:17,360 --> 01:29:20,640
because it's sort of like, you know, military modernization cycles and basically extreme

1225
01:29:20,640 --> 01:29:24,160
fiscal tightening on the military budget in the United States over the last decade or two

1226
01:29:24,960 --> 01:29:29,120
has meant that sort of we're in this kind of like, you know, trough in, in, in the late 20s of like,

1227
01:29:29,120 --> 01:29:32,400
you know, basically overall naval capacity. And, you know, that's sort of when China is saying

1228
01:29:32,400 --> 01:29:35,760
they want to be ready. So it's already kind of like, it's kind of pitching, you know,

1229
01:29:35,760 --> 01:29:39,600
there's some sort of like, you know, parallel timeline there. Yeah. Look, it looks appealing

1230
01:29:39,600 --> 01:29:43,120
to invade Taiwan. I mean, maybe not because they, you know, basically remote cutoff of

1231
01:29:43,120 --> 01:29:48,800
the chips. And so then it doesn't mean they get the chips, but it just means they, they,

1232
01:29:49,760 --> 01:29:54,160
you know, it's just, it's, you know, the machines are deactivated. But look, I mean, imagine if

1233
01:29:54,160 --> 01:29:58,640
during the Cold War, you know, all of the world's uranium deposits had been in Berlin,

1234
01:29:58,640 --> 01:30:02,720
you know, and Berlin was already, I mean, almost multiple times it was caused a nuclear war.

1235
01:30:02,720 --> 01:30:09,600
So God help us all. Well, the Groves had a plan after the after the war, that the plan was that

1236
01:30:09,680 --> 01:30:14,320
America would go around the world and getting the rights to every single uranium deposit,

1237
01:30:14,320 --> 01:30:16,800
because they didn't realize how much uranium there was in the world. And they thought this

1238
01:30:16,800 --> 01:30:20,160
was the thing that was feasible. Not realizing, of course, that there's like huge deposits in

1239
01:30:20,160 --> 01:30:25,760
the Soviet Union itself. Right. Okay. East Germany too. There's a, there's always,

1240
01:30:25,760 --> 01:30:29,040
there's a lot of East German workers who kind of got screwed and got cancer.

1241
01:30:30,880 --> 01:30:35,680
Okay. So the framing we've been talking about that we've been assuming, and I'm not sure I buy

1242
01:30:35,680 --> 01:30:41,680
yet, is that the United States, this is our leverage, this is our data center,

1243
01:30:41,680 --> 01:30:45,680
the China is the competitor. Right now, obviously, that's not the way things are progressing.

1244
01:30:45,680 --> 01:30:49,280
Private companies control these AIs. They're deploying them. It's a market-based thing.

1245
01:30:51,840 --> 01:30:56,000
Why will it be the case that it's like the United States, it has this leverage,

1246
01:30:56,000 --> 01:30:58,400
or is doing this thing versus China is doing this thing?

1247
01:30:59,920 --> 01:31:03,520
Yeah. I mean, look, look on the, on the project, you know, I mean, there's sort of descriptive

1248
01:31:03,600 --> 01:31:06,960
and prescriptive claims or sort of normative positive claims. I think the main thing I'm trying

1249
01:31:06,960 --> 01:31:10,720
to say is, you know, you know, look, we're at, we're at these SF parties or whatever.

1250
01:31:10,720 --> 01:31:14,560
And I think people talk about AGI and they're always just talking about the private AI labs.

1251
01:31:14,560 --> 01:31:17,280
And I think I just really want to challenge that assumption. It just seems like,

1252
01:31:18,080 --> 01:31:21,840
seems pretty likely to me, you know, as we've talked about, for reasons we've talked about,

1253
01:31:21,840 --> 01:31:25,360
that look like the national security state is going to get involved.

1254
01:31:25,360 --> 01:31:28,800
And, you know, I think there's a lot of ways this could look like, right? Is it,

1255
01:31:28,800 --> 01:31:32,400
is it like nationalization? Is it a public-private partnership? Is it a kind of defense

1256
01:31:32,400 --> 01:31:36,080
contra-elect or like a relationship? Is it a sort of government project that soaks up all the people?

1257
01:31:37,360 --> 01:31:44,080
And so there's a spectrum there. But I think people are just vastly underrating the chances of

1258
01:31:44,080 --> 01:31:48,080
this more or less looking like a government project. And look, I mean, look, if, if,

1259
01:31:49,520 --> 01:31:52,400
you know, it's sort of like, you know, do you think, do you think like we all have literal,

1260
01:31:52,400 --> 01:31:55,520
like, you know, when we have like literal superintelligence on our cluster, right? And it's

1261
01:31:55,520 --> 01:31:59,040
like, you know, you have 100 billion, they're like, sorry, you have a billion like superintelligence

1262
01:31:59,040 --> 01:32:02,880
scientists that they can like hack everything. They can like stuxnet the Chinese data centers,

1263
01:32:02,880 --> 01:32:05,520
you know, they're starting to build the robo armies, you know, you like, you really think

1264
01:32:05,520 --> 01:32:09,520
they'll be like a private company and the government would be like, oh my God, what is going on? You

1265
01:32:09,520 --> 01:32:16,080
know, like, yeah. Suppose there's no China. Suppose there's people like Iran, North Korea,

1266
01:32:16,080 --> 01:32:20,080
who theoretically at some point will be able to do superintelligence, but they're not on our heels

1267
01:32:20,080 --> 01:32:24,480
and don't have the ability to be on our heels. In that world, are you advocating for the national

1268
01:32:24,480 --> 01:32:30,160
project or do you prefer the private path forward? Yeah. So I mean, two responses to this. One is,

1269
01:32:30,160 --> 01:32:34,160
I mean, you still have like Russia, you still have these other countries, you know, you've got to have

1270
01:32:34,160 --> 01:32:38,560
Russia proof security, right? It's like, you can't, you can't just have Russia steal all your stuff.

1271
01:32:38,560 --> 01:32:41,520
And like, maybe their clusters aren't going to be as big, but like, they're still going to be able

1272
01:32:41,520 --> 01:32:46,160
to make the crazy bio weapons and the, you know, the mosquito-sized drones, you know, and so on.

1273
01:32:46,160 --> 01:32:51,840
And so, I mean, I think, I think, I think the security component is just actually a pretty

1274
01:32:51,920 --> 01:32:56,320
large component of the project in the sense of like, I currently do not see another way

1275
01:32:56,320 --> 01:33:00,880
where we don't kind of like instantly proliferate this to everybody. And so, yeah. So I think it's

1276
01:33:00,880 --> 01:33:03,920
sort of like, you still have to deal with Russia, you know, Iran, North Korea, and you know, like,

1277
01:33:03,920 --> 01:33:06,960
you know, Saudi and Iran are going to be trying to get it because they want to screw each other,

1278
01:33:06,960 --> 01:33:09,520
and you know, Pakistan and India, because they want to screw each other. There's like this enormous

1279
01:33:09,520 --> 01:33:14,000
destabilization still. That said, look, I agree with you. If, if, you know, if, you know, by some,

1280
01:33:14,000 --> 01:33:18,000
somehow things are shaking out differently, and like, you know, AGI would have been in 2005,

1281
01:33:18,960 --> 01:33:23,200
you know, sort of like unparalleled, you know, American hegemony. I think there would have

1282
01:33:23,200 --> 01:33:28,640
been more scope for less government involvement. But again, you know, as we're talking about

1283
01:33:28,640 --> 01:33:32,000
earlier, I think that would have been sort of this like very unique moment in history. And I

1284
01:33:32,000 --> 01:33:35,280
think basically, you know, almost all other moments in history, there would have been the

1285
01:33:35,280 --> 01:33:42,640
sort of great power competitor. So, okay, so let's get into this debate. So I, my position here

1286
01:33:42,640 --> 01:33:47,360
is if you look at the people who are involved in the Manhattan Project itself, many of them

1287
01:33:47,360 --> 01:33:52,320
regretted their participation, as you said. Now we can infer from that that we should sort of

1288
01:33:53,040 --> 01:34:00,320
start off with a cautious approach to the nationalized ASI project. Then you might say, well,

1289
01:34:00,320 --> 01:34:04,720
listen, obviously they're super. Did they regret their participation because of the project or

1290
01:34:04,720 --> 01:34:08,720
because of the technology itself? I think people will regret it. But I think it's, it's, it's about

1291
01:34:08,720 --> 01:34:13,760
the nature of the technology, and it's not about project. I think they also probably had a sense

1292
01:34:13,760 --> 01:34:18,160
that different decisions would have been made if it wasn't some concerted effort that everybody

1293
01:34:18,160 --> 01:34:23,360
had agreed to participate in. That if it wasn't in the context of this, we need to race to beat

1294
01:34:23,360 --> 01:34:28,080
Germany and Japan, you might not develop, so that's a technology part, but also like you wouldn't

1295
01:34:28,080 --> 01:34:32,160
actually like hit them. It's like the sort of, the destructive potential, the sort of,

1296
01:34:32,160 --> 01:34:35,760
you know, military potential, it's not, it's not because of the project, it is because of the

1297
01:34:35,760 --> 01:34:42,080
technology. And that will unfold regardless. You know, I think this underrates the power of

1298
01:34:42,960 --> 01:34:47,200
Imagine you go through like the 20th century in like, you know, a decade. You know, it's just

1299
01:34:47,200 --> 01:34:51,360
the sort of, the sort of, yes, great technological progress. Let's just actually run to that example.

1300
01:34:51,360 --> 01:34:54,880
So suppose you actually, there was some reason that the 20th century would be run through in one

1301
01:34:54,880 --> 01:35:00,560
decade. Do you think the cause of that should have been, should have been like the technologies

1302
01:35:00,560 --> 01:35:04,560
that happened through the 20th century shouldn't have been privatized? That it should have been a

1303
01:35:04,560 --> 01:35:12,480
more sort of concerted government led project. You know, look, there is a history of just dual

1304
01:35:12,480 --> 01:35:16,640
use technologies, right? And so I think AI in some sense is going to be dual use in the same way.

1305
01:35:16,640 --> 01:35:20,400
And so there's going to be lots of civilian uses of it, right? Like nuclear energy, it's like

1306
01:35:20,400 --> 01:35:23,040
itself, right? It was like, you know, there's the government project developed the military

1307
01:35:23,040 --> 01:35:26,400
angle of it. And then, you know, it was like, you know, then the government worked with private

1308
01:35:26,400 --> 01:35:29,440
companies, there's a sort of like real like flourishing of nuclear energy until you know,

1309
01:35:29,440 --> 01:35:35,200
the environmentalists stopped it. You know, planes, right? Like Boeing, right? Actually,

1310
01:35:35,200 --> 01:35:38,720
you know, the Manhattan project wasn't the biggest defense R&D project during World War II. It was

1311
01:35:38,720 --> 01:35:42,320
the B-29 bomber, right? Because they needed the bomber that had long enough range to reach Japan

1312
01:35:43,360 --> 01:35:47,360
to destroy their cities. And then, you know, Boeing made some Boeing made that,

1313
01:35:47,360 --> 01:35:51,600
B Boeing made the B-47, made the B-52, you know, the plane the US military uses today.

1314
01:35:51,600 --> 01:35:57,280
And then they use that technology later on to, you know, build the 707 and the sort of the

1315
01:35:58,000 --> 01:36:04,240
later on mean this context because in the other, like I get what it means after a war to privatize,

1316
01:36:04,240 --> 01:36:09,520
but if you have the government has ASI, maybe just let me back up and explain my concern.

1317
01:36:09,520 --> 01:36:13,760
So you have the only institution in our society, which has a monopoly on violence.

1318
01:36:15,040 --> 01:36:20,160
And then we're going to give the, give it some, in a way that's not broadly deployed,

1319
01:36:20,160 --> 01:36:25,200
access to the ASI, the counterfactual, and this may be sound silly, but listen,

1320
01:36:25,200 --> 01:36:30,160
we're going to go through higher and higher levels of intelligence. Private companies will

1321
01:36:30,160 --> 01:36:35,280
be required by regulation to increase their security, but they'll still be private companies

1322
01:36:35,280 --> 01:36:39,040
and they're deployed this and they're going to release the AGI, now McDonald's and JP

1323
01:36:39,040 --> 01:36:42,320
Morgan and some random startup are now more effective organizations because they have a bunch

1324
01:36:42,320 --> 01:36:47,040
of AGI workers. And it'll be sort of like the industrial revolution in the sense that the

1325
01:36:47,040 --> 01:36:52,160
benefits were widely diffused. If you don't end up in a situation like that, then the,

1326
01:36:53,120 --> 01:36:57,280
I mean, even backing up, like, what is it we're trying to, why do we want to win against China?

1327
01:36:57,280 --> 01:37:02,560
We want to win against China because we don't want a top down authoritarian system to win.

1328
01:37:03,120 --> 01:37:09,200
Now, if the way to beat that is that the most important technology that humanity will have

1329
01:37:09,200 --> 01:37:15,840
has to be controlled by a top down government, like, what was the point? Like, maybe, so let's

1330
01:37:15,840 --> 01:37:19,520
like run our cards with privatization. That's the way we get to the classic liberal

1331
01:37:19,520 --> 01:37:23,920
market based system we want for the ESIs. Yeah. All right. So a lot of talk about here. Yeah.

1332
01:37:23,920 --> 01:37:27,040
I think, yeah, maybe I'll start a bit about like actually looking at what the private world would

1333
01:37:27,040 --> 01:37:30,640
look like. And I think this is part of where the sort of there's no alternative comes from.

1334
01:37:30,640 --> 01:37:33,920
And then let's look like, look at like what the government project looks like, what checks and

1335
01:37:33,920 --> 01:37:38,880
balances look like and so on. All right. Private world. And first of all, okay, so right, like a

1336
01:37:38,880 --> 01:37:41,680
lot of people right now talk about open source. And I think there's this sort of misconception

1337
01:37:41,680 --> 01:37:45,440
that like AGI development is going to be like, Oh, it's going to be some like beautiful decentralized

1338
01:37:45,440 --> 01:37:48,880
thing. And you know, like, you know, some giddy community of coders who gets to like, you know,

1339
01:37:48,880 --> 01:37:52,320
collaborate on it. That's not how it's going to look like, right? You know, it's, you know,

1340
01:37:52,320 --> 01:37:55,200
$100 billion trillion dollar cluster. It's not going to be that many people that have it. The

1341
01:37:55,200 --> 01:37:58,480
algorithms, you know, it's like right now, open source is kind of good, because people just use

1342
01:37:58,480 --> 01:38:01,760
the stuff that was published. And so they basically, you know, the algorithms were published, or, you

1343
01:38:01,760 --> 01:38:05,120
know, as Mistral, they just kind of like leave deep mind and take all the secrets with them. And

1344
01:38:05,120 --> 01:38:10,800
they just kind of replicate it. That's not going to continue being the case. And so, you know,

1345
01:38:10,800 --> 01:38:14,720
the sort of like open source, I mean, also people say stuff like, you know, 1026 flops,

1346
01:38:14,720 --> 01:38:18,080
it will be in my phone, you know, it's no, it won't, you know, it's like Moore's Law is really

1347
01:38:18,080 --> 01:38:21,600
slow. I mean, AI chips are getting better. But like, you know, the $100 billion computer will

1348
01:38:21,600 --> 01:38:26,480
not cost, you know, like $1,000, you know, within your lifetime or whatever, aside from me. So anyway,

1349
01:38:26,480 --> 01:38:32,720
so it's going to be, it's going to be like two or three, you know, big players on the private world.

1350
01:38:33,840 --> 01:38:40,400
And so look, a few things. So first of all, you know, you talk about the sort of like, you know,

1351
01:38:40,480 --> 01:38:44,560
enormous power that sort of superintelligence will have and the government will have.

1352
01:38:46,080 --> 01:38:50,400
I think it's pretty plausible that the alternative world is that like one AI company has that power,

1353
01:38:50,400 --> 01:38:52,880
right? And specifically, if we're talking about lead, you know, it's like, what, I don't know,

1354
01:38:52,880 --> 01:38:56,800
open AI has a six month lead. And then, you know, so then you're not talking, you're talking about

1355
01:38:56,800 --> 01:39:01,120
basically, you know, the most powerful weapon ever. And it's, you know, you're kind of making this

1356
01:39:01,120 --> 01:39:05,600
like radical bet on like a private company CEO is the benevolent dictator. No, no, this is not

1357
01:39:05,600 --> 01:39:09,760
necessarily like any other thing that's privatized, we don't account on them being benevolent. We

1358
01:39:09,760 --> 01:39:15,520
just look to think of, for example, somebody who manufactures industrial fertilizer, right?

1359
01:39:15,520 --> 01:39:20,880
This is the person with this factory, if they went back to an ancient civilization, they could

1360
01:39:20,880 --> 01:39:26,320
like blow up Rome, they could probably blow up Washington DC. And I think in their series,

1361
01:39:26,320 --> 01:39:30,800
you talk about Tyler Cowan's phrase of muddling through. And I think even with privatization,

1362
01:39:30,800 --> 01:39:35,120
people sort of underrate that there are actually a lot of private actors who have the ability to

1363
01:39:36,080 --> 01:39:41,200
there's a lot of people who control the water supply or whatever. And we can count on cooperation

1364
01:39:41,200 --> 01:39:45,760
and market based incentives to basically keep a balance of power. I get that things are proceeding

1365
01:39:45,760 --> 01:39:49,200
really fast. But we have a lot of historical evidence that this is the thing that works best.

1366
01:39:49,200 --> 01:39:54,160
So look, I mean, what do we do with nukes, right? The way we keep the sort of nukes in check is not

1367
01:39:54,160 --> 01:39:57,360
like, you know, a sort of beefed up second amendment where like each state has their own

1368
01:39:57,360 --> 01:40:01,520
like little nuclear arsenal and like, you know, Dario and Sam have their own little nuclear arsenal.

1369
01:40:01,520 --> 01:40:09,520
No, no, it's like, it's institutions, it's constitutions, it's laws, it's courts. And so I

1370
01:40:09,520 --> 01:40:12,720
don't actually, I'm not sure that this, you know, I'm not sure that the sort of balance of power

1371
01:40:12,720 --> 01:40:17,040
analogy holds. In fact, you know, sort of the government having the biggest guns was sort of

1372
01:40:17,040 --> 01:40:20,800
like an enormous civilizational achievement, right? Like Landfrieden in the sort of Holy Roman

1373
01:40:20,800 --> 01:40:24,320
Empire, right? You know, if somebody from the town over kind of committed a crime on you,

1374
01:40:24,320 --> 01:40:28,320
you know, you didn't kind of start a sort of a, you know, a big battle between the two towns.

1375
01:40:28,320 --> 01:40:32,640
No, you take it to a court of the Holy Roman Empire and they would decide. And it's a big

1376
01:40:32,640 --> 01:40:36,480
achievement. Now, the thing about, you know, the industrial fertilizer, I think the key difference

1377
01:40:36,480 --> 01:40:40,800
is kind of speed and often Stephen's balance issues, right? So it's like 20th century and,

1378
01:40:40,800 --> 01:40:47,280
you know, 10 years in a few years. That is an incredibly scary period. And it is incredibly

1379
01:40:47,280 --> 01:40:51,200
scary, you know, because it's, you know, you're going through just this sort of enormous array of

1380
01:40:51,200 --> 01:40:55,680
destructive technology and the sort of like enormous amount of like, you know, basically

1381
01:40:55,680 --> 01:40:59,360
military event. I mean, you would have gone from, you know, kind of like, you know, you know,

1382
01:40:59,360 --> 01:41:04,000
bayonets and horses to kind of like tank armies and fighter jets in like a couple years and then

1383
01:41:04,000 --> 01:41:07,760
from, you know, like, you know, and then to like, you know, nukes and, you know, ICBMs and stuff,

1384
01:41:07,760 --> 01:41:14,000
you know, it's just like in a matter of years. And so it is sort of that speed that creates,

1385
01:41:14,000 --> 01:41:16,960
I think basically the way I think about it is there's going to be this initial just incredibly

1386
01:41:16,960 --> 01:41:21,200
volatile and incredibly dangerous period. And somehow we have to make it through that. That's

1387
01:41:21,200 --> 01:41:26,240
going to be incredibly challenging. That's where you need the kind of government project.

1388
01:41:26,240 --> 01:41:30,000
If you can make it through that, then you kind of go to like, now we can, now, you know, the

1389
01:41:30,000 --> 01:41:33,360
situation has been stabilized, you know, we don't face this imminent national security threat.

1390
01:41:33,360 --> 01:41:36,880
You know, it's like, yes, there were kind of WMDs that came along the way, but either we've managed

1391
01:41:36,880 --> 01:41:40,400
to kind of like have a sort of stable offense, defense balance, right? Like I think bioweapons

1392
01:41:40,400 --> 01:41:44,080
initially are a huge issue, right? Like an attacker can just create like a thousand different

1393
01:41:44,080 --> 01:41:47,200
static, you know, viruses and spread them. And it's like going to be really hard for you to kind

1394
01:41:47,200 --> 01:41:50,480
of like make a defense against each, but maybe at some point you figure out the kind of like,

1395
01:41:50,560 --> 01:41:53,840
you know, universal defense against every possible virus. And then you're in a stable

1396
01:41:53,840 --> 01:41:57,280
situation and on the offense, defense balance, or you do the thing, you know, you do with planes

1397
01:41:57,280 --> 01:42:00,160
where it's there's like, you know, there's certain capabilities that the private sector

1398
01:42:00,160 --> 01:42:03,840
isn't allowed to have. And you've like figured out what's going on, restrict those. And then you

1399
01:42:03,840 --> 01:42:07,840
can kind of like let, let, you know, you let this sort of civilian, civilian uses.

1400
01:42:07,840 --> 01:42:14,640
So I'm skeptical of this because well, there's sorry, I mean, the other important thing is,

1401
01:42:14,640 --> 01:42:18,160
so I talked about the sort of, you know, maybe it's like, it's, it's a, you know,

1402
01:42:18,960 --> 01:42:21,920
it's, you know, it's one company with all this power. And I think it's like, I think it is

1403
01:42:21,920 --> 01:42:25,760
unprecedented because it's like the industrial fertilizer guy cannot overthrow the US government.

1404
01:42:25,760 --> 01:42:29,280
I think it's quite plausible that like the AI company with super intelligence can overthrow

1405
01:42:29,280 --> 01:42:32,240
the multiple companies, right? And I buy that one of them could be ahead.

1406
01:42:32,240 --> 01:42:36,080
So it's not obvious that it'll be multiple. I think it's again, if there's like a six monthly,

1407
01:42:36,080 --> 01:42:39,680
maybe, maybe there's two or three, but if there's two or three, then what you have is just like

1408
01:42:39,680 --> 01:42:43,200
the crazy race between these two or three companies, you know, it's like, you know, whatever,

1409
01:42:43,200 --> 01:42:46,400
Demis and Sam, they're just like, I don't want to let the other one win.

1410
01:42:46,400 --> 01:42:50,320
And, and they're both developing their nuclear arsenals in the road. It's just like, also like,

1411
01:42:50,320 --> 01:42:52,800
come on, the government is not going to let these people, you know, are they going to let,

1412
01:42:52,800 --> 01:42:56,080
like, you know, is Dario going to be the one developing the kind of like, you know,

1413
01:42:56,080 --> 01:42:59,680
you know, super hacking Stuxnet and like deploying against the Chinese data center.

1414
01:42:59,680 --> 01:43:03,760
The other issue though, is it won't just, if it's two or three, it won't just be two or three.

1415
01:43:03,760 --> 01:43:07,200
There'll be two or three and it'll be China and Russia and North Korea because the private

1416
01:43:07,200 --> 01:43:11,040
and the private lab world, there's no way they'll have security that is good enough.

1417
01:43:11,040 --> 01:43:15,120
I think we're also assuming that somehow if you nationalize it, like the security just,

1418
01:43:15,760 --> 01:43:20,880
especially in the world, where it did this stuff is priced in by the CCP,

1419
01:43:20,880 --> 01:43:25,520
that now you've like got it nailed down. And I'm not sure why we would expect that to be the case.

1420
01:43:25,520 --> 01:43:27,760
But on this, government's the only one who does this stuff.

1421
01:43:27,760 --> 01:43:32,080
So if it's not Sam or Dario, who's, we don't want to trust them to be benevolent,

1422
01:43:32,080 --> 01:43:40,160
dictator or whatever. So by here, we're counting on, if it's, because you can

1423
01:43:40,160 --> 01:43:43,120
cause a coup, the same capabilities are going to be true of the government project, right?

1424
01:43:43,120 --> 01:43:49,680
And so the modal president in 2020, 2025, but Donald Trump will be the person that you don't

1425
01:43:49,680 --> 01:43:55,280
trust Sam or Dario to have these capabilities. And why, okay, I agree that like I'm worried

1426
01:43:55,280 --> 01:44:00,480
if the Sam or Dario have a one year lead on ASI in that, in that world, then I'm like concerned

1427
01:44:00,480 --> 01:44:05,360
about this being privatized. But in that exact same world, I'm very concerned about Donald

1428
01:44:05,360 --> 01:44:09,120
Trump having the capability. And potentially if we're living in a world where the takeoff

1429
01:44:09,120 --> 01:44:12,960
is slower than you anticipate, in that world, I'm like very much I want the private companies.

1430
01:44:12,960 --> 01:44:17,280
So like in no part of this matrix, is it obviously true that the government

1431
01:44:17,280 --> 01:44:19,920
led project is better than the private project? Let's talk about the government

1432
01:44:19,920 --> 01:44:23,600
project a little bit and checks and balances. In some sense, I think my argument is a sort

1433
01:44:23,600 --> 01:44:28,720
of brookian argument, which is like American checks and balances have held for over 200 years

1434
01:44:28,720 --> 01:44:33,600
and through crazy technological revolutions. The US military could kill like every civilian

1435
01:44:33,600 --> 01:44:36,240
in the United States. But you're going to make that argument, the private public

1436
01:44:36,240 --> 01:44:40,640
balance of power itself for hundreds of years. But yeah, why has it held? Because the government

1437
01:44:40,640 --> 01:44:45,600
has the biggest guns and has never before has a single CEO or a random nonprofit board

1438
01:44:45,600 --> 01:44:50,400
had the ability to launch nukes. And so again, it's like, you know, what is the track record

1439
01:44:50,400 --> 01:44:53,120
of the government checks and balances versus the track record of the private company checks

1440
01:44:53,120 --> 01:44:57,120
and balances? Well, the iLab, you know, like first stress test, you know, went really badly,

1441
01:44:57,120 --> 01:45:02,480
you know, that didn't really work, you know? I mean, even worse in the sort of private

1442
01:45:02,480 --> 01:45:07,040
company world. So it's both like, it is like the two private companies and the CCP and they just

1443
01:45:07,040 --> 01:45:11,200
like instantly have all the shit. And then it's, you know, they probably won't have good enough

1444
01:45:11,200 --> 01:45:14,960
internal control. So it's like, not just like the random CEO, but it's like, you know, rogue

1445
01:45:14,960 --> 01:45:18,240
employees that can kind of like use these super intelligences to do whatever they want.

1446
01:45:18,240 --> 01:45:22,720
And this won't be true of the government? Like the rogue employees won't exist on the project?

1447
01:45:22,720 --> 01:45:27,360
Well, the government actually like, you know, has decades of experience and like actually

1448
01:45:27,360 --> 01:45:30,400
really cares about the stuff. I mean, it's like they deal, they deal with nukes, they deal with

1449
01:45:30,400 --> 01:45:34,160
really powerful technology. And it's, you know, this is like, this is the stuff that the national

1450
01:45:34,160 --> 01:45:37,680
security state cares about. You know, again, to the go, let's talk about the government checks

1451
01:45:37,680 --> 01:45:41,440
and balances a little bit. So, you know, what are checks and balances in the government world?

1452
01:45:41,440 --> 01:45:43,920
First of all, I think it's actually quite important that you have some amount of

1453
01:45:43,920 --> 01:45:47,040
international coalition. And I talked about these sort of two tiers before. Basically,

1454
01:45:47,040 --> 01:45:50,240
I think the inner tier is a sort of modeled on the cool record agreement, right? This was like

1455
01:45:50,240 --> 01:45:55,840
Churchill and Roosevelt, they kind of agreed secretly. We're going to like pull our efforts

1456
01:45:55,840 --> 01:45:58,880
on nukes, but we're not going to use them against each other. And we're not going to use them

1457
01:45:58,880 --> 01:46:02,640
against anyone else with their consent. And I think basically look, bring in, bring in the UK,

1458
01:46:02,720 --> 01:46:06,000
they have deep mind bringing in the kind of like Southeast Asian states who have the chip supply

1459
01:46:06,000 --> 01:46:10,560
chain, bring in some more of kind of like NATO close democratic allies for, you know, talent and

1460
01:46:10,560 --> 01:46:14,240
industrial resources. And you have the sort of like, you know, so you have, you have those checks

1461
01:46:14,240 --> 01:46:19,440
and balances in terms of like more international countries at the table. Sorry, somewhat separately,

1462
01:46:19,440 --> 01:46:22,640
but then you have the sort of second tier of coalitions, which is the sort of Adams for peace

1463
01:46:22,640 --> 01:46:26,000
thing, where you go to a bunch of countries, including like the UAE, and you're like, look,

1464
01:46:26,000 --> 01:46:29,600
we're going to basically like, you know, there's a deal similar to like the NPT stuff where it's

1465
01:46:30,080 --> 01:46:33,920
you're not allowed to like do the crazy military stuff, but we're going to share the civilian

1466
01:46:33,920 --> 01:46:38,240
applications. We're in fact going to help you and share the benefits and, you know, sort of kind

1467
01:46:38,240 --> 01:46:42,240
of like this new sort of post superintelligence world order. All right, US checks and balances,

1468
01:46:42,240 --> 01:46:45,360
right? So obviously, Congress is going to have to be involved, right? Appropriate in

1469
01:46:45,360 --> 01:46:49,600
trillions of dollars. I think probably ideally you have Congress needs to kind of like confirm

1470
01:46:49,600 --> 01:46:53,200
whoever's running this. So you have Congress, you have like different factions of the government,

1471
01:46:53,200 --> 01:46:56,800
you have the courts, I expect the First Amendment to continue being really important. And maybe that,

1472
01:46:56,800 --> 01:46:59,520
I think that sounds kind of crazy to people, but I actually think, again, I think these are

1473
01:47:00,160 --> 01:47:04,160
institutions that have held this test of time in a really sort of powerful way.

1474
01:47:05,040 --> 01:47:08,480
You know, eventually, you know, this is why honestly, alignment is important is like,

1475
01:47:08,480 --> 01:47:12,800
you know, the AIs, you program the AIs to follow the Constitution. And it's like, you know,

1476
01:47:12,800 --> 01:47:18,480
why does the military work? It's like generals, you know, are not allowed to follow unlawful

1477
01:47:18,480 --> 01:47:22,320
orders or not allowed to follow unconstitutional orders. You have the same thing for the AIs.

1478
01:47:22,320 --> 01:47:26,640
So what's wrong with this argument? When you say, listen, maybe you have a point in the world

1479
01:47:26,720 --> 01:47:29,840
where we have extremely fast takeoff. It's like one year from AGI to ASI.

1480
01:47:29,840 --> 01:47:33,920
Yeah. And then you have the like, years after ASI, where you have this like,

1481
01:47:33,920 --> 01:47:39,040
extraordinary explosion. Maybe you have a point. We don't know, you have these arguments,

1482
01:47:39,040 --> 01:47:41,760
we'll like get into the weeds on them about why that's a more likely world, but like maybe

1483
01:47:41,760 --> 01:47:46,720
that's not the world we live in. And in the other world, I'm like, very on the side of making sure

1484
01:47:46,720 --> 01:47:54,560
that these things are privately held. Now, when you nationalize, that's a one way function,

1485
01:47:54,560 --> 01:48:00,400
you can't go back. Why not wait until we have more evidence on which of those worlds we live in?

1486
01:48:02,080 --> 01:48:05,920
And I think like rushing on the nationalization might be a bad idea while we're not sure.

1487
01:48:06,560 --> 01:48:10,400
And okay, I'll just respond to that first. I mean, I don't, I don't expect us to nationalize

1488
01:48:10,400 --> 01:48:13,840
tomorrow. If anything, I expected to be kind of with COVID where it's like kind of too late,

1489
01:48:13,840 --> 01:48:17,360
like ideally you nationalize it early enough to like actually lock stuff down,

1490
01:48:17,360 --> 01:48:20,240
it'll probably be kind of chaotic and like, you know, you're going to be trying to like do this

1491
01:48:20,240 --> 01:48:23,520
crash program to lock stuff down and it'll be kind of late and it'll be kind of clear what's

1492
01:48:23,520 --> 01:48:26,800
happening. We're not going to nationalize when it's not clear what's happening. I think the whole

1493
01:48:28,080 --> 01:48:31,360
historically these institutions have held up well. First of all, they've actually

1494
01:48:31,360 --> 01:48:37,040
almost broken a bunch of times. It's like, this is the argument that some people who are saying

1495
01:48:37,040 --> 01:48:40,080
that we shouldn't be that concerned about nuclear war say, where it's like, listen,

1496
01:48:40,080 --> 01:48:44,400
we have the nuke for 80 years and like we've been fine so far, so the risk must be low.

1497
01:48:44,400 --> 01:48:49,120
And then the answer to that is no, actually, it is a really high risk. And the reason we've avoided

1498
01:48:49,120 --> 01:48:53,680
it is like people have gone through a lot of effort to make sure that this thing doesn't happen.

1499
01:48:53,680 --> 01:49:00,400
I don't think that giving government ASI without knowing what that implies is going through the

1500
01:49:00,400 --> 01:49:05,440
lot of effort. And I think the base rate, like you can talk about America, I think America is

1501
01:49:05,440 --> 01:49:10,320
very exceptional, not just in terms of dictatorship, but in terms of every other country in history

1502
01:49:10,320 --> 01:49:15,360
has had a complete drawdown of wealth because of war, revolution, something. America is very

1503
01:49:15,360 --> 01:49:18,960
unique in not having that. And the historical base rate, we're talking about Greek power

1504
01:49:18,960 --> 01:49:22,000
competition. I think that has a really big, that's something we haven't been thinking about

1505
01:49:22,000 --> 01:49:26,240
the last 80 years, but it's really big. Dictatorship is also something that is just

1506
01:49:26,240 --> 01:49:33,680
DD, false state of mankind. And I think relying on institutions, which in an ASI world,

1507
01:49:35,360 --> 01:49:40,240
it's fundamentally right now, if the government tried to overthrow, it's much harder if you

1508
01:49:40,240 --> 01:49:45,760
don't have the ASI, right? Like there's people who have AR-15s and there's like things that

1509
01:49:45,760 --> 01:49:48,960
had to make it harder. If government could crush the AR-15s. No, I think it'd actually be pretty

1510
01:49:48,960 --> 01:49:51,840
hard. The reason it was Vietnam and Afghanistan were pretty hard. They're just a new whole country.

1511
01:49:52,960 --> 01:49:56,240
Yeah, yeah, I agree, but like I'm... They could. I mean similar with ASI.

1512
01:49:57,920 --> 01:50:01,200
Yeah, I think it's just like easier if you have what you're talking about. But there are institutions,

1513
01:50:01,200 --> 01:50:04,800
there are constitutions, there are legal restraints, there are courts, there are checks and balances.

1514
01:50:05,680 --> 01:50:09,600
The crazy bet is the bet which are like private company CEOs. The same thing, by the way,

1515
01:50:09,600 --> 01:50:12,800
isn't the same thing true of nukes where we have these institutional agreements about

1516
01:50:12,800 --> 01:50:16,960
non-polar aspiration and whatever. And we're still very concerned about that being broken

1517
01:50:16,960 --> 01:50:19,360
and somebody getting nukes and like you should stay up that night worrying about that.

1518
01:50:19,360 --> 01:50:24,720
It's a precarious situation, but ASI is going to be a really precarious situation as well. And like

1519
01:50:24,720 --> 01:50:27,040
given how precarious nukes are, we've done pretty well.

1520
01:50:27,040 --> 01:50:29,040
And so what does privatization in this world even mean?

1521
01:50:29,040 --> 01:50:30,240
I mean, I think the other thing is...

1522
01:50:30,240 --> 01:50:31,120
Like what happened after?

1523
01:50:31,120 --> 01:50:33,280
I mean, the other thing, you know, because we're talking about like whether the government

1524
01:50:33,280 --> 01:50:36,640
project is good or not. And it's like, I have very mixed feelings about this as well.

1525
01:50:37,200 --> 01:50:39,200
Again, I think my primary argument is like,

1526
01:50:40,720 --> 01:50:43,120
you know, if you're at the point where this thing has like

1527
01:50:44,240 --> 01:50:47,920
vastly superhuman hacking capabilities, if you're at the point where this thing can develop,

1528
01:50:47,920 --> 01:50:51,200
you know, bio weapons, you know, like in crazy bio weapons, ones that are like targeted,

1529
01:50:51,200 --> 01:50:54,240
you know, can kill everybody but the hand Chinese or, you know, that, you know,

1530
01:50:55,280 --> 01:50:58,800
would wipe out, you know, entire countries, where you're talking about like building Robo

1531
01:50:58,800 --> 01:51:02,400
Armors, you're talking about kind of like drone swarms that are, you know, again,

1532
01:51:02,400 --> 01:51:04,400
the mosquito sized drones that could take it out, you know,

1533
01:51:07,600 --> 01:51:10,800
the United States national security state is going to be intimately involved with this.

1534
01:51:10,800 --> 01:51:13,600
And this will, you know, the labs, whether, you know, and I think, again, the government,

1535
01:51:13,600 --> 01:51:16,640
a lot of what I think is the government project looks like, it is basically a joint venture

1536
01:51:16,640 --> 01:51:20,400
between like, you know, the cloud providers between some of the labs and the government.

1537
01:51:20,400 --> 01:51:25,120
And so I think there is no world in which the government isn't intimately involved in this

1538
01:51:25,120 --> 01:51:28,640
like crazy period. The very least, basically, you know, like the intelligence agencies need

1539
01:51:28,640 --> 01:51:31,600
to be running security for these labs. So they're already kind of like, they're controlling

1540
01:51:31,600 --> 01:51:34,960
everything, they're controlling access to everything. Then they're going to be like,

1541
01:51:34,960 --> 01:51:38,400
probably again, if we're in this like really volatile international situation, like a lot

1542
01:51:38,400 --> 01:51:41,760
of the initial applications, it'll, it'll suck. It's not what I want to use ASI for,

1543
01:51:41,760 --> 01:51:46,880
will be like trying to somehow stabilize this crazy situation. Somehow we need to prevent

1544
01:51:46,880 --> 01:51:51,040
like proliferation of like some crazy new WMDs and like the undermining of mutually assured

1545
01:51:51,040 --> 01:51:57,840
destruction to kind of like, you know, North Korea and Russia and China. And so I think,

1546
01:51:57,920 --> 01:52:02,960
you know, I basically think your world, you know, I think there's much more spectrum than

1547
01:52:02,960 --> 01:52:06,080
you're acknowledging here. And I think basically the world in which it's private labs is like

1548
01:52:06,080 --> 01:52:09,280
extremely heavy government involvement. And really what we're debating is like, you know,

1549
01:52:09,280 --> 01:52:12,800
what form of government project, but it is going to look much more like, you know,

1550
01:52:12,800 --> 01:52:17,280
the national security state than anything it does look like, like a startup as it is right now.

1551
01:52:17,280 --> 01:52:18,480
And I think the, yeah.

1552
01:52:18,480 --> 01:52:23,120
Look, I think something like that makes sense. I would be, if it's like the Manhattan Project,

1553
01:52:23,200 --> 01:52:27,040
then I'm very worried where it's like, this is part of the U.S. military.

1554
01:52:28,480 --> 01:52:31,920
Where I've hit some more like, listen, you got to talk to Jake Sullivan before you

1555
01:52:31,920 --> 01:52:33,120
like run the next training one.

1556
01:52:33,120 --> 01:52:36,960
It's like Lockheed Martin, Skunkward's part of the U.S. military. It's like, they call the shops.

1557
01:52:37,520 --> 01:52:40,560
Yeah, I don't think that's great. I think that's, I think that's bad. I think it would be bad if

1558
01:52:40,560 --> 01:52:42,800
that happened with ASI. And like, what is it? What is the scenario?

1559
01:52:44,400 --> 01:52:45,520
What is the alternative?

1560
01:52:45,520 --> 01:52:48,640
Okay. So it's closer to my end of the spectrum where,

1561
01:52:48,640 --> 01:52:52,640
yeah, you do have to talk to Jake Sullivan before you can launch the next training cluster.

1562
01:52:52,640 --> 01:52:55,680
But there's many companies who are still going for it.

1563
01:52:55,680 --> 01:52:58,720
And the government will be intimately involved in the security.

1564
01:53:00,720 --> 01:53:01,920
But the like, three different companies are trying to-

1565
01:53:01,920 --> 01:53:03,360
Is Dario launching the Stuxnet attack?

1566
01:53:04,080 --> 01:53:06,640
Yeah. What are you, what are you, launching, launching. Okay.

1567
01:53:07,520 --> 01:53:10,560
So Dario's activating the Chinese data centers.

1568
01:53:10,560 --> 01:53:12,960
I think this is similar to the story you could tell about, there's a lot of,

1569
01:53:12,960 --> 01:53:17,440
like literally the big tech right now. I think Satya, if you wanted to, he probably

1570
01:53:17,440 --> 01:53:20,000
like could get his engineers like, what are the zero days in Windows?

1571
01:53:20,960 --> 01:53:25,040
And like, well, how do we get infiltrate the president's computer so that like-

1572
01:53:25,040 --> 01:53:25,920
Maybe shut down.

1573
01:53:26,560 --> 01:53:28,720
No, no, no. Like right now I'm saying Satya could do that, right?

1574
01:53:28,720 --> 01:53:29,120
Because he knows-

1575
01:53:29,120 --> 01:53:30,000
Maybe shut down.

1576
01:53:30,000 --> 01:53:30,720
What do you mean?

1577
01:53:30,720 --> 01:53:31,840
Government wouldn't let them do that.

1578
01:53:33,280 --> 01:53:36,000
Yeah. I think there's a story you could tell where like they could pull off a coup,

1579
01:53:36,000 --> 01:53:37,040
whatever. But like, I think there's like-

1580
01:53:37,040 --> 01:53:38,560
Maybe not pull off a coup.

1581
01:53:38,560 --> 01:53:38,800
Okay.

1582
01:53:38,800 --> 01:53:40,320
Maybe not pull off, come on.

1583
01:53:40,320 --> 01:53:43,120
Okay. Fine, fine, fine. I agree. I'm just saying like something closer to,

1584
01:53:43,920 --> 01:53:49,520
so what's wrong with the scenario where you, the government is, there's like multiple

1585
01:53:49,520 --> 01:53:54,880
companies going for it, but the AI is still broadly deployed and alignment works in the

1586
01:53:54,880 --> 01:53:58,240
sense that you can make sure that it's not, the system level prompt is like,

1587
01:53:58,240 --> 01:54:02,320
you can't help people make bio weapons or something, but these are still broadly deployed.

1588
01:54:02,320 --> 01:54:03,120
So that-

1589
01:54:03,120 --> 01:54:05,280
I mean, I expect the AIs to be broadly deployed. I mean, first of all-

1590
01:54:05,280 --> 01:54:06,240
Even if it's a government project?

1591
01:54:06,240 --> 01:54:08,720
Yeah. I mean, look, I think first of all, like, I think the matters of the world,

1592
01:54:08,720 --> 01:54:11,520
you know, open sourcing their eyes, you know, that are two years behind or whatever.

1593
01:54:11,520 --> 01:54:15,760
Yeah. Super valuable role. They're gonna like, you know, and so there's gonna be some question

1594
01:54:15,760 --> 01:54:19,600
of like, either the offense, defense balance is fine. And so like, even if they open sourced

1595
01:54:19,600 --> 01:54:22,880
two year old AIs, it's fine. Or it's like, there's some restrictions on the most extreme dual use

1596
01:54:22,880 --> 01:54:26,080
capabilities, like, you know, you don't let private companies sell kind of crazy weapons.

1597
01:54:26,800 --> 01:54:30,720
And that's great. And that will help with the diffusion. And, you know, after the government

1598
01:54:30,720 --> 01:54:33,760
project, you know, there's gonna be this initial tense period, hopefully that's stabilized.

1599
01:54:33,760 --> 01:54:36,320
And then look, yeah, like Boeing, they're gonna go out and they're gonna like make,

1600
01:54:37,280 --> 01:54:41,040
do all the flourishing civilian applications and, you know, like nuclear energy, you know,

1601
01:54:41,040 --> 01:54:45,360
it'll like all the civilian applications will have their day. I think part of my argument here is that-

1602
01:54:45,360 --> 01:54:49,200
And how does that proceed, right? Because in the other world, there's existing stocks of capital

1603
01:54:49,200 --> 01:54:52,720
that are worth a lot. Yeah, the clusters, they'll be still be Google clusters.

1604
01:54:52,720 --> 01:54:56,880
And so Google, because they got the contract from the government, they'll be the ones that control

1605
01:54:56,880 --> 01:55:01,040
the ASI. But like, why are they trading with anybody else? Why is there a random start up again?

1606
01:55:01,040 --> 01:55:04,000
It'll be the same. It'll be the same companies that would be doing it anyway. But in this,

1607
01:55:04,000 --> 01:55:07,600
in this world, they're just contracting with the government or like their DPA for all their compute

1608
01:55:07,600 --> 01:55:13,360
goes to the government. And, but in the world, it's very natural. It's like sort of how

1609
01:55:14,320 --> 01:55:18,320
after you get the ASI and we're building the robot armies and building fusion reactors or whatever,

1610
01:55:18,800 --> 01:55:22,080
that the, that's- Only the government will get to build robot armies.

1611
01:55:22,800 --> 01:55:26,160
Yeah, that one worried. Or like the fusion reactors and stuff.

1612
01:55:26,160 --> 01:55:28,800
That's what we do with this, because- It's the same situation we have today.

1613
01:55:28,800 --> 01:55:31,920
Because if you already have the robot armies and everything, like the existing society doesn't

1614
01:55:31,920 --> 01:55:35,200
have some leverage where it makes sense to the government to- But they don't have that today.

1615
01:55:35,200 --> 01:55:38,560
Yeah, they get in the sense that there's like a lot of capital that the government wants and

1616
01:55:38,560 --> 01:55:41,360
there's other things like, why was Boeing privatized after?

1617
01:55:41,360 --> 01:55:44,080
Government has the biggest guns. And the way we regulate is institutions,

1618
01:55:44,080 --> 01:55:45,440
constitutions, legal restraints.

1619
01:55:45,440 --> 01:55:48,560
Oh, because tell me what privatization should look like in the ASI world afterwards.

1620
01:55:48,560 --> 01:55:51,440
Afterwards. Like the Boeing example, right? It's like, you have this government-

1621
01:55:51,440 --> 01:55:54,080
Who gets it? Like Google, Microsoft, and last year-

1622
01:55:54,080 --> 01:55:56,160
And who are they selling it to? Like they already have the robot factory.

1623
01:55:56,160 --> 01:55:58,240
And then look at- Why are they selling it to us? Like they already have the,

1624
01:55:58,240 --> 01:56:01,680
they don't need like our, this is chum change in the ASI world.

1625
01:56:01,680 --> 01:56:05,920
Because we didn't get like the ASI broadly deployed throughout this takeoff.

1626
01:56:05,920 --> 01:56:09,520
So we don't have the robot. We don't have like the fusion reactors and whatever advanced,

1627
01:56:09,520 --> 01:56:11,840
decades of advanced science that you were talking about.

1628
01:56:11,840 --> 01:56:13,840
So like it just, what are they trading with us for?

1629
01:56:14,800 --> 01:56:16,000
Trading with whom for?

1630
01:56:16,000 --> 01:56:17,600
Everybody who was not part of the project.

1631
01:56:17,600 --> 01:56:19,360
They've got that technology that's decades ahead.

1632
01:56:19,360 --> 01:56:21,040
Yeah. I mean, look, that's a whole nother issue of like,

1633
01:56:21,040 --> 01:56:24,240
how does like economic distribution work or whatever? I don't know. That'll be rough.

1634
01:56:25,280 --> 01:56:26,560
Yeah. I think-

1635
01:56:26,560 --> 01:56:28,800
I'm just saying, I don't, I don't, basically I'm kind of like,

1636
01:56:28,800 --> 01:56:30,880
I don't see the alternative. The alternative is,

1637
01:56:30,880 --> 01:56:34,400
you like overturn a 500 year civilizational achievement of land fleeting.

1638
01:56:34,400 --> 01:56:38,000
You basically instantly leak the stuff to the CCP.

1639
01:56:38,000 --> 01:56:42,240
And either you like barely scrape out ahead and, but you're in this fever struggle,

1640
01:56:42,240 --> 01:56:44,000
you're like proliferating crazy WMDs.

1641
01:56:44,000 --> 01:56:45,920
It's just like enormously dangerous situation,

1642
01:56:45,920 --> 01:56:47,120
enormously dangerous on alignment,

1643
01:56:47,120 --> 01:56:49,280
because you're in this kind of like crazy race at the end.

1644
01:56:49,280 --> 01:56:51,760
And you don't have the ability to like take six months to get alignment.

1645
01:56:51,760 --> 01:56:55,760
Right. The alternative is, you know,

1646
01:56:55,760 --> 01:56:58,560
alternative is like you aren't actually bundling your efforts to kind of like win

1647
01:56:58,560 --> 01:57:00,080
the race against the authoritarian powers.

1648
01:57:00,160 --> 01:57:02,480
You know, yeah. And so,

1649
01:57:05,440 --> 01:57:09,520
you know, I don't like it. You know, I wish,

1650
01:57:09,520 --> 01:57:11,760
I wish the thing we use the ASI for is to like,

1651
01:57:11,760 --> 01:57:14,160
you know, cure the diseases and do all the good in the world.

1652
01:57:14,160 --> 01:57:18,560
But it is my prediction that sort of like by the, in the end game,

1653
01:57:21,520 --> 01:57:24,320
what will be at stake will not just be kind of cool products,

1654
01:57:24,320 --> 01:57:27,840
but what will be at stake is like whether liberal democracy survives,

1655
01:57:27,840 --> 01:57:32,320
like whether the CCP survives, like what the world order for the next century will be.

1656
01:57:32,320 --> 01:57:34,000
And when that is at stake,

1657
01:57:34,000 --> 01:57:37,680
forces will be activated that are sort of way beyond what we're talking about now.

1658
01:57:37,680 --> 01:57:41,840
And like, you know, in the sort of like crazy race at the end,

1659
01:57:42,400 --> 01:57:45,360
like the sort of national security implications will be the most important,

1660
01:57:45,360 --> 01:57:48,000
you know, sort of like, you know, World War II, it's like, yeah, you know,

1661
01:57:48,640 --> 01:57:49,840
nuclear energy, how did it stay?

1662
01:57:49,840 --> 01:57:53,040
But in the initial kind of period, when, you know,

1663
01:57:53,040 --> 01:57:54,560
when this technology was first discovered,

1664
01:57:54,560 --> 01:57:56,480
you had to stabilize the situation, you had to get nukes,

1665
01:57:56,480 --> 01:57:57,280
you had to do it right.

1666
01:57:58,560 --> 01:58:00,960
And then, and then the civilian applications out there.

1667
01:58:00,960 --> 01:58:03,760
I think of closer analogy to what this is, because nuclear,

1668
01:58:03,760 --> 01:58:05,920
I agree that nuclear energy is the thing that happens in Iran,

1669
01:58:05,920 --> 01:58:06,960
and it's like dual use in that way.

1670
01:58:06,960 --> 01:58:09,600
But it's, it's something that happened like literally a decade after nuclear weapons were

1671
01:58:09,600 --> 01:58:14,080
developed, whereas with AI, like the immediately all the applications are unlocked.

1672
01:58:14,080 --> 01:58:17,680
And it's closer to literally, I mean, this is analogy people are supposed to make in

1673
01:58:17,680 --> 01:58:23,280
the context of AGI is like, assume your society had 100 million more John Wayne Neumanns.

1674
01:58:23,280 --> 01:58:25,680
And I don't think like, if that was literally what happened,

1675
01:58:25,760 --> 01:58:28,720
if tomorrow you just have 100 million more of them, the approach should have been,

1676
01:58:28,720 --> 01:58:30,240
well, some of them will convert to ISIS.

1677
01:58:30,240 --> 01:58:32,400
And we need to like be really careful about that.

1678
01:58:32,400 --> 01:58:35,680
And then like, oh, you know, like what if a bunch of them are born in China?

1679
01:58:35,680 --> 01:58:38,880
And then we like, if we got to nationalize the John Wayne Neumanns,

1680
01:58:38,880 --> 01:58:41,440
I'm like, no, I think it'll be generally a good thing.

1681
01:58:41,440 --> 01:58:44,960
And I'd be concerned about one power had getting like all the John Wayne Neumanns.

1682
01:58:44,960 --> 01:58:48,240
I mean, I think the issue is the sort of like bottling up in the sort of intensely short

1683
01:58:48,240 --> 01:58:50,880
period of time, like this enormous sort of like, you know,

1684
01:58:52,000 --> 01:58:55,120
unfolding of technological progress of an industrial explosion.

1685
01:58:55,120 --> 01:58:57,280
And I think we do worry about the 100 million John Wayne Neumanns.

1686
01:58:57,280 --> 01:58:58,320
It's like rise of China.

1687
01:58:58,320 --> 01:58:59,920
Why are we worried about the rise of China?

1688
01:58:59,920 --> 01:59:03,440
Because it's like 100 billion people and they're able to do a lot of industry

1689
01:59:03,440 --> 01:59:04,640
and do a lot of technology.

1690
01:59:04,640 --> 01:59:07,680
And but it's just like, you know, the rise of China times like, you know, 100,

1691
01:59:07,680 --> 01:59:09,120
because it's not just 101 billion people.

1692
01:59:09,120 --> 01:59:13,280
It's like a billion super intelligent, crazy, you know, crazy things.

1693
01:59:13,280 --> 01:59:15,360
And in like, you know, a very short period.

1694
01:59:16,160 --> 01:59:20,800
Let's start practically, because if the goal is we need to beat China,

1695
01:59:20,800 --> 01:59:21,840
part of that is protecting.

1696
01:59:21,840 --> 01:59:22,800
I mean, that's one of the goals, right?

1697
01:59:22,800 --> 01:59:23,440
Yeah, I agree.

1698
01:59:23,440 --> 01:59:23,760
I agree.

1699
01:59:23,760 --> 01:59:24,880
One of the goals is to beat China.

1700
01:59:24,960 --> 01:59:28,080
And also manage this incredibly crazy, scary period.

1701
01:59:28,080 --> 01:59:28,240
Yeah.

1702
01:59:28,240 --> 01:59:28,560
Right.

1703
01:59:28,560 --> 01:59:31,760
So part of that is making sure we're not leaking algorithmic secrets to them.

1704
01:59:31,760 --> 01:59:32,240
Yep.

1705
01:59:32,240 --> 01:59:33,680
Part of that is a cluster.

1706
01:59:34,320 --> 01:59:35,600
I mean, building the trillion dollar cluster.

1707
01:59:35,600 --> 01:59:35,920
That's right.

1708
01:59:35,920 --> 01:59:36,000
Right.

1709
01:59:36,000 --> 01:59:36,240
Yeah.

1710
01:59:36,240 --> 01:59:40,000
But like your whole point, the Microsoft can release corporate bonds that are.

1711
01:59:40,000 --> 01:59:41,760
I think Microsoft can do the like hundreds of billions.

1712
01:59:41,760 --> 01:59:41,920
Yeah.

1713
01:59:41,920 --> 01:59:45,120
I think I think the trillion dollar cluster is closer to a national effort.

1714
01:59:45,840 --> 01:59:49,200
I thought that your earlier point was that American capital markets are deep.

1715
01:59:49,200 --> 01:59:49,600
They're good.

1716
01:59:49,600 --> 01:59:50,160
They're pretty good.

1717
01:59:50,160 --> 01:59:52,400
I mean, I think the trillion, I think it's possible it's private.

1718
01:59:52,400 --> 01:59:52,960
It's possible.

1719
01:59:53,280 --> 01:59:57,520
But it's going to be like, you know, by the way, at this point, we have a AGI that's

1720
01:59:57,520 --> 01:59:58,880
rapidly accelerating productivity.

1721
01:59:58,880 --> 02:00:01,840
I think the trillion dollar cluster is going to be planned before before the AGI.

1722
02:00:02,880 --> 02:00:06,880
I think it's sort of like you get the AGI on the like 10 gigawatt cluster, like intelligent.

1723
02:00:06,880 --> 02:00:09,600
Maybe you have like one more year where you're kind of doing some final on hobbling to fully

1724
02:00:09,600 --> 02:00:10,240
unlock it.

1725
02:00:10,240 --> 02:00:11,840
Then you have the intelligence explosion.

1726
02:00:11,840 --> 02:00:14,000
And meanwhile, the like trillion dollar cluster is almost finished.

1727
02:00:14,000 --> 02:00:16,880
And then you like, and then you do your super intelligence on your trillion dollar cluster,

1728
02:00:16,880 --> 02:00:18,480
or you run it on your trillion dollar cluster.

1729
02:00:18,480 --> 02:00:21,040
And by the way, you have not just your trillion dollar cluster, but like, you know,

1730
02:00:21,120 --> 02:00:23,600
hundreds of millions of GPUs on inference clusters everywhere.

1731
02:00:23,600 --> 02:00:27,200
And this isn't result, like, I think private, in this world, I think private companies have

1732
02:00:27,200 --> 02:00:28,800
the capital and can raise capital to do it.

1733
02:00:28,800 --> 02:00:31,120
I think you will need the government force to do it fast.

1734
02:00:31,120 --> 02:00:35,520
Well, I was just about to ask, like, wouldn't it be the, like, we know company companies are

1735
02:00:35,520 --> 02:00:41,440
on track to be able to do this and China, if they're unhindered by climate pledges or whatever.

1736
02:00:41,440 --> 02:00:42,400
Well, that's part of what I'm saying.

1737
02:00:42,400 --> 02:00:48,160
So if that's the case, if it really matters that we beat China, there's all kinds of

1738
02:00:48,160 --> 02:00:49,360
practical difficulties of like,

1739
02:00:49,920 --> 02:00:53,680
will the AI researchers actually join the AI effort?

1740
02:00:53,680 --> 02:00:58,480
If they do, there's going to be three different teams at least who are currently doing

1741
02:00:58,480 --> 02:01:02,480
private pre-training on different, different companies.

1742
02:01:02,480 --> 02:01:06,880
Now who decides, at some point, you're going to have the, you're like YOLO, the hyperparameters

1743
02:01:06,880 --> 02:01:11,040
of the trillion dollar cluster, who decides that?

1744
02:01:11,040 --> 02:01:15,920
Just like merging extremely complicated research and development processes

1745
02:01:15,920 --> 02:01:17,360
across very different organizations.

1746
02:01:18,320 --> 02:01:21,280
This is how it's supposed to speed up America against the Chinese.

1747
02:01:21,280 --> 02:01:22,480
Like, why don't we just let...

1748
02:01:22,480 --> 02:01:23,520
Brain and deep mind merge.

1749
02:01:23,520 --> 02:01:24,720
And it was like a little messy, but it was fine.

1750
02:01:24,720 --> 02:01:25,520
It was pretty messy.

1751
02:01:25,520 --> 02:01:28,560
And it was also the same company and also much earlier on in the process.

1752
02:01:28,560 --> 02:01:29,520
I mean, pretty similar, right?

1753
02:01:29,520 --> 02:01:32,880
Same code, different code bases and like lots of different infrastructure and different teams.

1754
02:01:32,880 --> 02:01:35,920
And it was like, you know, it wasn't like, it wasn't the smoothest of all processes,

1755
02:01:35,920 --> 02:01:37,520
but, you know, deep mind is doing, I think, very well.

1756
02:01:37,520 --> 02:01:41,520
I mean, look, you give the example of COVID and the COVID example is like, listen,

1757
02:01:41,520 --> 02:01:45,040
we woke up to it, maybe it was late, but then we deployed all this money.

1758
02:01:45,040 --> 02:01:48,320
And COVID response to government was a clusterfuck over.

1759
02:01:48,320 --> 02:01:51,520
And like the only part of it that was worked is I agree Warp Speed was like enabled by the

1760
02:01:51,520 --> 02:01:51,840
government.

1761
02:01:51,840 --> 02:01:55,920
It was literally just giving the permission that you can actually do.

1762
02:01:55,920 --> 02:01:58,240
Well, it was also taking, making like the big

1763
02:01:58,240 --> 02:01:59,120
commitments or whatever.

1764
02:01:59,120 --> 02:01:59,520
But I agree.

1765
02:01:59,520 --> 02:02:01,760
But it was like fundamentally it was like a private sector led effort.

1766
02:02:01,760 --> 02:02:02,080
Yeah.

1767
02:02:02,080 --> 02:02:03,440
That was the only part of COVID that worked.

1768
02:02:03,440 --> 02:02:06,080
I mean, I think, I think, again, I think the project will look closer to operation

1769
02:02:06,080 --> 02:02:06,640
Warp Speed.

1770
02:02:06,640 --> 02:02:09,600
And it's not even, I mean, I think, I think you'll have all the companies involved

1771
02:02:09,600 --> 02:02:10,640
in the government project.

1772
02:02:10,640 --> 02:02:12,800
I'm not that sold that merging is that difficult.

1773
02:02:12,800 --> 02:02:16,240
You know, you have one, okay, you select one code base and you know, you run free

1774
02:02:16,240 --> 02:02:18,480
training on like GPUs with, you know, one code base.

1775
02:02:18,480 --> 02:02:21,360
And then you do the sort of second RL step on the, you know, the other code base with

1776
02:02:21,360 --> 02:02:23,200
TPU is that I think it's fine.

1777
02:02:24,880 --> 02:02:26,880
I mean, to the topic of like, will people sign up for it?

1778
02:02:26,880 --> 02:02:28,160
It wouldn't sign up for it today.

1779
02:02:28,160 --> 02:02:29,680
I think this would be kind of crazy to people.

1780
02:02:30,640 --> 02:02:33,120
But also, you know, I mean, this is part of the like secrets thing, you know, people

1781
02:02:33,120 --> 02:02:37,120
gather parties or whatever, you know, you know this, you know, I don't think anyone

1782
02:02:37,120 --> 02:02:40,880
has really gotten up in front of these people and been like, look, you know, the thing you're

1783
02:02:40,880 --> 02:02:46,000
building is the most important thing for like the national security of the United States

1784
02:02:46,000 --> 02:02:49,280
for like weather, you know, like, you know, the free world will have another century ahead

1785
02:02:49,280 --> 02:02:49,600
of it.

1786
02:02:49,600 --> 02:02:53,840
Like this is the thing you're doing is really important, like for your country, for democracy.

1787
02:02:55,280 --> 02:02:57,920
And, you know, don't talk about the secrets.

1788
02:02:57,920 --> 02:03:01,280
And it's not just about, you know, deep mind or whatever, it's about, it's about, you know,

1789
02:03:01,280 --> 02:03:02,320
these really important things.

1790
02:03:03,760 --> 02:03:06,000
And so, you know, I don't know, like, again, we're talking about the Manhattan project,

1791
02:03:06,000 --> 02:03:06,080
right?

1792
02:03:06,080 --> 02:03:07,840
This stuff was really contentious initially.

1793
02:03:08,800 --> 02:03:11,600
But, you know, at some point, it was like clear that this stuff was coming.

1794
02:03:11,600 --> 02:03:15,440
It was clear that there was like sort of a real sort of like exigency on the military

1795
02:03:15,440 --> 02:03:16,880
national security front.

1796
02:03:16,880 --> 02:03:21,520
And, you know, I think a lot of people come around on the like weather will be competent.

1797
02:03:21,520 --> 02:03:22,320
I agree.

1798
02:03:22,320 --> 02:03:25,280
I mean, this is again, where it's like a lot of the stuff is more like predictive in the

1799
02:03:25,280 --> 02:03:27,280
sense, I think this is like reasonably likely.

1800
02:03:27,280 --> 02:03:28,720
And I think not enough people are thinking about it.

1801
02:03:28,720 --> 02:03:31,600
You know, like a lot of people think about like AI lab politics or whatever.

1802
02:03:32,640 --> 02:03:35,200
But like nobody has a plan for the project, you know, it's like, you know,

1803
02:03:35,520 --> 02:03:36,800
they think you're pessimistic about it.

1804
02:03:36,800 --> 02:03:38,160
And like, well, you don't have a plan for it.

1805
02:03:38,160 --> 02:03:40,800
We need to do it very soon because AGI is upon us.

1806
02:03:40,800 --> 02:03:45,760
Then fuck, the only capable competent technical institutions capable of making AI right now

1807
02:03:45,760 --> 02:03:46,560
are private companies.

1808
02:03:46,560 --> 02:03:48,480
And they're going to play that leading role.

1809
02:03:48,480 --> 02:03:49,920
It'll be a sort of a partnership basically.

1810
02:03:49,920 --> 02:03:52,720
But the other thing is like, you know, again, we talked about World War II and, you know,

1811
02:03:52,720 --> 02:03:56,240
American unpreparedness, the veneer of World War II is complete, you know, complete shambles,

1812
02:03:56,240 --> 02:03:56,640
right?

1813
02:03:56,640 --> 02:03:58,800
And so there is a sort of like very company.

1814
02:03:58,800 --> 02:04:03,120
I think America has a very deep bench of just like incredibly competent managerial talent.

1815
02:04:03,120 --> 02:04:06,880
You know, I think that, you know, there's a lot of really dedicated people.

1816
02:04:06,880 --> 02:04:11,440
And, you know, I think basically a sort of operational warp speed, public-private partnership,

1817
02:04:11,440 --> 02:04:14,560
something like that, you know, is sort of what I imagine it would look like.

1818
02:04:14,560 --> 02:04:14,880
Yeah.

1819
02:04:14,880 --> 02:04:20,000
I mean, the recruiting the talent is an interesting question because the same sort of thing where

1820
02:04:22,320 --> 02:04:24,640
initially for the Manhattan Project, you had to convince people,

1821
02:04:24,640 --> 02:04:26,960
we've got to beat the Nazis and you got to get on board.

1822
02:04:26,960 --> 02:04:31,200
I think a lot of them maybe regretted how much they accelerated the bomb.

1823
02:04:31,200 --> 02:04:36,480
And I want, I think this is generally a thing of the war where...

1824
02:04:36,480 --> 02:04:38,320
I mean, I think they're also wrong to regret it, but...

1825
02:04:40,560 --> 02:04:41,600
Yeah, I mean, why?

1826
02:04:42,400 --> 02:04:43,520
What's the reason for regretting it?

1827
02:04:44,080 --> 02:04:48,640
I think there's a world in which you don't have, the way in which nuclear weapons were

1828
02:04:48,640 --> 02:04:53,840
developed after the war was pretty explosive because there was a precedent that you actually

1829
02:04:53,840 --> 02:04:55,360
can use nuclear weapons.

1830
02:04:55,360 --> 02:04:58,800
Then because of the race that was set up, you immediately go to the H bomb.

1831
02:04:59,760 --> 02:05:03,600
I mean, I think my view is, again, this is related to the view on AI and maybe some of

1832
02:05:03,600 --> 02:05:05,680
our disagreement is like, that was inevitable.

1833
02:05:05,680 --> 02:05:11,680
Like, of course, there was this world war and then obviously there was the cold war right after.

1834
02:05:11,680 --> 02:05:17,760
Of course, the military and technology angle of this would be pursued with ferocious intensity.

1835
02:05:17,760 --> 02:05:20,000
And I don't really think there's a world in which that doesn't happen,

1836
02:05:20,000 --> 02:05:21,920
where it's like, ah, we're all not going to build nukes.

1837
02:05:21,920 --> 02:05:24,000
And also just like nukes went really well.

1838
02:05:24,000 --> 02:05:25,520
I think that could have gone terribly, right?

1839
02:05:26,080 --> 02:05:31,440
Again, I think this is not physically possible with nukes, this pocket nukes for everybody,

1840
02:05:31,440 --> 02:05:35,920
but I think WMDs that are proliferated and democratized and all the countries have it.

1841
02:05:36,560 --> 02:05:41,360
The US leading on nukes and then building this new world order that was US-led,

1842
02:05:41,360 --> 02:05:44,880
or at least a few great powers, and a non-proliferation regime for nukes,

1843
02:05:44,880 --> 02:05:50,080
a partnership and a deal that's like, look, no military application of nuclear technology,

1844
02:05:50,080 --> 02:05:51,920
but we're going to help you with the civilian technology.

1845
02:05:51,920 --> 02:05:53,840
We're going to enforce safety norms on the rest of the world.

1846
02:05:53,920 --> 02:05:57,040
That worked. It worked. And it could have gone so much worse.

1847
02:05:58,000 --> 02:05:59,440
So we're zooming on.

1848
02:05:59,440 --> 02:06:00,560
I don't know if you're talking about Nagasaki.

1849
02:06:00,560 --> 02:06:02,720
You know, I mean, this is, I mean, I say this a bit in the piece,

1850
02:06:02,720 --> 02:06:05,920
but it's like actually the A-bomb, you know, like the A-bomb on Hiroshima and Nagasaki was just like,

1851
02:06:05,920 --> 02:06:07,200
you know, the sort of firebombing.

1852
02:06:08,560 --> 02:06:13,760
I think the thing that really changed the game was like the super, you know, the H-bombs and ICBMs.

1853
02:06:13,760 --> 02:06:16,480
And then I think that's really when it took it to like a whole new level.

1854
02:06:16,480 --> 02:06:22,800
I think part of me thinks when you say, we'll tell the people that for the free world to survive,

1855
02:06:22,800 --> 02:06:24,080
we need to pursue this project.

1856
02:06:24,720 --> 02:06:27,760
It sounds similar to World War II is,

1857
02:06:28,800 --> 02:06:31,920
so World War II is a sad story, obviously in this fact that it happened,

1858
02:06:31,920 --> 02:06:34,400
but also like the victory is sad in the sense that

1859
02:06:35,120 --> 02:06:38,880
Britain goes in to protect Poland.

1860
02:06:38,880 --> 02:06:44,880
And at the end, the USSR, which is, you know, as your family knows,

1861
02:06:45,920 --> 02:06:50,800
is incredibly brutal, ends up occupying half of Europe.

1862
02:06:50,800 --> 02:06:56,880
And part of protecting the free world, that's why I got to rush the AI.

1863
02:06:56,880 --> 02:06:59,920
And like, if we end up with the American AI Leviathan,

1864
02:06:59,920 --> 02:07:01,920
I think there's a world where we look back on this,

1865
02:07:01,920 --> 02:07:09,680
where it has the same sort of twisted irony that Britain going into World War II had about trying to protect Poland.

1866
02:07:11,280 --> 02:07:13,840
Look, I mean, I think there's going to be a lot of unfortunate things that happen.

1867
02:07:13,840 --> 02:07:15,840
I'm just hoping we make it through.

1868
02:07:15,840 --> 02:07:19,520
I mean, to the point of it's like, I really don't think the pitch will only be the sort of like,

1869
02:07:19,520 --> 02:07:23,040
you know, the race, I think the race will be sort of a backdrop to it.

1870
02:07:23,040 --> 02:07:26,800
I think the sort of general like, look, it's important that democracy shape this technology.

1871
02:07:26,800 --> 02:07:30,400
We can't just like leak this stuff to, you know, North Korea is going to be important.

1872
02:07:30,400 --> 02:07:33,440
I think also for the just safety, including alignment,

1873
02:07:33,440 --> 02:07:37,520
including the sort of like creation of new WMDs, I'm not currently sold.

1874
02:07:37,520 --> 02:07:38,480
There's another path, right?

1875
02:07:38,480 --> 02:07:41,760
So it's like, if you just have the breakneck grace, both internationally,

1876
02:07:41,760 --> 02:07:44,400
because you're just instantly leaking all the stuff, including the weights,

1877
02:07:45,120 --> 02:07:48,560
and just, you know, the commercial race, you know, Demis and Dario and Sam, you know,

1878
02:07:48,560 --> 02:07:49,920
just kind of like, they all want to be first.

1879
02:07:51,280 --> 02:07:52,800
And then it's incredibly rough for safety.

1880
02:07:52,800 --> 02:07:54,800
And then you say, okay, safety regulation.

1881
02:07:54,800 --> 02:07:57,680
But, you know, it's sort of like, you know, the safety regulation that people talk about,

1882
02:07:57,680 --> 02:08:01,520
it's like, oh, well, NIST, and they take years and they figure out what the expert consensus is,

1883
02:08:01,520 --> 02:08:03,920
and then they write what's going to happen to the project as well.

1884
02:08:03,920 --> 02:08:07,920
But I think, I mean, I think the sort of alignment angle during the intelligence explosion,

1885
02:08:07,920 --> 02:08:10,640
it's going to, you know, it's not a process of like years of bureaucracy,

1886
02:08:10,640 --> 02:08:12,080
and then you can kind of write some standards.

1887
02:08:12,720 --> 02:08:16,160
I think it looks much more like basically a war and like you have a fog of war.

1888
02:08:16,160 --> 02:08:18,480
It's like, look, it's like, is it safe to do the next oom?

1889
02:08:18,480 --> 02:08:21,280
You know, and it's like, ah, you know, like, you know, we're like three ooms into the

1890
02:08:21,280 --> 02:08:22,080
intelligence explosion.

1891
02:08:22,080 --> 02:08:23,840
We don't really understand what's going on anymore.

1892
02:08:25,040 --> 02:08:29,600
You know, the, you know, like a bunch of our like generalization scaling curves are like,

1893
02:08:29,600 --> 02:08:30,960
kind of looking not great.

1894
02:08:30,960 --> 02:08:33,680
You know, some of our like automated AI researchers that are doing alignment are

1895
02:08:33,680 --> 02:08:35,680
saying it's fine, but we don't quite trust them.

1896
02:08:35,680 --> 02:08:39,280
In this test, you know, the like, the eyes started doing naughty things and,

1897
02:08:39,280 --> 02:08:41,680
ah, but then we like hammered it out and then it was fine.

1898
02:08:41,680 --> 02:08:43,760
And like, ah, should we, should we go ahead?

1899
02:08:43,760 --> 02:08:45,280
Should we take, you know, another six months?

1900
02:08:45,360 --> 02:08:47,680
Also, by the way, you know, like China just stole the weights.

1901
02:08:47,680 --> 02:08:49,760
Are we, you know, they're about to like deploy the rumor army.

1902
02:08:49,760 --> 02:08:50,400
Like what do we do?

1903
02:08:50,400 --> 02:08:52,960
I think it's this, I think it is this crazy situation.

1904
02:08:54,480 --> 02:08:59,840
And, um, you know, basically you, you were lying much more on kind of like a sane chain

1905
02:08:59,840 --> 02:09:04,160
of command than you are on sort of some like, you know, the Libertive Regulatory Scheme.

1906
02:09:04,160 --> 02:09:06,960
I wish you had, you were able to do the Libertive Regulatory Scheme.

1907
02:09:06,960 --> 02:09:08,720
And this is the thing about the private companies too.

1908
02:09:08,720 --> 02:09:13,280
I don't think, you know, they all claim they're going to do safety, but

1909
02:09:13,680 --> 02:09:17,680
I think it's really rough when you're in the commercial race and they're startups,

1910
02:09:17,680 --> 02:09:20,640
you know, and startups, startups or startups, you know,

1911
02:09:20,640 --> 02:09:22,400
I think they're not fit to handle WMDs.

1912
02:09:23,600 --> 02:09:25,440
Yeah, I'm coming closer to your position.

1913
02:09:26,960 --> 02:09:32,160
But part of me also, so with the responsible scaling policies,

1914
02:09:32,160 --> 02:09:35,920
I was told that people who are advancing that, that the way to think about this,

1915
02:09:35,920 --> 02:09:38,000
because they know I'm like a libertarian type of person.

1916
02:09:38,000 --> 02:09:38,560
Yeah, yeah, yeah.

1917
02:09:38,560 --> 02:09:45,440
And the way they approached me about it was that fundamentally this is a way to protect

1918
02:09:46,080 --> 02:09:51,120
market-based development of AGI in the sense that if you didn't have this at all,

1919
02:09:51,120 --> 02:09:54,880
then you would have the sort of misuse and then you would have to be nationalized.

1920
02:09:54,880 --> 02:09:59,120
And the RSPs are a way to make sure that through this deployment,

1921
02:09:59,120 --> 02:10:01,040
you can still have a market-based order.

1922
02:10:01,040 --> 02:10:04,480
But then there's these safeguards that make sure that things don't go off the rails.

1923
02:10:05,200 --> 02:10:12,560
And I wonder if it seems like your story seems self-consistent,

1924
02:10:13,200 --> 02:10:18,400
but it does feel, I know this was never your position, so I'm not looping you into this,

1925
02:10:18,400 --> 02:10:22,960
but sort of modern Bailey almost in the sense of...

1926
02:10:23,760 --> 02:10:26,000
Well, look, here's what I think about RSP-type stuff

1927
02:10:26,000 --> 02:10:28,160
or sort of safety regulation that's happening now.

1928
02:10:28,160 --> 02:10:30,720
I think they're important for helping us figure out what world we're in

1929
02:10:30,720 --> 02:10:33,200
and like flashing the warning signs when we're close, right?

1930
02:10:33,200 --> 02:10:38,800
And so the story we've been telling is sort of what I think the modal version of this decade is,

1931
02:10:38,800 --> 02:10:40,640
but it's like, I think there's lots of ways it could be wrong.

1932
02:10:40,640 --> 02:10:42,480
I really... We should talk about the data a while more.

1933
02:10:42,480 --> 02:10:45,360
I think there's like, again, I think there's a world where the stuff stagnates, right?

1934
02:10:45,360 --> 02:10:46,720
There's a world where we don't have AGI.

1935
02:10:47,920 --> 02:10:51,440
And so basically the RSP thing is preserving the optionality,

1936
02:10:51,440 --> 02:10:54,160
let's see how this stuff goes, but we need to be prepared.

1937
02:10:54,160 --> 02:10:57,520
Like if the red lights start flashing, if we're getting the automated eye researcher,

1938
02:10:57,520 --> 02:11:00,160
then it's like, and it's crunch time, and then it's time to go.

1939
02:11:00,160 --> 02:11:04,480
I think, okay, I can be on the same page on that, that we should have a very,

1940
02:11:04,480 --> 02:11:07,520
very strong prior on a proceeding in a market-based way,

1941
02:11:07,520 --> 02:11:12,640
unless you're right about what the explosion looks like, the intelligence explosion.

1942
02:11:12,640 --> 02:11:18,400
And so like, I don't move yet, but in that world where like really does seem like

1943
02:11:18,400 --> 02:11:24,000
Alec Radford can be automated, and that is the only bottleneck to getting TSI.

1944
02:11:24,000 --> 02:11:25,200
Okay, I think we can leave it at that.

1945
02:11:26,160 --> 02:11:30,240
I can, yeah, I am somewhat of the way there.

1946
02:11:30,240 --> 02:11:32,880
Okay, okay. I hope it goes well.

1947
02:11:34,000 --> 02:11:36,240
It's gonna be, ah, very stressful.

1948
02:11:36,240 --> 02:11:37,680
And again, right now is the chill time.

1949
02:11:39,760 --> 02:11:41,040
Enjoy your vacation a lot less.

1950
02:11:41,600 --> 02:11:45,680
It's funny to look out over, just like, this is San Francisco.

1951
02:11:45,680 --> 02:11:46,960
Yeah, yeah, yeah.

1952
02:11:46,960 --> 02:11:49,120
Open the eyes right there, you know, anthropics there.

1953
02:11:49,120 --> 02:11:51,120
I mean, again, this is kind of like, you know, it's like,

1954
02:11:51,120 --> 02:11:54,960
you guys have this enormous power over how it's gonna go for the next couple of years,

1955
02:11:54,960 --> 02:11:56,560
and that power is depreciating.

1956
02:11:57,680 --> 02:11:58,320
Who's you guys?

1957
02:11:58,880 --> 02:12:00,080
Like, you know, people at labs.

1958
02:12:00,080 --> 02:12:00,640
Yeah, yeah, yeah.

1959
02:12:01,760 --> 02:12:02,960
But it is a sort of crazy world.

1960
02:12:02,960 --> 02:12:05,120
And you're talking about like, you know, I feel like you talk about like,

1961
02:12:05,120 --> 02:12:06,560
oh, maybe they'll nationalize too soon.

1962
02:12:06,560 --> 02:12:11,120
It's like, you know, almost nobody like really like feels it, sees what's happening.

1963
02:12:11,120 --> 02:12:14,240
And it's, I think this is the thing that I find stressful about all the stuff is like,

1964
02:12:14,240 --> 02:12:15,440
look, maybe I'm wrong.

1965
02:12:15,440 --> 02:12:18,560
Like if I'm right, we're in this crazy situation where there's like, you know,

1966
02:12:18,560 --> 02:12:20,720
like a few hundred guys that are like paying attention.

1967
02:12:22,800 --> 02:12:24,480
And it's daunting.

1968
02:12:24,960 --> 02:12:26,880
I went to Washington a few months ago.

1969
02:12:26,880 --> 02:12:30,800
And I was talking to some people who are doing AI policy stuff there.

1970
02:12:30,800 --> 02:12:33,200
And I was asking them how likely they think nationalization is.

1971
02:12:34,080 --> 02:12:37,920
And they said, oh, you know, like, it's really hard to nationalize stuff.

1972
02:12:37,920 --> 02:12:39,200
It's been a long time since we've done it.

1973
02:12:39,200 --> 02:12:43,520
There's these very specific procedural constraints on what kinds of things can be nationalized.

1974
02:12:44,400 --> 02:12:49,760
And then I was asked, well, like ASI, so that means because there's,

1975
02:12:49,760 --> 02:12:53,280
there's constraints at a defense production act or whatever that won't be nationalized.

1976
02:12:53,600 --> 02:12:54,800
The Supreme Court would overturn that.

1977
02:12:55,760 --> 02:12:58,320
And they're like, yeah, I guess that would be nationalized.

1978
02:13:00,720 --> 02:13:04,080
That's the short summary of my post or my view on the project.

1979
02:13:10,320 --> 02:13:14,160
Okay. So before we go further on the ASF, let's just back off.

1980
02:13:16,000 --> 02:13:16,960
We began the conversation.

1981
02:13:16,960 --> 02:13:17,920
I think people will be confused.

1982
02:13:17,920 --> 02:13:20,640
You graduated valedictorian of Columbia when you were 19.

1983
02:13:20,640 --> 02:13:22,480
So you got to college when you were 15.

1984
02:13:23,040 --> 02:13:25,600
And you were in Germany, then you got to college at 15.

1985
02:13:27,040 --> 02:13:28,000
How the fuck did that happen?

1986
02:13:29,520 --> 02:13:30,960
I really wanted out of Germany.

1987
02:13:34,160 --> 02:13:36,240
I went to kind of a German public school.

1988
02:13:36,240 --> 02:13:38,160
It was not a good environment for me.

1989
02:13:41,040 --> 02:13:41,600
In what sense?

1990
02:13:41,600 --> 02:13:42,720
There's just like no peers.

1991
02:13:43,440 --> 02:13:45,600
Yeah, look, I mean, it was, yeah, it was, you know,

1992
02:13:46,560 --> 02:13:49,440
there's, I mean, there's also just a sense in which sort of like,

1993
02:13:49,440 --> 02:13:51,200
there's this particular sort of German cultural sense.

1994
02:13:51,200 --> 02:13:53,360
I think in the US, you know, there's all these like amazing high schools

1995
02:13:53,360 --> 02:13:55,040
and like sort of an appreciation of excellence.

1996
02:13:55,040 --> 02:13:58,800
And in Germany, there's really this sort of like Paul Poppy syndrome of us, right?

1997
02:13:58,800 --> 02:14:01,920
Where it's, you know, you're the curious kid in class and you want to learn more

1998
02:14:01,920 --> 02:14:03,920
instead of the teacher being like, ah, that's great.

1999
02:14:03,920 --> 02:14:05,440
They're like, they kind of resent you for it.

2000
02:14:05,440 --> 02:14:06,640
And they're like trying to crush you.

2001
02:14:07,920 --> 02:14:10,000
I mean, there's also like, there's no kind of like elite universities

2002
02:14:10,000 --> 02:14:11,520
for undergraduate, which is kind of crazy.

2003
02:14:13,520 --> 02:14:15,680
So, you know, the sort of, you know, there's sort of like,

2004
02:14:16,240 --> 02:14:20,080
basically like the meritocracy was kind of crushed in Germany at some point.

2005
02:14:21,200 --> 02:14:24,240
Also, I mean, there's a sort of incredible sense of complacency,

2006
02:14:25,920 --> 02:14:27,760
you know, across the board.

2007
02:14:27,760 --> 02:14:30,400
I mean, one of the things that always puzzles me is like, you know,

2008
02:14:31,120 --> 02:14:34,080
even just going to a US college was just kind of like radical act.

2009
02:14:34,080 --> 02:14:36,880
And like, you know, it doesn't seem radical to anyone here because it's like,

2010
02:14:36,880 --> 02:14:39,840
ah, this is obviously the thing you do and you can go to Columbia, you go to Columbia.

2011
02:14:39,840 --> 02:14:41,920
But it's, you know, it is very unusual.

2012
02:14:41,920 --> 02:14:44,720
And it's, it's, it's wild to me because it's like, you know,

2013
02:14:44,720 --> 02:14:45,920
this is where stuff is happening.

2014
02:14:45,920 --> 02:14:47,520
You can get so much of a better education.

2015
02:14:47,520 --> 02:14:50,960
And, you know, like America is where, you know, it's where, where, where,

2016
02:14:50,960 --> 02:14:53,920
where all the stuff is and people don't do it.

2017
02:14:53,920 --> 02:14:59,520
And, and so, um, yeah, anyway, so I, you know, I know I skipped a few grades and,

2018
02:14:59,520 --> 02:15:03,120
and, you know, I think, um, at the time it seemed very normal to me to kind of like

2019
02:15:03,120 --> 02:15:05,280
go to college and come to America.

2020
02:15:05,280 --> 02:15:10,080
I think, um, you know, now one of my sisters is now like turning 15, you know.

2021
02:15:10,080 --> 02:15:12,080
And so then I, you know, and I look at her and I'm like,

2022
02:15:12,880 --> 02:15:14,080
now I understand how my mother.

2023
02:15:14,880 --> 02:15:19,040
And as you get to college, you're like presumably the only 15 year old.

2024
02:15:19,040 --> 02:15:19,840
Yeah, yeah.

2025
02:15:19,840 --> 02:15:22,000
As it was just like normal for you to be a 15 year old.

2026
02:15:22,000 --> 02:15:23,520
Like, what was the initial years?

2027
02:15:23,520 --> 02:15:24,400
It felt so normal at the time.

2028
02:15:24,400 --> 02:15:25,040
You know, I didn't, yeah.

2029
02:15:25,040 --> 02:15:27,440
So yeah, it's like, now I understand why my mother's worried.

2030
02:15:27,440 --> 02:15:30,080
And, you know, I think, you know, I worked, I worked on my parents for a while.

2031
02:15:30,080 --> 02:15:32,160
You know, eventually I was, you know, I persuaded them.

2032
02:15:32,160 --> 02:15:34,240
No, but yeah, it felt, felt very normal at the time.

2033
02:15:34,240 --> 02:15:34,800
And it was great.

2034
02:15:34,800 --> 02:15:37,200
It was also great because I, you know, I actually really like college, right?

2035
02:15:37,840 --> 02:15:39,840
And in some sense it sort of came at the right time for me.

2036
02:15:40,480 --> 02:15:45,360
Where, you know, I, I mean, I, you know, for example, I really appreciate the sort of like

2037
02:15:45,360 --> 02:15:48,240
liberal arts education and, you know, like the core curriculum and reading sort of

2038
02:15:48,240 --> 02:15:50,880
core works of political philosophy and, and literature.

2039
02:15:50,880 --> 02:15:52,400
And you did what you can.

2040
02:15:52,400 --> 02:15:56,000
And I mean, my majors were math and statistics and economics.

2041
02:15:57,200 --> 02:16:00,800
But, you know, Columbia has a sort of pretty heavy core curriculum and liberal arts education.

2042
02:16:00,800 --> 02:16:03,200
And honestly, like, you know, I shouldn't have done all the majors.

2043
02:16:03,200 --> 02:16:05,920
I should have just, I mean, the best courses were sort of the courses where it's like,

2044
02:16:05,920 --> 02:16:08,320
there's some amazing professor and it's some history class.

2045
02:16:08,400 --> 02:16:13,200
And it's, I mean, that's, that's honestly the thing I would recommend people spend their time

2046
02:16:13,200 --> 02:16:13,920
on in college.

2047
02:16:14,800 --> 02:16:16,560
Was there one professor or class that stood out that way?

2048
02:16:17,280 --> 02:16:23,040
I mean, if you, there's like a class by Richard Betz on war, peace and strategy.

2049
02:16:23,920 --> 02:16:25,600
Adam too is obviously fantastic.

2050
02:16:27,040 --> 02:16:28,800
And, you know, has written very riveting books.

2051
02:16:29,600 --> 02:16:30,160
Yeah.

2052
02:16:30,160 --> 02:16:31,440
You should have them on the podcast, by the way.

2053
02:16:31,440 --> 02:16:31,920
I've tried.

2054
02:16:31,920 --> 02:16:32,240
Okay.

2055
02:16:32,240 --> 02:16:32,800
Try it.

2056
02:16:32,800 --> 02:16:34,160
I think you try it for me.

2057
02:16:34,160 --> 02:16:35,760
Yeah, you gotta give it on the pod.

2058
02:16:35,760 --> 02:16:36,160
Yeah.

2059
02:16:36,160 --> 02:16:37,040
Oh, it'd be so good.

2060
02:16:38,640 --> 02:16:38,960
Okay.

2061
02:16:38,960 --> 02:16:46,320
So then in a couple of years, we were talking to Tyler Cowan recently and he said that when,

2062
02:16:46,320 --> 02:16:52,000
the way we, he first encountered you was you wrote this paper on economic growth and existential

2063
02:16:52,000 --> 02:16:57,440
risk and he said, I, when I found, read it, I couldn't believe that a 17 year old had written

2064
02:16:57,440 --> 02:16:57,760
it.

2065
02:16:57,760 --> 02:17:01,040
I thought if this was a MIT dissertation, I'd be impressed.

2066
02:17:01,040 --> 02:17:06,160
So you were like, how did you go from your, I guess we were the junior of them,

2067
02:17:07,040 --> 02:17:12,000
you're writing, you're writing, you know, pretty novel economic papers.

2068
02:17:13,600 --> 02:17:15,680
Why did you get interested in this, this kind of thing?

2069
02:17:15,680 --> 02:17:17,280
And what was the process to get in that?

2070
02:17:18,880 --> 02:17:19,280
I don't know.

2071
02:17:19,280 --> 02:17:21,040
I just, you know, I get interested in things in some sense.

2072
02:17:21,040 --> 02:17:23,280
It's sort of like, it feels very natural to me.

2073
02:17:23,280 --> 02:17:24,560
It's like, I get excited about a thing.

2074
02:17:24,560 --> 02:17:25,200
I read about it.

2075
02:17:25,200 --> 02:17:25,920
I immerse myself.

2076
02:17:25,920 --> 02:17:29,120
I think I can, you know, I can learn information very quickly and understand it.

2077
02:17:30,240 --> 02:17:35,200
The, I mean, I think to the paper, I mean, I think one actual, at least for the way I work,

2078
02:17:35,200 --> 02:17:39,200
I feel like sort of moments of peak productivity matter much more than sort of average productivity.

2079
02:17:39,200 --> 02:17:42,080
I think there's some jobs, you know, like CUO or something, you know, like average

2080
02:17:42,080 --> 02:17:43,360
productivity really matters.

2081
02:17:43,360 --> 02:17:47,520
But I think there's sort of a, I often feel like I have periods of like, you know,

2082
02:17:47,520 --> 02:17:50,560
there's some, there's a couple months where there's sort of nephrolessence and I'm like,

2083
02:17:50,560 --> 02:17:53,520
you know, and the other times I'm sort of computing stuff in the background.

2084
02:17:53,520 --> 02:17:56,400
And at some point, you know, like writing the series, this is also kind of similar.

2085
02:17:56,400 --> 02:18:00,400
And it's just like you, you write it and, and it's, it's like, it's really flowing.

2086
02:18:00,400 --> 02:18:02,800
And that's sort of what ends up mattering.

2087
02:18:02,880 --> 02:18:06,480
I think even for CEOs, it might be the case that the peak productivity is very important.

2088
02:18:06,480 --> 02:18:11,520
There's one of our following chat-off-house rules, one of our friends in a group chat

2089
02:18:11,520 --> 02:18:17,040
has pointed out how many famous CEOs and founders have been bipolar manic,

2090
02:18:17,920 --> 02:18:21,840
which is very much the peak, like the call option on your productivity is the most

2091
02:18:21,840 --> 02:18:25,520
important thing and you get it by just increasing the volatility through bipolar.

2092
02:18:27,120 --> 02:18:28,560
Okay. So that's interesting.

2093
02:18:28,560 --> 02:18:30,800
And so you get interested in economics first.

2094
02:18:30,800 --> 02:18:31,840
First of all, why economics?

2095
02:18:31,840 --> 02:18:33,520
Like you could read about anything at this move.

2096
02:18:33,520 --> 02:18:36,560
Like you, if you wanted, you know, you kind of got a slow start on them.

2097
02:18:39,120 --> 02:18:42,560
You reached it all these years on the econ, there's an alternative world where you're

2098
02:18:42,560 --> 02:18:45,600
like on the super alignment team at 17 instead of 21 or whatever it was.

2099
02:18:51,760 --> 02:18:53,920
I mean, in some sense, I'm still doing economics, right?

2100
02:18:53,920 --> 02:18:55,760
You know, what is, what is straight lines on a graph?

2101
02:18:55,760 --> 02:18:59,040
I'm looking at the log-log thoughts and like figuring out what the trends are

2102
02:18:59,040 --> 02:19:02,240
and like thinking about the feedback loops and equilibrium arms control dynamics.

2103
02:19:02,240 --> 02:19:06,560
And, you know, it's, I think it is a sort of a way of thinking that I find very useful.

2104
02:19:07,680 --> 02:19:12,640
And, you know, like what, you know, Dario and Ilya seeing scaling early in some sense,

2105
02:19:12,640 --> 02:19:13,920
that is a sort of very economic way.

2106
02:19:13,920 --> 02:19:16,400
And also the sort of physics, kind of like empirical physics, you know,

2107
02:19:16,400 --> 02:19:17,200
a lot of them are physicists.

2108
02:19:17,200 --> 02:19:19,840
I think the economists usually can't code well enough and that's their issue.

2109
02:19:19,840 --> 02:19:21,920
But I think it's that sort of way of thinking.

2110
02:19:22,880 --> 02:19:27,600
I mean, the other thing is, you know, I thought they were sort of, you know, I thought of a lot

2111
02:19:27,600 --> 02:19:30,000
of the sort of like core ideas of economics.

2112
02:19:30,000 --> 02:19:30,960
I thought we're just beautiful.

2113
02:19:32,160 --> 02:19:34,640
And, you know, in some sense, I feel like I was a little duped, you know,

2114
02:19:34,640 --> 02:19:37,040
where it's like actually econ academia is kind of decadent now.

2115
02:19:37,040 --> 02:19:40,080
You know, I think that, you know, for example, the paper I wrote, you know,

2116
02:19:40,080 --> 02:19:43,120
it's sort of, I think the takeaway, you know, it's a long paper,

2117
02:19:43,120 --> 02:19:44,480
it's 100 pages of math or whatever.

2118
02:19:44,480 --> 02:19:48,560
I think the core takeaway I can, you know, kind of give the core intuition for in like,

2119
02:19:48,560 --> 02:19:50,320
you know, 30 seconds and it makes sense.

2120
02:19:50,320 --> 02:19:52,480
And it's, and it's like, you don't actually need the math.

2121
02:19:52,480 --> 02:19:56,320
I think that's the sort of the best pieces of economics are like that where you do the work,

2122
02:19:56,400 --> 02:20:00,960
but you do the work to kind of uncover insights that weren't obvious to you before.

2123
02:20:00,960 --> 02:20:04,640
Once, once you've done the work, it's like some sort of like mechanism falls out of it

2124
02:20:04,640 --> 02:20:08,880
that like makes a lot of crisp intuitive sense that like explains some facts about the world

2125
02:20:08,880 --> 02:20:10,160
that you can then use in arguments.

2126
02:20:10,160 --> 02:20:13,120
And I think, you know, I think, you know, like a lot of econ one-on-one like this,

2127
02:20:13,120 --> 02:20:13,680
and it's great.

2128
02:20:13,680 --> 02:20:18,240
A lot of econ in the, you know, the 50s and the 60s, you know, was like this.

2129
02:20:18,240 --> 02:20:21,040
And, you know, Chad Jones papers are often like this.

2130
02:20:21,040 --> 02:20:22,960
I really like Chad Jones papers for this.

2131
02:20:22,960 --> 02:20:29,600
You know, I think, you know, why did I ultimately not pursue econ academia was number of reasons.

2132
02:20:29,600 --> 02:20:30,640
One of them was Tyler Cowan.

2133
02:20:33,280 --> 02:20:36,240
You know, he kind of took me aside and he was kind of like, look, I think you're one of the

2134
02:20:36,240 --> 02:20:39,280
like top young economists I've ever met, but also you should probably not go to grad school.

2135
02:20:39,280 --> 02:20:39,840
Oh, interesting.

2136
02:20:39,840 --> 02:20:41,040
Yeah, I didn't realize that.

2137
02:20:41,040 --> 02:20:41,280
Well, yeah.

2138
02:20:41,280 --> 02:20:44,800
And it was, it was, it was good because he kind of introduced me to the, you know,

2139
02:20:44,800 --> 02:20:48,880
I don't know, like the Twitter weirdos or just like, you know, and I think the takeaway from that

2140
02:20:48,880 --> 02:20:52,000
was kind of, you know, got to move out last one more time.

2141
02:20:52,080 --> 02:20:53,840
Wait, Tyler introduced you to the Twitter weirdos?

2142
02:20:53,840 --> 02:20:54,320
A little bit.

2143
02:20:54,320 --> 02:20:54,480
Yeah.

2144
02:20:54,480 --> 02:20:55,840
Or just kind of like the sort of brought, you know,

2145
02:20:55,840 --> 02:20:59,920
Like the 60 year old, the old economist in GCT that Twitter.

2146
02:20:59,920 --> 02:21:00,480
Yeah.

2147
02:21:00,480 --> 02:21:03,840
Well, you know, I had been, I had, so I went from Germany, you know, completely, you know,

2148
02:21:03,840 --> 02:21:08,080
on the periphery to kind of like, you know, in the U.S. elite institution and sort of got some

2149
02:21:08,080 --> 02:21:12,720
vibe of like sort of, you know, meritocratic elite, you know, U.S. society.

2150
02:21:12,720 --> 02:21:16,240
And then sort of, yeah, basically this sort of like, there was a sort of directory then to

2151
02:21:16,240 --> 02:21:18,640
being like, look, I, you know, find the true American spirit.

2152
02:21:18,640 --> 02:21:19,440
I got to come out here.

2153
02:21:19,920 --> 02:21:23,200
The other reason I didn't become an economist was because, or at least econ academia,

2154
02:21:23,200 --> 02:21:25,760
was because I think sort of econ academia has become a bit decadent.

2155
02:21:25,760 --> 02:21:29,040
And maybe it's just ideas getting harder to find and maybe it's sort of things, you know,

2156
02:21:29,040 --> 02:21:31,360
and the sort of beautiful, simple things have been discovered.

2157
02:21:31,360 --> 02:21:33,200
But you know, like what are econ papers these days?

2158
02:21:33,200 --> 02:21:38,480
You know, it's like, you know, it's like 200 pages of like empirical analyses on what happened

2159
02:21:38,480 --> 02:21:41,840
when, you know, like Wisconsin bought, you know, 100,000 more textbooks on like educational

2160
02:21:41,840 --> 02:21:42,320
outcomes.

2161
02:21:42,320 --> 02:21:44,080
And I'm really happy that work happened.

2162
02:21:44,080 --> 02:21:46,800
I think it's important work, but I think it is not in government and covering these

2163
02:21:46,800 --> 02:21:50,160
sort of like fundamental insights and sort of mechanisms in society.

2164
02:21:51,440 --> 02:21:54,720
Or, you know, it's like, even the theory work is kind of like, here's a really complicated

2165
02:21:54,720 --> 02:21:58,800
model and the model spits out, you know, if the Fed does X, you know, then Y happens,

2166
02:21:58,800 --> 02:22:00,640
you have no idea what that hat, why that happened?

2167
02:22:00,640 --> 02:22:03,680
Because it's like gazillion parameters and they're all calibrated in some way.

2168
02:22:03,680 --> 02:22:05,280
And it's some computer simulation.

2169
02:22:05,280 --> 02:22:07,360
You have no idea about the validity, you know, yeah.

2170
02:22:07,360 --> 02:22:10,880
So I think, I think the sort of, you know, the most important insights are the ones where

2171
02:22:10,880 --> 02:22:12,320
you have to do a lot of work to get them.

2172
02:22:12,320 --> 02:22:14,240
But then there's this crisp intuition.

2173
02:22:14,560 --> 02:22:14,720
Yeah.

2174
02:22:14,720 --> 02:22:21,040
The P versus NP of, that's really interesting.

2175
02:22:21,040 --> 02:22:26,480
So just going back to your time in college, you say that peak productivity kind of explains

2176
02:22:26,480 --> 02:22:31,840
the, this paper and things, but the valedictorian, that's getting straight A's or whatever is very

2177
02:22:31,840 --> 02:22:35,680
much average productivity phenomenon.

2178
02:22:35,680 --> 02:22:36,320
Right.

2179
02:22:36,320 --> 02:22:40,400
So there's one award for the highest GPA, which I won, but the valedictorian is like among the

2180
02:22:40,400 --> 02:22:43,600
people which have the highest GPA and then like selected by faculty.

2181
02:22:43,680 --> 02:22:44,000
Okay.

2182
02:22:44,000 --> 02:22:44,480
Yeah.

2183
02:22:44,480 --> 02:22:47,040
So it's just not, but it's not just peak productivity.

2184
02:22:47,040 --> 02:22:49,920
It's just, it's just, I generally just love this stuff.

2185
02:22:49,920 --> 02:22:53,120
You know, I just, I was curious and I thought it was really interesting and I love learning

2186
02:22:53,120 --> 02:22:57,920
about it and, and I love kind of like, it made sense to me and, you know, it was very natural.

2187
02:22:57,920 --> 02:23:01,840
And so, you know, I think I'm, you know, I'm not, you know, I think one of my faults is

2188
02:23:01,840 --> 02:23:03,440
I'm not that good at eating glass or whatever.

2189
02:23:03,440 --> 02:23:04,880
I think there's some people who are very good at it.

2190
02:23:04,880 --> 02:23:09,200
I think the sort of like, the sort of moments of peak productivity come when I, you know,

2191
02:23:09,200 --> 02:23:13,040
I'm just really excited and engaged and, and, and, and love it.

2192
02:23:13,040 --> 02:23:18,080
And, you know, I, I, you know, if you take like courses, you know, that's what you got in college.

2193
02:23:18,080 --> 02:23:21,120
And it's, it's, it's, it's the Bruce Banner code and Avengers.

2194
02:23:21,760 --> 02:23:23,120
You know, I'm always angry.

2195
02:23:24,080 --> 02:23:24,960
I'm always excited.

2196
02:23:24,960 --> 02:23:25,920
I'm always curious.

2197
02:23:25,920 --> 02:23:27,200
That's why I'm always the equitability.

2198
02:23:29,040 --> 02:23:32,560
So it's interesting, by the way, when you were in college, I was also in college.

2199
02:23:32,560 --> 02:23:38,480
I think you were, despite being a year younger than me, I think you're ahead in college than

2200
02:23:38,480 --> 02:23:40,080
me or at least two years, maybe two years ahead.

2201
02:23:40,720 --> 02:23:44,240
And we met around this time.

2202
02:23:44,240 --> 02:23:45,360
Yeah, yeah, yeah.

2203
02:23:45,360 --> 02:23:48,800
We also met, I think through the Tyler Cowan universe.

2204
02:23:48,800 --> 02:23:49,520
Yeah, yeah, yeah.

2205
02:23:49,520 --> 02:23:52,480
And it's very insane how small the world is.

2206
02:23:52,480 --> 02:23:54,000
I think I, did I reach out to you?

2207
02:23:54,000 --> 02:23:54,800
I must have.

2208
02:23:54,800 --> 02:23:54,960
Yeah.

2209
02:23:54,960 --> 02:24:00,400
About when I had a couple of videos and they had a couple hundred views or something.

2210
02:24:00,400 --> 02:24:00,960
Yeah.

2211
02:24:00,960 --> 02:24:02,160
It's a small world.

2212
02:24:02,160 --> 02:24:04,080
I mean, this is the crazy thing about the eye world, right?

2213
02:24:04,080 --> 02:24:08,560
It's kind of like, it's the same few people at the kind of SF parties and they're the ones,

2214
02:24:09,040 --> 02:24:12,400
running the models at DeepMind and OpenAI and Anthropic.

2215
02:24:12,400 --> 02:24:16,800
And I mean, I think some other friends of ours have mentioned this,

2216
02:24:16,800 --> 02:24:20,480
who are now later in their career and very successful, that they actually met all the

2217
02:24:20,480 --> 02:24:23,040
people who are also kind of very successful in Silicon Valley now.

2218
02:24:23,040 --> 02:24:26,800
Like when they're in their 20s or when they're really 20.

2219
02:24:29,120 --> 02:24:31,680
I mean, look, I actually think, why is it a small world?

2220
02:24:32,320 --> 02:24:36,560
I mean, I think one of the things is some amount of some sort of agency.

2221
02:24:37,520 --> 02:24:42,640
And I think in a funny way, this is a thing I sort of took away from the sort of Germany

2222
02:24:42,640 --> 02:24:46,560
experience where it was, I mean, look, I, I, it was crushing.

2223
02:24:46,560 --> 02:24:47,760
I really didn't like it.

2224
02:24:47,760 --> 02:24:52,080
And it was like, it was such an unusual move to kind of skip grades and such an unusual move to

2225
02:24:52,080 --> 02:24:52,880
come to the United States.

2226
02:24:52,880 --> 02:24:55,840
And, you know, a lot of these things I did were kind of unusual moves.

2227
02:24:55,840 --> 02:25:02,640
And, you know, there's some amount where like, just like, just trying to do it.

2228
02:25:02,640 --> 02:25:04,000
And then it was fine.

2229
02:25:04,000 --> 02:25:04,560
And it worked.

2230
02:25:05,680 --> 02:25:08,800
That kind of reinforced, like, you know, you don't, you don't just have to kind of conform

2231
02:25:08,800 --> 02:25:10,080
to what the opportune window is.

2232
02:25:10,080 --> 02:25:12,640
You can just kind of like try to do the thing, the thing that seems right to you.

2233
02:25:13,680 --> 02:25:16,800
And like, you know, most people can be wrong and I know things like that.

2234
02:25:16,800 --> 02:25:20,240
And I think that was kind of a, you know, valuable kind of like early experience,

2235
02:25:20,240 --> 02:25:21,200
those sort of formative.

2236
02:25:21,840 --> 02:25:22,160
Okay.

2237
02:25:22,160 --> 02:25:23,760
So after college, what did you do?

2238
02:25:24,400 --> 02:25:27,360
I did econ research for a little bit, you know, Oxford and stuff.

2239
02:25:27,360 --> 02:25:29,520
And then, then I worked at Future Fund.

2240
02:25:30,320 --> 02:25:30,880
Yeah.

2241
02:25:30,880 --> 02:25:31,200
Okay.

2242
02:25:31,200 --> 02:25:32,880
So, and so tell me about it.

2243
02:25:35,440 --> 02:25:39,440
Future Fund was, you know, it was a foundation that was, you know, funded by

2244
02:25:39,440 --> 02:25:40,320
San Bank and Freed.

2245
02:25:40,320 --> 02:25:42,800
I mean, we were our own thing, you know, we were based in the Bay.

2246
02:25:44,080 --> 02:25:46,560
You know, at the time, this was in sort of early 22.

2247
02:25:48,160 --> 02:25:50,960
It was, it was this just like incredibly exciting opportunity, right?

2248
02:25:50,960 --> 02:25:54,480
It was basically like a startup, you know, foundation, which is like, you know, it doesn't

2249
02:25:54,480 --> 02:25:57,680
come along that, that often that, you know, we thought we'd be able to give away billions

2250
02:25:57,680 --> 02:26:01,120
of dollars, you know, thought we'd be able to kind of like, you know, remake how philanthropy

2251
02:26:01,120 --> 02:26:05,040
is done, you know, from first principles, thought we'd be able to have, you know,

2252
02:26:05,040 --> 02:26:08,880
this like great impact, you know, we, the causes we focused on were, you know,

2253
02:26:08,880 --> 02:26:15,040
biosecurity, you know, AI, you know, finding exceptional talent and putting them to work

2254
02:26:15,040 --> 02:26:15,840
on hard problems.

2255
02:26:17,360 --> 02:26:20,000
And, you know, like a lot of the stuff we did, I was, I was really excited about,

2256
02:26:20,000 --> 02:26:23,120
you know, like academics who would, you know, usually take six months would send us emails

2257
02:26:23,120 --> 02:26:24,080
like, ah, you know, this is great.

2258
02:26:24,080 --> 02:26:27,360
This is so quick and, you know, and straightforward, you know, in general,

2259
02:26:27,360 --> 02:26:30,560
I feel like I've often find that with like, you know, a little bit of encouragement, a little

2260
02:26:30,560 --> 02:26:34,080
bit of sort of empowerment, kind of like removing excuses, making the process easy,

2261
02:26:34,080 --> 02:26:36,560
you know, you can kind of like get people to do great things.

2262
02:26:37,760 --> 02:26:42,800
I think on the future front, the thing is context for people who might not realize,

2263
02:26:43,680 --> 02:26:48,320
not only were you guys planning on deploying billions of dollars, but it was a team of four

2264
02:26:48,320 --> 02:26:48,800
people.

2265
02:26:48,800 --> 02:26:49,600
Yeah, yeah, yeah.

2266
02:26:49,600 --> 02:26:55,600
So you at 18 are on a team of four people that is in charge of deploying billions of dollars.

2267
02:26:55,600 --> 02:26:56,080
Yeah.

2268
02:26:56,080 --> 02:26:58,240
I mean, just, I mean, yeah, I'm a future fund, you know, the,

2269
02:26:59,360 --> 02:27:01,840
yeah, I mean, the, you know, so that was, that was sort of the heyday, right?

2270
02:27:02,480 --> 02:27:05,680
And then obviously, you know, when, when in sort of, you know, November of 22,

2271
02:27:07,520 --> 02:27:10,880
you know, it was kind of revealed that Sam was this, you know, giant fraud.

2272
02:27:10,880 --> 02:27:13,280
And from one day to the next, you know, the whole thing collapsed.

2273
02:27:15,040 --> 02:27:16,480
That was just really tough.

2274
02:27:16,480 --> 02:27:18,400
I mean, you know, obviously it was devastating.

2275
02:27:18,400 --> 02:27:21,280
It was devastating, obviously for the people at their money on FTX,

2276
02:27:21,840 --> 02:27:26,400
you know, closer to home, you know, all the, you know, all these grantees, you know,

2277
02:27:26,400 --> 02:27:28,960
we'd wanted to help them and we thought they were doing amazing projects.

2278
02:27:28,960 --> 02:27:32,400
And so, but instead of helping them, we ended up saddling them with like a giant problem.

2279
02:27:33,920 --> 02:27:35,920
You know, personally, it was, you know, it was a startup, right?

2280
02:27:35,920 --> 02:27:38,480
And so I, you know, I'd worked 70 hour weeks every week for, you know,

2281
02:27:38,480 --> 02:27:41,200
basically a year on this to kind of build this up, you know, we're a tiny team.

2282
02:27:42,480 --> 02:27:45,680
And then from one day to the next, it was all gone and not just gone.

2283
02:27:45,680 --> 02:27:47,360
It was associated with this giant fraud.

2284
02:27:49,200 --> 02:27:50,960
And so, you know, that was incredibly tough.

2285
02:27:51,920 --> 02:27:52,640
Yeah.

2286
02:27:52,640 --> 02:27:54,960
And then were there any signs early on that

2287
02:27:56,000 --> 02:27:56,800
SPF was?

2288
02:27:58,240 --> 02:27:58,480
Yeah.

2289
02:27:58,480 --> 02:28:00,640
And like, obviously I didn't know he was a fraud and the whole, you know,

2290
02:28:01,600 --> 02:28:03,440
I would have never worked there, you know.

2291
02:28:04,400 --> 02:28:06,480
And, you know, we weren't, you know, we were a separate thing.

2292
02:28:06,480 --> 02:28:07,920
We weren't working with the business.

2293
02:28:09,280 --> 02:28:11,360
I mean, I think, I do think there were some takeaways for me.

2294
02:28:11,360 --> 02:28:16,480
I think one takeaway was, you know, I think there's a, I had this tendency,

2295
02:28:16,480 --> 02:28:18,720
I think people in general have this tendency to kind of like, you know,

2296
02:28:18,720 --> 02:28:21,120
give successful CEOs a pass on their behavior.

2297
02:28:21,120 --> 02:28:23,440
Because, you know, there's successful CEOs and that's how they are.

2298
02:28:23,440 --> 02:28:25,440
And that's just the successful CEO things.

2299
02:28:25,440 --> 02:28:30,720
And, you know, I didn't know Sandbank Manfredo was a fraud, but I knew SPF.

2300
02:28:30,720 --> 02:28:33,200
And I knew he was extremely risk-taking, right?

2301
02:28:33,200 --> 02:28:35,520
I knew he, he was narcissistic.

2302
02:28:37,840 --> 02:28:40,400
He didn't tolerate this agreement well, you know, sort of by the end,

2303
02:28:40,400 --> 02:28:42,640
he and I just like didn't get along well.

2304
02:28:42,640 --> 02:28:45,520
And sort of, I think the reason for that was like, there's some biosecurity grants

2305
02:28:45,520 --> 02:28:48,320
he really liked because they're kind of cool and flashy.

2306
02:28:48,320 --> 02:28:50,880
And at some point I'd kind of run the numbers and it didn't really seem

2307
02:28:50,880 --> 02:28:51,760
that cost effective.

2308
02:28:51,760 --> 02:28:54,480
And I pointed that out and he was pretty unhappy about that.

2309
02:28:55,520 --> 02:28:56,640
And so I knew his character.

2310
02:28:58,640 --> 02:29:01,520
And I think, you know, I feel like one takeaway for me was,

2311
02:29:03,440 --> 02:29:07,280
was, you know, like, I think it's really worth paying attention to people's character,

2312
02:29:07,280 --> 02:29:09,680
including like people you work for and successful CEOs.

2313
02:29:10,880 --> 02:29:13,760
And, you know, that can save you a lot of pain down the line.

2314
02:29:14,720 --> 02:29:18,160
Okay, so after that, I picked some clothes and you're out.

2315
02:29:18,480 --> 02:29:28,000
And then you got into, you went to OpenAI, the Super Alignment team had just started.

2316
02:29:28,000 --> 02:29:30,080
I think you were like part of the initial team.

2317
02:29:30,640 --> 02:29:33,680
And so what was the original idea?

2318
02:29:33,680 --> 02:29:35,440
What was compelling about that for you to join?

2319
02:29:36,080 --> 02:29:36,880
Yeah, totally.

2320
02:29:37,920 --> 02:29:39,920
So, I mean, what was the goal of the Super Alignment team?

2321
02:29:40,960 --> 02:29:46,560
You know, the Alignment team at OpenAI, you know, at other labs, sort of like several years ago,

2322
02:29:46,560 --> 02:29:49,040
kind of had done sort of basic research and they developed RLHF,

2323
02:29:49,040 --> 02:29:51,040
Reinforcement Learning from Human Feedback.

2324
02:29:51,040 --> 02:29:54,880
And that was sort of a, you know, ended up being really successful technique

2325
02:29:54,880 --> 02:29:56,960
for controlling sort of current generation of AI models.

2326
02:29:59,280 --> 02:30:02,080
What we were trying to do was basically kind of be the basic research

2327
02:30:02,080 --> 02:30:04,560
bet to figure out what is the successor to RLHF.

2328
02:30:04,560 --> 02:30:06,640
And the reason that we needed that is, you know, basically, you know,

2329
02:30:06,640 --> 02:30:09,280
RLHF probably won't scale to superhuman systems.

2330
02:30:09,280 --> 02:30:12,560
RLHF relies on sort of human raiders who kind of thumbs up, thumbs down, you know,

2331
02:30:12,560 --> 02:30:15,120
like the model said something, it looks fine, it looks good to me.

2332
02:30:15,200 --> 02:30:17,440
At some point, you know, the superhuman models, the superintelligence,

2333
02:30:17,440 --> 02:30:20,320
it's going to write, you know, a million lines of, you know, crazy complex code,

2334
02:30:20,320 --> 02:30:22,080
you don't know at all what's going on anymore.

2335
02:30:22,080 --> 02:30:24,560
And so how do you kind of steer and control these systems?

2336
02:30:24,560 --> 02:30:25,760
How do you hide side constraints?

2337
02:30:26,560 --> 02:30:31,360
You know, the reason I joined was I thought this was an important problem

2338
02:30:31,360 --> 02:30:33,680
and I thought it was just a really solvable problem, right?

2339
02:30:33,680 --> 02:30:36,880
I thought this was basically, you know, there's, I think there's a, I still do.

2340
02:30:36,880 --> 02:30:41,200
I mean, even more so do I think there's a lot of just really promising sort of ML research

2341
02:30:41,200 --> 02:30:44,240
on alignment on sort of aligning superhuman systems.

2342
02:30:45,440 --> 02:30:50,320
And maybe we should talk about that a bit more later, but so, and then it was so solvable,

2343
02:30:50,320 --> 02:30:51,120
you solved it in a year.

2344
02:30:55,120 --> 02:30:58,800
Anyway, so look, opening, I wanted to do this like really ambitious effort on alignment and,

2345
02:30:58,800 --> 02:31:01,840
you know, Elliot was backing it and, you know, I liked a lot of the people there.

2346
02:31:01,840 --> 02:31:04,640
And so I was, you know, I was really excited and I was kind of like, you know,

2347
02:31:04,640 --> 02:31:07,760
I think there was a lot of people sort of on alignment.

2348
02:31:07,760 --> 02:31:09,680
There's always a lot of people kind of making hay about it.

2349
02:31:09,680 --> 02:31:13,680
And, you know, I appreciate people highlighting the importance of the problem.

2350
02:31:13,680 --> 02:31:15,840
And I was just really into like, let's just try to solve it.

2351
02:31:15,840 --> 02:31:17,920
And let's do the ambitious effort, you know, let's do the, you know,

2352
02:31:17,920 --> 02:31:20,160
operation warp speed for solving alignment.

2353
02:31:20,160 --> 02:31:22,880
And it seemed like an amazing opportunity to do so.

2354
02:31:23,760 --> 02:31:26,880
Okay. And now basically the team doesn't exist.

2355
02:31:26,880 --> 02:31:28,720
I think the head of it has left.

2356
02:31:28,720 --> 02:31:31,520
Both heads of it have left, Jan and Ilya.

2357
02:31:31,520 --> 02:31:32,800
That's the news of last week.

2358
02:31:34,080 --> 02:31:35,600
What happened? Why did the thing break down?

2359
02:31:37,040 --> 02:31:40,240
I think OpenAI sort of decided to take things in a somewhat different direction.

2360
02:31:41,200 --> 02:31:42,320
Meaning what?

2361
02:31:43,520 --> 02:31:46,320
I mean, that super alignment isn't the best way to frame the...

2362
02:31:47,440 --> 02:31:49,920
No, I mean, look, obviously, sort of after the November board events,

2363
02:31:49,920 --> 02:31:51,120
you know, there were personnel changes.

2364
02:31:51,120 --> 02:31:54,320
I think Ilya leaving was just incredibly tragic for OpenAI.

2365
02:31:54,320 --> 02:31:58,000
And, you know, I think some amount of repartilization,

2366
02:31:58,000 --> 02:32:00,000
I think some amount of, you know, I mean,

2367
02:32:00,000 --> 02:32:02,240
there's been some reporting on the Superalignment Compute Commitment.

2368
02:32:02,240 --> 02:32:04,160
You know, there's this 20% compute commitment as part of,

2369
02:32:04,160 --> 02:32:05,520
you know, how a lot of people were recruited.

2370
02:32:05,520 --> 02:32:07,920
You know, it's like, we're going to do this ambitious effort on alignment.

2371
02:32:07,920 --> 02:32:11,680
And, you know, some amount of, you know,

2372
02:32:11,680 --> 02:32:14,240
not keeping that and deciding to go in a different direction.

2373
02:32:15,440 --> 02:32:18,160
Okay, so now Jan has left, Ilya has left.

2374
02:32:19,280 --> 02:32:20,720
So this team itself has dissolved,

2375
02:32:20,720 --> 02:32:24,960
but you were the sort of first person who left or was forced to leave.

2376
02:32:24,960 --> 02:32:28,960
You were the information reported that you were fired for leaking.

2377
02:32:28,960 --> 02:32:30,320
But what happened? Was this accurate?

2378
02:32:31,360 --> 02:32:35,520
Yeah. Look, why don't I tell you what they claim I leaked

2379
02:32:35,520 --> 02:32:36,640
and you can tell me what you think.

2380
02:32:37,280 --> 02:32:40,480
Yeah. So OpenAI did claim to employees that I was fired for leaking.

2381
02:32:41,120 --> 02:32:44,240
And, you know, I and others have sort of pushed them to say what the leak is.

2382
02:32:44,240 --> 02:32:45,760
And so here's their response in full.

2383
02:32:47,280 --> 02:32:52,960
You know, sometime last year, I had written a sort of brainstorming document on preparedness

2384
02:32:52,960 --> 02:32:56,320
on safety and security measures we need in the future on the Path to AGI.

2385
02:32:57,120 --> 02:32:59,920
And I shared that with three external researchers for feedback.

2386
02:32:59,920 --> 02:33:01,280
So that's it. That's the leak.

2387
02:33:02,640 --> 02:33:03,920
You know, I think for context,

2388
02:33:03,920 --> 02:33:07,360
it was totally normal at OpenAI at the time to share sort of safety ideas

2389
02:33:07,360 --> 02:33:08,880
with external researchers for feedback.

2390
02:33:09,760 --> 02:33:10,880
You know, it happened all the time.

2391
02:33:11,680 --> 02:33:14,880
You know, the doc was sort of my idea is, you know, before I shared it,

2392
02:33:15,520 --> 02:33:17,520
I reviewed it for anything sensitive.

2393
02:33:19,440 --> 02:33:21,680
The internal version had a reference to a future cluster,

2394
02:33:21,680 --> 02:33:23,600
but I redacted that for the external copy.

2395
02:33:24,480 --> 02:33:28,160
You know, there's a link in there to some slides of mine, internal slides.

2396
02:33:28,880 --> 02:33:31,520
You know, that was a dead link to the external people I shared it with.

2397
02:33:31,520 --> 02:33:32,880
You know, the slides weren't shared with them.

2398
02:33:33,520 --> 02:33:37,440
And so, obviously, I pressed them to sort of tell me,

2399
02:33:37,440 --> 02:33:39,600
what is the confidential information in this document?

2400
02:33:40,160 --> 02:33:46,800
And what they came back with was a line in the doc about planning for AGI by 2728,

2401
02:33:46,800 --> 02:33:48,480
and not setting timelines for preparedness.

2402
02:33:50,720 --> 02:33:54,320
You know, I wrote this doc, you know, a couple months after the super alignment

2403
02:33:54,320 --> 02:33:57,120
announcement, we had put out, you know, this sort of four-year planning horizon.

2404
02:33:57,120 --> 02:33:59,280
I didn't think that planning horizon was sensitive.

2405
02:33:59,280 --> 02:34:02,240
You know, it's the sort of thing Sam says publicly all the time.

2406
02:34:03,680 --> 02:34:06,400
Hey, I think sort of John said it on the podcast a couple weeks ago.

2407
02:34:07,200 --> 02:34:08,800
Anyway, so that's it.

2408
02:34:08,800 --> 02:34:09,280
That's it?

2409
02:34:09,280 --> 02:34:14,800
So that seems pretty thin for, if the cause was leaking, that seems pretty thin.

2410
02:34:14,800 --> 02:34:15,840
Was there anything else to it?

2411
02:34:16,960 --> 02:34:19,600
Yeah, I mean, so that was the leaking claim.

2412
02:34:19,600 --> 02:34:22,800
I mean, you can say a bit more about sort of what happened in the final.

2413
02:34:22,800 --> 02:34:23,040
Yeah.

2414
02:34:24,240 --> 02:34:28,560
So one thing was last year, I had written a memo,

2415
02:34:28,560 --> 02:34:30,880
internal memo, about opening iSecurity.

2416
02:34:30,880 --> 02:34:32,480
I thought it was, you know, egregiously insufficient.

2417
02:34:32,480 --> 02:34:34,560
You know, I thought it wasn't sufficient to protect, you know,

2418
02:34:34,560 --> 02:34:37,840
the theft of model weights or key algorithmic secrets from foreign actors.

2419
02:34:38,640 --> 02:34:40,080
So I wrote this memo.

2420
02:34:40,080 --> 02:34:42,880
I shared it with a few colleagues, a couple members of leadership,

2421
02:34:43,520 --> 02:34:44,960
who sort of mostly said it was helpful.

2422
02:34:46,640 --> 02:34:50,560
But then, you know, a couple weeks later, a sort of major security incident occurred.

2423
02:34:51,600 --> 02:34:54,560
And that prompted me to share the memo with a couple members of the board.

2424
02:34:54,560 --> 02:34:59,120
And so after I did that, you know, days later, it was made very clear to me that leadership

2425
02:34:59,120 --> 02:35:01,680
was very unhappy with me having shared this memo with the board.

2426
02:35:02,800 --> 02:35:06,480
You know, apparently the board had hassled leadership about security.

2427
02:35:07,440 --> 02:35:10,240
And then I got sort of an official HR warning for this memo,

2428
02:35:10,880 --> 02:35:12,240
you know, for sharing it with the board.

2429
02:35:13,440 --> 02:35:16,720
The HR person told me it was racist to worry about CCPS view.

2430
02:35:18,480 --> 02:35:20,240
And they said it was sort of unconstructive.

2431
02:35:21,920 --> 02:35:24,640
And, you know, look, I think I probably wasn't that my most diplomatic,

2432
02:35:24,640 --> 02:35:26,640
you know, I definitely could have been more politically savvy.

2433
02:35:27,520 --> 02:35:29,920
But, you know, I thought it was a really, really important issue.

2434
02:35:29,920 --> 02:35:32,960
And, you know, the security incident had been really worried.

2435
02:35:34,160 --> 02:35:37,200
Anyway, and so I guess the reason I bring this up is when I was fired,

2436
02:35:37,200 --> 02:35:40,880
it was sort of made very explicit that the security memo is a major reason for my being fired.

2437
02:35:41,600 --> 02:35:43,120
You know, I think it was something like, you know,

2438
02:35:43,120 --> 02:35:46,240
the reason that this is a firing and not a warning is because of the security memo.

2439
02:35:47,760 --> 02:35:49,280
But you were sharing it with the board?

2440
02:35:50,160 --> 02:35:52,080
The warning I'd gotten for the security memo.

2441
02:35:52,080 --> 02:35:57,280
Hmm. Anyway, and I mean, some other, you know, what might also be helpful context

2442
02:35:57,280 --> 02:35:59,360
is the sort of questions they asked me when they fired me.

2443
02:35:59,360 --> 02:36:03,920
So, you know, this was a bit over a month ago, I was pulled aside for a chat with a lawyer,

2444
02:36:03,920 --> 02:36:05,840
you know, that quickly turned very adversarial.

2445
02:36:06,560 --> 02:36:15,120
And, you know, the questions were all about my views on AI progress on AGI on the level

2446
02:36:15,120 --> 02:36:21,520
security appropriate for AGI on, you know, whether government should be involved in AGI on

2447
02:36:23,040 --> 02:36:29,520
whether I and super alignment were loyal to the company on, you know, what I was up to

2448
02:36:29,520 --> 02:36:31,600
during the opening of board events, you know, things like that.

2449
02:36:32,160 --> 02:36:35,200
And, you know, then they, you know, chatted to a couple of my colleagues,

2450
02:36:35,200 --> 02:36:37,040
and then they came back and told me I was fired.

2451
02:36:37,760 --> 02:36:40,320
And, you know, they'd gone through all of my digital artifacts from the time at my,

2452
02:36:40,320 --> 02:36:42,320
you know, time at opening messages docs.

2453
02:36:42,320 --> 02:36:43,920
And that's when they found, you know, the leak.

2454
02:36:45,680 --> 02:36:48,640
Yeah. And so anyway, so the main claim they made was a leaking allegation.

2455
02:36:48,640 --> 02:36:50,080
You know, that's what they told employees.

2456
02:36:51,040 --> 02:36:52,800
They, you know, the security memo.

2457
02:36:54,160 --> 02:36:56,160
There's a couple of other allegations they threw in.

2458
02:36:56,720 --> 02:37:00,160
One thing they said was that I was unforthcoming during the investigation,

2459
02:37:00,160 --> 02:37:02,560
because I didn't initially remember who I'd shared the doc with,

2460
02:37:02,560 --> 02:37:06,800
the sort of preparedness brainstorming doc, only that I had sort of spoken to some external

2461
02:37:06,800 --> 02:37:08,080
researchers about these ideas.

2462
02:37:08,720 --> 02:37:11,120
And, you know, look, the doc was over six months old.

2463
02:37:11,120 --> 02:37:12,320
You know, I spent the day on it.

2464
02:37:12,960 --> 02:37:15,280
You know, as a Google doc, I shared with my opening email.

2465
02:37:15,280 --> 02:37:17,760
It wasn't a, you know, screenshot or anything I was trying to hide.

2466
02:37:18,560 --> 02:37:20,640
It simply didn't stick because it was such a non-issue.

2467
02:37:22,240 --> 02:37:26,240
And then they also claim that I was engaging on policy in a way that they didn't like.

2468
02:37:27,120 --> 02:37:31,440
And so what they cited there was that I had spoken to a couple of external researchers,

2469
02:37:31,440 --> 02:37:35,920
you know, somebody got a think tank about my view that AGI would become a government project,

2470
02:37:35,920 --> 02:37:36,800
you know, as we discussed.

2471
02:37:37,440 --> 02:37:40,560
You know, in fact, I was speaking to lots of sort of people in the field about that at the time.

2472
02:37:40,560 --> 02:37:42,320
I thought it was a really important thing to think about.

2473
02:37:43,520 --> 02:37:47,280
Anyway, and so they found, you know, they found a DM that I'd written to like a friendly colleague,

2474
02:37:47,280 --> 02:37:51,440
you know, five or six months ago, where I relayed this and, you know, they cited that.

2475
02:37:52,640 --> 02:37:56,000
And, you know, I had thought it was well within open-eyed norms to kind of talk about

2476
02:37:56,000 --> 02:37:59,200
high-level issues on the future of AGI with the external people in the field.

2477
02:38:00,080 --> 02:38:01,760
So anyway, so that's what they alleged.

2478
02:38:01,760 --> 02:38:02,480
That's what happened.

2479
02:38:04,240 --> 02:38:07,840
You know, I've spoken to kind of a few dozen former colleagues about this, you know,

2480
02:38:07,840 --> 02:38:11,760
since I think the sort of universal reaction is kind of like, you know, that's insane.

2481
02:38:13,520 --> 02:38:15,760
I was sort of surprised as well.

2482
02:38:15,760 --> 02:38:19,200
You know, I had been promoted just a few months before.

2483
02:38:20,640 --> 02:38:24,480
I think, you know, I think Ilya's comment for the promotion case at the time was something like,

2484
02:38:24,480 --> 02:38:25,680
you know, Leopold's amazing.

2485
02:38:25,680 --> 02:38:26,560
We're lucky to have him.

2486
02:38:29,600 --> 02:38:33,040
But look, I mean, I think the thing I understand, and I think in some sense is reasonable,

2487
02:38:33,040 --> 02:38:34,720
is like, you know, I think I ruffled some feathers.

2488
02:38:34,720 --> 02:38:37,040
And, you know, I think I was probably kind of annoying at times.

2489
02:38:37,040 --> 02:38:41,360
You know, it's like, I security stuff, and I kind of like repeatedly raised that,

2490
02:38:41,360 --> 02:38:43,200
and maybe not always in the most diplomatic way.

2491
02:38:43,360 --> 02:38:48,880
You know, I didn't sign the employee letter during the board events, you know, despite

2492
02:38:48,880 --> 02:38:50,400
pressure to do so.

2493
02:38:51,760 --> 02:38:54,000
And you were one of like eight people or something.

2494
02:38:54,000 --> 02:38:55,360
Not that many people.

2495
02:38:55,360 --> 02:38:59,040
I guess the, I think the sort of two senior most people didn't sign were Andrey,

2496
02:38:59,040 --> 02:39:07,520
and yeah, I mean, on the letter, by the way, I, by the time on sort of Monday morning,

2497
02:39:07,520 --> 02:39:10,080
when that letter was going around, I think probably it was appropriate for the board

2498
02:39:10,160 --> 02:39:10,560
to resign.

2499
02:39:10,560 --> 02:39:13,680
I think they'd kind of like lost too much credibility and trust with the employees.

2500
02:39:15,280 --> 02:39:17,280
But I thought the letter had a bunch of issues.

2501
02:39:17,280 --> 02:39:20,240
I mean, I think one of them was it just didn't call for an independent board.

2502
02:39:20,240 --> 02:39:22,960
I think it's sort of like basics of corporate governance to have an independent board.

2503
02:39:23,600 --> 02:39:27,680
Anyway, you know, it's other things, you know, I am in sort of other discussions,

2504
02:39:27,680 --> 02:39:31,680
I press leadership for sort of opening eye to abide by its public commitments.

2505
02:39:32,960 --> 02:39:37,520
You know, I raised a bunch of tough questions about whether it was consistent with the

2506
02:39:37,520 --> 02:39:41,280
opening I mission and consistent with the national interests to sort of partner with

2507
02:39:41,280 --> 02:39:44,160
authoritarian dictatorships to build the core infrastructure for AGI.

2508
02:39:45,920 --> 02:39:48,560
So, you know, look, you know, it's a free country, right?

2509
02:39:49,360 --> 02:39:50,560
That's what I love about this country.

2510
02:39:50,560 --> 02:39:51,520
You know, we talked about it.

2511
02:39:52,400 --> 02:39:55,040
And so they have no obligation to keep me on staff.

2512
02:39:56,960 --> 02:40:00,800
And, you know, I think in some sense, I think it would have been perfectly reasonable

2513
02:40:00,800 --> 02:40:04,800
for them to come to me and say, look, you know, we're taking the company in a different direction.

2514
02:40:04,800 --> 02:40:06,320
You know, we disagree with your point of view.

2515
02:40:07,280 --> 02:40:10,800
You know, we don't trust you enough to sort of tow the company line anymore.

2516
02:40:10,800 --> 02:40:15,440
And, you know, thank you so much for your work at Open AI, but I think it's time to part ways.

2517
02:40:15,440 --> 02:40:16,400
I think that would have made sense.

2518
02:40:16,400 --> 02:40:20,720
I think, you know, we did start sort of materially diverging on sort of views on important issues.

2519
02:40:20,720 --> 02:40:24,240
I'd come in very excited and aligned with Open AI, but that sort of changed over time.

2520
02:40:24,880 --> 02:40:29,840
And look, I think there would have been a very amicable way to part ways.

2521
02:40:29,840 --> 02:40:33,360
And I think it's a bit of a shame that it sort of this is the way it went down.

2522
02:40:34,320 --> 02:40:39,200
You know, all that being said, I think, you know, I really want to emphasize

2523
02:40:40,160 --> 02:40:42,400
there's just a lot of really incredible people at Open AI.

2524
02:40:42,400 --> 02:40:45,360
And it was an incredible privilege to work with them.

2525
02:40:45,360 --> 02:40:48,640
And, you know, overall, I'm just extremely grateful for my time there.

2526
02:40:49,600 --> 02:40:53,120
When you left, now that there's now there's been reporting about

2527
02:40:54,560 --> 02:41:01,120
an NDA that former employees have to sign in order to have access to their vested equity.

2528
02:41:01,760 --> 02:41:03,680
Did you sign some such NDA?

2529
02:41:04,480 --> 02:41:07,280
No. My situation is a little different.

2530
02:41:07,280 --> 02:41:09,200
And that is sort of basically right before my cliff.

2531
02:41:10,080 --> 02:41:11,920
But then, you know, they still offered me the equity.

2532
02:41:13,600 --> 02:41:16,640
But I didn't want to sign a nondisparagement, you know, freedom is priceless.

2533
02:41:16,640 --> 02:41:18,320
And how much was how much was the equity?

2534
02:41:18,320 --> 02:41:20,400
It's like close to a million dollars.

2535
02:41:21,120 --> 02:41:25,280
So it was definitely a thing you were you and others aware of that this is like

2536
02:41:25,920 --> 02:41:29,040
a choice that Open AI is explicitly offering you.

2537
02:41:29,040 --> 02:41:29,680
Yeah.

2538
02:41:29,680 --> 02:41:34,560
And presumably the person on Open AI staff knew that we're offering them equity,

2539
02:41:34,560 --> 02:41:37,760
but they had to sign this NDA that has these conditions that you can't,

2540
02:41:38,320 --> 02:41:43,680
for example, give the kind of statements about your thoughts on AGI and Open AI that

2541
02:41:43,680 --> 02:41:45,360
you're giving on this podcast right now.

2542
02:41:45,360 --> 02:41:46,800
Like, I don't know what the whole situation is.

2543
02:41:46,800 --> 02:41:51,200
I certainly think sort of vested equity is pretty rough if you're conditioning that on NDA.

2544
02:41:51,200 --> 02:41:53,920
It might be a somewhat different situation if it's a sort of separate agreement.

2545
02:41:53,920 --> 02:41:54,400
Right.

2546
02:41:54,400 --> 02:41:59,360
But an Open AI employee who had signed it presumably could not give the podcast that you're giving today.

2547
02:42:00,640 --> 02:42:01,920
Quite plausibly not.

2548
02:42:01,920 --> 02:42:02,560
Yeah.

2549
02:42:02,560 --> 02:42:02,960
Yeah.

2550
02:42:02,960 --> 02:42:03,440
I don't know.

2551
02:42:04,800 --> 02:42:05,200
Okay.

2552
02:42:05,200 --> 02:42:12,400
So analyzing the situation here, I guess if you were to, yeah, the board thing is really tough

2553
02:42:12,400 --> 02:42:16,480
because if you were trying to defend them, you would say, well, listen,

2554
02:42:16,480 --> 02:42:19,040
you were just kind of going outside the regular chain of command.

2555
02:42:19,040 --> 02:42:24,080
And maybe there's a point there, although the way in which the person from HR thinks

2556
02:42:24,080 --> 02:42:27,760
that you have an adversarial relationship with or you're supposed to have an adversarial

2557
02:42:27,760 --> 02:42:34,560
relationship with the board where to give the board some information, which is relevant to

2558
02:42:36,320 --> 02:42:40,080
whether Open AI is fulfilling its mission and whether it can do that in a better way

2559
02:42:40,080 --> 02:42:45,760
is part of the leak as if the board is that is supposed to ensure that Open AI is following

2560
02:42:45,760 --> 02:42:47,680
its mission as some sort of external actor.

2561
02:42:48,240 --> 02:42:49,200
That seems pretty...

2562
02:42:49,200 --> 02:42:53,040
I mean, I think, I mean, to be clear, the leak allegation was just that sort of document

2563
02:42:53,040 --> 02:42:53,840
I changed the feedback.

2564
02:42:53,840 --> 02:42:55,840
This is just sort of a separate thing that they cited and they said,

2565
02:42:55,840 --> 02:42:57,920
I wouldn't have been fired if not for the security memo.

2566
02:42:57,920 --> 02:42:59,760
They said you wouldn't have been fired.

2567
02:42:59,760 --> 02:43:03,280
They said the reason this is a firing and not a warning is because of the warning you had

2568
02:43:03,280 --> 02:43:04,480
gotten for the security memo.

2569
02:43:04,480 --> 02:43:11,280
Oh, before you left, the incidents with the board happened, were Sam was fired and then

2570
02:43:11,280 --> 02:43:13,280
Rihard, a CEO, and now he's on the board.

2571
02:43:14,240 --> 02:43:20,320
Now, Ilya and Yan, who are the heads of the super lineman team and Ilya, who is a co-founder of

2572
02:43:20,320 --> 02:43:25,680
Open AI, obviously the most significant in terms of stash or a member of Open AI for

2573
02:43:25,680 --> 02:43:27,920
a research perspective, they've left.

2574
02:43:27,920 --> 02:43:31,680
It seems like, especially with regards to super lineman stuff and just generally the

2575
02:43:31,680 --> 02:43:36,160
Open AI, a lot of the sort of personnel drama has happened over the last few months.

2576
02:43:36,800 --> 02:43:37,680
What's going on?

2577
02:43:37,680 --> 02:43:38,960
Yeah, there's a lot of drama.

2578
02:43:41,760 --> 02:43:43,280
Yeah, so why is there so much drama?

2579
02:43:45,360 --> 02:43:49,120
You know, I think there would be a lot less drama all Open AI claim to be with sort of

2580
02:43:49,120 --> 02:43:51,440
building chat, GPT or building business software.

2581
02:43:51,760 --> 02:43:55,360
I think where a lot of the drama comes from is, you know, Open AI really believes they're

2582
02:43:55,360 --> 02:43:56,720
building AGI, right?

2583
02:43:56,720 --> 02:44:02,640
And it's not just, you know, a claim they make for marketing purposes, you know, whatever,

2584
02:44:02,640 --> 02:44:06,080
you know, there's this report that Sam is raising, you know, $7 trillion for chips and

2585
02:44:06,080 --> 02:44:09,200
it's like, that stuff only makes sense if you really believe in AGI.

2586
02:44:10,400 --> 02:44:13,680
And so I think what gets people sometimes is sort of the cognitive dissonance between

2587
02:44:13,680 --> 02:44:17,440
sort of really believing in AGI, but then sort of not taking some of the other implications

2588
02:44:17,440 --> 02:44:20,320
seriously, you know, it's, this is going to be incredible.

2589
02:44:20,480 --> 02:44:23,520
This is going to be incredibly powerful technology, both for good and for bad.

2590
02:44:23,520 --> 02:44:27,600
And that implicates really important issues like the national security issues we spoke

2591
02:44:27,600 --> 02:44:30,480
about, like, you know, are you protecting the secrets from the CCP?

2592
02:44:30,480 --> 02:44:34,640
Like, you know, does America control the core AGI infrastructure or does it, you know, a

2593
02:44:34,640 --> 02:44:37,120
Middle Eastern dictator control the core AGI infrastructure?

2594
02:44:39,360 --> 02:44:44,640
And then I mean, I think the thing that, you know, really gets people is the sort of tendency

2595
02:44:44,640 --> 02:44:49,440
to kind of then make commitments and sort of like, you know, they say they take these

2596
02:44:49,440 --> 02:44:53,120
issues really seriously, they make big commitments on them, but then sort of frequently don't

2597
02:44:53,120 --> 02:44:56,800
follow through, right? So, you know, again, as mentioned, there was this commitment around

2598
02:44:56,800 --> 02:45:00,720
super alignment compute, you know, sort of 20% of compute for this long term safety research

2599
02:45:00,720 --> 02:45:04,640
effort. And I think, you know, you and I could have a totally reasonable debate about what is

2600
02:45:04,640 --> 02:45:09,600
the appropriate level of compute for super alignment. But that's not really the issue.

2601
02:45:09,600 --> 02:45:13,280
The issue is that this commitment was made and it was used to recruit people and, you know,

2602
02:45:13,280 --> 02:45:18,560
it was, it was very public. And it was made because, you know, there's a recognition that

2603
02:45:18,560 --> 02:45:21,520
there would always be something more urgent than a long term safety research effort, you know,

2604
02:45:21,520 --> 02:45:26,160
like some new product or whatever. And then in fact, they just, you know, really didn't keep the

2605
02:45:26,160 --> 02:45:29,920
commitment. And so, you know, there was always something more urgent than long term safety

2606
02:45:29,920 --> 02:45:34,320
research. I mean, I think, I think another example of this is, you know, when I raised these issues

2607
02:45:34,320 --> 02:45:39,680
about security, you know, they would, they would tell me, you know, securities are number one priority.

2608
02:45:41,120 --> 02:45:46,720
But then, you know, invariably, when, when it came time to sort of invest serious resources,

2609
02:45:46,720 --> 02:45:50,800
when it came time to make trade offs, to sort of take some pretty basic measures,

2610
02:45:51,840 --> 02:45:55,840
security would not be prioritized. And so, yeah, I think it's the cognitive dissonance,

2611
02:45:55,840 --> 02:46:00,000
and I think it's the sort of unreliability that causes a bunch of the drama.

2612
02:46:00,960 --> 02:46:08,320
So let's zoom out, talk about the part, a big part of the story, and also a big motivation

2613
02:46:08,320 --> 02:46:12,480
of the way in which it must proceed with regards to geopolitics and everything,

2614
02:46:12,480 --> 02:46:18,160
is that once you have the AGI, pretty soon after you proceed to ASI, because superintelligence,

2615
02:46:18,160 --> 02:46:25,520
because you have these AGI's, which can function as researchers into further AI progress. And with

2616
02:46:25,520 --> 02:46:32,560
a matter of years, maybe less, you go to something that is like superintelligence. And at the high,

2617
02:46:32,560 --> 02:46:36,160
and then from there, then you can do up in according to your story, do all this research

2618
02:46:36,160 --> 02:46:40,800
and development and robotics and pocket nukes and whatever other crazy shit.

2619
02:46:41,600 --> 02:46:55,360
But at a high level, it's not clear to me this input-output model of research is how things

2620
02:46:55,360 --> 02:47:01,120
actually happen in research. We can look at economy-wide, right? Patrick Hollis and others

2621
02:47:01,120 --> 02:47:05,920
have made this point that from compared to 100 years ago, we have 100x more researchers in the

2622
02:47:05,920 --> 02:47:11,520
world. It's not like progress is happening 100x faster. So it's clearly not the case that you

2623
02:47:11,520 --> 02:47:16,880
can just pump in more population into research and you get higher research on the other end.

2624
02:47:18,000 --> 02:47:20,400
I don't know why it would be different for the AI researchers themselves.

2625
02:47:20,400 --> 02:47:23,920
Okay, great. So this is getting into some good stuff. I have a classic disagreement

2626
02:47:23,920 --> 02:47:29,840
I have with Patrick and others. So obviously, inputs matter. So it's like United States

2627
02:47:29,840 --> 02:47:35,840
produces a lot more scientific and technological progress than Liechtenstein or Switzerland.

2628
02:47:36,160 --> 02:47:40,160
And even if I made Patrick Hollis and dictator of Liechtenstein or Switzerland,

2629
02:47:40,160 --> 02:47:44,320
and Patrick Hollis was able to implement his Utopia of Ideal Institutions,

2630
02:47:44,320 --> 02:47:48,160
keeping the talent pool fixed. He's not able to do some crazy high school immigration thing or

2631
02:47:49,200 --> 02:47:52,320
whatever, some crazy genetic breeding scheme or whatever he wants to do.

2632
02:47:54,160 --> 02:47:58,080
Keeping the talent pool fixed, but amazing institutions. I claim that still,

2633
02:47:58,080 --> 02:48:01,280
even if you made Patrick Hollis and dictator of Switzerland, maybe you get some factor,

2634
02:48:01,280 --> 02:48:04,560
but Switzerland is not going to be able to outcompete the United States in scientific

2635
02:48:04,560 --> 02:48:09,440
and technological parts. Obviously, magnitudes matter. Okay. No, I actually, I'm not sure I

2636
02:48:09,440 --> 02:48:14,160
agree with this. There's been many examples in history where you have small groups of people

2637
02:48:14,160 --> 02:48:18,400
who are part of like Bell Labs or Skunkworks or something. There's a couple hundred researchers

2638
02:48:18,400 --> 02:48:22,800
open AI, right? Couple hundred researchers, they do highly selected though, right? You know,

2639
02:48:22,800 --> 02:48:26,800
it's like, it's like saying, you know, that's part of Patrick Hollis and his dictator is going

2640
02:48:26,800 --> 02:48:30,720
to do a good job of this. Well, yes, if you can highly select all the best AI researchers in

2641
02:48:30,720 --> 02:48:33,920
the world, you might only need a few hundred, but if you, you know, that's, that's the talent

2642
02:48:33,920 --> 02:48:37,280
pool. It's like you have the, you know, 300 best AI researchers in the world.

2643
02:48:37,280 --> 02:48:41,120
But, but there's, there has been, it's not a case that from a hundred years to now, there haven't

2644
02:48:41,120 --> 02:48:45,360
been, the population has increased massively. A lot of the, in fact, you would expect the density

2645
02:48:45,360 --> 02:48:49,600
of talent to have increased in the sense that malnutrition and other kinds of debilitation,

2646
02:48:49,600 --> 02:48:54,320
poverty, whatever, that have debilitated past talent at the same sort of level is no longer

2647
02:48:54,320 --> 02:48:58,000
debilitated in the same way. To the 100X point, right? So I don't know if it's 100X. I think it's

2648
02:48:58,000 --> 02:49:02,240
easy to inflate these things, probably at least 10X. And so people are sometimes like, ah, you

2649
02:49:02,240 --> 02:49:05,840
know, like, you know, come on, ideas haven't gotten them much harder to find, you know, why would

2650
02:49:05,840 --> 02:49:10,160
you have needed this 10X increase in research effort? Whereas to me, I think this is an extremely

2651
02:49:10,160 --> 02:49:14,000
natural story. And why is it a natural story? It's a straight line on a log log plot. This is sort

2652
02:49:14,000 --> 02:49:18,320
of a, you know, deep learning researchers dream, right? What is this log log plot on the X axis?

2653
02:49:18,320 --> 02:49:24,080
You have log cumulative research effort on the Y axis. You have some log GDP or ooms of algorithmic

2654
02:49:24,160 --> 02:49:28,800
progress, or, you know, log transistors per square inch, or, you know, in the sort of

2655
02:49:28,800 --> 02:49:33,120
experience curve for solar, kind of like, you know, whatever the log of, you know, the price for a

2656
02:49:33,120 --> 02:49:37,600
gigawatt of solar. And it's extremely natural for that to be a straight line. You know, this is

2657
02:49:37,600 --> 02:49:41,920
sort of a class, yeah, it's a classic. And, you know, it's basically the first thing is very easy,

2658
02:49:41,920 --> 02:49:45,120
then basically, you know, you have to have log increments of cumulative research effort to

2659
02:49:45,120 --> 02:49:50,480
find the next thing. And so, you know, in some sense, I think this is a natural story. Now,

2660
02:49:50,480 --> 02:49:54,880
one objection kind of people then make is like, oh, you know, isn't it suspicious, right? That

2661
02:49:54,880 --> 02:49:59,920
like ideas, you know, well, we increased research effort 10x, and ideas also just got 10x harder

2662
02:49:59,920 --> 02:50:05,520
defined. And so it perfectly, you know, equilibrates. And to there, I say, you know, it's just, it's an

2663
02:50:05,520 --> 02:50:10,080
equilibrium. It's an dodges equilibrium, right? So it's like, you know, isn't it a coincidence that

2664
02:50:10,080 --> 02:50:14,640
supply equals demand, you know, on the market clears, right? And that's, and the same thing here,

2665
02:50:14,640 --> 02:50:18,640
right? So it's, you know, ideas getting, how much ideas have gotten harder to find as a function

2666
02:50:18,640 --> 02:50:23,440
of how much progress you've made. And then, you know, what the overall growth rate has been is a

2667
02:50:23,440 --> 02:50:27,600
function of how much ideas have gotten harder to find in ratio to how much you've been able to,

2668
02:50:27,600 --> 02:50:30,640
like, increase research effort, what is the sort of growth, the log cumulative research effort.

2669
02:50:30,640 --> 02:50:34,400
So in some sense, I think the story is sort of like, fairly natural. And you see this, you see

2670
02:50:34,400 --> 02:50:37,760
this not just economy wide, you see it in kind of experience curve for all sorts of individual

2671
02:50:37,760 --> 02:50:43,040
technologies. So I think there's some process like this. And it's totally plausible that, you know,

2672
02:50:43,040 --> 02:50:46,560
institutions have gotten worse by some factor. Obviously, there's some sort of exponent of

2673
02:50:46,560 --> 02:50:50,800
diminishing returns on more people, right? So like serial time is better than just paralyzing.

2674
02:50:51,840 --> 02:50:54,400
But still, I think it's like, clearly inputs matter.

2675
02:50:55,280 --> 02:51:02,640
Yeah, I agree. But if the coefficient of how fast they diminish as you grow, the input

2676
02:51:02,640 --> 02:51:07,600
is high enough, then the, and the abstract, the fact that inputs matter isn't that relevant.

2677
02:51:07,600 --> 02:51:11,360
Okay, so I mean, we're talking at a very high level, but just like take it down to the actual

2678
02:51:11,360 --> 02:51:18,720
concrete thing here. Open AI has a staff of at most low hundreds who are directly involved

2679
02:51:18,720 --> 02:51:23,760
in the algorithmic progress in future models. If it was really the case that you could just

2680
02:51:23,760 --> 02:51:28,480
arbitrarily scale this number, and you can have much faster algorithmic progress, and that would

2681
02:51:28,480 --> 02:51:34,400
result in much higher, much better AI store open AI basically, then it's not clear why open AI

2682
02:51:34,400 --> 02:51:38,320
doesn't just go out and hire every single person with 150 IQ, of which there are hundreds of

2683
02:51:38,320 --> 02:51:45,520
thousands in the world. And my, my story there is there's transaction costs to managing all these

2684
02:51:45,520 --> 02:51:51,440
people that don't just go away if you have a bunch of AI's that there, these tasks aren't easy to

2685
02:51:51,440 --> 02:51:57,520
parallelize. And I think you, I'm not sure how you would explain the fact of like, why does an

2686
02:51:57,520 --> 02:52:01,280
open AI go on a recruiting binge of every single genius in the world?

2687
02:52:01,280 --> 02:52:04,160
All right, great. So let's talk about the open AI example, and let's talk about the automated

2688
02:52:04,160 --> 02:52:07,600
AI researchers. So I mean, in the open AI case, I mean, just, you know, just kind of like look

2689
02:52:07,600 --> 02:52:11,200
at the inflation of like AI researcher salaries over the last year. I mean, I think like, I don't

2690
02:52:11,200 --> 02:52:15,040
know, I don't know what it is, you know, 4x, 5x is kind of crazy. So they're clearly really trying

2691
02:52:15,040 --> 02:52:18,880
to recruit the best AI researchers in the world. And, you know, I don't know, it's,

2692
02:52:18,880 --> 02:52:23,120
they do find the best AI researchers in the world. I think my response to your thing is like, you know,

2693
02:52:23,120 --> 02:52:26,400
almost all of these 150 IQ people, you know, if you just hire them tomorrow, they wouldn't be

2694
02:52:26,400 --> 02:52:30,960
good AI researchers, they wouldn't be an Alec Radford. But they're willing to make investments

2695
02:52:30,960 --> 02:52:35,440
that take years to pan out of the four. The data centers they're buying right now will come

2696
02:52:35,440 --> 02:52:40,720
online in 2026 or something. Why wouldn't they be able to make every 150 IQ person? Some of them

2697
02:52:40,720 --> 02:52:44,320
won't work out. Some of them won't have the traits we like. But some of them by 2026 will be amazing

2698
02:52:44,320 --> 02:52:48,480
AI researchers. Why aren't they making that bet? Yeah. And so sometimes this happens, right? Like,

2699
02:52:48,480 --> 02:52:51,280
smart physicists have been really good at AI research, you know, it's like all the anthropocopies

2700
02:52:51,280 --> 02:52:56,240
co-found. But like, if you talk to, I've had Daria in the podcast, they have this very careful

2701
02:52:56,240 --> 02:53:00,240
policy of like, we're not going to just hire arbitrarily, we're going to be extremely selective.

2702
02:53:01,520 --> 02:53:05,680
Training is not as easily scalable, right? So training is very hard. You know, if you just

2703
02:53:05,680 --> 02:53:09,920
hired, you know, 100,000 people, it's like, you, I mean, you couldn't train them all. If you're

2704
02:53:09,920 --> 02:53:13,120
really hard to train them all, you know, you wouldn't be doing any AI research. Like, you know,

2705
02:53:13,120 --> 02:53:16,400
there's, there's huge costs to bringing on a new person training them. This is very different

2706
02:53:16,400 --> 02:53:19,280
with the AIs, right? And I think this is, it's really important to talk about the sort of like

2707
02:53:19,280 --> 02:53:23,200
advantages the AIs will have. So it's like, you know, training, right? It's like, what does it take

2708
02:53:23,200 --> 02:53:26,800
to be an Alec Radford? You know, we need to be in a really good engineer, right? The AIs,

2709
02:53:26,800 --> 02:53:29,520
they're going to be an amazing engineer. They're going to be amazing at coding. You can just train

2710
02:53:29,520 --> 02:53:33,840
them to do that. They need to have, you know, not just be a good engineer, but have really good

2711
02:53:33,840 --> 02:53:37,520
research intuitions and like really understand deep learning. And this is stuff that, you know,

2712
02:53:37,520 --> 02:53:41,440
Alec Radford, or, you know, somebody like him has acquired over years of research over just like

2713
02:53:41,440 --> 02:53:46,880
being deeply immersed in deep learning, having tried lots of things himself and failed. The AIs,

2714
02:53:46,880 --> 02:53:49,920
you know, they're going to be able to read every research paper I've written, every experiment

2715
02:53:49,920 --> 02:53:53,120
ever run at the lab, you know, like gain the intuitions from all of this, they're going to be

2716
02:53:53,120 --> 02:53:56,800
able to learn in parallel from all of each other's experiment, you know, experiences.

2717
02:53:57,920 --> 02:54:00,000
You know, I don't know what else, you know, it's like, what does it take to be an Alec

2718
02:54:00,000 --> 02:54:03,840
Radford? Well, there's a, there's a sort of cultural acclimation aspect of it, right? You know,

2719
02:54:03,840 --> 02:54:08,080
if you hire somebody new, there's like politicking, maybe they don't fit in. Well, in the AI case,

2720
02:54:08,080 --> 02:54:12,080
you just make replicas, right? There's a like motivation aspect for it, right? So it's like,

2721
02:54:12,080 --> 02:54:15,920
you know, Alec, you know, they could just like duplicate Alec Radford. And before I run every

2722
02:54:15,920 --> 02:54:19,760
experiment, I haven't spent like, you know, a decade's worth of human time, like double checking

2723
02:54:19,760 --> 02:54:22,800
the code and thinking really careful, be careful about it. I mean, first of all,

2724
02:54:22,800 --> 02:54:27,520
on how that many Alec Radfords, and you know, he wouldn't care. And he would not be motivated.

2725
02:54:27,520 --> 02:54:30,800
But you know, the AI is it can just be like, look, I have a hundred million of you guys,

2726
02:54:30,800 --> 02:54:34,400
I'm just going to put you on just like really making sure this code is correct. There are no

2727
02:54:34,400 --> 02:54:39,840
bugs. This experiment is thought through every hyperparameter is correct. Final thing I'll say

2728
02:54:39,840 --> 02:54:44,080
is, you know, the 100 million human equivalent AI researchers, that is just a way to visualize it.

2729
02:54:44,080 --> 02:54:47,200
So that doesn't mean you're going to have literally 100 million copies. You know,

2730
02:54:47,200 --> 02:54:51,440
so there's tradeoffs you can make between serial speed and in parallel. So you might make the

2731
02:54:51,440 --> 02:54:55,520
tradeoff is look, we're going to run them at, you know, 10x 100x serial speed. It's going to

2732
02:54:55,520 --> 02:54:59,280
result in fewer tokens overall, because it's sort of inherent tradeoffs. But you know, then we have,

2733
02:54:59,280 --> 02:55:03,520
I don't know what the numbers would be, but then we have, you know, 100,000 of them running at 100x

2734
02:55:03,520 --> 02:55:07,920
human speed and thinking and you know, and there's other things you can do on coordination, you

2735
02:55:07,920 --> 02:55:11,680
know, they can kind of like share latent space, attend to each other's context. There's basically

2736
02:55:11,680 --> 02:55:15,360
this huge range of possibilities of things you can do. The 100 million thing is more, I mean,

2737
02:55:15,360 --> 02:55:19,120
another illustration of this is, you know, if you kind of, I run the math in my series,

2738
02:55:19,120 --> 02:55:22,960
and it's basically, you know, 27, 28, you have this automated AI researcher,

2739
02:55:24,080 --> 02:55:28,640
you're going to be able to generate an entire internet's worth of tokens every single day.

2740
02:55:28,640 --> 02:55:32,560
So it's clearly sort of a huge amount of like intellectual work they can do.

2741
02:55:32,560 --> 02:55:38,960
I think the analogous thing there is today we generate more patents in a year than during the

2742
02:55:38,960 --> 02:55:42,960
actual physics revolution in the early 20th century, they were generating across like half a

2743
02:55:42,960 --> 02:55:47,520
century or something. And are you making more physics progress in a year today than you made?

2744
02:55:47,520 --> 02:55:52,080
So yeah, you're going to generate all these tokens. Are you generating as much

2745
02:55:53,200 --> 02:55:58,160
codified knowledge as humanity has been able to generate in the initial creation of the internet?

2746
02:55:58,160 --> 02:56:02,080
Internet tokens are usually final output, right? A lot of these tokens, if we talk, we talked about

2747
02:56:02,080 --> 02:56:06,480
the unhobbling, right? And I think of a kind of like, you know, a GPDN token is sort of like one

2748
02:56:06,480 --> 02:56:09,920
token of my internal monologue, right? And so that's how I do this math on human equivalents,

2749
02:56:09,920 --> 02:56:13,840
you know, it's like 100 tokens a minute, and then, you know, humans working for X hours and,

2750
02:56:13,840 --> 02:56:18,800
you know, what is the equivalent there? I think this goes back to something we were talking about

2751
02:56:18,800 --> 02:56:24,400
earlier where, well, I haven't seen the huge revenues from people often ask this question,

2752
02:56:24,400 --> 02:56:29,040
that if you took GPD for back 10 years and you show people this and they think this is going to

2753
02:56:29,040 --> 02:56:33,840
automate, this is already automated, half the jobs. And so there's a sort of a modus ponens,

2754
02:56:33,840 --> 02:56:37,600
modus tolens here where part of the explanation is like, oh, it's like just on the verge,

2755
02:56:37,600 --> 02:56:41,920
you need to do these unhobblings. And part of that is probably true. But there is another lesson

2756
02:56:41,920 --> 02:56:48,800
to learn there, which is that just looking at face value outside of abilities, there's probably more

2757
02:56:48,800 --> 02:56:53,040
sort of hobblings that you don't realize that are hidden behind the scenes. I think the same will

2758
02:56:53,040 --> 02:56:57,680
be true of the AGI that you have running as AI researchers. I think a lot of things basically

2759
02:56:57,680 --> 02:57:01,760
agree, right? I think my story here is like, you know, I talk about, I think there's going to be

2760
02:57:01,760 --> 02:57:05,520
some long tail, right? And so part, you know, maybe it's like, you know, 26, 27, you know,

2761
02:57:05,520 --> 02:57:08,560
like the proto-automated engineer, and it's really good at engineering. It doesn't have the

2762
02:57:08,560 --> 02:57:12,800
research intuition yet. You don't quite know how to put them to work. But, you know, the sort of

2763
02:57:12,800 --> 02:57:16,880
even the underlying pace of AI progress is already so fast, right? In three years from not being able

2764
02:57:16,880 --> 02:57:21,760
to do any kind of like math at all to now crushing, crushing these math competitions. And so you have

2765
02:57:21,760 --> 02:57:25,840
the initial thing in like 26, 27, maybe the sort of automated, it's an automated research engineer,

2766
02:57:25,840 --> 02:57:29,600
speeds you up by 2x, you go through a lot more progress in that year. By the end of the year,

2767
02:57:29,600 --> 02:57:33,120
you figured out like the remaining kind of unhobblings, you've like got a smarter model,

2768
02:57:33,120 --> 02:57:36,720
and you know, maybe then that thing, or maybe it's two years, you know, and that thing, just like

2769
02:57:36,720 --> 02:57:40,640
that thing really can do automate 100%. And again, you know, they don't need to be doing

2770
02:57:40,640 --> 02:57:43,680
everything. They don't need to be making coffee, you know, they don't need to like, you know,

2771
02:57:43,680 --> 02:57:47,680
maybe there's a bunch of, you know, tacit knowledge and a bunch of other fields. But you know,

2772
02:57:48,320 --> 02:57:52,560
AI researchers at AI labs really know the job of an AI researcher. And it's in some sense,

2773
02:57:52,560 --> 02:57:56,640
it's a sort of, there's lots of clear metrics, it's all virtual, there's code, it's things you

2774
02:57:56,640 --> 02:58:02,160
can kind of develop and train for. So I mean, another thing is how do you actually manage a

2775
02:58:02,240 --> 02:58:09,120
million AI researchers? Humans, the sort of comparative ability we have that we've been

2776
02:58:09,120 --> 02:58:14,160
especially trained for is like working in teams. And despite this fact, we have, for thousands of

2777
02:58:14,160 --> 02:58:19,200
years, we've been learning about how we work together in groups. And despite this, management

2778
02:58:19,200 --> 02:58:24,720
is a clusterfock, right? It's like most companies are badly managed. It's really hard to do this

2779
02:58:24,720 --> 02:58:35,120
stuff. For AIs, the sort of like, we talk about AGI, but it'll be some bespoke set of abilities,

2780
02:58:35,120 --> 02:58:40,000
some of which will be higher than humans, some of which will be at human level. And so it'll be

2781
02:58:40,000 --> 02:58:46,160
some bundle and we'll need to figure out how to put these bundles together with their human

2782
02:58:46,160 --> 02:58:52,560
overseers, with the equipment and everything. And the idea that as soon as you get the bundle,

2783
02:58:52,560 --> 02:58:57,760
you'll figure out how to get, like just shove millions of them together and manage them.

2784
02:58:57,760 --> 02:59:04,560
I'm just very skeptical of like any other revolution, technological revolution in history

2785
02:59:04,560 --> 02:59:09,600
has been very piecemeal, much more piecemeal than you would expect on paper. If you just thought

2786
02:59:09,600 --> 02:59:14,800
about what is the industrial revolution? Well, we dig up coal that powers the steam engines,

2787
02:59:14,800 --> 02:59:18,720
you use the steam engines to run these railroads, that helps us get more coal out. And there's

2788
02:59:18,720 --> 02:59:23,600
sort of like factorial store, you can tell, where in like a six hours, you can be pumping

2789
02:59:24,240 --> 02:59:29,120
thousands of times more coal. But in real life, it takes centuries often, right? In fact, the

2790
02:59:29,120 --> 02:59:36,000
electrification, there's this famous study about how to initially to electrify factories.

2791
02:59:37,200 --> 02:59:43,760
It was decades after electricity to change from the pull, pulleys and water wheel based

2792
02:59:43,760 --> 02:59:48,240
system that we had for steam engines to one that's works with more spread out electrical

2793
02:59:48,240 --> 02:59:51,120
motors and everything. I think this will be the same kind of thing. It might take like decades

2794
02:59:51,120 --> 02:59:54,800
to actually get millions of AI researchers to work together. Okay, great. This is great.

2795
02:59:54,800 --> 02:59:58,480
Okay, so a few responses to that. First of all, I mean, I totally agree with the kind of like

2796
02:59:58,480 --> 03:00:02,960
real world bottlenecks type of thing. I think this is sort of, you know, I think it's easy to under

2797
03:00:02,960 --> 03:00:06,640
rate, you know, basically what we're doing is we're removing the labor constraint, we automate

2798
03:00:06,640 --> 03:00:10,480
labor and we like kind of exploit technology. But you know, there's still lots of other bottlenecks

2799
03:00:10,480 --> 03:00:13,680
in the world. And so I think it's part of why the story is it kind of like starts pretty narrow at

2800
03:00:13,680 --> 03:00:17,440
the thing where you don't have these bottlenecks. And then only over time as we let it kind of

2801
03:00:17,440 --> 03:00:21,760
expand to sort of broader areas. AI, this is part of why I think it's like initially this sort of

2802
03:00:21,760 --> 03:00:25,520
AI research explosion, right? It's like AI research doesn't run into these real world bottlenecks.

2803
03:00:25,520 --> 03:00:29,520
It doesn't require, you know, like plow a field or dig up coal. It's just you're just doing AI

2804
03:00:29,520 --> 03:00:33,840
research. The other thing, you know, the other thing about like in your model, AI research,

2805
03:00:33,840 --> 03:00:38,320
it's not complicated like about flipping a burger, it's just AI research.

2806
03:00:39,520 --> 03:00:44,480
I mean, this is because people make these arguments like, oh, you know, AGI won't do anything

2807
03:00:44,480 --> 03:00:47,520
because it can't flip a burger. I'm like, yeah, it won't be able to flip a burger, but it's going

2808
03:00:47,520 --> 03:00:51,680
to be able to do algorithmic progress, you know, and then, and then, and then when it does algorithmic

2809
03:00:51,680 --> 03:00:58,560
progress, it'll figure out how to flip a burger. You know, look, the, the, the, sorry, the other

2810
03:00:58,560 --> 03:01:01,840
thing is about, you know, again, these are the sort of quantities are lower bound, right? So

2811
03:01:01,840 --> 03:01:05,840
it's like, this is just like, we can definitely run 100 million of these. Probably what will happen

2812
03:01:05,840 --> 03:01:10,240
is one of the first things we're going to try to figure out is how to like, again, run like,

2813
03:01:10,240 --> 03:01:14,560
you know, translate quantity into quality, right? And so it's like, even at the baseline rate of

2814
03:01:14,560 --> 03:01:18,240
progress, you're like quickly getting smarter and smarter systems, right? If we said it was like,

2815
03:01:18,240 --> 03:01:21,680
you know, four years between the preschooler and the high schooler, right? So I think, you know,

2816
03:01:21,680 --> 03:01:25,280
pretty quickly, you know, there's probably some like simple algorithmic changes you find, you know,

2817
03:01:25,280 --> 03:01:29,280
if instead of one Alec Radford, you have 100, you know, you don't even need 100 million. And then,

2818
03:01:29,280 --> 03:01:33,040
and then you get even smarter systems. And now these systems are, you know, they're capable of

2819
03:01:33,040 --> 03:01:36,560
sort of creative, complicated behavior, you don't understand. Maybe there's some way to like use

2820
03:01:36,640 --> 03:01:39,920
all this test time compute in a more unified way rather than all these parallel copies.

2821
03:01:41,440 --> 03:01:45,680
And, you know, so there won't just be quantitatively superhuman, they'll pretty quickly become

2822
03:01:45,680 --> 03:01:49,280
qualitatively superhuman. You know, it's sort of like, it looked like, you know, you're a high

2823
03:01:49,280 --> 03:01:53,200
school student, you're like trying to wrap yourself, wrap your mind around kind of standard physics.

2824
03:01:53,200 --> 03:01:56,960
And then there's some like super smart professor who is like, quantum physics, it all makes sense

2825
03:01:56,960 --> 03:02:00,880
to him. And you're just like, what is going on? And sort of, I think pretty quickly, you kind of

2826
03:02:00,880 --> 03:02:05,520
enter that regime, just given even the underlying pace of AI progress, but even more quickly than

2827
03:02:05,520 --> 03:02:09,600
that, because you have the sort of accelerated force of now this automated AI research.

2828
03:02:09,600 --> 03:02:15,920
I agree that over time, you would, I'm not denying that ASI is, I think that's possible.

2829
03:02:15,920 --> 03:02:16,560
Because the time is just not that much, you know?

2830
03:02:16,560 --> 03:02:20,400
I'm just like, you know, how is this happening in a year? Like you've, okay, first of all.

2831
03:02:20,400 --> 03:02:23,600
So I think the story is sort of like, basically, I think it's a little bit more continuous,

2832
03:02:23,600 --> 03:02:27,040
you know, right? Like, I think already, you know, like I talked about, you know, 25, 26,

2833
03:02:27,040 --> 03:02:29,600
you're basically going to have models as good as a college graduate. And I, you know,

2834
03:02:30,240 --> 03:02:33,520
I don't, I don't know where the unhobbling is going to be. But I think it's plausible that even

2835
03:02:33,520 --> 03:02:37,840
then you have kind of the proto-automated engineer. So there's, I think there is a bit of like a smear,

2836
03:02:37,840 --> 03:02:41,440
kind of an AGI smear or whatever, where it's like, there's sort of unhobblings that you're

2837
03:02:41,440 --> 03:02:44,640
missing. There's kind of like ways of connecting them you're missing. There's like some level

2838
03:02:44,640 --> 03:02:48,000
intelligence you're missing. But then at some point, you are going to get the thing that is like

2839
03:02:48,000 --> 03:02:53,680
an 100% automated Alec Radford. And once you have that, you know, things really take off, I think.

2840
03:02:54,560 --> 03:02:59,520
Yeah. Okay. So let's go back to the unhobblings. Is there, we're going to get a bunch of models

2841
03:02:59,520 --> 03:03:04,400
by the end of the year. Is there something, let's suppose we didn't get some capacity by the end of

2842
03:03:04,400 --> 03:03:08,800
the year. Is there some such capacity, which lacking would suggest that AI progress is going to take

2843
03:03:08,800 --> 03:03:12,800
longer than you are projecting? Yeah. I mean, I think there's, there's two kind of key things.

2844
03:03:12,800 --> 03:03:15,680
There's the unhobbling and there's the data wall, right? I think we should talk about the data wall

2845
03:03:15,680 --> 03:03:19,840
for a moment. I think the data wall is, you know, even though kind of like all this stuff has been

2846
03:03:19,840 --> 03:03:22,800
about, you know, crazy AI progress, I think the data wall is actually sort of underrated. I think

2847
03:03:22,800 --> 03:03:26,880
there's like a real scenario where we're just stagnating. You know, because we've been running

2848
03:03:26,880 --> 03:03:30,400
this tailwind of just like, it's really easy to bootstrap and you just do unsupervised learning

2849
03:03:30,400 --> 03:03:34,480
next token prediction that learns these amazing world models, like bam, you know, great model,

2850
03:03:34,480 --> 03:03:37,760
and you just got to buy some more compute, you know, do some simple efficiency changes,

2851
03:03:38,960 --> 03:03:42,080
you know, and, and again, like so much of deep learning, all these like big gains on

2852
03:03:42,080 --> 03:03:45,680
efficiency have been like pretty dumb things, right? Like, you know, you add a normalization layer,

2853
03:03:45,680 --> 03:03:49,760
you know, you know, you fix the scaling laws, you know, and these already have been huge things,

2854
03:03:49,760 --> 03:03:54,320
let alone kind of like obvious ways in which these models aren't good yet. Anyway, so data

2855
03:03:54,320 --> 03:03:59,120
wall big deal, you know, I don't know, some like put some numbers on this, you know,

2856
03:03:59,760 --> 03:04:03,920
some like you do common crawl, you know, online is like, you know, 30 trillion tokens,

2857
03:04:03,920 --> 03:04:07,840
llama three trained on 15 trillion tokens. So you're basically already using all the data.

2858
03:04:07,840 --> 03:04:11,600
And then, you know, you can get somewhat further by repeating it. So there's an academic paper by,

2859
03:04:11,600 --> 03:04:16,320
you know, Boaz Barak and some others that does scaling laws for this. And they're basically

2860
03:04:16,320 --> 03:04:20,880
like, yeah, you can repeat it sometime. After 16 times of reputation, just like returns basically

2861
03:04:20,880 --> 03:04:24,720
go to zero, you're just completely screwed. And so I don't know, say you can get another 10x on

2862
03:04:24,720 --> 03:04:29,120
data from, say like llama three, and GP four, you know, llama three is already kind of like

2863
03:04:29,120 --> 03:04:32,640
at the limit of all the data, you know, maybe you can get 10x more by repeating data.

2864
03:04:33,680 --> 03:04:38,080
You know, I don't know, maybe that's like at most 100x better model than GP for, which is like,

2865
03:04:38,080 --> 03:04:42,000
you know, 100x effective compute from GP four is, you know, not that much, you know, if you do half

2866
03:04:42,000 --> 03:04:45,360
in order magnitude a year of compute half in order magnitude a year of algorithmic progress,

2867
03:04:45,360 --> 03:04:49,760
you know, that's kind of like two years from GP four. So, you know, GP four finished pretreading

2868
03:04:49,760 --> 03:04:56,240
in 22, you know, 24. So I think one thing that really matters, I think we won't quite know by

2869
03:04:56,240 --> 03:05:00,560
end of the year, but you know, 25, 26, are we cracking the data wall?

2870
03:05:02,400 --> 03:05:08,640
Okay, so suppose we had three orders of magnitude less data in common crawl on the internet than

2871
03:05:08,640 --> 03:05:14,720
we just happen to have now. And for decades, the internet, other things, we've been rapidly

2872
03:05:14,720 --> 03:05:21,120
increasing the stock of data that humanity has. Is it your view that for contingent reasons,

2873
03:05:21,120 --> 03:05:28,560
we just happen to have enough data to train models that are just powerful enough at 4.5 level,

2874
03:05:28,560 --> 03:05:36,240
where they can kick off the self play RL loop? Or is it just that we, you know, if it had been

2875
03:05:36,240 --> 03:05:40,880
three rooms higher, then it would progress would have been slightly faster. In that world, we

2876
03:05:40,880 --> 03:05:43,920
would have been looking back at like, oh, how hard it would have been to like kick off the RL

2877
03:05:44,000 --> 03:05:48,240
explosion with just 4.5, but we would have figured it out. And then so in this world, we would have

2878
03:05:48,240 --> 03:05:52,880
gotten to GP three and then we'd have to kick us on sort of RL explosion. But we would have still

2879
03:05:52,880 --> 03:05:56,880
figured it out. The sort of the we didn't just like luck out on the amount of data we happen to

2880
03:05:56,880 --> 03:06:00,480
have in the world. I mean, three rooms is pretty rough, right? Like three rooms, if less data means

2881
03:06:00,480 --> 03:06:04,400
like six rooms smaller, six rooms, less compute model and scale scaling laws, you know, that's

2882
03:06:04,400 --> 03:06:08,880
it's basically like capping out at like GP two, but I think that would be really rough. I think

2883
03:06:08,880 --> 03:06:13,360
you do make an interesting point about the contingency. You know, I guess earlier, we were

2884
03:06:13,360 --> 03:06:17,040
talking about the sort of like when in the sort of human trajectory, are you able to learn from

2885
03:06:17,040 --> 03:06:21,120
yourself? And so, you know, if we go with that analogy, again, like if you'd only gotten the

2886
03:06:21,120 --> 03:06:24,160
preschooler model, it can't learn from itself. You know, if you only got in the elementary

2887
03:06:24,160 --> 03:06:28,000
school or model, can't learn from itself. And you know, maybe GP for, you know, smart high

2888
03:06:28,000 --> 03:06:31,520
school is really where it starts. Ideally, you have a somewhat better model than it really is

2889
03:06:31,520 --> 03:06:36,240
able to kind of like learn from itself or learn by itself. So yeah, I think there's an interesting,

2890
03:06:36,240 --> 03:06:41,280
I mean, I think maybe one room less data, I would be like more iffy, but maybe still doable.

2891
03:06:42,080 --> 03:06:44,720
Yeah, I think it would feel chiller if we had, you know, like one or two.

2892
03:06:44,720 --> 03:06:49,040
It would be an interesting exercise to get probably distributions of HEI contingent on

2893
03:06:49,040 --> 03:06:54,160
across like data. Yeah, okay. I think the thing that makes me skeptical of this story

2894
03:06:54,160 --> 03:06:59,440
is that the things it totally makes sense for free training works so well. Yeah. These other

2895
03:06:59,440 --> 03:07:06,160
things, their stories of in principle, why they ought to work like a humans can learn this way

2896
03:07:06,240 --> 03:07:11,440
and so on. Yes. And maybe they're true, but I worry that a lot of this case is based on

2897
03:07:11,440 --> 03:07:17,280
sort of first principles with evaluation of how learning happens that fundamentally,

2898
03:07:17,280 --> 03:07:20,800
we don't understand how humans learn and maybe there's some key thing we're missing.

2899
03:07:20,800 --> 03:07:24,720
Yeah. On the sort of sample efficiency, yeah, humans actually, maybe there's,

2900
03:07:25,920 --> 03:07:29,680
you say, well, the fact that these things are way of a less sample efficient in terms of learning

2901
03:07:29,680 --> 03:07:33,680
than humans are suggests that there's a lot of room for improvement. Yeah. Another perspective

2902
03:07:33,680 --> 03:07:38,640
is that we are just on the wrong path altogether, right? That's why there's a sample inefficient

2903
03:07:38,640 --> 03:07:43,920
when it comes to pre-training. Yeah. So, yeah, I mean, I'm just like, there's a lot of like

2904
03:07:44,720 --> 03:07:47,920
first principles arguments stack on top of each other where you get these unhoplings and then

2905
03:07:47,920 --> 03:07:52,640
you get to HEI. Yeah. Then you, because of these reasons why you can stack all these things on

2906
03:07:52,640 --> 03:07:56,720
top of each other, you get to ASI. Yeah. And I'm worried that there's too many steps of this.

2907
03:07:56,720 --> 03:08:03,520
Yeah. Sort of first principles thinking. I mean, we'll see, right? I mean, on the sort

2908
03:08:03,520 --> 03:08:07,840
of sample efficiency thing, again, sort of first principles, but I think, again, there's this

2909
03:08:07,840 --> 03:08:13,280
clear sort of missing middle. And so, you know, and sort of like, you know, people hadn't been

2910
03:08:13,280 --> 03:08:17,840
trying. Now people are really trying, you know, and so it's sort of, you know, I think often again

2911
03:08:17,840 --> 03:08:21,680
in deep learning, something like the obvious thing works. And there's a lot of details to get

2912
03:08:21,680 --> 03:08:24,880
right. So it might take some time, but it's now people are really trying. So I think we get a

2913
03:08:24,880 --> 03:08:32,880
lot of signal in the next couple of years. You know, on a hobbling, I mean, what is the signal

2914
03:08:32,960 --> 03:08:36,320
on hobbling that I think would be interesting? I think, I think the question is basically like,

2915
03:08:36,320 --> 03:08:40,080
are you making progress on this test time compute thing, right? Like is this thing able to think

2916
03:08:40,080 --> 03:08:43,440
longer horizon than just a couple hundred tokens, right? That was unlocked by chain of thought.

2917
03:08:43,920 --> 03:08:49,120
And on that point in particular, the many people who have longer timelines have come on the podcast

2918
03:08:49,120 --> 03:08:55,920
have made the point that the way to train this long horizon RL, it's not, I mean, earlier talking

2919
03:08:55,920 --> 03:09:00,320
about like, well, they can think for five minutes, but not for longer. But it's not because they

2920
03:09:00,320 --> 03:09:05,040
can't physically output an hours or the tokens. It's just really, at least from what I understand

2921
03:09:05,040 --> 03:09:08,640
what they say, right? Like even like Gemini has like a million in context and the million of context

2922
03:09:08,640 --> 03:09:12,400
is actually great for consumption. And it solves one important on hobbling, which is the sort of

2923
03:09:12,400 --> 03:09:18,000
onboarding problem, right? Which is, you know, a new coworker, you know, in your first five minutes,

2924
03:09:18,000 --> 03:09:21,920
like a new smart high school intern first five minutes, not useful at all, a month in, you know,

2925
03:09:21,920 --> 03:09:25,760
much more useful, right? Because they've like looked at the mono repo and understand how the

2926
03:09:25,760 --> 03:09:29,520
code works. And they've read your internal docs. And so being able to put that in context, great,

2927
03:09:29,520 --> 03:09:33,360
solve this onboarding problem. Yeah, but they're not good at sort of the production of a million

2928
03:09:33,360 --> 03:09:39,840
tokens yet. Yeah, right. But on the production of a million tokens, there's no public evidence that

2929
03:09:39,840 --> 03:09:45,920
there's some easy loss function where you can GP for has gotten a lot better since it's actually

2930
03:09:45,920 --> 03:09:50,960
so the GP for gains since launch, I think are a huge indicator that there's like, you know, so

2931
03:09:50,960 --> 03:09:55,040
you talked about this with John on the podcast, John said this was mostly post training gains.

2932
03:09:55,120 --> 03:10:00,080
Right. You know, if you look at the sort of LMS scores, you know, it's like 100 ELO or something,

2933
03:10:00,080 --> 03:10:04,000
it's like a bigger gap than between Claude III, Opus and Claude III, Haiku. And the price difference

2934
03:10:04,000 --> 03:10:09,600
between those is 60X. But it's not more agentic. It's like better in the same channel. Right? Like,

2935
03:10:09,600 --> 03:10:14,000
you know, it went from like, you know, 40% to 70% math. The crux is like whether or like be able to

2936
03:10:14,000 --> 03:10:17,440
like, but I think I think it indicates that clearly there's stuff to be done on hobbling.

2937
03:10:18,240 --> 03:10:21,840
I think yeah, I think I think the interesting question is like this time of year from now,

2938
03:10:21,840 --> 03:10:25,920
you know, is there a model that is able to think for like, you know, a few thousand tokens

2939
03:10:25,920 --> 03:10:30,000
coherently, cohesively, agentically. And I think probably there's, you know,

2940
03:10:30,000 --> 03:10:33,840
again, this is what I'd feel better if we had an humor to more data, because it's like the scaling

2941
03:10:33,840 --> 03:10:38,480
just gives you this sort of like tailwind, right? We're like, for example, tools, right tools, I

2942
03:10:38,480 --> 03:10:41,840
think, you know, talking to people who try to make things work with tools, you know, actually

2943
03:10:41,840 --> 03:10:46,080
sort of GP for is really when tools start to work. And it's like, you can kind of make them work with

2944
03:10:46,080 --> 03:10:51,440
GP 3.5, but it's just really tough. And so it's just like having GP for you can kind of help it

2945
03:10:51,440 --> 03:10:57,120
learn tools in a much easier way. And so just a bit more tailwind from scaling. And then and then

2946
03:10:57,120 --> 03:11:02,080
yeah, and does does I don't know if it'll work, but it's a key question. Okay, I think it's a good

2947
03:11:02,080 --> 03:11:07,680
place to sort of close that part where we know what the crux is and what the progress, what

2948
03:11:07,680 --> 03:11:13,920
what evidence that would look like on the AGI to super intelligence. Maybe it's a case that the

2949
03:11:13,920 --> 03:11:18,000
games are really easier right now, and you can just sort of let loose and Alec Ratt for giving

2950
03:11:18,080 --> 03:11:22,320
a compute budget, and he comes out the other end with something that is an additive,

2951
03:11:23,600 --> 03:11:26,800
like change as part of the code, this is compute multiplier changes to the part.

2952
03:11:28,320 --> 03:11:34,400
What other parts of the world? Maybe there here's an interesting way to ask this. Yeah, how many

2953
03:11:34,400 --> 03:11:41,280
other domains in the world are like this, where you think you could get the equivalent of in one

2954
03:11:41,280 --> 03:11:47,040
year, you just throw enough intelligence across multiple instances, and you just come out the

2955
03:11:47,120 --> 03:11:54,720
other end with something that is remarkably decades centuries ahead. Yeah, like you start off with

2956
03:11:54,720 --> 03:12:00,400
no flight and then the right brother is a million instances of GPT six, and you come out the other

2957
03:12:00,400 --> 03:12:07,120
end with starlink. Yeah. Like is that your model of how things work? I think I think you're exaggerating

2958
03:12:07,120 --> 03:12:10,400
the timelines a little bit, but but you know, I think you know, a decade's worth of progress in

2959
03:12:10,400 --> 03:12:15,200
a year or something. I think that's a reasonable prompt. So I think this is where, you know,

2960
03:12:15,840 --> 03:12:19,760
basically the sort of automated AI researcher comes in because it gives you this enormous tail

2961
03:12:19,760 --> 03:12:23,600
headwind on all the other stuff, right? So it's like, you know, you automate AI research with

2962
03:12:23,600 --> 03:12:27,280
your sort of automated Alec Radford's, you come out the other end, you've done another five booms,

2963
03:12:27,280 --> 03:12:32,080
you have a thing that is like vastly smarter, not only is it vastly smarter, you like, you know,

2964
03:12:32,080 --> 03:12:35,120
you've been able to make it good at everything else, right? You're like, you're solving robotics,

2965
03:12:35,120 --> 03:12:38,560
the robots are important, right? Because like, for a lot of other things, you do actually need to

2966
03:12:38,560 --> 03:12:42,560
like try things in the physical world. I mean, I don't know, maybe you can do a lot in simulation,

2967
03:12:42,560 --> 03:12:46,080
those are the really quick worlds. I don't know if you saw the like last Nvidia GTC, you know,

2968
03:12:46,080 --> 03:12:49,760
it was all about the like digital twins and just like having all your manufacturing processes and

2969
03:12:49,760 --> 03:12:53,840
simulation, like, I don't know, like, again, if you have these like, you know, super intelligent,

2970
03:12:53,840 --> 03:12:57,040
like cognitive workers, like, can they just like make simulations of everything, you know,

2971
03:12:57,040 --> 03:13:01,600
kind of off the float style, and then, and then, you know, make a lot of progress and simulation

2972
03:13:01,600 --> 03:13:07,440
possible. But I also just think you're going to get the robots. Again, I agree about like,

2973
03:13:07,440 --> 03:13:12,160
there are a lot of real world bottlenecks, right? And so, you know, I don't know, it's quite possible

2974
03:13:12,160 --> 03:13:15,520
that we're going to have, you know, crazy drone forms, but also, you know, like lawyers and

2975
03:13:15,520 --> 03:13:20,640
doctors still need to be humans because of like, you know, regulation. But, you know, I think,

2976
03:13:20,640 --> 03:13:24,400
you know, you kind of start narrowly, you broaden, and then the world's in which you kind of let them

2977
03:13:24,400 --> 03:13:27,520
loose, which again, because of I think these competitive pressures, we will have to let them

2978
03:13:27,520 --> 03:13:33,600
loose in some degree on, you know, various national security applications. I think like,

2979
03:13:33,600 --> 03:13:37,760
quite rapid progress is possible. The other thing, though, is it's sort of, you know,

2980
03:13:37,760 --> 03:13:40,880
basically in the sort of an explosion after there's kind of two components, there's the A,

2981
03:13:40,880 --> 03:13:44,400
right in the production function, like growth of technology, and that's massively accelerated

2982
03:13:44,400 --> 03:13:48,240
by, you know, you have a billion super intelligent scientists and engineers and technicians,

2983
03:13:48,240 --> 03:13:52,720
you know, superbly competent and everything. You also just automated labor, right? And so,

2984
03:13:52,720 --> 03:13:56,480
it's like, even without the whole technological explosion thing, you have this industrial explosion,

2985
03:13:56,480 --> 03:13:59,840
at least if you let them let them loose, which is like, now you can just build, you know,

2986
03:13:59,840 --> 03:14:03,840
you can cover Nevada and like, you know, you start with one robot factory is producing more robots,

2987
03:14:03,840 --> 03:14:08,640
and basically this like just the cumulative process because you've taken labor out of the equation.

2988
03:14:08,880 --> 03:14:14,720
Yeah. That's super interesting. Yeah. Although when you increase the

2989
03:14:15,360 --> 03:14:22,400
K or the L without increasing the A, you can look at the Soviet Union or China where they

2990
03:14:22,400 --> 03:14:27,920
rapidly increase inputs. Yeah. And that does have the effect of being geopolitically game changing,

2991
03:14:27,920 --> 03:14:33,440
where you, it is remarkable, like you go to Shanghai over a set of these crazy cities in

2992
03:14:33,440 --> 03:14:36,800
a decade. Right. Right. I mean, the closest thing to like, people talk about 30% growth

2993
03:14:36,800 --> 03:14:43,200
rates or whatever. Yeah. 10%. It's totally possible. Yeah. But without productivity gains,

2994
03:14:43,200 --> 03:14:47,360
it's not like the industrial revolution, where like, you're from the perspective of

2995
03:14:47,360 --> 03:14:50,480
you're looking at a system from the outside, your goods have gotten cheaper, or they can

2996
03:14:50,480 --> 03:14:55,680
manufacture more things. But, you know, it's not like the next century is coming at you.

2997
03:14:55,680 --> 03:14:58,560
Yeah. It's both. It's both. So it's, you know, both that are important. The other thing I'll say

2998
03:14:58,560 --> 03:15:02,480
is like, all this stuff, I think the magnitudes are really, really important, right? So,

2999
03:15:03,280 --> 03:15:07,680
you know, we talked about a 10x of research effort, or maybe 10, 30x over a decade,

3000
03:15:07,680 --> 03:15:11,920
you know, even without any kind of like self-improvement type loop, you know, we talk,

3001
03:15:11,920 --> 03:15:15,520
the sort of, even in the sort of Jeep before the AGI story, we're talking about an order of magnitude

3002
03:15:15,520 --> 03:15:19,280
of effective compute increase a year, right? Half an order of magnitude of compute, half an order

3003
03:15:19,280 --> 03:15:23,680
of magnitude of algorithmic progress that sort of translates into effective compute. And so,

3004
03:15:24,560 --> 03:15:28,480
you're doing a 10x a year, right? Basically on your labor force, right? So it's like,

3005
03:15:28,480 --> 03:15:32,080
it's a radically different world if you're doing a 10x or 30x in a century versus a

3006
03:15:32,080 --> 03:15:36,160
10x a year on your labor force. So the magnitudes really matter. They also really matter on the

3007
03:15:36,160 --> 03:15:39,840
sort of intelligence explosion, right? So like just the automated AI research part. So, you know,

3008
03:15:39,840 --> 03:15:43,920
one story you could tell there is like, well, ideas get harder to find, right? Algorithmic

3009
03:15:43,920 --> 03:15:47,120
progress is going to get harder. Yeah, right now you have the easy wins, but in like four or five

3010
03:15:47,120 --> 03:15:51,120
years, there'll be fewer easy wins. And so the sort of automated AI researchers are just going

3011
03:15:51,120 --> 03:15:54,560
to be what's necessary to just keep it going, right? Because it's gotten harder. But that's

3012
03:15:54,560 --> 03:15:57,920
sort of, it's like a really weird knife edge assumption economics where you assume it's just

3013
03:15:58,000 --> 03:16:02,320
enough. But isn't that the equilibrium story you were just telling with why the economy as a whole

3014
03:16:02,320 --> 03:16:06,400
has 2% economic growth? Because you just pursued on the equal, I guess you're saying by the time

3015
03:16:06,400 --> 03:16:10,240
you get to the equilibrium here is it's like way faster, at least, you know, and it's at least,

3016
03:16:10,240 --> 03:16:13,280
and it depends on the sort of exponents, but it's basically it's the increase that like,

3017
03:16:13,280 --> 03:16:17,200
suppose you need to like 10x effective research effort in AI research in the last,

3018
03:16:17,200 --> 03:16:20,480
you know, four or five years to keep the pace of progress, we're not just getting a 10x, you're

3019
03:16:20,480 --> 03:16:23,680
getting, you know, a million x or a hundred thousand x, there's just the magnitudes really

3020
03:16:23,680 --> 03:16:27,600
matter. And the magnitude is just basically, you know, one, one way to think about this is that

3021
03:16:27,600 --> 03:16:31,520
you have kind of two exponentials, you have your sort of like normal economy that's growing at,

3022
03:16:31,520 --> 03:16:36,000
you know, 2% a year, and you have a really AI economy, and that's going at like 10x a year.

3023
03:16:36,000 --> 03:16:39,920
And it's starting out really small, but sort of eventually it's going to, it's just, it's, it's,

3024
03:16:39,920 --> 03:16:43,920
it's way faster and eventually it's going to overtake, right? And even if you have, you can

3025
03:16:43,920 --> 03:16:47,520
almost sort of just do the simple revenue extrapolation, right? If you think your AI economy,

3026
03:16:47,520 --> 03:16:51,440
you know, that has some growth rate, I mean, it's very simplistic way and so on. But there's,

3027
03:16:51,520 --> 03:16:55,040
there's this sort of 10x a year process and that will eventually kind of like,

3028
03:16:55,040 --> 03:16:59,200
you're going to transition the sort of whole economy from, as it broadens from the sort of,

3029
03:16:59,200 --> 03:17:04,320
you know, 2% a year to the sort of much faster growing process. And I don't know, I think that's

3030
03:17:04,320 --> 03:17:09,600
very like consistent with historical change, you know, stories, right? There's this sort of like,

3031
03:17:10,480 --> 03:17:14,880
you know, there's this sort of long run hyperbolic trend, you know, manifested in the sort of like,

3032
03:17:14,880 --> 03:17:18,640
sort of change in growth mode and the austral, you know, revolution, but there's just this

3033
03:17:18,640 --> 03:17:23,040
long run hyperbolic trend. And, you know, now you have this sort of, now you have that another

3034
03:17:23,040 --> 03:17:26,480
sort of change in growth mode. Yeah, yeah. I mean, that was one of the questions I asked Tyler,

3035
03:17:26,480 --> 03:17:33,600
when I had him on the podcast, is that you do go from the fact that after 1776, you go from a regime

3036
03:17:33,600 --> 03:17:38,880
of negligible economic growth, 2% is really interesting. It shows that, I mean, from the

3037
03:17:38,880 --> 03:17:44,160
perspective of somebody in the Middle Ages or before, 2% is equivalent to the sort of 10%.

3038
03:17:44,960 --> 03:17:47,760
I guess you're projecting even higher for the AI economy, but

3039
03:17:47,760 --> 03:17:51,200
yeah, I mean, I think again, and it's all this stuff, you know, I have a lot of uncertainty,

3040
03:17:51,200 --> 03:17:54,480
right? So a lot of the time I'm trying to kind of tell the modal story. I think it's important

3041
03:17:54,480 --> 03:17:58,800
to be kind of concrete and visceral. And I, you know, I have, I have a lot of uncertainty

3042
03:17:58,800 --> 03:18:02,720
basically over how the 2030s play out. And basically, the thing I know is it's going to be

3043
03:18:02,720 --> 03:18:07,760
fucking crazy. But, but, you know, exactly what, you know, where the bottlenecks are and so on,

3044
03:18:07,760 --> 03:18:13,120
I think that will be kind of like. So let's talk through the numbers here. You hundreds of millions

3045
03:18:13,120 --> 03:18:22,160
of AI researchers. So right now, GPT-40 turbo is like 15 bucks for a million tokens outputted.

3046
03:18:22,160 --> 03:18:27,360
And a human thinks 150 tokens a minute or something. And if you do the math on that,

3047
03:18:27,360 --> 03:18:34,880
I think it's for an hour's worth of human output. You, it's like 10 cents or something.

3048
03:18:35,760 --> 03:18:40,240
Now, cheaper than a human worker. Cheaper than a human worker. But it can't do the job yet.

3049
03:18:40,240 --> 03:18:43,920
That's right. That's right. But by the time you're talking about models that are trained on the 10

3050
03:18:43,920 --> 03:18:50,160
gigawatt cluster, then you have something that is four orders of magnitude more expensive via

3051
03:18:50,160 --> 03:18:55,520
inference, three orders of magnitude, something like that. So that's like $100 an hour of labor.

3052
03:18:55,520 --> 03:19:01,760
And now you're having hundreds of millions of such laborers. Is there enough compute to do

3053
03:19:01,760 --> 03:19:06,000
with the model that is a thousand times bigger, this kind of labor?

3054
03:19:06,000 --> 03:19:09,840
Great. Okay. Great question. So I actually don't think inference costs for sort of frontier

3055
03:19:09,840 --> 03:19:13,280
models are necessarily going to go up that much. So I mean, one historical data point.

3056
03:19:13,280 --> 03:19:16,320
But isn't the test time sort of thing that it will go up even higher?

3057
03:19:16,320 --> 03:19:19,200
I mean, we're just doing per token, right? And then I'm just saying, you know, if,

3058
03:19:19,200 --> 03:19:23,440
suppose each model token was the same as sort of a human token thing at 100 tokens a minute.

3059
03:19:23,440 --> 03:19:26,640
So it's like, yeah, it'll use more. But the sort of, if you just, the token calculations

3060
03:19:26,640 --> 03:19:33,200
is already pricing that in the, the question is like per token pricing, right? And so like GPT-3

3061
03:19:33,200 --> 03:19:39,120
went at launch was like actually more expensive than GPT-4 now. And so over just like, you know,

3062
03:19:39,120 --> 03:19:42,480
fast increases in capability gains, inference costs has remained constant.

3063
03:19:43,840 --> 03:19:47,440
That's sort of wild. And I think it's worth appreciating. And I think it gestures that

3064
03:19:47,440 --> 03:19:52,080
sort of an underlying pace of algorithmic progress. I think there's a sort of like more

3065
03:19:52,080 --> 03:19:55,840
theoretically grounded way to why, why inference costs would stay constant. And it's the fourth

3066
03:19:55,840 --> 03:19:59,840
following story, right? So on Chichilly scaling laws, right, you, you know,

3067
03:19:59,840 --> 03:20:04,000
half of the additional compute you allocate to bigger models and half of it you allocate to more

3068
03:20:04,000 --> 03:20:08,720
data, right? But also if we go with the sort of basic story of half an order a year more

3069
03:20:08,720 --> 03:20:12,560
compute and half an order of magnitude a year of algorithmic progress, you're also kind of like

3070
03:20:12,560 --> 03:20:16,320
you're saving half an order of magnitude a year. And so that kind of would exactly compensate for

3071
03:20:16,320 --> 03:20:20,240
making the model bigger. The caveat on that is, you know, obviously not all training efficiencies

3072
03:20:20,240 --> 03:20:24,560
are also inference efficiencies, you know, bunch of the time they are separately, you can find

3073
03:20:24,560 --> 03:20:28,480
inference efficiencies. So I don't know, given this historical trend, given the sort of like,

3074
03:20:28,480 --> 03:20:35,360
you know, baseline sort of theoretical reason, you know, I don't know, I, I think it's not crazy

3075
03:20:35,520 --> 03:20:38,800
baseline assumption that actually these models, frontier models are not necessarily going to get

3076
03:20:38,800 --> 03:20:44,480
more expensive per token. Oh, really? Yeah. Like, okay, that's, that's wild. We'll see, we'll see.

3077
03:20:44,480 --> 03:20:47,600
I mean, the other thing, you know, maybe they get, you know, even if they get like 10x more

3078
03:20:47,600 --> 03:20:50,720
expensive than, you know, you have 10 million instead of 100 million, you know, so it's like,

3079
03:20:50,720 --> 03:20:55,360
it's not really, you know, like, but okay, so part of the internal explosion is that each of them has

3080
03:20:55,360 --> 03:21:03,520
to run experiments that are gb4 size and the result of, so that takes up a bunch of compute.

3081
03:21:03,760 --> 03:21:07,840
Then you're going to consolidate the results of the experiments and what is the synthesized

3082
03:21:08,720 --> 03:21:11,120
weight. I mean, you have a much bigger influence street anyway than your training.

3083
03:21:11,120 --> 03:21:13,840
Sure. Okay. But I think the experiment compute is a constraint.

3084
03:21:13,840 --> 03:21:19,040
Yeah. Okay. I'm going back to maybe a sort of bigger fundamental thing we're talking about here.

3085
03:21:20,400 --> 03:21:28,400
We're projecting, in a series, you say we should denominate the probability of getting to AGI in

3086
03:21:28,400 --> 03:21:34,240
terms of orders of magnitude of affected compute effective here, accounting for the fact that

3087
03:21:34,240 --> 03:21:38,000
there's a compute quote unquote compute multiplier if you have better algorithms.

3088
03:21:39,520 --> 03:21:47,200
And I'm not sure that it makes sense to be confident that this is a sensible way to project

3089
03:21:47,200 --> 03:21:52,240
progress. It might be, but I'm just like, I have a lot of uncertainty about it. It seems similar to

3090
03:21:52,240 --> 03:21:56,400
somebody trying to project when we're going to get to the moon and they're like looking at the

3091
03:21:56,400 --> 03:22:01,520
Apollo program in the four fifties or something and they're like, we have some amount of effective

3092
03:22:01,520 --> 03:22:09,200
jet fuel. And if we get more efficient engines, then we have more effective jet fuel. And so

3093
03:22:09,200 --> 03:22:13,200
we're going to like probability of getting to the moon based on the amount of effective jet fuel we

3094
03:22:13,200 --> 03:22:18,800
have. And I don't deny that jet fuel is important to launch rockets, but that seems like an odd

3095
03:22:18,800 --> 03:22:23,760
way to denominate when you're going to get to the moon. Yeah. Yeah. So I mean, I think these cases

3096
03:22:23,760 --> 03:22:26,880
are pretty different. I don't know. I didn't, I don't think there was a sort of clear, I don't

3097
03:22:26,880 --> 03:22:30,080
know how rocket science works, but I didn't, I didn't get the impression that there's some

3098
03:22:30,080 --> 03:22:35,520
clear scaling behavior with like, you know, the amount of jet fuel. I think the, I think in AI,

3099
03:22:35,520 --> 03:22:39,680
you know, I mean, first of all, the scaling laws, you know, they've just helped, right? And so if

3100
03:22:39,680 --> 03:22:43,120
you, a friend of mine pointed this out, and I think it's a great point, if you kind of concatenate

3101
03:22:43,120 --> 03:22:47,440
both the sort of original Kaplan scaling laws paper that I think went from 10 to the negative

3102
03:22:47,440 --> 03:22:53,040
nine to 10 petaflop days, and then, you know, concatenate additional compute to from there to

3103
03:22:53,120 --> 03:22:56,800
kind of GP four, you assume some algorithmic progress, you know, it's like the scaling laws

3104
03:22:56,800 --> 03:23:00,640
have held, you know, like probably over 15 ooms, you know, I know it was rough, probably maybe

3105
03:23:00,640 --> 03:23:05,120
even more held for a lot of ooms. They held for the specific loss function, which they're training

3106
03:23:05,120 --> 03:23:11,520
on, which is training max token. Whereas the, the, the progress you are forecasting will be

3107
03:23:11,520 --> 03:23:16,640
required for further progress in capabilities. Yeah. It was specifically, we know that scaling

3108
03:23:16,640 --> 03:23:20,400
can't work because of the data wall. And so there's some new thing that has to happen. And I'm not

3109
03:23:20,480 --> 03:23:25,680
sure whether the, you can extrapolate that same scaling curve to tell us whether these

3110
03:23:25,680 --> 03:23:29,440
hobblings will also, like it's, is this not on the same graph? The hobblings are just a separate

3111
03:23:29,440 --> 03:23:33,600
thing. Yeah, exactly. So this is, this is sort of like, you know, it's, yeah. So I mean, a few,

3112
03:23:33,600 --> 03:23:38,880
a few things here, right? Okay. So the, on the, on the effective compute scaling, the, you know,

3113
03:23:38,880 --> 03:23:42,080
in some sense, I think it's like people center the scaling laws because they're easy to explain

3114
03:23:42,080 --> 03:23:47,040
and the sort of like, why, why is scaling matter? The scaling laws like came way after people, at

3115
03:23:47,040 --> 03:23:50,560
least, you know, like Dario, Ilya realized that scaling mattered. And I think, you know, I think

3116
03:23:50,560 --> 03:23:54,800
that almost more important than the sort of loss curve is just like, just in general, make, you

3117
03:23:54,800 --> 03:23:58,080
know, there's this great quote from Dario on your, on your, on your, on your podcasts, it's just like,

3118
03:23:58,080 --> 03:24:01,520
you know, Ilya was like, the models, they just want to learn, you know, you make them bigger,

3119
03:24:01,520 --> 03:24:06,320
they learn more. And, and that just applied just across domains, generally, you know, all the

3120
03:24:06,320 --> 03:24:10,560
capabilities. And so, um, and you can look at this in benchmarks. Again, like you say,

3121
03:24:10,560 --> 03:24:14,560
headwind data wall, I'm sort of bracketing that and talking about that separately.

3122
03:24:14,640 --> 03:24:18,480
The other thing is on hobblings, right? If you just put them on the effective compute graph,

3123
03:24:18,480 --> 03:24:21,360
these on hobblings would be kind of huge, right? So like, I think,

3124
03:24:21,360 --> 03:24:24,080
What does it even mean? Like, what is it? What is on the y axis here?

3125
03:24:24,960 --> 03:24:29,520
Like, say MLPR on this benchmark or whatever, right? And so, you know, like, you know, we mentioned

3126
03:24:29,520 --> 03:24:33,920
the sort of, you know, the LMSYS differences, you know, RLHF, you know, again, as good as 100x

3127
03:24:33,920 --> 03:24:37,360
small chain of thought, right? You know, just going from this prompting change, a simple argument

3128
03:24:37,360 --> 03:24:41,600
change can be like 10x effective increase, compute increases on like math benchmarks.

3129
03:24:41,600 --> 03:24:44,960
I think this is like, you know, I think this is useful to illustrate that on hobblings are

3130
03:24:44,960 --> 03:24:49,040
large. Um, but I think they're like, I kind of think of them as like slightly separate things.

3131
03:24:49,040 --> 03:24:53,120
And the kind of the way I think about is that like, at a per token level, I think GP four is

3132
03:24:53,120 --> 03:24:58,560
not that far away from like a token of my internal monologue, right? Even like 3.5 to four took us

3133
03:24:58,560 --> 03:25:02,400
kind of from like the bottom of the human range, the top of the human range on like a lot of,

3134
03:25:02,400 --> 03:25:06,400
you know, on a lot of, uh, you know, kind of like high school tests. And so it's like a few more

3135
03:25:06,400 --> 03:25:11,040
3.5 to four jumps per token basis, like per token intelligence. And then you've got to unlock the

3136
03:25:11,040 --> 03:25:15,440
test time, you've got to solve the onboarding problem, make it use a computer. Um, and then

3137
03:25:15,440 --> 03:25:21,440
you're getting real close. Um, I'm reminded of, again, the story might be wrong, but I think it

3138
03:25:21,440 --> 03:25:25,600
is strikingly plausible. I agree. And so I think actually, I mean, the other thing I'll say is

3139
03:25:25,600 --> 03:25:30,320
like, you know, I say this 2027 timeline, I think it's unlikely, but I do think there's worlds

3140
03:25:30,320 --> 03:25:34,320
that are like AGI next year. And that's basically if the test time compute overhang is really easy

3141
03:25:34,320 --> 03:25:38,400
to crack. If it's really easy to crack, then you do like four rooms of test and compute, you know,

3142
03:25:38,400 --> 03:25:42,320
from a few hundred tokens to a few million tokens, you know, quickly. And then, you know, again,

3143
03:25:42,320 --> 03:25:46,720
maybe it's maybe only takes one or two 3.5 to four jumps per token, like one or two of those

3144
03:25:46,720 --> 03:25:51,120
jumps for token plus uses test time compute. And you basically have the proto automated engineer.

3145
03:25:51,600 --> 03:25:59,280
Um, so I'm reminded of, uh, uh, Stephen Pinker releases his book on, um, what is it? The Better

3146
03:25:59,280 --> 03:26:03,920
Angels of Our Nature. And it's like a couple of years ago or something. And he says the secular

3147
03:26:04,000 --> 03:26:09,520
decline in violence and war and everything. And you can just like plot the line from the end of

3148
03:26:09,520 --> 03:26:13,280
World War Two. And in fact, before World War Two, then these are just aberrations, whatever.

3149
03:26:13,280 --> 03:26:19,920
And basically, as soon as it happens, Ukraine, Gaza, the, everything is like, so

3150
03:26:19,920 --> 03:26:31,040
I think this is a sort of thing that happens in history where you see history align and you're

3151
03:26:31,040 --> 03:26:35,440
like, oh my gosh. And then just like, as soon as you make that prediction, who was that famous

3152
03:26:35,440 --> 03:26:39,280
author? So yeah, this, you know, again, people are predicting deep learning will hold a wall

3153
03:26:39,280 --> 03:26:43,840
every year. Maybe one year, they're right. But it's like gone a long way and hasn't hit a wall.

3154
03:26:43,840 --> 03:26:48,480
And I don't have that much more to go. And, and you know, so yeah, I guess I think this is a sort

3155
03:26:48,480 --> 03:26:54,400
of plausible story. And let's just run with it and see what it implies. Yeah. So we were talk

3156
03:26:54,400 --> 03:27:00,880
in your series, you talk about a lineman from the perspective of this is not about

3157
03:27:00,880 --> 03:27:06,080
some doomer scheme to get the point zero and personal probability distribution,

3158
03:27:06,080 --> 03:27:10,160
where things don't go off the rails. It's more about just controlling the systems,

3159
03:27:10,160 --> 03:27:15,920
making sure they do what we intend them to do. If that's the case, and we're going to be in the

3160
03:27:15,920 --> 03:27:22,320
sort of geopolitical conflict with China, and part of that will involve and what we're worried

3161
03:27:22,320 --> 03:27:28,800
about is them making the CCP bots that go out and take the red flag of Mao across the galaxies

3162
03:27:28,800 --> 03:27:37,280
or something. Then shouldn't we be worried about alignment as something that if in the wrong hands,

3163
03:27:37,280 --> 03:27:44,400
this is the thing that enables brainwashing, sort of dictatorial control. This seems like a

3164
03:27:44,400 --> 03:27:48,160
worrying thing. This should be part of this sort of algorithmic secrets we keep hidden, right?

3165
03:27:48,160 --> 03:27:52,000
The how to align these models, because that's also something the CCP can use to control their

3166
03:27:52,000 --> 03:27:55,280
models. I mean, I think in the world where you get the Democratic coalition, yeah, I mean,

3167
03:27:55,280 --> 03:27:59,360
also just alignment is often dual use, right? Like RLHF, it's like alignment team developed,

3168
03:27:59,360 --> 03:28:03,440
it was great, it was a big win for alignment, but it's also obviously makes these models useful.

3169
03:28:06,240 --> 03:28:11,520
But yeah, so yeah, alignment enables the CCP bots. Alignment also is what you need to get the

3170
03:28:11,520 --> 03:28:18,480
sort of whatever USAIs, follow the Constitution and disobey unlawful orders and respect separation

3171
03:28:18,480 --> 03:28:24,320
of powers and checks and balances. So yeah, you need alignment for whatever you want to do. It's

3172
03:28:24,320 --> 03:28:28,240
the sort of underlying technique. Tell me what you make of this take. I'm going to stream with

3173
03:28:28,240 --> 03:28:34,080
this a little bit. Okay. So fundamentally, there's many different ways the future could go. Yeah.

3174
03:28:34,080 --> 03:28:40,080
There's one path in which the LA's are type crazy AI's with nanobots take the future and

3175
03:28:40,080 --> 03:28:45,200
determine everything to gray goo or paper clips. And the more you solve alignment, the more that

3176
03:28:45,200 --> 03:28:50,160
path of the decision tree is circumscribed. And then so the more you solve alignment,

3177
03:28:50,160 --> 03:28:53,840
the more it is just different humans and divisions they have. And of course,

3178
03:28:53,920 --> 03:28:57,440
we know from history that things don't turn out the way you expect. So it's not like you can decide

3179
03:28:57,440 --> 03:29:01,280
the future, but it will appear. It's part of the beauty of it, right? You want these mechanisms,

3180
03:29:01,280 --> 03:29:05,920
the error correction, pluralism. But from the perspective of anybody who's looking at the system,

3181
03:29:05,920 --> 03:29:11,280
it will be like, I can control where this thing is going to end up. And so the more you solve

3182
03:29:11,280 --> 03:29:18,000
alignment and the more you circumscribe the different futures that are the results of AI will,

3183
03:29:18,000 --> 03:29:23,520
the more that accentuates the conflict between humans and their visions of the future. And so

3184
03:29:23,520 --> 03:29:27,600
in the world where alignment is solved and the world in which alignment is solved is the one,

3185
03:29:27,600 --> 03:29:30,720
is the world in which you have the most sort of human conflict over where to take AI.

3186
03:29:31,280 --> 03:29:34,880
Yeah. I mean, by removing the worlds in which the AI's take over, then like, you know,

3187
03:29:34,880 --> 03:29:38,240
the remaining worlds are the ones where it's like the humans decide what happens. And then

3188
03:29:38,240 --> 03:29:42,320
as we talked about, there's a whole lot of, yeah, a whole lot of worlds and how that could go.

3189
03:29:42,320 --> 03:29:46,480
And I worry. So when you think about alignment, and this is just controlling these things,

3190
03:29:46,880 --> 03:29:54,160
just think a little forward. And there's worlds in which hopefully, you know, human

3191
03:29:54,160 --> 03:29:58,960
descendants or some version of things in the future merge with super intelligences and they

3192
03:29:58,960 --> 03:30:05,520
have the rules of their own, but they're in some sort of law and market based order. I worry about

3193
03:30:05,520 --> 03:30:11,520
if you have things that are conscious and should be treated with rights. If you read about what

3194
03:30:11,520 --> 03:30:15,840
alignment schemes actually are, and then you read these books about what actually happened

3195
03:30:15,920 --> 03:30:20,720
during the Cultural Revolution, what happened when Stalin took over Russia, and you have

3196
03:30:21,840 --> 03:30:26,960
very strong monitoring from different instances where one, everybody's tasked with watching

3197
03:30:26,960 --> 03:30:32,560
each other. You have brainwashing, you have red teaming, where you have the spice stuff

3198
03:30:32,560 --> 03:30:36,320
you were talking about, where you try to convince somebody you're on like a defector and you see

3199
03:30:36,320 --> 03:30:41,920
if they defect with you. And if they do, then you realize they're an enemy. And listen, maybe

3200
03:30:41,920 --> 03:30:48,560
I'm stretching the analogy too far, but the way, like the ease of these alignment techniques

3201
03:30:48,560 --> 03:30:53,040
actually map on to something you could have read about during like mouse culture revolution

3202
03:30:53,040 --> 03:30:57,760
is a little bit troubling. Yeah, I mean, look, I think sentient AI is a whole other topic. I

3203
03:30:57,760 --> 03:31:01,200
don't know if we want to talk about it. I agree that like it's going to be very important how we

3204
03:31:01,200 --> 03:31:06,080
treat them. You know, in terms of like what you're actually programming these systems to do, again,

3205
03:31:06,080 --> 03:31:10,560
it's like alignment is just, it's a technical, it's a technical problem, a technical solution,

3206
03:31:10,560 --> 03:31:15,760
enables the CCP bots. I mean, in some sense, I think the, you know, I almost feel like the

3207
03:31:15,760 --> 03:31:18,800
sort of model and also about talking about checks and balances is sort of, you know, like the Federal

3208
03:31:18,800 --> 03:31:22,560
Reserve or Supreme Court justices. And there's a funny way in which they're kind of this like

3209
03:31:22,560 --> 03:31:25,840
very dedicated order, you know, Supreme Court justices, and it's amazing, they're actually

3210
03:31:25,840 --> 03:31:30,480
quite high quality, right? And they like really smart people, they really believe in the Constitution,

3211
03:31:30,480 --> 03:31:34,000
they love the Constitution, they believe in their principles, they have, you know, these, these,

3212
03:31:34,000 --> 03:31:37,920
these wonderful, you know, back, you know, and yeah, they have different persuasions, but they

3213
03:31:37,920 --> 03:31:41,920
have sort of, I think very sincere kind of debates about what is the meaning of the Constitution,

3214
03:31:41,920 --> 03:31:46,000
you know, what is the best actuation of these principles? You know, I guess, you know, by the

3215
03:31:46,000 --> 03:31:50,160
way, recommendation sort of skittish or arguments is like the best podcast, you know, when I run

3216
03:31:50,160 --> 03:31:54,000
out of high quality content on the Internet. I mean, I think there's going to be a process of

3217
03:31:54,000 --> 03:31:57,120
like figuring out what the Constitution should be. I think, you know, this Constitution has like

3218
03:31:57,120 --> 03:32:00,480
worked for a long time, you start with that, maybe eventually things change enough that you want

3219
03:32:00,480 --> 03:32:04,400
added to that. But anyway, you want them to like, you know, for example, for the checks and balances,

3220
03:32:04,400 --> 03:32:08,160
they like, they really love the Constitution, and they believe in it, and they take it really

3221
03:32:08,160 --> 03:32:12,640
seriously. And like, look, at some point, yeah, you are going to have like AI police and AI military,

3222
03:32:12,640 --> 03:32:17,360
but I think sort of like, you know, being able to ensure that they like, you know, believe in it

3223
03:32:17,360 --> 03:32:20,800
in the way that like a Supreme Court justice does, or like in the way that like a federal reserve

3224
03:32:20,800 --> 03:32:26,480
job, you know, official takes their job really seriously. Yeah. And I guess a big open question

3225
03:32:26,480 --> 03:32:30,400
is whether if you do the project or something like the project, the other important thing is

3226
03:32:30,400 --> 03:32:34,240
like a bunch of different factions need their own AIs, right? And so it's, it's really important

3227
03:32:34,240 --> 03:32:37,440
that like each political party gets to like, have their own, you know, and like, whatever

3228
03:32:37,440 --> 03:32:40,560
creeds it, you might totally disagree with their values, but it's like, it's really important

3229
03:32:40,560 --> 03:32:44,480
that they get to like, have their own kind of like super intelligence. And, and again, I think

3230
03:32:44,480 --> 03:32:48,320
it's that these sort of like classical liberal processes play out, including like, different

3231
03:32:48,320 --> 03:32:52,240
people of different persuasions and so on. And I don't know, maybe the advisors might not make

3232
03:32:52,240 --> 03:32:56,320
them, you know, wise, they might not follow the advice or whatever, but I think it's important.

3233
03:32:56,320 --> 03:33:00,960
Okay. So speaking of alignment, you seem pretty optimistic. So let's run, run through

3234
03:33:01,600 --> 03:33:06,800
the source of the optimism. Yeah. I think there you laid out different worlds in which we could

3235
03:33:06,800 --> 03:33:11,360
get AI. Yeah. There's one that you think is low probability of next year, where a GPT-4 plus

3236
03:33:11,360 --> 03:33:16,400
scaffolding plus unhoplings gets you to AGI. Not GPT-4, you know, like, oh, sorry, sorry. It's

3237
03:33:16,400 --> 03:33:21,680
a GP-5. Yeah. Yeah. And there's ones where it takes much longer. There's ones where it's

3238
03:33:21,680 --> 03:33:27,760
something that's a couple years. Yeah. In a modal world. Yeah. So GPT-4 seems pretty aligned in

3239
03:33:27,760 --> 03:33:31,600
the sense that I don't expect it to go off the rails. Yeah. Maybe with scaffolding things might

3240
03:33:31,600 --> 03:33:37,600
change. Looks pretty good. Yeah. Exactly. So the, and maybe you will keep turning at, there's cranks

3241
03:33:37,600 --> 03:33:43,440
to keep going up and one of the cranks gets you to ASI. Yeah. Is there any point at which the

3242
03:33:43,440 --> 03:33:48,880
sharp left turn happens? Is it when you start, is it the case that you think plausibly when they

3243
03:33:49,120 --> 03:33:53,520
act more like agents, this is the thing to worry about? Yeah. Is there anything qualitatively

3244
03:33:53,520 --> 03:33:56,640
that you expect to change with regards to the enlightenment perspective at these cranks?

3245
03:33:56,640 --> 03:33:59,760
Yeah. So I don't know if I believe in this concept of sharp left turn, but I do think there's

3246
03:33:59,760 --> 03:34:03,520
basically, I think there's important qualitative changes that happen between now and kind of like

3247
03:34:03,520 --> 03:34:07,440
somewhat super human systems, kind of like early on the intelligence explosion. And then important

3248
03:34:07,440 --> 03:34:11,680
qualitative changes that happen from like early in intelligence explosion to kind of like true

3249
03:34:11,680 --> 03:34:15,600
super intelligence and all its power and might. And let's talk about both of those.

3250
03:34:16,480 --> 03:34:19,920
And so, okay. So the first part of the problem is one, we're going to have to solve ourselves,

3251
03:34:19,920 --> 03:34:22,880
right? We have to kind of have to align the like initial AI and the intelligence explosion,

3252
03:34:22,880 --> 03:34:26,880
you know, the sort of automated out of Bradford. I think there's kind of like, I mean, two important

3253
03:34:26,880 --> 03:34:33,040
things that change from GBD4, right? So one of them is, if you believe the story on like,

3254
03:34:33,040 --> 03:34:37,600
you know, synthetic data or L or self play to get past the data wall, and if you believe this on

3255
03:34:37,600 --> 03:34:41,120
hobbling story, you know, at the end, you're going to have things, you know, they're agents,

3256
03:34:41,120 --> 03:34:46,240
right? Including they do long term plans, right? They have long, long, you know, they're somehow

3257
03:34:46,240 --> 03:34:49,280
they're able to act over long horizons, right? But you need that, right? That's the sort of

3258
03:34:49,280 --> 03:34:55,200
prerequisite to be able to do the sort of automated AI research. And so, you know, I think

3259
03:34:55,200 --> 03:34:58,640
there's basically, you know, I basically think sort of pre training is sort of alignment neutral

3260
03:34:58,640 --> 03:35:01,920
in the sense of like, it has all these representations that has good representations

3261
03:35:01,920 --> 03:35:06,480
that you know, as representations of doing bad things, you know, but but there's, there's,

3262
03:35:06,480 --> 03:35:11,040
it's not like, you know, scheming against you or whatever. I think the sort of misalignment can

3263
03:35:11,040 --> 03:35:15,360
arise once you're doing more kind of long horizon training, right? And so you're training, you know,

3264
03:35:15,360 --> 03:35:18,960
again, two simplified example, but to kind of illustrate, you know, you're training an AI to

3265
03:35:18,960 --> 03:35:23,920
make money. And, you know, if you're just doing that with reinforcement learning, you know, it's,

3266
03:35:23,920 --> 03:35:28,960
you know, it might learn to commit fraud or lie or to see you for seek power, simply because those

3267
03:35:28,960 --> 03:35:32,320
are successful strategies in the real world, right? So maybe, you know, RL is basically it

3268
03:35:32,320 --> 03:35:36,560
explores, maybe it figures out like, oh, it tries to like hack, and then it gets some money and that

3269
03:35:36,560 --> 03:35:39,680
made more money, you know, and then if that's successful, if that gets reward, that's just

3270
03:35:39,680 --> 03:35:43,440
reinforced. So basically, I think there's sort of more serious misalignments, kind of like

3271
03:35:43,440 --> 03:35:48,320
misaligned long term goals that could arise between now and or that sort of necessarily

3272
03:35:48,320 --> 03:35:51,760
have to be able to arise if you're able to get long horizon system, that's one.

3273
03:35:52,880 --> 03:35:56,560
What you want to do in that situation is you want to add side constraints, right? So you want to add,

3274
03:35:56,560 --> 03:36:01,920
you know, don't lie, don't deceive, don't commit fraud. And so how do you add those side constraints,

3275
03:36:01,920 --> 03:36:05,520
right? The sort of basic idea you might have is like RLHF, right? You're kind of like, yeah,

3276
03:36:05,520 --> 03:36:09,120
it has this goal of like, you know, make money or whatever, but you're watching what it's doing,

3277
03:36:09,120 --> 03:36:12,320
it starts trying to like, you know, lie or deceive or fraud or whatever,

3278
03:36:12,320 --> 03:36:15,600
break the law, you're just kind of like, thumbs down, don't do that, you anti reinforce that.

3279
03:36:16,480 --> 03:36:20,160
The sort of critical issue that comes in is that these eye systems are getting superhuman,

3280
03:36:20,160 --> 03:36:23,920
right? And they're going to be able to do things that are too complex for humans to evaluate,

3281
03:36:23,920 --> 03:36:28,000
right? So again, even early on, you know, in the intelligence explosion, the automated AI

3282
03:36:28,000 --> 03:36:31,200
researchers and engineers, you know, they might write millions, you know, billions,

3283
03:36:31,200 --> 03:36:34,480
trillions of lines of complicated code, you know, they might be doing all sorts of stuff,

3284
03:36:34,480 --> 03:36:39,040
you just like, don't understand anymore. And so, you know, in the million lines of code,

3285
03:36:39,040 --> 03:36:42,560
you know, is it somewhere kind of like, you know, hacking, hacking, or like exultating itself,

3286
03:36:42,560 --> 03:36:45,920
or like, you know, trying to go for the nukes or whatever, you know, like, you don't know anymore,

3287
03:36:45,920 --> 03:36:50,400
right? And so this sort of like, thumbs up, thumbs down, pure RLHF doesn't fully work anymore.

3288
03:36:51,200 --> 03:36:55,280
Second part of the picture, and maybe talk more about this first part of the picture,

3289
03:36:55,280 --> 03:36:57,920
I think it's going to be like, there's a hard technical problem of what do you do,

3290
03:36:57,920 --> 03:37:01,280
sort of post RLHF, but I think it's a solvable problem. And it's like, you know,

3291
03:37:01,280 --> 03:37:04,720
there's various things in bullish on, I think there's like ways in which deep learning has

3292
03:37:04,720 --> 03:37:08,720
shaped out favorably. The second part of the problem is you're going from your like initial

3293
03:37:08,720 --> 03:37:11,920
systems and intelligence explosion to like super intelligence, and you know, it's like,

3294
03:37:11,920 --> 03:37:16,400
many ooms, it ends up being like, by the end of it, you have a thing that's vastly smarter than humans.

3295
03:37:18,000 --> 03:37:21,760
I think the intelligence explosion is really scary from an alignment point of view,

3296
03:37:21,760 --> 03:37:25,040
because basically, if you have this rapid intelligence explosion, you know, less than a

3297
03:37:25,040 --> 03:37:28,720
year or two years or whatever, you're going say in the period of a year from systems where like,

3298
03:37:28,720 --> 03:37:32,640
you know, failure would be bad, but it's not kind of strapped back to like, you know, saying a bad

3299
03:37:32,640 --> 03:37:38,320
word, it's like, you know, it's, it's something goes awry to like, you know, failure is like,

3300
03:37:38,320 --> 03:37:41,920
you know, it extra traded itself, it starts hacking the military can do really bad things.

3301
03:37:41,920 --> 03:37:44,960
You're going less than a year from sort of a world in which like, you know,

3302
03:37:44,960 --> 03:37:48,560
it's some descendant of current systems, and you kind of understand it, and it's like, you know,

3303
03:37:48,560 --> 03:37:51,840
it has good properties. There's something that potentially has a very sort of alien and different

3304
03:37:51,840 --> 03:37:56,160
architecture, right? After having gone through another decade of imal advances. I think one

3305
03:37:56,160 --> 03:38:01,280
example there that's very salient to me is legible and faithful chain of thought, right? So a lot

3306
03:38:01,280 --> 03:38:04,160
of the time when we're talking about these things, we're talking about, you know, it has tokens of

3307
03:38:04,160 --> 03:38:08,880
thinking, and then it uses many tokens of thinking. And, you know, maybe we bootstrap ourselves by,

3308
03:38:08,880 --> 03:38:12,320
you know, it's pre-trained, it learns to think in English, and we do something else on top,

3309
03:38:12,320 --> 03:38:18,240
so it can do the sort of longer chains of thought. And so, you know, it's very plausible to me that

3310
03:38:18,320 --> 03:38:21,600
like, for the initial automated alignment researchers, you know, we don't need to do any

3311
03:38:21,600 --> 03:38:25,600
complicated mechanistic interpretability, and just like literally you read what they're thinking,

3312
03:38:25,600 --> 03:38:32,960
which is great. You know, it's like huge advantage, right? However, I'm very likely not the most

3313
03:38:32,960 --> 03:38:36,800
efficient way to do it, right? There's like probably some way to have a recurrent architecture.

3314
03:38:36,800 --> 03:38:39,840
It's all internal states. There's a much more efficient way to do it. That's what you get

3315
03:38:39,840 --> 03:38:44,880
by the end of the year. You know, you're going this year from like RLHF plus plus some extension

3316
03:38:44,880 --> 03:38:51,200
works to like, it's vastly superhuman. It's like, you know, it's to us like, you know, an expert

3317
03:38:51,200 --> 03:38:55,440
in the field might be to like an elementary school or middle schooler. And so, you know,

3318
03:38:55,440 --> 03:38:59,920
I think it's this sort of incredibly sort of like, hairy period for alignment.

3319
03:39:00,960 --> 03:39:05,040
Thing you do have is you have the automated AI researchers, right? And so, you can use the

3320
03:39:05,040 --> 03:39:11,440
automated AI researchers to also do alignment. And so, in this world, why are we optimistic

3321
03:39:11,440 --> 03:39:18,160
that the project is being run by people who are thinking, I think, so here's something to think

3322
03:39:18,160 --> 03:39:26,240
about. The open AI starts off with people who are very explicitly thinking about exactly these

3323
03:39:26,240 --> 03:39:31,760
kinds of things, right? But are they still there? No, no, but you're still here. Here's the thing.

3324
03:39:31,760 --> 03:39:35,520
No, no, even the people who are there, even like the current leadership is like exactly these things

3325
03:39:35,520 --> 03:39:41,840
that can find them in interviews in their blog posts talking about. And what happens is when,

3326
03:39:41,840 --> 03:39:46,880
as you were talking about, when some sort of trivial, and Yon talked about it, this is not just

3327
03:39:46,880 --> 03:39:53,440
you, Yon talked about in his tweet thread, when there is some trade off that has to be made with

3328
03:39:53,440 --> 03:39:56,800
we need to do this flashy release this week and not next week, because whatever Google

3329
03:39:56,800 --> 03:40:00,960
IO is the next week, so we're going to get it. And then the trade off is made in favor of

3330
03:40:01,680 --> 03:40:10,880
the less the more careless decision. When we have the government or the national security advisor,

3331
03:40:10,880 --> 03:40:15,520
the military or whatever, which is much less familiar with this kind of discourse is a

3332
03:40:15,520 --> 03:40:19,360
naturally thinking in this way about how I'm worried the chain of thought isn't faithful and

3333
03:40:19,360 --> 03:40:23,760
how do we think about the features that are represented here? Why should it be optimistic

3334
03:40:23,760 --> 03:40:29,200
that a project run by people like that will be thoughtful about these kinds of considerations?

3335
03:40:29,440 --> 03:40:42,640
I mean, they might not be. I agree. I think a few thoughts, right? First of all, I think the private

3336
03:40:42,640 --> 03:40:46,800
world, even if they sort of nominally care is extremely tough for alignment, a couple of reasons.

3337
03:40:46,800 --> 03:40:50,240
One, you just have the race between the sort of commercial labs, right? And it's like, you don't

3338
03:40:50,240 --> 03:40:53,600
have any headroom there to like be like, actually, we're going to hold back for three months, like

3339
03:40:53,600 --> 03:40:57,600
get this right. And we're going to dedicate 90% of our compute to automated alignment research

3340
03:40:57,600 --> 03:41:02,080
instead of just like pushing the next zoom. The other thing, though, is like in the private world,

3341
03:41:02,080 --> 03:41:05,280
you know, China has stolen your age, China has your secrets, they're right on your tails,

3342
03:41:05,280 --> 03:41:10,560
you're in this fever struggle, no room at all for maneuver. They're like the way it's like

3343
03:41:10,560 --> 03:41:14,080
absolutely essential to get alignment right and you get it during this intelligence explosion,

3344
03:41:14,080 --> 03:41:17,040
you get it right, because you need to have that room to maneuver and you need to have that clear

3345
03:41:17,040 --> 03:41:23,680
lead. And, you know, again, maybe you've made the deal or whatever, but I think you're an incredibly

3346
03:41:23,680 --> 03:41:28,320
tough space, tough spot if you don't have this clearly. So I think the sort of private world

3347
03:41:28,320 --> 03:41:31,680
is kind of rough there on like whether people will take it seriously, you know, I don't know,

3348
03:41:31,680 --> 03:41:36,480
I have some faith in sort of sort of normal mechanisms of a liberal society, sort of if

3349
03:41:36,480 --> 03:41:40,080
alignment is an issue, which, you know, we don't fully know yet, but sort of the science will

3350
03:41:40,080 --> 03:41:44,000
develop, we're going to get better measurements of alignment, you know, and the case will be

3351
03:41:44,000 --> 03:41:48,880
clear and obvious. I worry that there's, you know, I worry about worlds where evidence is

3352
03:41:48,880 --> 03:41:53,200
ambiguous. And I think a lot of a lot of the most scary kind of intelligence explosion scenarios are

3353
03:41:53,200 --> 03:41:57,760
worlds in which evidence is ambiguous. But again, it's sort of like, I, if evidence is ambiguous,

3354
03:41:57,760 --> 03:42:01,040
then that's the world in which you really want the safety margins. And that's also the world in

3355
03:42:01,040 --> 03:42:04,240
which kind of like running the intelligence explosion is sort of like, you know, running a war,

3356
03:42:04,240 --> 03:42:08,320
right? It's like, the evidence is ambiguous, we have to make these really tough trade offs.

3357
03:42:08,320 --> 03:42:11,680
And you like, you better have a really good chain of command for that. And it's not just like, you

3358
03:42:11,680 --> 03:42:16,160
know, you're going at, well, let's go, you know, it's cool. Yeah. Let's talk a little bit about

3359
03:42:16,160 --> 03:42:22,640
Germany. We're making the analogy to World War Two. And you made a really interesting point,

3360
03:42:22,640 --> 03:42:37,360
many hours ago. The fact that throughout history, World War Two is not unique, at least when you

3361
03:42:37,360 --> 03:42:46,480
think in proportion to the size of the population. But these other sorts of catastrophes where

3362
03:42:46,480 --> 03:42:52,480
some significant portion of the population has been killed off. After that, the nation recovers

3363
03:42:52,560 --> 03:42:58,560
and they get back to their heights. And so what's interesting after World War Two

3364
03:42:59,200 --> 03:43:03,600
is that Germany especially, and maybe Europe as a whole, obviously they experienced

3365
03:43:03,600 --> 03:43:09,600
fast economic growth in the direct aftermath because of catch-up growth. But subsequently,

3366
03:43:09,600 --> 03:43:15,040
we just don't think of Germany as, we're not talking about Germany potentially launching an

3367
03:43:15,040 --> 03:43:18,560
intelligence explosion and they're going to get into the AI table. We were talking about

3368
03:43:18,560 --> 03:43:21,440
Iran and North Korea and Russia. We didn't talk about Germany, right?

3369
03:43:21,440 --> 03:43:22,640
Well, because they're allies.

3370
03:43:22,640 --> 03:43:29,040
Yeah. But so what happened? I mean, World War Two and now it didn't like come back

3371
03:43:29,040 --> 03:43:30,720
over the seven years war or something, right?

3372
03:43:30,720 --> 03:43:34,320
Yeah. Yeah. Yeah. I mean, look, I'm generally very bearish on Germany. I think in this context,

3373
03:43:34,320 --> 03:43:37,040
I'm kind of like, you know, it's a little bit, you know, I think you're underrating a little bit.

3374
03:43:37,040 --> 03:43:40,160
I think it's probably still one of the, you know, top five most important countries in the world.

3375
03:43:41,200 --> 03:43:45,200
You know, I mean, Europe overall, you know, it still has, I mean, it's a GDP that's like

3376
03:43:45,200 --> 03:43:50,480
close to the United States the size of the GDP, you know, and there's things actually that Germany

3377
03:43:50,480 --> 03:43:55,040
is kind of good at, right? Like state capacity, right? Like, you know, the, you know, the roads

3378
03:43:55,040 --> 03:43:59,520
are good and they're clean and they're well maintained and, you know, in some sense, the sort

3379
03:43:59,520 --> 03:44:03,520
of, a lot of this is the sort of flip side of things that I think are bad about Germany, right?

3380
03:44:03,520 --> 03:44:06,560
So in the US, it's a little bit like there's a bit more of a sort of Wild West feeling to the

3381
03:44:06,560 --> 03:44:11,600
United States, right? And it includes the kind of like crazy bursts of creativity. It includes like,

3382
03:44:11,600 --> 03:44:16,960
you know, political candidates that are sort of, you know, there's a much broader spectrum and,

3383
03:44:16,960 --> 03:44:20,480
you know, much, you know, like both in Obama and Trump as somebody you just wouldn't see in the sort

3384
03:44:20,480 --> 03:44:24,800
of much more confined kind of German political debate. You know, I wrote this blog post at some

3385
03:44:24,800 --> 03:44:29,680
point, Europe's political stupor about this. But anyway, and so there's this sort of punctilious

3386
03:44:29,680 --> 03:44:33,440
sort of rule following that is like good in terms of like, you know, keeping your kind of state

3387
03:44:33,440 --> 03:44:41,120
capacity functioning. But that is also, you know, I think I kind of think there's a sort of very

3388
03:44:41,120 --> 03:44:46,320
constrained view of the world in some sense. You know, and that includes kind of, you know,

3389
03:44:46,320 --> 03:44:50,080
I think after World War Two, there's a real backlash against anything like elite, you know,

3390
03:44:50,080 --> 03:44:55,920
and, you know, again, no, you know, no elite high schools or elite colleges and sort of

3391
03:44:55,920 --> 03:44:57,600
what my is that the law?

3392
03:44:57,600 --> 03:44:59,680
excellence isn't cherished, you know, there's a yeah.

3393
03:44:59,680 --> 03:45:06,800
Why is that the logical intellectual thing to rebel against if what if you're trying to

3394
03:45:06,800 --> 03:45:10,560
overcorrect from the Nazis? Yeah, was it because the Nazis were very much into elitism?

3395
03:45:11,200 --> 03:45:14,240
I don't understand why that's a logical sort of counter reaction.

3396
03:45:14,240 --> 03:45:17,840
I know, maybe it was sort of a counter reaction against the sort of like whole like Aryan race

3397
03:45:17,840 --> 03:45:21,520
and sort of that sort of thing. I mean, I also just think there was a certain amount in what

3398
03:45:21,520 --> 03:45:26,240
amount certain, I mean, look at sort of World War One, end of World War One versus end of World

3399
03:45:26,240 --> 03:45:31,280
War Two for Germany, right? And sort of, you know, a common narrative is that the piece of Versailles,

3400
03:45:31,280 --> 03:45:36,160
you know, was too strict on Germany. You know, the piece imposed after World War Two was like

3401
03:45:36,160 --> 03:45:40,160
much more strict, right? It was a complete, you know, the whole country was destroyed,

3402
03:45:40,160 --> 03:45:43,920
you know, it was, you know, and all the main, most of the major cities, you know, over half of

3403
03:45:43,920 --> 03:45:48,320
the housing stock had been destroyed, right? Like, you know, in some birth cohorts, you know,

3404
03:45:48,320 --> 03:45:52,960
like 40% of the men had died. Half the population displaced. Oh, yeah. I mean,

3405
03:45:52,960 --> 03:45:57,360
almost 20 million people are displaced, huge, crazy, right? You know, like,

3406
03:45:57,360 --> 03:46:01,440
and the borders are way smaller than the Versailles borders. Yeah, exactly. And sort of

3407
03:46:01,440 --> 03:46:06,480
complete imposition of a new political system and, and, you know, on both sides, you know, and

3408
03:46:06,480 --> 03:46:13,840
yeah, so it was, but in some sense that worked out better than the post-World War One piece,

3409
03:46:14,400 --> 03:46:18,240
where then there was this kind of resurgence of German nationalism and, you know, in some sense,

3410
03:46:18,240 --> 03:46:21,120
the thing that has been a pattern. So it's sort of like, it's unclear if you want to wake the

3411
03:46:21,120 --> 03:46:25,280
sleeping beast. I do think that at this point, you know, it's gotten a bit too sleepy. Yeah.

3412
03:46:27,440 --> 03:46:30,560
I do think it's an interesting point about we underrate the American political system. Yeah.

3413
03:46:30,640 --> 03:46:36,080
I've been making the same correction myself. Yeah. There's, there was this book about

3414
03:46:36,080 --> 03:46:41,840
burdened by a Chinese economist called China's World View. And overall, I wasn't a big fan,

3415
03:46:41,840 --> 03:46:48,480
but they made a really interesting point in there, which was the way in which candidates rise up

3416
03:46:48,480 --> 03:46:56,080
through the Chinese hierarchy for politics, for administration, in some sense, that selects for

3417
03:46:56,080 --> 03:46:58,960
you're not going to get some Marjorie Taylor Greene or somebody running some.

3418
03:47:00,160 --> 03:47:04,560
Don't get that in Germany either. Right. Yeah. But you're, he explicitly made the point in the

3419
03:47:04,560 --> 03:47:08,320
book that that also means we're never going to get a Henry Kissinger or Barack Obama. Right.

3420
03:47:08,320 --> 03:47:13,280
In China. We're going to get like, by the time they end up in charge of the, the Politburo,

3421
03:47:13,280 --> 03:47:16,960
the Politburo, there'll be like some 60 year old Democrat who's never like ruffled any feathers.

3422
03:47:16,960 --> 03:47:19,920
Yeah. Yeah. Yeah. I mean, I think, I think there's something really important about the sort of

3423
03:47:19,920 --> 03:47:24,800
like very raucous political debate. And I mean, yeah, in general, kind of like, you know, there's

3424
03:47:24,800 --> 03:47:28,320
the sense in which in America, you know, lots of people live in their kind of like own world.

3425
03:47:28,320 --> 03:47:32,560
I mean, like we live in this kind of bizarre little like bubble in San Francisco and people,

3426
03:47:33,360 --> 03:47:38,480
you know, and, and, but I think that's important for the sort of evolution of ideas,

3427
03:47:38,480 --> 03:47:43,280
error correction and that sort of thing. You know, there's other ways in which the

3428
03:47:43,280 --> 03:47:47,920
German system is more functional. Yeah. But it's interesting that there's major mistakes,

3429
03:47:47,920 --> 03:47:50,800
right? Like the sort of defense spending, right? And you know, then, you know, Russia

3430
03:47:50,800 --> 03:47:55,440
made Ukraine and, and you're like, wow, what did we do? Right? No, that's a really good point,

3431
03:47:55,440 --> 03:48:01,840
right? The main issues, there's everybody agrees, but exactly. Yeah. So consensus blob kind of thing.

3432
03:48:01,840 --> 03:48:05,920
Right. And on the China point, you know, just having this experience of like reading German

3433
03:48:05,920 --> 03:48:09,600
newspapers, and I think how much, you know, how much more poorly I would understand the sort of

3434
03:48:09,600 --> 03:48:15,520
German debate and sort of the sort of state of mind from just kind of afar. I worry a lot about,

3435
03:48:15,520 --> 03:48:20,480
you know, or I think it is interesting just how kind of impenetrable China is to me.

3436
03:48:20,800 --> 03:48:24,640
It's a billion people, right? And like, you know, almost everything else is really globalized.

3437
03:48:24,640 --> 03:48:28,480
You have a globalized internet and I kind of, I kind of a sense what's happening in the UK.

3438
03:48:28,480 --> 03:48:31,040
You know, I probably, even if I didn't read German newspapers, just sort of would have a

3439
03:48:31,040 --> 03:48:35,840
sense of what's happening in Germany. But I really don't feel like I have a sense of what like,

3440
03:48:37,120 --> 03:48:40,960
you know, what is the state of mind or what are the state of political debate, you know,

3441
03:48:40,960 --> 03:48:45,520
of a sort of average Chinese person or like an average Chinese leader. And yeah, I think that,

3442
03:48:45,520 --> 03:48:49,680
that I find that distance kind of worrying. And I, you know, and there's, you know, there's

3443
03:48:49,680 --> 03:48:52,880
some people who do this and they do really great work where they kind of go through the like party

3444
03:48:52,880 --> 03:48:57,360
documents and the party speeches. And it seems to require a kind of a lot of interpretive ability

3445
03:48:57,360 --> 03:49:01,120
where there's like very specific words and mentoring that like mean we'll have one connotation,

3446
03:49:01,120 --> 03:49:05,840
not the other connotation. But yeah, I think it's sort of interesting given how globalized

3447
03:49:05,840 --> 03:49:09,520
everything is. And like, I mean, now we have basically perfect translation machines and it's

3448
03:49:09,520 --> 03:49:14,960
still so impenetrable. That's really interesting. I've been, I should, I'm sort of ashamed almost

3449
03:49:14,960 --> 03:49:20,240
that I haven't done this yet. I think many months ago, when Alexi interviewed me on his

3450
03:49:20,240 --> 03:49:24,400
YouTube channel, I said, I'm meaning to go to China to actually see for myself what's going on.

3451
03:49:24,400 --> 03:49:29,520
And actually I'm, I should, so by the way, if anybody listening has a lot of context on China,

3452
03:49:29,520 --> 03:49:32,560
if I went to China, who could introduce me to people, please email me.

3453
03:49:33,120 --> 03:49:36,320
You got to do some pods and you got to find some of the Chinese AI researchers, man.

3454
03:49:36,320 --> 03:49:40,160
I know. I was thinking at some point, again, this is the fact that I'm...

3455
03:49:40,160 --> 03:49:43,360
Can I speak freely, but you know, I don't know if they can speak freely, but...

3456
03:49:43,360 --> 03:49:47,360
I was thinking of there's, so they had these papers and on the paper, they'll say who's a

3457
03:49:47,360 --> 03:49:53,360
co-author. It's funny because, well, I was thinking of just emailing, cold emailing everybody,

3458
03:49:53,360 --> 03:49:56,800
like, here's my calendar. Let's just talk. I just want to see what is the vibe. Even

3459
03:49:56,800 --> 03:50:00,000
they don't tell me anything. I'm just like, what kind of person is this? How westernized are they?

3460
03:50:01,200 --> 03:50:08,080
But as I was saying this, I just remembered that in fact, by Dan's, according to mutual

3461
03:50:08,080 --> 03:50:13,520
friends we have at Google, they cold emailed every single person on the Gemini paper and said,

3462
03:50:13,520 --> 03:50:18,640
if you come work for By Dan's, we'll make you an allied engineer, you'll report directly to the CTO,

3463
03:50:18,640 --> 03:50:22,240
and in fact, this actually... That's how the secrets go over, right?

3464
03:50:22,240 --> 03:50:26,560
Right. No, I meant to ask this earlier, but suppose they hired what...

3465
03:50:26,560 --> 03:50:30,160
If there's only a hundred or so people, or maybe less, we're working on the key algorithmic secrets.

3466
03:50:30,960 --> 03:50:34,720
If they hired one such person, is all the alpha gone that these labs have?

3467
03:50:35,360 --> 03:50:39,040
If this person was intentional about it, they could get a lot. I mean, they couldn't get the sort

3468
03:50:39,040 --> 03:50:42,000
of like... I mean, actually, you could probably just also exfiltrate the code. They could get a

3469
03:50:42,000 --> 03:50:46,240
lot of the key ideas. Again, up until recently stuff was published, but they could get a lot

3470
03:50:46,240 --> 03:50:50,240
of the key ideas if they tried. I think there's a lot of people who don't actually look around

3471
03:50:50,240 --> 03:50:56,160
to see what the other teams are doing, but I think you can. But yeah, I mean, they could. It's scary.

3472
03:50:56,880 --> 03:51:01,200
Right. I think the project makes more sense there where you can't just recruit a Manhattan

3473
03:51:01,200 --> 03:51:05,920
project engineer and then just get... I mean, these are secrets that can be used for like

3474
03:51:05,920 --> 03:51:09,520
probably every training around the future that'll be like, maybe are the key to the data wall that

3475
03:51:09,520 --> 03:51:14,000
are like, they can't go on or they can't go on that are like, they're going to be worth giving

3476
03:51:14,000 --> 03:51:17,120
sort of like the multipliers on compute, hundreds of billions, trillions of dollars,

3477
03:51:17,920 --> 03:51:21,680
and all it takes is China to offer a hundred million dollars to somebody and be like,

3478
03:51:21,680 --> 03:51:28,160
yeah, come work for us. And then... I mean, yeah, I'm really uncertain on how

3479
03:51:28,160 --> 03:51:33,680
sort of seriously China is taking AGI right now. One anecdote that was relate to me on

3480
03:51:33,680 --> 03:51:37,920
the topic of the anecdotes, by another sort of like kind of researcher in the field was

3481
03:51:37,920 --> 03:51:41,520
at some point they were at a conference with somebody, Chinese AI researcher,

3482
03:51:41,520 --> 03:51:44,640
and he was talking to him and he was like, I think it's really good that you're here and like,

3483
03:51:44,640 --> 03:51:49,920
we got to have the international coordination stuff. And apparently this guy said that I'm the

3484
03:51:49,920 --> 03:51:53,840
kind of most senior most person that they're going to let leave the country to come to things like

3485
03:51:53,840 --> 03:51:59,520
this. Wait, what's the takeaway? As in they're not letting really senior

3486
03:51:59,520 --> 03:52:03,600
AI researchers leave the country. Interesting. Kind of classic, you know, Eastern Block move.

3487
03:52:03,600 --> 03:52:09,040
Yeah. I don't know if this is true, but it's what I heard. It's interesting. So I thought the point

3488
03:52:09,040 --> 03:52:14,800
you made earlier about being exposed to German newspapers and also to, because earlier you

3489
03:52:14,800 --> 03:52:20,960
were interested in economics and law and national security, you have the variety in intellectual

3490
03:52:20,960 --> 03:52:24,800
diet there has exposed you to thinking about the geopolitical question here and why is others

3491
03:52:25,360 --> 03:52:28,240
talking about AI. I mean, this is the first episode I've done about this where we've talked

3492
03:52:28,240 --> 03:52:32,080
about things like this, which is now that I think about it weird to give that this is an obvious

3493
03:52:32,080 --> 03:52:37,040
thing in retrospect, I should have been thinking about. Anyways, so that's one thing we've been

3494
03:52:37,040 --> 03:52:42,000
missing. What are you missing? And national security you're thinking about so you can't say

3495
03:52:42,000 --> 03:52:47,520
national security. What like perspective are you probably underexposed to as a result?

3496
03:52:47,520 --> 03:52:50,720
And China, I guess you mentioned. Yeah. So I think the China one is an important one.

3497
03:52:52,480 --> 03:52:55,760
I mean, I think another one would be a sort of very Tyler Cowan-esque take, which is like,

3498
03:52:55,760 --> 03:53:00,000
you're not exposed to how, like, how will a normal person in America, like, you know,

3499
03:53:00,000 --> 03:53:05,040
both like use AI, you know, probably not, you know, and that being kind of like bottlenecks

3500
03:53:05,040 --> 03:53:08,720
to the fusion of these things. I'm overrating the revenue because I'm kind of like, ah, you know,

3501
03:53:08,720 --> 03:53:12,640
everyone has to stop adopting it, but you know, kind of like, you know, Joe Schmo engineer at a

3502
03:53:12,640 --> 03:53:16,800
company, you know, like, ah, will they, will they be able to integrate it? And also the reaction

3503
03:53:16,800 --> 03:53:20,480
to it, right? You know, I mean, I think this was a question again, hours ago, where it was

3504
03:53:21,920 --> 03:53:26,640
about like, you know, won't people kind of rebel against this? Yeah. And they won't want to do the

3505
03:53:26,640 --> 03:53:32,480
project. I don't know, maybe they will. Yeah. Here's a political reaction that I didn't anticipate.

3506
03:53:32,480 --> 03:53:37,520
Yeah. So Tucker Carlson was recently on the Joe Rogan episode. I already told you about this

3507
03:53:37,520 --> 03:53:43,680
part. I'm just gonna tell the story again. So Tucker Carlson is on Joe Rogan. Yeah. And they

3508
03:53:43,680 --> 03:53:49,520
start talking about World War II and Tucker says, well, listen, I'm going to say something that my

3509
03:53:49,520 --> 03:53:54,640
fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously

3510
03:53:54,640 --> 03:54:00,880
immoral that we use them on Nagasaki and Hiroshima. And then he says, in fact, nuclear weapons are

3511
03:54:00,880 --> 03:54:07,280
always immoral, except when we would use them on data centers. In fact, it would be immoral not to

3512
03:54:07,280 --> 03:54:11,760
use them on data centers because look, we're, DC people in Silicon Valley, these fucking nerds

3513
03:54:11,760 --> 03:54:18,720
are making super intelligent. And they say that it could enslave humanity. We made machines to

3514
03:54:18,720 --> 03:54:24,080
serve humanity, not to enslave humanity. And they're just going on and making these machines.

3515
03:54:24,080 --> 03:54:32,640
And so we should of course be nuking the data centers. And that is definitely not a political

3516
03:54:32,640 --> 03:54:39,120
reaction in 2024. I was expecting. I mean, who knows? It's gonna be crazy. It's gonna be crazy.

3517
03:54:39,200 --> 03:54:44,720
The thing we learned with COVID is that also the left, right reactions that you would anticipate

3518
03:54:44,720 --> 03:54:50,080
just based on hunches, it completely flipped. Initially, like kind of the right is like, you

3519
03:54:50,080 --> 03:54:53,920
know, it's like so contingent. And then, and then, and then, and the right left was like, this is

3520
03:54:53,920 --> 03:54:58,000
racist. And then it flipped, you know, the left was really into the code. Yeah. And the whole

3521
03:54:58,000 --> 03:55:02,720
thing also is just like so blunt and crude. And so, yeah, I think, I think probably in general,

3522
03:55:02,720 --> 03:55:06,320
you know, I think people are really under, you know, people like to make sort of complicated

3523
03:55:06,400 --> 03:55:11,040
technocratic AI policy proposals. And I think, especially if things go kind of fairly rapidly

3524
03:55:11,040 --> 03:55:16,800
on the last AGI, you know, there might not actually be that much space for kind of like

3525
03:55:16,800 --> 03:55:20,640
complicated kind of like, you know, clever proposals that might just be kind of much

3526
03:55:20,640 --> 03:55:27,120
cruder reactions. Yeah. Look, and then also when you mentioned the spies and national security

3527
03:55:27,120 --> 03:55:32,880
getting involved and everything, and you can talk about that in the abstract, but now that we're

3528
03:55:32,880 --> 03:55:36,640
living in San Francisco and we know many of the people who are doing the top AI research

3529
03:55:37,680 --> 03:55:40,560
is also a little scary to think about people I personally know and friends with.

3530
03:55:41,520 --> 03:55:46,240
It's not unfeasible if they have secrets in their head that are worth $100 billion or something,

3531
03:55:46,240 --> 03:55:51,280
kidnapping, assassination, sabotage. It's scary. Oh, they're family, or yeah, it's really bad.

3532
03:55:51,280 --> 03:55:54,160
Yeah, yeah. I mean, this is to the point on security, you know, like right now, it's just

3533
03:55:54,160 --> 03:56:00,160
really foreign. But, you know, at some point, as it becomes like really serious, it's, you know,

3534
03:56:00,160 --> 03:56:06,960
you're going to want the security cards. Yeah. Yeah. So presumably you have thought about the

3535
03:56:06,960 --> 03:56:11,200
fact that people in China will be listening to this and will be reading your series. Yeah.

3536
03:56:11,840 --> 03:56:20,320
And somehow you made the trade off that it's better to let the whole world know. Yeah. And

3537
03:56:20,320 --> 03:56:24,640
also including China and make them up to AGI, which is part of the thing you're worried about

3538
03:56:24,640 --> 03:56:30,480
is China making up to AGI than to stay silent. Yeah. I'm just curious, walk me through how you've

3539
03:56:30,480 --> 03:56:34,640
thought about that trade off. Yeah, I actually, look, I think this is a tough trade off. I thought

3540
03:56:34,640 --> 03:56:39,120
about this a bunch, you know, I think, you know, I think people on the PRC will read this.

3541
03:56:44,880 --> 03:56:48,320
I think, you know, I think there's some extent to which sort of cat is out of the bag, you know,

3542
03:56:48,320 --> 03:56:52,160
this is like not, you know, AGI being a thing people are thinking about very seriously is not

3543
03:56:52,160 --> 03:56:55,520
new anymore. There's sort of, you know, a lot of these takes are kind of old or, you know, I've had,

3544
03:56:55,520 --> 03:56:59,760
I had, you know, similar views a year ago, might not have written it up a year ago, in part because

3545
03:56:59,760 --> 03:57:04,640
I think this cat wasn't out of the bag enough. You know, I think the other thing is

3546
03:57:08,160 --> 03:57:12,960
I think to be able to manage this challenge, you know, I think much broader swaths in society

3547
03:57:12,960 --> 03:57:15,920
will need to wake up, right? And if we're going to get the project, you know, we actually need

3548
03:57:16,000 --> 03:57:20,560
sort of like, you know, abroad by partisan understanding, the challenges facing us. And

3549
03:57:21,440 --> 03:57:25,920
so, you know, I think it's a tough trade off, but I think the sort of need to wake up people in the

3550
03:57:25,920 --> 03:57:31,680
United States in the sort of Western world and the Democratic coalition is ultimately imperative.

3551
03:57:31,680 --> 03:57:35,360
And, you know, I think my hope is more people here will read it than the PRC.

3552
03:57:37,600 --> 03:57:40,880
You know, and I think people sometimes underrate the importance of just kind of like writing it

3553
03:57:41,360 --> 03:57:46,160
laying out the strategic picture. And, you know, I think you've done actually a great service to

3554
03:57:46,160 --> 03:57:54,000
sort of mankind in some sense by, you know, with your podcast. And, you know, I think it's overall

3555
03:57:54,000 --> 03:57:58,880
been good. Okay, so by the way, you know, on the topic of, you know, Germany, you know, we were

3556
03:57:58,880 --> 03:58:02,720
talking at some point about kind of immigration story, right? Like you have a kind of interesting

3557
03:58:02,720 --> 03:58:09,040
story you haven't told. And I think you should tell. So a couple of years ago, I was in college and

3558
03:58:09,360 --> 03:58:15,760
I was 20. Yeah, I was about to turn 21. Yeah, I think it was, yeah, you came from India when you

3559
03:58:15,760 --> 03:58:21,760
were really right. Yeah. So I was eight or eight or nine. I lived in India and then we moved around

3560
03:58:21,760 --> 03:58:28,240
all over the place. But because of the backlog for Indians, the green card backlog, yeah, it's

3561
03:58:29,680 --> 03:58:34,080
we were we've been in the queue for like decades, even though you came at eight, you're still on

3562
03:58:34,080 --> 03:58:39,600
the H1B. Yeah. And when you're 21, you get kicked off the queue and you had to restart the process.

3563
03:58:39,600 --> 03:58:43,360
So I'm on my dad's, my dad's a doctor and I'm on his H1B as it depended. But when you're 21,

3564
03:58:43,360 --> 03:58:48,240
you get kicked off. Yeah. And so I'm 20 and I just like kind of dawns on me that this is my situation.

3565
03:58:48,240 --> 03:58:52,640
Yeah. And you're completely screwed. Right. And so I also had experience that my dad,

3566
03:58:53,200 --> 03:58:57,680
yeah, we've like moved all around the country. They have to prove that him as a doctor is like,

3567
03:58:57,680 --> 03:59:03,040
you can't get native talent. Yeah. And you can't start up. Yeah. So where can you not get like

3568
03:59:03,040 --> 03:59:07,920
even getting the H1B for you would have been like 20% lottery. So if you're lucky, you're in this

3569
03:59:07,920 --> 03:59:10,240
time. And they had to prove that they can't get native talent, which means like for him,

3570
03:59:10,240 --> 03:59:14,000
I'm like, we lived in North Dakota for three years, West Virginia for three years, Maryland,

3571
03:59:14,000 --> 03:59:19,040
West Texas. Yeah. And so kind of dawn on me, this is my situation. Is that turn 21, I'll be like

3572
03:59:19,600 --> 03:59:23,760
on this lottery, even if I get the lottery, I'll be a fucking code monkey for the rest of my life

3573
03:59:23,760 --> 03:59:29,040
because this thing isn't going to let up. Yeah. Can't do a startup. Exactly. And so at the same time,

3574
03:59:29,120 --> 03:59:32,640
I had been reading for the last year, I've been super obsessed with Paul Graham essays.

3575
03:59:33,360 --> 03:59:37,600
My plan at the time was to make a startup or something. I was super excited about that.

3576
03:59:37,600 --> 03:59:41,840
And it just occurred to me that I couldn't do this. Yeah. That like, this is just not in the

3577
03:59:41,840 --> 03:59:48,800
cars for me. Yeah. And so I was kind of depressed about it. I remember I kind of just, I was in

3578
03:59:48,800 --> 03:59:53,200
a daze through finals, because I had like, it just occurred to me and I was really like

3579
03:59:54,160 --> 04:00:01,360
anxious about it. Yeah. And I remember thinking to myself at the time that if somehow I end up

3580
04:00:01,360 --> 04:00:06,400
getting my green card before I turn 21, there's no fucking way I'm turning, becoming a code monkey

3581
04:00:06,400 --> 04:00:11,920
because the thing that I've, like this feeling of dread that I have is this realization that

3582
04:00:12,640 --> 04:00:17,920
I'm just going to have to be a code monkey. And I realize that's my default path. Yeah. If I,

3583
04:00:17,920 --> 04:00:21,680
if I hadn't sort of made a proactive effort not to do that, I would have graduated college as a

3584
04:00:21,680 --> 04:00:24,800
computer science student and I would have just done that. And that's the thing I was super scared

3585
04:00:24,800 --> 04:00:31,200
about. Yeah. So that was an important sort of realization for me. Anyway, so COVID happened

3586
04:00:31,200 --> 04:00:36,640
because of that, since there weren't foreigners coming, the backlog cleared fast. And by the skin

3587
04:00:36,640 --> 04:00:41,440
of my teeth, like a few months before I turned 21, extremely contingent reasons, I ended up getting

3588
04:00:41,440 --> 04:00:46,240
a green card because I got a green card. I could, you know, the whole podcast, right? Exactly.

3589
04:00:46,240 --> 04:00:51,200
I graduated college and I was like bumming around and I got, it was like, I graduated

3590
04:00:51,200 --> 04:00:55,200
just a semester early. I'm going to like do this podcast, see what happens. And it was,

3591
04:00:55,200 --> 04:01:01,200
it hadn't, it didn't have a green card. And it only existed because, yeah, it's actually,

3592
04:01:01,200 --> 04:01:05,360
because I think it's hard. It's probably, it's, you know, what is the impact of like immigration

3593
04:01:05,360 --> 04:01:09,040
reform? Like what is the impact of clearing, you know, like whatever 50,000 green cards in

3594
04:01:09,040 --> 04:01:13,920
the backlog? And you're such like an amazing example of like, you know, all of this is only

3595
04:01:14,000 --> 04:01:18,960
possible. And it's, yeah, it's, I mean, it's just incredibly tragic that this is so dysfunctional.

3596
04:01:18,960 --> 04:01:24,560
Yeah, yeah, yeah. No, it's insane. I'm glad you did it. I'm glad you kind of like, you know,

3597
04:01:24,560 --> 04:01:29,600
tried the, you know, the, the, the unusual path. Well, yeah, but I could only do it.

3598
04:01:30,240 --> 04:01:34,880
Obviously, I was extremely fortunate that I got the green card. I was like,

3599
04:01:36,000 --> 04:01:40,640
I had a little bit of saved up money and I got a small grant out of college. Thanks to the

3600
04:01:41,280 --> 04:01:45,040
future fund to like do this for basically the equivalent of six months.

3601
04:01:45,040 --> 04:01:50,560
And so it turned out really well. And then at each time when I was like, oh, okay, podcast,

3602
04:01:50,560 --> 04:01:55,280
come on, like, I wasted a few months on this, let's now go do something real. Something big would

3603
04:01:55,280 --> 04:02:00,880
happen. I would, Jeff Bezos would, huh? You kept with it. Yeah. Yeah. But there would always be

3604
04:02:00,880 --> 04:02:04,160
just like the moment I'm about to quit the podcast, something like Jeff Bezos will say

3605
04:02:04,160 --> 04:02:08,080
there's something nice about me on Twitter. The daily episodes gets like half a million views,

3606
04:02:08,080 --> 04:02:11,840
you know, and then now this is my career, but it was a sort of very,

3607
04:02:12,880 --> 04:02:16,080
looking back on it, incredibly contingent that things worked out the right way.

3608
04:02:16,080 --> 04:02:19,440
Yeah. I mean, look, if, if the AGI stuff goes down, you know, it'll be,

3609
04:02:20,160 --> 04:02:23,920
it'll be the most important kind of like, you know, source of, it'll be how,

3610
04:02:23,920 --> 04:02:28,640
maybe most of the people who kind of end up feeling the AGI are sure about it.

3611
04:02:28,640 --> 04:02:33,600
Yeah. Yeah. Yeah. Yeah. Also very much, you're very linked with the story in many ways. First,

3612
04:02:34,160 --> 04:02:40,560
the, I got like a $20,000 grant from a future fund right out of college. Yeah.

3613
04:02:40,560 --> 04:02:46,080
And that sustained me for six months or however long it was. Yeah. And without that,

3614
04:02:46,080 --> 04:02:48,480
I wouldn't, it was kind of crazy. Yeah. 10 grand or what was it?

3615
04:02:48,480 --> 04:02:53,120
It was, no, it just, it's tiny, but you know, it goes to show kind of how small grants can go.

3616
04:02:53,120 --> 04:02:57,680
Yeah. It's sort of the immersion ventures too. Yeah. Exactly. The immersion ventures and the,

3617
04:02:58,800 --> 04:03:01,920
well, the last year I've been in San Francisco, we've just been

3618
04:03:02,160 --> 04:03:06,640
in close contact the entire time and just bouncing ideas back and forth.

3619
04:03:06,640 --> 04:03:11,520
We're just basically the alpha I have, I think people would be surprised by how much I got from

3620
04:03:11,520 --> 04:03:16,080
you, Sholto, Trent and a couple others. I mean, it's been, it's been an absolute pleasure.

3621
04:03:16,080 --> 04:03:18,240
Yeah. Likewise. Likewise. It's been super fun. Yeah.

3622
04:03:19,840 --> 04:03:24,080
Okay. So some random questions for you. Yeah. If you could convert to Mormonism.

3623
04:03:24,080 --> 04:03:27,120
Yeah. And you could really believe it. Yeah. Would you do it? Would you push the button?

3624
04:03:27,680 --> 04:03:33,200
Yeah. Well, okay. Okay. Before I answer that question, one sort of observation about the

3625
04:03:33,200 --> 04:03:36,480
Mormons. So there's actually, there's an article that actually made a big impact on me.

3626
04:03:36,480 --> 04:03:39,360
Yeah. I think it was by McKick Hop and at some point, you know, in the Atlantic or whatever

3627
04:03:39,360 --> 04:03:44,400
about the Mormons. And I think the thing he kind of, you know, and I think he even was like

3628
04:03:44,400 --> 04:03:47,840
interviewed Romney and so on. And I think the thing I thought was really interesting in this

3629
04:03:47,840 --> 04:03:52,320
article was he kind of talked about how the experience of kind of growing up different,

3630
04:03:52,320 --> 04:03:55,360
you know, growing up very unusual, especially if you grow up Mormon outside of Utah, you know,

3631
04:03:55,360 --> 04:03:58,960
like the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird.

3632
04:04:00,080 --> 04:04:05,360
How that kind of got people prepared for being willing to be kind of outside of the norm later

3633
04:04:05,360 --> 04:04:09,120
on. And like, you know, Romney, you know, was willing to kind of take stands alone, you know,

3634
04:04:09,120 --> 04:04:13,840
in his party, because he believed, you know, what he believed is true. And I don't, I mean,

3635
04:04:13,840 --> 04:04:16,880
probably not to the same way, but I feel a little bit like this from kind of having grown up in

3636
04:04:16,880 --> 04:04:20,560
Germany, you know, and really not having like this sort of German system and having been kind

3637
04:04:20,560 --> 04:04:24,880
of an outsider or something. I think there's a certain amount in which kind of, yeah, growing

3638
04:04:24,880 --> 04:04:29,680
up in an outsider gives you kind of unusual strength later on to be kind of like willing to

3639
04:04:29,680 --> 04:04:34,400
say what you think. And so that is one thing I really appreciate about the Mormons, at least the

3640
04:04:34,400 --> 04:04:37,680
ones that grew up outside of Utah. I think, you know, the fertility rates, they're good, they're

3641
04:04:37,680 --> 04:04:42,080
important. They're going down as well, right? This is the thing that really clinched the kind of

3642
04:04:42,080 --> 04:04:46,400
fertility decline story for you. Even the Mormons. Yeah, even the Mormons, right? You're like, oh,

3643
04:04:46,400 --> 04:04:49,520
this is like a good sort of good story. The Mormons will replace everybody. Well, no, I don't know

3644
04:04:49,520 --> 04:04:52,400
if it's good, but it's like, at least, you know, at least come on, you know, like at least some

3645
04:04:52,400 --> 04:04:55,600
people will maintain high, you know, but it's no, no, you know, even the Mormons and sort of

3646
04:04:55,600 --> 04:04:59,360
basically, once these religious subgroups have high fertility rates, once they kind of grow big

3647
04:04:59,360 --> 04:05:03,840
enough, they become, they're too close in contact with sort of normal society and become normalized,

3648
04:05:03,840 --> 04:05:08,000
Mormon fertility rates drop from, I don't remember the exact numbers, maybe like four to two in the

3649
04:05:08,000 --> 04:05:12,560
course of 10, 20 years. Anyway, so it's like, you know, now people point to the Amish or whatever,

3650
04:05:12,560 --> 04:05:16,000
but I'm just like, it's probably just not scalable. And if you grow big enough, then there's just like,

3651
04:05:16,000 --> 04:05:20,080
you know, the sort of like, you know, the sort of like overwhelming force of modernity kind of gets

3652
04:05:20,080 --> 04:05:26,080
you. Yeah. No, if I could convert to Mormonism, look, I think there's something, I don't believe

3653
04:05:26,080 --> 04:05:29,280
it, right? If I believed it, I obviously would convert to Mormonism, right? Because it's, you

3654
04:05:29,280 --> 04:05:31,840
gotta, you gotta, but you can choose a world in which you do believe it.

3655
04:05:37,200 --> 04:05:40,640
I think there's something really valuable and kind of believing in something greater than

3656
04:05:40,640 --> 04:05:48,560
yourself and believing and having a certain amount of faith. You do, right? And you know,

3657
04:05:49,280 --> 04:05:54,000
you know, there's a, you know, feeling some sort of duty to the thing greater than yourself.

3658
04:05:54,000 --> 04:05:57,920
Yeah. And you know, maybe my version of this is somewhat different. You know, I think I feel

3659
04:05:57,920 --> 04:06:01,920
some sort of duty to like, I feel like there's some sort of historical weight on like how this

3660
04:06:01,920 --> 04:06:05,600
might play out. And I feel some sort of duty to like make that go well. I feel some sort of duty

3661
04:06:05,600 --> 04:06:13,600
to, you know, our country, to the national security of the United States. And, you know,

3662
04:06:13,600 --> 04:06:16,720
I think, I think that, I think it can be a force for a lot of good.

3663
04:06:17,280 --> 04:06:20,720
I, the, going back to the opening, I think just,

3664
04:06:24,320 --> 04:06:30,800
the thing that's especially impressive about that is, look, there's people who, at the company,

3665
04:06:30,800 --> 04:06:37,600
who have through years and decades of building up savings from working in tech have probably

3666
04:06:37,600 --> 04:06:44,640
10 civilians, liquid, more than that in terms of their equity. And the person, very many people

3667
04:06:44,640 --> 04:06:51,040
were concerned about the clusters and the Middle East and the secrets leaking to China and all

3668
04:06:51,040 --> 04:06:57,120
these things. But the person who actually made a hassle about it, and I think hassling people is

3669
04:06:57,120 --> 04:07:03,760
so underrated, I think that one person who made a hassle about it is the 22 year old who has less

3670
04:07:03,840 --> 04:07:10,240
than a year at the company who doesn't have savings built up. Who isn't like a solidified member of the,

3671
04:07:11,760 --> 04:07:16,080
I think that's a sort of like, maybe, maybe it's me being naive and, you know, not knowing how big

3672
04:07:16,080 --> 04:07:20,480
companies work. And, you know, but like, there's a, you know, I think sometimes a bit of a speech

3673
04:07:20,480 --> 04:07:24,560
deontologist, you know, I kind of believe in saying what you think. Sometimes friends tell me I

3674
04:07:24,560 --> 04:07:33,520
should be more of a speech consequentialist. No, I think I really think the amount of people who

3675
04:07:33,600 --> 04:07:38,880
when they have the opportunity to talk to the person will just bring up the thing. I've been

3676
04:07:38,880 --> 04:07:41,760
with you in multiple contexts, and I guess I shouldn't reveal who the person is or what the

3677
04:07:41,760 --> 04:07:47,360
context was, but I've just been like very impressed that the dinner begins and by the end, somebody

3678
04:07:47,360 --> 04:07:54,080
who has a major voice in how things go is seriously thinking about a worldview they would have found

3679
04:07:54,080 --> 04:07:59,920
incredibly alien before the dinner or something. And I've been impressed that like, just like,

3680
04:08:00,480 --> 04:08:07,520
give them the spiel and hassle them. I mean, look, I just, I think, I think I feel this stuff

3681
04:08:07,520 --> 04:08:10,800
pretty viscerally now. You know, I think there's a time, you know, there's a time when I thought

3682
04:08:10,800 --> 04:08:14,720
about the stuff a lot, but it was kind of like econ models and like, you know, kind of like these

3683
04:08:14,720 --> 04:08:18,240
sort of theoretical abstractions and, you know, you talk about human brain size or whatever.

3684
04:08:18,240 --> 04:08:23,760
Right. And I think, you know, since, I think since at least last year, you know, I feel like,

3685
04:08:23,760 --> 04:08:29,120
you know, I feel like I can see it, you know, and I just, I feel it. And I think I can like,

3686
04:08:29,120 --> 04:08:33,920
you know, I can sort of see the cluster that I can see the kind of rough combination of algorithms

3687
04:08:33,920 --> 04:08:39,120
and the people that be involved and how this is going to play out. And, you know, I think,

3688
04:08:39,120 --> 04:08:42,240
look, we'll see how it plays out. There's many ways this could be wrong. There's many ways it

3689
04:08:42,240 --> 04:08:48,400
could go. But I think this could get very real. Yeah. Should we talk about what you're up to next?

3690
04:08:48,400 --> 04:08:53,040
Sure. Yeah. Okay. So you're starting an investment firm anchor investments from

3691
04:08:53,120 --> 04:08:56,560
Nat Friedman, Daniel Gross, Patrick Lawson, John Collison.

3692
04:08:58,640 --> 04:09:03,120
First of all, why is this thing to do? You believe the AGI is coming in a few years?

3693
04:09:04,800 --> 04:09:09,520
Why the investment firm? A good question, fair question. Wait, so I mean, a couple things. One

3694
04:09:09,520 --> 04:09:12,720
is just, you know, I think we talked about this earlier, but it's like the screen doesn't go blank,

3695
04:09:12,720 --> 04:09:15,600
you know, when sort of AGI is different intelligence happens, I think people really

3696
04:09:15,600 --> 04:09:18,560
underrate the sort of basically the sort of decade after it, you have the intelligence

3697
04:09:18,560 --> 04:09:22,560
explosion, that's maybe the most sort of wild period. But I think the decade after is also

3698
04:09:22,560 --> 04:09:26,720
going to be wild. And you know, this combination of human institutions, but super intelligence,

3699
04:09:26,720 --> 04:09:29,920
you have crazy kind of geopolitical things going on, you have the sort of broadening of

3700
04:09:29,920 --> 04:09:34,720
this explosive growth. And basically, yeah, I think it's going to be a really important period.

3701
04:09:34,720 --> 04:09:37,200
I think capital will really matter, you know, eventually, you know, like, you know, going to

3702
04:09:37,200 --> 04:09:41,840
go to the stars, you know, going to go to the galaxies. So anyway, so part of the answer is

3703
04:09:41,840 --> 04:09:45,600
just like, look, I think done, done, right, there's a lot of money to be made, you know,

3704
04:09:45,600 --> 04:09:49,040
I think if AGI were priced in tomorrow, you could maybe make 100x, probably you can make

3705
04:09:49,040 --> 04:09:55,040
even way more than that because of the sequencing. And, and, and, you know, capital matters.

3706
04:09:56,080 --> 04:10:01,840
I think the other reason is just, you know, some amount of freedom and independence. And I think,

3707
04:10:01,840 --> 04:10:06,960
you know, you know, I think there's some people who are very smart about this AGI stuff and who

3708
04:10:06,960 --> 04:10:10,800
are kind of like see it coming. But I think almost all of them, you know, are kind of,

3709
04:10:10,800 --> 04:10:14,000
you know, constrained in various ways right there in the labs, you know, they're in some,

3710
04:10:14,000 --> 04:10:18,560
you know, some other position where they can't really talk about the stuff. And, you know,

3711
04:10:18,560 --> 04:10:21,680
in some sense, I've really admired sort of the thing you've done, which is I think it's really

3712
04:10:21,680 --> 04:10:24,880
important that there's sort of voices of reason on this stuff publicly, or people who are in

3713
04:10:24,880 --> 04:10:28,560
positions to kind of advise important actors and so on. And so I think there's a, you know,

3714
04:10:29,200 --> 04:10:32,400
basically the thing this investment firm will be, will be kind of like, you know, a brain trust

3715
04:10:32,400 --> 04:10:35,760
on AI, it's going to be all about situational awareness. We're going to have the best situational

3716
04:10:35,760 --> 04:10:38,720
awareness in the business, you know, we're going to have way more situational business than any of

3717
04:10:38,720 --> 04:10:41,920
the people who manage money in New York. We're definitely going to, you know, we're going to do

3718
04:10:41,920 --> 04:10:46,240
great on investing. But it's the same sort of situational awareness that I think is going to

3719
04:10:46,240 --> 04:10:52,480
be important for understanding what's happening, being a voice of reason publicly and, and, and

3720
04:10:52,480 --> 04:10:59,440
sort of being able to be in a position to advise. Yeah. I, there was the book about Peter Thiel.

3721
04:10:59,440 --> 04:11:05,120
Yeah. They had an interesting quote about his hedge fund. I think it got terrible return. So

3722
04:11:05,680 --> 04:11:07,840
this isn't the example. Right, right. That's the, that's the sort of

3723
04:11:07,840 --> 04:11:11,920
bare case, right? It's like two theoretical and sure. Yeah. But they had an interesting quote

3724
04:11:12,000 --> 04:11:16,640
that it's, it's, that it's like basically a think tank inside of a hedge fund.

3725
04:11:16,640 --> 04:11:23,200
Yeah. So we're trying to build. Right. Yeah. So presumably you've thought about the ways in which

3726
04:11:23,200 --> 04:11:26,880
these kinds of things can blow. There's a very, there's a lot of interesting business history

3727
04:11:26,880 --> 04:11:34,400
books about people who got the pieces right, but timed it wrong. Yeah. Where they, they buy that

3728
04:11:34,400 --> 04:11:38,240
internet's going to be a big deal. Yeah. They sell at the wrong time and buy the wrong time

3729
04:11:38,320 --> 04:11:42,400
in the dotcom boom. And so they miss out on the gains, even though they're right about the,

3730
04:11:42,400 --> 04:11:46,400
anyways, yeah. What, what is that trick to preventing that kind of thing?

3731
04:11:46,400 --> 04:11:50,000
Yeah. I mean, look, obviously you can't, you know, not blowing up as sort of like, you know,

3732
04:11:50,000 --> 04:11:55,040
task number one and two or whatever. I mean, you know, I think this investment firm, it is going

3733
04:11:55,040 --> 04:11:58,800
to just be betting on AGI, you know, betting on AGI and super intelligence before the decade

3734
04:11:58,800 --> 04:12:02,560
is out, taking that seriously, making the bets you would make, you know, if you took that seriously.

3735
04:12:02,560 --> 04:12:07,280
So, you know, I think if that's wrong, you know, firm is not going to do that well. The thing you

3736
04:12:07,280 --> 04:12:10,400
have to be resistant to is like, you have to be able to resist and get, you know, one or a couple

3737
04:12:10,400 --> 04:12:14,080
or a few kind of individual calls, right? You know, it's like AI stagnates for a year because of the

3738
04:12:14,080 --> 04:12:18,720
data wall or like, you know, you got, you got the call wrong on like when revenue would go up. And

3739
04:12:18,720 --> 04:12:23,040
so anyway, that's pretty critical. You have to get timing right. I do think in general that the

3740
04:12:23,040 --> 04:12:26,960
sort of sequence of bets on the way to AGI is actually pretty critical. And I think a thing

3741
04:12:26,960 --> 04:12:32,480
people underrate. So, all right. I mean, yeah. So like, where does the story start, right? So like,

3742
04:12:32,560 --> 04:12:38,160
obviously, the sort of only bet over the last year was NVIDIA. And, you know, it's obvious now,

3743
04:12:39,520 --> 04:12:44,000
very few people did it. This is sort of also, you know, a classic debate I and a friend had with

3744
04:12:44,000 --> 04:12:47,760
another colleague of ours, where this colleague was really into TSM, you know, TSMC. And he was

3745
04:12:47,760 --> 04:12:51,680
just kind of like, well, you know, like, these tabs are going to be so valuable. And also like,

3746
04:12:51,680 --> 04:12:54,720
NVIDIA, there's just a lot of videos in credit risk, right? It's like, maybe somebody else makes

3747
04:12:54,720 --> 04:12:59,680
better GPUs. That was basically right. But sort of only NVIDIA had the AI beta, right? Because

3748
04:12:59,680 --> 04:13:03,440
only NVIDIA was kind of like a large fraction AI. The next few doublings would just like

3749
04:13:03,440 --> 04:13:07,440
meaningfully explode their revenue. Whereas TSMC was, you know, a couple percent AI. So,

3750
04:13:07,440 --> 04:13:10,400
you know, even though there's going to be a few doublings of AI, not going to make that big of

3751
04:13:10,400 --> 04:13:14,720
an impact. All right. So it's sort of like, the only place to find the AI beta basically was NVIDIA

3752
04:13:14,720 --> 04:13:21,520
for a while. You know, now it's broadening, right? So now TSM is like, you know, 20 percent AI by

3753
04:13:21,520 --> 04:13:25,280
like 27 or something is what they're saying. One more doubling, it'll be kind of like a large

3754
04:13:25,280 --> 04:13:28,480
fraction of what they're doing. And, you know, there's a whole, you know, whole stack, you know,

3755
04:13:28,480 --> 04:13:33,360
there's like, you know, there's people making memory and COAS and, you know, power, you know,

3756
04:13:33,360 --> 04:13:36,880
utility companies are starting to get excited about AI. And they're like, oh, it'll, you know,

3757
04:13:37,520 --> 04:13:41,600
power production in the United States will grow, you know, not 2.5 percent, 5 percent of the next

3758
04:13:41,600 --> 04:13:48,560
five years. And I'm like, no, it'll grow more. You know, at some point, you know, you know,

3759
04:13:48,560 --> 04:13:52,080
like a Google or something becomes interesting, you know, people are excited about them with AI

3760
04:13:52,080 --> 04:13:55,440
because it's like, oh, you know, AI revenue will be, you know, 10 billion or tens of billions.

3761
04:13:55,440 --> 04:13:58,880
And I'm kind of like, ah, I don't really care about them before then. I care about it, you know,

3762
04:13:58,880 --> 04:14:02,560
once it, you know, once you get the AI beta, right? And so at some point, you know, Google

3763
04:14:02,560 --> 04:14:06,400
will get, you know, $100 billion of revenue from AI. Probably their stock will explode,

3764
04:14:06,400 --> 04:14:08,960
you know, they're going to become, you know, 5 trillion, 10 trillion dollar company.

3765
04:14:09,680 --> 04:14:12,400
Anyway, so the timing there is very important. You have to get the timing right. You have to

3766
04:14:12,400 --> 04:14:15,520
get the sequence right. You know, at some point, actually, I think like, you know, there's going

3767
04:14:15,520 --> 04:14:20,240
to be real tailwind to equities from real interest rates, right? So basically in these sort of

3768
04:14:20,320 --> 04:14:24,960
explosive growths worlds, you would expect real interest rates to go up a lot, both on the sort

3769
04:14:24,960 --> 04:14:29,520
of like, you know, a basic both sides of the equation, right? On the supply side or on the

3770
04:14:29,520 --> 04:14:35,280
sort of demand for money side, because, you know, people are going to be making these crazy investments,

3771
04:14:35,280 --> 04:14:38,240
you know, initially in clusters and then in the robo factories or whatever, right? And so they're

3772
04:14:38,240 --> 04:14:43,760
going to be borrowing like crazy. They want all this capital, higher AI. And then on the sort of

3773
04:14:43,760 --> 04:14:48,800
like consumer saving side, right, to like, you know, to give up all this capital, you know,

3774
04:14:48,800 --> 04:14:52,160
this sort of like Euler equation, standard sort of intratemporal transfer, you know,

3775
04:14:53,280 --> 04:14:55,840
trade off of consumption. So standard.

3776
04:14:58,880 --> 04:15:02,000
Some of our friends have a paper on this, you know, basically, if you expect, you know,

3777
04:15:02,000 --> 04:15:05,280
if consumers expect real growth rates to be higher, you know, interest rates are going to be

3778
04:15:05,280 --> 04:15:08,480
higher because they're less willing to give up consumption, you know, consumption in the

3779
04:15:10,240 --> 04:15:12,720
less willing to give up consumption day for consumption in the future.

3780
04:15:13,280 --> 04:15:16,480
Anyway, so at some point, real interest rates will go up if sort of ADA is greater than one,

3781
04:15:16,480 --> 04:15:20,720
that actually means equities, you know, higher growth rate expectations mean equities go down

3782
04:15:20,720 --> 04:15:23,200
because the sort of interest rate effect outweighs the growth rate effect.

3783
04:15:23,840 --> 04:15:27,200
And so, you know, at some point, there's like big, the big bond short, you got to get that right,

3784
04:15:27,200 --> 04:15:30,880
you got to get it right, that, you know, nationalization, you know, like, you got, you know,

3785
04:15:30,880 --> 04:15:33,600
anyway, so there's this whole sequence of things, you got to get that right.

3786
04:15:33,600 --> 04:15:36,800
And the unknown unknowns, unknown unknowns. Yeah. And so you've, look, you've got to be

3787
04:15:36,800 --> 04:15:40,080
really, really careful about your like overall like this positioning, right? And because, you

3788
04:15:40,080 --> 04:15:43,280
know, you know, if you expect these kind of crazy events to play out, there's going to be crazy

3789
04:15:43,280 --> 04:15:47,920
things you didn't see. You know, you do also want to make the sort of kind of bets that are tailored

3790
04:15:47,920 --> 04:15:51,760
to your scenarios in the sense of like, you know, you want to find bets that are bets on the tails,

3791
04:15:51,760 --> 04:15:56,240
right? You know, I don't think anyone is expecting, you know, interest rates to go above,

3792
04:15:56,240 --> 04:16:00,240
you know, 10% like real interest rates. But, you know, I think there's at least a serious chance

3793
04:16:00,240 --> 04:16:04,320
of that, you know, before the decade is out. And so, you know, maybe there's some like cheap

3794
04:16:04,320 --> 04:16:08,880
insurance you can buy on that, you know, very silly question. In these worlds,

3795
04:16:09,680 --> 04:16:14,240
are financial markets where you make these kinds of bets going to be respected? And

3796
04:16:15,120 --> 04:16:19,680
like, you know, like, is my fidelity account going to mean anything when we have their 50%

3797
04:16:19,680 --> 04:16:23,280
economic growth? Like, who's, who's like, we got to respect his property rights?

3798
04:16:23,280 --> 04:16:26,400
That's pretty deep into it. The bond short, the sort of 50 and 52nd hour growth, that's pretty

3799
04:16:26,400 --> 04:16:29,440
deep into it. I mean, again, there's this whole sequence of things. But yeah, no, I think property

3800
04:16:29,440 --> 04:16:34,240
rights will be instructed again, in the sort of modal world, the project. Yeah. At some point,

3801
04:16:34,240 --> 04:16:37,040
at some point, there's going to be figuring out the property rights for the galaxies, you know,

3802
04:16:37,040 --> 04:16:43,440
and that'll be interesting. So there's an interesting question about

3803
04:16:45,440 --> 04:16:50,240
going back to your strategy about, well, the 30s will really matter a lot about how the rest of

3804
04:16:50,240 --> 04:16:55,280
the future goes. And you want to be in a position of influence by that point, because of capital.

3805
04:16:56,480 --> 04:17:01,200
It's worth considering, as far as I know, there's probably a whole bunch of literature on this,

3806
04:17:01,200 --> 04:17:07,600
I'm just riffing. But the, the landed gentry during the, before the beginning of the industrial

3807
04:17:07,600 --> 04:17:13,920
revolution, I'm not sure if they were able to leverage their position in a sort of georgist

3808
04:17:14,560 --> 04:17:23,840
or pickety type sense in order to accrue the returns that were realized through the industrial

3809
04:17:23,840 --> 04:17:27,920
revolution. And I don't know what happened. At some point, they were just wearing the

3810
04:17:28,000 --> 04:17:34,320
landed gentry. But I'd be concerned that even if you make great investment calls,

3811
04:17:34,320 --> 04:17:38,640
you'll be like the guy who owned a lot of land, farmland before the industrial revolution,

3812
04:17:38,640 --> 04:17:42,640
and like the guy who's actually going to make a bunch of money is the one with the C mentioned,

3813
04:17:42,640 --> 04:17:46,080
even if he doesn't make that much money, most of the benefits are sort of widely diffused and so

3814
04:17:46,080 --> 04:17:51,280
forth. I mean, I think that the analog is like you sell your land, you put it all and sort of

3815
04:17:51,280 --> 04:17:56,560
that, you know, that the people who are building the new industry. I think the, I mean, I think

3816
04:17:56,560 --> 04:18:01,840
the sort of like real depreciating asset, you know, for me is human capital, right? Yeah, no,

3817
04:18:01,840 --> 04:18:05,520
look, I'm serious, right? It's like, you know, there's something about like, you know, I don't

3818
04:18:05,520 --> 04:18:08,000
know, it was like valedictorian of Columbia, you know, the thing that made you special is

3819
04:18:08,000 --> 04:18:11,520
you're smart, right? But actually, like, you know, that might not matter in like four years,

3820
04:18:11,520 --> 04:18:15,520
you know, because it's actually automatable. Right. And so anyway, a friend joke that the

3821
04:18:15,520 --> 04:18:20,000
sort of investment firm is perfectly hedged for me. It's like, you know, either like AGI this

3822
04:18:20,000 --> 04:18:24,640
decade, and yeah, your human capital is depreciated, but you've turned that into financial capital,

3823
04:18:24,640 --> 04:18:28,480
or you know, like no AGI this decade, in which case, maybe the firm doesn't do that well,

3824
04:18:28,480 --> 04:18:30,800
but you know, you're still in your 20s and you're still smart.

3825
04:18:32,640 --> 04:18:38,320
Excellent. And what's your story for why AGI hasn't been priced in the story?

3826
04:18:39,360 --> 04:18:43,600
Financial markets are supposed to be very efficient to say very hard to get an edge here.

3827
04:18:45,360 --> 04:18:50,480
Naively, you just say, well, I've looked at these scaling curves and they imply that we're

3828
04:18:50,480 --> 04:18:54,400
going to be buying much more computed energy than the analysts realize.

3829
04:18:55,280 --> 04:18:57,520
Shouldn't those analysts be broke by now? What's going on?

3830
04:18:59,280 --> 04:19:06,800
Yeah. I mean, I used to be a true EMH guy. I was an economist, you know. I think the thing I,

3831
04:19:07,600 --> 04:19:12,080
you know, changed my mind on is that I think there can be kind of groups of people, smart people,

3832
04:19:12,080 --> 04:19:15,200
you know, who are, you know, say they're in San Francisco, who do just have

3833
04:19:16,080 --> 04:19:20,960
off over the rest of society and kind of seeing the future. And so like COVID, right? Like,

3834
04:19:20,960 --> 04:19:25,520
I think there's just honestly kind of similar group of people who just saw that and called it

3835
04:19:25,520 --> 04:19:30,080
completely correctly. And, you know, they showed at the market they did really well.

3836
04:19:31,280 --> 04:19:43,200
You know, a bunch of other sort of things like that. So, you know, why is AGI not priced in?

3837
04:19:43,200 --> 04:19:47,360
You know, it's sort of, you know, why hasn't the government nationalized the labs yet, right?

3838
04:19:47,360 --> 04:19:50,720
It's like, you know, this, you know, society hasn't priced it in yet and sort of it hasn't

3839
04:19:50,720 --> 04:19:56,080
completely diffused. And, you know, again, it might be wrong, right? But I just think sort of,

3840
04:19:57,680 --> 04:20:00,880
you know, not that many people take these ideas seriously yet. Yeah. Yeah.

3841
04:20:01,760 --> 04:20:07,360
Yeah. A couple of other sort of ideas that I was playing around with with regards to

3842
04:20:07,360 --> 04:20:14,480
reading it a chance to talk about, but the systems competition, there's a very interesting,

3843
04:20:16,320 --> 04:20:18,480
one of my favorite books about World War II is the Victor Davis Hansen

3844
04:20:21,760 --> 04:20:28,000
summary of everything. And he explains why the Allies made better decisions than the Axis.

3845
04:20:28,000 --> 04:20:32,240
Why did they? And so obviously, there were some decisions that the Axis made that were pretty

3846
04:20:32,240 --> 04:20:35,440
like Blitzkrieg, whatever. That was sort of by accident, though.

3847
04:20:35,520 --> 04:20:37,840
In what sense? That they just had the infrastructure left over?

3848
04:20:37,840 --> 04:20:42,080
Well, no, I mean, the sort of, I think, I mean, I don't, I mean, I think sort of my read of it is

3849
04:20:42,080 --> 04:20:45,440
Blitzkrieg wasn't kind of some like a genius strategy. It was just kind of, it was like more

3850
04:20:45,440 --> 04:20:50,080
like their hand was forced. I mean, this is sort of the very Adam Tuzi and story of World War II,

3851
04:20:50,080 --> 04:20:53,680
right? But it was, you know, there's sort of this long war versus short war. I think it's

3852
04:20:53,680 --> 04:20:57,680
actually kind of an important concept. I think sort of Germany realized that if they were in a

3853
04:20:57,680 --> 04:21:02,560
long war, including the United States, you know, they would not be able to compete industrially.

3854
04:21:02,560 --> 04:21:07,360
So their only path to victory was like make it a short war, right? And that, that sort of worked

3855
04:21:07,360 --> 04:21:11,360
much more spectacularly than they thought, right? And sort of take over France and take over much

3856
04:21:11,360 --> 04:21:15,200
of Europe. And so then, you know, the decision to invade the Soviet Union, it was, you know,

3857
04:21:15,760 --> 04:21:20,240
it was, it was, um, look, if it was, it was about the Western front in some sense, because it was

3858
04:21:20,240 --> 04:21:23,920
like, we've got to get the resources. You know, we don't, we're actually, we don't actually have a

3859
04:21:23,920 --> 04:21:27,520
bunch of the stuff we need, like, you know, oil and so on. You know, Auschwitz was actually just

3860
04:21:27,520 --> 04:21:30,640
this giant chemical plant to make kind of like synthetic oil and a bunch of these things was

3861
04:21:30,640 --> 04:21:35,920
the largest industrial project in Nazi Germany. And so, you know, and sort of they thought, well,

3862
04:21:35,920 --> 04:21:39,360
you know, we completely crushed them in World War I, you know, it'll be easy, we'll invade them,

3863
04:21:39,360 --> 04:21:43,440
we'll get the resources, and then we can fight on the Western front. And even during the sort of

3864
04:21:43,440 --> 04:21:47,040
whole invasion of the Soviet Union, even though kind of like a large amount of the sort of,

3865
04:21:47,040 --> 04:21:50,400
you know, the sort of deaths happened there, you know, like a large fraction of German industrial

3866
04:21:50,400 --> 04:21:54,880
production was actually, you know, like planes and naval, you know, and so on, those directed,

3867
04:21:54,880 --> 04:21:58,640
you know, towards the Western front and towards the, you know, the Western allies.

3868
04:21:58,800 --> 04:22:01,440
Well, and then so the point that Hansen was making was,

3869
04:22:02,000 --> 04:22:05,280
by the way, I think this concept of like long war and short war is kind of interesting and with

3870
04:22:05,280 --> 04:22:10,160
respect to thinking about the China competition, which is like, you know, I worry a lot about kind

3871
04:22:10,160 --> 04:22:15,920
of, you know, the decline of sort of American, like late in American industrial capacity, you know,

3872
04:22:15,920 --> 04:22:22,240
like, I think China builds like 200 times more ships than we do right now. You know, some crazy

3873
04:22:22,240 --> 04:22:26,000
way. And so it's like, maybe we have this superiority, say in the non AI worlds, we have

3874
04:22:26,000 --> 04:22:29,600
the superiority in military material, kind of like win a short war, at least, you know,

3875
04:22:29,600 --> 04:22:33,760
kind of defend Taiwan in some sense. But like if it actually goes on, you know, it's like maybe

3876
04:22:33,760 --> 04:22:39,200
China is much better able to mobilize, mobilize industrial resources in a way that like we just

3877
04:22:39,200 --> 04:22:44,080
don't have that same ability anymore. I think this is also relevant to the AI thing in the sense of

3878
04:22:44,080 --> 04:22:48,640
like, if it comes down to sort of a game about building, right, including like, maybe AGI takes

3879
04:22:48,640 --> 04:22:52,320
the trillion dollar cluster, not the hundred billion dollar cluster, maybe, or even maybe AGI

3880
04:22:52,320 --> 04:22:56,240
takes the, you know, is on the hundred billion dollar cluster. But, you know, it really matters

3881
04:22:56,240 --> 04:22:59,520
if you can run, you know, 10x, you can do one more order of magnitude of compute for your super

3882
04:22:59,520 --> 04:23:05,040
intelligence or whatever. That, you know, maybe right now they're behind, but they just have this

3883
04:23:05,040 --> 04:23:09,360
sort of like raw, late in industrial capacity to help build us. And that matters both in the

3884
04:23:09,360 --> 04:23:13,600
run up to AGI and after, right, where it's like, you have this super intelligence on your cluster,

3885
04:23:13,600 --> 04:23:17,920
now it's time to kind of like expand the explosive growth. And, you know, like, will we let the

3886
04:23:17,920 --> 04:23:22,400
ROA factories run wild? Like, maybe not. But like, maybe China will. Or like, you know, will we,

3887
04:23:22,400 --> 04:23:25,200
will, yeah, will we produce the, how many, how many of the drones will we produce?

3888
04:23:26,240 --> 04:23:29,360
And I think, yeah, so there's some sort of like outbuilding in the industrial explosion that I

3889
04:23:29,360 --> 04:23:33,360
worked on. You've got to be one of the few people in the world who is both concerned about alignment,

3890
04:23:33,360 --> 04:23:38,720
but also wants to make sure that we'll let the ROA factories proceed once we get the ASI to beat

3891
04:23:38,720 --> 04:23:44,880
out China. Like, it's all, it's all part of the picture. Yeah, yeah, yeah.

3892
04:23:45,120 --> 04:23:51,840
And by the way, speaking of the ASIs and the robot factories, one of the interesting things,

3893
04:23:51,840 --> 04:23:56,800
RoboArmy's too. Yeah, one of the interesting things, there's this question of what you do

3894
04:23:56,800 --> 04:24:02,480
with industrial scale intelligence. And obviously, it's not chatbots, but it's a, I think it's very

3895
04:24:02,480 --> 04:24:12,000
hard to predict. Yeah, yeah. But the history of oil is very interesting. We're in the, I think it's

3896
04:24:12,000 --> 04:24:18,160
in the 1860s that we figure out how to refine oil, some geologist. And so then standard oil

3897
04:24:18,160 --> 04:24:24,720
got started, there's this huge boom. It changes American politics, entire legislators are getting

3898
04:24:24,720 --> 04:24:31,360
bought out by oil interest and presidents are getting elected based on the divisions about

3899
04:24:31,360 --> 04:24:36,720
oil and breaking them up and everything. And all of this has happened. The world has never

3900
04:24:36,800 --> 04:24:44,320
revolutionized before the car has been invented. And so when the light bulb was invented, I think

3901
04:24:44,320 --> 04:24:49,840
it was like 50 years after oil refining had been discovered, majority of standard oil's history

3902
04:24:49,840 --> 04:24:54,960
is before the car is invented. Carousel lamps. Exactly. So it's just used for lighting.

3903
04:24:54,960 --> 04:24:58,800
Then they thought oil would just no longer be relevant. Yeah, yeah. So there was a concern

3904
04:24:58,800 --> 04:25:05,200
that standard oil would go to brain corrupt when the light bulb was invented. But then

3905
04:25:06,000 --> 04:25:10,320
there's sort of, you realize that there's immense amount of compressed energy here.

3906
04:25:10,960 --> 04:25:15,280
You're going to have billions of gallons of this stuff a year. And it's hard to

3907
04:25:15,280 --> 04:25:20,960
sort of predict in advance what you can do with that. And then later on, it turns out transportation

3908
04:25:21,520 --> 04:25:28,480
cars with, that's what it's used for. Anyways, with intelligence, maybe one answer is the

3909
04:25:28,480 --> 04:25:33,440
intelligence explosion. But even after that, so you have all these ASIs and you have enough

3910
04:25:33,440 --> 04:25:38,480
compute, especially the compute they'll build to run hundreds of millions of GPUs will hum.

3911
04:25:38,480 --> 04:25:42,560
Yeah. But what are we doing with that? And it's very hard to predict in advance. I think it would

3912
04:25:42,560 --> 04:25:49,360
be very interesting to figure out what the Jupiter brains will be doing. So look, there's

3913
04:25:49,360 --> 04:25:55,840
situational awareness of where things stand now. And we've gotten a good dose of that.

3914
04:25:58,160 --> 04:26:00,480
Obviously, a lot of the things we're talking about now, you couldn't have

3915
04:26:01,200 --> 04:26:08,720
prejudged many years back in the past. And part of your role implies that things will accelerate

3916
04:26:08,720 --> 04:26:15,200
because of AI getting the process. But many other things that are unpredictable fundamentally,

3917
04:26:15,200 --> 04:26:19,200
basically how people will react, how the political system will react, how foreign adversaries will

3918
04:26:19,200 --> 04:26:26,720
react. Those things will become evident over time. So the situational awareness is not just

3919
04:26:26,800 --> 04:26:32,000
knowing where the picture stands now, but being in a position to react appropriately to new information,

3920
04:26:32,960 --> 04:26:36,640
to change your worldview as a result, to change your recommendations as a result.

3921
04:26:37,680 --> 04:26:43,440
What is the appropriate way to think about situational awareness as a continuous process

3922
04:26:43,440 --> 04:26:48,720
rather than as a one-time thing you realized? Yep. No, I think this is great. Look, I think

3923
04:26:48,720 --> 04:26:52,640
there's a sort of mental flexibility and willing to change your mind. That's really important.

3924
04:26:52,640 --> 04:26:56,480
I actually think this is sort of like how a lot of brains have been broken in the AGI debate,

3925
04:26:57,040 --> 04:27:02,320
the tumors who actually, I think we're really prescient on AGI thinking about the stuff a decade

3926
04:27:02,320 --> 04:27:06,320
ago, but they haven't actually updated on the empirical realities of deep learning. They're

3927
04:27:06,320 --> 04:27:10,320
sort of like, the proposals are really kind of even unworkable. This doesn't really make sense.

3928
04:27:11,040 --> 04:27:13,920
There's people who come in with sort of a predefined ideology. They're just kind of like,

3929
04:27:13,920 --> 04:27:18,000
the EX a little bit. They like to shitpost about technology, but they're not actually thinking

3930
04:27:18,000 --> 04:27:22,160
through. I mean, either the sort of stagnationists who think this stuff is only going to be a

3931
04:27:22,160 --> 04:27:25,760
chatbot, and so of course it isn't risky, or they're just not thinking through the kind of like

3932
04:27:25,760 --> 04:27:30,880
actually immense national security implications and how that's going to go. I actually think

3933
04:27:30,880 --> 04:27:34,640
there's kind of a risk in kind of like having written this stuff down and put it online.

3934
04:27:37,120 --> 04:27:40,080
I think this sometimes happens to people as a sort of calcification of the worldview,

3935
04:27:40,080 --> 04:27:44,400
because now they've publicly articulated this position. Maybe there's some evidence against

3936
04:27:44,400 --> 04:27:49,600
it, but they're clinging to it. I actually want to give the big disclaimer on like,

3937
04:27:49,600 --> 04:27:53,040
I think it's really valuable to paint this sort of very concrete and visceral picture.

3938
04:27:54,400 --> 04:27:58,960
I think this is currently my best guess on how this decade will go. I think if it goes

3939
04:27:58,960 --> 04:28:05,440
anywhere like this, it will be wild, but given the rapid pace of progress, we're going to keep

3940
04:28:05,440 --> 04:28:11,920
getting a lot more information. I think it's important to sort of keep your head on straight

3941
04:28:11,920 --> 04:28:19,440
about that. I feel like the most important thing here is that, and this relates to some of the

3942
04:28:19,440 --> 04:28:26,160
stuff we've talked about and the world being surprisingly small and so on. I feel like I

3943
04:28:26,160 --> 04:28:29,200
used to have this worldview of like, look, there's important things happening in the world, but there's

3944
04:28:29,200 --> 04:28:32,720
like people who are taking care of it, and there's like the people in government, and there's again,

3945
04:28:32,720 --> 04:28:38,960
even like AI labs have idealized, and people are on it. Surely there must be on it, right?

3946
04:28:38,960 --> 04:28:42,560
And I think just some of this personal experience, even seeing how kind of COVID went,

3947
04:28:44,160 --> 04:28:47,680
people aren't necessarily, there's not some, not that somebody else is just kind of on it and

3948
04:28:47,760 --> 04:28:54,720
making sure this goes well, however it goes. You know, the thing that I think will really

3949
04:28:54,720 --> 04:28:59,680
matter is that there are sort of good people who take this stuff as seriously as it deserves,

3950
04:28:59,680 --> 04:29:03,920
and who are willing to kind of take the implication seriously, who are willing to, you know,

3951
04:29:03,920 --> 04:29:07,680
who have situational awareness, are willing to change their minds, are willing to sort of

3952
04:29:07,680 --> 04:29:14,000
stare the picture in the face, and you know, I'm counting on those good people.

3953
04:29:14,000 --> 04:29:17,920
All right, that's a great place to close Leopold.

3954
04:29:17,920 --> 04:29:21,360
Thanks so much, Tarkash. This is the absolute joy.

3955
04:29:21,360 --> 04:29:26,320
Hey everybody, I hope you enjoyed that episode with Leopold. There's actually one more riff

3956
04:29:26,320 --> 04:29:31,120
about German history that he had after a break, and it was pretty interesting, so I didn't want

3957
04:29:31,120 --> 04:29:36,400
to cut it out, so I've just included it after this outro. You can advertise on the show now,

3958
04:29:36,400 --> 04:29:41,200
so if you're interested, you can reach out at the forum in the description below.

3959
04:29:41,280 --> 04:29:45,680
Other than that, the most helpful thing you can do is to share the episode if you enjoyed it.

3960
04:29:45,680 --> 04:29:50,720
Send it to group chats, Twitter, wherever else you think people who might like this episode

3961
04:29:50,720 --> 04:29:56,000
might congregate, and other than that, I guess here's this riff on Frederick the Great. See you

3962
04:29:56,000 --> 04:30:00,640
on the next one. I mean, I think the actual funny thing is, you know, a lot of the sort of German

3963
04:30:00,640 --> 04:30:05,120
history stuff we've talked about is sort of like not actually stuff I learned in Germany,

3964
04:30:05,120 --> 04:30:07,600
it's sort of like stuff that I learned after, and there's actually, you know,

3965
04:30:07,600 --> 04:30:11,040
a funny thing where I kind of would go back to Germany over Christmas or whatever. Suddenly

3966
04:30:11,040 --> 04:30:14,160
I understand the street names, you know, it's like, you know, Gneisenau and Scharnhorst,

3967
04:30:14,160 --> 04:30:18,160
and they're all these like Prussian military reformers, and you're like finally understood,

3968
04:30:18,160 --> 04:30:21,440
you know, Sansa C, and you're like, Frederick, you know, Frederick the Great is this really

3969
04:30:21,440 --> 04:30:29,440
interesting figure, where, so he's this sort of, in some sense, kind of like gay lover of arts,

3970
04:30:30,000 --> 04:30:36,080
right, where he, you know, he hates speaking German, he only wants to speak French, you know,

3971
04:30:36,080 --> 04:30:40,000
he like plays the flute, he composes, he has all the sort of great, you know, artists of his day,

3972
04:30:40,000 --> 04:30:47,520
you know, over at Sansa C, and he actually had this sort of like really tough upbringing, where

3973
04:30:47,520 --> 04:30:55,040
his father was this sort of like really stern sort of Prussian military man, and he had had a,

3974
04:30:56,080 --> 04:30:59,840
Frederick the Great as sort of a 17-year-old or whatever, he basically had a male lover,

3975
04:31:00,400 --> 04:31:07,840
and what his father did was imprison his son, and then I think hang his male lover in front of him,

3976
04:31:08,480 --> 04:31:12,160
and again, his father was this kind of very stern Prussian guy, he was this kind of gay,

3977
04:31:12,160 --> 04:31:16,560
you know, lover of arts, but then later on Frederick the Great turns out to be this like,

3978
04:31:16,560 --> 04:31:22,880
you know, one of the most kind of like, you know, successful kind of Prussian conquerors,

3979
04:31:22,880 --> 04:31:26,640
right, like he gets Silesia, he wins the Seven Years War, you know, also, you know,

3980
04:31:26,640 --> 04:31:30,320
amazing military strategists, you know, amazing military strategy at the time consisted of like,

3981
04:31:30,320 --> 04:31:34,400
he was able to like flank the army, and that was crazy, you know, and that was brilliant,

3982
04:31:34,400 --> 04:31:38,320
and then they like almost lose the Seven Years War, and at the very end, you know, the sort of,

3983
04:31:39,120 --> 04:31:43,280
the Russian Tsar changes, and he's like, ah, I'm actually kind of a Prussian stan, you know,

3984
04:31:43,280 --> 04:31:47,360
I think I'm like, I'm into this stuff, and then he lets, you know, let's Frederick the Great lose,

3985
04:31:47,360 --> 04:31:55,200
and he had, let's their army be okay, and anyway, sort of like, yeah, kind of bizarre,

3986
04:31:55,200 --> 04:31:57,200
interesting figure in German history.

