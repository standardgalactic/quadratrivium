1
00:00:00,000 --> 00:00:03,200
So I wouldn't be surprised if we had AGI-like systems

2
00:00:03,200 --> 00:00:04,360
within the next decade.

3
00:00:04,360 --> 00:00:06,400
It was pretty surprising to almost everyone,

4
00:00:06,400 --> 00:00:08,880
including the people who first worked on the scaling

5
00:00:08,880 --> 00:00:10,880
hypotheses, that how far it's gone.

6
00:00:10,880 --> 00:00:13,400
In a way, I look at the large models today,

7
00:00:13,400 --> 00:00:14,920
and I think they're almost unreasonably

8
00:00:14,920 --> 00:00:16,240
effective for what they are.

9
00:00:16,240 --> 00:00:18,760
It's an empirical question whether that will hit an asymptote

10
00:00:18,760 --> 00:00:19,760
or a brick wall.

11
00:00:19,760 --> 00:00:21,080
I think no one knows.

12
00:00:21,080 --> 00:00:23,000
When you think about superhuman intelligence,

13
00:00:23,000 --> 00:00:25,600
is it still controlled by a private company?

14
00:00:25,600 --> 00:00:28,040
As Gemini becoming more multimodal,

15
00:00:28,040 --> 00:00:30,240
and we start ingesting audio-visual data,

16
00:00:30,240 --> 00:00:32,840
as well as text data, I do think our systems

17
00:00:32,840 --> 00:00:36,280
are going to start to understand the physics of the real world

18
00:00:36,280 --> 00:00:36,920
better.

19
00:00:36,920 --> 00:00:38,560
The world's about to become very exciting,

20
00:00:38,560 --> 00:00:40,160
I think, in the next few years as we start

21
00:00:40,160 --> 00:00:44,240
getting used to the idea of what true multimodality means.

22
00:00:44,240 --> 00:00:48,240
OK, today it is a true honor to speak with Demis Isavis, who

23
00:00:48,240 --> 00:00:50,240
is the CEO of DeepMind.

24
00:00:50,240 --> 00:00:51,640
Demis, welcome to the podcast.

25
00:00:51,640 --> 00:00:52,720
Thanks for having me.

26
00:00:52,720 --> 00:00:55,480
First question, given your neuroscience background,

27
00:00:55,480 --> 00:00:56,880
how do you think about intelligence?

28
00:00:56,920 --> 00:01:00,000
Specifically, do you think it's like one higher-level general

29
00:01:00,000 --> 00:01:01,480
reasoning circuit, or do you think

30
00:01:01,480 --> 00:01:05,560
it's thousands of independent sub-scales and heuristics?

31
00:01:05,560 --> 00:01:09,880
Well, it's interesting because intelligence is so broad,

32
00:01:09,880 --> 00:01:14,760
and what we use it for is so generally applicable.

33
00:01:14,760 --> 00:01:17,120
I think that suggests that there must

34
00:01:17,120 --> 00:01:21,840
be some sort of high-level common things,

35
00:01:21,840 --> 00:01:24,160
common algorithmic themes, I think,

36
00:01:24,160 --> 00:01:27,320
around how the brain processes the world around us.

37
00:01:27,320 --> 00:01:31,680
So of course, then there are specialized parts of the brain

38
00:01:31,680 --> 00:01:34,080
that do specific things.

39
00:01:34,080 --> 00:01:36,440
But I think there are probably some underlying principles

40
00:01:36,440 --> 00:01:37,800
that underpin all of that.

41
00:01:37,800 --> 00:01:40,680
Yeah, how do you make sense of the fact that in these LLMs,

42
00:01:40,680 --> 00:01:42,720
though, when you give them a lot of data

43
00:01:42,720 --> 00:01:44,160
in any specific domain, they tend

44
00:01:44,160 --> 00:01:47,680
to get asymmetrically better in that domain?

45
00:01:47,680 --> 00:01:50,040
Wouldn't we expect a general improvement

46
00:01:50,040 --> 00:01:51,520
across all the different areas?

47
00:01:51,520 --> 00:01:53,600
Well, I think you first of all, I think you do actually

48
00:01:53,640 --> 00:01:56,480
sometimes get surprising improvement in other domains

49
00:01:56,480 --> 00:01:58,200
when you improve in a specific domain.

50
00:01:58,200 --> 00:02:01,680
So for example, when these large models sort of improve

51
00:02:01,680 --> 00:02:05,280
at coding, that can actually improve their general reasoning.

52
00:02:05,280 --> 00:02:07,520
So there is some evidence of some transfer,

53
00:02:07,520 --> 00:02:11,600
though I think we would like a lot more evidence of that.

54
00:02:11,600 --> 00:02:14,360
But also, that's how the human brain learns, too,

55
00:02:14,360 --> 00:02:17,480
is if we experience and practice a lot of things like chess

56
00:02:17,480 --> 00:02:20,120
or writing, creative writing, or whatever that is,

57
00:02:20,120 --> 00:02:22,200
we also tend to specialize and get better

58
00:02:22,200 --> 00:02:23,440
at that specific thing.

59
00:02:23,440 --> 00:02:26,600
Even though we're using sort of general learning techniques

60
00:02:26,600 --> 00:02:28,520
and general learning systems in order

61
00:02:28,520 --> 00:02:31,080
to get good at that domain.

62
00:02:31,080 --> 00:02:33,160
Yeah, well, what's been the most surprising example

63
00:02:33,160 --> 00:02:34,960
of this kind of transfer for you?

64
00:02:34,960 --> 00:02:37,840
Like, you see language and code or images and text?

65
00:02:37,840 --> 00:02:40,080
Yeah, I think probably, I mean, I'm

66
00:02:40,080 --> 00:02:42,760
hoping we're going to see a lot more of this China transfer.

67
00:02:42,760 --> 00:02:46,600
But I think things like getting better at coding and math

68
00:02:46,600 --> 00:02:48,920
and generally improving your reasoning,

69
00:02:48,920 --> 00:02:51,520
that is how it works with us as human learners.

70
00:02:51,520 --> 00:02:53,000
But I think it's interesting seeing

71
00:02:53,000 --> 00:02:55,880
that in these artificial systems.

72
00:02:55,880 --> 00:02:59,120
And can you see the sort of mechanistic way in which,

73
00:02:59,120 --> 00:03:00,720
let's say in the language and code example,

74
00:03:00,720 --> 00:03:03,120
there's like, I found a place in a neural network that's

75
00:03:03,120 --> 00:03:04,920
getting better with both the language and the code.

76
00:03:04,920 --> 00:03:07,000
Is that too far down the weeds?

77
00:03:07,000 --> 00:03:09,840
Yeah, well, I don't think our analysis techniques

78
00:03:09,840 --> 00:03:13,480
are quite sophisticated enough to be able to hone in on that.

79
00:03:13,480 --> 00:03:15,080
I think that's actually one of the areas

80
00:03:15,080 --> 00:03:17,600
that a lot more research needs to be done

81
00:03:17,600 --> 00:03:20,640
on kind of mechanistic analysis of the representations

82
00:03:20,680 --> 00:03:21,920
that these systems build up.

83
00:03:21,920 --> 00:03:25,360
And I sometimes like to call it virtual brain analytics.

84
00:03:25,360 --> 00:03:29,360
In a way, it's a bit like doing fMRI or a single cell

85
00:03:29,360 --> 00:03:31,880
recording from a real brain.

86
00:03:31,880 --> 00:03:34,640
What's the analogous sort of analysis techniques

87
00:03:34,640 --> 00:03:36,320
for these artificial minds?

88
00:03:36,320 --> 00:03:38,560
And there's a lot of great work going on

89
00:03:38,560 --> 00:03:39,400
on this sort of stuff.

90
00:03:39,400 --> 00:03:41,840
People like Chris Ola, I really like his work.

91
00:03:41,840 --> 00:03:44,040
And a lot of computational neuroscience techniques,

92
00:03:44,040 --> 00:03:46,040
I think, could be brought to bear

93
00:03:46,040 --> 00:03:48,560
on analyzing these current systems we're building.

94
00:03:48,560 --> 00:03:50,840
In fact, I try to encourage a lot of my computational

95
00:03:50,840 --> 00:03:54,000
neuroscience friends to start thinking in that direction

96
00:03:54,000 --> 00:03:58,440
and applying their know-how to the large models.

97
00:03:58,440 --> 00:04:01,680
Yeah, what do other researchers not understand

98
00:04:01,680 --> 00:04:05,200
about human intelligence that you have some sort of insight

99
00:04:05,200 --> 00:04:06,680
on, given your neuroscience background?

100
00:04:06,680 --> 00:04:10,600
I think neuroscience has added a lot.

101
00:04:10,600 --> 00:04:12,760
If you look at the last sort of 10, 20 years

102
00:04:12,760 --> 00:04:14,480
that we've been at it, at least.

103
00:04:14,480 --> 00:04:18,080
And I've been thinking about this for 30 plus years.

104
00:04:18,080 --> 00:04:21,760
I think in the earlier days of this sort of new wave of AI,

105
00:04:21,760 --> 00:04:24,760
I think neuroscience was providing a lot of interesting

106
00:04:24,760 --> 00:04:26,160
directional clues.

107
00:04:26,160 --> 00:04:28,000
So things like reinforcement learning,

108
00:04:28,000 --> 00:04:29,640
combining that with deep learning,

109
00:04:29,640 --> 00:04:31,640
some of our pioneering work we did there,

110
00:04:31,640 --> 00:04:35,400
things like experience replay, even the notion of attention,

111
00:04:35,400 --> 00:04:37,840
which has become super important.

112
00:04:37,840 --> 00:04:41,240
A lot of those original sort of inspirations

113
00:04:41,240 --> 00:04:44,040
come from some understanding about how the brain works.

114
00:04:44,040 --> 00:04:45,200
Not the exact specifics.

115
00:04:45,200 --> 00:04:47,280
Of course, one's an engineered system.

116
00:04:47,280 --> 00:04:48,440
The other one's a natural system.

117
00:04:48,440 --> 00:04:50,840
So it's not so much about a one-to-one mapping

118
00:04:50,840 --> 00:04:52,240
of a specific algorithm.

119
00:04:52,240 --> 00:04:54,480
It's more kind of inspirational direction,

120
00:04:54,480 --> 00:04:57,240
maybe some ideas for architecture or algorithmic ideas

121
00:04:57,240 --> 00:04:59,160
or representational ideas.

122
00:04:59,160 --> 00:05:01,640
And because you know the brains in existence

123
00:05:01,640 --> 00:05:04,200
prove that general intelligence is possible at all,

124
00:05:04,200 --> 00:05:07,160
I think the history of human endeavors

125
00:05:07,160 --> 00:05:09,680
has been that once you know something's possible,

126
00:05:09,680 --> 00:05:11,760
it's easier to push hard in that direction.

127
00:05:11,760 --> 00:05:14,080
Because you know it's a question of effort then

128
00:05:14,080 --> 00:05:17,080
and sort of a question of when, not if.

129
00:05:17,080 --> 00:05:19,440
And that allows you to I think make progress

130
00:05:19,440 --> 00:05:20,520
a lot more quickly.

131
00:05:20,520 --> 00:05:23,240
So I think neurosciences has had a lot of,

132
00:05:24,680 --> 00:05:27,000
has inspired a lot of the thinking,

133
00:05:27,000 --> 00:05:30,080
at least in a soft way behind where we are today.

134
00:05:31,000 --> 00:05:33,400
But as for going forwards,

135
00:05:33,400 --> 00:05:36,960
I think that there's still a lot of interesting things

136
00:05:36,960 --> 00:05:38,760
to be resolved around planning

137
00:05:38,760 --> 00:05:42,560
and how does the brain construct the right world models.

138
00:05:43,680 --> 00:05:47,060
I studied for example, how the brain does imagination,

139
00:05:47,060 --> 00:05:50,020
or you can think of it as a mental simulation.

140
00:05:50,020 --> 00:05:54,260
So how do we create very rich visual spatial simulations

141
00:05:54,260 --> 00:05:56,100
of the world in order for us to plan better?

142
00:05:56,100 --> 00:05:57,540
Yeah, actually I'm curious how you think

143
00:05:57,540 --> 00:05:59,340
that will sort of interface with LLM.

144
00:05:59,340 --> 00:06:01,500
So obviously DeepMinders are the frontier

145
00:06:01,500 --> 00:06:03,340
and has been for many years,

146
00:06:03,340 --> 00:06:05,140
systems like AlphaZero and so forth,

147
00:06:05,140 --> 00:06:06,980
of having these agents who can like think through

148
00:06:06,980 --> 00:06:09,460
different steps to get to an end outcome.

149
00:06:09,460 --> 00:06:10,820
All right, will this just be,

150
00:06:10,820 --> 00:06:13,020
is a path for LLMs to have this sort of

151
00:06:13,020 --> 00:06:14,740
tree search kind of thing on top of them?

152
00:06:14,740 --> 00:06:15,780
How do you think about this?

153
00:06:15,780 --> 00:06:18,860
I think that's a super promising direction in my opinion.

154
00:06:18,860 --> 00:06:22,740
So we've got to carry on improving the large models

155
00:06:22,740 --> 00:06:25,820
and we've got to carry on basically making

156
00:06:25,820 --> 00:06:28,020
the more and more accurate predictors of the world.

157
00:06:28,020 --> 00:06:30,300
So in effect, making them more and more reliable

158
00:06:30,300 --> 00:06:32,580
world models, that's clearly unnecessary,

159
00:06:32,580 --> 00:06:34,740
but I would say probably not sufficient component

160
00:06:34,740 --> 00:06:36,500
of an AGI system.

161
00:06:36,500 --> 00:06:37,820
And then on top of that,

162
00:06:37,820 --> 00:06:41,140
I would, we're working on things like AlphaZero,

163
00:06:41,140 --> 00:06:44,700
like planning mechanisms on top that make use of that model

164
00:06:44,700 --> 00:06:46,580
in order to make concrete plans

165
00:06:46,580 --> 00:06:48,900
to achieve certain goals in the world

166
00:06:48,900 --> 00:06:52,780
and perhaps sort of chain thought together

167
00:06:52,780 --> 00:06:54,380
or lines of reasoning together

168
00:06:54,380 --> 00:06:56,740
and maybe use search to kind of explore

169
00:06:56,740 --> 00:06:58,380
massive spaces of possibility.

170
00:06:58,380 --> 00:06:59,860
I think that's kind of missing

171
00:06:59,860 --> 00:07:01,980
from our current large models.

172
00:07:01,980 --> 00:07:05,860
How do you get past the sort of immense amount of compute

173
00:07:05,860 --> 00:07:07,220
that these approaches tend to require?

174
00:07:07,220 --> 00:07:11,500
So even the AlphaGo system was a pretty expensive system

175
00:07:11,540 --> 00:07:14,740
because you had to do the sort of running an LLM on each node

176
00:07:14,740 --> 00:07:16,100
of the tree.

177
00:07:16,100 --> 00:07:17,660
How do you anticipate that will get more,

178
00:07:17,660 --> 00:07:18,500
made more efficient?

179
00:07:18,500 --> 00:07:22,820
Well, I mean, one thing is Moore's law tends to help

180
00:07:22,820 --> 00:07:27,620
if every year, of course, more computation comes in,

181
00:07:27,620 --> 00:07:30,060
but we focus a lot on efficient,

182
00:07:30,060 --> 00:07:34,460
sample efficient methods and reusing existing data,

183
00:07:34,460 --> 00:07:36,220
things like experience replay,

184
00:07:36,220 --> 00:07:39,140
and also just looking at more efficient ways.

185
00:07:39,140 --> 00:07:40,980
I mean, the better your world model is,

186
00:07:40,980 --> 00:07:42,660
the more efficient your search can be.

187
00:07:42,660 --> 00:07:44,620
So one example I always get with AlphaZero,

188
00:07:44,620 --> 00:07:47,620
our system to play Go and chess and any game,

189
00:07:47,620 --> 00:07:51,340
is that it's stronger than world champion level,

190
00:07:51,340 --> 00:07:53,940
human world champion level at all these games.

191
00:07:53,940 --> 00:07:58,020
And it uses a lot less search than a brute force method

192
00:07:58,020 --> 00:07:59,940
like Deep Blue, say to play chess.

193
00:07:59,940 --> 00:08:02,460
Deep Blue, one of these traditional stockfish

194
00:08:02,460 --> 00:08:05,380
or Deep Blue systems would maybe look at millions

195
00:08:05,380 --> 00:08:08,740
of possible moves for every decision it's gonna make.

196
00:08:08,740 --> 00:08:11,660
AlphaZero and AlphaGo made, you know,

197
00:08:11,660 --> 00:08:15,860
looked at around tens of thousands of possible positions

198
00:08:15,860 --> 00:08:18,260
in order to make a decision about what to move next.

199
00:08:18,260 --> 00:08:21,500
But a human grandmaster, a human world champion,

200
00:08:21,500 --> 00:08:23,980
probably only looks at a few hundreds of moves,

201
00:08:23,980 --> 00:08:25,020
even the top ones,

202
00:08:25,020 --> 00:08:27,980
in order to make their very good decision

203
00:08:27,980 --> 00:08:29,260
about what to play next.

204
00:08:29,260 --> 00:08:32,500
So that suggests that obviously the brute force systems

205
00:08:32,500 --> 00:08:35,020
don't have any real model other than heuristics

206
00:08:35,020 --> 00:08:36,100
about the game.

207
00:08:36,140 --> 00:08:39,660
AlphaZero has quite a decent model,

208
00:08:39,660 --> 00:08:41,860
but the human, you know,

209
00:08:41,860 --> 00:08:44,020
top human players have a much richer,

210
00:08:44,020 --> 00:08:46,860
much more accurate model than of Go or chess.

211
00:08:46,860 --> 00:08:48,500
So that allows them to make, you know,

212
00:08:48,500 --> 00:08:51,780
world class decisions on a very small amount of search.

213
00:08:51,780 --> 00:08:52,860
So I think there's still,

214
00:08:52,860 --> 00:08:54,180
there's a sort of trade off there,

215
00:08:54,180 --> 00:08:56,020
like, you know, if you improve the models,

216
00:08:56,020 --> 00:08:58,420
then I think your search can be more efficient

217
00:08:58,420 --> 00:09:00,180
and therefore you can get further with your search.

218
00:09:00,180 --> 00:09:02,500
Yeah, I have two questions based on that.

219
00:09:02,500 --> 00:09:04,220
The first being with AlphaZero,

220
00:09:04,220 --> 00:09:06,900
you had a very concrete win condition of, you know,

221
00:09:06,900 --> 00:09:07,740
at the end of the day,

222
00:09:07,740 --> 00:09:08,580
do I win this game or not?

223
00:09:08,580 --> 00:09:10,420
And you can reinforce on that.

224
00:09:10,420 --> 00:09:12,420
When you're just thinking of like an LLM,

225
00:09:12,420 --> 00:09:13,500
putting out thought,

226
00:09:13,500 --> 00:09:15,780
what will, do you think there'll be this kind of ability

227
00:09:15,780 --> 00:09:17,420
to discriminate in the end,

228
00:09:17,420 --> 00:09:19,980
whether that was like a good thing to reward or not?

229
00:09:19,980 --> 00:09:21,500
Well, of course that's why we, you know,

230
00:09:21,500 --> 00:09:23,740
we pioneered and DeepMind sort of famous

231
00:09:23,740 --> 00:09:26,940
for using games as a proving ground,

232
00:09:26,940 --> 00:09:28,620
partly because obviously it's efficient

233
00:09:28,620 --> 00:09:30,100
to research in that domain.

234
00:09:30,100 --> 00:09:31,980
But the other reason is obviously it's, you know,

235
00:09:31,980 --> 00:09:34,060
extremely easy to specify a reward function,

236
00:09:34,060 --> 00:09:35,740
winning the game or improving the score,

237
00:09:35,740 --> 00:09:38,100
something like that sort of built into most games.

238
00:09:38,100 --> 00:09:41,940
So that is one of the challenges of real world systems

239
00:09:41,940 --> 00:09:44,860
is how does one define the right objective function,

240
00:09:44,860 --> 00:09:47,540
the right reward function and the right goals

241
00:09:47,540 --> 00:09:51,500
and specify them in a, you know, in a general way,

242
00:09:51,500 --> 00:09:52,660
but that's specific enough

243
00:09:52,660 --> 00:09:55,660
and actually points the system in the right direction.

244
00:09:55,660 --> 00:09:58,940
And for real world problems, that can be a lot harder.

245
00:09:58,940 --> 00:10:01,260
But actually, if you think about it

246
00:10:01,260 --> 00:10:03,420
in even scientific problems,

247
00:10:03,420 --> 00:10:05,580
there are usually ways that you can specify

248
00:10:05,580 --> 00:10:07,220
the goal that you're after.

249
00:10:07,220 --> 00:10:08,660
And then when you think about human intelligence,

250
00:10:08,660 --> 00:10:09,860
you're just saying, well, you know,

251
00:10:09,860 --> 00:10:10,940
the humans thinking about these thoughts

252
00:10:10,940 --> 00:10:12,860
are just super sample efficient.

253
00:10:12,860 --> 00:10:15,140
How do you, I understand coming up with relativity, right?

254
00:10:15,140 --> 00:10:16,900
There's just like thousands of possible permutations

255
00:10:16,900 --> 00:10:17,980
of the equations.

256
00:10:17,980 --> 00:10:19,660
Do you think it's also this sort of sense

257
00:10:19,660 --> 00:10:20,980
of like different heuristics of like,

258
00:10:20,980 --> 00:10:22,340
I'm going to try out this approach instead of this

259
00:10:22,340 --> 00:10:25,300
or is it a totally different way of approaching

260
00:10:25,300 --> 00:10:27,740
coming up with a solution than, you know,

261
00:10:27,740 --> 00:10:29,340
what AlphaGo does to plan the next move?

262
00:10:29,340 --> 00:10:30,740
Yeah, well, look, I think it's different

263
00:10:30,740 --> 00:10:32,500
because our brains are not built

264
00:10:32,540 --> 00:10:35,220
for doing Monte Carlo research, right?

265
00:10:35,220 --> 00:10:39,980
It's just not the way our organic brains would work.

266
00:10:39,980 --> 00:10:42,740
So I think that in order to compensate for that,

267
00:10:42,740 --> 00:10:44,860
you know, people like Einstein have come up, you know,

268
00:10:44,860 --> 00:10:47,140
their brains have using their intuition

269
00:10:47,140 --> 00:10:49,260
and, you know, we maybe come to what intuition is,

270
00:10:49,260 --> 00:10:51,420
but they use their sort of knowledge

271
00:10:51,420 --> 00:10:54,540
and their experience to build extremely, you know,

272
00:10:54,540 --> 00:10:57,500
in Einstein's case, extremely accurate models of physics,

273
00:10:57,500 --> 00:10:59,860
including these sort of mental simulations.

274
00:10:59,860 --> 00:11:01,180
I think if you read about Einstein

275
00:11:01,180 --> 00:11:03,620
and how he came up with things, he used to visualize

276
00:11:03,620 --> 00:11:07,780
and sort of really kind of feel

277
00:11:07,780 --> 00:11:09,740
what these physical systems should be like,

278
00:11:09,740 --> 00:11:10,980
not just the mathematics of it,

279
00:11:10,980 --> 00:11:12,780
but have a really intuitive feel

280
00:11:12,780 --> 00:11:14,500
for what they would be like in reality.

281
00:11:14,500 --> 00:11:17,580
And that allowed him to think these sort of very outlandish

282
00:11:17,580 --> 00:11:19,140
thoughts at the time.

283
00:11:19,140 --> 00:11:21,740
So I think that it's the sophistication

284
00:11:21,740 --> 00:11:23,420
of the world models that we're building,

285
00:11:23,420 --> 00:11:25,940
which then, you know, if you imagine your world model

286
00:11:25,940 --> 00:11:29,260
can get you to a certain node in a tree that you're searching,

287
00:11:29,260 --> 00:11:31,300
and then you just do a little bit of search

288
00:11:31,300 --> 00:11:33,580
around that node, that leaf node,

289
00:11:33,580 --> 00:11:35,940
and that gets you to these original places.

290
00:11:35,940 --> 00:11:37,940
But obviously, if your model is,

291
00:11:37,940 --> 00:11:41,020
and your judgment on that model is very, very good,

292
00:11:41,020 --> 00:11:43,980
then you can pick which leaf nodes you should sort of expand

293
00:11:43,980 --> 00:11:45,900
with search much more accurately.

294
00:11:45,900 --> 00:11:48,140
So therefore, overall, you do a lot less search.

295
00:11:48,140 --> 00:11:49,900
I mean, there's no way that, you know,

296
00:11:49,900 --> 00:11:52,460
any human could do a kind of brute force search

297
00:11:52,460 --> 00:11:54,580
over any kind of significant space.

298
00:11:54,580 --> 00:11:55,940
Yeah, yeah, yeah.

299
00:11:55,940 --> 00:11:57,540
A big sort of open question right now

300
00:11:57,540 --> 00:12:00,620
is whether RL will allow these models to do the self-placed

301
00:12:00,620 --> 00:12:02,540
synthetic data to get over the data bottleneck.

302
00:12:02,540 --> 00:12:04,060
It sounds like you're optimistic about this.

303
00:12:04,060 --> 00:12:05,580
Yeah, I'm very optimistic about that.

304
00:12:05,580 --> 00:12:07,620
I mean, I think, well, first of all,

305
00:12:07,620 --> 00:12:09,780
there's still a lot more data, I think, that can be used,

306
00:12:09,780 --> 00:12:12,300
especially if one views like multimodal and video

307
00:12:12,300 --> 00:12:13,300
and these kind of things.

308
00:12:13,300 --> 00:12:16,700
And obviously, you know, society's adding more data

309
00:12:16,700 --> 00:12:18,580
all the time.

310
00:12:18,580 --> 00:12:21,580
But I think to the internet and things like that.

311
00:12:21,580 --> 00:12:23,980
But I think that there's a lot of scope

312
00:12:23,980 --> 00:12:26,140
for creating synthetic data.

313
00:12:26,140 --> 00:12:29,620
We're looking at different ways partly through simulation

314
00:12:29,620 --> 00:12:32,100
and using very realistic games environments,

315
00:12:32,100 --> 00:12:35,260
for example, to generate realistic data,

316
00:12:35,260 --> 00:12:37,100
but also self-play.

317
00:12:37,100 --> 00:12:41,300
So that's where systems interact with each other

318
00:12:41,300 --> 00:12:43,460
or converse with each other.

319
00:12:43,460 --> 00:12:44,780
And in the sense of, you know,

320
00:12:44,780 --> 00:12:46,620
work very well for us with AlphaGo and AlphaZero,

321
00:12:46,620 --> 00:12:49,100
where we got the systems to play against each other

322
00:12:49,100 --> 00:12:50,940
and actually learn from each other's mistakes

323
00:12:50,940 --> 00:12:52,900
and build up a knowledge base that way.

324
00:12:52,900 --> 00:12:54,820
And I think there are some good analogies for that.

325
00:12:54,820 --> 00:12:55,980
It's a little bit more complicated,

326
00:12:55,980 --> 00:12:59,820
but to build a general kind of world data.

327
00:12:59,820 --> 00:13:01,820
How do you get to the point where these models,

328
00:13:01,820 --> 00:13:03,900
the sort of synthetic data they're outputting

329
00:13:03,900 --> 00:13:05,460
and their self-play they're doing,

330
00:13:05,460 --> 00:13:07,500
is not just more of what they've already got

331
00:13:07,500 --> 00:13:08,340
in their data set,

332
00:13:08,340 --> 00:13:10,500
but is something they haven't seen before?

333
00:13:10,500 --> 00:13:12,260
You know what I mean, to actually improve the abilities.

334
00:13:12,260 --> 00:13:15,940
Yeah, so there, I think there's a whole science needed.

335
00:13:15,940 --> 00:13:17,940
And I think we're still in the nascent stage of this

336
00:13:17,940 --> 00:13:20,020
of data curation and data analysis.

337
00:13:20,020 --> 00:13:22,340
So actually analyzing the holes

338
00:13:22,340 --> 00:13:24,300
that you have in your data distribution.

339
00:13:24,300 --> 00:13:26,580
And this is important for things like fairness and bias

340
00:13:26,580 --> 00:13:28,980
and other stuff to remove that from the system is to,

341
00:13:28,980 --> 00:13:31,500
is to try and really make sure that your data set

342
00:13:31,500 --> 00:13:33,020
is representative of the distribution

343
00:13:33,020 --> 00:13:34,220
you're trying to learn.

344
00:13:34,220 --> 00:13:36,540
And, you know, there are many tricks there.

345
00:13:36,540 --> 00:13:38,020
One can use like over-weighting

346
00:13:38,020 --> 00:13:40,020
or replaying certain parts of the data.

347
00:13:40,020 --> 00:13:42,540
Or you could imagine if you identify some,

348
00:13:42,540 --> 00:13:44,020
some gap in your data set,

349
00:13:44,020 --> 00:13:46,820
that's where you put your synthetic generation capabilities

350
00:13:46,820 --> 00:13:47,660
to work on.

351
00:13:47,660 --> 00:13:50,580
Yeah, so, you know, nowadays people are paying attention

352
00:13:50,580 --> 00:13:55,420
to the RL stuff that Alfa deep-minded many years before.

353
00:13:55,420 --> 00:13:58,060
What are the sort of either early research directions

354
00:13:58,060 --> 00:13:59,860
or something that was done way back in the past,

355
00:13:59,860 --> 00:14:01,820
but people just haven't been paying attention to,

356
00:14:01,820 --> 00:14:03,100
that you think will be a big deal, right?

357
00:14:03,100 --> 00:14:04,700
Like there's a time where people weren't paying attention

358
00:14:04,700 --> 00:14:05,540
to scaling.

359
00:14:05,540 --> 00:14:07,060
What's the thing now where it's like totally underrated?

360
00:14:07,060 --> 00:14:08,500
Well, actually, I think that, you know,

361
00:14:08,500 --> 00:14:11,100
there's the history of the sort of last couple of decades

362
00:14:11,100 --> 00:14:13,300
has been things coming in and out of fashion, right?

363
00:14:13,300 --> 00:14:15,980
And I do feel like a while ago,

364
00:14:15,980 --> 00:14:17,500
when, you know, maybe five plus years ago,

365
00:14:17,500 --> 00:14:19,220
when we were pioneering with AlphaGo

366
00:14:19,220 --> 00:14:22,420
and before that, DQN, where it was the first system,

367
00:14:22,420 --> 00:14:23,780
you know, that worked on Atari,

368
00:14:23,780 --> 00:14:26,780
about how first big system really more than 10 years ago now

369
00:14:26,780 --> 00:14:29,660
that scaled up Q learning and reinforcement learning techniques

370
00:14:29,660 --> 00:14:32,140
to deal, you know, combine that with deep learning

371
00:14:32,140 --> 00:14:34,020
to create deep reinforcement learning

372
00:14:34,020 --> 00:14:37,260
and then use that to scale up to complete some,

373
00:14:37,260 --> 00:14:39,180
you know, master some pretty complex tasks

374
00:14:39,180 --> 00:14:41,380
like playing Atari games just from the pixels.

375
00:14:41,380 --> 00:14:45,500
And I do actually think a lot of those ideas

376
00:14:45,500 --> 00:14:46,940
need to come back in again.

377
00:14:46,940 --> 00:14:48,100
And as we talked about earlier,

378
00:14:48,100 --> 00:14:51,540
combine it with the new advances in large models

379
00:14:51,540 --> 00:14:52,740
and large multimodal models,

380
00:14:52,740 --> 00:14:54,060
which is obviously very exciting as well.

381
00:14:54,060 --> 00:14:56,460
So I do think there's a lot of potential

382
00:14:56,460 --> 00:14:59,460
for combining some of those older ideas together

383
00:14:59,460 --> 00:15:00,460
with the newer ones.

384
00:15:00,460 --> 00:15:03,460
Is there any potential for something to come,

385
00:15:03,460 --> 00:15:06,380
the AGI to eventually come from just a pure RRL approach?

386
00:15:06,380 --> 00:15:07,940
Like the way we're talking about it,

387
00:15:07,940 --> 00:15:09,700
it sounds like there'll be,

388
00:15:09,700 --> 00:15:12,140
the LLM will form the gripe fryer

389
00:15:12,140 --> 00:15:13,820
and then this sort of research will go on top of that.

390
00:15:13,820 --> 00:15:15,460
Or is it a possibility to just like completely

391
00:15:15,460 --> 00:15:16,300
out of the dark?

392
00:15:16,300 --> 00:15:17,220
I think I certainly, you know,

393
00:15:17,220 --> 00:15:18,940
theoretically I think there's no reason

394
00:15:18,940 --> 00:15:21,500
why you couldn't go full alpha zero like on it.

395
00:15:21,500 --> 00:15:25,340
And there are some people here at Google DeepMind

396
00:15:25,340 --> 00:15:28,140
and in the RL community who work on that, right?

397
00:15:28,140 --> 00:15:32,540
And fully assuming no priors, no data

398
00:15:32,540 --> 00:15:35,780
and just build all knowledge from scratch.

399
00:15:35,780 --> 00:15:38,620
And I think that's valuable because of course, you know,

400
00:15:38,620 --> 00:15:40,500
those ideas and those algorithms

401
00:15:40,500 --> 00:15:43,420
should also work when you have some knowledge too.

402
00:15:43,420 --> 00:15:44,260
But having said that,

403
00:15:44,260 --> 00:15:46,380
I think by far probably my betting

404
00:15:46,380 --> 00:15:48,220
would be the quickest way to get to AGI

405
00:15:48,220 --> 00:15:49,820
in the most likely plausible way

406
00:15:49,820 --> 00:15:51,660
is to use all the knowledge

407
00:15:51,660 --> 00:15:53,180
that's existing in the world right now

408
00:15:53,180 --> 00:15:55,140
on things like the web and that we've collected

409
00:15:55,140 --> 00:15:57,660
and we have these scalable algorithms

410
00:15:57,660 --> 00:16:00,740
like transformers that are capable

411
00:16:00,740 --> 00:16:03,020
of ingesting all of that information.

412
00:16:03,020 --> 00:16:06,660
And I don't see why you wouldn't start with a model

413
00:16:06,660 --> 00:16:09,820
as a kind of prior or to build on and to make predictions

414
00:16:09,820 --> 00:16:11,860
that helps bootstrap your learning.

415
00:16:11,860 --> 00:16:13,980
I just think it doesn't make sense

416
00:16:13,980 --> 00:16:15,260
not to make use of that.

417
00:16:15,300 --> 00:16:18,820
So my betting would be is that, you know,

418
00:16:18,820 --> 00:16:23,460
the final AGI system will have these large multimodals

419
00:16:23,460 --> 00:16:26,180
as part of the overall solution

420
00:16:26,180 --> 00:16:28,500
but probably won't be enough on their own.

421
00:16:28,500 --> 00:16:31,260
You will need this additional planning search on top.

422
00:16:31,260 --> 00:16:32,780
Okay, this sounds like the answer

423
00:16:32,780 --> 00:16:34,980
to the question we're about to ask which is

424
00:16:34,980 --> 00:16:37,420
what is somebody who's been in this field

425
00:16:37,420 --> 00:16:39,580
for a long time and seen different trends come and go,

426
00:16:39,580 --> 00:16:41,460
what do you think that the strong version

427
00:16:41,460 --> 00:16:42,820
of the scaling hypothesis gets right

428
00:16:42,820 --> 00:16:43,660
and what does it get wrong?

429
00:16:43,660 --> 00:16:44,580
It's just the idea that you just throw

430
00:16:44,580 --> 00:16:46,540
and have computed a wide enough distribution of data

431
00:16:46,540 --> 00:16:47,380
and you get intelligence.

432
00:16:47,380 --> 00:16:49,180
Yeah, look, my view is this is kind

433
00:16:49,180 --> 00:16:50,620
of an empirical question right now.

434
00:16:50,620 --> 00:16:52,140
So I think it was pretty surprising

435
00:16:52,140 --> 00:16:55,660
to almost everyone, including the people who first worked

436
00:16:55,660 --> 00:16:58,140
on the scaling hypotheses that how far it's gone.

437
00:16:58,140 --> 00:17:01,900
In a way, I mean, I sort of look at the large models today

438
00:17:01,900 --> 00:17:03,900
and I think they're almost unreasonably effective

439
00:17:03,900 --> 00:17:04,900
for what they are.

440
00:17:04,900 --> 00:17:07,100
You know, I think it's pretty surprising some

441
00:17:07,100 --> 00:17:09,540
of the properties that emerge, things like, you know,

442
00:17:09,540 --> 00:17:12,860
it's clearly in my opinion got some form of concepts

443
00:17:12,860 --> 00:17:15,060
and abstractions and some things like that.

444
00:17:15,060 --> 00:17:17,340
And I think if we were talking five plus years ago,

445
00:17:17,340 --> 00:17:19,540
I would have said to you, maybe we need an additional

446
00:17:19,540 --> 00:17:22,540
algorithmic breakthrough in order to do that.

447
00:17:22,540 --> 00:17:25,020
Like, you know, maybe more like the brain works.

448
00:17:25,020 --> 00:17:26,540
And I think that's still true

449
00:17:26,540 --> 00:17:29,580
if we want explicit abstract concepts, need concepts,

450
00:17:29,580 --> 00:17:32,460
but it seems that these systems can implicitly learn that.

451
00:17:32,460 --> 00:17:35,100
Another really interesting, I think an unexpected thing

452
00:17:35,100 --> 00:17:38,620
was that these systems have some sort of grounding.

453
00:17:38,620 --> 00:17:40,340
You know, even though they don't experience the world

454
00:17:40,340 --> 00:17:42,140
multimodally or at least until more recently

455
00:17:42,140 --> 00:17:43,820
we have the multimodal models.

456
00:17:43,820 --> 00:17:46,740
And that's surprising that the amount of information

457
00:17:46,740 --> 00:17:49,580
that can be, and models that can be built up

458
00:17:49,580 --> 00:17:50,620
just from language.

459
00:17:50,620 --> 00:17:53,700
And I think that I have some hypotheses about why that is.

460
00:17:53,700 --> 00:17:57,060
I think we get some grounding through the RLHF feedback systems

461
00:17:57,060 --> 00:17:59,900
because obviously the human raters are by definition

462
00:17:59,900 --> 00:18:04,380
grounded people, we're grounded, right, in reality.

463
00:18:04,380 --> 00:18:06,420
So our feedback's also grounded.

464
00:18:06,420 --> 00:18:08,620
So perhaps there's some grounding coming in through there.

465
00:18:08,620 --> 00:18:11,060
And also maybe language contains more grounding,

466
00:18:11,060 --> 00:18:13,620
you know, if you're able to ingest all of it,

467
00:18:13,620 --> 00:18:17,140
then we perhaps thought, linguists perhaps thought before.

468
00:18:17,140 --> 00:18:19,300
So that's just some very interesting philosophical questions.

469
00:18:19,300 --> 00:18:21,820
I think we haven't, people haven't even really

470
00:18:21,820 --> 00:18:23,540
scratched the surface off yet,

471
00:18:23,540 --> 00:18:26,020
looking at the advances that have been made.

472
00:18:26,900 --> 00:18:28,220
You know, it's quite interesting to think about

473
00:18:28,220 --> 00:18:29,700
where it's going to go next.

474
00:18:29,700 --> 00:18:32,660
But in terms of your question of like the large models,

475
00:18:32,660 --> 00:18:36,180
I think we've got to push scaling as hard as we can.

476
00:18:36,180 --> 00:18:37,500
And that's what we're doing here.

477
00:18:37,500 --> 00:18:39,140
And you know, it's an empirical question

478
00:18:39,140 --> 00:18:41,860
whether that will hit an asymptote or brick wall.

479
00:18:41,860 --> 00:18:44,260
And there are, you know, different people argue about that.

480
00:18:44,260 --> 00:18:45,780
But actually, I think we should just test it.

481
00:18:45,780 --> 00:18:47,420
I think no one knows.

482
00:18:47,420 --> 00:18:48,940
And but in the meantime,

483
00:18:48,940 --> 00:18:52,940
we should also double down on innovation and invention.

484
00:18:52,940 --> 00:18:55,620
And this is something that the Google research

485
00:18:55,620 --> 00:18:58,700
and DeepMind and Google Brain have, you know,

486
00:18:58,700 --> 00:19:00,900
we've pioneered many, many things over the last decade.

487
00:19:00,900 --> 00:19:02,700
That's something that's our bread and butter.

488
00:19:02,700 --> 00:19:05,340
And, you know, you can think of half our effort

489
00:19:05,340 --> 00:19:07,100
as to do with scaling and half our efforts

490
00:19:07,380 --> 00:19:09,900
to do with inventing the next architectures,

491
00:19:09,900 --> 00:19:12,100
the next algorithms that will be needed,

492
00:19:12,100 --> 00:19:15,060
knowing that you've got this scaled larger and larger model

493
00:19:15,060 --> 00:19:16,380
coming along the lines.

494
00:19:16,380 --> 00:19:19,020
So my betting right now,

495
00:19:19,020 --> 00:19:21,860
but it's a loose betting is that you would need both.

496
00:19:21,860 --> 00:19:23,300
But I think, you know,

497
00:19:23,300 --> 00:19:25,540
it's you've got to push both of them as hard as possible.

498
00:19:25,540 --> 00:19:27,300
And we're in a lucky position that we can do that.

499
00:19:27,300 --> 00:19:28,660
Yeah. I want to ask more about the grounding.

500
00:19:28,660 --> 00:19:30,820
So you can imagine two things that might change,

501
00:19:30,820 --> 00:19:32,580
which would make the grounding more difficult.

502
00:19:32,580 --> 00:19:34,780
One is that if these models gets from Arder,

503
00:19:34,780 --> 00:19:37,460
they're going to be able to operate in domains

504
00:19:37,460 --> 00:19:39,660
where we just can generate enough human labels,

505
00:19:39,660 --> 00:19:40,940
just because we're not smart enough, right?

506
00:19:40,940 --> 00:19:42,860
So if it does like a million line pull request,

507
00:19:42,860 --> 00:19:44,260
you know, how do we tell it?

508
00:19:44,260 --> 00:19:46,580
Like this is within the constraints of our morality

509
00:19:46,580 --> 00:19:48,700
and the end goal we wanted and this isn't.

510
00:19:48,700 --> 00:19:50,820
And the other is it sounds like you're saying

511
00:19:50,820 --> 00:19:53,100
more of the compute of so far we've been doing,

512
00:19:53,100 --> 00:19:54,100
you know, next token prediction

513
00:19:54,100 --> 00:19:55,500
and in some sense it's a guardrail

514
00:19:55,500 --> 00:19:57,500
because you have to talk as a human would talk

515
00:19:57,500 --> 00:19:58,940
and think as a human would think.

516
00:19:58,940 --> 00:20:01,660
Now, if additional compute is going to come

517
00:20:01,660 --> 00:20:04,140
in the form of reinforcement learning,

518
00:20:04,140 --> 00:20:06,100
where just to get to the end objective,

519
00:20:06,100 --> 00:20:08,780
we can't really trace how you got there.

520
00:20:08,780 --> 00:20:09,940
When you combine those two,

521
00:20:09,940 --> 00:20:13,180
how worried are you that the sort of grounding goes away?

522
00:20:13,180 --> 00:20:16,780
Well, look, I think if the grounding,

523
00:20:16,780 --> 00:20:18,260
you know, if it's not properly grounded,

524
00:20:18,260 --> 00:20:21,020
the system won't be able to achieve those goals properly,

525
00:20:21,020 --> 00:20:21,860
right? I think so.

526
00:20:21,860 --> 00:20:24,620
I think in a sense you sort of have to have the grounding

527
00:20:24,620 --> 00:20:26,820
or at least some of it in order for a system

528
00:20:26,820 --> 00:20:29,420
to actually achieve goals in the real world.

529
00:20:29,420 --> 00:20:31,900
I do actually think that as these systems

530
00:20:32,020 --> 00:20:34,580
and things like Gemini are becoming more multimodal

531
00:20:34,580 --> 00:20:36,900
and we start ingesting things like video

532
00:20:36,900 --> 00:20:41,500
and, you know, audio visual data as well as text data.

533
00:20:41,500 --> 00:20:43,180
And then, you know, the system starts

534
00:20:43,180 --> 00:20:44,900
correlating those things together.

535
00:20:45,780 --> 00:20:49,900
I think that is a form of proper grounding actually.

536
00:20:49,900 --> 00:20:54,380
So I do think our systems are going to start to understand,

537
00:20:54,380 --> 00:20:56,540
you know, the physics of the real world better.

538
00:20:56,540 --> 00:20:58,820
And then one could imagine the active version of that

539
00:20:58,820 --> 00:21:00,780
is being in a very realistic simulation

540
00:21:00,780 --> 00:21:03,660
or game environment where you're starting to learn

541
00:21:03,660 --> 00:21:06,060
about what your actions do in the world

542
00:21:06,060 --> 00:21:10,340
and how that affects the world itself,

543
00:21:10,340 --> 00:21:11,340
the world stay itself,

544
00:21:11,340 --> 00:21:14,060
but also what next learning episode you're getting.

545
00:21:14,060 --> 00:21:16,660
So, you know, these RL agents we've always been working on

546
00:21:16,660 --> 00:21:19,180
and pioneered like AlphaZero and AlphaGo,

547
00:21:19,180 --> 00:21:21,220
they actually affect their active learners.

548
00:21:21,220 --> 00:21:23,260
What they decide to do next affects

549
00:21:23,260 --> 00:21:25,980
what the next learning piece of data

550
00:21:25,980 --> 00:21:27,620
or experience they're going to get.

551
00:21:27,620 --> 00:21:29,380
So there's this very interesting sort of feedback loop.

552
00:21:29,380 --> 00:21:31,940
And of course, if we ever want to be good at things like robotics,

553
00:21:31,940 --> 00:21:33,300
we're going to have to understand

554
00:21:33,300 --> 00:21:35,540
how to act in the real world.

555
00:21:35,540 --> 00:21:37,300
Yeah, so there's a grounding in terms of

556
00:21:37,300 --> 00:21:39,260
will the capabilities be able to proceed?

557
00:21:39,260 --> 00:21:41,340
Will they be like enough in touch with the reality

558
00:21:41,340 --> 00:21:42,820
to be able to like do the things we want?

559
00:21:42,820 --> 00:21:45,300
And there's another sense of grounding of

560
00:21:45,300 --> 00:21:46,580
we've gotten lucky in the sense that

561
00:21:46,580 --> 00:21:47,940
since they're trained on human thought,

562
00:21:47,940 --> 00:21:49,620
they like maybe think like a human.

563
00:21:49,620 --> 00:21:51,380
To what extent does that stay true

564
00:21:51,380 --> 00:21:53,780
when more of the compute for trading comes from

565
00:21:53,780 --> 00:21:56,380
just did you get the right outcome and not guard real?

566
00:21:56,380 --> 00:21:58,540
Like, are you like proceeding on the next token

567
00:21:58,540 --> 00:21:59,380
as a human would?

568
00:21:59,380 --> 00:22:01,780
Maybe the broader question I'll like post to you is

569
00:22:01,780 --> 00:22:03,020
and this is what I asked Shane as well,

570
00:22:03,020 --> 00:22:04,580
what would it take to align a system

571
00:22:04,580 --> 00:22:05,620
that's smarter than a human?

572
00:22:05,620 --> 00:22:07,860
Maybe things in alien concepts

573
00:22:07,860 --> 00:22:09,260
and you can't like really monitor

574
00:22:09,260 --> 00:22:10,260
the million line pull request

575
00:22:10,260 --> 00:22:12,180
because you can't really understand the whole thing.

576
00:22:12,180 --> 00:22:14,500
Yeah, I mean, look, this is something Shane and I

577
00:22:14,500 --> 00:22:15,660
and many others here,

578
00:22:15,660 --> 00:22:17,100
we've had that forefront of our minds

579
00:22:17,100 --> 00:22:19,300
for since before we started DeMind

580
00:22:19,300 --> 00:22:21,860
and because we planned for success crazy,

581
00:22:21,860 --> 00:22:23,660
you know, 2010, no one was thinking about AI,

582
00:22:23,660 --> 00:22:24,980
let alone AGI,

583
00:22:24,980 --> 00:22:27,460
but we already knew that if we could make progress

584
00:22:27,460 --> 00:22:29,460
with these systems and these ideas,

585
00:22:29,460 --> 00:22:31,100
it, you know, the technology

586
00:22:31,100 --> 00:22:31,940
where there would be creator

587
00:22:31,940 --> 00:22:33,700
being unbelievably transformative.

588
00:22:33,700 --> 00:22:35,460
So we already were thinking, you know,

589
00:22:35,460 --> 00:22:37,540
20 years ago about, well, how, you know,

590
00:22:37,540 --> 00:22:39,060
what would the consequences of that be?

591
00:22:39,060 --> 00:22:40,540
Both positive and negative.

592
00:22:40,540 --> 00:22:43,100
Of course, the positive direction is amazing science,

593
00:22:43,100 --> 00:22:44,140
things like alpha fold,

594
00:22:44,140 --> 00:22:46,500
incredible breakthroughs in health and science

595
00:22:46,500 --> 00:22:50,220
and maths and discovery, scientific discovery.

596
00:22:50,220 --> 00:22:52,060
But then also we got to make sure these systems

597
00:22:52,060 --> 00:22:54,100
are sort of understandable and controllable.

598
00:22:54,100 --> 00:22:56,180
And I think there's sort of several, you know,

599
00:22:56,180 --> 00:22:58,220
this would be a whole sort of discussion in itself,

600
00:22:58,220 --> 00:23:00,980
but there are many, many ideas that people have

601
00:23:00,980 --> 00:23:03,500
from much more stringent eval systems.

602
00:23:03,500 --> 00:23:04,580
I think we don't have good enough

603
00:23:04,580 --> 00:23:07,300
at evaluations and benchmarks for things like

604
00:23:07,300 --> 00:23:09,180
can the system deceive you?

605
00:23:09,180 --> 00:23:10,540
Can it exotrate its own code?

606
00:23:10,540 --> 00:23:13,100
It was sort of undesirable behaviors.

607
00:23:13,100 --> 00:23:15,620
And then there's, you know,

608
00:23:15,620 --> 00:23:19,500
ideas of actually using AI, maybe narrow AIs.

609
00:23:19,500 --> 00:23:20,820
So not general learning ones,

610
00:23:20,820 --> 00:23:23,460
but systems that are specialized for a domain

611
00:23:23,460 --> 00:23:27,580
to help us as the human scientists analyze

612
00:23:27,580 --> 00:23:30,860
and summarize what the more general system is doing, right?

613
00:23:30,860 --> 00:23:33,540
So kind of narrow AI tools.

614
00:23:33,540 --> 00:23:35,340
I think that there's a lot of promise

615
00:23:35,340 --> 00:23:38,540
in creating hardened sandboxes or simulations

616
00:23:38,540 --> 00:23:42,540
so that are hardened with cybersecurity arrangements

617
00:23:44,020 --> 00:23:47,460
around the simulation, both to keep the AI in,

618
00:23:47,460 --> 00:23:50,940
but also as cybersecurity to keep hackers out.

619
00:23:50,940 --> 00:23:53,420
And then you could experiment a lot more

620
00:23:53,420 --> 00:23:55,500
freely within that sandbox domain.

621
00:23:55,500 --> 00:23:58,340
And I think a lot of these ideas are,

622
00:23:58,340 --> 00:23:59,820
and there's many, many others,

623
00:23:59,820 --> 00:24:01,940
including the analysis stuff we talked about earlier

624
00:24:01,940 --> 00:24:04,380
where can we analyze and understand

625
00:24:04,380 --> 00:24:06,420
what the concepts are that this system is building,

626
00:24:06,420 --> 00:24:07,780
what the representations are.

627
00:24:07,780 --> 00:24:09,700
So maybe they're not so alien to us

628
00:24:09,700 --> 00:24:11,540
and we can actually keep track

629
00:24:11,540 --> 00:24:13,820
of the kind of knowledge that it's building.

630
00:24:13,820 --> 00:24:14,820
Yeah, yeah.

631
00:24:14,820 --> 00:24:15,660
So big backup fit.

632
00:24:15,660 --> 00:24:16,820
I'm curious what your timelines are.

633
00:24:16,820 --> 00:24:19,540
So Shane said he's like, I think modal outcome is 2028.

634
00:24:19,540 --> 00:24:20,580
I think that's maybe he's median.

635
00:24:20,580 --> 00:24:21,420
Yeah.

636
00:24:21,420 --> 00:24:22,260
What is yours?

637
00:24:22,380 --> 00:24:26,060
I don't have prescribed kind of specific numbers to it

638
00:24:26,060 --> 00:24:28,860
because I think there's so many unknowns and uncertainties

639
00:24:28,860 --> 00:24:32,540
and human ingenuity and endeavor

640
00:24:32,540 --> 00:24:34,380
comes up with surprises all the time.

641
00:24:34,380 --> 00:24:37,940
So that could meaningfully move the timelines.

642
00:24:37,940 --> 00:24:41,540
But I will say that when we started DeepMind back in 2010,

643
00:24:41,540 --> 00:24:43,540
we thought of it as a 20 year project.

644
00:24:43,540 --> 00:24:45,780
And actually, I think we're on track,

645
00:24:45,780 --> 00:24:47,820
which is kind of amazing for 20 year projects.

646
00:24:47,820 --> 00:24:49,820
Because usually they're always 20 years away.

647
00:24:49,820 --> 00:24:51,700
So that's the joke about whatever it is

648
00:24:51,700 --> 00:24:54,540
that you use in quantum AI, take your pick.

649
00:24:54,540 --> 00:24:56,980
And but I think we're on track.

650
00:24:56,980 --> 00:25:00,780
So I wouldn't be surprised if we had AGI like systems

651
00:25:00,780 --> 00:25:02,220
within the next decade.

652
00:25:02,220 --> 00:25:04,860
And do you buy the model that once you have an AGI,

653
00:25:04,860 --> 00:25:07,580
you have a system that basically speeds up further AI research?

654
00:25:07,580 --> 00:25:09,100
Maybe not like an overnight sense,

655
00:25:09,100 --> 00:25:10,940
but over the course of months and years,

656
00:25:10,940 --> 00:25:11,860
you have much faster progress

657
00:25:11,860 --> 00:25:12,700
than you would have on the right side.

658
00:25:12,700 --> 00:25:15,260
I think that's potentially possible.

659
00:25:15,260 --> 00:25:18,220
I think it partly depends what we decide,

660
00:25:18,220 --> 00:25:20,740
we as society decide to use the first

661
00:25:20,740 --> 00:25:24,660
nascent AGI systems or even proto AGI systems for.

662
00:25:24,660 --> 00:25:29,580
So, even the current LLMs seem to be pretty good at coding.

663
00:25:29,580 --> 00:25:32,300
So, and we have systems like AlphaCode,

664
00:25:32,300 --> 00:25:33,980
we also got theorem proving systems.

665
00:25:33,980 --> 00:25:37,940
So one could imagine combining these ideas together

666
00:25:37,940 --> 00:25:39,900
and making them a lot better.

667
00:25:39,900 --> 00:25:43,460
And then I could imagine these systems being quite good

668
00:25:43,460 --> 00:25:47,060
at designing and helping us build future versions

669
00:25:47,060 --> 00:25:48,380
of themselves.

670
00:25:48,380 --> 00:25:50,180
But we also have to think about the safety implications

671
00:25:50,220 --> 00:25:51,100
of that, of course.

672
00:25:51,100 --> 00:25:52,060
Yeah, I'm curious what you think about that.

673
00:25:52,060 --> 00:25:54,380
So, I mean, I'm not saying this is happening this year

674
00:25:54,380 --> 00:25:56,780
or anything, but eventually you'll be developing a model

675
00:25:56,780 --> 00:25:58,260
where during the process of development,

676
00:25:58,260 --> 00:25:59,980
you think, you know, there's some chance

677
00:25:59,980 --> 00:26:01,340
that once this is fully developed,

678
00:26:01,340 --> 00:26:03,300
it'll be capable of like an intelligence explosion

679
00:26:03,300 --> 00:26:04,980
like dynamic.

680
00:26:04,980 --> 00:26:07,780
What would have to be true of that model at that point

681
00:26:07,780 --> 00:26:09,500
where you're like, you know,

682
00:26:09,500 --> 00:26:11,020
I've seen these specific evals,

683
00:26:11,020 --> 00:26:13,780
I've like, I've like understand it's internal thinking enough

684
00:26:13,780 --> 00:26:14,860
and like it's future thinking

685
00:26:14,860 --> 00:26:17,220
that I'm comfortable continuing development of the system.

686
00:26:17,220 --> 00:26:20,300
Well, look, we need a lot more understanding

687
00:26:20,300 --> 00:26:21,420
of the systems than we do today

688
00:26:21,420 --> 00:26:23,060
before I would be even confident

689
00:26:23,060 --> 00:26:26,900
of even explaining to you what we would need to tick box there.

690
00:26:26,900 --> 00:26:28,620
So I think actually what we've got to do

691
00:26:28,620 --> 00:26:30,300
in the next few years and the time we have

692
00:26:30,300 --> 00:26:33,540
before those systems start arriving is come up

693
00:26:33,540 --> 00:26:36,220
with the right evaluations and metrics

694
00:26:36,220 --> 00:26:38,580
and maybe ideally formal proofs,

695
00:26:38,580 --> 00:26:40,020
but you know, it's going to be hard

696
00:26:40,020 --> 00:26:40,940
for these types of systems,

697
00:26:40,940 --> 00:26:43,860
but at least empirical bounds

698
00:26:43,860 --> 00:26:46,100
around what these systems can do.

699
00:26:46,100 --> 00:26:49,380
And that's why I think about things like deception

700
00:26:49,380 --> 00:26:52,340
and has been quite root node traits that you don't want

701
00:26:52,340 --> 00:26:54,540
because if you're confident that your system

702
00:26:54,540 --> 00:26:58,780
is sort of exposing what it actually thinks,

703
00:26:58,780 --> 00:27:00,180
then you could potentially,

704
00:27:00,180 --> 00:27:03,020
that opens up possibilities of using the system itself

705
00:27:03,020 --> 00:27:06,020
to explain aspects of itself to you.

706
00:27:06,020 --> 00:27:08,620
The way I think about that actually is like,

707
00:27:08,620 --> 00:27:11,020
if I was to play a game of chess against Gary Kasparov,

708
00:27:11,020 --> 00:27:13,260
right, which I played in the past or Magnus Carlson,

709
00:27:13,260 --> 00:27:15,660
you know, the amazing chess players with graceful time,

710
00:27:16,100 --> 00:27:18,820
I wouldn't be able to come up with a move that they could,

711
00:27:18,820 --> 00:27:22,860
but they could explain to me why they came up

712
00:27:22,860 --> 00:27:26,740
with that move and I could understand it post hoc, right?

713
00:27:26,740 --> 00:27:29,220
And that's the sort of thing one could imagine

714
00:27:30,580 --> 00:27:34,700
one of the capabilities that we could make use

715
00:27:34,700 --> 00:27:37,540
of these systems is for them to explain it to us

716
00:27:37,540 --> 00:27:39,780
and even maybe the proofs behind why they're thinking

717
00:27:39,780 --> 00:27:41,860
something, certainly in a mathematical,

718
00:27:41,860 --> 00:27:42,980
any mathematical problem.

719
00:27:42,980 --> 00:27:43,820
Got it.

720
00:27:43,820 --> 00:27:46,220
Do you have a sense of what the converse answer would be?

721
00:27:46,220 --> 00:27:48,220
So what would have to be true where tomorrow morning,

722
00:27:48,220 --> 00:27:50,660
you're like, oh man, I didn't anticipate this.

723
00:27:50,660 --> 00:27:52,220
You see some specific observation tomorrow morning

724
00:27:52,220 --> 00:27:54,100
where like, we got to stop Gemini II training.

725
00:27:54,100 --> 00:27:55,860
Like, what would specifically...

726
00:27:55,860 --> 00:27:57,500
Yeah, I could imagine that.

727
00:27:57,500 --> 00:28:00,700
And this is where things like the sandbox simulations,

728
00:28:00,700 --> 00:28:03,020
I would hope we're experimenting

729
00:28:03,020 --> 00:28:06,060
in a safe, secure environment.

730
00:28:06,060 --> 00:28:08,700
And then something happens in it

731
00:28:08,700 --> 00:28:11,220
where very unexpected happens

732
00:28:11,220 --> 00:28:13,060
and you unexpected capability

733
00:28:13,060 --> 00:28:15,660
or something that we didn't want explicitly told the system

734
00:28:15,660 --> 00:28:18,460
we didn't want and that it did, but then lied about.

735
00:28:18,460 --> 00:28:21,540
These are the kinds of things where one would want to

736
00:28:21,540 --> 00:28:26,540
then dig in carefully with the systems that are around today

737
00:28:26,540 --> 00:28:29,140
which are not dangerous in my opinion today,

738
00:28:29,140 --> 00:28:32,460
but in a few years they might be, have potential.

739
00:28:33,420 --> 00:28:37,020
And then you would sort of ideally kind of pause

740
00:28:37,020 --> 00:28:40,580
and then really get to the bottom of why it was doing

741
00:28:40,580 --> 00:28:42,500
those things before one continued.

742
00:28:42,500 --> 00:28:44,020
Yeah, going back to Gemini,

743
00:28:44,020 --> 00:28:47,340
I'm curious what the bottlenecks were in the development.

744
00:28:47,340 --> 00:28:49,540
Like, why not make it immediately one order of magnitude

745
00:28:49,540 --> 00:28:52,460
bigger if scaling works?

746
00:28:52,460 --> 00:28:54,620
Well, look, first of all, there are practical limits.

747
00:28:54,620 --> 00:28:57,940
How much compute can you actually fit in one data center?

748
00:28:57,940 --> 00:29:01,500
And actually, you're bumping up against very interesting

749
00:29:04,380 --> 00:29:06,700
distributed computing kind of challenges, right?

750
00:29:06,700 --> 00:29:08,540
Unfortunately, we have some of the best people in the world

751
00:29:08,660 --> 00:29:11,900
on those challenges and cross data center training,

752
00:29:11,900 --> 00:29:14,260
all these kinds of things, very interesting challenges,

753
00:29:14,260 --> 00:29:16,860
hardware challenges, and we have our TPUs and so on

754
00:29:16,860 --> 00:29:18,780
that we're building and designing all the time

755
00:29:18,780 --> 00:29:20,500
as well as using GPUs.

756
00:29:20,500 --> 00:29:22,820
And so there's all of that.

757
00:29:22,820 --> 00:29:25,740
And then you also have to, the scaling laws,

758
00:29:25,740 --> 00:29:27,220
they didn't just work by magic.

759
00:29:27,220 --> 00:29:30,060
You sort of, you still need to scale up the hyperparameters

760
00:29:30,060 --> 00:29:32,380
and various innovations are going in all the time

761
00:29:32,380 --> 00:29:33,300
with each new scale.

762
00:29:33,300 --> 00:29:35,980
It's not just about repeating the same recipe.

763
00:29:35,980 --> 00:29:38,460
At each new scale, you have to adjust the recipe

764
00:29:39,180 --> 00:29:40,940
and that's a bit of an art form in a way.

765
00:29:40,940 --> 00:29:43,300
And you have to sort of almost get new data points.

766
00:29:43,300 --> 00:29:45,340
If you try and extend your predictions

767
00:29:45,340 --> 00:29:48,500
and extrapolate them, say several orders of magnitude out,

768
00:29:48,500 --> 00:29:50,460
sometimes they don't hold anymore, right?

769
00:29:50,460 --> 00:29:52,580
Because new capabilities,

770
00:29:52,580 --> 00:29:55,540
there can be step functions in terms of new capabilities

771
00:29:55,540 --> 00:29:58,300
and some things just, some things hold

772
00:29:58,300 --> 00:29:59,300
and other things don't.

773
00:29:59,300 --> 00:30:02,380
So often you do need those intermediate data points

774
00:30:02,380 --> 00:30:06,380
actually to correct some of your hyperparameter optimization

775
00:30:06,380 --> 00:30:07,220
and other things.

776
00:30:07,580 --> 00:30:09,820
That the scaling law continues to be true.

777
00:30:09,820 --> 00:30:13,700
So there's sort of various practical limitations

778
00:30:13,700 --> 00:30:15,380
on to that.

779
00:30:15,380 --> 00:30:18,700
So kind of one order of magnitude is about probably

780
00:30:18,700 --> 00:30:21,420
the maximum that you want to carry on.

781
00:30:21,420 --> 00:30:24,140
You want to sort of do between each era.

782
00:30:24,140 --> 00:30:25,700
Oh, that's so fascinating.

783
00:30:25,700 --> 00:30:26,980
In the GPT for technical report,

784
00:30:26,980 --> 00:30:29,660
they say that they were able to predict the training loss

785
00:30:31,340 --> 00:30:33,860
tens of thousands of times less compute than GPT-4

786
00:30:33,860 --> 00:30:34,860
that they could see the curve.

787
00:30:34,860 --> 00:30:36,900
But at the point you're making is that the actual capabilities

788
00:30:36,900 --> 00:30:39,100
that loss implies may not be so clear.

789
00:30:39,100 --> 00:30:41,140
Yeah, the downstream capabilities sometimes don't follow

790
00:30:41,140 --> 00:30:43,580
from the, you can often predict the core metrics

791
00:30:43,580 --> 00:30:45,660
like training loss or something like that.

792
00:30:45,660 --> 00:30:48,820
But then it doesn't actually translate into MMLU

793
00:30:48,820 --> 00:30:52,500
or math or some other actual capability

794
00:30:52,500 --> 00:30:53,340
that you care about.

795
00:30:53,340 --> 00:30:56,020
They're not necessarily linear all the time.

796
00:30:56,020 --> 00:30:57,580
So there's sort of non-linear effects there.

797
00:30:57,580 --> 00:30:58,580
What was the biggest surprise to you

798
00:30:58,580 --> 00:31:00,020
during the development of Gemini?

799
00:31:00,020 --> 00:31:02,620
So something like this happening?

800
00:31:02,620 --> 00:31:05,460
Well, I mean, I wouldn't say there was one big surprise,

801
00:31:05,460 --> 00:31:08,060
but it was very interesting trying to train things

802
00:31:08,060 --> 00:31:13,060
at that size and learning about all sorts of things

803
00:31:13,060 --> 00:31:15,460
from organizational, how to babysit such a system

804
00:31:15,460 --> 00:31:16,660
and to track it.

805
00:31:16,660 --> 00:31:20,900
And I think things like getting a better understanding

806
00:31:20,900 --> 00:31:23,180
of the metrics you're optimizing

807
00:31:23,180 --> 00:31:26,740
versus the final capabilities that you want.

808
00:31:26,740 --> 00:31:30,460
I would say that's still not a perfectly understood mapping.

809
00:31:30,460 --> 00:31:31,580
But it's an interesting one

810
00:31:31,580 --> 00:31:32,860
that we're getting better and better at.

811
00:31:32,860 --> 00:31:33,700
Yeah, yeah.

812
00:31:33,700 --> 00:31:34,900
There's a perception that maybe other labs

813
00:31:34,900 --> 00:31:38,660
are more compute efficient than DeepMind has been

814
00:31:38,660 --> 00:31:39,500
with Gemini.

815
00:31:39,500 --> 00:31:40,620
I don't know what you make of that for something.

816
00:31:40,620 --> 00:31:41,820
I don't think that's the case.

817
00:31:41,820 --> 00:31:46,660
I mean, I think that actually Gemini one

818
00:31:46,660 --> 00:31:48,220
use roughly the same amount of compute,

819
00:31:48,220 --> 00:31:50,500
maybe slightly more than what was rumored for GPT-4.

820
00:31:50,500 --> 00:31:51,980
I don't know exactly what was used.

821
00:31:51,980 --> 00:31:55,620
So I think it was in the same ballpark.

822
00:31:55,620 --> 00:31:57,220
I think we're very efficient with our compute

823
00:31:57,220 --> 00:31:59,260
and we use our compute for many things.

824
00:31:59,260 --> 00:32:00,340
One is not just the scaling,

825
00:32:00,340 --> 00:32:02,820
but going back to earlier to these more innovation

826
00:32:03,460 --> 00:32:05,540
and ideas, you've got to,

827
00:32:05,540 --> 00:32:08,300
it's only useful a new innovation, a new invention

828
00:32:08,300 --> 00:32:10,340
if it's also can scale.

829
00:32:10,340 --> 00:32:13,580
So in a way, you also need quite a lot of compute

830
00:32:13,580 --> 00:32:17,140
to do new invention because you've got to test many things

831
00:32:17,140 --> 00:32:18,980
at least some reasonable scale

832
00:32:18,980 --> 00:32:20,860
and make sure that they work at that scale.

833
00:32:20,860 --> 00:32:24,020
And also some new ideas may not work at a toy scale,

834
00:32:24,020 --> 00:32:26,060
but do work at a larger scale.

835
00:32:26,060 --> 00:32:27,740
And in fact, those are the more valuable ones.

836
00:32:27,740 --> 00:32:30,300
So you actually, if you think about that exploration process,

837
00:32:30,300 --> 00:32:33,500
you need quite a lot of compute to be able to do that.

838
00:32:33,500 --> 00:32:36,420
I mean, the good news is, is I think we,

839
00:32:36,420 --> 00:32:38,300
we're pretty lucky at Google that we,

840
00:32:38,300 --> 00:32:40,060
I think this year certainly we're going to have

841
00:32:40,060 --> 00:32:42,860
the most compute by far of any sort of research lab.

842
00:32:42,860 --> 00:32:45,420
And we hope to make very efficient and good use of that

843
00:32:45,420 --> 00:32:49,380
in terms of both scaling and the capability of our systems

844
00:32:49,380 --> 00:32:50,980
and also new inventions.

845
00:32:50,980 --> 00:32:51,820
Yeah.

846
00:32:51,820 --> 00:32:53,100
What's been the biggest surprise to you

847
00:32:53,100 --> 00:32:55,580
if you go back to yourself in 2010

848
00:32:55,580 --> 00:32:56,580
when you were starting DeepMind

849
00:32:56,580 --> 00:32:58,660
in terms of what AI progress has looked like?

850
00:32:58,660 --> 00:33:01,660
Did you anticipate back then that it would in some large sense

851
00:33:01,660 --> 00:33:03,980
amount to spend as, you know, dumping billions of dollars

852
00:33:03,980 --> 00:33:04,820
into these models?

853
00:33:04,820 --> 00:33:05,940
Or did you have a different sense of what it would look like?

854
00:33:05,940 --> 00:33:07,460
We thought that, and actually, you know,

855
00:33:07,460 --> 00:33:09,780
if you, I know you've interviewed my colleague Shane

856
00:33:09,780 --> 00:33:14,660
and he always thought that in terms of like compute curves

857
00:33:14,660 --> 00:33:17,420
and then maybe comparing roughly to like the brain

858
00:33:17,420 --> 00:33:19,860
and how many neurons and synapses there are very loosely,

859
00:33:19,860 --> 00:33:22,420
but we're actually interestingly in that kind of regime

860
00:33:22,420 --> 00:33:24,660
that roughly in the right order of magnitude of,

861
00:33:24,660 --> 00:33:26,220
you know, number of synapses in the brain

862
00:33:26,380 --> 00:33:28,780
and the sort of compute that we have.

863
00:33:28,780 --> 00:33:30,980
But I think more fundamentally, you know,

864
00:33:30,980 --> 00:33:36,340
we always thought that we bet on generality and learning, right?

865
00:33:36,340 --> 00:33:39,820
So those were always at the core of the any technique we would use.

866
00:33:39,820 --> 00:33:41,980
That's why we triangulated on reinforcement learning

867
00:33:41,980 --> 00:33:44,820
and search and deep learning, right?

868
00:33:44,820 --> 00:33:48,780
As three types of algorithms that would scale

869
00:33:48,780 --> 00:33:51,620
and would be very general

870
00:33:51,620 --> 00:33:55,020
and not require a lot of handcrafted human priors,

871
00:33:55,020 --> 00:33:57,580
which we thought was the sort of failure mode, really,

872
00:33:57,580 --> 00:34:00,940
of the efforts to build AI in the 90s, right?

873
00:34:00,940 --> 00:34:04,580
Places like MIT where there were very logic-based systems,

874
00:34:04,580 --> 00:34:07,740
expert systems, you know, masses of hand-coded,

875
00:34:07,740 --> 00:34:10,060
hand-crafted human information going into them

876
00:34:10,060 --> 00:34:12,420
that turned out to be wrong or too rigid.

877
00:34:12,420 --> 00:34:13,860
So we wanted to move away from that.

878
00:34:13,860 --> 00:34:17,300
And I think we spotted that trend early and became, you know,

879
00:34:17,300 --> 00:34:19,780
and obviously we use games as our proving ground

880
00:34:19,780 --> 00:34:21,220
and we did very well with that.

881
00:34:21,220 --> 00:34:23,220
And I think all of that was very successful

882
00:34:23,220 --> 00:34:26,900
and I think maybe inspired others to, you know, things like AlphaGo.

883
00:34:26,900 --> 00:34:29,740
I think it was a big moment for inspiring many others to think,

884
00:34:29,740 --> 00:34:32,420
oh, actually, these systems are ready to scale.

885
00:34:32,420 --> 00:34:34,540
And then, of course, with the advent of Transformers

886
00:34:34,540 --> 00:34:37,540
invented by our colleagues at Google, you know, research and brain,

887
00:34:37,540 --> 00:34:40,740
that was then, you know, the type of deep learning

888
00:34:40,740 --> 00:34:44,500
that allowed us to ingest masses of amounts of information.

889
00:34:44,500 --> 00:34:47,780
And that, of course, is really turbocharged where we are today.

890
00:34:47,780 --> 00:34:49,700
So I think that's all part of the same lineage.

891
00:34:49,700 --> 00:34:52,740
You know, we couldn't have predicted every twist and turn there,

892
00:34:52,740 --> 00:34:56,940
but I think the general direction we were going in was the right one.

893
00:34:56,940 --> 00:34:59,660
Yeah. And in fact, it's like fascinating because actually,

894
00:34:59,660 --> 00:35:02,060
if you like read your old papers or Shane's old papers,

895
00:35:02,060 --> 00:35:04,500
Shane's thesis, I think in 2009, he said, like, well, you know,

896
00:35:04,500 --> 00:35:07,020
the way we would test for AI is if it can you come press Wikipedia.

897
00:35:07,020 --> 00:35:08,780
And that's like literally the last function of our labs

898
00:35:08,780 --> 00:35:11,340
or like your own paper in like 2016 before Transformers

899
00:35:11,340 --> 00:35:14,340
where we said, like, you were comparing your science and AI.

900
00:35:14,340 --> 00:35:16,260
And he said, attention is what is needed.

901
00:35:16,260 --> 00:35:17,540
Exactly. Exactly.

902
00:35:17,540 --> 00:35:20,900
So we had these things called out and actually we had some early attention

903
00:35:20,980 --> 00:35:24,580
papers, but they weren't as elegant as Transformers in the end,

904
00:35:24,580 --> 00:35:26,380
like, Neuroturing Machines and things like this.

905
00:35:26,380 --> 00:35:29,260
Yeah. And then Transformers was the was the nicer

906
00:35:29,260 --> 00:35:30,820
and more general architecture of that.

907
00:35:30,820 --> 00:35:31,980
Yeah, yeah, yeah.

908
00:35:31,980 --> 00:35:34,140
When you extrapolate all this out forward,

909
00:35:34,140 --> 00:35:38,460
anything about superhuman intelligence or is like,

910
00:35:38,460 --> 00:35:39,740
what does that landscape look like to you?

911
00:35:39,740 --> 00:35:42,420
Is it like still controlled by a private company?

912
00:35:42,420 --> 00:35:45,340
Like, what should the governance of that look like concretely?

913
00:35:45,340 --> 00:35:49,020
Yeah, look, I would love, you know, I think that this has to be.

914
00:35:49,980 --> 00:35:52,140
This is so consequential, this technology.

915
00:35:52,140 --> 00:35:57,340
I think it's much bigger than any one company or or or even industry in general.

916
00:35:57,340 --> 00:36:01,340
I think it has to be a big collaboration with many stakeholders

917
00:36:01,500 --> 00:36:04,380
from civil society, academia, government.

918
00:36:04,540 --> 00:36:08,060
And the good news is I think with the popularity of the recent chatbot systems

919
00:36:08,060 --> 00:36:12,540
and so on, I think that has woken up many of these other parts of society

920
00:36:12,540 --> 00:36:15,900
that this is coming and what it will be like to interact with these systems.

921
00:36:16,060 --> 00:36:16,740
And that's great.

922
00:36:16,780 --> 00:36:20,100
So it's opened up lots of doors for very good conversations.

923
00:36:20,220 --> 00:36:24,620
I mean, an example of that was the safety summit in the UK hosted a few months ago,

924
00:36:24,620 --> 00:36:28,020
which I thought was a big success to start getting this international dialogue going.

925
00:36:28,260 --> 00:36:32,420
And and and, you know, I think the whole society needs to be involved in deciding

926
00:36:32,420 --> 00:36:34,620
what do we want to deploy these models for?

927
00:36:34,620 --> 00:36:35,660
How do we want to use them?

928
00:36:35,660 --> 00:36:37,140
What do we not want to use them for?

929
00:36:37,140 --> 00:36:40,380
You know, I think we've got to try and get some international consensus around that.

930
00:36:40,820 --> 00:36:44,260
And then also making sure that the benefits of these systems

931
00:36:45,140 --> 00:36:48,340
benefit everyone, you know, for the good of everyone and society in general.

932
00:36:48,500 --> 00:36:51,580
And that's why I push so hard things like AI for science.

933
00:36:51,580 --> 00:36:55,140
And and I hope that, you know, with things like our spin out isomorphic,

934
00:36:55,140 --> 00:36:58,300
we're going to start curing diseases, you know, terrible diseases with AI

935
00:36:58,300 --> 00:37:01,980
and accelerate drug discovery, amazing things, climate change and other things.

936
00:37:01,980 --> 00:37:05,180
I think big challenges that face us and face humanity.

937
00:37:05,900 --> 00:37:09,100
Massive challenges, actually, which I'm optimistic we can solve

938
00:37:09,540 --> 00:37:13,420
because we've got this incredibly powerful tool coming along down the line of AI

939
00:37:13,940 --> 00:37:17,740
that we can apply and I think help us and solve many of these problems.

940
00:37:17,740 --> 00:37:20,380
So, you know, ideally, we would have a big

941
00:37:21,420 --> 00:37:25,260
consensus around that and a big discussion, you know, sort of almost like

942
00:37:25,260 --> 00:37:27,140
the UN level, if possible.

943
00:37:27,140 --> 00:37:29,500
You know, one interesting thing is if you look at these systems,

944
00:37:29,500 --> 00:37:32,940
they you chat with them and they're immensely powerful and intelligent.

945
00:37:33,540 --> 00:37:37,020
But it's interesting to the extent of which they haven't like automated

946
00:37:37,020 --> 00:37:38,700
large sections of the economy yet.

947
00:37:38,700 --> 00:37:41,260
Whereas a five years ago, I showed you a Gemini, you'd be like, wow,

948
00:37:41,260 --> 00:37:43,500
this is like, you know, totally coming for a lot of things.

949
00:37:43,660 --> 00:37:45,100
So how do you account for that?

950
00:37:45,100 --> 00:37:48,180
Like what's going on where it hasn't had a broader impact yet?

951
00:37:48,180 --> 00:37:50,980
I think it's we're still I think that just shows we're still at the beginning

952
00:37:50,980 --> 00:37:53,380
of this new era. Yeah.

953
00:37:53,380 --> 00:37:56,180
And I think that for these systems, I think there are some interesting

954
00:37:56,180 --> 00:38:00,420
use cases, you know, you know, where you can use things to some,

955
00:38:00,420 --> 00:38:04,020
you know, these these these chatbot systems to summarize stuff for you

956
00:38:04,020 --> 00:38:09,780
and and maybe do some simple writing and maybe more kind of boilerplate type writing.

957
00:38:09,940 --> 00:38:13,700
But that's only a small part of what, you know, we all do every day.

958
00:38:13,700 --> 00:38:18,140
So I think for more general use cases, I think we need still need new

959
00:38:18,140 --> 00:38:22,460
capabilities, things like planning and search, but also maybe things like

960
00:38:22,460 --> 00:38:26,020
personalization and memory, episodic memory.

961
00:38:26,020 --> 00:38:29,340
So not just long context windows, but actually remembering what I what

962
00:38:29,340 --> 00:38:31,420
we spoke about a hundred conversations ago.

963
00:38:32,220 --> 00:38:35,820
And I think once they start coming in, I mean, I'm really looking forward

964
00:38:35,820 --> 00:38:39,540
to things like recommendation systems that that help me find better,

965
00:38:39,540 --> 00:38:43,300
more enriching material, whether that's books or films or music and so on.

966
00:38:43,420 --> 00:38:45,260
You know, I would use that type of system every day.

967
00:38:45,260 --> 00:38:49,820
So I think we're just scratching the surface of what these AI,

968
00:38:50,020 --> 00:38:54,420
say, assistants could actually do for us in our general everyday lives.

969
00:38:54,580 --> 00:38:58,420
And also in our work context as well, I think they're not reliable yet enough

970
00:38:58,420 --> 00:39:00,420
to do things like science with them.

971
00:39:00,420 --> 00:39:03,900
But I think one day, you know, once we fix factuality and grounding and other things,

972
00:39:04,460 --> 00:39:07,140
I think they could end up becoming like, you know, the world's best

973
00:39:07,140 --> 00:39:12,580
research assistant for you as a scientist or as a clinician.

974
00:39:13,540 --> 00:39:16,460
I want to ask about memory, by the way, you had this fascinating paper

975
00:39:16,460 --> 00:39:20,220
in 2007 where you talk about the links between memory and imagination

976
00:39:20,220 --> 00:39:22,060
and how they, in some sense, are very similar.

977
00:39:23,860 --> 00:39:26,340
People often claim that these models are just memorizing.

978
00:39:26,580 --> 00:39:28,860
How do you think about that claim that people make?

979
00:39:29,300 --> 00:39:30,900
Is memorization all you need?

980
00:39:30,900 --> 00:39:32,900
Because in some some deep sense, that's compression.

981
00:39:32,900 --> 00:39:34,300
Or, you know, what's your intuition?

982
00:39:34,300 --> 00:39:37,740
Yeah, I mean, sort of at the limit, one maybe could try and memorize everything,

983
00:39:37,740 --> 00:39:40,180
but it wouldn't generalize out of out of your distribution.

984
00:39:40,180 --> 00:39:43,340
And I think these systems are clearly I think the early the early

985
00:39:44,940 --> 00:39:49,020
criticisms of these early systems were that they were just regurgitating

986
00:39:49,020 --> 00:39:53,300
and memorizing, but I think clearly the new era, the Gemini GPT-4 type era,

987
00:39:53,300 --> 00:39:56,300
they are definitely generalizing to new constructs.

988
00:39:57,380 --> 00:40:00,580
So but actually, you know, in my thesis and that paper,

989
00:40:00,580 --> 00:40:04,860
particularly, that started that era of imagination in neuroscience was showing

990
00:40:04,860 --> 00:40:07,900
that, you know, first of all, memory, certainly at least human memory

991
00:40:07,900 --> 00:40:09,220
is a reconstructive process.

992
00:40:09,220 --> 00:40:10,300
It's not a videotape, right?

993
00:40:10,300 --> 00:40:13,820
We sort of put it together back from components that seems familiar to us,

994
00:40:13,820 --> 00:40:16,860
that the ensemble, and that's what made me think that imagination

995
00:40:16,860 --> 00:40:20,980
might be the same thing, except in this case, you're using the same semantic components.

996
00:40:21,140 --> 00:40:24,140
But now you're putting it together into a way that your brain thinks is novel,

997
00:40:24,260 --> 00:40:26,060
right, for a particular purpose like planning.

998
00:40:26,300 --> 00:40:31,620
And and so I do think that that kind of idea is still probably missing

999
00:40:31,620 --> 00:40:36,180
from our current systems, this sort of pulling together different parts

1000
00:40:36,180 --> 00:40:40,620
of your world model to simulate something new that then helps with your planning,

1001
00:40:40,940 --> 00:40:43,020
which is what I would call imagination.

1002
00:40:43,020 --> 00:40:43,860
Yeah, for sure.

1003
00:40:43,860 --> 00:40:46,580
So again, now you guys have the best models in the world,

1004
00:40:47,300 --> 00:40:49,780
you know, with the Gemini models.

1005
00:40:49,780 --> 00:40:53,060
Do you have do you plan on putting out some sort of framework like the other

1006
00:40:53,060 --> 00:40:56,500
two major labs have of, you know, once we see these specific capabilities,

1007
00:40:56,820 --> 00:41:00,420
unless we have these specific safeguards, we're not going to continue development

1008
00:41:00,420 --> 00:41:02,580
or we're not going to ship the product out.

1009
00:41:02,580 --> 00:41:06,060
Yes, we have actually we I mean, we have already lots of internal checks

1010
00:41:06,060 --> 00:41:09,020
and balances, but we're going to start publishing actually, you know,

1011
00:41:09,020 --> 00:41:10,100
sort of watch the spaces.

1012
00:41:10,100 --> 00:41:13,580
We're working on a whole bunch of blog posts and technical papers

1013
00:41:13,860 --> 00:41:17,420
that we'll be putting out in the next few months that, you know,

1014
00:41:17,420 --> 00:41:20,300
along the similar lines of things like responsible scaling laws and so on.

1015
00:41:20,420 --> 00:41:25,380
We have those implicitly internally in various safety councils and so on,

1016
00:41:25,380 --> 00:41:27,780
people like Shane, Chair and so on.

1017
00:41:27,780 --> 00:41:30,980
But but it's time for us to talk about that more publicly, I think.

1018
00:41:30,980 --> 00:41:33,100
So we'll be doing that throughout the course of the year.

1019
00:41:33,100 --> 00:41:34,060
That's great to hear.

1020
00:41:34,060 --> 00:41:37,500
And another thing I'm curious about is so it's not only the risk of,

1021
00:41:37,500 --> 00:41:41,700
like, you know, the deployed model being something that people can use to do bad things,

1022
00:41:41,700 --> 00:41:46,700
but also rogue actors, bad foreign agents, so forth, being able to steal the weights

1023
00:41:46,700 --> 00:41:48,260
and then fine tune them to do crazy things.

1024
00:41:48,660 --> 00:41:52,620
How do you think about securing the weights to make sure something like this

1025
00:41:52,620 --> 00:41:56,700
doesn't happen, making sure a very like key group of people have access to them

1026
00:41:56,700 --> 00:41:57,140
and so forth?

1027
00:41:57,140 --> 00:41:57,900
Yeah, it's interesting.

1028
00:41:57,900 --> 00:41:59,500
So first of all, there's sort of two parts of this.

1029
00:41:59,500 --> 00:42:02,180
One is security, one is open source, maybe we can discuss.

1030
00:42:02,180 --> 00:42:04,380
But the security, I think, is super key.

1031
00:42:04,380 --> 00:42:08,100
Like just a sort of normal cyber security type things.

1032
00:42:08,100 --> 00:42:10,140
And I think we're lucky at Google DeepMind.

1033
00:42:10,140 --> 00:42:14,260
We're kind of behind Google's firewall and cloud protection, which is, you know,

1034
00:42:14,260 --> 00:42:17,620
I think best, you know, best in class in the world, corporately.

1035
00:42:17,740 --> 00:42:19,260
So we already have that protection.

1036
00:42:19,260 --> 00:42:25,260
And then behind that, we have specific DeepMind protections within our code base.

1037
00:42:25,260 --> 00:42:27,300
So it's sort of a double layer of protection.

1038
00:42:27,300 --> 00:42:28,700
So I feel pretty good about that.

1039
00:42:28,700 --> 00:42:31,700
That that's, I mean, we, you know, you can never be complacent on that.

1040
00:42:31,700 --> 00:42:37,580
But I feel it's already sort of best in the world in terms of cyber defences.

1041
00:42:37,580 --> 00:42:39,500
But we've got to carry on improving that.

1042
00:42:39,500 --> 00:42:43,740
And again, things like the hard and sandboxes could be a way of doing that as well.

1043
00:42:43,780 --> 00:42:48,540
And maybe even there are, you know, specifically secure data centers

1044
00:42:48,540 --> 00:42:51,060
or hardware solutions to this, too, that we're thinking about.

1045
00:42:51,060 --> 00:42:55,820
I think that maybe in the next three, four, five years, we would also want air gaps

1046
00:42:55,820 --> 00:42:58,980
and various other things that are known in the security community.

1047
00:42:58,980 --> 00:42:59,780
So I think that's key.

1048
00:42:59,780 --> 00:43:03,020
And I think all frontier labs should be doing that because otherwise, you know,

1049
00:43:03,020 --> 00:43:06,780
nation states and other things, rogue nation, you know, states and other other

1050
00:43:06,780 --> 00:43:10,180
dangerous actors, that there would be obviously a lot of incentive for them

1051
00:43:10,180 --> 00:43:11,780
to to steal things like the weights.

1052
00:43:12,700 --> 00:43:15,580
And then, you know, of course, open source is another interesting question,

1053
00:43:15,580 --> 00:43:18,420
which is we're huge proponents of open source and open science.

1054
00:43:18,420 --> 00:43:21,420
I mean, almost every, you know, we've published thousands of papers

1055
00:43:21,420 --> 00:43:24,420
and things like Alpha Fold and Transformers, of course.

1056
00:43:24,420 --> 00:43:28,620
And Alpha Gold, all of these things we put out there into the world, published

1057
00:43:28,620 --> 00:43:33,220
and open source, many of them, GraphCast, most recently, our weather prediction system.

1058
00:43:33,220 --> 00:43:37,860
But when it comes to, you know, the core technology, the foundational technology

1059
00:43:37,860 --> 00:43:41,460
and very general purpose, I think the question I would have is,

1060
00:43:42,380 --> 00:43:47,020
if you, you know, first of all, open source proponents is that how does one

1061
00:43:47,020 --> 00:43:54,020
stop bad actors, individuals or, you know, up to rogue states, taking those

1062
00:43:54,020 --> 00:43:58,020
same open source systems and repurposing them because their general purpose

1063
00:43:58,020 --> 00:43:59,780
for harmful ends, right?

1064
00:43:59,780 --> 00:44:01,860
So we have to answer that question.

1065
00:44:01,860 --> 00:44:05,780
And I haven't heard a compelling, I mean, I don't know what the answer is to that,

1066
00:44:05,780 --> 00:44:10,700
but I haven't heard a compelling, clear answer to that from proponents

1067
00:44:10,900 --> 00:44:12,900
of just sort of open sourcing everything.

1068
00:44:12,900 --> 00:44:15,940
So I think there has to be some balance there, but, you know,

1069
00:44:15,940 --> 00:44:18,180
obviously it's a complex question of what that is.

1070
00:44:18,180 --> 00:44:21,100
Yeah, yeah, I feel like tech doesn't get the credit it deserves for, like,

1071
00:44:21,100 --> 00:44:23,420
funding, you know, hundreds of billions of dollars worth of R&D.

1072
00:44:24,220 --> 00:44:26,900
And, you know, obviously I have deep bind with systems like Alpha Fold and so on.

1073
00:44:27,620 --> 00:44:30,940
Well, but when we talk about securing the weights, you know, as we said,

1074
00:44:30,940 --> 00:44:33,900
like maybe right now, it's not something that, like, is going to cause the end

1075
00:44:33,900 --> 00:44:35,980
of the world or anything, but as these systems get better and better,

1076
00:44:35,980 --> 00:44:39,300
the worry that, yes, a foreign agent or something gets access to them.

1077
00:44:39,580 --> 00:44:42,020
Presumably right now, there's like dozens to hundreds of researchers

1078
00:44:42,020 --> 00:44:43,220
who have access to the weights.

1079
00:44:43,220 --> 00:44:46,020
How do you, well, what's a plan for, like, getting into, like,

1080
00:44:46,020 --> 00:44:47,780
the situation or getting the weights in the situation rooms?

1081
00:44:47,780 --> 00:44:50,580
If you're like, if you need to access to them, it's like, you know,

1082
00:44:50,580 --> 00:44:52,060
some extremely strenuous process.

1083
00:44:52,060 --> 00:44:54,340
You know, nobody, nobody individual can really take them out.

1084
00:44:54,340 --> 00:44:55,020
Yeah, yeah.

1085
00:44:55,020 --> 00:44:58,540
I mean, one has to balance that with, with, with allowing for collaboration

1086
00:44:58,540 --> 00:44:59,420
and speed of progress.

1087
00:44:59,420 --> 00:45:03,140
Actually, another interesting thing is, of course, you want, you know,

1088
00:45:03,140 --> 00:45:07,500
brilliant independent researchers from academia or, or things like the UK

1089
00:45:07,580 --> 00:45:13,860
AI Safety Institute and US1 to be able to kind of red team these systems.

1090
00:45:13,860 --> 00:45:17,420
So, so one has to expose them to a certain extent, although that's not

1091
00:45:17,420 --> 00:45:18,340
necessarily the weights.

1092
00:45:18,980 --> 00:45:22,900
And then, you know, we have a lot of processes in place about making sure

1093
00:45:22,900 --> 00:45:27,020
that, you know, only if you need them that, that you have access to, you

1094
00:45:27,020 --> 00:45:29,260
know, those people who need access, have access.

1095
00:45:29,900 --> 00:45:33,740
And right now, I think we're still in the early days of those kinds of

1096
00:45:33,740 --> 00:45:35,220
systems being at risk.

1097
00:45:35,420 --> 00:45:38,340
And as that, as these systems become more powerful and more general and

1098
00:45:38,340 --> 00:45:41,940
more capable, I think one has to look at the, the access question.

1099
00:45:42,820 --> 00:45:45,860
So some of these other labs have specialized in different things relative

1100
00:45:45,860 --> 00:45:48,460
to safety, like Anthropoc, for example, with interoperability.

1101
00:45:48,460 --> 00:45:52,900
And do you have some sense of where you guys might have an edge where as so

1102
00:45:52,900 --> 00:45:54,980
that, you know, now that you have the frontier model, you're going to

1103
00:45:54,980 --> 00:45:58,100
scale up safety, where you guys are going to be able to put out the best

1104
00:45:58,100 --> 00:45:59,180
frontier research on safety.

1105
00:45:59,180 --> 00:46:02,380
I think, you know, well, we helped pioneer RLHF and other things like that,

1106
00:46:02,380 --> 00:46:05,340
which can also be obviously used for performance, but also for safety.

1107
00:46:06,220 --> 00:46:11,100
I think that, you know, a lot of the self-play ideas and these kinds of

1108
00:46:11,100 --> 00:46:17,620
things could also be used potentially to, to auto-test a lot of the boundary

1109
00:46:17,620 --> 00:46:19,540
conditions that you have with the new systems.

1110
00:46:19,700 --> 00:46:23,420
I mean, part of the issue is that with these sort of very general systems,

1111
00:46:24,020 --> 00:46:28,140
there's so much surface area to cover, like about how these systems behave.

1112
00:46:28,340 --> 00:46:31,780
So I think we are going to need some automated testing.

1113
00:46:31,940 --> 00:46:36,540
And again, with things like simulations and games environment, very realistic

1114
00:46:36,540 --> 00:46:40,740
environments, virtual environments, I think we have a long history in that

1115
00:46:40,740 --> 00:46:46,260
and using those kinds of systems and making use of them for building AI algorithms.

1116
00:46:46,260 --> 00:46:49,060
So I think we can leverage all of that history.

1117
00:46:49,700 --> 00:46:52,380
And then, you know, around at Google, we're very lucky we have some of the

1118
00:46:52,380 --> 00:46:55,940
world's best cybersecurity experts, hardware designers.

1119
00:46:56,140 --> 00:47:00,380
So I think we can bring that to bear in, you know, for security and safety as well.

1120
00:47:00,580 --> 00:47:02,260
Great, great. Let's talk about Gemini.

1121
00:47:02,820 --> 00:47:05,060
So, you know, now you guys have the best model in the world.

1122
00:47:06,260 --> 00:47:09,900
So I'm curious, you know, the default way to interact with these systems has

1123
00:47:09,900 --> 00:47:12,060
been through chat so far.

1124
00:47:12,220 --> 00:47:14,900
Now that we have multimodal and all these new capabilities, how do you

1125
00:47:15,020 --> 00:47:15,780
anticipate that changing?

1126
00:47:15,780 --> 00:47:17,060
Or do you think that will still be the case?

1127
00:47:17,580 --> 00:47:20,420
Yeah, I think we're just at the beginning of actually understanding what a

1128
00:47:20,420 --> 00:47:25,820
full multimodal model system, how exciting that might be to interact with

1129
00:47:25,820 --> 00:47:29,540
them, and it will be quite different to, I think, what we're used to today with

1130
00:47:29,540 --> 00:47:30,260
the chat bots.

1131
00:47:30,260 --> 00:47:34,980
I think the next versions of this over the next year, 18 months, you know,

1132
00:47:35,180 --> 00:47:38,500
maybe we'll have some contextual understanding around the environment around

1133
00:47:38,500 --> 00:47:40,740
you through a camera or whatever it is, a phone.

1134
00:47:41,620 --> 00:47:44,580
You know, I could imagine that as the next awesome glasses at the next step.

1135
00:47:45,340 --> 00:47:50,060
And then I think that we'll start becoming more fluid in understanding, oh,

1136
00:47:50,260 --> 00:47:52,260
let's sample from a video.

1137
00:47:52,260 --> 00:47:53,620
Let's use voice.

1138
00:47:54,620 --> 00:47:59,460
Maybe even eventually things like touch and, you know, if you think about robotics

1139
00:47:59,460 --> 00:48:02,420
and other things, you know, sensors, other types of sensors.

1140
00:48:02,620 --> 00:48:05,580
So I think the world's about to become very exciting.

1141
00:48:05,580 --> 00:48:08,540
I think in the next few years, as we start getting used to the idea of what

1142
00:48:08,700 --> 00:48:10,300
true multimodality means.

1143
00:48:11,500 --> 00:48:15,620
On the robotic subject, Ilya said when he was on the podcast that the reason

1144
00:48:15,620 --> 00:48:18,740
opening I gave up on robotics was because they didn't have enough data in that

1145
00:48:18,740 --> 00:48:20,300
domain, at least at the time they were pursuing it.

1146
00:48:21,300 --> 00:48:24,220
I mean, you guys have put out different things like Robo Transformer and other things.

1147
00:48:24,420 --> 00:48:27,780
How do you think that's still a bottleneck for robotics progress or will we

1148
00:48:27,780 --> 00:48:30,700
see progress in the world of atoms as well as the world of bits?

1149
00:48:30,700 --> 00:48:35,060
We're very excited about our progress with things like GATO and RT2, you know,

1150
00:48:35,060 --> 00:48:40,180
Robotic Transformer, and we actually think so we've always liked robotics

1151
00:48:40,180 --> 00:48:44,420
and we've had, you know, amazing research and now we still have that going now

1152
00:48:44,620 --> 00:48:48,940
because we like the fact that it's a data poor regime because that pushes us

1153
00:48:49,140 --> 00:48:51,980
on some very interesting research directions that we think are going to be

1154
00:48:52,180 --> 00:48:56,100
useful anyway, like sampling efficiency and data efficiency in general and transfer

1155
00:48:56,300 --> 00:48:59,700
learning, learning from simulation, transferring that to reality.

1156
00:48:59,900 --> 00:49:03,860
All of these very, you know, similar to real, all of these very interesting

1157
00:49:04,060 --> 00:49:07,620
actually general challenges that we would like to solve.

1158
00:49:07,820 --> 00:49:09,220
So the control problem.

1159
00:49:09,420 --> 00:49:11,420
So we've always pushed hard on that.

1160
00:49:11,620 --> 00:49:15,580
And actually, I think so Ilya is right that that is more challenging

1161
00:49:15,580 --> 00:49:19,820
because of the data problem, but it's also I think we're starting to see the

1162
00:49:20,020 --> 00:49:25,500
beginnings of these large models being transferable to the robotics regime,

1163
00:49:25,700 --> 00:49:28,340
learning in the general domain, language domain and other things.

1164
00:49:28,540 --> 00:49:32,580
And then just treating tokens like GATO as any type of token, you know,

1165
00:49:32,580 --> 00:49:35,900
the token could be an action, it could be a word, it could be part of an image,

1166
00:49:35,900 --> 00:49:37,100
a pixel or whatever it is.

1167
00:49:37,300 --> 00:49:39,660
And that's what I think true multimodality is.

1168
00:49:39,860 --> 00:49:44,140
And to begin with, it's harder to train a system like that than a straightforward

1169
00:49:44,260 --> 00:49:46,340
text language system.

1170
00:49:46,540 --> 00:49:48,860
But actually, you know, going back to our

1171
00:49:49,060 --> 00:49:53,860
early conversation of transfer learning, you start seeing that a true multimodal

1172
00:49:54,060 --> 00:49:58,260
system, the other modalities benefit some different modality.

1173
00:49:58,260 --> 00:50:02,020
So you get better at language because you now understand a little bit about video.

1174
00:50:02,220 --> 00:50:07,340
So I do think it's harder to get going, but actually ultimately

1175
00:50:07,540 --> 00:50:10,220
we'll have a more general, more capable system like that.

1176
00:50:10,420 --> 00:50:11,460
Whatever happened to GATO?

1177
00:50:11,700 --> 00:50:14,820
That was super fascinating that you could have like play games and also do like

1178
00:50:15,020 --> 00:50:16,180
video and also do text.

1179
00:50:16,380 --> 00:50:19,780
We're still working on those kinds of systems, but you can imagine we're just

1180
00:50:19,980 --> 00:50:25,060
trying to, those ideas we're trying to build into our future generations of Gemini.

1181
00:50:25,260 --> 00:50:30,100
You know, to be able to do all of those things and robotics transformers and things like

1182
00:50:30,300 --> 00:50:33,620
that, you can think of them as sort of follow-ups to that.

1183
00:50:33,820 --> 00:50:37,180
Well, we see asymmetric progress towards the domains in which the self-play

1184
00:50:37,180 --> 00:50:39,660
kinds of things we're talking about will be especially powerful.

1185
00:50:39,660 --> 00:50:42,740
So math and code, you know, obviously recently you have these papers out about

1186
00:50:42,940 --> 00:50:47,780
this or yeah, you can use these things to do really cool novel things.

1187
00:50:47,980 --> 00:50:49,860
Will they just be like superhuman coders?

1188
00:50:49,860 --> 00:50:52,140
But like in other ways, they might be still worse than humans?

1189
00:50:52,140 --> 00:50:52,820
Or how do you think about that?

1190
00:50:53,020 --> 00:50:58,940
So look, I think that we're making great progress with math and things like

1191
00:50:59,140 --> 00:51:03,660
theorem proving and coding, but it's still interesting.

1192
00:51:03,860 --> 00:51:08,540
If one looks at, I mean, creativity in general and scientific endeavor in general,

1193
00:51:08,660 --> 00:51:12,260
I think we're getting to the stage where our systems could help the best human

1194
00:51:12,460 --> 00:51:16,260
scientists make their breakthroughs quicker, like almost triage the search space

1195
00:51:16,460 --> 00:51:21,380
in some ways or perhaps find a solution like AlphaFold does with a protein structure.

1196
00:51:21,580 --> 00:51:25,660
But it can't, they're not at the level where they can create the hypothesis

1197
00:51:25,860 --> 00:51:28,060
themselves or ask the right question.

1198
00:51:28,260 --> 00:51:32,380
And as any top scientists will tell you, that that's the hardest part of science

1199
00:51:32,580 --> 00:51:36,380
is actually asking the right question, boiling down that space to like, what's

1200
00:51:36,420 --> 00:51:39,620
the critical question we should go after the critical problem and then

1201
00:51:39,820 --> 00:51:42,100
formulating that problem in the right way to attack it.

1202
00:51:42,300 --> 00:51:46,740
And that's not something our systems will we have really have any idea how our

1203
00:51:46,940 --> 00:51:49,620
systems could do, but they can.

1204
00:51:49,820 --> 00:51:54,900
They are suitable for searching large combinatorial spaces if one can specify

1205
00:51:55,100 --> 00:51:57,580
the problem in that way with a clear objective function.

1206
00:51:57,780 --> 00:52:01,500
So that's very useful for already many of the problems we deal with today,

1207
00:52:01,700 --> 00:52:05,140
but not the most high level creative problems.

1208
00:52:05,300 --> 00:52:09,260
Right, so deep mind obviously has published all kinds of interesting stuff

1209
00:52:09,460 --> 00:52:12,020
and, you know, speeding up science in different areas.

1210
00:52:12,220 --> 00:52:15,460
How do you think about that in the context of if you think AGI is going to happen

1211
00:52:15,660 --> 00:52:19,300
in the next 10, 20 years, why not just wait for the AGI to do it for you?

1212
00:52:19,500 --> 00:52:21,500
Why build these domain specific solutions?

1213
00:52:21,700 --> 00:52:23,580
Well, I think

1214
00:52:23,780 --> 00:52:25,740
we don't know how long AGI is going to be.

1215
00:52:25,940 --> 00:52:31,660
And we always used to say, you know, back even when we started DeepMind that

1216
00:52:31,900 --> 00:52:37,300
we don't have to wait for AGI in order to bring incredible benefits to the world.

1217
00:52:37,500 --> 00:52:44,140
And especially, you know, my personal passion has been AI for science and health.

1218
00:52:44,340 --> 00:52:47,940
And you can see that with things like AlphaFold and all of our various

1219
00:52:47,940 --> 00:52:50,940
nature papers of different domains and material science work and so on.

1220
00:52:50,940 --> 00:52:54,460
I think there's lots of exciting directions and also impact in the world

1221
00:52:54,460 --> 00:52:55,300
through products, too.

1222
00:52:55,500 --> 00:52:59,100
I think it's very exciting and a huge opportunity, a unique opportunity we have

1223
00:52:59,140 --> 00:53:05,980
as part of Google, of, you know, they got dozens of billion user products, right?

1224
00:53:06,180 --> 00:53:10,220
That we can immediately ship our advances into and then

1225
00:53:10,420 --> 00:53:14,740
billions of people can, you know, improve their daily lives, right?

1226
00:53:14,740 --> 00:53:17,340
And enriches their daily lives and enhances their daily lives.

1227
00:53:17,540 --> 00:53:21,500
So I think it's a fantastic opportunity for impact on all those fronts.

1228
00:53:21,700 --> 00:53:26,020
And I think the other reason from a point of view of AGI specifically is

1229
00:53:26,220 --> 00:53:29,260
that it battle tests your ideas, right?

1230
00:53:29,460 --> 00:53:33,180
So you don't want to be in a sort of research bunker where you just,

1231
00:53:33,380 --> 00:53:35,900
you know, theoretically are pushing things, some things forward.

1232
00:53:36,100 --> 00:53:40,620
But then actually your internal metrics start deviating from

1233
00:53:40,820 --> 00:53:43,420
real world things that people would care about, right?

1234
00:53:43,620 --> 00:53:44,820
Or real world impact.

1235
00:53:45,020 --> 00:53:48,300
So you get a lot of feedback, direct feedback from these real world

1236
00:53:48,500 --> 00:53:52,860
applications that then tells you whether your systems really are scaling or or

1237
00:53:52,900 --> 00:53:56,900
actually is, you know, do we need to be more data efficient or sample efficient

1238
00:53:57,100 --> 00:54:00,780
because most real world challenges require that, right?

1239
00:54:00,980 --> 00:54:05,460
And so it kind of keeps you honest and pushes you, you know, keep sort of

1240
00:54:05,660 --> 00:54:09,660
nudging and steering your research directions to make sure they're on the right path.

1241
00:54:09,660 --> 00:54:11,060
So I think it's fantastic.

1242
00:54:11,060 --> 00:54:15,060
And of course, the world benefits from that society benefits from that on the way.

1243
00:54:15,260 --> 00:54:18,140
Many, many, maybe many, many years before AGI arrives.

1244
00:54:18,340 --> 00:54:22,100
Yeah. Well, the development of Gemini is super interesting because it comes right

1245
00:54:22,140 --> 00:54:26,380
at the heels of merging these different organizations, Brain and DeepMind.

1246
00:54:26,580 --> 00:54:28,540
Yeah, I'm curious, what have been the challenges there?

1247
00:54:28,540 --> 00:54:30,180
What have been the synergies?

1248
00:54:30,380 --> 00:54:32,740
And it's been successful in the sense that you have the best model in the world now.

1249
00:54:32,940 --> 00:54:36,180
Well, look, it's been fantastic actually over the last year.

1250
00:54:36,180 --> 00:54:40,340
Of course, it's been challenging to do that, like any big integration coming together.

1251
00:54:40,540 --> 00:54:44,500
But you're talking about two, you know, world-class organizations,

1252
00:54:44,700 --> 00:54:48,740
long storied histories of inventing many, many important things, you know,

1253
00:54:48,740 --> 00:54:50,620
from deep reinforcement learning to transformers.

1254
00:54:50,740 --> 00:54:54,180
And so it's very exciting, actually, pooling all of that together and

1255
00:54:54,380 --> 00:54:55,980
and collaborating much more closely.

1256
00:54:56,180 --> 00:55:00,060
We always used to be collaborating, but more on a on a on a, you know,

1257
00:55:00,260 --> 00:55:05,180
sort of project by project basis versus a much deeper, broader collaboration

1258
00:55:05,380 --> 00:55:10,100
like we have now in Gemini is the first fruit of of that collaboration,

1259
00:55:10,300 --> 00:55:13,060
including the name Gemini actually, you know, implying twins.

1260
00:55:13,260 --> 00:55:16,220
And and of course, a lot of other things are made more efficient,

1261
00:55:16,420 --> 00:55:19,780
like pooling compute resources together and ideas and engineering,

1262
00:55:20,020 --> 00:55:24,500
which I think at the stage we're at now, where there's huge amounts of world-class

1263
00:55:24,500 --> 00:55:27,620
engineering that has to go on to build the frontier systems.

1264
00:55:27,820 --> 00:55:30,780
I think it makes sense to to coordinate that more closely.

1265
00:55:30,780 --> 00:55:34,740
Yeah. So I mean, you and Shane started DeepMind partly because you were concerned

1266
00:55:34,940 --> 00:55:38,540
about safety and you saw AGI coming as like a live possibility.

1267
00:55:38,740 --> 00:55:42,820
Do you do you think the people who were formerly part of brain, the half of Google

1268
00:55:42,820 --> 00:55:45,220
DeepMind now, do they do you think they approach it in the same way?

1269
00:55:45,220 --> 00:55:47,260
Have there been cultural differences there in terms of that question?

1270
00:55:47,300 --> 00:55:48,620
Yeah, no, I think overrun.

1271
00:55:48,660 --> 00:55:52,100
And this is why, you know, I think one of the reasons we joined forces with Google

1272
00:55:52,300 --> 00:55:56,860
back in 2014 was I think the entirety of Google and Alphabet, not just brain

1273
00:55:56,860 --> 00:56:00,340
and DeepMind, take these questions very seriously of responsibility.

1274
00:56:00,340 --> 00:56:04,500
And, you know, I kind of mantra is to try and be bold and responsible with these systems.

1275
00:56:04,700 --> 00:56:08,500
So, you know, I would I would classify as I'm obviously a huge techno optimist,

1276
00:56:08,700 --> 00:56:12,860
but I want us to be cautious with that, given the transformative power of what

1277
00:56:13,060 --> 00:56:15,860
we're bringing, bringing into the world, you know, collectively.

1278
00:56:15,980 --> 00:56:19,940
And I think it's important, you know, I think it's going to be one of the most

1279
00:56:20,140 --> 00:56:22,380
important technologies humanity will ever invent.

1280
00:56:22,580 --> 00:56:26,300
So we've got to put, you know, all our efforts into getting this right and be

1281
00:56:26,500 --> 00:56:31,460
thoughtful and sort of also humble about what we know and don't know about

1282
00:56:31,660 --> 00:56:33,980
the systems that are coming and the uncertainties around that.

1283
00:56:34,180 --> 00:56:37,780
And in my view, the only the only sensible approach when you have huge

1284
00:56:37,980 --> 00:56:42,220
uncertainty is to be sort of cautiously optimistic and use the scientific method

1285
00:56:42,260 --> 00:56:45,780
to try and have as much foresight and understanding about what's coming down

1286
00:56:45,780 --> 00:56:48,300
the line and the consequences of that before it happens.

1287
00:56:48,500 --> 00:56:51,780
You know, you don't want to be live A, B testing out in the world with these

1288
00:56:51,980 --> 00:56:56,380
very consequential systems, because unintended consequences may be maybe quite severe.

1289
00:56:56,580 --> 00:57:01,820
So, you know, I want us to move away as a as a field from a sort of move fast

1290
00:57:01,820 --> 00:57:05,060
and break things attitude, which is, you know, maybe serve the valley very well

1291
00:57:05,260 --> 00:57:09,460
in the past and obviously created important innovations.

1292
00:57:09,620 --> 00:57:14,340
But but I think in this case, you know, we want to be bold with the with the

1293
00:57:14,340 --> 00:57:18,020
positive things that it can do and make sure we realize things like medicine

1294
00:57:18,220 --> 00:57:22,500
and science and advancing all of those things whilst being, you know,

1295
00:57:22,700 --> 00:57:27,820
responsible and thoughtful with with as far as possible with with mitigating the risks.

1296
00:57:28,020 --> 00:57:30,660
Yeah, yeah. And that's why it seems like the responsible

1297
00:57:30,660 --> 00:57:33,740
scaling process or something like that is a very good empirical way to

1298
00:57:33,740 --> 00:57:34,980
pre-commit to these kinds of things.

1299
00:57:34,980 --> 00:57:35,860
Yes, exactly.

1300
00:57:35,860 --> 00:57:38,340
Yeah. And I'm curious if you have a sense of like, for example, when you're

1301
00:57:38,420 --> 00:57:42,300
doing these evaluations, if it turns out your next model could help a layperson

1302
00:57:42,300 --> 00:57:46,180
build a pandemic class or bio-weapon or something, how you would think, first of

1303
00:57:46,380 --> 00:57:50,140
all, of making sure those weights are secure so that that doesn't get out?

1304
00:57:50,140 --> 00:57:53,540
And second, what would have to be true for you to be comfortable deploying

1305
00:57:53,540 --> 00:57:54,660
that system? How comfortable?

1306
00:57:54,660 --> 00:57:58,340
Like, how would you make sure that that that lane capability isn't exposed?

1307
00:57:58,340 --> 00:58:01,940
Yeah. Well, first, I mean, you know, the secure model part, I think we've covered

1308
00:58:01,940 --> 00:58:05,340
with the cybersecurity and make sure that's well class and you're monitoring

1309
00:58:05,340 --> 00:58:10,300
all those things. I think if the capability was discovered like that

1310
00:58:10,300 --> 00:58:15,580
through red teaming or external testing by, you know, government institutes

1311
00:58:15,780 --> 00:58:20,060
or academia or whatever, independent testers, then we would have to fix

1312
00:58:20,060 --> 00:58:22,740
that loophole depending what it was, right?

1313
00:58:23,220 --> 00:58:29,740
If that required more a different kind of perhaps constitution or different

1314
00:58:29,740 --> 00:58:33,860
guardrails or more RLHF to avoid that or removing some training data,

1315
00:58:34,420 --> 00:58:36,700
they could, I mean, depending on what the problem is, I think there could be a

1316
00:58:36,700 --> 00:58:38,660
number of mitigations.

1317
00:58:38,820 --> 00:58:42,780
And so the first part is making sure you detect it ahead of time.

1318
00:58:42,860 --> 00:58:46,500
So that's about the right evaluations and right benchmarking and right and

1319
00:58:46,500 --> 00:58:50,900
right testing. And then the question is how one would fix that before, you know,

1320
00:58:50,900 --> 00:58:54,180
you deployed it. But I think it would need to be fixed before it was deployed

1321
00:58:54,180 --> 00:58:57,180
generally, for sure, if that was an exposure surface.

1322
00:58:57,180 --> 00:58:59,180
Right. Right. Final question.

1323
00:59:00,380 --> 00:59:03,660
You know, you've been thinking in terms of like the end goal of Asia at a time

1324
00:59:03,660 --> 00:59:06,460
when other people thought it was ridiculous in 2010, now that we're

1325
00:59:06,460 --> 00:59:10,300
seeing this like slow takeoff where we're actually seeing these like generalization

1326
00:59:10,300 --> 00:59:14,300
and intelligence, what is like the psychologically seeing this?

1327
00:59:14,300 --> 00:59:15,300
What has that been like?

1328
00:59:15,300 --> 00:59:17,260
Has it just like sort of priced into your role model?

1329
00:59:17,260 --> 00:59:20,300
So you like it's not new news for you or is it like actually just seeing it live?

1330
00:59:20,300 --> 00:59:23,820
You're like, wow, like this is something's like really changed or what does it feel

1331
00:59:23,820 --> 00:59:28,220
like? Yeah, well, for me, yes, it's already priced into my world, one of how

1332
00:59:28,220 --> 00:59:30,420
things were going to go, at least from the technology side.

1333
00:59:30,420 --> 00:59:35,100
But obviously, I didn't we didn't necessarily anticipate the general

1334
00:59:35,100 --> 00:59:39,580
public would be that interested this early in the sequence, right, of things

1335
00:59:39,580 --> 00:59:44,460
like maybe one could think of if we were to produce more, if say like a chat

1336
00:59:44,460 --> 00:59:48,740
GPT and chatbots hadn't got the kind of got the interest they'd ended up getting.

1337
00:59:48,860 --> 00:59:51,500
So I think it was quite surprising to everyone that people were ready to use

1338
00:59:51,500 --> 00:59:55,220
these things, even though they were lacking in certain directions, right?

1339
00:59:55,220 --> 00:59:59,220
Impressive, though they are, then we would have produced more specialized

1340
00:59:59,220 --> 01:00:03,460
systems, I think, built off of the main track, like Alpha Folds and Alpha goes

1341
01:00:03,460 --> 01:00:05,940
and and so on and our scientific work.

1342
01:00:06,140 --> 01:00:12,540
And then I think the general public maybe would have only paid attention

1343
01:00:12,540 --> 01:00:15,620
later down the road, where in a few years time, we have more generally

1344
01:00:15,620 --> 01:00:17,500
useful assistant type systems.

1345
01:00:17,780 --> 01:00:19,060
So that's been interesting.

1346
01:00:19,060 --> 01:00:22,380
So that's created a different type of environment that we're now all

1347
01:00:22,380 --> 01:00:25,260
operating in as a field.

1348
01:00:25,420 --> 01:00:29,100
So I mean, it's a little bit more chaotic because there's so many more things

1349
01:00:29,100 --> 01:00:33,300
going on and there's so much VC money going into it and everyone sort of

1350
01:00:33,460 --> 01:00:37,740
almost losing their minds over it, I think, and what I just the thing I

1351
01:00:37,740 --> 01:00:41,860
worry about is I want to make sure that as a field, we act responsibly

1352
01:00:41,860 --> 01:00:45,660
and thoughtfully and scientifically about this and use the scientific

1353
01:00:45,660 --> 01:00:50,540
method to approach this in a in a, as I said, an optimistic, but careful way.

1354
01:00:50,660 --> 01:00:54,340
And I think that's the I've always believed that's the right approach for

1355
01:00:54,380 --> 01:00:59,300
something like AI, and I just hope that doesn't get lost in this huge rush.

1356
01:00:59,380 --> 01:01:00,060
Sure, sure.

1357
01:01:00,220 --> 01:01:01,580
Well, I think that's a great place to close.

1358
01:01:01,620 --> 01:01:02,540
Dennis, so much thanks to you.

1359
01:01:02,540 --> 01:01:04,220
Thank you so much for your time and for coming on the podcast.

1360
01:01:04,220 --> 01:01:04,500
Thanks.

1361
01:01:04,500 --> 01:01:05,260
It's been a real pleasure.

1362
01:01:07,420 --> 01:01:09,820
Hey, everybody, I hope we enjoyed that episode.

1363
01:01:10,380 --> 01:01:14,060
As always, the most helpful thing you can do is to share the podcast,

1364
01:01:14,380 --> 01:01:17,740
send it to people you think might enjoy it, put it in Twitter, your group chats,

1365
01:01:17,740 --> 01:01:21,300
et cetera, just splits the world, appreciate your listening.

1366
01:01:21,340 --> 01:01:22,300
I'll see you next time.

1367
01:01:22,460 --> 01:01:22,900
Cheers.

