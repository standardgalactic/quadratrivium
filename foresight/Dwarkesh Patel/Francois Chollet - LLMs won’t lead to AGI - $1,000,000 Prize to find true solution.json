{"text": " LLMs are very good at memorizing static programs. If you scale up the size of your database, you are not increasing the intelligence of the system one bit. I feel like you're using words like memorization, which we would never use for human children. If they can just solve any arbitrary algebraic problem, you wouldn't say they've memorized algebra. They'd say they've learned algebra. So you've got a million dollar price pool, and there's a $500,000 price for the first team that can get to the 85% benchmark. If ARC survives three months from here, we'll pull up the price. Open AI basically set back progress towards HGI by probably like five to 10 years. They caused this complete closing down of frontier research publishing. And now LLMs have sucked the oxygen out of the room, like everyone is just doing LLMs. OK, today I have the pleasure to speak with Francois Chollet, who is a AI researcher at Google and creator of Keras. And he's launching a prize in collaboration with Mike Canouf, the co-founder of Xavier, who we'll also be talking to in a second, a million dollar prize to solve the ARC benchmark that he created. So first question, what is the ARC benchmark, and why do we even need this prize? Why won't the biggest LLM we have in a year be able to just saturate it? Sure. So ARC is intended as a kind of IQ test for machine intelligence. And what makes it different from most LLM benchmarks out there is that it's designed to be resistant to memorization. So if you look at the way LLMs work, they're basically this big interpolative memory. And the way you scale up their capabilities is by trying to cram as much knowledge and patterns as possible into them. And by contrast, ARC does not require a lot of knowledge at all. It's designed to only require what's known as core knowledge, which is basic knowledge about things like elementary physics, objectness, counting, that sort of thing, the sort of knowledge that any four-year-old or five-year-old possesses. But what's interesting is that each puzzle in ARC is novel, is something that you've probably not encountered before, even if you've memorized the entire internet. And that's what makes ARC challenging for LLMs. And so far, LLMs have not been doing very well on it. In fact, the approaches that are working well are more towards discrete program search, program synthesis. So first of all, I'll make a comment that I'm glad that as a skeptic of LLM, you have put out yourself a benchmark that is it accurate to say that, suppose that the biggest model we have in a year is able to get 80% on this, then your view would be we are on track to AGI with LLMs. How would you think about that? Right. I'm pretty skeptical that we're going to see LLM do 80% in a year. That said, if we do see it, you would also have to look at how this was achieved. If you just train the model and millions or billions of puzzles similar to ARC so that you're relying on the ability to have some overlap between the tasks that you train on and the tasks that you're going to see at this time, then you're still using memorization. And maybe it can work. Hopefully, ARC is going to be good enough that it's going to be resistant to this sort of attempt at brute forcing. But you never know. Maybe it could happen. I'm not saying it's not going to happen. ARC is not a perfect benchmark. Maybe it has flaws. Maybe it could be hacked in that way. So I guess I'm curious about what would GPTI have to do that you're very confident that it's on the path to AGI? What would make me change my mind about LLMs is basically, if I start seeing a critical mass of cases where you show the model with something it has not seen before, a task that's actually novel from the perspective of its training data, something that's not in the training data, and if it can actually adapt on the fly. And this is true for LLMs. But really, this would catch my attention with any for any AI technique out there. If I can see the ability to adapt to novelty on the fly to pick up new skills efficiently, then I would be extremely interested. I would think this is on the path to AGI. So the advantage they have is that they do get to see everything. Maybe I'll take issue with how much they are relying on that. But let's suppose that they are relying. Obviously, they're relying on that more than humans do. To the extent that they do have so much indistribution, to the extent that we have trouble distinguishing whether an example is indistribution or not, well, if they have everything in distribution, then they can do everything that we can do. Maybe it's not indistribution for us. Why is it so crucial that it has to be out of distribution for them? Why can't we just leverage the fact that they do get to see everything? Right. You're asking basically what's the difference between actual intelligence, which is the ability to adapt to things you've not been prepared for, and pure memorization, like reciting what you've seen before. And it's not just some semantic difference. The big difference is that you can never pre-train on everything that you might see at test time, because the world changes all the time. So it's not just the fact that the space of possible tasks is infinite. And even if you're trained on millions of them, you've only seen zero person out of the total space. It's also the fact that the world is changing every day. This is why we, the human species, developed intelligence in the first place. If there was a shifting as a distribution for the world, for the universe, for our lives, then we would not need intelligence at all. In fact, many creatures, many insects, for instance, do not have intelligence. Instead, what they have is they have in their connectome, in their genes, hard-coded programs, behavioral programs that map some stimuli to appropriate response. And they can actually navigate their lives to environments in a way that's very evolutionary fit. That way, without needing to learn anything. And while if our environment was static enough, predictable enough, what would have happened is that evolution would have found the perfect behavioral program, a hard-coded, static behavioral program. We'd have written it into our genes. We would have a hard-coded brain connectome. And that's what we were running on. But no, that's not what happened. Instead, we have general intelligence. We are born with extremely little knowledge about the world. But we are born with the ability to learn very efficiently and to adapt in the face of things that we've never seen before. And that's what makes us unique. And that's what is really, really challenging to recreate in machines. I want to wrap it all in that a little bit. But before I do that, maybe I'm going to overlay some examples of what an arc-like challenge looks like for the YouTube audience. But maybe for people listening on audio, can you just describe what an example arc challenge will look like? Sure. So one arc puzzle, it looks kind of like an IQ test puzzle. You've got a number of demonstration input-adput pairs. So one pair is made of two grids. So one grid shows you an input. And the second grid shows you what you should produce as a response to that input. And you get a couple pairs like this to demonstrate the nature of the task, to demonstrate what you're supposed to do with your inputs. And then you get a new test input. And your job is to produce the corresponding test outputs. You look at the demonstration pairs. And from that, you figure out what you're supposed to do. And you show that you've understood it on this new test pair. And importantly, in order to the knowledge basis that you need, in order to approach these challenges, is you just need core knowledge. And core knowledge is basically the knowledge of what makes an object, basic counting, basic geometry, topology, symmetries, that sort of thing. So extremely basic knowledge. LLMs for sure possess such knowledge. Any child possesses such knowledge. And what's really interesting is that each puzzle is new. So it's not something that you're going to find elsewhere on the internet, for instance. And that means that whether it's as a human or as a machine, every puzzle you have to approach it from scratch. You have to actually reason your way through it. You cannot just fetch the response from your memory. So the core knowledge, one contention here is we are only now getting multimodal models who, because of the data that are trained on, are trained to do spatial reasoning. Whereas, obviously, not only humans, but for billions of years of revolution, we've had our ancestors have had to learn how to understand abstract, physical, and spatial properties and recognize the patterns there. And so one view would be, in the next year, as we gain models that are multimodal native, that isn't just a second class that is an add-on, but the multimodal capability is a priority. That it will understand these kinds of patterns because that's something we see natively. Whereas, right now, what Arc sees is some JSON string of 100100, and it's supposed to recognize a pattern there. And even if you showed a sequence of these kinds of numbers, it would have a challenge making sense of what kind of question you're asking it. So why want it to be the case that, as soon as we get multimodal models, which we're on the path to unlock right now, they're going to be so much better at Arc-type spatial reasoning? That's an incredibly cool question, so I guess we're going to see the answer within a few months. But my answer to that is Arc grids, they're just discrete 2D grids of symbols. They're pretty small, like it's not like... If you flatten an image as a sequence of pixels, for instance, then you get something that's actually very, very difficult to parse. But that's not true for Arc because the grids are very small. You only have 10 possible symbols. So there's these 2D grids that are actually very easy to flatten as sequences. And transformers, LLMs, they're very good at processing the sequences. In fact, you can show that LLMs do fine with processing Arc-like data by simply fine-tuning LLMs on some subsets of the tasks and then trying to test it on small variations of these tasks. And you see that, yeah, the LLMs can encode just fine solution programs for tasks that they've seen before. So it does not really have a problem parsing the input or figuring out the program. The reason why LLMs don't do well on Arc is really just the unfamiliarity aspect. The fact that each new task is different from every other task. You cannot, basically, you cannot memorize the solution programs in advance. You have to synthesize a new solution program on the fly for each new task. And that's really what LLMs are struggling with. So before I do more devil's advocate, I just want to step back and explain why I'm especially interested in having this conversation. And obviously the million dollar Arc prize, I'm excited to actually play with it myself. And hopefully the Vesuvius challenge, which was Nat Friedman's prize for solving decoding scrolls, the winner of that, decoding the scrolls from that were buried in the volcanoes in the Herculaneum library that was solved by a 22 year old who was listening to the podcast, Luke Farator. So hopefully somebody listening will find this challenge intriguing and find a solution. So I'm, and the reason I've had on recently a lot of people who are bullish on LLMs and I've had discussions with them before interviewing you about how do we explain the fact that LLMs don't seem to be natively performing that well on Arc. And I found their explanations somewhat contrived and I'll try out some of the reasons on you. But it is actually an intriguing fact that they actually, these are, some of these problems are relatively straightforward for humans to understand. And they do struggle with them if you just input them natively. All of them are very easy for humans. Like any smart human should be able to do 90%, 95% on Arc. Smart human. A smart human, but even a five year old. So with very, very little knowledge, they could definitely do over 50%. So let's talk about that because you, I agree that smart humans will do very well on this test, but the average human will probably do mediocre. Not really. So we actually tried with average humans, the score about 85. That was with Amazon Mechanical Turk workers, right? I honestly don't know the demographic profile of Amazon Mechanical Turk workers, but imagine just interacting with the platform that Amazon has set up to do remote work. That's not the median human across the planet, I'm guessing. I mean, the broader point here being that, so we see the spectrum in humans where humans obviously have AGI, but even within humans, you see a spectrum where some people are relatively dumber and they'll do perform work on IQ like tests. For example, Raven's progressive matrices. If you look at how the average person performs on that and you look at the quick kind of questions that is this sort of midtermists, half of people will get it right, half of people will get it wrong. Some of them are like pretty trivial. For us, we might think like this was kind of trivial. And so humans have AGI, but from relatively small tweaks, you can go from somebody who misses these kinds of basic IQ test questions to somebody who gets them all right, which suggests that actually, if these models are doing natively, we'll talk about some of the previous performances that people have tried with these models, but somebody with a Jack Cole with a 240 million parameter model got 35%. Doesn't that suggest that they're on this spectrum that clearly exists within humans and they're gonna be saturated at pretty soon? Yeah, so that's a bunch of interesting points here. So there is indeed a branch of LLM approaches suspended by Jack Cole that are doing quite well, that are in fact a state of the art. But you have to look at what's going on there. So there are two things. The first thing is that to guess these numbers, you need to pre-train your LLM on millions of generated art tasks. And of course, if you compare that to a five-year-old child looking at art for the first time, the child has never done like you did before, has never seen something like an art task before. The only overlap between what they know and what they have to do in the test is core knowledge, is knowing about like counting and objects and symmetries and things like that. And still, they're gonna do really well and they're gonna do much better than the LLM trained on millions of similar tasks. And the second thing that's something to note about the Jack Cole approach is one thing that's really critical to making the model work at all is test time fine tuning. And that's something that's really missing, by the way, from LLM approaches right now is that, you know, most of the time when you're using an LLM, it's just doing static inference. The model is frozen and you're just prompting it and then you're getting an answer. So the model is not actually learning anything on the fly. Its state is not adapting to the task at hand. What Jack Cole is actually doing is that for every test problem is on the fly, is fine tuning a version of the LLM for that task. And that's really what's unlocking performance. If you don't do that, you get like 1%, 2%. So basically something completely negligible. And if you do test time fine tuning and you add a bunch of tricks on top, then you end up with interesting performance numbers. So I think what he's doing is trying to address one of the key limitations of LLMs today, which is the lack of active inference, is actually adding active inference to LLMs. And that's working extremely well actually. So that's fascinating to me. That there's so many interesting rabbit holes there. Should I take them in sequence or deal with them all at once? Let me just start. So the point you made about the fact that you need to unlock the adapter compute slash test time compute, a lot of the scale maximalist, I think this will be interesting rabbit hole to explore with you, because a lot of the scaling maximalist have your broader perspective in the sense that they think that in addition to scaling, you need these kinds of things, like unlocking adaptive compute or doing some sort of RL to get the system to working. And their perspective is that this is a relatively straightforward thing that will be added atop the representations that a scaled up model has greater access to. No, it's not just a technical detail. It's not a straightforward thing. It is everything. It is the important part. And the scale maximalist argument, you know, it boils down to, you know, these people, they refer to scaling loss, which is this empirical relationship that you can draw between how much compute you spend on training a model and the performance you're getting on benchmarks, right? And the key question here, of course, is, well, how do you measure performance? What it is that you're actually improving by adding more compute and more data? And, well, it's benchmark performance, right? And the thing is, the way you measure performance is not a technical detail. It's not an afterthought because it's gonna narrow down the set of questions that you're asking. And so, accordingly, it's gonna narrow down the set of answers that you're looking for. If you look at the benchmarks we're using for LMS, they're all memorization-based benchmarks. Like, sometimes they are literally just knowledge-based, like a school test. And even if you look at the ones that are, you know, explicitly about reasoning, you realize, if you look closely, that it's, in order to solve them, it's enough to memorize a finite set of reasoning patterns. And then you just reapply them. They're like static programs. LMS are very good at memorizing static programs, small static programs. And they've got this sort of like bank of solution programs. And when you give them a new puzzle, they can just fetch the appropriate program, apply it. And it's looking like it's reasoning, but really it's not doing any sort of on-the-flight program synthesis. All it's doing is program fetching. So you can actually solve all these benchmarks with memorization. And so, what you're scaling up here, like if you look at the models, they are big parametric curves fitted to a data distribution, which I call in descent. So they're basically these big interpolative databases, interpolative memories. And of course, if you scale up the size of your database and you cram into it more knowledge, more patterns and so on, you are gonna be increasing its performance as measured by a memorization benchmark. That's kind of obvious. But as you're doing it, you are not increasing the intelligence of the system one bit. You are increasing the skill of the system. You are increasing its usefulness, its scope of applicability, but not its intelligence because skill is not intelligence. And that's the fundamental confusion that people run into is that they're confusing skill and intelligence. Yeah, there's a lot of fascinating things to talk about here. So skill, intelligence, interpolation. I mean, okay, so the thing about they're fitting some manifold into that maps the input data, there's a reductionist way to talk about what happens in the human brain that says that it's just axons firing at each other. But we don't care about the reductionist explanation of what's happening. We care about what the sort of meta at the macroscopic level, what happens when these things combine. As far as the interpolation goes, so okay, let's look at one of the benchmarks here. There's one benchmark that does great school math and these are problems that like a smart high schooler would be able to solve. It's called GSM 8K and these models get 95% on these. Like basically they always nail it. That's memorization benchmark. Okay, let's talk about what that means. So here's one question about from that benchmark. So 30 students are in a class, one fifth of them are 12 year olds, one third are 13 year old, one 10th are 11 year olds. How many of them are not 11, 12 or 13 years old? So I agree, like this is not rocket science, right? You can write down on paper how you go through this problem and a high school kid, at least a smart high school kid should be able to solve it. Now, when you say memorization, it still has to reason through how to think about fractions and what is the context of the whole problem and then combining the different calculations it's doing. It depends how you want to define reasoning, but there are two definitions you can use. So one is I have available a set of program templates. It's like the structure of the puzzle, which can also generate its solution. And I'm just gonna identify the right template, which is in my memory. I'm gonna input the new values into the template, run the program, get the solution. And you could say this is reasoning. And I say, yeah, sure, okay. But another definition you can use is reasoning is the ability to, when you're faced with a puzzle, given that you don't have already a program in memory to solve it, you must synthesize on-the-fly a new program based on bits of pieces of existing programs that you have. You have to do on-the-fly program synthesis. And it's actually dramatically harder than just fetching the right memorized program and replying it. So I think maybe we are overestimating the extent to which humans are so sample efficient. They also don't need training in this way where they have to drill in these kinds of pathways of reasoning through certain kinds of problems. So let's take math, for example. It's not like you can just show a baby the axioms of set theory. And now they know math, right? So when they're growing up, you had to do years of teaching them pre-algebra. Then you got to do a year of teaching them doing drills and going through the same kind of problem in algebra, then geometry, pre-calculus, calculus. Absolutely, so training? Yeah, but isn't that like the same kind of thing where you can't just see one example and now you have the program or whatever. You actually had to drill it. These models also had to drill it with a bunch of returning data. Sure, I mean, in order to do on-the-fly program synthesis, you actually need building blocks to work from. So knowledge and memory are actually tremendously important in the process. I'm not saying it's memory versus reasoning. In order to do effective reasoning, you need memory. But it sounds like it's compatible with your story that through seeing a lot of different kinds of examples, these things can learn to reason within the context of those examples. And we can also see within bigger and bigger models. So that was an example of a high school level math problem. Let's say a model that's smaller than GPT-3 couldn't do that at all. As these models get bigger, they seem to be able to pick up bigger and bigger. It's not really a size issue. It's more like a training data issue in this case. Well, bigger models can pick up these kinds of circuits which smaller models apparently don't do a good job of doing this even if you were to train them on this kind of data. Doesn't that just suggest that you have bigger and bigger models? They can pick up bigger and bigger pathways or more general ways of reasoning. Absolutely. But then isn't that intelligence? No, no, it's not. If you scale up your database and you keep adding to it more knowledge, more program templates, then sure it becomes more and more skillful. You can apply it to more and more tasks. But general intelligence is not tasks with six skills scaled up to many skills. Because there is an infinite space of possible skills. General intelligence is the ability to approach any problem, any skill, and very quickly master it using valid or data. Because this is what makes you able to face anything you might ever encounter. This is what makes, this is the definition of generality. Like generality is now specifically scaled up. It is the ability to apply your mind to anything at all, to arbitrary things. And this requires, fundamentally, it requires the ability to adapt, to learn on the fly efficiently. So, my claim is that by doing this free training on bigger and bigger models, you are gaining that capacity to then generalize very efficiently. Let me give you an example. Let me give you an example. So, your own company, Google, in their paper on Gemini 1.5, they had this very interesting example where they would give, in context, they would give the model, the grammar book and the dictionary of a language that has less than 200 living speakers. So, it's not in the free training data. And you just give them the dictionary and it basically is able to speak this language and translate to it, including the complex and organic ways in which languages are structured. So, a human, if you showed me a dictionary from English to Spanish, I'm not gonna be able to pick up the how to structure sentences and how to say things in Spanish. The fact that because of the representations that it has gained through this free training, it is able to now extremely efficiently learn a new language. Doesn't that show that this kind of free training actually does increase your ability to learn new tasks? If you're right, if you were right, LLMs would do really well on arch puzzles because arch puzzles are not complex. Each one of them requires very little knowledge. Each one of them is very low on complexity. You don't need to think very hard about it. They're actually extremely obvious for humans, like even children can do them. But LLMs cannot, even LLMs that have, you know, 100,000 times more knowledge than you do. They still cannot. And the only thing that makes arch special is that it was designed with this intent to resist memorization. This is the only thing. And this is the huge blocker for LLM performance, right? And so, you know, I think if you look at LLMs closely, it's pretty obvious that they're not really like synthesizing new programs on the fly to solve the tasks that they're faced with. They're very much replying things that they've stored in memory. For instance, one thing that's very striking is LLMs can solve a CISA cipher, you know, like a CISA cipher, like transposing letters to code a message. And well, that's a very complex algorithm, right? But it comes up quite a bit on the internet. So they've basically memorized it. And what's really interesting is that they can do it for a transposition length of like three or five because there are very, very common numbers in examples provided on the internet. But if you try to do it with an arbitrary number, like nine, it's gonna fail. Because it does not encode the generalized form of the algorithm, but only specific cases. It does memorize specific cases of the algorithm, right? And if it could actually synthesize on the fly the solver algorithm, then the value of N would not matter at all because it does not increase the problem complexity. I think this is true of humans as well, where what was the study that- Humans use memorization pattern matching all the time, of course, but humans are not limited to memorization pattern matching. They have this very unique ability to adapt to new situations on the fly. This is exactly what enables you to navigate every new day in your life. I'm forgetting the details, but there was some study that chess grandmasters will perform very well within the context of the moves that- Excellent example, because chess at the highest level is all about memorization, chess memorization. Okay, sure, we can leave that aside. What is your explanation for the original question of why in context the GPT- sorry, Gemini 1.5 was able to learn a language, including the complex grammar structure? Doesn't that show that they can pick up new knowledge? I would assume that it has simply mined from its extremely extensive and imaginably vast training data, it has mined the required template and then it's just reusing it. We know that they have a very poor ability to synthesize new program templates like this on the fly or even adapt existing ones. They're very much limited to fetching. Suppose there's a programmer at Google, they go into the office in the morning. At what point are they doing something that 100% cannot be due to fetching some template that even if they, suppose they were an LLM, they could not do if they had fetched some template from their program. At what point do they have to use this so-called extreme generalization capability? Forget about Google software developers. Every human, every day of their lives is full of novel things that they've not been prepared for. You cannot navigate your life based on memorization alone. It's impossible. I'm sort of denying the premise that they're, you also agree they're not doing like, quote-unquote memorization. It seems like you're saying they're less capable of generalization, but I'm just curious of like, the kind of generalization they do, if you get into the office and you try to do this kind of generalization, you're gonna fail at your job. But what is the first point, you're a programmer. What is the first point when you try to do that generalization, you would lose your job because you can't do the extreme generalization? I don't have any specific examples, but literally like, take this situation for instance, you've never been here in this room. Maybe you've been in this city a few times, I don't know, but there's a fair amount of novelty. You've never been interviewing me. There's a fair amount of novelty every hour of every day in your life. And it's in fact, by and large, more novelty than any LLM could handle. Like if you just put a LLM in a robot, it could not be doing all the things that you've been doing today, right? Or take on like cell driving cars, for instance. You take a cell driving car operating in the barrier. Do you think you could just drop it in New York City or drop it in London where people drive on the left? No, it's gonna fail. So not only can you drop, not like, make it generalize to a change of rules of driving rules, but you can not even make it generalize to a new city. It needs to be trained on each specific environment. I mean, I agree that self-driving cars aren't AGI. But it's the same type of model, they're transformers as well. I mean, I don't know, they also have brains with neurons in them, but they're less intelligent because they're small. It's not the same architecture. We can get into that. But so I still don't understand like a concrete thing of, we also need training. That's why education exists. That's why we had to spend the first 18 years of our life doing drills. We have a memory, but we are not a memory. We are not limited to just a memory. I'm denying the premise that that's necessarily the only thing these models are doing. And I'm still not sure what is the task that a remote worker would have to, suppose you do some remote work with an LLM and they're programmer, what is the first point that you realize this is not a human, this is an LLM? What about they just send them a knock puzzle and see how they do? No, like part of their job, you know? But you have to deal with novelty all the time. Okay, so if you, is there a world in which all the programmers are replaced? And then we're still saying, but they're only doing memorization late in programming tasks, but they're still producing a trillion dollars worth of output in the form of code. Software development is actually a pretty good example of a job where you're dealing with novelty all the time. Or if you're not, well, I'm not sure what you're doing. So I personally use Genetic VI very little in my software development job. And before LLMs, I think I was also using Stack Overflow very little. You know, some people maybe are just copy-pasting stuff from Stack Overflow, or nowadays copy-pasting stuff from an LLM. Personally, I try to focus on problem-solving. The syntax is just a technical detail. What's really important is the problem-solving. Like the essence of programming is engineering mental models, like mental representations of the problem you're trying to solve. But you can, you know, we have many, people can interact with these systems themselves and you can go to chat GPT and say, here's a specification of the kind of program I want. They'll build it for you. As long as there are many examples of this program on like GitHub and Stack Overflow and so on, sure, they will fetch the program for you from their memory. But you can change arbitrary details. You can say I need it to work on this different kind of server. If that were true, there would be no software engineers today. I agree. We're not at a full AGI yet, in the sense that these models have, let's say, less than a trillion parameters. A human brain has somewhere on the order of 10 to 30 trillion synapses. I mean, if you were just doing some naive math, you're like at least 10x under parameterized. So I agree we're not there yet, but I'm sort of confused on why we're not on the spectrum, where yes, I agree that there's many kinds of generalization they can do, but it seems like they're on this kind of smooth spectrum that we see even within humans, where some humans would have a hard time doing an ARC type test. We see that based on the performance on progressive Ravens matrices type IQ tests. I'm not a fan of IQ tests because for the most part, you can train on IQ tests and get better at them. So they have very much memorization based. And this is actually the main pitfall that ARC tries not to fall far. I'm still not confused. So if all remote jobs are automated in the next five years, let's say, at least that don't require you to be like sort of a service. It's not like a salesperson where you want the human to be talking, but like programming or whatever. In that world, would you say that that's not possible because a lot of what a programmer needs to do, definitely requires things that would not be in any free training corpus? Sure. I mean, in five years, there will be more software engineers than there are today and not too well. But I just want to understand. So I'm still not sure. I mean, I know how to, I studied computer science. I think if I had become a code monkey out of college, like what would I be doing? I go to my job. What is the first thing my boss tells me something to do? When does he realize I'm an LLM if I was an LLM? Probably on the first day, you know? Again, if it were true that LLMs could generalize to novel problems like this and you can actually develop software to solve a problem they've never seen before, you would not need software engineers anymore. In practice, if I look at how people are using LLMs in their software engineering job today, they're using it as a stack of a flow replacement. So they're using it as a way to copy paste code snippets to perform very common actions. And what they actually need is a database of code snippets. They don't actually need any of the abilities that actually make them software engineers. I mean, when we talk about interpolating between stack overflow databases, if you look at the kinds of math problems or coding problems, maybe to say that they're, maybe let's step back on interpolation and let me ask the question this way. Why can't creativity, why isn't creativity just interpolation in a higher dimension where if a bigger model can learn a more complex manifold, we're gonna use the ML language. And if you look at read a biography of a scientist, it doesn't feel like they're not zero shot in new scientific theories. They're playing with existing ideas. They're trying to juxtapose them in their head. They try out some like slightly ever in the tree of intellectual descendants, they try out a different evolutionary path. You sort of run the experiment there in terms of publishing the paper, whatever. It seems like a similar kind of thing humans are doing. There's like at a higher level of generalization. And what you see across bigger and bigger models is they seem to be approaching higher and higher level of generalization where GPT-2 couldn't do a great school level math problem that requires more generalization that it has capability for, even that skill. Then GPT-3 and 4 can. So not quite. So GPT-4 has a higher degree of skill and higher range of skills. Because it's- I don't want to get into semantics here, but I think- The same degree of generalization. I don't want to get into semantics here, but the question of why can't creativity be just interpolation on a higher dimension? I think interpolation can be creative, absolutely. And you know, to your point, I do think that on some level, humans also do a lot of memorization, a lot of reciting, a lot of pattern matching, a lot of interpolation as well. So it's very much a spectrum between pattern matching and true reasoning. It's a spectrum. And humans are never really at one end of the spectrum. They're never really doing pure pattern matching or pure reasoning. They're usually doing some mixture of both. Even if you're doing something that seems very reasoning heavy, like proving a mathematical theorem, as you're doing it, sure, you're doing quite a bit of discrete search in your mind, quite a bit of actual reasoning. But you're also very much guided by intuition, guided by pattern matching, guided by the shape of proofs that you've seen before, by your knowledge of mathematics. So it's never really, you know, all of our thoughts, everything we do is a mixture of this sort of like interpolative memorization based thinking, this sort of like type one thinking and type two thinking. Why are bigger models more sample efficient? Because they have more reusable building blocks that they can lean on to pick up new patterns in their train data. And does that pattern keep continuing as you keep getting bigger and bigger? To the extent that the new patterns you're giving the model to learn are good match for what it has learned before. If you present something that is actually novel, that is not in a state of distribution like an arc puzzle, for instance, it will fail. Let me make this claim. The program synthesis I think is a very, very useful intuition pump. Why can't it be the case that what's happening in the transformer is the early layers are doing the, figuring out how to represent the inputting tokens. And what the middle layers do is this kind of program search, program synthesis, where they combine the inputs to all the circuits in the model where they go from the low level representation to a higher level representation near the middle of the model. They use these programs, they combine these concepts, then what comes out at the other end is the reasoning based on that high level intelligence. Possibly, why not? But if these models were actually capable of synthesizing novel programs, however simple they should be able to do arc because for any arc task, if you write down the solution program in Python, it's not a complex program, it's extremely simple and humans can figure it out. So why can LLMs not do it? Okay, I think that's a fair point. And if I turn the question around to you, so suppose that it's the case that in a year, a multimodal model can solve arc, let's say get 80%, whatever the average human would get, then AGI? Quite possibly, yes. I think if you start, so honestly, what I would like to see is an LLM type model solving arc at like 80%, but after having only been trained on core knowledge related stuff. But human kids, I don't think we're necessarily just trading on, it's not just that we have in our show is object permanence. Okay, let me rephrase that. Only trained on information that is not explicitly trying to anticipate what's gonna be in the arc test set. But isn't the whole point of arc that you can't, sort of, it's a new type of intelligence every single time? Yes, that is the point. So if arc were perfect, flawless benchmark, it would be impossible to anticipate within the test set. And arc was released more than four years ago and so far it's been resistant to memorization. So I think it has, to some extent, passed a test of time. But I don't think it's perfect. I think if you try to make by hand hundreds of thousands of arc tasks and then you try to multiply them by programmatically generating variations and then you end up with maybe hundreds of millions of tasks. Just by brute forcing the task space, there will be enough overlap between what you're trained on and what's in the test set that you can actually score very highly. So, you know, with enough scale, you can always cheat. If you can do this for every single thing that supposedly requires intelligence, then what good is intelligence? Apparently you can just brute force intelligence. If the world, if your life, were a static distribution, then sure, you could just brute force the space of possible behaviors. You could like, you know, the way we think about intelligence, there are several metaphors, I like to use, but one of them is you can think of intelligence as a past finding algorithm in future situation space. Like, I don't know if you're familiar with game development, like RTS game development, but you have a map, right? And you have, it's like a 2D map. And you have partial information about it. Like there is some fog of war on your map. There are areas that you haven't explored yet. You know nothing about them. And then there are areas that you've explored, but you only know how they were like in the past. You don't know how they are like today. And now, instead of thinking about a 2D map, think about the space of possible future situations that you might encounter and how they're connected to each other. Intelligence is a past finding algorithm. So once you set a goal, it will tell you how to get there optimally. But of course, it's constrained by the information you have. It cannot pass find in an area that you know nothing about. It cannot also anticipate changes. And the thing is, if you had complete information about the map, then you could solve the past finding problem by simply memorizing every possible path, every mapping from point A to point B. You could solve the problem with pure memory. But the reason you cannot do that in real life is because you don't actually know what's going to happen in the future. I feel like you're using words like memorization, which we would never use for human children. If your kid learns to do algebra and then now learns to do calculus, you wouldn't say they memorized calculus. If they can just solve any arbitrary algebraic problem, you wouldn't say they memorized algebra. They say they've learned algebra. Humans are never redoing pure memorization or pure reasoning. But that's only because you're semantically labeling when the human does the skill. It's a memorization when the exact same skill is done by the LLM as you can measure by these benchmarks. And you can just plug in any sort of math problem. Sometimes humans are doing the exact same as the LLM is doing, which is just, for instance, I know if you learn to add numbers, you're memorizing an algorithm. You're memorizing a program. And then you can reapply it. You are not synthesizing on the fly the addition program. So obviously at some point, some human had to figure out how to do addition. But the way a kid learns it is not that they figure out from the actions of set theory how to do addition. I think what you're learning in school is mostly memorization. So my claim is that, listen, these models are vastly underparameterized relative to how many flops or how many parameters you have in the human brain. And so yeah, they're not going to be coming up with new theorems like the smartest humans can. But most humans can't do that either. What most humans do, it sounds like it's similar to what you were calling memorization, which is memorizing skills or memorizing techniques that you've learned. And so it sounds like it's compatible. Tell me if this is wrong. Is it compatible in your world if all the remote workers are gone, but they're doing skills which we can potentially make synthetic data of? So we record everybody's screen and every single remote worker screen. We sort of understand the skills they're performing there. And now we've trained a model that can do all this. All the remote workers are unemployed. We're generating trillions of dollars to economic activity for AI remote workers. In that world, are we still in the memorization regime? So sure. With memorization, you can automate almost anything as long as it's a static distribution, as long as you don't have to deal with change. Are most jobs part of such a static distribution? Potentially, there are lots of things that you can automate. And LLMs are an excellent tool for automation. And I think that's true. But you have to understand that automation is not the same as intelligence. I'm not saying that LLMs are useless. I've been a huge proponent of deep learning for many years. And for many years, I've been saying two things. I've been saying that if you keep scaling up deep learning, it will keep paying off. And at the same time, I've been saying, if you keep scaling up deep learning, this will not lead to a GI. So we can automate more and more things. And yes, this is economically valuable. And yes, potentially, there are many jobs. You could automate a way like this. And that would be economically valuable. But you're still not going to have intelligence. So you can ask, OK, so what does it matter if we can generate all this economic value? Maybe we don't need intelligence after all. Well, you need intelligence the moment you have to deal with change, with novelty, with uncertainty. As long as you are in a space that can be exactly described in advance, you can just automate your pure memorization. In fact, you can always solve any problem. You can always display arbitrary levels of skills on any task without leveraging any intelligence whatsoever, as long as it is possible to describe the problem and its solution very, very precisely. But when they do deal with novelty, then you just call it interpolation, right? And so interpolation is not enough to deal with all kinds of novelty if it were, then LLMs would be a GI. Well, I agree they're not a GI. I'm just trying to figure out how do we figure out we're on the path to a GI. And I think a sort of crux here is maybe that it seems to me that these things are on a spectrum and we're clearly covering the earliest part of the spectrum with LLMs. I think so. And oh, OK, interesting. But here's another sort of thing that I think is evidence for this, grokking, right? So clearly, even within deep learning, there's a difference between the memorization regime and the generalization regime, where at first they'll just memorize the data set of if you're doing modular addition, how to add digits. And then at some point, if you keep training on that, they'll learn the skill. So the fact that there is that distinction suggests that the generalized circuit, the deep learning can learn, there is a regime in enters where it generalizes. If you have an over-parameterized model, which you don't have in comparison to all the tasks we want these models to do right now. Grokking is a very, very old phenomenon. We've been observing it for decades. It's basically an instance of the minimum description length principle, where, sure, given a problem, you can just memorize a point-wise input-to-output mapping, which is completely overfit. So it does not generalize at all, but it solves the problem on the train data. And from there, you can actually keep proving it, keep making your mapping simpler and simpler and more compressed. And at some point, it will start generalizing. And so that's something called the minimum description length principle. It's this idea that the program that will generalize best is the shortest, right? And it doesn't mean that you're doing anything other than memorization, but you're doing memorization plus regularization. Right, AKA generalization. Yeah, and that is absolutely, at least to generalization. Right, and then so you do that within one skill, but then the pattern you see here of meta-learning is that it's more efficient to store a program that can perform many skills rather than one skill, which is what we might call fluid intelligence. And so as you get bigger and bigger models, you would expect it to go up this hierarchy of generalization where it generalizes to a skill, then it generalizes multiple skills. That's correct, that's correct. And you know, LLMs, they're not infinitely large. They have only a fixed number of parameters. And so they have to compress their knowledge as much as possible. And in practice, so LLMs are mostly storing reusable bits of programs, like vector programs. And because they have this need for compression, it means that every time they're learning a new program, they're going to try to express it in terms of existing bits and pieces of programs that they've already learned before, right? Isn't this the generalization? Absolutely. Oh, wait, so. This is why, you know, clearly LLMs have some degree of generalization. And this is precisely why, it's because they have to compress. And why is that intrinsically limited? Why can't you just go, at some point, it has to learn a higher level of generalization, a higher level, and then the highest level is the fluid intelligence. It's intrinsically limited because the substrate of your model is a big parametric curve. And all you can do with this is local generalization. If you want to go beyond this towards broader or even extreme generalization, you have to move to a different type of model. And my paradigm of choice is discrete program search, program synthesis. So and if you want to understand that, you can sort of like compare and contrast it with deep learning. So in deep learning, your model is a parametric curve, a differentiable parametric curve. In program synthesis, your model is a discrete graph of operators. So you've got like a set of logical operators, like a domain-specific language. You're picking instances of it. You're structuring that into a graph. That's a program. And that's actually very similar to like a program you might write in Python or C++ and so on. And in deep learning, your learning engine, because we are doing much learning here, like we're trying to automatically learn these models. In deep learning, your learning engine is quite in the sense. And quite in the sense is very compute efficient, because you have this very strong, informative feedback signal about where the solution is. So you can get to the solution very quickly. But it is very data inefficient, meaning that in order to make it work, you need a dense sampling of the operating space. You need a dense sampling of the data distribution. And then you're limited to only generalizing within that data distribution. And the reason why you have this limitation is because your model is a curve. And meanwhile, if you look at discrete program search, the learning engine is combinatorial search. You're just trying a bunch of programs until you find one that actually miss your spec. This process is extremely data efficient. You can learn a generalizable program from just one example, two examples, which is why it works so well on Arc, by the way. But the big limitation is that it's extremely compute inefficient, because you're running into combinatorial explosion, of course. And so you can sort of see here how the planning and discrete program search, they have very complementary strengths and limitations as well. Every limitation of deep learning has a strength, a corresponding strength in program synthesis and inversely. And I think the path forward is going to be to merge the two, to basically start doing. So another way you can think about it is, so these parametric curves, train with ground descent, there are great fits for everything that's system one type thinking, like pattern cognition, intuition, memorization, and so on. And discrete program search is a great fit for type two thinking, system two thinking. For instance, planning, reasoning, quickly figuring out a generalizable model, that matches just one or two examples, like for an archbishop, for instance. And I think humans are never doing pure system one or pure system two. They're always mixing and matching both. And right now, we have all the tools for system one. We have almost nothing for system two. The way forward is to create a hybrid system. And I think the form it's going to take is it's going to be mostly system two. So the outer structure is going to be a discrete program search system. But you're going to fix the fundamental limitation of discrete program search, which is combinator explosion. You're going to fix it with deep learning. You're going to leverage deep learning to guide, to provide intuition in program space, to guide the program search. And I think that's very similar to what you see, for instance, when you're playing chess or when you're trying to prove a theorem, is that it's mostly a reasoning thing, but you start out with some intuition about the shape of the solution. And that's very much something you can get via a deep learning model. Deep learning models, they're very much like intuition machines. They're pattern matching machines. So you start from this shape of the solution, and then you're going to do actual explicit discrete program search. But you're not going to do it via brute force. You're not going to try things kind of like randomly. You're actually going to ask another deep learning model for suggestions. Like, here's the best likely next step. Here's where in the graph you should be going. And you can also use yet another deep learning model for feedback about, well, here's what I had so far. Is it looking good? Should I just backtrack and try something new? So I think discrete program search is going to be the key, but you want to make it dramatically better, all those of magnitude more efficient, by leveraging deep learning. And by the way, another thing that you can use deep learning is, of course, things like common sense knowledge, and knowledge in general. And I think you're going to end up with this sort of system where you have this on-the-fly synthesis engine that can adapt to new situations. But the way it adapts is that it's going to fetch from a bank of patterns, modules that could be themselves, curves that could be differentiable modules, and some others that could be algorithmic in nature. It's going to assemble them via this process that's intuition-guided. And it's going to give you, for every new situation you might be faced with, it's going to give you with a generalizable model that was synthesized using very, very little data. Something like this would sort of arc. That's actually a really interesting prompt, because I think an interesting crux here is when I talk to my friends who are extremely optimistic about LLMs and expect AGI within the next couple of years, they also, in some sense, agree that scaling is not all you need, but that the rest of the progress is undergirded and enabled by scaling. But still, you need to add the system to the test time compute atop these models. And their perspective is that it's relatively straightforward to do that, because you have this library of representations that you built up from free training, but it's almost talking like, it's just like skimming through textbooks. You need some more deliberate way in which it engages with the material it learns. In-context learning is extremely sample-efficient. But to actually distill that into the weights, you need the model to talk through the things that sees and then add it back to the weights. As far as the system 2 goes, they talk about adding some kind of RL setup so that it is encouraged to proceed on the reasoning traces that end up being correct. And they think this is relatively straightforward stuff that will be added within the next couple of years. That's an empirical question. So I think we'll see. Your intuition, I assume, is not that. I'm curious. My intuition is, in fact, this whole system 2 architecture is the hard part. It's the very hard and non-obvious part. Scaling up the interpolative memory is the easy part. All you need is, like, it's literally just a big curve. All you need is more data. It's representation of a data set, interpolative representation of data set. That's the easy part. The hard part is the architecture of intelligence. Memory and intelligence are separate components. We have the memory. We don't have the intelligence yet. And I agree with you that, well, having the memory is actually very useful. And if you just had the intelligence, but it was not hooked up to an extensive memory, it would not be that useful, because it would not have enough material to work from. Yeah. The alternative hypothesis here that former guest Trenton Brickin advanced is that intelligence is just hierarchically associated memory where higher-level patterns, when Sherlock Holmes goes into a crime scene, and he's extremely sample-efficient, he can just look at a few clues and figure out who was a murderer, and the way he's able to do that is he has learned higher-level sort of associations. It's memory in some fundamental sense. But so here's one way to ask the question. In the brain, supposedly we do program synthesis, but it is just synapses connected to one another, each other, and so physically it's got to be that you just query the right circuit, right? You are, yeah, yeah, yeah. It's a matter of degree. But if you can learn it, if training in the environment human ancestors are trained in means you learn those circuits, training on the same kinds of outputs that humans produce, which to replicate require these kinds of circuits, wouldn't that train the same kind of whatever humans have? You know, it's a matter of degree. If you have a system that has a memory and is only capable of doing local generalization from that, it's not going to be very adaptable. To be really general, you need the memory plus the ability to search to quite some depth, to achieve broader even extramuralization. You know, like one of my favorite psychologists, so Jean Piaget was the founder of the Elemental Psychology. He had a very good quote about intelligence. He said, intelligence is what you use when you don't know what to do. And it's like, as a human living your life, in most situations you already know what to do, because you've been in this situation before. You already have the answer, right? And you're only going to need to use intelligence when you're faced with novelty, with something you didn't expect, with something that you weren't prepared for, either by your own experience, your own life experience, or by your evolutionary history. Like, this day that you're living right now is different in some important ways from every day you've lived before, but it's also different from any day ever lived by any of your ancestors. And still, you're capable of being functional, right? How is it possible? I'm not denying that generalization is extremely important, and is the basis for intelligence. That's not the correct, the correct is like, how much of that is happening in the models? But, okay, let me ask a separate question. We might keep going in the circle here. The differences in intelligence between humans, maybe the intelligence tests because of reasons you mentioned are not measuring it well, but clearly there's differences in intelligence between different humans. What is your explanation for what's going on there? Because I think that's sort of compatible with my story that there's a spectrum of generality and that these models are climbing up to a human level, and even some humans haven't even climbed up to the Einstein level or the Francois level, but. That's a great question, you know. There is extensive evidence that intelligence, difference in intelligence are mostly genetic in nature, right? Meaning that if you take someone who is not very intelligent, there is no amount of training, of like training data, you can expose that person to that would make them become Einstein. And this kind of points to the fact that you really need a better architecture, you need a better algorithm, and more training data is not in fact all you need. I think I agree with that. I think what, maybe the way I might phrase it is that the people who are smarter have in ML language better initializations, the neural wiring, if you just look at, it's more efficient, they have maybe greater density of firing. And so as some part of the story is scaling, there is some correlation between brain size and intelligence. And we also see within the context of quote unquote, scaling that people talk about within the context of LLMs, architectural improvements, where a model like Gemini 1.5 flash is performs as well as GPT-4 did when GPT-4 was released a year ago, but is 57 times cheaper on output. So the part of the scaling story is that the architectural improvements are, we're in like extremely low hanging fruit territory when it comes to those. Okay, we're back now with the co-founder of Zapier, Mike Canouf, we had to restart a few times there. And you're funding this prize and you're running this prize with Francois. And so tell me about how this came together, what more prompted you guys to launch this prize? Yeah, I guess I've been sort of like AI curious for 13 years, I co-founded Zapier, been running it for the last 13 years. And I think I first got introduced to your work and during COVID, I kind of went down the rabbit hole, we had a lot of free time. And it was right after you published your on measure of intelligence paper, you sort of introduced the concept of AGI, this like efficiency of skill acquisition is like the right definition and the arc puzzles. But I don't think the first Kaggle contest was done yet. I think it was still running. And so I kind of, it was interesting, but I just parked the idea. And my bigger fish to fry at Zapier were in this middle of this big turnaround of trying to get to our second product. And then it was January, 2022, when the chain of thought paper came out that really like awoken me to sort of the progress. I gave a whole presentation to the Zapier on like the GPT-3 paper events. I'd sort of felt like I had priced in everything that Elms could do and that paper was really shocking to me in terms of these latent capabilities that Elms have that I didn't expect that they had. And so I actually gave up my exact team role at Zapier. I was running half the company at that point and I went back to be an individual contributor and just to go do AI research alongside Brian, my co-founder. And all of that led me to back towards arc. I was looking into it again and I had sort of expected to see this saturation effect that MMLU has, that GMSK 8K has. And when I looked at the scores and the progress since the last four years, I was really again, shocked to see actually we've made very little objective progress towards it and it felt very, it felt like a really, really important Eval. And as I sort of spent the last year asking people, quizzing people about it and sort of my network and community, very few people even knew it existed. And that felt like, okay, if it's right that this is a really, really like globally singularly unique EGI Eval. And it's different from every other Eval that exists that are more narrowly measures AI skill. Like more people should know about this thing. I had my own ideas on how to beat the arc as well. So like I was working on nights and weekends on that and I flew up to meet Francois earlier this year to sort of quiz him, show him my ideas. And ultimately I was like, well, why don't you think more people know about arc? I think you should actually answer that. I think it's a really interesting question. Like why don't you think more people know about arc? Sure, you know, I think benchmarks that gain traction in the research community are benchmarks that are already fairly tractable because the dynamic that you see is that some research group is gonna make some initial breakthrough and then this is gonna catch the attention of everyone else. And so you're gonna get follow-up papers with people trying to beat the first team and so on. And for arc, this has not really happened because arc is actually very hard for existing AI techniques. Kind of arc requires you to try new ideas. And that's very much the point, by the way. Like the point is not that, yeah, you should just be able to apply existing technology and solve arc. The point is that existing technology has reached a plateau and if you want to go beyond that, if you want to start being able to tackle problems that you haven't memorized, that you haven't seen before, you need to try new ideas. And arc is not just meant to be this sort of like measure of how close we are to a GI. It's also meant to be a source of inspiration. Like I want researchers to look at these puzzles and be like, hey, it's really strange that these puzzles are so simple and most humans can just do them very quickly. Why is it so hard for existing AI systems? Why is it so hard for LLMs and so on? And it's true for LLMs, but arc was actually released before LLMs were really a thing. And the only thing that made it special at the time was that it was designed to be a resistance to memorization. And the fact that it has survived LLMs and Genia in general so well, kind of shows that yes, it is actually resistant to memorization. This is what nerds night me because I went and took a bunch of the puzzles myself. I've showed it to all my friends and family too and they're all like, oh yeah, this is like super easy. Are you sure AI can't solve this? Like that's the reaction in the same one for me as well. And the more you dig in, you're like, okay, yep, there's not just empirical evidence over the last four years that it's unbeaten, but there's theoretical like concepts behind why. And I completely agree at this point that like new ideas basically are needed to be dark. And there's a lot of current trends in the world that are actually, I think, working against that happening basically. I think we're actually less likely to generate new ideas right now. You know, I think one of the kind of trends is the closing up frontier research, right? The GP4 paper from Open AI had no technical details shared. The Gemini paper had no technical details shared and like the longer context part of that work. And yet that open innovation and open progress and sharing is what got us to transformers in the first place. That's what got us to LMS in the first place. So it's kind of disappointing a little bit actually that like so much frontier work has gone closed. It's really making a bet that like these individual labs are going to have the breakthrough and not the ecosystem is going to have the breakthrough. And I think sort of the internet open source has shown that that's like the most powerful innovation ecosystem that's ever existed probably in the entire world. I think that's actually really sad that frontier research is no longer being published. If you look back, you know, four years ago, well, everything was just openly shared like all the state of the art results were published and this is no longer the case. And it's very much, you know, Open AI single-handedly changed the game. And I think Open AI basically set back progress towards HGI by quite a few years, probably like five to 10 years for two reasons. And one is that, well, they cause this complete closing down of research, frontier research publishing, but also they trigger this initial burst of hype around LLMS. And now LLMS have sucked the oxygen out of the room like everything, everyone is just doing LLMS. And I see LLMS as a more often off-ramp on the path to HGI actually. And all these new resources, they're actually going to LLMS instead of everything else they could be going to. And, you know, if you look further into the past to like 2015, 2016, there were like a thousand times fewer people doing AI back then. And yet I feel like the rate of progress was higher because people were exploring more directions. The world felt more open-ended. Like you could just go and try, like have a cool idea of a launch and try it and get some interesting results. So there was this energy. And now everyone is very much doing some variation of the same thing. And the big labs also tried their hand on arc, but because they got bad results, they didn't publish anything. Like, you know, people only publish positive results. I wonder how much effort people have put into trying to prompt or scaffold, do some sort of maybe Devon type approach into getting the frontier models and the frontier models of today, not just a year ago, because a lot of post-training has gone into making them better. So Claude Friropas or GPT-40 into getting good solutions on arc. I hope that one of the things this episode does is get people to try out this open competition where they have to put in an open source model to compete. But also to like figure out if they're, maybe the like capability is latent in Claude Opus and just see if you can show that. I think that would be super interesting. So let's talk about the prize. How much do you win if you solve it? You know, get whatever percent on arc. How much do you get if you get the best of vision, but don't crack it? So we got a million dollar, actually a little over a million dollars is the price pool. We're running the contest on an annual basis. We're gonna, we're starting it today through the middle of November. And the goal is to get 85%. That's the lower bound and human average that you guys talked about earlier. And there's a $500,000 prize for the first team that can get to the 85% benchmark. We're also gonna run, we don't expect that to happen this year actually. One of the early statisticians that's up here giving this line that has always stuck with me that the longer it takes, the longer it takes. So my prior is that like arc is gonna take years to solve. And so we're gonna keep to, we're also gonna break down and do a progress price this year. So there's a $100,000 progress price, which we will pay out to the top scores. So $50,000 is gonna go to the top objective scores this year on the Kaggle leaderboard, which we're hosting it on Kaggle. And then we're gonna have a $50,000 pot set for a paper award for the best paper that explains conceptually the scores that they were able to achieve. And one of the I think interesting things we're also gonna be doing is, we're gonna be requiring that in order to win the prize money that you put the solution or your paper out into public domain. The reason for this is, typically with contests, you see a lot of like closed up sharing. People are kind of private secret. They wanna hold their alpha to themselves during the contest period. And because we expect it's gonna be multiple years, we wanna enter a game here. So the plan is at the end of November, we will award the $100,000 prize money to the top progress prize and then use the downtime between December, January, February to share out all the knowledge from the top scores and the approaches folks were taking in order to re-baseline the community up to whatever the state of the art is and then run the contest again next year. And keep doing that on a yearly basis until we get 85%. I'll give some people some context on why I think this prize is very interesting. I was having conversations with my friends who are very much believers in models as they exist today. And first of all, it was intriguing to me that they didn't know about ARC. These are experienced ML researchers. And so you show them this happened a couple of nights ago. We went to dinner and I showed them an example problem. And they said, of course, an LLM would be able to solve something like this. And then we take a screenshot of it. We just put it into our chat GPT app and it doesn't get the pattern. And so I think it's very interesting. Like it is a notable fact. I was sort of playing devil's advocate against you on these kinds of questions. But this is a very intriguing fact. And I'm extreme, I think this prize is extremely interesting because we're gonna learn, we're gonna learn something fascinating one way or another. So with regards to the 85%, separate from this prize, I'd be very curious if somebody could replicate that result because obviously in psychology and other kinds of fields, which this result seems to be analogous to when you run test on some small sample of people, often they're hard to replicate. So I'd be very curious if you try to replicate this, how, what does an average human perform on ARC? Ask for the difficulty on how long it will take to crack this benchmark. It's very interesting because the other benchmarks that are now fully saturated like MMLU math, actually the people who made them, Dan Hendricks and Colin Burns who did MMLU and math, I think they were grad students or college students when they made it. And the goal when they made it just a couple of years ago was that this will be a test of AGI. And of course it got totally saturated. And I know you all argue that these are test memorization, but I think the pattern we've seen, in fact, Epoch AI has a very interesting graph that I'll sort of overlay for the YouTube version here where you see this almost exponential where it gets 5%, 10%, 30%, 40% as you increase the compute across models and then it just shoots up. And in the GPT-4 technical report, they had this interesting graph of the human eval problem set, which was 22 coding problems. And they had to graph it on the mean log pass curve, basically because early on in training or even smaller models can have the right idea of how to solve this problem, but it takes a lot of reliability to make sure they stay on track to solve the whole problem. And so you really wanna up wait the signal where they get it right at least some of the time, maybe one in a hundred times or one in a thousand. And then so they go from like one in a thousand, one in a hundred, one in 10, and then they just like totally saturated. I guess the question I have, this is all leading up to, is why won't the same thing happen with ARC where people had to try really hard, bigger models. And now they figured out these techniques that Jack Cole has figured out with only a 240 million parameter language model that can get 35%. Shouldn't we see the same pattern we saw across all these other benchmarks where you just like sort of eke out. And then once you get the general idea, then you just go all the way to a hundred. That's an empirical question. So we'll see in practice what happens. But what Jack Cole is doing is actually very unique. It's not just pre-training an LLM and then prompting it, he's actually trying to do active inference. He's doing a test time, right? He's doing like test time functioning. Exactly, test time functioning. And this is actually trying to lift one of the key limitations of LLMs, which is that at inference time, they cannot learn anything new. They cannot adapt on the flight what they're seeing. And he's actually trying to learn. So what he's doing is effectively a form of program synthesis. Because the LLM contains a lot of useful building blocks, like programming building blocks, and by finding it on the task at test time, you are trying to assemble these building blocks into the right pattern that matches the task. This is exactly what program synthesis is about. And the way we contrast this approach with discrete program search is that in discrete program search, so you're trying to assemble a program from a set of primitives. You have very few primitives. So people working on discrete program search on Arc, for instance, they tend to work with DSLs that have like 100 to 200 primitive programs. So very small DSL, but then they're trying to combine these primitives into very complex programs. So there's very deep depths of search. And on the other hand, if you look at what Jack is doing with LLMs, is that he's got this sort of like vector program database, DSL of millions of building blocks in the LLM that are mined by pre-training the LLM, not just on a ton of programming problems, but also on millions of generated Arc-like tasks. So you have an extraordinarily large DSL. And then the fun tuning is very, very shallow recombination of these primitives. So discrete program search, very deep recombination, very small set of primitive programs. And the LLM approach is the same, but on the complete opposite end of that spectrum, where you scale up the memorization by a massive factor and you're doing very, very shallow search, but they are the same thing, just different ends of the spectrum. And I think where you're gonna get the most value for your compute cycles is gonna be somewhere in between. You want to leverage memorization to build up a richer, more useful bank of primitive programs. And you don't want them to be hard-coded like what we saw for the typical artist. You want them to be learned from examples. But then you also want to do some degree of deep search. As long as you're only doing very shallow search, you are limited to local generalization. If you want to generalize further, more broadly, this depth of search is gonna be critical. I might argue that the reason that he had to rely so heavily on the synthetic data was because he used a 240 million parameter model because the Kaggle competition at the time required him to use a P100 GPU, which has like a 10th or something of the flops of an H100. And so obviously he can't use, if you believe that sort of scaling will solve this kind of reasoning, then there you can just rely on the generalization, whereas if you're using a much smaller, for context for the listeners, by the way, the frontier models today are literally a thousand X bigger than that. And so for your competition, from what I remember, the submission you'll have to submit can't make any API calls, can't go online, and has to run on NVIDIA Tesla T4. P100. P100. Oh, is it P100? Yeah. Okay, so again, it's like significantly less powerful. There's a 12-hour runtime limit, basically. There's a forcing function of efficiency in the eval. But here's the thing, you only have 100 test tasks. So the amount of computing available for each task is actually quite a bit, especially if you contrast that with the simplicity of each task. So it would be seven minutes per task, basically, which for, people have tried to do these estimates of how many flops does a human brain have. And you can take them with a grain of salt, but as a sort of anchor, it's basically the amount of flops an H100 has. And I guess maybe you would argue with that, well, a human brain can solve this question in faster than 7.2 minutes. So even with a tenth of the compute, you should be able to do it in seven minutes. Obviously, we have less memory than, you know, like petabytes of fast access memory in the brain. And with these, you know, 29 or whatever gigabytes in this H100. Anyway, I guess the broader question I'm asking is, I wish there was a way to also test this prize with some sort of scaffolding on the biggest models as a way to test whether scaling is the path to get to solving ARC. Absolutely. So in the context of the computation, we want to see how much progress we can do with limited resources. But you're entirely right that it's a super interesting open question. What could the biggest model out there actually do on ARC? So we want to actually also make available a private sort of like one-off track where you can submit to us a VM and so you can put on it any model you want. Like you can take one of the largest open source models out there and find you need to do whatever you want and just give us an image. And then we run it on the H100 for like 24 hours or something and you see what you get. I think it's worth pointing out that there's two different test sets. There is a public test set that's in the public GitHub repository that anyone can use to train, you know, put it in an open API call, whatever you'd like to do. And then there's the private test set, which is the 100 that is actually measuring the state of the art. So I think it is pretty open and interesting to have folks attempt to at least use the public test set and go try it. Now, there is an asterisk on any score that's reported on against the public test set because it is public. It could have leaked into the training data. And this is actually what people are already doing. Like you can already try to prompt one of the best models like the latest Jaminar, the latest GPT-4 with tasks from the public evaluation set. And you know, again, the primary set, these tasks are available as JSON files on GitHub. These models are also trained on GitHub. So they're actually trained on these tasks. And yeah, that kind of creates uncertainty about if they can actually solve some of the tasks, is that because they memorized the answer or not. You know, maybe you would be better off trying to create your own private, arc-like, very novel test set. Don't make the task difficult. Don't make them complex. Make them very obvious for humans. But make sure to make them original as much as possible. Make them unique, different. And see how much your GPT-4 and so on GPT-5 does on them. Well, they're having tests on whether these models are being overtrained on these benchmarks. Scale recently did this where on the GSM- That's really interesting. AK, they basically replicated the benchmark with different questions. And so some of the models actually were extremely overfit on the benchmark like Mistral and so forth. And but the frontier models, Claude and GPT actually did as well on their novel benchmark that they did on the specific questions that were in the existing public benchmark. So I would be relatively optimistic about them just sort of training on the JSON. I was joking with Mike that you should allow API access but sort of keep an even more private validation set of these arc questions. And so allow API access, people can sort of play with GPT-4 scaffolding to enter into this contest. And if it turns out maybe later on you run the validation set on the API and if it performs worse than the test set that you allowed the API access to originally, that means that open AI is training on your API calls and you like go public with this and show them like, oh my God, they've like leaked your data. We do want to make, we want to evolve the arc data set. Like that is a goal that we want to do. I think Francois you mentioned, you know, it's not perfect. Yeah, no, arc is not perfect for perfect benchmark. I mean, I made it like four years ago over four years ago, almost five now. This was in a time before LMS. And I think we learned a lot actually since about what potential flaws there might be. I think there is some redundancy in the set of tasks which is of course against the goals of the benchmark. Every task is supposed to be unique in practice. That's not quite true. I think there's also, every task is supposed to be very novel, but in practice, they might not be. They might be structurally similar to something that you might find online somewhere. So we want to keep iterating and release an arc two version later this year. And I think when we do that, we're gonna want to make the old private test set available. So maybe we won't be releasing it publicly, but what we could do is just create a test server where you can query, get a task, you submit a solution, and of course you can use whatever frontier model you want there. So that way, because you actually have to query this API, you're making sure that no one is gonna buy accident train on this data. It's unlike like the current public article which is literally on GitHub. So there's no question about whether the models are actually trained on it. Yes, they are because they're trained on GitHub. So by sort of like gating access to querying this API with a various issue. And then we would see, you know, for people who actually wanna try whatever technique they have in mind using whatever resources they want, that would be a way for them to get an answer. I wonder what might happen. I'm not sure. One answer is that they come up with a whole new algorithm for AI with some explicit program synthesis that now we're on a new track. And another is they did something hacky with the existing models in a way that actually is valid, which reveals that movie intelligence is more of getting things to the right part of the distribution, but then it can reason. And in that world, I guess that will be interesting. And maybe that'll indicate that, you know, you had to do something hacky with current models as they get better, you won't have to do something hacky. I'm also gonna be very curious to see how these multimodal models, if they will perform natively much better at arc like tests. If arc survives three months from here, we'll blow up the price. I think we're about to make a really important moment of like contact with reality by blowing up the price, putting a much big price pool against it. We're gonna learn really quickly if there's like low hanging fruit of ideas. Again, I think new ideas are needed. I think anyone listening this might have the idea in their head. And I'd encourage everyone to like give it a try. And I think as time goes on, that adds strength to the argument that like we sort of stall that in progress and that new ideas are necessary to be dark. Yeah, that's the point of having a money price is that you attract more people, you get them to try to solve it. And if there's a easy way to hack the benchmark that reveals that the benchmark is valid, then you're gonna know about it. In fact, that was the point of the original Carol competition back in 2020 for arc. I was running this competition because I had released this dataset and I wanted to know if it was hackable, if you could cheat. So there was a small money price at the time, there was like 20K. And this was right around the same time as GPT-3 was released. So people of course tried GPT-3 on the public data, it scored zero. But I think what the first context the first context taught us is that there is no obvious shortcuts, right? And well, now there's more money, there's gonna be more people looking into it. Well, we're gonna find out, we're gonna see if the benchmark is gonna survive. And you know, if we end up with a solution that is not like trying to brute force the space of possible arc tasks that's just trained on core knowledge, I don't think it's necessarily gonna be in and by itself, AGI, but it's probably gonna be a huge milestone on the way to AGI. Because what it represents is the ability to synthesize, task a problem solving program from just two or three examples. And that alone is a new way to program. It's an entirely new paradigm for software development where you can start programming potentially quite complex programs that will generalize very well. And instead of programming them by coming up with the shape of the program in your mind and then tapping it up, you're actually just showing the computer what add what you want and you let the computer figure it out. I think that's what is extremely powerful. I wanna riff a little bit on what kinds of solutions might be possible here and which you would consider sort of defeating the purpose of arc and which are sort of valid. Here's one I'll mention which is my friends that Ryan and Buck stayed up last night because I told them about this and they were like, oh, of course, I was gonna solve this. Thank you for spreading the word. Of course, I was gonna solve this. And then so they were trying to prompt, I think Claude, Opus on this and they say they got 25% on the public arc test. And what they did was have other examples of some of the arc tests and in context explain the reasoning of why you went from one output to another output and then now you have the current problem. And I think also maybe expressing the JSON in a way that is more amenable to the tokenizer. And another thing was using the code interpreter. So I'm curious actually, if you think the code interpreter, which keeps getting better as these models get smarter is just the program synthesis right there because what they were able to do was the actual output of the cells, the JSON output, they got through the code interpreter, like write the Python program that gets right up here. Do you think that the program synthesis kind of researcher talking about will look like just using the code interpreter in large language models? I think whatever solution we see that will score well is gonna probably need to leverage some aspects from deep learning models and LLMs in particular. We've shown already that LLMs can do quite well, that's basically the jack code approach. We've also shown that pure discrete program search from a small DSL does very, very well before jack code, this was the state of the art. In fact, it's still extremely close to the state of the art. And there's no deep learning involved at all in these models. So we have two approaches that have basically no overlap that are doing quite well. And they're very much at two opposite ends of one spectrum, where on one end you have these extremely large banks of millions of vector programs, but very, very shallow recombination, like simplistic recombination. And on the other end, you have very simplistic DSLs, very simple, like 100 or 200 primitives, but very deep, very sophisticated program search. The solution is gonna be somewhere in between, right? So the people are gonna be winning the art competition and we're gonna be making the most progress towards near-term NGR are gonna be users that manage to merge the deep learning paradigm and the discrete program search paradigm into one elegant way. And you know, you ask like, what would be legitimate and what would be cheating, for instance? You wanna add a code interpreter to the system. I think that's great, that's sort of legitimate. The part that would be cheating is try to anticipate what might be in the test set, like brute force the space of possible tasks and then train a memorization system on it. And then rely on the fact that you're generating so many tasks, like millions and millions and millions, that inevitably there's gonna be some overlap between what you're generating and what's in the test set. I think that's defeating the purpose of benchmark because then you can just solve it with that and you need to adapt just by fetching a memorized solution. So hopefully arc will resist to that, but you know, nothing, no benchmark is necessarily perfect. So maybe there's a way to hack it and I guess we are gonna get an answer very soon. Although I think some amount of fine tuning is valid because these models don't natively think in terms of, especially the language models alone, which the open source models that they would have to use to be competitive here compete here. They're like natively language, so they need to be able to think in this kind of... Yes. The arc type way. You want to input corner ledge, like arc like corner ledge into the model, but surely you don't need tens of millions of tasks to do this, like corner ledge is extremely basic. If you look at some of these arc type questions, I actually do think they rely a little bit on things I have seen throughout my life. And for the same, like for example, like something bounces off a wall and comes back and you see that pattern. It's like I played arcade games and I've seen like pong or something. And I think for example, when you see the Flynn effect and people's intelligence has measured on very advanced progressive matrices increasing on these kinds of questions, it's probably a simpler story where since now, since childhood, we actually see these sorts of patterns in TV and whatever, spatial patterns. And so I don't think this is sort of core knowledge. I think actually this is also part of the quote unquote trying tuning that humans have as they grow up of seeing different kinds of spatial patterns and trying to pattern match to them. I would definitely file that under core knowledge. Like core knowledge includes basic physics, for instance, bouncing or trajectories, that would be included. But yeah, I think you're entirely right. The reason why as a human, you're able to quickly figure out the solution is because you have this set of building blocks, this set of patterns in your mind that you can recombine. Is core knowledge required to attain intelligence? Any algorithm you have, does the core knowledge have to be in some sense hard coded or can even the core knowledge be learned through intelligence? Core knowledge can be learned. And I think in the case of humans, some amount of core knowledge is something that you're born with. Like we're actually born with a small amount of knowledge about the world we're gonna live in. We're not blank slates. But most core knowledge is acquired through experience. But the thing with core knowledge that it's not gonna be acquired like for instance in school, it's actually acquired very, very early in the first like three to four years of your life. And by age four, you have all the core knowledge you're gonna need as an adult. Okay, interesting. So I mean, on the price itself, I'm super excited to see both the open source versions of maybe with a Lama 70B or something what people can score in the competition itself. Then if to sort of test specifically the scaling hypothesis, I'm very curious to see if you can prompt on the public version of ARC, which I guess when we compare, you will be able to submit to this competition itself. But I'd be very curious to see how, if people can sort of crack that and get ARC working there and if that would update your reviews on AGI. It's gonna be motivating. We're gonna keep running the contest until somebody puts a reproducible open source version into public domain. So even if somebody privately beats the ARC eval, we're gonna still keep the price money until someone can reproduce it and put the public reproducible version out there. Yeah, exactly. Like the goal is to accelerate progress towards AGI. And a key part of that is that any sort of meaningful bits of progress needs to be shared, needs to be public. So everyone can know about it and can try to iterate on it. If there's no sharing, there's no progress. What I'm especially curious about is sort of disaggregating the bets of like, can we make an open version of this versus is this a thing that's just possible with scaling? And we can, I guess test both of them based on the public and the private version. We're making contact with reality as well with this, right? We're gonna learn a lot, I think, about what the actual limits of the compute where if someone showed up and said, hey, here's a closed source model that like I'm getting 50 plus percent on, I think that would probably update us on like, okay, perhaps we should increase the amount of compute that we give on the private test set in order to balance some of the decisions that initially are somewhat arbitrary in order to learn about, okay, what do people want? What does progress look like? And I think both of us are sort of committed to evolving it over time in order to be the best, or the closest to perfect as we can get it. Awesome, and where can people go to learn more about the prize and maybe give their hand at it? ARCPrize.org. Which goes live today, so. It's live now. $70 million is on this line, people. Good luck. Thank you guys for coming on the podcast. It was super fun to go through all the cruxes on intelligence and get a different perspective, and also to announce a prize here. So this is awesome. Thank you for helping break news. Thank you, Finest.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " LLMs are very good at memorizing static programs.", "tokens": [50364, 441, 43, 26386, 366, 588, 665, 412, 10560, 3319, 13437, 4268, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 1, "seek": 0, "start": 3.0, "end": 5.88, "text": " If you scale up the size of your database,", "tokens": [50514, 759, 291, 4373, 493, 264, 2744, 295, 428, 8149, 11, 50658], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 2, "seek": 0, "start": 5.88, "end": 9.16, "text": " you are not increasing the intelligence of the system", "tokens": [50658, 291, 366, 406, 5662, 264, 7599, 295, 264, 1185, 50822], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 3, "seek": 0, "start": 9.16, "end": 9.64, "text": " one bit.", "tokens": [50822, 472, 857, 13, 50846], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 4, "seek": 0, "start": 9.64, "end": 11.52, "text": " I feel like you're using words like memorization, which", "tokens": [50846, 286, 841, 411, 291, 434, 1228, 2283, 411, 10560, 2144, 11, 597, 50940], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 5, "seek": 0, "start": 11.52, "end": 12.96, "text": " we would never use for human children.", "tokens": [50940, 321, 576, 1128, 764, 337, 1952, 2227, 13, 51012], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 6, "seek": 0, "start": 12.96, "end": 16.12, "text": " If they can just solve any arbitrary algebraic problem,", "tokens": [51012, 759, 436, 393, 445, 5039, 604, 23211, 21989, 299, 1154, 11, 51170], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 7, "seek": 0, "start": 16.12, "end": 17.92, "text": " you wouldn't say they've memorized algebra.", "tokens": [51170, 291, 2759, 380, 584, 436, 600, 46677, 21989, 13, 51260], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 8, "seek": 0, "start": 17.92, "end": 19.36, "text": " They'd say they've learned algebra.", "tokens": [51260, 814, 1116, 584, 436, 600, 3264, 21989, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 9, "seek": 0, "start": 19.36, "end": 20.88, "text": " So you've got a million dollar price pool,", "tokens": [51332, 407, 291, 600, 658, 257, 2459, 7241, 3218, 7005, 11, 51408], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 10, "seek": 0, "start": 20.88, "end": 23.8, "text": " and there's a $500,000 price for the first team that", "tokens": [51408, 293, 456, 311, 257, 1848, 7526, 11, 1360, 3218, 337, 264, 700, 1469, 300, 51554], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 11, "seek": 0, "start": 23.8, "end": 25.92, "text": " can get to the 85% benchmark.", "tokens": [51554, 393, 483, 281, 264, 14695, 4, 18927, 13, 51660], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 12, "seek": 0, "start": 25.92, "end": 29.0, "text": " If ARC survives three months from here, we'll pull up the price.", "tokens": [51660, 759, 8943, 34, 46231, 1045, 2493, 490, 510, 11, 321, 603, 2235, 493, 264, 3218, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2312183621563489, "compression_ratio": 1.6676300578034682, "no_speech_prob": 0.005459878593683243}, {"id": 13, "seek": 2900, "start": 29.0, "end": 32.12, "text": " Open AI basically set back progress towards HGI", "tokens": [50364, 7238, 7318, 1936, 992, 646, 4205, 3030, 389, 26252, 50520], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 14, "seek": 2900, "start": 32.12, "end": 33.8, "text": " by probably like five to 10 years.", "tokens": [50520, 538, 1391, 411, 1732, 281, 1266, 924, 13, 50604], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 15, "seek": 2900, "start": 33.8, "end": 35.88, "text": " They caused this complete closing down", "tokens": [50604, 814, 7008, 341, 3566, 10377, 760, 50708], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 16, "seek": 2900, "start": 35.88, "end": 37.32, "text": " of frontier research publishing.", "tokens": [50708, 295, 35853, 2132, 17832, 13, 50780], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 17, "seek": 2900, "start": 37.32, "end": 41.4, "text": " And now LLMs have sucked the oxygen out of the room,", "tokens": [50780, 400, 586, 441, 43, 26386, 362, 26503, 264, 9169, 484, 295, 264, 1808, 11, 50984], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 18, "seek": 2900, "start": 41.4, "end": 44.0, "text": " like everyone is just doing LLMs.", "tokens": [50984, 411, 1518, 307, 445, 884, 441, 43, 26386, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 19, "seek": 2900, "start": 44.0, "end": 46.879999999999995, "text": " OK, today I have the pleasure to speak", "tokens": [51114, 2264, 11, 965, 286, 362, 264, 6834, 281, 1710, 51258], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 20, "seek": 2900, "start": 46.879999999999995, "end": 51.6, "text": " with Francois Chollet, who is a AI researcher at Google", "tokens": [51258, 365, 34695, 271, 761, 1833, 302, 11, 567, 307, 257, 7318, 21751, 412, 3329, 51494], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 21, "seek": 2900, "start": 51.6, "end": 53.28, "text": " and creator of Keras.", "tokens": [51494, 293, 14181, 295, 591, 6985, 13, 51578], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 22, "seek": 2900, "start": 53.28, "end": 56.36, "text": " And he's launching a prize in collaboration", "tokens": [51578, 400, 415, 311, 18354, 257, 12818, 294, 9363, 51732], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 23, "seek": 2900, "start": 56.36, "end": 58.56, "text": " with Mike Canouf, the co-founder of Xavier,", "tokens": [51732, 365, 6602, 1664, 263, 69, 11, 264, 598, 12, 33348, 295, 44653, 11, 51842], "temperature": 0.0, "avg_logprob": -0.19761842346191405, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.0010804806370288134}, {"id": 24, "seek": 5856, "start": 58.56, "end": 60.28, "text": " who we'll also be talking to in a second,", "tokens": [50364, 567, 321, 603, 611, 312, 1417, 281, 294, 257, 1150, 11, 50450], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 25, "seek": 5856, "start": 60.28, "end": 63.84, "text": " a million dollar prize to solve the ARC benchmark", "tokens": [50450, 257, 2459, 7241, 12818, 281, 5039, 264, 8943, 34, 18927, 50628], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 26, "seek": 5856, "start": 63.84, "end": 64.72, "text": " that he created.", "tokens": [50628, 300, 415, 2942, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 27, "seek": 5856, "start": 64.72, "end": 66.96000000000001, "text": " So first question, what is the ARC benchmark,", "tokens": [50672, 407, 700, 1168, 11, 437, 307, 264, 8943, 34, 18927, 11, 50784], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 28, "seek": 5856, "start": 66.96000000000001, "end": 68.44, "text": " and why do we even need this prize?", "tokens": [50784, 293, 983, 360, 321, 754, 643, 341, 12818, 30, 50858], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 29, "seek": 5856, "start": 68.44, "end": 70.12, "text": " Why won't the biggest LLM we have in a year", "tokens": [50858, 1545, 1582, 380, 264, 3880, 441, 43, 44, 321, 362, 294, 257, 1064, 50942], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 30, "seek": 5856, "start": 70.12, "end": 71.8, "text": " be able to just saturate it?", "tokens": [50942, 312, 1075, 281, 445, 21160, 473, 309, 30, 51026], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 31, "seek": 5856, "start": 71.8, "end": 72.36, "text": " Sure.", "tokens": [51026, 4894, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 32, "seek": 5856, "start": 72.36, "end": 75.12, "text": " So ARC is intended as a kind of IQ test", "tokens": [51054, 407, 8943, 34, 307, 10226, 382, 257, 733, 295, 28921, 1500, 51192], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 33, "seek": 5856, "start": 75.12, "end": 76.64, "text": " for machine intelligence.", "tokens": [51192, 337, 3479, 7599, 13, 51268], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 34, "seek": 5856, "start": 76.64, "end": 80.08, "text": " And what makes it different from most LLM benchmarks out there", "tokens": [51268, 400, 437, 1669, 309, 819, 490, 881, 441, 43, 44, 43751, 484, 456, 51440], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 35, "seek": 5856, "start": 80.08, "end": 83.80000000000001, "text": " is that it's designed to be resistant to memorization.", "tokens": [51440, 307, 300, 309, 311, 4761, 281, 312, 20383, 281, 10560, 2144, 13, 51626], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 36, "seek": 5856, "start": 83.80000000000001, "end": 85.88, "text": " So if you look at the way LLMs work,", "tokens": [51626, 407, 498, 291, 574, 412, 264, 636, 441, 43, 26386, 589, 11, 51730], "temperature": 0.0, "avg_logprob": -0.14152809091516444, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0006065178895369172}, {"id": 37, "seek": 8588, "start": 85.88, "end": 89.28, "text": " they're basically this big interpolative memory.", "tokens": [50364, 436, 434, 1936, 341, 955, 44902, 1166, 4675, 13, 50534], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 38, "seek": 8588, "start": 89.28, "end": 91.47999999999999, "text": " And the way you scale up their capabilities", "tokens": [50534, 400, 264, 636, 291, 4373, 493, 641, 10862, 50644], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 39, "seek": 8588, "start": 91.47999999999999, "end": 95.08, "text": " is by trying to cram as much knowledge and patterns", "tokens": [50644, 307, 538, 1382, 281, 941, 335, 382, 709, 3601, 293, 8294, 50824], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 40, "seek": 8588, "start": 95.08, "end": 96.88, "text": " as possible into them.", "tokens": [50824, 382, 1944, 666, 552, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 41, "seek": 8588, "start": 96.88, "end": 101.44, "text": " And by contrast, ARC does not require a lot of knowledge", "tokens": [50914, 400, 538, 8712, 11, 8943, 34, 775, 406, 3651, 257, 688, 295, 3601, 51142], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 42, "seek": 8588, "start": 101.44, "end": 102.24, "text": " at all.", "tokens": [51142, 412, 439, 13, 51182], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 43, "seek": 8588, "start": 102.24, "end": 103.96, "text": " It's designed to only require what's", "tokens": [51182, 467, 311, 4761, 281, 787, 3651, 437, 311, 51268], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 44, "seek": 8588, "start": 103.96, "end": 108.36, "text": " known as core knowledge, which is basic knowledge about things", "tokens": [51268, 2570, 382, 4965, 3601, 11, 597, 307, 3875, 3601, 466, 721, 51488], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 45, "seek": 8588, "start": 108.36, "end": 112.36, "text": " like elementary physics, objectness, counting,", "tokens": [51488, 411, 16429, 10649, 11, 2657, 1287, 11, 13251, 11, 51688], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 46, "seek": 8588, "start": 112.36, "end": 114.64, "text": " that sort of thing, the sort of knowledge", "tokens": [51688, 300, 1333, 295, 551, 11, 264, 1333, 295, 3601, 51802], "temperature": 0.0, "avg_logprob": -0.12460877277232983, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.0016226907027885318}, {"id": 47, "seek": 11464, "start": 114.64, "end": 119.2, "text": " that any four-year-old or five-year-old possesses.", "tokens": [50364, 300, 604, 1451, 12, 5294, 12, 2641, 420, 1732, 12, 5294, 12, 2641, 17490, 279, 13, 50592], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 48, "seek": 11464, "start": 119.2, "end": 122.6, "text": " But what's interesting is that each puzzle in ARC", "tokens": [50592, 583, 437, 311, 1880, 307, 300, 1184, 12805, 294, 8943, 34, 50762], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 49, "seek": 11464, "start": 122.6, "end": 126.08, "text": " is novel, is something that you've probably not encountered", "tokens": [50762, 307, 7613, 11, 307, 746, 300, 291, 600, 1391, 406, 20381, 50936], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 50, "seek": 11464, "start": 126.08, "end": 129.68, "text": " before, even if you've memorized the entire internet.", "tokens": [50936, 949, 11, 754, 498, 291, 600, 46677, 264, 2302, 4705, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 51, "seek": 11464, "start": 129.68, "end": 135.8, "text": " And that's what makes ARC challenging for LLMs.", "tokens": [51116, 400, 300, 311, 437, 1669, 8943, 34, 7595, 337, 441, 43, 26386, 13, 51422], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 52, "seek": 11464, "start": 135.8, "end": 139.4, "text": " And so far, LLMs have not been doing very well on it.", "tokens": [51422, 400, 370, 1400, 11, 441, 43, 26386, 362, 406, 668, 884, 588, 731, 322, 309, 13, 51602], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 53, "seek": 11464, "start": 139.4, "end": 141.72, "text": " In fact, the approaches that are working well", "tokens": [51602, 682, 1186, 11, 264, 11587, 300, 366, 1364, 731, 51718], "temperature": 0.0, "avg_logprob": -0.11468194757850425, "compression_ratio": 1.547008547008547, "no_speech_prob": 0.0012267893180251122}, {"id": 54, "seek": 14172, "start": 141.72, "end": 144.4, "text": " are more towards discrete program search, program", "tokens": [50364, 366, 544, 3030, 27706, 1461, 3164, 11, 1461, 50498], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 55, "seek": 14172, "start": 144.4, "end": 145.76, "text": " synthesis.", "tokens": [50498, 30252, 13, 50566], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 56, "seek": 14172, "start": 145.76, "end": 148.32, "text": " So first of all, I'll make a comment", "tokens": [50566, 407, 700, 295, 439, 11, 286, 603, 652, 257, 2871, 50694], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 57, "seek": 14172, "start": 148.32, "end": 150.72, "text": " that I'm glad that as a skeptic of LLM,", "tokens": [50694, 300, 286, 478, 5404, 300, 382, 257, 19128, 299, 295, 441, 43, 44, 11, 50814], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 58, "seek": 14172, "start": 150.72, "end": 155.64, "text": " you have put out yourself a benchmark that is it accurate", "tokens": [50814, 291, 362, 829, 484, 1803, 257, 18927, 300, 307, 309, 8559, 51060], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 59, "seek": 14172, "start": 155.64, "end": 158.88, "text": " to say that, suppose that the biggest model we have in a year", "tokens": [51060, 281, 584, 300, 11, 7297, 300, 264, 3880, 2316, 321, 362, 294, 257, 1064, 51222], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 60, "seek": 14172, "start": 158.88, "end": 161.8, "text": " is able to get 80% on this, then your view would be", "tokens": [51222, 307, 1075, 281, 483, 4688, 4, 322, 341, 11, 550, 428, 1910, 576, 312, 51368], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 61, "seek": 14172, "start": 161.8, "end": 163.88, "text": " we are on track to AGI with LLMs.", "tokens": [51368, 321, 366, 322, 2837, 281, 316, 26252, 365, 441, 43, 26386, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 62, "seek": 14172, "start": 163.88, "end": 165.24, "text": " How would you think about that?", "tokens": [51472, 1012, 576, 291, 519, 466, 300, 30, 51540], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 63, "seek": 14172, "start": 165.24, "end": 167.4, "text": " Right.", "tokens": [51540, 1779, 13, 51648], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 64, "seek": 14172, "start": 167.4, "end": 168.84, "text": " I'm pretty skeptical that we're going", "tokens": [51648, 286, 478, 1238, 28601, 300, 321, 434, 516, 51720], "temperature": 0.0, "avg_logprob": -0.14808106800866505, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.00570249930024147}, {"id": 65, "seek": 16884, "start": 168.84, "end": 171.6, "text": " to see LLM do 80% in a year.", "tokens": [50364, 281, 536, 441, 43, 44, 360, 4688, 4, 294, 257, 1064, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 66, "seek": 16884, "start": 171.6, "end": 173.72, "text": " That said, if we do see it, you would also", "tokens": [50502, 663, 848, 11, 498, 321, 360, 536, 309, 11, 291, 576, 611, 50608], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 67, "seek": 16884, "start": 173.72, "end": 176.12, "text": " have to look at how this was achieved.", "tokens": [50608, 362, 281, 574, 412, 577, 341, 390, 11042, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 68, "seek": 16884, "start": 176.12, "end": 180.32, "text": " If you just train the model and millions or billions", "tokens": [50728, 759, 291, 445, 3847, 264, 2316, 293, 6803, 420, 17375, 50938], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 69, "seek": 16884, "start": 180.32, "end": 182.44, "text": " of puzzles similar to ARC so that you're", "tokens": [50938, 295, 24138, 2531, 281, 8943, 34, 370, 300, 291, 434, 51044], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 70, "seek": 16884, "start": 182.44, "end": 187.44, "text": " relying on the ability to have some overlap between the tasks", "tokens": [51044, 24140, 322, 264, 3485, 281, 362, 512, 19959, 1296, 264, 9608, 51294], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 71, "seek": 16884, "start": 187.44, "end": 188.8, "text": " that you train on and the tasks that you're", "tokens": [51294, 300, 291, 3847, 322, 293, 264, 9608, 300, 291, 434, 51362], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 72, "seek": 16884, "start": 188.8, "end": 190.52, "text": " going to see at this time, then you're still", "tokens": [51362, 516, 281, 536, 412, 341, 565, 11, 550, 291, 434, 920, 51448], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 73, "seek": 16884, "start": 190.52, "end": 192.48000000000002, "text": " using memorization.", "tokens": [51448, 1228, 10560, 2144, 13, 51546], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 74, "seek": 16884, "start": 192.48000000000002, "end": 194.44, "text": " And maybe it can work.", "tokens": [51546, 400, 1310, 309, 393, 589, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 75, "seek": 16884, "start": 194.44, "end": 197.52, "text": " Hopefully, ARC is going to be good enough", "tokens": [51644, 10429, 11, 8943, 34, 307, 516, 281, 312, 665, 1547, 51798], "temperature": 0.0, "avg_logprob": -0.1603042395539986, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.009856096468865871}, {"id": 76, "seek": 19752, "start": 197.56, "end": 200.52, "text": " that it's going to be resistant to this sort of attempt", "tokens": [50366, 300, 309, 311, 516, 281, 312, 20383, 281, 341, 1333, 295, 5217, 50514], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 77, "seek": 19752, "start": 200.52, "end": 202.28, "text": " at brute forcing.", "tokens": [50514, 412, 47909, 19030, 13, 50602], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 78, "seek": 19752, "start": 202.28, "end": 203.28, "text": " But you never know.", "tokens": [50602, 583, 291, 1128, 458, 13, 50652], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 79, "seek": 19752, "start": 203.28, "end": 204.56, "text": " Maybe it could happen.", "tokens": [50652, 2704, 309, 727, 1051, 13, 50716], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 80, "seek": 19752, "start": 204.56, "end": 206.32000000000002, "text": " I'm not saying it's not going to happen.", "tokens": [50716, 286, 478, 406, 1566, 309, 311, 406, 516, 281, 1051, 13, 50804], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 81, "seek": 19752, "start": 206.32000000000002, "end": 208.20000000000002, "text": " ARC is not a perfect benchmark.", "tokens": [50804, 8943, 34, 307, 406, 257, 2176, 18927, 13, 50898], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 82, "seek": 19752, "start": 208.20000000000002, "end": 210.20000000000002, "text": " Maybe it has flaws.", "tokens": [50898, 2704, 309, 575, 27108, 13, 50998], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 83, "seek": 19752, "start": 210.20000000000002, "end": 213.12, "text": " Maybe it could be hacked in that way.", "tokens": [50998, 2704, 309, 727, 312, 36218, 294, 300, 636, 13, 51144], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 84, "seek": 19752, "start": 213.12, "end": 217.12, "text": " So I guess I'm curious about what would GPTI", "tokens": [51144, 407, 286, 2041, 286, 478, 6369, 466, 437, 576, 26039, 5422, 51344], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 85, "seek": 19752, "start": 217.12, "end": 220.76000000000002, "text": " have to do that you're very confident that it's", "tokens": [51344, 362, 281, 360, 300, 291, 434, 588, 6679, 300, 309, 311, 51526], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 86, "seek": 19752, "start": 220.76000000000002, "end": 222.16000000000003, "text": " on the path to AGI?", "tokens": [51526, 322, 264, 3100, 281, 316, 26252, 30, 51596], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 87, "seek": 19752, "start": 222.16000000000003, "end": 224.72, "text": " What would make me change my mind about LLMs", "tokens": [51596, 708, 576, 652, 385, 1319, 452, 1575, 466, 441, 43, 26386, 51724], "temperature": 0.0, "avg_logprob": -0.17365653991699218, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0005697943852283061}, {"id": 88, "seek": 22472, "start": 224.72, "end": 229.92, "text": " is basically, if I start seeing a critical mass of cases", "tokens": [50364, 307, 1936, 11, 498, 286, 722, 2577, 257, 4924, 2758, 295, 3331, 50624], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 89, "seek": 22472, "start": 229.92, "end": 232.12, "text": " where you show the model with something", "tokens": [50624, 689, 291, 855, 264, 2316, 365, 746, 50734], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 90, "seek": 22472, "start": 232.12, "end": 235.07999999999998, "text": " it has not seen before, a task that's actually", "tokens": [50734, 309, 575, 406, 1612, 949, 11, 257, 5633, 300, 311, 767, 50882], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 91, "seek": 22472, "start": 235.07999999999998, "end": 237.92, "text": " novel from the perspective of its training data, something", "tokens": [50882, 7613, 490, 264, 4585, 295, 1080, 3097, 1412, 11, 746, 51024], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 92, "seek": 22472, "start": 237.92, "end": 239.96, "text": " that's not in the training data, and if it can actually", "tokens": [51024, 300, 311, 406, 294, 264, 3097, 1412, 11, 293, 498, 309, 393, 767, 51126], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 93, "seek": 22472, "start": 239.96, "end": 242.88, "text": " adapt on the fly.", "tokens": [51126, 6231, 322, 264, 3603, 13, 51272], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 94, "seek": 22472, "start": 242.88, "end": 243.96, "text": " And this is true for LLMs.", "tokens": [51272, 400, 341, 307, 2074, 337, 441, 43, 26386, 13, 51326], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 95, "seek": 22472, "start": 243.96, "end": 246.07999999999998, "text": " But really, this would catch my attention", "tokens": [51326, 583, 534, 11, 341, 576, 3745, 452, 3202, 51432], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 96, "seek": 22472, "start": 246.07999999999998, "end": 248.84, "text": " with any for any AI technique out there.", "tokens": [51432, 365, 604, 337, 604, 7318, 6532, 484, 456, 13, 51570], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 97, "seek": 22472, "start": 248.84, "end": 253.6, "text": " If I can see the ability to adapt to novelty on the fly", "tokens": [51570, 759, 286, 393, 536, 264, 3485, 281, 6231, 281, 44805, 322, 264, 3603, 51808], "temperature": 0.0, "avg_logprob": -0.15802014775636816, "compression_ratio": 1.7265625, "no_speech_prob": 0.013767492026090622}, {"id": 98, "seek": 25360, "start": 253.6, "end": 255.6, "text": " to pick up new skills efficiently,", "tokens": [50364, 281, 1888, 493, 777, 3942, 19621, 11, 50464], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 99, "seek": 25360, "start": 255.6, "end": 258.04, "text": " then I would be extremely interested.", "tokens": [50464, 550, 286, 576, 312, 4664, 3102, 13, 50586], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 100, "seek": 25360, "start": 258.04, "end": 261.44, "text": " I would think this is on the path to AGI.", "tokens": [50586, 286, 576, 519, 341, 307, 322, 264, 3100, 281, 316, 26252, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 101, "seek": 25360, "start": 261.44, "end": 264.68, "text": " So the advantage they have is that they do get to see everything.", "tokens": [50756, 407, 264, 5002, 436, 362, 307, 300, 436, 360, 483, 281, 536, 1203, 13, 50918], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 102, "seek": 25360, "start": 264.68, "end": 267.64, "text": " Maybe I'll take issue with how much they are relying on that.", "tokens": [50918, 2704, 286, 603, 747, 2734, 365, 577, 709, 436, 366, 24140, 322, 300, 13, 51066], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 103, "seek": 25360, "start": 267.64, "end": 269.04, "text": " But let's suppose that they are relying.", "tokens": [51066, 583, 718, 311, 7297, 300, 436, 366, 24140, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 104, "seek": 25360, "start": 269.04, "end": 272.2, "text": " Obviously, they're relying on that more than humans do.", "tokens": [51136, 7580, 11, 436, 434, 24140, 322, 300, 544, 813, 6255, 360, 13, 51294], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 105, "seek": 25360, "start": 272.2, "end": 275.56, "text": " To the extent that they do have so much indistribution,", "tokens": [51294, 1407, 264, 8396, 300, 436, 360, 362, 370, 709, 1016, 468, 30783, 11, 51462], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 106, "seek": 25360, "start": 275.56, "end": 277.84, "text": " to the extent that we have trouble distinguishing", "tokens": [51462, 281, 264, 8396, 300, 321, 362, 5253, 11365, 3807, 51576], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 107, "seek": 25360, "start": 277.84, "end": 281.52, "text": " whether an example is indistribution or not,", "tokens": [51576, 1968, 364, 1365, 307, 1016, 468, 30783, 420, 406, 11, 51760], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 108, "seek": 25360, "start": 281.52, "end": 283.24, "text": " well, if they have everything in distribution,", "tokens": [51760, 731, 11, 498, 436, 362, 1203, 294, 7316, 11, 51846], "temperature": 0.0, "avg_logprob": -0.14255228042602539, "compression_ratio": 1.8581314878892734, "no_speech_prob": 0.0008557005785405636}, {"id": 109, "seek": 28324, "start": 283.24, "end": 285.28000000000003, "text": " then they can do everything that we can do.", "tokens": [50364, 550, 436, 393, 360, 1203, 300, 321, 393, 360, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 110, "seek": 28324, "start": 285.28000000000003, "end": 287.72, "text": " Maybe it's not indistribution for us.", "tokens": [50466, 2704, 309, 311, 406, 1016, 468, 30783, 337, 505, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 111, "seek": 28324, "start": 287.72, "end": 290.16, "text": " Why is it so crucial that it has to be out of distribution", "tokens": [50588, 1545, 307, 309, 370, 11462, 300, 309, 575, 281, 312, 484, 295, 7316, 50710], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 112, "seek": 28324, "start": 290.16, "end": 291.52, "text": " for them?", "tokens": [50710, 337, 552, 30, 50778], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 113, "seek": 28324, "start": 291.52, "end": 292.92, "text": " Why can't we just leverage the fact", "tokens": [50778, 1545, 393, 380, 321, 445, 13982, 264, 1186, 50848], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 114, "seek": 28324, "start": 292.92, "end": 294.44, "text": " that they do get to see everything?", "tokens": [50848, 300, 436, 360, 483, 281, 536, 1203, 30, 50924], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 115, "seek": 28324, "start": 294.44, "end": 295.68, "text": " Right.", "tokens": [50924, 1779, 13, 50986], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 116, "seek": 28324, "start": 295.68, "end": 297.56, "text": " You're asking basically what's the difference", "tokens": [50986, 509, 434, 3365, 1936, 437, 311, 264, 2649, 51080], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 117, "seek": 28324, "start": 297.56, "end": 299.28000000000003, "text": " between actual intelligence, which", "tokens": [51080, 1296, 3539, 7599, 11, 597, 51166], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 118, "seek": 28324, "start": 299.28000000000003, "end": 301.92, "text": " is the ability to adapt to things you've not been prepared", "tokens": [51166, 307, 264, 3485, 281, 6231, 281, 721, 291, 600, 406, 668, 4927, 51298], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 119, "seek": 28324, "start": 301.92, "end": 306.04, "text": " for, and pure memorization, like reciting what you've seen", "tokens": [51298, 337, 11, 293, 6075, 10560, 2144, 11, 411, 850, 1748, 437, 291, 600, 1612, 51504], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 120, "seek": 28324, "start": 306.04, "end": 306.92, "text": " before.", "tokens": [51504, 949, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 121, "seek": 28324, "start": 306.92, "end": 310.12, "text": " And it's not just some semantic difference.", "tokens": [51548, 400, 309, 311, 406, 445, 512, 47982, 2649, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1752418460267963, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.00018227577675133944}, {"id": 122, "seek": 31012, "start": 310.12, "end": 313.84000000000003, "text": " The big difference is that you can never", "tokens": [50364, 440, 955, 2649, 307, 300, 291, 393, 1128, 50550], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 123, "seek": 31012, "start": 313.84000000000003, "end": 318.52, "text": " pre-train on everything that you might see at test time,", "tokens": [50550, 659, 12, 83, 7146, 322, 1203, 300, 291, 1062, 536, 412, 1500, 565, 11, 50784], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 124, "seek": 31012, "start": 318.52, "end": 320.84000000000003, "text": " because the world changes all the time.", "tokens": [50784, 570, 264, 1002, 2962, 439, 264, 565, 13, 50900], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 125, "seek": 31012, "start": 320.84000000000003, "end": 324.4, "text": " So it's not just the fact that the space of possible tasks", "tokens": [50900, 407, 309, 311, 406, 445, 264, 1186, 300, 264, 1901, 295, 1944, 9608, 51078], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 126, "seek": 31012, "start": 324.4, "end": 325.28000000000003, "text": " is infinite.", "tokens": [51078, 307, 13785, 13, 51122], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 127, "seek": 31012, "start": 325.28000000000003, "end": 328.08, "text": " And even if you're trained on millions of them,", "tokens": [51122, 400, 754, 498, 291, 434, 8895, 322, 6803, 295, 552, 11, 51262], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 128, "seek": 31012, "start": 328.08, "end": 330.12, "text": " you've only seen zero person out of the total space.", "tokens": [51262, 291, 600, 787, 1612, 4018, 954, 484, 295, 264, 3217, 1901, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 129, "seek": 31012, "start": 330.12, "end": 334.72, "text": " It's also the fact that the world is changing every day.", "tokens": [51364, 467, 311, 611, 264, 1186, 300, 264, 1002, 307, 4473, 633, 786, 13, 51594], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 130, "seek": 31012, "start": 334.72, "end": 337.8, "text": " This is why we, the human species,", "tokens": [51594, 639, 307, 983, 321, 11, 264, 1952, 6172, 11, 51748], "temperature": 0.0, "avg_logprob": -0.14197204349277256, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.0010702376021072268}, {"id": 131, "seek": 33780, "start": 337.84000000000003, "end": 340.48, "text": " developed intelligence in the first place.", "tokens": [50366, 4743, 7599, 294, 264, 700, 1081, 13, 50498], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 132, "seek": 33780, "start": 340.48, "end": 344.72, "text": " If there was a shifting as a distribution for the world,", "tokens": [50498, 759, 456, 390, 257, 17573, 382, 257, 7316, 337, 264, 1002, 11, 50710], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 133, "seek": 33780, "start": 344.72, "end": 347.08, "text": " for the universe, for our lives, then we would not", "tokens": [50710, 337, 264, 6445, 11, 337, 527, 2909, 11, 550, 321, 576, 406, 50828], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 134, "seek": 33780, "start": 347.08, "end": 348.40000000000003, "text": " need intelligence at all.", "tokens": [50828, 643, 7599, 412, 439, 13, 50894], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 135, "seek": 33780, "start": 348.40000000000003, "end": 351.84000000000003, "text": " In fact, many creatures, many insects, for instance,", "tokens": [50894, 682, 1186, 11, 867, 12281, 11, 867, 20201, 11, 337, 5197, 11, 51066], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 136, "seek": 33780, "start": 351.84000000000003, "end": 353.44, "text": " do not have intelligence.", "tokens": [51066, 360, 406, 362, 7599, 13, 51146], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 137, "seek": 33780, "start": 353.44, "end": 358.44, "text": " Instead, what they have is they have in their connectome,", "tokens": [51146, 7156, 11, 437, 436, 362, 307, 436, 362, 294, 641, 1745, 423, 11, 51396], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 138, "seek": 33780, "start": 358.44, "end": 361.12, "text": " in their genes, hard-coded programs,", "tokens": [51396, 294, 641, 14424, 11, 1152, 12, 66, 12340, 4268, 11, 51530], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 139, "seek": 33780, "start": 361.12, "end": 363.8, "text": " behavioral programs that map some stimuli", "tokens": [51530, 19124, 4268, 300, 4471, 512, 47752, 51664], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 140, "seek": 33780, "start": 363.8, "end": 365.32, "text": " to appropriate response.", "tokens": [51664, 281, 6854, 4134, 13, 51740], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 141, "seek": 33780, "start": 365.32, "end": 367.72, "text": " And they can actually navigate their lives", "tokens": [51740, 400, 436, 393, 767, 12350, 641, 2909, 51860], "temperature": 0.0, "avg_logprob": -0.17721495669112247, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.005313290283083916}, {"id": 142, "seek": 36772, "start": 367.72, "end": 371.16, "text": " to environments in a way that's very evolutionary fit.", "tokens": [50364, 281, 12388, 294, 257, 636, 300, 311, 588, 27567, 3318, 13, 50536], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 143, "seek": 36772, "start": 371.16, "end": 374.40000000000003, "text": " That way, without needing to learn anything.", "tokens": [50536, 663, 636, 11, 1553, 18006, 281, 1466, 1340, 13, 50698], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 144, "seek": 36772, "start": 374.40000000000003, "end": 377.56, "text": " And while if our environment was static enough,", "tokens": [50698, 400, 1339, 498, 527, 2823, 390, 13437, 1547, 11, 50856], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 145, "seek": 36772, "start": 377.56, "end": 379.92, "text": " predictable enough, what would have happened", "tokens": [50856, 27737, 1547, 11, 437, 576, 362, 2011, 50974], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 146, "seek": 36772, "start": 379.92, "end": 381.96000000000004, "text": " is that evolution would have found", "tokens": [50974, 307, 300, 9303, 576, 362, 1352, 51076], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 147, "seek": 36772, "start": 381.96000000000004, "end": 384.40000000000003, "text": " the perfect behavioral program, a hard-coded,", "tokens": [51076, 264, 2176, 19124, 1461, 11, 257, 1152, 12, 66, 12340, 11, 51198], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 148, "seek": 36772, "start": 384.40000000000003, "end": 385.72, "text": " static behavioral program.", "tokens": [51198, 13437, 19124, 1461, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 149, "seek": 36772, "start": 385.72, "end": 388.48, "text": " We'd have written it into our genes.", "tokens": [51264, 492, 1116, 362, 3720, 309, 666, 527, 14424, 13, 51402], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 150, "seek": 36772, "start": 388.48, "end": 390.88000000000005, "text": " We would have a hard-coded brain connectome.", "tokens": [51402, 492, 576, 362, 257, 1152, 12, 66, 12340, 3567, 1745, 423, 13, 51522], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 151, "seek": 36772, "start": 390.88000000000005, "end": 392.24, "text": " And that's what we were running on.", "tokens": [51522, 400, 300, 311, 437, 321, 645, 2614, 322, 13, 51590], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 152, "seek": 36772, "start": 392.24, "end": 393.44000000000005, "text": " But no, that's not what happened.", "tokens": [51590, 583, 572, 11, 300, 311, 406, 437, 2011, 13, 51650], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 153, "seek": 36772, "start": 393.44000000000005, "end": 396.24, "text": " Instead, we have general intelligence.", "tokens": [51650, 7156, 11, 321, 362, 2674, 7599, 13, 51790], "temperature": 0.0, "avg_logprob": -0.17534943514092025, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0005660054739564657}, {"id": 154, "seek": 39624, "start": 396.28000000000003, "end": 400.36, "text": " We are born with extremely little knowledge about the world.", "tokens": [50366, 492, 366, 4232, 365, 4664, 707, 3601, 466, 264, 1002, 13, 50570], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 155, "seek": 39624, "start": 400.36, "end": 403.76, "text": " But we are born with the ability to learn very efficiently", "tokens": [50570, 583, 321, 366, 4232, 365, 264, 3485, 281, 1466, 588, 19621, 50740], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 156, "seek": 39624, "start": 403.76, "end": 405.96000000000004, "text": " and to adapt in the face of things", "tokens": [50740, 293, 281, 6231, 294, 264, 1851, 295, 721, 50850], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 157, "seek": 39624, "start": 405.96000000000004, "end": 407.72, "text": " that we've never seen before.", "tokens": [50850, 300, 321, 600, 1128, 1612, 949, 13, 50938], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 158, "seek": 39624, "start": 407.72, "end": 408.92, "text": " And that's what makes us unique.", "tokens": [50938, 400, 300, 311, 437, 1669, 505, 3845, 13, 50998], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 159, "seek": 39624, "start": 408.92, "end": 411.96000000000004, "text": " And that's what is really, really challenging", "tokens": [50998, 400, 300, 311, 437, 307, 534, 11, 534, 7595, 51150], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 160, "seek": 39624, "start": 411.96000000000004, "end": 413.64, "text": " to recreate in machines.", "tokens": [51150, 281, 25833, 294, 8379, 13, 51234], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 161, "seek": 39624, "start": 413.64, "end": 415.24, "text": " I want to wrap it all in that a little bit.", "tokens": [51234, 286, 528, 281, 7019, 309, 439, 294, 300, 257, 707, 857, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 162, "seek": 39624, "start": 415.24, "end": 417.76, "text": " But before I do that, maybe I'm going", "tokens": [51314, 583, 949, 286, 360, 300, 11, 1310, 286, 478, 516, 51440], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 163, "seek": 39624, "start": 417.76, "end": 419.92, "text": " to overlay some examples of what an arc-like challenge looks", "tokens": [51440, 281, 31741, 512, 5110, 295, 437, 364, 10346, 12, 4092, 3430, 1542, 51548], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 164, "seek": 39624, "start": 419.92, "end": 421.76, "text": " like for the YouTube audience.", "tokens": [51548, 411, 337, 264, 3088, 4034, 13, 51640], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 165, "seek": 39624, "start": 421.76, "end": 423.8, "text": " But maybe for people listening on audio,", "tokens": [51640, 583, 1310, 337, 561, 4764, 322, 6278, 11, 51742], "temperature": 0.0, "avg_logprob": -0.19335108592097922, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0018043819582089782}, {"id": 166, "seek": 42380, "start": 423.8, "end": 426.68, "text": " can you just describe what an example arc challenge", "tokens": [50364, 393, 291, 445, 6786, 437, 364, 1365, 10346, 3430, 50508], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 167, "seek": 42380, "start": 426.68, "end": 427.16, "text": " will look like?", "tokens": [50508, 486, 574, 411, 30, 50532], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 168, "seek": 42380, "start": 427.16, "end": 427.64, "text": " Sure.", "tokens": [50532, 4894, 13, 50556], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 169, "seek": 42380, "start": 427.64, "end": 431.8, "text": " So one arc puzzle, it looks kind of like an IQ test puzzle.", "tokens": [50556, 407, 472, 10346, 12805, 11, 309, 1542, 733, 295, 411, 364, 28921, 1500, 12805, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 170, "seek": 42380, "start": 431.8, "end": 435.48, "text": " You've got a number of demonstration input-adput pairs.", "tokens": [50764, 509, 600, 658, 257, 1230, 295, 16520, 4846, 12, 345, 2582, 15494, 13, 50948], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 171, "seek": 42380, "start": 435.48, "end": 439.2, "text": " So one pair is made of two grids.", "tokens": [50948, 407, 472, 6119, 307, 1027, 295, 732, 677, 3742, 13, 51134], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 172, "seek": 42380, "start": 439.2, "end": 441.8, "text": " So one grid shows you an input.", "tokens": [51134, 407, 472, 10748, 3110, 291, 364, 4846, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 173, "seek": 42380, "start": 441.8, "end": 444.04, "text": " And the second grid shows you what", "tokens": [51264, 400, 264, 1150, 10748, 3110, 291, 437, 51376], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 174, "seek": 42380, "start": 444.04, "end": 447.84000000000003, "text": " you should produce as a response to that input.", "tokens": [51376, 291, 820, 5258, 382, 257, 4134, 281, 300, 4846, 13, 51566], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 175, "seek": 42380, "start": 447.84000000000003, "end": 450.6, "text": " And you get a couple pairs like this", "tokens": [51566, 400, 291, 483, 257, 1916, 15494, 411, 341, 51704], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 176, "seek": 42380, "start": 450.6, "end": 452.48, "text": " to demonstrate the nature of the task,", "tokens": [51704, 281, 11698, 264, 3687, 295, 264, 5633, 11, 51798], "temperature": 0.0, "avg_logprob": -0.14703875072931838, "compression_ratio": 1.689795918367347, "no_speech_prob": 0.0029426508117467165}, {"id": 177, "seek": 45248, "start": 452.48, "end": 455.08000000000004, "text": " to demonstrate what you're supposed to do with your inputs.", "tokens": [50364, 281, 11698, 437, 291, 434, 3442, 281, 360, 365, 428, 15743, 13, 50494], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 178, "seek": 45248, "start": 455.08000000000004, "end": 459.16, "text": " And then you get a new test input.", "tokens": [50494, 400, 550, 291, 483, 257, 777, 1500, 4846, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 179, "seek": 45248, "start": 459.16, "end": 463.0, "text": " And your job is to produce the corresponding test outputs.", "tokens": [50698, 400, 428, 1691, 307, 281, 5258, 264, 11760, 1500, 23930, 13, 50890], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 180, "seek": 45248, "start": 463.0, "end": 464.92, "text": " You look at the demonstration pairs.", "tokens": [50890, 509, 574, 412, 264, 16520, 15494, 13, 50986], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 181, "seek": 45248, "start": 464.92, "end": 468.64000000000004, "text": " And from that, you figure out what you're supposed to do.", "tokens": [50986, 400, 490, 300, 11, 291, 2573, 484, 437, 291, 434, 3442, 281, 360, 13, 51172], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 182, "seek": 45248, "start": 468.64000000000004, "end": 471.24, "text": " And you show that you've understood it on this new test", "tokens": [51172, 400, 291, 855, 300, 291, 600, 7320, 309, 322, 341, 777, 1500, 51302], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 183, "seek": 45248, "start": 471.24, "end": 472.8, "text": " pair.", "tokens": [51302, 6119, 13, 51380], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 184, "seek": 45248, "start": 472.8, "end": 477.96000000000004, "text": " And importantly, in order to the knowledge basis", "tokens": [51380, 400, 8906, 11, 294, 1668, 281, 264, 3601, 5143, 51638], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 185, "seek": 45248, "start": 477.96000000000004, "end": 481.64000000000004, "text": " that you need, in order to approach these challenges,", "tokens": [51638, 300, 291, 643, 11, 294, 1668, 281, 3109, 613, 4759, 11, 51822], "temperature": 0.0, "avg_logprob": -0.1477798301482869, "compression_ratio": 1.8274336283185841, "no_speech_prob": 0.0012779930839315057}, {"id": 186, "seek": 48164, "start": 481.64, "end": 483.4, "text": " is you just need core knowledge.", "tokens": [50364, 307, 291, 445, 643, 4965, 3601, 13, 50452], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 187, "seek": 48164, "start": 483.4, "end": 486.2, "text": " And core knowledge is basically the knowledge", "tokens": [50452, 400, 4965, 3601, 307, 1936, 264, 3601, 50592], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 188, "seek": 48164, "start": 486.2, "end": 490.76, "text": " of what makes an object, basic counting, basic geometry,", "tokens": [50592, 295, 437, 1669, 364, 2657, 11, 3875, 13251, 11, 3875, 18426, 11, 50820], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 189, "seek": 48164, "start": 490.76, "end": 493.4, "text": " topology, symmetries, that sort of thing.", "tokens": [50820, 1192, 1793, 11, 14232, 302, 2244, 11, 300, 1333, 295, 551, 13, 50952], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 190, "seek": 48164, "start": 493.4, "end": 495.36, "text": " So extremely basic knowledge.", "tokens": [50952, 407, 4664, 3875, 3601, 13, 51050], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 191, "seek": 48164, "start": 495.36, "end": 497.64, "text": " LLMs for sure possess such knowledge.", "tokens": [51050, 441, 43, 26386, 337, 988, 17490, 1270, 3601, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 192, "seek": 48164, "start": 497.64, "end": 501.47999999999996, "text": " Any child possesses such knowledge.", "tokens": [51164, 2639, 1440, 17490, 279, 1270, 3601, 13, 51356], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 193, "seek": 48164, "start": 501.47999999999996, "end": 504.8, "text": " And what's really interesting is that each puzzle is new.", "tokens": [51356, 400, 437, 311, 534, 1880, 307, 300, 1184, 12805, 307, 777, 13, 51522], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 194, "seek": 48164, "start": 504.8, "end": 506.03999999999996, "text": " So it's not something that you're", "tokens": [51522, 407, 309, 311, 406, 746, 300, 291, 434, 51584], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 195, "seek": 48164, "start": 506.03999999999996, "end": 511.0, "text": " going to find elsewhere on the internet, for instance.", "tokens": [51584, 516, 281, 915, 14517, 322, 264, 4705, 11, 337, 5197, 13, 51832], "temperature": 0.0, "avg_logprob": -0.14497411251068115, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.007474902551621199}, {"id": 196, "seek": 51100, "start": 511.04, "end": 514.36, "text": " And that means that whether it's as a human or as a machine,", "tokens": [50366, 400, 300, 1355, 300, 1968, 309, 311, 382, 257, 1952, 420, 382, 257, 3479, 11, 50532], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 197, "seek": 51100, "start": 514.36, "end": 517.52, "text": " every puzzle you have to approach it from scratch.", "tokens": [50532, 633, 12805, 291, 362, 281, 3109, 309, 490, 8459, 13, 50690], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 198, "seek": 51100, "start": 517.52, "end": 519.52, "text": " You have to actually reason your way through it.", "tokens": [50690, 509, 362, 281, 767, 1778, 428, 636, 807, 309, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 199, "seek": 51100, "start": 519.52, "end": 523.0, "text": " You cannot just fetch the response from your memory.", "tokens": [50790, 509, 2644, 445, 23673, 264, 4134, 490, 428, 4675, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 200, "seek": 51100, "start": 523.0, "end": 527.64, "text": " So the core knowledge, one contention here", "tokens": [50964, 407, 264, 4965, 3601, 11, 472, 660, 1251, 510, 51196], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 201, "seek": 51100, "start": 527.64, "end": 532.6, "text": " is we are only now getting multimodal models who,", "tokens": [51196, 307, 321, 366, 787, 586, 1242, 32972, 378, 304, 5245, 567, 11, 51444], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 202, "seek": 51100, "start": 532.6, "end": 534.52, "text": " because of the data that are trained on,", "tokens": [51444, 570, 295, 264, 1412, 300, 366, 8895, 322, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 203, "seek": 51100, "start": 534.52, "end": 537.28, "text": " are trained to do spatial reasoning.", "tokens": [51540, 366, 8895, 281, 360, 23598, 21577, 13, 51678], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 204, "seek": 51100, "start": 537.28, "end": 539.28, "text": " Whereas, obviously, not only humans,", "tokens": [51678, 13813, 11, 2745, 11, 406, 787, 6255, 11, 51778], "temperature": 0.0, "avg_logprob": -0.1371486317027699, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0010645532747730613}, {"id": 205, "seek": 53928, "start": 539.28, "end": 541.76, "text": " but for billions of years of revolution,", "tokens": [50364, 457, 337, 17375, 295, 924, 295, 8894, 11, 50488], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 206, "seek": 53928, "start": 541.76, "end": 544.1999999999999, "text": " we've had our ancestors have had to learn", "tokens": [50488, 321, 600, 632, 527, 18069, 362, 632, 281, 1466, 50610], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 207, "seek": 53928, "start": 544.1999999999999, "end": 548.6, "text": " how to understand abstract, physical, and spatial properties", "tokens": [50610, 577, 281, 1223, 12649, 11, 4001, 11, 293, 23598, 7221, 50830], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 208, "seek": 53928, "start": 548.6, "end": 550.6, "text": " and recognize the patterns there.", "tokens": [50830, 293, 5521, 264, 8294, 456, 13, 50930], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 209, "seek": 53928, "start": 550.6, "end": 554.92, "text": " And so one view would be, in the next year,", "tokens": [50930, 400, 370, 472, 1910, 576, 312, 11, 294, 264, 958, 1064, 11, 51146], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 210, "seek": 53928, "start": 554.92, "end": 557.4, "text": " as we gain models that are multimodal native,", "tokens": [51146, 382, 321, 6052, 5245, 300, 366, 32972, 378, 304, 8470, 11, 51270], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 211, "seek": 53928, "start": 557.4, "end": 560.8399999999999, "text": " that isn't just a second class that is an add-on,", "tokens": [51270, 300, 1943, 380, 445, 257, 1150, 1508, 300, 307, 364, 909, 12, 266, 11, 51442], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 212, "seek": 53928, "start": 560.8399999999999, "end": 563.6, "text": " but the multimodal capability is a priority.", "tokens": [51442, 457, 264, 32972, 378, 304, 13759, 307, 257, 9365, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 213, "seek": 53928, "start": 563.6, "end": 566.6, "text": " That it will understand these kinds of patterns", "tokens": [51580, 663, 309, 486, 1223, 613, 3685, 295, 8294, 51730], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 214, "seek": 53928, "start": 566.6, "end": 568.48, "text": " because that's something we see natively.", "tokens": [51730, 570, 300, 311, 746, 321, 536, 8470, 356, 13, 51824], "temperature": 0.0, "avg_logprob": -0.14276419567460774, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00023779782350175083}, {"id": 215, "seek": 56848, "start": 568.48, "end": 571.8000000000001, "text": " Whereas, right now, what Arc sees is some JSON string", "tokens": [50364, 13813, 11, 558, 586, 11, 437, 21727, 8194, 307, 512, 31828, 6798, 50530], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 216, "seek": 56848, "start": 571.8000000000001, "end": 575.6, "text": " of 100100, and it's supposed to recognize a pattern there.", "tokens": [50530, 295, 2319, 6879, 11, 293, 309, 311, 3442, 281, 5521, 257, 5102, 456, 13, 50720], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 217, "seek": 56848, "start": 575.6, "end": 579.72, "text": " And even if you showed a sequence of these kinds of numbers,", "tokens": [50720, 400, 754, 498, 291, 4712, 257, 8310, 295, 613, 3685, 295, 3547, 11, 50926], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 218, "seek": 56848, "start": 579.72, "end": 581.6, "text": " it would have a challenge making sense", "tokens": [50926, 309, 576, 362, 257, 3430, 1455, 2020, 51020], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 219, "seek": 56848, "start": 581.6, "end": 584.24, "text": " of what kind of question you're asking it.", "tokens": [51020, 295, 437, 733, 295, 1168, 291, 434, 3365, 309, 13, 51152], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 220, "seek": 56848, "start": 584.24, "end": 586.08, "text": " So why want it to be the case that,", "tokens": [51152, 407, 983, 528, 309, 281, 312, 264, 1389, 300, 11, 51244], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 221, "seek": 56848, "start": 586.08, "end": 587.48, "text": " as soon as we get multimodal models,", "tokens": [51244, 382, 2321, 382, 321, 483, 32972, 378, 304, 5245, 11, 51314], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 222, "seek": 56848, "start": 587.48, "end": 589.44, "text": " which we're on the path to unlock right now,", "tokens": [51314, 597, 321, 434, 322, 264, 3100, 281, 11634, 558, 586, 11, 51412], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 223, "seek": 56848, "start": 589.44, "end": 590.36, "text": " they're going to be so much better", "tokens": [51412, 436, 434, 516, 281, 312, 370, 709, 1101, 51458], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 224, "seek": 56848, "start": 590.36, "end": 592.0, "text": " at Arc-type spatial reasoning?", "tokens": [51458, 412, 21727, 12, 20467, 23598, 21577, 30, 51540], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 225, "seek": 56848, "start": 592.0, "end": 593.4, "text": " That's an incredibly cool question,", "tokens": [51540, 663, 311, 364, 6252, 1627, 1168, 11, 51610], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 226, "seek": 56848, "start": 593.4, "end": 594.8000000000001, "text": " so I guess we're going to see the answer", "tokens": [51610, 370, 286, 2041, 321, 434, 516, 281, 536, 264, 1867, 51680], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 227, "seek": 56848, "start": 594.8000000000001, "end": 595.9200000000001, "text": " within a few months.", "tokens": [51680, 1951, 257, 1326, 2493, 13, 51736], "temperature": 0.0, "avg_logprob": -0.18974488659908897, "compression_ratio": 1.6371951219512195, "no_speech_prob": 0.0004877941682934761}, {"id": 228, "seek": 59592, "start": 595.92, "end": 599.16, "text": " But my answer to that is Arc grids,", "tokens": [50364, 583, 452, 1867, 281, 300, 307, 21727, 677, 3742, 11, 50526], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 229, "seek": 59592, "start": 599.16, "end": 602.24, "text": " they're just discrete 2D grids of symbols.", "tokens": [50526, 436, 434, 445, 27706, 568, 35, 677, 3742, 295, 16944, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 230, "seek": 59592, "start": 602.24, "end": 605.28, "text": " They're pretty small, like it's not like...", "tokens": [50680, 814, 434, 1238, 1359, 11, 411, 309, 311, 406, 411, 485, 50832], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 231, "seek": 59592, "start": 605.28, "end": 608.28, "text": " If you flatten an image as a sequence of pixels,", "tokens": [50832, 759, 291, 24183, 364, 3256, 382, 257, 8310, 295, 18668, 11, 50982], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 232, "seek": 59592, "start": 608.28, "end": 610.1999999999999, "text": " for instance, then you get something", "tokens": [50982, 337, 5197, 11, 550, 291, 483, 746, 51078], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 233, "seek": 59592, "start": 610.1999999999999, "end": 612.12, "text": " that's actually very, very difficult to parse.", "tokens": [51078, 300, 311, 767, 588, 11, 588, 2252, 281, 48377, 13, 51174], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 234, "seek": 59592, "start": 612.12, "end": 615.1999999999999, "text": " But that's not true for Arc because the grids are very small.", "tokens": [51174, 583, 300, 311, 406, 2074, 337, 21727, 570, 264, 677, 3742, 366, 588, 1359, 13, 51328], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 235, "seek": 59592, "start": 615.1999999999999, "end": 617.12, "text": " You only have 10 possible symbols.", "tokens": [51328, 509, 787, 362, 1266, 1944, 16944, 13, 51424], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 236, "seek": 59592, "start": 617.12, "end": 618.8399999999999, "text": " So there's these 2D grids that are actually", "tokens": [51424, 407, 456, 311, 613, 568, 35, 677, 3742, 300, 366, 767, 51510], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 237, "seek": 59592, "start": 618.8399999999999, "end": 621.36, "text": " very easy to flatten as sequences.", "tokens": [51510, 588, 1858, 281, 24183, 382, 22978, 13, 51636], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 238, "seek": 59592, "start": 621.36, "end": 623.48, "text": " And transformers, LLMs, they're very good", "tokens": [51636, 400, 4088, 433, 11, 441, 43, 26386, 11, 436, 434, 588, 665, 51742], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 239, "seek": 59592, "start": 623.48, "end": 624.56, "text": " at processing the sequences.", "tokens": [51742, 412, 9007, 264, 22978, 13, 51796], "temperature": 0.0, "avg_logprob": -0.16735725136069984, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.0010604916606098413}, {"id": 240, "seek": 62456, "start": 624.56, "end": 629.2399999999999, "text": " In fact, you can show that LLMs do fine", "tokens": [50364, 682, 1186, 11, 291, 393, 855, 300, 441, 43, 26386, 360, 2489, 50598], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 241, "seek": 62456, "start": 629.2399999999999, "end": 632.2399999999999, "text": " with processing Arc-like data", "tokens": [50598, 365, 9007, 21727, 12, 4092, 1412, 50748], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 242, "seek": 62456, "start": 632.2399999999999, "end": 637.2399999999999, "text": " by simply fine-tuning LLMs on some subsets of the tasks", "tokens": [50748, 538, 2935, 2489, 12, 83, 37726, 441, 43, 26386, 322, 512, 2090, 1385, 295, 264, 9608, 50998], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 243, "seek": 62456, "start": 638.4399999999999, "end": 642.16, "text": " and then trying to test it on small variations", "tokens": [51058, 293, 550, 1382, 281, 1500, 309, 322, 1359, 17840, 51244], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 244, "seek": 62456, "start": 642.16, "end": 643.2399999999999, "text": " of these tasks.", "tokens": [51244, 295, 613, 9608, 13, 51298], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 245, "seek": 62456, "start": 643.2399999999999, "end": 646.2399999999999, "text": " And you see that, yeah, the LLMs can encode", "tokens": [51298, 400, 291, 536, 300, 11, 1338, 11, 264, 441, 43, 26386, 393, 2058, 1429, 51448], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 246, "seek": 62456, "start": 646.2399999999999, "end": 649.4399999999999, "text": " just fine solution programs for tasks", "tokens": [51448, 445, 2489, 3827, 4268, 337, 9608, 51608], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 247, "seek": 62456, "start": 649.4399999999999, "end": 650.8, "text": " that they've seen before.", "tokens": [51608, 300, 436, 600, 1612, 949, 13, 51676], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 248, "seek": 62456, "start": 650.8, "end": 652.56, "text": " So it does not really have a problem", "tokens": [51676, 407, 309, 775, 406, 534, 362, 257, 1154, 51764], "temperature": 0.0, "avg_logprob": -0.13346794808265006, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0005027260631322861}, {"id": 249, "seek": 65256, "start": 652.56, "end": 657.56, "text": " parsing the input or figuring out the program.", "tokens": [50364, 21156, 278, 264, 4846, 420, 15213, 484, 264, 1461, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 250, "seek": 65256, "start": 657.64, "end": 661.4, "text": " The reason why LLMs don't do well on Arc", "tokens": [50618, 440, 1778, 983, 441, 43, 26386, 500, 380, 360, 731, 322, 21727, 50806], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 251, "seek": 65256, "start": 661.4, "end": 664.64, "text": " is really just the unfamiliarity aspect.", "tokens": [50806, 307, 534, 445, 264, 29415, 507, 4171, 13, 50968], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 252, "seek": 65256, "start": 664.64, "end": 667.56, "text": " The fact that each new task is different", "tokens": [50968, 440, 1186, 300, 1184, 777, 5633, 307, 819, 51114], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 253, "seek": 65256, "start": 667.56, "end": 669.2399999999999, "text": " from every other task.", "tokens": [51114, 490, 633, 661, 5633, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 254, "seek": 65256, "start": 669.2399999999999, "end": 671.88, "text": " You cannot, basically, you cannot memorize", "tokens": [51198, 509, 2644, 11, 1936, 11, 291, 2644, 27478, 51330], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 255, "seek": 65256, "start": 671.88, "end": 673.8399999999999, "text": " the solution programs in advance.", "tokens": [51330, 264, 3827, 4268, 294, 7295, 13, 51428], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 256, "seek": 65256, "start": 673.8399999999999, "end": 676.64, "text": " You have to synthesize a new solution program", "tokens": [51428, 509, 362, 281, 26617, 1125, 257, 777, 3827, 1461, 51568], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 257, "seek": 65256, "start": 676.64, "end": 678.4799999999999, "text": " on the fly for each new task.", "tokens": [51568, 322, 264, 3603, 337, 1184, 777, 5633, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 258, "seek": 65256, "start": 678.4799999999999, "end": 680.8, "text": " And that's really what LLMs are struggling with.", "tokens": [51660, 400, 300, 311, 534, 437, 441, 43, 26386, 366, 9314, 365, 13, 51776], "temperature": 0.0, "avg_logprob": -0.13564780906394677, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002204886404797435}, {"id": 259, "seek": 68080, "start": 680.8399999999999, "end": 682.4799999999999, "text": " So before I do more devil's advocate,", "tokens": [50366, 407, 949, 286, 360, 544, 13297, 311, 14608, 11, 50448], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 260, "seek": 68080, "start": 682.4799999999999, "end": 684.1999999999999, "text": " I just want to step back and explain", "tokens": [50448, 286, 445, 528, 281, 1823, 646, 293, 2903, 50534], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 261, "seek": 68080, "start": 684.1999999999999, "end": 687.24, "text": " why I'm especially interested in having this conversation.", "tokens": [50534, 983, 286, 478, 2318, 3102, 294, 1419, 341, 3761, 13, 50686], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 262, "seek": 68080, "start": 687.24, "end": 689.4799999999999, "text": " And obviously the million dollar Arc prize,", "tokens": [50686, 400, 2745, 264, 2459, 7241, 21727, 12818, 11, 50798], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 263, "seek": 68080, "start": 689.4799999999999, "end": 691.92, "text": " I'm excited to actually play with it myself.", "tokens": [50798, 286, 478, 2919, 281, 767, 862, 365, 309, 2059, 13, 50920], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 264, "seek": 68080, "start": 691.92, "end": 695.4, "text": " And hopefully the Vesuvius challenge,", "tokens": [50920, 400, 4696, 264, 691, 279, 9350, 4872, 3430, 11, 51094], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 265, "seek": 68080, "start": 695.4, "end": 699.76, "text": " which was Nat Friedman's prize for solving decoding scrolls,", "tokens": [51094, 597, 390, 6821, 17605, 1601, 311, 12818, 337, 12606, 979, 8616, 11369, 82, 11, 51312], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 266, "seek": 68080, "start": 699.76, "end": 702.0, "text": " the winner of that, decoding the scrolls from", "tokens": [51312, 264, 8507, 295, 300, 11, 979, 8616, 264, 11369, 82, 490, 51424], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 267, "seek": 68080, "start": 702.0, "end": 703.5999999999999, "text": " that were buried in the volcanoes", "tokens": [51424, 300, 645, 14101, 294, 264, 48221, 51504], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 268, "seek": 68080, "start": 703.5999999999999, "end": 706.76, "text": " in the Herculaneum library that was solved", "tokens": [51504, 294, 264, 3204, 2444, 1929, 449, 6405, 300, 390, 13041, 51662], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 269, "seek": 68080, "start": 706.76, "end": 708.68, "text": " by a 22 year old who was listening", "tokens": [51662, 538, 257, 5853, 1064, 1331, 567, 390, 4764, 51758], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 270, "seek": 68080, "start": 708.68, "end": 709.8399999999999, "text": " to the podcast, Luke Farator.", "tokens": [51758, 281, 264, 7367, 11, 13044, 9067, 1639, 13, 51816], "temperature": 0.0, "avg_logprob": -0.18745483621193545, "compression_ratio": 1.60062893081761, "no_speech_prob": 0.0034826346673071384}, {"id": 271, "seek": 70984, "start": 709.84, "end": 711.8000000000001, "text": " So hopefully somebody listening will find", "tokens": [50364, 407, 4696, 2618, 4764, 486, 915, 50462], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 272, "seek": 70984, "start": 711.8000000000001, "end": 714.24, "text": " this challenge intriguing and find a solution.", "tokens": [50462, 341, 3430, 32503, 293, 915, 257, 3827, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 273, "seek": 70984, "start": 714.24, "end": 718.2, "text": " So I'm, and the reason I've had on recently", "tokens": [50584, 407, 286, 478, 11, 293, 264, 1778, 286, 600, 632, 322, 3938, 50782], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 274, "seek": 70984, "start": 718.2, "end": 721.6800000000001, "text": " a lot of people who are bullish on LLMs", "tokens": [50782, 257, 688, 295, 561, 567, 366, 38692, 322, 441, 43, 26386, 50956], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 275, "seek": 70984, "start": 721.6800000000001, "end": 723.5600000000001, "text": " and I've had discussions with them", "tokens": [50956, 293, 286, 600, 632, 11088, 365, 552, 51050], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 276, "seek": 70984, "start": 723.5600000000001, "end": 725.64, "text": " before interviewing you about how do we explain the fact", "tokens": [51050, 949, 26524, 291, 466, 577, 360, 321, 2903, 264, 1186, 51154], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 277, "seek": 70984, "start": 725.64, "end": 727.44, "text": " that LLMs don't seem to be natively performing", "tokens": [51154, 300, 441, 43, 26386, 500, 380, 1643, 281, 312, 8470, 356, 10205, 51244], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 278, "seek": 70984, "start": 727.44, "end": 728.8000000000001, "text": " that well on Arc.", "tokens": [51244, 300, 731, 322, 21727, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 279, "seek": 70984, "start": 728.8000000000001, "end": 732.44, "text": " And I found their explanations somewhat contrived", "tokens": [51312, 400, 286, 1352, 641, 28708, 8344, 660, 470, 937, 51494], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 280, "seek": 70984, "start": 732.44, "end": 735.12, "text": " and I'll try out some of the reasons on you.", "tokens": [51494, 293, 286, 603, 853, 484, 512, 295, 264, 4112, 322, 291, 13, 51628], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 281, "seek": 70984, "start": 735.12, "end": 737.64, "text": " But it is actually an intriguing fact", "tokens": [51628, 583, 309, 307, 767, 364, 32503, 1186, 51754], "temperature": 0.0, "avg_logprob": -0.10487430076288982, "compression_ratio": 1.65, "no_speech_prob": 0.00020986875460948795}, {"id": 282, "seek": 73764, "start": 737.84, "end": 738.96, "text": " that they actually, these are,", "tokens": [50374, 300, 436, 767, 11, 613, 366, 11, 50430], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 283, "seek": 73764, "start": 738.96, "end": 741.12, "text": " some of these problems are relatively straightforward", "tokens": [50430, 512, 295, 613, 2740, 366, 7226, 15325, 50538], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 284, "seek": 73764, "start": 741.12, "end": 742.4, "text": " for humans to understand.", "tokens": [50538, 337, 6255, 281, 1223, 13, 50602], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 285, "seek": 73764, "start": 742.4, "end": 744.36, "text": " And they do struggle with them", "tokens": [50602, 400, 436, 360, 7799, 365, 552, 50700], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 286, "seek": 73764, "start": 744.36, "end": 745.92, "text": " if you just input them natively.", "tokens": [50700, 498, 291, 445, 4846, 552, 8470, 356, 13, 50778], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 287, "seek": 73764, "start": 745.92, "end": 747.64, "text": " All of them are very easy for humans.", "tokens": [50778, 1057, 295, 552, 366, 588, 1858, 337, 6255, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 288, "seek": 73764, "start": 747.64, "end": 749.76, "text": " Like any smart human should be able", "tokens": [50864, 1743, 604, 4069, 1952, 820, 312, 1075, 50970], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 289, "seek": 73764, "start": 749.76, "end": 752.56, "text": " to do 90%, 95% on Arc.", "tokens": [50970, 281, 360, 4289, 8923, 13420, 4, 322, 21727, 13, 51110], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 290, "seek": 73764, "start": 752.56, "end": 753.4, "text": " Smart human.", "tokens": [51110, 12923, 1952, 13, 51152], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 291, "seek": 73764, "start": 753.4, "end": 755.36, "text": " A smart human, but even a five year old.", "tokens": [51152, 316, 4069, 1952, 11, 457, 754, 257, 1732, 1064, 1331, 13, 51250], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 292, "seek": 73764, "start": 755.36, "end": 757.3199999999999, "text": " So with very, very little knowledge,", "tokens": [51250, 407, 365, 588, 11, 588, 707, 3601, 11, 51348], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 293, "seek": 73764, "start": 757.3199999999999, "end": 760.28, "text": " they could definitely do over 50%.", "tokens": [51348, 436, 727, 2138, 360, 670, 2625, 6856, 51496], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 294, "seek": 73764, "start": 760.28, "end": 764.56, "text": " So let's talk about that because you,", "tokens": [51496, 407, 718, 311, 751, 466, 300, 570, 291, 11, 51710], "temperature": 0.0, "avg_logprob": -0.21556077464934317, "compression_ratio": 1.6353383458646618, "no_speech_prob": 0.004606245085597038}, {"id": 295, "seek": 76456, "start": 765.4799999999999, "end": 768.4799999999999, "text": " I agree that smart humans will do very well on this test,", "tokens": [50410, 286, 3986, 300, 4069, 6255, 486, 360, 588, 731, 322, 341, 1500, 11, 50560], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 296, "seek": 76456, "start": 768.4799999999999, "end": 773.4799999999999, "text": " but the average human will probably do mediocre.", "tokens": [50560, 457, 264, 4274, 1952, 486, 1391, 360, 45415, 13, 50810], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 297, "seek": 76456, "start": 773.68, "end": 774.64, "text": " Not really.", "tokens": [50820, 1726, 534, 13, 50868], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 298, "seek": 76456, "start": 774.64, "end": 776.92, "text": " So we actually tried with average humans,", "tokens": [50868, 407, 321, 767, 3031, 365, 4274, 6255, 11, 50982], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 299, "seek": 76456, "start": 776.92, "end": 778.28, "text": " the score about 85.", "tokens": [50982, 264, 6175, 466, 14695, 13, 51050], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 300, "seek": 76456, "start": 778.28, "end": 780.64, "text": " That was with Amazon Mechanical Turk workers, right?", "tokens": [51050, 663, 390, 365, 6795, 30175, 804, 15714, 5600, 11, 558, 30, 51168], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 301, "seek": 76456, "start": 780.64, "end": 782.8, "text": " I honestly don't know the demographic profile", "tokens": [51168, 286, 6095, 500, 380, 458, 264, 26331, 7964, 51276], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 302, "seek": 76456, "start": 782.8, "end": 784.04, "text": " of Amazon Mechanical Turk workers,", "tokens": [51276, 295, 6795, 30175, 804, 15714, 5600, 11, 51338], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 303, "seek": 76456, "start": 784.04, "end": 787.3199999999999, "text": " but imagine just interacting with the platform", "tokens": [51338, 457, 3811, 445, 18017, 365, 264, 3663, 51502], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 304, "seek": 76456, "start": 787.3199999999999, "end": 789.0799999999999, "text": " that Amazon has set up to do remote work.", "tokens": [51502, 300, 6795, 575, 992, 493, 281, 360, 8607, 589, 13, 51590], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 305, "seek": 76456, "start": 789.0799999999999, "end": 791.92, "text": " That's not the median human across the planet, I'm guessing.", "tokens": [51590, 663, 311, 406, 264, 26779, 1952, 2108, 264, 5054, 11, 286, 478, 17939, 13, 51732], "temperature": 0.0, "avg_logprob": -0.21151427260967864, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.002980609191581607}, {"id": 306, "seek": 79192, "start": 792.12, "end": 794.68, "text": " I mean, the broader point here being that,", "tokens": [50374, 286, 914, 11, 264, 13227, 935, 510, 885, 300, 11, 50502], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 307, "seek": 79192, "start": 794.68, "end": 797.1999999999999, "text": " so we see the spectrum in humans", "tokens": [50502, 370, 321, 536, 264, 11143, 294, 6255, 50628], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 308, "seek": 79192, "start": 797.1999999999999, "end": 800.16, "text": " where humans obviously have AGI,", "tokens": [50628, 689, 6255, 2745, 362, 316, 26252, 11, 50776], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 309, "seek": 79192, "start": 800.16, "end": 802.0, "text": " but even within humans, you see a spectrum", "tokens": [50776, 457, 754, 1951, 6255, 11, 291, 536, 257, 11143, 50868], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 310, "seek": 79192, "start": 802.0, "end": 804.1999999999999, "text": " where some people are relatively dumber", "tokens": [50868, 689, 512, 561, 366, 7226, 274, 4182, 50978], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 311, "seek": 79192, "start": 804.1999999999999, "end": 807.92, "text": " and they'll do perform work on IQ like tests.", "tokens": [50978, 293, 436, 603, 360, 2042, 589, 322, 28921, 411, 6921, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 312, "seek": 79192, "start": 807.92, "end": 809.4799999999999, "text": " For example, Raven's progressive matrices.", "tokens": [51164, 1171, 1365, 11, 28956, 311, 16131, 32284, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 313, "seek": 79192, "start": 809.4799999999999, "end": 811.68, "text": " If you look at how the average person performs on that", "tokens": [51242, 759, 291, 574, 412, 577, 264, 4274, 954, 26213, 322, 300, 51352], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 314, "seek": 79192, "start": 811.68, "end": 812.92, "text": " and you look at the quick kind of questions", "tokens": [51352, 293, 291, 574, 412, 264, 1702, 733, 295, 1651, 51414], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 315, "seek": 79192, "start": 812.92, "end": 814.4, "text": " that is this sort of midtermists,", "tokens": [51414, 300, 307, 341, 1333, 295, 2062, 7039, 1751, 11, 51488], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 316, "seek": 79192, "start": 814.4, "end": 815.36, "text": " half of people will get it right,", "tokens": [51488, 1922, 295, 561, 486, 483, 309, 558, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 317, "seek": 79192, "start": 815.36, "end": 816.36, "text": " half of people will get it wrong.", "tokens": [51536, 1922, 295, 561, 486, 483, 309, 2085, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 318, "seek": 79192, "start": 816.36, "end": 817.8, "text": " Some of them are like pretty trivial.", "tokens": [51586, 2188, 295, 552, 366, 411, 1238, 26703, 13, 51658], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 319, "seek": 79192, "start": 817.8, "end": 820.56, "text": " For us, we might think like this was kind of trivial.", "tokens": [51658, 1171, 505, 11, 321, 1062, 519, 411, 341, 390, 733, 295, 26703, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1568155227168914, "compression_ratio": 1.790625, "no_speech_prob": 0.005382768344134092}, {"id": 320, "seek": 82056, "start": 820.56, "end": 822.1999999999999, "text": " And so humans have AGI,", "tokens": [50364, 400, 370, 6255, 362, 316, 26252, 11, 50446], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 321, "seek": 82056, "start": 822.1999999999999, "end": 825.1999999999999, "text": " but from relatively small tweaks,", "tokens": [50446, 457, 490, 7226, 1359, 46664, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 322, "seek": 82056, "start": 825.1999999999999, "end": 827.4399999999999, "text": " you can go from somebody who misses", "tokens": [50596, 291, 393, 352, 490, 2618, 567, 29394, 50708], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 323, "seek": 82056, "start": 827.4399999999999, "end": 828.92, "text": " these kinds of basic IQ test questions", "tokens": [50708, 613, 3685, 295, 3875, 28921, 1500, 1651, 50782], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 324, "seek": 82056, "start": 828.92, "end": 830.2399999999999, "text": " to somebody who gets them all right,", "tokens": [50782, 281, 2618, 567, 2170, 552, 439, 558, 11, 50848], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 325, "seek": 82056, "start": 830.2399999999999, "end": 831.56, "text": " which suggests that actually,", "tokens": [50848, 597, 13409, 300, 767, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 326, "seek": 82056, "start": 831.56, "end": 834.4799999999999, "text": " if these models are doing natively,", "tokens": [50914, 498, 613, 5245, 366, 884, 8470, 356, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 327, "seek": 82056, "start": 834.4799999999999, "end": 836.2399999999999, "text": " we'll talk about some of the previous performances", "tokens": [51060, 321, 603, 751, 466, 512, 295, 264, 3894, 16087, 51148], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 328, "seek": 82056, "start": 836.2399999999999, "end": 837.1199999999999, "text": " that people have tried with these models,", "tokens": [51148, 300, 561, 362, 3031, 365, 613, 5245, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 329, "seek": 82056, "start": 837.1199999999999, "end": 839.0, "text": " but somebody with a Jack Cole", "tokens": [51192, 457, 2618, 365, 257, 4718, 20394, 51286], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 330, "seek": 82056, "start": 839.0, "end": 842.8, "text": " with a 240 million parameter model got 35%.", "tokens": [51286, 365, 257, 26837, 2459, 13075, 2316, 658, 6976, 6856, 51476], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 331, "seek": 82056, "start": 842.8, "end": 845.0799999999999, "text": " Doesn't that suggest that they're on this spectrum", "tokens": [51476, 12955, 380, 300, 3402, 300, 436, 434, 322, 341, 11143, 51590], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 332, "seek": 82056, "start": 845.0799999999999, "end": 846.4799999999999, "text": " that clearly exists within humans", "tokens": [51590, 300, 4448, 8198, 1951, 6255, 51660], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 333, "seek": 82056, "start": 846.4799999999999, "end": 848.56, "text": " and they're gonna be saturated at pretty soon?", "tokens": [51660, 293, 436, 434, 799, 312, 25408, 412, 1238, 2321, 30, 51764], "temperature": 0.0, "avg_logprob": -0.12898182339138456, "compression_ratio": 1.6898734177215189, "no_speech_prob": 0.0004044488014187664}, {"id": 334, "seek": 84856, "start": 848.56, "end": 851.3599999999999, "text": " Yeah, so that's a bunch of interesting points here.", "tokens": [50364, 865, 11, 370, 300, 311, 257, 3840, 295, 1880, 2793, 510, 13, 50504], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 335, "seek": 84856, "start": 851.3599999999999, "end": 856.3599999999999, "text": " So there is indeed a branch of LLM approaches", "tokens": [50504, 407, 456, 307, 6451, 257, 9819, 295, 441, 43, 44, 11587, 50754], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 336, "seek": 84856, "start": 856.8, "end": 859.9599999999999, "text": " suspended by Jack Cole that are doing quite well,", "tokens": [50776, 23437, 538, 4718, 20394, 300, 366, 884, 1596, 731, 11, 50934], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 337, "seek": 84856, "start": 859.9599999999999, "end": 863.0, "text": " that are in fact a state of the art.", "tokens": [50934, 300, 366, 294, 1186, 257, 1785, 295, 264, 1523, 13, 51086], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 338, "seek": 84856, "start": 863.0, "end": 865.52, "text": " But you have to look at what's going on there.", "tokens": [51086, 583, 291, 362, 281, 574, 412, 437, 311, 516, 322, 456, 13, 51212], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 339, "seek": 84856, "start": 865.52, "end": 866.3599999999999, "text": " So there are two things.", "tokens": [51212, 407, 456, 366, 732, 721, 13, 51254], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 340, "seek": 84856, "start": 866.3599999999999, "end": 869.4799999999999, "text": " The first thing is that to guess these numbers,", "tokens": [51254, 440, 700, 551, 307, 300, 281, 2041, 613, 3547, 11, 51410], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 341, "seek": 84856, "start": 869.4799999999999, "end": 871.8399999999999, "text": " you need to pre-train your LLM", "tokens": [51410, 291, 643, 281, 659, 12, 83, 7146, 428, 441, 43, 44, 51528], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 342, "seek": 84856, "start": 871.8399999999999, "end": 874.52, "text": " on millions of generated art tasks.", "tokens": [51528, 322, 6803, 295, 10833, 1523, 9608, 13, 51662], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 343, "seek": 84856, "start": 874.52, "end": 877.8399999999999, "text": " And of course, if you compare that to a five-year-old child", "tokens": [51662, 400, 295, 1164, 11, 498, 291, 6794, 300, 281, 257, 1732, 12, 5294, 12, 2641, 1440, 51828], "temperature": 0.0, "avg_logprob": -0.11368555534543015, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0007163568516261876}, {"id": 344, "seek": 87784, "start": 877.88, "end": 879.6800000000001, "text": " looking at art for the first time,", "tokens": [50366, 1237, 412, 1523, 337, 264, 700, 565, 11, 50456], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 345, "seek": 87784, "start": 879.6800000000001, "end": 881.4, "text": " the child has never done like you did before,", "tokens": [50456, 264, 1440, 575, 1128, 1096, 411, 291, 630, 949, 11, 50542], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 346, "seek": 87784, "start": 881.4, "end": 884.0400000000001, "text": " has never seen something like an art task before.", "tokens": [50542, 575, 1128, 1612, 746, 411, 364, 1523, 5633, 949, 13, 50674], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 347, "seek": 87784, "start": 884.0400000000001, "end": 886.1600000000001, "text": " The only overlap between what they know", "tokens": [50674, 440, 787, 19959, 1296, 437, 436, 458, 50780], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 348, "seek": 87784, "start": 886.1600000000001, "end": 889.32, "text": " and what they have to do in the test is core knowledge,", "tokens": [50780, 293, 437, 436, 362, 281, 360, 294, 264, 1500, 307, 4965, 3601, 11, 50938], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 349, "seek": 87784, "start": 889.32, "end": 891.5600000000001, "text": " is knowing about like counting and objects", "tokens": [50938, 307, 5276, 466, 411, 13251, 293, 6565, 51050], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 350, "seek": 87784, "start": 891.5600000000001, "end": 893.0400000000001, "text": " and symmetries and things like that.", "tokens": [51050, 293, 14232, 302, 2244, 293, 721, 411, 300, 13, 51124], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 351, "seek": 87784, "start": 893.0400000000001, "end": 896.1600000000001, "text": " And still, they're gonna do really well", "tokens": [51124, 400, 920, 11, 436, 434, 799, 360, 534, 731, 51280], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 352, "seek": 87784, "start": 896.1600000000001, "end": 897.88, "text": " and they're gonna do much better than the LLM", "tokens": [51280, 293, 436, 434, 799, 360, 709, 1101, 813, 264, 441, 43, 44, 51366], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 353, "seek": 87784, "start": 897.88, "end": 900.5600000000001, "text": " trained on millions of similar tasks.", "tokens": [51366, 8895, 322, 6803, 295, 2531, 9608, 13, 51500], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 354, "seek": 87784, "start": 900.5600000000001, "end": 903.4000000000001, "text": " And the second thing that's something to note", "tokens": [51500, 400, 264, 1150, 551, 300, 311, 746, 281, 3637, 51642], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 355, "seek": 87784, "start": 903.4000000000001, "end": 907.36, "text": " about the Jack Cole approach is one thing", "tokens": [51642, 466, 264, 4718, 20394, 3109, 307, 472, 551, 51840], "temperature": 0.0, "avg_logprob": -0.14755975228768808, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.0009678893839009106}, {"id": 356, "seek": 90736, "start": 907.36, "end": 910.52, "text": " that's really critical to making the model work at all", "tokens": [50364, 300, 311, 534, 4924, 281, 1455, 264, 2316, 589, 412, 439, 50522], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 357, "seek": 90736, "start": 910.52, "end": 912.5600000000001, "text": " is test time fine tuning.", "tokens": [50522, 307, 1500, 565, 2489, 15164, 13, 50624], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 358, "seek": 90736, "start": 912.5600000000001, "end": 914.32, "text": " And that's something that's really missing, by the way,", "tokens": [50624, 400, 300, 311, 746, 300, 311, 534, 5361, 11, 538, 264, 636, 11, 50712], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 359, "seek": 90736, "start": 914.32, "end": 919.32, "text": " from LLM approaches right now is that, you know,", "tokens": [50712, 490, 441, 43, 44, 11587, 558, 586, 307, 300, 11, 291, 458, 11, 50962], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 360, "seek": 90736, "start": 919.36, "end": 921.6800000000001, "text": " most of the time when you're using an LLM,", "tokens": [50964, 881, 295, 264, 565, 562, 291, 434, 1228, 364, 441, 43, 44, 11, 51080], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 361, "seek": 90736, "start": 921.6800000000001, "end": 923.76, "text": " it's just doing static inference.", "tokens": [51080, 309, 311, 445, 884, 13437, 38253, 13, 51184], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 362, "seek": 90736, "start": 923.76, "end": 926.92, "text": " The model is frozen and you're just prompting it", "tokens": [51184, 440, 2316, 307, 12496, 293, 291, 434, 445, 12391, 278, 309, 51342], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 363, "seek": 90736, "start": 926.92, "end": 928.28, "text": " and then you're getting an answer.", "tokens": [51342, 293, 550, 291, 434, 1242, 364, 1867, 13, 51410], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 364, "seek": 90736, "start": 928.28, "end": 931.4, "text": " So the model is not actually learning anything on the fly.", "tokens": [51410, 407, 264, 2316, 307, 406, 767, 2539, 1340, 322, 264, 3603, 13, 51566], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 365, "seek": 90736, "start": 931.4, "end": 935.88, "text": " Its state is not adapting to the task at hand.", "tokens": [51566, 6953, 1785, 307, 406, 34942, 281, 264, 5633, 412, 1011, 13, 51790], "temperature": 0.0, "avg_logprob": -0.12939992288904867, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.0015686597907915711}, {"id": 366, "seek": 93588, "start": 936.12, "end": 938.28, "text": " What Jack Cole is actually doing is that", "tokens": [50376, 708, 4718, 20394, 307, 767, 884, 307, 300, 50484], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 367, "seek": 93588, "start": 938.28, "end": 941.96, "text": " for every test problem is on the fly,", "tokens": [50484, 337, 633, 1500, 1154, 307, 322, 264, 3603, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 368, "seek": 93588, "start": 941.96, "end": 946.96, "text": " is fine tuning a version of the LLM for that task.", "tokens": [50668, 307, 2489, 15164, 257, 3037, 295, 264, 441, 43, 44, 337, 300, 5633, 13, 50918], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 369, "seek": 93588, "start": 947.08, "end": 948.92, "text": " And that's really what's unlocking performance.", "tokens": [50924, 400, 300, 311, 534, 437, 311, 49620, 3389, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 370, "seek": 93588, "start": 948.92, "end": 951.72, "text": " If you don't do that, you get like 1%, 2%.", "tokens": [51016, 759, 291, 500, 380, 360, 300, 11, 291, 483, 411, 502, 8923, 568, 6856, 51156], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 371, "seek": 93588, "start": 951.72, "end": 955.0, "text": " So basically something completely negligible.", "tokens": [51156, 407, 1936, 746, 2584, 32570, 964, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 372, "seek": 93588, "start": 955.0, "end": 957.08, "text": " And if you do test time fine tuning", "tokens": [51320, 400, 498, 291, 360, 1500, 565, 2489, 15164, 51424], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 373, "seek": 93588, "start": 957.08, "end": 958.88, "text": " and you add a bunch of tricks on top,", "tokens": [51424, 293, 291, 909, 257, 3840, 295, 11733, 322, 1192, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 374, "seek": 93588, "start": 958.88, "end": 961.08, "text": " then you end up with interesting performance numbers.", "tokens": [51514, 550, 291, 917, 493, 365, 1880, 3389, 3547, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 375, "seek": 93588, "start": 961.08, "end": 964.24, "text": " So I think what he's doing is trying to address", "tokens": [51624, 407, 286, 519, 437, 415, 311, 884, 307, 1382, 281, 2985, 51782], "temperature": 0.0, "avg_logprob": -0.1267089055589408, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.001444097957573831}, {"id": 376, "seek": 96424, "start": 964.24, "end": 967.08, "text": " one of the key limitations of LLMs today,", "tokens": [50364, 472, 295, 264, 2141, 15705, 295, 441, 43, 26386, 965, 11, 50506], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 377, "seek": 96424, "start": 967.08, "end": 968.6800000000001, "text": " which is the lack of active inference,", "tokens": [50506, 597, 307, 264, 5011, 295, 4967, 38253, 11, 50586], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 378, "seek": 96424, "start": 968.6800000000001, "end": 971.6, "text": " is actually adding active inference to LLMs.", "tokens": [50586, 307, 767, 5127, 4967, 38253, 281, 441, 43, 26386, 13, 50732], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 379, "seek": 96424, "start": 971.6, "end": 973.36, "text": " And that's working extremely well actually.", "tokens": [50732, 400, 300, 311, 1364, 4664, 731, 767, 13, 50820], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 380, "seek": 96424, "start": 973.36, "end": 974.8, "text": " So that's fascinating to me.", "tokens": [50820, 407, 300, 311, 10343, 281, 385, 13, 50892], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 381, "seek": 96424, "start": 974.8, "end": 977.2, "text": " That there's so many interesting rabbit holes there.", "tokens": [50892, 663, 456, 311, 370, 867, 1880, 19509, 8118, 456, 13, 51012], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 382, "seek": 96424, "start": 978.36, "end": 980.12, "text": " Should I take them in sequence or deal with them all at once?", "tokens": [51070, 6454, 286, 747, 552, 294, 8310, 420, 2028, 365, 552, 439, 412, 1564, 30, 51158], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 383, "seek": 96424, "start": 980.12, "end": 981.2, "text": " Let me just start.", "tokens": [51158, 961, 385, 445, 722, 13, 51212], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 384, "seek": 96424, "start": 981.2, "end": 984.48, "text": " So the point you made about the fact", "tokens": [51212, 407, 264, 935, 291, 1027, 466, 264, 1186, 51376], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 385, "seek": 96424, "start": 984.48, "end": 986.88, "text": " that you need to unlock the adapter compute", "tokens": [51376, 300, 291, 643, 281, 11634, 264, 22860, 14722, 51496], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 386, "seek": 96424, "start": 986.88, "end": 990.64, "text": " slash test time compute, a lot of the scale maximalist,", "tokens": [51496, 17330, 1500, 565, 14722, 11, 257, 688, 295, 264, 4373, 49336, 468, 11, 51684], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 387, "seek": 96424, "start": 990.64, "end": 991.8, "text": " I think this will be interesting rabbit hole", "tokens": [51684, 286, 519, 341, 486, 312, 1880, 19509, 5458, 51742], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 388, "seek": 96424, "start": 991.8, "end": 992.72, "text": " to explore with you,", "tokens": [51742, 281, 6839, 365, 291, 11, 51788], "temperature": 0.0, "avg_logprob": -0.16214796213003305, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0008040227694436908}, {"id": 389, "seek": 99272, "start": 992.72, "end": 995.0400000000001, "text": " because a lot of the scaling maximalist", "tokens": [50364, 570, 257, 688, 295, 264, 21589, 49336, 468, 50480], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 390, "seek": 99272, "start": 995.0400000000001, "end": 997.6, "text": " have your broader perspective in the sense", "tokens": [50480, 362, 428, 13227, 4585, 294, 264, 2020, 50608], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 391, "seek": 99272, "start": 997.6, "end": 1000.4, "text": " that they think that in addition to scaling,", "tokens": [50608, 300, 436, 519, 300, 294, 4500, 281, 21589, 11, 50748], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 392, "seek": 99272, "start": 1000.4, "end": 1001.96, "text": " you need these kinds of things,", "tokens": [50748, 291, 643, 613, 3685, 295, 721, 11, 50826], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 393, "seek": 99272, "start": 1001.96, "end": 1004.08, "text": " like unlocking adaptive compute", "tokens": [50826, 411, 49620, 27912, 14722, 50932], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 394, "seek": 99272, "start": 1004.08, "end": 1007.4, "text": " or doing some sort of RL to get the system to working.", "tokens": [50932, 420, 884, 512, 1333, 295, 497, 43, 281, 483, 264, 1185, 281, 1364, 13, 51098], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 395, "seek": 99272, "start": 1007.4, "end": 1009.9200000000001, "text": " And their perspective is that this is a relatively", "tokens": [51098, 400, 641, 4585, 307, 300, 341, 307, 257, 7226, 51224], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 396, "seek": 99272, "start": 1009.9200000000001, "end": 1012.32, "text": " straightforward thing that will be added atop", "tokens": [51224, 15325, 551, 300, 486, 312, 3869, 412, 404, 51344], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 397, "seek": 99272, "start": 1012.32, "end": 1015.44, "text": " the representations that a scaled up model", "tokens": [51344, 264, 33358, 300, 257, 36039, 493, 2316, 51500], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 398, "seek": 99272, "start": 1015.44, "end": 1017.88, "text": " has greater access to.", "tokens": [51500, 575, 5044, 2105, 281, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 399, "seek": 99272, "start": 1017.88, "end": 1020.4, "text": " No, it's not just a technical detail.", "tokens": [51622, 883, 11, 309, 311, 406, 445, 257, 6191, 2607, 13, 51748], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 400, "seek": 99272, "start": 1020.4, "end": 1021.84, "text": " It's not a straightforward thing.", "tokens": [51748, 467, 311, 406, 257, 15325, 551, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1407212659347156, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.00034595458419062197}, {"id": 401, "seek": 102184, "start": 1021.84, "end": 1023.24, "text": " It is everything.", "tokens": [50364, 467, 307, 1203, 13, 50434], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 402, "seek": 102184, "start": 1023.24, "end": 1025.1200000000001, "text": " It is the important part.", "tokens": [50434, 467, 307, 264, 1021, 644, 13, 50528], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 403, "seek": 102184, "start": 1025.1200000000001, "end": 1028.8, "text": " And the scale maximalist argument,", "tokens": [50528, 400, 264, 4373, 49336, 468, 6770, 11, 50712], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 404, "seek": 102184, "start": 1028.8, "end": 1033.08, "text": " you know, it boils down to, you know,", "tokens": [50712, 291, 458, 11, 309, 35049, 760, 281, 11, 291, 458, 11, 50926], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 405, "seek": 102184, "start": 1033.08, "end": 1035.32, "text": " these people, they refer to scaling loss,", "tokens": [50926, 613, 561, 11, 436, 2864, 281, 21589, 4470, 11, 51038], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 406, "seek": 102184, "start": 1035.32, "end": 1037.44, "text": " which is this empirical relationship", "tokens": [51038, 597, 307, 341, 31886, 2480, 51144], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 407, "seek": 102184, "start": 1037.44, "end": 1039.72, "text": " that you can draw between how much compute", "tokens": [51144, 300, 291, 393, 2642, 1296, 577, 709, 14722, 51258], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 408, "seek": 102184, "start": 1039.72, "end": 1041.0, "text": " you spend on training a model", "tokens": [51258, 291, 3496, 322, 3097, 257, 2316, 51322], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 409, "seek": 102184, "start": 1041.0, "end": 1043.6000000000001, "text": " and the performance you're getting on benchmarks, right?", "tokens": [51322, 293, 264, 3389, 291, 434, 1242, 322, 43751, 11, 558, 30, 51452], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 410, "seek": 102184, "start": 1043.6000000000001, "end": 1046.32, "text": " And the key question here, of course, is,", "tokens": [51452, 400, 264, 2141, 1168, 510, 11, 295, 1164, 11, 307, 11, 51588], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 411, "seek": 102184, "start": 1046.32, "end": 1048.16, "text": " well, how do you measure performance?", "tokens": [51588, 731, 11, 577, 360, 291, 3481, 3389, 30, 51680], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 412, "seek": 102184, "start": 1048.16, "end": 1051.0, "text": " What it is that you're actually improving", "tokens": [51680, 708, 309, 307, 300, 291, 434, 767, 11470, 51822], "temperature": 0.0, "avg_logprob": -0.17475393613179524, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004222518182359636}, {"id": 413, "seek": 105100, "start": 1051.0, "end": 1053.08, "text": " by adding more compute and more data?", "tokens": [50364, 538, 5127, 544, 14722, 293, 544, 1412, 30, 50468], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 414, "seek": 105100, "start": 1053.08, "end": 1055.48, "text": " And, well, it's benchmark performance, right?", "tokens": [50468, 400, 11, 731, 11, 309, 311, 18927, 3389, 11, 558, 30, 50588], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 415, "seek": 105100, "start": 1055.48, "end": 1058.4, "text": " And the thing is, the way you measure performance", "tokens": [50588, 400, 264, 551, 307, 11, 264, 636, 291, 3481, 3389, 50734], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 416, "seek": 105100, "start": 1058.4, "end": 1061.36, "text": " is not a technical detail.", "tokens": [50734, 307, 406, 257, 6191, 2607, 13, 50882], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 417, "seek": 105100, "start": 1061.36, "end": 1066.16, "text": " It's not an afterthought because it's gonna narrow down", "tokens": [50882, 467, 311, 406, 364, 934, 43135, 570, 309, 311, 799, 9432, 760, 51122], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 418, "seek": 105100, "start": 1066.16, "end": 1067.84, "text": " the set of questions that you're asking.", "tokens": [51122, 264, 992, 295, 1651, 300, 291, 434, 3365, 13, 51206], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 419, "seek": 105100, "start": 1067.84, "end": 1070.4, "text": " And so, accordingly, it's gonna narrow down", "tokens": [51206, 400, 370, 11, 19717, 11, 309, 311, 799, 9432, 760, 51334], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 420, "seek": 105100, "start": 1070.4, "end": 1073.28, "text": " the set of answers that you're looking for.", "tokens": [51334, 264, 992, 295, 6338, 300, 291, 434, 1237, 337, 13, 51478], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 421, "seek": 105100, "start": 1073.28, "end": 1076.84, "text": " If you look at the benchmarks we're using for LMS,", "tokens": [51478, 759, 291, 574, 412, 264, 43751, 321, 434, 1228, 337, 441, 10288, 11, 51656], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 422, "seek": 105100, "start": 1076.84, "end": 1079.64, "text": " they're all memorization-based benchmarks.", "tokens": [51656, 436, 434, 439, 10560, 2144, 12, 6032, 43751, 13, 51796], "temperature": 0.0, "avg_logprob": -0.12781259218851726, "compression_ratio": 1.756, "no_speech_prob": 0.0005242477054707706}, {"id": 423, "seek": 107964, "start": 1079.64, "end": 1081.88, "text": " Like, sometimes they are literally just knowledge-based,", "tokens": [50364, 1743, 11, 2171, 436, 366, 3736, 445, 3601, 12, 6032, 11, 50476], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 424, "seek": 107964, "start": 1081.88, "end": 1083.44, "text": " like a school test.", "tokens": [50476, 411, 257, 1395, 1500, 13, 50554], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 425, "seek": 107964, "start": 1083.44, "end": 1084.76, "text": " And even if you look at the ones", "tokens": [50554, 400, 754, 498, 291, 574, 412, 264, 2306, 50620], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 426, "seek": 107964, "start": 1084.76, "end": 1088.92, "text": " that are, you know, explicitly about reasoning,", "tokens": [50620, 300, 366, 11, 291, 458, 11, 20803, 466, 21577, 11, 50828], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 427, "seek": 107964, "start": 1088.92, "end": 1090.64, "text": " you realize, if you look closely,", "tokens": [50828, 291, 4325, 11, 498, 291, 574, 8185, 11, 50914], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 428, "seek": 107964, "start": 1090.64, "end": 1093.0, "text": " that it's, in order to solve them,", "tokens": [50914, 300, 309, 311, 11, 294, 1668, 281, 5039, 552, 11, 51032], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 429, "seek": 107964, "start": 1093.0, "end": 1098.0, "text": " it's enough to memorize a finite set of reasoning patterns.", "tokens": [51032, 309, 311, 1547, 281, 27478, 257, 19362, 992, 295, 21577, 8294, 13, 51282], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 430, "seek": 107964, "start": 1099.0800000000002, "end": 1100.5200000000002, "text": " And then you just reapply them.", "tokens": [51336, 400, 550, 291, 445, 35638, 356, 552, 13, 51408], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 431, "seek": 107964, "start": 1100.5200000000002, "end": 1102.8400000000001, "text": " They're like static programs.", "tokens": [51408, 814, 434, 411, 13437, 4268, 13, 51524], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 432, "seek": 107964, "start": 1102.8400000000001, "end": 1105.6000000000001, "text": " LMS are very good at memorizing static programs,", "tokens": [51524, 441, 10288, 366, 588, 665, 412, 10560, 3319, 13437, 4268, 11, 51662], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 433, "seek": 107964, "start": 1105.6000000000001, "end": 1106.5600000000002, "text": " small static programs.", "tokens": [51662, 1359, 13437, 4268, 13, 51710], "temperature": 0.0, "avg_logprob": -0.17875091031066373, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0008275324362330139}, {"id": 434, "seek": 110656, "start": 1106.56, "end": 1111.56, "text": " And they've got this sort of like bank of solution programs.", "tokens": [50364, 400, 436, 600, 658, 341, 1333, 295, 411, 3765, 295, 3827, 4268, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 435, "seek": 110656, "start": 1111.76, "end": 1113.72, "text": " And when you give them a new puzzle,", "tokens": [50624, 400, 562, 291, 976, 552, 257, 777, 12805, 11, 50722], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 436, "seek": 110656, "start": 1113.72, "end": 1117.24, "text": " they can just fetch the appropriate program, apply it.", "tokens": [50722, 436, 393, 445, 23673, 264, 6854, 1461, 11, 3079, 309, 13, 50898], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 437, "seek": 110656, "start": 1117.24, "end": 1119.1599999999999, "text": " And it's looking like it's reasoning,", "tokens": [50898, 400, 309, 311, 1237, 411, 309, 311, 21577, 11, 50994], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 438, "seek": 110656, "start": 1119.1599999999999, "end": 1121.0, "text": " but really it's not doing any sort of", "tokens": [50994, 457, 534, 309, 311, 406, 884, 604, 1333, 295, 51086], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 439, "seek": 110656, "start": 1121.0, "end": 1122.6399999999999, "text": " on-the-flight program synthesis.", "tokens": [51086, 322, 12, 3322, 12, 43636, 1461, 30252, 13, 51168], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 440, "seek": 110656, "start": 1122.6399999999999, "end": 1125.28, "text": " All it's doing is program fetching.", "tokens": [51168, 1057, 309, 311, 884, 307, 1461, 23673, 278, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 441, "seek": 110656, "start": 1125.28, "end": 1127.3999999999999, "text": " So you can actually solve all these benchmarks", "tokens": [51300, 407, 291, 393, 767, 5039, 439, 613, 43751, 51406], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 442, "seek": 110656, "start": 1127.3999999999999, "end": 1128.6799999999998, "text": " with memorization.", "tokens": [51406, 365, 10560, 2144, 13, 51470], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 443, "seek": 110656, "start": 1128.6799999999998, "end": 1131.8, "text": " And so, what you're scaling up here,", "tokens": [51470, 400, 370, 11, 437, 291, 434, 21589, 493, 510, 11, 51626], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 444, "seek": 110656, "start": 1131.8, "end": 1133.36, "text": " like if you look at the models,", "tokens": [51626, 411, 498, 291, 574, 412, 264, 5245, 11, 51704], "temperature": 0.0, "avg_logprob": -0.12673409258733032, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0004732560773845762}, {"id": 445, "seek": 113336, "start": 1133.36, "end": 1136.6799999999998, "text": " they are big parametric curves", "tokens": [50364, 436, 366, 955, 6220, 17475, 19490, 50530], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 446, "seek": 113336, "start": 1136.6799999999998, "end": 1138.32, "text": " fitted to a data distribution,", "tokens": [50530, 26321, 281, 257, 1412, 7316, 11, 50612], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 447, "seek": 113336, "start": 1138.32, "end": 1139.4799999999998, "text": " which I call in descent.", "tokens": [50612, 597, 286, 818, 294, 23475, 13, 50670], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 448, "seek": 113336, "start": 1139.4799999999998, "end": 1143.6, "text": " So they're basically these big interpolative databases,", "tokens": [50670, 407, 436, 434, 1936, 613, 955, 44902, 1166, 22380, 11, 50876], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 449, "seek": 113336, "start": 1143.6, "end": 1144.9599999999998, "text": " interpolative memories.", "tokens": [50876, 44902, 1166, 8495, 13, 50944], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 450, "seek": 113336, "start": 1144.9599999999998, "end": 1148.56, "text": " And of course, if you scale up the size of your database", "tokens": [50944, 400, 295, 1164, 11, 498, 291, 4373, 493, 264, 2744, 295, 428, 8149, 51124], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 451, "seek": 113336, "start": 1148.56, "end": 1151.3999999999999, "text": " and you cram into it more knowledge,", "tokens": [51124, 293, 291, 941, 335, 666, 309, 544, 3601, 11, 51266], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 452, "seek": 113336, "start": 1151.3999999999999, "end": 1153.6399999999999, "text": " more patterns and so on,", "tokens": [51266, 544, 8294, 293, 370, 322, 11, 51378], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 453, "seek": 113336, "start": 1153.6399999999999, "end": 1156.76, "text": " you are gonna be increasing its performance", "tokens": [51378, 291, 366, 799, 312, 5662, 1080, 3389, 51534], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 454, "seek": 113336, "start": 1156.76, "end": 1159.3999999999999, "text": " as measured by a memorization benchmark.", "tokens": [51534, 382, 12690, 538, 257, 10560, 2144, 18927, 13, 51666], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 455, "seek": 113336, "start": 1159.3999999999999, "end": 1160.9599999999998, "text": " That's kind of obvious.", "tokens": [51666, 663, 311, 733, 295, 6322, 13, 51744], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 456, "seek": 113336, "start": 1160.9599999999998, "end": 1162.4399999999998, "text": " But as you're doing it,", "tokens": [51744, 583, 382, 291, 434, 884, 309, 11, 51818], "temperature": 0.0, "avg_logprob": -0.17789897584078604, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.0004550756129901856}, {"id": 457, "seek": 116244, "start": 1162.44, "end": 1165.0800000000002, "text": " you are not increasing the intelligence", "tokens": [50364, 291, 366, 406, 5662, 264, 7599, 50496], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 458, "seek": 116244, "start": 1165.0800000000002, "end": 1166.76, "text": " of the system one bit.", "tokens": [50496, 295, 264, 1185, 472, 857, 13, 50580], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 459, "seek": 116244, "start": 1166.76, "end": 1168.64, "text": " You are increasing the skill of the system.", "tokens": [50580, 509, 366, 5662, 264, 5389, 295, 264, 1185, 13, 50674], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 460, "seek": 116244, "start": 1168.64, "end": 1170.6000000000001, "text": " You are increasing its usefulness,", "tokens": [50674, 509, 366, 5662, 1080, 4420, 1287, 11, 50772], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 461, "seek": 116244, "start": 1170.6000000000001, "end": 1174.3600000000001, "text": " its scope of applicability, but not its intelligence", "tokens": [50772, 1080, 11923, 295, 2580, 2310, 11, 457, 406, 1080, 7599, 50960], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 462, "seek": 116244, "start": 1174.3600000000001, "end": 1176.64, "text": " because skill is not intelligence.", "tokens": [50960, 570, 5389, 307, 406, 7599, 13, 51074], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 463, "seek": 116244, "start": 1176.64, "end": 1178.48, "text": " And that's the fundamental confusion", "tokens": [51074, 400, 300, 311, 264, 8088, 15075, 51166], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 464, "seek": 116244, "start": 1179.48, "end": 1182.1200000000001, "text": " that people run into is that", "tokens": [51216, 300, 561, 1190, 666, 307, 300, 51348], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 465, "seek": 116244, "start": 1182.1200000000001, "end": 1184.16, "text": " they're confusing skill and intelligence.", "tokens": [51348, 436, 434, 13181, 5389, 293, 7599, 13, 51450], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 466, "seek": 116244, "start": 1184.16, "end": 1185.64, "text": " Yeah, there's a lot of fascinating things", "tokens": [51450, 865, 11, 456, 311, 257, 688, 295, 10343, 721, 51524], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 467, "seek": 116244, "start": 1185.64, "end": 1186.48, "text": " to talk about here.", "tokens": [51524, 281, 751, 466, 510, 13, 51566], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 468, "seek": 116244, "start": 1186.48, "end": 1190.64, "text": " So skill, intelligence, interpolation.", "tokens": [51566, 407, 5389, 11, 7599, 11, 44902, 399, 13, 51774], "temperature": 0.0, "avg_logprob": -0.17409714707383164, "compression_ratio": 1.9553571428571428, "no_speech_prob": 0.00030031579080969095}, {"id": 469, "seek": 119064, "start": 1190.64, "end": 1192.24, "text": " I mean, okay, so the thing about", "tokens": [50364, 286, 914, 11, 1392, 11, 370, 264, 551, 466, 50444], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 470, "seek": 119064, "start": 1192.24, "end": 1196.72, "text": " they're fitting some manifold into that maps the input data,", "tokens": [50444, 436, 434, 15669, 512, 47138, 666, 300, 11317, 264, 4846, 1412, 11, 50668], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 471, "seek": 119064, "start": 1196.72, "end": 1198.44, "text": " there's a reductionist way to talk about what happens", "tokens": [50668, 456, 311, 257, 11004, 468, 636, 281, 751, 466, 437, 2314, 50754], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 472, "seek": 119064, "start": 1198.44, "end": 1199.96, "text": " in the human brain that says", "tokens": [50754, 294, 264, 1952, 3567, 300, 1619, 50830], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 473, "seek": 119064, "start": 1199.96, "end": 1203.2, "text": " that it's just axons firing at each other.", "tokens": [50830, 300, 309, 311, 445, 6360, 892, 16045, 412, 1184, 661, 13, 50992], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 474, "seek": 119064, "start": 1203.2, "end": 1205.88, "text": " But we don't care about the reductionist explanation", "tokens": [50992, 583, 321, 500, 380, 1127, 466, 264, 11004, 468, 10835, 51126], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 475, "seek": 119064, "start": 1205.88, "end": 1206.72, "text": " of what's happening.", "tokens": [51126, 295, 437, 311, 2737, 13, 51168], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 476, "seek": 119064, "start": 1206.72, "end": 1211.2, "text": " We care about what the sort of meta at the", "tokens": [51168, 492, 1127, 466, 437, 264, 1333, 295, 19616, 412, 264, 51392], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 477, "seek": 119064, "start": 1211.2, "end": 1213.8400000000001, "text": " macroscopic level, what happens when these things combine.", "tokens": [51392, 7912, 38006, 299, 1496, 11, 437, 2314, 562, 613, 721, 10432, 13, 51524], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 478, "seek": 119064, "start": 1213.8400000000001, "end": 1215.96, "text": " As far as the interpolation goes,", "tokens": [51524, 1018, 1400, 382, 264, 44902, 399, 1709, 11, 51630], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 479, "seek": 119064, "start": 1215.96, "end": 1219.1200000000001, "text": " so okay, let's look at one of the benchmarks here.", "tokens": [51630, 370, 1392, 11, 718, 311, 574, 412, 472, 295, 264, 43751, 510, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13943632868410066, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.00109858566429466}, {"id": 480, "seek": 121912, "start": 1219.12, "end": 1222.6799999999998, "text": " There's one benchmark that does great school math", "tokens": [50364, 821, 311, 472, 18927, 300, 775, 869, 1395, 5221, 50542], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 481, "seek": 121912, "start": 1222.6799999999998, "end": 1226.7199999999998, "text": " and these are problems that like a smart high schooler", "tokens": [50542, 293, 613, 366, 2740, 300, 411, 257, 4069, 1090, 1395, 260, 50744], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 482, "seek": 121912, "start": 1226.7199999999998, "end": 1228.4799999999998, "text": " would be able to solve.", "tokens": [50744, 576, 312, 1075, 281, 5039, 13, 50832], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 483, "seek": 121912, "start": 1228.4799999999998, "end": 1231.76, "text": " It's called GSM 8K and these models get 95% on these.", "tokens": [50832, 467, 311, 1219, 460, 26693, 1649, 42, 293, 613, 5245, 483, 13420, 4, 322, 613, 13, 50996], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 484, "seek": 121912, "start": 1231.76, "end": 1233.1999999999998, "text": " Like basically they always nail it.", "tokens": [50996, 1743, 1936, 436, 1009, 10173, 309, 13, 51068], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 485, "seek": 121912, "start": 1233.1999999999998, "end": 1234.32, "text": " That's memorization benchmark.", "tokens": [51068, 663, 311, 10560, 2144, 18927, 13, 51124], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 486, "seek": 121912, "start": 1234.32, "end": 1235.52, "text": " Okay, let's talk about what that means.", "tokens": [51124, 1033, 11, 718, 311, 751, 466, 437, 300, 1355, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 487, "seek": 121912, "start": 1235.52, "end": 1238.4799999999998, "text": " So here's one question about from that benchmark.", "tokens": [51184, 407, 510, 311, 472, 1168, 466, 490, 300, 18927, 13, 51332], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 488, "seek": 121912, "start": 1238.4799999999998, "end": 1240.12, "text": " So 30 students are in a class,", "tokens": [51332, 407, 2217, 1731, 366, 294, 257, 1508, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 489, "seek": 121912, "start": 1240.12, "end": 1241.8799999999999, "text": " one fifth of them are 12 year olds,", "tokens": [51414, 472, 9266, 295, 552, 366, 2272, 1064, 41972, 11, 51502], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 490, "seek": 121912, "start": 1241.8799999999999, "end": 1245.04, "text": " one third are 13 year old, one 10th are 11 year olds.", "tokens": [51502, 472, 2636, 366, 3705, 1064, 1331, 11, 472, 1266, 392, 366, 2975, 1064, 41972, 13, 51660], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 491, "seek": 121912, "start": 1245.04, "end": 1248.32, "text": " How many of them are not 11, 12 or 13 years old?", "tokens": [51660, 1012, 867, 295, 552, 366, 406, 2975, 11, 2272, 420, 3705, 924, 1331, 30, 51824], "temperature": 0.0, "avg_logprob": -0.15577460308464205, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0012841594871133566}, {"id": 492, "seek": 124832, "start": 1248.32, "end": 1250.4399999999998, "text": " So I agree, like this is not rocket science, right?", "tokens": [50364, 407, 286, 3986, 11, 411, 341, 307, 406, 13012, 3497, 11, 558, 30, 50470], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 493, "seek": 124832, "start": 1250.4399999999998, "end": 1253.2, "text": " You can write down on paper how you go through this problem", "tokens": [50470, 509, 393, 2464, 760, 322, 3035, 577, 291, 352, 807, 341, 1154, 50608], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 494, "seek": 124832, "start": 1253.2, "end": 1254.6, "text": " and a high school kid,", "tokens": [50608, 293, 257, 1090, 1395, 1636, 11, 50678], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 495, "seek": 124832, "start": 1254.6, "end": 1256.6399999999999, "text": " at least a smart high school kid should be able to solve it.", "tokens": [50678, 412, 1935, 257, 4069, 1090, 1395, 1636, 820, 312, 1075, 281, 5039, 309, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 496, "seek": 124832, "start": 1256.6399999999999, "end": 1258.9199999999998, "text": " Now, when you say memorization,", "tokens": [50780, 823, 11, 562, 291, 584, 10560, 2144, 11, 50894], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 497, "seek": 124832, "start": 1258.9199999999998, "end": 1263.0, "text": " it still has to reason through how to think about fractions", "tokens": [50894, 309, 920, 575, 281, 1778, 807, 577, 281, 519, 466, 36058, 51098], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 498, "seek": 124832, "start": 1263.0, "end": 1265.0, "text": " and what is the context of the whole problem", "tokens": [51098, 293, 437, 307, 264, 4319, 295, 264, 1379, 1154, 51198], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 499, "seek": 124832, "start": 1265.0, "end": 1267.84, "text": " and then combining the different calculations it's doing.", "tokens": [51198, 293, 550, 21928, 264, 819, 20448, 309, 311, 884, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 500, "seek": 124832, "start": 1267.84, "end": 1270.2, "text": " It depends how you want to define reasoning,", "tokens": [51340, 467, 5946, 577, 291, 528, 281, 6964, 21577, 11, 51458], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 501, "seek": 124832, "start": 1270.2, "end": 1272.2, "text": " but there are two definitions you can use.", "tokens": [51458, 457, 456, 366, 732, 21988, 291, 393, 764, 13, 51558], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 502, "seek": 124832, "start": 1272.2, "end": 1277.2, "text": " So one is I have available a set of program templates.", "tokens": [51558, 407, 472, 307, 286, 362, 2435, 257, 992, 295, 1461, 21165, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1237369314597471, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.00026117166271433234}, {"id": 503, "seek": 127720, "start": 1277.68, "end": 1281.2, "text": " It's like the structure of the puzzle,", "tokens": [50388, 467, 311, 411, 264, 3877, 295, 264, 12805, 11, 50564], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 504, "seek": 127720, "start": 1281.2, "end": 1282.96, "text": " which can also generate its solution.", "tokens": [50564, 597, 393, 611, 8460, 1080, 3827, 13, 50652], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 505, "seek": 127720, "start": 1282.96, "end": 1285.6000000000001, "text": " And I'm just gonna identify the right template,", "tokens": [50652, 400, 286, 478, 445, 799, 5876, 264, 558, 12379, 11, 50784], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 506, "seek": 127720, "start": 1285.6000000000001, "end": 1286.96, "text": " which is in my memory.", "tokens": [50784, 597, 307, 294, 452, 4675, 13, 50852], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 507, "seek": 127720, "start": 1287.8, "end": 1289.8400000000001, "text": " I'm gonna input the new values into the template,", "tokens": [50894, 286, 478, 799, 4846, 264, 777, 4190, 666, 264, 12379, 11, 50996], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 508, "seek": 127720, "start": 1289.8400000000001, "end": 1291.8400000000001, "text": " run the program, get the solution.", "tokens": [50996, 1190, 264, 1461, 11, 483, 264, 3827, 13, 51096], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 509, "seek": 127720, "start": 1291.8400000000001, "end": 1293.48, "text": " And you could say this is reasoning.", "tokens": [51096, 400, 291, 727, 584, 341, 307, 21577, 13, 51178], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 510, "seek": 127720, "start": 1293.48, "end": 1295.4, "text": " And I say, yeah, sure, okay.", "tokens": [51178, 400, 286, 584, 11, 1338, 11, 988, 11, 1392, 13, 51274], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 511, "seek": 127720, "start": 1295.4, "end": 1297.56, "text": " But another definition you can use is reasoning", "tokens": [51274, 583, 1071, 7123, 291, 393, 764, 307, 21577, 51382], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 512, "seek": 127720, "start": 1297.56, "end": 1301.28, "text": " is the ability to, when you're faced with a puzzle,", "tokens": [51382, 307, 264, 3485, 281, 11, 562, 291, 434, 11446, 365, 257, 12805, 11, 51568], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 513, "seek": 127720, "start": 1301.28, "end": 1304.48, "text": " given that you don't have already a program in memory", "tokens": [51568, 2212, 300, 291, 500, 380, 362, 1217, 257, 1461, 294, 4675, 51728], "temperature": 0.0, "avg_logprob": -0.16112042236328125, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.001369524747133255}, {"id": 514, "seek": 130448, "start": 1304.48, "end": 1309.48, "text": " to solve it, you must synthesize on-the-fly a new program", "tokens": [50364, 281, 5039, 309, 11, 291, 1633, 26617, 1125, 322, 12, 3322, 12, 14061, 257, 777, 1461, 50614], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 515, "seek": 130448, "start": 1309.68, "end": 1312.96, "text": " based on bits of pieces of existing programs that you have.", "tokens": [50624, 2361, 322, 9239, 295, 3755, 295, 6741, 4268, 300, 291, 362, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 516, "seek": 130448, "start": 1312.96, "end": 1315.48, "text": " You have to do on-the-fly program synthesis.", "tokens": [50788, 509, 362, 281, 360, 322, 12, 3322, 12, 14061, 1461, 30252, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 517, "seek": 130448, "start": 1315.48, "end": 1317.28, "text": " And it's actually dramatically harder", "tokens": [50914, 400, 309, 311, 767, 17548, 6081, 51004], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 518, "seek": 130448, "start": 1317.28, "end": 1320.04, "text": " than just fetching the right memorized program", "tokens": [51004, 813, 445, 23673, 278, 264, 558, 46677, 1461, 51142], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 519, "seek": 130448, "start": 1320.04, "end": 1321.32, "text": " and replying it.", "tokens": [51142, 293, 1085, 7310, 309, 13, 51206], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 520, "seek": 130448, "start": 1321.32, "end": 1324.56, "text": " So I think maybe we are overestimating", "tokens": [51206, 407, 286, 519, 1310, 321, 366, 670, 377, 332, 990, 51368], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 521, "seek": 130448, "start": 1324.56, "end": 1326.84, "text": " the extent to which humans are so sample efficient.", "tokens": [51368, 264, 8396, 281, 597, 6255, 366, 370, 6889, 7148, 13, 51482], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 522, "seek": 130448, "start": 1326.84, "end": 1330.2, "text": " They also don't need training in this way", "tokens": [51482, 814, 611, 500, 380, 643, 3097, 294, 341, 636, 51650], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 523, "seek": 130448, "start": 1330.2, "end": 1332.76, "text": " where they have to drill in these kinds", "tokens": [51650, 689, 436, 362, 281, 11392, 294, 613, 3685, 51778], "temperature": 0.0, "avg_logprob": -0.11857017420106016, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0008828415884636343}, {"id": 524, "seek": 133276, "start": 1332.8, "end": 1336.76, "text": " of pathways of reasoning through certain kinds of problems.", "tokens": [50366, 295, 22988, 295, 21577, 807, 1629, 3685, 295, 2740, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 525, "seek": 133276, "start": 1336.76, "end": 1339.2, "text": " So let's take math, for example.", "tokens": [50564, 407, 718, 311, 747, 5221, 11, 337, 1365, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 526, "seek": 133276, "start": 1339.2, "end": 1341.24, "text": " It's not like you can just show a baby", "tokens": [50686, 467, 311, 406, 411, 291, 393, 445, 855, 257, 3186, 50788], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 527, "seek": 133276, "start": 1341.24, "end": 1342.84, "text": " the axioms of set theory.", "tokens": [50788, 264, 6360, 72, 4785, 295, 992, 5261, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 528, "seek": 133276, "start": 1342.84, "end": 1344.08, "text": " And now they know math, right?", "tokens": [50868, 400, 586, 436, 458, 5221, 11, 558, 30, 50930], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 529, "seek": 133276, "start": 1344.08, "end": 1345.68, "text": " So when they're growing up,", "tokens": [50930, 407, 562, 436, 434, 4194, 493, 11, 51010], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 530, "seek": 133276, "start": 1345.68, "end": 1348.12, "text": " you had to do years of teaching them pre-algebra.", "tokens": [51010, 291, 632, 281, 360, 924, 295, 4571, 552, 659, 12, 304, 19983, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 531, "seek": 133276, "start": 1348.12, "end": 1350.36, "text": " Then you got to do a year of teaching them doing drills", "tokens": [51132, 1396, 291, 658, 281, 360, 257, 1064, 295, 4571, 552, 884, 36126, 51244], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 532, "seek": 133276, "start": 1350.36, "end": 1352.24, "text": " and going through the same kind of problem in algebra,", "tokens": [51244, 293, 516, 807, 264, 912, 733, 295, 1154, 294, 21989, 11, 51338], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 533, "seek": 133276, "start": 1352.24, "end": 1355.04, "text": " then geometry, pre-calculus, calculus.", "tokens": [51338, 550, 18426, 11, 659, 12, 9895, 36002, 11, 33400, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 534, "seek": 133276, "start": 1355.04, "end": 1356.52, "text": " Absolutely, so training?", "tokens": [51478, 7021, 11, 370, 3097, 30, 51552], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 535, "seek": 133276, "start": 1356.52, "end": 1357.8799999999999, "text": " Yeah, but isn't that like the same kind of thing", "tokens": [51552, 865, 11, 457, 1943, 380, 300, 411, 264, 912, 733, 295, 551, 51620], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 536, "seek": 133276, "start": 1357.8799999999999, "end": 1360.08, "text": " where you can't just see one example", "tokens": [51620, 689, 291, 393, 380, 445, 536, 472, 1365, 51730], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 537, "seek": 133276, "start": 1360.08, "end": 1361.96, "text": " and now you have the program or whatever.", "tokens": [51730, 293, 586, 291, 362, 264, 1461, 420, 2035, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1382541185543861, "compression_ratio": 1.8063492063492064, "no_speech_prob": 0.0014546307502314448}, {"id": 538, "seek": 136196, "start": 1361.96, "end": 1362.8, "text": " You actually had to drill it.", "tokens": [50364, 509, 767, 632, 281, 11392, 309, 13, 50406], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 539, "seek": 136196, "start": 1362.8, "end": 1363.8, "text": " These models also had to drill it", "tokens": [50406, 1981, 5245, 611, 632, 281, 11392, 309, 50456], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 540, "seek": 136196, "start": 1363.8, "end": 1365.08, "text": " with a bunch of returning data.", "tokens": [50456, 365, 257, 3840, 295, 12678, 1412, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 541, "seek": 136196, "start": 1365.08, "end": 1368.32, "text": " Sure, I mean, in order to do on-the-fly program synthesis,", "tokens": [50520, 4894, 11, 286, 914, 11, 294, 1668, 281, 360, 322, 12, 3322, 12, 14061, 1461, 30252, 11, 50682], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 542, "seek": 136196, "start": 1368.32, "end": 1371.92, "text": " you actually need building blocks to work from.", "tokens": [50682, 291, 767, 643, 2390, 8474, 281, 589, 490, 13, 50862], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 543, "seek": 136196, "start": 1371.92, "end": 1375.44, "text": " So knowledge and memory are actually tremendously important", "tokens": [50862, 407, 3601, 293, 4675, 366, 767, 27985, 1021, 51038], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 544, "seek": 136196, "start": 1375.44, "end": 1376.28, "text": " in the process.", "tokens": [51038, 294, 264, 1399, 13, 51080], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 545, "seek": 136196, "start": 1376.28, "end": 1380.56, "text": " I'm not saying it's memory versus reasoning.", "tokens": [51080, 286, 478, 406, 1566, 309, 311, 4675, 5717, 21577, 13, 51294], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 546, "seek": 136196, "start": 1380.56, "end": 1384.64, "text": " In order to do effective reasoning, you need memory.", "tokens": [51294, 682, 1668, 281, 360, 4942, 21577, 11, 291, 643, 4675, 13, 51498], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 547, "seek": 136196, "start": 1384.64, "end": 1387.68, "text": " But it sounds like it's compatible with your story", "tokens": [51498, 583, 309, 3263, 411, 309, 311, 18218, 365, 428, 1657, 51650], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 548, "seek": 136196, "start": 1387.68, "end": 1390.44, "text": " that through seeing a lot of different kinds of examples,", "tokens": [51650, 300, 807, 2577, 257, 688, 295, 819, 3685, 295, 5110, 11, 51788], "temperature": 0.0, "avg_logprob": -0.12044544982910156, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0003250183362979442}, {"id": 549, "seek": 139044, "start": 1390.44, "end": 1391.96, "text": " these things can learn to reason", "tokens": [50364, 613, 721, 393, 1466, 281, 1778, 50440], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 550, "seek": 139044, "start": 1391.96, "end": 1393.8, "text": " within the context of those examples.", "tokens": [50440, 1951, 264, 4319, 295, 729, 5110, 13, 50532], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 551, "seek": 139044, "start": 1393.8, "end": 1396.16, "text": " And we can also see within bigger and bigger models.", "tokens": [50532, 400, 321, 393, 611, 536, 1951, 3801, 293, 3801, 5245, 13, 50650], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 552, "seek": 139044, "start": 1396.16, "end": 1398.8, "text": " So that was an example of a high school level math problem.", "tokens": [50650, 407, 300, 390, 364, 1365, 295, 257, 1090, 1395, 1496, 5221, 1154, 13, 50782], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 553, "seek": 139044, "start": 1399.88, "end": 1402.1200000000001, "text": " Let's say a model that's smaller than GPT-3", "tokens": [50836, 961, 311, 584, 257, 2316, 300, 311, 4356, 813, 26039, 51, 12, 18, 50948], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 554, "seek": 139044, "start": 1402.1200000000001, "end": 1403.68, "text": " couldn't do that at all.", "tokens": [50948, 2809, 380, 360, 300, 412, 439, 13, 51026], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 555, "seek": 139044, "start": 1403.68, "end": 1404.76, "text": " As these models get bigger,", "tokens": [51026, 1018, 613, 5245, 483, 3801, 11, 51080], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 556, "seek": 139044, "start": 1404.76, "end": 1406.4, "text": " they seem to be able to pick up bigger and bigger.", "tokens": [51080, 436, 1643, 281, 312, 1075, 281, 1888, 493, 3801, 293, 3801, 13, 51162], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 557, "seek": 139044, "start": 1406.4, "end": 1408.4, "text": " It's not really a size issue.", "tokens": [51162, 467, 311, 406, 534, 257, 2744, 2734, 13, 51262], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 558, "seek": 139044, "start": 1408.4, "end": 1410.6000000000001, "text": " It's more like a training data issue in this case.", "tokens": [51262, 467, 311, 544, 411, 257, 3097, 1412, 2734, 294, 341, 1389, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 559, "seek": 139044, "start": 1410.6000000000001, "end": 1413.68, "text": " Well, bigger models can pick up these kinds of circuits", "tokens": [51372, 1042, 11, 3801, 5245, 393, 1888, 493, 613, 3685, 295, 26354, 51526], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 560, "seek": 139044, "start": 1413.68, "end": 1416.04, "text": " which smaller models apparently don't do a good job", "tokens": [51526, 597, 4356, 5245, 7970, 500, 380, 360, 257, 665, 1691, 51644], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 561, "seek": 139044, "start": 1416.04, "end": 1417.28, "text": " of doing this even if you were to train them", "tokens": [51644, 295, 884, 341, 754, 498, 291, 645, 281, 3847, 552, 51706], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 562, "seek": 139044, "start": 1417.28, "end": 1418.24, "text": " on this kind of data.", "tokens": [51706, 322, 341, 733, 295, 1412, 13, 51754], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 563, "seek": 139044, "start": 1418.24, "end": 1419.0800000000002, "text": " Doesn't that just suggest", "tokens": [51754, 12955, 380, 300, 445, 3402, 51796], "temperature": 0.0, "avg_logprob": -0.13986560437806259, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.0035932722967118025}, {"id": 564, "seek": 141908, "start": 1419.08, "end": 1420.3999999999999, "text": " that you have bigger and bigger models?", "tokens": [50364, 300, 291, 362, 3801, 293, 3801, 5245, 30, 50430], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 565, "seek": 141908, "start": 1420.3999999999999, "end": 1422.24, "text": " They can pick up bigger and bigger pathways", "tokens": [50430, 814, 393, 1888, 493, 3801, 293, 3801, 22988, 50522], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 566, "seek": 141908, "start": 1422.24, "end": 1424.04, "text": " or more general ways of reasoning.", "tokens": [50522, 420, 544, 2674, 2098, 295, 21577, 13, 50612], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 567, "seek": 141908, "start": 1424.04, "end": 1424.96, "text": " Absolutely.", "tokens": [50612, 7021, 13, 50658], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 568, "seek": 141908, "start": 1424.96, "end": 1426.3999999999999, "text": " But then isn't that intelligence?", "tokens": [50658, 583, 550, 1943, 380, 300, 7599, 30, 50730], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 569, "seek": 141908, "start": 1426.3999999999999, "end": 1427.6, "text": " No, no, it's not.", "tokens": [50730, 883, 11, 572, 11, 309, 311, 406, 13, 50790], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 570, "seek": 141908, "start": 1427.6, "end": 1429.52, "text": " If you scale up your database", "tokens": [50790, 759, 291, 4373, 493, 428, 8149, 50886], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 571, "seek": 141908, "start": 1429.52, "end": 1432.28, "text": " and you keep adding to it more knowledge,", "tokens": [50886, 293, 291, 1066, 5127, 281, 309, 544, 3601, 11, 51024], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 572, "seek": 141908, "start": 1432.28, "end": 1433.6399999999999, "text": " more program templates,", "tokens": [51024, 544, 1461, 21165, 11, 51092], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 573, "seek": 141908, "start": 1433.6399999999999, "end": 1435.56, "text": " then sure it becomes more and more skillful.", "tokens": [51092, 550, 988, 309, 3643, 544, 293, 544, 5389, 906, 13, 51188], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 574, "seek": 141908, "start": 1435.56, "end": 1437.3999999999999, "text": " You can apply it to more and more tasks.", "tokens": [51188, 509, 393, 3079, 309, 281, 544, 293, 544, 9608, 13, 51280], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 575, "seek": 141908, "start": 1437.3999999999999, "end": 1441.12, "text": " But general intelligence is not tasks with six skills", "tokens": [51280, 583, 2674, 7599, 307, 406, 9608, 365, 2309, 3942, 51466], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 576, "seek": 141908, "start": 1441.12, "end": 1443.3999999999999, "text": " scaled up to many skills.", "tokens": [51466, 36039, 493, 281, 867, 3942, 13, 51580], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 577, "seek": 141908, "start": 1443.3999999999999, "end": 1446.6399999999999, "text": " Because there is an infinite space of possible skills.", "tokens": [51580, 1436, 456, 307, 364, 13785, 1901, 295, 1944, 3942, 13, 51742], "temperature": 0.0, "avg_logprob": -0.17818493629569437, "compression_ratio": 1.7570422535211268, "no_speech_prob": 0.0030640142504125834}, {"id": 578, "seek": 144664, "start": 1446.68, "end": 1449.2, "text": " General intelligence is the ability to approach", "tokens": [50366, 6996, 7599, 307, 264, 3485, 281, 3109, 50492], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 579, "seek": 144664, "start": 1449.2, "end": 1451.0400000000002, "text": " any problem, any skill,", "tokens": [50492, 604, 1154, 11, 604, 5389, 11, 50584], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 580, "seek": 144664, "start": 1451.0400000000002, "end": 1453.96, "text": " and very quickly master it using valid or data.", "tokens": [50584, 293, 588, 2661, 4505, 309, 1228, 7363, 420, 1412, 13, 50730], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 581, "seek": 144664, "start": 1453.96, "end": 1456.92, "text": " Because this is what makes you able to face", "tokens": [50730, 1436, 341, 307, 437, 1669, 291, 1075, 281, 1851, 50878], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 582, "seek": 144664, "start": 1456.92, "end": 1458.24, "text": " anything you might ever encounter.", "tokens": [50878, 1340, 291, 1062, 1562, 8593, 13, 50944], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 583, "seek": 144664, "start": 1458.24, "end": 1459.3600000000001, "text": " This is what makes,", "tokens": [50944, 639, 307, 437, 1669, 11, 51000], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 584, "seek": 144664, "start": 1460.8000000000002, "end": 1462.6000000000001, "text": " this is the definition of generality.", "tokens": [51072, 341, 307, 264, 7123, 295, 1337, 1860, 13, 51162], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 585, "seek": 144664, "start": 1462.6000000000001, "end": 1465.64, "text": " Like generality is now specifically scaled up.", "tokens": [51162, 1743, 1337, 1860, 307, 586, 4682, 36039, 493, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 586, "seek": 144664, "start": 1465.64, "end": 1469.1200000000001, "text": " It is the ability to apply your mind", "tokens": [51314, 467, 307, 264, 3485, 281, 3079, 428, 1575, 51488], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 587, "seek": 144664, "start": 1469.1200000000001, "end": 1471.48, "text": " to anything at all, to arbitrary things.", "tokens": [51488, 281, 1340, 412, 439, 11, 281, 23211, 721, 13, 51606], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 588, "seek": 144664, "start": 1471.48, "end": 1473.0400000000002, "text": " And this requires, fundamentally,", "tokens": [51606, 400, 341, 7029, 11, 17879, 11, 51684], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 589, "seek": 144664, "start": 1473.0400000000002, "end": 1475.0400000000002, "text": " it requires the ability to adapt,", "tokens": [51684, 309, 7029, 264, 3485, 281, 6231, 11, 51784], "temperature": 0.0, "avg_logprob": -0.19423986303395238, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.0003932247927878052}, {"id": 590, "seek": 147504, "start": 1475.08, "end": 1477.08, "text": " to learn on the fly efficiently.", "tokens": [50366, 281, 1466, 322, 264, 3603, 19621, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 591, "seek": 147504, "start": 1477.08, "end": 1481.28, "text": " So, my claim is that by doing this free training", "tokens": [50466, 407, 11, 452, 3932, 307, 300, 538, 884, 341, 1737, 3097, 50676], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 592, "seek": 147504, "start": 1481.28, "end": 1482.68, "text": " on bigger and bigger models,", "tokens": [50676, 322, 3801, 293, 3801, 5245, 11, 50746], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 593, "seek": 147504, "start": 1482.68, "end": 1484.44, "text": " you are gaining that capacity", "tokens": [50746, 291, 366, 19752, 300, 6042, 50834], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 594, "seek": 147504, "start": 1484.44, "end": 1486.76, "text": " to then generalize very efficiently.", "tokens": [50834, 281, 550, 2674, 1125, 588, 19621, 13, 50950], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 595, "seek": 147504, "start": 1486.76, "end": 1488.12, "text": " Let me give you an example.", "tokens": [50950, 961, 385, 976, 291, 364, 1365, 13, 51018], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 596, "seek": 147504, "start": 1488.12, "end": 1489.28, "text": " Let me give you an example.", "tokens": [51018, 961, 385, 976, 291, 364, 1365, 13, 51076], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 597, "seek": 147504, "start": 1489.28, "end": 1491.24, "text": " So, your own company, Google,", "tokens": [51076, 407, 11, 428, 1065, 2237, 11, 3329, 11, 51174], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 598, "seek": 147504, "start": 1491.24, "end": 1494.24, "text": " in their paper on Gemini 1.5,", "tokens": [51174, 294, 641, 3035, 322, 22894, 3812, 502, 13, 20, 11, 51324], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 599, "seek": 147504, "start": 1494.24, "end": 1495.92, "text": " they had this very interesting example", "tokens": [51324, 436, 632, 341, 588, 1880, 1365, 51408], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 600, "seek": 147504, "start": 1495.92, "end": 1500.52, "text": " where they would give, in context,", "tokens": [51408, 689, 436, 576, 976, 11, 294, 4319, 11, 51638], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 601, "seek": 147504, "start": 1500.52, "end": 1501.84, "text": " they would give the model,", "tokens": [51638, 436, 576, 976, 264, 2316, 11, 51704], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 602, "seek": 147504, "start": 1501.84, "end": 1504.36, "text": " the grammar book and the dictionary", "tokens": [51704, 264, 22317, 1446, 293, 264, 25890, 51830], "temperature": 0.0, "avg_logprob": -0.14960772092225122, "compression_ratio": 1.799163179916318, "no_speech_prob": 0.0013446395751088858}, {"id": 603, "seek": 150436, "start": 1504.36, "end": 1507.52, "text": " of a language that has less than 200 living speakers.", "tokens": [50364, 295, 257, 2856, 300, 575, 1570, 813, 2331, 2647, 9518, 13, 50522], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 604, "seek": 150436, "start": 1507.52, "end": 1509.4799999999998, "text": " So, it's not in the free training data.", "tokens": [50522, 407, 11, 309, 311, 406, 294, 264, 1737, 3097, 1412, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 605, "seek": 150436, "start": 1509.4799999999998, "end": 1511.52, "text": " And you just give them the dictionary", "tokens": [50620, 400, 291, 445, 976, 552, 264, 25890, 50722], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 606, "seek": 150436, "start": 1511.52, "end": 1514.12, "text": " and it basically is able to speak this language", "tokens": [50722, 293, 309, 1936, 307, 1075, 281, 1710, 341, 2856, 50852], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 607, "seek": 150436, "start": 1514.12, "end": 1515.08, "text": " and translate to it,", "tokens": [50852, 293, 13799, 281, 309, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 608, "seek": 150436, "start": 1515.08, "end": 1517.6399999999999, "text": " including the complex and organic ways", "tokens": [50900, 3009, 264, 3997, 293, 10220, 2098, 51028], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 609, "seek": 150436, "start": 1517.6399999999999, "end": 1520.1999999999998, "text": " in which languages are structured.", "tokens": [51028, 294, 597, 8650, 366, 18519, 13, 51156], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 610, "seek": 150436, "start": 1520.1999999999998, "end": 1521.76, "text": " So, a human, if you showed me a dictionary", "tokens": [51156, 407, 11, 257, 1952, 11, 498, 291, 4712, 385, 257, 25890, 51234], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 611, "seek": 150436, "start": 1521.76, "end": 1522.7199999999998, "text": " from English to Spanish,", "tokens": [51234, 490, 3669, 281, 8058, 11, 51282], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 612, "seek": 150436, "start": 1522.7199999999998, "end": 1524.56, "text": " I'm not gonna be able to pick up the", "tokens": [51282, 286, 478, 406, 799, 312, 1075, 281, 1888, 493, 264, 51374], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 613, "seek": 150436, "start": 1524.56, "end": 1525.84, "text": " how to structure sentences", "tokens": [51374, 577, 281, 3877, 16579, 51438], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 614, "seek": 150436, "start": 1525.84, "end": 1528.04, "text": " and how to say things in Spanish.", "tokens": [51438, 293, 577, 281, 584, 721, 294, 8058, 13, 51548], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 615, "seek": 150436, "start": 1528.04, "end": 1530.28, "text": " The fact that because of the representations", "tokens": [51548, 440, 1186, 300, 570, 295, 264, 33358, 51660], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 616, "seek": 150436, "start": 1530.28, "end": 1533.12, "text": " that it has gained through this free training,", "tokens": [51660, 300, 309, 575, 12634, 807, 341, 1737, 3097, 11, 51802], "temperature": 0.0, "avg_logprob": -0.10939392940603572, "compression_ratio": 1.75, "no_speech_prob": 0.006684153340756893}, {"id": 617, "seek": 153312, "start": 1533.12, "end": 1535.2399999999998, "text": " it is able to now extremely efficiently", "tokens": [50364, 309, 307, 1075, 281, 586, 4664, 19621, 50470], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 618, "seek": 153312, "start": 1535.2399999999998, "end": 1536.36, "text": " learn a new language.", "tokens": [50470, 1466, 257, 777, 2856, 13, 50526], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 619, "seek": 153312, "start": 1536.36, "end": 1538.6, "text": " Doesn't that show that this kind of free training", "tokens": [50526, 12955, 380, 300, 855, 300, 341, 733, 295, 1737, 3097, 50638], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 620, "seek": 153312, "start": 1538.6, "end": 1541.2399999999998, "text": " actually does increase your ability to learn new tasks?", "tokens": [50638, 767, 775, 3488, 428, 3485, 281, 1466, 777, 9608, 30, 50770], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 621, "seek": 153312, "start": 1541.2399999999998, "end": 1543.1999999999998, "text": " If you're right, if you were right,", "tokens": [50770, 759, 291, 434, 558, 11, 498, 291, 645, 558, 11, 50868], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 622, "seek": 153312, "start": 1543.1999999999998, "end": 1545.4399999999998, "text": " LLMs would do really well on arch puzzles", "tokens": [50868, 441, 43, 26386, 576, 360, 534, 731, 322, 3912, 24138, 50980], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 623, "seek": 153312, "start": 1545.4399999999998, "end": 1547.6799999999998, "text": " because arch puzzles are not complex.", "tokens": [50980, 570, 3912, 24138, 366, 406, 3997, 13, 51092], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 624, "seek": 153312, "start": 1547.6799999999998, "end": 1549.8799999999999, "text": " Each one of them requires very little knowledge.", "tokens": [51092, 6947, 472, 295, 552, 7029, 588, 707, 3601, 13, 51202], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 625, "seek": 153312, "start": 1549.8799999999999, "end": 1552.28, "text": " Each one of them is very low on complexity.", "tokens": [51202, 6947, 472, 295, 552, 307, 588, 2295, 322, 14024, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 626, "seek": 153312, "start": 1552.28, "end": 1554.6, "text": " You don't need to think very hard about it.", "tokens": [51322, 509, 500, 380, 643, 281, 519, 588, 1152, 466, 309, 13, 51438], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 627, "seek": 153312, "start": 1554.6, "end": 1556.52, "text": " They're actually extremely obvious for humans,", "tokens": [51438, 814, 434, 767, 4664, 6322, 337, 6255, 11, 51534], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 628, "seek": 153312, "start": 1556.52, "end": 1558.2399999999998, "text": " like even children can do them.", "tokens": [51534, 411, 754, 2227, 393, 360, 552, 13, 51620], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 629, "seek": 153312, "start": 1558.2399999999998, "end": 1562.9199999999998, "text": " But LLMs cannot, even LLMs that have, you know,", "tokens": [51620, 583, 441, 43, 26386, 2644, 11, 754, 441, 43, 26386, 300, 362, 11, 291, 458, 11, 51854], "temperature": 0.0, "avg_logprob": -0.14027908686045054, "compression_ratio": 1.6882716049382716, "no_speech_prob": 0.0005017280927859247}, {"id": 630, "seek": 156292, "start": 1562.92, "end": 1565.3600000000001, "text": " 100,000 times more knowledge than you do.", "tokens": [50364, 2319, 11, 1360, 1413, 544, 3601, 813, 291, 360, 13, 50486], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 631, "seek": 156292, "start": 1565.3600000000001, "end": 1566.4, "text": " They still cannot.", "tokens": [50486, 814, 920, 2644, 13, 50538], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 632, "seek": 156292, "start": 1566.4, "end": 1569.5600000000002, "text": " And the only thing that makes arch special", "tokens": [50538, 400, 264, 787, 551, 300, 1669, 3912, 2121, 50696], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 633, "seek": 156292, "start": 1569.5600000000002, "end": 1571.68, "text": " is that it was designed with this intent", "tokens": [50696, 307, 300, 309, 390, 4761, 365, 341, 8446, 50802], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 634, "seek": 156292, "start": 1571.68, "end": 1573.0800000000002, "text": " to resist memorization.", "tokens": [50802, 281, 4597, 10560, 2144, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 635, "seek": 156292, "start": 1573.0800000000002, "end": 1574.3200000000002, "text": " This is the only thing.", "tokens": [50872, 639, 307, 264, 787, 551, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 636, "seek": 156292, "start": 1574.3200000000002, "end": 1578.96, "text": " And this is the huge blocker for LLM performance, right?", "tokens": [50934, 400, 341, 307, 264, 2603, 3461, 260, 337, 441, 43, 44, 3389, 11, 558, 30, 51166], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 637, "seek": 156292, "start": 1578.96, "end": 1583.96, "text": " And so, you know, I think if you look at LLMs closely,", "tokens": [51166, 400, 370, 11, 291, 458, 11, 286, 519, 498, 291, 574, 412, 441, 43, 26386, 8185, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 638, "seek": 156292, "start": 1585.24, "end": 1588.28, "text": " it's pretty obvious that they're not really like", "tokens": [51480, 309, 311, 1238, 6322, 300, 436, 434, 406, 534, 411, 51632], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 639, "seek": 156292, "start": 1588.28, "end": 1590.6000000000001, "text": " synthesizing new programs on the fly", "tokens": [51632, 26617, 3319, 777, 4268, 322, 264, 3603, 51748], "temperature": 0.0, "avg_logprob": -0.1278176818575178, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0003434770042076707}, {"id": 640, "seek": 159060, "start": 1590.6, "end": 1593.1599999999999, "text": " to solve the tasks that they're faced with.", "tokens": [50364, 281, 5039, 264, 9608, 300, 436, 434, 11446, 365, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 641, "seek": 159060, "start": 1593.1599999999999, "end": 1594.6399999999999, "text": " They're very much replying things", "tokens": [50492, 814, 434, 588, 709, 1085, 7310, 721, 50566], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 642, "seek": 159060, "start": 1594.6399999999999, "end": 1596.6399999999999, "text": " that they've stored in memory.", "tokens": [50566, 300, 436, 600, 12187, 294, 4675, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 643, "seek": 159060, "start": 1596.6399999999999, "end": 1599.28, "text": " For instance, one thing that's very striking", "tokens": [50666, 1171, 5197, 11, 472, 551, 300, 311, 588, 18559, 50798], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 644, "seek": 159060, "start": 1599.28, "end": 1602.4399999999998, "text": " is LLMs can solve a CISA cipher,", "tokens": [50798, 307, 441, 43, 26386, 393, 5039, 257, 383, 26183, 269, 21240, 11, 50956], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 645, "seek": 159060, "start": 1602.4399999999998, "end": 1603.76, "text": " you know, like a CISA cipher,", "tokens": [50956, 291, 458, 11, 411, 257, 383, 26183, 269, 21240, 11, 51022], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 646, "seek": 159060, "start": 1603.76, "end": 1608.76, "text": " like transposing letters to code a message.", "tokens": [51022, 411, 7132, 6110, 7825, 281, 3089, 257, 3636, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 647, "seek": 159060, "start": 1609.1999999999998, "end": 1612.6, "text": " And well, that's a very complex algorithm, right?", "tokens": [51294, 400, 731, 11, 300, 311, 257, 588, 3997, 9284, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 648, "seek": 159060, "start": 1612.6, "end": 1614.9199999999998, "text": " But it comes up quite a bit on the internet.", "tokens": [51464, 583, 309, 1487, 493, 1596, 257, 857, 322, 264, 4705, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 649, "seek": 159060, "start": 1614.9199999999998, "end": 1616.4399999999998, "text": " So they've basically memorized it.", "tokens": [51580, 407, 436, 600, 1936, 46677, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 650, "seek": 159060, "start": 1616.4399999999998, "end": 1619.6799999999998, "text": " And what's really interesting is that they can do it", "tokens": [51656, 400, 437, 311, 534, 1880, 307, 300, 436, 393, 360, 309, 51818], "temperature": 0.0, "avg_logprob": -0.1734050336734269, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.0013604150153696537}, {"id": 651, "seek": 161968, "start": 1619.8, "end": 1622.76, "text": " for a transposition length of like three or five", "tokens": [50370, 337, 257, 7132, 5830, 4641, 295, 411, 1045, 420, 1732, 50518], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 652, "seek": 161968, "start": 1622.76, "end": 1624.3600000000001, "text": " because there are very, very common numbers", "tokens": [50518, 570, 456, 366, 588, 11, 588, 2689, 3547, 50598], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 653, "seek": 161968, "start": 1624.3600000000001, "end": 1625.88, "text": " in examples provided on the internet.", "tokens": [50598, 294, 5110, 5649, 322, 264, 4705, 13, 50674], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 654, "seek": 161968, "start": 1625.88, "end": 1629.0, "text": " But if you try to do it with an arbitrary number,", "tokens": [50674, 583, 498, 291, 853, 281, 360, 309, 365, 364, 23211, 1230, 11, 50830], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 655, "seek": 161968, "start": 1629.0, "end": 1631.1200000000001, "text": " like nine, it's gonna fail.", "tokens": [50830, 411, 4949, 11, 309, 311, 799, 3061, 13, 50936], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 656, "seek": 161968, "start": 1631.1200000000001, "end": 1634.52, "text": " Because it does not encode the generalized form", "tokens": [50936, 1436, 309, 775, 406, 2058, 1429, 264, 44498, 1254, 51106], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 657, "seek": 161968, "start": 1634.52, "end": 1636.52, "text": " of the algorithm, but only specific cases.", "tokens": [51106, 295, 264, 9284, 11, 457, 787, 2685, 3331, 13, 51206], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 658, "seek": 161968, "start": 1636.52, "end": 1640.04, "text": " It does memorize specific cases of the algorithm, right?", "tokens": [51206, 467, 775, 27478, 2685, 3331, 295, 264, 9284, 11, 558, 30, 51382], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 659, "seek": 161968, "start": 1640.04, "end": 1643.16, "text": " And if it could actually synthesize on the fly", "tokens": [51382, 400, 498, 309, 727, 767, 26617, 1125, 322, 264, 3603, 51538], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 660, "seek": 161968, "start": 1643.16, "end": 1646.8400000000001, "text": " the solver algorithm, then the value of N", "tokens": [51538, 264, 1404, 331, 9284, 11, 550, 264, 2158, 295, 426, 51722], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 661, "seek": 161968, "start": 1646.8400000000001, "end": 1648.52, "text": " would not matter at all", "tokens": [51722, 576, 406, 1871, 412, 439, 51806], "temperature": 0.0, "avg_logprob": -0.13105296313278075, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00043810694478452206}, {"id": 662, "seek": 164852, "start": 1648.52, "end": 1650.8799999999999, "text": " because it does not increase the problem complexity.", "tokens": [50364, 570, 309, 775, 406, 3488, 264, 1154, 14024, 13, 50482], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 663, "seek": 164852, "start": 1650.8799999999999, "end": 1652.44, "text": " I think this is true of humans as well,", "tokens": [50482, 286, 519, 341, 307, 2074, 295, 6255, 382, 731, 11, 50560], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 664, "seek": 164852, "start": 1652.44, "end": 1654.36, "text": " where what was the study that-", "tokens": [50560, 689, 437, 390, 264, 2979, 300, 12, 50656], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 665, "seek": 164852, "start": 1654.36, "end": 1657.08, "text": " Humans use memorization pattern matching all the time,", "tokens": [50656, 35809, 764, 10560, 2144, 5102, 14324, 439, 264, 565, 11, 50792], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 666, "seek": 164852, "start": 1657.08, "end": 1659.8799999999999, "text": " of course, but humans are not limited", "tokens": [50792, 295, 1164, 11, 457, 6255, 366, 406, 5567, 50932], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 667, "seek": 164852, "start": 1659.8799999999999, "end": 1661.44, "text": " to memorization pattern matching.", "tokens": [50932, 281, 10560, 2144, 5102, 14324, 13, 51010], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 668, "seek": 164852, "start": 1661.44, "end": 1663.16, "text": " They have this very unique ability", "tokens": [51010, 814, 362, 341, 588, 3845, 3485, 51096], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 669, "seek": 164852, "start": 1663.16, "end": 1665.56, "text": " to adapt to new situations on the fly.", "tokens": [51096, 281, 6231, 281, 777, 6851, 322, 264, 3603, 13, 51216], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 670, "seek": 164852, "start": 1665.56, "end": 1668.16, "text": " This is exactly what enables you to navigate", "tokens": [51216, 639, 307, 2293, 437, 17077, 291, 281, 12350, 51346], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 671, "seek": 164852, "start": 1669.16, "end": 1670.76, "text": " every new day in your life.", "tokens": [51396, 633, 777, 786, 294, 428, 993, 13, 51476], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 672, "seek": 164852, "start": 1670.76, "end": 1671.6, "text": " I'm forgetting the details,", "tokens": [51476, 286, 478, 25428, 264, 4365, 11, 51518], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 673, "seek": 164852, "start": 1671.6, "end": 1674.12, "text": " but there was some study that chess grandmasters", "tokens": [51518, 457, 456, 390, 512, 2979, 300, 24122, 2697, 3799, 1559, 51644], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 674, "seek": 164852, "start": 1674.12, "end": 1676.84, "text": " will perform very well within the context of the moves that-", "tokens": [51644, 486, 2042, 588, 731, 1951, 264, 4319, 295, 264, 6067, 300, 12, 51780], "temperature": 0.0, "avg_logprob": -0.17722054088816924, "compression_ratio": 1.7774086378737541, "no_speech_prob": 0.0005190627416595817}, {"id": 675, "seek": 167684, "start": 1676.84, "end": 1679.56, "text": " Excellent example, because chess at the highest level", "tokens": [50364, 16723, 1365, 11, 570, 24122, 412, 264, 6343, 1496, 50500], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 676, "seek": 167684, "start": 1679.56, "end": 1682.0, "text": " is all about memorization, chess memorization.", "tokens": [50500, 307, 439, 466, 10560, 2144, 11, 24122, 10560, 2144, 13, 50622], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 677, "seek": 167684, "start": 1682.0, "end": 1682.9599999999998, "text": " Okay, sure, we can leave that aside.", "tokens": [50622, 1033, 11, 988, 11, 321, 393, 1856, 300, 7359, 13, 50670], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 678, "seek": 167684, "start": 1682.9599999999998, "end": 1685.52, "text": " What is your explanation for the original question of", "tokens": [50670, 708, 307, 428, 10835, 337, 264, 3380, 1168, 295, 50798], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 679, "seek": 167684, "start": 1685.52, "end": 1690.52, "text": " why in context the GPT- sorry, Gemini 1.5", "tokens": [50798, 983, 294, 4319, 264, 26039, 51, 12, 2597, 11, 22894, 3812, 502, 13, 20, 51048], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 680, "seek": 167684, "start": 1691.8, "end": 1693.52, "text": " was able to learn a language,", "tokens": [51112, 390, 1075, 281, 1466, 257, 2856, 11, 51198], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 681, "seek": 167684, "start": 1693.52, "end": 1695.6, "text": " including the complex grammar structure?", "tokens": [51198, 3009, 264, 3997, 22317, 3877, 30, 51302], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 682, "seek": 167684, "start": 1695.6, "end": 1697.3999999999999, "text": " Doesn't that show that they can pick up new knowledge?", "tokens": [51302, 12955, 380, 300, 855, 300, 436, 393, 1888, 493, 777, 3601, 30, 51392], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 683, "seek": 167684, "start": 1697.3999999999999, "end": 1700.12, "text": " I would assume that it has simply mined", "tokens": [51392, 286, 576, 6552, 300, 309, 575, 2935, 923, 292, 51528], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 684, "seek": 167684, "start": 1700.12, "end": 1703.8799999999999, "text": " from its extremely extensive and imaginably vast", "tokens": [51528, 490, 1080, 4664, 13246, 293, 23427, 1188, 8369, 51716], "temperature": 0.0, "avg_logprob": -0.23043042215807685, "compression_ratio": 1.5395189003436427, "no_speech_prob": 0.003878908697515726}, {"id": 685, "seek": 170388, "start": 1703.88, "end": 1707.4, "text": " training data, it has mined the required template", "tokens": [50364, 3097, 1412, 11, 309, 575, 923, 292, 264, 4739, 12379, 50540], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 686, "seek": 170388, "start": 1707.4, "end": 1708.88, "text": " and then it's just reusing it.", "tokens": [50540, 293, 550, 309, 311, 445, 319, 7981, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 687, "seek": 170388, "start": 1708.88, "end": 1711.3600000000001, "text": " We know that they have a very poor ability", "tokens": [50614, 492, 458, 300, 436, 362, 257, 588, 4716, 3485, 50738], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 688, "seek": 170388, "start": 1711.3600000000001, "end": 1714.8400000000001, "text": " to synthesize new program templates like this on the fly", "tokens": [50738, 281, 26617, 1125, 777, 1461, 21165, 411, 341, 322, 264, 3603, 50912], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 689, "seek": 170388, "start": 1714.8400000000001, "end": 1716.64, "text": " or even adapt existing ones.", "tokens": [50912, 420, 754, 6231, 6741, 2306, 13, 51002], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 690, "seek": 170388, "start": 1716.64, "end": 1718.7600000000002, "text": " They're very much limited to fetching.", "tokens": [51002, 814, 434, 588, 709, 5567, 281, 23673, 278, 13, 51108], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 691, "seek": 170388, "start": 1718.7600000000002, "end": 1720.88, "text": " Suppose there's a programmer at Google,", "tokens": [51108, 21360, 456, 311, 257, 32116, 412, 3329, 11, 51214], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 692, "seek": 170388, "start": 1720.88, "end": 1722.88, "text": " they go into the office in the morning.", "tokens": [51214, 436, 352, 666, 264, 3398, 294, 264, 2446, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 693, "seek": 170388, "start": 1722.88, "end": 1724.2800000000002, "text": " At what point are they doing something", "tokens": [51314, 1711, 437, 935, 366, 436, 884, 746, 51384], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 694, "seek": 170388, "start": 1724.2800000000002, "end": 1727.7600000000002, "text": " that 100% cannot be due to fetching some template", "tokens": [51384, 300, 2319, 4, 2644, 312, 3462, 281, 23673, 278, 512, 12379, 51558], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 695, "seek": 170388, "start": 1727.7600000000002, "end": 1730.5200000000002, "text": " that even if they, suppose they were an LLM,", "tokens": [51558, 300, 754, 498, 436, 11, 7297, 436, 645, 364, 441, 43, 44, 11, 51696], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 696, "seek": 170388, "start": 1730.5200000000002, "end": 1732.1200000000001, "text": " they could not do if they had fetched some template", "tokens": [51696, 436, 727, 406, 360, 498, 436, 632, 23673, 292, 512, 12379, 51776], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 697, "seek": 170388, "start": 1732.1200000000001, "end": 1732.96, "text": " from their program.", "tokens": [51776, 490, 641, 1461, 13, 51818], "temperature": 0.0, "avg_logprob": -0.17081004175646552, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0055442978627979755}, {"id": 698, "seek": 173296, "start": 1733.0, "end": 1733.92, "text": " At what point do they have to use", "tokens": [50366, 1711, 437, 935, 360, 436, 362, 281, 764, 50412], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 699, "seek": 173296, "start": 1733.92, "end": 1735.68, "text": " this so-called extreme generalization capability?", "tokens": [50412, 341, 370, 12, 11880, 8084, 2674, 2144, 13759, 30, 50500], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 700, "seek": 173296, "start": 1735.68, "end": 1737.56, "text": " Forget about Google software developers.", "tokens": [50500, 18675, 466, 3329, 4722, 8849, 13, 50594], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 701, "seek": 173296, "start": 1737.56, "end": 1740.4, "text": " Every human, every day of their lives", "tokens": [50594, 2048, 1952, 11, 633, 786, 295, 641, 2909, 50736], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 702, "seek": 173296, "start": 1740.4, "end": 1743.92, "text": " is full of novel things that they've not been prepared for.", "tokens": [50736, 307, 1577, 295, 7613, 721, 300, 436, 600, 406, 668, 4927, 337, 13, 50912], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 703, "seek": 173296, "start": 1743.92, "end": 1747.88, "text": " You cannot navigate your life based on memorization alone.", "tokens": [50912, 509, 2644, 12350, 428, 993, 2361, 322, 10560, 2144, 3312, 13, 51110], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 704, "seek": 173296, "start": 1747.88, "end": 1748.72, "text": " It's impossible.", "tokens": [51110, 467, 311, 6243, 13, 51152], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 705, "seek": 173296, "start": 1748.72, "end": 1751.44, "text": " I'm sort of denying the premise that they're,", "tokens": [51152, 286, 478, 1333, 295, 30363, 264, 22045, 300, 436, 434, 11, 51288], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 706, "seek": 173296, "start": 1751.44, "end": 1753.04, "text": " you also agree they're not doing like,", "tokens": [51288, 291, 611, 3986, 436, 434, 406, 884, 411, 11, 51368], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 707, "seek": 173296, "start": 1753.04, "end": 1754.6000000000001, "text": " quote-unquote memorization.", "tokens": [51368, 6513, 12, 409, 25016, 10560, 2144, 13, 51446], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 708, "seek": 173296, "start": 1754.6000000000001, "end": 1757.4, "text": " It seems like you're saying they're less capable", "tokens": [51446, 467, 2544, 411, 291, 434, 1566, 436, 434, 1570, 8189, 51586], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 709, "seek": 173296, "start": 1757.4, "end": 1759.44, "text": " of generalization, but I'm just curious of like,", "tokens": [51586, 295, 2674, 2144, 11, 457, 286, 478, 445, 6369, 295, 411, 11, 51688], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 710, "seek": 173296, "start": 1759.44, "end": 1761.64, "text": " the kind of generalization they do,", "tokens": [51688, 264, 733, 295, 2674, 2144, 436, 360, 11, 51798], "temperature": 0.0, "avg_logprob": -0.21468430095248753, "compression_ratio": 1.7412140575079873, "no_speech_prob": 0.0010311754886060953}, {"id": 711, "seek": 176164, "start": 1761.96, "end": 1764.24, "text": " if you get into the office", "tokens": [50380, 498, 291, 483, 666, 264, 3398, 50494], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 712, "seek": 176164, "start": 1764.24, "end": 1765.68, "text": " and you try to do this kind of generalization,", "tokens": [50494, 293, 291, 853, 281, 360, 341, 733, 295, 2674, 2144, 11, 50566], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 713, "seek": 176164, "start": 1765.68, "end": 1766.64, "text": " you're gonna fail at your job.", "tokens": [50566, 291, 434, 799, 3061, 412, 428, 1691, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 714, "seek": 176164, "start": 1766.64, "end": 1768.64, "text": " But what is the first point, you're a programmer.", "tokens": [50614, 583, 437, 307, 264, 700, 935, 11, 291, 434, 257, 32116, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 715, "seek": 176164, "start": 1768.64, "end": 1770.64, "text": " What is the first point when you try to do that generalization,", "tokens": [50714, 708, 307, 264, 700, 935, 562, 291, 853, 281, 360, 300, 2674, 2144, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 716, "seek": 176164, "start": 1770.64, "end": 1772.1200000000001, "text": " you would lose your job", "tokens": [50814, 291, 576, 3624, 428, 1691, 50888], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 717, "seek": 176164, "start": 1772.1200000000001, "end": 1774.64, "text": " because you can't do the extreme generalization?", "tokens": [50888, 570, 291, 393, 380, 360, 264, 8084, 2674, 2144, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 718, "seek": 176164, "start": 1774.64, "end": 1776.1200000000001, "text": " I don't have any specific examples,", "tokens": [51014, 286, 500, 380, 362, 604, 2685, 5110, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 719, "seek": 176164, "start": 1776.1200000000001, "end": 1781.1200000000001, "text": " but literally like, take this situation for instance,", "tokens": [51088, 457, 3736, 411, 11, 747, 341, 2590, 337, 5197, 11, 51338], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 720, "seek": 176164, "start": 1781.1200000000001, "end": 1783.68, "text": " you've never been here in this room.", "tokens": [51338, 291, 600, 1128, 668, 510, 294, 341, 1808, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 721, "seek": 176164, "start": 1783.68, "end": 1786.4, "text": " Maybe you've been in this city a few times, I don't know,", "tokens": [51466, 2704, 291, 600, 668, 294, 341, 2307, 257, 1326, 1413, 11, 286, 500, 380, 458, 11, 51602], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 722, "seek": 176164, "start": 1786.4, "end": 1789.2800000000002, "text": " but there's a fair amount of novelty.", "tokens": [51602, 457, 456, 311, 257, 3143, 2372, 295, 44805, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1559238831202189, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.0006221430958248675}, {"id": 723, "seek": 178928, "start": 1789.28, "end": 1791.8799999999999, "text": " You've never been interviewing me.", "tokens": [50364, 509, 600, 1128, 668, 26524, 385, 13, 50494], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 724, "seek": 178928, "start": 1791.8799999999999, "end": 1793.8, "text": " There's a fair amount of novelty", "tokens": [50494, 821, 311, 257, 3143, 2372, 295, 44805, 50590], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 725, "seek": 178928, "start": 1793.8, "end": 1795.8799999999999, "text": " every hour of every day in your life.", "tokens": [50590, 633, 1773, 295, 633, 786, 294, 428, 993, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 726, "seek": 178928, "start": 1795.8799999999999, "end": 1798.92, "text": " And it's in fact, by and large,", "tokens": [50694, 400, 309, 311, 294, 1186, 11, 538, 293, 2416, 11, 50846], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 727, "seek": 178928, "start": 1798.92, "end": 1802.28, "text": " more novelty than any LLM could handle.", "tokens": [50846, 544, 44805, 813, 604, 441, 43, 44, 727, 4813, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 728, "seek": 178928, "start": 1802.28, "end": 1804.84, "text": " Like if you just put a LLM in a robot,", "tokens": [51014, 1743, 498, 291, 445, 829, 257, 441, 43, 44, 294, 257, 7881, 11, 51142], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 729, "seek": 178928, "start": 1804.84, "end": 1806.2, "text": " it could not be doing all the things", "tokens": [51142, 309, 727, 406, 312, 884, 439, 264, 721, 51210], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 730, "seek": 178928, "start": 1806.2, "end": 1808.8, "text": " that you've been doing today, right?", "tokens": [51210, 300, 291, 600, 668, 884, 965, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 731, "seek": 178928, "start": 1808.8, "end": 1811.6, "text": " Or take on like cell driving cars, for instance.", "tokens": [51340, 1610, 747, 322, 411, 2815, 4840, 5163, 11, 337, 5197, 13, 51480], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 732, "seek": 178928, "start": 1811.6, "end": 1815.32, "text": " You take a cell driving car operating in the barrier.", "tokens": [51480, 509, 747, 257, 2815, 4840, 1032, 7447, 294, 264, 13357, 13, 51666], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 733, "seek": 178928, "start": 1815.32, "end": 1818.0, "text": " Do you think you could just drop it in New York City", "tokens": [51666, 1144, 291, 519, 291, 727, 445, 3270, 309, 294, 1873, 3609, 4392, 51800], "temperature": 0.0, "avg_logprob": -0.16040502067740636, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.0002409614680800587}, {"id": 734, "seek": 181800, "start": 1818.0, "end": 1821.24, "text": " or drop it in London where people drive on the left?", "tokens": [50364, 420, 3270, 309, 294, 7042, 689, 561, 3332, 322, 264, 1411, 30, 50526], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 735, "seek": 181800, "start": 1821.24, "end": 1822.36, "text": " No, it's gonna fail.", "tokens": [50526, 883, 11, 309, 311, 799, 3061, 13, 50582], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 736, "seek": 181800, "start": 1822.36, "end": 1824.88, "text": " So not only can you drop, not like,", "tokens": [50582, 407, 406, 787, 393, 291, 3270, 11, 406, 411, 11, 50708], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 737, "seek": 181800, "start": 1824.88, "end": 1829.88, "text": " make it generalize to a change of rules of driving rules,", "tokens": [50708, 652, 309, 2674, 1125, 281, 257, 1319, 295, 4474, 295, 4840, 4474, 11, 50958], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 738, "seek": 181800, "start": 1831.6, "end": 1834.48, "text": " but you can not even make it generalize to a new city.", "tokens": [51044, 457, 291, 393, 406, 754, 652, 309, 2674, 1125, 281, 257, 777, 2307, 13, 51188], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 739, "seek": 181800, "start": 1834.48, "end": 1838.2, "text": " It needs to be trained on each specific environment.", "tokens": [51188, 467, 2203, 281, 312, 8895, 322, 1184, 2685, 2823, 13, 51374], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 740, "seek": 181800, "start": 1838.2, "end": 1841.68, "text": " I mean, I agree that self-driving cars aren't AGI.", "tokens": [51374, 286, 914, 11, 286, 3986, 300, 2698, 12, 47094, 5163, 3212, 380, 316, 26252, 13, 51548], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 741, "seek": 181800, "start": 1841.68, "end": 1842.84, "text": " But it's the same type of model,", "tokens": [51548, 583, 309, 311, 264, 912, 2010, 295, 2316, 11, 51606], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 742, "seek": 181800, "start": 1842.84, "end": 1844.12, "text": " they're transformers as well.", "tokens": [51606, 436, 434, 4088, 433, 382, 731, 13, 51670], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 743, "seek": 181800, "start": 1844.12, "end": 1845.64, "text": " I mean, I don't know,", "tokens": [51670, 286, 914, 11, 286, 500, 380, 458, 11, 51746], "temperature": 0.0, "avg_logprob": -0.20421334514467734, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.0004436369927134365}, {"id": 744, "seek": 184564, "start": 1846.24, "end": 1848.0400000000002, "text": " they also have brains with neurons in them,", "tokens": [50394, 436, 611, 362, 15442, 365, 22027, 294, 552, 11, 50484], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 745, "seek": 184564, "start": 1848.0400000000002, "end": 1849.88, "text": " but they're less intelligent because they're small.", "tokens": [50484, 457, 436, 434, 1570, 13232, 570, 436, 434, 1359, 13, 50576], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 746, "seek": 184564, "start": 1849.88, "end": 1850.72, "text": " It's not the same architecture.", "tokens": [50576, 467, 311, 406, 264, 912, 9482, 13, 50618], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 747, "seek": 184564, "start": 1850.72, "end": 1851.5600000000002, "text": " We can get into that.", "tokens": [50618, 492, 393, 483, 666, 300, 13, 50660], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 748, "seek": 184564, "start": 1851.5600000000002, "end": 1856.5600000000002, "text": " But so I still don't understand like a concrete thing of,", "tokens": [50660, 583, 370, 286, 920, 500, 380, 1223, 411, 257, 9859, 551, 295, 11, 50910], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 749, "seek": 184564, "start": 1857.5600000000002, "end": 1858.92, "text": " we also need training.", "tokens": [50960, 321, 611, 643, 3097, 13, 51028], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 750, "seek": 184564, "start": 1858.92, "end": 1860.0, "text": " That's why education exists.", "tokens": [51028, 663, 311, 983, 3309, 8198, 13, 51082], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 751, "seek": 184564, "start": 1860.0, "end": 1862.2, "text": " That's why we had to spend the first 18 years of our life", "tokens": [51082, 663, 311, 983, 321, 632, 281, 3496, 264, 700, 2443, 924, 295, 527, 993, 51192], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 752, "seek": 184564, "start": 1862.2, "end": 1863.2800000000002, "text": " doing drills.", "tokens": [51192, 884, 36126, 13, 51246], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 753, "seek": 184564, "start": 1863.2800000000002, "end": 1866.16, "text": " We have a memory, but we are not a memory.", "tokens": [51246, 492, 362, 257, 4675, 11, 457, 321, 366, 406, 257, 4675, 13, 51390], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 754, "seek": 184564, "start": 1866.16, "end": 1868.1200000000001, "text": " We are not limited to just a memory.", "tokens": [51390, 492, 366, 406, 5567, 281, 445, 257, 4675, 13, 51488], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 755, "seek": 184564, "start": 1868.1200000000001, "end": 1870.0, "text": " I'm denying the premise that that's necessarily", "tokens": [51488, 286, 478, 30363, 264, 22045, 300, 300, 311, 4725, 51582], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 756, "seek": 184564, "start": 1870.0, "end": 1871.1200000000001, "text": " the only thing these models are doing.", "tokens": [51582, 264, 787, 551, 613, 5245, 366, 884, 13, 51638], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 757, "seek": 184564, "start": 1871.1200000000001, "end": 1873.72, "text": " And I'm still not sure what is the task", "tokens": [51638, 400, 286, 478, 920, 406, 988, 437, 307, 264, 5633, 51768], "temperature": 0.0, "avg_logprob": -0.23406866212554325, "compression_ratio": 1.7411003236245954, "no_speech_prob": 0.004068530164659023}, {"id": 758, "seek": 187372, "start": 1873.72, "end": 1877.0, "text": " that a remote worker would have to,", "tokens": [50364, 300, 257, 8607, 11346, 576, 362, 281, 11, 50528], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 759, "seek": 187372, "start": 1877.0, "end": 1879.2, "text": " suppose you do some remote work with an LLM", "tokens": [50528, 7297, 291, 360, 512, 8607, 589, 365, 364, 441, 43, 44, 50638], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 760, "seek": 187372, "start": 1879.2, "end": 1880.48, "text": " and they're programmer,", "tokens": [50638, 293, 436, 434, 32116, 11, 50702], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 761, "seek": 187372, "start": 1880.48, "end": 1882.04, "text": " what is the first point that you realize", "tokens": [50702, 437, 307, 264, 700, 935, 300, 291, 4325, 50780], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 762, "seek": 187372, "start": 1882.04, "end": 1883.76, "text": " this is not a human, this is an LLM?", "tokens": [50780, 341, 307, 406, 257, 1952, 11, 341, 307, 364, 441, 43, 44, 30, 50866], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 763, "seek": 187372, "start": 1883.76, "end": 1886.08, "text": " What about they just send them a knock puzzle", "tokens": [50866, 708, 466, 436, 445, 2845, 552, 257, 6728, 12805, 50982], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 764, "seek": 187372, "start": 1886.08, "end": 1887.28, "text": " and see how they do?", "tokens": [50982, 293, 536, 577, 436, 360, 30, 51042], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 765, "seek": 187372, "start": 1887.28, "end": 1889.24, "text": " No, like part of their job, you know?", "tokens": [51042, 883, 11, 411, 644, 295, 641, 1691, 11, 291, 458, 30, 51140], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 766, "seek": 187372, "start": 1889.24, "end": 1892.44, "text": " But you have to deal with novelty all the time.", "tokens": [51140, 583, 291, 362, 281, 2028, 365, 44805, 439, 264, 565, 13, 51300], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 767, "seek": 187372, "start": 1892.44, "end": 1894.72, "text": " Okay, so if you, is there a world in which", "tokens": [51300, 1033, 11, 370, 498, 291, 11, 307, 456, 257, 1002, 294, 597, 51414], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 768, "seek": 187372, "start": 1894.72, "end": 1896.56, "text": " all the programmers are replaced?", "tokens": [51414, 439, 264, 41504, 366, 10772, 30, 51506], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 769, "seek": 187372, "start": 1896.56, "end": 1898.3600000000001, "text": " And then we're still saying,", "tokens": [51506, 400, 550, 321, 434, 920, 1566, 11, 51596], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 770, "seek": 187372, "start": 1898.3600000000001, "end": 1900.1200000000001, "text": " but they're only doing memorization", "tokens": [51596, 457, 436, 434, 787, 884, 10560, 2144, 51684], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 771, "seek": 187372, "start": 1900.1200000000001, "end": 1901.44, "text": " late in programming tasks,", "tokens": [51684, 3469, 294, 9410, 9608, 11, 51750], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 772, "seek": 187372, "start": 1901.44, "end": 1903.16, "text": " but they're still producing a trillion dollars", "tokens": [51750, 457, 436, 434, 920, 10501, 257, 18723, 3808, 51836], "temperature": 0.0, "avg_logprob": -0.18575867466956564, "compression_ratio": 1.7515923566878981, "no_speech_prob": 0.010311213321983814}, {"id": 773, "seek": 190316, "start": 1904.16, "end": 1906.6000000000001, "text": " worth of output in the form of code.", "tokens": [50414, 3163, 295, 5598, 294, 264, 1254, 295, 3089, 13, 50536], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 774, "seek": 190316, "start": 1906.6000000000001, "end": 1908.4, "text": " Software development is actually a pretty good example", "tokens": [50536, 27428, 3250, 307, 767, 257, 1238, 665, 1365, 50626], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 775, "seek": 190316, "start": 1908.4, "end": 1911.64, "text": " of a job where you're dealing with novelty all the time.", "tokens": [50626, 295, 257, 1691, 689, 291, 434, 6260, 365, 44805, 439, 264, 565, 13, 50788], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 776, "seek": 190316, "start": 1911.64, "end": 1913.72, "text": " Or if you're not, well, I'm not sure what you're doing.", "tokens": [50788, 1610, 498, 291, 434, 406, 11, 731, 11, 286, 478, 406, 988, 437, 291, 434, 884, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 777, "seek": 190316, "start": 1913.72, "end": 1917.6000000000001, "text": " So I personally use Genetic VI very little", "tokens": [50892, 407, 286, 5665, 764, 3632, 3532, 27619, 588, 707, 51086], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 778, "seek": 190316, "start": 1917.6000000000001, "end": 1919.72, "text": " in my software development job.", "tokens": [51086, 294, 452, 4722, 3250, 1691, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 779, "seek": 190316, "start": 1919.72, "end": 1923.0800000000002, "text": " And before LLMs, I think I was also using", "tokens": [51192, 400, 949, 441, 43, 26386, 11, 286, 519, 286, 390, 611, 1228, 51360], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 780, "seek": 190316, "start": 1923.0800000000002, "end": 1925.0, "text": " Stack Overflow very little.", "tokens": [51360, 37649, 4886, 10565, 588, 707, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 781, "seek": 190316, "start": 1925.0, "end": 1927.44, "text": " You know, some people maybe are just copy-pasting stuff", "tokens": [51456, 509, 458, 11, 512, 561, 1310, 366, 445, 5055, 12, 79, 30587, 1507, 51578], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 782, "seek": 190316, "start": 1927.44, "end": 1928.28, "text": " from Stack Overflow,", "tokens": [51578, 490, 37649, 4886, 10565, 11, 51620], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 783, "seek": 190316, "start": 1928.28, "end": 1930.68, "text": " or nowadays copy-pasting stuff from an LLM.", "tokens": [51620, 420, 13434, 5055, 12, 79, 30587, 1507, 490, 364, 441, 43, 44, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2583023636429398, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0008150682551786304}, {"id": 784, "seek": 193068, "start": 1931.68, "end": 1934.96, "text": " Personally, I try to focus on problem-solving.", "tokens": [50414, 21079, 11, 286, 853, 281, 1879, 322, 1154, 12, 30926, 798, 13, 50578], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 785, "seek": 193068, "start": 1934.96, "end": 1936.92, "text": " The syntax is just a technical detail.", "tokens": [50578, 440, 28431, 307, 445, 257, 6191, 2607, 13, 50676], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 786, "seek": 193068, "start": 1936.92, "end": 1939.3600000000001, "text": " What's really important is the problem-solving.", "tokens": [50676, 708, 311, 534, 1021, 307, 264, 1154, 12, 30926, 798, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 787, "seek": 193068, "start": 1939.3600000000001, "end": 1943.96, "text": " Like the essence of programming is engineering", "tokens": [50798, 1743, 264, 12801, 295, 9410, 307, 7043, 51028], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 788, "seek": 193068, "start": 1943.96, "end": 1947.3600000000001, "text": " mental models, like mental representations", "tokens": [51028, 4973, 5245, 11, 411, 4973, 33358, 51198], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 789, "seek": 193068, "start": 1947.3600000000001, "end": 1949.48, "text": " of the problem you're trying to solve.", "tokens": [51198, 295, 264, 1154, 291, 434, 1382, 281, 5039, 13, 51304], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 790, "seek": 193068, "start": 1949.48, "end": 1952.52, "text": " But you can, you know, we have many,", "tokens": [51304, 583, 291, 393, 11, 291, 458, 11, 321, 362, 867, 11, 51456], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 791, "seek": 193068, "start": 1952.52, "end": 1954.2, "text": " people can interact with these systems themselves", "tokens": [51456, 561, 393, 4648, 365, 613, 3652, 2969, 51540], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 792, "seek": 193068, "start": 1954.2, "end": 1956.2, "text": " and you can go to chat GPT and say,", "tokens": [51540, 293, 291, 393, 352, 281, 5081, 26039, 51, 293, 584, 11, 51640], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 793, "seek": 193068, "start": 1956.2, "end": 1958.64, "text": " here's a specification of the kind of program I want.", "tokens": [51640, 510, 311, 257, 31256, 295, 264, 733, 295, 1461, 286, 528, 13, 51762], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 794, "seek": 193068, "start": 1958.64, "end": 1959.5600000000002, "text": " They'll build it for you.", "tokens": [51762, 814, 603, 1322, 309, 337, 291, 13, 51808], "temperature": 0.0, "avg_logprob": -0.16937710571289064, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0006985983927734196}, {"id": 795, "seek": 195956, "start": 1959.6399999999999, "end": 1961.8, "text": " As long as there are many examples of this program", "tokens": [50368, 1018, 938, 382, 456, 366, 867, 5110, 295, 341, 1461, 50476], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 796, "seek": 195956, "start": 1961.8, "end": 1963.6, "text": " on like GitHub and Stack Overflow and so on,", "tokens": [50476, 322, 411, 23331, 293, 37649, 4886, 10565, 293, 370, 322, 11, 50566], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 797, "seek": 195956, "start": 1963.6, "end": 1967.32, "text": " sure, they will fetch the program for you from their memory.", "tokens": [50566, 988, 11, 436, 486, 23673, 264, 1461, 337, 291, 490, 641, 4675, 13, 50752], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 798, "seek": 195956, "start": 1967.32, "end": 1969.1599999999999, "text": " But you can change arbitrary details.", "tokens": [50752, 583, 291, 393, 1319, 23211, 4365, 13, 50844], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 799, "seek": 195956, "start": 1969.1599999999999, "end": 1972.32, "text": " You can say I need it to work on this different kind of server.", "tokens": [50844, 509, 393, 584, 286, 643, 309, 281, 589, 322, 341, 819, 733, 295, 7154, 13, 51002], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 800, "seek": 195956, "start": 1972.32, "end": 1975.9199999999998, "text": " If that were true, there would be no software engineers today.", "tokens": [51002, 759, 300, 645, 2074, 11, 456, 576, 312, 572, 4722, 11955, 965, 13, 51182], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 801, "seek": 195956, "start": 1975.9199999999998, "end": 1978.28, "text": " I agree. We're not at a full AGI yet,", "tokens": [51182, 286, 3986, 13, 492, 434, 406, 412, 257, 1577, 316, 26252, 1939, 11, 51300], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 802, "seek": 195956, "start": 1978.28, "end": 1980.36, "text": " in the sense that these models have,", "tokens": [51300, 294, 264, 2020, 300, 613, 5245, 362, 11, 51404], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 803, "seek": 195956, "start": 1980.36, "end": 1982.56, "text": " let's say, less than a trillion parameters.", "tokens": [51404, 718, 311, 584, 11, 1570, 813, 257, 18723, 9834, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 804, "seek": 195956, "start": 1982.56, "end": 1984.2, "text": " A human brain has somewhere on the order", "tokens": [51514, 316, 1952, 3567, 575, 4079, 322, 264, 1668, 51596], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 805, "seek": 195956, "start": 1984.2, "end": 1985.96, "text": " of 10 to 30 trillion synapses.", "tokens": [51596, 295, 1266, 281, 2217, 18723, 5451, 2382, 279, 13, 51684], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 806, "seek": 195956, "start": 1985.96, "end": 1988.48, "text": " I mean, if you were just doing some naive math,", "tokens": [51684, 286, 914, 11, 498, 291, 645, 445, 884, 512, 29052, 5221, 11, 51810], "temperature": 0.0, "avg_logprob": -0.19535609829810358, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0008397938217967749}, {"id": 807, "seek": 198848, "start": 1988.52, "end": 1991.1200000000001, "text": " you're like at least 10x under parameterized.", "tokens": [50366, 291, 434, 411, 412, 1935, 1266, 87, 833, 13075, 1602, 13, 50496], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 808, "seek": 198848, "start": 1991.1200000000001, "end": 1992.64, "text": " So I agree we're not there yet,", "tokens": [50496, 407, 286, 3986, 321, 434, 406, 456, 1939, 11, 50572], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 809, "seek": 198848, "start": 1992.64, "end": 1997.28, "text": " but I'm sort of confused on why we're not on the spectrum,", "tokens": [50572, 457, 286, 478, 1333, 295, 9019, 322, 983, 321, 434, 406, 322, 264, 11143, 11, 50804], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 810, "seek": 198848, "start": 1997.28, "end": 1999.4, "text": " where yes, I agree that there's many kinds", "tokens": [50804, 689, 2086, 11, 286, 3986, 300, 456, 311, 867, 3685, 50910], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 811, "seek": 198848, "start": 1999.4, "end": 2000.88, "text": " of generalization they can do,", "tokens": [50910, 295, 2674, 2144, 436, 393, 360, 11, 50984], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 812, "seek": 198848, "start": 2000.88, "end": 2002.76, "text": " but it seems like they're on this kind of smooth spectrum", "tokens": [50984, 457, 309, 2544, 411, 436, 434, 322, 341, 733, 295, 5508, 11143, 51078], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 813, "seek": 198848, "start": 2002.76, "end": 2004.16, "text": " that we see even within humans,", "tokens": [51078, 300, 321, 536, 754, 1951, 6255, 11, 51148], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 814, "seek": 198848, "start": 2004.16, "end": 2007.3600000000001, "text": " where some humans would have a hard time doing an ARC type test.", "tokens": [51148, 689, 512, 6255, 576, 362, 257, 1152, 565, 884, 364, 8943, 34, 2010, 1500, 13, 51308], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 815, "seek": 198848, "start": 2007.3600000000001, "end": 2008.8, "text": " We see that based on the performance", "tokens": [51308, 492, 536, 300, 2361, 322, 264, 3389, 51380], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 816, "seek": 198848, "start": 2008.8, "end": 2011.04, "text": " on progressive Ravens matrices type IQ tests.", "tokens": [51380, 322, 16131, 28956, 82, 32284, 2010, 28921, 6921, 13, 51492], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 817, "seek": 198848, "start": 2011.04, "end": 2014.04, "text": " I'm not a fan of IQ tests because for the most part,", "tokens": [51492, 286, 478, 406, 257, 3429, 295, 28921, 6921, 570, 337, 264, 881, 644, 11, 51642], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 818, "seek": 198848, "start": 2014.04, "end": 2017.72, "text": " you can train on IQ tests and get better at them.", "tokens": [51642, 291, 393, 3847, 322, 28921, 6921, 293, 483, 1101, 412, 552, 13, 51826], "temperature": 0.0, "avg_logprob": -0.15711672894366377, "compression_ratio": 1.7381703470031546, "no_speech_prob": 0.0002531379577703774}, {"id": 819, "seek": 201772, "start": 2017.76, "end": 2019.88, "text": " So they have very much memorization based.", "tokens": [50366, 407, 436, 362, 588, 709, 10560, 2144, 2361, 13, 50472], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 820, "seek": 201772, "start": 2019.88, "end": 2022.3600000000001, "text": " And this is actually the main pitfall", "tokens": [50472, 400, 341, 307, 767, 264, 2135, 10147, 6691, 50596], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 821, "seek": 201772, "start": 2022.3600000000001, "end": 2025.88, "text": " that ARC tries not to fall far.", "tokens": [50596, 300, 8943, 34, 9898, 406, 281, 2100, 1400, 13, 50772], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 822, "seek": 201772, "start": 2025.88, "end": 2026.76, "text": " I'm still not confused.", "tokens": [50772, 286, 478, 920, 406, 9019, 13, 50816], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 823, "seek": 201772, "start": 2026.76, "end": 2029.88, "text": " So if all remote jobs are automated", "tokens": [50816, 407, 498, 439, 8607, 4782, 366, 18473, 50972], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 824, "seek": 201772, "start": 2029.88, "end": 2032.24, "text": " in the next five years, let's say,", "tokens": [50972, 294, 264, 958, 1732, 924, 11, 718, 311, 584, 11, 51090], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 825, "seek": 201772, "start": 2032.24, "end": 2034.8, "text": " at least that don't require you to be like sort of a service.", "tokens": [51090, 412, 1935, 300, 500, 380, 3651, 291, 281, 312, 411, 1333, 295, 257, 2643, 13, 51218], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 826, "seek": 201772, "start": 2034.8, "end": 2036.0, "text": " It's not like a salesperson", "tokens": [51218, 467, 311, 406, 411, 257, 5763, 10813, 51278], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 827, "seek": 201772, "start": 2036.0, "end": 2037.48, "text": " where you want the human to be talking,", "tokens": [51278, 689, 291, 528, 264, 1952, 281, 312, 1417, 11, 51352], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 828, "seek": 201772, "start": 2037.48, "end": 2038.84, "text": " but like programming or whatever.", "tokens": [51352, 457, 411, 9410, 420, 2035, 13, 51420], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 829, "seek": 201772, "start": 2038.84, "end": 2042.84, "text": " In that world, would you say that that's not possible", "tokens": [51420, 682, 300, 1002, 11, 576, 291, 584, 300, 300, 311, 406, 1944, 51620], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 830, "seek": 201772, "start": 2042.84, "end": 2045.6000000000001, "text": " because a lot of what a programmer needs to do,", "tokens": [51620, 570, 257, 688, 295, 437, 257, 32116, 2203, 281, 360, 11, 51758], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 831, "seek": 201772, "start": 2045.6000000000001, "end": 2047.64, "text": " definitely requires things that would not be", "tokens": [51758, 2138, 7029, 721, 300, 576, 406, 312, 51860], "temperature": 0.0, "avg_logprob": -0.19139181243048775, "compression_ratio": 1.6763754045307444, "no_speech_prob": 0.0002232010883744806}, {"id": 832, "seek": 204764, "start": 2047.64, "end": 2048.7200000000003, "text": " in any free training corpus?", "tokens": [50364, 294, 604, 1737, 3097, 1181, 31624, 30, 50418], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 833, "seek": 204764, "start": 2048.7200000000003, "end": 2050.04, "text": " Sure. I mean, in five years,", "tokens": [50418, 4894, 13, 286, 914, 11, 294, 1732, 924, 11, 50484], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 834, "seek": 204764, "start": 2050.04, "end": 2051.56, "text": " there will be more software engineers", "tokens": [50484, 456, 486, 312, 544, 4722, 11955, 50560], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 835, "seek": 204764, "start": 2051.56, "end": 2053.2000000000003, "text": " than there are today and not too well.", "tokens": [50560, 813, 456, 366, 965, 293, 406, 886, 731, 13, 50642], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 836, "seek": 204764, "start": 2053.2000000000003, "end": 2054.28, "text": " But I just want to understand.", "tokens": [50642, 583, 286, 445, 528, 281, 1223, 13, 50696], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 837, "seek": 204764, "start": 2054.28, "end": 2056.04, "text": " So I'm still not sure.", "tokens": [50696, 407, 286, 478, 920, 406, 988, 13, 50784], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 838, "seek": 204764, "start": 2056.04, "end": 2058.1600000000003, "text": " I mean, I know how to, I studied computer science.", "tokens": [50784, 286, 914, 11, 286, 458, 577, 281, 11, 286, 9454, 3820, 3497, 13, 50890], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 839, "seek": 204764, "start": 2058.1600000000003, "end": 2060.12, "text": " I think if I had become a code monkey out of college,", "tokens": [50890, 286, 519, 498, 286, 632, 1813, 257, 3089, 17847, 484, 295, 3859, 11, 50988], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 840, "seek": 204764, "start": 2060.12, "end": 2062.04, "text": " like what would I be doing?", "tokens": [50988, 411, 437, 576, 286, 312, 884, 30, 51084], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 841, "seek": 204764, "start": 2062.04, "end": 2063.32, "text": " I go to my job.", "tokens": [51084, 286, 352, 281, 452, 1691, 13, 51148], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 842, "seek": 204764, "start": 2063.32, "end": 2066.36, "text": " What is the first thing my boss tells me something to do?", "tokens": [51148, 708, 307, 264, 700, 551, 452, 5741, 5112, 385, 746, 281, 360, 30, 51300], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 843, "seek": 204764, "start": 2066.36, "end": 2070.28, "text": " When does he realize I'm an LLM if I was an LLM?", "tokens": [51300, 1133, 775, 415, 4325, 286, 478, 364, 441, 43, 44, 498, 286, 390, 364, 441, 43, 44, 30, 51496], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 844, "seek": 204764, "start": 2070.28, "end": 2072.56, "text": " Probably on the first day, you know?", "tokens": [51496, 9210, 322, 264, 700, 786, 11, 291, 458, 30, 51610], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 845, "seek": 204764, "start": 2072.56, "end": 2073.4, "text": " Again,", "tokens": [51610, 3764, 11, 51652], "temperature": 0.0, "avg_logprob": -0.23089697422125402, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.0035596289671957493}, {"id": 846, "seek": 207340, "start": 2073.6800000000003, "end": 2078.6800000000003, "text": " if it were true that LLMs could generalize", "tokens": [50378, 498, 309, 645, 2074, 300, 441, 43, 26386, 727, 2674, 1125, 50628], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 847, "seek": 207340, "start": 2080.0, "end": 2081.56, "text": " to novel problems like this", "tokens": [50694, 281, 7613, 2740, 411, 341, 50772], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 848, "seek": 207340, "start": 2081.56, "end": 2085.04, "text": " and you can actually develop software", "tokens": [50772, 293, 291, 393, 767, 1499, 4722, 50946], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 849, "seek": 207340, "start": 2085.04, "end": 2086.6800000000003, "text": " to solve a problem they've never seen before,", "tokens": [50946, 281, 5039, 257, 1154, 436, 600, 1128, 1612, 949, 11, 51028], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 850, "seek": 207340, "start": 2086.6800000000003, "end": 2088.7200000000003, "text": " you would not need software engineers anymore.", "tokens": [51028, 291, 576, 406, 643, 4722, 11955, 3602, 13, 51130], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 851, "seek": 207340, "start": 2088.7200000000003, "end": 2091.36, "text": " In practice, if I look at how people are using LLMs", "tokens": [51130, 682, 3124, 11, 498, 286, 574, 412, 577, 561, 366, 1228, 441, 43, 26386, 51262], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 852, "seek": 207340, "start": 2091.36, "end": 2093.1600000000003, "text": " in their software engineering job today,", "tokens": [51262, 294, 641, 4722, 7043, 1691, 965, 11, 51352], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 853, "seek": 207340, "start": 2093.1600000000003, "end": 2096.64, "text": " they're using it as a stack of a flow replacement.", "tokens": [51352, 436, 434, 1228, 309, 382, 257, 8630, 295, 257, 3095, 14419, 13, 51526], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 854, "seek": 207340, "start": 2096.64, "end": 2100.84, "text": " So they're using it as a way to copy paste code snippets", "tokens": [51526, 407, 436, 434, 1228, 309, 382, 257, 636, 281, 5055, 9163, 3089, 35623, 1385, 51736], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 855, "seek": 207340, "start": 2100.84, "end": 2103.08, "text": " to perform very common actions.", "tokens": [51736, 281, 2042, 588, 2689, 5909, 13, 51848], "temperature": 0.0, "avg_logprob": -0.14238331006920857, "compression_ratio": 1.7086614173228347, "no_speech_prob": 0.0005097962566651404}, {"id": 856, "seek": 210308, "start": 2103.08, "end": 2106.6, "text": " And what they actually need is a database of code snippets.", "tokens": [50364, 400, 437, 436, 767, 643, 307, 257, 8149, 295, 3089, 35623, 1385, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 857, "seek": 210308, "start": 2106.6, "end": 2109.56, "text": " They don't actually need any of the abilities", "tokens": [50540, 814, 500, 380, 767, 643, 604, 295, 264, 11582, 50688], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 858, "seek": 210308, "start": 2109.56, "end": 2110.88, "text": " that actually make them software engineers.", "tokens": [50688, 300, 767, 652, 552, 4722, 11955, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 859, "seek": 210308, "start": 2110.88, "end": 2113.56, "text": " I mean, when we talk about interpolating", "tokens": [50754, 286, 914, 11, 562, 321, 751, 466, 44902, 990, 50888], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 860, "seek": 210308, "start": 2113.56, "end": 2115.24, "text": " between stack overflow databases,", "tokens": [50888, 1296, 8630, 37772, 22380, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 861, "seek": 210308, "start": 2115.24, "end": 2116.6, "text": " if you look at the kinds of math problems", "tokens": [50972, 498, 291, 574, 412, 264, 3685, 295, 5221, 2740, 51040], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 862, "seek": 210308, "start": 2116.6, "end": 2120.04, "text": " or coding problems, maybe to say that they're,", "tokens": [51040, 420, 17720, 2740, 11, 1310, 281, 584, 300, 436, 434, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 863, "seek": 210308, "start": 2121.52, "end": 2122.96, "text": " maybe let's step back on interpolation", "tokens": [51286, 1310, 718, 311, 1823, 646, 322, 44902, 399, 51358], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 864, "seek": 210308, "start": 2122.96, "end": 2124.64, "text": " and let me ask the question this way.", "tokens": [51358, 293, 718, 385, 1029, 264, 1168, 341, 636, 13, 51442], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 865, "seek": 210308, "start": 2124.64, "end": 2126.0, "text": " Why can't creativity,", "tokens": [51442, 1545, 393, 380, 12915, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 866, "seek": 210308, "start": 2126.0, "end": 2128.2799999999997, "text": " why isn't creativity just interpolation", "tokens": [51510, 983, 1943, 380, 12915, 445, 44902, 399, 51624], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 867, "seek": 210308, "start": 2128.2799999999997, "end": 2131.4, "text": " in a higher dimension where if a bigger model", "tokens": [51624, 294, 257, 2946, 10139, 689, 498, 257, 3801, 2316, 51780], "temperature": 0.0, "avg_logprob": -0.1609327962079386, "compression_ratio": 1.7659574468085106, "no_speech_prob": 7.601710240123793e-05}, {"id": 868, "seek": 213140, "start": 2131.4, "end": 2133.32, "text": " can learn a more complex manifold,", "tokens": [50364, 393, 1466, 257, 544, 3997, 47138, 11, 50460], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 869, "seek": 213140, "start": 2133.32, "end": 2134.8, "text": " we're gonna use the ML language.", "tokens": [50460, 321, 434, 799, 764, 264, 21601, 2856, 13, 50534], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 870, "seek": 213140, "start": 2134.8, "end": 2138.2400000000002, "text": " And if you look at read a biography of a scientist,", "tokens": [50534, 400, 498, 291, 574, 412, 1401, 257, 37062, 295, 257, 12662, 11, 50706], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 871, "seek": 213140, "start": 2138.2400000000002, "end": 2140.36, "text": " it doesn't feel like they're not zero shot", "tokens": [50706, 309, 1177, 380, 841, 411, 436, 434, 406, 4018, 3347, 50812], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 872, "seek": 213140, "start": 2140.36, "end": 2141.28, "text": " in new scientific theories.", "tokens": [50812, 294, 777, 8134, 13667, 13, 50858], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 873, "seek": 213140, "start": 2141.28, "end": 2143.1600000000003, "text": " They're playing with existing ideas.", "tokens": [50858, 814, 434, 2433, 365, 6741, 3487, 13, 50952], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 874, "seek": 213140, "start": 2143.1600000000003, "end": 2145.2000000000003, "text": " They're trying to juxtapose them in their head.", "tokens": [50952, 814, 434, 1382, 281, 3649, 734, 569, 541, 552, 294, 641, 1378, 13, 51054], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 875, "seek": 213140, "start": 2145.2000000000003, "end": 2149.7200000000003, "text": " They try out some like slightly ever in the tree", "tokens": [51054, 814, 853, 484, 512, 411, 4748, 1562, 294, 264, 4230, 51280], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 876, "seek": 213140, "start": 2149.7200000000003, "end": 2151.6, "text": " of intellectual descendants,", "tokens": [51280, 295, 12576, 31693, 11, 51374], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 877, "seek": 213140, "start": 2151.6, "end": 2153.6800000000003, "text": " they try out a different evolutionary path.", "tokens": [51374, 436, 853, 484, 257, 819, 27567, 3100, 13, 51478], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 878, "seek": 213140, "start": 2153.6800000000003, "end": 2155.96, "text": " You sort of run the experiment there", "tokens": [51478, 509, 1333, 295, 1190, 264, 5120, 456, 51592], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 879, "seek": 213140, "start": 2155.96, "end": 2157.92, "text": " in terms of publishing the paper, whatever.", "tokens": [51592, 294, 2115, 295, 17832, 264, 3035, 11, 2035, 13, 51690], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 880, "seek": 213140, "start": 2157.92, "end": 2159.48, "text": " It seems like a similar kind of thing humans are doing.", "tokens": [51690, 467, 2544, 411, 257, 2531, 733, 295, 551, 6255, 366, 884, 13, 51768], "temperature": 0.0, "avg_logprob": -0.16550491877964565, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.015417121350765228}, {"id": 881, "seek": 215948, "start": 2159.48, "end": 2161.8, "text": " There's like at a higher level of generalization.", "tokens": [50364, 821, 311, 411, 412, 257, 2946, 1496, 295, 2674, 2144, 13, 50480], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 882, "seek": 215948, "start": 2161.8, "end": 2164.04, "text": " And what you see across bigger and bigger models", "tokens": [50480, 400, 437, 291, 536, 2108, 3801, 293, 3801, 5245, 50592], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 883, "seek": 215948, "start": 2164.04, "end": 2165.36, "text": " is they seem to be approaching", "tokens": [50592, 307, 436, 1643, 281, 312, 14908, 50658], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 884, "seek": 215948, "start": 2165.36, "end": 2166.96, "text": " higher and higher level of generalization", "tokens": [50658, 2946, 293, 2946, 1496, 295, 2674, 2144, 50738], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 885, "seek": 215948, "start": 2166.96, "end": 2170.36, "text": " where GPT-2 couldn't do a great school level math problem", "tokens": [50738, 689, 26039, 51, 12, 17, 2809, 380, 360, 257, 869, 1395, 1496, 5221, 1154, 50908], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 886, "seek": 215948, "start": 2170.36, "end": 2171.44, "text": " that requires more generalization", "tokens": [50908, 300, 7029, 544, 2674, 2144, 50962], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 887, "seek": 215948, "start": 2171.44, "end": 2173.64, "text": " that it has capability for, even that skill.", "tokens": [50962, 300, 309, 575, 13759, 337, 11, 754, 300, 5389, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 888, "seek": 215948, "start": 2173.64, "end": 2175.76, "text": " Then GPT-3 and 4 can.", "tokens": [51072, 1396, 26039, 51, 12, 18, 293, 1017, 393, 13, 51178], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 889, "seek": 215948, "start": 2175.76, "end": 2176.6, "text": " So not quite.", "tokens": [51178, 407, 406, 1596, 13, 51220], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 890, "seek": 215948, "start": 2176.6, "end": 2179.96, "text": " So GPT-4 has a higher degree of skill", "tokens": [51220, 407, 26039, 51, 12, 19, 575, 257, 2946, 4314, 295, 5389, 51388], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 891, "seek": 215948, "start": 2179.96, "end": 2181.52, "text": " and higher range of skills.", "tokens": [51388, 293, 2946, 3613, 295, 3942, 13, 51466], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 892, "seek": 215948, "start": 2181.52, "end": 2182.36, "text": " Because it's-", "tokens": [51466, 1436, 309, 311, 12, 51508], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 893, "seek": 215948, "start": 2182.36, "end": 2183.2, "text": " I don't want to get into semantics here,", "tokens": [51508, 286, 500, 380, 528, 281, 483, 666, 4361, 45298, 510, 11, 51550], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 894, "seek": 215948, "start": 2183.2, "end": 2184.04, "text": " but I think-", "tokens": [51550, 457, 286, 519, 12, 51592], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 895, "seek": 215948, "start": 2184.04, "end": 2184.88, "text": " The same degree of generalization.", "tokens": [51592, 440, 912, 4314, 295, 2674, 2144, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 896, "seek": 215948, "start": 2184.88, "end": 2185.72, "text": " I don't want to get into semantics here,", "tokens": [51634, 286, 500, 380, 528, 281, 483, 666, 4361, 45298, 510, 11, 51676], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 897, "seek": 215948, "start": 2185.72, "end": 2188.64, "text": " but the question of why can't creativity", "tokens": [51676, 457, 264, 1168, 295, 983, 393, 380, 12915, 51822], "temperature": 0.0, "avg_logprob": -0.2308310359888683, "compression_ratio": 1.9381107491856677, "no_speech_prob": 0.0009696499328128994}, {"id": 898, "seek": 218864, "start": 2188.64, "end": 2192.7999999999997, "text": " be just interpolation on a higher dimension?", "tokens": [50364, 312, 445, 44902, 399, 322, 257, 2946, 10139, 30, 50572], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 899, "seek": 218864, "start": 2192.7999999999997, "end": 2195.3199999999997, "text": " I think interpolation can be creative, absolutely.", "tokens": [50572, 286, 519, 44902, 399, 393, 312, 5880, 11, 3122, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 900, "seek": 218864, "start": 2195.3199999999997, "end": 2196.92, "text": " And you know, to your point,", "tokens": [50698, 400, 291, 458, 11, 281, 428, 935, 11, 50778], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 901, "seek": 218864, "start": 2196.92, "end": 2199.16, "text": " I do think that on some level,", "tokens": [50778, 286, 360, 519, 300, 322, 512, 1496, 11, 50890], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 902, "seek": 218864, "start": 2199.16, "end": 2201.64, "text": " humans also do a lot of memorization,", "tokens": [50890, 6255, 611, 360, 257, 688, 295, 10560, 2144, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 903, "seek": 218864, "start": 2201.64, "end": 2203.44, "text": " a lot of reciting, a lot of pattern matching,", "tokens": [51014, 257, 688, 295, 850, 1748, 11, 257, 688, 295, 5102, 14324, 11, 51104], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 904, "seek": 218864, "start": 2203.44, "end": 2205.12, "text": " a lot of interpolation as well.", "tokens": [51104, 257, 688, 295, 44902, 399, 382, 731, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 905, "seek": 218864, "start": 2205.12, "end": 2207.64, "text": " So it's very much a spectrum", "tokens": [51188, 407, 309, 311, 588, 709, 257, 11143, 51314], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 906, "seek": 218864, "start": 2208.92, "end": 2211.7599999999998, "text": " between pattern matching and true reasoning.", "tokens": [51378, 1296, 5102, 14324, 293, 2074, 21577, 13, 51520], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 907, "seek": 218864, "start": 2211.7599999999998, "end": 2212.6, "text": " It's a spectrum.", "tokens": [51520, 467, 311, 257, 11143, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 908, "seek": 218864, "start": 2212.6, "end": 2217.48, "text": " And humans are never really at one end of the spectrum.", "tokens": [51562, 400, 6255, 366, 1128, 534, 412, 472, 917, 295, 264, 11143, 13, 51806], "temperature": 0.0, "avg_logprob": -0.1525451596043691, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0009524605702608824}, {"id": 909, "seek": 221748, "start": 2217.48, "end": 2219.72, "text": " They're never really doing pure pattern matching", "tokens": [50364, 814, 434, 1128, 534, 884, 6075, 5102, 14324, 50476], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 910, "seek": 221748, "start": 2219.72, "end": 2220.56, "text": " or pure reasoning.", "tokens": [50476, 420, 6075, 21577, 13, 50518], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 911, "seek": 221748, "start": 2220.56, "end": 2222.92, "text": " They're usually doing some mixture of both.", "tokens": [50518, 814, 434, 2673, 884, 512, 9925, 295, 1293, 13, 50636], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 912, "seek": 221748, "start": 2222.92, "end": 2226.4, "text": " Even if you're doing something that seems very reasoning heavy,", "tokens": [50636, 2754, 498, 291, 434, 884, 746, 300, 2544, 588, 21577, 4676, 11, 50810], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 913, "seek": 221748, "start": 2226.4, "end": 2228.88, "text": " like proving a mathematical theorem,", "tokens": [50810, 411, 27221, 257, 18894, 20904, 11, 50934], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 914, "seek": 221748, "start": 2228.88, "end": 2229.96, "text": " as you're doing it, sure,", "tokens": [50934, 382, 291, 434, 884, 309, 11, 988, 11, 50988], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 915, "seek": 221748, "start": 2229.96, "end": 2232.48, "text": " you're doing quite a bit of discrete search in your mind,", "tokens": [50988, 291, 434, 884, 1596, 257, 857, 295, 27706, 3164, 294, 428, 1575, 11, 51114], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 916, "seek": 221748, "start": 2232.48, "end": 2234.36, "text": " quite a bit of actual reasoning.", "tokens": [51114, 1596, 257, 857, 295, 3539, 21577, 13, 51208], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 917, "seek": 221748, "start": 2234.36, "end": 2238.12, "text": " But you're also very much guided by intuition,", "tokens": [51208, 583, 291, 434, 611, 588, 709, 19663, 538, 24002, 11, 51396], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 918, "seek": 221748, "start": 2238.12, "end": 2239.28, "text": " guided by pattern matching,", "tokens": [51396, 19663, 538, 5102, 14324, 11, 51454], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 919, "seek": 221748, "start": 2239.28, "end": 2242.96, "text": " guided by the shape of proofs that you've seen before,", "tokens": [51454, 19663, 538, 264, 3909, 295, 8177, 82, 300, 291, 600, 1612, 949, 11, 51638], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 920, "seek": 221748, "start": 2242.96, "end": 2245.0, "text": " by your knowledge of mathematics.", "tokens": [51638, 538, 428, 3601, 295, 18666, 13, 51740], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 921, "seek": 221748, "start": 2245.0, "end": 2246.84, "text": " So it's never really,", "tokens": [51740, 407, 309, 311, 1128, 534, 11, 51832], "temperature": 0.0, "avg_logprob": -0.144858240198206, "compression_ratio": 1.9581749049429658, "no_speech_prob": 0.0015699166106060147}, {"id": 922, "seek": 224684, "start": 2246.88, "end": 2248.2000000000003, "text": " you know, all of our thoughts,", "tokens": [50366, 291, 458, 11, 439, 295, 527, 4598, 11, 50432], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 923, "seek": 224684, "start": 2248.2000000000003, "end": 2251.76, "text": " everything we do is a mixture of this sort of like", "tokens": [50432, 1203, 321, 360, 307, 257, 9925, 295, 341, 1333, 295, 411, 50610], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 924, "seek": 224684, "start": 2251.76, "end": 2254.28, "text": " interpolative memorization based thinking,", "tokens": [50610, 44902, 1166, 10560, 2144, 2361, 1953, 11, 50736], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 925, "seek": 224684, "start": 2254.28, "end": 2258.6000000000004, "text": " this sort of like type one thinking and type two thinking.", "tokens": [50736, 341, 1333, 295, 411, 2010, 472, 1953, 293, 2010, 732, 1953, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 926, "seek": 224684, "start": 2260.28, "end": 2263.2000000000003, "text": " Why are bigger models more sample efficient?", "tokens": [51036, 1545, 366, 3801, 5245, 544, 6889, 7148, 30, 51182], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 927, "seek": 224684, "start": 2263.2000000000003, "end": 2267.6000000000004, "text": " Because they have more reusable building blocks", "tokens": [51182, 1436, 436, 362, 544, 41807, 2390, 8474, 51402], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 928, "seek": 224684, "start": 2267.6000000000004, "end": 2272.1200000000003, "text": " that they can lean on to pick up new patterns", "tokens": [51402, 300, 436, 393, 11659, 322, 281, 1888, 493, 777, 8294, 51628], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 929, "seek": 224684, "start": 2272.1200000000003, "end": 2272.96, "text": " in their train data.", "tokens": [51628, 294, 641, 3847, 1412, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 930, "seek": 224684, "start": 2272.96, "end": 2274.6800000000003, "text": " And does that pattern keep continuing", "tokens": [51670, 400, 775, 300, 5102, 1066, 9289, 51756], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 931, "seek": 224684, "start": 2274.6800000000003, "end": 2276.04, "text": " as you keep getting bigger and bigger?", "tokens": [51756, 382, 291, 1066, 1242, 3801, 293, 3801, 30, 51824], "temperature": 0.0, "avg_logprob": -0.1462725309225229, "compression_ratio": 1.68, "no_speech_prob": 0.0002674022689461708}, {"id": 932, "seek": 227604, "start": 2276.04, "end": 2278.2799999999997, "text": " To the extent that the new patterns", "tokens": [50364, 1407, 264, 8396, 300, 264, 777, 8294, 50476], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 933, "seek": 227604, "start": 2278.2799999999997, "end": 2280.52, "text": " you're giving the model to learn", "tokens": [50476, 291, 434, 2902, 264, 2316, 281, 1466, 50588], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 934, "seek": 227604, "start": 2280.52, "end": 2283.0, "text": " are good match for what it has learned before.", "tokens": [50588, 366, 665, 2995, 337, 437, 309, 575, 3264, 949, 13, 50712], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 935, "seek": 227604, "start": 2283.0, "end": 2285.24, "text": " If you present something that is actually novel,", "tokens": [50712, 759, 291, 1974, 746, 300, 307, 767, 7613, 11, 50824], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 936, "seek": 227604, "start": 2285.24, "end": 2287.6, "text": " that is not in a state of distribution like an arc puzzle,", "tokens": [50824, 300, 307, 406, 294, 257, 1785, 295, 7316, 411, 364, 10346, 12805, 11, 50942], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 937, "seek": 227604, "start": 2287.6, "end": 2289.24, "text": " for instance, it will fail.", "tokens": [50942, 337, 5197, 11, 309, 486, 3061, 13, 51024], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 938, "seek": 227604, "start": 2289.24, "end": 2290.24, "text": " Let me make this claim.", "tokens": [51024, 961, 385, 652, 341, 3932, 13, 51074], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 939, "seek": 227604, "start": 2290.24, "end": 2292.36, "text": " The program synthesis I think is a very,", "tokens": [51074, 440, 1461, 30252, 286, 519, 307, 257, 588, 11, 51180], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 940, "seek": 227604, "start": 2292.36, "end": 2294.12, "text": " very useful intuition pump.", "tokens": [51180, 588, 4420, 24002, 5889, 13, 51268], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 941, "seek": 227604, "start": 2294.12, "end": 2295.6, "text": " Why can't it be the case that what's happening", "tokens": [51268, 1545, 393, 380, 309, 312, 264, 1389, 300, 437, 311, 2737, 51342], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 942, "seek": 227604, "start": 2295.6, "end": 2299.44, "text": " in the transformer is the early layers are doing the,", "tokens": [51342, 294, 264, 31782, 307, 264, 2440, 7914, 366, 884, 264, 11, 51534], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 943, "seek": 227604, "start": 2299.44, "end": 2302.24, "text": " figuring out how to represent the inputting tokens.", "tokens": [51534, 15213, 484, 577, 281, 2906, 264, 4846, 783, 22667, 13, 51674], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 944, "seek": 227604, "start": 2302.24, "end": 2304.6, "text": " And what the middle layers do is this kind of program search,", "tokens": [51674, 400, 437, 264, 2808, 7914, 360, 307, 341, 733, 295, 1461, 3164, 11, 51792], "temperature": 0.0, "avg_logprob": -0.151790540512294, "compression_ratio": 1.704268292682927, "no_speech_prob": 0.0006047153729014099}, {"id": 945, "seek": 230460, "start": 2304.6, "end": 2307.44, "text": " program synthesis, where they combine the inputs", "tokens": [50364, 1461, 30252, 11, 689, 436, 10432, 264, 15743, 50506], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 946, "seek": 230460, "start": 2307.44, "end": 2311.08, "text": " to all the circuits in the model", "tokens": [50506, 281, 439, 264, 26354, 294, 264, 2316, 50688], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 947, "seek": 230460, "start": 2311.08, "end": 2313.92, "text": " where they go from the low level representation", "tokens": [50688, 689, 436, 352, 490, 264, 2295, 1496, 10290, 50830], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 948, "seek": 230460, "start": 2313.92, "end": 2315.24, "text": " to a higher level representation", "tokens": [50830, 281, 257, 2946, 1496, 10290, 50896], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 949, "seek": 230460, "start": 2315.24, "end": 2316.3199999999997, "text": " near the middle of the model.", "tokens": [50896, 2651, 264, 2808, 295, 264, 2316, 13, 50950], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 950, "seek": 230460, "start": 2316.3199999999997, "end": 2319.68, "text": " They use these programs, they combine these concepts,", "tokens": [50950, 814, 764, 613, 4268, 11, 436, 10432, 613, 10392, 11, 51118], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 951, "seek": 230460, "start": 2319.68, "end": 2322.6, "text": " then what comes out at the other end is the reasoning", "tokens": [51118, 550, 437, 1487, 484, 412, 264, 661, 917, 307, 264, 21577, 51264], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 952, "seek": 230460, "start": 2322.6, "end": 2325.16, "text": " based on that high level intelligence.", "tokens": [51264, 2361, 322, 300, 1090, 1496, 7599, 13, 51392], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 953, "seek": 230460, "start": 2325.16, "end": 2326.64, "text": " Possibly, why not?", "tokens": [51392, 33112, 3545, 11, 983, 406, 30, 51466], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 954, "seek": 230460, "start": 2327.72, "end": 2330.8399999999997, "text": " But if these models were actually capable", "tokens": [51520, 583, 498, 613, 5245, 645, 767, 8189, 51676], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 955, "seek": 230460, "start": 2330.8399999999997, "end": 2334.44, "text": " of synthesizing novel programs,", "tokens": [51676, 295, 26617, 3319, 7613, 4268, 11, 51856], "temperature": 0.0, "avg_logprob": -0.19830507602331773, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.00106393254827708}, {"id": 956, "seek": 233444, "start": 2334.48, "end": 2337.36, "text": " however simple they should be able to do arc", "tokens": [50366, 4461, 2199, 436, 820, 312, 1075, 281, 360, 10346, 50510], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 957, "seek": 233444, "start": 2337.36, "end": 2339.4, "text": " because for any arc task,", "tokens": [50510, 570, 337, 604, 10346, 5633, 11, 50612], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 958, "seek": 233444, "start": 2339.4, "end": 2342.32, "text": " if you write down the solution program in Python,", "tokens": [50612, 498, 291, 2464, 760, 264, 3827, 1461, 294, 15329, 11, 50758], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 959, "seek": 233444, "start": 2342.32, "end": 2345.6, "text": " it's not a complex program, it's extremely simple", "tokens": [50758, 309, 311, 406, 257, 3997, 1461, 11, 309, 311, 4664, 2199, 50922], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 960, "seek": 233444, "start": 2345.6, "end": 2347.88, "text": " and humans can figure it out.", "tokens": [50922, 293, 6255, 393, 2573, 309, 484, 13, 51036], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 961, "seek": 233444, "start": 2347.88, "end": 2350.2400000000002, "text": " So why can LLMs not do it?", "tokens": [51036, 407, 983, 393, 441, 43, 26386, 406, 360, 309, 30, 51154], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 962, "seek": 233444, "start": 2350.2400000000002, "end": 2352.88, "text": " Okay, I think that's a fair point.", "tokens": [51154, 1033, 11, 286, 519, 300, 311, 257, 3143, 935, 13, 51286], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 963, "seek": 233444, "start": 2352.88, "end": 2355.48, "text": " And if I turn the question around to you,", "tokens": [51286, 400, 498, 286, 1261, 264, 1168, 926, 281, 291, 11, 51416], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 964, "seek": 233444, "start": 2355.48, "end": 2358.96, "text": " so suppose that it's the case that in a year,", "tokens": [51416, 370, 7297, 300, 309, 311, 264, 1389, 300, 294, 257, 1064, 11, 51590], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 965, "seek": 233444, "start": 2358.96, "end": 2362.2000000000003, "text": " a multimodal model can solve arc,", "tokens": [51590, 257, 32972, 378, 304, 2316, 393, 5039, 10346, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13301363484612827, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00023401899670716375}, {"id": 966, "seek": 236220, "start": 2362.24, "end": 2365.52, "text": " let's say get 80%, whatever the average human would get,", "tokens": [50366, 718, 311, 584, 483, 4688, 8923, 2035, 264, 4274, 1952, 576, 483, 11, 50530], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 967, "seek": 236220, "start": 2365.52, "end": 2367.2, "text": " then AGI?", "tokens": [50530, 550, 316, 26252, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 968, "seek": 236220, "start": 2367.2, "end": 2368.3599999999997, "text": " Quite possibly, yes.", "tokens": [50614, 20464, 6264, 11, 2086, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 969, "seek": 236220, "start": 2368.3599999999997, "end": 2370.96, "text": " I think if you start, so honestly,", "tokens": [50672, 286, 519, 498, 291, 722, 11, 370, 6095, 11, 50802], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 970, "seek": 236220, "start": 2370.96, "end": 2374.72, "text": " what I would like to see is an LLM type model", "tokens": [50802, 437, 286, 576, 411, 281, 536, 307, 364, 441, 43, 44, 2010, 2316, 50990], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 971, "seek": 236220, "start": 2374.72, "end": 2376.7599999999998, "text": " solving arc at like 80%,", "tokens": [50990, 12606, 10346, 412, 411, 4688, 8923, 51092], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 972, "seek": 236220, "start": 2376.7599999999998, "end": 2379.8799999999997, "text": " but after having only been trained", "tokens": [51092, 457, 934, 1419, 787, 668, 8895, 51248], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 973, "seek": 236220, "start": 2379.8799999999997, "end": 2382.8399999999997, "text": " on core knowledge related stuff.", "tokens": [51248, 322, 4965, 3601, 4077, 1507, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 974, "seek": 236220, "start": 2382.8399999999997, "end": 2385.3199999999997, "text": " But human kids, I don't think we're necessarily", "tokens": [51396, 583, 1952, 2301, 11, 286, 500, 380, 519, 321, 434, 4725, 51520], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 975, "seek": 236220, "start": 2385.3199999999997, "end": 2387.3599999999997, "text": " just trading on, it's not just that we have", "tokens": [51520, 445, 9529, 322, 11, 309, 311, 406, 445, 300, 321, 362, 51622], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 976, "seek": 236220, "start": 2387.3599999999997, "end": 2388.7999999999997, "text": " in our show is object permanence.", "tokens": [51622, 294, 527, 855, 307, 2657, 8105, 655, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 977, "seek": 236220, "start": 2388.7999999999997, "end": 2390.6, "text": " Okay, let me rephrase that.", "tokens": [51694, 1033, 11, 718, 385, 319, 44598, 651, 300, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2066641379529097, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.00512693403288722}, {"id": 978, "seek": 239060, "start": 2390.6, "end": 2395.6, "text": " Only trained on information that is not explicitly", "tokens": [50364, 5686, 8895, 322, 1589, 300, 307, 406, 20803, 50614], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 979, "seek": 239060, "start": 2395.72, "end": 2398.96, "text": " trying to anticipate what's gonna be in the arc test set.", "tokens": [50620, 1382, 281, 21685, 437, 311, 799, 312, 294, 264, 10346, 1500, 992, 13, 50782], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 980, "seek": 239060, "start": 2398.96, "end": 2401.2799999999997, "text": " But isn't the whole point of arc that you can't,", "tokens": [50782, 583, 1943, 380, 264, 1379, 935, 295, 10346, 300, 291, 393, 380, 11, 50898], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 981, "seek": 239060, "start": 2401.2799999999997, "end": 2403.64, "text": " sort of, it's a new type of intelligence", "tokens": [50898, 1333, 295, 11, 309, 311, 257, 777, 2010, 295, 7599, 51016], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 982, "seek": 239060, "start": 2403.64, "end": 2404.48, "text": " every single time?", "tokens": [51016, 633, 2167, 565, 30, 51058], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 983, "seek": 239060, "start": 2404.48, "end": 2405.3199999999997, "text": " Yes, that is the point.", "tokens": [51058, 1079, 11, 300, 307, 264, 935, 13, 51100], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 984, "seek": 239060, "start": 2405.3199999999997, "end": 2407.88, "text": " So if arc were perfect, flawless benchmark,", "tokens": [51100, 407, 498, 10346, 645, 2176, 11, 45693, 18927, 11, 51228], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 985, "seek": 239060, "start": 2407.88, "end": 2410.68, "text": " it would be impossible to anticipate within the test set.", "tokens": [51228, 309, 576, 312, 6243, 281, 21685, 1951, 264, 1500, 992, 13, 51368], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 986, "seek": 239060, "start": 2410.68, "end": 2414.16, "text": " And arc was released more than four years ago", "tokens": [51368, 400, 10346, 390, 4736, 544, 813, 1451, 924, 2057, 51542], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 987, "seek": 239060, "start": 2414.16, "end": 2416.64, "text": " and so far it's been resistant to memorization.", "tokens": [51542, 293, 370, 1400, 309, 311, 668, 20383, 281, 10560, 2144, 13, 51666], "temperature": 0.0, "avg_logprob": -0.19541795844705695, "compression_ratio": 1.6553030303030303, "no_speech_prob": 0.0007086164550855756}, {"id": 988, "seek": 241664, "start": 2416.64, "end": 2420.8399999999997, "text": " So I think it has, to some extent, passed a test of time.", "tokens": [50364, 407, 286, 519, 309, 575, 11, 281, 512, 8396, 11, 4678, 257, 1500, 295, 565, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 989, "seek": 241664, "start": 2420.8399999999997, "end": 2422.96, "text": " But I don't think it's perfect.", "tokens": [50574, 583, 286, 500, 380, 519, 309, 311, 2176, 13, 50680], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 990, "seek": 241664, "start": 2422.96, "end": 2426.2, "text": " I think if you try to make by hand", "tokens": [50680, 286, 519, 498, 291, 853, 281, 652, 538, 1011, 50842], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 991, "seek": 241664, "start": 2427.12, "end": 2429.24, "text": " hundreds of thousands of arc tasks", "tokens": [50888, 6779, 295, 5383, 295, 10346, 9608, 50994], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 992, "seek": 241664, "start": 2429.24, "end": 2431.7999999999997, "text": " and then you try to multiply them", "tokens": [50994, 293, 550, 291, 853, 281, 12972, 552, 51122], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 993, "seek": 241664, "start": 2432.7599999999998, "end": 2435.0, "text": " by programmatically generating variations", "tokens": [51170, 538, 37648, 5030, 17746, 17840, 51282], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 994, "seek": 241664, "start": 2435.0, "end": 2438.52, "text": " and then you end up with maybe hundreds of millions of tasks.", "tokens": [51282, 293, 550, 291, 917, 493, 365, 1310, 6779, 295, 6803, 295, 9608, 13, 51458], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 995, "seek": 241664, "start": 2438.52, "end": 2441.04, "text": " Just by brute forcing the task space,", "tokens": [51458, 1449, 538, 47909, 19030, 264, 5633, 1901, 11, 51584], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 996, "seek": 241664, "start": 2441.04, "end": 2443.68, "text": " there will be enough overlap between what you're trained on", "tokens": [51584, 456, 486, 312, 1547, 19959, 1296, 437, 291, 434, 8895, 322, 51716], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 997, "seek": 241664, "start": 2443.68, "end": 2445.44, "text": " and what's in the test set that you can actually score", "tokens": [51716, 293, 437, 311, 294, 264, 1500, 992, 300, 291, 393, 767, 6175, 51804], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 998, "seek": 241664, "start": 2445.44, "end": 2446.2799999999997, "text": " very highly.", "tokens": [51804, 588, 5405, 13, 51846], "temperature": 0.0, "avg_logprob": -0.17726479507074122, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.0002481259289197624}, {"id": 999, "seek": 244628, "start": 2446.28, "end": 2449.6000000000004, "text": " So, you know, with enough scale, you can always cheat.", "tokens": [50364, 407, 11, 291, 458, 11, 365, 1547, 4373, 11, 291, 393, 1009, 17470, 13, 50530], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1000, "seek": 244628, "start": 2449.6000000000004, "end": 2451.2400000000002, "text": " If you can do this for every single thing", "tokens": [50530, 759, 291, 393, 360, 341, 337, 633, 2167, 551, 50612], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1001, "seek": 244628, "start": 2451.2400000000002, "end": 2452.88, "text": " that supposedly requires intelligence,", "tokens": [50612, 300, 20581, 7029, 7599, 11, 50694], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1002, "seek": 244628, "start": 2452.88, "end": 2453.92, "text": " then what good is intelligence?", "tokens": [50694, 550, 437, 665, 307, 7599, 30, 50746], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1003, "seek": 244628, "start": 2453.92, "end": 2455.5600000000004, "text": " Apparently you can just brute force intelligence.", "tokens": [50746, 16755, 291, 393, 445, 47909, 3464, 7599, 13, 50828], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1004, "seek": 244628, "start": 2455.5600000000004, "end": 2459.92, "text": " If the world, if your life, were a static distribution,", "tokens": [50828, 759, 264, 1002, 11, 498, 428, 993, 11, 645, 257, 13437, 7316, 11, 51046], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1005, "seek": 244628, "start": 2459.92, "end": 2461.52, "text": " then sure, you could just brute force", "tokens": [51046, 550, 988, 11, 291, 727, 445, 47909, 3464, 51126], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1006, "seek": 244628, "start": 2461.52, "end": 2463.7200000000003, "text": " the space of possible behaviors.", "tokens": [51126, 264, 1901, 295, 1944, 15501, 13, 51236], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1007, "seek": 244628, "start": 2463.7200000000003, "end": 2467.96, "text": " You could like, you know, the way we think about intelligence,", "tokens": [51236, 509, 727, 411, 11, 291, 458, 11, 264, 636, 321, 519, 466, 7599, 11, 51448], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1008, "seek": 244628, "start": 2467.96, "end": 2469.96, "text": " there are several metaphors, I like to use,", "tokens": [51448, 456, 366, 2940, 30946, 830, 11, 286, 411, 281, 764, 11, 51548], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1009, "seek": 244628, "start": 2469.96, "end": 2472.6400000000003, "text": " but one of them is you can think of intelligence", "tokens": [51548, 457, 472, 295, 552, 307, 291, 393, 519, 295, 7599, 51682], "temperature": 0.0, "avg_logprob": -0.20286968350410461, "compression_ratio": 1.858736059479554, "no_speech_prob": 0.0004627227899618447}, {"id": 1010, "seek": 247264, "start": 2472.64, "end": 2477.0, "text": " as a past finding algorithm in future situation space.", "tokens": [50364, 382, 257, 1791, 5006, 9284, 294, 2027, 2590, 1901, 13, 50582], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1011, "seek": 247264, "start": 2477.0, "end": 2479.16, "text": " Like, I don't know if you're familiar with game development,", "tokens": [50582, 1743, 11, 286, 500, 380, 458, 498, 291, 434, 4963, 365, 1216, 3250, 11, 50690], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1012, "seek": 247264, "start": 2479.16, "end": 2483.4, "text": " like RTS game development, but you have a map, right?", "tokens": [50690, 411, 497, 7327, 1216, 3250, 11, 457, 291, 362, 257, 4471, 11, 558, 30, 50902], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1013, "seek": 247264, "start": 2483.4, "end": 2486.0, "text": " And you have, it's like a 2D map.", "tokens": [50902, 400, 291, 362, 11, 309, 311, 411, 257, 568, 35, 4471, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1014, "seek": 247264, "start": 2486.0, "end": 2488.7999999999997, "text": " And you have partial information about it.", "tokens": [51032, 400, 291, 362, 14641, 1589, 466, 309, 13, 51172], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1015, "seek": 247264, "start": 2488.7999999999997, "end": 2491.8799999999997, "text": " Like there is some fog of war on your map.", "tokens": [51172, 1743, 456, 307, 512, 13648, 295, 1516, 322, 428, 4471, 13, 51326], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1016, "seek": 247264, "start": 2491.8799999999997, "end": 2494.04, "text": " There are areas that you haven't explored yet.", "tokens": [51326, 821, 366, 3179, 300, 291, 2378, 380, 24016, 1939, 13, 51434], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1017, "seek": 247264, "start": 2494.04, "end": 2495.08, "text": " You know nothing about them.", "tokens": [51434, 509, 458, 1825, 466, 552, 13, 51486], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1018, "seek": 247264, "start": 2495.08, "end": 2496.64, "text": " And then there are areas that you've explored,", "tokens": [51486, 400, 550, 456, 366, 3179, 300, 291, 600, 24016, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1019, "seek": 247264, "start": 2496.64, "end": 2499.92, "text": " but you only know how they were like in the past.", "tokens": [51564, 457, 291, 787, 458, 577, 436, 645, 411, 294, 264, 1791, 13, 51728], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1020, "seek": 247264, "start": 2499.92, "end": 2501.52, "text": " You don't know how they are like today.", "tokens": [51728, 509, 500, 380, 458, 577, 436, 366, 411, 965, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12244380112235428, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.001056051580235362}, {"id": 1021, "seek": 250152, "start": 2501.52, "end": 2506.7599999999998, "text": " And now, instead of thinking about a 2D map,", "tokens": [50364, 400, 586, 11, 2602, 295, 1953, 466, 257, 568, 35, 4471, 11, 50626], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1022, "seek": 250152, "start": 2506.7599999999998, "end": 2510.04, "text": " think about the space of possible future situations", "tokens": [50626, 519, 466, 264, 1901, 295, 1944, 2027, 6851, 50790], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1023, "seek": 250152, "start": 2510.04, "end": 2512.64, "text": " that you might encounter and how they're connected to each other.", "tokens": [50790, 300, 291, 1062, 8593, 293, 577, 436, 434, 4582, 281, 1184, 661, 13, 50920], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1024, "seek": 250152, "start": 2512.64, "end": 2514.44, "text": " Intelligence is a past finding algorithm.", "tokens": [50920, 27274, 307, 257, 1791, 5006, 9284, 13, 51010], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1025, "seek": 250152, "start": 2514.44, "end": 2517.4, "text": " So once you set a goal, it will tell you", "tokens": [51010, 407, 1564, 291, 992, 257, 3387, 11, 309, 486, 980, 291, 51158], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1026, "seek": 250152, "start": 2517.4, "end": 2520.68, "text": " how to get there optimally.", "tokens": [51158, 577, 281, 483, 456, 5028, 379, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1027, "seek": 250152, "start": 2520.68, "end": 2525.24, "text": " But of course, it's constrained by the information you have.", "tokens": [51322, 583, 295, 1164, 11, 309, 311, 38901, 538, 264, 1589, 291, 362, 13, 51550], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1028, "seek": 250152, "start": 2525.24, "end": 2529.24, "text": " It cannot pass find in an area that you know nothing about.", "tokens": [51550, 467, 2644, 1320, 915, 294, 364, 1859, 300, 291, 458, 1825, 466, 13, 51750], "temperature": 0.0, "avg_logprob": -0.14539408216289446, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0004669378977268934}, {"id": 1029, "seek": 252924, "start": 2529.24, "end": 2532.64, "text": " It cannot also anticipate changes.", "tokens": [50364, 467, 2644, 611, 21685, 2962, 13, 50534], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1030, "seek": 252924, "start": 2532.64, "end": 2540.04, "text": " And the thing is, if you had complete information about the map,", "tokens": [50534, 400, 264, 551, 307, 11, 498, 291, 632, 3566, 1589, 466, 264, 4471, 11, 50904], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1031, "seek": 252924, "start": 2540.04, "end": 2542.3599999999997, "text": " then you could solve the past finding problem", "tokens": [50904, 550, 291, 727, 5039, 264, 1791, 5006, 1154, 51020], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1032, "seek": 252924, "start": 2542.3599999999997, "end": 2546.04, "text": " by simply memorizing every possible path, every mapping", "tokens": [51020, 538, 2935, 10560, 3319, 633, 1944, 3100, 11, 633, 18350, 51204], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1033, "seek": 252924, "start": 2546.04, "end": 2551.3199999999997, "text": " from point A to point B. You could solve the problem", "tokens": [51204, 490, 935, 316, 281, 935, 363, 13, 509, 727, 5039, 264, 1154, 51468], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1034, "seek": 252924, "start": 2551.3199999999997, "end": 2552.3999999999996, "text": " with pure memory.", "tokens": [51468, 365, 6075, 4675, 13, 51522], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1035, "seek": 252924, "start": 2552.3999999999996, "end": 2555.04, "text": " But the reason you cannot do that in real life", "tokens": [51522, 583, 264, 1778, 291, 2644, 360, 300, 294, 957, 993, 51654], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1036, "seek": 252924, "start": 2555.04, "end": 2557.12, "text": " is because you don't actually know what's", "tokens": [51654, 307, 570, 291, 500, 380, 767, 458, 437, 311, 51758], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1037, "seek": 252924, "start": 2557.12, "end": 2558.9199999999996, "text": " going to happen in the future.", "tokens": [51758, 516, 281, 1051, 294, 264, 2027, 13, 51848], "temperature": 0.0, "avg_logprob": -0.0937087345123291, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0011814608005806804}, {"id": 1038, "seek": 255892, "start": 2559.8, "end": 2561.6, "text": " I feel like you're using words like memorization, which", "tokens": [50408, 286, 841, 411, 291, 434, 1228, 2283, 411, 10560, 2144, 11, 597, 50498], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1039, "seek": 255892, "start": 2561.6, "end": 2563.0, "text": " we would never use for human children.", "tokens": [50498, 321, 576, 1128, 764, 337, 1952, 2227, 13, 50568], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1040, "seek": 255892, "start": 2563.0, "end": 2566.6800000000003, "text": " If your kid learns to do algebra and then now learns", "tokens": [50568, 759, 428, 1636, 27152, 281, 360, 21989, 293, 550, 586, 27152, 50752], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1041, "seek": 255892, "start": 2566.6800000000003, "end": 2568.76, "text": " to do calculus, you wouldn't say they memorized calculus.", "tokens": [50752, 281, 360, 33400, 11, 291, 2759, 380, 584, 436, 46677, 33400, 13, 50856], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1042, "seek": 255892, "start": 2568.76, "end": 2572.16, "text": " If they can just solve any arbitrary algebraic problem,", "tokens": [50856, 759, 436, 393, 445, 5039, 604, 23211, 21989, 299, 1154, 11, 51026], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1043, "seek": 255892, "start": 2572.16, "end": 2574.0, "text": " you wouldn't say they memorized algebra.", "tokens": [51026, 291, 2759, 380, 584, 436, 46677, 21989, 13, 51118], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1044, "seek": 255892, "start": 2574.0, "end": 2575.08, "text": " They say they've learned algebra.", "tokens": [51118, 814, 584, 436, 600, 3264, 21989, 13, 51172], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1045, "seek": 255892, "start": 2575.08, "end": 2578.16, "text": " Humans are never redoing pure memorization or pure reasoning.", "tokens": [51172, 35809, 366, 1128, 29956, 278, 6075, 10560, 2144, 420, 6075, 21577, 13, 51326], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1046, "seek": 255892, "start": 2578.16, "end": 2579.52, "text": " But that's only because you're semantically", "tokens": [51326, 583, 300, 311, 787, 570, 291, 434, 4361, 49505, 51394], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1047, "seek": 255892, "start": 2579.52, "end": 2581.16, "text": " labeling when the human does the skill.", "tokens": [51394, 40244, 562, 264, 1952, 775, 264, 5389, 13, 51476], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1048, "seek": 255892, "start": 2581.16, "end": 2583.56, "text": " It's a memorization when the exact same skill is done by the LLM", "tokens": [51476, 467, 311, 257, 10560, 2144, 562, 264, 1900, 912, 5389, 307, 1096, 538, 264, 441, 43, 44, 51596], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1049, "seek": 255892, "start": 2583.56, "end": 2584.88, "text": " as you can measure by these benchmarks.", "tokens": [51596, 382, 291, 393, 3481, 538, 613, 43751, 13, 51662], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1050, "seek": 255892, "start": 2584.88, "end": 2586.64, "text": " And you can just plug in any sort of math problem.", "tokens": [51662, 400, 291, 393, 445, 5452, 294, 604, 1333, 295, 5221, 1154, 13, 51750], "temperature": 0.0, "avg_logprob": -0.25943849715718464, "compression_ratio": 1.939209726443769, "no_speech_prob": 0.09489472210407257}, {"id": 1051, "seek": 258664, "start": 2586.68, "end": 2588.7599999999998, "text": " Sometimes humans are doing the exact same as the LLM", "tokens": [50366, 4803, 6255, 366, 884, 264, 1900, 912, 382, 264, 441, 43, 44, 50470], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1052, "seek": 258664, "start": 2588.7599999999998, "end": 2590.8399999999997, "text": " is doing, which is just, for instance,", "tokens": [50470, 307, 884, 11, 597, 307, 445, 11, 337, 5197, 11, 50574], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1053, "seek": 258664, "start": 2590.8399999999997, "end": 2592.64, "text": " I know if you learn to add numbers,", "tokens": [50574, 286, 458, 498, 291, 1466, 281, 909, 3547, 11, 50664], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1054, "seek": 258664, "start": 2592.64, "end": 2594.7999999999997, "text": " you're memorizing an algorithm.", "tokens": [50664, 291, 434, 10560, 3319, 364, 9284, 13, 50772], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1055, "seek": 258664, "start": 2594.7999999999997, "end": 2595.96, "text": " You're memorizing a program.", "tokens": [50772, 509, 434, 10560, 3319, 257, 1461, 13, 50830], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1056, "seek": 258664, "start": 2595.96, "end": 2597.64, "text": " And then you can reapply it.", "tokens": [50830, 400, 550, 291, 393, 35638, 356, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1057, "seek": 258664, "start": 2597.64, "end": 2601.72, "text": " You are not synthesizing on the fly the addition program.", "tokens": [50914, 509, 366, 406, 26617, 3319, 322, 264, 3603, 264, 4500, 1461, 13, 51118], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1058, "seek": 258664, "start": 2601.72, "end": 2604.0, "text": " So obviously at some point, some human had to figure out", "tokens": [51118, 407, 2745, 412, 512, 935, 11, 512, 1952, 632, 281, 2573, 484, 51232], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1059, "seek": 258664, "start": 2604.0, "end": 2604.6, "text": " how to do addition.", "tokens": [51232, 577, 281, 360, 4500, 13, 51262], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1060, "seek": 258664, "start": 2604.6, "end": 2607.0, "text": " But the way a kid learns it is not", "tokens": [51262, 583, 264, 636, 257, 1636, 27152, 309, 307, 406, 51382], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1061, "seek": 258664, "start": 2607.0, "end": 2610.16, "text": " that they figure out from the actions of set theory", "tokens": [51382, 300, 436, 2573, 484, 490, 264, 5909, 295, 992, 5261, 51540], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1062, "seek": 258664, "start": 2610.16, "end": 2610.7999999999997, "text": " how to do addition.", "tokens": [51540, 577, 281, 360, 4500, 13, 51572], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1063, "seek": 258664, "start": 2610.7999999999997, "end": 2612.12, "text": " I think what you're learning in school", "tokens": [51572, 286, 519, 437, 291, 434, 2539, 294, 1395, 51638], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1064, "seek": 258664, "start": 2612.12, "end": 2613.96, "text": " is mostly memorization.", "tokens": [51638, 307, 5240, 10560, 2144, 13, 51730], "temperature": 0.0, "avg_logprob": -0.19486921425633782, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0009696435881778598}, {"id": 1065, "seek": 261396, "start": 2614.0, "end": 2617.08, "text": " So my claim is that, listen, these models", "tokens": [50366, 407, 452, 3932, 307, 300, 11, 2140, 11, 613, 5245, 50520], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1066, "seek": 261396, "start": 2617.08, "end": 2620.88, "text": " are vastly underparameterized relative to how many flops", "tokens": [50520, 366, 41426, 833, 2181, 335, 2398, 1602, 4972, 281, 577, 867, 932, 3370, 50710], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1067, "seek": 261396, "start": 2620.88, "end": 2623.2400000000002, "text": " or how many parameters you have in the human brain.", "tokens": [50710, 420, 577, 867, 9834, 291, 362, 294, 264, 1952, 3567, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1068, "seek": 261396, "start": 2623.2400000000002, "end": 2625.8, "text": " And so yeah, they're not going to be coming up", "tokens": [50828, 400, 370, 1338, 11, 436, 434, 406, 516, 281, 312, 1348, 493, 50956], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1069, "seek": 261396, "start": 2625.8, "end": 2628.8, "text": " with new theorems like the smartest humans can.", "tokens": [50956, 365, 777, 10299, 2592, 411, 264, 41491, 6255, 393, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1070, "seek": 261396, "start": 2628.8, "end": 2631.12, "text": " But most humans can't do that either.", "tokens": [51106, 583, 881, 6255, 393, 380, 360, 300, 2139, 13, 51222], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1071, "seek": 261396, "start": 2631.12, "end": 2632.88, "text": " What most humans do, it sounds like it's", "tokens": [51222, 708, 881, 6255, 360, 11, 309, 3263, 411, 309, 311, 51310], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1072, "seek": 261396, "start": 2632.88, "end": 2635.04, "text": " similar to what you were calling memorization, which", "tokens": [51310, 2531, 281, 437, 291, 645, 5141, 10560, 2144, 11, 597, 51418], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1073, "seek": 261396, "start": 2635.04, "end": 2640.12, "text": " is memorizing skills or memorizing techniques", "tokens": [51418, 307, 10560, 3319, 3942, 420, 10560, 3319, 7512, 51672], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1074, "seek": 261396, "start": 2640.12, "end": 2641.08, "text": " that you've learned.", "tokens": [51672, 300, 291, 600, 3264, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1075, "seek": 261396, "start": 2641.08, "end": 2643.56, "text": " And so it sounds like it's compatible.", "tokens": [51720, 400, 370, 309, 3263, 411, 309, 311, 18218, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1763572256073697, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005527148023247719}, {"id": 1076, "seek": 264356, "start": 2643.68, "end": 2644.72, "text": " Tell me if this is wrong.", "tokens": [50370, 5115, 385, 498, 341, 307, 2085, 13, 50422], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1077, "seek": 264356, "start": 2644.72, "end": 2647.6, "text": " Is it compatible in your world if all the remote workers", "tokens": [50422, 1119, 309, 18218, 294, 428, 1002, 498, 439, 264, 8607, 5600, 50566], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1078, "seek": 264356, "start": 2647.6, "end": 2649.96, "text": " are gone, but they're doing skills", "tokens": [50566, 366, 2780, 11, 457, 436, 434, 884, 3942, 50684], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1079, "seek": 264356, "start": 2649.96, "end": 2652.12, "text": " which we can potentially make synthetic data of?", "tokens": [50684, 597, 321, 393, 7263, 652, 23420, 1412, 295, 30, 50792], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1080, "seek": 264356, "start": 2652.12, "end": 2655.44, "text": " So we record everybody's screen and every single remote worker", "tokens": [50792, 407, 321, 2136, 2201, 311, 2568, 293, 633, 2167, 8607, 11346, 50958], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1081, "seek": 264356, "start": 2655.44, "end": 2656.16, "text": " screen.", "tokens": [50958, 2568, 13, 50994], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1082, "seek": 264356, "start": 2656.16, "end": 2658.68, "text": " We sort of understand the skills they're performing there.", "tokens": [50994, 492, 1333, 295, 1223, 264, 3942, 436, 434, 10205, 456, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1083, "seek": 264356, "start": 2658.68, "end": 2660.68, "text": " And now we've trained a model that can do all this.", "tokens": [51120, 400, 586, 321, 600, 8895, 257, 2316, 300, 393, 360, 439, 341, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1084, "seek": 264356, "start": 2660.68, "end": 2662.36, "text": " All the remote workers are unemployed.", "tokens": [51220, 1057, 264, 8607, 5600, 366, 34411, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1085, "seek": 264356, "start": 2662.36, "end": 2663.7599999999998, "text": " We're generating trillions of dollars", "tokens": [51304, 492, 434, 17746, 504, 46279, 295, 3808, 51374], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1086, "seek": 264356, "start": 2663.7599999999998, "end": 2666.6, "text": " to economic activity for AI remote workers.", "tokens": [51374, 281, 4836, 5191, 337, 7318, 8607, 5600, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1087, "seek": 264356, "start": 2666.6, "end": 2668.92, "text": " In that world, are we still in the memorization regime?", "tokens": [51516, 682, 300, 1002, 11, 366, 321, 920, 294, 264, 10560, 2144, 13120, 30, 51632], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1088, "seek": 264356, "start": 2668.92, "end": 2670.32, "text": " So sure.", "tokens": [51632, 407, 988, 13, 51702], "temperature": 0.0, "avg_logprob": -0.1716946726259978, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.005055573303252459}, {"id": 1089, "seek": 267032, "start": 2670.36, "end": 2673.84, "text": " With memorization, you can automate almost anything", "tokens": [50366, 2022, 10560, 2144, 11, 291, 393, 31605, 1920, 1340, 50540], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1090, "seek": 267032, "start": 2673.84, "end": 2675.88, "text": " as long as it's a static distribution,", "tokens": [50540, 382, 938, 382, 309, 311, 257, 13437, 7316, 11, 50642], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1091, "seek": 267032, "start": 2675.88, "end": 2678.0, "text": " as long as you don't have to deal with change.", "tokens": [50642, 382, 938, 382, 291, 500, 380, 362, 281, 2028, 365, 1319, 13, 50748], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1092, "seek": 267032, "start": 2678.0, "end": 2681.0, "text": " Are most jobs part of such a static distribution?", "tokens": [50748, 2014, 881, 4782, 644, 295, 1270, 257, 13437, 7316, 30, 50898], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1093, "seek": 267032, "start": 2681.0, "end": 2684.4, "text": " Potentially, there are lots of things that you can automate.", "tokens": [50898, 9145, 3137, 11, 456, 366, 3195, 295, 721, 300, 291, 393, 31605, 13, 51068], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1094, "seek": 267032, "start": 2684.4, "end": 2686.88, "text": " And LLMs are an excellent tool for automation.", "tokens": [51068, 400, 441, 43, 26386, 366, 364, 7103, 2290, 337, 17769, 13, 51192], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1095, "seek": 267032, "start": 2686.88, "end": 2688.2400000000002, "text": " And I think that's true.", "tokens": [51192, 400, 286, 519, 300, 311, 2074, 13, 51260], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1096, "seek": 267032, "start": 2688.2400000000002, "end": 2690.52, "text": " But you have to understand that automation is not", "tokens": [51260, 583, 291, 362, 281, 1223, 300, 17769, 307, 406, 51374], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1097, "seek": 267032, "start": 2690.52, "end": 2691.44, "text": " the same as intelligence.", "tokens": [51374, 264, 912, 382, 7599, 13, 51420], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1098, "seek": 267032, "start": 2691.44, "end": 2693.8, "text": " I'm not saying that LLMs are useless.", "tokens": [51420, 286, 478, 406, 1566, 300, 441, 43, 26386, 366, 14115, 13, 51538], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1099, "seek": 267032, "start": 2693.8, "end": 2697.0, "text": " I've been a huge proponent of deep learning for many years.", "tokens": [51538, 286, 600, 668, 257, 2603, 2365, 30365, 295, 2452, 2539, 337, 867, 924, 13, 51698], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1100, "seek": 267032, "start": 2697.0, "end": 2698.92, "text": " And for many years, I've been saying two things.", "tokens": [51698, 400, 337, 867, 924, 11, 286, 600, 668, 1566, 732, 721, 13, 51794], "temperature": 0.0, "avg_logprob": -0.14022784619717985, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.010450058616697788}, {"id": 1101, "seek": 269892, "start": 2698.92, "end": 2701.6800000000003, "text": " I've been saying that if you keep scaling up deep learning,", "tokens": [50364, 286, 600, 668, 1566, 300, 498, 291, 1066, 21589, 493, 2452, 2539, 11, 50502], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1102, "seek": 269892, "start": 2701.6800000000003, "end": 2703.2000000000003, "text": " it will keep paying off.", "tokens": [50502, 309, 486, 1066, 6229, 766, 13, 50578], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1103, "seek": 269892, "start": 2703.2000000000003, "end": 2704.7200000000003, "text": " And at the same time, I've been saying,", "tokens": [50578, 400, 412, 264, 912, 565, 11, 286, 600, 668, 1566, 11, 50654], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1104, "seek": 269892, "start": 2704.7200000000003, "end": 2706.2000000000003, "text": " if you keep scaling up deep learning,", "tokens": [50654, 498, 291, 1066, 21589, 493, 2452, 2539, 11, 50728], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1105, "seek": 269892, "start": 2706.2000000000003, "end": 2708.64, "text": " this will not lead to a GI.", "tokens": [50728, 341, 486, 406, 1477, 281, 257, 26634, 13, 50850], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1106, "seek": 269892, "start": 2708.64, "end": 2710.6800000000003, "text": " So we can automate more and more things.", "tokens": [50850, 407, 321, 393, 31605, 544, 293, 544, 721, 13, 50952], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1107, "seek": 269892, "start": 2710.6800000000003, "end": 2712.52, "text": " And yes, this is economically valuable.", "tokens": [50952, 400, 2086, 11, 341, 307, 26811, 8263, 13, 51044], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1108, "seek": 269892, "start": 2712.52, "end": 2714.48, "text": " And yes, potentially, there are many jobs.", "tokens": [51044, 400, 2086, 11, 7263, 11, 456, 366, 867, 4782, 13, 51142], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1109, "seek": 269892, "start": 2714.48, "end": 2715.84, "text": " You could automate a way like this.", "tokens": [51142, 509, 727, 31605, 257, 636, 411, 341, 13, 51210], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1110, "seek": 269892, "start": 2715.84, "end": 2717.88, "text": " And that would be economically valuable.", "tokens": [51210, 400, 300, 576, 312, 26811, 8263, 13, 51312], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1111, "seek": 269892, "start": 2717.88, "end": 2720.08, "text": " But you're still not going to have intelligence.", "tokens": [51312, 583, 291, 434, 920, 406, 516, 281, 362, 7599, 13, 51422], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1112, "seek": 269892, "start": 2720.08, "end": 2722.4, "text": " So you can ask, OK, so what does it", "tokens": [51422, 407, 291, 393, 1029, 11, 2264, 11, 370, 437, 775, 309, 51538], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1113, "seek": 269892, "start": 2722.4, "end": 2724.6800000000003, "text": " matter if we can generate all this economic value?", "tokens": [51538, 1871, 498, 321, 393, 8460, 439, 341, 4836, 2158, 30, 51652], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1114, "seek": 269892, "start": 2724.6800000000003, "end": 2726.28, "text": " Maybe we don't need intelligence after all.", "tokens": [51652, 2704, 321, 500, 380, 643, 7599, 934, 439, 13, 51732], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1115, "seek": 269892, "start": 2726.28, "end": 2728.44, "text": " Well, you need intelligence the moment", "tokens": [51732, 1042, 11, 291, 643, 7599, 264, 1623, 51840], "temperature": 0.0, "avg_logprob": -0.15080890541305086, "compression_ratio": 1.955128205128205, "no_speech_prob": 0.0015645193634554744}, {"id": 1116, "seek": 272844, "start": 2728.48, "end": 2732.12, "text": " you have to deal with change, with novelty, with uncertainty.", "tokens": [50366, 291, 362, 281, 2028, 365, 1319, 11, 365, 44805, 11, 365, 15697, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1117, "seek": 272844, "start": 2732.12, "end": 2734.0, "text": " As long as you are in a space that", "tokens": [50548, 1018, 938, 382, 291, 366, 294, 257, 1901, 300, 50642], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1118, "seek": 272844, "start": 2734.0, "end": 2737.2400000000002, "text": " can be exactly described in advance,", "tokens": [50642, 393, 312, 2293, 7619, 294, 7295, 11, 50804], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1119, "seek": 272844, "start": 2737.2400000000002, "end": 2741.6, "text": " you can just automate your pure memorization.", "tokens": [50804, 291, 393, 445, 31605, 428, 6075, 10560, 2144, 13, 51022], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1120, "seek": 272844, "start": 2741.6, "end": 2744.12, "text": " In fact, you can always solve any problem.", "tokens": [51022, 682, 1186, 11, 291, 393, 1009, 5039, 604, 1154, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1121, "seek": 272844, "start": 2744.12, "end": 2748.76, "text": " You can always display arbitrary levels of skills", "tokens": [51148, 509, 393, 1009, 4674, 23211, 4358, 295, 3942, 51380], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1122, "seek": 272844, "start": 2748.76, "end": 2754.64, "text": " on any task without leveraging any intelligence whatsoever,", "tokens": [51380, 322, 604, 5633, 1553, 32666, 604, 7599, 17076, 11, 51674], "temperature": 0.0, "avg_logprob": -0.1379675453091845, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.001351191196590662}, {"id": 1123, "seek": 275464, "start": 2754.64, "end": 2758.8399999999997, "text": " as long as it is possible to describe the problem", "tokens": [50364, 382, 938, 382, 309, 307, 1944, 281, 6786, 264, 1154, 50574], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1124, "seek": 275464, "start": 2758.8399999999997, "end": 2761.3199999999997, "text": " and its solution very, very precisely.", "tokens": [50574, 293, 1080, 3827, 588, 11, 588, 13402, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1125, "seek": 275464, "start": 2761.3199999999997, "end": 2763.2, "text": " But when they do deal with novelty,", "tokens": [50698, 583, 562, 436, 360, 2028, 365, 44805, 11, 50792], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1126, "seek": 275464, "start": 2763.2, "end": 2765.2, "text": " then you just call it interpolation, right?", "tokens": [50792, 550, 291, 445, 818, 309, 44902, 399, 11, 558, 30, 50892], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1127, "seek": 275464, "start": 2765.2, "end": 2768.24, "text": " And so interpolation is not enough", "tokens": [50892, 400, 370, 44902, 399, 307, 406, 1547, 51044], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1128, "seek": 275464, "start": 2768.24, "end": 2769.92, "text": " to deal with all kinds of novelty", "tokens": [51044, 281, 2028, 365, 439, 3685, 295, 44805, 51128], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1129, "seek": 275464, "start": 2769.92, "end": 2773.3599999999997, "text": " if it were, then LLMs would be a GI.", "tokens": [51128, 498, 309, 645, 11, 550, 441, 43, 26386, 576, 312, 257, 26634, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1130, "seek": 275464, "start": 2773.3599999999997, "end": 2774.3599999999997, "text": " Well, I agree they're not a GI.", "tokens": [51300, 1042, 11, 286, 3986, 436, 434, 406, 257, 26634, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1131, "seek": 275464, "start": 2774.3599999999997, "end": 2776.52, "text": " I'm just trying to figure out how do we figure out", "tokens": [51350, 286, 478, 445, 1382, 281, 2573, 484, 577, 360, 321, 2573, 484, 51458], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1132, "seek": 275464, "start": 2776.52, "end": 2777.3199999999997, "text": " we're on the path to a GI.", "tokens": [51458, 321, 434, 322, 264, 3100, 281, 257, 26634, 13, 51498], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1133, "seek": 275464, "start": 2777.3199999999997, "end": 2780.68, "text": " And I think a sort of crux here is maybe", "tokens": [51498, 400, 286, 519, 257, 1333, 295, 5140, 87, 510, 307, 1310, 51666], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1134, "seek": 275464, "start": 2780.68, "end": 2783.8399999999997, "text": " that it seems to me that these things are on a spectrum", "tokens": [51666, 300, 309, 2544, 281, 385, 300, 613, 721, 366, 322, 257, 11143, 51824], "temperature": 0.0, "avg_logprob": -0.1608219813633632, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009519120678305626}, {"id": 1135, "seek": 278384, "start": 2783.88, "end": 2786.6400000000003, "text": " and we're clearly covering the earliest part of the spectrum", "tokens": [50366, 293, 321, 434, 4448, 10322, 264, 20573, 644, 295, 264, 11143, 50504], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1136, "seek": 278384, "start": 2786.6400000000003, "end": 2787.48, "text": " with LLMs.", "tokens": [50504, 365, 441, 43, 26386, 13, 50546], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1137, "seek": 278384, "start": 2787.48, "end": 2788.32, "text": " I think so.", "tokens": [50546, 286, 519, 370, 13, 50588], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1138, "seek": 278384, "start": 2788.32, "end": 2789.44, "text": " And oh, OK, interesting.", "tokens": [50588, 400, 1954, 11, 2264, 11, 1880, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1139, "seek": 278384, "start": 2789.44, "end": 2791.52, "text": " But here's another sort of thing", "tokens": [50644, 583, 510, 311, 1071, 1333, 295, 551, 50748], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1140, "seek": 278384, "start": 2791.52, "end": 2794.2400000000002, "text": " that I think is evidence for this, grokking, right?", "tokens": [50748, 300, 286, 519, 307, 4467, 337, 341, 11, 4634, 74, 5092, 11, 558, 30, 50884], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1141, "seek": 278384, "start": 2794.2400000000002, "end": 2796.76, "text": " So clearly, even within deep learning,", "tokens": [50884, 407, 4448, 11, 754, 1951, 2452, 2539, 11, 51010], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1142, "seek": 278384, "start": 2796.76, "end": 2799.56, "text": " there's a difference between the memorization regime", "tokens": [51010, 456, 311, 257, 2649, 1296, 264, 10560, 2144, 13120, 51150], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1143, "seek": 278384, "start": 2799.56, "end": 2802.6400000000003, "text": " and the generalization regime, where at first they'll just", "tokens": [51150, 293, 264, 2674, 2144, 13120, 11, 689, 412, 700, 436, 603, 445, 51304], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1144, "seek": 278384, "start": 2802.6400000000003, "end": 2806.8, "text": " memorize the data set of if you're doing modular addition,", "tokens": [51304, 27478, 264, 1412, 992, 295, 498, 291, 434, 884, 31111, 4500, 11, 51512], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1145, "seek": 278384, "start": 2806.8, "end": 2807.92, "text": " how to add digits.", "tokens": [51512, 577, 281, 909, 27011, 13, 51568], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1146, "seek": 278384, "start": 2807.92, "end": 2810.1200000000003, "text": " And then at some point, if you keep training on that,", "tokens": [51568, 400, 550, 412, 512, 935, 11, 498, 291, 1066, 3097, 322, 300, 11, 51678], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1147, "seek": 278384, "start": 2810.1200000000003, "end": 2811.28, "text": " they'll learn the skill.", "tokens": [51678, 436, 603, 1466, 264, 5389, 13, 51736], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1148, "seek": 278384, "start": 2811.28, "end": 2813.36, "text": " So the fact that there is that distinction", "tokens": [51736, 407, 264, 1186, 300, 456, 307, 300, 16844, 51840], "temperature": 0.0, "avg_logprob": -0.18141996623664502, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.001366688753478229}, {"id": 1149, "seek": 281336, "start": 2813.36, "end": 2816.4, "text": " suggests that the generalized circuit, the deep learning", "tokens": [50364, 13409, 300, 264, 44498, 9048, 11, 264, 2452, 2539, 50516], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1150, "seek": 281336, "start": 2816.4, "end": 2819.56, "text": " can learn, there is a regime in enters where it generalizes.", "tokens": [50516, 393, 1466, 11, 456, 307, 257, 13120, 294, 18780, 689, 309, 2674, 5660, 13, 50674], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1151, "seek": 281336, "start": 2819.56, "end": 2821.1200000000003, "text": " If you have an over-parameterized model,", "tokens": [50674, 759, 291, 362, 364, 670, 12, 2181, 335, 2398, 1602, 2316, 11, 50752], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1152, "seek": 281336, "start": 2821.1200000000003, "end": 2822.96, "text": " which you don't have in comparison to all the tasks", "tokens": [50752, 597, 291, 500, 380, 362, 294, 9660, 281, 439, 264, 9608, 50844], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1153, "seek": 281336, "start": 2822.96, "end": 2824.7200000000003, "text": " we want these models to do right now.", "tokens": [50844, 321, 528, 613, 5245, 281, 360, 558, 586, 13, 50932], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1154, "seek": 281336, "start": 2824.7200000000003, "end": 2826.32, "text": " Grokking is a very, very old phenomenon.", "tokens": [50932, 12981, 74, 5092, 307, 257, 588, 11, 588, 1331, 14029, 13, 51012], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1155, "seek": 281336, "start": 2826.32, "end": 2829.1600000000003, "text": " We've been observing it for decades.", "tokens": [51012, 492, 600, 668, 22107, 309, 337, 7878, 13, 51154], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1156, "seek": 281336, "start": 2829.1600000000003, "end": 2833.48, "text": " It's basically an instance of the minimum description length", "tokens": [51154, 467, 311, 1936, 364, 5197, 295, 264, 7285, 3855, 4641, 51370], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1157, "seek": 281336, "start": 2833.48, "end": 2836.6800000000003, "text": " principle, where, sure, given a problem,", "tokens": [51370, 8665, 11, 689, 11, 988, 11, 2212, 257, 1154, 11, 51530], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1158, "seek": 281336, "start": 2836.6800000000003, "end": 2842.76, "text": " you can just memorize a point-wise input-to-output mapping,", "tokens": [51530, 291, 393, 445, 27478, 257, 935, 12, 3711, 4846, 12, 1353, 12, 346, 2582, 18350, 11, 51834], "temperature": 0.0, "avg_logprob": -0.2220140081463438, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0013879359466955066}, {"id": 1159, "seek": 284276, "start": 2842.8, "end": 2844.1600000000003, "text": " which is completely overfit.", "tokens": [50366, 597, 307, 2584, 670, 6845, 13, 50434], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1160, "seek": 284276, "start": 2844.1600000000003, "end": 2846.0, "text": " So it does not generalize at all,", "tokens": [50434, 407, 309, 775, 406, 2674, 1125, 412, 439, 11, 50526], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1161, "seek": 284276, "start": 2846.0, "end": 2849.5600000000004, "text": " but it solves the problem on the train data.", "tokens": [50526, 457, 309, 39890, 264, 1154, 322, 264, 3847, 1412, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1162, "seek": 284276, "start": 2849.5600000000004, "end": 2853.32, "text": " And from there, you can actually keep proving it,", "tokens": [50704, 400, 490, 456, 11, 291, 393, 767, 1066, 27221, 309, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1163, "seek": 284276, "start": 2853.32, "end": 2856.2400000000002, "text": " keep making your mapping simpler and simpler and more", "tokens": [50892, 1066, 1455, 428, 18350, 18587, 293, 18587, 293, 544, 51038], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1164, "seek": 284276, "start": 2856.2400000000002, "end": 2857.32, "text": " compressed.", "tokens": [51038, 30353, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1165, "seek": 284276, "start": 2857.32, "end": 2860.5200000000004, "text": " And at some point, it will start generalizing.", "tokens": [51092, 400, 412, 512, 935, 11, 309, 486, 722, 2674, 3319, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1166, "seek": 284276, "start": 2860.5200000000004, "end": 2864.28, "text": " And so that's something called the minimum description", "tokens": [51252, 400, 370, 300, 311, 746, 1219, 264, 7285, 3855, 51440], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1167, "seek": 284276, "start": 2864.28, "end": 2865.0400000000004, "text": " length principle.", "tokens": [51440, 4641, 8665, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1168, "seek": 284276, "start": 2865.0400000000004, "end": 2868.5200000000004, "text": " It's this idea that the program that will generalize best", "tokens": [51478, 467, 311, 341, 1558, 300, 264, 1461, 300, 486, 2674, 1125, 1151, 51652], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1169, "seek": 284276, "start": 2868.5200000000004, "end": 2871.28, "text": " is the shortest, right?", "tokens": [51652, 307, 264, 31875, 11, 558, 30, 51790], "temperature": 0.0, "avg_logprob": -0.1439793450491769, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.0011505046859383583}, {"id": 1170, "seek": 287128, "start": 2871.28, "end": 2874.52, "text": " And it doesn't mean that you're doing anything", "tokens": [50364, 400, 309, 1177, 380, 914, 300, 291, 434, 884, 1340, 50526], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1171, "seek": 287128, "start": 2874.52, "end": 2875.84, "text": " other than memorization, but you're", "tokens": [50526, 661, 813, 10560, 2144, 11, 457, 291, 434, 50592], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1172, "seek": 287128, "start": 2875.84, "end": 2878.48, "text": " doing memorization plus regularization.", "tokens": [50592, 884, 10560, 2144, 1804, 3890, 2144, 13, 50724], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1173, "seek": 287128, "start": 2878.48, "end": 2880.6800000000003, "text": " Right, AKA generalization.", "tokens": [50724, 1779, 11, 45933, 2674, 2144, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1174, "seek": 287128, "start": 2880.6800000000003, "end": 2883.6400000000003, "text": " Yeah, and that is absolutely, at least to generalization.", "tokens": [50834, 865, 11, 293, 300, 307, 3122, 11, 412, 1935, 281, 2674, 2144, 13, 50982], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1175, "seek": 287128, "start": 2883.6400000000003, "end": 2885.5600000000004, "text": " Right, and then so you do that within one skill,", "tokens": [50982, 1779, 11, 293, 550, 370, 291, 360, 300, 1951, 472, 5389, 11, 51078], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1176, "seek": 287128, "start": 2885.5600000000004, "end": 2887.7200000000003, "text": " but then the pattern you see here of meta-learning", "tokens": [51078, 457, 550, 264, 5102, 291, 536, 510, 295, 19616, 12, 47204, 51186], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1177, "seek": 287128, "start": 2887.7200000000003, "end": 2890.76, "text": " is that it's more efficient to store a program that can perform", "tokens": [51186, 307, 300, 309, 311, 544, 7148, 281, 3531, 257, 1461, 300, 393, 2042, 51338], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1178, "seek": 287128, "start": 2890.76, "end": 2893.2400000000002, "text": " many skills rather than one skill, which is what we might", "tokens": [51338, 867, 3942, 2831, 813, 472, 5389, 11, 597, 307, 437, 321, 1062, 51462], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1179, "seek": 287128, "start": 2893.2400000000002, "end": 2894.52, "text": " call fluid intelligence.", "tokens": [51462, 818, 9113, 7599, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1180, "seek": 287128, "start": 2894.52, "end": 2896.0400000000004, "text": " And so as you get bigger and bigger models,", "tokens": [51526, 400, 370, 382, 291, 483, 3801, 293, 3801, 5245, 11, 51602], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1181, "seek": 287128, "start": 2896.0400000000004, "end": 2898.7200000000003, "text": " you would expect it to go up this hierarchy of generalization", "tokens": [51602, 291, 576, 2066, 309, 281, 352, 493, 341, 22333, 295, 2674, 2144, 51736], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1182, "seek": 287128, "start": 2898.7200000000003, "end": 2900.88, "text": " where it generalizes to a skill, then it generalizes", "tokens": [51736, 689, 309, 2674, 5660, 281, 257, 5389, 11, 550, 309, 2674, 5660, 51844], "temperature": 0.0, "avg_logprob": -0.2194511799872676, "compression_ratio": 1.915625, "no_speech_prob": 0.0025892583653330803}, {"id": 1183, "seek": 290088, "start": 2901.08, "end": 2902.04, "text": " multiple skills.", "tokens": [50374, 3866, 3942, 13, 50422], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1184, "seek": 290088, "start": 2902.04, "end": 2903.4, "text": " That's correct, that's correct.", "tokens": [50422, 663, 311, 3006, 11, 300, 311, 3006, 13, 50490], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1185, "seek": 290088, "start": 2903.4, "end": 2907.2000000000003, "text": " And you know, LLMs, they're not infinitely large.", "tokens": [50490, 400, 291, 458, 11, 441, 43, 26386, 11, 436, 434, 406, 36227, 2416, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1186, "seek": 290088, "start": 2907.2000000000003, "end": 2909.56, "text": " They have only a fixed number of parameters.", "tokens": [50680, 814, 362, 787, 257, 6806, 1230, 295, 9834, 13, 50798], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1187, "seek": 290088, "start": 2909.56, "end": 2912.44, "text": " And so they have to compress their knowledge", "tokens": [50798, 400, 370, 436, 362, 281, 14778, 641, 3601, 50942], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1188, "seek": 290088, "start": 2912.44, "end": 2913.7200000000003, "text": " as much as possible.", "tokens": [50942, 382, 709, 382, 1944, 13, 51006], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1189, "seek": 290088, "start": 2913.7200000000003, "end": 2915.56, "text": " And in practice, so LLMs are mostly", "tokens": [51006, 400, 294, 3124, 11, 370, 441, 43, 26386, 366, 5240, 51098], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1190, "seek": 290088, "start": 2915.56, "end": 2920.4, "text": " storing reusable bits of programs, like vector programs.", "tokens": [51098, 26085, 41807, 9239, 295, 4268, 11, 411, 8062, 4268, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1191, "seek": 290088, "start": 2920.4, "end": 2922.8, "text": " And because they have this need for compression,", "tokens": [51340, 400, 570, 436, 362, 341, 643, 337, 19355, 11, 51460], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1192, "seek": 290088, "start": 2922.8, "end": 2924.96, "text": " it means that every time they're learning a new program,", "tokens": [51460, 309, 1355, 300, 633, 565, 436, 434, 2539, 257, 777, 1461, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1193, "seek": 290088, "start": 2924.96, "end": 2927.12, "text": " they're going to try to express it", "tokens": [51568, 436, 434, 516, 281, 853, 281, 5109, 309, 51676], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1194, "seek": 290088, "start": 2927.12, "end": 2930.1600000000003, "text": " in terms of existing bits and pieces of programs", "tokens": [51676, 294, 2115, 295, 6741, 9239, 293, 3755, 295, 4268, 51828], "temperature": 0.0, "avg_logprob": -0.1609191312134721, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.0024420444387942553}, {"id": 1195, "seek": 293016, "start": 2930.16, "end": 2932.44, "text": " that they've already learned before, right?", "tokens": [50364, 300, 436, 600, 1217, 3264, 949, 11, 558, 30, 50478], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1196, "seek": 293016, "start": 2932.44, "end": 2934.56, "text": " Isn't this the generalization?", "tokens": [50478, 6998, 380, 341, 264, 2674, 2144, 30, 50584], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1197, "seek": 293016, "start": 2934.56, "end": 2935.7599999999998, "text": " Absolutely.", "tokens": [50584, 7021, 13, 50644], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1198, "seek": 293016, "start": 2935.7599999999998, "end": 2937.2, "text": " Oh, wait, so.", "tokens": [50644, 876, 11, 1699, 11, 370, 13, 50716], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1199, "seek": 293016, "start": 2937.2, "end": 2939.3599999999997, "text": " This is why, you know, clearly LLMs", "tokens": [50716, 639, 307, 983, 11, 291, 458, 11, 4448, 441, 43, 26386, 50824], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1200, "seek": 293016, "start": 2939.3599999999997, "end": 2941.3999999999996, "text": " have some degree of generalization.", "tokens": [50824, 362, 512, 4314, 295, 2674, 2144, 13, 50926], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1201, "seek": 293016, "start": 2941.3999999999996, "end": 2943.8799999999997, "text": " And this is precisely why, it's because they have to compress.", "tokens": [50926, 400, 341, 307, 13402, 983, 11, 309, 311, 570, 436, 362, 281, 14778, 13, 51050], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1202, "seek": 293016, "start": 2943.8799999999997, "end": 2945.68, "text": " And why is that intrinsically limited?", "tokens": [51050, 400, 983, 307, 300, 28621, 984, 5567, 30, 51140], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1203, "seek": 293016, "start": 2945.68, "end": 2947.7599999999998, "text": " Why can't you just go, at some point,", "tokens": [51140, 1545, 393, 380, 291, 445, 352, 11, 412, 512, 935, 11, 51244], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1204, "seek": 293016, "start": 2947.7599999999998, "end": 2949.6, "text": " it has to learn a higher level of generalization,", "tokens": [51244, 309, 575, 281, 1466, 257, 2946, 1496, 295, 2674, 2144, 11, 51336], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1205, "seek": 293016, "start": 2949.6, "end": 2951.3599999999997, "text": " a higher level, and then the highest level", "tokens": [51336, 257, 2946, 1496, 11, 293, 550, 264, 6343, 1496, 51424], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1206, "seek": 293016, "start": 2951.3599999999997, "end": 2952.44, "text": " is the fluid intelligence.", "tokens": [51424, 307, 264, 9113, 7599, 13, 51478], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1207, "seek": 293016, "start": 2952.44, "end": 2955.08, "text": " It's intrinsically limited because the substrate", "tokens": [51478, 467, 311, 28621, 984, 5567, 570, 264, 27585, 51610], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1208, "seek": 293016, "start": 2955.08, "end": 2958.3599999999997, "text": " of your model is a big parametric curve.", "tokens": [51610, 295, 428, 2316, 307, 257, 955, 6220, 17475, 7605, 13, 51774], "temperature": 0.0, "avg_logprob": -0.208497124749261, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.0032660476863384247}, {"id": 1209, "seek": 295836, "start": 2958.36, "end": 2961.96, "text": " And all you can do with this is local generalization.", "tokens": [50364, 400, 439, 291, 393, 360, 365, 341, 307, 2654, 2674, 2144, 13, 50544], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1210, "seek": 295836, "start": 2961.96, "end": 2965.48, "text": " If you want to go beyond this towards broader", "tokens": [50544, 759, 291, 528, 281, 352, 4399, 341, 3030, 13227, 50720], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1211, "seek": 295836, "start": 2965.48, "end": 2967.52, "text": " or even extreme generalization, you", "tokens": [50720, 420, 754, 8084, 2674, 2144, 11, 291, 50822], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1212, "seek": 295836, "start": 2967.52, "end": 2969.84, "text": " have to move to a different type of model.", "tokens": [50822, 362, 281, 1286, 281, 257, 819, 2010, 295, 2316, 13, 50938], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1213, "seek": 295836, "start": 2969.84, "end": 2974.0, "text": " And my paradigm of choice is discrete program search,", "tokens": [50938, 400, 452, 24709, 295, 3922, 307, 27706, 1461, 3164, 11, 51146], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1214, "seek": 295836, "start": 2974.0, "end": 2975.04, "text": " program synthesis.", "tokens": [51146, 1461, 30252, 13, 51198], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1215, "seek": 295836, "start": 2975.04, "end": 2977.28, "text": " So and if you want to understand that,", "tokens": [51198, 407, 293, 498, 291, 528, 281, 1223, 300, 11, 51310], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1216, "seek": 295836, "start": 2977.28, "end": 2981.52, "text": " you can sort of like compare and contrast it with deep learning.", "tokens": [51310, 291, 393, 1333, 295, 411, 6794, 293, 8712, 309, 365, 2452, 2539, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1217, "seek": 295836, "start": 2981.52, "end": 2985.4, "text": " So in deep learning, your model is a parametric curve,", "tokens": [51522, 407, 294, 2452, 2539, 11, 428, 2316, 307, 257, 6220, 17475, 7605, 11, 51716], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1218, "seek": 295836, "start": 2985.4, "end": 2987.08, "text": " a differentiable parametric curve.", "tokens": [51716, 257, 819, 9364, 6220, 17475, 7605, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1829360298488451, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.0012046496849507093}, {"id": 1219, "seek": 298708, "start": 2987.08, "end": 2989.64, "text": " In program synthesis, your model", "tokens": [50364, 682, 1461, 30252, 11, 428, 2316, 50492], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1220, "seek": 298708, "start": 2989.64, "end": 2992.56, "text": " is a discrete graph of operators.", "tokens": [50492, 307, 257, 27706, 4295, 295, 19077, 13, 50638], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1221, "seek": 298708, "start": 2992.56, "end": 2995.08, "text": " So you've got like a set of logical operators,", "tokens": [50638, 407, 291, 600, 658, 411, 257, 992, 295, 14978, 19077, 11, 50764], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1222, "seek": 298708, "start": 2995.08, "end": 2997.4, "text": " like a domain-specific language.", "tokens": [50764, 411, 257, 9274, 12, 29258, 2856, 13, 50880], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1223, "seek": 298708, "start": 2997.4, "end": 2999.7999999999997, "text": " You're picking instances of it.", "tokens": [50880, 509, 434, 8867, 14519, 295, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1224, "seek": 298708, "start": 2999.7999999999997, "end": 3001.96, "text": " You're structuring that into a graph.", "tokens": [51000, 509, 434, 6594, 1345, 300, 666, 257, 4295, 13, 51108], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1225, "seek": 298708, "start": 3001.96, "end": 3003.24, "text": " That's a program.", "tokens": [51108, 663, 311, 257, 1461, 13, 51172], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1226, "seek": 298708, "start": 3003.24, "end": 3005.48, "text": " And that's actually very similar to like a program", "tokens": [51172, 400, 300, 311, 767, 588, 2531, 281, 411, 257, 1461, 51284], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1227, "seek": 298708, "start": 3005.48, "end": 3009.4, "text": " you might write in Python or C++ and so on.", "tokens": [51284, 291, 1062, 2464, 294, 15329, 420, 383, 25472, 293, 370, 322, 13, 51480], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1228, "seek": 298708, "start": 3009.4, "end": 3011.88, "text": " And in deep learning, your learning engine,", "tokens": [51480, 400, 294, 2452, 2539, 11, 428, 2539, 2848, 11, 51604], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1229, "seek": 298708, "start": 3011.88, "end": 3013.6, "text": " because we are doing much learning here,", "tokens": [51604, 570, 321, 366, 884, 709, 2539, 510, 11, 51690], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1230, "seek": 298708, "start": 3013.6, "end": 3016.7999999999997, "text": " like we're trying to automatically learn these models.", "tokens": [51690, 411, 321, 434, 1382, 281, 6772, 1466, 613, 5245, 13, 51850], "temperature": 0.0, "avg_logprob": -0.17610180471825787, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.00032088346779346466}, {"id": 1231, "seek": 301680, "start": 3016.8, "end": 3021.32, "text": " In deep learning, your learning engine is quite in the sense.", "tokens": [50364, 682, 2452, 2539, 11, 428, 2539, 2848, 307, 1596, 294, 264, 2020, 13, 50590], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1232, "seek": 301680, "start": 3021.32, "end": 3024.44, "text": " And quite in the sense is very compute efficient,", "tokens": [50590, 400, 1596, 294, 264, 2020, 307, 588, 14722, 7148, 11, 50746], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1233, "seek": 301680, "start": 3024.44, "end": 3027.0800000000004, "text": " because you have this very strong, informative feedback", "tokens": [50746, 570, 291, 362, 341, 588, 2068, 11, 27759, 5824, 50878], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1234, "seek": 301680, "start": 3027.0800000000004, "end": 3029.76, "text": " signal about where the solution is.", "tokens": [50878, 6358, 466, 689, 264, 3827, 307, 13, 51012], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1235, "seek": 301680, "start": 3029.76, "end": 3031.76, "text": " So you can get to the solution very quickly.", "tokens": [51012, 407, 291, 393, 483, 281, 264, 3827, 588, 2661, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1236, "seek": 301680, "start": 3031.76, "end": 3035.04, "text": " But it is very data inefficient, meaning", "tokens": [51112, 583, 309, 307, 588, 1412, 43495, 11, 3620, 51276], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1237, "seek": 301680, "start": 3035.04, "end": 3036.84, "text": " that in order to make it work, you", "tokens": [51276, 300, 294, 1668, 281, 652, 309, 589, 11, 291, 51366], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1238, "seek": 301680, "start": 3036.84, "end": 3039.6400000000003, "text": " need a dense sampling of the operating space.", "tokens": [51366, 643, 257, 18011, 21179, 295, 264, 7447, 1901, 13, 51506], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1239, "seek": 301680, "start": 3039.6400000000003, "end": 3041.8, "text": " You need a dense sampling of the data distribution.", "tokens": [51506, 509, 643, 257, 18011, 21179, 295, 264, 1412, 7316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1240, "seek": 301680, "start": 3041.8, "end": 3044.4, "text": " And then you're limited to only generalizing", "tokens": [51614, 400, 550, 291, 434, 5567, 281, 787, 2674, 3319, 51744], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1241, "seek": 301680, "start": 3044.4, "end": 3046.0800000000004, "text": " within that data distribution.", "tokens": [51744, 1951, 300, 1412, 7316, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16223004969154917, "compression_ratio": 1.8721804511278195, "no_speech_prob": 0.0008104197913780808}, {"id": 1242, "seek": 304608, "start": 3046.08, "end": 3048.16, "text": " And the reason why you have this limitation", "tokens": [50364, 400, 264, 1778, 983, 291, 362, 341, 27432, 50468], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1243, "seek": 304608, "start": 3048.16, "end": 3050.0, "text": " is because your model is a curve.", "tokens": [50468, 307, 570, 428, 2316, 307, 257, 7605, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1244, "seek": 304608, "start": 3050.0, "end": 3053.68, "text": " And meanwhile, if you look at discrete program search,", "tokens": [50560, 400, 29252, 11, 498, 291, 574, 412, 27706, 1461, 3164, 11, 50744], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1245, "seek": 304608, "start": 3053.68, "end": 3056.84, "text": " the learning engine is combinatorial search.", "tokens": [50744, 264, 2539, 2848, 307, 2512, 31927, 831, 3164, 13, 50902], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1246, "seek": 304608, "start": 3056.84, "end": 3058.92, "text": " You're just trying a bunch of programs", "tokens": [50902, 509, 434, 445, 1382, 257, 3840, 295, 4268, 51006], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1247, "seek": 304608, "start": 3058.92, "end": 3061.7999999999997, "text": " until you find one that actually miss your spec.", "tokens": [51006, 1826, 291, 915, 472, 300, 767, 1713, 428, 1608, 13, 51150], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1248, "seek": 304608, "start": 3061.7999999999997, "end": 3064.24, "text": " This process is extremely data efficient.", "tokens": [51150, 639, 1399, 307, 4664, 1412, 7148, 13, 51272], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1249, "seek": 304608, "start": 3064.24, "end": 3066.12, "text": " You can learn a generalizable program", "tokens": [51272, 509, 393, 1466, 257, 2674, 22395, 1461, 51366], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1250, "seek": 304608, "start": 3066.12, "end": 3068.48, "text": " from just one example, two examples, which", "tokens": [51366, 490, 445, 472, 1365, 11, 732, 5110, 11, 597, 51484], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1251, "seek": 304608, "start": 3068.48, "end": 3070.72, "text": " is why it works so well on Arc, by the way.", "tokens": [51484, 307, 983, 309, 1985, 370, 731, 322, 21727, 11, 538, 264, 636, 13, 51596], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1252, "seek": 304608, "start": 3070.72, "end": 3074.4, "text": " But the big limitation is that it's extremely compute", "tokens": [51596, 583, 264, 955, 27432, 307, 300, 309, 311, 4664, 14722, 51780], "temperature": 0.0, "avg_logprob": -0.13552518044748613, "compression_ratio": 1.6875, "no_speech_prob": 0.0005578970303758979}, {"id": 1253, "seek": 307440, "start": 3074.4, "end": 3077.32, "text": " inefficient, because you're running into combinatorial", "tokens": [50364, 43495, 11, 570, 291, 434, 2614, 666, 2512, 31927, 831, 50510], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1254, "seek": 307440, "start": 3077.32, "end": 3078.8, "text": " explosion, of course.", "tokens": [50510, 15673, 11, 295, 1164, 13, 50584], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1255, "seek": 307440, "start": 3078.8, "end": 3082.32, "text": " And so you can sort of see here how", "tokens": [50584, 400, 370, 291, 393, 1333, 295, 536, 510, 577, 50760], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1256, "seek": 307440, "start": 3082.32, "end": 3084.88, "text": " the planning and discrete program search,", "tokens": [50760, 264, 5038, 293, 27706, 1461, 3164, 11, 50888], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1257, "seek": 307440, "start": 3084.88, "end": 3089.6800000000003, "text": " they have very complementary strengths and limitations", "tokens": [50888, 436, 362, 588, 40705, 16986, 293, 15705, 51128], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1258, "seek": 307440, "start": 3089.6800000000003, "end": 3090.2400000000002, "text": " as well.", "tokens": [51128, 382, 731, 13, 51156], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1259, "seek": 307440, "start": 3090.2400000000002, "end": 3093.6800000000003, "text": " Every limitation of deep learning has a strength,", "tokens": [51156, 2048, 27432, 295, 2452, 2539, 575, 257, 3800, 11, 51328], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1260, "seek": 307440, "start": 3093.6800000000003, "end": 3097.44, "text": " a corresponding strength in program synthesis and inversely.", "tokens": [51328, 257, 11760, 3800, 294, 1461, 30252, 293, 21378, 736, 13, 51516], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1261, "seek": 307440, "start": 3097.44, "end": 3100.64, "text": " And I think the path forward is going to be to merge the two,", "tokens": [51516, 400, 286, 519, 264, 3100, 2128, 307, 516, 281, 312, 281, 22183, 264, 732, 11, 51676], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1262, "seek": 307440, "start": 3100.64, "end": 3102.04, "text": " to basically start doing.", "tokens": [51676, 281, 1936, 722, 884, 13, 51746], "temperature": 0.0, "avg_logprob": -0.22984547708548753, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.004655980039387941}, {"id": 1263, "seek": 310204, "start": 3102.08, "end": 3104.24, "text": " So another way you can think about it", "tokens": [50366, 407, 1071, 636, 291, 393, 519, 466, 309, 50474], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1264, "seek": 310204, "start": 3104.24, "end": 3108.7599999999998, "text": " is, so these parametric curves, train with ground descent,", "tokens": [50474, 307, 11, 370, 613, 6220, 17475, 19490, 11, 3847, 365, 2727, 23475, 11, 50700], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1265, "seek": 310204, "start": 3108.7599999999998, "end": 3111.08, "text": " there are great fits for everything", "tokens": [50700, 456, 366, 869, 9001, 337, 1203, 50816], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1266, "seek": 310204, "start": 3111.08, "end": 3115.6, "text": " that's system one type thinking, like pattern cognition,", "tokens": [50816, 300, 311, 1185, 472, 2010, 1953, 11, 411, 5102, 46905, 11, 51042], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1267, "seek": 310204, "start": 3115.6, "end": 3118.52, "text": " intuition, memorization, and so on.", "tokens": [51042, 24002, 11, 10560, 2144, 11, 293, 370, 322, 13, 51188], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1268, "seek": 310204, "start": 3118.52, "end": 3122.32, "text": " And discrete program search is a great fit", "tokens": [51188, 400, 27706, 1461, 3164, 307, 257, 869, 3318, 51378], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1269, "seek": 310204, "start": 3122.32, "end": 3126.0, "text": " for type two thinking, system two thinking.", "tokens": [51378, 337, 2010, 732, 1953, 11, 1185, 732, 1953, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1270, "seek": 310204, "start": 3126.0, "end": 3128.96, "text": " For instance, planning, reasoning,", "tokens": [51562, 1171, 5197, 11, 5038, 11, 21577, 11, 51710], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1271, "seek": 310204, "start": 3128.96, "end": 3131.36, "text": " quickly figuring out a generalizable model,", "tokens": [51710, 2661, 15213, 484, 257, 2674, 22395, 2316, 11, 51830], "temperature": 0.0, "avg_logprob": -0.2222155561350813, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002963702427223325}, {"id": 1272, "seek": 313136, "start": 3131.36, "end": 3134.0, "text": " that matches just one or two examples,", "tokens": [50364, 300, 10676, 445, 472, 420, 732, 5110, 11, 50496], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1273, "seek": 313136, "start": 3134.0, "end": 3136.0, "text": " like for an archbishop, for instance.", "tokens": [50496, 411, 337, 364, 3912, 65, 18091, 11, 337, 5197, 13, 50596], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1274, "seek": 313136, "start": 3136.0, "end": 3140.6400000000003, "text": " And I think humans are never doing pure system one", "tokens": [50596, 400, 286, 519, 6255, 366, 1128, 884, 6075, 1185, 472, 50828], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1275, "seek": 313136, "start": 3140.6400000000003, "end": 3141.6, "text": " or pure system two.", "tokens": [50828, 420, 6075, 1185, 732, 13, 50876], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1276, "seek": 313136, "start": 3141.6, "end": 3144.6, "text": " They're always mixing and matching both.", "tokens": [50876, 814, 434, 1009, 11983, 293, 14324, 1293, 13, 51026], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1277, "seek": 313136, "start": 3144.6, "end": 3147.04, "text": " And right now, we have all the tools for system one.", "tokens": [51026, 400, 558, 586, 11, 321, 362, 439, 264, 3873, 337, 1185, 472, 13, 51148], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1278, "seek": 313136, "start": 3147.04, "end": 3149.32, "text": " We have almost nothing for system two.", "tokens": [51148, 492, 362, 1920, 1825, 337, 1185, 732, 13, 51262], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1279, "seek": 313136, "start": 3149.32, "end": 3152.08, "text": " The way forward is to create a hybrid system.", "tokens": [51262, 440, 636, 2128, 307, 281, 1884, 257, 13051, 1185, 13, 51400], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1280, "seek": 313136, "start": 3152.08, "end": 3154.1600000000003, "text": " And I think the form it's going to take", "tokens": [51400, 400, 286, 519, 264, 1254, 309, 311, 516, 281, 747, 51504], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1281, "seek": 313136, "start": 3154.1600000000003, "end": 3157.2400000000002, "text": " is it's going to be mostly system two.", "tokens": [51504, 307, 309, 311, 516, 281, 312, 5240, 1185, 732, 13, 51658], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1282, "seek": 313136, "start": 3157.2400000000002, "end": 3160.56, "text": " So the outer structure is going to be a discrete program", "tokens": [51658, 407, 264, 10847, 3877, 307, 516, 281, 312, 257, 27706, 1461, 51824], "temperature": 0.0, "avg_logprob": -0.16302493238073634, "compression_ratio": 1.848, "no_speech_prob": 0.0004944087122566998}, {"id": 1283, "seek": 316056, "start": 3160.56, "end": 3162.04, "text": " search system.", "tokens": [50364, 3164, 1185, 13, 50438], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1284, "seek": 316056, "start": 3162.04, "end": 3164.68, "text": " But you're going to fix the fundamental limitation", "tokens": [50438, 583, 291, 434, 516, 281, 3191, 264, 8088, 27432, 50570], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1285, "seek": 316056, "start": 3164.68, "end": 3166.84, "text": " of discrete program search, which is combinator explosion.", "tokens": [50570, 295, 27706, 1461, 3164, 11, 597, 307, 2512, 31927, 15673, 13, 50678], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1286, "seek": 316056, "start": 3166.84, "end": 3169.44, "text": " You're going to fix it with deep learning.", "tokens": [50678, 509, 434, 516, 281, 3191, 309, 365, 2452, 2539, 13, 50808], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1287, "seek": 316056, "start": 3169.44, "end": 3172.4, "text": " You're going to leverage deep learning to guide,", "tokens": [50808, 509, 434, 516, 281, 13982, 2452, 2539, 281, 5934, 11, 50956], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1288, "seek": 316056, "start": 3172.4, "end": 3175.48, "text": " to provide intuition in program space,", "tokens": [50956, 281, 2893, 24002, 294, 1461, 1901, 11, 51110], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1289, "seek": 316056, "start": 3175.48, "end": 3177.64, "text": " to guide the program search.", "tokens": [51110, 281, 5934, 264, 1461, 3164, 13, 51218], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1290, "seek": 316056, "start": 3177.64, "end": 3180.96, "text": " And I think that's very similar to what you see,", "tokens": [51218, 400, 286, 519, 300, 311, 588, 2531, 281, 437, 291, 536, 11, 51384], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1291, "seek": 316056, "start": 3180.96, "end": 3183.7999999999997, "text": " for instance, when you're playing chess", "tokens": [51384, 337, 5197, 11, 562, 291, 434, 2433, 24122, 51526], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1292, "seek": 316056, "start": 3183.7999999999997, "end": 3186.48, "text": " or when you're trying to prove a theorem,", "tokens": [51526, 420, 562, 291, 434, 1382, 281, 7081, 257, 20904, 11, 51660], "temperature": 0.0, "avg_logprob": -0.14112435446845162, "compression_ratio": 1.7887931034482758, "no_speech_prob": 0.002570037730038166}, {"id": 1293, "seek": 318648, "start": 3186.52, "end": 3191.4, "text": " is that it's mostly a reasoning thing,", "tokens": [50366, 307, 300, 309, 311, 5240, 257, 21577, 551, 11, 50610], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1294, "seek": 318648, "start": 3191.4, "end": 3193.6, "text": " but you start out with some intuition", "tokens": [50610, 457, 291, 722, 484, 365, 512, 24002, 50720], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1295, "seek": 318648, "start": 3193.6, "end": 3195.44, "text": " about the shape of the solution.", "tokens": [50720, 466, 264, 3909, 295, 264, 3827, 13, 50812], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1296, "seek": 318648, "start": 3195.44, "end": 3198.04, "text": " And that's very much something you can get", "tokens": [50812, 400, 300, 311, 588, 709, 746, 291, 393, 483, 50942], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1297, "seek": 318648, "start": 3198.04, "end": 3199.64, "text": " via a deep learning model.", "tokens": [50942, 5766, 257, 2452, 2539, 2316, 13, 51022], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1298, "seek": 318648, "start": 3199.64, "end": 3203.36, "text": " Deep learning models, they're very much like intuition machines.", "tokens": [51022, 14895, 2539, 5245, 11, 436, 434, 588, 709, 411, 24002, 8379, 13, 51208], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1299, "seek": 318648, "start": 3203.36, "end": 3205.36, "text": " They're pattern matching machines.", "tokens": [51208, 814, 434, 5102, 14324, 8379, 13, 51308], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1300, "seek": 318648, "start": 3205.36, "end": 3210.16, "text": " So you start from this shape of the solution,", "tokens": [51308, 407, 291, 722, 490, 341, 3909, 295, 264, 3827, 11, 51548], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1301, "seek": 318648, "start": 3210.16, "end": 3213.76, "text": " and then you're going to do actual explicit discrete", "tokens": [51548, 293, 550, 291, 434, 516, 281, 360, 3539, 13691, 27706, 51728], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1302, "seek": 318648, "start": 3213.76, "end": 3215.16, "text": " program search.", "tokens": [51728, 1461, 3164, 13, 51798], "temperature": 0.0, "avg_logprob": -0.18133212552212252, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006882977322675288}, {"id": 1303, "seek": 321516, "start": 3215.16, "end": 3218.12, "text": " But you're not going to do it via brute force.", "tokens": [50364, 583, 291, 434, 406, 516, 281, 360, 309, 5766, 47909, 3464, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1304, "seek": 321516, "start": 3218.12, "end": 3222.56, "text": " You're not going to try things kind of like randomly.", "tokens": [50512, 509, 434, 406, 516, 281, 853, 721, 733, 295, 411, 16979, 13, 50734], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1305, "seek": 321516, "start": 3222.56, "end": 3225.92, "text": " You're actually going to ask another deep learning model", "tokens": [50734, 509, 434, 767, 516, 281, 1029, 1071, 2452, 2539, 2316, 50902], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1306, "seek": 321516, "start": 3225.92, "end": 3226.96, "text": " for suggestions.", "tokens": [50902, 337, 13396, 13, 50954], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1307, "seek": 321516, "start": 3226.96, "end": 3229.8399999999997, "text": " Like, here's the best likely next step.", "tokens": [50954, 1743, 11, 510, 311, 264, 1151, 3700, 958, 1823, 13, 51098], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1308, "seek": 321516, "start": 3229.8399999999997, "end": 3232.04, "text": " Here's where in the graph you should be going.", "tokens": [51098, 1692, 311, 689, 294, 264, 4295, 291, 820, 312, 516, 13, 51208], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1309, "seek": 321516, "start": 3232.04, "end": 3234.56, "text": " And you can also use yet another deep learning model", "tokens": [51208, 400, 291, 393, 611, 764, 1939, 1071, 2452, 2539, 2316, 51334], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1310, "seek": 321516, "start": 3234.56, "end": 3237.72, "text": " for feedback about, well, here's what I had so far.", "tokens": [51334, 337, 5824, 466, 11, 731, 11, 510, 311, 437, 286, 632, 370, 1400, 13, 51492], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1311, "seek": 321516, "start": 3237.72, "end": 3238.7599999999998, "text": " Is it looking good?", "tokens": [51492, 1119, 309, 1237, 665, 30, 51544], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1312, "seek": 321516, "start": 3238.7599999999998, "end": 3241.24, "text": " Should I just backtrack and try something new?", "tokens": [51544, 6454, 286, 445, 646, 19466, 293, 853, 746, 777, 30, 51668], "temperature": 0.0, "avg_logprob": -0.16249473956452698, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.0009061893215402961}, {"id": 1313, "seek": 324124, "start": 3241.24, "end": 3246.04, "text": " So I think discrete program search is going to be the key,", "tokens": [50364, 407, 286, 519, 27706, 1461, 3164, 307, 516, 281, 312, 264, 2141, 11, 50604], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1314, "seek": 324124, "start": 3246.04, "end": 3248.2, "text": " but you want to make it dramatically better,", "tokens": [50604, 457, 291, 528, 281, 652, 309, 17548, 1101, 11, 50712], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1315, "seek": 324124, "start": 3248.2, "end": 3249.8399999999997, "text": " all those of magnitude more efficient,", "tokens": [50712, 439, 729, 295, 15668, 544, 7148, 11, 50794], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1316, "seek": 324124, "start": 3249.8399999999997, "end": 3251.12, "text": " by leveraging deep learning.", "tokens": [50794, 538, 32666, 2452, 2539, 13, 50858], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1317, "seek": 324124, "start": 3251.12, "end": 3253.52, "text": " And by the way, another thing that you can use deep learning", "tokens": [50858, 400, 538, 264, 636, 11, 1071, 551, 300, 291, 393, 764, 2452, 2539, 50978], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1318, "seek": 324124, "start": 3253.52, "end": 3256.64, "text": " is, of course, things like common sense knowledge,", "tokens": [50978, 307, 11, 295, 1164, 11, 721, 411, 2689, 2020, 3601, 11, 51134], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1319, "seek": 324124, "start": 3256.64, "end": 3258.9199999999996, "text": " and knowledge in general.", "tokens": [51134, 293, 3601, 294, 2674, 13, 51248], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1320, "seek": 324124, "start": 3258.9199999999996, "end": 3260.56, "text": " And I think you're going to end up", "tokens": [51248, 400, 286, 519, 291, 434, 516, 281, 917, 493, 51330], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1321, "seek": 324124, "start": 3260.56, "end": 3262.56, "text": " with this sort of system where you", "tokens": [51330, 365, 341, 1333, 295, 1185, 689, 291, 51430], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1322, "seek": 324124, "start": 3262.56, "end": 3267.3599999999997, "text": " have this on-the-fly synthesis engine that", "tokens": [51430, 362, 341, 322, 12, 3322, 12, 14061, 30252, 2848, 300, 51670], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1323, "seek": 324124, "start": 3267.3599999999997, "end": 3269.3999999999996, "text": " can adapt to new situations.", "tokens": [51670, 393, 6231, 281, 777, 6851, 13, 51772], "temperature": 0.0, "avg_logprob": -0.16868293185194, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.5789779657498e-05}, {"id": 1324, "seek": 326940, "start": 3269.44, "end": 3271.1600000000003, "text": " But the way it adapts is that it's", "tokens": [50366, 583, 264, 636, 309, 23169, 1373, 307, 300, 309, 311, 50452], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1325, "seek": 326940, "start": 3271.1600000000003, "end": 3275.96, "text": " going to fetch from a bank of patterns,", "tokens": [50452, 516, 281, 23673, 490, 257, 3765, 295, 8294, 11, 50692], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1326, "seek": 326940, "start": 3275.96, "end": 3279.1600000000003, "text": " modules that could be themselves,", "tokens": [50692, 16679, 300, 727, 312, 2969, 11, 50852], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1327, "seek": 326940, "start": 3279.1600000000003, "end": 3282.2400000000002, "text": " curves that could be differentiable modules,", "tokens": [50852, 19490, 300, 727, 312, 819, 9364, 16679, 11, 51006], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1328, "seek": 326940, "start": 3282.2400000000002, "end": 3284.56, "text": " and some others that could be algorithmic in nature.", "tokens": [51006, 293, 512, 2357, 300, 727, 312, 9284, 299, 294, 3687, 13, 51122], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1329, "seek": 326940, "start": 3284.56, "end": 3288.76, "text": " It's going to assemble them via this process that's", "tokens": [51122, 467, 311, 516, 281, 22364, 552, 5766, 341, 1399, 300, 311, 51332], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1330, "seek": 326940, "start": 3288.76, "end": 3290.36, "text": " intuition-guided.", "tokens": [51332, 24002, 12, 2794, 2112, 13, 51412], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1331, "seek": 326940, "start": 3290.36, "end": 3292.64, "text": " And it's going to give you, for every new situation you", "tokens": [51412, 400, 309, 311, 516, 281, 976, 291, 11, 337, 633, 777, 2590, 291, 51526], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1332, "seek": 326940, "start": 3292.64, "end": 3294.2400000000002, "text": " might be faced with, it's going to give you", "tokens": [51526, 1062, 312, 11446, 365, 11, 309, 311, 516, 281, 976, 291, 51606], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1333, "seek": 326940, "start": 3294.2400000000002, "end": 3297.4, "text": " with a generalizable model that was synthesized", "tokens": [51606, 365, 257, 2674, 22395, 2316, 300, 390, 26617, 1602, 51764], "temperature": 0.0, "avg_logprob": -0.14920060530952786, "compression_ratio": 1.7890295358649788, "no_speech_prob": 0.0008990451460704207}, {"id": 1334, "seek": 329740, "start": 3297.4, "end": 3300.6, "text": " using very, very little data.", "tokens": [50364, 1228, 588, 11, 588, 707, 1412, 13, 50524], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1335, "seek": 329740, "start": 3300.6, "end": 3302.52, "text": " Something like this would sort of arc.", "tokens": [50524, 6595, 411, 341, 576, 1333, 295, 10346, 13, 50620], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1336, "seek": 329740, "start": 3302.52, "end": 3305.64, "text": " That's actually a really interesting prompt,", "tokens": [50620, 663, 311, 767, 257, 534, 1880, 12391, 11, 50776], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1337, "seek": 329740, "start": 3305.64, "end": 3308.7200000000003, "text": " because I think an interesting crux here", "tokens": [50776, 570, 286, 519, 364, 1880, 5140, 87, 510, 50930], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1338, "seek": 329740, "start": 3308.7200000000003, "end": 3311.44, "text": " is when I talk to my friends who are extremely", "tokens": [50930, 307, 562, 286, 751, 281, 452, 1855, 567, 366, 4664, 51066], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1339, "seek": 329740, "start": 3311.44, "end": 3317.48, "text": " optimistic about LLMs and expect AGI within the next couple", "tokens": [51066, 19397, 466, 441, 43, 26386, 293, 2066, 316, 26252, 1951, 264, 958, 1916, 51368], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1340, "seek": 329740, "start": 3317.48, "end": 3320.52, "text": " of years, they also, in some sense,", "tokens": [51368, 295, 924, 11, 436, 611, 11, 294, 512, 2020, 11, 51520], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1341, "seek": 329740, "start": 3320.52, "end": 3323.76, "text": " agree that scaling is not all you need,", "tokens": [51520, 3986, 300, 21589, 307, 406, 439, 291, 643, 11, 51682], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1342, "seek": 329740, "start": 3323.76, "end": 3326.6, "text": " but that the rest of the progress is undergirded", "tokens": [51682, 457, 300, 264, 1472, 295, 264, 4205, 307, 833, 70, 1271, 292, 51824], "temperature": 0.0, "avg_logprob": -0.14366255260649183, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.008840091526508331}, {"id": 1343, "seek": 332660, "start": 3326.64, "end": 3328.64, "text": " and enabled by scaling.", "tokens": [50366, 293, 15172, 538, 21589, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1344, "seek": 332660, "start": 3328.64, "end": 3331.52, "text": " But still, you need to add the system", "tokens": [50466, 583, 920, 11, 291, 643, 281, 909, 264, 1185, 50610], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1345, "seek": 332660, "start": 3331.52, "end": 3334.92, "text": " to the test time compute atop these models.", "tokens": [50610, 281, 264, 1500, 565, 14722, 412, 404, 613, 5245, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1346, "seek": 332660, "start": 3334.92, "end": 3336.88, "text": " And their perspective is that it's relatively", "tokens": [50780, 400, 641, 4585, 307, 300, 309, 311, 7226, 50878], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1347, "seek": 332660, "start": 3336.88, "end": 3338.96, "text": " straightforward to do that, because you", "tokens": [50878, 15325, 281, 360, 300, 11, 570, 291, 50982], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1348, "seek": 332660, "start": 3338.96, "end": 3341.6, "text": " have this library of representations", "tokens": [50982, 362, 341, 6405, 295, 33358, 51114], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1349, "seek": 332660, "start": 3341.6, "end": 3343.56, "text": " that you built up from free training,", "tokens": [51114, 300, 291, 3094, 493, 490, 1737, 3097, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1350, "seek": 332660, "start": 3343.56, "end": 3346.7999999999997, "text": " but it's almost talking like, it's just", "tokens": [51212, 457, 309, 311, 1920, 1417, 411, 11, 309, 311, 445, 51374], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1351, "seek": 332660, "start": 3346.7999999999997, "end": 3348.64, "text": " like skimming through textbooks.", "tokens": [51374, 411, 1110, 40471, 807, 33587, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1352, "seek": 332660, "start": 3348.64, "end": 3352.12, "text": " You need some more deliberate way in which it engages", "tokens": [51466, 509, 643, 512, 544, 30515, 636, 294, 597, 309, 45576, 51640], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1353, "seek": 332660, "start": 3352.12, "end": 3353.48, "text": " with the material it learns.", "tokens": [51640, 365, 264, 2527, 309, 27152, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1354, "seek": 332660, "start": 3353.48, "end": 3356.52, "text": " In-context learning is extremely sample-efficient.", "tokens": [51708, 682, 12, 9000, 3828, 2539, 307, 4664, 6889, 12, 68, 7816, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1830832215606189, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.00012730673188343644}, {"id": 1355, "seek": 335652, "start": 3356.52, "end": 3359.0, "text": " But to actually distill that into the weights,", "tokens": [50364, 583, 281, 767, 42923, 300, 666, 264, 17443, 11, 50488], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1356, "seek": 335652, "start": 3359.0, "end": 3361.48, "text": " you need the model to talk through the things that sees", "tokens": [50488, 291, 643, 264, 2316, 281, 751, 807, 264, 721, 300, 8194, 50612], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1357, "seek": 335652, "start": 3361.48, "end": 3363.0, "text": " and then add it back to the weights.", "tokens": [50612, 293, 550, 909, 309, 646, 281, 264, 17443, 13, 50688], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1358, "seek": 335652, "start": 3363.0, "end": 3365.64, "text": " As far as the system 2 goes, they talk about adding some kind", "tokens": [50688, 1018, 1400, 382, 264, 1185, 568, 1709, 11, 436, 751, 466, 5127, 512, 733, 50820], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1359, "seek": 335652, "start": 3365.64, "end": 3368.28, "text": " of RL setup so that it is encouraged", "tokens": [50820, 295, 497, 43, 8657, 370, 300, 309, 307, 14658, 50952], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1360, "seek": 335652, "start": 3368.28, "end": 3372.64, "text": " to proceed on the reasoning traces that end up being correct.", "tokens": [50952, 281, 8991, 322, 264, 21577, 26076, 300, 917, 493, 885, 3006, 13, 51170], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1361, "seek": 335652, "start": 3372.64, "end": 3374.72, "text": " And they think this is relatively straightforward stuff", "tokens": [51170, 400, 436, 519, 341, 307, 7226, 15325, 1507, 51274], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1362, "seek": 335652, "start": 3374.72, "end": 3376.56, "text": " that will be added within the next couple of years.", "tokens": [51274, 300, 486, 312, 3869, 1951, 264, 958, 1916, 295, 924, 13, 51366], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1363, "seek": 335652, "start": 3376.56, "end": 3377.84, "text": " That's an empirical question.", "tokens": [51366, 663, 311, 364, 31886, 1168, 13, 51430], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1364, "seek": 335652, "start": 3377.84, "end": 3378.88, "text": " So I think we'll see.", "tokens": [51430, 407, 286, 519, 321, 603, 536, 13, 51482], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1365, "seek": 335652, "start": 3378.88, "end": 3380.44, "text": " Your intuition, I assume, is not that.", "tokens": [51482, 2260, 24002, 11, 286, 6552, 11, 307, 406, 300, 13, 51560], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1366, "seek": 335652, "start": 3380.44, "end": 3381.12, "text": " I'm curious.", "tokens": [51560, 286, 478, 6369, 13, 51594], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1367, "seek": 335652, "start": 3381.12, "end": 3384.84, "text": " My intuition is, in fact, this whole system", "tokens": [51594, 1222, 24002, 307, 11, 294, 1186, 11, 341, 1379, 1185, 51780], "temperature": 0.0, "avg_logprob": -0.18041708662703232, "compression_ratio": 1.6899696048632218, "no_speech_prob": 0.0003920129092875868}, {"id": 1368, "seek": 338484, "start": 3384.84, "end": 3387.1600000000003, "text": " 2 architecture is the hard part.", "tokens": [50364, 568, 9482, 307, 264, 1152, 644, 13, 50480], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1369, "seek": 338484, "start": 3387.1600000000003, "end": 3389.04, "text": " It's the very hard and non-obvious part.", "tokens": [50480, 467, 311, 264, 588, 1152, 293, 2107, 12, 996, 1502, 644, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1370, "seek": 338484, "start": 3389.04, "end": 3392.8, "text": " Scaling up the interpolative memory is the easy part.", "tokens": [50574, 2747, 4270, 493, 264, 44902, 1166, 4675, 307, 264, 1858, 644, 13, 50762], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1371, "seek": 338484, "start": 3392.8, "end": 3397.08, "text": " All you need is, like, it's literally just a big curve.", "tokens": [50762, 1057, 291, 643, 307, 11, 411, 11, 309, 311, 3736, 445, 257, 955, 7605, 13, 50976], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1372, "seek": 338484, "start": 3397.08, "end": 3398.1200000000003, "text": " All you need is more data.", "tokens": [50976, 1057, 291, 643, 307, 544, 1412, 13, 51028], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1373, "seek": 338484, "start": 3398.1200000000003, "end": 3399.56, "text": " It's representation of a data set,", "tokens": [51028, 467, 311, 10290, 295, 257, 1412, 992, 11, 51100], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1374, "seek": 338484, "start": 3399.56, "end": 3401.84, "text": " interpolative representation of data set.", "tokens": [51100, 44902, 1166, 10290, 295, 1412, 992, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1375, "seek": 338484, "start": 3401.84, "end": 3402.84, "text": " That's the easy part.", "tokens": [51214, 663, 311, 264, 1858, 644, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1376, "seek": 338484, "start": 3402.84, "end": 3405.96, "text": " The hard part is the architecture of intelligence.", "tokens": [51264, 440, 1152, 644, 307, 264, 9482, 295, 7599, 13, 51420], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1377, "seek": 338484, "start": 3405.96, "end": 3408.6000000000004, "text": " Memory and intelligence are separate components.", "tokens": [51420, 38203, 293, 7599, 366, 4994, 6677, 13, 51552], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1378, "seek": 338484, "start": 3408.6000000000004, "end": 3409.48, "text": " We have the memory.", "tokens": [51552, 492, 362, 264, 4675, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1379, "seek": 338484, "start": 3409.48, "end": 3411.04, "text": " We don't have the intelligence yet.", "tokens": [51596, 492, 500, 380, 362, 264, 7599, 1939, 13, 51674], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1380, "seek": 338484, "start": 3411.04, "end": 3413.4, "text": " And I agree with you that, well, having the memory", "tokens": [51674, 400, 286, 3986, 365, 291, 300, 11, 731, 11, 1419, 264, 4675, 51792], "temperature": 0.0, "avg_logprob": -0.17385889600206922, "compression_ratio": 1.9846153846153847, "no_speech_prob": 0.012519496493041515}, {"id": 1381, "seek": 341340, "start": 3413.4, "end": 3414.92, "text": " is actually very useful.", "tokens": [50364, 307, 767, 588, 4420, 13, 50440], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1382, "seek": 341340, "start": 3414.92, "end": 3417.08, "text": " And if you just had the intelligence,", "tokens": [50440, 400, 498, 291, 445, 632, 264, 7599, 11, 50548], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1383, "seek": 341340, "start": 3417.08, "end": 3419.08, "text": " but it was not hooked up to an extensive memory,", "tokens": [50548, 457, 309, 390, 406, 20410, 493, 281, 364, 13246, 4675, 11, 50648], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1384, "seek": 341340, "start": 3419.08, "end": 3421.32, "text": " it would not be that useful, because it would not", "tokens": [50648, 309, 576, 406, 312, 300, 4420, 11, 570, 309, 576, 406, 50760], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1385, "seek": 341340, "start": 3421.32, "end": 3424.2400000000002, "text": " have enough material to work from.", "tokens": [50760, 362, 1547, 2527, 281, 589, 490, 13, 50906], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1386, "seek": 341340, "start": 3424.2400000000002, "end": 3424.96, "text": " Yeah.", "tokens": [50906, 865, 13, 50942], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1387, "seek": 341340, "start": 3424.96, "end": 3427.7200000000003, "text": " The alternative hypothesis here that former guest Trenton", "tokens": [50942, 440, 8535, 17291, 510, 300, 5819, 8341, 40119, 266, 51080], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1388, "seek": 341340, "start": 3427.7200000000003, "end": 3431.36, "text": " Brickin advanced is that intelligence", "tokens": [51080, 1603, 618, 259, 7339, 307, 300, 7599, 51262], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1389, "seek": 341340, "start": 3431.36, "end": 3434.84, "text": " is just hierarchically associated memory", "tokens": [51262, 307, 445, 35250, 984, 6615, 4675, 51436], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1390, "seek": 341340, "start": 3434.84, "end": 3438.0, "text": " where higher-level patterns, when Sherlock Holmes goes", "tokens": [51436, 689, 2946, 12, 12418, 8294, 11, 562, 37769, 27474, 1709, 51594], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1391, "seek": 341340, "start": 3438.0, "end": 3440.6, "text": " into a crime scene, and he's extremely sample-efficient,", "tokens": [51594, 666, 257, 7206, 4145, 11, 293, 415, 311, 4664, 6889, 12, 68, 7816, 11, 51724], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1392, "seek": 341340, "start": 3440.6, "end": 3442.36, "text": " he can just look at a few clues and figure out", "tokens": [51724, 415, 393, 445, 574, 412, 257, 1326, 20936, 293, 2573, 484, 51812], "temperature": 0.0, "avg_logprob": -0.21233115609236589, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.001281787408515811}, {"id": 1393, "seek": 344236, "start": 3442.4, "end": 3443.92, "text": " who was a murderer, and the way he's", "tokens": [50366, 567, 390, 257, 28703, 11, 293, 264, 636, 415, 311, 50442], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1394, "seek": 344236, "start": 3443.92, "end": 3446.8, "text": " able to do that is he has learned higher-level", "tokens": [50442, 1075, 281, 360, 300, 307, 415, 575, 3264, 2946, 12, 12418, 50586], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1395, "seek": 344236, "start": 3446.8, "end": 3448.08, "text": " sort of associations.", "tokens": [50586, 1333, 295, 26597, 13, 50650], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1396, "seek": 344236, "start": 3448.08, "end": 3450.28, "text": " It's memory in some fundamental sense.", "tokens": [50650, 467, 311, 4675, 294, 512, 8088, 2020, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1397, "seek": 344236, "start": 3450.28, "end": 3453.96, "text": " But so here's one way to ask the question.", "tokens": [50760, 583, 370, 510, 311, 472, 636, 281, 1029, 264, 1168, 13, 50944], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1398, "seek": 344236, "start": 3453.96, "end": 3457.4, "text": " In the brain, supposedly we do program synthesis,", "tokens": [50944, 682, 264, 3567, 11, 20581, 321, 360, 1461, 30252, 11, 51116], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1399, "seek": 344236, "start": 3457.4, "end": 3460.4, "text": " but it is just synapses connected to one another,", "tokens": [51116, 457, 309, 307, 445, 5451, 2382, 279, 4582, 281, 472, 1071, 11, 51266], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1400, "seek": 344236, "start": 3460.4, "end": 3463.08, "text": " each other, and so physically it's", "tokens": [51266, 1184, 661, 11, 293, 370, 9762, 309, 311, 51400], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1401, "seek": 344236, "start": 3463.08, "end": 3465.56, "text": " got to be that you just query the right circuit, right?", "tokens": [51400, 658, 281, 312, 300, 291, 445, 14581, 264, 558, 9048, 11, 558, 30, 51524], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1402, "seek": 344236, "start": 3465.56, "end": 3466.7200000000003, "text": " You are, yeah, yeah, yeah.", "tokens": [51524, 509, 366, 11, 1338, 11, 1338, 11, 1338, 13, 51582], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1403, "seek": 344236, "start": 3466.7200000000003, "end": 3468.32, "text": " It's a matter of degree.", "tokens": [51582, 467, 311, 257, 1871, 295, 4314, 13, 51662], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1404, "seek": 344236, "start": 3468.32, "end": 3471.8, "text": " But if you can learn it, if training in the environment", "tokens": [51662, 583, 498, 291, 393, 1466, 309, 11, 498, 3097, 294, 264, 2823, 51836], "temperature": 0.0, "avg_logprob": -0.19818576515143646, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.004608112387359142}, {"id": 1405, "seek": 347180, "start": 3472.36, "end": 3473.96, "text": " human ancestors are trained in means", "tokens": [50392, 1952, 18069, 366, 8895, 294, 1355, 50472], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1406, "seek": 347180, "start": 3473.96, "end": 3475.6800000000003, "text": " you learn those circuits, training", "tokens": [50472, 291, 1466, 729, 26354, 11, 3097, 50558], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1407, "seek": 347180, "start": 3475.6800000000003, "end": 3477.52, "text": " on the same kinds of outputs that humans produce,", "tokens": [50558, 322, 264, 912, 3685, 295, 23930, 300, 6255, 5258, 11, 50650], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1408, "seek": 347180, "start": 3477.52, "end": 3480.0800000000004, "text": " which to replicate require these kinds of circuits,", "tokens": [50650, 597, 281, 25356, 3651, 613, 3685, 295, 26354, 11, 50778], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1409, "seek": 347180, "start": 3480.0800000000004, "end": 3483.48, "text": " wouldn't that train the same kind of whatever humans have?", "tokens": [50778, 2759, 380, 300, 3847, 264, 912, 733, 295, 2035, 6255, 362, 30, 50948], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1410, "seek": 347180, "start": 3483.48, "end": 3485.04, "text": " You know, it's a matter of degree.", "tokens": [50948, 509, 458, 11, 309, 311, 257, 1871, 295, 4314, 13, 51026], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1411, "seek": 347180, "start": 3487.7200000000003, "end": 3489.5600000000004, "text": " If you have a system that has a memory", "tokens": [51160, 759, 291, 362, 257, 1185, 300, 575, 257, 4675, 51252], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1412, "seek": 347180, "start": 3489.5600000000004, "end": 3493.6800000000003, "text": " and is only capable of doing local generalization from that,", "tokens": [51252, 293, 307, 787, 8189, 295, 884, 2654, 2674, 2144, 490, 300, 11, 51458], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1413, "seek": 347180, "start": 3493.6800000000003, "end": 3496.76, "text": " it's not going to be very adaptable.", "tokens": [51458, 309, 311, 406, 516, 281, 312, 588, 6231, 712, 13, 51612], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1414, "seek": 347180, "start": 3496.76, "end": 3499.1600000000003, "text": " To be really general, you need the memory", "tokens": [51612, 1407, 312, 534, 2674, 11, 291, 643, 264, 4675, 51732], "temperature": 0.0, "avg_logprob": -0.17325286865234374, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.00047210388584062457}, {"id": 1415, "seek": 349916, "start": 3499.16, "end": 3503.2, "text": " plus the ability to search to quite some depth,", "tokens": [50364, 1804, 264, 3485, 281, 3164, 281, 1596, 512, 7161, 11, 50566], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1416, "seek": 349916, "start": 3503.2, "end": 3506.8799999999997, "text": " to achieve broader even extramuralization.", "tokens": [50566, 281, 4584, 13227, 754, 1279, 2356, 1807, 2144, 13, 50750], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1417, "seek": 349916, "start": 3506.8799999999997, "end": 3511.6, "text": " You know, like one of my favorite psychologists,", "tokens": [50750, 509, 458, 11, 411, 472, 295, 452, 2954, 41562, 11, 50986], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1418, "seek": 349916, "start": 3511.6, "end": 3515.3199999999997, "text": " so Jean Piaget was the founder of the Elemental Psychology.", "tokens": [50986, 370, 13854, 17741, 559, 302, 390, 264, 14917, 295, 264, 20900, 304, 42827, 13, 51172], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1419, "seek": 349916, "start": 3515.3199999999997, "end": 3517.72, "text": " He had a very good quote about intelligence.", "tokens": [51172, 634, 632, 257, 588, 665, 6513, 466, 7599, 13, 51292], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1420, "seek": 349916, "start": 3517.72, "end": 3520.96, "text": " He said, intelligence is what you use when you don't know what", "tokens": [51292, 634, 848, 11, 7599, 307, 437, 291, 764, 562, 291, 500, 380, 458, 437, 51454], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1421, "seek": 349916, "start": 3520.96, "end": 3522.0, "text": " to do.", "tokens": [51454, 281, 360, 13, 51506], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1422, "seek": 349916, "start": 3522.0, "end": 3526.48, "text": " And it's like, as a human living your life,", "tokens": [51506, 400, 309, 311, 411, 11, 382, 257, 1952, 2647, 428, 993, 11, 51730], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1423, "seek": 349916, "start": 3526.48, "end": 3528.7599999999998, "text": " in most situations you already know what to do,", "tokens": [51730, 294, 881, 6851, 291, 1217, 458, 437, 281, 360, 11, 51844], "temperature": 0.0, "avg_logprob": -0.26877085367838544, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0005028275190852582}, {"id": 1424, "seek": 352876, "start": 3528.76, "end": 3530.6800000000003, "text": " because you've been in this situation before.", "tokens": [50364, 570, 291, 600, 668, 294, 341, 2590, 949, 13, 50460], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1425, "seek": 352876, "start": 3530.6800000000003, "end": 3533.44, "text": " You already have the answer, right?", "tokens": [50460, 509, 1217, 362, 264, 1867, 11, 558, 30, 50598], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1426, "seek": 352876, "start": 3533.44, "end": 3535.5600000000004, "text": " And you're only going to need to use intelligence", "tokens": [50598, 400, 291, 434, 787, 516, 281, 643, 281, 764, 7599, 50704], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1427, "seek": 352876, "start": 3535.5600000000004, "end": 3539.28, "text": " when you're faced with novelty, with something you didn't expect,", "tokens": [50704, 562, 291, 434, 11446, 365, 44805, 11, 365, 746, 291, 994, 380, 2066, 11, 50890], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1428, "seek": 352876, "start": 3539.28, "end": 3541.2400000000002, "text": " with something that you weren't prepared for,", "tokens": [50890, 365, 746, 300, 291, 4999, 380, 4927, 337, 11, 50988], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1429, "seek": 352876, "start": 3541.2400000000002, "end": 3544.96, "text": " either by your own experience, your own life experience,", "tokens": [50988, 2139, 538, 428, 1065, 1752, 11, 428, 1065, 993, 1752, 11, 51174], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1430, "seek": 352876, "start": 3544.96, "end": 3547.48, "text": " or by your evolutionary history.", "tokens": [51174, 420, 538, 428, 27567, 2503, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1431, "seek": 352876, "start": 3547.48, "end": 3551.5600000000004, "text": " Like, this day that you're living right now is different", "tokens": [51300, 1743, 11, 341, 786, 300, 291, 434, 2647, 558, 586, 307, 819, 51504], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1432, "seek": 352876, "start": 3551.5600000000004, "end": 3554.84, "text": " in some important ways from every day you've lived before,", "tokens": [51504, 294, 512, 1021, 2098, 490, 633, 786, 291, 600, 5152, 949, 11, 51668], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1433, "seek": 352876, "start": 3554.84, "end": 3557.44, "text": " but it's also different from any day ever lived", "tokens": [51668, 457, 309, 311, 611, 819, 490, 604, 786, 1562, 5152, 51798], "temperature": 0.0, "avg_logprob": -0.13666833706987583, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.0004406347288750112}, {"id": 1434, "seek": 355744, "start": 3557.44, "end": 3558.88, "text": " by any of your ancestors.", "tokens": [50364, 538, 604, 295, 428, 18069, 13, 50436], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1435, "seek": 355744, "start": 3558.88, "end": 3562.6, "text": " And still, you're capable of being functional, right?", "tokens": [50436, 400, 920, 11, 291, 434, 8189, 295, 885, 11745, 11, 558, 30, 50622], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1436, "seek": 355744, "start": 3562.6, "end": 3563.48, "text": " How is it possible?", "tokens": [50622, 1012, 307, 309, 1944, 30, 50666], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1437, "seek": 355744, "start": 3563.48, "end": 3565.76, "text": " I'm not denying that generalization is extremely important,", "tokens": [50666, 286, 478, 406, 30363, 300, 2674, 2144, 307, 4664, 1021, 11, 50780], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1438, "seek": 355744, "start": 3565.76, "end": 3568.96, "text": " and is the basis for intelligence.", "tokens": [50780, 293, 307, 264, 5143, 337, 7599, 13, 50940], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1439, "seek": 355744, "start": 3568.96, "end": 3570.6, "text": " That's not the correct, the correct is like,", "tokens": [50940, 663, 311, 406, 264, 3006, 11, 264, 3006, 307, 411, 11, 51022], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1440, "seek": 355744, "start": 3570.6, "end": 3572.12, "text": " how much of that is happening in the models?", "tokens": [51022, 577, 709, 295, 300, 307, 2737, 294, 264, 5245, 30, 51098], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1441, "seek": 355744, "start": 3572.12, "end": 3575.0, "text": " But, okay, let me ask a separate question.", "tokens": [51098, 583, 11, 1392, 11, 718, 385, 1029, 257, 4994, 1168, 13, 51242], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1442, "seek": 355744, "start": 3575.0, "end": 3578.52, "text": " We might keep going in the circle here.", "tokens": [51242, 492, 1062, 1066, 516, 294, 264, 6329, 510, 13, 51418], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1443, "seek": 355744, "start": 3578.52, "end": 3581.0, "text": " The differences in intelligence between humans,", "tokens": [51418, 440, 7300, 294, 7599, 1296, 6255, 11, 51542], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1444, "seek": 355744, "start": 3581.0, "end": 3583.48, "text": " maybe the intelligence tests because of reasons", "tokens": [51542, 1310, 264, 7599, 6921, 570, 295, 4112, 51666], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1445, "seek": 355744, "start": 3583.48, "end": 3584.6, "text": " you mentioned are not measuring it well,", "tokens": [51666, 291, 2835, 366, 406, 13389, 309, 731, 11, 51722], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1446, "seek": 355744, "start": 3584.6, "end": 3586.08, "text": " but clearly there's differences in intelligence", "tokens": [51722, 457, 4448, 456, 311, 7300, 294, 7599, 51796], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1447, "seek": 355744, "start": 3586.08, "end": 3587.08, "text": " between different humans.", "tokens": [51796, 1296, 819, 6255, 13, 51846], "temperature": 0.0, "avg_logprob": -0.20621607550259294, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0020995689556002617}, {"id": 1448, "seek": 358708, "start": 3587.68, "end": 3589.64, "text": " What is your explanation for what's going on there?", "tokens": [50394, 708, 307, 428, 10835, 337, 437, 311, 516, 322, 456, 30, 50492], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1449, "seek": 358708, "start": 3589.64, "end": 3592.16, "text": " Because I think that's sort of compatible with my story", "tokens": [50492, 1436, 286, 519, 300, 311, 1333, 295, 18218, 365, 452, 1657, 50618], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1450, "seek": 358708, "start": 3592.16, "end": 3593.72, "text": " that there's a spectrum of generality", "tokens": [50618, 300, 456, 311, 257, 11143, 295, 1337, 1860, 50696], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1451, "seek": 358708, "start": 3593.72, "end": 3596.52, "text": " and that these models are climbing up to a human level,", "tokens": [50696, 293, 300, 613, 5245, 366, 14780, 493, 281, 257, 1952, 1496, 11, 50836], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1452, "seek": 358708, "start": 3596.52, "end": 3598.2799999999997, "text": " and even some humans haven't even climbed up", "tokens": [50836, 293, 754, 512, 6255, 2378, 380, 754, 28691, 493, 50924], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1453, "seek": 358708, "start": 3598.2799999999997, "end": 3602.4, "text": " to the Einstein level or the Francois level, but.", "tokens": [50924, 281, 264, 23486, 1496, 420, 264, 34695, 271, 1496, 11, 457, 13, 51130], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1454, "seek": 358708, "start": 3602.4, "end": 3604.44, "text": " That's a great question, you know.", "tokens": [51130, 663, 311, 257, 869, 1168, 11, 291, 458, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1455, "seek": 358708, "start": 3604.44, "end": 3607.96, "text": " There is extensive evidence that intelligence,", "tokens": [51232, 821, 307, 13246, 4467, 300, 7599, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1456, "seek": 358708, "start": 3607.96, "end": 3611.0, "text": " difference in intelligence are mostly genetic in nature,", "tokens": [51408, 2649, 294, 7599, 366, 5240, 12462, 294, 3687, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1457, "seek": 358708, "start": 3611.0, "end": 3611.84, "text": " right?", "tokens": [51560, 558, 30, 51602], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1458, "seek": 358708, "start": 3611.84, "end": 3614.4, "text": " Meaning that if you take someone who is not very intelligent,", "tokens": [51602, 19948, 300, 498, 291, 747, 1580, 567, 307, 406, 588, 13232, 11, 51730], "temperature": 0.0, "avg_logprob": -0.1724592996022058, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.00048773520393297076}, {"id": 1459, "seek": 361440, "start": 3614.4, "end": 3617.88, "text": " there is no amount of training, of like training data,", "tokens": [50364, 456, 307, 572, 2372, 295, 3097, 11, 295, 411, 3097, 1412, 11, 50538], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1460, "seek": 361440, "start": 3617.88, "end": 3619.7200000000003, "text": " you can expose that person to that would", "tokens": [50538, 291, 393, 19219, 300, 954, 281, 300, 576, 50630], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1461, "seek": 361440, "start": 3621.12, "end": 3622.88, "text": " make them become Einstein.", "tokens": [50700, 652, 552, 1813, 23486, 13, 50788], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1462, "seek": 361440, "start": 3622.88, "end": 3625.36, "text": " And this kind of points to the fact", "tokens": [50788, 400, 341, 733, 295, 2793, 281, 264, 1186, 50912], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1463, "seek": 361440, "start": 3625.36, "end": 3628.56, "text": " that you really need a better architecture,", "tokens": [50912, 300, 291, 534, 643, 257, 1101, 9482, 11, 51072], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1464, "seek": 361440, "start": 3628.56, "end": 3630.2400000000002, "text": " you need a better algorithm,", "tokens": [51072, 291, 643, 257, 1101, 9284, 11, 51156], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1465, "seek": 361440, "start": 3630.2400000000002, "end": 3634.04, "text": " and more training data is not in fact all you need.", "tokens": [51156, 293, 544, 3097, 1412, 307, 406, 294, 1186, 439, 291, 643, 13, 51346], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1466, "seek": 361440, "start": 3634.04, "end": 3635.92, "text": " I think I agree with that.", "tokens": [51346, 286, 519, 286, 3986, 365, 300, 13, 51440], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1467, "seek": 361440, "start": 3635.92, "end": 3639.0, "text": " I think what, maybe the way I might phrase it is that", "tokens": [51440, 286, 519, 437, 11, 1310, 264, 636, 286, 1062, 9535, 309, 307, 300, 51594], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1468, "seek": 361440, "start": 3639.0, "end": 3642.2400000000002, "text": " the people who are smarter have in ML language", "tokens": [51594, 264, 561, 567, 366, 20294, 362, 294, 21601, 2856, 51756], "temperature": 0.0, "avg_logprob": -0.18318275104869497, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0006871605291962624}, {"id": 1469, "seek": 364224, "start": 3642.2799999999997, "end": 3645.68, "text": " better initializations, the neural wiring,", "tokens": [50366, 1101, 5883, 14455, 11, 264, 18161, 27520, 11, 50536], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1470, "seek": 364224, "start": 3645.68, "end": 3648.04, "text": " if you just look at, it's more efficient,", "tokens": [50536, 498, 291, 445, 574, 412, 11, 309, 311, 544, 7148, 11, 50654], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1471, "seek": 364224, "start": 3648.04, "end": 3651.3999999999996, "text": " they have maybe greater density of firing.", "tokens": [50654, 436, 362, 1310, 5044, 10305, 295, 16045, 13, 50822], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1472, "seek": 364224, "start": 3651.3999999999996, "end": 3653.2, "text": " And so as some part of the story is scaling,", "tokens": [50822, 400, 370, 382, 512, 644, 295, 264, 1657, 307, 21589, 11, 50912], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1473, "seek": 364224, "start": 3653.2, "end": 3655.3199999999997, "text": " there is some correlation between brain size", "tokens": [50912, 456, 307, 512, 20009, 1296, 3567, 2744, 51018], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1474, "seek": 364224, "start": 3655.3199999999997, "end": 3656.6, "text": " and intelligence.", "tokens": [51018, 293, 7599, 13, 51082], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1475, "seek": 364224, "start": 3656.6, "end": 3660.16, "text": " And we also see within the context of quote unquote,", "tokens": [51082, 400, 321, 611, 536, 1951, 264, 4319, 295, 6513, 37557, 11, 51260], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1476, "seek": 364224, "start": 3660.16, "end": 3661.3599999999997, "text": " scaling that people talk about", "tokens": [51260, 21589, 300, 561, 751, 466, 51320], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1477, "seek": 364224, "start": 3661.3599999999997, "end": 3664.3999999999996, "text": " within the context of LLMs, architectural improvements,", "tokens": [51320, 1951, 264, 4319, 295, 441, 43, 26386, 11, 26621, 13797, 11, 51472], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1478, "seek": 364224, "start": 3664.3999999999996, "end": 3667.9199999999996, "text": " where a model like Gemini 1.5 flash", "tokens": [51472, 689, 257, 2316, 411, 22894, 3812, 502, 13, 20, 7319, 51648], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1479, "seek": 364224, "start": 3667.9199999999996, "end": 3670.72, "text": " is performs as well as GPT-4 did", "tokens": [51648, 307, 26213, 382, 731, 382, 26039, 51, 12, 19, 630, 51788], "temperature": 0.0, "avg_logprob": -0.18318278849625788, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.006288454402238131}, {"id": 1480, "seek": 367072, "start": 3670.72, "end": 3672.56, "text": " when GPT-4 was released a year ago,", "tokens": [50364, 562, 26039, 51, 12, 19, 390, 4736, 257, 1064, 2057, 11, 50456], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1481, "seek": 367072, "start": 3672.56, "end": 3675.3199999999997, "text": " but is 57 times cheaper on output.", "tokens": [50456, 457, 307, 21423, 1413, 12284, 322, 5598, 13, 50594], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1482, "seek": 367072, "start": 3675.3199999999997, "end": 3677.48, "text": " So the part of the scaling story", "tokens": [50594, 407, 264, 644, 295, 264, 21589, 1657, 50702], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1483, "seek": 367072, "start": 3677.48, "end": 3679.24, "text": " is that the architectural improvements are,", "tokens": [50702, 307, 300, 264, 26621, 13797, 366, 11, 50790], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1484, "seek": 367072, "start": 3679.24, "end": 3681.64, "text": " we're in like extremely low hanging fruit territory", "tokens": [50790, 321, 434, 294, 411, 4664, 2295, 8345, 6773, 11360, 50910], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1485, "seek": 367072, "start": 3681.64, "end": 3683.16, "text": " when it comes to those.", "tokens": [50910, 562, 309, 1487, 281, 729, 13, 50986], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1486, "seek": 367072, "start": 3683.16, "end": 3687.56, "text": " Okay, we're back now with the co-founder of Zapier,", "tokens": [50986, 1033, 11, 321, 434, 646, 586, 365, 264, 598, 12, 33348, 295, 34018, 811, 11, 51206], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1487, "seek": 367072, "start": 3687.56, "end": 3691.04, "text": " Mike Canouf, we had to restart a few times there.", "tokens": [51206, 6602, 1664, 263, 69, 11, 321, 632, 281, 21022, 257, 1326, 1413, 456, 13, 51380], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1488, "seek": 367072, "start": 3691.04, "end": 3692.8799999999997, "text": " And you're funding this prize", "tokens": [51380, 400, 291, 434, 6137, 341, 12818, 51472], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1489, "seek": 367072, "start": 3692.8799999999997, "end": 3695.7999999999997, "text": " and you're running this prize with Francois.", "tokens": [51472, 293, 291, 434, 2614, 341, 12818, 365, 34695, 271, 13, 51618], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1490, "seek": 367072, "start": 3695.7999999999997, "end": 3698.7599999999998, "text": " And so tell me about how this came together,", "tokens": [51618, 400, 370, 980, 385, 466, 577, 341, 1361, 1214, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1383062497837337, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0005883249104954302}, {"id": 1491, "seek": 369876, "start": 3699.7200000000003, "end": 3701.76, "text": " what more prompted you guys to launch this prize?", "tokens": [50412, 437, 544, 31042, 291, 1074, 281, 4025, 341, 12818, 30, 50514], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1492, "seek": 369876, "start": 3701.76, "end": 3704.2400000000002, "text": " Yeah, I guess I've been sort of like AI curious", "tokens": [50514, 865, 11, 286, 2041, 286, 600, 668, 1333, 295, 411, 7318, 6369, 50638], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1493, "seek": 369876, "start": 3704.2400000000002, "end": 3706.6800000000003, "text": " for 13 years, I co-founded Zapier,", "tokens": [50638, 337, 3705, 924, 11, 286, 598, 12, 49547, 34018, 811, 11, 50760], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1494, "seek": 369876, "start": 3706.6800000000003, "end": 3708.5200000000004, "text": " been running it for the last 13 years.", "tokens": [50760, 668, 2614, 309, 337, 264, 1036, 3705, 924, 13, 50852], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1495, "seek": 369876, "start": 3708.5200000000004, "end": 3711.2400000000002, "text": " And I think I first got introduced to your work", "tokens": [50852, 400, 286, 519, 286, 700, 658, 7268, 281, 428, 589, 50988], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1496, "seek": 369876, "start": 3711.2400000000002, "end": 3714.5600000000004, "text": " and during COVID, I kind of went down the rabbit hole,", "tokens": [50988, 293, 1830, 4566, 11, 286, 733, 295, 1437, 760, 264, 19509, 5458, 11, 51154], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1497, "seek": 369876, "start": 3714.5600000000004, "end": 3716.32, "text": " we had a lot of free time.", "tokens": [51154, 321, 632, 257, 688, 295, 1737, 565, 13, 51242], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1498, "seek": 369876, "start": 3716.32, "end": 3718.96, "text": " And it was right after you published your", "tokens": [51242, 400, 309, 390, 558, 934, 291, 6572, 428, 51374], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1499, "seek": 369876, "start": 3718.96, "end": 3719.96, "text": " on measure of intelligence paper,", "tokens": [51374, 322, 3481, 295, 7599, 3035, 11, 51424], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1500, "seek": 369876, "start": 3719.96, "end": 3721.84, "text": " you sort of introduced the concept of AGI,", "tokens": [51424, 291, 1333, 295, 7268, 264, 3410, 295, 316, 26252, 11, 51518], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1501, "seek": 369876, "start": 3721.84, "end": 3723.4, "text": " this like efficiency of skill acquisition", "tokens": [51518, 341, 411, 10493, 295, 5389, 21668, 51596], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1502, "seek": 369876, "start": 3723.4, "end": 3726.28, "text": " is like the right definition and the arc puzzles.", "tokens": [51596, 307, 411, 264, 558, 7123, 293, 264, 10346, 24138, 13, 51740], "temperature": 0.0, "avg_logprob": -0.21918191155083747, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0007096061017364264}, {"id": 1503, "seek": 372628, "start": 3726.32, "end": 3729.0400000000004, "text": " But I don't think the first Kaggle contest was done yet.", "tokens": [50366, 583, 286, 500, 380, 519, 264, 700, 48751, 22631, 10287, 390, 1096, 1939, 13, 50502], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1504, "seek": 372628, "start": 3729.0400000000004, "end": 3730.48, "text": " I think it was still running.", "tokens": [50502, 286, 519, 309, 390, 920, 2614, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1505, "seek": 372628, "start": 3730.48, "end": 3732.6000000000004, "text": " And so I kind of, it was interesting,", "tokens": [50574, 400, 370, 286, 733, 295, 11, 309, 390, 1880, 11, 50680], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1506, "seek": 372628, "start": 3732.6000000000004, "end": 3734.88, "text": " but I just parked the idea.", "tokens": [50680, 457, 286, 445, 28491, 264, 1558, 13, 50794], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1507, "seek": 372628, "start": 3734.88, "end": 3736.92, "text": " And my bigger fish to fry at Zapier", "tokens": [50794, 400, 452, 3801, 3506, 281, 13776, 412, 34018, 811, 50896], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1508, "seek": 372628, "start": 3736.92, "end": 3738.48, "text": " were in this middle of this big turnaround", "tokens": [50896, 645, 294, 341, 2808, 295, 341, 955, 46114, 50974], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1509, "seek": 372628, "start": 3738.48, "end": 3741.2400000000002, "text": " of trying to get to our second product.", "tokens": [50974, 295, 1382, 281, 483, 281, 527, 1150, 1674, 13, 51112], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1510, "seek": 372628, "start": 3741.2400000000002, "end": 3744.0400000000004, "text": " And then it was January, 2022,", "tokens": [51112, 400, 550, 309, 390, 7061, 11, 20229, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1511, "seek": 372628, "start": 3744.0400000000004, "end": 3745.6800000000003, "text": " when the chain of thought paper came out", "tokens": [51252, 562, 264, 5021, 295, 1194, 3035, 1361, 484, 51334], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1512, "seek": 372628, "start": 3745.6800000000003, "end": 3748.5600000000004, "text": " that really like awoken me to sort of the progress.", "tokens": [51334, 300, 534, 411, 1714, 8406, 385, 281, 1333, 295, 264, 4205, 13, 51478], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1513, "seek": 372628, "start": 3748.5600000000004, "end": 3750.5600000000004, "text": " I gave a whole presentation to the Zapier", "tokens": [51478, 286, 2729, 257, 1379, 5860, 281, 264, 34018, 811, 51578], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1514, "seek": 372628, "start": 3750.5600000000004, "end": 3751.88, "text": " on like the GPT-3 paper events.", "tokens": [51578, 322, 411, 264, 26039, 51, 12, 18, 3035, 3931, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1515, "seek": 372628, "start": 3751.88, "end": 3753.7200000000003, "text": " I'd sort of felt like I had priced in everything", "tokens": [51644, 286, 1116, 1333, 295, 2762, 411, 286, 632, 30349, 294, 1203, 51736], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1516, "seek": 372628, "start": 3753.7200000000003, "end": 3755.7200000000003, "text": " that Elms could do and that paper was", "tokens": [51736, 300, 2699, 2592, 727, 360, 293, 300, 3035, 390, 51836], "temperature": 0.0, "avg_logprob": -0.14416040693010604, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.0003569542313925922}, {"id": 1517, "seek": 375572, "start": 3755.7599999999998, "end": 3757.3599999999997, "text": " really shocking to me in terms of", "tokens": [50366, 534, 18776, 281, 385, 294, 2115, 295, 50446], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1518, "seek": 375572, "start": 3757.3599999999997, "end": 3759.24, "text": " these latent capabilities that Elms have", "tokens": [50446, 613, 48994, 10862, 300, 2699, 2592, 362, 50540], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1519, "seek": 375572, "start": 3759.24, "end": 3761.52, "text": " that I didn't expect that they had.", "tokens": [50540, 300, 286, 994, 380, 2066, 300, 436, 632, 13, 50654], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1520, "seek": 375572, "start": 3761.52, "end": 3765.6, "text": " And so I actually gave up my exact team role at Zapier.", "tokens": [50654, 400, 370, 286, 767, 2729, 493, 452, 1900, 1469, 3090, 412, 34018, 811, 13, 50858], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1521, "seek": 375572, "start": 3765.6, "end": 3766.72, "text": " I was running half the company at that point", "tokens": [50858, 286, 390, 2614, 1922, 264, 2237, 412, 300, 935, 50914], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1522, "seek": 375572, "start": 3766.72, "end": 3768.7999999999997, "text": " and I went back to be an individual contributor", "tokens": [50914, 293, 286, 1437, 646, 281, 312, 364, 2609, 42859, 51018], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1523, "seek": 375572, "start": 3768.7999999999997, "end": 3770.8799999999997, "text": " and just to go do AI research", "tokens": [51018, 293, 445, 281, 352, 360, 7318, 2132, 51122], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1524, "seek": 375572, "start": 3770.8799999999997, "end": 3772.48, "text": " alongside Brian, my co-founder.", "tokens": [51122, 12385, 10765, 11, 452, 598, 12, 33348, 13, 51202], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1525, "seek": 375572, "start": 3773.9199999999996, "end": 3776.56, "text": " And all of that led me to back towards arc.", "tokens": [51274, 400, 439, 295, 300, 4684, 385, 281, 646, 3030, 10346, 13, 51406], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1526, "seek": 375572, "start": 3776.56, "end": 3777.7999999999997, "text": " I was looking into it again", "tokens": [51406, 286, 390, 1237, 666, 309, 797, 51468], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1527, "seek": 375572, "start": 3777.7999999999997, "end": 3782.12, "text": " and I had sort of expected to see this saturation effect", "tokens": [51468, 293, 286, 632, 1333, 295, 5176, 281, 536, 341, 27090, 1802, 51684], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1528, "seek": 375572, "start": 3782.12, "end": 3785.68, "text": " that MMLU has, that GMSK 8K has.", "tokens": [51684, 300, 376, 12683, 52, 575, 11, 300, 460, 10288, 42, 1649, 42, 575, 13, 51862], "temperature": 0.0, "avg_logprob": -0.17334476415661798, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.0012446203036233783}, {"id": 1529, "seek": 378568, "start": 3785.68, "end": 3787.8399999999997, "text": " And when I looked at the scores and the progress", "tokens": [50364, 400, 562, 286, 2956, 412, 264, 13444, 293, 264, 4205, 50472], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1530, "seek": 378568, "start": 3787.8399999999997, "end": 3791.16, "text": " since the last four years, I was really again, shocked to see", "tokens": [50472, 1670, 264, 1036, 1451, 924, 11, 286, 390, 534, 797, 11, 12763, 281, 536, 50638], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1531, "seek": 378568, "start": 3791.16, "end": 3793.44, "text": " actually we've made very little objective progress", "tokens": [50638, 767, 321, 600, 1027, 588, 707, 10024, 4205, 50752], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1532, "seek": 378568, "start": 3793.44, "end": 3796.64, "text": " towards it and it felt very,", "tokens": [50752, 3030, 309, 293, 309, 2762, 588, 11, 50912], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1533, "seek": 378568, "start": 3796.64, "end": 3798.2799999999997, "text": " it felt like a really, really important Eval.", "tokens": [50912, 309, 2762, 411, 257, 534, 11, 534, 1021, 462, 3337, 13, 50994], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1534, "seek": 378568, "start": 3798.2799999999997, "end": 3799.64, "text": " And as I sort of spent the last year", "tokens": [50994, 400, 382, 286, 1333, 295, 4418, 264, 1036, 1064, 51062], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1535, "seek": 378568, "start": 3799.64, "end": 3801.12, "text": " asking people, quizzing people about it", "tokens": [51062, 3365, 561, 11, 43425, 278, 561, 466, 309, 51136], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1536, "seek": 378568, "start": 3801.12, "end": 3802.8799999999997, "text": " and sort of my network and community,", "tokens": [51136, 293, 1333, 295, 452, 3209, 293, 1768, 11, 51224], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1537, "seek": 378568, "start": 3803.7599999999998, "end": 3806.04, "text": " very few people even knew it existed.", "tokens": [51268, 588, 1326, 561, 754, 2586, 309, 13135, 13, 51382], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1538, "seek": 378568, "start": 3806.04, "end": 3809.0, "text": " And that felt like, okay, if it's right", "tokens": [51382, 400, 300, 2762, 411, 11, 1392, 11, 498, 309, 311, 558, 51530], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1539, "seek": 378568, "start": 3809.0, "end": 3811.44, "text": " that this is a really, really like globally", "tokens": [51530, 300, 341, 307, 257, 534, 11, 534, 411, 18958, 51652], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1540, "seek": 378568, "start": 3811.44, "end": 3814.8399999999997, "text": " singularly unique EGI Eval.", "tokens": [51652, 20010, 356, 3845, 462, 26252, 462, 3337, 13, 51822], "temperature": 0.0, "avg_logprob": -0.18787748796226333, "compression_ratio": 1.7765957446808511, "no_speech_prob": 0.00031500833574682474}, {"id": 1541, "seek": 381484, "start": 3814.84, "end": 3816.44, "text": " And it's different from every other Eval that exists", "tokens": [50364, 400, 309, 311, 819, 490, 633, 661, 462, 3337, 300, 8198, 50444], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1542, "seek": 381484, "start": 3816.44, "end": 3820.2000000000003, "text": " that are more narrowly measures AI skill.", "tokens": [50444, 300, 366, 544, 9432, 356, 8000, 7318, 5389, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1543, "seek": 381484, "start": 3820.2000000000003, "end": 3822.1200000000003, "text": " Like more people should know about this thing.", "tokens": [50632, 1743, 544, 561, 820, 458, 466, 341, 551, 13, 50728], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1544, "seek": 381484, "start": 3822.1200000000003, "end": 3824.4, "text": " I had my own ideas on how to beat the arc as well.", "tokens": [50728, 286, 632, 452, 1065, 3487, 322, 577, 281, 4224, 264, 10346, 382, 731, 13, 50842], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1545, "seek": 381484, "start": 3824.4, "end": 3826.32, "text": " So like I was working on nights and weekends on that", "tokens": [50842, 407, 411, 286, 390, 1364, 322, 13249, 293, 23595, 322, 300, 50938], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1546, "seek": 381484, "start": 3826.32, "end": 3829.6000000000004, "text": " and I flew up to meet Francois earlier this year", "tokens": [50938, 293, 286, 15728, 493, 281, 1677, 34695, 271, 3071, 341, 1064, 51102], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1547, "seek": 381484, "start": 3829.6000000000004, "end": 3831.48, "text": " to sort of quiz him, show him my ideas.", "tokens": [51102, 281, 1333, 295, 15450, 796, 11, 855, 796, 452, 3487, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1548, "seek": 381484, "start": 3831.48, "end": 3834.1600000000003, "text": " And ultimately I was like, well,", "tokens": [51196, 400, 6284, 286, 390, 411, 11, 731, 11, 51330], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1549, "seek": 381484, "start": 3834.1600000000003, "end": 3836.48, "text": " why don't you think more people know about arc?", "tokens": [51330, 983, 500, 380, 291, 519, 544, 561, 458, 466, 10346, 30, 51446], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1550, "seek": 381484, "start": 3836.48, "end": 3837.44, "text": " I think you should actually answer that.", "tokens": [51446, 286, 519, 291, 820, 767, 1867, 300, 13, 51494], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1551, "seek": 381484, "start": 3837.44, "end": 3839.28, "text": " I think it's a really interesting question.", "tokens": [51494, 286, 519, 309, 311, 257, 534, 1880, 1168, 13, 51586], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1552, "seek": 381484, "start": 3839.28, "end": 3841.32, "text": " Like why don't you think more people know about arc?", "tokens": [51586, 1743, 983, 500, 380, 291, 519, 544, 561, 458, 466, 10346, 30, 51688], "temperature": 0.0, "avg_logprob": -0.13453748067220053, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.002182364696636796}, {"id": 1553, "seek": 384132, "start": 3841.32, "end": 3845.0, "text": " Sure, you know, I think benchmarks that gain traction", "tokens": [50364, 4894, 11, 291, 458, 11, 286, 519, 43751, 300, 6052, 23558, 50548], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1554, "seek": 384132, "start": 3845.0, "end": 3846.52, "text": " in the research community are benchmarks", "tokens": [50548, 294, 264, 2132, 1768, 366, 43751, 50624], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1555, "seek": 384132, "start": 3846.52, "end": 3848.6800000000003, "text": " that are already fairly tractable", "tokens": [50624, 300, 366, 1217, 6457, 24207, 712, 50732], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1556, "seek": 384132, "start": 3848.6800000000003, "end": 3851.6000000000004, "text": " because the dynamic that you see is that some research group", "tokens": [50732, 570, 264, 8546, 300, 291, 536, 307, 300, 512, 2132, 1594, 50878], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1557, "seek": 384132, "start": 3851.6000000000004, "end": 3853.7200000000003, "text": " is gonna make some initial breakthrough", "tokens": [50878, 307, 799, 652, 512, 5883, 22397, 50984], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1558, "seek": 384132, "start": 3853.7200000000003, "end": 3856.76, "text": " and then this is gonna catch the attention of everyone else.", "tokens": [50984, 293, 550, 341, 307, 799, 3745, 264, 3202, 295, 1518, 1646, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1559, "seek": 384132, "start": 3856.76, "end": 3858.48, "text": " And so you're gonna get follow-up papers", "tokens": [51136, 400, 370, 291, 434, 799, 483, 1524, 12, 1010, 10577, 51222], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1560, "seek": 384132, "start": 3858.48, "end": 3862.2000000000003, "text": " with people trying to beat the first team and so on.", "tokens": [51222, 365, 561, 1382, 281, 4224, 264, 700, 1469, 293, 370, 322, 13, 51408], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1561, "seek": 384132, "start": 3862.2000000000003, "end": 3864.52, "text": " And for arc, this has not really happened", "tokens": [51408, 400, 337, 10346, 11, 341, 575, 406, 534, 2011, 51524], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1562, "seek": 384132, "start": 3864.52, "end": 3866.36, "text": " because arc is actually very hard", "tokens": [51524, 570, 10346, 307, 767, 588, 1152, 51616], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1563, "seek": 384132, "start": 3866.36, "end": 3867.8, "text": " for existing AI techniques.", "tokens": [51616, 337, 6741, 7318, 7512, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1564, "seek": 384132, "start": 3867.8, "end": 3870.92, "text": " Kind of arc requires you to try new ideas.", "tokens": [51688, 9242, 295, 10346, 7029, 291, 281, 853, 777, 3487, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1373921614426833, "compression_ratio": 1.740983606557377, "no_speech_prob": 0.0014945283764973283}, {"id": 1565, "seek": 387092, "start": 3871.0, "end": 3873.28, "text": " And that's very much the point, by the way.", "tokens": [50368, 400, 300, 311, 588, 709, 264, 935, 11, 538, 264, 636, 13, 50482], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1566, "seek": 387092, "start": 3873.28, "end": 3875.28, "text": " Like the point is not that, yeah,", "tokens": [50482, 1743, 264, 935, 307, 406, 300, 11, 1338, 11, 50582], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1567, "seek": 387092, "start": 3875.28, "end": 3877.64, "text": " you should just be able to apply existing technology", "tokens": [50582, 291, 820, 445, 312, 1075, 281, 3079, 6741, 2899, 50700], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1568, "seek": 387092, "start": 3877.64, "end": 3878.48, "text": " and solve arc.", "tokens": [50700, 293, 5039, 10346, 13, 50742], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1569, "seek": 387092, "start": 3878.48, "end": 3882.92, "text": " The point is that existing technology has reached a plateau", "tokens": [50742, 440, 935, 307, 300, 6741, 2899, 575, 6488, 257, 39885, 50964], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1570, "seek": 387092, "start": 3882.92, "end": 3884.8, "text": " and if you want to go beyond that,", "tokens": [50964, 293, 498, 291, 528, 281, 352, 4399, 300, 11, 51058], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1571, "seek": 387092, "start": 3884.8, "end": 3887.76, "text": " if you want to start being able to tackle problems", "tokens": [51058, 498, 291, 528, 281, 722, 885, 1075, 281, 14896, 2740, 51206], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1572, "seek": 387092, "start": 3887.76, "end": 3890.8, "text": " that you haven't memorized, that you haven't seen before,", "tokens": [51206, 300, 291, 2378, 380, 46677, 11, 300, 291, 2378, 380, 1612, 949, 11, 51358], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1573, "seek": 387092, "start": 3890.8, "end": 3892.44, "text": " you need to try new ideas.", "tokens": [51358, 291, 643, 281, 853, 777, 3487, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1574, "seek": 387092, "start": 3892.44, "end": 3897.44, "text": " And arc is not just meant to be this sort of like measure", "tokens": [51440, 400, 10346, 307, 406, 445, 4140, 281, 312, 341, 1333, 295, 411, 3481, 51690], "temperature": 0.0, "avg_logprob": -0.11393243175441936, "compression_ratio": 1.8008298755186722, "no_speech_prob": 0.0003162616631016135}, {"id": 1575, "seek": 389744, "start": 3898.44, "end": 3901.0, "text": " of how close we are to a GI.", "tokens": [50414, 295, 577, 1998, 321, 366, 281, 257, 26634, 13, 50542], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1576, "seek": 389744, "start": 3901.0, "end": 3904.36, "text": " It's also meant to be a source of inspiration.", "tokens": [50542, 467, 311, 611, 4140, 281, 312, 257, 4009, 295, 10249, 13, 50710], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1577, "seek": 389744, "start": 3904.36, "end": 3906.68, "text": " Like I want researchers to look at these puzzles", "tokens": [50710, 1743, 286, 528, 10309, 281, 574, 412, 613, 24138, 50826], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1578, "seek": 389744, "start": 3906.68, "end": 3909.08, "text": " and be like, hey, it's really strange", "tokens": [50826, 293, 312, 411, 11, 4177, 11, 309, 311, 534, 5861, 50946], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1579, "seek": 389744, "start": 3909.08, "end": 3911.56, "text": " that these puzzles are so simple", "tokens": [50946, 300, 613, 24138, 366, 370, 2199, 51070], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1580, "seek": 389744, "start": 3911.56, "end": 3915.32, "text": " and most humans can just do them very quickly.", "tokens": [51070, 293, 881, 6255, 393, 445, 360, 552, 588, 2661, 13, 51258], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1581, "seek": 389744, "start": 3915.32, "end": 3918.4, "text": " Why is it so hard for existing AI systems?", "tokens": [51258, 1545, 307, 309, 370, 1152, 337, 6741, 7318, 3652, 30, 51412], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1582, "seek": 389744, "start": 3918.4, "end": 3920.8, "text": " Why is it so hard for LLMs and so on?", "tokens": [51412, 1545, 307, 309, 370, 1152, 337, 441, 43, 26386, 293, 370, 322, 30, 51532], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1583, "seek": 389744, "start": 3920.8, "end": 3923.48, "text": " And it's true for LLMs, but arc was actually released", "tokens": [51532, 400, 309, 311, 2074, 337, 441, 43, 26386, 11, 457, 10346, 390, 767, 4736, 51666], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1584, "seek": 389744, "start": 3923.48, "end": 3925.48, "text": " before LLMs were really a thing.", "tokens": [51666, 949, 441, 43, 26386, 645, 534, 257, 551, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14583412418520547, "compression_ratio": 1.64, "no_speech_prob": 0.0005184804904274642}, {"id": 1585, "seek": 392548, "start": 3925.52, "end": 3928.88, "text": " And the only thing that made it special at the time", "tokens": [50366, 400, 264, 787, 551, 300, 1027, 309, 2121, 412, 264, 565, 50534], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1586, "seek": 392548, "start": 3928.88, "end": 3932.2400000000002, "text": " was that it was designed to be a resistance to memorization.", "tokens": [50534, 390, 300, 309, 390, 4761, 281, 312, 257, 7335, 281, 10560, 2144, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1587, "seek": 392548, "start": 3932.2400000000002, "end": 3934.8, "text": " And the fact that it has survived LLMs", "tokens": [50702, 400, 264, 1186, 300, 309, 575, 14433, 441, 43, 26386, 50830], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1588, "seek": 392548, "start": 3934.8, "end": 3937.48, "text": " and Genia in general so well,", "tokens": [50830, 293, 3632, 654, 294, 2674, 370, 731, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1589, "seek": 392548, "start": 3937.48, "end": 3938.68, "text": " kind of shows that yes,", "tokens": [50964, 733, 295, 3110, 300, 2086, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1590, "seek": 392548, "start": 3938.68, "end": 3940.8, "text": " it is actually resistant to memorization.", "tokens": [51024, 309, 307, 767, 20383, 281, 10560, 2144, 13, 51130], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1591, "seek": 392548, "start": 3940.8, "end": 3942.28, "text": " This is what nerds night me", "tokens": [51130, 639, 307, 437, 23229, 82, 1818, 385, 51204], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1592, "seek": 392548, "start": 3942.28, "end": 3944.2, "text": " because I went and took a bunch of the puzzles myself.", "tokens": [51204, 570, 286, 1437, 293, 1890, 257, 3840, 295, 264, 24138, 2059, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1593, "seek": 392548, "start": 3944.2, "end": 3945.72, "text": " I've showed it to all my friends and family too", "tokens": [51300, 286, 600, 4712, 309, 281, 439, 452, 1855, 293, 1605, 886, 51376], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1594, "seek": 392548, "start": 3945.72, "end": 3948.56, "text": " and they're all like, oh yeah, this is like super easy.", "tokens": [51376, 293, 436, 434, 439, 411, 11, 1954, 1338, 11, 341, 307, 411, 1687, 1858, 13, 51518], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1595, "seek": 392548, "start": 3949.48, "end": 3951.2400000000002, "text": " Are you sure AI can't solve this?", "tokens": [51564, 2014, 291, 988, 7318, 393, 380, 5039, 341, 30, 51652], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1596, "seek": 392548, "start": 3951.2400000000002, "end": 3954.16, "text": " Like that's the reaction in the same one for me as well.", "tokens": [51652, 1743, 300, 311, 264, 5480, 294, 264, 912, 472, 337, 385, 382, 731, 13, 51798], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1597, "seek": 392548, "start": 3954.16, "end": 3955.44, "text": " And the more you dig in, you're like, okay,", "tokens": [51798, 400, 264, 544, 291, 2528, 294, 11, 291, 434, 411, 11, 1392, 11, 51862], "temperature": 0.0, "avg_logprob": -0.1510288585316051, "compression_ratio": 1.7190332326283988, "no_speech_prob": 7.965756230987608e-05}, {"id": 1598, "seek": 395544, "start": 3955.44, "end": 3957.32, "text": " yep, there's not just empirical evidence", "tokens": [50364, 18633, 11, 456, 311, 406, 445, 31886, 4467, 50458], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1599, "seek": 395544, "start": 3957.32, "end": 3958.7200000000003, "text": " over the last four years that it's unbeaten,", "tokens": [50458, 670, 264, 1036, 1451, 924, 300, 309, 311, 517, 4169, 268, 11, 50528], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1600, "seek": 395544, "start": 3958.7200000000003, "end": 3961.8, "text": " but there's theoretical like concepts behind why.", "tokens": [50528, 457, 456, 311, 20864, 411, 10392, 2261, 983, 13, 50682], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1601, "seek": 395544, "start": 3962.64, "end": 3964.2000000000003, "text": " And I completely agree at this point", "tokens": [50724, 400, 286, 2584, 3986, 412, 341, 935, 50802], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1602, "seek": 395544, "start": 3964.2000000000003, "end": 3966.2400000000002, "text": " that like new ideas basically are needed to be dark.", "tokens": [50802, 300, 411, 777, 3487, 1936, 366, 2978, 281, 312, 2877, 13, 50904], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1603, "seek": 395544, "start": 3966.2400000000002, "end": 3968.0, "text": " And there's a lot of current trends in the world", "tokens": [50904, 400, 456, 311, 257, 688, 295, 2190, 13892, 294, 264, 1002, 50992], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1604, "seek": 395544, "start": 3968.0, "end": 3969.2400000000002, "text": " that are actually, I think,", "tokens": [50992, 300, 366, 767, 11, 286, 519, 11, 51054], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1605, "seek": 395544, "start": 3969.2400000000002, "end": 3972.28, "text": " working against that happening basically.", "tokens": [51054, 1364, 1970, 300, 2737, 1936, 13, 51206], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1606, "seek": 395544, "start": 3972.28, "end": 3973.32, "text": " I think we're actually less likely", "tokens": [51206, 286, 519, 321, 434, 767, 1570, 3700, 51258], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1607, "seek": 395544, "start": 3973.32, "end": 3974.88, "text": " to generate new ideas right now.", "tokens": [51258, 281, 8460, 777, 3487, 558, 586, 13, 51336], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1608, "seek": 395544, "start": 3975.84, "end": 3977.68, "text": " You know, I think one of the kind of trends", "tokens": [51384, 509, 458, 11, 286, 519, 472, 295, 264, 733, 295, 13892, 51476], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1609, "seek": 395544, "start": 3977.68, "end": 3979.48, "text": " is the closing up frontier research, right?", "tokens": [51476, 307, 264, 10377, 493, 35853, 2132, 11, 558, 30, 51566], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1610, "seek": 395544, "start": 3979.48, "end": 3982.7200000000003, "text": " The GP4 paper from Open AI had no technical details shared.", "tokens": [51566, 440, 26039, 19, 3035, 490, 7238, 7318, 632, 572, 6191, 4365, 5507, 13, 51728], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1611, "seek": 395544, "start": 3982.7200000000003, "end": 3984.48, "text": " The Gemini paper had no technical details shared", "tokens": [51728, 440, 22894, 3812, 3035, 632, 572, 6191, 4365, 5507, 51816], "temperature": 0.0, "avg_logprob": -0.149545159309533, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.001169259543530643}, {"id": 1612, "seek": 398448, "start": 3984.52, "end": 3987.16, "text": " and like the longer context part of that work.", "tokens": [50366, 293, 411, 264, 2854, 4319, 644, 295, 300, 589, 13, 50498], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1613, "seek": 398448, "start": 3987.16, "end": 3990.2, "text": " And yet that open innovation and open progress and sharing", "tokens": [50498, 400, 1939, 300, 1269, 8504, 293, 1269, 4205, 293, 5414, 50650], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1614, "seek": 398448, "start": 3990.2, "end": 3992.12, "text": " is what got us to transformers in the first place.", "tokens": [50650, 307, 437, 658, 505, 281, 4088, 433, 294, 264, 700, 1081, 13, 50746], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1615, "seek": 398448, "start": 3992.12, "end": 3995.12, "text": " That's what got us to LMS in the first place.", "tokens": [50746, 663, 311, 437, 658, 505, 281, 441, 10288, 294, 264, 700, 1081, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1616, "seek": 398448, "start": 3995.12, "end": 3997.92, "text": " So it's kind of disappointing a little bit actually", "tokens": [50896, 407, 309, 311, 733, 295, 25054, 257, 707, 857, 767, 51036], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1617, "seek": 398448, "start": 3997.92, "end": 4000.0, "text": " that like so much frontier work has gone closed.", "tokens": [51036, 300, 411, 370, 709, 35853, 589, 575, 2780, 5395, 13, 51140], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1618, "seek": 398448, "start": 4000.0, "end": 4002.56, "text": " It's really making a bet that like these individual labs", "tokens": [51140, 467, 311, 534, 1455, 257, 778, 300, 411, 613, 2609, 20339, 51268], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1619, "seek": 398448, "start": 4002.56, "end": 4003.76, "text": " are going to have the breakthrough", "tokens": [51268, 366, 516, 281, 362, 264, 22397, 51328], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1620, "seek": 398448, "start": 4003.76, "end": 4006.2400000000002, "text": " and not the ecosystem is going to have the breakthrough.", "tokens": [51328, 293, 406, 264, 11311, 307, 516, 281, 362, 264, 22397, 13, 51452], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1621, "seek": 398448, "start": 4006.2400000000002, "end": 4008.36, "text": " And I think sort of the internet open source has shown", "tokens": [51452, 400, 286, 519, 1333, 295, 264, 4705, 1269, 4009, 575, 4898, 51558], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1622, "seek": 398448, "start": 4008.36, "end": 4010.36, "text": " that that's like the most powerful innovation ecosystem", "tokens": [51558, 300, 300, 311, 411, 264, 881, 4005, 8504, 11311, 51658], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1623, "seek": 398448, "start": 4010.36, "end": 4012.48, "text": " that's ever existed probably in the entire world.", "tokens": [51658, 300, 311, 1562, 13135, 1391, 294, 264, 2302, 1002, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13156660451184984, "compression_ratio": 1.9096573208722742, "no_speech_prob": 0.0007793100667186081}, {"id": 1624, "seek": 401248, "start": 4012.48, "end": 4014.08, "text": " I think that's actually really sad", "tokens": [50364, 286, 519, 300, 311, 767, 534, 4227, 50444], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1625, "seek": 401248, "start": 4014.08, "end": 4017.72, "text": " that frontier research is no longer being published.", "tokens": [50444, 300, 35853, 2132, 307, 572, 2854, 885, 6572, 13, 50626], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1626, "seek": 401248, "start": 4017.72, "end": 4019.96, "text": " If you look back, you know, four years ago,", "tokens": [50626, 759, 291, 574, 646, 11, 291, 458, 11, 1451, 924, 2057, 11, 50738], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1627, "seek": 401248, "start": 4021.28, "end": 4023.08, "text": " well, everything was just openly shared", "tokens": [50804, 731, 11, 1203, 390, 445, 23109, 5507, 50894], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1628, "seek": 401248, "start": 4023.08, "end": 4025.88, "text": " like all the state of the art results were published", "tokens": [50894, 411, 439, 264, 1785, 295, 264, 1523, 3542, 645, 6572, 51034], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1629, "seek": 401248, "start": 4025.88, "end": 4027.16, "text": " and this is no longer the case.", "tokens": [51034, 293, 341, 307, 572, 2854, 264, 1389, 13, 51098], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1630, "seek": 401248, "start": 4027.16, "end": 4028.4, "text": " And it's very much, you know,", "tokens": [51098, 400, 309, 311, 588, 709, 11, 291, 458, 11, 51160], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1631, "seek": 401248, "start": 4028.4, "end": 4031.4, "text": " Open AI single-handedly changed the game.", "tokens": [51160, 7238, 7318, 2167, 12, 5543, 13516, 3105, 264, 1216, 13, 51310], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1632, "seek": 401248, "start": 4031.4, "end": 4036.4, "text": " And I think Open AI basically set back progress towards HGI", "tokens": [51310, 400, 286, 519, 7238, 7318, 1936, 992, 646, 4205, 3030, 389, 26252, 51560], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1633, "seek": 401248, "start": 4037.56, "end": 4040.16, "text": " by quite a few years, probably like five to 10 years", "tokens": [51618, 538, 1596, 257, 1326, 924, 11, 1391, 411, 1732, 281, 1266, 924, 51748], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1634, "seek": 401248, "start": 4040.16, "end": 4041.0, "text": " for two reasons.", "tokens": [51748, 337, 732, 4112, 13, 51790], "temperature": 0.0, "avg_logprob": -0.15170792610414566, "compression_ratio": 1.6594202898550725, "no_speech_prob": 0.00024095852859318256}, {"id": 1635, "seek": 404100, "start": 4041.0, "end": 4045.64, "text": " And one is that, well, they cause this complete closing down", "tokens": [50364, 400, 472, 307, 300, 11, 731, 11, 436, 3082, 341, 3566, 10377, 760, 50596], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1636, "seek": 404100, "start": 4045.64, "end": 4048.2, "text": " of research, frontier research publishing,", "tokens": [50596, 295, 2132, 11, 35853, 2132, 17832, 11, 50724], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1637, "seek": 404100, "start": 4048.2, "end": 4053.2, "text": " but also they trigger this initial burst of hype", "tokens": [50724, 457, 611, 436, 7875, 341, 5883, 12712, 295, 24144, 50974], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1638, "seek": 404100, "start": 4054.48, "end": 4055.52, "text": " around LLMS.", "tokens": [51038, 926, 441, 43, 10288, 13, 51090], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1639, "seek": 404100, "start": 4055.52, "end": 4059.64, "text": " And now LLMS have sucked the oxygen out of the room", "tokens": [51090, 400, 586, 441, 43, 10288, 362, 26503, 264, 9169, 484, 295, 264, 1808, 51296], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1640, "seek": 404100, "start": 4059.64, "end": 4063.32, "text": " like everything, everyone is just doing LLMS.", "tokens": [51296, 411, 1203, 11, 1518, 307, 445, 884, 441, 43, 10288, 13, 51480], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1641, "seek": 404100, "start": 4063.32, "end": 4067.08, "text": " And I see LLMS as a more often off-ramp", "tokens": [51480, 400, 286, 536, 441, 43, 10288, 382, 257, 544, 2049, 766, 12, 81, 1215, 51668], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1642, "seek": 404100, "start": 4067.08, "end": 4069.84, "text": " on the path to HGI actually.", "tokens": [51668, 322, 264, 3100, 281, 389, 26252, 767, 13, 51806], "temperature": 0.0, "avg_logprob": -0.20112500871930802, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.00020575481175910681}, {"id": 1643, "seek": 406984, "start": 4069.88, "end": 4071.96, "text": " And all these new resources,", "tokens": [50366, 400, 439, 613, 777, 3593, 11, 50470], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1644, "seek": 406984, "start": 4071.96, "end": 4074.1600000000003, "text": " they're actually going to LLMS instead", "tokens": [50470, 436, 434, 767, 516, 281, 441, 43, 10288, 2602, 50580], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1645, "seek": 406984, "start": 4074.1600000000003, "end": 4076.92, "text": " of everything else they could be going to.", "tokens": [50580, 295, 1203, 1646, 436, 727, 312, 516, 281, 13, 50718], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1646, "seek": 406984, "start": 4076.92, "end": 4079.7200000000003, "text": " And, you know, if you look further into the past", "tokens": [50718, 400, 11, 291, 458, 11, 498, 291, 574, 3052, 666, 264, 1791, 50858], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1647, "seek": 406984, "start": 4079.7200000000003, "end": 4082.76, "text": " to like 2015, 2016,", "tokens": [50858, 281, 411, 7546, 11, 6549, 11, 51010], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1648, "seek": 406984, "start": 4082.76, "end": 4085.52, "text": " there were like a thousand times fewer people", "tokens": [51010, 456, 645, 411, 257, 4714, 1413, 13366, 561, 51148], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1649, "seek": 406984, "start": 4085.52, "end": 4087.28, "text": " doing AI back then.", "tokens": [51148, 884, 7318, 646, 550, 13, 51236], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1650, "seek": 406984, "start": 4087.28, "end": 4090.7400000000002, "text": " And yet I feel like the rate of progress was higher", "tokens": [51236, 400, 1939, 286, 841, 411, 264, 3314, 295, 4205, 390, 2946, 51409], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1651, "seek": 406984, "start": 4090.7400000000002, "end": 4094.48, "text": " because people were exploring more directions.", "tokens": [51409, 570, 561, 645, 12736, 544, 11095, 13, 51596], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1652, "seek": 406984, "start": 4094.48, "end": 4096.400000000001, "text": " The world felt more open-ended.", "tokens": [51596, 440, 1002, 2762, 544, 1269, 12, 3502, 13, 51692], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1653, "seek": 406984, "start": 4096.400000000001, "end": 4098.400000000001, "text": " Like you could just go and try,", "tokens": [51692, 1743, 291, 727, 445, 352, 293, 853, 11, 51792], "temperature": 0.0, "avg_logprob": -0.13484620625993846, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0002706995001062751}, {"id": 1654, "seek": 409840, "start": 4098.4, "end": 4100.4, "text": " like have a cool idea of a launch", "tokens": [50364, 411, 362, 257, 1627, 1558, 295, 257, 4025, 50464], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1655, "seek": 409840, "start": 4100.4, "end": 4102.36, "text": " and try it and get some interesting results.", "tokens": [50464, 293, 853, 309, 293, 483, 512, 1880, 3542, 13, 50562], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1656, "seek": 409840, "start": 4102.36, "end": 4104.44, "text": " So there was this energy.", "tokens": [50562, 407, 456, 390, 341, 2281, 13, 50666], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1657, "seek": 409840, "start": 4104.44, "end": 4107.44, "text": " And now everyone is very much doing some variation", "tokens": [50666, 400, 586, 1518, 307, 588, 709, 884, 512, 12990, 50816], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1658, "seek": 409840, "start": 4107.44, "end": 4108.799999999999, "text": " of the same thing.", "tokens": [50816, 295, 264, 912, 551, 13, 50884], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1659, "seek": 409840, "start": 4108.799999999999, "end": 4112.719999999999, "text": " And the big labs also tried their hand on arc,", "tokens": [50884, 400, 264, 955, 20339, 611, 3031, 641, 1011, 322, 10346, 11, 51080], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1660, "seek": 409840, "start": 4112.719999999999, "end": 4114.5599999999995, "text": " but because they got bad results,", "tokens": [51080, 457, 570, 436, 658, 1578, 3542, 11, 51172], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1661, "seek": 409840, "start": 4114.5599999999995, "end": 4115.839999999999, "text": " they didn't publish anything.", "tokens": [51172, 436, 994, 380, 11374, 1340, 13, 51236], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1662, "seek": 409840, "start": 4115.839999999999, "end": 4119.5199999999995, "text": " Like, you know, people only publish positive results.", "tokens": [51236, 1743, 11, 291, 458, 11, 561, 787, 11374, 3353, 3542, 13, 51420], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1663, "seek": 409840, "start": 4119.5199999999995, "end": 4123.839999999999, "text": " I wonder how much effort people have put into", "tokens": [51420, 286, 2441, 577, 709, 4630, 561, 362, 829, 666, 51636], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1664, "seek": 409840, "start": 4123.839999999999, "end": 4126.4, "text": " trying to prompt or scaffold,", "tokens": [51636, 1382, 281, 12391, 420, 44094, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14725771036233987, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.00020978625980205834}, {"id": 1665, "seek": 412640, "start": 4126.4, "end": 4128.92, "text": " do some sort of maybe Devon type approach", "tokens": [50364, 360, 512, 1333, 295, 1310, 9096, 266, 2010, 3109, 50490], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1666, "seek": 412640, "start": 4128.92, "end": 4132.28, "text": " into getting the frontier models", "tokens": [50490, 666, 1242, 264, 35853, 5245, 50658], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1667, "seek": 412640, "start": 4132.28, "end": 4134.28, "text": " and the frontier models of today, not just a year ago,", "tokens": [50658, 293, 264, 35853, 5245, 295, 965, 11, 406, 445, 257, 1064, 2057, 11, 50758], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1668, "seek": 412640, "start": 4134.28, "end": 4135.44, "text": " because a lot of post-training", "tokens": [50758, 570, 257, 688, 295, 2183, 12, 17227, 1760, 50816], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1669, "seek": 412640, "start": 4135.44, "end": 4137.12, "text": " has gone into making them better.", "tokens": [50816, 575, 2780, 666, 1455, 552, 1101, 13, 50900], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1670, "seek": 412640, "start": 4137.12, "end": 4140.0, "text": " So Claude Friropas or GPT-40", "tokens": [50900, 407, 12947, 2303, 479, 470, 1513, 296, 420, 26039, 51, 12, 5254, 51044], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1671, "seek": 412640, "start": 4140.0, "end": 4142.879999999999, "text": " into getting good solutions on arc.", "tokens": [51044, 666, 1242, 665, 6547, 322, 10346, 13, 51188], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1672, "seek": 412640, "start": 4144.759999999999, "end": 4146.759999999999, "text": " I hope that one of the things this episode does", "tokens": [51282, 286, 1454, 300, 472, 295, 264, 721, 341, 3500, 775, 51382], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1673, "seek": 412640, "start": 4146.759999999999, "end": 4149.2, "text": " is get people to try out this open competition", "tokens": [51382, 307, 483, 561, 281, 853, 484, 341, 1269, 6211, 51504], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1674, "seek": 412640, "start": 4149.2, "end": 4152.92, "text": " where they have to put in an open source model to compete.", "tokens": [51504, 689, 436, 362, 281, 829, 294, 364, 1269, 4009, 2316, 281, 11831, 13, 51690], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1675, "seek": 412640, "start": 4152.92, "end": 4154.879999999999, "text": " But also to like figure out if they're,", "tokens": [51690, 583, 611, 281, 411, 2573, 484, 498, 436, 434, 11, 51788], "temperature": 0.0, "avg_logprob": -0.19657588568259413, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.008059616200625896}, {"id": 1676, "seek": 415488, "start": 4154.88, "end": 4157.88, "text": " maybe the like capability is latent in Claude Opus", "tokens": [50364, 1310, 264, 411, 13759, 307, 48994, 294, 12947, 2303, 12011, 301, 50514], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1677, "seek": 415488, "start": 4157.88, "end": 4160.24, "text": " and just see if you can show that.", "tokens": [50514, 293, 445, 536, 498, 291, 393, 855, 300, 13, 50632], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1678, "seek": 415488, "start": 4160.24, "end": 4161.92, "text": " I think that would be super interesting.", "tokens": [50632, 286, 519, 300, 576, 312, 1687, 1880, 13, 50716], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1679, "seek": 415488, "start": 4161.92, "end": 4163.24, "text": " So let's talk about the prize.", "tokens": [50716, 407, 718, 311, 751, 466, 264, 12818, 13, 50782], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1680, "seek": 415488, "start": 4163.24, "end": 4165.76, "text": " How much do you win if you solve it?", "tokens": [50782, 1012, 709, 360, 291, 1942, 498, 291, 5039, 309, 30, 50908], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1681, "seek": 415488, "start": 4165.76, "end": 4167.92, "text": " You know, get whatever percent on arc.", "tokens": [50908, 509, 458, 11, 483, 2035, 3043, 322, 10346, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1682, "seek": 415488, "start": 4167.92, "end": 4169.68, "text": " How much do you get if you get the best of vision,", "tokens": [51016, 1012, 709, 360, 291, 483, 498, 291, 483, 264, 1151, 295, 5201, 11, 51104], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1683, "seek": 415488, "start": 4169.68, "end": 4170.6, "text": " but don't crack it?", "tokens": [51104, 457, 500, 380, 6226, 309, 30, 51150], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1684, "seek": 415488, "start": 4170.6, "end": 4171.6, "text": " So we got a million dollar,", "tokens": [51150, 407, 321, 658, 257, 2459, 7241, 11, 51200], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1685, "seek": 415488, "start": 4171.6, "end": 4172.6, "text": " actually a little over a million dollars", "tokens": [51200, 767, 257, 707, 670, 257, 2459, 3808, 51250], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1686, "seek": 415488, "start": 4172.6, "end": 4173.64, "text": " is the price pool.", "tokens": [51250, 307, 264, 3218, 7005, 13, 51302], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1687, "seek": 415488, "start": 4173.64, "end": 4175.88, "text": " We're running the contest on an annual basis.", "tokens": [51302, 492, 434, 2614, 264, 10287, 322, 364, 9784, 5143, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1688, "seek": 415488, "start": 4175.88, "end": 4177.52, "text": " We're gonna, we're starting it today", "tokens": [51414, 492, 434, 799, 11, 321, 434, 2891, 309, 965, 51496], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1689, "seek": 415488, "start": 4177.52, "end": 4179.8, "text": " through the middle of November.", "tokens": [51496, 807, 264, 2808, 295, 7674, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1690, "seek": 415488, "start": 4179.8, "end": 4181.84, "text": " And the goal is to get 85%.", "tokens": [51610, 400, 264, 3387, 307, 281, 483, 14695, 6856, 51712], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1691, "seek": 415488, "start": 4181.84, "end": 4183.32, "text": " That's the lower bound and human average", "tokens": [51712, 663, 311, 264, 3126, 5472, 293, 1952, 4274, 51786], "temperature": 0.0, "avg_logprob": -0.16772801535470144, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003682756796479225}, {"id": 1692, "seek": 418332, "start": 4183.32, "end": 4184.92, "text": " that you guys talked about earlier.", "tokens": [50364, 300, 291, 1074, 2825, 466, 3071, 13, 50444], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1693, "seek": 418332, "start": 4184.92, "end": 4188.08, "text": " And there's a $500,000 prize for the first team", "tokens": [50444, 400, 456, 311, 257, 1848, 7526, 11, 1360, 12818, 337, 264, 700, 1469, 50602], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1694, "seek": 418332, "start": 4188.08, "end": 4190.5599999999995, "text": " that can get to the 85% benchmark.", "tokens": [50602, 300, 393, 483, 281, 264, 14695, 4, 18927, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1695, "seek": 418332, "start": 4190.5599999999995, "end": 4191.719999999999, "text": " We're also gonna run,", "tokens": [50726, 492, 434, 611, 799, 1190, 11, 50784], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1696, "seek": 418332, "start": 4191.719999999999, "end": 4194.0, "text": " we don't expect that to happen this year actually.", "tokens": [50784, 321, 500, 380, 2066, 300, 281, 1051, 341, 1064, 767, 13, 50898], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1697, "seek": 418332, "start": 4194.0, "end": 4197.04, "text": " One of the early statisticians that's up here", "tokens": [50898, 1485, 295, 264, 2440, 29588, 2567, 300, 311, 493, 510, 51050], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1698, "seek": 418332, "start": 4197.04, "end": 4199.36, "text": " giving this line that has always stuck with me", "tokens": [51050, 2902, 341, 1622, 300, 575, 1009, 5541, 365, 385, 51166], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1699, "seek": 418332, "start": 4199.36, "end": 4201.28, "text": " that the longer it takes, the longer it takes.", "tokens": [51166, 300, 264, 2854, 309, 2516, 11, 264, 2854, 309, 2516, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1700, "seek": 418332, "start": 4201.28, "end": 4204.599999999999, "text": " So my prior is that like arc is gonna take years to solve.", "tokens": [51262, 407, 452, 4059, 307, 300, 411, 10346, 307, 799, 747, 924, 281, 5039, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1701, "seek": 418332, "start": 4205.36, "end": 4206.24, "text": " And so we're gonna keep to,", "tokens": [51466, 400, 370, 321, 434, 799, 1066, 281, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1702, "seek": 418332, "start": 4206.24, "end": 4208.84, "text": " we're also gonna break down and do a progress price this year.", "tokens": [51510, 321, 434, 611, 799, 1821, 760, 293, 360, 257, 4205, 3218, 341, 1064, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1703, "seek": 418332, "start": 4208.84, "end": 4210.96, "text": " So there's a $100,000 progress price,", "tokens": [51640, 407, 456, 311, 257, 1848, 6879, 11, 1360, 4205, 3218, 11, 51746], "temperature": 0.0, "avg_logprob": -0.1516615976423225, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0010648792376741767}, {"id": 1704, "seek": 421096, "start": 4210.96, "end": 4213.76, "text": " which we will pay out to the top scores.", "tokens": [50364, 597, 321, 486, 1689, 484, 281, 264, 1192, 13444, 13, 50504], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1705, "seek": 421096, "start": 4213.76, "end": 4218.64, "text": " So $50,000 is gonna go to the top objective scores this year", "tokens": [50504, 407, 1848, 2803, 11, 1360, 307, 799, 352, 281, 264, 1192, 10024, 13444, 341, 1064, 50748], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1706, "seek": 421096, "start": 4218.64, "end": 4219.6, "text": " on the Kaggle leaderboard,", "tokens": [50748, 322, 264, 48751, 22631, 5263, 3787, 11, 50796], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1707, "seek": 421096, "start": 4219.6, "end": 4221.32, "text": " which we're hosting it on Kaggle.", "tokens": [50796, 597, 321, 434, 16058, 309, 322, 48751, 22631, 13, 50882], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1708, "seek": 421096, "start": 4221.32, "end": 4223.16, "text": " And then we're gonna have a $50,000 pot set", "tokens": [50882, 400, 550, 321, 434, 799, 362, 257, 1848, 2803, 11, 1360, 1847, 992, 50974], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1709, "seek": 421096, "start": 4223.16, "end": 4226.2, "text": " for a paper award for the best paper", "tokens": [50974, 337, 257, 3035, 7130, 337, 264, 1151, 3035, 51126], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1710, "seek": 421096, "start": 4226.2, "end": 4228.72, "text": " that explains conceptually the scores", "tokens": [51126, 300, 13948, 3410, 671, 264, 13444, 51252], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1711, "seek": 421096, "start": 4228.72, "end": 4230.16, "text": " that they were able to achieve.", "tokens": [51252, 300, 436, 645, 1075, 281, 4584, 13, 51324], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1712, "seek": 421096, "start": 4230.16, "end": 4231.6, "text": " And one of the I think interesting things", "tokens": [51324, 400, 472, 295, 264, 286, 519, 1880, 721, 51396], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1713, "seek": 421096, "start": 4231.6, "end": 4233.68, "text": " we're also gonna be doing is,", "tokens": [51396, 321, 434, 611, 799, 312, 884, 307, 11, 51500], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1714, "seek": 421096, "start": 4233.68, "end": 4235.88, "text": " we're gonna be requiring that in order to win the prize money", "tokens": [51500, 321, 434, 799, 312, 24165, 300, 294, 1668, 281, 1942, 264, 12818, 1460, 51610], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1715, "seek": 421096, "start": 4235.88, "end": 4238.04, "text": " that you put the solution or your paper", "tokens": [51610, 300, 291, 829, 264, 3827, 420, 428, 3035, 51718], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1716, "seek": 421096, "start": 4238.04, "end": 4239.36, "text": " out into public domain.", "tokens": [51718, 484, 666, 1908, 9274, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10282167237380456, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.00040445581544190645}, {"id": 1717, "seek": 423936, "start": 4240.32, "end": 4241.96, "text": " The reason for this is,", "tokens": [50412, 440, 1778, 337, 341, 307, 11, 50494], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1718, "seek": 423936, "start": 4241.96, "end": 4243.36, "text": " typically with contests,", "tokens": [50494, 5850, 365, 660, 4409, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1719, "seek": 423936, "start": 4243.36, "end": 4245.32, "text": " you see a lot of like closed up sharing.", "tokens": [50564, 291, 536, 257, 688, 295, 411, 5395, 493, 5414, 13, 50662], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1720, "seek": 423936, "start": 4245.32, "end": 4246.599999999999, "text": " People are kind of private secret.", "tokens": [50662, 3432, 366, 733, 295, 4551, 4054, 13, 50726], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1721, "seek": 423936, "start": 4246.599999999999, "end": 4247.719999999999, "text": " They wanna hold their alpha to themselves", "tokens": [50726, 814, 1948, 1797, 641, 8961, 281, 2969, 50782], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1722, "seek": 423936, "start": 4247.719999999999, "end": 4249.12, "text": " during the contest period.", "tokens": [50782, 1830, 264, 10287, 2896, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1723, "seek": 423936, "start": 4249.12, "end": 4252.08, "text": " And because we expect it's gonna be multiple years,", "tokens": [50852, 400, 570, 321, 2066, 309, 311, 799, 312, 3866, 924, 11, 51000], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1724, "seek": 423936, "start": 4252.08, "end": 4253.16, "text": " we wanna enter a game here.", "tokens": [51000, 321, 1948, 3242, 257, 1216, 510, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1725, "seek": 423936, "start": 4253.16, "end": 4256.5199999999995, "text": " So the plan is at the end of November,", "tokens": [51054, 407, 264, 1393, 307, 412, 264, 917, 295, 7674, 11, 51222], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1726, "seek": 423936, "start": 4256.5199999999995, "end": 4258.5199999999995, "text": " we will award the $100,000 prize money", "tokens": [51222, 321, 486, 7130, 264, 1848, 6879, 11, 1360, 12818, 1460, 51322], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1727, "seek": 423936, "start": 4258.5199999999995, "end": 4259.92, "text": " to the top progress prize", "tokens": [51322, 281, 264, 1192, 4205, 12818, 51392], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1728, "seek": 423936, "start": 4259.92, "end": 4263.44, "text": " and then use the downtime between December, January, February", "tokens": [51392, 293, 550, 764, 264, 49648, 1296, 7687, 11, 7061, 11, 8711, 51568], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1729, "seek": 423936, "start": 4263.44, "end": 4266.839999999999, "text": " to share out all the knowledge from the top scores", "tokens": [51568, 281, 2073, 484, 439, 264, 3601, 490, 264, 1192, 13444, 51738], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1730, "seek": 423936, "start": 4266.839999999999, "end": 4268.2, "text": " and the approaches folks were taking", "tokens": [51738, 293, 264, 11587, 4024, 645, 1940, 51806], "temperature": 0.0, "avg_logprob": -0.14402685165405274, "compression_ratio": 1.616564417177914, "no_speech_prob": 0.000391993613447994}, {"id": 1731, "seek": 426820, "start": 4268.2, "end": 4270.08, "text": " in order to re-baseline the community", "tokens": [50364, 294, 1668, 281, 319, 12, 16342, 5440, 264, 1768, 50458], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1732, "seek": 426820, "start": 4270.08, "end": 4271.72, "text": " up to whatever the state of the art is", "tokens": [50458, 493, 281, 2035, 264, 1785, 295, 264, 1523, 307, 50540], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1733, "seek": 426820, "start": 4271.72, "end": 4273.36, "text": " and then run the contest again next year.", "tokens": [50540, 293, 550, 1190, 264, 10287, 797, 958, 1064, 13, 50622], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1734, "seek": 426820, "start": 4273.36, "end": 4276.4, "text": " And keep doing that on a yearly basis until we get 85%.", "tokens": [50622, 400, 1066, 884, 300, 322, 257, 39102, 5143, 1826, 321, 483, 14695, 6856, 50774], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1735, "seek": 426820, "start": 4276.4, "end": 4277.76, "text": " I'll give some people some context", "tokens": [50774, 286, 603, 976, 512, 561, 512, 4319, 50842], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1736, "seek": 426820, "start": 4277.76, "end": 4280.44, "text": " on why I think this prize is very interesting.", "tokens": [50842, 322, 983, 286, 519, 341, 12818, 307, 588, 1880, 13, 50976], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1737, "seek": 426820, "start": 4280.44, "end": 4282.72, "text": " I was having conversations with my friends", "tokens": [50976, 286, 390, 1419, 7315, 365, 452, 1855, 51090], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1738, "seek": 426820, "start": 4282.72, "end": 4286.36, "text": " who are very much believers in models as they exist today.", "tokens": [51090, 567, 366, 588, 709, 23125, 294, 5245, 382, 436, 2514, 965, 13, 51272], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1739, "seek": 426820, "start": 4286.36, "end": 4288.2, "text": " And first of all, it was intriguing to me", "tokens": [51272, 400, 700, 295, 439, 11, 309, 390, 32503, 281, 385, 51364], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1740, "seek": 426820, "start": 4288.2, "end": 4289.96, "text": " that they didn't know about ARC.", "tokens": [51364, 300, 436, 994, 380, 458, 466, 8943, 34, 13, 51452], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1741, "seek": 426820, "start": 4289.96, "end": 4292.24, "text": " These are experienced ML researchers.", "tokens": [51452, 1981, 366, 6751, 21601, 10309, 13, 51566], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1742, "seek": 426820, "start": 4292.24, "end": 4295.5599999999995, "text": " And so you show them this happened a couple of nights ago.", "tokens": [51566, 400, 370, 291, 855, 552, 341, 2011, 257, 1916, 295, 13249, 2057, 13, 51732], "temperature": 0.0, "avg_logprob": -0.09744878496442522, "compression_ratio": 1.6012084592145015, "no_speech_prob": 0.0003149953263346106}, {"id": 1743, "seek": 429556, "start": 4295.56, "end": 4298.240000000001, "text": " We went to dinner and I showed them an example problem.", "tokens": [50364, 492, 1437, 281, 6148, 293, 286, 4712, 552, 364, 1365, 1154, 13, 50498], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1744, "seek": 429556, "start": 4298.240000000001, "end": 4299.160000000001, "text": " And they said, of course,", "tokens": [50498, 400, 436, 848, 11, 295, 1164, 11, 50544], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1745, "seek": 429556, "start": 4299.160000000001, "end": 4301.04, "text": " an LLM would be able to solve something like this.", "tokens": [50544, 364, 441, 43, 44, 576, 312, 1075, 281, 5039, 746, 411, 341, 13, 50638], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1746, "seek": 429556, "start": 4301.04, "end": 4302.280000000001, "text": " And then we take a screenshot of it.", "tokens": [50638, 400, 550, 321, 747, 257, 27712, 295, 309, 13, 50700], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1747, "seek": 429556, "start": 4302.280000000001, "end": 4304.240000000001, "text": " We just put it into our chat GPT app", "tokens": [50700, 492, 445, 829, 309, 666, 527, 5081, 26039, 51, 724, 50798], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1748, "seek": 429556, "start": 4304.240000000001, "end": 4305.52, "text": " and it doesn't get the pattern.", "tokens": [50798, 293, 309, 1177, 380, 483, 264, 5102, 13, 50862], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1749, "seek": 429556, "start": 4305.52, "end": 4308.6, "text": " And so I think it's very interesting.", "tokens": [50862, 400, 370, 286, 519, 309, 311, 588, 1880, 13, 51016], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1750, "seek": 429556, "start": 4308.6, "end": 4309.72, "text": " Like it is a notable fact.", "tokens": [51016, 1743, 309, 307, 257, 22556, 1186, 13, 51072], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1751, "seek": 429556, "start": 4309.72, "end": 4311.6, "text": " I was sort of playing devil's advocate against you", "tokens": [51072, 286, 390, 1333, 295, 2433, 13297, 311, 14608, 1970, 291, 51166], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1752, "seek": 429556, "start": 4311.6, "end": 4312.4400000000005, "text": " on these kinds of questions.", "tokens": [51166, 322, 613, 3685, 295, 1651, 13, 51208], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1753, "seek": 429556, "start": 4312.4400000000005, "end": 4313.92, "text": " But this is a very intriguing fact.", "tokens": [51208, 583, 341, 307, 257, 588, 32503, 1186, 13, 51282], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1754, "seek": 429556, "start": 4313.92, "end": 4316.8, "text": " And I'm extreme, I think this prize is extremely interesting", "tokens": [51282, 400, 286, 478, 8084, 11, 286, 519, 341, 12818, 307, 4664, 1880, 51426], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1755, "seek": 429556, "start": 4316.8, "end": 4318.240000000001, "text": " because we're gonna learn,", "tokens": [51426, 570, 321, 434, 799, 1466, 11, 51498], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1756, "seek": 429556, "start": 4318.240000000001, "end": 4321.240000000001, "text": " we're gonna learn something fascinating one way or another.", "tokens": [51498, 321, 434, 799, 1466, 746, 10343, 472, 636, 420, 1071, 13, 51648], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1757, "seek": 429556, "start": 4321.240000000001, "end": 4323.92, "text": " So with regards to the 85%,", "tokens": [51648, 407, 365, 14258, 281, 264, 14695, 8923, 51782], "temperature": 0.0, "avg_logprob": -0.15926435322104815, "compression_ratio": 1.6713483146067416, "no_speech_prob": 0.0016482657520100474}, {"id": 1758, "seek": 432392, "start": 4323.96, "end": 4324.92, "text": " separate from this prize,", "tokens": [50366, 4994, 490, 341, 12818, 11, 50414], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1759, "seek": 432392, "start": 4324.92, "end": 4327.64, "text": " I'd be very curious if somebody could replicate that result", "tokens": [50414, 286, 1116, 312, 588, 6369, 498, 2618, 727, 25356, 300, 1874, 50550], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1760, "seek": 432392, "start": 4327.64, "end": 4331.64, "text": " because obviously in psychology and other kinds of fields,", "tokens": [50550, 570, 2745, 294, 15105, 293, 661, 3685, 295, 7909, 11, 50750], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1761, "seek": 432392, "start": 4331.64, "end": 4335.0, "text": " which this result seems to be analogous to", "tokens": [50750, 597, 341, 1874, 2544, 281, 312, 16660, 563, 281, 50918], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1762, "seek": 432392, "start": 4335.0, "end": 4338.92, "text": " when you run test on some small sample of people,", "tokens": [50918, 562, 291, 1190, 1500, 322, 512, 1359, 6889, 295, 561, 11, 51114], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1763, "seek": 432392, "start": 4338.92, "end": 4340.2, "text": " often they're hard to replicate.", "tokens": [51114, 2049, 436, 434, 1152, 281, 25356, 13, 51178], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1764, "seek": 432392, "start": 4340.2, "end": 4342.0, "text": " So I'd be very curious if you try to replicate this,", "tokens": [51178, 407, 286, 1116, 312, 588, 6369, 498, 291, 853, 281, 25356, 341, 11, 51268], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1765, "seek": 432392, "start": 4342.0, "end": 4345.4400000000005, "text": " how, what does an average human perform on ARC?", "tokens": [51268, 577, 11, 437, 775, 364, 4274, 1952, 2042, 322, 8943, 34, 30, 51440], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1766, "seek": 432392, "start": 4345.4400000000005, "end": 4347.36, "text": " Ask for the difficulty on how long it will take", "tokens": [51440, 12320, 337, 264, 10360, 322, 577, 938, 309, 486, 747, 51536], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1767, "seek": 432392, "start": 4347.36, "end": 4348.92, "text": " to crack this benchmark.", "tokens": [51536, 281, 6226, 341, 18927, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1768, "seek": 432392, "start": 4348.92, "end": 4351.16, "text": " It's very interesting because the other benchmarks", "tokens": [51614, 467, 311, 588, 1880, 570, 264, 661, 43751, 51726], "temperature": 0.0, "avg_logprob": -0.12943223571777343, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0005702319904230535}, {"id": 1769, "seek": 435116, "start": 4351.2, "end": 4354.04, "text": " that are now fully saturated like MMLU math,", "tokens": [50366, 300, 366, 586, 4498, 25408, 411, 34191, 43, 52, 5221, 11, 50508], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1770, "seek": 435116, "start": 4354.04, "end": 4356.4, "text": " actually the people who made them,", "tokens": [50508, 767, 264, 561, 567, 1027, 552, 11, 50626], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1771, "seek": 435116, "start": 4356.4, "end": 4359.68, "text": " Dan Hendricks and Colin Burns who did MMLU and math,", "tokens": [50626, 3394, 28594, 81, 7663, 293, 29253, 41195, 567, 630, 34191, 43, 52, 293, 5221, 11, 50790], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1772, "seek": 435116, "start": 4359.68, "end": 4361.84, "text": " I think they were grad students or college students", "tokens": [50790, 286, 519, 436, 645, 2771, 1731, 420, 3859, 1731, 50898], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1773, "seek": 435116, "start": 4361.84, "end": 4363.16, "text": " when they made it.", "tokens": [50898, 562, 436, 1027, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1774, "seek": 435116, "start": 4363.16, "end": 4365.4, "text": " And the goal when they made it just a couple of years ago", "tokens": [50964, 400, 264, 3387, 562, 436, 1027, 309, 445, 257, 1916, 295, 924, 2057, 51076], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1775, "seek": 435116, "start": 4365.4, "end": 4367.96, "text": " was that this will be a test of AGI.", "tokens": [51076, 390, 300, 341, 486, 312, 257, 1500, 295, 316, 26252, 13, 51204], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1776, "seek": 435116, "start": 4367.96, "end": 4369.24, "text": " And of course it got totally saturated.", "tokens": [51204, 400, 295, 1164, 309, 658, 3879, 25408, 13, 51268], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1777, "seek": 435116, "start": 4369.24, "end": 4372.48, "text": " And I know you all argue that these are test memorization,", "tokens": [51268, 400, 286, 458, 291, 439, 9695, 300, 613, 366, 1500, 10560, 2144, 11, 51430], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1778, "seek": 435116, "start": 4372.48, "end": 4374.12, "text": " but I think the pattern we've seen,", "tokens": [51430, 457, 286, 519, 264, 5102, 321, 600, 1612, 11, 51512], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1779, "seek": 435116, "start": 4374.12, "end": 4377.12, "text": " in fact, Epoch AI has a very interesting graph", "tokens": [51512, 294, 1186, 11, 462, 2259, 339, 7318, 575, 257, 588, 1880, 4295, 51662], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1780, "seek": 435116, "start": 4377.12, "end": 4379.44, "text": " that I'll sort of overlay for the YouTube version here", "tokens": [51662, 300, 286, 603, 1333, 295, 31741, 337, 264, 3088, 3037, 510, 51778], "temperature": 0.0, "avg_logprob": -0.15700945536295574, "compression_ratio": 1.6930379746835442, "no_speech_prob": 0.004198087844997644}, {"id": 1781, "seek": 437944, "start": 4379.48, "end": 4382.2, "text": " where you see this almost exponential", "tokens": [50366, 689, 291, 536, 341, 1920, 21510, 50502], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1782, "seek": 437944, "start": 4382.2, "end": 4386.12, "text": " where it gets 5%, 10%, 30%, 40%", "tokens": [50502, 689, 309, 2170, 1025, 8923, 1266, 8923, 2217, 8923, 3356, 4, 50698], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1783, "seek": 437944, "start": 4386.12, "end": 4387.879999999999, "text": " as you increase the compute across models", "tokens": [50698, 382, 291, 3488, 264, 14722, 2108, 5245, 50786], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1784, "seek": 437944, "start": 4387.879999999999, "end": 4389.919999999999, "text": " and then it just shoots up.", "tokens": [50786, 293, 550, 309, 445, 20704, 493, 13, 50888], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1785, "seek": 437944, "start": 4389.919999999999, "end": 4392.639999999999, "text": " And in the GPT-4 technical report,", "tokens": [50888, 400, 294, 264, 26039, 51, 12, 19, 6191, 2275, 11, 51024], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1786, "seek": 437944, "start": 4392.639999999999, "end": 4394.16, "text": " they had this interesting graph", "tokens": [51024, 436, 632, 341, 1880, 4295, 51100], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1787, "seek": 437944, "start": 4394.16, "end": 4396.24, "text": " of the human eval problem set,", "tokens": [51100, 295, 264, 1952, 1073, 304, 1154, 992, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1788, "seek": 437944, "start": 4396.24, "end": 4398.4, "text": " which was 22 coding problems.", "tokens": [51204, 597, 390, 5853, 17720, 2740, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1789, "seek": 437944, "start": 4398.4, "end": 4402.2, "text": " And they had to graph it on the mean log pass curve,", "tokens": [51312, 400, 436, 632, 281, 4295, 309, 322, 264, 914, 3565, 1320, 7605, 11, 51502], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1790, "seek": 437944, "start": 4402.2, "end": 4404.96, "text": " basically because early on in training", "tokens": [51502, 1936, 570, 2440, 322, 294, 3097, 51640], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1791, "seek": 437944, "start": 4404.96, "end": 4408.2, "text": " or even smaller models can have the right idea", "tokens": [51640, 420, 754, 4356, 5245, 393, 362, 264, 558, 1558, 51802], "temperature": 0.0, "avg_logprob": -0.10672344480242048, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.0004305182083044201}, {"id": 1792, "seek": 440820, "start": 4408.2, "end": 4409.76, "text": " of how to solve this problem,", "tokens": [50364, 295, 577, 281, 5039, 341, 1154, 11, 50442], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1793, "seek": 440820, "start": 4409.76, "end": 4411.72, "text": " but it takes a lot of reliability", "tokens": [50442, 457, 309, 2516, 257, 688, 295, 24550, 50540], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1794, "seek": 440820, "start": 4411.72, "end": 4414.0, "text": " to make sure they stay on track to solve the whole problem.", "tokens": [50540, 281, 652, 988, 436, 1754, 322, 2837, 281, 5039, 264, 1379, 1154, 13, 50654], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1795, "seek": 440820, "start": 4414.0, "end": 4416.12, "text": " And so you really wanna up wait the signal", "tokens": [50654, 400, 370, 291, 534, 1948, 493, 1699, 264, 6358, 50760], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1796, "seek": 440820, "start": 4416.12, "end": 4418.0, "text": " where they get it right at least some of the time,", "tokens": [50760, 689, 436, 483, 309, 558, 412, 1935, 512, 295, 264, 565, 11, 50854], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1797, "seek": 440820, "start": 4418.0, "end": 4419.72, "text": " maybe one in a hundred times or one in a thousand.", "tokens": [50854, 1310, 472, 294, 257, 3262, 1413, 420, 472, 294, 257, 4714, 13, 50940], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1798, "seek": 440820, "start": 4419.72, "end": 4421.5599999999995, "text": " And then so they go from like one in a thousand,", "tokens": [50940, 400, 550, 370, 436, 352, 490, 411, 472, 294, 257, 4714, 11, 51032], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1799, "seek": 440820, "start": 4421.5599999999995, "end": 4422.48, "text": " one in a hundred, one in 10,", "tokens": [51032, 472, 294, 257, 3262, 11, 472, 294, 1266, 11, 51078], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1800, "seek": 440820, "start": 4422.48, "end": 4424.62, "text": " and then they just like totally saturated.", "tokens": [51078, 293, 550, 436, 445, 411, 3879, 25408, 13, 51185], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1801, "seek": 440820, "start": 4424.62, "end": 4426.28, "text": " I guess the question I have,", "tokens": [51185, 286, 2041, 264, 1168, 286, 362, 11, 51268], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1802, "seek": 440820, "start": 4426.28, "end": 4427.16, "text": " this is all leading up to,", "tokens": [51268, 341, 307, 439, 5775, 493, 281, 11, 51312], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1803, "seek": 440820, "start": 4427.16, "end": 4429.9, "text": " is why won't the same thing happen with ARC", "tokens": [51312, 307, 983, 1582, 380, 264, 912, 551, 1051, 365, 8943, 34, 51449], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1804, "seek": 440820, "start": 4429.9, "end": 4433.48, "text": " where people had to try really hard, bigger models.", "tokens": [51449, 689, 561, 632, 281, 853, 534, 1152, 11, 3801, 5245, 13, 51628], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1805, "seek": 440820, "start": 4434.4, "end": 4436.24, "text": " And now they figured out these techniques", "tokens": [51674, 400, 586, 436, 8932, 484, 613, 7512, 51766], "temperature": 0.0, "avg_logprob": -0.180937084150903, "compression_ratio": 1.81055900621118, "no_speech_prob": 0.0024721829686313868}, {"id": 1806, "seek": 443624, "start": 4436.28, "end": 4437.12, "text": " that Jack Cole has figured out", "tokens": [50366, 300, 4718, 20394, 575, 8932, 484, 50408], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1807, "seek": 443624, "start": 4437.12, "end": 4440.12, "text": " with only a 240 million parameter language model", "tokens": [50408, 365, 787, 257, 26837, 2459, 13075, 2856, 2316, 50558], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1808, "seek": 443624, "start": 4440.12, "end": 4442.44, "text": " that can get 35%.", "tokens": [50558, 300, 393, 483, 6976, 6856, 50674], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1809, "seek": 443624, "start": 4442.44, "end": 4443.5599999999995, "text": " Shouldn't we see the same pattern we saw", "tokens": [50674, 34170, 380, 321, 536, 264, 912, 5102, 321, 1866, 50730], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1810, "seek": 443624, "start": 4443.5599999999995, "end": 4444.48, "text": " across all these other benchmarks", "tokens": [50730, 2108, 439, 613, 661, 43751, 50776], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1811, "seek": 443624, "start": 4444.48, "end": 4445.88, "text": " where you just like sort of eke out.", "tokens": [50776, 689, 291, 445, 411, 1333, 295, 308, 330, 484, 13, 50846], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1812, "seek": 443624, "start": 4445.88, "end": 4447.5599999999995, "text": " And then once you get the general idea,", "tokens": [50846, 400, 550, 1564, 291, 483, 264, 2674, 1558, 11, 50930], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1813, "seek": 443624, "start": 4447.5599999999995, "end": 4449.8, "text": " then you just go all the way to a hundred.", "tokens": [50930, 550, 291, 445, 352, 439, 264, 636, 281, 257, 3262, 13, 51042], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1814, "seek": 443624, "start": 4449.8, "end": 4450.92, "text": " That's an empirical question.", "tokens": [51042, 663, 311, 364, 31886, 1168, 13, 51098], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1815, "seek": 443624, "start": 4450.92, "end": 4452.84, "text": " So we'll see in practice what happens.", "tokens": [51098, 407, 321, 603, 536, 294, 3124, 437, 2314, 13, 51194], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1816, "seek": 443624, "start": 4453.84, "end": 4456.5599999999995, "text": " But what Jack Cole is doing is actually very unique.", "tokens": [51244, 583, 437, 4718, 20394, 307, 884, 307, 767, 588, 3845, 13, 51380], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1817, "seek": 443624, "start": 4456.5599999999995, "end": 4459.679999999999, "text": " It's not just pre-training an LLM and then prompting it,", "tokens": [51380, 467, 311, 406, 445, 659, 12, 17227, 1760, 364, 441, 43, 44, 293, 550, 12391, 278, 309, 11, 51536], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1818, "seek": 443624, "start": 4459.679999999999, "end": 4461.96, "text": " he's actually trying to do active inference.", "tokens": [51536, 415, 311, 767, 1382, 281, 360, 4967, 38253, 13, 51650], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1819, "seek": 443624, "start": 4461.96, "end": 4463.16, "text": " He's doing a test time, right?", "tokens": [51650, 634, 311, 884, 257, 1500, 565, 11, 558, 30, 51710], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1820, "seek": 443624, "start": 4463.16, "end": 4464.0, "text": " He's doing like test time functioning.", "tokens": [51710, 634, 311, 884, 411, 1500, 565, 18483, 13, 51752], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1821, "seek": 443624, "start": 4464.0, "end": 4465.679999999999, "text": " Exactly, test time functioning.", "tokens": [51752, 7587, 11, 1500, 565, 18483, 13, 51836], "temperature": 0.0, "avg_logprob": -0.23268462605559068, "compression_ratio": 1.7119113573407203, "no_speech_prob": 0.0008558051194995642}, {"id": 1822, "seek": 446568, "start": 4465.68, "end": 4467.68, "text": " And this is actually trying to lift", "tokens": [50364, 400, 341, 307, 767, 1382, 281, 5533, 50464], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1823, "seek": 446568, "start": 4467.68, "end": 4469.68, "text": " one of the key limitations of LLMs,", "tokens": [50464, 472, 295, 264, 2141, 15705, 295, 441, 43, 26386, 11, 50564], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1824, "seek": 446568, "start": 4469.68, "end": 4471.52, "text": " which is that at inference time,", "tokens": [50564, 597, 307, 300, 412, 38253, 565, 11, 50656], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1825, "seek": 446568, "start": 4471.52, "end": 4472.68, "text": " they cannot learn anything new.", "tokens": [50656, 436, 2644, 1466, 1340, 777, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1826, "seek": 446568, "start": 4472.68, "end": 4475.52, "text": " They cannot adapt on the flight what they're seeing.", "tokens": [50714, 814, 2644, 6231, 322, 264, 7018, 437, 436, 434, 2577, 13, 50856], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1827, "seek": 446568, "start": 4475.52, "end": 4478.72, "text": " And he's actually trying to learn.", "tokens": [50856, 400, 415, 311, 767, 1382, 281, 1466, 13, 51016], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1828, "seek": 446568, "start": 4478.72, "end": 4480.8, "text": " So what he's doing is effectively", "tokens": [51016, 407, 437, 415, 311, 884, 307, 8659, 51120], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1829, "seek": 446568, "start": 4480.8, "end": 4482.96, "text": " a form of program synthesis.", "tokens": [51120, 257, 1254, 295, 1461, 30252, 13, 51228], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1830, "seek": 446568, "start": 4484.08, "end": 4486.92, "text": " Because the LLM contains a lot of useful building blocks,", "tokens": [51284, 1436, 264, 441, 43, 44, 8306, 257, 688, 295, 4420, 2390, 8474, 11, 51426], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1831, "seek": 446568, "start": 4486.92, "end": 4488.68, "text": " like programming building blocks,", "tokens": [51426, 411, 9410, 2390, 8474, 11, 51514], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1832, "seek": 446568, "start": 4488.68, "end": 4492.08, "text": " and by finding it on the task at test time,", "tokens": [51514, 293, 538, 5006, 309, 322, 264, 5633, 412, 1500, 565, 11, 51684], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1833, "seek": 446568, "start": 4492.08, "end": 4494.6, "text": " you are trying to assemble these building blocks", "tokens": [51684, 291, 366, 1382, 281, 22364, 613, 2390, 8474, 51810], "temperature": 0.0, "avg_logprob": -0.15161396789550782, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.00025518398615531623}, {"id": 1834, "seek": 449460, "start": 4494.6, "end": 4497.68, "text": " into the right pattern that matches the task.", "tokens": [50364, 666, 264, 558, 5102, 300, 10676, 264, 5633, 13, 50518], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1835, "seek": 449460, "start": 4497.68, "end": 4500.56, "text": " This is exactly what program synthesis is about.", "tokens": [50518, 639, 307, 2293, 437, 1461, 30252, 307, 466, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1836, "seek": 449460, "start": 4500.56, "end": 4503.64, "text": " And the way we contrast this approach", "tokens": [50662, 400, 264, 636, 321, 8712, 341, 3109, 50816], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1837, "seek": 449460, "start": 4503.64, "end": 4505.92, "text": " with discrete program search is that", "tokens": [50816, 365, 27706, 1461, 3164, 307, 300, 50930], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1838, "seek": 449460, "start": 4505.92, "end": 4507.76, "text": " in discrete program search,", "tokens": [50930, 294, 27706, 1461, 3164, 11, 51022], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1839, "seek": 449460, "start": 4507.76, "end": 4510.240000000001, "text": " so you're trying to assemble a program", "tokens": [51022, 370, 291, 434, 1382, 281, 22364, 257, 1461, 51146], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1840, "seek": 449460, "start": 4510.240000000001, "end": 4512.160000000001, "text": " from a set of primitives.", "tokens": [51146, 490, 257, 992, 295, 2886, 38970, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1841, "seek": 449460, "start": 4512.160000000001, "end": 4513.52, "text": " You have very few primitives.", "tokens": [51242, 509, 362, 588, 1326, 2886, 38970, 13, 51310], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1842, "seek": 449460, "start": 4513.52, "end": 4515.8, "text": " So people working on discrete program search on Arc,", "tokens": [51310, 407, 561, 1364, 322, 27706, 1461, 3164, 322, 21727, 11, 51424], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1843, "seek": 449460, "start": 4515.8, "end": 4518.8, "text": " for instance, they tend to work with DSLs that have like", "tokens": [51424, 337, 5197, 11, 436, 3928, 281, 589, 365, 15816, 43, 82, 300, 362, 411, 51574], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1844, "seek": 449460, "start": 4518.8, "end": 4521.92, "text": " 100 to 200 primitive programs.", "tokens": [51574, 2319, 281, 2331, 28540, 4268, 13, 51730], "temperature": 0.0, "avg_logprob": -0.1400529912539891, "compression_ratio": 1.7673469387755103, "no_speech_prob": 0.0017842876259237528}, {"id": 1845, "seek": 452192, "start": 4521.92, "end": 4523.64, "text": " So very small DSL,", "tokens": [50364, 407, 588, 1359, 15816, 43, 11, 50450], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1846, "seek": 452192, "start": 4523.64, "end": 4526.72, "text": " but then they're trying to combine these primitives", "tokens": [50450, 457, 550, 436, 434, 1382, 281, 10432, 613, 2886, 38970, 50604], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1847, "seek": 452192, "start": 4526.72, "end": 4529.16, "text": " into very complex programs.", "tokens": [50604, 666, 588, 3997, 4268, 13, 50726], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1848, "seek": 452192, "start": 4529.16, "end": 4532.28, "text": " So there's very deep depths of search.", "tokens": [50726, 407, 456, 311, 588, 2452, 28439, 295, 3164, 13, 50882], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1849, "seek": 452192, "start": 4532.28, "end": 4534.6, "text": " And on the other hand,", "tokens": [50882, 400, 322, 264, 661, 1011, 11, 50998], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1850, "seek": 452192, "start": 4534.6, "end": 4537.12, "text": " if you look at what Jack is doing with LLMs,", "tokens": [50998, 498, 291, 574, 412, 437, 4718, 307, 884, 365, 441, 43, 26386, 11, 51124], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1851, "seek": 452192, "start": 4537.12, "end": 4542.12, "text": " is that he's got this sort of like vector program database,", "tokens": [51124, 307, 300, 415, 311, 658, 341, 1333, 295, 411, 8062, 1461, 8149, 11, 51374], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1852, "seek": 452192, "start": 4543.2, "end": 4547.4400000000005, "text": " DSL of millions of building blocks in the LLM", "tokens": [51428, 15816, 43, 295, 6803, 295, 2390, 8474, 294, 264, 441, 43, 44, 51640], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1853, "seek": 452192, "start": 4547.4400000000005, "end": 4550.24, "text": " that are mined by pre-training the LLM,", "tokens": [51640, 300, 366, 923, 292, 538, 659, 12, 17227, 1760, 264, 441, 43, 44, 11, 51780], "temperature": 0.0, "avg_logprob": -0.14656349464699073, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.00019195236382074654}, {"id": 1854, "seek": 455024, "start": 4550.24, "end": 4552.719999999999, "text": " not just on a ton of programming problems,", "tokens": [50364, 406, 445, 322, 257, 2952, 295, 9410, 2740, 11, 50488], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1855, "seek": 455024, "start": 4552.719999999999, "end": 4556.4, "text": " but also on millions of generated Arc-like tasks.", "tokens": [50488, 457, 611, 322, 6803, 295, 10833, 21727, 12, 4092, 9608, 13, 50672], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1856, "seek": 455024, "start": 4556.4, "end": 4559.679999999999, "text": " So you have an extraordinarily large DSL.", "tokens": [50672, 407, 291, 362, 364, 34557, 2416, 15816, 43, 13, 50836], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1857, "seek": 455024, "start": 4559.679999999999, "end": 4563.0, "text": " And then the fun tuning is very, very shallow", "tokens": [50836, 400, 550, 264, 1019, 15164, 307, 588, 11, 588, 20488, 51002], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1858, "seek": 455024, "start": 4563.0, "end": 4564.88, "text": " recombination of these primitives.", "tokens": [51002, 850, 3548, 2486, 295, 613, 2886, 38970, 13, 51096], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1859, "seek": 455024, "start": 4564.88, "end": 4568.5599999999995, "text": " So discrete program search, very deep recombination,", "tokens": [51096, 407, 27706, 1461, 3164, 11, 588, 2452, 850, 3548, 2486, 11, 51280], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1860, "seek": 455024, "start": 4568.5599999999995, "end": 4572.0, "text": " very small set of primitive programs.", "tokens": [51280, 588, 1359, 992, 295, 28540, 4268, 13, 51452], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1861, "seek": 455024, "start": 4572.0, "end": 4574.16, "text": " And the LLM approach is the same,", "tokens": [51452, 400, 264, 441, 43, 44, 3109, 307, 264, 912, 11, 51560], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1862, "seek": 455024, "start": 4574.16, "end": 4577.26, "text": " but on the complete opposite end of that spectrum,", "tokens": [51560, 457, 322, 264, 3566, 6182, 917, 295, 300, 11143, 11, 51715], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1863, "seek": 455024, "start": 4577.26, "end": 4579.4, "text": " where you scale up the memorization", "tokens": [51715, 689, 291, 4373, 493, 264, 10560, 2144, 51822], "temperature": 0.0, "avg_logprob": -0.13697562301368044, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0003821471182163805}, {"id": 1864, "seek": 457940, "start": 4579.4, "end": 4583.5599999999995, "text": " by a massive factor and you're doing very, very shallow search,", "tokens": [50364, 538, 257, 5994, 5952, 293, 291, 434, 884, 588, 11, 588, 20488, 3164, 11, 50572], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1865, "seek": 457940, "start": 4583.5599999999995, "end": 4585.4, "text": " but they are the same thing,", "tokens": [50572, 457, 436, 366, 264, 912, 551, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1866, "seek": 457940, "start": 4585.4, "end": 4587.4, "text": " just different ends of the spectrum.", "tokens": [50664, 445, 819, 5314, 295, 264, 11143, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1867, "seek": 457940, "start": 4587.4, "end": 4591.16, "text": " And I think where you're gonna get the most value", "tokens": [50764, 400, 286, 519, 689, 291, 434, 799, 483, 264, 881, 2158, 50952], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1868, "seek": 457940, "start": 4592.2, "end": 4596.2, "text": " for your compute cycles is gonna be somewhere in between.", "tokens": [51004, 337, 428, 14722, 17796, 307, 799, 312, 4079, 294, 1296, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1869, "seek": 457940, "start": 4596.2, "end": 4600.679999999999, "text": " You want to leverage memorization to build up a richer,", "tokens": [51204, 509, 528, 281, 13982, 10560, 2144, 281, 1322, 493, 257, 29021, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1870, "seek": 457940, "start": 4600.679999999999, "end": 4603.799999999999, "text": " more useful bank of primitive programs.", "tokens": [51428, 544, 4420, 3765, 295, 28540, 4268, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1871, "seek": 457940, "start": 4603.799999999999, "end": 4606.08, "text": " And you don't want them to be hard-coded", "tokens": [51584, 400, 291, 500, 380, 528, 552, 281, 312, 1152, 12, 66, 12340, 51698], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1872, "seek": 457940, "start": 4606.08, "end": 4608.4, "text": " like what we saw for the typical artist.", "tokens": [51698, 411, 437, 321, 1866, 337, 264, 7476, 5748, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1939745989712802, "compression_ratio": 1.583969465648855, "no_speech_prob": 0.0005315588787198067}, {"id": 1873, "seek": 460840, "start": 4608.4, "end": 4611.2, "text": " You want them to be learned from examples.", "tokens": [50364, 509, 528, 552, 281, 312, 3264, 490, 5110, 13, 50504], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1874, "seek": 460840, "start": 4611.2, "end": 4615.839999999999, "text": " But then you also want to do some degree of deep search.", "tokens": [50504, 583, 550, 291, 611, 528, 281, 360, 512, 4314, 295, 2452, 3164, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1875, "seek": 460840, "start": 4615.839999999999, "end": 4618.16, "text": " As long as you're only doing very shallow search,", "tokens": [50736, 1018, 938, 382, 291, 434, 787, 884, 588, 20488, 3164, 11, 50852], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1876, "seek": 460840, "start": 4618.16, "end": 4620.12, "text": " you are limited to local generalization.", "tokens": [50852, 291, 366, 5567, 281, 2654, 2674, 2144, 13, 50950], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1877, "seek": 460840, "start": 4620.12, "end": 4621.96, "text": " If you want to generalize further,", "tokens": [50950, 759, 291, 528, 281, 2674, 1125, 3052, 11, 51042], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1878, "seek": 460840, "start": 4621.96, "end": 4626.96, "text": " more broadly, this depth of search is gonna be critical.", "tokens": [51042, 544, 19511, 11, 341, 7161, 295, 3164, 307, 799, 312, 4924, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1879, "seek": 460840, "start": 4627.24, "end": 4631.839999999999, "text": " I might argue that the reason that he had to rely so heavily", "tokens": [51306, 286, 1062, 9695, 300, 264, 1778, 300, 415, 632, 281, 10687, 370, 10950, 51536], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1880, "seek": 460840, "start": 4631.839999999999, "end": 4636.839999999999, "text": " on the synthetic data was because he used a 240 million", "tokens": [51536, 322, 264, 23420, 1412, 390, 570, 415, 1143, 257, 26837, 2459, 51786], "temperature": 0.0, "avg_logprob": -0.1361738316063742, "compression_ratio": 1.6088709677419355, "no_speech_prob": 9.456442785449326e-05}, {"id": 1881, "seek": 463684, "start": 4636.84, "end": 4639.6, "text": " parameter model because the Kaggle competition at the time", "tokens": [50364, 13075, 2316, 570, 264, 48751, 22631, 6211, 412, 264, 565, 50502], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1882, "seek": 463684, "start": 4639.6, "end": 4642.52, "text": " required him to use a P100 GPU,", "tokens": [50502, 4739, 796, 281, 764, 257, 430, 6879, 18407, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1883, "seek": 463684, "start": 4642.52, "end": 4646.400000000001, "text": " which has like a 10th or something of the flops of an H100.", "tokens": [50648, 597, 575, 411, 257, 1266, 392, 420, 746, 295, 264, 932, 3370, 295, 364, 389, 6879, 13, 50842], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1884, "seek": 463684, "start": 4646.400000000001, "end": 4648.88, "text": " And so obviously he can't use,", "tokens": [50842, 400, 370, 2745, 415, 393, 380, 764, 11, 50966], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1885, "seek": 463684, "start": 4648.88, "end": 4652.400000000001, "text": " if you believe that sort of scaling will solve", "tokens": [50966, 498, 291, 1697, 300, 1333, 295, 21589, 486, 5039, 51142], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1886, "seek": 463684, "start": 4652.400000000001, "end": 4653.76, "text": " this kind of reasoning,", "tokens": [51142, 341, 733, 295, 21577, 11, 51210], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1887, "seek": 463684, "start": 4653.76, "end": 4656.56, "text": " then there you can just rely on the generalization,", "tokens": [51210, 550, 456, 291, 393, 445, 10687, 322, 264, 2674, 2144, 11, 51350], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1888, "seek": 463684, "start": 4656.56, "end": 4658.400000000001, "text": " whereas if you're using a much smaller,", "tokens": [51350, 9735, 498, 291, 434, 1228, 257, 709, 4356, 11, 51442], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1889, "seek": 463684, "start": 4658.400000000001, "end": 4659.92, "text": " for context for the listeners, by the way,", "tokens": [51442, 337, 4319, 337, 264, 23274, 11, 538, 264, 636, 11, 51518], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1890, "seek": 463684, "start": 4659.92, "end": 4661.52, "text": " the frontier models today are literally", "tokens": [51518, 264, 35853, 5245, 965, 366, 3736, 51598], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1891, "seek": 463684, "start": 4661.52, "end": 4663.08, "text": " a thousand X bigger than that.", "tokens": [51598, 257, 4714, 1783, 3801, 813, 300, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1892, "seek": 463684, "start": 4663.08, "end": 4666.28, "text": " And so for your competition,", "tokens": [51676, 400, 370, 337, 428, 6211, 11, 51836], "temperature": 0.0, "avg_logprob": -0.1663818359375, "compression_ratio": 1.6342281879194631, "no_speech_prob": 0.005381568800657988}, {"id": 1893, "seek": 466628, "start": 4666.28, "end": 4667.96, "text": " from what I remember,", "tokens": [50364, 490, 437, 286, 1604, 11, 50448], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1894, "seek": 466628, "start": 4667.96, "end": 4670.5199999999995, "text": " the submission you'll have to submit", "tokens": [50448, 264, 23689, 291, 603, 362, 281, 10315, 50576], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1895, "seek": 466628, "start": 4670.5199999999995, "end": 4673.36, "text": " can't make any API calls, can't go online,", "tokens": [50576, 393, 380, 652, 604, 9362, 5498, 11, 393, 380, 352, 2950, 11, 50718], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1896, "seek": 466628, "start": 4673.36, "end": 4677.639999999999, "text": " and has to run on NVIDIA Tesla T4.", "tokens": [50718, 293, 575, 281, 1190, 322, 426, 3958, 6914, 13666, 314, 19, 13, 50932], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1897, "seek": 466628, "start": 4677.639999999999, "end": 4678.48, "text": " P100.", "tokens": [50932, 430, 6879, 13, 50974], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1898, "seek": 466628, "start": 4678.48, "end": 4679.32, "text": " P100.", "tokens": [50974, 430, 6879, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1899, "seek": 466628, "start": 4679.32, "end": 4680.16, "text": " Oh, is it P100?", "tokens": [51016, 876, 11, 307, 309, 430, 6879, 30, 51058], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1900, "seek": 466628, "start": 4680.16, "end": 4681.0, "text": " Yeah.", "tokens": [51058, 865, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1901, "seek": 466628, "start": 4681.0, "end": 4682.32, "text": " Okay, so again, it's like significantly less powerful.", "tokens": [51100, 1033, 11, 370, 797, 11, 309, 311, 411, 10591, 1570, 4005, 13, 51166], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1902, "seek": 466628, "start": 4682.32, "end": 4683.759999999999, "text": " There's a 12-hour runtime limit, basically.", "tokens": [51166, 821, 311, 257, 2272, 12, 18048, 34474, 4948, 11, 1936, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1903, "seek": 466628, "start": 4683.759999999999, "end": 4686.28, "text": " There's a forcing function of efficiency in the eval.", "tokens": [51238, 821, 311, 257, 19030, 2445, 295, 10493, 294, 264, 1073, 304, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1904, "seek": 466628, "start": 4686.28, "end": 4689.599999999999, "text": " But here's the thing, you only have 100 test tasks.", "tokens": [51364, 583, 510, 311, 264, 551, 11, 291, 787, 362, 2319, 1500, 9608, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1905, "seek": 466628, "start": 4689.599999999999, "end": 4692.04, "text": " So the amount of computing available for each task", "tokens": [51530, 407, 264, 2372, 295, 15866, 2435, 337, 1184, 5633, 51652], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1906, "seek": 466628, "start": 4692.04, "end": 4693.04, "text": " is actually quite a bit,", "tokens": [51652, 307, 767, 1596, 257, 857, 11, 51702], "temperature": 0.0, "avg_logprob": -0.2555934058295356, "compression_ratio": 1.5445205479452055, "no_speech_prob": 0.0008968263864517212}, {"id": 1907, "seek": 469304, "start": 4693.08, "end": 4694.72, "text": " especially if you contrast that", "tokens": [50366, 2318, 498, 291, 8712, 300, 50448], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1908, "seek": 469304, "start": 4694.72, "end": 4696.64, "text": " with the simplicity of each task.", "tokens": [50448, 365, 264, 25632, 295, 1184, 5633, 13, 50544], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1909, "seek": 469304, "start": 4696.64, "end": 4699.6, "text": " So it would be seven minutes per task, basically,", "tokens": [50544, 407, 309, 576, 312, 3407, 2077, 680, 5633, 11, 1936, 11, 50692], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1910, "seek": 469304, "start": 4699.6, "end": 4702.5199999999995, "text": " which for, people have tried to do these estimates", "tokens": [50692, 597, 337, 11, 561, 362, 3031, 281, 360, 613, 20561, 50838], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1911, "seek": 469304, "start": 4702.5199999999995, "end": 4704.6, "text": " of how many flops does a human brain have.", "tokens": [50838, 295, 577, 867, 932, 3370, 775, 257, 1952, 3567, 362, 13, 50942], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1912, "seek": 469304, "start": 4704.6, "end": 4706.08, "text": " And you can take them with a grain of salt,", "tokens": [50942, 400, 291, 393, 747, 552, 365, 257, 12837, 295, 5139, 11, 51016], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1913, "seek": 469304, "start": 4706.08, "end": 4708.24, "text": " but as a sort of anchor,", "tokens": [51016, 457, 382, 257, 1333, 295, 18487, 11, 51124], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1914, "seek": 469304, "start": 4708.24, "end": 4711.36, "text": " it's basically the amount of flops an H100 has.", "tokens": [51124, 309, 311, 1936, 264, 2372, 295, 932, 3370, 364, 389, 6879, 575, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1915, "seek": 469304, "start": 4711.36, "end": 4712.96, "text": " And I guess maybe you would argue with that,", "tokens": [51280, 400, 286, 2041, 1310, 291, 576, 9695, 365, 300, 11, 51360], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1916, "seek": 469304, "start": 4712.96, "end": 4715.4, "text": " well, a human brain can solve this question", "tokens": [51360, 731, 11, 257, 1952, 3567, 393, 5039, 341, 1168, 51482], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1917, "seek": 469304, "start": 4715.4, "end": 4716.56, "text": " in faster than 7.2 minutes.", "tokens": [51482, 294, 4663, 813, 1614, 13, 17, 2077, 13, 51540], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1918, "seek": 469304, "start": 4716.56, "end": 4718.04, "text": " So even with a tenth of the compute,", "tokens": [51540, 407, 754, 365, 257, 27269, 295, 264, 14722, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1919, "seek": 469304, "start": 4718.04, "end": 4720.56, "text": " you should be able to do it in seven minutes.", "tokens": [51614, 291, 820, 312, 1075, 281, 360, 309, 294, 3407, 2077, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12447030385335286, "compression_ratio": 1.7475083056478404, "no_speech_prob": 0.00023043350665830076}, {"id": 1920, "seek": 472056, "start": 4720.56, "end": 4723.04, "text": " Obviously, we have less memory than, you know,", "tokens": [50364, 7580, 11, 321, 362, 1570, 4675, 813, 11, 291, 458, 11, 50488], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1921, "seek": 472056, "start": 4723.04, "end": 4725.88, "text": " like petabytes of fast access memory in the brain.", "tokens": [50488, 411, 3817, 24538, 295, 2370, 2105, 4675, 294, 264, 3567, 13, 50630], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1922, "seek": 472056, "start": 4725.88, "end": 4728.84, "text": " And with these, you know, 29 or whatever gigabytes", "tokens": [50630, 400, 365, 613, 11, 291, 458, 11, 9413, 420, 2035, 42741, 50778], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1923, "seek": 472056, "start": 4728.84, "end": 4730.400000000001, "text": " in this H100.", "tokens": [50778, 294, 341, 389, 6879, 13, 50856], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1924, "seek": 472056, "start": 4730.400000000001, "end": 4732.4800000000005, "text": " Anyway, I guess the broader question I'm asking is,", "tokens": [50856, 5684, 11, 286, 2041, 264, 13227, 1168, 286, 478, 3365, 307, 11, 50960], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1925, "seek": 472056, "start": 4735.360000000001, "end": 4738.56, "text": " I wish there was a way to also test this prize", "tokens": [51104, 286, 3172, 456, 390, 257, 636, 281, 611, 1500, 341, 12818, 51264], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1926, "seek": 472056, "start": 4738.56, "end": 4741.120000000001, "text": " with some sort of scaffolding on the biggest models", "tokens": [51264, 365, 512, 1333, 295, 44094, 278, 322, 264, 3880, 5245, 51392], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1927, "seek": 472056, "start": 4741.120000000001, "end": 4744.0, "text": " as a way to test whether scaling is the path", "tokens": [51392, 382, 257, 636, 281, 1500, 1968, 21589, 307, 264, 3100, 51536], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1928, "seek": 472056, "start": 4744.0, "end": 4747.88, "text": " to get to solving ARC.", "tokens": [51536, 281, 483, 281, 12606, 8943, 34, 13, 51730], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1929, "seek": 472056, "start": 4747.88, "end": 4748.72, "text": " Absolutely.", "tokens": [51730, 7021, 13, 51772], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1930, "seek": 472056, "start": 4748.72, "end": 4749.92, "text": " So in the context of the computation,", "tokens": [51772, 407, 294, 264, 4319, 295, 264, 24903, 11, 51832], "temperature": 0.0, "avg_logprob": -0.18482750751933114, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0011869556037709117}, {"id": 1931, "seek": 474992, "start": 4749.92, "end": 4752.32, "text": " we want to see how much progress we can do", "tokens": [50364, 321, 528, 281, 536, 577, 709, 4205, 321, 393, 360, 50484], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1932, "seek": 474992, "start": 4752.32, "end": 4753.8, "text": " with limited resources.", "tokens": [50484, 365, 5567, 3593, 13, 50558], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1933, "seek": 474992, "start": 4753.8, "end": 4756.32, "text": " But you're entirely right that it's a super interesting", "tokens": [50558, 583, 291, 434, 7696, 558, 300, 309, 311, 257, 1687, 1880, 50684], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1934, "seek": 474992, "start": 4756.32, "end": 4757.36, "text": " open question.", "tokens": [50684, 1269, 1168, 13, 50736], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1935, "seek": 474992, "start": 4757.36, "end": 4760.6, "text": " What could the biggest model out there actually do on ARC?", "tokens": [50736, 708, 727, 264, 3880, 2316, 484, 456, 767, 360, 322, 8943, 34, 30, 50898], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1936, "seek": 474992, "start": 4760.6, "end": 4764.76, "text": " So we want to actually also make available a private", "tokens": [50898, 407, 321, 528, 281, 767, 611, 652, 2435, 257, 4551, 51106], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1937, "seek": 474992, "start": 4764.76, "end": 4769.4800000000005, "text": " sort of like one-off track where you can submit to us a VM", "tokens": [51106, 1333, 295, 411, 472, 12, 4506, 2837, 689, 291, 393, 10315, 281, 505, 257, 18038, 51342], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1938, "seek": 474992, "start": 4769.4800000000005, "end": 4772.0, "text": " and so you can put on it any model you want.", "tokens": [51342, 293, 370, 291, 393, 829, 322, 309, 604, 2316, 291, 528, 13, 51468], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1939, "seek": 474992, "start": 4772.0, "end": 4774.56, "text": " Like you can take one of the largest open source models", "tokens": [51468, 1743, 291, 393, 747, 472, 295, 264, 6443, 1269, 4009, 5245, 51596], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1940, "seek": 474992, "start": 4774.56, "end": 4777.2, "text": " out there and find you need to do whatever you want", "tokens": [51596, 484, 456, 293, 915, 291, 643, 281, 360, 2035, 291, 528, 51728], "temperature": 0.0, "avg_logprob": -0.179867009647557, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0009326120489276946}, {"id": 1941, "seek": 477720, "start": 4777.2, "end": 4779.88, "text": " and just give us an image.", "tokens": [50364, 293, 445, 976, 505, 364, 3256, 13, 50498], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1942, "seek": 477720, "start": 4779.88, "end": 4782.96, "text": " And then we run it on the H100 for like 24 hours", "tokens": [50498, 400, 550, 321, 1190, 309, 322, 264, 389, 6879, 337, 411, 4022, 2496, 50652], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1943, "seek": 477720, "start": 4782.96, "end": 4784.679999999999, "text": " or something and you see what you get.", "tokens": [50652, 420, 746, 293, 291, 536, 437, 291, 483, 13, 50738], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1944, "seek": 477720, "start": 4784.679999999999, "end": 4787.76, "text": " I think it's worth pointing out that there's two different", "tokens": [50738, 286, 519, 309, 311, 3163, 12166, 484, 300, 456, 311, 732, 819, 50892], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1945, "seek": 477720, "start": 4787.76, "end": 4788.599999999999, "text": " test sets.", "tokens": [50892, 1500, 6352, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1946, "seek": 477720, "start": 4788.599999999999, "end": 4790.76, "text": " There is a public test set that's in the public", "tokens": [50934, 821, 307, 257, 1908, 1500, 992, 300, 311, 294, 264, 1908, 51042], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1947, "seek": 477720, "start": 4790.76, "end": 4793.72, "text": " GitHub repository that anyone can use to train, you know,", "tokens": [51042, 23331, 25841, 300, 2878, 393, 764, 281, 3847, 11, 291, 458, 11, 51190], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1948, "seek": 477720, "start": 4793.72, "end": 4796.4, "text": " put it in an open API call, whatever you'd like to do.", "tokens": [51190, 829, 309, 294, 364, 1269, 9362, 818, 11, 2035, 291, 1116, 411, 281, 360, 13, 51324], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1949, "seek": 477720, "start": 4796.4, "end": 4797.72, "text": " And then there's the private test set,", "tokens": [51324, 400, 550, 456, 311, 264, 4551, 1500, 992, 11, 51390], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1950, "seek": 477720, "start": 4797.72, "end": 4799.32, "text": " which is the 100 that is actually measuring", "tokens": [51390, 597, 307, 264, 2319, 300, 307, 767, 13389, 51470], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1951, "seek": 477720, "start": 4799.32, "end": 4800.72, "text": " the state of the art.", "tokens": [51470, 264, 1785, 295, 264, 1523, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1952, "seek": 477720, "start": 4800.72, "end": 4803.4, "text": " So I think it is pretty open and interesting to have folks", "tokens": [51540, 407, 286, 519, 309, 307, 1238, 1269, 293, 1880, 281, 362, 4024, 51674], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1953, "seek": 477720, "start": 4803.4, "end": 4805.92, "text": " attempt to at least use the public test set and go try it.", "tokens": [51674, 5217, 281, 412, 1935, 764, 264, 1908, 1500, 992, 293, 352, 853, 309, 13, 51800], "temperature": 0.0, "avg_logprob": -0.13639736775332276, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.001133499201387167}, {"id": 1954, "seek": 480592, "start": 4805.92, "end": 4809.16, "text": " Now, there is an asterisk on any score that's reported on", "tokens": [50364, 823, 11, 456, 307, 364, 257, 3120, 7797, 322, 604, 6175, 300, 311, 7055, 322, 50526], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1955, "seek": 480592, "start": 4809.16, "end": 4811.32, "text": " against the public test set because it is public.", "tokens": [50526, 1970, 264, 1908, 1500, 992, 570, 309, 307, 1908, 13, 50634], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1956, "seek": 480592, "start": 4811.32, "end": 4813.32, "text": " It could have leaked into the training data.", "tokens": [50634, 467, 727, 362, 31779, 666, 264, 3097, 1412, 13, 50734], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1957, "seek": 480592, "start": 4813.32, "end": 4815.24, "text": " And this is actually what people are already doing.", "tokens": [50734, 400, 341, 307, 767, 437, 561, 366, 1217, 884, 13, 50830], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1958, "seek": 480592, "start": 4815.24, "end": 4818.76, "text": " Like you can already try to prompt one of the best models", "tokens": [50830, 1743, 291, 393, 1217, 853, 281, 12391, 472, 295, 264, 1151, 5245, 51006], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1959, "seek": 480592, "start": 4818.76, "end": 4822.88, "text": " like the latest Jaminar, the latest GPT-4 with tasks", "tokens": [51006, 411, 264, 6792, 10372, 6470, 11, 264, 6792, 26039, 51, 12, 19, 365, 9608, 51212], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1960, "seek": 480592, "start": 4822.88, "end": 4824.36, "text": " from the public evaluation set.", "tokens": [51212, 490, 264, 1908, 13344, 992, 13, 51286], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1961, "seek": 480592, "start": 4824.36, "end": 4827.4, "text": " And you know, again, the primary set, these tasks", "tokens": [51286, 400, 291, 458, 11, 797, 11, 264, 6194, 992, 11, 613, 9608, 51438], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1962, "seek": 480592, "start": 4827.4, "end": 4830.4, "text": " are available as JSON files on GitHub.", "tokens": [51438, 366, 2435, 382, 31828, 7098, 322, 23331, 13, 51588], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1963, "seek": 480592, "start": 4830.4, "end": 4832.4400000000005, "text": " These models are also trained on GitHub.", "tokens": [51588, 1981, 5245, 366, 611, 8895, 322, 23331, 13, 51690], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1964, "seek": 480592, "start": 4832.4400000000005, "end": 4834.8, "text": " So they're actually trained on these tasks.", "tokens": [51690, 407, 436, 434, 767, 8895, 322, 613, 9608, 13, 51808], "temperature": 0.0, "avg_logprob": -0.21643225119931855, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.0005356943584047258}, {"id": 1965, "seek": 483480, "start": 4835.72, "end": 4838.96, "text": " And yeah, that kind of creates uncertainty about", "tokens": [50410, 400, 1338, 11, 300, 733, 295, 7829, 15697, 466, 50572], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1966, "seek": 483480, "start": 4838.96, "end": 4840.96, "text": " if they can actually solve some of the tasks,", "tokens": [50572, 498, 436, 393, 767, 5039, 512, 295, 264, 9608, 11, 50672], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1967, "seek": 483480, "start": 4840.96, "end": 4843.96, "text": " is that because they memorized the answer or not.", "tokens": [50672, 307, 300, 570, 436, 46677, 264, 1867, 420, 406, 13, 50822], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1968, "seek": 483480, "start": 4843.96, "end": 4847.08, "text": " You know, maybe you would be better off trying to create", "tokens": [50822, 509, 458, 11, 1310, 291, 576, 312, 1101, 766, 1382, 281, 1884, 50978], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1969, "seek": 483480, "start": 4847.08, "end": 4853.08, "text": " your own private, arc-like, very novel test set.", "tokens": [50978, 428, 1065, 4551, 11, 10346, 12, 4092, 11, 588, 7613, 1500, 992, 13, 51278], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1970, "seek": 483480, "start": 4853.12, "end": 4854.72, "text": " Don't make the task difficult.", "tokens": [51280, 1468, 380, 652, 264, 5633, 2252, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1971, "seek": 483480, "start": 4854.72, "end": 4855.76, "text": " Don't make them complex.", "tokens": [51360, 1468, 380, 652, 552, 3997, 13, 51412], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1972, "seek": 483480, "start": 4855.76, "end": 4857.16, "text": " Make them very obvious for humans.", "tokens": [51412, 4387, 552, 588, 6322, 337, 6255, 13, 51482], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1973, "seek": 483480, "start": 4857.16, "end": 4860.84, "text": " But make sure to make them original as much as possible.", "tokens": [51482, 583, 652, 988, 281, 652, 552, 3380, 382, 709, 382, 1944, 13, 51666], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1974, "seek": 483480, "start": 4860.84, "end": 4862.4800000000005, "text": " Make them unique, different.", "tokens": [51666, 4387, 552, 3845, 11, 819, 13, 51748], "temperature": 0.0, "avg_logprob": -0.17815630059493215, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.0009196579921990633}, {"id": 1975, "seek": 486248, "start": 4862.48, "end": 4866.719999999999, "text": " And see how much your GPT-4 and so on GPT-5 does on them.", "tokens": [50364, 400, 536, 577, 709, 428, 26039, 51, 12, 19, 293, 370, 322, 26039, 51, 12, 20, 775, 322, 552, 13, 50576], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1976, "seek": 486248, "start": 4866.719999999999, "end": 4868.719999999999, "text": " Well, they're having tests on whether these models", "tokens": [50576, 1042, 11, 436, 434, 1419, 6921, 322, 1968, 613, 5245, 50676], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1977, "seek": 486248, "start": 4868.719999999999, "end": 4871.04, "text": " are being overtrained on these benchmarks.", "tokens": [50676, 366, 885, 17038, 31774, 322, 613, 43751, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1978, "seek": 486248, "start": 4871.04, "end": 4874.12, "text": " Scale recently did this where on the GSM-", "tokens": [50792, 42999, 3938, 630, 341, 689, 322, 264, 460, 26693, 12, 50946], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1979, "seek": 486248, "start": 4874.12, "end": 4874.959999999999, "text": " That's really interesting.", "tokens": [50946, 663, 311, 534, 1880, 13, 50988], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1980, "seek": 486248, "start": 4874.959999999999, "end": 4877.5599999999995, "text": " AK, they basically replicated the benchmark", "tokens": [50988, 24789, 11, 436, 1936, 46365, 264, 18927, 51118], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1981, "seek": 486248, "start": 4877.5599999999995, "end": 4878.839999999999, "text": " with different questions.", "tokens": [51118, 365, 819, 1651, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1982, "seek": 486248, "start": 4878.839999999999, "end": 4880.48, "text": " And so some of the models actually were extremely", "tokens": [51182, 400, 370, 512, 295, 264, 5245, 767, 645, 4664, 51264], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1983, "seek": 486248, "start": 4880.48, "end": 4884.0, "text": " overfit on the benchmark like Mistral and so forth.", "tokens": [51264, 670, 6845, 322, 264, 18927, 411, 20166, 2155, 293, 370, 5220, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1984, "seek": 486248, "start": 4884.0, "end": 4888.599999999999, "text": " And but the frontier models, Claude and GPT actually did", "tokens": [51440, 400, 457, 264, 35853, 5245, 11, 12947, 2303, 293, 26039, 51, 767, 630, 51670], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1985, "seek": 486248, "start": 4888.599999999999, "end": 4890.759999999999, "text": " as well on their novel benchmark that they did", "tokens": [51670, 382, 731, 322, 641, 7613, 18927, 300, 436, 630, 51778], "temperature": 0.0, "avg_logprob": -0.2133429432643279, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.0010319410357624292}, {"id": 1986, "seek": 489076, "start": 4890.76, "end": 4892.16, "text": " on the specific questions that were", "tokens": [50364, 322, 264, 2685, 1651, 300, 645, 50434], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1987, "seek": 489076, "start": 4892.16, "end": 4895.16, "text": " in the existing public benchmark.", "tokens": [50434, 294, 264, 6741, 1908, 18927, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1988, "seek": 489076, "start": 4895.16, "end": 4897.88, "text": " So I would be relatively optimistic about them", "tokens": [50584, 407, 286, 576, 312, 7226, 19397, 466, 552, 50720], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1989, "seek": 489076, "start": 4897.88, "end": 4900.320000000001, "text": " just sort of training on the JSON.", "tokens": [50720, 445, 1333, 295, 3097, 322, 264, 31828, 13, 50842], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1990, "seek": 489076, "start": 4900.320000000001, "end": 4904.08, "text": " I was joking with Mike that you should allow API access", "tokens": [50842, 286, 390, 17396, 365, 6602, 300, 291, 820, 2089, 9362, 2105, 51030], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1991, "seek": 489076, "start": 4904.08, "end": 4909.08, "text": " but sort of keep an even more private validation set", "tokens": [51030, 457, 1333, 295, 1066, 364, 754, 544, 4551, 24071, 992, 51280], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1992, "seek": 489076, "start": 4909.16, "end": 4911.4400000000005, "text": " of these arc questions.", "tokens": [51284, 295, 613, 10346, 1651, 13, 51398], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1993, "seek": 489076, "start": 4911.4400000000005, "end": 4913.76, "text": " And so allow API access, people can sort of play", "tokens": [51398, 400, 370, 2089, 9362, 2105, 11, 561, 393, 1333, 295, 862, 51514], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1994, "seek": 489076, "start": 4913.76, "end": 4916.400000000001, "text": " with GPT-4 scaffolding to enter into this contest.", "tokens": [51514, 365, 26039, 51, 12, 19, 44094, 278, 281, 3242, 666, 341, 10287, 13, 51646], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1995, "seek": 489076, "start": 4916.400000000001, "end": 4918.96, "text": " And if it turns out maybe later on you run the validation", "tokens": [51646, 400, 498, 309, 4523, 484, 1310, 1780, 322, 291, 1190, 264, 24071, 51774], "temperature": 0.0, "avg_logprob": -0.12122366258076259, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.0013667774619534612}, {"id": 1996, "seek": 491896, "start": 4918.96, "end": 4921.44, "text": " set on the API and if it performs worse", "tokens": [50364, 992, 322, 264, 9362, 293, 498, 309, 26213, 5324, 50488], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 1997, "seek": 491896, "start": 4921.44, "end": 4923.88, "text": " than the test set that you allowed the API access", "tokens": [50488, 813, 264, 1500, 992, 300, 291, 4350, 264, 9362, 2105, 50610], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 1998, "seek": 491896, "start": 4923.88, "end": 4927.16, "text": " to originally, that means that open AI is training", "tokens": [50610, 281, 7993, 11, 300, 1355, 300, 1269, 7318, 307, 3097, 50774], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 1999, "seek": 491896, "start": 4927.16, "end": 4929.76, "text": " on your API calls and you like go public with this", "tokens": [50774, 322, 428, 9362, 5498, 293, 291, 411, 352, 1908, 365, 341, 50904], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2000, "seek": 491896, "start": 4929.76, "end": 4930.76, "text": " and show them like, oh my God,", "tokens": [50904, 293, 855, 552, 411, 11, 1954, 452, 1265, 11, 50954], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2001, "seek": 491896, "start": 4930.76, "end": 4933.08, "text": " they've like leaked your data.", "tokens": [50954, 436, 600, 411, 31779, 428, 1412, 13, 51070], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2002, "seek": 491896, "start": 4933.08, "end": 4935.68, "text": " We do want to make, we want to evolve the arc data set.", "tokens": [51070, 492, 360, 528, 281, 652, 11, 321, 528, 281, 16693, 264, 10346, 1412, 992, 13, 51200], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2003, "seek": 491896, "start": 4935.68, "end": 4937.56, "text": " Like that is a goal that we want to do.", "tokens": [51200, 1743, 300, 307, 257, 3387, 300, 321, 528, 281, 360, 13, 51294], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2004, "seek": 491896, "start": 4937.56, "end": 4939.52, "text": " I think Francois you mentioned, you know, it's not perfect.", "tokens": [51294, 286, 519, 34695, 271, 291, 2835, 11, 291, 458, 11, 309, 311, 406, 2176, 13, 51392], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2005, "seek": 491896, "start": 4939.52, "end": 4942.08, "text": " Yeah, no, arc is not perfect for perfect benchmark.", "tokens": [51392, 865, 11, 572, 11, 10346, 307, 406, 2176, 337, 2176, 18927, 13, 51520], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2006, "seek": 491896, "start": 4942.08, "end": 4944.08, "text": " I mean, I made it like four years ago", "tokens": [51520, 286, 914, 11, 286, 1027, 309, 411, 1451, 924, 2057, 51620], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2007, "seek": 491896, "start": 4944.08, "end": 4946.72, "text": " over four years ago, almost five now.", "tokens": [51620, 670, 1451, 924, 2057, 11, 1920, 1732, 586, 13, 51752], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2008, "seek": 491896, "start": 4946.72, "end": 4948.84, "text": " This was in a time before LMS.", "tokens": [51752, 639, 390, 294, 257, 565, 949, 441, 10288, 13, 51858], "temperature": 0.0, "avg_logprob": -0.20084260882753313, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0019566239789128304}, {"id": 2009, "seek": 494884, "start": 4948.84, "end": 4951.8, "text": " And I think we learned a lot actually since", "tokens": [50364, 400, 286, 519, 321, 3264, 257, 688, 767, 1670, 50512], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2010, "seek": 494884, "start": 4951.8, "end": 4954.2, "text": " about what potential flaws there might be.", "tokens": [50512, 466, 437, 3995, 27108, 456, 1062, 312, 13, 50632], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2011, "seek": 494884, "start": 4954.2, "end": 4957.56, "text": " I think there is some redundancy in the set of tasks", "tokens": [50632, 286, 519, 456, 307, 512, 27830, 6717, 294, 264, 992, 295, 9608, 50800], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2012, "seek": 494884, "start": 4957.56, "end": 4960.2, "text": " which is of course against the goals of the benchmark.", "tokens": [50800, 597, 307, 295, 1164, 1970, 264, 5493, 295, 264, 18927, 13, 50932], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2013, "seek": 494884, "start": 4960.2, "end": 4962.68, "text": " Every task is supposed to be unique in practice.", "tokens": [50932, 2048, 5633, 307, 3442, 281, 312, 3845, 294, 3124, 13, 51056], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2014, "seek": 494884, "start": 4962.68, "end": 4964.0, "text": " That's not quite true.", "tokens": [51056, 663, 311, 406, 1596, 2074, 13, 51122], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2015, "seek": 494884, "start": 4964.0, "end": 4967.08, "text": " I think there's also, every task is supposed", "tokens": [51122, 286, 519, 456, 311, 611, 11, 633, 5633, 307, 3442, 51276], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2016, "seek": 494884, "start": 4967.08, "end": 4969.76, "text": " to be very novel, but in practice, they might not be.", "tokens": [51276, 281, 312, 588, 7613, 11, 457, 294, 3124, 11, 436, 1062, 406, 312, 13, 51410], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2017, "seek": 494884, "start": 4969.76, "end": 4972.4400000000005, "text": " They might be structurally similar to something", "tokens": [51410, 814, 1062, 312, 6594, 6512, 2531, 281, 746, 51544], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2018, "seek": 494884, "start": 4972.4400000000005, "end": 4974.68, "text": " that you might find online somewhere.", "tokens": [51544, 300, 291, 1062, 915, 2950, 4079, 13, 51656], "temperature": 0.0, "avg_logprob": -0.12071104946299496, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.00022238795645534992}, {"id": 2019, "seek": 497468, "start": 4974.68, "end": 4976.4800000000005, "text": " So we want to keep iterating", "tokens": [50364, 407, 321, 528, 281, 1066, 17138, 990, 50454], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2020, "seek": 497468, "start": 4976.4800000000005, "end": 4980.240000000001, "text": " and release an arc two version later this year.", "tokens": [50454, 293, 4374, 364, 10346, 732, 3037, 1780, 341, 1064, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2021, "seek": 497468, "start": 4980.240000000001, "end": 4981.72, "text": " And I think when we do that,", "tokens": [50642, 400, 286, 519, 562, 321, 360, 300, 11, 50716], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2022, "seek": 497468, "start": 4981.72, "end": 4986.72, "text": " we're gonna want to make the old private test set available.", "tokens": [50716, 321, 434, 799, 528, 281, 652, 264, 1331, 4551, 1500, 992, 2435, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2023, "seek": 497468, "start": 4986.8, "end": 4988.72, "text": " So maybe we won't be releasing it publicly,", "tokens": [50970, 407, 1310, 321, 1582, 380, 312, 16327, 309, 14843, 11, 51066], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2024, "seek": 497468, "start": 4988.72, "end": 4993.400000000001, "text": " but what we could do is just create a test server", "tokens": [51066, 457, 437, 321, 727, 360, 307, 445, 1884, 257, 1500, 7154, 51300], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2025, "seek": 497468, "start": 4993.400000000001, "end": 4996.52, "text": " where you can query, get a task, you submit a solution,", "tokens": [51300, 689, 291, 393, 14581, 11, 483, 257, 5633, 11, 291, 10315, 257, 3827, 11, 51456], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2026, "seek": 497468, "start": 4996.52, "end": 4998.360000000001, "text": " and of course you can use whatever frontier model", "tokens": [51456, 293, 295, 1164, 291, 393, 764, 2035, 35853, 2316, 51548], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2027, "seek": 497468, "start": 4998.360000000001, "end": 4999.64, "text": " you want there.", "tokens": [51548, 291, 528, 456, 13, 51612], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2028, "seek": 497468, "start": 4999.64, "end": 5002.72, "text": " So that way, because you actually have to query this API,", "tokens": [51612, 407, 300, 636, 11, 570, 291, 767, 362, 281, 14581, 341, 9362, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1160157337661617, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.0010871326085180044}, {"id": 2029, "seek": 500272, "start": 5002.76, "end": 5006.16, "text": " you're making sure that no one is gonna buy accident train", "tokens": [50366, 291, 434, 1455, 988, 300, 572, 472, 307, 799, 2256, 6398, 3847, 50536], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2030, "seek": 500272, "start": 5006.16, "end": 5007.0, "text": " on this data.", "tokens": [50536, 322, 341, 1412, 13, 50578], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2031, "seek": 500272, "start": 5007.0, "end": 5009.8, "text": " It's unlike like the current public article", "tokens": [50578, 467, 311, 8343, 411, 264, 2190, 1908, 7222, 50718], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2032, "seek": 500272, "start": 5009.8, "end": 5011.240000000001, "text": " which is literally on GitHub.", "tokens": [50718, 597, 307, 3736, 322, 23331, 13, 50790], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2033, "seek": 500272, "start": 5011.240000000001, "end": 5013.16, "text": " So there's no question about whether the models", "tokens": [50790, 407, 456, 311, 572, 1168, 466, 1968, 264, 5245, 50886], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2034, "seek": 500272, "start": 5013.16, "end": 5014.0, "text": " are actually trained on it.", "tokens": [50886, 366, 767, 8895, 322, 309, 13, 50928], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2035, "seek": 500272, "start": 5014.0, "end": 5016.4400000000005, "text": " Yes, they are because they're trained on GitHub.", "tokens": [50928, 1079, 11, 436, 366, 570, 436, 434, 8895, 322, 23331, 13, 51050], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2036, "seek": 500272, "start": 5016.4400000000005, "end": 5019.52, "text": " So by sort of like gating access", "tokens": [51050, 407, 538, 1333, 295, 411, 290, 990, 2105, 51204], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2037, "seek": 500272, "start": 5019.52, "end": 5022.52, "text": " to querying this API with a various issue.", "tokens": [51204, 281, 7083, 1840, 341, 9362, 365, 257, 3683, 2734, 13, 51354], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2038, "seek": 500272, "start": 5022.52, "end": 5024.12, "text": " And then we would see, you know,", "tokens": [51354, 400, 550, 321, 576, 536, 11, 291, 458, 11, 51434], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2039, "seek": 500272, "start": 5024.12, "end": 5026.92, "text": " for people who actually wanna try", "tokens": [51434, 337, 561, 567, 767, 1948, 853, 51574], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2040, "seek": 500272, "start": 5026.92, "end": 5028.12, "text": " whatever technique they have in mind", "tokens": [51574, 2035, 6532, 436, 362, 294, 1575, 51634], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2041, "seek": 500272, "start": 5028.12, "end": 5030.76, "text": " using whatever resources they want,", "tokens": [51634, 1228, 2035, 3593, 436, 528, 11, 51766], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2042, "seek": 500272, "start": 5030.76, "end": 5032.6, "text": " that would be a way for them to get an answer.", "tokens": [51766, 300, 576, 312, 257, 636, 337, 552, 281, 483, 364, 1867, 13, 51858], "temperature": 0.0, "avg_logprob": -0.20046306156611943, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0007494846940971911}, {"id": 2043, "seek": 503260, "start": 5032.64, "end": 5034.160000000001, "text": " I wonder what might happen.", "tokens": [50366, 286, 2441, 437, 1062, 1051, 13, 50442], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2044, "seek": 503260, "start": 5034.160000000001, "end": 5035.320000000001, "text": " I'm not sure.", "tokens": [50442, 286, 478, 406, 988, 13, 50500], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2045, "seek": 503260, "start": 5035.320000000001, "end": 5038.52, "text": " One answer is that they come up with a whole new algorithm", "tokens": [50500, 1485, 1867, 307, 300, 436, 808, 493, 365, 257, 1379, 777, 9284, 50660], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2046, "seek": 503260, "start": 5038.52, "end": 5042.320000000001, "text": " for AI with some explicit program synthesis", "tokens": [50660, 337, 7318, 365, 512, 13691, 1461, 30252, 50850], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2047, "seek": 503260, "start": 5042.320000000001, "end": 5043.84, "text": " that now we're on a new track.", "tokens": [50850, 300, 586, 321, 434, 322, 257, 777, 2837, 13, 50926], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2048, "seek": 503260, "start": 5043.84, "end": 5046.68, "text": " And another is they did something hacky", "tokens": [50926, 400, 1071, 307, 436, 630, 746, 10339, 88, 51068], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2049, "seek": 503260, "start": 5046.68, "end": 5050.320000000001, "text": " with the existing models in a way that actually is valid,", "tokens": [51068, 365, 264, 6741, 5245, 294, 257, 636, 300, 767, 307, 7363, 11, 51250], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2050, "seek": 503260, "start": 5050.320000000001, "end": 5052.8, "text": " which reveals that movie intelligence is more", "tokens": [51250, 597, 20893, 300, 3169, 7599, 307, 544, 51374], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2051, "seek": 503260, "start": 5052.8, "end": 5055.320000000001, "text": " of getting things to the right part of the distribution,", "tokens": [51374, 295, 1242, 721, 281, 264, 558, 644, 295, 264, 7316, 11, 51500], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2052, "seek": 503260, "start": 5055.320000000001, "end": 5056.64, "text": " but then it can reason.", "tokens": [51500, 457, 550, 309, 393, 1778, 13, 51566], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2053, "seek": 503260, "start": 5056.64, "end": 5059.76, "text": " And in that world, I guess that will be interesting.", "tokens": [51566, 400, 294, 300, 1002, 11, 286, 2041, 300, 486, 312, 1880, 13, 51722], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2054, "seek": 503260, "start": 5059.76, "end": 5061.4400000000005, "text": " And maybe that'll indicate that, you know,", "tokens": [51722, 400, 1310, 300, 603, 13330, 300, 11, 291, 458, 11, 51806], "temperature": 0.0, "avg_logprob": -0.14529980957963085, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0005702459020540118}, {"id": 2055, "seek": 506144, "start": 5061.48, "end": 5063.24, "text": " you had to do something hacky with current models", "tokens": [50366, 291, 632, 281, 360, 746, 10339, 88, 365, 2190, 5245, 50454], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2056, "seek": 506144, "start": 5063.24, "end": 5064.08, "text": " as they get better,", "tokens": [50454, 382, 436, 483, 1101, 11, 50496], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2057, "seek": 506144, "start": 5064.08, "end": 5065.839999999999, "text": " you won't have to do something hacky.", "tokens": [50496, 291, 1582, 380, 362, 281, 360, 746, 10339, 88, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2058, "seek": 506144, "start": 5067.12, "end": 5069.2, "text": " I'm also gonna be very curious to see", "tokens": [50648, 286, 478, 611, 799, 312, 588, 6369, 281, 536, 50752], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2059, "seek": 506144, "start": 5069.2, "end": 5070.719999999999, "text": " how these multimodal models,", "tokens": [50752, 577, 613, 32972, 378, 304, 5245, 11, 50828], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2060, "seek": 506144, "start": 5070.719999999999, "end": 5073.839999999999, "text": " if they will perform natively much better at arc like tests.", "tokens": [50828, 498, 436, 486, 2042, 8470, 356, 709, 1101, 412, 10346, 411, 6921, 13, 50984], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2061, "seek": 506144, "start": 5073.839999999999, "end": 5075.36, "text": " If arc survives three months from here,", "tokens": [50984, 759, 10346, 46231, 1045, 2493, 490, 510, 11, 51060], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2062, "seek": 506144, "start": 5075.36, "end": 5077.0, "text": " we'll blow up the price.", "tokens": [51060, 321, 603, 6327, 493, 264, 3218, 13, 51142], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2063, "seek": 506144, "start": 5077.0, "end": 5079.0, "text": " I think we're about to make a really important moment", "tokens": [51142, 286, 519, 321, 434, 466, 281, 652, 257, 534, 1021, 1623, 51242], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2064, "seek": 506144, "start": 5079.0, "end": 5081.639999999999, "text": " of like contact with reality by blowing up the price,", "tokens": [51242, 295, 411, 3385, 365, 4103, 538, 15068, 493, 264, 3218, 11, 51374], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2065, "seek": 506144, "start": 5081.639999999999, "end": 5083.36, "text": " putting a much big price pool against it.", "tokens": [51374, 3372, 257, 709, 955, 3218, 7005, 1970, 309, 13, 51460], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2066, "seek": 506144, "start": 5083.36, "end": 5084.4, "text": " We're gonna learn really quickly", "tokens": [51460, 492, 434, 799, 1466, 534, 2661, 51512], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2067, "seek": 506144, "start": 5084.4, "end": 5086.599999999999, "text": " if there's like low hanging fruit of ideas.", "tokens": [51512, 498, 456, 311, 411, 2295, 8345, 6773, 295, 3487, 13, 51622], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2068, "seek": 506144, "start": 5086.599999999999, "end": 5087.919999999999, "text": " Again, I think new ideas are needed.", "tokens": [51622, 3764, 11, 286, 519, 777, 3487, 366, 2978, 13, 51688], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2069, "seek": 506144, "start": 5087.919999999999, "end": 5089.0, "text": " I think anyone listening this", "tokens": [51688, 286, 519, 2878, 4764, 341, 51742], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2070, "seek": 506144, "start": 5089.0, "end": 5091.16, "text": " might have the idea in their head.", "tokens": [51742, 1062, 362, 264, 1558, 294, 641, 1378, 13, 51850], "temperature": 0.0, "avg_logprob": -0.10167572110198266, "compression_ratio": 1.8074712643678161, "no_speech_prob": 0.003944701980799437}, {"id": 2071, "seek": 509116, "start": 5091.16, "end": 5093.5199999999995, "text": " And I'd encourage everyone to like give it a try.", "tokens": [50364, 400, 286, 1116, 5373, 1518, 281, 411, 976, 309, 257, 853, 13, 50482], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2072, "seek": 509116, "start": 5093.5199999999995, "end": 5095.5199999999995, "text": " And I think as time goes on,", "tokens": [50482, 400, 286, 519, 382, 565, 1709, 322, 11, 50582], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2073, "seek": 509116, "start": 5095.5199999999995, "end": 5096.92, "text": " that adds strength to the argument", "tokens": [50582, 300, 10860, 3800, 281, 264, 6770, 50652], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2074, "seek": 509116, "start": 5096.92, "end": 5099.28, "text": " that like we sort of stall that in progress", "tokens": [50652, 300, 411, 321, 1333, 295, 19633, 300, 294, 4205, 50770], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2075, "seek": 509116, "start": 5099.28, "end": 5100.92, "text": " and that new ideas are necessary to be dark.", "tokens": [50770, 293, 300, 777, 3487, 366, 4818, 281, 312, 2877, 13, 50852], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2076, "seek": 509116, "start": 5100.92, "end": 5103.599999999999, "text": " Yeah, that's the point of having a money price", "tokens": [50852, 865, 11, 300, 311, 264, 935, 295, 1419, 257, 1460, 3218, 50986], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2077, "seek": 509116, "start": 5103.599999999999, "end": 5106.12, "text": " is that you attract more people,", "tokens": [50986, 307, 300, 291, 5049, 544, 561, 11, 51112], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2078, "seek": 509116, "start": 5106.12, "end": 5107.72, "text": " you get them to try to solve it.", "tokens": [51112, 291, 483, 552, 281, 853, 281, 5039, 309, 13, 51192], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2079, "seek": 509116, "start": 5107.72, "end": 5109.96, "text": " And if there's a easy way to hack the benchmark", "tokens": [51192, 400, 498, 456, 311, 257, 1858, 636, 281, 10339, 264, 18927, 51304], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2080, "seek": 509116, "start": 5109.96, "end": 5111.48, "text": " that reveals that the benchmark is valid,", "tokens": [51304, 300, 20893, 300, 264, 18927, 307, 7363, 11, 51380], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2081, "seek": 509116, "start": 5111.48, "end": 5112.639999999999, "text": " then you're gonna know about it.", "tokens": [51380, 550, 291, 434, 799, 458, 466, 309, 13, 51438], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2082, "seek": 509116, "start": 5112.639999999999, "end": 5113.5199999999995, "text": " In fact, that was the point", "tokens": [51438, 682, 1186, 11, 300, 390, 264, 935, 51482], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2083, "seek": 509116, "start": 5113.5199999999995, "end": 5118.2, "text": " of the original Carol competition back in 2020 for arc.", "tokens": [51482, 295, 264, 3380, 7925, 6211, 646, 294, 4808, 337, 10346, 13, 51716], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2084, "seek": 509116, "start": 5119.04, "end": 5120.28, "text": " I was running this competition", "tokens": [51758, 286, 390, 2614, 341, 6211, 51820], "temperature": 0.0, "avg_logprob": -0.15022555150483785, "compression_ratio": 1.766773162939297, "no_speech_prob": 4.264573726686649e-05}, {"id": 2085, "seek": 512028, "start": 5120.32, "end": 5122.44, "text": " because I had released this dataset", "tokens": [50366, 570, 286, 632, 4736, 341, 28872, 50472], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2086, "seek": 512028, "start": 5122.44, "end": 5126.639999999999, "text": " and I wanted to know if it was hackable, if you could cheat.", "tokens": [50472, 293, 286, 1415, 281, 458, 498, 309, 390, 10339, 712, 11, 498, 291, 727, 17470, 13, 50682], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2087, "seek": 512028, "start": 5126.639999999999, "end": 5128.8, "text": " So there was a small money price at the time,", "tokens": [50682, 407, 456, 390, 257, 1359, 1460, 3218, 412, 264, 565, 11, 50790], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2088, "seek": 512028, "start": 5128.8, "end": 5130.5199999999995, "text": " there was like 20K.", "tokens": [50790, 456, 390, 411, 945, 42, 13, 50876], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2089, "seek": 512028, "start": 5130.5199999999995, "end": 5132.36, "text": " And this was right around the same time", "tokens": [50876, 400, 341, 390, 558, 926, 264, 912, 565, 50968], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2090, "seek": 512028, "start": 5132.36, "end": 5134.719999999999, "text": " as GPT-3 was released.", "tokens": [50968, 382, 26039, 51, 12, 18, 390, 4736, 13, 51086], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2091, "seek": 512028, "start": 5134.719999999999, "end": 5137.5599999999995, "text": " So people of course tried GPT-3 on the public data,", "tokens": [51086, 407, 561, 295, 1164, 3031, 26039, 51, 12, 18, 322, 264, 1908, 1412, 11, 51228], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2092, "seek": 512028, "start": 5137.5599999999995, "end": 5138.48, "text": " it scored zero.", "tokens": [51228, 309, 18139, 4018, 13, 51274], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2093, "seek": 512028, "start": 5139.5599999999995, "end": 5142.5599999999995, "text": " But I think what the first context", "tokens": [51328, 583, 286, 519, 437, 264, 700, 4319, 51478], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2094, "seek": 512028, "start": 5142.5599999999995, "end": 5145.24, "text": " the first context taught us is that", "tokens": [51478, 264, 700, 4319, 5928, 505, 307, 300, 51612], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2095, "seek": 512028, "start": 5145.24, "end": 5148.16, "text": " there is no obvious shortcuts, right?", "tokens": [51612, 456, 307, 572, 6322, 34620, 11, 558, 30, 51758], "temperature": 0.0, "avg_logprob": -0.22452500287224264, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0003295288479421288}, {"id": 2096, "seek": 514816, "start": 5149.16, "end": 5150.68, "text": " And well, now there's more money,", "tokens": [50414, 400, 731, 11, 586, 456, 311, 544, 1460, 11, 50490], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2097, "seek": 514816, "start": 5150.68, "end": 5153.96, "text": " there's gonna be more people looking into it.", "tokens": [50490, 456, 311, 799, 312, 544, 561, 1237, 666, 309, 13, 50654], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2098, "seek": 514816, "start": 5153.96, "end": 5156.12, "text": " Well, we're gonna find out,", "tokens": [50654, 1042, 11, 321, 434, 799, 915, 484, 11, 50762], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2099, "seek": 514816, "start": 5156.12, "end": 5158.72, "text": " we're gonna see if the benchmark is gonna survive.", "tokens": [50762, 321, 434, 799, 536, 498, 264, 18927, 307, 799, 7867, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2100, "seek": 514816, "start": 5158.72, "end": 5162.4, "text": " And you know, if we end up with a solution", "tokens": [50892, 400, 291, 458, 11, 498, 321, 917, 493, 365, 257, 3827, 51076], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2101, "seek": 514816, "start": 5162.4, "end": 5165.84, "text": " that is not like trying to brute force", "tokens": [51076, 300, 307, 406, 411, 1382, 281, 47909, 3464, 51248], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2102, "seek": 514816, "start": 5165.84, "end": 5167.4, "text": " the space of possible arc tasks", "tokens": [51248, 264, 1901, 295, 1944, 10346, 9608, 51326], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2103, "seek": 514816, "start": 5167.4, "end": 5169.92, "text": " that's just trained on core knowledge,", "tokens": [51326, 300, 311, 445, 8895, 322, 4965, 3601, 11, 51452], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2104, "seek": 514816, "start": 5169.92, "end": 5173.8, "text": " I don't think it's necessarily gonna be in and by itself, AGI,", "tokens": [51452, 286, 500, 380, 519, 309, 311, 4725, 799, 312, 294, 293, 538, 2564, 11, 316, 26252, 11, 51646], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2105, "seek": 514816, "start": 5173.8, "end": 5176.44, "text": " but it's probably gonna be a huge milestone", "tokens": [51646, 457, 309, 311, 1391, 799, 312, 257, 2603, 28048, 51778], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2106, "seek": 514816, "start": 5176.44, "end": 5178.04, "text": " on the way to AGI.", "tokens": [51778, 322, 264, 636, 281, 316, 26252, 13, 51858], "temperature": 0.0, "avg_logprob": -0.14804354310035706, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.00029186534811742604}, {"id": 2107, "seek": 517804, "start": 5178.12, "end": 5183.12, "text": " Because what it represents is the ability to synthesize,", "tokens": [50368, 1436, 437, 309, 8855, 307, 264, 3485, 281, 26617, 1125, 11, 50618], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2108, "seek": 517804, "start": 5185.8, "end": 5188.56, "text": " task a problem solving program", "tokens": [50752, 5633, 257, 1154, 12606, 1461, 50890], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2109, "seek": 517804, "start": 5188.56, "end": 5191.92, "text": " from just two or three examples.", "tokens": [50890, 490, 445, 732, 420, 1045, 5110, 13, 51058], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2110, "seek": 517804, "start": 5191.92, "end": 5195.08, "text": " And that alone is a new way to program.", "tokens": [51058, 400, 300, 3312, 307, 257, 777, 636, 281, 1461, 13, 51216], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2111, "seek": 517804, "start": 5195.08, "end": 5198.0, "text": " It's an entirely new paradigm for software development", "tokens": [51216, 467, 311, 364, 7696, 777, 24709, 337, 4722, 3250, 51362], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2112, "seek": 517804, "start": 5198.0, "end": 5199.76, "text": " where you can start programming", "tokens": [51362, 689, 291, 393, 722, 9410, 51450], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2113, "seek": 517804, "start": 5199.76, "end": 5201.72, "text": " potentially quite complex programs", "tokens": [51450, 7263, 1596, 3997, 4268, 51548], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2114, "seek": 517804, "start": 5201.72, "end": 5204.0, "text": " that will generalize very well.", "tokens": [51548, 300, 486, 2674, 1125, 588, 731, 13, 51662], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2115, "seek": 517804, "start": 5204.0, "end": 5206.68, "text": " And instead of programming them by coming up", "tokens": [51662, 400, 2602, 295, 9410, 552, 538, 1348, 493, 51796], "temperature": 0.0, "avg_logprob": -0.12734194689018782, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0005383560783229768}, {"id": 2116, "seek": 520668, "start": 5206.72, "end": 5209.92, "text": " with the shape of the program in your mind", "tokens": [50366, 365, 264, 3909, 295, 264, 1461, 294, 428, 1575, 50526], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2117, "seek": 520668, "start": 5209.92, "end": 5212.08, "text": " and then tapping it up,", "tokens": [50526, 293, 550, 21444, 309, 493, 11, 50634], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2118, "seek": 520668, "start": 5212.08, "end": 5214.8, "text": " you're actually just showing the computer", "tokens": [50634, 291, 434, 767, 445, 4099, 264, 3820, 50770], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2119, "seek": 520668, "start": 5214.8, "end": 5215.96, "text": " what add what you want", "tokens": [50770, 437, 909, 437, 291, 528, 50828], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2120, "seek": 520668, "start": 5215.96, "end": 5218.52, "text": " and you let the computer figure it out.", "tokens": [50828, 293, 291, 718, 264, 3820, 2573, 309, 484, 13, 50956], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2121, "seek": 520668, "start": 5218.52, "end": 5220.400000000001, "text": " I think that's what is extremely powerful.", "tokens": [50956, 286, 519, 300, 311, 437, 307, 4664, 4005, 13, 51050], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2122, "seek": 520668, "start": 5220.400000000001, "end": 5223.360000000001, "text": " I wanna riff a little bit on what kinds of solutions", "tokens": [51050, 286, 1948, 36798, 257, 707, 857, 322, 437, 3685, 295, 6547, 51198], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2123, "seek": 520668, "start": 5223.360000000001, "end": 5224.200000000001, "text": " might be possible here", "tokens": [51198, 1062, 312, 1944, 510, 51240], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2124, "seek": 520668, "start": 5224.200000000001, "end": 5226.400000000001, "text": " and which you would consider sort of defeating", "tokens": [51240, 293, 597, 291, 576, 1949, 1333, 295, 38381, 51350], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2125, "seek": 520668, "start": 5226.400000000001, "end": 5229.320000000001, "text": " the purpose of arc and which are sort of valid.", "tokens": [51350, 264, 4334, 295, 10346, 293, 597, 366, 1333, 295, 7363, 13, 51496], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2126, "seek": 520668, "start": 5230.6, "end": 5234.0, "text": " Here's one I'll mention which is my friends", "tokens": [51560, 1692, 311, 472, 286, 603, 2152, 597, 307, 452, 1855, 51730], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2127, "seek": 520668, "start": 5234.0, "end": 5236.4800000000005, "text": " that Ryan and Buck stayed up last night", "tokens": [51730, 300, 9116, 293, 22006, 9181, 493, 1036, 1818, 51854], "temperature": 0.0, "avg_logprob": -0.21001176911640942, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.0007421169430017471}, {"id": 2128, "seek": 523648, "start": 5236.48, "end": 5237.839999999999, "text": " because I told them about this", "tokens": [50364, 570, 286, 1907, 552, 466, 341, 50432], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2129, "seek": 523648, "start": 5237.839999999999, "end": 5239.24, "text": " and they were like, oh, of course,", "tokens": [50432, 293, 436, 645, 411, 11, 1954, 11, 295, 1164, 11, 50502], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2130, "seek": 523648, "start": 5239.24, "end": 5240.08, "text": " I was gonna solve this.", "tokens": [50502, 286, 390, 799, 5039, 341, 13, 50544], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2131, "seek": 523648, "start": 5240.08, "end": 5240.919999999999, "text": " Thank you for spreading the word.", "tokens": [50544, 1044, 291, 337, 15232, 264, 1349, 13, 50586], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2132, "seek": 523648, "start": 5240.919999999999, "end": 5241.759999999999, "text": " Of course, I was gonna solve this.", "tokens": [50586, 2720, 1164, 11, 286, 390, 799, 5039, 341, 13, 50628], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2133, "seek": 523648, "start": 5241.759999999999, "end": 5243.5599999999995, "text": " And then so they were trying to prompt,", "tokens": [50628, 400, 550, 370, 436, 645, 1382, 281, 12391, 11, 50718], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2134, "seek": 523648, "start": 5243.5599999999995, "end": 5245.4, "text": " I think Claude, Opus on this", "tokens": [50718, 286, 519, 12947, 2303, 11, 12011, 301, 322, 341, 50810], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2135, "seek": 523648, "start": 5245.4, "end": 5249.639999999999, "text": " and they say they got 25% on the public arc test.", "tokens": [50810, 293, 436, 584, 436, 658, 3552, 4, 322, 264, 1908, 10346, 1500, 13, 51022], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2136, "seek": 523648, "start": 5250.5199999999995, "end": 5253.32, "text": " And what they did was have other examples", "tokens": [51066, 400, 437, 436, 630, 390, 362, 661, 5110, 51206], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2137, "seek": 523648, "start": 5253.32, "end": 5254.759999999999, "text": " of some of the arc tests", "tokens": [51206, 295, 512, 295, 264, 10346, 6921, 51278], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2138, "seek": 523648, "start": 5254.759999999999, "end": 5256.759999999999, "text": " and in context explain the reasoning", "tokens": [51278, 293, 294, 4319, 2903, 264, 21577, 51378], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2139, "seek": 523648, "start": 5256.759999999999, "end": 5259.5199999999995, "text": " of why you went from one output to another output", "tokens": [51378, 295, 983, 291, 1437, 490, 472, 5598, 281, 1071, 5598, 51516], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2140, "seek": 523648, "start": 5259.5199999999995, "end": 5261.4, "text": " and then now you have the current problem.", "tokens": [51516, 293, 550, 586, 291, 362, 264, 2190, 1154, 13, 51610], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2141, "seek": 523648, "start": 5261.4, "end": 5264.959999999999, "text": " And I think also maybe expressing the JSON in a way", "tokens": [51610, 400, 286, 519, 611, 1310, 22171, 264, 31828, 294, 257, 636, 51788], "temperature": 0.0, "avg_logprob": -0.18447113037109375, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.0004877787141595036}, {"id": 2142, "seek": 526496, "start": 5264.96, "end": 5268.24, "text": " that is more amenable to the tokenizer.", "tokens": [50364, 300, 307, 544, 18497, 712, 281, 264, 14862, 6545, 13, 50528], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2143, "seek": 526496, "start": 5268.24, "end": 5271.88, "text": " And another thing was using the code interpreter.", "tokens": [50528, 400, 1071, 551, 390, 1228, 264, 3089, 34132, 13, 50710], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2144, "seek": 526496, "start": 5271.88, "end": 5274.08, "text": " So I'm curious actually,", "tokens": [50710, 407, 286, 478, 6369, 767, 11, 50820], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2145, "seek": 526496, "start": 5274.08, "end": 5275.8, "text": " if you think the code interpreter,", "tokens": [50820, 498, 291, 519, 264, 3089, 34132, 11, 50906], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2146, "seek": 526496, "start": 5275.8, "end": 5278.32, "text": " which keeps getting better as these models get smarter", "tokens": [50906, 597, 5965, 1242, 1101, 382, 613, 5245, 483, 20294, 51032], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2147, "seek": 526496, "start": 5278.32, "end": 5280.64, "text": " is just the program synthesis right there", "tokens": [51032, 307, 445, 264, 1461, 30252, 558, 456, 51148], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2148, "seek": 526496, "start": 5280.64, "end": 5282.2, "text": " because what they were able to do", "tokens": [51148, 570, 437, 436, 645, 1075, 281, 360, 51226], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2149, "seek": 526496, "start": 5282.2, "end": 5286.28, "text": " was the actual output of the cells, the JSON output,", "tokens": [51226, 390, 264, 3539, 5598, 295, 264, 5438, 11, 264, 31828, 5598, 11, 51430], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2150, "seek": 526496, "start": 5286.28, "end": 5288.64, "text": " they got through the code interpreter,", "tokens": [51430, 436, 658, 807, 264, 3089, 34132, 11, 51548], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2151, "seek": 526496, "start": 5288.64, "end": 5290.88, "text": " like write the Python program that gets right up here.", "tokens": [51548, 411, 2464, 264, 15329, 1461, 300, 2170, 558, 493, 510, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2152, "seek": 526496, "start": 5290.88, "end": 5293.24, "text": " Do you think that the program synthesis", "tokens": [51660, 1144, 291, 519, 300, 264, 1461, 30252, 51778], "temperature": 0.0, "avg_logprob": -0.13303297963635674, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.0005527121829800308}, {"id": 2153, "seek": 529324, "start": 5293.24, "end": 5294.44, "text": " kind of researcher talking about", "tokens": [50364, 733, 295, 21751, 1417, 466, 50424], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2154, "seek": 529324, "start": 5294.44, "end": 5296.8, "text": " will look like just using the code interpreter", "tokens": [50424, 486, 574, 411, 445, 1228, 264, 3089, 34132, 50542], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2155, "seek": 529324, "start": 5296.8, "end": 5297.76, "text": " in large language models?", "tokens": [50542, 294, 2416, 2856, 5245, 30, 50590], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2156, "seek": 529324, "start": 5297.76, "end": 5300.32, "text": " I think whatever solution we see that will score well", "tokens": [50590, 286, 519, 2035, 3827, 321, 536, 300, 486, 6175, 731, 50718], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2157, "seek": 529324, "start": 5300.32, "end": 5304.5599999999995, "text": " is gonna probably need to leverage some aspects", "tokens": [50718, 307, 799, 1391, 643, 281, 13982, 512, 7270, 50930], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2158, "seek": 529324, "start": 5304.5599999999995, "end": 5307.04, "text": " from deep learning models and LLMs in particular.", "tokens": [50930, 490, 2452, 2539, 5245, 293, 441, 43, 26386, 294, 1729, 13, 51054], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2159, "seek": 529324, "start": 5307.04, "end": 5310.04, "text": " We've shown already that LLMs can do quite well,", "tokens": [51054, 492, 600, 4898, 1217, 300, 441, 43, 26386, 393, 360, 1596, 731, 11, 51204], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2160, "seek": 529324, "start": 5310.04, "end": 5312.4, "text": " that's basically the jack code approach.", "tokens": [51204, 300, 311, 1936, 264, 7109, 3089, 3109, 13, 51322], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2161, "seek": 529324, "start": 5312.4, "end": 5315.08, "text": " We've also shown that pure discrete program search", "tokens": [51322, 492, 600, 611, 4898, 300, 6075, 27706, 1461, 3164, 51456], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2162, "seek": 529324, "start": 5315.08, "end": 5317.48, "text": " from a small DSL does very, very well", "tokens": [51456, 490, 257, 1359, 15816, 43, 775, 588, 11, 588, 731, 51576], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2163, "seek": 529324, "start": 5317.48, "end": 5319.12, "text": " before jack code, this was the state of the art.", "tokens": [51576, 949, 7109, 3089, 11, 341, 390, 264, 1785, 295, 264, 1523, 13, 51658], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2164, "seek": 529324, "start": 5319.12, "end": 5321.32, "text": " In fact, it's still extremely close to the state of the art.", "tokens": [51658, 682, 1186, 11, 309, 311, 920, 4664, 1998, 281, 264, 1785, 295, 264, 1523, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1528277805873326, "compression_ratio": 1.70625, "no_speech_prob": 0.000625736138317734}, {"id": 2165, "seek": 532132, "start": 5321.44, "end": 5324.16, "text": " And there's no deep learning involved at all in these models.", "tokens": [50370, 400, 456, 311, 572, 2452, 2539, 3288, 412, 439, 294, 613, 5245, 13, 50506], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2166, "seek": 532132, "start": 5324.16, "end": 5328.24, "text": " So we have two approaches that have basically no overlap", "tokens": [50506, 407, 321, 362, 732, 11587, 300, 362, 1936, 572, 19959, 50710], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2167, "seek": 532132, "start": 5328.24, "end": 5329.28, "text": " that are doing quite well.", "tokens": [50710, 300, 366, 884, 1596, 731, 13, 50762], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2168, "seek": 532132, "start": 5329.28, "end": 5333.639999999999, "text": " And they're very much at two opposite ends of one spectrum,", "tokens": [50762, 400, 436, 434, 588, 709, 412, 732, 6182, 5314, 295, 472, 11143, 11, 50980], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2169, "seek": 532132, "start": 5333.639999999999, "end": 5336.84, "text": " where on one end you have these extremely large banks", "tokens": [50980, 689, 322, 472, 917, 291, 362, 613, 4664, 2416, 10237, 51140], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2170, "seek": 532132, "start": 5336.84, "end": 5338.799999999999, "text": " of millions of vector programs,", "tokens": [51140, 295, 6803, 295, 8062, 4268, 11, 51238], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2171, "seek": 532132, "start": 5338.799999999999, "end": 5341.12, "text": " but very, very shallow recombination,", "tokens": [51238, 457, 588, 11, 588, 20488, 850, 3548, 2486, 11, 51354], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2172, "seek": 532132, "start": 5341.12, "end": 5342.719999999999, "text": " like simplistic recombination.", "tokens": [51354, 411, 44199, 850, 3548, 2486, 13, 51434], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2173, "seek": 532132, "start": 5342.719999999999, "end": 5345.84, "text": " And on the other end, you have very simplistic DSLs,", "tokens": [51434, 400, 322, 264, 661, 917, 11, 291, 362, 588, 44199, 15816, 43, 82, 11, 51590], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2174, "seek": 532132, "start": 5345.84, "end": 5348.84, "text": " very simple, like 100 or 200 primitives,", "tokens": [51590, 588, 2199, 11, 411, 2319, 420, 2331, 2886, 38970, 11, 51740], "temperature": 0.0, "avg_logprob": -0.1375409775421399, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.001186493318527937}, {"id": 2175, "seek": 534884, "start": 5348.84, "end": 5351.92, "text": " but very deep, very sophisticated program search.", "tokens": [50364, 457, 588, 2452, 11, 588, 16950, 1461, 3164, 13, 50518], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2176, "seek": 534884, "start": 5352.84, "end": 5355.16, "text": " The solution is gonna be somewhere in between, right?", "tokens": [50564, 440, 3827, 307, 799, 312, 4079, 294, 1296, 11, 558, 30, 50680], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2177, "seek": 534884, "start": 5355.16, "end": 5359.72, "text": " So the people are gonna be winning the art competition", "tokens": [50680, 407, 264, 561, 366, 799, 312, 8224, 264, 1523, 6211, 50908], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2178, "seek": 534884, "start": 5359.72, "end": 5361.84, "text": " and we're gonna be making the most progress", "tokens": [50908, 293, 321, 434, 799, 312, 1455, 264, 881, 4205, 51014], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2179, "seek": 534884, "start": 5361.84, "end": 5363.96, "text": " towards near-term NGR are gonna be users", "tokens": [51014, 3030, 2651, 12, 7039, 426, 23971, 366, 799, 312, 5022, 51120], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2180, "seek": 534884, "start": 5363.96, "end": 5367.0, "text": " that manage to merge the deep learning paradigm", "tokens": [51120, 300, 3067, 281, 22183, 264, 2452, 2539, 24709, 51272], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2181, "seek": 534884, "start": 5367.0, "end": 5368.8, "text": " and the discrete program search paradigm", "tokens": [51272, 293, 264, 27706, 1461, 3164, 24709, 51362], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2182, "seek": 534884, "start": 5368.8, "end": 5371.56, "text": " into one elegant way.", "tokens": [51362, 666, 472, 21117, 636, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2183, "seek": 534884, "start": 5371.56, "end": 5373.28, "text": " And you know, you ask like,", "tokens": [51500, 400, 291, 458, 11, 291, 1029, 411, 11, 51586], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2184, "seek": 534884, "start": 5373.28, "end": 5376.56, "text": " what would be legitimate and what would be cheating,", "tokens": [51586, 437, 576, 312, 17956, 293, 437, 576, 312, 18309, 11, 51750], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2185, "seek": 534884, "start": 5376.56, "end": 5377.400000000001, "text": " for instance?", "tokens": [51750, 337, 5197, 30, 51792], "temperature": 0.0, "avg_logprob": -0.2170429229736328, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00014580854622181505}, {"id": 2186, "seek": 537740, "start": 5377.839999999999, "end": 5381.08, "text": " You wanna add a code interpreter to the system.", "tokens": [50386, 509, 1948, 909, 257, 3089, 34132, 281, 264, 1185, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2187, "seek": 537740, "start": 5381.08, "end": 5382.759999999999, "text": " I think that's great, that's sort of legitimate.", "tokens": [50548, 286, 519, 300, 311, 869, 11, 300, 311, 1333, 295, 17956, 13, 50632], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2188, "seek": 537740, "start": 5382.759999999999, "end": 5385.08, "text": " The part that would be cheating is try to", "tokens": [50632, 440, 644, 300, 576, 312, 18309, 307, 853, 281, 50748], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2189, "seek": 537740, "start": 5387.0, "end": 5389.24, "text": " anticipate what might be in the test set,", "tokens": [50844, 21685, 437, 1062, 312, 294, 264, 1500, 992, 11, 50956], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2190, "seek": 537740, "start": 5389.24, "end": 5392.32, "text": " like brute force the space of possible tasks", "tokens": [50956, 411, 47909, 3464, 264, 1901, 295, 1944, 9608, 51110], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2191, "seek": 537740, "start": 5392.32, "end": 5395.16, "text": " and then train a memorization system on it.", "tokens": [51110, 293, 550, 3847, 257, 10560, 2144, 1185, 322, 309, 13, 51252], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2192, "seek": 537740, "start": 5395.16, "end": 5397.719999999999, "text": " And then rely on the fact that you're generating", "tokens": [51252, 400, 550, 10687, 322, 264, 1186, 300, 291, 434, 17746, 51380], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2193, "seek": 537740, "start": 5397.719999999999, "end": 5399.879999999999, "text": " so many tasks, like millions and millions and millions,", "tokens": [51380, 370, 867, 9608, 11, 411, 6803, 293, 6803, 293, 6803, 11, 51488], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2194, "seek": 537740, "start": 5399.879999999999, "end": 5402.16, "text": " that inevitably there's gonna be some overlap", "tokens": [51488, 300, 28171, 456, 311, 799, 312, 512, 19959, 51602], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2195, "seek": 537740, "start": 5402.16, "end": 5404.719999999999, "text": " between what you're generating and what's in the test set.", "tokens": [51602, 1296, 437, 291, 434, 17746, 293, 437, 311, 294, 264, 1500, 992, 13, 51730], "temperature": 0.0, "avg_logprob": -0.14087186094190254, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.00045119752758182585}, {"id": 2196, "seek": 540472, "start": 5404.76, "end": 5407.96, "text": " I think that's defeating the purpose of benchmark", "tokens": [50366, 286, 519, 300, 311, 38381, 264, 4334, 295, 18927, 50526], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2197, "seek": 540472, "start": 5407.96, "end": 5409.84, "text": " because then you can just solve it with that", "tokens": [50526, 570, 550, 291, 393, 445, 5039, 309, 365, 300, 50620], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2198, "seek": 540472, "start": 5409.84, "end": 5413.280000000001, "text": " and you need to adapt just by fetching a memorized solution.", "tokens": [50620, 293, 291, 643, 281, 6231, 445, 538, 23673, 278, 257, 46677, 3827, 13, 50792], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2199, "seek": 540472, "start": 5413.280000000001, "end": 5415.64, "text": " So hopefully arc will resist to that,", "tokens": [50792, 407, 4696, 10346, 486, 4597, 281, 300, 11, 50910], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2200, "seek": 540472, "start": 5415.64, "end": 5418.16, "text": " but you know, nothing, no benchmark is necessarily perfect.", "tokens": [50910, 457, 291, 458, 11, 1825, 11, 572, 18927, 307, 4725, 2176, 13, 51036], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2201, "seek": 540472, "start": 5418.16, "end": 5420.16, "text": " So maybe there's a way to hack it", "tokens": [51036, 407, 1310, 456, 311, 257, 636, 281, 10339, 309, 51136], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2202, "seek": 540472, "start": 5420.16, "end": 5422.2, "text": " and I guess we are gonna get an answer very soon.", "tokens": [51136, 293, 286, 2041, 321, 366, 799, 483, 364, 1867, 588, 2321, 13, 51238], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2203, "seek": 540472, "start": 5422.2, "end": 5424.12, "text": " Although I think some amount of fine tuning is valid", "tokens": [51238, 5780, 286, 519, 512, 2372, 295, 2489, 15164, 307, 7363, 51334], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2204, "seek": 540472, "start": 5424.12, "end": 5427.52, "text": " because these models don't natively think in terms of,", "tokens": [51334, 570, 613, 5245, 500, 380, 8470, 356, 519, 294, 2115, 295, 11, 51504], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2205, "seek": 540472, "start": 5427.52, "end": 5428.84, "text": " especially the language models alone,", "tokens": [51504, 2318, 264, 2856, 5245, 3312, 11, 51570], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2206, "seek": 540472, "start": 5428.84, "end": 5431.08, "text": " which the open source models that they would have to use", "tokens": [51570, 597, 264, 1269, 4009, 5245, 300, 436, 576, 362, 281, 764, 51682], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2207, "seek": 540472, "start": 5431.08, "end": 5433.2, "text": " to be competitive here compete here.", "tokens": [51682, 281, 312, 10043, 510, 11831, 510, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15853995747036403, "compression_ratio": 1.7172619047619047, "no_speech_prob": 0.0003052499087061733}, {"id": 2208, "seek": 543320, "start": 5433.8, "end": 5434.92, "text": " They're like natively language,", "tokens": [50394, 814, 434, 411, 8470, 356, 2856, 11, 50450], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2209, "seek": 543320, "start": 5434.92, "end": 5438.08, "text": " so they need to be able to think in this kind of...", "tokens": [50450, 370, 436, 643, 281, 312, 1075, 281, 519, 294, 341, 733, 295, 485, 50608], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2210, "seek": 543320, "start": 5438.08, "end": 5438.92, "text": " Yes.", "tokens": [50608, 1079, 13, 50650], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2211, "seek": 543320, "start": 5438.92, "end": 5439.76, "text": " The arc type way.", "tokens": [50650, 440, 10346, 2010, 636, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2212, "seek": 543320, "start": 5439.76, "end": 5441.48, "text": " You want to input corner ledge,", "tokens": [50692, 509, 528, 281, 4846, 4538, 47109, 11, 50778], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2213, "seek": 543320, "start": 5441.48, "end": 5444.44, "text": " like arc like corner ledge into the model,", "tokens": [50778, 411, 10346, 411, 4538, 47109, 666, 264, 2316, 11, 50926], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2214, "seek": 543320, "start": 5444.44, "end": 5447.28, "text": " but surely you don't need tens of millions of tasks", "tokens": [50926, 457, 11468, 291, 500, 380, 643, 10688, 295, 6803, 295, 9608, 51068], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2215, "seek": 543320, "start": 5447.28, "end": 5450.08, "text": " to do this, like corner ledge is extremely basic.", "tokens": [51068, 281, 360, 341, 11, 411, 4538, 47109, 307, 4664, 3875, 13, 51208], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2216, "seek": 543320, "start": 5450.08, "end": 5452.96, "text": " If you look at some of these arc type questions,", "tokens": [51208, 759, 291, 574, 412, 512, 295, 613, 10346, 2010, 1651, 11, 51352], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2217, "seek": 543320, "start": 5454.36, "end": 5456.48, "text": " I actually do think they rely a little bit", "tokens": [51422, 286, 767, 360, 519, 436, 10687, 257, 707, 857, 51528], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2218, "seek": 543320, "start": 5456.48, "end": 5459.639999999999, "text": " on things I have seen throughout my life.", "tokens": [51528, 322, 721, 286, 362, 1612, 3710, 452, 993, 13, 51686], "temperature": 0.0, "avg_logprob": -0.1844344381558693, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010311028454452753}, {"id": 2219, "seek": 545964, "start": 5459.64, "end": 5462.8, "text": " And for the same, like for example,", "tokens": [50364, 400, 337, 264, 912, 11, 411, 337, 1365, 11, 50522], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2220, "seek": 545964, "start": 5462.8, "end": 5464.64, "text": " like something bounces off a wall", "tokens": [50522, 411, 746, 46901, 766, 257, 2929, 50614], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2221, "seek": 545964, "start": 5464.64, "end": 5466.160000000001, "text": " and comes back and you see that pattern.", "tokens": [50614, 293, 1487, 646, 293, 291, 536, 300, 5102, 13, 50690], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2222, "seek": 545964, "start": 5466.160000000001, "end": 5467.4800000000005, "text": " It's like I played arcade games", "tokens": [50690, 467, 311, 411, 286, 3737, 25664, 2813, 50756], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2223, "seek": 545964, "start": 5467.4800000000005, "end": 5469.6, "text": " and I've seen like pong or something.", "tokens": [50756, 293, 286, 600, 1612, 411, 36164, 420, 746, 13, 50862], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2224, "seek": 545964, "start": 5469.6, "end": 5471.4400000000005, "text": " And I think for example, when you see the Flynn effect", "tokens": [50862, 400, 286, 519, 337, 1365, 11, 562, 291, 536, 264, 40391, 1802, 50954], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2225, "seek": 545964, "start": 5471.4400000000005, "end": 5474.12, "text": " and people's intelligence has measured on", "tokens": [50954, 293, 561, 311, 7599, 575, 12690, 322, 51088], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2226, "seek": 545964, "start": 5474.12, "end": 5475.56, "text": " very advanced progressive matrices", "tokens": [51088, 588, 7339, 16131, 32284, 51160], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2227, "seek": 545964, "start": 5475.56, "end": 5477.360000000001, "text": " increasing on these kinds of questions,", "tokens": [51160, 5662, 322, 613, 3685, 295, 1651, 11, 51250], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2228, "seek": 545964, "start": 5477.360000000001, "end": 5479.360000000001, "text": " it's probably a simpler story where since now,", "tokens": [51250, 309, 311, 1391, 257, 18587, 1657, 689, 1670, 586, 11, 51350], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2229, "seek": 545964, "start": 5479.360000000001, "end": 5481.160000000001, "text": " since childhood, we actually see these sorts of patterns", "tokens": [51350, 1670, 9278, 11, 321, 767, 536, 613, 7527, 295, 8294, 51440], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2230, "seek": 545964, "start": 5481.160000000001, "end": 5483.400000000001, "text": " in TV and whatever, spatial patterns.", "tokens": [51440, 294, 3558, 293, 2035, 11, 23598, 8294, 13, 51552], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2231, "seek": 545964, "start": 5483.400000000001, "end": 5486.200000000001, "text": " And so I don't think this is sort of core knowledge.", "tokens": [51552, 400, 370, 286, 500, 380, 519, 341, 307, 1333, 295, 4965, 3601, 13, 51692], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2232, "seek": 545964, "start": 5486.200000000001, "end": 5489.240000000001, "text": " I think actually this is also part of the quote unquote", "tokens": [51692, 286, 519, 767, 341, 307, 611, 644, 295, 264, 6513, 37557, 51844], "temperature": 0.0, "avg_logprob": -0.16597386410361842, "compression_ratio": 1.8, "no_speech_prob": 0.0011333636939525604}, {"id": 2233, "seek": 548924, "start": 5489.24, "end": 5491.719999999999, "text": " trying tuning that humans have as they grow up", "tokens": [50364, 1382, 15164, 300, 6255, 362, 382, 436, 1852, 493, 50488], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2234, "seek": 548924, "start": 5491.719999999999, "end": 5494.08, "text": " of seeing different kinds of spatial patterns", "tokens": [50488, 295, 2577, 819, 3685, 295, 23598, 8294, 50606], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2235, "seek": 548924, "start": 5494.08, "end": 5495.48, "text": " and trying to pattern match to them.", "tokens": [50606, 293, 1382, 281, 5102, 2995, 281, 552, 13, 50676], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2236, "seek": 548924, "start": 5495.48, "end": 5497.8, "text": " I would definitely file that under core knowledge.", "tokens": [50676, 286, 576, 2138, 3991, 300, 833, 4965, 3601, 13, 50792], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2237, "seek": 548924, "start": 5497.8, "end": 5500.599999999999, "text": " Like core knowledge includes basic physics,", "tokens": [50792, 1743, 4965, 3601, 5974, 3875, 10649, 11, 50932], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2238, "seek": 548924, "start": 5500.599999999999, "end": 5503.16, "text": " for instance, bouncing or trajectories,", "tokens": [50932, 337, 5197, 11, 27380, 420, 18257, 2083, 11, 51060], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2239, "seek": 548924, "start": 5503.16, "end": 5504.4, "text": " that would be included.", "tokens": [51060, 300, 576, 312, 5556, 13, 51122], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2240, "seek": 548924, "start": 5504.4, "end": 5505.8, "text": " But yeah, I think you're entirely right.", "tokens": [51122, 583, 1338, 11, 286, 519, 291, 434, 7696, 558, 13, 51192], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2241, "seek": 548924, "start": 5505.8, "end": 5507.16, "text": " The reason why as a human,", "tokens": [51192, 440, 1778, 983, 382, 257, 1952, 11, 51260], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2242, "seek": 548924, "start": 5507.16, "end": 5509.04, "text": " you're able to quickly figure out the solution", "tokens": [51260, 291, 434, 1075, 281, 2661, 2573, 484, 264, 3827, 51354], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2243, "seek": 548924, "start": 5509.04, "end": 5511.84, "text": " is because you have this set of building blocks,", "tokens": [51354, 307, 570, 291, 362, 341, 992, 295, 2390, 8474, 11, 51494], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2244, "seek": 548924, "start": 5511.84, "end": 5514.16, "text": " this set of patterns in your mind that you can recombine.", "tokens": [51494, 341, 992, 295, 8294, 294, 428, 1575, 300, 291, 393, 850, 3548, 533, 13, 51610], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2245, "seek": 548924, "start": 5514.16, "end": 5517.24, "text": " Is core knowledge required to attain intelligence?", "tokens": [51610, 1119, 4965, 3601, 4739, 281, 23766, 7599, 30, 51764], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2246, "seek": 548924, "start": 5517.24, "end": 5518.679999999999, "text": " Any algorithm you have,", "tokens": [51764, 2639, 9284, 291, 362, 11, 51836], "temperature": 0.0, "avg_logprob": -0.15536941894113201, "compression_ratio": 1.751497005988024, "no_speech_prob": 0.0007551051676273346}, {"id": 2247, "seek": 551868, "start": 5518.76, "end": 5520.76, "text": " does the core knowledge have to be in some sense hard coded", "tokens": [50368, 775, 264, 4965, 3601, 362, 281, 312, 294, 512, 2020, 1152, 34874, 50468], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2248, "seek": 551868, "start": 5520.76, "end": 5523.76, "text": " or can even the core knowledge be learned through intelligence?", "tokens": [50468, 420, 393, 754, 264, 4965, 3601, 312, 3264, 807, 7599, 30, 50618], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2249, "seek": 551868, "start": 5523.76, "end": 5525.04, "text": " Core knowledge can be learned.", "tokens": [50618, 14798, 3601, 393, 312, 3264, 13, 50682], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2250, "seek": 551868, "start": 5525.04, "end": 5527.200000000001, "text": " And I think in the case of humans,", "tokens": [50682, 400, 286, 519, 294, 264, 1389, 295, 6255, 11, 50790], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2251, "seek": 551868, "start": 5527.200000000001, "end": 5529.6, "text": " some amount of core knowledge is something", "tokens": [50790, 512, 2372, 295, 4965, 3601, 307, 746, 50910], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2252, "seek": 551868, "start": 5529.6, "end": 5530.64, "text": " that you're born with.", "tokens": [50910, 300, 291, 434, 4232, 365, 13, 50962], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2253, "seek": 551868, "start": 5530.64, "end": 5533.64, "text": " Like we're actually born with a small amount of knowledge", "tokens": [50962, 1743, 321, 434, 767, 4232, 365, 257, 1359, 2372, 295, 3601, 51112], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2254, "seek": 551868, "start": 5533.64, "end": 5535.56, "text": " about the world we're gonna live in.", "tokens": [51112, 466, 264, 1002, 321, 434, 799, 1621, 294, 13, 51208], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2255, "seek": 551868, "start": 5535.56, "end": 5537.08, "text": " We're not blank slates.", "tokens": [51208, 492, 434, 406, 8247, 1061, 1024, 13, 51284], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2256, "seek": 551868, "start": 5537.08, "end": 5540.64, "text": " But most core knowledge is acquired through experience.", "tokens": [51284, 583, 881, 4965, 3601, 307, 17554, 807, 1752, 13, 51462], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2257, "seek": 551868, "start": 5540.64, "end": 5542.0, "text": " But the thing with core knowledge", "tokens": [51462, 583, 264, 551, 365, 4965, 3601, 51530], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2258, "seek": 551868, "start": 5542.0, "end": 5545.320000000001, "text": " that it's not gonna be acquired like for instance in school,", "tokens": [51530, 300, 309, 311, 406, 799, 312, 17554, 411, 337, 5197, 294, 1395, 11, 51696], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2259, "seek": 551868, "start": 5545.320000000001, "end": 5547.52, "text": " it's actually acquired very, very early", "tokens": [51696, 309, 311, 767, 17554, 588, 11, 588, 2440, 51806], "temperature": 0.0, "avg_logprob": -0.15661887047995984, "compression_ratio": 1.9824561403508771, "no_speech_prob": 0.00021642702631652355}, {"id": 2260, "seek": 554752, "start": 5547.52, "end": 5550.400000000001, "text": " in the first like three to four years of your life.", "tokens": [50364, 294, 264, 700, 411, 1045, 281, 1451, 924, 295, 428, 993, 13, 50508], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2261, "seek": 554752, "start": 5550.400000000001, "end": 5551.4400000000005, "text": " And by age four,", "tokens": [50508, 400, 538, 3205, 1451, 11, 50560], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2262, "seek": 554752, "start": 5551.4400000000005, "end": 5554.56, "text": " you have all the core knowledge you're gonna need as an adult.", "tokens": [50560, 291, 362, 439, 264, 4965, 3601, 291, 434, 799, 643, 382, 364, 5075, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2263, "seek": 554752, "start": 5554.56, "end": 5556.040000000001, "text": " Okay, interesting.", "tokens": [50716, 1033, 11, 1880, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2264, "seek": 554752, "start": 5556.040000000001, "end": 5557.8, "text": " So I mean, on the price itself,", "tokens": [50790, 407, 286, 914, 11, 322, 264, 3218, 2564, 11, 50878], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2265, "seek": 554752, "start": 5557.8, "end": 5560.96, "text": " I'm super excited to see both the open source versions", "tokens": [50878, 286, 478, 1687, 2919, 281, 536, 1293, 264, 1269, 4009, 9606, 51036], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2266, "seek": 554752, "start": 5560.96, "end": 5564.120000000001, "text": " of maybe with a Lama 70B or something", "tokens": [51036, 295, 1310, 365, 257, 441, 2404, 5285, 33, 420, 746, 51194], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2267, "seek": 554752, "start": 5564.120000000001, "end": 5566.52, "text": " what people can score in the competition itself.", "tokens": [51194, 437, 561, 393, 6175, 294, 264, 6211, 2564, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2268, "seek": 554752, "start": 5566.52, "end": 5570.52, "text": " Then if to sort of test specifically", "tokens": [51314, 1396, 498, 281, 1333, 295, 1500, 4682, 51514], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2269, "seek": 554752, "start": 5570.52, "end": 5571.6, "text": " the scaling hypothesis,", "tokens": [51514, 264, 21589, 17291, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2270, "seek": 554752, "start": 5571.6, "end": 5573.56, "text": " I'm very curious to see if you can prompt", "tokens": [51568, 286, 478, 588, 6369, 281, 536, 498, 291, 393, 12391, 51666], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2271, "seek": 554752, "start": 5573.56, "end": 5575.120000000001, "text": " on the public version of ARC,", "tokens": [51666, 322, 264, 1908, 3037, 295, 8943, 34, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2272, "seek": 554752, "start": 5575.120000000001, "end": 5576.080000000001, "text": " which I guess when we compare,", "tokens": [51744, 597, 286, 2041, 562, 321, 6794, 11, 51792], "temperature": 0.0, "avg_logprob": -0.1335847826971524, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.00044296783744357526}, {"id": 2273, "seek": 557608, "start": 5576.08, "end": 5578.64, "text": " you will be able to submit to this competition itself.", "tokens": [50364, 291, 486, 312, 1075, 281, 10315, 281, 341, 6211, 2564, 13, 50492], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2274, "seek": 557608, "start": 5578.64, "end": 5580.08, "text": " But I'd be very curious to see how,", "tokens": [50492, 583, 286, 1116, 312, 588, 6369, 281, 536, 577, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2275, "seek": 557608, "start": 5580.08, "end": 5582.8, "text": " if people can sort of crack that and get ARC working there", "tokens": [50564, 498, 561, 393, 1333, 295, 6226, 300, 293, 483, 8943, 34, 1364, 456, 50700], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2276, "seek": 557608, "start": 5582.8, "end": 5584.64, "text": " and if that would update your reviews on AGI.", "tokens": [50700, 293, 498, 300, 576, 5623, 428, 10229, 322, 316, 26252, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2277, "seek": 557608, "start": 5584.64, "end": 5585.64, "text": " It's gonna be motivating.", "tokens": [50792, 467, 311, 799, 312, 41066, 13, 50842], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2278, "seek": 557608, "start": 5585.64, "end": 5586.76, "text": " We're gonna keep running the contest", "tokens": [50842, 492, 434, 799, 1066, 2614, 264, 10287, 50898], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2279, "seek": 557608, "start": 5586.76, "end": 5589.04, "text": " until somebody puts a reproducible open source version", "tokens": [50898, 1826, 2618, 8137, 257, 11408, 32128, 1269, 4009, 3037, 51012], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2280, "seek": 557608, "start": 5589.04, "end": 5589.88, "text": " into public domain.", "tokens": [51012, 666, 1908, 9274, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2281, "seek": 557608, "start": 5589.88, "end": 5593.76, "text": " So even if somebody privately beats the ARC eval,", "tokens": [51054, 407, 754, 498, 2618, 31919, 16447, 264, 8943, 34, 1073, 304, 11, 51248], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2282, "seek": 557608, "start": 5593.76, "end": 5594.88, "text": " we're gonna still keep the price money", "tokens": [51248, 321, 434, 799, 920, 1066, 264, 3218, 1460, 51304], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2283, "seek": 557608, "start": 5594.88, "end": 5596.08, "text": " until someone can reproduce it", "tokens": [51304, 1826, 1580, 393, 29501, 309, 51364], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2284, "seek": 557608, "start": 5596.08, "end": 5598.5599999999995, "text": " and put the public reproducible version out there.", "tokens": [51364, 293, 829, 264, 1908, 11408, 32128, 3037, 484, 456, 13, 51488], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2285, "seek": 557608, "start": 5598.5599999999995, "end": 5599.4, "text": " Yeah, exactly.", "tokens": [51488, 865, 11, 2293, 13, 51530], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2286, "seek": 557608, "start": 5599.4, "end": 5602.28, "text": " Like the goal is to accelerate progress towards AGI.", "tokens": [51530, 1743, 264, 3387, 307, 281, 21341, 4205, 3030, 316, 26252, 13, 51674], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2287, "seek": 557608, "start": 5602.28, "end": 5604.28, "text": " And a key part of that is that", "tokens": [51674, 400, 257, 2141, 644, 295, 300, 307, 300, 51774], "temperature": 0.0, "avg_logprob": -0.14895672531601806, "compression_ratio": 1.7228571428571429, "no_speech_prob": 0.0010004396317526698}, {"id": 2288, "seek": 560428, "start": 5604.28, "end": 5606.16, "text": " any sort of meaningful bits of progress", "tokens": [50364, 604, 1333, 295, 10995, 9239, 295, 4205, 50458], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2289, "seek": 560428, "start": 5606.16, "end": 5608.8, "text": " needs to be shared, needs to be public.", "tokens": [50458, 2203, 281, 312, 5507, 11, 2203, 281, 312, 1908, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2290, "seek": 560428, "start": 5608.8, "end": 5610.44, "text": " So everyone can know about it", "tokens": [50590, 407, 1518, 393, 458, 466, 309, 50672], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2291, "seek": 560428, "start": 5610.44, "end": 5612.16, "text": " and can try to iterate on it.", "tokens": [50672, 293, 393, 853, 281, 44497, 322, 309, 13, 50758], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2292, "seek": 560428, "start": 5612.16, "end": 5613.719999999999, "text": " If there's no sharing, there's no progress.", "tokens": [50758, 759, 456, 311, 572, 5414, 11, 456, 311, 572, 4205, 13, 50836], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2293, "seek": 560428, "start": 5613.719999999999, "end": 5615.04, "text": " What I'm especially curious about", "tokens": [50836, 708, 286, 478, 2318, 6369, 466, 50902], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2294, "seek": 560428, "start": 5615.04, "end": 5616.639999999999, "text": " is sort of disaggregating the bets", "tokens": [50902, 307, 1333, 295, 10414, 11027, 990, 264, 39922, 50982], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2295, "seek": 560428, "start": 5616.639999999999, "end": 5619.84, "text": " of like, can we make an open version of this", "tokens": [50982, 295, 411, 11, 393, 321, 652, 364, 1269, 3037, 295, 341, 51142], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2296, "seek": 560428, "start": 5619.84, "end": 5623.32, "text": " versus is this a thing that's just possible with scaling?", "tokens": [51142, 5717, 307, 341, 257, 551, 300, 311, 445, 1944, 365, 21589, 30, 51316], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2297, "seek": 560428, "start": 5623.32, "end": 5625.679999999999, "text": " And we can, I guess test both of them", "tokens": [51316, 400, 321, 393, 11, 286, 2041, 1500, 1293, 295, 552, 51434], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2298, "seek": 560428, "start": 5625.679999999999, "end": 5627.84, "text": " based on the public and the private version.", "tokens": [51434, 2361, 322, 264, 1908, 293, 264, 4551, 3037, 13, 51542], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2299, "seek": 560428, "start": 5627.84, "end": 5630.679999999999, "text": " We're making contact with reality as well with this, right?", "tokens": [51542, 492, 434, 1455, 3385, 365, 4103, 382, 731, 365, 341, 11, 558, 30, 51684], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2300, "seek": 560428, "start": 5630.679999999999, "end": 5631.5199999999995, "text": " We're gonna learn a lot, I think,", "tokens": [51684, 492, 434, 799, 1466, 257, 688, 11, 286, 519, 11, 51726], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2301, "seek": 560428, "start": 5631.5199999999995, "end": 5632.92, "text": " about what the actual limits of the compute", "tokens": [51726, 466, 437, 264, 3539, 10406, 295, 264, 14722, 51796], "temperature": 0.0, "avg_logprob": -0.12001838924000098, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0019875457510352135}, {"id": 2302, "seek": 563292, "start": 5632.92, "end": 5633.92, "text": " where if someone showed up and said,", "tokens": [50364, 689, 498, 1580, 4712, 493, 293, 848, 11, 50414], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2303, "seek": 563292, "start": 5633.92, "end": 5635.12, "text": " hey, here's a closed source model", "tokens": [50414, 4177, 11, 510, 311, 257, 5395, 4009, 2316, 50474], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2304, "seek": 563292, "start": 5635.12, "end": 5637.24, "text": " that like I'm getting 50 plus percent on,", "tokens": [50474, 300, 411, 286, 478, 1242, 2625, 1804, 3043, 322, 11, 50580], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2305, "seek": 563292, "start": 5637.24, "end": 5638.72, "text": " I think that would probably update us on like,", "tokens": [50580, 286, 519, 300, 576, 1391, 5623, 505, 322, 411, 11, 50654], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2306, "seek": 563292, "start": 5638.72, "end": 5640.4, "text": " okay, perhaps we should increase the amount of compute", "tokens": [50654, 1392, 11, 4317, 321, 820, 3488, 264, 2372, 295, 14722, 50738], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2307, "seek": 563292, "start": 5640.4, "end": 5641.6, "text": " that we give on the private test set", "tokens": [50738, 300, 321, 976, 322, 264, 4551, 1500, 992, 50798], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2308, "seek": 563292, "start": 5641.6, "end": 5643.76, "text": " in order to balance some of the decisions", "tokens": [50798, 294, 1668, 281, 4772, 512, 295, 264, 5327, 50906], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2309, "seek": 563292, "start": 5643.76, "end": 5644.96, "text": " that initially are somewhat arbitrary", "tokens": [50906, 300, 9105, 366, 8344, 23211, 50966], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2310, "seek": 563292, "start": 5644.96, "end": 5647.24, "text": " in order to learn about, okay, what do people want?", "tokens": [50966, 294, 1668, 281, 1466, 466, 11, 1392, 11, 437, 360, 561, 528, 30, 51080], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2311, "seek": 563292, "start": 5647.24, "end": 5648.24, "text": " What does progress look like?", "tokens": [51080, 708, 775, 4205, 574, 411, 30, 51130], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2312, "seek": 563292, "start": 5648.24, "end": 5650.4800000000005, "text": " And I think both of us are sort of committed", "tokens": [51130, 400, 286, 519, 1293, 295, 505, 366, 1333, 295, 7784, 51242], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2313, "seek": 563292, "start": 5650.4800000000005, "end": 5652.64, "text": " to evolving it over time in order to be the best,", "tokens": [51242, 281, 21085, 309, 670, 565, 294, 1668, 281, 312, 264, 1151, 11, 51350], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2314, "seek": 563292, "start": 5652.64, "end": 5654.36, "text": " or the closest to perfect as we can get it.", "tokens": [51350, 420, 264, 13699, 281, 2176, 382, 321, 393, 483, 309, 13, 51436], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2315, "seek": 563292, "start": 5654.36, "end": 5655.84, "text": " Awesome, and where can people go to learn more", "tokens": [51436, 10391, 11, 293, 689, 393, 561, 352, 281, 1466, 544, 51510], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2316, "seek": 563292, "start": 5655.84, "end": 5658.28, "text": " about the prize and maybe give their hand at it?", "tokens": [51510, 466, 264, 12818, 293, 1310, 976, 641, 1011, 412, 309, 30, 51632], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2317, "seek": 563292, "start": 5658.28, "end": 5659.32, "text": " ARCPrize.org.", "tokens": [51632, 8943, 34, 47, 470, 1381, 13, 4646, 13, 51684], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2318, "seek": 563292, "start": 5659.32, "end": 5661.0, "text": " Which goes live today, so.", "tokens": [51684, 3013, 1709, 1621, 965, 11, 370, 13, 51768], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2319, "seek": 563292, "start": 5661.0, "end": 5661.84, "text": " It's live now.", "tokens": [51768, 467, 311, 1621, 586, 13, 51810], "temperature": 0.0, "avg_logprob": -0.16557410269072562, "compression_ratio": 1.7822784810126582, "no_speech_prob": 0.005384302698075771}, {"id": 2320, "seek": 566184, "start": 5661.88, "end": 5663.64, "text": " $70 million is on this line, people.", "tokens": [50366, 1848, 5867, 2459, 307, 322, 341, 1622, 11, 561, 13, 50454], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2321, "seek": 566184, "start": 5663.64, "end": 5664.4800000000005, "text": " Good luck.", "tokens": [50454, 2205, 3668, 13, 50496], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2322, "seek": 566184, "start": 5664.4800000000005, "end": 5665.32, "text": " Thank you guys for coming on the podcast.", "tokens": [50496, 1044, 291, 1074, 337, 1348, 322, 264, 7367, 13, 50538], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2323, "seek": 566184, "start": 5665.32, "end": 5666.84, "text": " It was super fun to go through all the cruxes", "tokens": [50538, 467, 390, 1687, 1019, 281, 352, 807, 439, 264, 5140, 87, 279, 50614], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2324, "seek": 566184, "start": 5666.84, "end": 5668.92, "text": " on intelligence and get a different perspective,", "tokens": [50614, 322, 7599, 293, 483, 257, 819, 4585, 11, 50718], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2325, "seek": 566184, "start": 5668.92, "end": 5670.88, "text": " and also to announce a prize here.", "tokens": [50718, 293, 611, 281, 7478, 257, 12818, 510, 13, 50816], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2326, "seek": 566184, "start": 5670.88, "end": 5671.8, "text": " So this is awesome.", "tokens": [50816, 407, 341, 307, 3476, 13, 50862], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2327, "seek": 566184, "start": 5671.8, "end": 5672.96, "text": " Thank you for helping break news.", "tokens": [50862, 1044, 291, 337, 4315, 1821, 2583, 13, 50920], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}, {"id": 2328, "seek": 566184, "start": 5672.96, "end": 5673.96, "text": " Thank you, Finest.", "tokens": [50920, 1044, 291, 11, 3773, 377, 13, 50970], "temperature": 0.0, "avg_logprob": -0.29892550939801094, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.5223575830459595}], "language": "en"}