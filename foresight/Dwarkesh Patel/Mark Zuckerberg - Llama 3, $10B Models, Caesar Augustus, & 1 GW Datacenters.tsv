start	end	text
0	1440	That's not even a question for me,
1440	4080	whether we're going to go take a swing at building the next thing.
4080	6640	I'm just incapable of not doing that.
6640	9840	There's a bunch of times when we wanted to launch features
9840	12480	and then Apple's just like, nope, you're not launching that.
12480	13360	I was like, that sucks.
14160	18960	Are we set up for that with AI, where you're going to get a handful of companies
18960	22240	that run these closed models that are going to be in control of the APIs
22240	24480	and therefore are going to be able to tell you what you can build?
24480	29360	Then when you start getting into building a data center that's like 300 megawatts
29360	31920	or 500 megawatts or a gigawatt,
31920	34320	just no one has built a single gigawatt data center yet.
34320	37200	But from wherever you sit, there's going to be some actor who you don't trust.
37200	39600	If they're the ones who have like the super strong AI,
39600	43040	I think that that's potentially a much bigger risk.
43920	45280	Mark, welcome to the podcast.
45280	47200	Hey, thanks for having me, big fan of your podcast.
47200	48560	Oh, thank you. That's very nice of you to say.
49920	54000	Okay, so let's start by talking about the releases that will go out
54000	55040	when this interview goes out.
55920	57920	Tell me about the models, tell me about meta AI,
57920	59440	what's new, what's exciting about them?
59440	62640	Yeah, sure. I think the main thing that most people in the world
62640	64240	are going to see is the new version of meta AI.
66640	70400	The most important thing about what we're doing is the upgrade to the model.
70400	71600	We're rolling out Lama 3.
71600	74960	We're doing it both as open source for the dev community,
74960	77120	and it is now going to be powering meta AI.
78880	80880	There's a lot that I'm sure we'll go into around Lama 3,
80880	84160	but I think the bottom line on this is that with Lama 3,
84160	88080	we now think that meta AI is the most intelligent AI assistant
88080	90000	that people can use that's freely available.
90720	93760	We're also integrating Google and Bing for real-time knowledge.
94480	97200	We're going to make it a lot more prominent across our apps.
97200	100960	So basically, at the top of WhatsApp and Instagram
100960	105680	and Facebook and Messenger, you'll just be able to use the search box
105680	107280	right there to ask it any question.
108080	110720	And there's a bunch of new creation features that we added
110720	112720	that I think are pretty cool that I think people enjoy.
114400	116480	And I think animations is a good one.
117200	119360	You can basically just take any image and animate it.
119360	123520	But I think one that people are going to find pretty wild is
124400	127680	it now generates high-quality images so quickly.
127680	129360	I don't know if you've gotten a chance to play with this,
129360	132880	that it actually generates it as you're typing and updates it in real-time.
132880	136880	So you're typing your query and it's honing in on...
136880	142480	And it's like, okay, here, show me a picture of a cow in a field
142560	145920	with mountains in the background and just like eating macadamia nuts,
145920	150080	drinking beer and it's updating the image in real-time.
150800	151520	It's pretty wild.
151520	152720	I think people are going to enjoy that.
153920	157200	So yeah, that's what most people are going to see in the world.
157200	159360	We're rolling that out, not everywhere,
159360	161760	but we're starting in a handful of countries
161760	164080	and we'll do more over the coming weeks and months.
165520	167760	So that I think is going to be a pretty big deal.
168880	170560	And I'm really excited to get that in people's hands.
171040	173680	It's a big step forward for Met AI.
175440	177680	But I think if you want to get under the hood a bit,
178480	181520	the Llama 3 stuff is obviously the most technically interesting.
181520	184240	So we're basically, for the first version,
184240	188720	we're training three versions, an 8 billion and a 70 billion,
188720	193360	which we're releasing today and a 405 billion dense model,
193360	196080	which is still training, so we're not releasing that today.
196480	203040	But the 8 and 70, I mean, I'm pretty excited about how they turned out.
203040	206800	I mean, they're leading for their scale.
208800	212880	You know, it's, I mean, we'll release a blog post with all the benchmarks
212880	215200	so people can check it out themselves and obviously it's open source
215200	216720	so people get a chance to play with it.
217840	220080	We have a roadmap of new releases coming
220960	224640	that are going to bring multi-modality, more multi-linguality,
225600	227520	bigger context windows to those as well.
228720	231520	And then, you know, hopefully sometime later in the year,
231520	236320	we'll get to roll out the 405, which I think is, in training,
236320	241040	it's still training, but for where it is right now in training,
241040	248720	it is already at around 85 mmlu and just,
248720	250640	we expect that it's going to have leading benchmarks
250640	252800	on a bunch of the benchmarks.
252800	254640	So, I'm pretty excited about all of that.
254640	258480	I mean, the 70 billion is great too.
258480	259440	I mean, we're releasing that today.
259440	263200	It's around 82 mmlu and has leading scores on math and reasoning.
263200	265280	So, I mean, it's, I think just getting this in people's hands
265280	266480	is going to be pretty wild.
266480	267040	Oh, interesting.
267040	268240	Yeah, that's the first time hearing this benchmark.
268240	268960	That's super impressive.
268960	275840	Yeah, and the 8 billion is nearly as powerful
275840	278240	as the biggest version of Llama 2 that we released.
278240	281120	So, it's like the smallest Llama 3 is basically as powerful
281200	283360	as the biggest Llama 2.
283360	285680	Okay, so before we dig into these models,
285680	287280	I actually want to go back in time.
287840	291520	2022 is, I'm assuming, when you started acquiring these H100s,
292560	294960	or you can tell me when, where you're like,
294960	296400	stock price is getting hammered.
296400	298400	People are like, what's happening with all this capex?
298400	299920	People aren't buying the metaverse.
299920	301360	And presumably, you're spending that capex
301360	302720	to get these H100s.
302720	305040	How, back then, how did you know to get the H100s?
305040	306560	How did you know we'll need the GPUs?
307760	310160	I think it was because we were working on Reels.
310160	315440	So, we got into this situation where we always
316160	319520	want to have enough capacity to build something
319520	323280	that we can't quite see that were on the horizon yet.
324160	326640	And we got into this position with Reels,
326640	330800	where we needed more GPUs to train the models.
330800	333920	It was this big evolution for our services,
333920	335760	where instead of just ranking content from people
335760	339200	who you follow, or your friends, and whatever pages you follow,
341040	345040	we made this big push to basically start recommending
345600	347280	what we call unconnected content,
347280	349520	to basically connect content from people
349520	350800	or pages that you're not following.
350800	355760	So, now, kind of the corpus of kind of content candidates
355760	357840	that we could potentially show you expanded from,
357840	359680	you know, on the order of thousands
359680	362480	to on the order of hundreds of millions.
362480	364240	So, completely different infrastructure.
364960	368720	And we started working on doing that,
368720	372960	and we were constrained on basically the infrastructure
372960	375600	that we had to catch up to what TikTok was doing
375600	376880	as quickly as we would have wanted to.
377760	379040	So, I basically looked at that, and I was like,
379040	382560	hey, we have to make sure that we're never in this situation again.
382560	386320	So, let's order enough GPUs to do what we need to do
386320	388640	on Reels and ranking content and feed,
388640	390480	but let's also double that, right?
390480	392800	Because, again, like our normal principle is,
392800	394240	there's going to be something on the horizon
394240	395040	that we can't see yet.
395040	395840	Did you know it would be AI?
396400	398480	Well, we thought it would be,
399440	400320	we thought it was going to be something
400320	402400	that I had to do with training large models, right?
402400	404000	I mean, but at the time, I thought it was probably
404000	406080	going to be more something that I had to do with content.
406080	406800	But I don't know.
406800	409520	I mean, it's almost just the pattern matching
409520	413680	and running the company is there's always another thing, right?
413680	415920	So, I'm not even sure I had, at that time,
415920	418480	I was so deep in just, you know, trying to get,
418480	420800	you know, the recommendations working for Reels
420800	422560	and other content, because I mean,
422560	424960	that's just such a big unlock for Instagram and Facebook
424960	426720	to now being able to show people content
426720	428240	that's interesting to them that they're from people
428240	429840	that they're not even following.
429840	435280	But, yeah, that ended up being a very good decision
435280	436000	in retrospect.
436000	436960	Yeah, yeah.
436960	438320	Okay, and it came from being behind.
438320	440640	So, then it wasn't like I was, you know,
440640	442160	it wasn't like, oh, I was so far ahead.
442160	443440	Actually, most of the times, I think,
443440	445680	where we kind of make some decision
445680	448800	that ends up seeming good is because we messed something up
448800	450640	before and just didn't want to repeat the mistake.
451280	452160	This is a total detour,
452160	453840	but actually, I want to ask about this while we're on this.
453840	455680	We'll get back to AI in a second.
456480	458240	So, you didn't suffer one billion,
458240	459520	but presumably there's some amount
459520	460720	you would have sold for, right?
460720	462480	Did you write down in your head, like,
462480	465120	I think the actual valuation of Facebook at the time is this
465120	466960	and they're not actually getting the valuation right?
466960	469040	Like, the average $5 trillion, of course, you would have sold.
469040	472000	So, like, how did you think about that choice?
472720	473520	Yeah, I don't know.
473520	476080	I mean, look, I think some of these things are just personal.
478160	481040	I don't know at the time that I was sophisticated enough
481040	482080	to do that analysis,
482080	485280	but I had all these people around me who were making
485280	489120	all these arguments for how, like, a billion dollars was,
489120	491200	you know, it's like, here's the revenue that we need to make
491200	492640	and here's how big we need to be
492640	494640	and, like, it's clearly so many years in the future.
494640	496560	Like, and it was, it was very far ahead
496560	498080	of where we were at the time.
498080	503040	And I don't know, I didn't really have the financial sophistication
503040	506240	to really even engage with that kind of debate.
506240	509520	I just, I think I sort of deep down believed
509520	510640	in what we were doing.
510640	512000	And I did some analysis.
513840	518640	I was like, okay, well, what would I go do if I wasn't doing this?
518640	521360	It's like, well, I really like building things
521360	523120	and I like helping people communicate
523120	526560	and I like understanding what's going on with people
526560	528000	and the dynamics between people.
528000	530080	So, I think if I sold this company,
530080	531920	I'd just go build another company like this.
531920	534480	And I kind of like the one I have.
534480	538560	So, so, I mean, you know, what's, why, why, right?
538560	540960	But I don't know.
540960	544080	I think a lot of the biggest bets that people make
545520	548240	are often just based on conviction and values.
549200	553200	Not, it's actually usually very hard to do the analyses
553200	554560	trying to connect the dots forward.
554560	555120	Yeah.
555120	558160	So, you've had Facebook AI research for a long time.
559280	561760	Now it's become seemingly central to your company.
563120	567040	At what point did making AGI or whatever,
567040	568640	however you consider that mission,
568640	569440	at what point is that like,
569440	571520	this is a Cree priority of what Meta is doing?
572800	572960	Yeah.
572960	574960	I mean, it's been a big deal for a while.
574960	578720	So, we started fair about 10 years ago.
578720	584320	And the idea was that along the way to general intelligence
584320	586640	or AI, like full AI, whatever you want to call it,
587360	589280	there can be all these different innovations
589280	591680	and that's going to just improve everything that we do.
591680	595440	So, we didn't kind of conceive it as a product.
595440	597520	It was more kind of a research group.
598080	601280	And over the last 10 years,
601280	603360	it has created a lot of different things
603360	606320	that have basically improved all of our products
607040	609840	and advanced the field and allowed other people in the field
609840	611760	to create things that have improved our products too.
611760	613040	So, I think that that's been great.
613600	618400	But there's obviously a big change in the last few years
618400	620560	when ChatGPT comes out,
621200	623520	the diffusion models or an image creation come out.
624000	625760	I mean, this is some pretty wild stuff
625760	627920	that I think is pretty clearly going to affect
627920	632320	how people interact with every app that's out there.
634400	639600	At that point, we started a second group, the GenAI group,
640480	644400	with the goal of basically bringing that stuff into our products,
644400	646480	so building leading foundation models
646480	648880	that would sort of power all these different products.
649440	651920	And initially, when we started doing that,
653760	655120	the theory at first was, hey,
655680	658560	a lot of the stuff that we're doing is pretty social, right?
658560	661920	So, it's helping people interact with creators,
661920	664880	helping people interact with businesses.
664880	668160	So, the businesses can sell things or do customer support
668160	671200	or basic assistant functionality for,
672160	674880	you know, whether it's for our apps or the smart glasses
674880	676960	or VR or like all these different things.
677600	681600	So, initially, it wasn't completely clear
681600	685280	that you were going to need kind of full AGI
686080	687680	to be able to support those use cases.
687680	689520	But then through working on them,
689520	691520	I think it's actually become clear that you do, right?
691520	692480	In all these subtle ways.
692480	695680	So, for example, for Llama 2, when we were working on it,
695680	697200	we didn't prioritize coding.
697200	699280	And the reason why we didn't prioritize coding
699280	701520	is because people aren't going to ask MetaAI
701520	703360	a lot of coding questions in WhatsApp.
703360	704400	Now they will, right?
704400	704880	Well, I don't know.
704880	706960	I'm not sure that WhatsApp is like the UI
706960	708960	that people are going to be doing a lot of coding questions.
708960	711120	So, we're like, all right, look, in terms of the things that,
711200	712880	you know, or Facebook or Instagram
712880	714400	or, you know, those different services,
714400	718160	maybe the website, right, meta.ai that we're launching, I think.
718160	721440	But the thing that was sort of, I think,
721440	726160	has been a somewhat surprising result over the last 18 months
726160	730720	is that it turns out that coding is important
730720	732480	for a lot of domains, not just coding, right?
732480	735440	So, even if people aren't asking coding questions to the models,
736160	740080	training the models on coding helps them just be more rigorous
740080	743360	and answer the question and kind of help reason
743360	745280	across a lot of different types of domains.
745280	746400	Okay, so that's one example where it's like,
746400	747360	all right, so for Llama 3,
747360	749680	we're like really focused on training it with a lot of coding
749680	750320	because it's like, all right,
750320	752160	that's going to make it better on all these things,
752160	753520	even if people aren't answering,
753520	755280	aren't asking primarily coding questions.
756080	757680	Reasoning, I think, is another example.
758320	761600	It's like, okay, yeah, maybe you want to chat with a creator
761600	762960	or, you know, you're a business
762960	765200	and you're trying to interact with a customer.
765200	766720	You know, that interaction is not just like,
766720	769200	okay, the person sends you a message
769200	770640	and you just reply, right?
770640	773040	It's like a multi-step interaction
773040	774560	where you're trying to think through
774560	776400	how do I accomplish the person's goals
776400	778720	and, you know, a lot of times when a customer comes,
778720	780640	they don't necessarily know exactly
780640	782640	what they're looking for or how to ask their questions.
782640	784960	So, it's not really the job of the AI
784960	786400	to just respond to the question.
786400	788640	It's like, you need to kind of think about it more holistically.
788640	790400	It really becomes a reasoning problem, right?
790400	791520	So, if someone else, you know,
791520	794160	solves reasoning or makes good advances on reasoning,
794160	797040	and we're sitting here with a basic chat bot,
797040	798480	then, like, our product is lame
798480	800240	compared to what other people are building.
800240	801520	So, it's like, it's okay.
801520	803360	So, at the end of the day, we've got,
803360	805520	we, you know, we basically realized
805520	807200	we've got to solve general intelligence
808320	811040	and we just kind of upped the ante and the investment
811040	812480	to make sure that we could do that.
812480	818480	So, the version of Lama that's going to solve
818480	820960	all these use cases for users,
820960	823040	is that the version that will be powerful enough
823040	825840	to, like, replace a programmer you might have in this building?
826720	827920	I mean, I just think that all this stuff
827920	829360	is going to be progressive over time.
829360	830960	But, in case, Lama 10.
833920	836400	I mean, I think that there's a lot baked into that question.
836400	838800	I'm not sure that we're replacing people
838800	842080	as much as giving people tools to do more stuff.
842080	843920	Is a programmer in this building 10x more productive
843920	844320	after Lama 10?
844320	845120	I would have more.
845120	848320	But no, I mean, look, I'm not,
848320	849520	I don't believe that there's, like,
849520	852560	a single threshold of intelligence for humanity,
852560	854560	because, I mean, people have different skills.
854560	856400	And at some point, I think that AI is going to be,
857520	861280	is probably going to surpass people at most of those things,
861280	863040	depending on how powerful the models are.
863040	866960	But I think it's progressive.
866960	868400	And I don't think AGI is one thing.
868400	870960	I think it's, you're basically adding different capabilities.
870960	875360	So, multimodality is kind of a key one that we're focused on now,
875360	878000	initially with photos and images and text,
878000	879360	but eventually with videos.
879360	881280	And then, because we're so focused on the metaverse,
881360	883440	kind of 3D type stuff is important.
885040	887680	One modality that I'm pretty focused on that I haven't seen
887680	890560	as many other people in the industry focus on this
890560	893680	is sort of like emotional understanding.
893680	896320	Like, I mean, so much of the human brain
896320	899200	is just dedicated to understanding people
899200	902240	and kind of like understanding your expressions and emotions.
902240	904800	And I think that that's like its own whole modality, right?
904800	906400	That, I mean, you could say, okay,
906400	907840	maybe it's just video or image,
907840	910880	but it's like clearly a very specialized version of those too.
910880	912880	So, there's all these different capabilities
912880	917680	that I think you wanna basically train the models to focus on,
917680	919840	as well as getting a lot better at reasoning,
919840	921040	getting a lot better at memory,
921040	923040	which I think is kind of its own whole thing.
923040	924480	It's, I mean, I don't think we're gonna be,
924480	926720	you know, primarily shoving context
926720	931120	or kind of things into a query context window in the future
931120	932880	to ask more complicated questions.
932880	935360	I think that there'll be kind of different stores of memory
935360	936480	or different custom models
936480	939760	that are maybe more personalized to people.
939760	941440	But I don't know, I think that these are all
941440	942640	just different capabilities.
942640	944400	And then obviously making them big and small,
944400	946800	we care about both because, you know, we wanna,
946800	949200	you know, if you're running something like meta AI,
949200	952480	then we have the ability to, that's pretty server-based,
952480	954320	but we also want it running on smart glasses.
954320	956720	And, you know, there's not a lot of space in smart glasses.
956720	960080	So, you wanna have something that's very efficient for that.
960080	963120	What is the use case that if you're doing tens of billions
963120	964000	of dollars worth of inference,
964000	965600	or even eventually hundreds of billions of dollars
965600	969120	worth of inference, using intelligence in an industrial scale,
969120	970240	what is the use case?
970240	971360	Is it simulations?
971360	973120	Is it the AIs that will be in the metaverse?
973120	975520	What will we be using the data centers for?
979120	981200	I mean, our bet is that it's gonna,
981200	983440	this is basically gonna change all of the products, right?
983440	987760	So, I think that there's gonna be a kind of meta AI
987760	989440	general assistant product.
989440	992480	And I think that that will shift from something
992480	994080	that feels more like a chat bot
994080	995600	where it's like you just ask a question
995600	997440	and it kind of formulates an answer
997440	998720	to things where you're increasingly
998720	1000320	giving it more complicated tasks
1000320	1001600	and that goes away and does them.
1002400	1004480	So, that's gonna take a lot of inference.
1004480	1006240	It's gonna take a lot of compute in other ways too.
1007920	1010880	Then I think that there's a big part of what we're gonna do
1010880	1016560	that is like interacting with other agents for other people.
1016560	1018400	So, whether it's businesses or creators,
1020160	1021760	I guess a big part of my theory on this
1021760	1024080	is that there's not just gonna be like one singular AI
1024080	1025280	that you interact with,
1025280	1028960	because I think every business is gonna like want an AI
1028960	1030240	that represents their interests.
1030240	1032800	They're not gonna like wanna primarily interact with you
1032800	1036240	through an AI that is gonna sell their competitors' customers.
1036240	1038160	So, sorry, their competitors' products.
1039040	1045280	So, yeah, so I think creators is gonna be a big one.
1045280	1048640	I mean, there are about 200 million creators on our platforms.
1048640	1050480	They all basically have the pattern where
1051600	1053040	they want to engage their community,
1053040	1054560	but they're limited by hours in the day
1054560	1056800	and their community generally wants to engage them,
1056800	1058800	but they don't have, they're limited by hours in the day.
1059520	1063840	So, if you could create something where an AI could basically,
1064400	1066640	that creator can basically own the AI
1066640	1068080	and train it in the way that they want
1069920	1071600	and can engage their community,
1071600	1073760	I think that that's gonna be super powerful too.
1073760	1076240	So, I think that there's gonna be a ton of engagement
1076240	1077200	across all these things.
1079440	1081120	But these are just the consumer use cases.
1081120	1082800	I mean, I think when you think about stuff like,
1083680	1086640	I mean, I run our foundation,
1087440	1089120	Chan Zuckerberg Initiative with my wife,
1089120	1091200	and we're doing a bunch of stuff on science,
1091200	1093760	and there's obviously a lot of AI work
1093760	1096880	that I think is gonna advance science and healthcare
1096880	1097680	and all these things too.
1097680	1099200	So, I think that it's like,
1099200	1101040	this is, I think, an end up affecting
1101040	1105360	basically every area of the products and the economy.
1105360	1107200	The thing you mentioned about an AI
1107200	1109040	that can just go out and do something for you
1109040	1111520	that's multi-step, is that a bigger model?
1111520	1114240	Is that you'll make, like, Lama 4 will still,
1114240	1115840	there'll be a version that's still 70B,
1115840	1118000	but will just be, you'll just train it on the right data,
1118000	1119840	and that will be super powerful.
1119840	1121280	Like, what does the progression look like?
1121280	1123600	Is it scaling? Is it just same size,
1123600	1125680	but different banks like you were talking about?
1129200	1131600	I don't know that we know the answer to that.
1131600	1135840	So, I think one thing that seems to be a pattern
1135840	1138000	is that you have the Lama,
1138000	1139680	sorry, the Lama model,
1139680	1144000	and then you build some kind of other
1144000	1146320	application-specific code around it, right?
1146320	1148640	So, some of it is the fine-tuning for the use case,
1148640	1152560	but some of it is just like logic for, okay, how,
1154320	1156240	like, how Met AI should integrate,
1157040	1159280	that should work with tools like Google or Bing
1159280	1160560	to bring in real-time knowledge.
1160560	1162080	I mean, that's not part of the base Lama model.
1162080	1162960	That's like part of it.
1162960	1165840	Okay, so, for Lama 2, we had some of that,
1166640	1169440	and it was a little more kind of hand-engineered.
1169440	1171440	And then part of our goal for Lama 3
1172080	1174480	was to bring more of that into the model itself.
1175120	1176320	And, but for Lama 3,
1176320	1179440	as we start getting into more of these agent-like behaviors,
1180000	1182880	I think some of that is going to be more hand-engineered.
1182880	1184960	And then I think our goal for Lama 4
1184960	1186800	will be to bring more of that into the model.
1186800	1190080	So, I think at each point, like at each step along the way,
1190080	1193280	you kind of have a sense of what's going to be possible
1193280	1194000	on the horizon.
1194000	1196080	You start messing with it and hacking around it.
1196880	1199360	And then I think that that helps you hone your intuition
1199680	1201520	for what you want to try to train
1201520	1203440	into the next version of the model itself.
1203440	1203920	Interesting.
1203920	1204960	Which makes it more general,
1204960	1207200	because obviously anything that you're hand-coding
1207200	1210240	is, you know, you can unlock some use cases,
1210240	1212320	but it's just inherently brittle and non-general.
1213520	1214400	Hey, everybody.
1214400	1216720	Real quick, I want to tell you about a tool
1216720	1219120	that I wish more applications used.
1219120	1221920	So, obviously, you've noticed every single company
1221920	1225120	is trying to add an AI chatbot to their website.
1225120	1228480	But as a user, I usually find them really annoying
1228480	1231600	because they give these long, generic, often useless answers.
1232320	1235360	Command Bar is a user assistant that you can just embed
1235360	1236880	into your website or application.
1237440	1240640	And it feels like you're talking to a friendly human support
1240640	1243440	agent who is browsing with you and for you.
1244000	1247520	And it's much more personalized than a regular chatbot.
1247520	1249440	It can actually look up users' history
1249440	1251600	and respond differently based on that.
1251600	1254560	It can use APIs to perform actions.
1254560	1258080	It can even practically nudge users to explore new features.
1258560	1260160	One thing that I think is really cool
1260160	1262640	is that instead of just outputting text,
1262640	1265680	Command Bar can kind of just say, here, let me show you
1265680	1267920	and start browsing alongside the user.
1268560	1271120	Anyways, they're in a bunch of great products already.
1271120	1274960	You can learn more about them at commandbar.com.
1275520	1277520	Thanks to them for sponsoring this episode.
1277520	1278560	And now back to Mark.
1279120	1280560	What do you say into the model itself?
1280560	1284080	You train it on the thing that you want in the model itself?
1284080	1285920	What do you mean by into the model itself?
1285920	1289520	Well, I think the example that I gave for Llama 2,
1289520	1297040	where for Llama 2, the tool use was very specific.
1297920	1300800	Whereas Llama 3 has the ability to have much better tool use.
1300800	1304080	So we don't have to hand code all the stuff
1304080	1307280	to have it use Google to go do a search.
1308160	1309680	It just kind of can do that.
1311840	1314800	And similarly for coding and kind of running code
1314800	1316640	and a bunch of stuff like that.
1319040	1321040	But I think once you kind of get that capability,
1321920	1324960	then you get a peek of, okay, well, what can we start doing next?
1324960	1327760	Okay, well, I don't necessarily want to wait until Llama 4 is around
1327760	1329280	to start building those capabilities.
1329280	1330640	So let's start hacking around it.
1330640	1333040	And so you do a bunch of hand coding
1333040	1336320	and that makes the products better for the interim.
1336320	1338240	But then that also helps show the way
1338240	1341280	of what we want to try to build into the next version of the model.
1341280	1343600	What is the community fine tune of Llama 3
1343600	1344640	you're most excited by?
1344640	1346400	Maybe not the one that will be most useful to you,
1346400	1348320	but Jess, you'll just enjoy playing it with the most.
1349760	1351200	They like fine tune it on antiquity
1351200	1352960	and you'll just be like talking to Virgil or something.
1352960	1354320	What are you excited about?
1354320	1354880	I don't know.
1356320	1358400	I mean, I think the nature of the stuff is it's like,
1359600	1361200	you get surprised, right?
1361200	1364960	So I think like any specific thing that I sort of
1366560	1369520	thought would be valuable, we'd probably be building, right?
1369520	1374960	So, but I think you'll get distilled versions.
1374960	1377120	I think you'll get kind of smaller versions.
1377120	1381760	I mean, one thing that I think is 8 billion,
1381760	1385440	I don't think is quite small enough for a bunch of use cases, right?
1385440	1389680	I think like over time, I'd love to get a billion parameter model
1389680	1391680	or a 2 billion parameter model
1391680	1394720	or even like a, I don't know, maybe like a 500 million parameter model
1394720	1395680	and see what you can do with that.
1395680	1399520	Because I mean, as they start getting, if with 8 billion parameters
1399520	1403440	we're basically nearly as powerful as the largest llama 2 model,
1403440	1405360	then with a billion parameters,
1405360	1407200	you should be able to do something that's interesting, right?
1407200	1410720	And faster, good for classification
1410720	1413200	or a lot of kind of like basic things that people do
1413200	1417920	before kind of understanding the intent of a user query
1417920	1419600	and feeding it to the most powerful model
1419600	1422320	to kind of hone what the prompt should be.
1424000	1424480	So I don't know.
1424480	1426640	I think that's one thing that maybe the community can help fill in.
1426640	1429200	But I mean, we'll also, we're also thinking about getting around
1429200	1431520	to distilling some of these ourselves,
1431520	1435840	but right now the GPUs are pegged training the 405.
1435840	1437840	So what, okay, so you have all these GPUs,
1440080	1442080	I think 350,000 by the end of the year.
1442080	1442960	That's the whole fleet.
1442960	1450000	I mean, we built two, I think it's like 22, 24,000 clusters
1450000	1452320	that are kind of the single clusters that we have
1452320	1453440	for training the big models.
1453840	1456000	I mean, obviously across a lot of the stuff that we do,
1456000	1459120	a lot of our stuff goes towards training like reels models
1459120	1462160	and like Facebook news feed and Instagram feed.
1462160	1463840	And then inference is a huge thing for us
1463840	1465360	because we serve a ton of people, right?
1465360	1472320	So our ratio of inference compute required to training
1472320	1474640	is probably much higher than most other companies
1474640	1476800	that are doing this stuff just because of the sheer volume
1476800	1478800	of the community that we're serving.
1478800	1479840	Yeah, yeah.
1479840	1482240	That was really interesting in the material they shared with me before
1482240	1485200	that you trained it on more data than is computer optimal
1485200	1487680	just for training because the inference is such a big deal
1487680	1489360	for you guys and also for the community
1489360	1490960	that it makes sense to just have this thing
1490960	1492800	and have a trillion to tokens in there.
1492800	1493520	Yeah, yeah.
1493520	1495920	Although, and one of the interesting things about it
1495920	1498320	that we saw even with the 70 billion is we thought
1498320	1502240	it would get more saturated at, you know,
1502240	1504720	it's like we trained on around 15 trillion tokens.
1504720	1505520	Yeah.
1505520	1507920	We, I guess our prediction going in was that
1508560	1510720	it was going to ask some to it more,
1510720	1513920	but even by the end it was still learning, right?
1513920	1517200	It's like we probably could have fed it more tokens
1517200	1519040	and it would have gotten somewhat better.
1519040	1521200	But I mean, at some point, you know, you're running a company
1521200	1524240	you need to do these meta reasoning questions of like,
1524240	1526400	all right, how do I want to spend our GPUs
1526400	1529040	on like training this 70 billion model further?
1529040	1531280	Do we want to kind of get on with it
1531280	1533840	so we can start testing hypotheses for Llama 4?
1533840	1536800	So we kind of needed to make that call.
1536800	1538000	And I think we got it,
1538000	1539280	I think we got to a reasonable balance
1539760	1541200	for this version of the 70 billion.
1542880	1544080	There will be others in the future
1544080	1545760	where, you know, 70 billion multimodal one
1545760	1547760	that'll come over the next period.
1547760	1551280	But yeah, I mean, that was fascinating
1551280	1553280	that you could just, that it's the architectures
1553280	1555600	at this point can just take so much data.
1555600	1556400	Yeah, that's really interesting.
1556400	1558240	So what is this imply by future models?
1558960	1562320	You mentioned that the Llama 3 8B is better
1562320	1563520	than the Llama 270B?
1563520	1564720	No, no, no, it's nearly as good.
1564720	1565440	Okay.
1565440	1566320	I don't overstep.
1566320	1567440	But does that mean like the Llama 4?
1567440	1568320	The same order of magnitude.
1568400	1569280	Does that mean like the Llama 4?
1569280	1570800	70B will be as good as the Llama 3?
1570800	1571760	4 or 5B?
1571760	1575120	I mean, this is one of the great questions, right?
1575120	1577920	That I think no one knows is basically,
1579680	1582800	you know, it's one of the trickiest things in the world
1582800	1585280	to plan around is when you have an exponential curve,
1585280	1586880	how long does it keep going for?
1587520	1592080	And I think it's likely enough that it will keep going,
1592080	1596240	that it is worth investing the tens or, you know,
1596320	1599360	100 billion plus in building the infrastructure
1599360	1602480	to assume that if that kind of keeps going,
1602480	1604720	you're going to get some really amazing things
1604720	1606480	that are just going to make amazing products.
1607040	1610640	But I don't think anyone in the industry can really tell you
1611600	1615200	that it will continue scaling at that rate for sure, right?
1615200	1618640	In general, in history, you hit bottlenecks at certain points.
1618640	1620800	And now there's so much energy on this
1620800	1623920	that maybe those bottlenecks get knocked over pretty quickly.
1623920	1628080	But I don't know. I think that's an interesting question.
1628080	1631120	What does the world look like where there aren't these bottlenecks?
1631120	1634240	Suppose like progress just continues at this pace,
1634240	1637520	which seems like plausible, like zooming out.
1637520	1639840	Well, they're going to be different bottlenecks.
1640560	1642800	Right. So if not training, then like, oh, yeah, go ahead.
1643680	1647600	Well, I think at some point, over the last few years,
1647600	1651200	I think there was this issue of GPU production.
1651200	1653760	Yeah. Right. So even companies that had the models,
1655200	1657120	sorry, that had the money to pay for the GPUs,
1658800	1660640	couldn't necessarily get as many as they wanted
1660640	1663120	because there were all these supply constraints.
1663120	1665680	Now I think that's sort of getting less.
1666240	1670320	So now I think you're seeing a bunch of companies think about,
1670320	1672640	wow, we should just like really invest a lot of money
1672640	1673840	in building out these things.
1673840	1676960	And I think that that will go for some period of time.
1677840	1682400	I think there's a, there is a capital question of like, okay,
1683280	1686640	at what point does it stop being worth it to put the capital in?
1686640	1689040	But I actually think before we hit that,
1689040	1691120	you're going to run into energy constraints.
1691120	1694080	Right. Because I just, I mean,
1694080	1699120	I don't think anyone's built a gigawatt single training cluster yet.
1699120	1701600	Right. And then you run into these things
1701600	1703200	that just end up being slower in the world.
1703200	1706400	Like getting energy permitted
1706400	1711040	is like a very heavily regulated government function.
1711040	1714320	Right. So you're going from on the one hand software,
1714320	1716240	which is somewhat regulated.
1716240	1718320	I'd argue that it is more regulated
1718320	1721360	than I think a lot of people in the tech community feel,
1721360	1722480	although it's obviously different.
1722480	1723680	If you're starting a small company,
1723680	1725680	maybe you feel that less if you're a big company,
1725680	1727360	you know, we just interact with people,
1727360	1729440	but different governments and regulators are,
1729440	1732560	you know, we have kind of lots of rules
1732560	1733600	that we need to kind of follow
1733600	1735280	and make sure we do a good job with around the world.
1736720	1738800	But I think that there's no doubt that like energy,
1738800	1742720	and if you're talking about building large new power plants
1742720	1745520	or large buildouts and then building transmission lines
1745520	1749920	that cross other private or public land,
1749920	1751760	that is just a heavily regulated thing.
1751760	1754480	So you're talking about many years of lead time.
1754480	1758800	So if we wanted to stand up to some like massive facility
1758800	1763040	to power that, I think that that is,
1764080	1766800	that's a very long-term project, right?
1766800	1769600	And so I don't know, I think that that's,
1769600	1770960	I think people will do it,
1770960	1773280	but I don't think that this is like something
1773280	1775440	that can be quite as magical as just like,
1775440	1777760	okay, you get a level of AI and you get a bunch of capital
1777760	1779200	and you put it in and then like all of a sudden
1779200	1781440	the models are just going to kind of like interest,
1781440	1783840	like I think you do hit different bottlenecks along the way.
1783840	1786320	Yeah. Is there something, a project,
1786320	1788160	maybe I realized maybe not,
1788160	1791360	that even a company like Meta doesn't have the resources for,
1791360	1794640	like if your R&D budget or CapEx budget was 10x what it is now,
1794640	1796800	then you could pursue it, like it's in the back of your mind,
1796800	1800000	but Meta today, maybe you could like,
1800000	1801680	even you can't even issue a stock or bond for it,
1801680	1803520	it's like just 10x bigger than your budget.
1803520	1805280	Well, I think energy is one piece, right?
1806400	1810160	I think we would probably build out bigger clusters
1810160	1815840	than we currently can if we could get the energy to do it.
1815840	1821920	So I think that's fundamentally money bottlenecked in the limit,
1821920	1823040	like if you had a trillion dollars.
1823040	1824640	I think it's time, right?
1826320	1828720	Well, if you look at it in terms of,
1828720	1831520	but it depends on how far the exponential curves go, right?
1831520	1834160	Like I think a number of companies are working on,
1834160	1836800	you know, right now I think a lot of data centers
1836800	1839120	are on the order of 50 megawatts or 100 megawatts,
1839120	1841360	or like a big one might be 150 megawatts.
1841360	1844160	Okay, so you take a whole data center and you fill it up with
1844240	1846160	just all the stuff that you need to do for training
1846160	1847520	and you build the biggest cluster you can.
1847520	1849680	I think that's kind of,
1849680	1851760	I think a bunch of companies are running at stuff like that.
1853200	1857920	But then when you start getting into building a data center
1857920	1862640	that's like 300 megawatts or 500 megawatts or a gigawatt,
1862640	1866320	I mean, just no one has built a single gigawatt data center yet.
1866320	1867600	So I think it will happen, right?
1867600	1868800	I mean, this is only a matter of time,
1868800	1871840	but it's not going to be like next year, right?
1872400	1876240	I think that some of these things will take, I don't know,
1877040	1879120	some number of years to build out.
1879120	1880880	And then the question is, okay, well, if you,
1882400	1884240	I mean, just to, I guess, put this in perspective,
1885440	1889520	I think a gigawatt, it's like around the size of like
1890160	1892400	a meaningful nuclear power plant
1892400	1894800	only going towards training a model.
1894800	1896400	Didn't Amazon do this?
1896400	1898960	There's like, they have a 950 megawatt thing.
1898960	1900560	Yeah, I'm not exactly sure what you did.
1901120	1902240	What they did, you'd have to ask them.
1903840	1905280	But it doesn't have to be in the same place, right?
1905280	1907200	If distributed training works, it can be distributed.
1907200	1908080	That I think is a big question.
1908080	1908400	Yeah.
1908400	1909920	Right, is basically how that's going to work.
1909920	1911120	And I do think in the future,
1912160	1916560	it seems quite possible that more of what we call training
1916560	1921680	for these big models is actually more along the lines
1922240	1924880	of inference generating synthetic data
1924880	1926560	to then go feed into the model.
1926560	1928560	So I don't know what that ratio is going to be,
1928560	1932400	but I consider the generation of synthetic data
1932400	1934480	to be more inference than training today.
1934480	1936800	But obviously, if you're doing it in order to train a model,
1936800	1938720	it's part of the broader training process.
1939280	1943440	So I don't know, that's an open question,
1943440	1945440	is to kind of where, what the balance of that
1945440	1946480	and how that plays out.
1946480	1949760	If that's the case, would that potentially also
1949760	1951200	be the case with Lama 3?
1951200	1954000	And maybe like Lama 4 onwards, where you put this out
1954000	1955840	and if somebody has a ton of compute,
1955840	1957680	then using the models that you've put out,
1957760	1959920	you can just keep making these things arbitrarily smarter.
1961120	1964720	Some Kuwait or UAE or some random country has a ton of compute,
1965600	1968240	and they can just actually just use Lama 4
1968240	1969440	to just make something much smarter.
1972240	1975600	I do think that there are going to be dynamics like that,
1975600	1980960	but I also think that there is a fundamental limitation
1981520	1985760	on kind of the network architecture,
1986320	1988080	the kind of model architecture.
1988080	1991360	So I think like a 70 billion model
1992320	1994560	that kind of we trained with the Lama 3 architecture
1994560	1996800	can get better, it can keep going.
1996800	2000160	Like I was saying, we felt like if we kept on feeding it
2000160	2004080	more data or rotated the high value tokens through again,
2004080	2006160	then it would continue getting better.
2006880	2011520	But, and we've seen a bunch of other people around the world,
2012480	2015280	you know, different companies basically take the Lama 2
2015920	2018240	70 billion base, like take that model architecture
2018240	2019200	and then build a new model.
2021360	2024160	It's still the case that when you make a generational improvement
2024160	2027200	to the kind of Lama 3 70 billion or the Lama 3 405,
2027200	2029920	there's nothing open source anything like that today, right?
2029920	2033120	Like it's not, I think that that's like,
2033120	2034960	it's a big step function
2034960	2037120	and what people are going to be able to build on top of
2037120	2039760	that I don't think can go infinitely from there.
2039840	2042480	I think it can, there can be some optimization in that
2042480	2043920	until you get to the next step function.
2044800	2045680	Yeah. Okay.
2045680	2048720	So let's zoom out a little bit from specific models
2048720	2052000	and even the many years lead times you would need
2052000	2053920	to get energy approvals and so on.
2053920	2056240	Like big picture, these next couple of decades,
2056240	2057280	what's happening with AI?
2058080	2060000	Does it feel like another technology,
2060000	2061440	like metaverse or social,
2061440	2063520	or does it feel like a fundamentally different thing
2063520	2064720	in the course of human history?
2065600	2070000	I think it's going to be pretty fundamental.
2070000	2073280	I think it's going to be more like the creation
2073280	2076160	of computing in the first place, right?
2076160	2081840	So you'll get all these new apps in the same way
2082640	2085440	that when you got the web or you got mobile phones,
2085440	2088800	you got like people basically rethought all these experiences
2088800	2090400	and a lot of things that weren't possible
2090400	2091760	before now became possible.
2092080	2093440	Something that will happen,
2093440	2096240	but I think it's a much lower level innovation.
2096240	2099040	It's going to be more like going from
2099040	2102240	people didn't have computers to people have computers,
2102240	2103200	is my sense.
2105360	2109440	But it's also, it's, I don't know,
2109440	2114400	it's very hard to reason about exactly how this goes.
2114400	2117440	I tend to think that, you know,
2117440	2119120	in like the cosmic scale, obviously,
2119360	2122480	it'll happen quickly over a couple of decades or something.
2122480	2125680	But I do think that there is some set of people
2125680	2127680	who are afraid of like, you know,
2127680	2130080	it really just kind of spins and goes from being
2130080	2132960	like somewhat intelligent to extremely intelligent overnight.
2132960	2134880	And I just think that there's all these physical constraints
2134880	2137360	that make that, so that that's unlikely to happen.
2137360	2141280	I just don't, I don't really see that playing out.
2141280	2142480	So I think you'll have,
2142480	2144720	I think we'll have time to kind of acclimate a bit,
2144720	2146320	but it will really change.
2146320	2149760	The way that we work and give people all these creative tools
2149760	2152880	to do different things that they, yeah.
2152880	2154560	I think it's going to be,
2154560	2156720	it's going to really enable people to do
2156720	2159120	the things that they want a lot more, as is my view.
2160400	2161840	Okay, so maybe not overnight,
2161840	2164560	but is it your view that like on a cosmic scale,
2164560	2168000	if you think like humans evolved and then like AI happened
2168000	2170000	and then they like went out through the galaxy
2170000	2173200	or maybe it takes many decades, maybe it takes a century,
2173200	2174720	but like, you know,
2174960	2176320	is that like the grand scheme
2176320	2178000	of what's happening right now in history?
2179600	2180640	Sorry, in what sense?
2180640	2182800	I mean, in the sense that there were other technologies
2182800	2184160	like computers and even like fire,
2184160	2186800	but like the AI happening is as significant
2186800	2188640	as like humans evolving in the first place.
2189600	2191200	I think that's tricky.
2191200	2195040	I think people like to, you know,
2195040	2197360	the history of humanity, I think has been
2198240	2200640	people basically, you know,
2200640	2202400	thinking that certain things
2203360	2211200	of humanity are like really unique in different ways.
2211840	2215600	And then coming to grips with the fact
2215600	2216480	that that's not true,
2216480	2219280	but humanity is actually still super special, right?
2219280	2223920	So it's like we thought that the earth
2223920	2225200	was the center of the universe.
2225200	2228400	And it's like, it's not, but like humans
2228400	2229840	are still pretty awesome, right?
2229840	2231280	And pretty unique.
2232480	2234720	I think that another bias that people tend to have
2235520	2238000	is thinking that intelligence is somehow
2240160	2243360	kind of fundamentally connected to life.
2244000	2246960	And it's not actually clear that it is, right?
2246960	2248960	I think like people think that,
2251520	2252880	I mean, I don't know that we have
2252880	2254800	a clear enough definition of consciousness
2254800	2259920	or life to kind of fully interrogate this,
2259920	2263280	but there's all this science fiction about,
2263280	2264880	okay, you create intelligence
2264880	2267520	and now it like starts taking on all these human
2267520	2270320	like behaviors and things like that.
2270320	2272560	But I actually think that the current incarnation
2272560	2274400	of all this stuff at least kind of feels
2274400	2275440	like it's going in a direction
2275440	2277760	where intelligence can be pretty separated
2277760	2281200	from consciousness and agency and things like that,
2281200	2285040	that I think just makes it a super valuable tool.
2285040	2286000	So I don't know.
2286000	2288160	I mean, obviously it's very difficult to predict
2288160	2290320	what direction the stuff goes in over time,
2290320	2294160	which is why I don't think anyone should be dogmatic
2294160	2297360	about how they plan to develop it or what they plan to do.
2297360	2299680	I think you want to kind of look at like each release.
2299680	2301920	You know, it's like, we're obviously very pro open source,
2301920	2303520	but I haven't committed that we're going to like release
2303520	2304880	every single thing that we do.
2304880	2309120	But it's basically, I'm just generally very inclined
2309120	2310320	to thinking that open sourcing it
2310320	2312640	is going to be good for the community
2312640	2313840	and also good for us, right?
2313840	2316320	Because we'll benefit from the innovations.
2316560	2320080	But if at some point like there's some qualitative change
2320080	2322000	in what the thing is capable of,
2322000	2324720	and we feel like it's just not responsible to open source it,
2324720	2328640	then we won't, but so I don't know.
2328640	2330640	It's all very difficult to predict.
2330640	2331600	Yeah.
2331600	2333520	What is a kind of qualitative change,
2333520	2334800	like a specific thing?
2334800	2337920	You're training lamify, lamaphore, and you've seen this,
2337920	2339360	and like, you know what?
2339360	2340560	I'm not sure about open sourcing it.
2340560	2346800	I think that that, it's a little hard to answer that
2346800	2350160	in the abstract because there are negative behaviors
2350800	2354320	that any product can exhibit that as long as you can mitigate it,
2354960	2357440	it's like, it's okay, right?
2357440	2360640	So, I mean, there's bad things about social media
2360640	2362080	that we work to mitigate, right?
2362080	2364000	There's bad things about llama two
2364000	2366000	that we spend a lot of time trying to make sure
2366000	2369440	that it's not like, you know, helping people commit violent acts
2369440	2370400	or things like that, right?
2370400	2374480	I mean, that doesn't mean that it's like a kind of autonomous
2374480	2375760	or intelligent agent.
2375760	2377840	It just means that it's learned a lot about the world,
2377840	2379280	and it can answer a set of questions
2379280	2381840	that we think it would be unhelpful for it to answer.
2383440	2387200	So, I don't know.
2387200	2391680	I think the question isn't really what behaviors would it show.
2391680	2394000	It's what things would we not be able to mitigate
2394000	2398480	after it shows that, and I don't know.
2399120	2401520	I think that there's so many ways
2401520	2403440	in which something can be good or bad
2403440	2405360	that it's hard to actually enumerate them all up front.
2405360	2410320	If you even look at what we've had to deal with in social media
2410320	2412880	and the different types of harms, we've basically gotten to.
2412880	2416240	It's like, there's like 18 or 19 categories of harmful things
2416240	2419840	that people do, and we've basically built AI systems
2419840	2422080	to try to go identify what those things are
2422080	2423520	that people are doing and try to make sure
2423520	2426400	that that doesn't happen on our network as much as possible.
2426400	2428400	So, yeah, I think you can...
2428400	2430000	Over time, I think you'll be able to break down
2431600	2433200	this into more of a taxonomy, too,
2433200	2436000	and I think this is a thing that we spend time researching, too,
2436000	2437360	because we want to make sure that we understand that.
2438240	2440080	So, one of the things I asked Mark
2440080	2444000	is what industrial-scale use of LLMs would look like.
2444000	2446000	You see this in previous technological revolutions
2446000	2448400	where, at first, they're thinking in a very small-scale way
2448400	2449440	about what's enabled,
2449440	2452080	and I think that's what chatbots might be for LLMs.
2452080	2453920	And I think the large-scale use case
2453920	2456480	might look something like what V7 Go is.
2456480	2458400	And, by the way, it's made by V7 Labs
2458400	2459680	who's sponsoring this episode.
2460240	2462160	So, it's like a spreadsheet.
2462160	2466160	You put in raw information, like documents, images, whatever,
2466160	2467600	and they become rows,
2467600	2471120	and the columns are populated by an LLM of your choice.
2471120	2473680	And, in fact, I used it to prepare for Mark,
2473680	2476160	so I fed in a bunch of blog posts and papers
2476160	2477920	from Metas AI Research,
2477920	2480080	and, as you can see, if you're on YouTube,
2480080	2483280	it summarizes and extracts exactly the information I want
2483360	2484320	as columns.
2484320	2486480	And, obviously, mine is a small use case,
2486480	2489440	but you can imagine, for example, a company like FedEx
2489440	2492080	has to process half a million documents a day.
2492080	2494080	Obviously, a chatbot can't do that.
2494080	2495280	A spreadsheet can,
2495280	2497440	because this is just like a fire hose of intelligence
2497440	2498480	in there, right?
2498480	2500160	Anyways, you can learn more about them
2500160	2502560	at v7labs.com slash go,
2502560	2504160	or the link in the description.
2504160	2505280	Back to Mark.
2505280	2505840	Yeah.
2505840	2508480	Like, it seems to me it would be a good idea.
2508480	2510000	I would be disappointed in a future
2510000	2511680	where AI systems aren't broadly deployed
2511680	2513040	and everybody doesn't have access to them.
2514000	2514960	At the same time,
2514960	2517120	I want to better understand the mitigations,
2518000	2520480	because if the mitigation is the fine-tuning,
2520480	2522320	well, the whole thing about open weights
2522320	2525680	is that you can then remove the fine-tuning,
2525680	2527920	which is often superficial on top of these capabilities.
2527920	2530240	Like, if it's like talking on Slack
2530240	2532000	with a biology researcher,
2532000	2534000	and again, I think models are very far from this.
2534000	2535120	Right now, they're like Google search,
2536160	2537920	but I can show them my Petri disk
2537920	2538560	and they can next lane.
2538560	2541680	Like, here's why your smallpox sample didn't grow.
2541680	2542400	Here's what to change.
2543600	2544720	How do you mitigate that?
2544720	2547040	Because somebody can just fine-tune that in there, right?
2547840	2550640	Yeah. I mean, that's true.
2550640	2552800	I think a lot of people will basically use
2552800	2554880	the off-the-shelf model,
2554880	2558320	and some people who have basically bad faith
2558320	2560560	are going to try to strip out all the bad stuff,
2560560	2561600	so I do think that that's an issue.
2564720	2566720	The flip side of this is that,
2566720	2567920	and this is one of the reasons
2567920	2571280	why I'm kind of philosophically so pro-open source,
2572160	2576720	is I do think that a concentration of AI in the future
2577840	2580240	has the potential to be as dangerous
2580880	2583760	as kind of it being widespread.
2583760	2584560	So, I think a lot of people,
2585360	2587120	they think about the questions of,
2587120	2588400	okay, well, if we can do this stuff,
2588400	2590080	is it bad for it to be out wild?
2590080	2592640	Like, just kind of widely available.
2594800	2596720	I think another version of this is like,
2596720	2599360	okay, well, it's probably also pretty bad
2600000	2604800	for one institution to have an AI
2604800	2607920	that is way more powerful than everyone else's AI, right?
2607920	2609680	So, if you look at, like, I guess,
2609680	2612240	one security analogy that I think of is,
2614240	2618160	you know, it doesn't take AI to basically,
2618160	2620560	okay, there's security holes in so many different things,
2621200	2624720	and if you could travel back in time a year or two years,
2624720	2626880	right, it's like, that's not AI,
2626880	2628240	it's like you just, let's say you just have,
2628320	2630240	like, one year or two years more knowledge
2630240	2631520	of the security holes,
2631520	2633760	it's pretty much hack into, like, any system, right?
2633760	2636000	So, it's not that far-fetched to believe
2636800	2640240	that a very intelligent AI would probably be able
2640240	2643520	to identify some holes and basically be,
2643520	2644800	like, a human who could potentially
2644800	2646000	go back in time a year or two
2646000	2647200	and compromise all these systems.
2647200	2649840	Okay, so how have we dealt with that as a society?
2649840	2653200	Well, one big part is open-source software
2653200	2654720	that makes it so that when improvements
2654720	2656080	are made to the software,
2656080	2657680	it doesn't just kind of get stuck
2657680	2659440	in one company's products,
2659440	2662080	but it can kind of be broadly deployed
2662080	2663440	to a lot of different systems,
2663440	2666080	whether it's banks or hospitals or government stuff,
2666080	2668000	and, like, just everyone can kind of,
2668000	2669520	like, as the software gets hardened,
2670160	2671840	which happens because more people can see it
2671840	2673040	and more people can bang on it,
2673680	2675680	and there are standards on how this stuff works,
2677040	2679920	the world can kind of get upgraded together pretty quickly.
2679920	2683680	And I kind of think that a world where AI
2683680	2685440	is very widely deployed
2685760	2689680	in a way where it's gotten hardened progressively over time
2690880	2694240	is one where all the different systems will be in check
2694240	2696880	in a way that seems like it is fundamentally
2696880	2698000	more healthy to me
2698000	2700160	than one where this is more concentrated.
2700160	2702960	So there are risks on all sides,
2702960	2707440	but I think that that's one risk that I think people,
2707440	2709120	I don't hear them talking about quite as much.
2709120	2711520	I think, like, there's sort of the risk of, like,
2711520	2713520	okay, well, what if the AI system does something bad?
2714000	2718400	I am more, like, you know, I stay up at night more worrying,
2718400	2722240	well, what if, like, some actor that, whatever.
2722240	2723360	It's like, from wherever you sit,
2723360	2725360	there's going to be some actor who you don't trust
2725360	2727520	if they're the ones who have, like, the super strong AI,
2727520	2729360	whether it's some, like, other government
2729360	2732880	that is sort of, like, an opponent of our country
2732880	2735680	or some company that you don't trust or whatever it is.
2737680	2743120	Like, I think that that's potentially a much bigger risk
2743200	2747360	as in they could, like, overthrow our government
2747360	2750080	because they have a weapon that, like, nobody else has.
2750080	2751600	Cause a lot of mayhem.
2751600	2754880	Right, it's, I think it's, like, I mean,
2754880	2757360	I think the intuition is that this stuff ends up being
2757360	2760960	pretty kind of important and valuable
2760960	2763920	for both kind of economic and kind of security
2763920	2764640	and other things.
2764640	2768000	And I don't know, I just think, yeah, if, like,
2768000	2770880	if someone who you don't trust or is an adversary of you
2770960	2772960	gets something that is more powerful,
2772960	2775120	then I think that that could be an issue.
2775120	2777360	And I think probably the best way to mitigate that
2777360	2780400	is to have good open source AI
2780400	2782640	that basically becomes the standard
2783440	2786160	and in a lot of ways kind of can become the leader.
2786160	2790080	And in that way, it just ensures that it's a much more
2790080	2792960	kind of even and balanced playing field.
2792960	2794400	Yeah, that seems plausible to me.
2794400	2796800	And if that works out, that would be the future I prefer.
2797920	2800560	I guess I want to understand, like, mechanistically
2800640	2804080	how if somebody was going to cause mayhem with AI systems,
2804080	2806480	how the fact that there are other open source systems
2806480	2808080	in the world prevents that?
2808080	2809920	Like the specific example of, like,
2809920	2811200	somebody coming with a bio weapon,
2811840	2813280	is it just that we'll do a bunch of, like,
2813280	2815040	R&D in the rest of the world to, like,
2815040	2816400	figure out vaccines really fast?
2816400	2817280	Like, what's happening?
2817280	2818480	Would you take, like, the computer,
2818480	2820640	the security one that I was talking about?
2820640	2822400	I think someone with a weaker AI
2822400	2824640	trying to hack into a system that is, like,
2824640	2827040	protected by a stronger AI will succeed less.
2827760	2830560	Right, so I think that that's, I mean,
2830560	2831760	that's, like, in terms of software security.
2831760	2833360	How do you know everything in the world is like that?
2833360	2835040	Like, what if bio weapons aren't like that?
2836080	2837920	No, I mean, I don't know that everything
2837920	2838800	in the world is like that.
2841520	2844160	I think that that's, I guess,
2844160	2846080	one of the, bio weapons are one of the areas
2846080	2848320	where I think the people who are most worried
2848320	2849920	about this stuff are focused.
2849920	2853200	And I think that that's, I think that makes
2853200	2854320	a lot of sense to think about that.
2855200	2858800	The, and I think that there are certain mitigations.
2858800	2861600	You can try to not train certain knowledge
2861600	2862880	into the model, right?
2862880	2865760	There's different things, but, yeah,
2865760	2868400	I mean, it's some level, I mean,
2868400	2870880	if you get a sufficiently bad actor
2870880	2874560	and you don't have other AI that can sort of balance them
2874560	2877840	and understand what's going on and what the threats are,
2877840	2880320	then that could be a risk.
2880320	2881440	So I think that that's one of the things
2881440	2882560	that we need to watch out for.
2882960	2886640	Is there something you could see
2886640	2888000	in the deployment of these systems
2888000	2892160	where you observe like you're training Lama 4
2892160	2893680	and it's like, it lied to you
2893680	2895520	because you thought you were noticing or something.
2895520	2898160	And you're like, whoa, what's going on here?
2898160	2900480	Not that this is probably not likely
2900480	2901440	with the Lama 4.0 system,
2901440	2903360	but is there something you can imagine like that
2903360	2906480	where you'd be really concerned about deceptiveness
2906480	2908960	and if billions of copies of things are out in the wild?
2909840	2914400	Yeah, I mean, I think that that's not necessarily,
2914400	2917680	I mean, right now, we see a lot of hallucinations, right?
2917680	2919360	So I think it's more that.
2921200	2922640	I think it's an interesting question
2922640	2923680	how you would tell the difference
2923680	2926320	between a hallucination and deception.
2926320	2927520	But yeah, I mean, look, I mean,
2927520	2929920	I think there's a lot of risks and things to think about.
2929920	2935120	The flip side of all this is that there are also a lot of,
2936080	2939040	I try to, in running our company at least,
2940000	2945120	balance what I think of as these longer term theoretical risks
2947440	2950640	with what I actually think are quite real risks that exist today.
2950640	2954400	So like when you talk about deception,
2954400	2956080	the form of that that I worry about most
2956080	2958560	is people using this to generate misinformation
2958560	2959920	and then like pump that through
2959920	2961840	whether it's our networks or others.
2961920	2965680	So the way that we've basically combated
2965680	2967840	a lot of this type of harmful content
2968400	2970000	is by building AI systems
2970000	2972240	that are smarter than the adversarial ones.
2972240	2973920	And I guess this is part of,
2973920	2975920	this kind of informs part of my theory on this, right?
2975920	2978000	Is if you look at like the different types of harm
2978000	2981680	that people do or try to do through social networks,
2984320	2986800	there are ones that are not very adversarial.
2986800	2991520	So for example, like hate speech,
2991520	2993920	I would say is not super adversarial
2993920	2996240	in the sense that like people aren't getting
2997280	2999440	better at being racist, right?
2999440	3001920	They're just like, it's, you just like, okay,
3001920	3004560	if you kind of, that's one where I think the AIs
3004560	3007680	are generally just getting way more sophisticated,
3007680	3009600	faster than people are at those issues.
3009600	3011360	So we have, and we have issues both ways.
3011360	3014240	It's like people do bad things
3014240	3016480	that whether they're trying to incite violence or something.
3018240	3019840	But we also have a lot of false positives, right?
3019920	3022240	So where we basically censor stuff that we shouldn't,
3022240	3025280	and I think understandably make a lot of people annoyed.
3025280	3027920	So I think having an AI that just gets increasingly
3027920	3030560	precise on that, that's gonna be good over time.
3030560	3031680	But let me give you another example,
3031680	3034320	which is like nation states trying to interfere in elections.
3034880	3037200	That's an example where they're absolutely,
3037200	3038800	they have cutting edge technology
3038800	3041440	and absolutely get better each year.
3041440	3044640	So we block some technique, they learn what we did,
3044640	3046640	they come at us with a different technique, right?
3046640	3052240	It's not like a person trying to say mean things, right?
3052240	3055120	It's like, they're basically, they have a goal,
3055120	3056960	they're sophisticated, they have a lot of technology.
3058320	3060320	In those cases, I still think the ability
3060320	3064800	to kind of have RAI systems grow in sophistication
3064800	3067680	at a faster rate than theirs have, it's an arms race,
3067680	3070640	but I think we're at least currently winning that arms race.
3072240	3073920	So I don't know, I think that that's,
3073920	3075040	but this is like a lot of the stuff
3075040	3077200	that I spend time thinking about is like, okay,
3078240	3082240	yes, it is possible that whether it's llama four
3082240	3084240	or llama five or llama six, yeah,
3084240	3086480	we need to think about what behaviors we're observing
3086480	3087200	and it's not just us.
3087200	3088800	And part of the reason why you make this open source
3088800	3090960	is that there are a lot of other people who study this too.
3090960	3094240	So yeah, we wanna see what other people are observing,
3094240	3096720	what we're observing, what we can mitigate,
3096720	3098480	and then we'll make our assessment
3098480	3101680	on whether we can make it open source.
3101680	3104960	But I think for the foreseeable future,
3104960	3107200	I'm optimistic we will be able to.
3107200	3111040	And in the near term, I don't wanna take our eye off the ball
3111040	3112960	of what our actual bad things
3112960	3114960	that people are trying to use the models for today,
3114960	3116160	even if they're not existential,
3116160	3120560	but they're like pretty bad kind of day-to-day harms
3120560	3123120	that we are familiar with in running our services.
3124880	3126400	That's actually a lot of what we have to,
3126400	3127680	I think, spend our time on as well.
3127680	3128720	Yeah, yeah.
3128720	3131440	Actually, I found the synthetic data thing really curious.
3132640	3134640	I'm actually interested in why you don't think,
3135440	3136720	like current models, it makes sense
3136720	3138080	why there might be an asymptote
3138080	3140480	with just doing the synthetic data again and again.
3140480	3142320	If it gets smarter and uses the kind of techniques
3142320	3144320	you talk about in the paper or the blog post
3144320	3146880	that's coming out on the day this will be released
3146880	3150080	where it goes to the thought chain
3150080	3152480	that is the most correct.
3153040	3155680	Why this wouldn't like lead to a loop
3155680	3157200	that, of course, it wouldn't be overnight,
3157200	3158800	but over many months or years of training,
3158800	3160320	potentially, with a smarter model,
3160320	3161920	it gets smarter, makes better output,
3161920	3163040	gets smarter, and so forth.
3165280	3167200	Well, I think it could within the parameter
3167200	3169520	of whatever the model architecture is.
3169520	3173920	It's just that at some level, I don't know,
3174560	3178640	I think today is eight billion parameter models.
3179280	3182560	I just don't think you're going to be able to get to be as good
3182560	3186160	as the state-of-the-art multi-hundred billion
3186240	3188720	parameter models that are incorporating new research
3188720	3190080	into the architecture itself.
3192640	3194160	But those will be open source as well, right?
3194880	3196160	Well, yeah, but I think that that's,
3197200	3200880	I mean, subject to all the questions
3200880	3201760	that we just talked about.
3201760	3203920	Yes, I mean, we would hope that that'll be the case,
3203920	3206880	but I think that at each point, I don't know,
3206880	3209360	it's like when you're building software,
3209360	3211920	there's like a ton of stuff that you can do with software,
3211920	3213840	but then at some level, you're constrained
3213840	3215920	by the chips that it's running on.
3216240	3219600	Right? So there are always going to be different
3220400	3221680	physical constraints.
3221680	3224960	And it's like how big are the models is going to be constrained
3224960	3229840	by how much energy you can get and use for inference.
3230800	3236160	So I guess I'm simultaneously very optimistic
3236160	3238160	that this stuff will continue to improve quickly.
3238960	3245600	And also a little more measured than I think
3245680	3249520	some people are about kind of it's,
3250480	3252800	I just don't think the runaway case
3252800	3255040	is like a particularly likely one.
3256080	3258320	I think it makes sense to keep your options open.
3258320	3259440	Like there's so much we don't know.
3260480	3262080	There's a case in which like it's really important
3262080	3263040	to keep the balance of power.
3263040	3265200	So when nobody becomes like a technology or a dictator,
3265200	3266480	there's a case in which like,
3266480	3268640	you don't want to open source the architecture
3268640	3272240	because like China can use it to catch up to America's AIs
3272240	3273840	and like there is an intelligence explosion
3273920	3274800	and they like win that.
3275840	3277120	Yeah, a lot of things are impossible.
3277120	3278640	Just like keeping your options open,
3278640	3280800	considering all of them seems reasonable.
3280800	3280960	Yeah.
3282480	3283520	Let's talk about some other things.
3283520	3284080	Go for it.
3284080	3288560	Okay. Metaverse, what time period in human history
3288560	3290640	would you be most interested in going into?
3290640	3292640	A 100,000 BCE to now.
3292640	3293680	You just want to see what it was like.
3293680	3294720	Well, that's through the past.
3294720	3295120	Huh?
3295120	3295840	It has to be the past.
3295840	3296800	Oh yeah, it has to be the past.
3301760	3302400	I don't know.
3302400	3304400	I mean, I have the periods of time that I'm interested.
3304400	3306240	I mean, I'm really interested in American history
3306240	3310320	and classical history and I'm really interested
3310320	3311520	in the history of science too.
3311520	3315920	So I actually think seeing and trying to understand more
3318720	3320960	about how some of the big advances came about.
3320960	3324400	I mean, all we have are like somewhat limited writings
3324400	3325920	about some of that stuff.
3325920	3327520	I'm not sure the metaverse is going to let you do that
3327520	3329440	because I mean, it's, you know, we can't,
3329520	3332640	it's going to be hard to kind of go back in time
3332640	3334560	for things that we don't have records of.
3334560	3339040	But I'm actually not sure that going back in time
3339040	3341520	is going to be that important thing for them.
3341520	3343440	I mean, I think it's going to be cool for history classes
3343440	3346960	and stuff, but that's probably not the use case
3346960	3349600	that I'm most excited about for the metaverse overall.
3349600	3352960	I mean, the main thing is just the ability
3352960	3355280	to feel present with people no matter where you are.
3355280	3356640	I think that's going to be killer.
3356640	3361360	I mean, there's, I mean, in the AI conversation
3361360	3364560	that we were having, I mean, it's, you know,
3364560	3366640	so much of it is about physical constraints
3366640	3369120	that kind of underlie all of this, right?
3369120	3372320	And you want to move, I mean, one lesson of technology
3372320	3375600	is you want to move things from the physical constraint realm
3375600	3377200	into software as much as possible
3377200	3381120	because software is so much easier to build and evolve.
3381120	3382800	And like you can democratize it more
3382800	3385520	because like not everyone is going to have a data center,
3385520	3388800	but like a lot of people can kind of write code
3388800	3390880	and take open source code and modify it.
3393280	3395840	The metaverse version of this is I think
3395840	3398480	enabling realistic digital presence
3399600	3403760	is going to be just an absolutely huge difference
3403760	3408000	for making it so that people don't feel
3408000	3409840	like they have to physically be together
3409840	3410880	for as many things.
3411520	3412720	Now, I mean, I think that there are going to be things
3412720	3414240	that are better about being physically together.
3416240	3418000	So it's not, I mean, these things aren't binary,
3418000	3418960	it's not going to be like, okay,
3418960	3420960	now it's, you don't need to do that anymore.
3420960	3427200	But overall, I mean, I think that this,
3427200	3429760	it's just going to be really powerful for socializing,
3429760	3432640	for feeling connected with people, for working,
3433600	3438000	for, I don't know, parts of industry, for medicine,
3438000	3439920	for like so many things.
3439920	3441040	I want to go back to something you said
3441040	3442240	at the beginning of the conversation
3442240	3444960	where you didn't sell the company for a billion dollars
3444960	3446640	and like the metaverse, you knew we were going to do this
3446640	3449520	even though the market was hammering you for it.
3449520	3450960	And then I'm actually curious,
3450960	3452720	like what is the source of that edge?
3452720	3455280	And you said like, oh, values, I have this intuition,
3455280	3456800	but like everybody says that, right?
3457360	3459200	If you had to say something that's specific to you,
3459200	3461200	what is, how would you express what that is?
3461200	3463280	Like why were you so convinced about the metaverse?
3469920	3471440	Well, I think that those are different questions.
3471440	3476560	So what, I mean, what are the things that kind of power me?
3478640	3479920	I think we've talked about a bunch of things.
3479920	3483200	So it's, I mean, I just really like building things.
3485200	3489280	I specifically like building things around how people communicate
3489280	3491680	and sort of understanding how people express themselves
3491680	3492800	and how people work, right?
3492800	3494000	And when I was in college, I was,
3494000	3496560	I was studying computer science and psychology.
3496560	3497760	I think a lot of other people in the industry
3497760	3499440	started studying computer science, right?
3499440	3503600	So it's always been sort of the intersection
3503600	3505120	of those two things for me.
3505120	3511920	But I think it's also sort of this like really deep drive.
3511920	3515760	I don't know how to explain it, but I just feel like in,
3515760	3519280	like constitutionally, like I'm doing something wrong
3519280	3521920	if I'm not building something new, right?
3521920	3526560	And so I think that there's like
3529760	3532480	even when we're putting together the business case for,
3533440	3537360	you know, investing like $100 billion in AI
3537360	3539360	or some huge amount in the metaverse.
3539360	3541520	So it's like, yeah, I mean, we have plans
3541520	3543840	that I think make it pretty clear that if our stuff works,
3543840	3545360	it'll be a good investment.
3545360	3548320	But like, you can't know for certain from the outset.
3548960	3552000	And there's all these arguments that people have,
3552000	3554880	you know, whether it's like, you know, with advisors
3554880	3557680	or different folks, it's like, well, how, how could you,
3557760	3560320	like it's how are you confident enough to do this?
3560320	3564880	And it's like, well, the day I stop trying to build new things,
3565520	3566240	I'm just done.
3566240	3568240	I'm going to go build new things somewhere else, right?
3568240	3572880	It's like, it's like, it is, I'm fundamentally incapable
3573520	3578560	of running something or in my own life
3578560	3581520	and like not trying to build new things
3581520	3582880	that I think are interesting.
3582880	3585040	It's like, that's not even a question for me, right?
3585040	3587680	It's like whether, like whether we're going to go take a swing
3587680	3589040	at like building the next thing.
3589040	3592960	It's like, it's like, I'm just incapable of not doing that.
3594800	3599760	And I don't know, and I'm kind of like this
3599760	3601760	in like all the different aspects of my life, right?
3601760	3604320	It's like we built this like, you know,
3604320	3608720	family built this ranch in Kauai and like, I just like
3610240	3611840	worked to like design all these buildings.
3611840	3615200	I'm like kind of trying to like, we started raising cattle
3615200	3616560	and I'm like, all right, well, I want to make
3616560	3618160	like the best cattle in the world, right?
3618160	3620240	So it's like, how do we like, how do we architect this
3620240	3622560	so that way we can figure this out and like and build
3623200	3625200	and call the stuff up that we need to try to do that?
3626640	3627840	So I don't know, that's me.
3629600	3630800	What was the other part of the question?
3632080	3634880	Look, Metta is just a really amazing tech company, right?
3634880	3636960	They have all these great software engineers
3636960	3639840	and even they work with Stripe to handle payments.
3639840	3641360	And I think that's just a really notable fact.
3641840	3645200	That Stripe's ability to engineer these checkout experiences
3645200	3648720	is so good that big companies like Ford, Zoom, Metta,
3648720	3652000	even OpenAI, they work with Stripe to handle payments
3652000	3653840	because just think about how many different possibilities
3653840	3654880	you have to handle.
3654880	3656880	If you're in a different country, you'll pay a different way
3656880	3658480	and if you're buying a certain kind of item
3658480	3660400	that might affect how you decide to pay.
3660400	3663840	And Stripe is able to test these fine-grained optimizations
3663840	3666400	across tens of billions of transactions a day
3666400	3668560	to figure out what will convert people
3668560	3671280	and obviously conversion means more revenue for you.
3671280	3673680	And look, I'm not a big company like Metta or anything,
3673680	3675280	but I've been using Stripe since long
3675280	3676800	before they were advertisers.
3676800	3680080	Stripe Atlas was just the easiest way for me to set up an LLC
3680080	3682160	and they have these payments and invoicing features
3682160	3686000	that make it super convenient for me to get money from advertisers.
3686000	3688080	And obviously without that, it would have been much harder
3688080	3690000	for me to earn money from the podcast.
3690000	3691600	And so it's been great for me.
3691600	3693680	Go to stripe.com to learn more.
3693680	3695440	Thanks to them for sponsoring the episode.
3695440	3696240	Now back to Mark.
3697120	3699280	I'm not sure, but I'm actually curious about something else
3699280	3704400	which is, so the 19-year-old Mark reads a bunch
3704400	3707280	of like antiquity in classics, high school, college.
3707920	3709440	What important lesson did you learn from it?
3709440	3710560	Not just interesting things you found,
3710560	3713200	but like there aren't that many tokens you consumed
3713200	3714160	by the time you're 19.
3714160	3715680	A bunch of them were about the classics.
3715680	3716880	Clearly that was important in some way.
3716880	3718240	And that many tokens you consumed.
3725680	3726080	I don't know.
3726080	3727120	That's a good question.
3727120	3728320	I mean, one of the things that I thought
3728320	3737440	was really fascinating is, so when Augustus was first,
3738000	3744480	so he became emperor, and he was trying to establish peace.
3744480	3750400	And there was no real conception of peace at the time.
3750400	3753040	Like the people's understanding of peace was,
3754000	3756400	it is the temporary time between when you're
3756400	3758960	and amuse will inevitably attack you again,
3758960	3760240	so you get like a short rest.
3760800	3763200	And he had this view, which is like, look,
3763200	3766720	like we want to change the economy from instead of being
3766720	3770080	so mercenary and like in kind of militaristic
3770960	3773920	to like actually this positive something.
3774640	3777760	It's like a very novel idea at the time.
3782400	3782960	I don't know.
3782960	3784560	I think that there's like something that's
3784560	3786080	just really fundamental about that.
3786080	3789280	It's like in terms of the bounds on like what people
3789280	3792960	can conceive at the time of like what are rational ways to work.
3793680	3797280	And I don't know, I mean, going back to like,
3797280	3799600	I mean, this applies to both the metaverse and the AI stuff,
3799600	3802400	but like a lot of investors and just different people
3802400	3804880	just can't wrap their head around why we would open source this.
3805440	3809440	And it's like, I don't understand.
3809440	3810320	It's like open source.
3810320	3812160	That must just be like the temporary time
3812160	3814080	between which you're making things proprietary.
3814080	3815200	Right.
3815200	3817840	And it's, but I actually think it's like
3818480	3824160	this very profound thing in tech that has actually,
3824160	3826240	it creates a lot of winners.
3826240	3826560	Right.
3826560	3830720	And it's and so I don't want to strain the analogy too much,
3830720	3834960	but I do think that there's a lot of times,
3834960	3836480	I think ways where you can
3839360	3841760	that are just like models for building things
3842000	3845520	that people can't even like they just like often can't wrap
3845520	3848240	their head around how that would be a valuable thing
3848240	3851760	for people to go do, or like a reasonable state of the world
3851760	3857680	that it's, I mean, it's, I think that there's more reasonable
3857680	3858880	things than people think.
3858880	3860720	That's super fascinating.
3860720	3862800	Can I give you my answer when I was thinking
3862800	3864480	what you might have gotten from it?
3864480	3868160	This is probably totally off, but just how young
3868160	3870960	some of these people are who have very important roles
3871840	3872480	in the empire.
3872480	3874720	Like Caesar Augustus, like by the time he's 19,
3874720	3877120	he's actually incredibly one of the most prominent people
3877120	3878320	in Roman politics.
3878320	3879760	And he's like leading battles and forming
3879760	3881040	the second prime emirate.
3881040	3882800	I wonder if you're like the 19 year old is like,
3882800	3884000	I can actually do this because like
3884000	3886800	I think that's an interesting example,
3886800	3890240	both from a lot of history and American history.
3890240	3890720	Yeah.
3890720	3895040	I mean, it's, I mean, one of my favorite quotes is,
3895040	3897680	it's this Picasso quote that all children are artists
3897680	3899520	and the challenge is how do you remain an artist
3899520	3900560	when you grow up?
3900560	3903280	And it's like basically I think because when you're younger,
3904080	3909680	I think it's just easier to have kind of wild ideas
3909680	3911360	and you're not, you know, you have no,
3912560	3915200	there are all these analogies to the innovators dilemma
3915200	3918640	that exist in your life as well as your company
3918640	3919840	or whatever you've built, right?
3919840	3922080	So, you know, you're kind of earlier on your trajectory.
3922080	3924960	It's easier to pivot and take in new ideas
3924960	3926800	without it disrupting other commitments
3926800	3929040	that you've made to different things.
3929040	3931920	And so, I don't know.
3931920	3934560	I think that's an interesting part of running a company
3934560	3936960	is like how do you kind of stay dynamic?
3938640	3940400	Going back to the investors in open source,
3941440	3942880	the $10 billion model,
3942880	3945840	suppose it's totally safe, you've done these evaluations
3945840	3947280	and unlike in this case,
3947280	3948960	the evaluators can also fine tune the model,
3949680	3951360	which hopefully will be the case in future models.
3952560	3954560	Would you open source that, the $10 billion model?
3955360	3957280	Well, I mean, as long as it's helping us, then yeah.
3957280	3959360	But would it like to $10 billion of R&D
3959360	3960960	and then now it's like open source or anything?
3960960	3962880	Well, I think here's, I think a question,
3962880	3966640	which we'll have to evaluate this as time goes on too, but
3970320	3972960	we have a long history of open sourcing software, right?
3972960	3975520	We don't tend to open source our product, right?
3975520	3978880	So, it's not like we don't take like the code for Instagram
3978880	3979760	and make it open source,
3979760	3982880	but we take like a lot of the low level infrastructure
3983440	3985600	and we make that open source, right?
3986080	3989520	Probably the biggest one in our history was open compute project
3989520	3994640	where we took the designs for kind of all of our servers
3994640	3996240	and network switches and data centers
3996240	3998880	and made it open source and ended up being super helpful
3998880	4001520	because, you know, I mean, a lot of people can design servers,
4001520	4003520	but now like the industry standardized on our design,
4003520	4005280	which meant that the supply chains
4006160	4007920	basically all got built out around our design,
4007920	4008720	the volumes went up,
4008720	4011600	so it got cheaper for everyone and saved us billions of dollars.
4011600	4013360	So, awesome, right?
4013440	4016240	Okay, so there's multiple ways where open source,
4016240	4017920	I think, could be helpful for us.
4017920	4021440	One is if people figure out how to run the models more cheaply.
4021440	4024640	Well, we're going to be spending tens or like 100 billion dollars
4024640	4027040	or more over time on all this stuff.
4027680	4030160	So, if we can do that 10% more effectively,
4030160	4032720	we're saving billions or tens of billions of dollars.
4032720	4034320	Okay, that's probably worth a lot by itself,
4035920	4037840	especially if there's other competitive models out there.
4037840	4039760	It's not like our thing is like
4039760	4041760	be giving away some kind of crazy advantage.
4042720	4045600	So, is there a view that the trading will be commodified?
4049040	4050880	I think there's a bunch of ways that this could play out.
4050880	4051840	That's one.
4051840	4058560	The other is that, so commodity kind of implies
4059120	4060480	that it's going to get very cheap
4060480	4063040	because there's lots of options.
4063040	4064560	The other direction that this could go in
4065360	4067040	is qualitative improvements.
4067040	4069360	So, you mentioned fine-tuning, right?
4069360	4071760	It's like right now, it's pretty limited,
4071760	4075120	what you can do with fine-tuning major other models out there.
4075120	4076400	And there are some options,
4076400	4078400	but generally not for the biggest models.
4079840	4081280	So, I think being able to do that
4081280	4086160	and be able to kind of do different app-specific things
4086160	4087440	or use case-specific things
4087440	4089120	or build them into specific tool chains,
4090880	4095600	I think will not only enable kind of more efficient development,
4095600	4097520	it could enable qualitatively different things.
4098480	4099680	Here's one analogy on this.
4102160	4104240	So, one thing that I think generally sucks
4104240	4105520	about the mobile ecosystem
4106080	4110160	is that you have these two gatekeeper companies,
4110160	4111200	Apple and Google,
4111200	4113200	that can tell you what you're allowed to build.
4113200	4115440	And there are lots of times in our history,
4115440	4116800	so there's the economic version of that,
4116800	4118080	which is like, all right, we build something in there,
4118080	4119600	just like, I'm going to take a bunch of your money.
4119600	4123520	But then there's the qualitative version,
4123520	4126080	which is actually what kind of upsets me more,
4126160	4127520	which is there's a bunch of times
4127520	4130160	when we've launched or wanted to launch features,
4130720	4133280	and then Apple's just like, nope, you're not launching that.
4133280	4135520	So, it's like, that sucks, right?
4135520	4139440	And so, the question is, what is,
4139440	4144560	like, are we kind of set up for a world like that with AI,
4144560	4147440	where like, you're going to get a handful of companies
4147440	4148720	that run these closed models
4148720	4150640	that are going to be in control of the APIs,
4150640	4151840	and therefore are going to be able to tell you
4151840	4152640	what you can build.
4153520	4155520	Well, for one, I can say, for us,
4156480	4158800	it is worth it to go build a model ourselves
4158800	4160960	to make sure that we're not in that position, right?
4160960	4163680	Like, I don't want any of those other companies
4163680	4164800	telling us what we can build.
4166000	4168000	But from an open source perspective,
4168000	4169920	I think a lot of developers don't want those companies
4169920	4171200	telling them what they can build either.
4172560	4175680	So, the question is, what is the ecosystem
4175680	4176880	that gets built out around that?
4176880	4178400	What are interesting new things?
4178400	4180080	How much does that improve our products?
4182560	4183920	I think that there's a lot of cases
4183920	4186000	where if this ends up being like, you know,
4186000	4190160	like our databases or caching systems or architecture,
4190720	4192800	we'll get valuable contributions from the community
4192800	4194000	that will make our stuff better,
4194000	4196320	and then our app-specific work that we do
4196320	4197920	will still be so differentiated
4197920	4199280	that it won't really matter, right?
4199280	4201360	It's like, we'll be able to do what we do,
4201360	4202800	we'll benefit in all the systems,
4202800	4204320	ours and the communities will be better
4204320	4205280	because it's open source.
4206080	4210320	There is one world where maybe it's not that.
4210320	4211520	I mean, maybe the model just ends up
4211520	4213200	being more of the product itself.
4213280	4217760	In that case, then I think it's a trickier economic
4217760	4220000	calculation about whether you open source that
4220000	4223200	because then you are kind of commoditizing yourself a lot.
4223840	4224800	From what I can see so far,
4224800	4225920	it doesn't seem like we're in that zone.
4226560	4228480	Do you expect to earn significant revenue
4228480	4230640	from licensing your model to the cloud providers?
4230640	4232720	So, they have to pay you a fee to actually serve the model?
4236160	4238640	We want to have an arrangement like that,
4238640	4240800	but I don't know how significant it'll be.
4240800	4244640	And we have this, this is basically our license for Lama.
4246800	4249040	In a lot of ways, it's like a very permissive
4249040	4251680	open source license, except that we have a limit
4251680	4253440	for the largest companies using it.
4253440	4256240	And this is why we put that limit in,
4256240	4258640	is we're not trying to prevent them from using it.
4258640	4259920	We just want them to come talk to us
4259920	4261120	because if they're going to just basically
4261120	4263680	take what we built and resell it and make money off of it,
4263680	4266320	then it's like, okay, well, if you're like,
4267200	4269200	Microsoft Azure or Amazon,
4269680	4271200	yeah, if you're going to reselling the model,
4271200	4272720	then we should have some revenue share on that.
4272720	4274800	So, just come talk to us before you go do that.
4274800	4275760	And that's how that's played out.
4275760	4279520	So, for Lama 2, it's, I mean, we basically just have deals
4279520	4282320	with all these major cloud companies,
4282320	4286480	and Lama 2 is available as a hosted service on all those clouds.
4288480	4291200	I assume that as we release bigger and bigger models,
4291200	4292240	that'll become a bigger thing.
4292240	4293440	It's not the main thing that we're doing,
4293440	4295040	but I just think if others are,
4295040	4296640	if those companies are going to be selling our models,
4296640	4298240	it makes sense that we should, you know,
4298240	4299600	share the upside of that somehow.
4299600	4302480	Yeah. With regards to the other open source dangers,
4302480	4304720	I think I have a genuine legion of points
4304720	4305920	about the balance of power stuff,
4306960	4309280	and potentially like the harms you can get rid of
4309280	4311280	because we have better alignment techniques or something.
4312240	4314320	I wish there was some sort of framework that Meta had,
4314320	4316320	like other labs have this where they say like,
4316320	4318400	if we see this is a concrete thing,
4318400	4320720	then that's a no-go on the open source,
4320720	4323200	or like even potentially on deployment,
4323200	4326080	just like writing it down so like the company is ready for it,
4326720	4328800	people have expectations around it and so forth.
4328800	4330640	Yeah. No, I think that that's a fair point
4330640	4332000	on the existential risk side.
4332000	4334800	Right now, we focus more on the types of risks
4334800	4337760	that we see today, which are more of these content risks.
4337760	4343120	So, you know, we have lines on, we don't want the model to be
4344080	4347040	basically doing things that are helping people commit violence
4347040	4350320	or fraud or, you know, just harming people in different ways.
4350320	4355200	So in practice for today's models,
4355200	4357920	and I would guess the next generation
4357920	4359600	and maybe even the generation after that,
4360320	4364800	I think while it is somewhat more maybe intellectually interesting
4364800	4367040	to talk about the existential risks,
4367600	4373840	I actually think the real harms that need more energy being mitigated
4373840	4377840	are things that are going to like have someone take a model
4377840	4380960	and do something to hurt a person with today's parameters
4380960	4384240	of and kind of the types of kind of more mundane harms
4384320	4387280	that we see today, like people kind of committing fraud
4387280	4388720	against each other or things like that.
4388720	4393360	So that, I just don't want to short change that.
4393360	4395280	I think we have a responsibility
4395280	4396960	to make sure we do a good job on that.
4396960	4398880	Yeah, Meta's a big company, you can handle both.
4398880	4402960	Yeah. Okay, so as far as the open source goes,
4402960	4405520	I'm actually curious if you think the impact of the open source
4405520	4408320	from PyTorch, React, open compute, these things
4408320	4409840	has been bigger for the world
4409840	4412000	than even the social media aspects of Meta.
4412000	4413600	Because I like talk to people who use these services
4413600	4414800	and think like it's plausible
4414800	4416560	because a big part of the internet runs on these things.
4418960	4420160	It's an interesting question.
4420160	4423360	I mean, I think almost half the world uses our...
4423360	4424800	Yeah, that's an interesting point.
4427360	4429760	So I think it's hard to beat that.
4429760	4432880	But no, I think open sources,
4434560	4437360	it's really powerful as a new way of building things.
4437360	4441600	And yeah, I mean, it's possible.
4441600	4443920	I mean, it's maybe one of these things where...
4446320	4449520	I don't know, like Bell Labs, where they...
4449520	4452880	It's like they were working on the transistor
4452880	4455360	because they wanted to enable long distance calling.
4455920	4457280	And they did.
4457280	4459120	And it ended up being really profitable for them
4459120	4461120	that they were able to enable long distance calling.
4461120	4465440	And if you ask them five to 10 years out from that,
4467280	4469600	what was the most useful thing that they invented?
4469600	4471440	It's like, okay, well, we enable long distance calling
4471440	4473040	and now all these people are long distance calling.
4473040	4474400	But if you ask 100 years later,
4474400	4475760	maybe it's a different question.
4475760	4480320	So I think that that's true of a lot of the things
4480320	4481280	that we're building, right?
4481280	4485280	Reality Labs, some of the AI stuff, some of the open source stuff.
4485280	4488480	I think it's like the specific products evolve
4488480	4490400	and to some degree come and go.
4490400	4494080	But I think like the advances for humanity persist.
4494080	4497360	And that's like a cool part of what we all get to do.
4498080	4499920	By when will the Lama models be trained
4499920	4501040	on your own custom silicon?
4506480	4508320	Soon, not Lama 4.
4509760	4514640	The approach that we took is first we basically built
4514640	4516560	custom silicon that could handle inference
4517200	4520000	for our ranking and recommendation type stuff.
4520000	4522560	So reels, newsfeed, ads.
4523200	4526800	And that was consuming a lot of GPUs.
4528000	4530800	But when we were able to move that to our own silicon,
4530800	4534720	we now were able to use the more expensive Nvidia GPUs
4535440	4536480	only for training.
4537120	4544480	So at some point, we will hopefully have silicon ourselves
4544480	4547600	that we can be using for probably first training
4547600	4549680	some of the simpler things that eventually training
4550560	4553200	these like really large models.
4553840	4558160	But in the meantime, I'd say the program is going quite well
4558160	4559920	and we're just rolling it out methodically
4559920	4561600	and have a long-term roadmap for it.
4562800	4563760	Final question.
4563760	4565120	This is sort of the out of left field,
4565120	4568640	but if you were made CEO of Google+, could you have made it work?
4568640	4570240	Google+, oof.
4571440	4572800	Well, I don't know.
4574160	4574720	I don't know.
4574720	4578800	That's a very difficult, very difficult counterfactual.
4579440	4581040	Okay, then the real final question will be
4581040	4582480	when Gemini was launched,
4582480	4584720	was there any chance that somebody in the office
4584720	4586080	uttered Karthica Dalinda Est?
4587600	4589520	No, I think we're tamer now.
4590720	4591520	Cool, cool.
4591520	4592160	Awesome, Mark.
4595680	4596800	Yeah, I don't know.
4596800	4597360	It's a good question.
4598720	4601120	The problem is there was no CEO of Google+,
4601120	4603120	it was just like a division within a company.
4603840	4606160	I think it's like, and you asked before about
4606160	4608560	what are the kind of scarcest commodities,
4608560	4610240	but you asked about it in terms of dollars.
4610800	4612800	And I actually think for most companies,
4613680	4617280	it's, of this scale at least, it's focus, right?
4617280	4618400	It's like when you're a startup,
4618400	4620000	maybe you're more constrained on capital.
4621120	4623200	You know, you just are working on one idea
4623200	4625280	and you might not have all the resources.
4625280	4627360	I think you cross some threshold at some point
4627360	4629600	where the nature of what you're doing,
4629600	4631120	you're building multiple things
4631120	4633120	and you're creating more value across them,
4633120	4637360	but you become more constrained on what can you direct
4637360	4639200	and to go well.
4639200	4641920	And like, there's always the cases
4641920	4644000	where something just random awesome happens
4644000	4645840	in the organization, I don't even know about it.
4645840	4648160	And those are, that's great.
4648160	4649520	But like, but I think in general,
4650640	4653680	the organization's capacity is largely limited
4653680	4658480	by what like the CEO and the management team
4658480	4661440	are able to kind of oversee and kind of manage.
4662480	4665360	I think that that's just been a big focus for us.
4665360	4669600	It's like, all right, keep the, as I guess Ben Horowitz says,
4669600	4671920	keep the main thing, the main thing, right?
4671920	4677440	And try to kind of stay focused on your key priorities.
4678000	4679600	Yeah, all right, awesome.
4679600	4680400	That was excellent, Mark.
4680400	4681520	Thanks so much. That was a lot of fun.
4681520	4682720	Yeah, really fun.
4682720	4683440	Thanks for having me.
4683440	4684160	Yeah, absolutely.
4684960	4685840	Hey, everybody.
4685840	4687920	I hope you enjoyed that episode with Mark.
4687920	4689760	As you can see, I'm now doing ads.
4689760	4692800	So if you're interested in advertising on the podcast,
4692800	4694160	go to the link in the description.
4694800	4697520	Otherwise, as you know, the most helpful thing you can do
4697520	4700240	is just share the podcast with people who you think
4700240	4701040	might enjoy it.
4701040	4703040	You know, your friends, group chats, Twitter,
4703680	4704560	I guess threads.
4705120	4707680	Yeah, I hope you enjoyed and I'll see you on the next one.
