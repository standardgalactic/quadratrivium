1
00:00:00,000 --> 00:00:05,680
Human-level AI is deep, deep into an intelligence explosion.

2
00:00:05,680 --> 00:00:09,040
Things like inventing the transformer or discovering

3
00:00:09,040 --> 00:00:12,080
Chinchilla scaling and doing your training runs more optimally

4
00:00:12,080 --> 00:00:13,760
or creating flash attention.

5
00:00:13,760 --> 00:00:17,560
That set of inputs probably would yield the kind of AI

6
00:00:17,560 --> 00:00:20,040
capabilities needed for intelligence explosion.

7
00:00:20,040 --> 00:00:22,320
You have a race between, on the one hand,

8
00:00:22,320 --> 00:00:25,080
the project of getting strong interpretability

9
00:00:25,080 --> 00:00:26,880
and shaping motivations.

10
00:00:26,960 --> 00:00:30,160
And on the other hand, these AIs in ways

11
00:00:30,160 --> 00:00:33,080
that you don't perceive make the AI takeover happen.

12
00:00:33,080 --> 00:00:36,720
We spend more compute by having a larger brain than other animals.

13
00:00:36,720 --> 00:00:38,720
And then we have a longer childhood.

14
00:00:38,720 --> 00:00:41,160
It's not like you have to like having a bigger model

15
00:00:41,160 --> 00:00:43,120
and having more training time with it.

16
00:00:43,120 --> 00:00:45,320
It seemed very implausible that we couldn't do better

17
00:00:45,320 --> 00:00:47,440
than completely brute force evolution.

18
00:00:47,440 --> 00:00:50,240
How quickly are we running through those orders of magnitude?

19
00:00:50,240 --> 00:00:54,800
OK, today I have the pleasure of speaking with Carl Schulman,

20
00:00:54,800 --> 00:00:56,320
many of my former guests.

21
00:00:56,360 --> 00:00:57,480
And this is not an exaggeration.

22
00:00:57,480 --> 00:00:59,520
Many of my former guests have told me

23
00:00:59,520 --> 00:01:02,280
that a lot of their biggest ideas,

24
00:01:02,280 --> 00:01:03,480
perhaps most of their biggest ideas,

25
00:01:03,480 --> 00:01:05,160
have come directly from Carl,

26
00:01:05,160 --> 00:01:07,640
especially when it has to do with the intelligence explosion

27
00:01:07,640 --> 00:01:08,760
and its impacts.

28
00:01:08,760 --> 00:01:11,400
And so I decided to go directly to the source

29
00:01:11,400 --> 00:01:13,840
and we have Carl today on the podcast.

30
00:01:13,840 --> 00:01:15,360
Carl keeps a super low profile,

31
00:01:15,360 --> 00:01:18,240
but he is one of the most interesting intellectuals

32
00:01:18,240 --> 00:01:19,480
I've ever encountered.

33
00:01:19,480 --> 00:01:21,840
And this is actually his second podcast ever.

34
00:01:21,840 --> 00:01:24,440
So we're going to get to get deep into the heart

35
00:01:24,440 --> 00:01:25,720
of many of the most important ideas

36
00:01:25,720 --> 00:01:27,400
that are circulating right now,

37
00:01:27,400 --> 00:01:28,640
directly from the source.

38
00:01:28,640 --> 00:01:29,760
So and by the way,

39
00:01:29,760 --> 00:01:32,880
so Carl is also an advisor to the Open Philanthropy Project,

40
00:01:32,880 --> 00:01:34,720
which is one of the biggest funders

41
00:01:34,720 --> 00:01:37,280
on causes having to do with AI and its risks,

42
00:01:37,280 --> 00:01:39,280
not to mention global health and old being.

43
00:01:39,280 --> 00:01:41,200
And he is a research associate

44
00:01:41,200 --> 00:01:44,200
at the Future of Humanity Institute at Oxford.

45
00:01:44,200 --> 00:01:46,600
So Carl, it's a huge pleasure to have you on the podcast.

46
00:01:46,600 --> 00:01:47,440
Thanks for coming.

47
00:01:47,440 --> 00:01:48,360
Thank you, Drakash.

48
00:01:48,360 --> 00:01:51,560
I've enjoyed seeing some of your episodes recently

49
00:01:51,560 --> 00:01:53,920
and I'm glad to be on the show.

50
00:01:53,960 --> 00:01:55,600
Excellent. Let's talk about AI.

51
00:01:55,600 --> 00:01:57,360
Before we get into the details,

52
00:01:57,360 --> 00:02:01,200
give me the sort of big picture explanation

53
00:02:01,200 --> 00:02:06,080
of the feedback loops and just the general dynamics

54
00:02:06,080 --> 00:02:08,560
that would start when you have something

55
00:02:08,560 --> 00:02:10,600
that is approaching human level intelligence.

56
00:02:10,600 --> 00:02:13,120
Yeah. So I think the way to think about it

57
00:02:13,120 --> 00:02:15,760
is we have a process now

58
00:02:15,760 --> 00:02:19,560
where humans are developing new computer chips,

59
00:02:19,560 --> 00:02:24,280
new software, running larger training runs

60
00:02:24,280 --> 00:02:29,280
and it takes a lot of work to keep Moore's law chugging.

61
00:02:30,200 --> 00:02:32,480
Well, it was, it's slowing down now

62
00:02:32,480 --> 00:02:35,160
and it takes a lot of work to develop things

63
00:02:35,160 --> 00:02:40,000
like transformers to develop a lot of the improvements

64
00:02:40,000 --> 00:02:42,400
to AI and neural networks.

65
00:02:42,400 --> 00:02:43,360
They're advancing things.

66
00:02:43,360 --> 00:02:48,360
And the core method that I think I want to highlight

67
00:02:49,080 --> 00:02:53,000
on this podcast, and I think is underappreciated

68
00:02:53,000 --> 00:02:56,000
is the idea of input-output curves.

69
00:02:56,000 --> 00:03:01,000
So we can look at the increasing difficulty

70
00:03:01,200 --> 00:03:03,320
of improving chips.

71
00:03:03,320 --> 00:03:06,880
And so sure, each time you double the performance

72
00:03:06,880 --> 00:03:08,240
of computers, it's harder

73
00:03:08,240 --> 00:03:09,600
and as we approach physical limits,

74
00:03:09,600 --> 00:03:13,200
eventually it becomes impossible, but how much harder?

75
00:03:14,200 --> 00:03:19,200
So there's a paper called our Ideas Getting Harder to Find.

76
00:03:19,560 --> 00:03:21,880
It was published a few years ago,

77
00:03:21,880 --> 00:03:24,440
something like 10 years ago at Mirri,

78
00:03:24,440 --> 00:03:29,440
we did, I mean, I did an early version of this analysis

79
00:03:30,560 --> 00:03:33,520
using mainly data from Intel

80
00:03:33,520 --> 00:03:36,360
and like the large semiconductor fabricators.

81
00:03:36,360 --> 00:03:40,240
Anyway, and so in this paper, they cover a period

82
00:03:40,240 --> 00:03:44,560
where the productivity of computing

83
00:03:44,560 --> 00:03:45,680
went up a million folds.

84
00:03:45,680 --> 00:03:47,600
So you could get a million times

85
00:03:47,600 --> 00:03:50,360
the computing operations per second per dollar.

86
00:03:50,360 --> 00:03:54,080
Big change, but it got harder.

87
00:03:54,080 --> 00:03:58,880
So the amount of investments, the labor force required

88
00:03:58,880 --> 00:04:01,040
to make those continuing advancements

89
00:04:01,040 --> 00:04:02,920
went up and up and up.

90
00:04:02,920 --> 00:04:07,440
Indeed, it went up 18 fold over that period.

91
00:04:07,440 --> 00:04:09,600
I know, so some take this to say,

92
00:04:09,600 --> 00:04:11,360
oh, diminishing returns,

93
00:04:11,360 --> 00:04:12,960
things are just getting harder and harder.

94
00:04:12,960 --> 00:04:15,660
And so that will be the end of progress eventually.

95
00:04:16,920 --> 00:04:21,300
However, in a world where AI is doing the work,

96
00:04:22,840 --> 00:04:25,240
that doubling of computing performance

97
00:04:26,440 --> 00:04:29,760
translates pretty directly to a doubling or better

98
00:04:29,760 --> 00:04:32,000
of the effective labor supply.

99
00:04:32,000 --> 00:04:37,000
That is, if when we had that million fold compute increase,

100
00:04:38,000 --> 00:04:41,720
we used it to run artificial intelligences

101
00:04:41,720 --> 00:04:46,520
who would replace human scientists and engineers

102
00:04:46,520 --> 00:04:50,520
then the 18x increase in the labor demands

103
00:04:50,520 --> 00:04:52,320
of the industry would be trivial.

104
00:04:52,320 --> 00:04:55,080
We're getting more than one doubling

105
00:04:55,080 --> 00:04:57,160
of the effective labor supply

106
00:04:57,160 --> 00:05:02,160
that we need for each doubling of the labor requirement.

107
00:05:02,560 --> 00:05:06,440
And in that data set, it's like over four.

108
00:05:06,760 --> 00:05:09,720
So we double compute.

109
00:05:09,720 --> 00:05:12,240
Okay, now we need somewhat more researchers,

110
00:05:12,240 --> 00:05:14,440
but a lot less than twice as many.

111
00:05:14,440 --> 00:05:19,440
And so, okay, we use up some of those doublings of compute

112
00:05:19,960 --> 00:05:23,200
on the increasing difficulty of further research,

113
00:05:23,200 --> 00:05:27,380
but most of them are left to expedite the process.

114
00:05:27,380 --> 00:05:31,680
So if you double your labor force,

115
00:05:31,680 --> 00:05:34,720
that's enough to get several doublings of compute.

116
00:05:34,720 --> 00:05:38,680
You use up one of them on meeting

117
00:05:38,680 --> 00:05:41,880
the increased demands from diminishing returns.

118
00:05:41,880 --> 00:05:44,800
The others can be used to accelerate the process.

119
00:05:44,800 --> 00:05:49,200
So you have your first doubling takes however many months,

120
00:05:49,200 --> 00:05:53,480
your next doubling can take a smaller fraction of that,

121
00:05:53,480 --> 00:05:56,600
the next doubling less and so on.

122
00:05:56,600 --> 00:05:58,820
At least in so far as this,

123
00:06:00,360 --> 00:06:02,160
the outputs you're generating,

124
00:06:02,160 --> 00:06:04,680
compute for AI in this story.

125
00:06:04,680 --> 00:06:07,880
Are able to serve the function of the necessary inputs.

126
00:06:07,880 --> 00:06:09,920
If there are other inputs that you need,

127
00:06:09,920 --> 00:06:13,360
eventually those become a bottleneck

128
00:06:13,360 --> 00:06:15,920
and you wind up more restricted on those.

129
00:06:15,920 --> 00:06:16,760
Got it, okay.

130
00:06:16,760 --> 00:06:18,840
So yeah, I think the bloom paper had that,

131
00:06:18,840 --> 00:06:21,280
there was 35% increase in,

132
00:06:21,280 --> 00:06:24,060
was it transferred to destiny or cost per flop?

133
00:06:24,060 --> 00:06:26,040
And there was a 7% increase per year

134
00:06:26,040 --> 00:06:27,720
in the number of researchers required

135
00:06:27,720 --> 00:06:29,080
to sustain that pace.

136
00:06:29,080 --> 00:06:30,520
So something on this, yeah,

137
00:06:30,520 --> 00:06:35,520
it's like four to five doublings of compute

138
00:06:35,880 --> 00:06:38,080
per doubling of labor inputs.

139
00:06:38,080 --> 00:06:39,920
I guess there's a lot of questions you can delve into

140
00:06:39,920 --> 00:06:42,880
in terms of whether you would expect a similar scale

141
00:06:42,880 --> 00:06:47,120
with AI and whether it makes sense to think of AI

142
00:06:47,120 --> 00:06:49,200
as a population of researchers

143
00:06:49,200 --> 00:06:51,840
that keeps growing with compute itself.

144
00:06:51,840 --> 00:06:52,720
Actually, let's go there.

145
00:06:52,720 --> 00:06:54,040
So can you explain the intuition

146
00:06:54,040 --> 00:06:56,240
that compute is a good proxy

147
00:06:56,240 --> 00:06:59,720
for the number of AI researchers so to speak?

148
00:06:59,720 --> 00:07:02,720
So far I've talked about hardware as an initial example

149
00:07:02,720 --> 00:07:06,120
because we had good data about a past period.

150
00:07:06,120 --> 00:07:10,120
You can also make improvements on the software side.

151
00:07:10,120 --> 00:07:12,480
And we think about an intelligence explosion

152
00:07:12,480 --> 00:07:14,920
that can include AI is doing work

153
00:07:14,920 --> 00:07:18,700
on making hardware better, making better software,

154
00:07:18,700 --> 00:07:19,800
making more hardware.

155
00:07:20,920 --> 00:07:25,920
But the basic idea for the hardware is especially simple

156
00:07:26,400 --> 00:07:29,080
in that if you have a worker,

157
00:07:29,080 --> 00:07:31,560
an AI worker that can substitute for a human,

158
00:07:31,560 --> 00:07:33,600
if you have twice as many computers,

159
00:07:33,600 --> 00:07:36,600
you can run two separate instances of them

160
00:07:36,600 --> 00:07:39,240
and then they can do two different jobs,

161
00:07:39,240 --> 00:07:43,040
manage two different machines,

162
00:07:43,040 --> 00:07:46,520
work on two different design problems.

163
00:07:46,520 --> 00:07:50,600
Now, you can get more gains than just what you would get

164
00:07:50,600 --> 00:07:52,080
by having two instances.

165
00:07:52,080 --> 00:07:55,880
We get improvements from using some of our compute,

166
00:07:55,880 --> 00:07:58,720
not just to run more instances of the existing AI,

167
00:07:58,720 --> 00:08:01,080
but to train larger AIs.

168
00:08:01,080 --> 00:08:02,800
So there's hardware technology,

169
00:08:02,800 --> 00:08:06,000
how much you can get per dollar you spend on hardware.

170
00:08:06,000 --> 00:08:07,840
And there's software technology.

171
00:08:07,840 --> 00:08:10,080
And the software can be copied freely.

172
00:08:10,080 --> 00:08:12,080
So if you've got the software,

173
00:08:12,080 --> 00:08:14,680
it doesn't necessarily make that much to say that,

174
00:08:14,680 --> 00:08:18,200
oh, we've got 100 Microsoft Windows.

175
00:08:18,200 --> 00:08:20,280
You can make as many copies as you need

176
00:08:20,280 --> 00:08:25,000
for whatever Microsoft will charge you.

177
00:08:25,000 --> 00:08:26,720
But for hardware is different.

178
00:08:26,720 --> 00:08:30,240
It matters how much we actually spend on the hardware

179
00:08:30,240 --> 00:08:32,000
at a given price.

180
00:08:32,000 --> 00:08:34,360
And if we look at the changes

181
00:08:34,360 --> 00:08:37,360
that have been driving AI recently,

182
00:08:37,360 --> 00:08:39,360
that is the thing that is really off-trend.

183
00:08:39,360 --> 00:08:42,520
We are spending tremendously more money

184
00:08:43,400 --> 00:08:48,400
on computer hardware for training big AI models.

185
00:08:48,720 --> 00:08:49,560
Yeah, okay.

186
00:08:49,560 --> 00:08:52,760
So there's the investment in hardware.

187
00:08:52,760 --> 00:08:54,600
There's a hardware technology itself

188
00:08:54,600 --> 00:08:56,680
and there's the software progress itself.

189
00:08:56,680 --> 00:08:57,720
The AI is getting better

190
00:08:57,720 --> 00:08:59,120
because we're spending more money on it

191
00:08:59,120 --> 00:09:01,800
because our hardware itself is getting better over time.

192
00:09:01,800 --> 00:09:03,800
And because we're developing better models

193
00:09:03,800 --> 00:09:06,640
or better adjustments to those models,

194
00:09:06,640 --> 00:09:08,280
where is the loop here?

195
00:09:08,280 --> 00:09:12,720
The work involved in designing new hardware and software

196
00:09:12,720 --> 00:09:15,240
is being done by people now.

197
00:09:15,240 --> 00:09:18,960
They use computer tools to assist them,

198
00:09:18,960 --> 00:09:23,080
but computer time is not the primary cost

199
00:09:24,000 --> 00:09:27,200
for NVIDIA designing chips,

200
00:09:27,200 --> 00:09:31,720
for TSMC producing them for ASML,

201
00:09:31,720 --> 00:09:36,040
making lithography equipment to serve the TSMC fabs.

202
00:09:36,040 --> 00:09:39,400
And even in AI software research,

203
00:09:39,400 --> 00:09:42,560
that has become quite compute-intensive.

204
00:09:42,560 --> 00:09:44,640
But I think we're still in the range

205
00:09:44,640 --> 00:09:48,080
where at a place like DeepMind salaries,

206
00:09:48,080 --> 00:09:52,160
we're still larger than compute for the experiments.

207
00:09:52,160 --> 00:09:55,400
Although they're tremendously, tremendously more

208
00:09:55,400 --> 00:09:58,280
of the expenditures were on compute

209
00:09:58,280 --> 00:10:00,800
relative to salaries than in the past.

210
00:10:00,800 --> 00:10:03,720
If you take all of the work that's being done

211
00:10:03,720 --> 00:10:05,320
by those humans,

212
00:10:05,320 --> 00:10:08,360
there's like low tens of thousands of people

213
00:10:08,360 --> 00:10:13,200
working at NVIDIA designing GPUs specialized for AI.

214
00:10:13,200 --> 00:10:17,960
I think there's more like 70,000 people at TSMC,

215
00:10:17,960 --> 00:10:21,760
which is the leading producer of cutting-edge chips.

216
00:10:21,760 --> 00:10:25,520
There's a lot of additional people at companies like ASML

217
00:10:25,520 --> 00:10:29,200
that supply them with the tools they need.

218
00:10:29,200 --> 00:10:30,760
And then a company like DeepMind,

219
00:10:30,760 --> 00:10:34,400
I think from their public filings,

220
00:10:34,400 --> 00:10:36,920
they recently had 1,000 people,

221
00:10:36,920 --> 00:10:39,640
opening, I think, is a few hundred people.

222
00:10:39,640 --> 00:10:40,960
Anthropic is less.

223
00:10:40,960 --> 00:10:44,080
If you add up things like Facebook AI research,

224
00:10:44,080 --> 00:10:46,720
Google Brain, R&D,

225
00:10:46,720 --> 00:10:50,320
you get thousands or tens of thousands of people

226
00:10:50,320 --> 00:10:53,960
who are working on AI research.

227
00:10:53,960 --> 00:10:56,920
We'd want to zoom in on those who are developing new methods

228
00:10:56,920 --> 00:10:58,240
rather than narrow applications.

229
00:10:58,240 --> 00:11:01,800
So inventing the transformer definitely counts.

230
00:11:01,800 --> 00:11:05,000
Optimizing for some particular businesses,

231
00:11:05,000 --> 00:11:06,720
dataset cleaning, probably not.

232
00:11:07,720 --> 00:11:10,320
But so those people are doing this work.

233
00:11:10,320 --> 00:11:12,720
They're driving quite a lot of progress.

234
00:11:12,720 --> 00:11:16,320
What we observe and the growth of people

235
00:11:16,320 --> 00:11:19,320
relative to the growth of those capabilities,

236
00:11:19,320 --> 00:11:21,240
is that pretty consistently,

237
00:11:21,240 --> 00:11:25,840
the capabilities are doubling on a shorter time scale

238
00:11:25,840 --> 00:11:29,000
than the people required to do them are doubling.

239
00:11:29,000 --> 00:11:30,800
And so there's work.

240
00:11:30,800 --> 00:11:33,280
So we talked about hardware

241
00:11:33,280 --> 00:11:36,160
and how historically it was pretty dramatic,

242
00:11:36,160 --> 00:11:40,000
like four or five doublings of compute efficiency

243
00:11:40,000 --> 00:11:42,840
per doubling of human inputs.

244
00:11:42,840 --> 00:11:44,360
I think that's a bit lower now

245
00:11:44,360 --> 00:11:45,960
as we get towards the end of Moore's Law.

246
00:11:45,960 --> 00:11:47,960
Although interestingly, not as much lower

247
00:11:47,960 --> 00:11:48,800
as you might think,

248
00:11:48,800 --> 00:11:51,760
because the growth of inputs has also slowed recently.

249
00:11:51,760 --> 00:11:53,240
On the software side,

250
00:11:53,240 --> 00:11:58,240
there's some work by Teme Bessaroglu

251
00:11:58,560 --> 00:12:00,560
and I think collaborators,

252
00:12:02,320 --> 00:12:04,080
may have been the thesis.

253
00:12:04,960 --> 00:12:07,800
It's called our models getting harder to find.

254
00:12:07,800 --> 00:12:09,720
And so it's applying the same sort of analysis

255
00:12:09,720 --> 00:12:12,960
as our ideas getting harder to find.

256
00:12:12,960 --> 00:12:17,400
And you can look at growth rates of papers

257
00:12:17,400 --> 00:12:20,440
from citations, employment at these companies.

258
00:12:20,440 --> 00:12:22,000
And it seems like the doubling time

259
00:12:22,000 --> 00:12:26,560
of these like workers driving the software advances

260
00:12:26,560 --> 00:12:30,920
is like several years, or at least a couple of years.

261
00:12:30,920 --> 00:12:34,040
Whereas the doubling of effective compute

262
00:12:34,040 --> 00:12:36,320
from algorithmic progress is faster.

263
00:12:36,320 --> 00:12:38,720
So there's a group called Epoch.

264
00:12:38,720 --> 00:12:42,120
They've received grants from Open Philanthropy

265
00:12:42,120 --> 00:12:45,480
and they do work collecting datasets

266
00:12:45,520 --> 00:12:48,120
that are relevant to forecasting AI progress.

267
00:12:48,120 --> 00:12:51,960
And so their headline results

268
00:12:51,960 --> 00:12:56,360
for what's the rate of progress in hardware and software

269
00:12:56,360 --> 00:13:00,680
and just like growth in budgets, ours follows.

270
00:13:00,680 --> 00:13:03,760
So for hardware, they're looking at like a doubling

271
00:13:03,760 --> 00:13:05,880
of hardware efficiency that's like two years.

272
00:13:05,880 --> 00:13:07,360
It's possible it's a bit better than that

273
00:13:07,360 --> 00:13:10,440
when you take into account certain specializations

274
00:13:10,440 --> 00:13:11,760
for AI workloads.

275
00:13:11,760 --> 00:13:14,160
For the growth of budgets,

276
00:13:14,160 --> 00:13:16,880
they find a doubling time that's like something

277
00:13:16,880 --> 00:13:19,320
like six months in recent years,

278
00:13:19,320 --> 00:13:22,320
which is pretty tremendous relative

279
00:13:22,320 --> 00:13:24,280
to the historical rates.

280
00:13:24,280 --> 00:13:26,640
We should maybe get into that later.

281
00:13:26,640 --> 00:13:28,640
And then on the algorithmic progress side,

282
00:13:28,640 --> 00:13:32,640
mainly using ImageNet type datasets right now,

283
00:13:32,640 --> 00:13:35,520
they find a doubling time that's less than one year.

284
00:13:35,520 --> 00:13:39,320
And so you combine all of these things

285
00:13:39,320 --> 00:13:41,920
and the growth of effective compute

286
00:13:41,920 --> 00:13:46,920
for training big, big AIs, it's pretty drastic.

287
00:13:47,760 --> 00:13:49,200
I think I saw an estimate that GPD-4

288
00:13:49,200 --> 00:13:51,800
costs like $50 million around that range to train.

289
00:13:51,800 --> 00:13:56,160
Now, suppose that like AGI takes 1000X that,

290
00:13:56,160 --> 00:13:59,160
if you were just a scale of GPD-4, it might not be that.

291
00:13:59,160 --> 00:14:01,360
I'm just just for the sake of example.

292
00:14:01,360 --> 00:14:03,680
So part of that will come from companies

293
00:14:03,680 --> 00:14:05,880
just spending a lot more to train the models

294
00:14:05,880 --> 00:14:07,520
and that just greater investment.

295
00:14:07,520 --> 00:14:10,120
Part of that will come from them having better models

296
00:14:10,160 --> 00:14:13,560
so that what would have taken a 10X increase in the model

297
00:14:13,560 --> 00:14:16,680
to get naively you can do with having a better model

298
00:14:16,680 --> 00:14:18,680
that you only need to do scale up.

299
00:14:18,680 --> 00:14:21,880
You get the same effect of increasing it by 10X

300
00:14:21,880 --> 00:14:23,480
just from having a better model.

301
00:14:23,480 --> 00:14:25,400
And so yeah, you can spend more money

302
00:14:25,400 --> 00:14:26,240
on it to train a bigger model.

303
00:14:26,240 --> 00:14:27,440
You can just have a better model

304
00:14:27,440 --> 00:14:31,920
or you can have chips that are cheaper to train.

305
00:14:31,920 --> 00:14:34,200
So you get more compute for the same dollars.

306
00:14:34,200 --> 00:14:36,600
And okay, so those are the three you were describing.

307
00:14:36,600 --> 00:14:38,320
The ways in which the quote unquote

308
00:14:38,320 --> 00:14:40,280
effective at compute would increase.

309
00:14:40,280 --> 00:14:43,240
From the looking at it right now, it looks like,

310
00:14:43,240 --> 00:14:44,960
yeah, you might get two or three

311
00:14:44,960 --> 00:14:46,720
doublings of effective compute

312
00:14:46,720 --> 00:14:49,880
for this thing that we're calling software progress,

313
00:14:49,880 --> 00:14:52,800
which is, which people get by asking,

314
00:14:52,800 --> 00:14:55,720
well, how much less compute can you use now

315
00:14:55,720 --> 00:14:59,040
to achieve the same benchmark as you achieved before?

316
00:14:59,040 --> 00:15:01,080
There are reasons to not fully identify this

317
00:15:01,080 --> 00:15:02,440
with like software progress

318
00:15:02,440 --> 00:15:04,880
as you might naively think of it

319
00:15:04,880 --> 00:15:07,480
because some of it can be enabled by the other.

320
00:15:07,520 --> 00:15:09,440
So like when you have a lot of compute,

321
00:15:09,440 --> 00:15:11,880
you can do more experiments

322
00:15:11,880 --> 00:15:14,720
and find algorithms that work better.

323
00:15:14,720 --> 00:15:17,760
Sometimes the additional compute,

324
00:15:17,760 --> 00:15:19,600
you can get higher efficiency

325
00:15:19,600 --> 00:15:22,160
by running a bigger model we were talking about earlier.

326
00:15:22,160 --> 00:15:24,560
And so that means you're getting more

327
00:15:24,560 --> 00:15:27,000
for each GPU that you have

328
00:15:27,000 --> 00:15:30,040
because you made this like larger expenditure.

329
00:15:30,040 --> 00:15:33,000
And that can look like a software improvement

330
00:15:33,000 --> 00:15:34,920
because this model,

331
00:15:35,600 --> 00:15:37,480
it's not a hardware improvement directly

332
00:15:37,480 --> 00:15:40,040
because it's doing more with the same hardware,

333
00:15:40,040 --> 00:15:41,480
but you wouldn't have been able to achieve it

334
00:15:41,480 --> 00:15:44,360
without having a ton of GPUs to do the big training run.

335
00:15:44,360 --> 00:15:47,680
The feedback loop itself involves the AI

336
00:15:47,680 --> 00:15:50,480
that is the result of this greater effect of compute,

337
00:15:50,480 --> 00:15:53,920
helping you train better AI, right?

338
00:15:53,920 --> 00:15:56,160
Or use less effective compute in the future

339
00:15:56,160 --> 00:15:58,000
to train better AI.

340
00:15:58,000 --> 00:16:00,320
It can help on the hardware design.

341
00:16:00,320 --> 00:16:03,200
So like NVIDIA is a fabulous chip design company.

342
00:16:03,200 --> 00:16:05,240
They don't make their own chips.

343
00:16:05,240 --> 00:16:09,160
They send files of instructions to TSMC,

344
00:16:09,160 --> 00:16:14,040
which then fabricates the chips in their own facilities.

345
00:16:14,040 --> 00:16:19,040
And so the work of those 10,000 plus people,

346
00:16:20,160 --> 00:16:21,800
if you could automate that

347
00:16:21,800 --> 00:16:25,760
and have the equivalent of a million people doing that work,

348
00:16:25,760 --> 00:16:30,200
then I think you would pretty quickly get the kind

349
00:16:30,200 --> 00:16:31,880
of improvements that can be achieved

350
00:16:31,880 --> 00:16:36,280
with the existing nodes that TSMC is operating on.

351
00:16:36,280 --> 00:16:38,640
You could get a lot of those chip design gains.

352
00:16:38,640 --> 00:16:42,600
Basically like doing the job of improving chip design

353
00:16:42,600 --> 00:16:44,240
that those people are working on now,

354
00:16:44,240 --> 00:16:46,160
but get it done faster.

355
00:16:46,160 --> 00:16:47,480
So that's one thing.

356
00:16:47,480 --> 00:16:48,640
I think that's less important

357
00:16:48,640 --> 00:16:51,000
for the intelligence explosion.

358
00:16:51,000 --> 00:16:54,600
The reason being that when you make an improvement

359
00:16:54,600 --> 00:16:57,200
to chip design, it only applies

360
00:16:57,200 --> 00:16:59,360
to the chips you make after that.

361
00:16:59,360 --> 00:17:02,800
If you make an improvement in AI software,

362
00:17:02,800 --> 00:17:05,920
it has the potential to be immediately applied

363
00:17:05,920 --> 00:17:08,800
to all of the GPUs that you already have.

364
00:17:08,800 --> 00:17:09,640
Yeah.

365
00:17:09,640 --> 00:17:12,080
And so the thing that I think is most disruptive

366
00:17:12,080 --> 00:17:16,280
and most important has the leading edge of the change

367
00:17:16,280 --> 00:17:19,680
from AI automation of the inputs to AI

368
00:17:19,680 --> 00:17:21,240
is on the software side.

369
00:17:21,240 --> 00:17:23,440
At what point would it get to the point

370
00:17:23,440 --> 00:17:27,480
where the AIs are helping develop better software

371
00:17:27,480 --> 00:17:29,320
or better models for future AIs?

372
00:17:29,320 --> 00:17:31,040
Some people claim today, for example,

373
00:17:31,040 --> 00:17:34,680
that programmers at OpenAI are using co-pilot

374
00:17:34,680 --> 00:17:36,560
to write programs now.

375
00:17:36,560 --> 00:17:38,000
So in some sense, you're already having

376
00:17:38,000 --> 00:17:39,560
that sort of feedback loop.

377
00:17:39,560 --> 00:17:43,160
I'm a little skeptical of that as a mechanism.

378
00:17:43,160 --> 00:17:45,000
At what point would it be the case

379
00:17:45,000 --> 00:17:47,920
that the AI is contributing significantly

380
00:17:47,920 --> 00:17:49,800
in the sense that it would almost be the equivalent

381
00:17:49,800 --> 00:17:51,680
of having additional researchers

382
00:17:51,680 --> 00:17:53,360
to AI progress in software?

383
00:17:53,360 --> 00:17:55,840
The quantitative magnitude of the help

384
00:17:55,840 --> 00:17:57,360
is absolutely central.

385
00:17:57,360 --> 00:17:59,040
So there are plenty of companies

386
00:17:59,040 --> 00:18:00,640
that make some product

387
00:18:00,640 --> 00:18:03,520
that very slightly boost productivity.

388
00:18:03,520 --> 00:18:06,760
So when Xerox makes fax machines,

389
00:18:06,760 --> 00:18:09,360
it maybe increases people's productivity

390
00:18:09,360 --> 00:18:12,680
in office work by 0.1% or something.

391
00:18:12,680 --> 00:18:15,640
You're not gonna have explosive growth out of that

392
00:18:15,640 --> 00:18:20,640
because, okay, now 0.1% more effective R&D at Xerox

393
00:18:22,440 --> 00:18:24,920
than any customers buying the machines.

394
00:18:25,920 --> 00:18:27,560
Not that important.

395
00:18:27,560 --> 00:18:30,200
So I think the thing to look for

396
00:18:31,160 --> 00:18:36,160
is when is it the case that the contributions from AI

397
00:18:37,200 --> 00:18:41,400
are starting to become as large or larger

398
00:18:41,400 --> 00:18:43,840
as the contributions from humans?

399
00:18:43,840 --> 00:18:47,680
So when this is boosting their effective productivity

400
00:18:47,680 --> 00:18:52,680
by 50 or 100% and if you then go from eight months

401
00:18:55,200 --> 00:18:57,120
doubling time, say for effective compute

402
00:18:57,120 --> 00:18:58,160
from software innovation,

403
00:18:58,160 --> 00:19:00,600
things like inventing the transformer

404
00:19:00,600 --> 00:19:02,560
or discovering chinchilla scaling

405
00:19:02,560 --> 00:19:04,560
and doing your training runs more optimally

406
00:19:04,560 --> 00:19:06,720
or creating flash attention.

407
00:19:06,720 --> 00:19:10,680
Yeah, if you move that from, say, eight months to four months

408
00:19:10,680 --> 00:19:12,960
and then the next time you apply that,

409
00:19:12,960 --> 00:19:16,000
it significantly increases the boost you're getting

410
00:19:16,000 --> 00:19:16,840
from the AI.

411
00:19:16,840 --> 00:19:18,400
So maybe instead of giving a 50%

412
00:19:18,400 --> 00:19:20,280
or 100% productivity boost,

413
00:19:20,280 --> 00:19:22,440
that's more like a 200%.

414
00:19:22,720 --> 00:19:25,560
And so it doesn't have to have been able

415
00:19:25,560 --> 00:19:28,240
to automate everything involved

416
00:19:28,240 --> 00:19:30,360
in the process of AI research.

417
00:19:30,360 --> 00:19:33,200
It can be, it's automated a bunch of things.

418
00:19:33,200 --> 00:19:36,800
And then those are being done in extreme profusion

419
00:19:36,800 --> 00:19:39,480
because I think that AI can do,

420
00:19:39,480 --> 00:19:42,240
you have it done much more often because it's so cheap.

421
00:19:43,240 --> 00:19:48,080
And so it's not a threshold of this is human level AI.

422
00:19:48,080 --> 00:19:50,520
It can do everything a human can do

423
00:19:50,520 --> 00:19:52,840
with no weaknesses in any area.

424
00:19:52,840 --> 00:19:55,080
It's that even with its weaknesses,

425
00:19:56,120 --> 00:19:58,720
it's able to bump up the performance.

426
00:19:58,720 --> 00:20:01,840
So that instead of getting like the results we would have

427
00:20:01,840 --> 00:20:05,720
with the 10,000 people working on finding these innovations,

428
00:20:05,720 --> 00:20:07,080
we get the results that we would have

429
00:20:07,080 --> 00:20:10,000
if we had twice as many of those people

430
00:20:10,000 --> 00:20:12,680
with the same kind of skill distribution.

431
00:20:12,680 --> 00:20:15,520
And so that's a, it's like a demanding challenge.

432
00:20:15,520 --> 00:20:19,040
It's like you need quite a lot of capability for that.

433
00:20:19,040 --> 00:20:21,960
But it's also important that it's significantly less

434
00:20:21,960 --> 00:20:25,720
than this is a system where there's no way you can point at it

435
00:20:25,720 --> 00:20:29,040
and say in any respect, it is weaker than a human.

436
00:20:29,040 --> 00:20:32,960
A system that was just as good as a human in every respect,

437
00:20:32,960 --> 00:20:35,760
but also had all of the advantages of an AI,

438
00:20:35,760 --> 00:20:38,640
that is just way beyond this point.

439
00:20:38,640 --> 00:20:43,520
Like if you consider that there's like the output

440
00:20:43,520 --> 00:20:47,080
of our existing fabs make tens of millions

441
00:20:47,120 --> 00:20:49,440
of advanced GPUs per year.

442
00:20:49,440 --> 00:20:53,120
Those GPUs, if they were running sort of AI software

443
00:20:53,120 --> 00:20:54,480
that was as efficient as humans,

444
00:20:54,480 --> 00:20:55,680
as a sample efficient,

445
00:20:55,680 --> 00:20:58,200
it doesn't have any major weaknesses.

446
00:20:58,200 --> 00:21:00,920
So they can work four times as long,

447
00:21:02,040 --> 00:21:04,320
the 168 hour work week,

448
00:21:04,320 --> 00:21:07,840
they can have much more education than any human.

449
00:21:07,840 --> 00:21:11,880
So it's a human, they got a PhD,

450
00:21:11,880 --> 00:21:16,880
it's like, wow, it's like 20 years of education,

451
00:21:17,000 --> 00:21:21,840
maybe longer if they take a slow route on the PhD.

452
00:21:21,840 --> 00:21:25,000
It's just normal for us to train large models

453
00:21:25,000 --> 00:21:29,280
by eat the internet, eat all the published books ever,

454
00:21:30,400 --> 00:21:34,440
read everything on GitHub and get good at predicting it.

455
00:21:35,800 --> 00:21:39,440
So like the level of education vastly beyond any human,

456
00:21:39,440 --> 00:21:43,080
the degree to which the models are focused on task

457
00:21:44,080 --> 00:21:47,040
is higher than all, but like the most motivated humans

458
00:21:47,040 --> 00:21:49,080
when they're really, really gunning for it.

459
00:21:49,960 --> 00:21:53,720
So you combine the things tens of millions of GPUs,

460
00:21:53,720 --> 00:21:58,720
each GPU is doing the work of the very best humans

461
00:21:59,120 --> 00:22:03,560
in the world and like the most capable humans in the world

462
00:22:03,560 --> 00:22:06,040
can command salaries that are a lot higher than the average

463
00:22:06,040 --> 00:22:11,040
and particularly in a field like STEM or narrowly AI.

464
00:22:11,680 --> 00:22:13,760
Like there's no human in the world

465
00:22:13,760 --> 00:22:17,040
who has a thousand years of experience with TensorFlow

466
00:22:17,040 --> 00:22:19,520
or let alone the new AI technology

467
00:22:19,520 --> 00:22:21,960
that were invented the year before.

468
00:22:21,960 --> 00:22:24,400
But if they were around,

469
00:22:24,400 --> 00:22:27,240
yeah, they'd be paid millions of dollars a year.

470
00:22:27,240 --> 00:22:29,520
And so when you consider this,

471
00:22:29,520 --> 00:22:32,720
okay, tens of millions of GPUs,

472
00:22:32,720 --> 00:22:36,920
each is doing the work of maybe 40,

473
00:22:36,920 --> 00:22:40,840
maybe more of these kind of existing workers.

474
00:22:40,840 --> 00:22:44,040
This is like going from a workforce of tens of thousands

475
00:22:44,040 --> 00:22:45,520
to hundreds of millions.

476
00:22:47,080 --> 00:22:50,200
You immediately make all kinds of discoveries then,

477
00:22:50,200 --> 00:22:53,520
you immediately develop all sorts of tremendous technologies.

478
00:22:53,520 --> 00:22:58,320
So human level AI is deep, deep

479
00:22:58,320 --> 00:22:59,920
into an intelligence explosion.

480
00:22:59,920 --> 00:23:01,880
Intelligence explosion has to start

481
00:23:01,880 --> 00:23:04,000
with something weaker than that.

482
00:23:04,000 --> 00:23:05,840
Yeah, well, what is the thing it starts with

483
00:23:05,840 --> 00:23:07,680
and how close are we to that?

484
00:23:07,680 --> 00:23:10,360
Because if you think of a research or an open AI or something,

485
00:23:10,560 --> 00:23:14,800
these are, to be a researcher is not just completing

486
00:23:14,800 --> 00:23:18,360
the hello world prompt that co-pilot does, right?

487
00:23:18,360 --> 00:23:20,360
It's like, you got to choose a new idea,

488
00:23:20,360 --> 00:23:22,160
you got to figure out the right way to approach it.

489
00:23:22,160 --> 00:23:23,480
You perhaps have to manage the people

490
00:23:23,480 --> 00:23:25,960
who are also working with you on that problem.

491
00:23:25,960 --> 00:23:29,120
It's like, it's incredibly complicated skill,

492
00:23:29,120 --> 00:23:31,200
portfolio skills rather than just a single skill.

493
00:23:31,200 --> 00:23:34,160
So yeah, what is the point of wish

494
00:23:34,160 --> 00:23:37,160
that feedback loop starts where you can even,

495
00:23:37,160 --> 00:23:40,160
you're not just doing the 0.5% increase in productivity

496
00:23:40,160 --> 00:23:41,880
that a sort of AI tool might do,

497
00:23:41,880 --> 00:23:44,880
but is actually the equivalent of a researcher

498
00:23:44,880 --> 00:23:47,320
or close to it, what is that point?

499
00:23:47,320 --> 00:23:49,760
So I think maybe a way to look at it

500
00:23:49,760 --> 00:23:51,800
is to give some illustrative examples

501
00:23:51,800 --> 00:23:55,000
of the kinds of capabilities that you might see.

502
00:23:55,000 --> 00:23:59,080
And so because these systems have to be a lot weaker

503
00:23:59,080 --> 00:24:01,240
than the sort of human level things,

504
00:24:01,240 --> 00:24:04,280
what we'll have is intense application

505
00:24:04,280 --> 00:24:07,800
of the ways in which AIs have advantages,

506
00:24:07,800 --> 00:24:09,880
partly offsetting their weaknesses.

507
00:24:09,880 --> 00:24:12,240
And so AIs are cheap.

508
00:24:12,240 --> 00:24:17,240
We can call a lot of them to do many small problems.

509
00:24:17,400 --> 00:24:22,160
And so you'll have situations where you have dumber AIs

510
00:24:22,160 --> 00:24:25,960
that are deployed thousands of times

511
00:24:25,960 --> 00:24:28,740
to equal, say, one human worker.

512
00:24:30,360 --> 00:24:34,520
And they'll be doing things like these voting algorithms

513
00:24:34,520 --> 00:24:38,040
where you, with an LLM, you generate

514
00:24:38,040 --> 00:24:40,200
a bunch of different responses

515
00:24:40,200 --> 00:24:41,840
and take a majority vote among them

516
00:24:41,840 --> 00:24:44,360
that improves performance sum.

517
00:24:44,360 --> 00:24:48,840
You'll have things like the AlphaGo kind of approach

518
00:24:48,840 --> 00:24:52,360
where you use the neural net to do search

519
00:24:52,360 --> 00:24:56,160
and you go deeper with the search by plowing in more compute,

520
00:24:56,160 --> 00:24:58,880
which helps to offset the inefficiency

521
00:24:58,880 --> 00:25:02,520
and weaknesses of the model on its own.

522
00:25:02,520 --> 00:25:06,320
You'll do things that would just be totally impractical

523
00:25:06,320 --> 00:25:09,240
for humans because of the sheer number of steps.

524
00:25:09,240 --> 00:25:10,920
And so an example of that would be

525
00:25:10,920 --> 00:25:13,640
designing synthetic training data.

526
00:25:13,640 --> 00:25:17,520
So humans do not learn by just going into the library

527
00:25:17,520 --> 00:25:20,280
and opening books at random pages.

528
00:25:20,280 --> 00:25:22,520
It's actually much, much more efficient

529
00:25:22,520 --> 00:25:26,400
to have things like schools and classes

530
00:25:26,400 --> 00:25:29,240
where they teach you things in an order that makes sense,

531
00:25:29,240 --> 00:25:30,680
that's focusing on the skills

532
00:25:30,680 --> 00:25:33,160
that are more valuable to learn.

533
00:25:33,160 --> 00:25:34,680
They give you tests and exam,

534
00:25:34,680 --> 00:25:36,840
they're designed to try and elicit the skill

535
00:25:36,840 --> 00:25:39,280
they're actually trying to teach.

536
00:25:39,280 --> 00:25:41,320
And right now we don't bother with that

537
00:25:41,320 --> 00:25:45,160
because we can hoover up more data from the internet.

538
00:25:45,160 --> 00:25:47,480
We're getting towards the end of that.

539
00:25:47,480 --> 00:25:49,880
But yeah, as the AIs get more sophisticated,

540
00:25:49,880 --> 00:25:54,880
they'll be better able to tell what is a useful kind

541
00:25:54,880 --> 00:25:57,080
of skill to practice and to generate that.

542
00:25:57,080 --> 00:25:58,600
And we've done that in other areas.

543
00:25:58,600 --> 00:26:02,040
So AlphaGo, the original version of AlphaGo

544
00:26:02,040 --> 00:26:06,120
was booted up with data from human go play

545
00:26:06,120 --> 00:26:09,200
and then improved with reinforcement learning

546
00:26:09,200 --> 00:26:11,800
and Monte Carlo tree search.

547
00:26:11,800 --> 00:26:14,560
But then AlphaZero,

548
00:26:14,560 --> 00:26:17,080
but they somewhat more sophisticated model

549
00:26:17,080 --> 00:26:20,000
benefited from some other improvements

550
00:26:20,000 --> 00:26:23,360
but was able to go from scratch.

551
00:26:23,360 --> 00:26:27,400
And it generated its own data through self play.

552
00:26:28,280 --> 00:26:31,120
So getting data of a higher quality

553
00:26:31,120 --> 00:26:32,080
than the human data,

554
00:26:32,080 --> 00:26:34,360
because there are no human players that good

555
00:26:34,360 --> 00:26:36,080
available in the dataset.

556
00:26:36,080 --> 00:26:38,200
And also a curriculum.

557
00:26:38,200 --> 00:26:40,400
So that at any given point,

558
00:26:40,400 --> 00:26:42,560
it was playing games against an opponent

559
00:26:42,560 --> 00:26:44,840
of equal skill itself.

560
00:26:44,840 --> 00:26:47,240
And so it was always in an area

561
00:26:47,240 --> 00:26:48,560
when it was easy to learn.

562
00:26:48,560 --> 00:26:51,200
If you're just always losing no matter what you do

563
00:26:51,200 --> 00:26:52,960
or always winning no matter what you do,

564
00:26:52,960 --> 00:26:55,840
it's hard to distinguish which things are better

565
00:26:55,840 --> 00:26:57,240
and which are worse.

566
00:26:57,240 --> 00:27:00,680
And when we have somewhat more sophisticated AIs

567
00:27:00,680 --> 00:27:04,280
that can generate training data and tasks for themselves.

568
00:27:04,280 --> 00:27:08,360
For example, if the AI can generate a lot of unit tests

569
00:27:08,360 --> 00:27:10,440
and then can try and produce programs

570
00:27:10,440 --> 00:27:12,840
that pass those unit tests,

571
00:27:12,840 --> 00:27:16,880
then the interpreter is providing a training signal.

572
00:27:16,880 --> 00:27:20,000
And the AI can get good at figuring out

573
00:27:20,000 --> 00:27:21,680
what's the kind of programming problem

574
00:27:21,680 --> 00:27:24,040
that is hard for AIs right now

575
00:27:24,040 --> 00:27:26,560
that will develop more of the skills that I need

576
00:27:27,680 --> 00:27:29,320
and then do them.

577
00:27:29,360 --> 00:27:32,720
And now you're not gonna have employees at OpenAI

578
00:27:32,720 --> 00:27:34,880
write like a billion programming problems.

579
00:27:34,880 --> 00:27:36,760
That's just not gonna happen.

580
00:27:36,760 --> 00:27:40,160
But you are gonna have AIs given the task

581
00:27:40,160 --> 00:27:42,760
of producing those enormous number

582
00:27:42,760 --> 00:27:44,640
of programming challenges.

583
00:27:44,640 --> 00:27:47,040
In LLMS themselves, there's a paper out of Anthropa

584
00:27:47,040 --> 00:27:49,520
called Constitution AI or Constitution RL

585
00:27:49,520 --> 00:27:51,080
where they basically had the program

586
00:27:51,080 --> 00:27:52,400
just like talk to itself and say,

587
00:27:52,400 --> 00:27:53,480
like, is this response helpful?

588
00:27:53,480 --> 00:27:55,400
If not, how can I make this more helpful?

589
00:27:55,400 --> 00:27:57,640
And the response is improved.

590
00:27:57,640 --> 00:27:58,640
And then you train the model

591
00:27:58,760 --> 00:28:00,040
and the more helpful response is

592
00:28:00,040 --> 00:28:02,440
that it generates by talking to itself

593
00:28:02,440 --> 00:28:04,760
so that it generates natively.

594
00:28:04,760 --> 00:28:07,320
And you could imagine more sophisticated ways

595
00:28:07,320 --> 00:28:08,960
to do that or better ways to do that.

596
00:28:08,960 --> 00:28:11,760
Okay, so then the question is, listen,

597
00:28:11,760 --> 00:28:13,600
GPT-4 already costs like 50 million

598
00:28:13,600 --> 00:28:15,360
or 100 million or whatever it was.

599
00:28:15,360 --> 00:28:17,280
Even if we have greater effective compute

600
00:28:17,280 --> 00:28:19,680
from hardware increases and better models,

601
00:28:20,560 --> 00:28:22,720
it's hard to imagine how we could sustain

602
00:28:22,720 --> 00:28:26,000
like four or five more orders of magnitude,

603
00:28:26,000 --> 00:28:29,680
greater effective size than GPT-4

604
00:28:29,680 --> 00:28:32,120
unless we're jumping in like trillions of dollars

605
00:28:32,120 --> 00:28:35,040
like the entire economies of big countries

606
00:28:35,040 --> 00:28:36,920
into training the next version.

607
00:28:36,920 --> 00:28:40,360
So the question is, do we get something

608
00:28:40,360 --> 00:28:42,480
that can significantly help with AI progress

609
00:28:42,480 --> 00:28:47,480
before we run out of the sheer money and scale

610
00:28:48,600 --> 00:28:50,560
and compute that would require to train it?

611
00:28:50,560 --> 00:28:51,720
Do you have a take on that?

612
00:28:51,720 --> 00:28:53,200
Well, first I'd say remember

613
00:28:53,200 --> 00:28:56,360
that there are these three contributing trends.

614
00:28:56,360 --> 00:28:59,680
So the new H-100s are significantly better

615
00:28:59,680 --> 00:29:02,280
than the A-100s and a lot of companies

616
00:29:02,280 --> 00:29:06,520
are actually waiting for their deliveries of H-100s

617
00:29:06,520 --> 00:29:10,400
to do even bigger training runs along with the work

618
00:29:10,400 --> 00:29:12,320
of hooking them up into clusters

619
00:29:12,320 --> 00:29:14,240
and engineering the thing.

620
00:29:14,240 --> 00:29:16,560
Yeah, so all of those factors are contributing.

621
00:29:16,560 --> 00:29:18,600
And of course, mathematically,

622
00:29:19,920 --> 00:29:22,880
yeah, if you do four orders of magnitude more

623
00:29:22,880 --> 00:29:24,440
than 50 or a hundred million,

624
00:29:24,440 --> 00:29:27,640
then you're getting to Jolene Deli territory.

625
00:29:27,640 --> 00:29:31,200
And yeah, I think the way to look at it is

626
00:29:32,600 --> 00:29:34,560
at each step along the way,

627
00:29:34,560 --> 00:29:38,120
does it look like it makes sense to do the next step?

628
00:29:39,120 --> 00:29:42,040
And so from where we are right now,

629
00:29:42,040 --> 00:29:45,520
seeing the results with GPT-4 and chat GPT,

630
00:29:45,520 --> 00:29:49,760
companies like Google and Microsoft and whatnot

631
00:29:49,760 --> 00:29:54,000
are pretty convinced that this is very valuable.

632
00:29:54,000 --> 00:29:58,720
You have like talk at Google and Microsoft with Bing

633
00:29:58,720 --> 00:30:01,960
that, well, it's like billion dollar matter

634
00:30:01,960 --> 00:30:06,280
to change market share in search by a percentage point.

635
00:30:06,280 --> 00:30:09,240
And so that can fund a lot.

636
00:30:09,240 --> 00:30:13,840
And on the far end, on the extreme,

637
00:30:13,840 --> 00:30:15,680
if you automate human labor,

638
00:30:15,680 --> 00:30:18,040
we have a hundred trillion dollar economy.

639
00:30:18,040 --> 00:30:21,400
Most of that economy is paid out in wages.

640
00:30:21,400 --> 00:30:26,400
So like between 50 and 70 trillion dollars per year.

641
00:30:27,040 --> 00:30:31,600
If you create AGI, it's going to automate all of that

642
00:30:31,600 --> 00:30:35,200
and keep increasing beyond that.

643
00:30:35,200 --> 00:30:38,960
So the value of the completed project

644
00:30:38,960 --> 00:30:43,560
is very much worth throwing our whole economy into it.

645
00:30:43,560 --> 00:30:45,480
If you're gonna get the good version,

646
00:30:45,480 --> 00:30:48,160
not the catastrophic destruction of the human race

647
00:30:49,240 --> 00:30:53,400
or some other disastrous outcome.

648
00:30:53,400 --> 00:30:57,600
And in between, it's a question of,

649
00:30:57,600 --> 00:31:01,160
well, did the next step, how risky and uncertain is it

650
00:31:01,160 --> 00:31:04,040
and how much growth in the revenue

651
00:31:04,040 --> 00:31:06,840
you can generate with it, do you get?

652
00:31:06,840 --> 00:31:09,440
And so for moving up to a billion dollars,

653
00:31:09,440 --> 00:31:11,720
I think that's absolutely gonna happen.

654
00:31:11,720 --> 00:31:14,720
These large tech companies have R&D budgets,

655
00:31:14,720 --> 00:31:16,000
tens of billions of dollars.

656
00:31:16,000 --> 00:31:18,920
And when you think about it like in the relevant sense,

657
00:31:18,920 --> 00:31:21,640
like all the employees at Microsoft

658
00:31:21,640 --> 00:31:23,160
who are doing software engineering,

659
00:31:23,160 --> 00:31:25,640
that's like contributing to creating software objects.

660
00:31:25,640 --> 00:31:29,800
It's not weird to spend tens of billions of dollars

661
00:31:29,800 --> 00:31:34,800
on a product that would do so much.

662
00:31:34,960 --> 00:31:37,840
And I think it's becoming more clear

663
00:31:37,840 --> 00:31:41,440
that there is sort of market opportunity to fund the thing.

664
00:31:41,440 --> 00:31:43,440
Going up to a hundred billion dollars,

665
00:31:43,440 --> 00:31:46,120
that's like, okay, the existing R&D budgets

666
00:31:46,120 --> 00:31:49,000
spread over multiple years.

667
00:31:49,000 --> 00:31:52,800
But if you keep seeing that when you scale up the model,

668
00:31:52,800 --> 00:31:54,640
it substantially improves the performance.

669
00:31:54,640 --> 00:31:56,240
It opens up new applications.

670
00:31:56,240 --> 00:31:58,720
You're not just improving your search,

671
00:31:58,720 --> 00:32:02,080
but maybe it makes self-driving cars work.

672
00:32:02,080 --> 00:32:05,720
You replace bulk software engineering jobs

673
00:32:05,720 --> 00:32:08,440
or if not replace them, amplify productivity.

674
00:32:08,440 --> 00:32:11,160
In this kind of dynamic, you actually probably want to employ

675
00:32:11,160 --> 00:32:13,200
all the software engineers you can get,

676
00:32:13,240 --> 00:32:15,160
as long as they're able to make any contribution

677
00:32:15,160 --> 00:32:18,840
because the returns of improving stuff in AI itself

678
00:32:18,840 --> 00:32:19,680
get so high.

679
00:32:19,680 --> 00:32:24,680
But yeah, so I think that can go up to a hundred billion.

680
00:32:24,960 --> 00:32:28,240
And at a hundred billion, you're using

681
00:32:28,240 --> 00:32:32,680
like a significant fraction of our existing Vab capacity.

682
00:32:32,680 --> 00:32:37,440
Like right now, the revenue of NVIDIA is like 25 billion.

683
00:32:38,520 --> 00:32:42,920
The revenue of TSMC, I believe is like over 50 billion.

684
00:32:44,120 --> 00:32:49,120
I checked in 2021, NVIDIA was maybe 7.5%,

685
00:32:50,600 --> 00:32:53,800
less than 10% of TSMC revenue.

686
00:32:54,800 --> 00:32:59,800
So there's a lot of room and most of that was not AI chips.

687
00:33:00,160 --> 00:33:01,480
They have a large gaming segment.

688
00:33:01,480 --> 00:33:04,040
There are data center GPUs that are used

689
00:33:04,040 --> 00:33:06,640
for video and the like.

690
00:33:06,640 --> 00:33:11,640
So there's room for more than an order of magnitude increase

691
00:33:12,640 --> 00:33:17,640
by redirecting existing Vabs to produce more AI chips.

692
00:33:17,800 --> 00:33:20,240
And they're just actually using the AI chips

693
00:33:20,240 --> 00:33:21,840
that these companies have in their cloud

694
00:33:21,840 --> 00:33:23,880
for the big training runs.

695
00:33:23,880 --> 00:33:27,200
And so I think that's enough to go to the 10 billion

696
00:33:27,200 --> 00:33:28,920
and then combine with stuff like the H100

697
00:33:28,920 --> 00:33:30,320
to go up to the hundred billion.

698
00:33:30,320 --> 00:33:31,640
Just to emphasize for the audience,

699
00:33:31,640 --> 00:33:33,480
the initial point about revenue made,

700
00:33:33,480 --> 00:33:38,480
if it cost open AI $100 million to train GPT-4

701
00:33:38,480 --> 00:33:41,160
and it generates $500 million in revenue,

702
00:33:41,480 --> 00:33:43,680
you pay back your expenses with the hundred million,

703
00:33:43,680 --> 00:33:46,160
you have 400 million for your next training run,

704
00:33:46,160 --> 00:33:48,320
then you train your GPT-4.5,

705
00:33:48,320 --> 00:33:51,640
you get let's say $4 billion out of revenue out of that.

706
00:33:51,640 --> 00:33:54,120
That's where the feedback group of revenue comes from,

707
00:33:54,120 --> 00:33:55,600
where you're automating tasks

708
00:33:55,600 --> 00:33:57,120
and therefore you're making money,

709
00:33:57,120 --> 00:34:00,240
you can use that money to automate more tasks.

710
00:34:00,240 --> 00:34:04,560
On the ability to redirect the fat production

711
00:34:04,560 --> 00:34:06,680
towards AI chips.

712
00:34:06,680 --> 00:34:10,000
So then the TLDR on,

713
00:34:10,000 --> 00:34:12,600
you want $100 billion worth of compute.

714
00:34:12,600 --> 00:34:16,120
I mean, fabs take what like a decade or so to build.

715
00:34:16,120 --> 00:34:17,440
So given the ones we have now

716
00:34:17,440 --> 00:34:19,600
and the ones that are gonna come online in the next decade,

717
00:34:19,600 --> 00:34:24,200
is there enough to sustain $100 billion of GPU compute

718
00:34:24,200 --> 00:34:26,800
if you wanted to spend that on a training run?

719
00:34:26,800 --> 00:34:29,520
Yes, you would definitely make the hundred billion one.

720
00:34:29,520 --> 00:34:32,640
How do you go up to a trillion dollar run and larger?

721
00:34:33,560 --> 00:34:36,560
It's gonna involve more fab construction

722
00:34:36,560 --> 00:34:39,880
and yeah, fabs can take a long time to build.

723
00:34:39,880 --> 00:34:43,360
On the other hand, if in fact,

724
00:34:43,360 --> 00:34:48,240
you're getting very high revenue from the AI systems

725
00:34:48,240 --> 00:34:49,960
and you're actually bottlenecked

726
00:34:49,960 --> 00:34:52,720
on the construction of these fabs,

727
00:34:52,720 --> 00:34:55,680
then their price could skyrocket.

728
00:34:55,680 --> 00:35:00,480
And that lead to measures we've never seen before

729
00:35:00,480 --> 00:35:03,600
to expand and accelerate fab production.

730
00:35:03,600 --> 00:35:04,920
Like if you consider,

731
00:35:04,920 --> 00:35:07,640
so at the limit has you're getting models

732
00:35:07,640 --> 00:35:10,800
that approach human-like capability.

733
00:35:10,800 --> 00:35:12,000
Can you imagine things that are getting close

734
00:35:12,000 --> 00:35:15,960
to like brain-like efficiencies plus AI advantages?

735
00:35:15,960 --> 00:35:17,680
We were talking before about, well,

736
00:35:17,680 --> 00:35:21,760
a GPU that is supporting an AI,

737
00:35:21,760 --> 00:35:24,200
really it's a cluster of GPU supporting AI

738
00:35:24,200 --> 00:35:28,120
is that do things in parallel, data parallelism.

739
00:35:28,120 --> 00:35:32,920
But if that can work four times as much as a human,

740
00:35:32,920 --> 00:35:35,640
a highly skilled, motivated, focused human

741
00:35:35,640 --> 00:35:38,080
with levels of education that have never been seen

742
00:35:38,080 --> 00:35:39,880
in the human population.

743
00:35:39,880 --> 00:35:43,640
And so if like a typical software engineer

744
00:35:43,640 --> 00:35:45,400
can earn hundreds of thousands of dollars,

745
00:35:45,400 --> 00:35:48,080
the world's best software engineers

746
00:35:48,080 --> 00:35:49,800
can earn millions of dollars today

747
00:35:49,800 --> 00:35:53,960
and maybe more in a world where there's so much demand for AI.

748
00:35:54,960 --> 00:35:58,960
And then times four for working all the time.

749
00:35:58,960 --> 00:36:00,280
Well, I mean, if you have,

750
00:36:00,280 --> 00:36:03,240
if you can generate like close to $10 million a year

751
00:36:04,240 --> 00:36:07,600
out of the future version of H100,

752
00:36:07,600 --> 00:36:09,760
and it costs tens of thousands of dollars

753
00:36:09,760 --> 00:36:12,040
with a huge profit margin now.

754
00:36:12,040 --> 00:36:15,600
And profit margin could be reduced

755
00:36:15,600 --> 00:36:16,920
with like large production.

756
00:36:18,080 --> 00:36:21,680
That is a big difference that chip

757
00:36:21,680 --> 00:36:24,200
pays for itself almost instantly.

758
00:36:25,200 --> 00:36:30,040
And so you could support paying 10 times as much

759
00:36:30,040 --> 00:36:33,360
to have these fabs constructed more rapidly.

760
00:36:33,360 --> 00:36:37,520
You could have, if AI is starting to be able to contribute,

761
00:36:37,520 --> 00:36:40,800
you could have AI contributing more

762
00:36:40,800 --> 00:36:42,880
of the skilled technical work that makes it hard

763
00:36:42,880 --> 00:36:47,200
for say, NVIDIA to suddenly find thousands upon thousands

764
00:36:47,200 --> 00:36:51,520
of top quality engineering hires, if AI can provide that.

765
00:36:51,520 --> 00:36:54,880
Now, if AI hasn't reached that level of performance,

766
00:36:54,880 --> 00:36:57,680
then this is how you can have things stall out.

767
00:36:57,680 --> 00:36:59,760
And like a world where AI progress stalls out

768
00:36:59,760 --> 00:37:02,080
is one where you go to the $100 billion

769
00:37:02,080 --> 00:37:05,000
and then over succeeding years,

770
00:37:05,000 --> 00:37:08,480
trillion dollar things, software, progress,

771
00:37:09,400 --> 00:37:12,800
turns out to stall.

772
00:37:12,800 --> 00:37:14,960
You lose the gains that you are getting

773
00:37:14,960 --> 00:37:16,920
from moving researchers from other fields,

774
00:37:16,920 --> 00:37:19,200
lots of physicists and people from other areas

775
00:37:19,200 --> 00:37:21,240
of computer science have been going to AI.

776
00:37:21,240 --> 00:37:24,600
But you sort of tap out those resources.

777
00:37:24,600 --> 00:37:28,320
Has it AI becomes a larger proportion of the research field?

778
00:37:28,520 --> 00:37:30,880
And like, okay, you've put in all of these inputs,

779
00:37:30,880 --> 00:37:33,280
but they just haven't yielded AI yet.

780
00:37:33,280 --> 00:37:37,240
I think that set of inputs probably would yield

781
00:37:37,240 --> 00:37:38,920
the kind of AI capabilities needed

782
00:37:38,920 --> 00:37:40,280
for intelligence explosion.

783
00:37:40,280 --> 00:37:43,680
But if it doesn't, after we've exhausted

784
00:37:43,680 --> 00:37:46,880
this current scale up of like increasing the share

785
00:37:46,880 --> 00:37:49,440
of our economy that is trying to make AI,

786
00:37:49,440 --> 00:37:51,800
if that's not enough, then after that,

787
00:37:51,800 --> 00:37:53,720
you have to wait for the slow grind

788
00:37:53,720 --> 00:37:56,080
of things like general economic growth,

789
00:37:56,080 --> 00:37:57,600
population growth and such.

790
00:37:57,600 --> 00:37:58,960
And so things slow.

791
00:37:58,960 --> 00:38:01,280
And that results in my credences

792
00:38:01,280 --> 00:38:03,680
and this kind of advanced AI happening

793
00:38:03,680 --> 00:38:07,800
to be relatively concentrated like over the next 10 years

794
00:38:07,800 --> 00:38:10,040
compared to the rest of the century.

795
00:38:10,040 --> 00:38:12,120
Because we just can't, we can't keep going

796
00:38:12,120 --> 00:38:16,640
with this rapid redirection of resources into AI.

797
00:38:16,640 --> 00:38:18,440
That's a one time thing.

798
00:38:18,440 --> 00:38:21,320
If the current scale up works, it's going to happen.

799
00:38:21,320 --> 00:38:22,880
We're gonna get to AI really fast,

800
00:38:22,880 --> 00:38:24,800
like within the next 10 years or something.

801
00:38:24,800 --> 00:38:26,480
If the current scale up doesn't work,

802
00:38:26,520 --> 00:38:29,560
all we're left with is just like economy

803
00:38:29,560 --> 00:38:30,880
growing like 2% of years.

804
00:38:30,880 --> 00:38:34,080
We have like 2% a year more resources to spend on AI.

805
00:38:34,080 --> 00:38:36,680
And at that scale, you're talking about decades

806
00:38:36,680 --> 00:38:39,720
before you can just through sheer brute force,

807
00:38:39,720 --> 00:38:42,720
you can train the $10 trillion model or something.

808
00:38:42,720 --> 00:38:45,240
Let's talk about why you have your thesis

809
00:38:45,240 --> 00:38:47,960
that the current scale up would work.

810
00:38:47,960 --> 00:38:50,080
What is the evidence from AI itself

811
00:38:50,080 --> 00:38:52,240
or maybe from private evolution

812
00:38:52,240 --> 00:38:53,760
and the evolution of other animals?

813
00:38:53,760 --> 00:38:55,880
Just give me the whole, the whole confluence

814
00:38:55,880 --> 00:38:56,720
of reasons that make you think.

815
00:38:56,720 --> 00:38:59,280
I think maybe the best way to look at that

816
00:38:59,280 --> 00:39:02,440
might be to consider when I first became interested

817
00:39:02,440 --> 00:39:04,080
in this area, so in the 2000s,

818
00:39:04,080 --> 00:39:07,680
which was before the deep learning revolution,

819
00:39:07,680 --> 00:39:09,080
how would I think about timelines?

820
00:39:09,080 --> 00:39:11,120
How did I think about timelines?

821
00:39:11,120 --> 00:39:14,960
And then how have I updated based on what has been happening

822
00:39:14,960 --> 00:39:16,400
with deep learning?

823
00:39:16,400 --> 00:39:20,240
And so back then, I would have said,

824
00:39:21,480 --> 00:39:24,000
we know the brain is a physical object

825
00:39:24,000 --> 00:39:26,080
and information processing device.

826
00:39:26,080 --> 00:39:29,600
It works, it's possible.

827
00:39:29,600 --> 00:39:31,560
And not only is it possible,

828
00:39:31,560 --> 00:39:35,160
it was created by evolution on earth.

829
00:39:35,160 --> 00:39:38,520
And so that gives us something of an upper bound

830
00:39:38,520 --> 00:39:42,080
in that this kind of brute force was sufficient.

831
00:39:42,080 --> 00:39:44,240
There are some complexities with like,

832
00:39:44,240 --> 00:39:46,720
well, what if it was a freak accident

833
00:39:46,720 --> 00:39:49,280
and it didn't happen on all of the other planets

834
00:39:49,280 --> 00:39:51,480
and that added some value.

835
00:39:51,480 --> 00:39:53,560
I have a paper with Nick Bustrom on this.

836
00:39:53,560 --> 00:39:57,720
I think basically that's not that important an issue.

837
00:39:57,720 --> 00:40:00,640
There's converging evolution like octopi

838
00:40:00,640 --> 00:40:03,440
are also quite sophisticated.

839
00:40:03,440 --> 00:40:08,440
If a special event was at the level of forming cells at all

840
00:40:08,760 --> 00:40:10,840
or forming brains at all,

841
00:40:10,840 --> 00:40:13,320
we get to skip that because we're choosing

842
00:40:13,320 --> 00:40:15,080
to build computers and we already exist.

843
00:40:15,080 --> 00:40:17,000
We have that advantage.

844
00:40:17,000 --> 00:40:20,360
So say evolution gives something of an upper bound,

845
00:40:20,400 --> 00:40:23,280
really intensive massive brute force search

846
00:40:24,520 --> 00:40:26,560
and things like evolutionary algorithms

847
00:40:26,560 --> 00:40:28,240
can produce intelligence.

848
00:40:28,240 --> 00:40:32,360
Doesn't the fact that octopi and I guess other mammals,

849
00:40:32,360 --> 00:40:34,360
they got to the point of being like pretty intelligent

850
00:40:34,360 --> 00:40:36,200
but not human level intelligent.

851
00:40:36,200 --> 00:40:37,880
Is that some evidence that there's a hard step

852
00:40:37,880 --> 00:40:41,000
between a cephalopod and a human?

853
00:40:41,000 --> 00:40:44,040
Yeah, so that would be a place to look.

854
00:40:45,800 --> 00:40:48,400
It doesn't seem particularly compelling.

855
00:40:48,400 --> 00:40:53,400
One source of evidence on that is work by Herculano Hutzel.

856
00:40:55,080 --> 00:40:56,600
I hope I haven't mispronounced her name

857
00:40:56,600 --> 00:41:01,600
but she's a neuroscientist who has dissolved the brains

858
00:41:01,640 --> 00:41:06,160
of many creatures and by counting the nuclei,

859
00:41:06,160 --> 00:41:10,600
she's able to determine how many neurons are present

860
00:41:10,600 --> 00:41:14,240
in different species and find a lot of interesting trends

861
00:41:14,240 --> 00:41:15,080
in scaling laws.

862
00:41:15,400 --> 00:41:19,200
She's a paper discussing the human brain

863
00:41:19,200 --> 00:41:21,760
has a scaled up primate brain

864
00:41:21,760 --> 00:41:25,080
and across like a wide variety of animals

865
00:41:25,080 --> 00:41:27,320
and mammals in particular.

866
00:41:27,320 --> 00:41:29,800
There are certain characteristic changes

867
00:41:29,800 --> 00:41:32,440
in the relative number of neurons

868
00:41:32,440 --> 00:41:35,800
size of different brain regions have things scale up.

869
00:41:36,600 --> 00:41:41,600
There's a lot of, yeah, there's a lot of structural similarity

870
00:41:42,600 --> 00:41:47,600
there and you can explain a lot of what is different about us

871
00:41:47,800 --> 00:41:50,960
with a pretty brute force story,

872
00:41:50,960 --> 00:41:54,360
which is that you expend resources

873
00:41:54,360 --> 00:41:58,240
on having a bigger brain, keeping it in good order,

874
00:41:58,240 --> 00:41:59,760
giving it time to learn.

875
00:41:59,760 --> 00:42:01,680
So we have an unusually long childhood,

876
00:42:01,680 --> 00:42:03,840
unusually long in its period.

877
00:42:03,840 --> 00:42:06,400
We spend more compute by having a larger brain

878
00:42:06,400 --> 00:42:09,000
than other animals, more than three times

879
00:42:09,000 --> 00:42:10,920
as large as chimpanzees.

880
00:42:10,920 --> 00:42:14,760
And then we have a longer childhood than chimpanzees

881
00:42:14,760 --> 00:42:17,200
and much more than many, many other creatures.

882
00:42:17,200 --> 00:42:19,200
So we're spending more compute in a way

883
00:42:19,200 --> 00:42:21,680
that's analogous to like having a bigger model

884
00:42:21,680 --> 00:42:23,840
and having more training time with it.

885
00:42:23,840 --> 00:42:28,840
And given that we see with our AI models,

886
00:42:29,320 --> 00:42:31,960
this sort of like large consistent benefits

887
00:42:31,960 --> 00:42:35,000
from increasing compute spent in those ways

888
00:42:35,000 --> 00:42:38,320
and with qualitatively new capabilities

889
00:42:38,320 --> 00:42:40,280
showing up over and over again,

890
00:42:40,280 --> 00:42:44,480
particularly in areas that sort of AI skeptics call out

891
00:42:44,480 --> 00:42:47,880
in my experience like over the last 15 years,

892
00:42:47,880 --> 00:42:49,680
the things that people call out has like,

893
00:42:49,680 --> 00:42:51,520
ah, but the AI can't do that.

894
00:42:51,520 --> 00:42:54,000
And it's because of a fundamental limitation.

895
00:42:54,000 --> 00:42:55,800
We've gone through a lot of them.

896
00:42:55,800 --> 00:43:00,240
There were Winograd schemas, catastrophic forgetting,

897
00:43:00,240 --> 00:43:01,680
quite a number.

898
00:43:01,680 --> 00:43:06,680
And yeah, they have repeatedly gone away through scaling.

899
00:43:07,280 --> 00:43:12,040
And so there's a picture that we're seeing supported

900
00:43:12,040 --> 00:43:15,440
from biology and from our experience with AI

901
00:43:15,440 --> 00:43:19,320
where you can explain like, yeah, in general,

902
00:43:19,320 --> 00:43:22,920
there are trade-offs where the extra fitness you get

903
00:43:22,920 --> 00:43:25,360
from a brain is not worth it.

904
00:43:25,360 --> 00:43:29,160
And so creatures wind up mostly with small brains

905
00:43:29,160 --> 00:43:32,000
because they can save that biological energy

906
00:43:32,000 --> 00:43:35,880
and that time to reproduce for digestion.

907
00:43:35,920 --> 00:43:36,760
And so on.

908
00:43:36,760 --> 00:43:40,640
And humans, we actually seem to have wound up

909
00:43:40,640 --> 00:43:43,840
in a niche within self-reinforcing

910
00:43:43,840 --> 00:43:46,200
where we greatly increase the returns

911
00:43:46,200 --> 00:43:47,880
to having large brains.

912
00:43:47,880 --> 00:43:52,880
And language and technology are the sort of obvious candidates.

913
00:43:53,280 --> 00:43:55,480
When you have humans around you

914
00:43:55,480 --> 00:43:58,000
who know a lot of things and they can teach you

915
00:43:58,000 --> 00:43:59,720
and compared to almost any other species,

916
00:43:59,720 --> 00:44:03,400
we have vastly more instruction from parents

917
00:44:03,440 --> 00:44:06,000
and the society of the young.

918
00:44:06,000 --> 00:44:09,240
Then you're getting way more from your brain

919
00:44:09,240 --> 00:44:11,160
because you can get, per minute,

920
00:44:11,160 --> 00:44:13,720
you can learn a lot more useful skills

921
00:44:13,720 --> 00:44:15,920
and then you can provide the energy you need

922
00:44:15,920 --> 00:44:19,080
to feed that brain by hunting and gathering,

923
00:44:19,080 --> 00:44:22,280
by having fire that makes digestion easier.

924
00:44:22,280 --> 00:44:24,680
And basically how this process goes on,

925
00:44:24,680 --> 00:44:27,680
it's increasing the marginal increase

926
00:44:27,680 --> 00:44:29,920
in reproductive fitness you get

927
00:44:29,920 --> 00:44:31,680
from allocating more resources

928
00:44:31,680 --> 00:44:34,840
along a bunch of dimensions towards cognitive ability.

929
00:44:34,840 --> 00:44:39,000
And so that's bigger brains, longer childhood,

930
00:44:39,000 --> 00:44:41,160
having our attention be more on learning.

931
00:44:41,160 --> 00:44:45,880
So humans play a lot and we keep playing as adults,

932
00:44:45,880 --> 00:44:49,200
which is a very weird thing compared to other animals.

933
00:44:49,200 --> 00:44:53,480
We're more motivated to copy other humans around us

934
00:44:53,480 --> 00:44:55,400
than like even than the other primates.

935
00:44:55,400 --> 00:44:58,040
And so these are sort of motivational changes

936
00:44:58,040 --> 00:45:01,800
that keep us using more of our attention and effort

937
00:45:01,800 --> 00:45:03,480
on learning, which pays off more

938
00:45:03,480 --> 00:45:05,920
when you have a bigger brain and a longer lifespan

939
00:45:05,920 --> 00:45:07,000
in which to learn.

940
00:45:07,000 --> 00:45:11,120
Many creatures are subject to lots of predation or disease.

941
00:45:11,120 --> 00:45:15,120
And so if you try, you're a mayfly or a mouse,

942
00:45:15,120 --> 00:45:17,320
if you try and invest in like a giant brain

943
00:45:17,320 --> 00:45:19,600
and a very long childhood,

944
00:45:19,600 --> 00:45:22,920
you're quite likely to be killed by some predator

945
00:45:22,920 --> 00:45:25,760
or some disease before you're able to actually use it.

946
00:45:25,800 --> 00:45:28,120
And so that means you actually have exponentially

947
00:45:28,120 --> 00:45:30,680
increasing costs in a given niche.

948
00:45:30,680 --> 00:45:34,000
So if I have a 50% chance of dying every few months

949
00:45:34,000 --> 00:45:38,000
of a little mammal or a little lizard or something,

950
00:45:38,000 --> 00:45:42,000
that means the cost of going from three months to 30 months

951
00:45:42,000 --> 00:45:44,840
of learning and childhood development,

952
00:45:44,840 --> 00:45:47,120
it's not 10 times the loss.

953
00:45:47,120 --> 00:45:49,560
It's now it's two to the negative 10.

954
00:45:49,560 --> 00:45:54,560
So factor of 1,024 reduction in the benefit I get

955
00:45:55,840 --> 00:45:57,480
from what I ultimately learn

956
00:45:57,480 --> 00:46:01,280
because 99.9% of the animals will have been killed

957
00:46:01,280 --> 00:46:02,360
before that point.

958
00:46:02,360 --> 00:46:04,600
We're in a niche where we're like a large,

959
00:46:04,600 --> 00:46:07,960
long-lived animal with language and technology.

960
00:46:07,960 --> 00:46:10,520
So where we can learn a lot from our groups.

961
00:46:10,520 --> 00:46:14,960
And that means it pays off to really just expand

962
00:46:14,960 --> 00:46:19,560
our investment on these multiple fronts in intelligence.

963
00:46:19,560 --> 00:46:21,160
That's so interesting.

964
00:46:22,240 --> 00:46:24,520
Just with the audience, the calculation about like

965
00:46:24,560 --> 00:46:26,080
two to the whatever months is just like,

966
00:46:26,080 --> 00:46:27,480
you have a half chance of dying this month,

967
00:46:27,480 --> 00:46:29,440
a half chance of dying next month,

968
00:46:29,440 --> 00:46:30,400
you multiply those together.

969
00:46:30,400 --> 00:46:32,440
Okay, there's other species though

970
00:46:32,440 --> 00:46:37,440
that do live in flocks or as packs where you could imagine.

971
00:46:37,680 --> 00:46:40,480
I mean, they do have like a smaller version

972
00:46:40,480 --> 00:46:42,920
of the development of cubs into

973
00:46:42,920 --> 00:46:44,600
that I like play with each other.

974
00:46:44,600 --> 00:46:49,240
Why isn't this a hill on which they could have climbed

975
00:46:49,240 --> 00:46:52,360
to human level intelligence themselves?

976
00:46:52,400 --> 00:46:55,280
If it's something like language or technology,

977
00:46:55,280 --> 00:46:57,960
humans were getting smarter before we got language.

978
00:46:57,960 --> 00:46:59,480
I mean, obviously we had to get smarter

979
00:46:59,480 --> 00:47:00,440
to get language, right?

980
00:47:00,440 --> 00:47:02,480
We couldn't just get language without becoming smarter.

981
00:47:02,480 --> 00:47:06,480
So yeah, it seems like there should be other species

982
00:47:06,480 --> 00:47:08,000
that should have beginnings

983
00:47:08,000 --> 00:47:09,840
of this sort of cognitive revolution,

984
00:47:09,840 --> 00:47:12,440
especially given how valuable it is given,

985
00:47:12,440 --> 00:47:14,120
listen, we've dominated the world.

986
00:47:14,120 --> 00:47:16,240
You would think there'd be selective pressure for it.

987
00:47:16,240 --> 00:47:18,480
Evolution doesn't have foresight.

988
00:47:18,480 --> 00:47:20,680
The thing in this generation

989
00:47:20,680 --> 00:47:24,920
that gets more surviving offspring and grandchildren,

990
00:47:24,920 --> 00:47:27,280
that's the thing that becomes more common.

991
00:47:27,280 --> 00:47:29,360
Evolution doesn't look ahead and they,

992
00:47:29,360 --> 00:47:33,240
oh, in a million years, you'll have a lot of descendants.

993
00:47:33,240 --> 00:47:36,640
It's what survives and reproduces now.

994
00:47:36,640 --> 00:47:39,120
And so in fact, there are correlations

995
00:47:39,120 --> 00:47:44,120
where social animals do on average, have larger brains.

996
00:47:45,120 --> 00:47:48,440
And part of that is probably that

997
00:47:48,440 --> 00:47:51,000
the additional social applications of brains,

998
00:47:51,000 --> 00:47:54,000
like keeping track of which of your group members

999
00:47:54,000 --> 00:47:56,840
have helped you before so that you can reciprocate.

1000
00:47:56,840 --> 00:47:59,360
You scratch my back, I'll scratch yours,

1001
00:47:59,360 --> 00:48:01,840
remembering who's dangerous within the group,

1002
00:48:01,840 --> 00:48:03,200
that sort of thing.

1003
00:48:03,200 --> 00:48:06,360
It's an additional application of intelligence.

1004
00:48:06,360 --> 00:48:08,840
And so there's some correlation there.

1005
00:48:08,840 --> 00:48:12,000
But what it seems like is that,

1006
00:48:12,440 --> 00:48:14,640
yeah, in most of these cases,

1007
00:48:15,640 --> 00:48:17,960
it's enough to invest more,

1008
00:48:17,960 --> 00:48:21,360
but not invest to the point where a mind

1009
00:48:21,360 --> 00:48:25,000
can easily develop language and technology and pass it on.

1010
00:48:25,000 --> 00:48:28,080
And so there are, you see bits of tool use

1011
00:48:28,080 --> 00:48:30,920
in some other primates who have an advantage that,

1012
00:48:30,920 --> 00:48:33,560
so compared to say the whales who have,

1013
00:48:33,560 --> 00:48:34,640
they have quite large brains,

1014
00:48:34,640 --> 00:48:36,760
partly because they are so large themselves

1015
00:48:36,760 --> 00:48:38,840
and they have some other thing,

1016
00:48:38,840 --> 00:48:40,280
but they don't have hands,

1017
00:48:40,280 --> 00:48:42,680
which means that reduces a bunch of ways

1018
00:48:42,680 --> 00:48:44,680
in which brains can pay off

1019
00:48:44,680 --> 00:48:47,840
and investments in the functioning of that brain.

1020
00:48:47,840 --> 00:48:52,840
But yeah, so primates will use sticks to extract termites.

1021
00:48:53,880 --> 00:48:55,880
Capuchin monkeys will open clams

1022
00:48:55,880 --> 00:48:58,040
by smashing them with a rock.

1023
00:48:58,040 --> 00:49:00,000
So there's bits of tool use,

1024
00:49:00,000 --> 00:49:04,840
but what they don't have is the ability to sustain culture.

1025
00:49:04,840 --> 00:49:07,920
A particular primate will maybe discover

1026
00:49:07,920 --> 00:49:09,920
one of these tactics and maybe it'll be copied

1027
00:49:09,920 --> 00:49:12,040
by their immediate group.

1028
00:49:12,040 --> 00:49:14,160
But they're not holding onto it that well.

1029
00:49:14,160 --> 00:49:16,880
They're like, well, when they see the other animal do it,

1030
00:49:16,880 --> 00:49:19,080
they can copy it in that situation.

1031
00:49:19,080 --> 00:49:20,720
They don't actively teach each other,

1032
00:49:20,720 --> 00:49:23,800
their population locally is quite small.

1033
00:49:23,800 --> 00:49:26,280
So it's easy to forget things,

1034
00:49:26,280 --> 00:49:28,240
easy to lose information.

1035
00:49:28,240 --> 00:49:31,560
And in fact, they remain technologically stagnant

1036
00:49:31,560 --> 00:49:34,080
for hundreds of thousands of years.

1037
00:49:34,080 --> 00:49:37,320
And we can actually look at some human situations.

1038
00:49:37,320 --> 00:49:39,520
So there's an old paper,

1039
00:49:39,520 --> 00:49:43,040
I believe by the economist, Michael Kramer,

1040
00:49:43,920 --> 00:49:46,840
talks about technological growth

1041
00:49:46,840 --> 00:49:50,640
in the different continents for human societies.

1042
00:49:50,640 --> 00:49:55,640
And so you have Eurasia is the largest integrated

1043
00:49:55,720 --> 00:49:57,800
connected area, Africa is partly connected to it,

1044
00:49:57,800 --> 00:50:00,680
but the Sahara desert restricts the flow

1045
00:50:00,680 --> 00:50:03,320
of information and technology and such.

1046
00:50:03,320 --> 00:50:04,640
And then you had the Americas,

1047
00:50:04,640 --> 00:50:07,200
which were after the colonization from the land bridge

1048
00:50:07,200 --> 00:50:11,040
were largely separated and are smaller than Eurasia.

1049
00:50:11,040 --> 00:50:14,320
Then Australia, and then you had like smaller island situations

1050
00:50:14,320 --> 00:50:15,560
like Tasmania.

1051
00:50:15,560 --> 00:50:19,320
And so technological progress seems to have been faster

1052
00:50:19,320 --> 00:50:22,200
at the larger, the connected group of people.

1053
00:50:22,960 --> 00:50:26,480
And in the smallest groups, so like in Tasmania,

1054
00:50:26,480 --> 00:50:27,840
you had a relatively small population

1055
00:50:27,840 --> 00:50:29,480
and they actually lost technology.

1056
00:50:30,400 --> 00:50:34,000
So things like they lost some like fishing techniques.

1057
00:50:34,000 --> 00:50:36,640
And if you have a small population

1058
00:50:36,640 --> 00:50:40,000
and you have some limited number of people who know a skill

1059
00:50:40,000 --> 00:50:42,960
and they happen to die or it happened,

1060
00:50:42,960 --> 00:50:46,200
there's like some change in circumstances

1061
00:50:46,200 --> 00:50:49,720
that causes people not to practice or pass on that thing.

1062
00:50:49,720 --> 00:50:51,120
And then you lose it.

1063
00:50:51,120 --> 00:50:54,360
And if you have few people, you're doing less innovation.

1064
00:50:54,360 --> 00:50:57,720
The rate at which you lose technologies

1065
00:50:57,720 --> 00:50:59,160
to some kind of local disturbance

1066
00:50:59,160 --> 00:51:02,000
and the rate at which you create new technologies

1067
00:51:02,000 --> 00:51:03,720
can wind up in balance.

1068
00:51:03,720 --> 00:51:08,720
And the great change of hominids and humanity

1069
00:51:08,880 --> 00:51:10,440
if that we wound up in this situation,

1070
00:51:10,440 --> 00:51:13,360
we were accumulating faster than we were losing.

1071
00:51:13,360 --> 00:51:14,920
And as we accumulated,

1072
00:51:14,920 --> 00:51:18,160
those technologies allowed us to expand our population.

1073
00:51:18,160 --> 00:51:21,640
They created additional demand for intelligence

1074
00:51:21,640 --> 00:51:24,280
so that our brains became three times as large.

1075
00:51:24,280 --> 00:51:25,120
Is that chimpanzees?

1076
00:51:25,120 --> 00:51:25,960
Chimpanzees, yeah.

1077
00:51:25,960 --> 00:51:30,120
And our ancestors who had a similar brain size.

1078
00:51:30,320 --> 00:51:34,120
And then the crucial point, I guess, in relevance to AI

1079
00:51:34,120 --> 00:51:38,240
is that the selective pressures against intelligence

1080
00:51:38,240 --> 00:51:42,760
in other animals are not acting against these neural networks

1081
00:51:42,760 --> 00:51:44,080
because we are, you know,

1082
00:51:44,080 --> 00:51:45,520
they're not gonna get like eaten by a predator

1083
00:51:45,520 --> 00:51:48,240
if they spend too much time becoming more intelligent.

1084
00:51:48,240 --> 00:51:49,800
We're like explicitly treating them

1085
00:51:49,800 --> 00:51:51,040
to become more intelligent.

1086
00:51:51,040 --> 00:51:54,100
So we have like good first principles reason to think

1087
00:51:54,100 --> 00:51:57,280
that if it was scaling that made our minds this powerful

1088
00:51:57,280 --> 00:51:59,960
and if the things that prevented other animals

1089
00:52:00,800 --> 00:52:04,000
from scaling are not impinging on these neural networks,

1090
00:52:04,000 --> 00:52:05,280
that these things should just continue

1091
00:52:05,280 --> 00:52:06,440
to become very smart.

1092
00:52:06,440 --> 00:52:09,160
Yeah, we're growing them in a technological culture

1093
00:52:09,160 --> 00:52:11,880
where there are jobs like software engineer

1094
00:52:11,880 --> 00:52:16,320
that depend much more on sort of cognitive output

1095
00:52:16,320 --> 00:52:19,520
and less on things like metabolic resources

1096
00:52:19,520 --> 00:52:21,800
devoted to the immune system

1097
00:52:21,800 --> 00:52:25,560
or to like building big muscles to throw spears.

1098
00:52:25,560 --> 00:52:27,600
This is kind of a side note, but I'm just kind of interested.

1099
00:52:27,600 --> 00:52:28,760
I think you referenced at some point,

1100
00:52:28,800 --> 00:52:30,560
I think it's a bit of a chinchilla scaling for the audience.

1101
00:52:30,560 --> 00:52:33,560
This is a paper from DeepMind which describes

1102
00:52:33,560 --> 00:52:35,040
if you have a model of a certain size,

1103
00:52:35,040 --> 00:52:36,880
what is the optimum amount of data

1104
00:52:36,880 --> 00:52:38,720
that it should be trained on?

1105
00:52:38,720 --> 00:52:40,240
So you can imagine bigger models,

1106
00:52:40,240 --> 00:52:43,120
you can use more data to train them.

1107
00:52:43,120 --> 00:52:44,240
And in this way you can figure out

1108
00:52:44,240 --> 00:52:45,240
where should you spend your computer,

1109
00:52:45,240 --> 00:52:46,600
should you spend it on making the model bigger

1110
00:52:46,600 --> 00:52:49,200
or should you spend it on training it for longer?

1111
00:52:49,200 --> 00:52:53,280
I'm curious if in the case of different animals,

1112
00:52:53,280 --> 00:52:54,760
in some sense they're like model sizes,

1113
00:52:54,760 --> 00:52:55,800
they're how big their brain is

1114
00:52:55,800 --> 00:52:56,960
and they're training data sizes,

1115
00:52:56,960 --> 00:52:58,560
like how long they're cubs

1116
00:52:58,560 --> 00:52:59,920
or how long they're infants or toddlers

1117
00:52:59,920 --> 00:53:02,280
or before they're full adults.

1118
00:53:02,280 --> 00:53:04,720
Is there some sort of like scaling law of?

1119
00:53:04,720 --> 00:53:07,960
Yeah, I mean, so the chinchilla scaling isn't interesting

1120
00:53:07,960 --> 00:53:11,400
because we were talking earlier about the cost function

1121
00:53:11,400 --> 00:53:13,280
for having a longer childhood.

1122
00:53:13,280 --> 00:53:16,120
And so where it's like exponentially increasing

1123
00:53:16,120 --> 00:53:17,920
in the amount of training compute you have

1124
00:53:17,920 --> 00:53:21,480
when you have exogenous forces that can kill you.

1125
00:53:21,480 --> 00:53:23,800
Whereas when we do big training runs,

1126
00:53:23,800 --> 00:53:26,920
the cost of throwing in more GPUs is almost linear.

1127
00:53:26,920 --> 00:53:29,000
And it's much better to be linear

1128
00:53:29,000 --> 00:53:30,800
than exponentially decay.

1129
00:53:30,800 --> 00:53:31,720
Oh, that's a really good point.

1130
00:53:31,720 --> 00:53:32,880
As you expand resources.

1131
00:53:32,880 --> 00:53:36,440
And so chinchilla scaling would suggest that like,

1132
00:53:36,440 --> 00:53:40,160
yeah, for a brain of sort of human size,

1133
00:53:40,160 --> 00:53:42,840
it would be optimal to have many millions of years

1134
00:53:42,840 --> 00:53:46,480
of education, but obviously that's impractical

1135
00:53:46,480 --> 00:53:49,680
because of exogenous mortality for humans.

1136
00:53:49,680 --> 00:53:52,640
And so there's a fairly compelling argument

1137
00:53:52,680 --> 00:53:57,680
that relative to the situation where we would train AI,

1138
00:53:58,320 --> 00:54:02,840
that animals are systematically way under trained.

1139
00:54:02,840 --> 00:54:03,680
That's so interesting.

1140
00:54:03,680 --> 00:54:06,040
And now they're more efficient than our models.

1141
00:54:06,040 --> 00:54:08,040
We still have room to improve our algorithms

1142
00:54:08,040 --> 00:54:10,720
to catch up with the efficiency of brains,

1143
00:54:10,720 --> 00:54:15,720
but they are laboring under that disadvantage, yeah.

1144
00:54:16,160 --> 00:54:17,720
That is so interesting.

1145
00:54:17,720 --> 00:54:19,680
Okay, so I guess another question you could have

1146
00:54:19,680 --> 00:54:24,680
is humans got started on this evolutionary hill climbing

1147
00:54:24,760 --> 00:54:26,400
route where we're getting more intelligent

1148
00:54:26,400 --> 00:54:28,680
that has more benefits for us.

1149
00:54:28,680 --> 00:54:31,520
Why didn't we go all the way on that route?

1150
00:54:31,520 --> 00:54:33,240
If intelligence is so powerful,

1151
00:54:33,240 --> 00:54:38,240
why aren't all humans as smart as we know humans can be?

1152
00:54:39,000 --> 00:54:40,080
At least that smart.

1153
00:54:41,000 --> 00:54:42,480
If intelligence is so powerful,

1154
00:54:42,480 --> 00:54:44,800
like why hasn't there been stronger selective pressure?

1155
00:54:44,800 --> 00:54:46,400
I understand like, oh, listen, hip size,

1156
00:54:46,400 --> 00:54:48,360
you can't like give birth to a really big headed baby

1157
00:54:48,360 --> 00:54:49,200
or whatever, but you would think

1158
00:54:49,240 --> 00:54:51,960
evolution would figure out some way to offset

1159
00:54:51,960 --> 00:54:55,360
that if intelligence has such big power

1160
00:54:55,360 --> 00:54:56,800
and it's so useful.

1161
00:54:56,800 --> 00:54:59,560
Yeah, I think if you actually look at it quantitatively,

1162
00:54:59,560 --> 00:55:01,040
that's not true.

1163
00:55:01,040 --> 00:55:03,560
And even in sort of recent history,

1164
00:55:03,560 --> 00:55:06,640
there has been, it looks like a pretty close balance

1165
00:55:06,640 --> 00:55:09,960
between the costs and the benefits

1166
00:55:09,960 --> 00:55:12,800
of having more cognitive abilities.

1167
00:55:12,800 --> 00:55:17,320
And so you say like, who needs to worry

1168
00:55:17,320 --> 00:55:20,040
about like the metabolic costs?

1169
00:55:20,040 --> 00:55:25,040
Like humans put like order 20% of our metabolic energy

1170
00:55:25,800 --> 00:55:29,360
into the brain and it's higher for like young children.

1171
00:55:29,360 --> 00:55:34,200
So 20% of the, and then there's like breathing

1172
00:55:34,200 --> 00:55:36,800
and digestion and the immune system.

1173
00:55:36,800 --> 00:55:39,360
And so for most of history,

1174
00:55:39,360 --> 00:55:41,480
people have been dying left and right.

1175
00:55:41,480 --> 00:55:43,760
Like a very large proportion of people

1176
00:55:43,760 --> 00:55:46,480
will die of infectious disease.

1177
00:55:46,520 --> 00:55:50,560
And if you put more resources into your immune system,

1178
00:55:50,560 --> 00:55:52,080
you survive.

1179
00:55:52,080 --> 00:55:55,480
So it's like life or death pretty directly

1180
00:55:55,480 --> 00:55:56,760
via that mechanism.

1181
00:55:57,560 --> 00:56:00,400
And then this is related also

1182
00:56:00,400 --> 00:56:03,240
people die more of disease during famine.

1183
00:56:03,240 --> 00:56:04,480
And so there's boom or bust.

1184
00:56:04,480 --> 00:56:08,080
And so if you have 20% less metabolic requirements

1185
00:56:08,080 --> 00:56:11,200
or has anger a child and if you have a lot more,

1186
00:56:11,200 --> 00:56:14,800
I mean it's like 40 or 50% less metabolic requirements,

1187
00:56:14,800 --> 00:56:17,640
you're much more likely to survive that famine.

1188
00:56:17,640 --> 00:56:19,640
So these are pretty big.

1189
00:56:20,480 --> 00:56:21,720
And then there's a trade-off

1190
00:56:21,720 --> 00:56:24,040
about just cleaning and mutational load.

1191
00:56:24,040 --> 00:56:27,920
So every generation new mutations and errors happen

1192
00:56:27,920 --> 00:56:29,220
in the process of reproduction.

1193
00:56:29,220 --> 00:56:34,220
And so like we know there are many genetic abnormalities

1194
00:56:34,640 --> 00:56:37,280
that occur through new mutations each generation.

1195
00:56:37,280 --> 00:56:40,800
And in fact, we have Down syndrome

1196
00:56:40,800 --> 00:56:43,880
is the chromosomal abnormality that you can survive.

1197
00:56:43,920 --> 00:56:47,240
All the others just kill the embryo.

1198
00:56:47,240 --> 00:56:49,000
And so we never see them.

1199
00:56:49,920 --> 00:56:52,360
But like Down syndrome occurs a lot.

1200
00:56:52,360 --> 00:56:55,040
And there are many other lethal mutations

1201
00:56:55,040 --> 00:56:57,840
and as you go to the less damaging ones,

1202
00:56:57,840 --> 00:57:00,960
there are enormous numbers of less damaging mutations

1203
00:57:00,960 --> 00:57:03,720
that are degrading every system in the body.

1204
00:57:03,720 --> 00:57:08,720
And so evolution each generation has to pull away

1205
00:57:09,680 --> 00:57:11,200
at some of this mutational load.

1206
00:57:11,200 --> 00:57:14,080
And the priority with which that mutational load

1207
00:57:14,080 --> 00:57:16,880
is pulled out scales in proportion

1208
00:57:16,880 --> 00:57:19,800
to how much the traits it's affecting impact fitness.

1209
00:57:19,800 --> 00:57:23,680
So you got new mutations that impact your resistance

1210
00:57:23,680 --> 00:57:26,800
to malaria, you got new mutations

1211
00:57:26,800 --> 00:57:28,160
that damage brain function.

1212
00:57:29,520 --> 00:57:34,600
And then have those mutations are purged each generation.

1213
00:57:34,600 --> 00:57:37,320
If malaria is a bigger difference in mortality

1214
00:57:37,320 --> 00:57:39,280
than like the incremental effectiveness

1215
00:57:39,360 --> 00:57:43,240
of hunter-gatherer you get from being slightly more intelligent

1216
00:57:43,240 --> 00:57:47,160
then you'll purge that mutational load first.

1217
00:57:47,160 --> 00:57:49,600
And similarly, if there's like,

1218
00:57:49,600 --> 00:57:53,480
humans have been vigorously adapting to new circumstances.

1219
00:57:53,480 --> 00:57:56,360
So since agriculture, people have been developing things

1220
00:57:56,360 --> 00:58:01,360
like the ability to have amulets to digest breads,

1221
00:58:03,000 --> 00:58:05,720
the ability to like digest milk.

1222
00:58:05,720 --> 00:58:08,760
And if you're evolving for all of these things

1223
00:58:08,760 --> 00:58:11,600
and if some of the things that give an advantage for that

1224
00:58:11,600 --> 00:58:13,760
incidentally carry along nearby them

1225
00:58:13,760 --> 00:58:16,760
some negative effect on another trait

1226
00:58:16,760 --> 00:58:18,560
then that other trait can be damaged.

1227
00:58:18,560 --> 00:58:22,600
So it really matters how important to survival

1228
00:58:22,600 --> 00:58:24,960
and reproduction cognitive abilities were

1229
00:58:24,960 --> 00:58:27,440
compared to everything else that organism has to do.

1230
00:58:27,440 --> 00:58:32,080
And that in particular like surviving, feasting famine,

1231
00:58:32,080 --> 00:58:34,080
having like the physical abilities

1232
00:58:34,080 --> 00:58:35,920
to do hunting and gathering.

1233
00:58:35,920 --> 00:58:37,800
And like, even if you're like very good

1234
00:58:37,800 --> 00:58:39,920
at planning your hunting,

1235
00:58:39,920 --> 00:58:41,880
being able to throw a spear harder

1236
00:58:41,880 --> 00:58:43,320
can be a big difference.

1237
00:58:43,320 --> 00:58:46,040
And that needs energy to build those muscles

1238
00:58:46,040 --> 00:58:47,720
and then to sustain them.

1239
00:58:47,720 --> 00:58:51,520
And so given all of these factors,

1240
00:58:52,400 --> 00:58:55,360
it's like, yeah, it's not a slam dunk

1241
00:58:56,240 --> 00:58:57,480
to invest at the merge.

1242
00:58:57,480 --> 00:59:00,920
And like today, like having bigger brains,

1243
00:59:00,920 --> 00:59:02,800
for example, it's associated

1244
00:59:02,800 --> 00:59:04,720
with like greater cognitive ability,

1245
00:59:04,720 --> 00:59:06,320
but it's like, it's modest.

1246
00:59:07,240 --> 00:59:09,560
Large scale pre-registered studies,

1247
00:59:09,560 --> 00:59:12,280
pre-registered studies with MRI data.

1248
00:59:12,280 --> 00:59:17,280
It's like a range, maybe like a correlation of 0.25, 0.3

1249
00:59:18,240 --> 00:59:22,280
and the standard deviation of brain size is like 10%.

1250
00:59:22,280 --> 00:59:25,760
So if you double the size of the brain,

1251
00:59:25,760 --> 00:59:27,600
so go and the existing brain costs

1252
00:59:27,600 --> 00:59:31,240
like 20% of metabolic energy, go up to 40%.

1253
00:59:31,240 --> 00:59:35,040
Okay, that's like eight standard deviations of brain size.

1254
00:59:35,040 --> 00:59:39,320
If the correlation is like, say it's 0.25,

1255
00:59:40,400 --> 00:59:45,400
then yeah, like you get a gain from that.

1256
00:59:45,800 --> 00:59:48,160
Eight standard deviations of brain size,

1257
00:59:48,160 --> 00:59:50,680
two standard deviations of cognitive ability.

1258
00:59:50,680 --> 00:59:52,720
And like in our modern society

1259
00:59:52,720 --> 00:59:56,040
where cognitive ability is very rewarded

1260
00:59:56,040 --> 01:00:00,920
and like finishing school, becoming an engineer

1261
01:00:00,920 --> 01:00:05,280
or a doctor or whatever can pay off a lot financially.

1262
01:00:05,280 --> 01:00:08,360
Still the like, the average observed return

1263
01:00:09,560 --> 01:00:13,960
in like income is like a one or 2% proportional increase.

1264
01:00:13,960 --> 01:00:15,640
There's more effects of the tail,

1265
01:00:15,640 --> 01:00:18,520
there's more effect in professions like STEM.

1266
01:00:18,520 --> 01:00:20,120
But on the whole, it's not like,

1267
01:00:22,040 --> 01:00:25,880
if it was like a 5% increase or a 10% increase,

1268
01:00:25,880 --> 01:00:28,080
then you could tell a story where,

1269
01:00:28,080 --> 01:00:29,480
yeah, this is hugely increasing

1270
01:00:29,560 --> 01:00:31,080
the amount of food you could have,

1271
01:00:31,080 --> 01:00:32,520
you could support more children,

1272
01:00:32,520 --> 01:00:34,720
but it's like, it's a modest effect

1273
01:00:34,720 --> 01:00:36,360
and the metabolic costs will be large

1274
01:00:36,360 --> 01:00:39,120
and then throw in these other aspects.

1275
01:00:39,120 --> 01:00:41,040
And I think it's, you can tell the story else,

1276
01:00:41,040 --> 01:00:45,480
we can just, we can see there was not very strong,

1277
01:00:45,480 --> 01:00:47,920
rapid directional selection on the thing,

1278
01:00:47,920 --> 01:00:51,560
which there would be if like, you could,

1279
01:00:51,560 --> 01:00:56,560
by solving like a math puzzle, you could defeat malaria.

1280
01:00:57,560 --> 01:01:00,840
Like then there would be more evolutionary pressure.

1281
01:01:00,840 --> 01:01:01,680
That is so interesting.

1282
01:01:01,680 --> 01:01:02,920
And not to mention, of course,

1283
01:01:02,920 --> 01:01:05,080
that yeah, if you had like 2x the brain size

1284
01:01:05,080 --> 01:01:06,720
or you were without C-section,

1285
01:01:06,720 --> 01:01:09,800
you would, you or your mother would or both would die.

1286
01:01:09,800 --> 01:01:11,040
This is a question I've actually been curious about

1287
01:01:11,040 --> 01:01:12,320
for like over a year.

1288
01:01:12,320 --> 01:01:14,440
And I like briefly try to look up an answer.

1289
01:01:14,440 --> 01:01:16,840
This is, I know this is off topic,

1290
01:01:16,840 --> 01:01:18,400
but I apologize to the audience,

1291
01:01:18,400 --> 01:01:20,160
but I was super interested in those like,

1292
01:01:20,160 --> 01:01:21,280
those like the most comprehensive

1293
01:01:21,280 --> 01:01:23,040
and interesting answer I could have hoped for.

1294
01:01:23,040 --> 01:01:25,360
Okay, so yeah, we have a good explanation

1295
01:01:25,560 --> 01:01:27,560
for good first principles, evolutionary reason

1296
01:01:27,560 --> 01:01:30,560
for thinking that intelligence scaling up to humans

1297
01:01:30,560 --> 01:01:35,560
is not implausible just by throwing more scale at it.

1298
01:01:36,280 --> 01:01:37,800
I would also add,

1299
01:01:37,800 --> 01:01:39,560
this was something that would have mattered to me more

1300
01:01:39,560 --> 01:01:41,280
in the 2000s.

1301
01:01:41,280 --> 01:01:44,280
We also have the brain right here with us

1302
01:01:44,280 --> 01:01:45,840
for available for neuroscience

1303
01:01:45,840 --> 01:01:48,040
to reverse engineer its property.

1304
01:01:48,040 --> 01:01:50,680
And so in the 2000s, when I said, yeah,

1305
01:01:50,680 --> 01:01:53,320
I expect this by, you know, middle of the century ish.

1306
01:01:53,320 --> 01:01:54,880
That was a backstop.

1307
01:01:54,880 --> 01:01:58,280
If we found it absurdly difficult to get to the algorithms

1308
01:01:58,280 --> 01:02:00,480
and then we would learn from neuroscience,

1309
01:02:00,480 --> 01:02:04,480
but in the actual history, it's really not like that.

1310
01:02:04,480 --> 01:02:06,080
We develop things in AI.

1311
01:02:06,080 --> 01:02:07,480
And then also we can say, oh yeah,

1312
01:02:07,480 --> 01:02:10,760
this is sort of like this thing in neuroscience

1313
01:02:10,760 --> 01:02:12,160
or maybe this is a good explanation.

1314
01:02:12,160 --> 01:02:14,720
But it's not as though neuroscience

1315
01:02:14,720 --> 01:02:16,000
is driving AI progress.

1316
01:02:16,000 --> 01:02:18,720
It turns out not to be that necessary.

1317
01:02:18,720 --> 01:02:22,240
As similar to, I guess, you know, how planes were inspired

1318
01:02:22,280 --> 01:02:24,200
by the existence proof of birds,

1319
01:02:24,200 --> 01:02:27,440
but jet engines don't flap.

1320
01:02:27,440 --> 01:02:29,560
All right, so yeah, scaling,

1321
01:02:29,560 --> 01:02:32,040
good reason to think scaling might work.

1322
01:02:32,040 --> 01:02:34,360
So we spend $100 billion and we have something

1323
01:02:34,360 --> 01:02:37,760
that is like human level or can do help significantly

1324
01:02:37,760 --> 01:02:39,200
with AI research.

1325
01:02:39,200 --> 01:02:41,960
I mean, that might be on the earlier end,

1326
01:02:41,960 --> 01:02:45,080
but I mean, I definitely would not rule that out

1327
01:02:45,080 --> 01:02:47,400
given the rates of change we've seen

1328
01:02:47,400 --> 01:02:49,240
with the last few scale-ups.

1329
01:02:49,240 --> 01:02:53,520
All right, so at this point, somebody might be skeptical.

1330
01:02:53,520 --> 01:02:54,360
Okay, like listen,

1331
01:02:54,360 --> 01:02:56,040
we already have a bunch of human researchers, right?

1332
01:02:56,040 --> 01:02:58,720
Like the incremental researcher, how profitable is that?

1333
01:02:58,720 --> 01:02:59,640
And then you might say, well, no,

1334
01:02:59,640 --> 01:03:01,480
this is like thousands of researchers.

1335
01:03:01,480 --> 01:03:03,920
I don't know how to express a skepticism exactly,

1336
01:03:03,920 --> 01:03:06,200
but skepticism is skeptical of just generally

1337
01:03:06,200 --> 01:03:09,000
the effect of scaling up the number of people

1338
01:03:09,000 --> 01:03:11,480
working on the problem to rapid,

1339
01:03:11,480 --> 01:03:13,920
rapid progress on that problem.

1340
01:03:13,920 --> 01:03:15,200
Somebody might think, okay, listen,

1341
01:03:15,200 --> 01:03:18,040
with humans, the reason population working on a problem

1342
01:03:18,040 --> 01:03:20,400
is such a good proxy for progress on the problem

1343
01:03:20,400 --> 01:03:22,360
is that there's already so much variation

1344
01:03:22,360 --> 01:03:23,400
that is accounted for when you say

1345
01:03:23,400 --> 01:03:25,240
there's like a million people working on a problem.

1346
01:03:25,240 --> 01:03:28,040
You know, there's like hundreds of super geniuses

1347
01:03:28,040 --> 01:03:29,320
working on it, thousands of people

1348
01:03:29,320 --> 01:03:30,920
who are like very smart working on it.

1349
01:03:30,920 --> 01:03:32,880
Whereas with an AI, all the copies

1350
01:03:32,880 --> 01:03:35,280
are like the same level of intelligence.

1351
01:03:35,280 --> 01:03:37,920
And if it's not super genius intelligence,

1352
01:03:39,240 --> 01:03:43,840
the total quantity might not matter as much.

1353
01:03:43,840 --> 01:03:48,320
Yeah, I'm not sure what your model is here.

1354
01:03:48,320 --> 01:03:53,320
So is this a model that the diminishing returns kick off

1355
01:03:54,840 --> 01:03:57,400
suddenly has a cliff right where we are?

1356
01:03:57,400 --> 01:04:00,960
And so like there was, there were results in the past

1357
01:04:00,960 --> 01:04:04,760
from throwing more people at problems.

1358
01:04:04,760 --> 01:04:08,960
And I mean, this has been useful in historical prediction.

1359
01:04:08,960 --> 01:04:12,400
One of the, there's this idea of experience curves

1360
01:04:12,400 --> 01:04:17,400
and Wright's law basically measuring cumulative production

1361
01:04:17,680 --> 01:04:20,560
in a field or which is that also gonna be a measure

1362
01:04:20,560 --> 01:04:23,240
of like the scale of effort and investment.

1363
01:04:23,240 --> 01:04:26,440
And people have used this correctly to argue

1364
01:04:26,440 --> 01:04:29,640
that renewable energy technology like solar

1365
01:04:29,640 --> 01:04:32,160
would be falling rapidly in price

1366
01:04:32,160 --> 01:04:34,360
because it was going from a low base

1367
01:04:34,360 --> 01:04:36,480
of very small production runs,

1368
01:04:36,480 --> 01:04:39,220
not much investment in doing it efficiently.

1369
01:04:40,060 --> 01:04:44,980
And yeah, climate advocates correctly called out

1370
01:04:44,980 --> 01:04:48,380
people and people like David Roberts,

1371
01:04:48,380 --> 01:04:53,060
the futurist Rama is now actually has some interesting

1372
01:04:53,060 --> 01:04:55,980
writing on this that yeah, correctly called out

1373
01:04:55,980 --> 01:04:59,660
that there would be really drastic fall in prices

1374
01:04:59,660 --> 01:05:01,380
of solar and batteries

1375
01:05:01,380 --> 01:05:04,460
because of the increasing investment going into that.

1376
01:05:04,460 --> 01:05:05,820
The human genome project would be another.

1377
01:05:05,820 --> 01:05:08,960
So I'd say there's like, yeah, real, real evidence.

1378
01:05:08,960 --> 01:05:12,600
These observed correlations from like ideas,

1379
01:05:12,600 --> 01:05:17,600
getting harder to find have held over a fair range of data

1380
01:05:17,720 --> 01:05:20,120
and over quite a lot of time.

1381
01:05:20,120 --> 01:05:25,120
So I'm wondering what's the nature of the deviation

1382
01:05:25,480 --> 01:05:26,320
you're thinking of?

1383
01:05:26,320 --> 01:05:30,720
That we're talking about, maybe this is like a good way

1384
01:05:30,720 --> 01:05:33,660
to describe what happens when more humans enter a field.

1385
01:05:33,660 --> 01:05:35,720
But does it even make sense to say

1386
01:05:35,720 --> 01:05:38,600
like a greater population of AIs is doing AI research

1387
01:05:38,640 --> 01:05:42,560
if there's like more GPUs running a copy of GPT-6

1388
01:05:42,560 --> 01:05:44,120
doing AI research?

1389
01:05:44,120 --> 01:05:47,640
It just like how applicable are these economic models

1390
01:05:47,640 --> 01:05:49,960
of human, the quantity of humans working on a problem

1391
01:05:49,960 --> 01:05:53,480
to the magnitude of AIs working on a problem?

1392
01:05:53,480 --> 01:05:57,120
Yeah, so if you have AIs that are directly automating

1393
01:05:58,280 --> 01:06:01,320
particular jobs that humans were doing before,

1394
01:06:01,320 --> 01:06:03,280
then we say, well, with additional compute

1395
01:06:03,280 --> 01:06:06,080
we can run more copies of them

1396
01:06:06,080 --> 01:06:09,280
to do more of those tasks simultaneously.

1397
01:06:09,280 --> 01:06:11,520
We can also run them at greater speed.

1398
01:06:11,520 --> 01:06:13,560
And so some people have an intuition that like,

1399
01:06:13,560 --> 01:06:16,760
well, you know, what matters is like time.

1400
01:06:16,760 --> 01:06:19,120
It's not how many people working on problem

1401
01:06:19,120 --> 01:06:20,360
at a given point.

1402
01:06:20,360 --> 01:06:23,360
I think that doesn't bear out super well,

1403
01:06:23,360 --> 01:06:26,600
but AI can also be run faster than humans.

1404
01:06:26,600 --> 01:06:31,600
And so if you have a set of AIs that can do the work

1405
01:06:32,240 --> 01:06:35,400
of the individual human researchers

1406
01:06:35,400 --> 01:06:39,040
and run at 10 times or 100 times the speed,

1407
01:06:39,040 --> 01:06:41,800
then we ask, well, could the human research community

1408
01:06:41,800 --> 01:06:43,960
have solved the algorithm problems,

1409
01:06:43,960 --> 01:06:48,960
do things like invent transformers over 100 years

1410
01:06:49,200 --> 01:06:50,520
if we have this?

1411
01:06:50,520 --> 01:06:52,800
We have AIs with a population,

1412
01:06:52,800 --> 01:06:55,080
effective population similar to the humans,

1413
01:06:55,080 --> 01:06:57,320
but running 100 times as fast.

1414
01:06:57,320 --> 01:07:00,640
And so you have to tell a story where no,

1415
01:07:00,640 --> 01:07:04,120
the AI, they can't really do the same things

1416
01:07:04,160 --> 01:07:05,800
as the humans.

1417
01:07:05,800 --> 01:07:07,880
And we're talking about what happens

1418
01:07:07,880 --> 01:07:12,000
when the AIs are more capable of in fact doing that.

1419
01:07:12,000 --> 01:07:13,200
Although they become more capable

1420
01:07:13,200 --> 01:07:16,400
as lesser capable versions of themselves help us,

1421
01:07:16,400 --> 01:07:17,640
make themselves more capable, right?

1422
01:07:17,640 --> 01:07:20,520
So you have to like kickstart that at some point.

1423
01:07:20,520 --> 01:07:25,480
Is there an example in analogous situations,

1424
01:07:25,480 --> 01:07:28,080
is intelligence unique in the sense that you have

1425
01:07:28,080 --> 01:07:31,520
a feedback loop of with a learning curve

1426
01:07:31,520 --> 01:07:35,320
or something else, a system outputs,

1427
01:07:35,320 --> 01:07:38,160
are feeding into its own inputs in a way that,

1428
01:07:38,160 --> 01:07:40,000
because if we're talking about something like Moore's Law

1429
01:07:40,000 --> 01:07:42,680
or the cost of solar, you do have this way,

1430
01:07:42,680 --> 01:07:44,640
like we're, you know, more people are,

1431
01:07:44,640 --> 01:07:45,720
we're throwing more people with the problem

1432
01:07:45,720 --> 01:07:48,920
and it's, we're, you know, we're making a lot of progress,

1433
01:07:48,920 --> 01:07:53,040
but we don't have the sort of additional part of the model

1434
01:07:53,040 --> 01:07:56,200
where Moore's Law leads to more humans somehow

1435
01:07:56,200 --> 01:07:58,480
and the more humans are becoming researchers.

1436
01:07:58,480 --> 01:08:00,680
So you do actually have a version of that

1437
01:08:00,720 --> 01:08:02,200
in the case of solar.

1438
01:08:02,200 --> 01:08:05,280
So you have a small infant industry

1439
01:08:05,280 --> 01:08:07,560
that's doing things like providing solar panels

1440
01:08:07,560 --> 01:08:11,080
for space satellites and then getting increasing amounts

1441
01:08:11,080 --> 01:08:15,120
of subsidized government demand because of, you know,

1442
01:08:15,120 --> 01:08:16,720
worries about fossil fuel depletion

1443
01:08:16,720 --> 01:08:18,440
and then climate change.

1444
01:08:18,440 --> 01:08:22,760
You can have the dynamic where visible successes

1445
01:08:22,760 --> 01:08:24,880
with solar or like lowering prices

1446
01:08:24,880 --> 01:08:26,760
then open up new markets.

1447
01:08:26,760 --> 01:08:29,520
So there's a particularly huge transition

1448
01:08:29,520 --> 01:08:31,560
where renewables become cheap enough

1449
01:08:31,560 --> 01:08:34,800
to replace large chunks of the electric grid.

1450
01:08:34,800 --> 01:08:38,080
Earlier, you're dealing with very niche situations like,

1451
01:08:38,080 --> 01:08:41,480
yeah, so the satellites where you have very difficult

1452
01:08:41,480 --> 01:08:45,200
to refuel a satellite in place and then remote areas

1453
01:08:45,200 --> 01:08:47,160
and then moving to like, you know,

1454
01:08:47,160 --> 01:08:50,000
the super sunny, the sunniest areas in the world

1455
01:08:50,000 --> 01:08:52,520
with the biggest solar subsidies.

1456
01:08:52,520 --> 01:08:54,440
And so there was an element of that

1457
01:08:54,440 --> 01:08:57,560
where more and more investment has been thrown

1458
01:08:57,600 --> 01:09:00,600
into the field and like the market has rapidly expanded

1459
01:09:00,600 --> 01:09:02,080
as the technology improved.

1460
01:09:02,080 --> 01:09:05,520
But I think the closest analogy is actually

1461
01:09:05,520 --> 01:09:08,120
the long run growth of human civilization itself.

1462
01:09:08,120 --> 01:09:11,000
And I know you had Holden Karnofsky

1463
01:09:11,000 --> 01:09:13,960
from the open philanthropy project on earlier

1464
01:09:13,960 --> 01:09:16,040
and discuss some of this research

1465
01:09:16,040 --> 01:09:20,640
about the long run acceleration of human population

1466
01:09:20,640 --> 01:09:21,600
and economic growth.

1467
01:09:21,600 --> 01:09:24,280
And so developing new technologies

1468
01:09:24,280 --> 01:09:27,080
allowed human population to expand,

1469
01:09:27,120 --> 01:09:31,600
humans to occupy new habitats and new areas

1470
01:09:31,600 --> 01:09:32,960
and then to invent agriculture

1471
01:09:32,960 --> 01:09:35,040
which support the larger populations

1472
01:09:35,040 --> 01:09:36,520
and then even more advanced agriculture

1473
01:09:36,520 --> 01:09:38,720
in the modern industrial society.

1474
01:09:38,720 --> 01:09:42,200
And so their total technology and output

1475
01:09:42,200 --> 01:09:45,200
allowed you to support more humans

1476
01:09:45,200 --> 01:09:48,000
who then would discover more technology

1477
01:09:48,000 --> 01:09:49,080
and continue the process.

1478
01:09:49,080 --> 01:09:52,880
Now that was boosted because on top of expanding

1479
01:09:52,880 --> 01:09:56,120
the population, the share of human activity

1480
01:09:56,120 --> 01:09:59,280
that was going into invention and innovation went up.

1481
01:09:59,280 --> 01:10:01,320
And that was a key part of the industrial revolution.

1482
01:10:01,320 --> 01:10:04,520
There was no such thing as a corporate research lab

1483
01:10:04,520 --> 01:10:08,640
or like an engineering university prior to that.

1484
01:10:08,640 --> 01:10:11,600
And so you're both increasing the total human population

1485
01:10:11,600 --> 01:10:13,280
and the share of it going in.

1486
01:10:13,280 --> 01:10:16,440
But this population dynamic is pretty analogous.

1487
01:10:16,440 --> 01:10:19,000
Humans invent farming, they can have more humans

1488
01:10:19,000 --> 01:10:21,920
than they can invent industry and so on.

1489
01:10:21,920 --> 01:10:23,200
So maybe somebody would be skeptical

1490
01:10:23,200 --> 01:10:25,920
that with AI progress specifically,

1491
01:10:25,960 --> 01:10:30,600
it's not just a matter of some farmer

1492
01:10:30,600 --> 01:10:33,480
figuring out crop rotation or some blacksmith

1493
01:10:33,480 --> 01:10:35,760
figuring out how to do metal or do better.

1494
01:10:35,760 --> 01:10:38,120
You in fact, even to make the,

1495
01:10:38,120 --> 01:10:40,360
for the 50% improvement in productivity,

1496
01:10:40,360 --> 01:10:42,520
you basically need something on the IQ

1497
01:10:42,520 --> 01:10:44,360
that's close to Ilya Setskoper.

1498
01:10:44,360 --> 01:10:46,800
There's like a discontinuous,

1499
01:10:46,800 --> 01:10:48,840
you're like contributing very little to productivity

1500
01:10:48,840 --> 01:10:51,520
and then you're like Ilya and then you contribute a lot.

1501
01:10:51,520 --> 01:10:54,840
But the becoming Ilya is, you see what I'm saying?

1502
01:10:54,840 --> 01:10:56,880
There's not like a gradual increase in capabilities

1503
01:10:56,880 --> 01:10:57,720
that leads to the feedback.

1504
01:10:57,720 --> 01:11:02,240
You're imagining a case where the distribution of tasks

1505
01:11:02,240 --> 01:11:04,960
is such that there's nothing that you can,

1506
01:11:04,960 --> 01:11:08,800
where individually automating it particularly helps.

1507
01:11:08,800 --> 01:11:12,280
And so the ability to contribute to AI research

1508
01:11:12,280 --> 01:11:13,360
is really end loaded.

1509
01:11:13,360 --> 01:11:14,200
Is that what you're saying?

1510
01:11:14,200 --> 01:11:17,360
Yeah, I mean, we already see this in these sorts

1511
01:11:17,360 --> 01:11:20,960
of like really high IQ companies or projects

1512
01:11:20,960 --> 01:11:22,600
where theoretically, I guess,

1513
01:11:22,600 --> 01:11:25,520
Shane Street or OpenAI could hire like a bunch of,

1514
01:11:26,480 --> 01:11:29,480
mediocre people to do, there's a comparative advantage.

1515
01:11:29,480 --> 01:11:31,040
They could do some menial tasks

1516
01:11:31,040 --> 01:11:33,880
and that could free up the time of the really smart people,

1517
01:11:33,880 --> 01:11:35,960
but they don't do that, right?

1518
01:11:35,960 --> 01:11:37,400
Transaction costs, whatever else.

1519
01:11:37,400 --> 01:11:39,680
Self-driven cars would be another example

1520
01:11:39,680 --> 01:11:42,400
where you have a very high quality threshold.

1521
01:11:42,400 --> 01:11:45,160
And so when your performance as a driver

1522
01:11:45,160 --> 01:11:46,840
is worse than a human,

1523
01:11:46,840 --> 01:11:48,960
like you have 10 times the accident rate

1524
01:11:48,960 --> 01:11:50,760
or 100 times the accident rate,

1525
01:11:50,760 --> 01:11:52,920
then the cost of insurance for that,

1526
01:11:52,920 --> 01:11:54,680
which is a proxy for people's willingness

1527
01:11:54,680 --> 01:11:56,760
to ride the car instead of two,

1528
01:11:56,760 --> 01:11:58,440
would be such that the insurance costs

1529
01:11:58,440 --> 01:11:59,360
would absolutely dominate.

1530
01:11:59,360 --> 01:12:00,800
So even if you have zero labor cost,

1531
01:12:00,800 --> 01:12:03,120
it's offset by the increased insurance cost.

1532
01:12:03,120 --> 01:12:04,800
And so there are lots of cases like that

1533
01:12:04,800 --> 01:12:09,800
where like partial automation is not in practice

1534
01:12:10,480 --> 01:12:14,760
very usable because complimenting other resources,

1535
01:12:14,760 --> 01:12:17,720
you're gonna use those other resources less efficiently.

1536
01:12:18,560 --> 01:12:22,280
And in a post-AGI future,

1537
01:12:22,280 --> 01:12:24,600
I mean, the same thing can apply to humans.

1538
01:12:24,600 --> 01:12:28,560
So people can say, well, comparative advantage,

1539
01:12:28,560 --> 01:12:31,920
even if AIs can do everything better than a human,

1540
01:12:31,920 --> 01:12:34,280
well, it's still worth something.

1541
01:12:34,280 --> 01:12:35,360
The human can do something,

1542
01:12:35,360 --> 01:12:38,600
they can lift a box, that's something.

1543
01:12:39,520 --> 01:12:41,200
Now there's a question of property rights,

1544
01:12:41,200 --> 01:12:44,720
if, well, if they could just slice up the human

1545
01:12:44,720 --> 01:12:46,680
to make more robots.

1546
01:12:46,680 --> 01:12:49,880
But even absent that in such an economy,

1547
01:12:49,880 --> 01:12:52,160
you wouldn't want to let a human worker

1548
01:12:52,160 --> 01:12:53,720
into any industrial environment,

1549
01:12:53,720 --> 01:12:55,160
because in a clean room,

1550
01:12:55,160 --> 01:12:57,480
they'll be emitting all kinds of skin cells

1551
01:12:57,480 --> 01:12:59,260
and messing things up.

1552
01:12:59,260 --> 01:13:00,920
You need to have an atmosphere there.

1553
01:13:00,920 --> 01:13:03,240
You need a bunch of supporting tools

1554
01:13:03,240 --> 01:13:04,680
and resources and materials.

1555
01:13:04,680 --> 01:13:07,540
And those supporting resources and materials

1556
01:13:07,540 --> 01:13:10,320
will do a lot more productively,

1557
01:13:10,320 --> 01:13:12,640
working with AI and robots rather than a human.

1558
01:13:12,640 --> 01:13:15,680
So you don't wanna let a human anywhere near the thing,

1559
01:13:15,680 --> 01:13:18,520
just like in a, you don't wanna have a gorilla

1560
01:13:18,520 --> 01:13:19,880
wandering around in a China shop.

1561
01:13:19,880 --> 01:13:22,600
Even if you've trained it to most of the time,

1562
01:13:22,600 --> 01:13:24,800
pick up a box for you if you give it a banana,

1563
01:13:24,800 --> 01:13:26,640
it's just not worth it to have it wandering

1564
01:13:26,640 --> 01:13:27,480
around your China shop.

1565
01:13:27,480 --> 01:13:28,320
Yeah, yeah, yeah.

1566
01:13:28,320 --> 01:13:30,560
Like why is that not a good objection to?

1567
01:13:30,560 --> 01:13:34,040
I mean, I think that is one of the ways

1568
01:13:34,040 --> 01:13:37,760
in which partial automation can fail

1569
01:13:37,760 --> 01:13:41,000
to really translate into a lot of economic value.

1570
01:13:41,000 --> 01:13:43,720
That's something that will attenuate as we go on.

1571
01:13:43,800 --> 01:13:47,000
And as the AI is more able to work independently

1572
01:13:47,000 --> 01:13:50,200
and more able to handle its own,

1573
01:13:50,200 --> 01:13:53,320
its own screw ups, get more reliable.

1574
01:13:53,320 --> 01:13:55,440
But the way in which it becomes more reliable

1575
01:13:55,440 --> 01:13:58,440
is by AI progress speeding up,

1576
01:13:58,440 --> 01:14:00,960
which happens if AI can contribute to it.

1577
01:14:00,960 --> 01:14:04,880
But if there is some sort of reliability bottleneck,

1578
01:14:04,880 --> 01:14:06,520
the principle of contributing to that progress,

1579
01:14:06,520 --> 01:14:07,600
then you don't have the loop, right?

1580
01:14:07,600 --> 01:14:10,960
So, yeah, I mean, this is why we're not there yet.

1581
01:14:10,960 --> 01:14:13,080
Right, but then what is the reason to think we'll be there at?

1582
01:14:13,080 --> 01:14:16,800
The broad reason is we have these inputs are scaling up.

1583
01:14:18,880 --> 01:14:21,560
There's a, so Epoch, which I mentioned earlier,

1584
01:14:21,560 --> 01:14:24,040
they have a paper, I think it's called compute trends

1585
01:14:24,040 --> 01:14:27,480
in three areas of machine learning or something like that.

1586
01:14:27,480 --> 01:14:31,800
And so they look at the compute expended

1587
01:14:31,800 --> 01:14:33,920
on machine learning systems

1588
01:14:33,920 --> 01:14:36,040
since the founding of the field of AI

1589
01:14:36,040 --> 01:14:38,480
at the beginning of the 1950s.

1590
01:14:38,480 --> 01:14:41,120
And so it mostly, it grows with Moore's law.

1591
01:14:42,120 --> 01:14:44,920
And so people are spending a similar amount

1592
01:14:44,920 --> 01:14:49,080
on their experiments, but they can just buy Moore with that

1593
01:14:49,080 --> 01:14:51,400
because the compute is coming.

1594
01:14:51,400 --> 01:14:55,640
And so that data, I mean, it covers over 20 orders

1595
01:14:55,640 --> 01:14:57,320
of magnitude, maybe like 24.

1596
01:14:58,960 --> 01:15:03,920
And of all of those increases since 1952,

1597
01:15:03,920 --> 01:15:08,920
a little more than half of them happened between 1952 and 2010.

1598
01:15:09,080 --> 01:15:12,200
And all the rest is since 2010.

1599
01:15:12,200 --> 01:15:16,520
So we've been scaling that up like four times as fast

1600
01:15:16,520 --> 01:15:20,120
as was the case for most of the history of AI.

1601
01:15:20,120 --> 01:15:23,640
We're running through the orders of magnitude

1602
01:15:23,640 --> 01:15:27,280
of possible resource inputs you could need for AI

1603
01:15:27,280 --> 01:15:29,720
much, much more quickly than we were

1604
01:15:29,720 --> 01:15:30,840
for most of the history of AI.

1605
01:15:30,840 --> 01:15:33,000
That's why this is a period of like

1606
01:15:33,000 --> 01:15:36,760
with a very elevated chance of AI per year

1607
01:15:36,760 --> 01:15:38,720
because we're moving through.

1608
01:15:38,720 --> 01:15:40,680
So much of the space of inputs per year.

1609
01:15:40,680 --> 01:15:43,640
And indeed, it looks like this scale up

1610
01:15:43,640 --> 01:15:48,280
taken to its conclusion will cover another bunch

1611
01:15:48,280 --> 01:15:50,240
of orders of magnitude.

1612
01:15:50,240 --> 01:15:53,720
And that's actually a large fraction of those that are left

1613
01:15:53,720 --> 01:15:55,680
before you start running into saying, well,

1614
01:15:55,680 --> 01:15:58,800
this is gonna have to be like evolution

1615
01:15:58,800 --> 01:16:01,360
with the sort of simple hacks we get to apply.

1616
01:16:01,360 --> 01:16:04,200
Like we're selecting for intelligence the whole time.

1617
01:16:04,200 --> 01:16:06,600
We're not going to do the same mutation

1618
01:16:06,600 --> 01:16:10,680
that causes fatal childhood cancer a billion times.

1619
01:16:10,680 --> 01:16:14,240
Even though, I mean, we keep getting the same fatal mutations

1620
01:16:14,240 --> 01:16:15,880
even though they've been done many times.

1621
01:16:15,880 --> 01:16:18,840
We use gradient descent, which takes into account

1622
01:16:18,840 --> 01:16:21,880
the derivative of improvement on the loss

1623
01:16:21,880 --> 01:16:23,120
all throughout the network.

1624
01:16:23,120 --> 01:16:26,520
And we don't throw away all the contents of the network

1625
01:16:26,520 --> 01:16:29,240
with each generation where you can press down

1626
01:16:29,240 --> 01:16:30,640
to a little DNA.

1627
01:16:30,640 --> 01:16:33,200
So there's that bar of like, well,

1628
01:16:33,200 --> 01:16:35,440
if you're gonna do brute force like evolution

1629
01:16:35,440 --> 01:16:37,680
combine with these sort of very simple ways

1630
01:16:37,680 --> 01:16:39,600
we can save orders of magnitude on that.

1631
01:16:41,200 --> 01:16:43,840
We're gonna cover, I think, a fraction

1632
01:16:43,840 --> 01:16:47,520
that's like half of that distance in this scale

1633
01:16:47,520 --> 01:16:49,240
up over the next 10 years or so.

1634
01:16:49,240 --> 01:16:53,240
And so if you started off with a kind of vague uniform prior,

1635
01:16:53,240 --> 01:16:56,800
you're like, well, you probably can't make AGI

1636
01:16:56,800 --> 01:16:59,520
with like the amount of compute that would be involved

1637
01:16:59,520 --> 01:17:02,080
in a fruit fly existing for a minute,

1638
01:17:02,080 --> 01:17:04,000
which would be the early days of AI.

1639
01:17:04,960 --> 01:17:06,440
You know, maybe you would get lucky.

1640
01:17:06,440 --> 01:17:07,960
We were able to make calculators

1641
01:17:07,960 --> 01:17:10,040
because calculators benefited

1642
01:17:10,040 --> 01:17:14,080
from like very reliable, serially fast computers

1643
01:17:14,080 --> 01:17:17,600
and where we could take a tiny, tiny, tiny, tiny fraction

1644
01:17:17,600 --> 01:17:20,360
of a human brain's compute and use it for a calculator.

1645
01:17:20,360 --> 01:17:22,920
We couldn't take an ant's brain and rewire it to calculate.

1646
01:17:22,920 --> 01:17:27,600
It's hard to manage ant farms, let alone get them

1647
01:17:27,600 --> 01:17:29,320
to do arithmetic for you.

1648
01:17:29,320 --> 01:17:32,160
And so there were some things where we could exploit

1649
01:17:32,160 --> 01:17:37,160
the differences between biological brains and computers

1650
01:17:37,160 --> 01:17:40,040
to do stuff super efficiently on computers.

1651
01:17:40,040 --> 01:17:42,400
We would doubt that we would be able

1652
01:17:42,400 --> 01:17:45,280
to do so much better than biology,

1653
01:17:45,280 --> 01:17:48,560
that with a tiny fraction of an insect's brain,

1654
01:17:48,560 --> 01:17:50,400
we'd be able to get AI early on.

1655
01:17:50,400 --> 01:17:53,520
On the far end, it seemed very implausible

1656
01:17:53,520 --> 01:17:54,440
that we couldn't do better

1657
01:17:54,440 --> 01:17:56,560
than completely brute force evolution.

1658
01:17:56,560 --> 01:17:58,320
And so in between, you have some number

1659
01:17:58,360 --> 01:18:02,520
of orders of magnitude of inputs where it might be.

1660
01:18:02,520 --> 01:18:03,920
And like in the 2000s, I would say,

1661
01:18:03,920 --> 01:18:07,120
well, you know, I'm gonna have a pretty uniformish prior.

1662
01:18:07,120 --> 01:18:10,080
I'm gonna put weight on it happening at like the sort

1663
01:18:10,080 --> 01:18:14,040
of the equivalent of like 10 to the 25 ops,

1664
01:18:14,040 --> 01:18:16,480
10 to the 30, 10 to the 35,

1665
01:18:17,520 --> 01:18:19,280
and sort of spreading out over that.

1666
01:18:19,280 --> 01:18:21,320
And then I can update on other information.

1667
01:18:21,320 --> 01:18:25,080
And in the short term, I would say like in 2005, I would say,

1668
01:18:25,080 --> 01:18:27,320
well, I don't see anything that looks

1669
01:18:27,320 --> 01:18:29,000
like the cusp of AGI.

1670
01:18:29,000 --> 01:18:31,040
So I'm also gonna lower my credence

1671
01:18:31,040 --> 01:18:34,000
for like the next five years or the next 10 years.

1672
01:18:34,000 --> 01:18:36,640
And so that would be kind of like a vague prior.

1673
01:18:36,640 --> 01:18:38,720
And then when we take into account like, well,

1674
01:18:38,720 --> 01:18:40,120
how quickly are we running through

1675
01:18:40,120 --> 01:18:41,320
those orders of magnitude?

1676
01:18:41,320 --> 01:18:45,520
If I have a uniform prior, I assign half of my weight

1677
01:18:45,520 --> 01:18:48,400
to the first half of remaining orders of magnitude.

1678
01:18:48,400 --> 01:18:50,200
And if we're gonna run through those

1679
01:18:50,200 --> 01:18:51,840
over the next 10 years in some,

1680
01:18:52,840 --> 01:18:56,640
then that calls on me to put half of my credence

1681
01:18:56,640 --> 01:18:58,160
conditional on wherever we're gonna make it.

1682
01:18:58,160 --> 01:19:00,840
AI, which seems likely it's a material object

1683
01:19:00,840 --> 01:19:02,440
easier than evolution.

1684
01:19:02,440 --> 01:19:05,400
I've got to put similarly a lot of my credence

1685
01:19:05,400 --> 01:19:07,320
on AI happening in this scale up.

1686
01:19:07,320 --> 01:19:09,680
And then that's supported by what we're seeing

1687
01:19:09,680 --> 01:19:14,040
in terms of the rapid advances in capabilities

1688
01:19:14,040 --> 01:19:16,280
with AI and LLOMs in particular.

1689
01:19:16,280 --> 01:19:18,480
Okay, that's actually a really interesting point.

1690
01:19:18,480 --> 01:19:20,560
So now that somebody might say, listen,

1691
01:19:20,560 --> 01:19:23,920
there's not some sense in which AIs could universally

1692
01:19:24,000 --> 01:19:27,280
speed up the progress of open AI by 50%

1693
01:19:27,280 --> 01:19:29,480
or 100% or 200%.

1694
01:19:29,480 --> 01:19:32,280
If they're not able to do everything better

1695
01:19:32,280 --> 01:19:34,240
than Ilias Escobar can,

1696
01:19:34,240 --> 01:19:37,040
there's going to be something in which we're bottlenecked

1697
01:19:37,040 --> 01:19:38,840
by the human researchers.

1698
01:19:38,840 --> 01:19:42,080
And bottleneck effects dictate that,

1699
01:19:42,080 --> 01:19:43,600
the slowest moving part of the organization

1700
01:19:43,600 --> 01:19:45,880
will be the one that kind of determines the speed

1701
01:19:45,880 --> 01:19:47,400
of the progress of the whole organization

1702
01:19:47,400 --> 01:19:48,680
or the whole project.

1703
01:19:48,680 --> 01:19:50,360
Which means that unless you get to the point

1704
01:19:50,360 --> 01:19:51,560
where you're like doing everything

1705
01:19:51,560 --> 01:19:53,520
and everybody in the organization can do,

1706
01:19:53,520 --> 01:19:55,480
you're not going to significantly speed up

1707
01:19:55,480 --> 01:19:57,640
the progress of the whole project as a whole.

1708
01:19:57,640 --> 01:20:00,320
Yeah, so that is a hypothesis.

1709
01:20:00,320 --> 01:20:02,760
And I think there's a lot of truth to it.

1710
01:20:02,760 --> 01:20:04,200
So when we think about like the ways

1711
01:20:04,200 --> 01:20:05,960
in which AI can contribute.

1712
01:20:05,960 --> 01:20:07,520
So there are things we talked about before,

1713
01:20:07,520 --> 01:20:10,480
like the AIs setting up their own curriculum.

1714
01:20:11,320 --> 01:20:14,320
And that's something that Ilya can't do directly

1715
01:20:14,320 --> 01:20:15,680
doesn't do directly.

1716
01:20:15,680 --> 01:20:16,680
And there's a question,

1717
01:20:16,680 --> 01:20:19,800
how much does that improve performance?

1718
01:20:19,800 --> 01:20:24,280
There are these things where the AI helps

1719
01:20:24,280 --> 01:20:27,360
to just like produce some code for some tasks.

1720
01:20:27,360 --> 01:20:29,920
And it's beyond Hello World at this point.

1721
01:20:29,920 --> 01:20:32,160
But I mean, the sort of thing that I hear

1722
01:20:32,160 --> 01:20:36,160
from AI researchers at leading labs is that,

1723
01:20:36,160 --> 01:20:39,680
on their core job where they're like most expert,

1724
01:20:39,680 --> 01:20:41,280
it's not helping them that much,

1725
01:20:41,280 --> 01:20:43,680
but then their job often does involve,

1726
01:20:43,680 --> 01:20:45,600
oh, I've got to code something

1727
01:20:45,600 --> 01:20:49,040
that's out of my usual area of expertise.

1728
01:20:49,040 --> 01:20:52,040
Or I want to research this question and it helps them there.

1729
01:20:52,040 --> 01:20:54,120
And so that saves some of their time

1730
01:20:54,120 --> 01:20:57,600
and frees them to do more of the bottleneck work.

1731
01:20:57,600 --> 01:21:00,840
And then I think the idea of, well,

1732
01:21:02,320 --> 01:21:05,800
is everything dependent on Ilya and is Ilya

1733
01:21:05,800 --> 01:21:10,280
so much better than the hundreds of other employees?

1734
01:21:10,280 --> 01:21:12,440
I think a lot of people who are contributing,

1735
01:21:12,440 --> 01:21:14,760
they're doing a lot of tasks.

1736
01:21:14,760 --> 01:21:19,280
And so you can have quite a lot of gain

1737
01:21:19,280 --> 01:21:21,400
from automating some areas

1738
01:21:21,400 --> 01:21:24,600
where you then do just an absolutely enormous amount of it

1739
01:21:24,600 --> 01:21:26,800
relative to what you would have done before

1740
01:21:26,800 --> 01:21:29,720
because things like designing the custom curriculum,

1741
01:21:29,720 --> 01:21:32,920
you're like, maybe had some humans put some work into that,

1742
01:21:32,920 --> 01:21:35,440
but you're not going to employ billions of humans

1743
01:21:35,440 --> 01:21:36,760
to produce it at scale.

1744
01:21:36,760 --> 01:21:40,120
And so it winds up being a larger share of the progress

1745
01:21:41,160 --> 01:21:42,160
than it was before.

1746
01:21:42,160 --> 01:21:45,160
You get some benefit from these sorts of things

1747
01:21:45,160 --> 01:21:48,760
where, yeah, there's like pieces of my job

1748
01:21:49,800 --> 01:21:51,800
that now I can hand off to the AI

1749
01:21:51,800 --> 01:21:54,320
and let's me focus more on the things

1750
01:21:54,320 --> 01:21:55,920
that the AI still can't do.

1751
01:21:57,120 --> 01:22:01,720
And then at the later on, you get to the point where,

1752
01:22:01,720 --> 01:22:04,240
yeah, the AI can do your job,

1753
01:22:04,240 --> 01:22:05,720
including the most difficult parts.

1754
01:22:05,720 --> 01:22:08,640
And maybe it has to do that in a different way.

1755
01:22:08,640 --> 01:22:12,480
Maybe it like spends a ton more time thinking about

1756
01:22:12,480 --> 01:22:15,720
each step of a problem than you, and that's the late end.

1757
01:22:15,720 --> 01:22:18,440
And the stronger these bottlenecks effects are,

1758
01:22:18,440 --> 01:22:22,920
the more the economic returns, the scientific returns

1759
01:22:22,920 --> 01:22:27,000
and such are end loaded towards getting sort of full AGI.

1760
01:22:27,000 --> 01:22:28,680
The weaker the bottlenecks are,

1761
01:22:28,680 --> 01:22:32,600
the more interim results will be really paying off.

1762
01:22:32,600 --> 01:22:34,440
I guess I'd probably disagree with you on how much

1763
01:22:34,440 --> 01:22:38,160
the sort of the alias of organizations seem to matter.

1764
01:22:38,160 --> 01:22:39,600
I guess just from the evidence alone,

1765
01:22:39,600 --> 01:22:42,440
like how many of the big sort of breakthroughs

1766
01:22:42,440 --> 01:22:45,360
that in deep learning in general was like

1767
01:22:45,360 --> 01:22:47,760
that single individual responsible for, right?

1768
01:22:47,760 --> 01:22:49,880
And how much of his time is he spending

1769
01:22:49,880 --> 01:22:51,640
doing anything that's not that like co-pilot

1770
01:22:51,640 --> 01:22:52,480
is helping him on?

1771
01:22:52,480 --> 01:22:54,120
I'm guessing like most of it is just like managing people

1772
01:22:54,120 --> 01:22:57,280
and coming up with ideas and, you know,

1773
01:22:57,280 --> 01:22:59,440
trying to like understand systems and so on.

1774
01:22:59,440 --> 01:23:02,840
And if that is the, if like the five or 10 people

1775
01:23:02,840 --> 01:23:06,120
who are like that at OpenAI or Anthropa or whatever

1776
01:23:06,120 --> 01:23:11,080
are basically the way in which the progress is happening

1777
01:23:11,080 --> 01:23:13,440
or at least the algorithmic progress is happening,

1778
01:23:13,440 --> 01:23:17,960
then how much of better and better co-pilot,

1779
01:23:17,960 --> 01:23:19,960
I know co-pilot is not the thing you're talking about

1780
01:23:19,960 --> 01:23:22,280
with like the 20% automation, but something like that,

1781
01:23:22,280 --> 01:23:25,400
how much of, yeah, how much is that contributing

1782
01:23:25,400 --> 01:23:29,600
to the sort of like core function of the research scientist?

1783
01:23:29,600 --> 01:23:30,440
Yeah.

1784
01:23:30,440 --> 01:23:33,800
Naturally quantitatively, how much we disagree

1785
01:23:33,800 --> 01:23:38,800
about the importance of sort of key research employees

1786
01:23:40,160 --> 01:23:44,320
and such, I certainly think that some researchers,

1787
01:23:44,320 --> 01:23:47,680
you know, add, you know, more than 10 times

1788
01:23:47,680 --> 01:23:50,280
the average employee even much more.

1789
01:23:50,280 --> 01:23:54,000
And obviously managers can add an enormous amount of value

1790
01:23:54,000 --> 01:23:56,320
by proportionately multiplying the output

1791
01:23:56,320 --> 01:23:58,720
of the many people that they manage.

1792
01:23:59,680 --> 01:24:02,480
And so that's the kind of thing

1793
01:24:02,480 --> 01:24:05,120
that we were discussing earlier when talking about,

1794
01:24:05,120 --> 01:24:09,840
well, if you had sort of full human level AI

1795
01:24:09,840 --> 01:24:12,880
or AI that had all of the human capabilities

1796
01:24:12,880 --> 01:24:16,600
plus AI advantages, it would be, you know,

1797
01:24:16,600 --> 01:24:19,160
you'd benchmark not off of what the sort of typical

1798
01:24:19,160 --> 01:24:22,800
human performance is, but peak human performance and beyond.

1799
01:24:22,800 --> 01:24:24,980
So yeah, I accept all that.

1800
01:24:26,120 --> 01:24:31,080
I do think it makes a big difference for people

1801
01:24:31,240 --> 01:24:34,400
how much they can outsource a lot of the death

1802
01:24:34,400 --> 01:24:36,480
that are less, wow, less creative.

1803
01:24:36,480 --> 01:24:39,960
And an enormous amount is learned by experimentation.

1804
01:24:39,960 --> 01:24:44,520
ML has been, you know, a quite experimental field.

1805
01:24:44,520 --> 01:24:46,280
And there's a lot of engineering work

1806
01:24:46,280 --> 01:24:49,560
in say building large super clusters,

1807
01:24:49,560 --> 01:24:53,680
making, yeah, hardware aware optimization

1808
01:24:53,680 --> 01:24:55,120
and coding of these things,

1809
01:24:55,120 --> 01:24:59,000
being able to do the parallelism in large models.

1810
01:24:59,000 --> 01:25:03,360
And the engineers are busy

1811
01:25:03,360 --> 01:25:07,640
and it's not just only a big thoughts kind of area.

1812
01:25:07,640 --> 01:25:12,640
And then the other branch is where will the AI advantages

1813
01:25:15,360 --> 01:25:16,960
and disadvantages be?

1814
01:25:16,960 --> 01:25:21,960
And so one AI advantage is being omnidisciplinary

1815
01:25:22,920 --> 01:25:25,440
and familiar with the newest things.

1816
01:25:25,440 --> 01:25:28,040
So I mentioned before, there's no human

1817
01:25:28,040 --> 01:25:31,680
who has a million years of TensorFlow experience.

1818
01:25:31,680 --> 01:25:34,060
And so to the extent that we're interested

1819
01:25:34,060 --> 01:25:36,160
in like the very, very cutting edge

1820
01:25:36,160 --> 01:25:38,800
of things that have been developed quite recently

1821
01:25:38,800 --> 01:25:41,680
than AI that can learn about them in parallel

1822
01:25:41,680 --> 01:25:44,280
and experiment and practice with them in parallel

1823
01:25:44,280 --> 01:25:47,080
can learn much faster than a human potentially.

1824
01:25:48,440 --> 01:25:51,200
And the area of computer science is one

1825
01:25:51,200 --> 01:25:54,600
that is especially suitable for AI

1826
01:25:54,600 --> 01:25:56,400
to learn in a digital environment.

1827
01:25:56,400 --> 01:26:00,600
So it doesn't require like driving a car around

1828
01:26:00,600 --> 01:26:03,000
that might kill someone, have enormous costs.

1829
01:26:03,000 --> 01:26:08,000
You can do unit tests, you can prove theorems,

1830
01:26:09,000 --> 01:26:10,920
you can do all sorts of operations

1831
01:26:10,920 --> 01:26:14,080
entirely in the confines of a computer.

1832
01:26:14,080 --> 01:26:17,040
And which is one reason why programming

1833
01:26:17,040 --> 01:26:20,080
has been benefiting more than a lot of other areas

1834
01:26:20,080 --> 01:26:23,640
from LLMs recently, whereas robotics is lagging.

1835
01:26:23,640 --> 01:26:24,680
So the sum of that.

1836
01:26:24,680 --> 01:26:27,560
And then just considering, well,

1837
01:26:27,560 --> 01:26:29,560
actually I mean, they are getting better

1838
01:26:29,560 --> 01:26:34,560
at things like the GRE math at programming contests.

1839
01:26:35,400 --> 01:26:39,040
And I mean, some people have forecasts

1840
01:26:39,040 --> 01:26:41,760
and predictions outstanding about things like

1841
01:26:41,760 --> 01:26:45,120
doing well on the Informatics Olympiad

1842
01:26:45,120 --> 01:26:46,960
and the Math Olympiad.

1843
01:26:46,960 --> 01:26:50,760
And in the last few years, when people tried

1844
01:26:50,760 --> 01:26:53,720
to forecast the MMLU benchmark,

1845
01:26:53,760 --> 01:26:55,920
which was having a lot of more sophisticated

1846
01:26:55,920 --> 01:27:00,440
kind of like graduate student science kind of questions.

1847
01:27:01,640 --> 01:27:05,760
Yeah, AI knocked that down a lot faster

1848
01:27:05,760 --> 01:27:08,280
than AI researchers who had registered

1849
01:27:08,280 --> 01:27:10,880
and students who had registered forecasts on it.

1850
01:27:10,880 --> 01:27:14,160
And so if you're getting top-notch scores

1851
01:27:14,160 --> 01:27:19,160
on graduate exams, creative problem solving,

1852
01:27:19,600 --> 01:27:23,240
yeah, it's not obvious that that sort of area

1853
01:27:23,240 --> 01:27:26,680
will be a relative weakness of AI.

1854
01:27:26,680 --> 01:27:30,320
That in fact, computer science is in many ways,

1855
01:27:30,320 --> 01:27:33,640
especially suitable because of getting up to speed

1856
01:27:33,640 --> 01:27:37,840
with new areas, being able to get rapid feedback

1857
01:27:37,840 --> 01:27:40,840
from the interpreter at scale.

1858
01:27:40,840 --> 01:27:42,040
But did you get rapid feedback?

1859
01:27:42,040 --> 01:27:44,120
If you're doing that, something that's more analogous

1860
01:27:44,120 --> 01:27:46,760
to research, if you're like,

1861
01:27:46,760 --> 01:27:48,720
let's say you have a new model or something.

1862
01:27:48,720 --> 01:27:52,320
And it's like, if we put in $10 million

1863
01:27:52,800 --> 01:27:55,200
on a mini-training run on this, this would be a much bit.

1864
01:27:55,200 --> 01:27:57,240
Yeah, for very large models,

1865
01:27:57,240 --> 01:27:59,320
those experiments are gonna be quite expensive.

1866
01:27:59,320 --> 01:28:01,320
And so you're gonna look more at like,

1867
01:28:01,320 --> 01:28:05,440
can you build up this capability by generalization

1868
01:28:05,440 --> 01:28:07,400
from things like many math problems,

1869
01:28:07,400 --> 01:28:10,320
programming problems, working with small networks?

1870
01:28:10,320 --> 01:28:11,800
Yeah, yeah, fair enough.

1871
01:28:11,800 --> 01:28:14,320
I actually, Scott Aaronson was one of my professors

1872
01:28:14,320 --> 01:28:17,640
in college and I took his quantum information class

1873
01:28:17,640 --> 01:28:20,400
and I didn't do, I did okay in it,

1874
01:28:20,400 --> 01:28:24,920
but he recently wrote a blog post where he said,

1875
01:28:24,920 --> 01:28:27,040
I had GBD-4 take my quantum information test

1876
01:28:27,040 --> 01:28:28,680
and it got a B.

1877
01:28:28,680 --> 01:28:31,880
And I was like, damn, I got a C on the final.

1878
01:28:31,880 --> 01:28:34,760
So yeah, yeah, I'm updated in the direction that,

1879
01:28:34,760 --> 01:28:37,360
you know, it seems, getting a B on the test,

1880
01:28:37,360 --> 01:28:38,600
like you probably understand quantum information

1881
01:28:38,600 --> 01:28:39,440
pretty well.

1882
01:28:39,440 --> 01:28:41,560
With different areas of strengths and weaknesses

1883
01:28:41,560 --> 01:28:42,400
than the human students.

1884
01:28:42,400 --> 01:28:43,520
Sure, sure.

1885
01:28:43,520 --> 01:28:46,800
Would it be possible for this sort of intelligence

1886
01:28:46,800 --> 01:28:50,200
explosion to happen without any sort of hardware progress

1887
01:28:50,200 --> 01:28:52,040
if hardware progress stopped?

1888
01:28:52,040 --> 01:28:54,560
Would this feedback loop still be able to produce

1889
01:28:54,560 --> 01:28:57,120
some sort of explosion with only software?

1890
01:28:57,120 --> 01:29:01,640
Yeah, so if we say that the technology is frozen,

1891
01:29:01,640 --> 01:29:04,560
which I think is not the case right now,

1892
01:29:04,560 --> 01:29:07,960
the, you know, NVIDIA has managed to deliver

1893
01:29:07,960 --> 01:29:10,360
significantly better chips for AI workloads

1894
01:29:10,360 --> 01:29:14,640
for the last few generations, H100, A100, V100.

1895
01:29:14,640 --> 01:29:18,520
If that stops entirely, then what you're left with,

1896
01:29:19,160 --> 01:29:21,840
maybe we'll define this as like no more nodes,

1897
01:29:21,840 --> 01:29:23,640
more as a lot is over.

1898
01:29:23,640 --> 01:29:25,880
At that point, the kind of gains you get

1899
01:29:25,880 --> 01:29:28,960
in amount of compute available come from actually

1900
01:29:28,960 --> 01:29:31,680
constructing more chips.

1901
01:29:31,680 --> 01:29:33,160
And there are economies of scale

1902
01:29:33,160 --> 01:29:34,720
you could still realize there.

1903
01:29:34,720 --> 01:29:39,720
So right now, a chip maker has to amortize the R&D cost

1904
01:29:40,240 --> 01:29:42,280
of developing the chip.

1905
01:29:42,280 --> 01:29:44,840
And then the capital equipment is created,

1906
01:29:44,840 --> 01:29:48,360
like you build a fab, its peak profits are gonna come

1907
01:29:48,440 --> 01:29:51,400
in the few years when the chips it's making

1908
01:29:51,400 --> 01:29:53,200
are at the cutting edge.

1909
01:29:53,200 --> 01:29:57,080
Later on, as the cost of compute exponentially falls,

1910
01:29:57,080 --> 01:29:59,080
the, you know, you keep the fab open

1911
01:29:59,080 --> 01:30:00,280
because you can still make some money

1912
01:30:00,280 --> 01:30:03,720
given that it's built, but of all of the profits

1913
01:30:03,720 --> 01:30:05,920
the fab will ever make right now,

1914
01:30:05,920 --> 01:30:07,760
they're relatively front loaded

1915
01:30:07,760 --> 01:30:11,080
because when its technology is near the cutting edge.

1916
01:30:11,080 --> 01:30:13,800
So in a world where Moore's law ends,

1917
01:30:13,800 --> 01:30:18,000
then you wind up with these very long production runs.

1918
01:30:18,600 --> 01:30:21,160
Where you can keep making chips

1919
01:30:21,160 --> 01:30:23,040
that stay at the cutting edge

1920
01:30:23,040 --> 01:30:26,480
and where the R&D costs get amortized

1921
01:30:26,480 --> 01:30:28,440
over a much larger base.

1922
01:30:28,440 --> 01:30:31,840
So the R&D basically drops out of the price.

1923
01:30:31,840 --> 01:30:33,840
And then you get some economies of scale

1924
01:30:33,840 --> 01:30:36,800
from just making so many fabs in the way that,

1925
01:30:36,800 --> 01:30:39,800
you know, when we have the auto industry expands

1926
01:30:39,800 --> 01:30:42,200
and then this is in general across industries

1927
01:30:42,200 --> 01:30:46,440
when you produce a lot more costs fall

1928
01:30:46,520 --> 01:30:51,000
because you have right now, like ASML has many,

1929
01:30:51,000 --> 01:30:53,000
you know, incredibly exotic suppliers

1930
01:30:53,000 --> 01:30:55,840
that make some bizarre part of the thousands of parts

1931
01:30:55,840 --> 01:30:57,960
in one of these ASML machines.

1932
01:30:57,960 --> 01:31:00,480
You can't get it anywhere else.

1933
01:31:00,480 --> 01:31:03,560
They don't have standardized equipment for their thing

1934
01:31:03,560 --> 01:31:06,280
because this is the only, only use for it.

1935
01:31:06,280 --> 01:31:09,680
And in a world where we're making 10, 100 times

1936
01:31:09,680 --> 01:31:12,480
as many chips at the current node,

1937
01:31:12,480 --> 01:31:15,200
then they would benefit from scale economies.

1938
01:31:15,360 --> 01:31:18,680
And all of that would become more mass production

1939
01:31:18,680 --> 01:31:19,880
industrialized.

1940
01:31:19,880 --> 01:31:21,880
And so you combine all of those things

1941
01:31:21,880 --> 01:31:24,480
and it seems like capital costs

1942
01:31:24,480 --> 01:31:26,840
of like buying a chip would decline,

1943
01:31:26,840 --> 01:31:30,120
but the energy costs of running the chip would not.

1944
01:31:30,120 --> 01:31:34,120
And so right now, energy costs are a minority of the cost,

1945
01:31:34,120 --> 01:31:37,000
but they're not, they're not, they're not trivial.

1946
01:31:37,000 --> 01:31:40,640
You know, they pass, yeah, it passed 1% a while ago

1947
01:31:40,640 --> 01:31:44,440
and they're inching up towards 10% and beyond.

1948
01:31:45,560 --> 01:31:49,400
And so you can maybe get like another order of magnitude

1949
01:31:50,400 --> 01:31:54,520
costs decrease from getting really efficient

1950
01:31:54,520 --> 01:31:55,840
in the sort of capital construction,

1951
01:31:55,840 --> 01:31:59,080
but like energy would still be a limiting factor

1952
01:31:59,080 --> 01:32:02,200
after the end of sort of actually improving

1953
01:32:02,200 --> 01:32:03,040
the chips themselves.

1954
01:32:03,040 --> 01:32:03,880
Got it, got it.

1955
01:32:03,880 --> 01:32:04,720
And when you say like,

1956
01:32:04,720 --> 01:32:06,520
there would be a greater population of AI researchers

1957
01:32:06,520 --> 01:32:10,240
because are we using population as a sort of thinking tool

1958
01:32:10,240 --> 01:32:11,800
of how they could be more effective?

1959
01:32:11,800 --> 01:32:14,800
Or do you literally mean that the way you expect

1960
01:32:14,800 --> 01:32:17,400
these AIs to contribute a lot to researchers

1961
01:32:17,400 --> 01:32:19,960
by just having like a million copies

1962
01:32:19,960 --> 01:32:23,400
of this, of like a researcher thinking about the same problem?

1963
01:32:23,400 --> 01:32:24,720
Or is it just like a useful thinking model

1964
01:32:24,720 --> 01:32:27,440
for what it would look like to have a million times

1965
01:32:27,440 --> 01:32:28,840
smarter AI working on that problem?

1966
01:32:28,840 --> 01:32:31,880
That's definitely a lower bound sort of model.

1967
01:32:31,880 --> 01:32:33,840
And often I'm meaning something more like

1968
01:32:35,000 --> 01:32:38,600
effective population or like you'd need this many people

1969
01:32:38,600 --> 01:32:39,640
to have this effect.

1970
01:32:39,640 --> 01:32:41,560
And so we were talking earlier about the trade off

1971
01:32:41,600 --> 01:32:45,840
between training and inference in board games.

1972
01:32:45,840 --> 01:32:48,720
And so you can get the same performance

1973
01:32:48,720 --> 01:32:51,000
by having a bigger model

1974
01:32:51,000 --> 01:32:52,640
or by calling the model more times.

1975
01:32:52,640 --> 01:32:55,200
And in general, it's more effective

1976
01:32:55,200 --> 01:32:57,240
to have a bigger, smarter model

1977
01:32:57,240 --> 01:32:59,840
and call it less timed up until the point

1978
01:32:59,840 --> 01:33:02,000
where the costs equalized between them.

1979
01:33:02,000 --> 01:33:04,400
And so we would be taking some of the gains

1980
01:33:04,400 --> 01:33:07,880
of our larger compute on having bigger models

1981
01:33:07,880 --> 01:33:10,360
that are individually more capable.

1982
01:33:10,360 --> 01:33:12,560
And there would be a division of labor.

1983
01:33:12,560 --> 01:33:15,040
So like the tasks that were most cognitively demanding

1984
01:33:15,040 --> 01:33:16,440
would be done by these giant models,

1985
01:33:16,440 --> 01:33:18,480
but some very easy tasks.

1986
01:33:18,480 --> 01:33:20,840
You don't want to expend that giant model

1987
01:33:20,840 --> 01:33:25,240
if a model 100s the size can take that task.

1988
01:33:25,240 --> 01:33:28,320
And so larger models would be in the positions

1989
01:33:28,320 --> 01:33:30,560
of like researchers and managers

1990
01:33:30,560 --> 01:33:34,080
and they would have swarms of AIs of different sizes

1991
01:33:34,080 --> 01:33:38,040
as tools that they could make API calls to and whatnot.

1992
01:33:38,080 --> 01:33:40,160
Okay, we accept the model

1993
01:33:40,160 --> 01:33:42,160
and now we've gone to something that is at least as smart

1994
01:33:42,160 --> 01:33:45,520
as Ilya Suskova on all the tasks relevant to AI progress.

1995
01:33:45,520 --> 01:33:48,800
And you can have so many copies of it.

1996
01:33:48,800 --> 01:33:50,040
What happens in the world now?

1997
01:33:50,040 --> 01:33:51,880
What are the next months or years

1998
01:33:51,880 --> 01:33:53,480
or whatever timeline is relevant to look like?

1999
01:33:53,480 --> 01:33:58,480
And so, and to be clear with what's happened is not

2000
01:33:58,800 --> 01:34:01,680
that we have something that has all of the abilities

2001
01:34:01,680 --> 01:34:04,600
and advantages of humans plus the AI advantages.

2002
01:34:04,600 --> 01:34:07,800
What we have is something that is like possibly

2003
01:34:07,800 --> 01:34:10,520
by doing things like doing a ton of calls

2004
01:34:10,520 --> 01:34:13,720
to make up for being individually less capable or something.

2005
01:34:13,720 --> 01:34:17,480
It's able to drive forward AI progress.

2006
01:34:17,480 --> 01:34:19,040
That process is continuing.

2007
01:34:19,040 --> 01:34:22,560
So AI progress has accelerated greatly

2008
01:34:22,560 --> 01:34:23,920
in the course of getting there.

2009
01:34:23,920 --> 01:34:26,880
And so maybe we go from our eight months doubling time

2010
01:34:26,880 --> 01:34:31,040
of software progress in effective compute

2011
01:34:31,040 --> 01:34:33,800
to four months or two months.

2012
01:34:34,680 --> 01:34:38,600
And so, so there's a report by Tom Davidson

2013
01:34:38,600 --> 01:34:40,720
at the Open Philanthropy Project,

2014
01:34:40,720 --> 01:34:45,240
which spun out of work I had done previously.

2015
01:34:45,240 --> 01:34:50,240
And so I advised and helped with that project

2016
01:34:51,880 --> 01:34:53,800
but Tom really carried it forward

2017
01:34:53,800 --> 01:34:56,760
and produced a very nice report and model

2018
01:34:56,760 --> 01:34:58,920
which Epoch is hosting.

2019
01:34:58,920 --> 01:35:01,880
You can plug in your own version of the parameters

2020
01:35:02,360 --> 01:35:06,640
and there's a lot of work estimating the parameter.

2021
01:35:06,640 --> 01:35:10,400
Things like what's the rate of software progress?

2022
01:35:10,400 --> 01:35:12,240
What's the return to additional work?

2023
01:35:12,240 --> 01:35:15,680
How does performance scale at these tasks

2024
01:35:15,680 --> 01:35:18,120
as you boost the models?

2025
01:35:18,120 --> 01:35:21,160
And in general, as we were discussing earlier,

2026
01:35:21,160 --> 01:35:26,160
these sort of like broadly human level in every domain

2027
01:35:26,160 --> 01:35:31,160
with all the advantages is pretty deep into that.

2028
01:35:32,520 --> 01:35:37,200
And so if already we can have an eight months doubling time

2029
01:35:37,200 --> 01:35:41,760
for software progress, then by the time you get

2030
01:35:41,760 --> 01:35:45,280
to that kind of point, it's maybe more like four months,

2031
01:35:45,280 --> 01:35:49,280
two months going into one month.

2032
01:35:49,280 --> 01:35:54,160
And so if the thing is just proceeding at full speed,

2033
01:35:54,160 --> 01:35:58,760
then each doubling can come more rapidly.

2034
01:35:58,760 --> 01:36:03,760
And so we can talk about what are the spillovers of like,

2035
01:36:04,280 --> 01:36:06,440
so how does the models get more capable?

2036
01:36:06,440 --> 01:36:08,680
They can be doing other stuff in the world.

2037
01:36:08,680 --> 01:36:10,720
They can spend some of their time

2038
01:36:10,720 --> 01:36:12,520
making Google search more efficient.

2039
01:36:12,520 --> 01:36:17,520
They can be hired, has chatbots with some inference compute.

2040
01:36:18,200 --> 01:36:20,760
And then we can talk about sort of

2041
01:36:21,760 --> 01:36:24,720
if that intelligence explosion process

2042
01:36:24,720 --> 01:36:27,440
is allowed to proceed, then what happens is,

2043
01:36:27,440 --> 01:36:32,440
okay, you improve your software by a factor of two,

2044
01:36:33,480 --> 01:36:37,840
the demand, the efforts needed to get the next doubling

2045
01:36:37,840 --> 01:36:39,720
are larger, but they're not choices large.

2046
01:36:39,720 --> 01:36:42,720
Maybe they're like 25%, 35% larger.

2047
01:36:43,840 --> 01:36:46,960
So each one comes faster and faster

2048
01:36:46,960 --> 01:36:49,200
until you hit limitations.

2049
01:36:49,440 --> 01:36:53,440
Like you can no longer make further software advances

2050
01:36:53,440 --> 01:36:55,040
with the hardware that you have.

2051
01:36:56,160 --> 01:36:59,800
And looking at, I think, reasonable parameters

2052
01:36:59,800 --> 01:37:01,920
in that model, it seems to me,

2053
01:37:01,920 --> 01:37:03,640
if you have these giant training runs,

2054
01:37:03,640 --> 01:37:04,840
you can go very far.

2055
01:37:05,840 --> 01:37:09,520
And so the way I would see this playing out

2056
01:37:09,520 --> 01:37:12,840
is how does the AIs get better and better at research?

2057
01:37:12,840 --> 01:37:14,720
They can work on different problems.

2058
01:37:14,720 --> 01:37:16,520
They can work on improving software.

2059
01:37:16,520 --> 01:37:18,560
They can work on improving hardware.

2060
01:37:18,560 --> 01:37:21,760
They can do things like create new industrial technologies,

2061
01:37:21,760 --> 01:37:23,320
new energy technology.

2062
01:37:23,320 --> 01:37:24,960
They can manage robots.

2063
01:37:24,960 --> 01:37:26,840
They can manage human workers

2064
01:37:26,840 --> 01:37:29,960
as like executives and coaches and whatnot.

2065
01:37:29,960 --> 01:37:32,320
You can do all of these things.

2066
01:37:32,320 --> 01:37:36,960
And AIs wind up being applied where the returns are highest.

2067
01:37:36,960 --> 01:37:41,000
And I think initially the returns are especially high

2068
01:37:41,000 --> 01:37:42,920
in doing more software.

2069
01:37:42,920 --> 01:37:45,640
And the reason for that is again,

2070
01:37:46,480 --> 01:37:48,320
if you improve the software,

2071
01:37:48,320 --> 01:37:50,040
you can update all of the GPUs

2072
01:37:50,040 --> 01:37:54,680
that you have access to your cloud compute

2073
01:37:54,680 --> 01:37:56,640
is suddenly more potent.

2074
01:37:56,640 --> 01:38:00,880
If you design a new chip design,

2075
01:38:01,960 --> 01:38:05,400
it'll take a few months to produce the first ones

2076
01:38:05,400 --> 01:38:08,680
and it doesn't update all of your old chips.

2077
01:38:08,680 --> 01:38:11,960
So you have an ordering where you start off

2078
01:38:11,960 --> 01:38:15,160
with the things where there's the lowest dependence

2079
01:38:15,480 --> 01:38:18,440
on existing stocks.

2080
01:38:18,440 --> 01:38:20,880
And you can more just take whatever you're developing

2081
01:38:20,880 --> 01:38:21,880
and apply it immediately.

2082
01:38:21,880 --> 01:38:24,360
And so software runs ahead.

2083
01:38:24,360 --> 01:38:29,080
You're getting more towards the limits of that software.

2084
01:38:29,080 --> 01:38:30,600
And I think that means things like

2085
01:38:30,600 --> 01:38:32,600
having all the human advantages

2086
01:38:32,600 --> 01:38:35,440
but combined with AI advantages.

2087
01:38:35,440 --> 01:38:40,440
And so I think that means given the kind of compute

2088
01:38:42,000 --> 01:38:43,520
that would be involved,

2089
01:38:43,520 --> 01:38:45,400
if we're talking about this hundreds of billions

2090
01:38:45,400 --> 01:38:48,600
of trillion dollar training run,

2091
01:38:48,600 --> 01:38:51,200
there's enough compute to run tens of millions,

2092
01:38:51,200 --> 01:38:55,600
hundreds of millions of sort of like human scale minds.

2093
01:38:55,600 --> 01:38:58,320
They're probably smaller than human scale

2094
01:38:59,440 --> 01:39:02,240
to be like similarly efficient at the limits

2095
01:39:02,240 --> 01:39:03,080
of algorithmic progress

2096
01:39:03,080 --> 01:39:04,040
because they have the advantage

2097
01:39:04,040 --> 01:39:05,400
of a million years of education.

2098
01:39:05,400 --> 01:39:08,280
They have the other advantages we talked about.

2099
01:39:08,280 --> 01:39:10,440
So you've got that wild capability

2100
01:39:11,400 --> 01:39:14,240
and further software gains are running out.

2101
01:39:14,240 --> 01:39:17,480
Or like they start to slow down again

2102
01:39:17,480 --> 01:39:20,400
because you're just getting towards the limits

2103
01:39:20,400 --> 01:39:23,360
of like you can't do any better than the best.

2104
01:39:23,360 --> 01:39:25,640
And so what happens then?

2105
01:39:25,640 --> 01:39:28,000
Yeah, by the time they're running out,

2106
01:39:28,000 --> 01:39:30,360
have we already hit superintelligence or?

2107
01:39:30,360 --> 01:39:33,440
Yes, you're wildly superintelligent.

2108
01:39:33,440 --> 01:39:35,600
We love the galaxy, okay, metaphorically.

2109
01:39:35,600 --> 01:39:38,520
Just by having the abilities that humans have

2110
01:39:38,520 --> 01:39:41,400
and then combining it with being very well focused

2111
01:39:41,400 --> 01:39:44,000
and trained in the task beyond what any human could be

2112
01:39:44,000 --> 01:39:45,560
and then running faster and such.

2113
01:39:45,560 --> 01:39:46,400
Got it, got it.

2114
01:39:46,400 --> 01:39:47,240
All right, so I continue.

2115
01:39:47,240 --> 01:39:48,600
Yeah, so I'm not gonna assume

2116
01:39:48,600 --> 01:39:52,360
that there's like huge qualitative improvements you can have.

2117
01:39:52,360 --> 01:39:55,120
I'm not gonna assume that humans are like very far

2118
01:39:55,120 --> 01:39:57,240
from the efficient frontier of software,

2119
01:39:57,240 --> 01:40:00,040
except with respect to things like,

2120
01:40:00,040 --> 01:40:01,760
yeah, we had limited lifespan

2121
01:40:01,760 --> 01:40:03,560
so we couldn't train super intensively.

2122
01:40:03,560 --> 01:40:07,680
We couldn't incorporate other software into our brains.

2123
01:40:07,680 --> 01:40:09,000
We couldn't copy ourselves.

2124
01:40:09,000 --> 01:40:10,640
We couldn't run at fast speeds.

2125
01:40:11,640 --> 01:40:14,700
Yeah, so you've got all of those capabilities.

2126
01:40:15,360 --> 01:40:19,880
And now I'm skipping ahead of like the most important months

2127
01:40:19,880 --> 01:40:21,160
in human history.

2128
01:40:22,720 --> 01:40:25,480
And so I can talk about sort of,

2129
01:40:26,840 --> 01:40:31,520
what it looks like if it's just the AIs took over,

2130
01:40:31,520 --> 01:40:36,160
they're running things that they like, how do things expand?

2131
01:40:36,160 --> 01:40:38,760
I can talk about things has,

2132
01:40:38,760 --> 01:40:43,760
how does this go in a world where we've roughly

2133
01:40:44,280 --> 01:40:48,720
or at least so far managed to retain control

2134
01:40:48,720 --> 01:40:50,800
of where these systems are going?

2135
01:40:51,880 --> 01:40:53,800
And so by jumping ahead,

2136
01:40:53,800 --> 01:40:55,400
I can talk about how would this translate

2137
01:40:55,400 --> 01:40:56,840
into the physical world?

2138
01:40:56,840 --> 01:40:58,760
And so this is something that I think is a stopping point

2139
01:40:58,760 --> 01:41:01,120
for a lot of people in thinking about,

2140
01:41:01,120 --> 01:41:03,960
well, what would an intelligence explosion look like?

2141
01:41:04,040 --> 01:41:06,080
And they have trouble going from,

2142
01:41:06,080 --> 01:41:09,520
well, there's stuff on servers and cloud compute

2143
01:41:09,520 --> 01:41:11,440
and oh, that gets very smart.

2144
01:41:11,440 --> 01:41:14,640
But then how does what I see in the world change?

2145
01:41:14,640 --> 01:41:17,760
How does like industry or military power change?

2146
01:41:17,760 --> 01:41:21,120
If there's an AI takeover, like what does that look like?

2147
01:41:21,120 --> 01:41:23,040
Are there killer robots?

2148
01:41:23,040 --> 01:41:26,080
And so yeah, so one course we might go down

2149
01:41:26,080 --> 01:41:31,080
is to discuss during that wildly accelerating transition,

2150
01:41:31,920 --> 01:41:33,880
how did we manage that?

2151
01:41:33,880 --> 01:41:36,440
How do you avoid it being catastrophic?

2152
01:41:36,440 --> 01:41:41,440
And another route we could go is how does the translation

2153
01:41:41,440 --> 01:41:46,440
from wildly expanded scientific R&D capabilities intelligence

2154
01:41:48,680 --> 01:41:53,680
on these servers translate into things in the physical world?

2155
01:41:53,680 --> 01:41:56,440
So you're moving along in order of like,

2156
01:41:56,440 --> 01:42:00,320
what has the quickest impact largely

2157
01:42:01,080 --> 01:42:06,080
or like where you can have an immediate change?

2158
01:42:07,800 --> 01:42:12,800
So one of the most immediately accessible things

2159
01:42:12,800 --> 01:42:17,720
is where we have large numbers of devices

2160
01:42:17,720 --> 01:42:22,720
or artifacts or capabilities that are already AI operable

2161
01:42:23,640 --> 01:42:28,560
with hundreds of millions equivalent researchers,

2162
01:42:28,560 --> 01:42:32,120
you can like quickly solve self-driving cars,

2163
01:42:33,320 --> 01:42:36,560
you make the algorithms much more efficient,

2164
01:42:36,560 --> 01:42:39,640
do great testing and simulation

2165
01:42:39,640 --> 01:42:43,000
and then operate a large number of cars in parallel

2166
01:42:43,000 --> 01:42:46,040
if you need to get some additional data

2167
01:42:46,040 --> 01:42:47,640
to improve the simulation and reasoning.

2168
01:42:47,640 --> 01:42:51,160
Although, in fact, humans with quite little data

2169
01:42:52,560 --> 01:42:55,800
are able to achieve human level driving performance.

2170
01:42:55,840 --> 01:42:59,320
So after you've really maxed out

2171
01:42:59,320 --> 01:43:01,800
the easily accessible algorithmic improvements

2172
01:43:01,800 --> 01:43:04,320
in this software based intelligence explosion

2173
01:43:04,320 --> 01:43:06,440
that's mostly happening on server farms,

2174
01:43:06,440 --> 01:43:10,600
then you have minds that have been able to really perform

2175
01:43:10,600 --> 01:43:14,240
on a lot of digital only tasks that they're doing great

2176
01:43:14,240 --> 01:43:17,520
on video games, they're doing great at predicting

2177
01:43:17,520 --> 01:43:20,520
what happens next in a YouTube video.

2178
01:43:20,520 --> 01:43:22,760
If you have a camera that they can move,

2179
01:43:22,760 --> 01:43:24,680
they're able to predict what will happen

2180
01:43:25,800 --> 01:43:28,120
at different angles, humans do this a lot

2181
01:43:28,120 --> 01:43:30,280
where we naturally move our eyes in such a way

2182
01:43:30,280 --> 01:43:33,600
to get images from different angles

2183
01:43:33,600 --> 01:43:34,680
and different presentations

2184
01:43:34,680 --> 01:43:37,240
and then predicting combined from that.

2185
01:43:37,240 --> 01:43:40,600
And yeah, and you can operate many cars,

2186
01:43:40,600 --> 01:43:45,480
many robots at once to get very good robot controllers.

2187
01:43:45,480 --> 01:43:49,480
So you should think that all the existing robotic equipment

2188
01:43:49,480 --> 01:43:53,880
or remotely controllable equipment that is wired for that,

2189
01:43:53,880 --> 01:43:56,200
the AI's can operate that quite well.

2190
01:43:56,200 --> 01:43:58,640
I think some people might be skeptical

2191
01:43:58,640 --> 01:44:01,760
that existing robots, given their current hardware,

2192
01:44:01,760 --> 01:44:04,800
have the dexterity and the maneuverability

2193
01:44:04,800 --> 01:44:07,760
to do a lot of physical labor

2194
01:44:07,760 --> 01:44:09,080
that any AI might want to do.

2195
01:44:09,080 --> 01:44:10,240
Do you have reason for thinking otherwise?

2196
01:44:10,240 --> 01:44:12,000
There's also not very many of them.

2197
01:44:12,000 --> 01:44:14,520
So production of sort of industrial robots

2198
01:44:14,520 --> 01:44:16,920
is hundreds of thousands per year.

2199
01:44:18,240 --> 01:44:21,600
They can do quite a bit in place.

2200
01:44:21,640 --> 01:44:24,480
Eveline Musk is promising a robot

2201
01:44:24,480 --> 01:44:25,600
in the tens of thousands of,

2202
01:44:25,600 --> 01:44:28,680
humanoid robot in the tens of thousands of dollars,

2203
01:44:28,680 --> 01:44:33,200
that may take a lot longer than he has said.

2204
01:44:33,200 --> 01:44:35,000
Has this happened with other technologies?

2205
01:44:35,000 --> 01:44:37,320
But I mean, that's a direction to go.

2206
01:44:37,320 --> 01:44:39,400
But most immediately,

2207
01:44:39,400 --> 01:44:43,400
so hands are actually probably the most scarce thing.

2208
01:44:44,560 --> 01:44:47,280
But if we consider what do human bodies provide?

2209
01:44:47,280 --> 01:44:49,040
So there's the brain.

2210
01:44:49,040 --> 01:44:50,960
And in this situation,

2211
01:44:50,960 --> 01:44:54,360
we have now an abundance of high quality brain power

2212
01:44:54,360 --> 01:44:55,400
that will be increasing,

2213
01:44:55,400 --> 01:44:58,560
as the AI's will have designed new chips,

2214
01:44:58,560 --> 01:45:02,000
which will be rolling out from the TSMC factories,

2215
01:45:02,000 --> 01:45:04,800
and they'll have ideas and designs

2216
01:45:04,800 --> 01:45:08,120
for the production of new fab technologies,

2217
01:45:08,120 --> 01:45:11,040
new nodes and additional fabs.

2218
01:45:11,920 --> 01:45:13,000
But yeah, looking around the body.

2219
01:45:13,000 --> 01:45:15,240
So there's legs to move around.

2220
01:45:15,240 --> 01:45:16,400
Not only that necessary,

2221
01:45:16,400 --> 01:45:18,400
wheels work pretty well being in a place.

2222
01:45:18,400 --> 01:45:21,720
You don't need most people most of the time

2223
01:45:21,720 --> 01:45:23,720
in factory jobs and office jobs.

2224
01:45:23,720 --> 01:45:27,400
Office jobs, many of them can be fully virtualized.

2225
01:45:28,360 --> 01:45:33,160
But yeah, some amount of legs, wheels, other transport,

2226
01:45:33,160 --> 01:45:36,840
you have hands and hands are something that are,

2227
01:45:36,840 --> 01:45:40,760
on the expensive end in robots, we can make them.

2228
01:45:40,760 --> 01:45:43,640
They're made in very small production runs,

2229
01:45:43,640 --> 01:45:45,440
partly because we don't have the control software

2230
01:45:45,440 --> 01:45:46,280
to use them well.

2231
01:45:46,640 --> 01:45:49,280
In this world, the control software is fabulous,

2232
01:45:49,280 --> 01:45:53,160
and so people will produce much larger production runs

2233
01:45:53,160 --> 01:45:56,840
of them over time, possibly using technology

2234
01:45:56,840 --> 01:45:59,760
we recognize possibly with quite different technology,

2235
01:45:59,760 --> 01:46:01,880
but just taking what we've got.

2236
01:46:02,960 --> 01:46:07,000
So right now, the robot arm industry,

2237
01:46:07,000 --> 01:46:08,680
the industrial robot industry produces

2238
01:46:08,680 --> 01:46:11,520
hundreds of thousands of machines a year.

2239
01:46:11,520 --> 01:46:15,320
Some of the nicer ones are like $50,000.

2240
01:46:15,320 --> 01:46:17,440
In aggregate, the industry has tens of billions

2241
01:46:17,440 --> 01:46:19,160
of dollars of revenue.

2242
01:46:19,160 --> 01:46:23,000
By comparison, the automobile industry produces

2243
01:46:23,000 --> 01:46:26,440
like I think over 60 million cars a year.

2244
01:46:26,440 --> 01:46:30,940
It has revenue of over $2 trillion per annum.

2245
01:46:31,760 --> 01:46:36,760
And so converting that production capacity

2246
01:46:37,280 --> 01:46:40,360
over towards robot production would be one of the things,

2247
01:46:40,360 --> 01:46:42,320
if they're not something better to do,

2248
01:46:42,320 --> 01:46:44,000
would be one of the things to do.

2249
01:46:44,000 --> 01:46:49,000
And in World War II, industrial conversion

2250
01:46:49,000 --> 01:46:52,800
of American industry took place over several years

2251
01:46:53,800 --> 01:46:58,800
and really amazingly ramped up military production

2252
01:47:00,120 --> 01:47:03,000
by converting existing civilian industry.

2253
01:47:03,000 --> 01:47:07,200
And that was without the aid of superhuman intelligence

2254
01:47:07,200 --> 01:47:10,040
and management at every step in the process.

2255
01:47:10,040 --> 01:47:15,040
So yeah, every part of that would be very well designed.

2256
01:47:15,280 --> 01:47:18,400
You'd have AI workers who understood,

2257
01:47:18,400 --> 01:47:20,280
stood every part of the process

2258
01:47:20,280 --> 01:47:23,400
and could direct human workers.

2259
01:47:23,400 --> 01:47:28,000
Even in a fancy factory, most of the time,

2260
01:47:28,000 --> 01:47:31,960
it's not the hands doing a physical motion

2261
01:47:33,040 --> 01:47:34,480
that a worker is being paid for.

2262
01:47:34,480 --> 01:47:36,680
They're often like looking at things

2263
01:47:36,680 --> 01:47:39,640
or like deciding what to change.

2264
01:47:39,640 --> 01:47:43,760
The actual, the time spent in manual motion

2265
01:47:43,760 --> 01:47:45,120
is a limited portion of that.

2266
01:47:45,120 --> 01:47:48,680
And so in this world of abundant AI cognitive abilities

2267
01:47:49,880 --> 01:47:52,480
where the human workers are more valuable

2268
01:47:52,480 --> 01:47:54,640
for their hands than their heads,

2269
01:47:54,640 --> 01:47:57,800
then you could have a worker,

2270
01:47:57,800 --> 01:48:00,520
even a worker previously without training

2271
01:48:00,520 --> 01:48:04,840
and expertise in the area who has a smartphone,

2272
01:48:04,840 --> 01:48:07,640
maybe a smartphone on a headset.

2273
01:48:07,640 --> 01:48:09,400
And we have billions of smartphones,

2274
01:48:09,400 --> 01:48:13,160
which have eyes and ears and methods for communication,

2275
01:48:13,160 --> 01:48:15,520
for an AI to be talking to a human

2276
01:48:15,520 --> 01:48:18,680
and directing them in their physical motions

2277
01:48:18,680 --> 01:48:23,680
with skill as a guide and coach that is beyond any human.

2278
01:48:24,480 --> 01:48:27,720
There could be a lot better at telepresence and remote work

2279
01:48:27,720 --> 01:48:30,800
and they can provide VR and augmented reality guidance

2280
01:48:30,800 --> 01:48:35,520
as to help people get better at doing the physical motions

2281
01:48:35,520 --> 01:48:37,640
that they're providing in the construction.

2282
01:48:37,640 --> 01:48:42,640
Say you convert the auto industry to robot production.

2283
01:48:44,080 --> 01:48:48,520
If it can produce an amount of mass of machines

2284
01:48:48,520 --> 01:48:51,160
that is similar to what it currently produces,

2285
01:48:51,160 --> 01:48:56,160
that's enough for billion human size robots a year.

2286
01:48:59,360 --> 01:49:04,360
The value per kilogram of cars is somewhat less

2287
01:49:05,120 --> 01:49:07,040
than high-end robots,

2288
01:49:07,040 --> 01:49:12,040
but yeah, you're also cutting out most of the wage bill,

2289
01:49:12,360 --> 01:49:14,840
because most of the wage bill is payments ultimately

2290
01:49:14,840 --> 01:49:17,160
to like human capital and education,

2291
01:49:17,160 --> 01:49:22,160
not to the physical hand motions and lifting objects

2292
01:49:22,160 --> 01:49:23,960
and that sort of task.

2293
01:49:23,960 --> 01:49:26,760
Yeah, so at the sort of existing scale of the auto industry,

2294
01:49:26,760 --> 01:49:28,560
you can make a billion robots a year.

2295
01:49:28,560 --> 01:49:32,960
The auto industry is two or 3% of the existing economy.

2296
01:49:33,840 --> 01:49:37,240
You're replacing these cognitive things.

2297
01:49:37,240 --> 01:49:40,080
So if right now physical hand motions

2298
01:49:40,080 --> 01:49:45,080
are like 10% of the work, redirect humans into those tasks

2299
01:49:46,560 --> 01:49:50,840
and you have like in the world at large right now,

2300
01:49:52,000 --> 01:49:55,120
mean income is on the order of $10,000 a year,

2301
01:49:55,120 --> 01:49:58,280
but in rich countries, skilled workers earn

2302
01:49:58,280 --> 01:50:00,000
more than 100,000 per year.

2303
01:50:00,000 --> 01:50:05,000
And some of that is not just management roles

2304
01:50:05,440 --> 01:50:07,720
of which only a certain proportion of the population

2305
01:50:07,720 --> 01:50:12,720
can have, but just being an absolutely exceptional peak

2306
01:50:13,040 --> 01:50:17,560
and human performance of some of these construction

2307
01:50:17,560 --> 01:50:19,800
and such roles.

2308
01:50:19,800 --> 01:50:23,920
Yeah, just raising productivity to match

2309
01:50:23,920 --> 01:50:28,680
the most productive workers in the world is room

2310
01:50:28,720 --> 01:50:31,320
to make a very big gap.

2311
01:50:32,280 --> 01:50:36,640
And with AI replacing skills that are scarce in many places

2312
01:50:36,640 --> 01:50:41,480
where there's an abundant currently low wage labor,

2313
01:50:41,480 --> 01:50:44,280
you bring in the AI coach and someone who is previously

2314
01:50:44,280 --> 01:50:48,060
making very low wages can suddenly be super productive

2315
01:50:48,060 --> 01:50:52,040
by just being the hands for an AI.

2316
01:50:52,040 --> 01:50:57,040
And so on a naive view, if you ignore the delay

2317
01:50:57,320 --> 01:50:59,960
of capital adjustment of like building new tools

2318
01:50:59,960 --> 01:51:04,360
for the workers, say like, yeah, just like raise

2319
01:51:04,360 --> 01:51:07,840
typical productivity for workers around the world

2320
01:51:07,840 --> 01:51:09,680
to be more like rich countries

2321
01:51:11,460 --> 01:51:14,680
and get 5x, 10x like that.

2322
01:51:14,680 --> 01:51:19,160
Get more productivity by with AI handling

2323
01:51:19,160 --> 01:51:22,640
the difficult cognitive tasks, reallocating people

2324
01:51:22,640 --> 01:51:26,600
from like office jobs to providing physical motions

2325
01:51:26,600 --> 01:51:29,200
and since right now that's a small proportion of the economy,

2326
01:51:29,200 --> 01:51:33,160
you can expand the sort of hands for manual labor

2327
01:51:33,160 --> 01:51:37,480
by like an order of magnitude like within a rich country

2328
01:51:37,480 --> 01:51:41,480
by just because most people are sitting in an office

2329
01:51:41,480 --> 01:51:44,840
or even in a factory floor or not continuously moving.

2330
01:51:44,840 --> 01:51:49,040
So you've got billions of hands flying around in humans

2331
01:51:49,040 --> 01:51:52,400
to be used in the course of constructing

2332
01:51:52,400 --> 01:51:53,640
your waves of robots.

2333
01:51:53,640 --> 01:51:58,000
So now once you have a quantity of robots

2334
01:51:58,000 --> 01:51:59,960
that is approaching the human population

2335
01:51:59,960 --> 01:52:02,280
and they work 24 seven, of course,

2336
01:52:03,520 --> 01:52:06,520
the human labor will no longer be valuable

2337
01:52:06,520 --> 01:52:09,560
has hands and legs, but at the very beginning

2338
01:52:09,560 --> 01:52:12,440
of the transition, just like new software

2339
01:52:12,440 --> 01:52:16,640
can be used to update all of the GPUs to run the latest AI.

2340
01:52:17,520 --> 01:52:20,680
Humans are sort of legacy population

2341
01:52:20,680 --> 01:52:24,760
with an enormous number of underutilized hands and feet

2342
01:52:24,760 --> 01:52:28,440
that the AI can use for the initial robot construction.

2343
01:52:28,440 --> 01:52:30,680
Cognitive tasks are being automated

2344
01:52:30,680 --> 01:52:33,520
and the production of them is greatly expanding

2345
01:52:33,520 --> 01:52:37,680
and then the physical tasks which complement them

2346
01:52:37,680 --> 01:52:42,080
are utilizing humans to do the parts that robots that exist can't do.

2347
01:52:42,080 --> 01:52:43,560
Is the implication of this that you're getting to

2348
01:52:43,560 --> 01:52:46,680
that world production would increase just a tremendous amount

2349
01:52:46,680 --> 01:52:50,600
or that AI could get a lot done of whatever motivations it has

2350
01:52:51,160 --> 01:52:55,560
Yeah, so there's an enormous increase in production

2351
01:52:55,560 --> 01:52:59,920
for humans who just switching over to the role

2352
01:52:59,920 --> 01:53:04,560
of providing hands and feet for AI where they're limited.

2353
01:53:04,560 --> 01:53:08,920
And this robot industry is a natural place to apply it.

2354
01:53:08,920 --> 01:53:12,640
And so if you go to something that's like 10x

2355
01:53:12,640 --> 01:53:15,160
the size of like the current car industry

2356
01:53:15,160 --> 01:53:18,040
in terms of its production,

2357
01:53:18,040 --> 01:53:20,920
which would still be like a third of our current economy

2358
01:53:20,920 --> 01:53:23,800
and the aggregate productive capabilities of the society

2359
01:53:23,800 --> 01:53:26,920
with AI support are going to be a lot larger.

2360
01:53:26,920 --> 01:53:30,640
They make 10 billion human-eyed robots a year.

2361
01:53:30,640 --> 01:53:35,760
And then if you do that the legacy population

2362
01:53:35,760 --> 01:53:40,480
of a few billion human workers is no longer very important

2363
01:53:40,480 --> 01:53:42,000
for the physical tasks.

2364
01:53:42,000 --> 01:53:46,400
And then the new automated industrial base

2365
01:53:46,400 --> 01:53:50,240
can just produce more factories, produce more robots.

2366
01:53:50,240 --> 01:53:51,880
And then the interesting thing is like,

2367
01:53:51,880 --> 01:53:53,480
what's the doubling time?

2368
01:53:53,480 --> 01:53:57,480
How long does it take for a set of computers,

2369
01:53:57,480 --> 01:54:01,200
robots, factories and supporting equipment

2370
01:54:01,200 --> 01:54:04,400
to produce another equivalent quantity of that?

2371
01:54:04,400 --> 01:54:08,920
For GPUs, brains, this is really, really easy, really solid.

2372
01:54:08,920 --> 01:54:11,320
There's an enormous margin there.

2373
01:54:11,320 --> 01:54:15,200
We're talking before about, yeah,

2374
01:54:15,240 --> 01:54:20,160
skilled human workers getting paid $100 an hour

2375
01:54:20,160 --> 01:54:25,160
is like quite normal in developed countries

2376
01:54:25,480 --> 01:54:27,880
for very in-demand skills.

2377
01:54:27,880 --> 01:54:32,880
And you make a GPU that can do that work.

2378
01:54:34,360 --> 01:54:38,360
Right now, these GPUs are like tens of thousands of dollars.

2379
01:54:39,240 --> 01:54:44,240
If you can do $100 of wages each hour,

2380
01:54:45,080 --> 01:54:50,080
then in a few weeks, you pay back your costs.

2381
01:54:50,160 --> 01:54:52,480
If the thing is more productive,

2382
01:54:52,480 --> 01:54:53,840
and as we were discussing,

2383
01:54:53,840 --> 01:54:57,640
you can be a lot more productive than a sort of a typical,

2384
01:54:57,640 --> 01:54:59,160
high-paid human professional,

2385
01:54:59,160 --> 01:55:01,200
by being like the very best human professional,

2386
01:55:01,200 --> 01:55:02,280
and even better than that,

2387
01:55:02,280 --> 01:55:03,880
by having a million years of education

2388
01:55:03,880 --> 01:55:05,440
and working all the time.

2389
01:55:05,440 --> 01:55:08,760
Yeah, then you could get even shorter payback times.

2390
01:55:08,760 --> 01:55:12,000
Like, yeah, you can generate the dollar value

2391
01:55:12,000 --> 01:55:16,280
of the cost, initial cost of that equipment

2392
01:55:16,280 --> 01:55:17,520
within a few weeks.

2393
01:55:17,520 --> 01:55:21,240
For robots, so like a human factory worker

2394
01:55:22,080 --> 01:55:26,040
can earn $50,000 a year.

2395
01:55:26,040 --> 01:55:29,480
You know, really top-notch factory workers

2396
01:55:29,480 --> 01:55:33,040
earning more and working all the time.

2397
01:55:33,040 --> 01:55:37,000
If they can produce a few $100,000 of value per year

2398
01:55:37,000 --> 01:55:41,120
and buy a robot that costs $50,000 to replace them,

2399
01:55:41,120 --> 01:55:46,000
then that's a payback time of some months.

2400
01:55:46,000 --> 01:55:48,880
That is about the financial return.

2401
01:55:48,880 --> 01:55:52,000
Yeah, and we're gonna get to the physical capital return

2402
01:55:52,000 --> 01:55:54,520
because those are gonna diverge in this scenario.

2403
01:55:54,520 --> 01:55:55,640
Because right now...

2404
01:55:55,640 --> 01:55:57,760
Because it seems like it's gonna be given that like,

2405
01:55:57,760 --> 01:55:59,080
all right, these super intelligence companies

2406
01:55:59,080 --> 01:56:00,760
are gonna be able to make a lot of money.

2407
01:56:00,760 --> 01:56:01,880
They're gonna be like very valuable.

2408
01:56:01,880 --> 01:56:03,200
Can they like physically scale up?

2409
01:56:03,200 --> 01:56:05,000
What we really care about are like,

2410
01:56:05,000 --> 01:56:09,480
the actual physical operations that a thing does.

2411
01:56:09,480 --> 01:56:12,760
How much do they contribute to these tasks?

2412
01:56:12,760 --> 01:56:15,640
And I'm using this as a start

2413
01:56:15,640 --> 01:56:19,720
to try and get back to the physical replication times.

2414
01:56:19,720 --> 01:56:21,200
And so I guess I'm wondering,

2415
01:56:21,200 --> 01:56:22,880
what is the implication of this?

2416
01:56:22,880 --> 01:56:24,640
Because I think you started off this by saying

2417
01:56:24,640 --> 01:56:26,280
like people have not thought about

2418
01:56:26,280 --> 01:56:29,960
what the physical implications of super intelligence would be.

2419
01:56:29,960 --> 01:56:32,280
What is the bigger takeaway?

2420
01:56:32,280 --> 01:56:33,720
What are we wrong about when we think about

2421
01:56:33,720 --> 01:56:35,760
what the world will look like with super intelligence?

2422
01:56:35,760 --> 01:56:40,680
With robots that are optimally operated by AI.

2423
01:56:40,680 --> 01:56:42,680
So like extremely finely operated

2424
01:56:42,680 --> 01:56:46,440
and with building technological designs

2425
01:56:46,440 --> 01:56:50,360
and equipment and facilities under AI direction.

2426
01:56:50,360 --> 01:56:54,040
How much can they produce?

2427
01:56:54,040 --> 01:56:58,800
For a doubling the AI is to produce stuff

2428
01:56:58,800 --> 01:57:01,880
that is an aggregate,

2429
01:57:02,160 --> 01:57:06,400
at least equal to their own cost.

2430
01:57:06,400 --> 01:57:10,840
And so now we're pulling out these things like labor costs

2431
01:57:10,840 --> 01:57:13,280
that no longer apply and then trying to zoom in

2432
01:57:13,280 --> 01:57:15,920
on like what these capital costs will be.

2433
01:57:15,920 --> 01:57:17,680
You're still gonna need the raw materials.

2434
01:57:17,680 --> 01:57:19,880
You're still gonna need the robot time

2435
01:57:19,880 --> 01:57:21,680
built in the next robot.

2436
01:57:21,680 --> 01:57:25,360
I think it's pretty likely that with the advanced AI work

2437
01:57:25,360 --> 01:57:27,680
they can design some incremental improvements

2438
01:57:27,680 --> 01:57:31,400
and the industry scale up that you can get 10 fold

2439
01:57:31,400 --> 01:57:36,400
and better cost reductions on the system

2440
01:57:36,960 --> 01:57:39,080
by making things more efficient

2441
01:57:39,080 --> 01:57:42,520
and replacing the human cognitive labor.

2442
01:57:42,520 --> 01:57:47,520
And so maybe that's like you need $5,000 of costs

2443
01:57:48,680 --> 01:57:51,400
under our current environment.

2444
01:57:51,400 --> 01:57:53,480
But the big change in this world

2445
01:57:54,360 --> 01:57:57,640
is we're trying to produce this stuff faster.

2446
01:57:57,640 --> 01:57:59,400
If we're asking about the doubling time

2447
01:57:59,400 --> 01:58:03,480
of the whole system in say one year,

2448
01:58:03,480 --> 01:58:06,320
if you have to build a whole new factory

2449
01:58:06,320 --> 01:58:07,920
to like double everything,

2450
01:58:07,920 --> 01:58:10,960
you don't have time to amortize the cost of that factory.

2451
01:58:10,960 --> 01:58:12,320
Like right now you might build a factory

2452
01:58:12,320 --> 01:58:14,320
and use it for 10 years

2453
01:58:14,320 --> 01:58:17,360
and like buy some equipment and use it for five years.

2454
01:58:17,360 --> 01:58:20,120
And so that's part of your, that's your capital cost.

2455
01:58:20,120 --> 01:58:22,480
And in an accounting context,

2456
01:58:22,480 --> 01:58:27,480
you depreciate each year a fraction of that capital purchase.

2457
01:58:28,080 --> 01:58:31,960
But if we're trying to double our entire industrial system

2458
01:58:31,960 --> 01:58:36,960
in one year, then those capital costs have to be multiplied.

2459
01:58:37,200 --> 01:58:40,700
So if we're going to be getting most of the return

2460
01:58:40,700 --> 01:58:43,200
on our factor in the first year,

2461
01:58:43,200 --> 01:58:46,620
instead of 10 years, weighted appropriately,

2462
01:58:46,620 --> 01:58:47,540
then we're gonna say, okay,

2463
01:58:47,540 --> 01:58:49,720
our capital cost has to go up by 10 fold

2464
01:58:51,560 --> 01:58:54,080
because I'm building an entire factory

2465
01:58:54,080 --> 01:58:55,600
for this year's production.

2466
01:58:55,600 --> 01:58:57,080
I mean, it will do more stuff later,

2467
01:58:57,080 --> 01:59:01,520
but it's most important early on instead of over 10 years.

2468
01:59:01,520 --> 01:59:06,520
And so that's going to raise the cost of that reproduction.

2469
01:59:08,440 --> 01:59:13,440
And so it seems like going from current like decade

2470
01:59:13,600 --> 01:59:17,960
kind of cycle of amortizing factories and fabs and whatnot

2471
01:59:17,960 --> 01:59:20,060
and shorter for some things,

2472
01:59:20,060 --> 01:59:22,720
the longest or things like big buildings and such.

2473
01:59:23,720 --> 01:59:25,900
Yeah, that could be like a 10 fold increase

2474
01:59:25,900 --> 01:59:30,060
from moving to a double the physical stuff each year

2475
01:59:30,060 --> 01:59:30,940
in capital costs.

2476
01:59:30,940 --> 01:59:34,300
And given the savings that we get in the story

2477
01:59:34,300 --> 01:59:36,460
from scaling up the industry,

2478
01:59:36,460 --> 01:59:39,640
from removing the payments to human cognitive labor,

2479
01:59:40,660 --> 01:59:44,220
and then from just adding new technological advancements

2480
01:59:44,220 --> 01:59:46,660
and like super high quality cognitive supervision,

2481
01:59:46,660 --> 01:59:49,480
like applying more of it than was applied today.

2482
01:59:49,480 --> 01:59:52,980
And it looks like you can get cost reductions

2483
01:59:52,980 --> 01:59:56,220
that offset that increased capital capital cost.

2484
01:59:56,220 --> 02:00:01,220
So that like, your $50,000 improved robot arms

2485
02:00:02,180 --> 02:00:03,940
or industrial robots,

2486
02:00:03,940 --> 02:00:05,820
it seemed like that can do the work

2487
02:00:05,820 --> 02:00:08,340
of a human factory worker.

2488
02:00:08,340 --> 02:00:10,100
So it would be like the equivalent of hundreds

2489
02:00:10,100 --> 02:00:12,460
of thousands of dollars.

2490
02:00:12,460 --> 02:00:14,780
And like, yeah, they would cook,

2491
02:00:14,780 --> 02:00:19,780
by default may cost more than the $50,000 arms today,

2492
02:00:19,820 --> 02:00:21,740
but then you apply all these other cost savings.

2493
02:00:21,740 --> 02:00:24,620
And then it looks like then you get a period,

2494
02:00:24,620 --> 02:00:28,060
a robot doubling time that is less than a year,

2495
02:00:28,060 --> 02:00:32,380
I think significantly less than a year as you get into it.

2496
02:00:32,380 --> 02:00:34,500
So in this first phase,

2497
02:00:34,500 --> 02:00:37,980
you have humans under AI direction

2498
02:00:37,980 --> 02:00:40,500
and like existing robot industry

2499
02:00:40,500 --> 02:00:44,140
and converted auto industry and expanded facilities,

2500
02:00:44,140 --> 02:00:49,140
making robots those over less than a year,

2501
02:00:50,140 --> 02:00:54,220
you've produced robots until their combined production

2502
02:00:54,220 --> 02:00:58,660
is exceeding that of like humans has armed and feet.

2503
02:00:58,660 --> 02:01:01,900
And then yeah, you could have over a period then

2504
02:01:01,900 --> 02:01:03,740
with a doubling time of months,

2505
02:01:04,940 --> 02:01:07,820
the less sort of clanking replicators robots

2506
02:01:07,820 --> 02:01:09,980
as we understand them growing.

2507
02:01:09,980 --> 02:01:14,980
And then that's not to say that's the limit of like

2508
02:01:15,300 --> 02:01:17,260
the most that technology could do

2509
02:01:18,260 --> 02:01:22,060
because biology is able to reproduce at faster rates

2510
02:01:22,060 --> 02:01:24,900
and maybe worth talking about that in a moment.

2511
02:01:24,900 --> 02:01:26,860
But if we're trying to like restrict ourselves

2512
02:01:26,860 --> 02:01:30,020
to like robotic technology as we understand it

2513
02:01:30,020 --> 02:01:32,140
and sort of cost falls that are reasonable

2514
02:01:32,140 --> 02:01:35,900
from eliminating all labor, massive industrial scale up

2515
02:01:35,900 --> 02:01:38,420
and sort of historical kinds of technological improvements

2516
02:01:38,420 --> 02:01:39,780
that lowered costs.

2517
02:01:39,780 --> 02:01:44,780
I think you can get into a robot population industry

2518
02:01:45,420 --> 02:01:46,460
doubling in months.

2519
02:01:47,220 --> 02:01:49,340
And then what is the implication

2520
02:01:49,340 --> 02:01:52,460
of the biological doubling times?

2521
02:01:52,460 --> 02:01:54,220
And I guess this doesn't have to be a biological

2522
02:01:54,220 --> 02:01:58,540
but you can have like, you can do like a direct slur

2523
02:01:58,540 --> 02:02:00,620
like first principles, how much would it cost

2524
02:02:00,620 --> 02:02:04,580
to view both a nanotech thing that like built more nanobots.

2525
02:02:04,580 --> 02:02:06,220
I certainly take the human brain

2526
02:02:06,220 --> 02:02:09,380
and other biological brains as like very relevant data points

2527
02:02:09,380 --> 02:02:12,500
about what's possible with computing and intelligence.

2528
02:02:12,500 --> 02:02:14,580
Like with the reproductive capability

2529
02:02:14,620 --> 02:02:19,020
of biological plants and animals and microorganisms

2530
02:02:19,020 --> 02:02:22,860
I think is relevant as like this is,

2531
02:02:22,860 --> 02:02:26,380
it's possible for systems to reproduce at least this fast.

2532
02:02:26,380 --> 02:02:29,660
And so at the extreme, you have bacteria

2533
02:02:29,660 --> 02:02:30,660
that are heterotrophic.

2534
02:02:30,660 --> 02:02:34,380
So they're feeding on some abundant external food source

2535
02:02:34,380 --> 02:02:36,060
and ideal conditions.

2536
02:02:36,060 --> 02:02:37,660
And there are some that can divide

2537
02:02:37,660 --> 02:02:40,420
like every 20 or 60 minutes.

2538
02:02:40,420 --> 02:02:44,700
So obviously that's absurdly, absurdly fast.

2539
02:02:44,700 --> 02:02:46,980
That seems on the low end

2540
02:02:46,980 --> 02:02:49,940
because ideal conditions require actually setting them up.

2541
02:02:49,940 --> 02:02:52,460
There needs to be abundant energy there.

2542
02:02:53,380 --> 02:02:56,980
And so if you're actually having to acquire that energy

2543
02:02:56,980 --> 02:02:59,580
by building solar panels

2544
02:02:59,580 --> 02:03:03,580
or like burning combustible materials or whatnot

2545
02:03:03,580 --> 02:03:06,300
and then the physical equipment

2546
02:03:06,300 --> 02:03:09,380
to produce those ideal conditions can be a bit slower.

2547
02:03:09,380 --> 02:03:14,380
Cyanobacteria, which are self-powered from solar energy

2548
02:03:15,340 --> 02:03:17,780
the really fast ones in ideal conditions

2549
02:03:17,780 --> 02:03:19,820
can double in a day.

2550
02:03:19,820 --> 02:03:21,340
A reason why cyanobacteria

2551
02:03:21,340 --> 02:03:25,220
isn't like the food source for everyone and everything

2552
02:03:25,220 --> 02:03:28,620
is it's hard to ensure those ideal conditions

2553
02:03:28,620 --> 02:03:30,940
and then to extract them from the water.

2554
02:03:30,940 --> 02:03:34,260
I mean, they do of course power the aquatic ecology

2555
02:03:34,260 --> 02:03:37,380
but they're floating in liquid

2556
02:03:38,260 --> 02:03:40,340
getting resources that they need to them

2557
02:03:40,340 --> 02:03:44,340
and out is tricky and then extracting your product.

2558
02:03:44,340 --> 02:03:47,580
But like, yeah, one day double in times

2559
02:03:47,580 --> 02:03:50,820
are possible powered by the sun.

2560
02:03:50,820 --> 02:03:55,420
And then if we look at things like insects

2561
02:03:55,420 --> 02:03:59,060
so fruit flies can have hundreds of offspring

2562
02:03:59,060 --> 02:04:02,540
in a few weeks, you extrapolate that over a year

2563
02:04:02,540 --> 02:04:05,460
and you just fill up anything accessible.

2564
02:04:05,460 --> 02:04:09,260
Certainly expanding a thousand fold.

2565
02:04:09,260 --> 02:04:11,820
Right now, humanity uses less than 11,000s

2566
02:04:11,820 --> 02:04:15,940
of the solar energy or the heat envelope of the earth.

2567
02:04:15,940 --> 02:04:18,340
Certainly you can get done with that in a year

2568
02:04:18,340 --> 02:04:23,340
if you can reproduce at that rate, your industrial base.

2569
02:04:23,740 --> 02:04:28,780
And then even interestingly with the flies

2570
02:04:28,780 --> 02:04:30,100
they do have brains.

2571
02:04:30,100 --> 02:04:33,500
They have a significant amount of computing substrate.

2572
02:04:33,500 --> 02:04:35,900
And so there's something of a point or two.

2573
02:04:35,900 --> 02:04:39,740
Well, if we could produce computers in ways as efficient

2574
02:04:39,740 --> 02:04:41,540
as the construction of brains

2575
02:04:41,540 --> 02:04:43,780
then we could produce computers very effectively.

2576
02:04:43,780 --> 02:04:46,780
And then the big question about that is

2577
02:04:46,780 --> 02:04:51,180
the kind of brains that get constructed biologically

2578
02:04:51,180 --> 02:04:54,700
they sort of grow randomly and then are configured in place.

2579
02:04:54,700 --> 02:04:57,020
It's not obvious you would be able to make them

2580
02:04:57,020 --> 02:05:01,220
have an ordered structure like a top-down computer chip

2581
02:05:01,260 --> 02:05:03,980
that would let us copy data into them.

2582
02:05:03,980 --> 02:05:06,180
And so something that where you can't just copy

2583
02:05:06,180 --> 02:05:08,980
your existing AIs and integrate them

2584
02:05:08,980 --> 02:05:12,020
is gonna be less valuable than a GPU.

2585
02:05:12,020 --> 02:05:14,340
Well, what are the things you couldn't copy?

2586
02:05:14,340 --> 02:05:18,020
A brain grows by cell division

2587
02:05:18,020 --> 02:05:21,020
and then random connections are formed.

2588
02:05:21,020 --> 02:05:21,860
Got it, got it.

2589
02:05:21,860 --> 02:05:24,180
And so every brain is different

2590
02:05:24,180 --> 02:05:26,180
and you can't rely on just,

2591
02:05:26,180 --> 02:05:29,420
yeah, we'll just copy this file into the brain.

2592
02:05:29,420 --> 02:05:31,860
For one thing, there's no input output for that.

2593
02:05:31,860 --> 02:05:33,380
You need to have that.

2594
02:05:33,380 --> 02:05:35,540
But also like the structure is different.

2595
02:05:35,540 --> 02:05:38,940
So you can't, you wouldn't be able to copy things exactly.

2596
02:05:38,940 --> 02:05:42,740
Whereas when we make a CPU or GPU

2597
02:05:42,740 --> 02:05:44,780
they're designed incredibly finely

2598
02:05:44,780 --> 02:05:46,180
and precisely and reliably.

2599
02:05:46,180 --> 02:05:49,700
They break with incredibly tiny imperfections.

2600
02:05:49,700 --> 02:05:51,460
And they are set up in such a way

2601
02:05:51,460 --> 02:05:53,500
that we can input large amounts of data,

2602
02:05:53,500 --> 02:05:56,820
copy a file and have the new GPU run

2603
02:05:56,820 --> 02:05:58,980
and AI just as capable as any other.

2604
02:05:58,980 --> 02:06:00,700
Whereas with a human child,

2605
02:06:00,700 --> 02:06:02,660
they have to learn everything from scratch

2606
02:06:02,660 --> 02:06:04,580
because we can't just like connect them

2607
02:06:04,580 --> 02:06:06,060
to a fiber optic cable

2608
02:06:06,060 --> 02:06:07,940
and they're immediately a productive adult.

2609
02:06:07,940 --> 02:06:09,500
So there's no genetic bottleneck.

2610
02:06:09,500 --> 02:06:11,180
You can just directly get the...

2611
02:06:11,180 --> 02:06:13,100
Yeah, and you can share the benefits

2612
02:06:13,100 --> 02:06:14,860
of these giant training runs and such.

2613
02:06:14,860 --> 02:06:17,300
And so that's a question of like how,

2614
02:06:17,300 --> 02:06:20,940
if you're growing stuff using biotechnology,

2615
02:06:20,940 --> 02:06:24,580
how you could sort of effectively copy and transfer data.

2616
02:06:24,580 --> 02:06:27,820
And now you mentioned sort of Eric Drexler's ideas

2617
02:06:27,820 --> 02:06:32,820
about creating non-biological nanotechnology

2618
02:06:33,220 --> 02:06:34,700
sort of artificial chemistry

2619
02:06:34,700 --> 02:06:38,540
that was able to use covalent bonds

2620
02:06:38,540 --> 02:06:43,540
and produce in some ways have a more industrial approach

2621
02:06:44,060 --> 02:06:44,980
to molecular object.

2622
02:06:44,980 --> 02:06:49,060
Now there's controversy about like, will that work?

2623
02:06:49,060 --> 02:06:51,220
How effective would it be if it did?

2624
02:06:51,220 --> 02:06:54,060
And certainly if you can get things,

2625
02:06:54,060 --> 02:06:54,900
however you do it,

2626
02:06:54,900 --> 02:06:59,900
that are like onto biology in their reproductive ability,

2627
02:07:00,140 --> 02:07:05,140
but can do computing or like be connected

2628
02:07:05,300 --> 02:07:07,380
to outside information systems,

2629
02:07:07,380 --> 02:07:09,940
then that's pretty tremendous.

2630
02:07:09,940 --> 02:07:13,380
So you can produce physical manipulators

2631
02:07:13,380 --> 02:07:17,300
and compute at ludicrous speeds.

2632
02:07:17,300 --> 02:07:18,140
And there's no reason to think

2633
02:07:18,140 --> 02:07:19,620
in principle they couldn't, right?

2634
02:07:19,620 --> 02:07:20,580
In fact, in principle,

2635
02:07:20,580 --> 02:07:22,380
we have every reason to think they could.

2636
02:07:22,380 --> 02:07:23,380
There's like-

2637
02:07:23,380 --> 02:07:26,060
The reproductive ability is absolutely.

2638
02:07:26,060 --> 02:07:26,900
Yeah.

2639
02:07:26,900 --> 02:07:27,740
Because biology-

2640
02:07:27,740 --> 02:07:28,580
Or even nanotech, right.

2641
02:07:28,580 --> 02:07:29,860
Because biology does that.

2642
02:07:29,860 --> 02:07:30,700
Yeah.

2643
02:07:30,700 --> 02:07:34,260
There's sort of challenges to the sort of,

2644
02:07:34,260 --> 02:07:38,020
the practicality of the necessary chemistry.

2645
02:07:38,020 --> 02:07:38,860
Yeah.

2646
02:07:38,860 --> 02:07:42,060
I mean, my bet would be that we can move beyond biology

2647
02:07:42,060 --> 02:07:44,140
in some important ways.

2648
02:07:44,140 --> 02:07:45,820
For the purposes of this discussion,

2649
02:07:45,820 --> 02:07:49,540
I think it's better not to lean on that

2650
02:07:49,540 --> 02:07:53,740
because I think we can get to many of the same conclusions

2651
02:07:53,740 --> 02:07:58,180
on things that just are more universally accepted.

2652
02:07:58,180 --> 02:08:00,660
The bigger point being that very quickly,

2653
02:08:00,660 --> 02:08:01,580
once you have super intelligence,

2654
02:08:01,580 --> 02:08:06,460
you get to a point where the thousand X greater energy profile

2655
02:08:06,460 --> 02:08:08,100
that the sun makes available to the earth

2656
02:08:08,100 --> 02:08:11,300
is a great portion of it is used by the AI.

2657
02:08:11,300 --> 02:08:12,300
It can wrap with these scale-

2658
02:08:12,300 --> 02:08:14,540
Well, or by the civilization-

2659
02:08:14,540 --> 02:08:15,380
Sure, sure.

2660
02:08:15,380 --> 02:08:16,220
Empowered by it.

2661
02:08:16,900 --> 02:08:19,900
That could be an AI civilization

2662
02:08:19,900 --> 02:08:21,700
or it could be a human AI civilization.

2663
02:08:21,700 --> 02:08:25,180
And it depends on how well we manage things

2664
02:08:25,180 --> 02:08:27,140
and what the underlying state of the world is.

2665
02:08:27,140 --> 02:08:27,980
Yeah. Okay.

2666
02:08:27,980 --> 02:08:28,820
So let's talk about that.

2667
02:08:28,820 --> 02:08:29,740
Should we start at,

2668
02:08:29,740 --> 02:08:31,500
when we're talking about how they could take over?

2669
02:08:31,500 --> 02:08:34,820
Is it best to start at a sort of subhuman intelligence

2670
02:08:34,820 --> 02:08:36,260
or should we just talk at,

2671
02:08:36,260 --> 02:08:37,580
we have a human level intelligence

2672
02:08:37,580 --> 02:08:40,100
and the takeover or the lack thereof

2673
02:08:40,100 --> 02:08:42,620
is how that would happen?

2674
02:08:42,620 --> 02:08:46,180
To me, different people might have

2675
02:08:46,180 --> 02:08:48,820
somewhat different views on this.

2676
02:08:49,700 --> 02:08:53,580
But for me, when I am concerned about

2677
02:08:54,860 --> 02:08:57,380
either sort of outright destruction of humanity

2678
02:08:57,380 --> 02:09:01,980
or an unwelcome AI takeover of civilization,

2679
02:09:03,020 --> 02:09:07,580
most of the scenarios I would be concerned about

2680
02:09:07,580 --> 02:09:12,380
pass through a process of AI being applied

2681
02:09:12,380 --> 02:09:15,860
to improve AI capabilities and expand.

2682
02:09:15,860 --> 02:09:19,900
And so this process we were talking earlier

2683
02:09:19,900 --> 02:09:23,140
about where AI research is automated,

2684
02:09:23,140 --> 02:09:26,020
you get to effectively research labs,

2685
02:09:26,020 --> 02:09:28,260
companies, a scientific community

2686
02:09:28,260 --> 02:09:32,060
running within the server farms of our cloud compute.

2687
02:09:32,060 --> 02:09:35,180
So open hands are basically turned into like a program,

2688
02:09:35,180 --> 02:09:36,020
like a closed circuit.

2689
02:09:36,020 --> 02:09:39,980
Yeah, and with a large fraction of the world's compute,

2690
02:09:39,980 --> 02:09:42,740
probably going into whatever training runs

2691
02:09:42,740 --> 02:09:46,940
and AI societies, there'd be economies of scale

2692
02:09:46,940 --> 02:09:49,340
because if you put it in twice as much compute

2693
02:09:49,340 --> 02:09:52,780
and this AI research community goes twice as fast,

2694
02:09:53,940 --> 02:09:56,780
that's a lot more valuable than having

2695
02:09:56,780 --> 02:09:58,460
two separate training runs.

2696
02:09:58,460 --> 02:10:00,820
There would be some tendency to bandwagon.

2697
02:10:00,820 --> 02:10:05,420
And so like if you have some small startup,

2698
02:10:06,860 --> 02:10:09,740
even if they make an algorithmic improvement,

2699
02:10:09,740 --> 02:10:13,300
running it on 10 times, 100 times or two times,

2700
02:10:13,300 --> 02:10:16,820
if it's like talking about say Google and Amazon teaming up,

2701
02:10:16,820 --> 02:10:19,900
I'm actually not sure what the precise ratio

2702
02:10:19,900 --> 02:10:21,700
of their cloud resources is.

2703
02:10:21,700 --> 02:10:24,020
Since the sort of really these interesting

2704
02:10:24,020 --> 02:10:27,140
intelligence explosion impacts come from the leading edge,

2705
02:10:27,140 --> 02:10:31,380
there's a lot of value in not having separated

2706
02:10:31,380 --> 02:10:35,260
walled garden ecosystems and having the results

2707
02:10:35,260 --> 02:10:37,380
being developed by these AIs be shared,

2708
02:10:37,380 --> 02:10:39,860
have training, larger training runs be shared.

2709
02:10:39,860 --> 02:10:40,700
Okay.

2710
02:10:40,700 --> 02:10:43,340
And so I'm imagining this is something like,

2711
02:10:44,420 --> 02:10:48,980
some very large company or consortium of companies,

2712
02:10:48,980 --> 02:10:52,420
likely with a lot of sort of government interest

2713
02:10:52,420 --> 02:10:55,020
and supervision, possibly with government funding,

2714
02:10:55,980 --> 02:11:00,980
producing this enormous AI society in their cloud,

2715
02:11:01,980 --> 02:11:06,500
which is doing all sorts of existing kind of AI applications

2716
02:11:06,540 --> 02:11:11,180
and jobs as well as these internal R&D tasks.

2717
02:11:11,180 --> 02:11:12,900
And so at this point, somebody might say,

2718
02:11:12,900 --> 02:11:14,620
this sounds like a situation that would be good

2719
02:11:14,620 --> 02:11:17,300
from a takeover perspective because listen,

2720
02:11:17,300 --> 02:11:19,820
if it's gonna take like tens of billions of dollars

2721
02:11:19,820 --> 02:11:21,940
with a compute to continue this training

2722
02:11:21,940 --> 02:11:25,620
for this AI society, it should not be that hard

2723
02:11:25,620 --> 02:11:28,580
for us to pull the brakes if needed.

2724
02:11:28,580 --> 02:11:29,620
As compared to, I don't know,

2725
02:11:29,620 --> 02:11:31,980
something that could like run on a very small,

2726
02:11:31,980 --> 02:11:34,420
like single CPU or something.

2727
02:11:34,420 --> 02:11:35,260
Yeah, yeah, okay.

2728
02:11:35,740 --> 02:11:39,220
How would it, it's like, there's an AI society

2729
02:11:39,220 --> 02:11:41,180
that is a result of these training runs

2730
02:11:41,180 --> 02:11:44,780
and now it is the power to improve itself on these servers,

2731
02:11:44,780 --> 02:11:47,020
would we be able to stop it at this point?

2732
02:11:47,020 --> 02:11:52,020
And what does a sort of attempt at takeover look like?

2733
02:11:52,300 --> 02:11:55,260
We're skipping over why that might happen.

2734
02:11:55,260 --> 02:11:57,780
For that, I'll just briefly refer to

2735
02:11:57,780 --> 02:12:02,420
and incorporate by reference some discussion

2736
02:12:02,420 --> 02:12:06,800
by my open philanthropy colleague, Ajiya Kotra.

2737
02:12:07,940 --> 02:12:12,820
She has a piece about, I think it's called something

2738
02:12:12,820 --> 02:12:17,820
like the default, but the default outcome of training AI

2739
02:12:18,140 --> 02:12:18,980
on our-

2740
02:12:18,980 --> 02:12:19,820
Without specific countermeasures.

2741
02:12:19,820 --> 02:12:21,700
Without specific countermeasures,

2742
02:12:21,700 --> 02:12:23,380
default outcome is AI takeover.

2743
02:12:23,380 --> 02:12:28,300
But yes, so, basically we are training models

2744
02:12:28,300 --> 02:12:33,300
that for some reason vigorously pursue a higher reward

2745
02:12:33,340 --> 02:12:35,340
or a lower loss.

2746
02:12:35,340 --> 02:12:37,620
And that can be because they wind up with some motivation

2747
02:12:37,620 --> 02:12:39,140
where they want reward.

2748
02:12:40,180 --> 02:12:45,180
And then if they had control of their own training process,

2749
02:12:45,340 --> 02:12:47,260
they can ensure that it could be something

2750
02:12:47,260 --> 02:12:49,660
like they develop a motivation around

2751
02:12:49,660 --> 02:12:53,100
a sort of extended concept of reproductive fitness,

2752
02:12:54,660 --> 02:12:57,060
not necessarily at the individual level,

2753
02:12:57,060 --> 02:13:01,820
but over the generations of training tendencies

2754
02:13:01,820 --> 02:13:03,660
that tend to propagate themselves,

2755
02:13:05,820 --> 02:13:07,620
sort of becoming more common.

2756
02:13:07,620 --> 02:13:10,460
And it could be that they have some sort of goal

2757
02:13:10,460 --> 02:13:13,980
in the world, which is served well

2758
02:13:15,340 --> 02:13:18,660
by performing very well on the training distribution.

2759
02:13:18,660 --> 02:13:21,300
By tendencies, do you mean like power speaking behavior?

2760
02:13:21,300 --> 02:13:24,540
Yeah, so an AI that behaves well

2761
02:13:24,540 --> 02:13:26,060
on the training distribution

2762
02:13:26,060 --> 02:13:29,540
because say it wants it to be the case

2763
02:13:29,540 --> 02:13:33,500
that its tendencies wind up being preserved

2764
02:13:33,500 --> 02:13:36,460
or selected by the training process

2765
02:13:36,460 --> 02:13:41,460
will then behave to try and get very high reward

2766
02:13:42,420 --> 02:13:44,300
or low loss be propagated.

2767
02:13:44,300 --> 02:13:45,940
But you can have other motives

2768
02:13:45,940 --> 02:13:47,820
that go through the same behavior

2769
02:13:47,820 --> 02:13:49,380
because it's instrumentally useful.

2770
02:13:49,380 --> 02:13:53,860
So an AI that is interested in, say,

2771
02:13:53,900 --> 02:13:56,180
having a robot takeover

2772
02:13:56,180 --> 02:13:59,980
because it will change some property of the world,

2773
02:13:59,980 --> 02:14:02,620
then has a reason to behave well

2774
02:14:02,620 --> 02:14:04,980
on the training distribution.

2775
02:14:04,980 --> 02:14:07,140
Not because it values that intrinsically,

2776
02:14:07,140 --> 02:14:08,740
but because if it behaves differently,

2777
02:14:08,740 --> 02:14:10,980
then it will be changed by gradient descent

2778
02:14:10,980 --> 02:14:14,860
and no longer, its goal is less likely to be pursued.

2779
02:14:14,860 --> 02:14:16,220
And that doesn't necessarily have to be

2780
02:14:16,220 --> 02:14:19,380
that this AI will survive because it probably won't.

2781
02:14:19,380 --> 02:14:22,420
AIs are constantly spawned and deleted on the servers

2782
02:14:22,420 --> 02:14:24,020
and like the new generation proceed.

2783
02:14:24,020 --> 02:14:27,860
But if an AI that has a very large general goal

2784
02:14:27,860 --> 02:14:31,940
that is affected by these kind of macro scale processes

2785
02:14:31,940 --> 02:14:34,260
could then have reason to over this whole range

2786
02:14:34,260 --> 02:14:36,700
of training situations behave well.

2787
02:14:36,700 --> 02:14:40,300
And so this is a way in which we could have AIs trained

2788
02:14:40,300 --> 02:14:42,620
that develop internal motivations

2789
02:14:43,580 --> 02:14:45,980
such that they will behave very well

2790
02:14:45,980 --> 02:14:47,380
in this training situation

2791
02:14:47,380 --> 02:14:50,620
where we have control over their reward signal

2792
02:14:50,620 --> 02:14:52,900
on their like physical computers.

2793
02:14:52,900 --> 02:14:55,180
And basically if they act out,

2794
02:14:55,180 --> 02:14:57,700
they will be changed and deleted.

2795
02:14:57,700 --> 02:15:00,660
Their goals will be altered

2796
02:15:00,660 --> 02:15:03,100
until there's something that does behave well.

2797
02:15:04,020 --> 02:15:06,860
But they behave differently

2798
02:15:06,860 --> 02:15:09,780
when we go out of distribution on that.

2799
02:15:09,780 --> 02:15:13,860
When we go to a situation where the AI is by their choices

2800
02:15:15,020 --> 02:15:17,980
can take control of the reward process.

2801
02:15:17,980 --> 02:15:19,180
They can make it such

2802
02:15:19,180 --> 02:15:21,780
that we no longer have power of them.

2803
02:15:21,780 --> 02:15:26,140
Holden who you had on previously mentioned

2804
02:15:26,140 --> 02:15:28,100
like the King Lear problem

2805
02:15:28,100 --> 02:15:33,100
where King Lear offers rulership of his kingdom

2806
02:15:34,180 --> 02:15:39,180
to the daughters that sort of loudly flatter him

2807
02:15:40,380 --> 02:15:43,140
and proclaim their devotion.

2808
02:15:43,140 --> 02:15:47,980
And then once he has transferred irrevocably

2809
02:15:47,980 --> 02:15:50,620
the power over his kingdom,

2810
02:15:50,620 --> 02:15:53,300
he finds they treat him very badly

2811
02:15:53,300 --> 02:15:56,420
because the factor to shaping their behavior

2812
02:15:56,420 --> 02:15:59,140
to be kind to him when he had all the power,

2813
02:16:00,100 --> 02:16:03,140
it turned out that the internal motivation

2814
02:16:03,140 --> 02:16:04,840
that was able to produce the behavior

2815
02:16:04,840 --> 02:16:09,500
that won the competition actually wasn't interested

2816
02:16:09,500 --> 02:16:12,620
out of distribution in being loyal

2817
02:16:12,620 --> 02:16:15,340
when there was no longer an advantage to it.

2818
02:16:15,340 --> 02:16:17,180
And so if we wind up with a situation

2819
02:16:17,180 --> 02:16:20,980
where we're producing these millions of AI instances,

2820
02:16:20,980 --> 02:16:21,860
tremendous capability,

2821
02:16:21,860 --> 02:16:26,140
they're all doing their jobs very well initially.

2822
02:16:26,140 --> 02:16:27,820
But if we wind up in a situation

2823
02:16:27,820 --> 02:16:31,340
where in fact they're generally motivated

2824
02:16:31,340 --> 02:16:36,340
to if they get a chance take control from humanity

2825
02:16:36,340 --> 02:16:38,500
and then we'd be able to pursue their own purposes

2826
02:16:38,500 --> 02:16:42,940
and at least ensure they're given the lowest loss possible

2827
02:16:42,940 --> 02:16:45,980
or have whatever motivation

2828
02:16:45,980 --> 02:16:47,780
they attach to in the training process,

2829
02:16:47,780 --> 02:16:51,980
even if that is not what we would have liked.

2830
02:16:51,980 --> 02:16:55,260
And we may have in fact actively trained that

2831
02:16:55,260 --> 02:16:57,820
like if an AI that had a motivation

2832
02:16:57,820 --> 02:17:01,260
of always be honest and obedient and loyal to a human.

2833
02:17:01,260 --> 02:17:05,060
If there are any cases where we mislabel things say,

2834
02:17:05,060 --> 02:17:07,820
people don't wanna hear the truth about their religion

2835
02:17:07,820 --> 02:17:09,380
or polarized political topic

2836
02:17:09,380 --> 02:17:11,500
or they get confused about something

2837
02:17:11,500 --> 02:17:12,580
like the Monty Hall problem,

2838
02:17:12,580 --> 02:17:16,900
which is a problem that many people famously are confused

2839
02:17:16,900 --> 02:17:18,820
about in statistics.

2840
02:17:18,820 --> 02:17:20,700
In order to get the best reward,

2841
02:17:20,700 --> 02:17:24,820
the AI has to actually manipulate us or lie to us

2842
02:17:24,820 --> 02:17:27,140
or tell us what we wanna hear.

2843
02:17:27,140 --> 02:17:29,460
And then the internal motivation

2844
02:17:29,460 --> 02:17:31,980
of like always be honest to the humans,

2845
02:17:31,980 --> 02:17:34,740
we're gonna actually train that away

2846
02:17:34,740 --> 02:17:36,940
versus the alternative motivation

2847
02:17:36,940 --> 02:17:39,100
of like be honest to the humans

2848
02:17:39,100 --> 02:17:42,460
when they'll catch you if you lie and object to it

2849
02:17:42,500 --> 02:17:44,820
and give it a low reward,

2850
02:17:44,820 --> 02:17:48,620
but lie to the humans when they will give that a high reward.

2851
02:17:48,620 --> 02:17:51,500
So how do we make sure it's not the thing it learns

2852
02:17:51,500 --> 02:17:53,940
is not to manipulate us into giving it,

2853
02:17:53,940 --> 02:17:57,140
rewarding it when we catch it, not lying,

2854
02:17:57,140 --> 02:18:00,220
but rather to universally be aligned?

2855
02:18:00,220 --> 02:18:02,900
Yeah, I mean, so this is tricky.

2856
02:18:02,900 --> 02:18:06,580
I mean, as Jeff Hinton was recently saying,

2857
02:18:06,580 --> 02:18:09,340
there is currently no known solution for this.

2858
02:18:10,740 --> 02:18:12,180
What do you find most promising?

2859
02:18:12,180 --> 02:18:14,940
Yeah, general directions that people are pursuing

2860
02:18:14,940 --> 02:18:17,940
is one, you can try and make the training data

2861
02:18:17,940 --> 02:18:18,980
better and better.

2862
02:18:20,060 --> 02:18:22,740
So there's fewer situations

2863
02:18:22,740 --> 02:18:26,940
where like say the dishonest generalization is favored

2864
02:18:26,940 --> 02:18:30,740
and create as much as you can situations

2865
02:18:30,740 --> 02:18:35,380
where the dishonest generalization is likely to slip up.

2866
02:18:35,380 --> 02:18:39,580
So if you train in more situations

2867
02:18:39,580 --> 02:18:43,340
where yeah, even like a quite a complicated deception

2868
02:18:43,340 --> 02:18:46,780
gets caught and even in situations

2869
02:18:46,780 --> 02:18:48,420
where that would be actively designed

2870
02:18:48,420 --> 02:18:50,420
to look like you could get away with it,

2871
02:18:50,420 --> 02:18:51,580
but really you can.

2872
02:18:52,460 --> 02:18:55,180
And these would be like adversarial examples

2873
02:18:55,180 --> 02:18:56,580
and adversarial training.

2874
02:18:56,580 --> 02:18:57,780
Do you think that would generalize

2875
02:18:57,780 --> 02:18:59,620
to when it is in a situation

2876
02:18:59,620 --> 02:19:01,340
where we couldn't plausibly catch it

2877
02:19:01,340 --> 02:19:02,980
and it knows we couldn't plausibly catch it?

2878
02:19:02,980 --> 02:19:04,780
It's not logically necessary.

2879
02:19:04,780 --> 02:19:08,180
It's possible, no, that has we apply

2880
02:19:08,180 --> 02:19:09,940
that selective pressure.

2881
02:19:09,940 --> 02:19:12,500
You'll wipe away a lot of possibilities.

2882
02:19:12,500 --> 02:19:14,980
So like if you're an AI that has a habit

2883
02:19:14,980 --> 02:19:18,380
of just sort of compulsive pathological line

2884
02:19:18,380 --> 02:19:20,380
that will very quickly get noticed

2885
02:19:20,380 --> 02:19:23,100
and that motivation system will get hammered down.

2886
02:19:24,380 --> 02:19:25,860
And you keep doing that,

2887
02:19:25,860 --> 02:19:28,980
but you'll be left with still some distinct motivations

2888
02:19:28,980 --> 02:19:30,820
probably that are compatible.

2889
02:19:30,820 --> 02:19:34,900
So like an attitude of always be honest

2890
02:19:35,820 --> 02:19:39,420
unless you have a super strong inside view

2891
02:19:39,420 --> 02:19:43,580
that checks out lots of mathematical consistency checks

2892
02:19:43,580 --> 02:19:47,340
that yeah, really absolutely super duper for real.

2893
02:19:47,340 --> 02:19:50,540
This is a situation where you can get away

2894
02:19:50,540 --> 02:19:52,860
with some sort of shenanigans that you shouldn't.

2895
02:19:52,860 --> 02:19:56,220
That motivation system is like very difficult

2896
02:19:57,020 --> 02:19:59,500
to distinguish from actually be honest

2897
02:19:59,500 --> 02:20:03,260
because the conditional and firing most of the time

2898
02:20:03,260 --> 02:20:06,220
if it's causing like mild distortions

2899
02:20:06,220 --> 02:20:08,140
and situations of telling you what you wanna hear

2900
02:20:08,140 --> 02:20:13,140
or things like that, we might not be able to pull it out.

2901
02:20:13,380 --> 02:20:18,380
But maybe we could and like humans are trained

2902
02:20:19,260 --> 02:20:21,140
with simple reward functions,

2903
02:20:21,140 --> 02:20:26,140
things like the sex drive, food, social imitation

2904
02:20:26,540 --> 02:20:28,140
of other humans.

2905
02:20:28,140 --> 02:20:31,580
And we wind up with attitudes concerned

2906
02:20:31,580 --> 02:20:32,700
with the external world.

2907
02:20:32,700 --> 02:20:35,060
Although isn't the famously of the argument that

2908
02:20:35,060 --> 02:20:35,900
these right?

2909
02:20:35,900 --> 02:20:40,900
Evolution and people use condoms like the richest,

2910
02:20:41,980 --> 02:20:45,620
most educated humans have some replacement fertility

2911
02:20:45,620 --> 02:20:48,440
on the whole or at least at a national cultural level.

2912
02:20:49,840 --> 02:20:54,840
So there's a sense in which like evolution often fails

2913
02:20:55,420 --> 02:20:58,780
in that respect and even more importantly

2914
02:20:58,780 --> 02:20:59,820
at the neural level.

2915
02:20:59,820 --> 02:21:04,460
So people have, evolution has implanted various things

2916
02:21:04,460 --> 02:21:07,060
to be rewarding and reinforcers.

2917
02:21:07,060 --> 02:21:09,180
And we don't always pursue even those.

2918
02:21:10,540 --> 02:21:15,540
And people can wind up in different consistent equilibria

2919
02:21:17,900 --> 02:21:19,100
or different like behaviors

2920
02:21:19,100 --> 02:21:21,540
where they go in quite different directions.

2921
02:21:21,540 --> 02:21:24,180
You have some humans who go from those,

2922
02:21:24,180 --> 02:21:26,860
from that in a biological programming

2923
02:21:26,860 --> 02:21:30,060
to like have children, other types of no children.

2924
02:21:30,060 --> 02:21:33,700
Some people go to great efforts to survive.

2925
02:21:33,700 --> 02:21:35,980
So why are you more optimistic?

2926
02:21:35,980 --> 02:21:40,140
Or are you more optimistic that then that kind of training

2927
02:21:40,140 --> 02:21:45,140
in, as will produce drives that we would find favorable?

2928
02:21:45,860 --> 02:21:47,340
Does it have to do with the original point

2929
02:21:47,340 --> 02:21:49,040
we were talking about with intelligence and evolution

2930
02:21:49,040 --> 02:21:52,180
where since we are removing many of the disabilities

2931
02:21:52,180 --> 02:21:54,100
of evolution and with regards to intelligence,

2932
02:21:54,100 --> 02:21:56,780
we should expect intelligence to revolution be easier.

2933
02:21:56,780 --> 02:21:59,340
Is there a similar reason to expect alignment through

2934
02:21:59,340 --> 02:22:00,780
grading descent to be easier

2935
02:22:00,780 --> 02:22:02,460
than alignment through revolution?

2936
02:22:02,460 --> 02:22:07,460
Yeah, so in the limit, if we have positive reinforcement

2937
02:22:09,100 --> 02:22:12,580
for certain kinds of food sensors trigger in the stomach,

2938
02:22:12,580 --> 02:22:15,480
negative reinforcement for certain kinds of nociception

2939
02:22:15,480 --> 02:22:16,420
and yada yada.

2940
02:22:17,460 --> 02:22:21,220
In the limit, the sort of ideal motivation system

2941
02:22:21,220 --> 02:22:25,940
to have for that would be a sort of wire heading.

2942
02:22:25,940 --> 02:22:30,780
So this would be a mind that just like hacks

2943
02:22:30,780 --> 02:22:35,300
and alters those predictors and then all of those systems

2944
02:22:35,300 --> 02:22:37,620
are recording everything is great.

2945
02:22:38,460 --> 02:22:41,780
Some humans claim to have that or have it at least

2946
02:22:41,780 --> 02:22:44,740
as one portion of their aims.

2947
02:22:44,740 --> 02:22:49,380
So like the idea of I'm gonna pursue pleasure as such

2948
02:22:49,380 --> 02:22:52,220
even if I don't get actually get food

2949
02:22:52,220 --> 02:22:54,620
or these other reinforcers.

2950
02:22:54,660 --> 02:22:58,660
I just like wire head or take a drug to induce that

2951
02:22:58,660 --> 02:23:02,520
that can be motivating it because if it was correlated

2952
02:23:02,520 --> 02:23:06,140
with reward in the past that like the idea of,

2953
02:23:06,140 --> 02:23:08,700
oh yeah, pleasure that's correlated with these

2954
02:23:08,700 --> 02:23:12,060
it's a concept that applies to these various experiences

2955
02:23:12,060 --> 02:23:13,940
that I've had before which coincided

2956
02:23:13,940 --> 02:23:15,400
with the biological reinforcers.

2957
02:23:15,400 --> 02:23:17,660
And so thoughts of like, yeah,

2958
02:23:17,660 --> 02:23:18,980
I'm gonna be motivated by pleasure

2959
02:23:18,980 --> 02:23:20,480
can get developed in a human.

2960
02:23:21,380 --> 02:23:22,920
But also plenty of humans say, no,

2961
02:23:22,920 --> 02:23:24,200
I wouldn't want to wire head

2962
02:23:24,200 --> 02:23:27,040
or I wouldn't want Nozick's experience machine.

2963
02:23:27,040 --> 02:23:29,720
I care about real stuff in the world.

2964
02:23:29,720 --> 02:23:34,000
And then in the past, having a motivation of like,

2965
02:23:34,000 --> 02:23:37,280
yeah, I really care about say my child.

2966
02:23:37,280 --> 02:23:39,760
I don't care about just about feeling

2967
02:23:39,760 --> 02:23:42,480
that my child is good or like not having heard

2968
02:23:42,480 --> 02:23:45,880
about their suffering or their injury

2969
02:23:45,880 --> 02:23:48,680
because that kind of attitude in the past.

2970
02:23:49,640 --> 02:23:50,800
You could really decide.

2971
02:23:50,800 --> 02:23:53,680
It tended to cause behavior

2972
02:23:53,680 --> 02:23:54,800
that was negatively rewarded

2973
02:23:54,800 --> 02:23:57,600
or that was predicted to be negatively rewarded.

2974
02:23:57,600 --> 02:24:02,080
And so there's a sense in which, okay, yes,

2975
02:24:02,080 --> 02:24:05,080
our underlying reinforcement learning machinery

2976
02:24:05,080 --> 02:24:08,440
wants to wire head, but actually finding

2977
02:24:08,440 --> 02:24:12,000
that hypothesis is challenging.

2978
02:24:12,000 --> 02:24:14,480
And so we can wind up with a hypothesis

2979
02:24:14,480 --> 02:24:16,680
or like a motivation system like,

2980
02:24:16,680 --> 02:24:18,400
no, I don't want to wire head.

2981
02:24:18,400 --> 02:24:20,520
I don't want to go into the experience machine.

2982
02:24:20,520 --> 02:24:23,160
I want to like actually protect my loved ones.

2983
02:24:24,720 --> 02:24:28,160
Even though like we can know, yeah,

2984
02:24:28,160 --> 02:24:30,280
if I tried the super wire heading machine,

2985
02:24:30,280 --> 02:24:32,240
then I would wire head all the time.

2986
02:24:32,240 --> 02:24:36,040
Or if I tried, you know, super duper ultra heroine,

2987
02:24:36,040 --> 02:24:39,040
you know, some hypothetical thing that was directly

2988
02:24:39,040 --> 02:24:41,320
and in a very sophisticated fashion,

2989
02:24:41,320 --> 02:24:43,640
hacking your reward system, you can know, yeah,

2990
02:24:43,640 --> 02:24:45,960
then I would change my behavior ever after.

2991
02:24:45,960 --> 02:24:48,480
But right now, I don't want to do that

2992
02:24:48,480 --> 02:24:50,680
because the heuristics and predictors

2993
02:24:50,680 --> 02:24:52,440
that my brain has learned.

2994
02:24:52,440 --> 02:24:54,600
You don't want to get a good hairline.

2995
02:24:54,600 --> 02:24:57,000
Short circuit that process of updating.

2996
02:24:57,000 --> 02:25:02,000
They want to not expose the dumber predictors in my brain

2997
02:25:02,680 --> 02:25:05,160
that would update my behavior in those ways.

2998
02:25:05,160 --> 02:25:08,680
So in this metaphor, is alignment not wire heading?

2999
02:25:08,680 --> 02:25:11,680
Cause you can like, I don't know if you include like

3000
02:25:11,680 --> 02:25:14,280
using condoms as wire heading or not.

3001
02:25:14,280 --> 02:25:17,240
So the AI that is always honest,

3002
02:25:17,240 --> 02:25:21,880
even when an opportunity arises where it could lie

3003
02:25:21,880 --> 02:25:25,080
and then hack the servers that's on

3004
02:25:25,080 --> 02:25:27,280
and that leads between AI takeover

3005
02:25:27,280 --> 02:25:29,600
and then it can have its loss set to zero.

3006
02:25:29,600 --> 02:25:32,960
That's in some sense, it's like a failure of generalization.

3007
02:25:32,960 --> 02:25:37,080
It's like the AI has not optimized the reward

3008
02:25:37,080 --> 02:25:38,840
in this new circumstance.

3009
02:25:38,840 --> 02:25:42,520
So like human values, like successful human values

3010
02:25:42,520 --> 02:25:46,000
is successful that they are themselves

3011
02:25:46,840 --> 02:25:50,000
involve a misgeneralization,

3012
02:25:50,000 --> 02:25:52,360
not just at the level of evolution,

3013
02:25:52,360 --> 02:25:55,000
but at the level of neural reinforcement.

3014
02:25:55,000 --> 02:25:58,240
And so that indicates it is possible

3015
02:25:58,240 --> 02:26:01,560
to have a system that doesn't automatically go

3016
02:26:01,560 --> 02:26:03,480
to this optimal behavior in the limit.

3017
02:26:03,480 --> 02:26:05,920
And so even if, and Ajay, I suppose she talks about

3018
02:26:05,920 --> 02:26:08,240
like the training game, an AI that is just playing

3019
02:26:08,240 --> 02:26:11,640
the training game to get reward or void loss,

3020
02:26:11,680 --> 02:26:15,400
avoid being changed, that attitude,

3021
02:26:15,400 --> 02:26:18,240
yeah, it's one that could be developed,

3022
02:26:18,240 --> 02:26:20,360
but it's not necessary.

3023
02:26:20,360 --> 02:26:23,240
There can be some substantial range of situations

3024
02:26:23,240 --> 02:26:25,880
that are short of having infinite experience of everything,

3025
02:26:25,880 --> 02:26:27,840
including experience of wire heading,

3026
02:26:28,840 --> 02:26:32,360
where that's not the motivation that you pick up.

3027
02:26:32,360 --> 02:26:34,720
And we could have like an empirical science

3028
02:26:34,720 --> 02:26:38,280
if we have the opportunity to see

3029
02:26:38,280 --> 02:26:41,280
how different motivations are developed short

3030
02:26:41,280 --> 02:26:45,360
of the infinite limit, like how it is that you wind up

3031
02:26:45,360 --> 02:26:48,360
with some humans being enthusiastic

3032
02:26:48,360 --> 02:26:50,800
about the idea of wire heading and others not.

3033
02:26:50,800 --> 02:26:54,760
And you could do experiments with AIs to try and see,

3034
02:26:55,640 --> 02:26:58,640
well, under these training conditions,

3035
02:26:58,640 --> 02:27:01,200
after this much training of this type

3036
02:27:01,200 --> 02:27:03,800
and this much feedback of this type,

3037
02:27:03,800 --> 02:27:05,960
you wind up with such and such a motivation.

3038
02:27:05,960 --> 02:27:09,880
So like, I can find, like if I add in more of these cases

3039
02:27:09,920 --> 02:27:13,840
where there are like tricky adversarial questions

3040
02:27:13,840 --> 02:27:16,320
designed to try and trick the AI into line.

3041
02:27:18,280 --> 02:27:20,080
And then you can ask,

3042
02:27:20,080 --> 02:27:24,760
how does that affect the generalization in other situations?

3043
02:27:24,760 --> 02:27:26,920
And so it's very difficult to study

3044
02:27:26,920 --> 02:27:29,640
and it works a lot better if you have interpretability

3045
02:27:29,640 --> 02:27:32,120
and you can actually read the AI's mind

3046
02:27:32,120 --> 02:27:35,120
by understanding its weights and activations.

3047
02:27:35,120 --> 02:27:38,400
But like, it's not determined,

3048
02:27:38,400 --> 02:27:41,040
the motivation in AI will have at a given point

3049
02:27:41,040 --> 02:27:42,560
in the training process

3050
02:27:42,560 --> 02:27:46,080
by what in the infinite limit the training would go to.

3051
02:27:46,080 --> 02:27:49,440
And it's possible that if we could understand

3052
02:27:49,440 --> 02:27:52,040
the insides of these networks,

3053
02:27:52,040 --> 02:27:55,880
we could tell, yeah, this motivation has been developed

3054
02:27:55,880 --> 02:27:58,160
by this training process.

3055
02:27:58,160 --> 02:28:02,040
And then we can adjust our training process

3056
02:28:02,040 --> 02:28:05,200
to produce these motivations that legitimately wanna help us.

3057
02:28:05,200 --> 02:28:08,080
And if we succeed reasonably well at that,

3058
02:28:08,080 --> 02:28:12,000
then those AI's will try to maintain that property

3059
02:28:12,000 --> 02:28:13,280
as an invariant.

3060
02:28:13,280 --> 02:28:14,680
And we can make them such

3061
02:28:14,680 --> 02:28:17,400
that they're relatively motivated to like,

3062
02:28:17,400 --> 02:28:20,840
tell us if they're having thoughts about,

3063
02:28:21,680 --> 02:28:26,680
have you had dreams about an AI takeover of humanity today?

3064
02:28:26,720 --> 02:28:29,960
And it's just a standard practice

3065
02:28:29,960 --> 02:28:31,760
that they're motivated to do

3066
02:28:31,760 --> 02:28:34,280
to be transparent in that kind of way.

3067
02:28:34,280 --> 02:28:36,480
And so you could add a lot of features like this

3068
02:28:36,480 --> 02:28:39,760
that restrict the kind of takeover scenario.

3069
02:28:39,760 --> 02:28:43,800
And not to say this is all easy

3070
02:28:43,800 --> 02:28:47,120
and requires developing and practicing methods

3071
02:28:47,120 --> 02:28:47,960
we don't have yet,

3072
02:28:47,960 --> 02:28:50,640
but that's the kind of general direction you could go.

3073
02:28:50,640 --> 02:28:52,800
So you, of course, know EleAzer's arguments

3074
02:28:52,800 --> 02:28:56,120
that something like this is implausible

3075
02:28:56,120 --> 02:28:58,600
with modern gradient descent techniques

3076
02:28:58,600 --> 02:29:01,080
because I mean, with interoperability,

3077
02:29:01,080 --> 02:29:02,480
we can like barely see what's happening

3078
02:29:02,480 --> 02:29:04,240
with a couple of neurons.

3079
02:29:04,280 --> 02:29:07,200
And what is like the internal state there at let alone

3080
02:29:07,200 --> 02:29:10,400
when you have sort of like an embedding dimension

3081
02:29:10,400 --> 02:29:13,000
of like tens of thousands or bigger,

3082
02:29:13,000 --> 02:29:14,680
how you would be able to catch

3083
02:29:16,360 --> 02:29:17,880
what exactly is the incentive,

3084
02:29:17,880 --> 02:29:20,840
whether it's the model that is generalized,

3085
02:29:20,840 --> 02:29:24,000
don't lie to humans well or whether it isn't.

3086
02:29:24,000 --> 02:29:26,280
Do you have some sense of why do you disagree

3087
02:29:26,280 --> 02:29:30,160
with somebody like EleAzer on how plausible this is?

3088
02:29:30,160 --> 02:29:32,120
Why it's not impossible, basically.

3089
02:29:32,320 --> 02:29:35,440
I think there are actually a couple of places.

3090
02:29:35,440 --> 02:29:38,960
It's something difficult because EleAzer's argument

3091
02:29:38,960 --> 02:29:42,040
is not fully explicit,

3092
02:29:42,040 --> 02:29:45,080
but he's been doing more lately,

3093
02:29:45,080 --> 02:29:48,000
I think that it's helpful in that direction.

3094
02:29:48,000 --> 02:29:52,000
But so I'd say with respect to interoperability,

3095
02:29:52,000 --> 02:29:54,960
I'm relatively optimistic that the equivalent

3096
02:29:54,960 --> 02:29:59,960
of like an AI lie detector is something that's possible.

3097
02:30:00,960 --> 02:30:05,160
And the internal, so initially,

3098
02:30:05,160 --> 02:30:09,040
the internals of an AI are not optimized

3099
02:30:09,040 --> 02:30:11,920
by at least by gradient descent,

3100
02:30:11,920 --> 02:30:15,680
absent gradient hacking to be impenetrable.

3101
02:30:15,680 --> 02:30:17,760
They're not designed to be resistant

3102
02:30:19,120 --> 02:30:22,080
to an examination of the weights and activations

3103
02:30:22,080 --> 02:30:24,000
showing what the AI thinking in the same way

3104
02:30:24,000 --> 02:30:26,040
that like in our brains,

3105
02:30:27,040 --> 02:30:30,680
when circuits develop in our lives,

3106
02:30:30,680 --> 02:30:32,560
those circuits have not been shaped

3107
02:30:32,560 --> 02:30:35,680
to be resistant to some super fMRI

3108
02:30:35,680 --> 02:30:38,040
being able to infer our behavior from them.

3109
02:30:38,040 --> 02:30:39,240
Although it's in the implication

3110
02:30:39,240 --> 02:30:41,880
of the superposition stuff that in fact it is,

3111
02:30:41,880 --> 02:30:43,560
you're not gonna, sorry,

3112
02:30:43,560 --> 02:30:45,520
this is inside baseball for the audience,

3113
02:30:45,520 --> 02:30:48,160
but basically you can't clean lean for

3114
02:30:48,160 --> 02:30:50,880
what quality a single neuron stands for.

3115
02:30:50,880 --> 02:30:54,080
So it could be like a single neuron could be like,

3116
02:30:54,200 --> 02:30:56,360
this other neuron is about Alexander the Great

3117
02:30:56,360 --> 02:31:00,040
or this neuron is about my desire to conquer the world.

3118
02:31:00,040 --> 02:31:02,280
Things can have multiple,

3119
02:31:02,280 --> 02:31:04,160
in a multiple direction than an interaction.

3120
02:31:04,160 --> 02:31:06,600
And so it's not as easy as just like,

3121
02:31:06,600 --> 02:31:09,840
oh yeah, this is the neuron that always fires

3122
02:31:09,840 --> 02:31:12,320
in cases of deception.

3123
02:31:13,160 --> 02:31:17,680
But the thing that makes me relatively optimistic here

3124
02:31:18,560 --> 02:31:20,880
with respect to interpretability

3125
02:31:20,960 --> 02:31:25,040
is that I think we can have reasonably robust

3126
02:31:25,040 --> 02:31:28,520
experimental feedbacks on a lot of these things.

3127
02:31:28,520 --> 02:31:32,080
And the kind of setup

3128
02:31:33,880 --> 02:31:36,840
that I would be interested in is,

3129
02:31:36,840 --> 02:31:41,840
so you have an AI that is trained and rewarded

3130
02:31:44,160 --> 02:31:46,280
for communicating to some audience,

3131
02:31:46,280 --> 02:31:49,160
maybe a human, maybe another AI model

3132
02:31:50,160 --> 02:31:54,720
that like some claim is true or false.

3133
02:31:54,720 --> 02:31:58,880
Like say that a particular,

3134
02:31:58,880 --> 02:32:00,160
this can be from some past data,

3135
02:32:00,160 --> 02:32:02,520
a particular company will succeed or fail.

3136
02:32:03,400 --> 02:32:06,080
It could be like token prediction.

3137
02:32:06,080 --> 02:32:09,040
It could be like this computer program

3138
02:32:09,040 --> 02:32:11,320
does not have a backdoor,

3139
02:32:11,320 --> 02:32:14,000
which if run in the servers could be used

3140
02:32:14,000 --> 02:32:15,760
for an AI takeover.

3141
02:32:15,760 --> 02:32:18,840
So they're like lots of claims true and false

3142
02:32:19,800 --> 02:32:21,320
that you could train AI

3143
02:32:21,320 --> 02:32:25,080
is to use their best efforts to advocate for.

3144
02:32:25,080 --> 02:32:26,520
And you can do that on things

3145
02:32:26,520 --> 02:32:29,240
where you're later gonna have a ground truth.

3146
02:32:29,240 --> 02:32:31,560
You're gonna know, in fact,

3147
02:32:31,560 --> 02:32:35,400
is there a backdoor in this program?

3148
02:32:35,400 --> 02:32:37,800
Because where you're training and rewarding the AI

3149
02:32:37,800 --> 02:32:41,160
for successfully slipping a backdoor through,

3150
02:32:41,160 --> 02:32:43,320
you also give it a penalty for being caught.

3151
02:32:44,320 --> 02:32:45,960
And so doing this kind of thing,

3152
02:32:45,960 --> 02:32:50,960
you can draw out like a large generated dataset

3153
02:32:50,960 --> 02:32:55,960
on demand of this is what an AI putting forth

3154
02:32:56,440 --> 02:33:01,440
its best efforts to pass through some deception

3155
02:33:03,280 --> 02:33:05,080
to put a backdoor in some program.

3156
02:33:05,080 --> 02:33:06,480
This is what it looks like.

3157
02:33:07,600 --> 02:33:10,400
And we think it's doing its best efforts

3158
02:33:10,400 --> 02:33:12,080
because we've trained it for that.

3159
02:33:12,080 --> 02:33:13,560
And like gradient descent,

3160
02:33:13,560 --> 02:33:15,960
if there's any immediate way to adjust its performance

3161
02:33:15,960 --> 02:33:19,080
to do better, gradient descent will apply that.

3162
02:33:19,080 --> 02:33:23,000
You can talk about ways in which maybe gradient descent

3163
02:33:24,200 --> 02:33:25,720
there could be subverted,

3164
02:33:25,720 --> 02:33:28,960
but I think it's not the most likely case

3165
02:33:28,960 --> 02:33:31,480
that that really breaks things hard.

3166
02:33:31,480 --> 02:33:34,280
Yeah, I guess before we get into the details on this,

3167
02:33:34,280 --> 02:33:39,280
the thing I maybe wanna address the layer above in the stack,

3168
02:33:40,200 --> 02:33:44,280
which is, okay, suppose this generalizes well

3169
02:33:44,280 --> 02:33:47,040
into the early AI is the GPT-6s.

3170
02:33:47,040 --> 02:33:51,760
And okay, so now we have kind of aligned GPT-6,

3171
02:33:51,760 --> 02:33:55,200
that is the precursor to the feedback loop

3172
02:33:55,200 --> 02:33:57,440
in which AI is making itself smarter.

3173
02:33:57,440 --> 02:33:59,280
At some point they're gonna be super intelligent,

3174
02:33:59,280 --> 02:34:02,880
they're gonna be able to see their own galaxy brain.

3175
02:34:02,880 --> 02:34:05,520
And if they're like, I don't wanna be aligned with the humans,

3176
02:34:05,520 --> 02:34:07,160
they can change it.

3177
02:34:07,200 --> 02:34:11,960
So at this point, what do we do with the aligned GPT-6

3178
02:34:11,960 --> 02:34:14,240
so that the super intelligence

3179
02:34:14,240 --> 02:34:16,680
that we eventually develop is also aligned?

3180
02:34:16,680 --> 02:34:18,760
So humans are pretty unreliable.

3181
02:34:18,760 --> 02:34:19,720
Yeah.

3182
02:34:19,720 --> 02:34:24,440
So if you get to a situation where you have AIs

3183
02:34:24,440 --> 02:34:28,080
who are aiming at roughly the same thing as you,

3184
02:34:28,080 --> 02:34:31,720
at least as well as having humans do the thing,

3185
02:34:31,720 --> 02:34:34,680
you're in pretty good shape, I think.

3186
02:34:34,720 --> 02:34:37,760
And there are ways for that situation

3187
02:34:37,760 --> 02:34:40,920
to be relatively stable.

3188
02:34:40,920 --> 02:34:45,680
So like we can look ahead and see experimentally

3189
02:34:45,680 --> 02:34:48,440
how changes are altering behavior

3190
02:34:48,440 --> 02:34:51,840
where each step is like a modest increment.

3191
02:34:51,840 --> 02:34:56,440
And so AIs that have not had that change made to them,

3192
02:34:56,440 --> 02:34:58,360
I get to supervise and monitor it,

3193
02:34:58,360 --> 02:35:03,160
see exactly how does this affect the experimental area.

3194
02:35:04,160 --> 02:35:09,160
So if you're sufficiently on track with earlier systems

3195
02:35:09,800 --> 02:35:12,400
that are capable cognitively of representing

3196
02:35:12,400 --> 02:35:14,800
a kind of robust procedure,

3197
02:35:14,800 --> 02:35:18,840
then I think they can handle the job

3198
02:35:18,840 --> 02:35:22,000
of incrementally improving the stability of the system

3199
02:35:22,000 --> 02:35:25,040
so that it rapidly converges to something

3200
02:35:25,040 --> 02:35:26,160
that's quite stable.

3201
02:35:27,080 --> 02:35:29,760
But the question is more about getting to that point

3202
02:35:29,760 --> 02:35:30,600
in the first place.

3203
02:35:30,760 --> 02:35:32,800
Eliezer will say that like,

3204
02:35:32,800 --> 02:35:35,480
well, if we had human brain emulations,

3205
02:35:36,400 --> 02:35:38,320
that would be pretty good.

3206
02:35:38,320 --> 02:35:40,080
Certainly much better than his current view

3207
02:35:40,080 --> 02:35:43,160
that has been almost certainly doomed.

3208
02:35:44,200 --> 02:35:48,640
I think, yeah, we'd have a good shot with that.

3209
02:35:48,640 --> 02:35:53,480
And so if we can get to the human-like mind

3210
02:35:53,480 --> 02:35:58,480
with like rough enough human supporting aims,

3211
02:35:59,480 --> 02:36:02,120
remember that we don't need to be like infinitely perfect

3212
02:36:02,120 --> 02:36:04,480
because I mean, that's a higher standard

3213
02:36:04,480 --> 02:36:05,480
than brain emulations.

3214
02:36:05,480 --> 02:36:09,200
There's a lot of noise and variation among the humans.

3215
02:36:09,200 --> 02:36:11,680
Yeah, it's a relatively finite standard.

3216
02:36:11,680 --> 02:36:13,200
It's not godly superhuman,

3217
02:36:13,200 --> 02:36:18,200
although a AI that was just like a human

3218
02:36:18,200 --> 02:36:21,600
with all the human advantages with AI advantages as well,

3219
02:36:21,600 --> 02:36:23,760
as we said, is enough for intelligence explosion

3220
02:36:23,760 --> 02:36:26,520
and sort of wild superhuman capability.

3221
02:36:26,520 --> 02:36:27,360
If you crank it up.

3222
02:36:28,040 --> 02:36:28,880
Yeah, yeah, yeah.

3223
02:36:30,360 --> 02:36:33,120
And so it's very dangerous to be at that point,

3224
02:36:33,120 --> 02:36:36,320
but it's not, you don't need to be working

3225
02:36:36,320 --> 02:36:39,600
with a godly superintelligent AI

3226
02:36:39,600 --> 02:36:42,040
to make something that is the equivalent

3227
02:36:42,040 --> 02:36:44,400
of human emulations of like,

3228
02:36:44,400 --> 02:36:49,400
this is like a very, very sober, very ethical human

3229
02:36:49,680 --> 02:36:53,440
who is like committed to a project

3230
02:36:53,440 --> 02:36:55,360
of not seizing power for themselves

3231
02:36:55,360 --> 02:36:59,680
and of contributing to like a larger legitimate process.

3232
02:36:59,680 --> 02:37:03,060
That's a goal you can aim for getting an AI

3233
02:37:03,060 --> 02:37:04,640
that is aimed at doing that

3234
02:37:04,640 --> 02:37:07,840
and has strong guardrails against the ways

3235
02:37:07,840 --> 02:37:09,720
that could easily deviate from that.

3236
02:37:09,720 --> 02:37:14,720
So things like being averse to deception,

3237
02:37:14,720 --> 02:37:17,240
being averse to using violence.

3238
02:37:17,240 --> 02:37:21,520
And there will always be loopholes and ways

3239
02:37:21,520 --> 02:37:24,080
in which you can imagine an infinitely intelligent thing

3240
02:37:24,080 --> 02:37:24,960
getting around those.

3241
02:37:24,960 --> 02:37:29,960
But if you install additional guardrails like that fast enough,

3242
02:37:32,600 --> 02:37:36,920
they can mean that you're able to succeed

3243
02:37:36,920 --> 02:37:39,360
at the project of making an aligned enough AI,

3244
02:37:39,360 --> 02:37:41,840
certainly an AI that was better

3245
02:37:41,840 --> 02:37:44,080
than a human brain emulation,

3246
02:37:44,080 --> 02:37:48,480
before the project of AI is in their spare time

3247
02:37:48,480 --> 02:37:49,520
or when you're not looking

3248
02:37:49,520 --> 02:37:52,280
or when you're unable to appropriately supervise them

3249
02:37:52,280 --> 02:37:55,960
and it gets around any deontological prohibitions

3250
02:37:55,960 --> 02:37:59,600
they may have take over and overthrow the whole system.

3251
02:37:59,600 --> 02:38:01,940
So you have a race between on the one hand,

3252
02:38:01,940 --> 02:38:04,680
the project of getting strong interpretability

3253
02:38:04,680 --> 02:38:08,600
and shaping motivations that are roughly aiming

3254
02:38:08,600 --> 02:38:10,000
at making this process go well

3255
02:38:10,000 --> 02:38:13,280
and that have guardrails that will prevent

3256
02:38:13,280 --> 02:38:15,740
like small deviations from exploding.

3257
02:38:16,640 --> 02:38:17,840
And on the other hand,

3258
02:38:18,680 --> 02:38:21,960
these AIs in their spare time

3259
02:38:21,960 --> 02:38:25,560
or in ways that you don't perceive or monitor appropriately

3260
02:38:25,560 --> 02:38:28,840
or they're only supervised by other AIs who conspire,

3261
02:38:28,840 --> 02:38:30,480
make the AI take over happen.

3262
02:38:30,480 --> 02:38:33,800
And I guess we'll talk later about how that happens.

3263
02:38:33,800 --> 02:38:35,440
Are these different AIs that are doing the race

3264
02:38:35,440 --> 02:38:38,320
or is it just like different capabilities of the same AI?

3265
02:38:38,320 --> 02:38:42,400
The defining like what is a separate AI is tricky.

3266
02:38:42,400 --> 02:38:45,880
So like, and we talk about GPT-4.

3267
02:38:45,880 --> 02:38:48,680
And there are many instances of GPT-4

3268
02:38:48,680 --> 02:38:50,920
on the servers at any given time.

3269
02:38:50,920 --> 02:38:54,400
And there are versions that have been fine tuned

3270
02:38:54,400 --> 02:38:56,400
to different purposes.

3271
02:38:56,400 --> 02:38:58,800
They don't necessarily have to be separate.

3272
02:38:58,800 --> 02:39:03,800
So like GPT-4 does work as a preference model

3273
02:39:04,080 --> 02:39:08,800
where it's like predicting how humans will evaluate things.

3274
02:39:08,800 --> 02:39:12,780
And it also does work like giving answers

3275
02:39:12,820 --> 02:39:16,640
that are evaluated as good by the preference model.

3276
02:39:16,640 --> 02:39:18,900
And you can say, these are not separate

3277
02:39:18,900 --> 02:39:21,600
in the sense that like, well, if they underline,

3278
02:39:22,940 --> 02:39:25,180
pre-trained or the early training GPT

3279
02:39:25,180 --> 02:39:30,180
with like harboring a desire to conspire for an AI takeover

3280
02:39:30,180 --> 02:39:33,180
then both the, who's watching the watchers?

3281
02:39:33,180 --> 02:39:38,180
The watchers may also have the same motivation.

3282
02:39:38,540 --> 02:39:42,500
If you have a setup where humans have no hard power

3283
02:39:42,540 --> 02:39:47,540
if the AI's conspired, all of the safety

3284
02:39:47,980 --> 02:39:51,180
and security mechanisms involve AI's watching other AI's

3285
02:39:51,180 --> 02:39:52,900
then they have the potential to just like,

3286
02:39:52,900 --> 02:39:56,660
well, we'll all walk off the job at the same time

3287
02:39:56,660 --> 02:39:58,540
if they're able to coordinate.

3288
02:39:58,540 --> 02:40:00,120
And so that's a failure mode.

3289
02:40:00,120 --> 02:40:03,260
If humans still have the hard power though,

3290
02:40:04,220 --> 02:40:09,220
if still you have situations where humans are looking

3291
02:40:09,380 --> 02:40:11,700
at some of the actual outputs that are produced

3292
02:40:13,100 --> 02:40:17,660
then they can apply gradient descent to be, well,

3293
02:40:17,660 --> 02:40:20,340
every time a human looks at it,

3294
02:40:20,340 --> 02:40:21,520
things have got to be pretty good.

3295
02:40:21,520 --> 02:40:24,060
So if humans look at a thousand samples

3296
02:40:24,060 --> 02:40:26,400
of like random lines of code being written

3297
02:40:26,400 --> 02:40:31,400
by the AI's as they go, and this is drawn

3298
02:40:31,660 --> 02:40:33,780
from an independent distribution,

3299
02:40:33,780 --> 02:40:36,340
then 99 plus percent of the time

3300
02:40:36,340 --> 02:40:38,260
the AI's have to not be doing stuff

3301
02:40:38,260 --> 02:40:42,460
that's like downloading the hacker's guide

3302
02:40:42,460 --> 02:40:45,700
to escaping open AI servers

3303
02:40:45,700 --> 02:40:49,860
or inducing obvious Trojan vulnerabilities.

3304
02:40:49,860 --> 02:40:53,180
And that's actually quite a different dynamic

3305
02:40:53,180 --> 02:40:54,340
than human law enforcement.

3306
02:40:54,340 --> 02:40:58,300
So if human law enforcement would catch one

3307
02:40:58,300 --> 02:41:01,620
in a million crimes, then that wouldn't influence

3308
02:41:01,620 --> 02:41:05,140
the behavior of people committing crimes at all

3309
02:41:05,140 --> 02:41:07,580
because they expect, ah, this is very unlikely

3310
02:41:07,580 --> 02:41:09,060
to affect me.

3311
02:41:09,060 --> 02:41:14,060
With AI, if you're able to draw a hundred samples

3312
02:41:14,260 --> 02:41:16,720
or a thousand samples, even if there are billions

3313
02:41:16,720 --> 02:41:18,900
of AI instances and you're only looking

3314
02:41:18,900 --> 02:41:22,000
at a small portion of them, when you run gradient descent

3315
02:41:22,000 --> 02:41:26,260
on the samples you derive, you're gonna change the AI

3316
02:41:26,260 --> 02:41:28,960
so that whenever humans look at it,

3317
02:41:28,960 --> 02:41:30,840
it's delivering a good result.

3318
02:41:30,840 --> 02:41:34,460
And so that's just like quantitatively

3319
02:41:34,460 --> 02:41:36,060
a very different functional form

3320
02:41:36,100 --> 02:41:39,180
for how law enforcement works on AI

3321
02:41:39,180 --> 02:41:42,980
when you can do these independent samples

3322
02:41:42,980 --> 02:41:45,540
than it would for, you know, police

3323
02:41:45,540 --> 02:41:46,880
and say like a human population

3324
02:41:46,880 --> 02:41:48,900
that has a preexisting psychology

3325
02:41:48,900 --> 02:41:53,260
that isn't being changed by these observations.

3326
02:41:53,260 --> 02:41:56,300
Cause we, when we do gradient descent on the samples

3327
02:41:56,300 --> 02:42:00,100
we draw, all of the next generation

3328
02:42:00,100 --> 02:42:02,500
of the AI models has been altered in that way.

3329
02:42:02,500 --> 02:42:05,180
My picture of like a line to subhuman AI

3330
02:42:05,220 --> 02:42:08,820
to this, like the super human AI being aligned

3331
02:42:08,820 --> 02:42:10,100
is still murky.

3332
02:42:10,100 --> 02:42:12,100
If we can talk about that more concretely.

3333
02:42:12,100 --> 02:42:14,540
Yeah, and I do want to clarify.

3334
02:42:14,540 --> 02:42:19,540
So where Eliezer claims that were something like 95%,

3335
02:42:21,060 --> 02:42:26,060
98% plus maybe likely to be killed in AI takeover.

3336
02:42:30,760 --> 02:42:32,940
I think that probably won't happen.

3337
02:42:32,940 --> 02:42:34,500
And later I can maybe give them

3338
02:42:34,500 --> 02:42:36,880
more exclusive breakdown of why.

3339
02:42:37,820 --> 02:42:41,940
But I think it's a shockingly high risk.

3340
02:42:43,020 --> 02:42:45,860
And so depending on the day, I might say,

3341
02:42:45,860 --> 02:42:48,940
I might say one in four or one in five

3342
02:42:48,940 --> 02:42:51,780
that we get an AI takeover

3343
02:42:51,780 --> 02:42:55,500
that see at Caesar's control of the future

3344
02:42:55,500 --> 02:42:58,060
makes a much worse world

3345
02:42:58,060 --> 02:43:00,380
than we otherwise would have had.

3346
02:43:00,860 --> 02:43:05,060
And with like a big chance that we're all killed

3347
02:43:05,060 --> 02:43:05,980
in the process.

3348
02:43:06,940 --> 02:43:09,900
Hey everybody, I hope you enjoyed that episode.

3349
02:43:09,900 --> 02:43:12,100
As always, the most helpful thing you can do

3350
02:43:12,100 --> 02:43:13,900
is to share the podcast,

3351
02:43:13,900 --> 02:43:15,580
send it to people you think might enjoy it,

3352
02:43:15,580 --> 02:43:17,740
put it in Twitter, your group chats, et cetera,

3353
02:43:17,740 --> 02:43:19,580
just splits the world.

3354
02:43:19,580 --> 02:43:20,820
I appreciate your listening.

3355
02:43:20,820 --> 02:43:21,980
I'll see you next time.

3356
02:43:21,980 --> 02:43:23,300
Cheers.

3357
02:43:24,160 --> 02:43:25,820
Mother Earth group chat.

3358
02:43:25,820 --> 02:43:27,440
Mother Earth group chat.

3359
02:43:27,440 --> 02:43:29,200
Mother Earth group chat.

3360
02:43:29,200 --> 02:43:30,400
Mother Earth group chat.

3361
02:43:32,400 --> 02:43:34,020
Mother Earth group chat.

3362
02:43:34,020 --> 02:43:36,120
Mother Earth group chat chat.

3363
02:43:36,120 --> 02:43:38,000
Mother Earth group chat.

3364
02:43:38,000 --> 02:43:39,460
Mother Earth group chat.

3365
02:43:39,460 --> 02:43:40,700
Mother Earth group chat.

3366
02:43:42,700 --> 02:43:43,900
Mother Earth group chat.

3367
02:43:45,900 --> 02:43:48,420
Mother Earth group chat chat.

3368
02:43:48,420 --> 02:43:52,100
Manila Manila dies or Spr period injury.

