start	end	text
0	3200	So I wouldn't be surprised if we had AGI-like systems
3200	4360	within the next decade.
4360	6400	It was pretty surprising to almost everyone,
6400	8880	including the people who first worked on the scaling
8880	10880	hypotheses, that how far it's gone.
10880	13400	In a way, I look at the large models today,
13400	14920	and I think they're almost unreasonably
14920	16240	effective for what they are.
16240	18760	It's an empirical question whether that will hit an asymptote
18760	19760	or a brick wall.
19760	21080	I think no one knows.
21080	23000	When you think about superhuman intelligence,
23000	25600	is it still controlled by a private company?
25600	28040	As Gemini becoming more multimodal,
28040	30240	and we start ingesting audio-visual data,
30240	32840	as well as text data, I do think our systems
32840	36280	are going to start to understand the physics of the real world
36280	36920	better.
36920	38560	The world's about to become very exciting,
38560	40160	I think, in the next few years as we start
40160	44240	getting used to the idea of what true multimodality means.
44240	48240	OK, today it is a true honor to speak with Demis Isavis, who
48240	50240	is the CEO of DeepMind.
50240	51640	Demis, welcome to the podcast.
51640	52720	Thanks for having me.
52720	55480	First question, given your neuroscience background,
55480	56880	how do you think about intelligence?
56920	60000	Specifically, do you think it's like one higher-level general
60000	61480	reasoning circuit, or do you think
61480	65560	it's thousands of independent sub-scales and heuristics?
65560	69880	Well, it's interesting because intelligence is so broad,
69880	74760	and what we use it for is so generally applicable.
74760	77120	I think that suggests that there must
77120	81840	be some sort of high-level common things,
81840	84160	common algorithmic themes, I think,
84160	87320	around how the brain processes the world around us.
87320	91680	So of course, then there are specialized parts of the brain
91680	94080	that do specific things.
94080	96440	But I think there are probably some underlying principles
96440	97800	that underpin all of that.
97800	100680	Yeah, how do you make sense of the fact that in these LLMs,
100680	102720	though, when you give them a lot of data
102720	104160	in any specific domain, they tend
104160	107680	to get asymmetrically better in that domain?
107680	110040	Wouldn't we expect a general improvement
110040	111520	across all the different areas?
111520	113600	Well, I think you first of all, I think you do actually
113640	116480	sometimes get surprising improvement in other domains
116480	118200	when you improve in a specific domain.
118200	121680	So for example, when these large models sort of improve
121680	125280	at coding, that can actually improve their general reasoning.
125280	127520	So there is some evidence of some transfer,
127520	131600	though I think we would like a lot more evidence of that.
131600	134360	But also, that's how the human brain learns, too,
134360	137480	is if we experience and practice a lot of things like chess
137480	140120	or writing, creative writing, or whatever that is,
140120	142200	we also tend to specialize and get better
142200	143440	at that specific thing.
143440	146600	Even though we're using sort of general learning techniques
146600	148520	and general learning systems in order
148520	151080	to get good at that domain.
151080	153160	Yeah, well, what's been the most surprising example
153160	154960	of this kind of transfer for you?
154960	157840	Like, you see language and code or images and text?
157840	160080	Yeah, I think probably, I mean, I'm
160080	162760	hoping we're going to see a lot more of this China transfer.
162760	166600	But I think things like getting better at coding and math
166600	168920	and generally improving your reasoning,
168920	171520	that is how it works with us as human learners.
171520	173000	But I think it's interesting seeing
173000	175880	that in these artificial systems.
175880	179120	And can you see the sort of mechanistic way in which,
179120	180720	let's say in the language and code example,
180720	183120	there's like, I found a place in a neural network that's
183120	184920	getting better with both the language and the code.
184920	187000	Is that too far down the weeds?
187000	189840	Yeah, well, I don't think our analysis techniques
189840	193480	are quite sophisticated enough to be able to hone in on that.
193480	195080	I think that's actually one of the areas
195080	197600	that a lot more research needs to be done
197600	200640	on kind of mechanistic analysis of the representations
200680	201920	that these systems build up.
201920	205360	And I sometimes like to call it virtual brain analytics.
205360	209360	In a way, it's a bit like doing fMRI or a single cell
209360	211880	recording from a real brain.
211880	214640	What's the analogous sort of analysis techniques
214640	216320	for these artificial minds?
216320	218560	And there's a lot of great work going on
218560	219400	on this sort of stuff.
219400	221840	People like Chris Ola, I really like his work.
221840	224040	And a lot of computational neuroscience techniques,
224040	226040	I think, could be brought to bear
226040	228560	on analyzing these current systems we're building.
228560	230840	In fact, I try to encourage a lot of my computational
230840	234000	neuroscience friends to start thinking in that direction
234000	238440	and applying their know-how to the large models.
238440	241680	Yeah, what do other researchers not understand
241680	245200	about human intelligence that you have some sort of insight
245200	246680	on, given your neuroscience background?
246680	250600	I think neuroscience has added a lot.
250600	252760	If you look at the last sort of 10, 20 years
252760	254480	that we've been at it, at least.
254480	258080	And I've been thinking about this for 30 plus years.
258080	261760	I think in the earlier days of this sort of new wave of AI,
261760	264760	I think neuroscience was providing a lot of interesting
264760	266160	directional clues.
266160	268000	So things like reinforcement learning,
268000	269640	combining that with deep learning,
269640	271640	some of our pioneering work we did there,
271640	275400	things like experience replay, even the notion of attention,
275400	277840	which has become super important.
277840	281240	A lot of those original sort of inspirations
281240	284040	come from some understanding about how the brain works.
284040	285200	Not the exact specifics.
285200	287280	Of course, one's an engineered system.
287280	288440	The other one's a natural system.
288440	290840	So it's not so much about a one-to-one mapping
290840	292240	of a specific algorithm.
292240	294480	It's more kind of inspirational direction,
294480	297240	maybe some ideas for architecture or algorithmic ideas
297240	299160	or representational ideas.
299160	301640	And because you know the brains in existence
301640	304200	prove that general intelligence is possible at all,
304200	307160	I think the history of human endeavors
307160	309680	has been that once you know something's possible,
309680	311760	it's easier to push hard in that direction.
311760	314080	Because you know it's a question of effort then
314080	317080	and sort of a question of when, not if.
317080	319440	And that allows you to I think make progress
319440	320520	a lot more quickly.
320520	323240	So I think neurosciences has had a lot of,
324680	327000	has inspired a lot of the thinking,
327000	330080	at least in a soft way behind where we are today.
331000	333400	But as for going forwards,
333400	336960	I think that there's still a lot of interesting things
336960	338760	to be resolved around planning
338760	342560	and how does the brain construct the right world models.
343680	347060	I studied for example, how the brain does imagination,
347060	350020	or you can think of it as a mental simulation.
350020	354260	So how do we create very rich visual spatial simulations
354260	356100	of the world in order for us to plan better?
356100	357540	Yeah, actually I'm curious how you think
357540	359340	that will sort of interface with LLM.
359340	361500	So obviously DeepMinders are the frontier
361500	363340	and has been for many years,
363340	365140	systems like AlphaZero and so forth,
365140	366980	of having these agents who can like think through
366980	369460	different steps to get to an end outcome.
369460	370820	All right, will this just be,
370820	373020	is a path for LLMs to have this sort of
373020	374740	tree search kind of thing on top of them?
374740	375780	How do you think about this?
375780	378860	I think that's a super promising direction in my opinion.
378860	382740	So we've got to carry on improving the large models
382740	385820	and we've got to carry on basically making
385820	388020	the more and more accurate predictors of the world.
388020	390300	So in effect, making them more and more reliable
390300	392580	world models, that's clearly unnecessary,
392580	394740	but I would say probably not sufficient component
394740	396500	of an AGI system.
396500	397820	And then on top of that,
397820	401140	I would, we're working on things like AlphaZero,
401140	404700	like planning mechanisms on top that make use of that model
404700	406580	in order to make concrete plans
406580	408900	to achieve certain goals in the world
408900	412780	and perhaps sort of chain thought together
412780	414380	or lines of reasoning together
414380	416740	and maybe use search to kind of explore
416740	418380	massive spaces of possibility.
418380	419860	I think that's kind of missing
419860	421980	from our current large models.
421980	425860	How do you get past the sort of immense amount of compute
425860	427220	that these approaches tend to require?
427220	431500	So even the AlphaGo system was a pretty expensive system
431540	434740	because you had to do the sort of running an LLM on each node
434740	436100	of the tree.
436100	437660	How do you anticipate that will get more,
437660	438500	made more efficient?
438500	442820	Well, I mean, one thing is Moore's law tends to help
442820	447620	if every year, of course, more computation comes in,
447620	450060	but we focus a lot on efficient,
450060	454460	sample efficient methods and reusing existing data,
454460	456220	things like experience replay,
456220	459140	and also just looking at more efficient ways.
459140	460980	I mean, the better your world model is,
460980	462660	the more efficient your search can be.
462660	464620	So one example I always get with AlphaZero,
464620	467620	our system to play Go and chess and any game,
467620	471340	is that it's stronger than world champion level,
471340	473940	human world champion level at all these games.
473940	478020	And it uses a lot less search than a brute force method
478020	479940	like Deep Blue, say to play chess.
479940	482460	Deep Blue, one of these traditional stockfish
482460	485380	or Deep Blue systems would maybe look at millions
485380	488740	of possible moves for every decision it's gonna make.
488740	491660	AlphaZero and AlphaGo made, you know,
491660	495860	looked at around tens of thousands of possible positions
495860	498260	in order to make a decision about what to move next.
498260	501500	But a human grandmaster, a human world champion,
501500	503980	probably only looks at a few hundreds of moves,
503980	505020	even the top ones,
505020	507980	in order to make their very good decision
507980	509260	about what to play next.
509260	512500	So that suggests that obviously the brute force systems
512500	515020	don't have any real model other than heuristics
515020	516100	about the game.
516140	519660	AlphaZero has quite a decent model,
519660	521860	but the human, you know,
521860	524020	top human players have a much richer,
524020	526860	much more accurate model than of Go or chess.
526860	528500	So that allows them to make, you know,
528500	531780	world class decisions on a very small amount of search.
531780	532860	So I think there's still,
532860	534180	there's a sort of trade off there,
534180	536020	like, you know, if you improve the models,
536020	538420	then I think your search can be more efficient
538420	540180	and therefore you can get further with your search.
540180	542500	Yeah, I have two questions based on that.
542500	544220	The first being with AlphaZero,
544220	546900	you had a very concrete win condition of, you know,
546900	547740	at the end of the day,
547740	548580	do I win this game or not?
548580	550420	And you can reinforce on that.
550420	552420	When you're just thinking of like an LLM,
552420	553500	putting out thought,
553500	555780	what will, do you think there'll be this kind of ability
555780	557420	to discriminate in the end,
557420	559980	whether that was like a good thing to reward or not?
559980	561500	Well, of course that's why we, you know,
561500	563740	we pioneered and DeepMind sort of famous
563740	566940	for using games as a proving ground,
566940	568620	partly because obviously it's efficient
568620	570100	to research in that domain.
570100	571980	But the other reason is obviously it's, you know,
571980	574060	extremely easy to specify a reward function,
574060	575740	winning the game or improving the score,
575740	578100	something like that sort of built into most games.
578100	581940	So that is one of the challenges of real world systems
581940	584860	is how does one define the right objective function,
584860	587540	the right reward function and the right goals
587540	591500	and specify them in a, you know, in a general way,
591500	592660	but that's specific enough
592660	595660	and actually points the system in the right direction.
595660	598940	And for real world problems, that can be a lot harder.
598940	601260	But actually, if you think about it
601260	603420	in even scientific problems,
603420	605580	there are usually ways that you can specify
605580	607220	the goal that you're after.
607220	608660	And then when you think about human intelligence,
608660	609860	you're just saying, well, you know,
609860	610940	the humans thinking about these thoughts
610940	612860	are just super sample efficient.
612860	615140	How do you, I understand coming up with relativity, right?
615140	616900	There's just like thousands of possible permutations
616900	617980	of the equations.
617980	619660	Do you think it's also this sort of sense
619660	620980	of like different heuristics of like,
620980	622340	I'm going to try out this approach instead of this
622340	625300	or is it a totally different way of approaching
625300	627740	coming up with a solution than, you know,
627740	629340	what AlphaGo does to plan the next move?
629340	630740	Yeah, well, look, I think it's different
630740	632500	because our brains are not built
632540	635220	for doing Monte Carlo research, right?
635220	639980	It's just not the way our organic brains would work.
639980	642740	So I think that in order to compensate for that,
642740	644860	you know, people like Einstein have come up, you know,
644860	647140	their brains have using their intuition
647140	649260	and, you know, we maybe come to what intuition is,
649260	651420	but they use their sort of knowledge
651420	654540	and their experience to build extremely, you know,
654540	657500	in Einstein's case, extremely accurate models of physics,
657500	659860	including these sort of mental simulations.
659860	661180	I think if you read about Einstein
661180	663620	and how he came up with things, he used to visualize
663620	667780	and sort of really kind of feel
667780	669740	what these physical systems should be like,
669740	670980	not just the mathematics of it,
670980	672780	but have a really intuitive feel
672780	674500	for what they would be like in reality.
674500	677580	And that allowed him to think these sort of very outlandish
677580	679140	thoughts at the time.
679140	681740	So I think that it's the sophistication
681740	683420	of the world models that we're building,
683420	685940	which then, you know, if you imagine your world model
685940	689260	can get you to a certain node in a tree that you're searching,
689260	691300	and then you just do a little bit of search
691300	693580	around that node, that leaf node,
693580	695940	and that gets you to these original places.
695940	697940	But obviously, if your model is,
697940	701020	and your judgment on that model is very, very good,
701020	703980	then you can pick which leaf nodes you should sort of expand
703980	705900	with search much more accurately.
705900	708140	So therefore, overall, you do a lot less search.
708140	709900	I mean, there's no way that, you know,
709900	712460	any human could do a kind of brute force search
712460	714580	over any kind of significant space.
714580	715940	Yeah, yeah, yeah.
715940	717540	A big sort of open question right now
717540	720620	is whether RL will allow these models to do the self-placed
720620	722540	synthetic data to get over the data bottleneck.
722540	724060	It sounds like you're optimistic about this.
724060	725580	Yeah, I'm very optimistic about that.
725580	727620	I mean, I think, well, first of all,
727620	729780	there's still a lot more data, I think, that can be used,
729780	732300	especially if one views like multimodal and video
732300	733300	and these kind of things.
733300	736700	And obviously, you know, society's adding more data
736700	738580	all the time.
738580	741580	But I think to the internet and things like that.
741580	743980	But I think that there's a lot of scope
743980	746140	for creating synthetic data.
746140	749620	We're looking at different ways partly through simulation
749620	752100	and using very realistic games environments,
752100	755260	for example, to generate realistic data,
755260	757100	but also self-play.
757100	761300	So that's where systems interact with each other
761300	763460	or converse with each other.
763460	764780	And in the sense of, you know,
764780	766620	work very well for us with AlphaGo and AlphaZero,
766620	769100	where we got the systems to play against each other
769100	770940	and actually learn from each other's mistakes
770940	772900	and build up a knowledge base that way.
772900	774820	And I think there are some good analogies for that.
774820	775980	It's a little bit more complicated,
775980	779820	but to build a general kind of world data.
779820	781820	How do you get to the point where these models,
781820	783900	the sort of synthetic data they're outputting
783900	785460	and their self-play they're doing,
785460	787500	is not just more of what they've already got
787500	788340	in their data set,
788340	790500	but is something they haven't seen before?
790500	792260	You know what I mean, to actually improve the abilities.
792260	795940	Yeah, so there, I think there's a whole science needed.
795940	797940	And I think we're still in the nascent stage of this
797940	800020	of data curation and data analysis.
800020	802340	So actually analyzing the holes
802340	804300	that you have in your data distribution.
804300	806580	And this is important for things like fairness and bias
806580	808980	and other stuff to remove that from the system is to,
808980	811500	is to try and really make sure that your data set
811500	813020	is representative of the distribution
813020	814220	you're trying to learn.
814220	816540	And, you know, there are many tricks there.
816540	818020	One can use like over-weighting
818020	820020	or replaying certain parts of the data.
820020	822540	Or you could imagine if you identify some,
822540	824020	some gap in your data set,
824020	826820	that's where you put your synthetic generation capabilities
826820	827660	to work on.
827660	830580	Yeah, so, you know, nowadays people are paying attention
830580	835420	to the RL stuff that Alfa deep-minded many years before.
835420	838060	What are the sort of either early research directions
838060	839860	or something that was done way back in the past,
839860	841820	but people just haven't been paying attention to,
841820	843100	that you think will be a big deal, right?
843100	844700	Like there's a time where people weren't paying attention
844700	845540	to scaling.
845540	847060	What's the thing now where it's like totally underrated?
847060	848500	Well, actually, I think that, you know,
848500	851100	there's the history of the sort of last couple of decades
851100	853300	has been things coming in and out of fashion, right?
853300	855980	And I do feel like a while ago,
855980	857500	when, you know, maybe five plus years ago,
857500	859220	when we were pioneering with AlphaGo
859220	862420	and before that, DQN, where it was the first system,
862420	863780	you know, that worked on Atari,
863780	866780	about how first big system really more than 10 years ago now
866780	869660	that scaled up Q learning and reinforcement learning techniques
869660	872140	to deal, you know, combine that with deep learning
872140	874020	to create deep reinforcement learning
874020	877260	and then use that to scale up to complete some,
877260	879180	you know, master some pretty complex tasks
879180	881380	like playing Atari games just from the pixels.
881380	885500	And I do actually think a lot of those ideas
885500	886940	need to come back in again.
886940	888100	And as we talked about earlier,
888100	891540	combine it with the new advances in large models
891540	892740	and large multimodal models,
892740	894060	which is obviously very exciting as well.
894060	896460	So I do think there's a lot of potential
896460	899460	for combining some of those older ideas together
899460	900460	with the newer ones.
900460	903460	Is there any potential for something to come,
903460	906380	the AGI to eventually come from just a pure RRL approach?
906380	907940	Like the way we're talking about it,
907940	909700	it sounds like there'll be,
909700	912140	the LLM will form the gripe fryer
912140	913820	and then this sort of research will go on top of that.
913820	915460	Or is it a possibility to just like completely
915460	916300	out of the dark?
916300	917220	I think I certainly, you know,
917220	918940	theoretically I think there's no reason
918940	921500	why you couldn't go full alpha zero like on it.
921500	925340	And there are some people here at Google DeepMind
925340	928140	and in the RL community who work on that, right?
928140	932540	And fully assuming no priors, no data
932540	935780	and just build all knowledge from scratch.
935780	938620	And I think that's valuable because of course, you know,
938620	940500	those ideas and those algorithms
940500	943420	should also work when you have some knowledge too.
943420	944260	But having said that,
944260	946380	I think by far probably my betting
946380	948220	would be the quickest way to get to AGI
948220	949820	in the most likely plausible way
949820	951660	is to use all the knowledge
951660	953180	that's existing in the world right now
953180	955140	on things like the web and that we've collected
955140	957660	and we have these scalable algorithms
957660	960740	like transformers that are capable
960740	963020	of ingesting all of that information.
963020	966660	And I don't see why you wouldn't start with a model
966660	969820	as a kind of prior or to build on and to make predictions
969820	971860	that helps bootstrap your learning.
971860	973980	I just think it doesn't make sense
973980	975260	not to make use of that.
975300	978820	So my betting would be is that, you know,
978820	983460	the final AGI system will have these large multimodals
983460	986180	as part of the overall solution
986180	988500	but probably won't be enough on their own.
988500	991260	You will need this additional planning search on top.
991260	992780	Okay, this sounds like the answer
992780	994980	to the question we're about to ask which is
994980	997420	what is somebody who's been in this field
997420	999580	for a long time and seen different trends come and go,
999580	1001460	what do you think that the strong version
1001460	1002820	of the scaling hypothesis gets right
1002820	1003660	and what does it get wrong?
1003660	1004580	It's just the idea that you just throw
1004580	1006540	and have computed a wide enough distribution of data
1006540	1007380	and you get intelligence.
1007380	1009180	Yeah, look, my view is this is kind
1009180	1010620	of an empirical question right now.
1010620	1012140	So I think it was pretty surprising
1012140	1015660	to almost everyone, including the people who first worked
1015660	1018140	on the scaling hypotheses that how far it's gone.
1018140	1021900	In a way, I mean, I sort of look at the large models today
1021900	1023900	and I think they're almost unreasonably effective
1023900	1024900	for what they are.
1024900	1027100	You know, I think it's pretty surprising some
1027100	1029540	of the properties that emerge, things like, you know,
1029540	1032860	it's clearly in my opinion got some form of concepts
1032860	1035060	and abstractions and some things like that.
1035060	1037340	And I think if we were talking five plus years ago,
1037340	1039540	I would have said to you, maybe we need an additional
1039540	1042540	algorithmic breakthrough in order to do that.
1042540	1045020	Like, you know, maybe more like the brain works.
1045020	1046540	And I think that's still true
1046540	1049580	if we want explicit abstract concepts, need concepts,
1049580	1052460	but it seems that these systems can implicitly learn that.
1052460	1055100	Another really interesting, I think an unexpected thing
1055100	1058620	was that these systems have some sort of grounding.
1058620	1060340	You know, even though they don't experience the world
1060340	1062140	multimodally or at least until more recently
1062140	1063820	we have the multimodal models.
1063820	1066740	And that's surprising that the amount of information
1066740	1069580	that can be, and models that can be built up
1069580	1070620	just from language.
1070620	1073700	And I think that I have some hypotheses about why that is.
1073700	1077060	I think we get some grounding through the RLHF feedback systems
1077060	1079900	because obviously the human raters are by definition
1079900	1084380	grounded people, we're grounded, right, in reality.
1084380	1086420	So our feedback's also grounded.
1086420	1088620	So perhaps there's some grounding coming in through there.
1088620	1091060	And also maybe language contains more grounding,
1091060	1093620	you know, if you're able to ingest all of it,
1093620	1097140	then we perhaps thought, linguists perhaps thought before.
1097140	1099300	So that's just some very interesting philosophical questions.
1099300	1101820	I think we haven't, people haven't even really
1101820	1103540	scratched the surface off yet,
1103540	1106020	looking at the advances that have been made.
1106900	1108220	You know, it's quite interesting to think about
1108220	1109700	where it's going to go next.
1109700	1112660	But in terms of your question of like the large models,
1112660	1116180	I think we've got to push scaling as hard as we can.
1116180	1117500	And that's what we're doing here.
1117500	1119140	And you know, it's an empirical question
1119140	1121860	whether that will hit an asymptote or brick wall.
1121860	1124260	And there are, you know, different people argue about that.
1124260	1125780	But actually, I think we should just test it.
1125780	1127420	I think no one knows.
1127420	1128940	And but in the meantime,
1128940	1132940	we should also double down on innovation and invention.
1132940	1135620	And this is something that the Google research
1135620	1138700	and DeepMind and Google Brain have, you know,
1138700	1140900	we've pioneered many, many things over the last decade.
1140900	1142700	That's something that's our bread and butter.
1142700	1145340	And, you know, you can think of half our effort
1145340	1147100	as to do with scaling and half our efforts
1147380	1149900	to do with inventing the next architectures,
1149900	1152100	the next algorithms that will be needed,
1152100	1155060	knowing that you've got this scaled larger and larger model
1155060	1156380	coming along the lines.
1156380	1159020	So my betting right now,
1159020	1161860	but it's a loose betting is that you would need both.
1161860	1163300	But I think, you know,
1163300	1165540	it's you've got to push both of them as hard as possible.
1165540	1167300	And we're in a lucky position that we can do that.
1167300	1168660	Yeah. I want to ask more about the grounding.
1168660	1170820	So you can imagine two things that might change,
1170820	1172580	which would make the grounding more difficult.
1172580	1174780	One is that if these models gets from Arder,
1174780	1177460	they're going to be able to operate in domains
1177460	1179660	where we just can generate enough human labels,
1179660	1180940	just because we're not smart enough, right?
1180940	1182860	So if it does like a million line pull request,
1182860	1184260	you know, how do we tell it?
1184260	1186580	Like this is within the constraints of our morality
1186580	1188700	and the end goal we wanted and this isn't.
1188700	1190820	And the other is it sounds like you're saying
1190820	1193100	more of the compute of so far we've been doing,
1193100	1194100	you know, next token prediction
1194100	1195500	and in some sense it's a guardrail
1195500	1197500	because you have to talk as a human would talk
1197500	1198940	and think as a human would think.
1198940	1201660	Now, if additional compute is going to come
1201660	1204140	in the form of reinforcement learning,
1204140	1206100	where just to get to the end objective,
1206100	1208780	we can't really trace how you got there.
1208780	1209940	When you combine those two,
1209940	1213180	how worried are you that the sort of grounding goes away?
1213180	1216780	Well, look, I think if the grounding,
1216780	1218260	you know, if it's not properly grounded,
1218260	1221020	the system won't be able to achieve those goals properly,
1221020	1221860	right? I think so.
1221860	1224620	I think in a sense you sort of have to have the grounding
1224620	1226820	or at least some of it in order for a system
1226820	1229420	to actually achieve goals in the real world.
1229420	1231900	I do actually think that as these systems
1232020	1234580	and things like Gemini are becoming more multimodal
1234580	1236900	and we start ingesting things like video
1236900	1241500	and, you know, audio visual data as well as text data.
1241500	1243180	And then, you know, the system starts
1243180	1244900	correlating those things together.
1245780	1249900	I think that is a form of proper grounding actually.
1249900	1254380	So I do think our systems are going to start to understand,
1254380	1256540	you know, the physics of the real world better.
1256540	1258820	And then one could imagine the active version of that
1258820	1260780	is being in a very realistic simulation
1260780	1263660	or game environment where you're starting to learn
1263660	1266060	about what your actions do in the world
1266060	1270340	and how that affects the world itself,
1270340	1271340	the world stay itself,
1271340	1274060	but also what next learning episode you're getting.
1274060	1276660	So, you know, these RL agents we've always been working on
1276660	1279180	and pioneered like AlphaZero and AlphaGo,
1279180	1281220	they actually affect their active learners.
1281220	1283260	What they decide to do next affects
1283260	1285980	what the next learning piece of data
1285980	1287620	or experience they're going to get.
1287620	1289380	So there's this very interesting sort of feedback loop.
1289380	1291940	And of course, if we ever want to be good at things like robotics,
1291940	1293300	we're going to have to understand
1293300	1295540	how to act in the real world.
1295540	1297300	Yeah, so there's a grounding in terms of
1297300	1299260	will the capabilities be able to proceed?
1299260	1301340	Will they be like enough in touch with the reality
1301340	1302820	to be able to like do the things we want?
1302820	1305300	And there's another sense of grounding of
1305300	1306580	we've gotten lucky in the sense that
1306580	1307940	since they're trained on human thought,
1307940	1309620	they like maybe think like a human.
1309620	1311380	To what extent does that stay true
1311380	1313780	when more of the compute for trading comes from
1313780	1316380	just did you get the right outcome and not guard real?
1316380	1318540	Like, are you like proceeding on the next token
1318540	1319380	as a human would?
1319380	1321780	Maybe the broader question I'll like post to you is
1321780	1323020	and this is what I asked Shane as well,
1323020	1324580	what would it take to align a system
1324580	1325620	that's smarter than a human?
1325620	1327860	Maybe things in alien concepts
1327860	1329260	and you can't like really monitor
1329260	1330260	the million line pull request
1330260	1332180	because you can't really understand the whole thing.
1332180	1334500	Yeah, I mean, look, this is something Shane and I
1334500	1335660	and many others here,
1335660	1337100	we've had that forefront of our minds
1337100	1339300	for since before we started DeMind
1339300	1341860	and because we planned for success crazy,
1341860	1343660	you know, 2010, no one was thinking about AI,
1343660	1344980	let alone AGI,
1344980	1347460	but we already knew that if we could make progress
1347460	1349460	with these systems and these ideas,
1349460	1351100	it, you know, the technology
1351100	1351940	where there would be creator
1351940	1353700	being unbelievably transformative.
1353700	1355460	So we already were thinking, you know,
1355460	1357540	20 years ago about, well, how, you know,
1357540	1359060	what would the consequences of that be?
1359060	1360540	Both positive and negative.
1360540	1363100	Of course, the positive direction is amazing science,
1363100	1364140	things like alpha fold,
1364140	1366500	incredible breakthroughs in health and science
1366500	1370220	and maths and discovery, scientific discovery.
1370220	1372060	But then also we got to make sure these systems
1372060	1374100	are sort of understandable and controllable.
1374100	1376180	And I think there's sort of several, you know,
1376180	1378220	this would be a whole sort of discussion in itself,
1378220	1380980	but there are many, many ideas that people have
1380980	1383500	from much more stringent eval systems.
1383500	1384580	I think we don't have good enough
1384580	1387300	at evaluations and benchmarks for things like
1387300	1389180	can the system deceive you?
1389180	1390540	Can it exotrate its own code?
1390540	1393100	It was sort of undesirable behaviors.
1393100	1395620	And then there's, you know,
1395620	1399500	ideas of actually using AI, maybe narrow AIs.
1399500	1400820	So not general learning ones,
1400820	1403460	but systems that are specialized for a domain
1403460	1407580	to help us as the human scientists analyze
1407580	1410860	and summarize what the more general system is doing, right?
1410860	1413540	So kind of narrow AI tools.
1413540	1415340	I think that there's a lot of promise
1415340	1418540	in creating hardened sandboxes or simulations
1418540	1422540	so that are hardened with cybersecurity arrangements
1424020	1427460	around the simulation, both to keep the AI in,
1427460	1430940	but also as cybersecurity to keep hackers out.
1430940	1433420	And then you could experiment a lot more
1433420	1435500	freely within that sandbox domain.
1435500	1438340	And I think a lot of these ideas are,
1438340	1439820	and there's many, many others,
1439820	1441940	including the analysis stuff we talked about earlier
1441940	1444380	where can we analyze and understand
1444380	1446420	what the concepts are that this system is building,
1446420	1447780	what the representations are.
1447780	1449700	So maybe they're not so alien to us
1449700	1451540	and we can actually keep track
1451540	1453820	of the kind of knowledge that it's building.
1453820	1454820	Yeah, yeah.
1454820	1455660	So big backup fit.
1455660	1456820	I'm curious what your timelines are.
1456820	1459540	So Shane said he's like, I think modal outcome is 2028.
1459540	1460580	I think that's maybe he's median.
1460580	1461420	Yeah.
1461420	1462260	What is yours?
1462380	1466060	I don't have prescribed kind of specific numbers to it
1466060	1468860	because I think there's so many unknowns and uncertainties
1468860	1472540	and human ingenuity and endeavor
1472540	1474380	comes up with surprises all the time.
1474380	1477940	So that could meaningfully move the timelines.
1477940	1481540	But I will say that when we started DeepMind back in 2010,
1481540	1483540	we thought of it as a 20 year project.
1483540	1485780	And actually, I think we're on track,
1485780	1487820	which is kind of amazing for 20 year projects.
1487820	1489820	Because usually they're always 20 years away.
1489820	1491700	So that's the joke about whatever it is
1491700	1494540	that you use in quantum AI, take your pick.
1494540	1496980	And but I think we're on track.
1496980	1500780	So I wouldn't be surprised if we had AGI like systems
1500780	1502220	within the next decade.
1502220	1504860	And do you buy the model that once you have an AGI,
1504860	1507580	you have a system that basically speeds up further AI research?
1507580	1509100	Maybe not like an overnight sense,
1509100	1510940	but over the course of months and years,
1510940	1511860	you have much faster progress
1511860	1512700	than you would have on the right side.
1512700	1515260	I think that's potentially possible.
1515260	1518220	I think it partly depends what we decide,
1518220	1520740	we as society decide to use the first
1520740	1524660	nascent AGI systems or even proto AGI systems for.
1524660	1529580	So, even the current LLMs seem to be pretty good at coding.
1529580	1532300	So, and we have systems like AlphaCode,
1532300	1533980	we also got theorem proving systems.
1533980	1537940	So one could imagine combining these ideas together
1537940	1539900	and making them a lot better.
1539900	1543460	And then I could imagine these systems being quite good
1543460	1547060	at designing and helping us build future versions
1547060	1548380	of themselves.
1548380	1550180	But we also have to think about the safety implications
1550220	1551100	of that, of course.
1551100	1552060	Yeah, I'm curious what you think about that.
1552060	1554380	So, I mean, I'm not saying this is happening this year
1554380	1556780	or anything, but eventually you'll be developing a model
1556780	1558260	where during the process of development,
1558260	1559980	you think, you know, there's some chance
1559980	1561340	that once this is fully developed,
1561340	1563300	it'll be capable of like an intelligence explosion
1563300	1564980	like dynamic.
1564980	1567780	What would have to be true of that model at that point
1567780	1569500	where you're like, you know,
1569500	1571020	I've seen these specific evals,
1571020	1573780	I've like, I've like understand it's internal thinking enough
1573780	1574860	and like it's future thinking
1574860	1577220	that I'm comfortable continuing development of the system.
1577220	1580300	Well, look, we need a lot more understanding
1580300	1581420	of the systems than we do today
1581420	1583060	before I would be even confident
1583060	1586900	of even explaining to you what we would need to tick box there.
1586900	1588620	So I think actually what we've got to do
1588620	1590300	in the next few years and the time we have
1590300	1593540	before those systems start arriving is come up
1593540	1596220	with the right evaluations and metrics
1596220	1598580	and maybe ideally formal proofs,
1598580	1600020	but you know, it's going to be hard
1600020	1600940	for these types of systems,
1600940	1603860	but at least empirical bounds
1603860	1606100	around what these systems can do.
1606100	1609380	And that's why I think about things like deception
1609380	1612340	and has been quite root node traits that you don't want
1612340	1614540	because if you're confident that your system
1614540	1618780	is sort of exposing what it actually thinks,
1618780	1620180	then you could potentially,
1620180	1623020	that opens up possibilities of using the system itself
1623020	1626020	to explain aspects of itself to you.
1626020	1628620	The way I think about that actually is like,
1628620	1631020	if I was to play a game of chess against Gary Kasparov,
1631020	1633260	right, which I played in the past or Magnus Carlson,
1633260	1635660	you know, the amazing chess players with graceful time,
1636100	1638820	I wouldn't be able to come up with a move that they could,
1638820	1642860	but they could explain to me why they came up
1642860	1646740	with that move and I could understand it post hoc, right?
1646740	1649220	And that's the sort of thing one could imagine
1650580	1654700	one of the capabilities that we could make use
1654700	1657540	of these systems is for them to explain it to us
1657540	1659780	and even maybe the proofs behind why they're thinking
1659780	1661860	something, certainly in a mathematical,
1661860	1662980	any mathematical problem.
1662980	1663820	Got it.
1663820	1666220	Do you have a sense of what the converse answer would be?
1666220	1668220	So what would have to be true where tomorrow morning,
1668220	1670660	you're like, oh man, I didn't anticipate this.
1670660	1672220	You see some specific observation tomorrow morning
1672220	1674100	where like, we got to stop Gemini II training.
1674100	1675860	Like, what would specifically...
1675860	1677500	Yeah, I could imagine that.
1677500	1680700	And this is where things like the sandbox simulations,
1680700	1683020	I would hope we're experimenting
1683020	1686060	in a safe, secure environment.
1686060	1688700	And then something happens in it
1688700	1691220	where very unexpected happens
1691220	1693060	and you unexpected capability
1693060	1695660	or something that we didn't want explicitly told the system
1695660	1698460	we didn't want and that it did, but then lied about.
1698460	1701540	These are the kinds of things where one would want to
1701540	1706540	then dig in carefully with the systems that are around today
1706540	1709140	which are not dangerous in my opinion today,
1709140	1712460	but in a few years they might be, have potential.
1713420	1717020	And then you would sort of ideally kind of pause
1717020	1720580	and then really get to the bottom of why it was doing
1720580	1722500	those things before one continued.
1722500	1724020	Yeah, going back to Gemini,
1724020	1727340	I'm curious what the bottlenecks were in the development.
1727340	1729540	Like, why not make it immediately one order of magnitude
1729540	1732460	bigger if scaling works?
1732460	1734620	Well, look, first of all, there are practical limits.
1734620	1737940	How much compute can you actually fit in one data center?
1737940	1741500	And actually, you're bumping up against very interesting
1744380	1746700	distributed computing kind of challenges, right?
1746700	1748540	Unfortunately, we have some of the best people in the world
1748660	1751900	on those challenges and cross data center training,
1751900	1754260	all these kinds of things, very interesting challenges,
1754260	1756860	hardware challenges, and we have our TPUs and so on
1756860	1758780	that we're building and designing all the time
1758780	1760500	as well as using GPUs.
1760500	1762820	And so there's all of that.
1762820	1765740	And then you also have to, the scaling laws,
1765740	1767220	they didn't just work by magic.
1767220	1770060	You sort of, you still need to scale up the hyperparameters
1770060	1772380	and various innovations are going in all the time
1772380	1773300	with each new scale.
1773300	1775980	It's not just about repeating the same recipe.
1775980	1778460	At each new scale, you have to adjust the recipe
1779180	1780940	and that's a bit of an art form in a way.
1780940	1783300	And you have to sort of almost get new data points.
1783300	1785340	If you try and extend your predictions
1785340	1788500	and extrapolate them, say several orders of magnitude out,
1788500	1790460	sometimes they don't hold anymore, right?
1790460	1792580	Because new capabilities,
1792580	1795540	there can be step functions in terms of new capabilities
1795540	1798300	and some things just, some things hold
1798300	1799300	and other things don't.
1799300	1802380	So often you do need those intermediate data points
1802380	1806380	actually to correct some of your hyperparameter optimization
1806380	1807220	and other things.
1807580	1809820	That the scaling law continues to be true.
1809820	1813700	So there's sort of various practical limitations
1813700	1815380	on to that.
1815380	1818700	So kind of one order of magnitude is about probably
1818700	1821420	the maximum that you want to carry on.
1821420	1824140	You want to sort of do between each era.
1824140	1825700	Oh, that's so fascinating.
1825700	1826980	In the GPT for technical report,
1826980	1829660	they say that they were able to predict the training loss
1831340	1833860	tens of thousands of times less compute than GPT-4
1833860	1834860	that they could see the curve.
1834860	1836900	But at the point you're making is that the actual capabilities
1836900	1839100	that loss implies may not be so clear.
1839100	1841140	Yeah, the downstream capabilities sometimes don't follow
1841140	1843580	from the, you can often predict the core metrics
1843580	1845660	like training loss or something like that.
1845660	1848820	But then it doesn't actually translate into MMLU
1848820	1852500	or math or some other actual capability
1852500	1853340	that you care about.
1853340	1856020	They're not necessarily linear all the time.
1856020	1857580	So there's sort of non-linear effects there.
1857580	1858580	What was the biggest surprise to you
1858580	1860020	during the development of Gemini?
1860020	1862620	So something like this happening?
1862620	1865460	Well, I mean, I wouldn't say there was one big surprise,
1865460	1868060	but it was very interesting trying to train things
1868060	1873060	at that size and learning about all sorts of things
1873060	1875460	from organizational, how to babysit such a system
1875460	1876660	and to track it.
1876660	1880900	And I think things like getting a better understanding
1880900	1883180	of the metrics you're optimizing
1883180	1886740	versus the final capabilities that you want.
1886740	1890460	I would say that's still not a perfectly understood mapping.
1890460	1891580	But it's an interesting one
1891580	1892860	that we're getting better and better at.
1892860	1893700	Yeah, yeah.
1893700	1894900	There's a perception that maybe other labs
1894900	1898660	are more compute efficient than DeepMind has been
1898660	1899500	with Gemini.
1899500	1900620	I don't know what you make of that for something.
1900620	1901820	I don't think that's the case.
1901820	1906660	I mean, I think that actually Gemini one
1906660	1908220	use roughly the same amount of compute,
1908220	1910500	maybe slightly more than what was rumored for GPT-4.
1910500	1911980	I don't know exactly what was used.
1911980	1915620	So I think it was in the same ballpark.
1915620	1917220	I think we're very efficient with our compute
1917220	1919260	and we use our compute for many things.
1919260	1920340	One is not just the scaling,
1920340	1922820	but going back to earlier to these more innovation
1923460	1925540	and ideas, you've got to,
1925540	1928300	it's only useful a new innovation, a new invention
1928300	1930340	if it's also can scale.
1930340	1933580	So in a way, you also need quite a lot of compute
1933580	1937140	to do new invention because you've got to test many things
1937140	1938980	at least some reasonable scale
1938980	1940860	and make sure that they work at that scale.
1940860	1944020	And also some new ideas may not work at a toy scale,
1944020	1946060	but do work at a larger scale.
1946060	1947740	And in fact, those are the more valuable ones.
1947740	1950300	So you actually, if you think about that exploration process,
1950300	1953500	you need quite a lot of compute to be able to do that.
1953500	1956420	I mean, the good news is, is I think we,
1956420	1958300	we're pretty lucky at Google that we,
1958300	1960060	I think this year certainly we're going to have
1960060	1962860	the most compute by far of any sort of research lab.
1962860	1965420	And we hope to make very efficient and good use of that
1965420	1969380	in terms of both scaling and the capability of our systems
1969380	1970980	and also new inventions.
1970980	1971820	Yeah.
1971820	1973100	What's been the biggest surprise to you
1973100	1975580	if you go back to yourself in 2010
1975580	1976580	when you were starting DeepMind
1976580	1978660	in terms of what AI progress has looked like?
1978660	1981660	Did you anticipate back then that it would in some large sense
1981660	1983980	amount to spend as, you know, dumping billions of dollars
1983980	1984820	into these models?
1984820	1985940	Or did you have a different sense of what it would look like?
1985940	1987460	We thought that, and actually, you know,
1987460	1989780	if you, I know you've interviewed my colleague Shane
1989780	1994660	and he always thought that in terms of like compute curves
1994660	1997420	and then maybe comparing roughly to like the brain
1997420	1999860	and how many neurons and synapses there are very loosely,
1999860	2002420	but we're actually interestingly in that kind of regime
2002420	2004660	that roughly in the right order of magnitude of,
2004660	2006220	you know, number of synapses in the brain
2006380	2008780	and the sort of compute that we have.
2008780	2010980	But I think more fundamentally, you know,
2010980	2016340	we always thought that we bet on generality and learning, right?
2016340	2019820	So those were always at the core of the any technique we would use.
2019820	2021980	That's why we triangulated on reinforcement learning
2021980	2024820	and search and deep learning, right?
2024820	2028780	As three types of algorithms that would scale
2028780	2031620	and would be very general
2031620	2035020	and not require a lot of handcrafted human priors,
2035020	2037580	which we thought was the sort of failure mode, really,
2037580	2040940	of the efforts to build AI in the 90s, right?
2040940	2044580	Places like MIT where there were very logic-based systems,
2044580	2047740	expert systems, you know, masses of hand-coded,
2047740	2050060	hand-crafted human information going into them
2050060	2052420	that turned out to be wrong or too rigid.
2052420	2053860	So we wanted to move away from that.
2053860	2057300	And I think we spotted that trend early and became, you know,
2057300	2059780	and obviously we use games as our proving ground
2059780	2061220	and we did very well with that.
2061220	2063220	And I think all of that was very successful
2063220	2066900	and I think maybe inspired others to, you know, things like AlphaGo.
2066900	2069740	I think it was a big moment for inspiring many others to think,
2069740	2072420	oh, actually, these systems are ready to scale.
2072420	2074540	And then, of course, with the advent of Transformers
2074540	2077540	invented by our colleagues at Google, you know, research and brain,
2077540	2080740	that was then, you know, the type of deep learning
2080740	2084500	that allowed us to ingest masses of amounts of information.
2084500	2087780	And that, of course, is really turbocharged where we are today.
2087780	2089700	So I think that's all part of the same lineage.
2089700	2092740	You know, we couldn't have predicted every twist and turn there,
2092740	2096940	but I think the general direction we were going in was the right one.
2096940	2099660	Yeah. And in fact, it's like fascinating because actually,
2099660	2102060	if you like read your old papers or Shane's old papers,
2102060	2104500	Shane's thesis, I think in 2009, he said, like, well, you know,
2104500	2107020	the way we would test for AI is if it can you come press Wikipedia.
2107020	2108780	And that's like literally the last function of our labs
2108780	2111340	or like your own paper in like 2016 before Transformers
2111340	2114340	where we said, like, you were comparing your science and AI.
2114340	2116260	And he said, attention is what is needed.
2116260	2117540	Exactly. Exactly.
2117540	2120900	So we had these things called out and actually we had some early attention
2120980	2124580	papers, but they weren't as elegant as Transformers in the end,
2124580	2126380	like, Neuroturing Machines and things like this.
2126380	2129260	Yeah. And then Transformers was the was the nicer
2129260	2130820	and more general architecture of that.
2130820	2131980	Yeah, yeah, yeah.
2131980	2134140	When you extrapolate all this out forward,
2134140	2138460	anything about superhuman intelligence or is like,
2138460	2139740	what does that landscape look like to you?
2139740	2142420	Is it like still controlled by a private company?
2142420	2145340	Like, what should the governance of that look like concretely?
2145340	2149020	Yeah, look, I would love, you know, I think that this has to be.
2149980	2152140	This is so consequential, this technology.
2152140	2157340	I think it's much bigger than any one company or or or even industry in general.
2157340	2161340	I think it has to be a big collaboration with many stakeholders
2161500	2164380	from civil society, academia, government.
2164540	2168060	And the good news is I think with the popularity of the recent chatbot systems
2168060	2172540	and so on, I think that has woken up many of these other parts of society
2172540	2175900	that this is coming and what it will be like to interact with these systems.
2176060	2176740	And that's great.
2176780	2180100	So it's opened up lots of doors for very good conversations.
2180220	2184620	I mean, an example of that was the safety summit in the UK hosted a few months ago,
2184620	2188020	which I thought was a big success to start getting this international dialogue going.
2188260	2192420	And and and, you know, I think the whole society needs to be involved in deciding
2192420	2194620	what do we want to deploy these models for?
2194620	2195660	How do we want to use them?
2195660	2197140	What do we not want to use them for?
2197140	2200380	You know, I think we've got to try and get some international consensus around that.
2200820	2204260	And then also making sure that the benefits of these systems
2205140	2208340	benefit everyone, you know, for the good of everyone and society in general.
2208500	2211580	And that's why I push so hard things like AI for science.
2211580	2215140	And and I hope that, you know, with things like our spin out isomorphic,
2215140	2218300	we're going to start curing diseases, you know, terrible diseases with AI
2218300	2221980	and accelerate drug discovery, amazing things, climate change and other things.
2221980	2225180	I think big challenges that face us and face humanity.
2225900	2229100	Massive challenges, actually, which I'm optimistic we can solve
2229540	2233420	because we've got this incredibly powerful tool coming along down the line of AI
2233940	2237740	that we can apply and I think help us and solve many of these problems.
2237740	2240380	So, you know, ideally, we would have a big
2241420	2245260	consensus around that and a big discussion, you know, sort of almost like
2245260	2247140	the UN level, if possible.
2247140	2249500	You know, one interesting thing is if you look at these systems,
2249500	2252940	they you chat with them and they're immensely powerful and intelligent.
2253540	2257020	But it's interesting to the extent of which they haven't like automated
2257020	2258700	large sections of the economy yet.
2258700	2261260	Whereas a five years ago, I showed you a Gemini, you'd be like, wow,
2261260	2263500	this is like, you know, totally coming for a lot of things.
2263660	2265100	So how do you account for that?
2265100	2268180	Like what's going on where it hasn't had a broader impact yet?
2268180	2270980	I think it's we're still I think that just shows we're still at the beginning
2270980	2273380	of this new era. Yeah.
2273380	2276180	And I think that for these systems, I think there are some interesting
2276180	2280420	use cases, you know, you know, where you can use things to some,
2280420	2284020	you know, these these these chatbot systems to summarize stuff for you
2284020	2289780	and and maybe do some simple writing and maybe more kind of boilerplate type writing.
2289940	2293700	But that's only a small part of what, you know, we all do every day.
2293700	2298140	So I think for more general use cases, I think we need still need new
2298140	2302460	capabilities, things like planning and search, but also maybe things like
2302460	2306020	personalization and memory, episodic memory.
2306020	2309340	So not just long context windows, but actually remembering what I what
2309340	2311420	we spoke about a hundred conversations ago.
2312220	2315820	And I think once they start coming in, I mean, I'm really looking forward
2315820	2319540	to things like recommendation systems that that help me find better,
2319540	2323300	more enriching material, whether that's books or films or music and so on.
2323420	2325260	You know, I would use that type of system every day.
2325260	2329820	So I think we're just scratching the surface of what these AI,
2330020	2334420	say, assistants could actually do for us in our general everyday lives.
2334580	2338420	And also in our work context as well, I think they're not reliable yet enough
2338420	2340420	to do things like science with them.
2340420	2343900	But I think one day, you know, once we fix factuality and grounding and other things,
2344460	2347140	I think they could end up becoming like, you know, the world's best
2347140	2352580	research assistant for you as a scientist or as a clinician.
2353540	2356460	I want to ask about memory, by the way, you had this fascinating paper
2356460	2360220	in 2007 where you talk about the links between memory and imagination
2360220	2362060	and how they, in some sense, are very similar.
2363860	2366340	People often claim that these models are just memorizing.
2366580	2368860	How do you think about that claim that people make?
2369300	2370900	Is memorization all you need?
2370900	2372900	Because in some some deep sense, that's compression.
2372900	2374300	Or, you know, what's your intuition?
2374300	2377740	Yeah, I mean, sort of at the limit, one maybe could try and memorize everything,
2377740	2380180	but it wouldn't generalize out of out of your distribution.
2380180	2383340	And I think these systems are clearly I think the early the early
2384940	2389020	criticisms of these early systems were that they were just regurgitating
2389020	2393300	and memorizing, but I think clearly the new era, the Gemini GPT-4 type era,
2393300	2396300	they are definitely generalizing to new constructs.
2397380	2400580	So but actually, you know, in my thesis and that paper,
2400580	2404860	particularly, that started that era of imagination in neuroscience was showing
2404860	2407900	that, you know, first of all, memory, certainly at least human memory
2407900	2409220	is a reconstructive process.
2409220	2410300	It's not a videotape, right?
2410300	2413820	We sort of put it together back from components that seems familiar to us,
2413820	2416860	that the ensemble, and that's what made me think that imagination
2416860	2420980	might be the same thing, except in this case, you're using the same semantic components.
2421140	2424140	But now you're putting it together into a way that your brain thinks is novel,
2424260	2426060	right, for a particular purpose like planning.
2426300	2431620	And and so I do think that that kind of idea is still probably missing
2431620	2436180	from our current systems, this sort of pulling together different parts
2436180	2440620	of your world model to simulate something new that then helps with your planning,
2440940	2443020	which is what I would call imagination.
2443020	2443860	Yeah, for sure.
2443860	2446580	So again, now you guys have the best models in the world,
2447300	2449780	you know, with the Gemini models.
2449780	2453060	Do you have do you plan on putting out some sort of framework like the other
2453060	2456500	two major labs have of, you know, once we see these specific capabilities,
2456820	2460420	unless we have these specific safeguards, we're not going to continue development
2460420	2462580	or we're not going to ship the product out.
2462580	2466060	Yes, we have actually we I mean, we have already lots of internal checks
2466060	2469020	and balances, but we're going to start publishing actually, you know,
2469020	2470100	sort of watch the spaces.
2470100	2473580	We're working on a whole bunch of blog posts and technical papers
2473860	2477420	that we'll be putting out in the next few months that, you know,
2477420	2480300	along the similar lines of things like responsible scaling laws and so on.
2480420	2485380	We have those implicitly internally in various safety councils and so on,
2485380	2487780	people like Shane, Chair and so on.
2487780	2490980	But but it's time for us to talk about that more publicly, I think.
2490980	2493100	So we'll be doing that throughout the course of the year.
2493100	2494060	That's great to hear.
2494060	2497500	And another thing I'm curious about is so it's not only the risk of,
2497500	2501700	like, you know, the deployed model being something that people can use to do bad things,
2501700	2506700	but also rogue actors, bad foreign agents, so forth, being able to steal the weights
2506700	2508260	and then fine tune them to do crazy things.
2508660	2512620	How do you think about securing the weights to make sure something like this
2512620	2516700	doesn't happen, making sure a very like key group of people have access to them
2516700	2517140	and so forth?
2517140	2517900	Yeah, it's interesting.
2517900	2519500	So first of all, there's sort of two parts of this.
2519500	2522180	One is security, one is open source, maybe we can discuss.
2522180	2524380	But the security, I think, is super key.
2524380	2528100	Like just a sort of normal cyber security type things.
2528100	2530140	And I think we're lucky at Google DeepMind.
2530140	2534260	We're kind of behind Google's firewall and cloud protection, which is, you know,
2534260	2537620	I think best, you know, best in class in the world, corporately.
2537740	2539260	So we already have that protection.
2539260	2545260	And then behind that, we have specific DeepMind protections within our code base.
2545260	2547300	So it's sort of a double layer of protection.
2547300	2548700	So I feel pretty good about that.
2548700	2551700	That that's, I mean, we, you know, you can never be complacent on that.
2551700	2557580	But I feel it's already sort of best in the world in terms of cyber defences.
2557580	2559500	But we've got to carry on improving that.
2559500	2563740	And again, things like the hard and sandboxes could be a way of doing that as well.
2563780	2568540	And maybe even there are, you know, specifically secure data centers
2568540	2571060	or hardware solutions to this, too, that we're thinking about.
2571060	2575820	I think that maybe in the next three, four, five years, we would also want air gaps
2575820	2578980	and various other things that are known in the security community.
2578980	2579780	So I think that's key.
2579780	2583020	And I think all frontier labs should be doing that because otherwise, you know,
2583020	2586780	nation states and other things, rogue nation, you know, states and other other
2586780	2590180	dangerous actors, that there would be obviously a lot of incentive for them
2590180	2591780	to to steal things like the weights.
2592700	2595580	And then, you know, of course, open source is another interesting question,
2595580	2598420	which is we're huge proponents of open source and open science.
2598420	2601420	I mean, almost every, you know, we've published thousands of papers
2601420	2604420	and things like Alpha Fold and Transformers, of course.
2604420	2608620	And Alpha Gold, all of these things we put out there into the world, published
2608620	2613220	and open source, many of them, GraphCast, most recently, our weather prediction system.
2613220	2617860	But when it comes to, you know, the core technology, the foundational technology
2617860	2621460	and very general purpose, I think the question I would have is,
2622380	2627020	if you, you know, first of all, open source proponents is that how does one
2627020	2634020	stop bad actors, individuals or, you know, up to rogue states, taking those
2634020	2638020	same open source systems and repurposing them because their general purpose
2638020	2639780	for harmful ends, right?
2639780	2641860	So we have to answer that question.
2641860	2645780	And I haven't heard a compelling, I mean, I don't know what the answer is to that,
2645780	2650700	but I haven't heard a compelling, clear answer to that from proponents
2650900	2652900	of just sort of open sourcing everything.
2652900	2655940	So I think there has to be some balance there, but, you know,
2655940	2658180	obviously it's a complex question of what that is.
2658180	2661100	Yeah, yeah, I feel like tech doesn't get the credit it deserves for, like,
2661100	2663420	funding, you know, hundreds of billions of dollars worth of R&D.
2664220	2666900	And, you know, obviously I have deep bind with systems like Alpha Fold and so on.
2667620	2670940	Well, but when we talk about securing the weights, you know, as we said,
2670940	2673900	like maybe right now, it's not something that, like, is going to cause the end
2673900	2675980	of the world or anything, but as these systems get better and better,
2675980	2679300	the worry that, yes, a foreign agent or something gets access to them.
2679580	2682020	Presumably right now, there's like dozens to hundreds of researchers
2682020	2683220	who have access to the weights.
2683220	2686020	How do you, well, what's a plan for, like, getting into, like,
2686020	2687780	the situation or getting the weights in the situation rooms?
2687780	2690580	If you're like, if you need to access to them, it's like, you know,
2690580	2692060	some extremely strenuous process.
2692060	2694340	You know, nobody, nobody individual can really take them out.
2694340	2695020	Yeah, yeah.
2695020	2698540	I mean, one has to balance that with, with, with allowing for collaboration
2698540	2699420	and speed of progress.
2699420	2703140	Actually, another interesting thing is, of course, you want, you know,
2703140	2707500	brilliant independent researchers from academia or, or things like the UK
2707580	2713860	AI Safety Institute and US1 to be able to kind of red team these systems.
2713860	2717420	So, so one has to expose them to a certain extent, although that's not
2717420	2718340	necessarily the weights.
2718980	2722900	And then, you know, we have a lot of processes in place about making sure
2722900	2727020	that, you know, only if you need them that, that you have access to, you
2727020	2729260	know, those people who need access, have access.
2729900	2733740	And right now, I think we're still in the early days of those kinds of
2733740	2735220	systems being at risk.
2735420	2738340	And as that, as these systems become more powerful and more general and
2738340	2741940	more capable, I think one has to look at the, the access question.
2742820	2745860	So some of these other labs have specialized in different things relative
2745860	2748460	to safety, like Anthropoc, for example, with interoperability.
2748460	2752900	And do you have some sense of where you guys might have an edge where as so
2752900	2754980	that, you know, now that you have the frontier model, you're going to
2754980	2758100	scale up safety, where you guys are going to be able to put out the best
2758100	2759180	frontier research on safety.
2759180	2762380	I think, you know, well, we helped pioneer RLHF and other things like that,
2762380	2765340	which can also be obviously used for performance, but also for safety.
2766220	2771100	I think that, you know, a lot of the self-play ideas and these kinds of
2771100	2777620	things could also be used potentially to, to auto-test a lot of the boundary
2777620	2779540	conditions that you have with the new systems.
2779700	2783420	I mean, part of the issue is that with these sort of very general systems,
2784020	2788140	there's so much surface area to cover, like about how these systems behave.
2788340	2791780	So I think we are going to need some automated testing.
2791940	2796540	And again, with things like simulations and games environment, very realistic
2796540	2800740	environments, virtual environments, I think we have a long history in that
2800740	2806260	and using those kinds of systems and making use of them for building AI algorithms.
2806260	2809060	So I think we can leverage all of that history.
2809700	2812380	And then, you know, around at Google, we're very lucky we have some of the
2812380	2815940	world's best cybersecurity experts, hardware designers.
2816140	2820380	So I think we can bring that to bear in, you know, for security and safety as well.
2820580	2822260	Great, great. Let's talk about Gemini.
2822820	2825060	So, you know, now you guys have the best model in the world.
2826260	2829900	So I'm curious, you know, the default way to interact with these systems has
2829900	2832060	been through chat so far.
2832220	2834900	Now that we have multimodal and all these new capabilities, how do you
2835020	2835780	anticipate that changing?
2835780	2837060	Or do you think that will still be the case?
2837580	2840420	Yeah, I think we're just at the beginning of actually understanding what a
2840420	2845820	full multimodal model system, how exciting that might be to interact with
2845820	2849540	them, and it will be quite different to, I think, what we're used to today with
2849540	2850260	the chat bots.
2850260	2854980	I think the next versions of this over the next year, 18 months, you know,
2855180	2858500	maybe we'll have some contextual understanding around the environment around
2858500	2860740	you through a camera or whatever it is, a phone.
2861620	2864580	You know, I could imagine that as the next awesome glasses at the next step.
2865340	2870060	And then I think that we'll start becoming more fluid in understanding, oh,
2870260	2872260	let's sample from a video.
2872260	2873620	Let's use voice.
2874620	2879460	Maybe even eventually things like touch and, you know, if you think about robotics
2879460	2882420	and other things, you know, sensors, other types of sensors.
2882620	2885580	So I think the world's about to become very exciting.
2885580	2888540	I think in the next few years, as we start getting used to the idea of what
2888700	2890300	true multimodality means.
2891500	2895620	On the robotic subject, Ilya said when he was on the podcast that the reason
2895620	2898740	opening I gave up on robotics was because they didn't have enough data in that
2898740	2900300	domain, at least at the time they were pursuing it.
2901300	2904220	I mean, you guys have put out different things like Robo Transformer and other things.
2904420	2907780	How do you think that's still a bottleneck for robotics progress or will we
2907780	2910700	see progress in the world of atoms as well as the world of bits?
2910700	2915060	We're very excited about our progress with things like GATO and RT2, you know,
2915060	2920180	Robotic Transformer, and we actually think so we've always liked robotics
2920180	2924420	and we've had, you know, amazing research and now we still have that going now
2924620	2928940	because we like the fact that it's a data poor regime because that pushes us
2929140	2931980	on some very interesting research directions that we think are going to be
2932180	2936100	useful anyway, like sampling efficiency and data efficiency in general and transfer
2936300	2939700	learning, learning from simulation, transferring that to reality.
2939900	2943860	All of these very, you know, similar to real, all of these very interesting
2944060	2947620	actually general challenges that we would like to solve.
2947820	2949220	So the control problem.
2949420	2951420	So we've always pushed hard on that.
2951620	2955580	And actually, I think so Ilya is right that that is more challenging
2955580	2959820	because of the data problem, but it's also I think we're starting to see the
2960020	2965500	beginnings of these large models being transferable to the robotics regime,
2965700	2968340	learning in the general domain, language domain and other things.
2968540	2972580	And then just treating tokens like GATO as any type of token, you know,
2972580	2975900	the token could be an action, it could be a word, it could be part of an image,
2975900	2977100	a pixel or whatever it is.
2977300	2979660	And that's what I think true multimodality is.
2979860	2984140	And to begin with, it's harder to train a system like that than a straightforward
2984260	2986340	text language system.
2986540	2988860	But actually, you know, going back to our
2989060	2993860	early conversation of transfer learning, you start seeing that a true multimodal
2994060	2998260	system, the other modalities benefit some different modality.
2998260	3002020	So you get better at language because you now understand a little bit about video.
3002220	3007340	So I do think it's harder to get going, but actually ultimately
3007540	3010220	we'll have a more general, more capable system like that.
3010420	3011460	Whatever happened to GATO?
3011700	3014820	That was super fascinating that you could have like play games and also do like
3015020	3016180	video and also do text.
3016380	3019780	We're still working on those kinds of systems, but you can imagine we're just
3019980	3025060	trying to, those ideas we're trying to build into our future generations of Gemini.
3025260	3030100	You know, to be able to do all of those things and robotics transformers and things like
3030300	3033620	that, you can think of them as sort of follow-ups to that.
3033820	3037180	Well, we see asymmetric progress towards the domains in which the self-play
3037180	3039660	kinds of things we're talking about will be especially powerful.
3039660	3042740	So math and code, you know, obviously recently you have these papers out about
3042940	3047780	this or yeah, you can use these things to do really cool novel things.
3047980	3049860	Will they just be like superhuman coders?
3049860	3052140	But like in other ways, they might be still worse than humans?
3052140	3052820	Or how do you think about that?
3053020	3058940	So look, I think that we're making great progress with math and things like
3059140	3063660	theorem proving and coding, but it's still interesting.
3063860	3068540	If one looks at, I mean, creativity in general and scientific endeavor in general,
3068660	3072260	I think we're getting to the stage where our systems could help the best human
3072460	3076260	scientists make their breakthroughs quicker, like almost triage the search space
3076460	3081380	in some ways or perhaps find a solution like AlphaFold does with a protein structure.
3081580	3085660	But it can't, they're not at the level where they can create the hypothesis
3085860	3088060	themselves or ask the right question.
3088260	3092380	And as any top scientists will tell you, that that's the hardest part of science
3092580	3096380	is actually asking the right question, boiling down that space to like, what's
3096420	3099620	the critical question we should go after the critical problem and then
3099820	3102100	formulating that problem in the right way to attack it.
3102300	3106740	And that's not something our systems will we have really have any idea how our
3106940	3109620	systems could do, but they can.
3109820	3114900	They are suitable for searching large combinatorial spaces if one can specify
3115100	3117580	the problem in that way with a clear objective function.
3117780	3121500	So that's very useful for already many of the problems we deal with today,
3121700	3125140	but not the most high level creative problems.
3125300	3129260	Right, so deep mind obviously has published all kinds of interesting stuff
3129460	3132020	and, you know, speeding up science in different areas.
3132220	3135460	How do you think about that in the context of if you think AGI is going to happen
3135660	3139300	in the next 10, 20 years, why not just wait for the AGI to do it for you?
3139500	3141500	Why build these domain specific solutions?
3141700	3143580	Well, I think
3143780	3145740	we don't know how long AGI is going to be.
3145940	3151660	And we always used to say, you know, back even when we started DeepMind that
3151900	3157300	we don't have to wait for AGI in order to bring incredible benefits to the world.
3157500	3164140	And especially, you know, my personal passion has been AI for science and health.
3164340	3167940	And you can see that with things like AlphaFold and all of our various
3167940	3170940	nature papers of different domains and material science work and so on.
3170940	3174460	I think there's lots of exciting directions and also impact in the world
3174460	3175300	through products, too.
3175500	3179100	I think it's very exciting and a huge opportunity, a unique opportunity we have
3179140	3185980	as part of Google, of, you know, they got dozens of billion user products, right?
3186180	3190220	That we can immediately ship our advances into and then
3190420	3194740	billions of people can, you know, improve their daily lives, right?
3194740	3197340	And enriches their daily lives and enhances their daily lives.
3197540	3201500	So I think it's a fantastic opportunity for impact on all those fronts.
3201700	3206020	And I think the other reason from a point of view of AGI specifically is
3206220	3209260	that it battle tests your ideas, right?
3209460	3213180	So you don't want to be in a sort of research bunker where you just,
3213380	3215900	you know, theoretically are pushing things, some things forward.
3216100	3220620	But then actually your internal metrics start deviating from
3220820	3223420	real world things that people would care about, right?
3223620	3224820	Or real world impact.
3225020	3228300	So you get a lot of feedback, direct feedback from these real world
3228500	3232860	applications that then tells you whether your systems really are scaling or or
3232900	3236900	actually is, you know, do we need to be more data efficient or sample efficient
3237100	3240780	because most real world challenges require that, right?
3240980	3245460	And so it kind of keeps you honest and pushes you, you know, keep sort of
3245660	3249660	nudging and steering your research directions to make sure they're on the right path.
3249660	3251060	So I think it's fantastic.
3251060	3255060	And of course, the world benefits from that society benefits from that on the way.
3255260	3258140	Many, many, maybe many, many years before AGI arrives.
3258340	3262100	Yeah. Well, the development of Gemini is super interesting because it comes right
3262140	3266380	at the heels of merging these different organizations, Brain and DeepMind.
3266580	3268540	Yeah, I'm curious, what have been the challenges there?
3268540	3270180	What have been the synergies?
3270380	3272740	And it's been successful in the sense that you have the best model in the world now.
3272940	3276180	Well, look, it's been fantastic actually over the last year.
3276180	3280340	Of course, it's been challenging to do that, like any big integration coming together.
3280540	3284500	But you're talking about two, you know, world-class organizations,
3284700	3288740	long storied histories of inventing many, many important things, you know,
3288740	3290620	from deep reinforcement learning to transformers.
3290740	3294180	And so it's very exciting, actually, pooling all of that together and
3294380	3295980	and collaborating much more closely.
3296180	3300060	We always used to be collaborating, but more on a on a on a, you know,
3300260	3305180	sort of project by project basis versus a much deeper, broader collaboration
3305380	3310100	like we have now in Gemini is the first fruit of of that collaboration,
3310300	3313060	including the name Gemini actually, you know, implying twins.
3313260	3316220	And and of course, a lot of other things are made more efficient,
3316420	3319780	like pooling compute resources together and ideas and engineering,
3320020	3324500	which I think at the stage we're at now, where there's huge amounts of world-class
3324500	3327620	engineering that has to go on to build the frontier systems.
3327820	3330780	I think it makes sense to to coordinate that more closely.
3330780	3334740	Yeah. So I mean, you and Shane started DeepMind partly because you were concerned
3334940	3338540	about safety and you saw AGI coming as like a live possibility.
3338740	3342820	Do you do you think the people who were formerly part of brain, the half of Google
3342820	3345220	DeepMind now, do they do you think they approach it in the same way?
3345220	3347260	Have there been cultural differences there in terms of that question?
3347300	3348620	Yeah, no, I think overrun.
3348660	3352100	And this is why, you know, I think one of the reasons we joined forces with Google
3352300	3356860	back in 2014 was I think the entirety of Google and Alphabet, not just brain
3356860	3360340	and DeepMind, take these questions very seriously of responsibility.
3360340	3364500	And, you know, I kind of mantra is to try and be bold and responsible with these systems.
3364700	3368500	So, you know, I would I would classify as I'm obviously a huge techno optimist,
3368700	3372860	but I want us to be cautious with that, given the transformative power of what
3373060	3375860	we're bringing, bringing into the world, you know, collectively.
3375980	3379940	And I think it's important, you know, I think it's going to be one of the most
3380140	3382380	important technologies humanity will ever invent.
3382580	3386300	So we've got to put, you know, all our efforts into getting this right and be
3386500	3391460	thoughtful and sort of also humble about what we know and don't know about
3391660	3393980	the systems that are coming and the uncertainties around that.
3394180	3397780	And in my view, the only the only sensible approach when you have huge
3397980	3402220	uncertainty is to be sort of cautiously optimistic and use the scientific method
3402260	3405780	to try and have as much foresight and understanding about what's coming down
3405780	3408300	the line and the consequences of that before it happens.
3408500	3411780	You know, you don't want to be live A, B testing out in the world with these
3411980	3416380	very consequential systems, because unintended consequences may be maybe quite severe.
3416580	3421820	So, you know, I want us to move away as a as a field from a sort of move fast
3421820	3425060	and break things attitude, which is, you know, maybe serve the valley very well
3425260	3429460	in the past and obviously created important innovations.
3429620	3434340	But but I think in this case, you know, we want to be bold with the with the
3434340	3438020	positive things that it can do and make sure we realize things like medicine
3438220	3442500	and science and advancing all of those things whilst being, you know,
3442700	3447820	responsible and thoughtful with with as far as possible with with mitigating the risks.
3448020	3450660	Yeah, yeah. And that's why it seems like the responsible
3450660	3453740	scaling process or something like that is a very good empirical way to
3453740	3454980	pre-commit to these kinds of things.
3454980	3455860	Yes, exactly.
3455860	3458340	Yeah. And I'm curious if you have a sense of like, for example, when you're
3458420	3462300	doing these evaluations, if it turns out your next model could help a layperson
3462300	3466180	build a pandemic class or bio-weapon or something, how you would think, first of
3466380	3470140	all, of making sure those weights are secure so that that doesn't get out?
3470140	3473540	And second, what would have to be true for you to be comfortable deploying
3473540	3474660	that system? How comfortable?
3474660	3478340	Like, how would you make sure that that that lane capability isn't exposed?
3478340	3481940	Yeah. Well, first, I mean, you know, the secure model part, I think we've covered
3481940	3485340	with the cybersecurity and make sure that's well class and you're monitoring
3485340	3490300	all those things. I think if the capability was discovered like that
3490300	3495580	through red teaming or external testing by, you know, government institutes
3495780	3500060	or academia or whatever, independent testers, then we would have to fix
3500060	3502740	that loophole depending what it was, right?
3503220	3509740	If that required more a different kind of perhaps constitution or different
3509740	3513860	guardrails or more RLHF to avoid that or removing some training data,
3514420	3516700	they could, I mean, depending on what the problem is, I think there could be a
3516700	3518660	number of mitigations.
3518820	3522780	And so the first part is making sure you detect it ahead of time.
3522860	3526500	So that's about the right evaluations and right benchmarking and right and
3526500	3530900	right testing. And then the question is how one would fix that before, you know,
3530900	3534180	you deployed it. But I think it would need to be fixed before it was deployed
3534180	3537180	generally, for sure, if that was an exposure surface.
3537180	3539180	Right. Right. Final question.
3540380	3543660	You know, you've been thinking in terms of like the end goal of Asia at a time
3543660	3546460	when other people thought it was ridiculous in 2010, now that we're
3546460	3550300	seeing this like slow takeoff where we're actually seeing these like generalization
3550300	3554300	and intelligence, what is like the psychologically seeing this?
3554300	3555300	What has that been like?
3555300	3557260	Has it just like sort of priced into your role model?
3557260	3560300	So you like it's not new news for you or is it like actually just seeing it live?
3560300	3563820	You're like, wow, like this is something's like really changed or what does it feel
3563820	3568220	like? Yeah, well, for me, yes, it's already priced into my world, one of how
3568220	3570420	things were going to go, at least from the technology side.
3570420	3575100	But obviously, I didn't we didn't necessarily anticipate the general
3575100	3579580	public would be that interested this early in the sequence, right, of things
3579580	3584460	like maybe one could think of if we were to produce more, if say like a chat
3584460	3588740	GPT and chatbots hadn't got the kind of got the interest they'd ended up getting.
3588860	3591500	So I think it was quite surprising to everyone that people were ready to use
3591500	3595220	these things, even though they were lacking in certain directions, right?
3595220	3599220	Impressive, though they are, then we would have produced more specialized
3599220	3603460	systems, I think, built off of the main track, like Alpha Folds and Alpha goes
3603460	3605940	and and so on and our scientific work.
3606140	3612540	And then I think the general public maybe would have only paid attention
3612540	3615620	later down the road, where in a few years time, we have more generally
3615620	3617500	useful assistant type systems.
3617780	3619060	So that's been interesting.
3619060	3622380	So that's created a different type of environment that we're now all
3622380	3625260	operating in as a field.
3625420	3629100	So I mean, it's a little bit more chaotic because there's so many more things
3629100	3633300	going on and there's so much VC money going into it and everyone sort of
3633460	3637740	almost losing their minds over it, I think, and what I just the thing I
3637740	3641860	worry about is I want to make sure that as a field, we act responsibly
3641860	3645660	and thoughtfully and scientifically about this and use the scientific
3645660	3650540	method to approach this in a in a, as I said, an optimistic, but careful way.
3650660	3654340	And I think that's the I've always believed that's the right approach for
3654380	3659300	something like AI, and I just hope that doesn't get lost in this huge rush.
3659380	3660060	Sure, sure.
3660220	3661580	Well, I think that's a great place to close.
3661620	3662540	Dennis, so much thanks to you.
3662540	3664220	Thank you so much for your time and for coming on the podcast.
3664220	3664500	Thanks.
3664500	3665260	It's been a real pleasure.
3667420	3669820	Hey, everybody, I hope we enjoyed that episode.
3670380	3674060	As always, the most helpful thing you can do is to share the podcast,
3674380	3677740	send it to people you think might enjoy it, put it in Twitter, your group chats,
3677740	3681300	et cetera, just splits the world, appreciate your listening.
3681340	3682300	I'll see you next time.
3682460	3682900	Cheers.
